<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 5]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Checkmate: Zero-Overhead Model Checkpointing via Network Gradient Replication](https://arxiv.org/abs/2507.13522)
*Ankit Bhardwaj,Weiyang Wang,Jeremy Carin,Adam Belay,Manya Ghobadi*

Main category: cs.DC

TL;DR: Checkmate 是一种无需训练减速的 DNN 训练检查点系统，通过梯度多播实现高效检查点，显著提升检查点频率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统检查点方法需要在训练暂停时复制模型状态，存在检查点频率与失败成本之间的权衡，Checkmate 旨在避免这种权衡。

Method: 利用数据并行训练中的梯度信息，通过多播抽象将梯度同时传递到基于 CPU 的影子集群，影子集群通过应用梯度维护模型副本作为检查点。

Result: Checkmate 实现了与理想无检查点基线相当的训练吞吐量，检查点频率比现有系统高 5 至 34.5 倍，每次失败的重复工作量减少 80% 至 97.1%。

Conclusion: Checkmate 系统通过创新的多播抽象和梯度应用方法，实现了无需训练减速的每迭代检查点，显著提高了检查点频率并减少了重复工作量。

Abstract: This paper presents Checkmate, a system that enables per-iteration
checkpointing in DNN training without any training slowdown. The traditional
approach to checkpointing requires a pause in training to copy model states to
a separate location, allowing the state to be restored in the event of failure.
This approach fundamentally has a tradeoff between the frequency of checkpoints
and the cost of a failure. We avoid this tradeoff; our key insight is that in
data-parallel training, all information necessary to create a checkpoint
already exists in the network as gradients. Our core contribution is a new
multicast abstraction that simultaneously delivers gradients to a separate
CPU-based shadow cluster. The shadow maintains a checkpoint by applying those
gradients to a copy of the model. Our evaluation shows that Checkmate performs
per-iteration checkpointing with training throughput comparable to an ideal
no-checkpoint baseline. Checkmate achieves 5 to 34.5x more frequent
checkpointing compared to state-of-the-art checkpointing systems, resulting in
80% to 97.1% reduction in repeated work per failure. At the same checkpointing
frequency, Checkmate delivers 1.3x to 6.5x throughput compared to other
systems.

</details>


### [2] [Leveraging Multi-Instance GPUs through moldable task scheduling](https://arxiv.org/abs/2507.13601)
*Jorge Villarrubia,Luis Costero,Francisco D. Igual,Katzalin Olcoz*

Main category: cs.DC

TL;DR: 本文提出FAR算法，通过三阶段方法优化NVIDIA MIG的动态重新配置任务调度，显著降低了makespan，并展示了MIG技术的研究潜力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索NVIDIA MIG技术通过动态重新配置进行可塑性任务调度的未开发潜力，并提出在MIG约束下多任务执行的makespan最小化问题。

Method: 我们提出了FAR算法，一个分三阶段解决问题的方法。第一阶段基于经典的任务可塑性方法，第二阶段结合了最长处理时间优先和列表调度，并引入了针对MIG约束的新颖重新分区树启发式方法，第三阶段通过任务移动和交换进行局部搜索。

Result: 实验结果表明，FAR算法在真实世界实验中表现优异，makespan相对于最优解的比率不超过1.22倍（已知基准测试）和1.10倍（受真实内核启发的合成输入）。

Conclusion: 本文展示了MIG技术的研究潜力，并提出了该领域未来工作中有用的指标、工作负载特征和评估技术。

Abstract: NVIDIA MIG (Multi-Instance GPU) allows partitioning a physical GPU into
multiple logical instances with fully-isolated resources, which can be
dynamically reconfigured. This work highlights the untapped potential of MIG
through moldable task scheduling with dynamic reconfigurations. Specifically,
we propose a makespan minimization problem for multi-task execution under MIG
constraints. Our profiling shows that assuming monotonicity in task work with
respect to resources is not viable, as is usual in multicore scheduling.
Relying on a state-of-the-art proposal that does not require such an
assumption, we present FAR, a 3-phase algorithm to solve the problem. Phase 1
of FAR builds on a classical task moldability method, phase 2 combines Longest
Processing Time First and List Scheduling with a novel repartitioning tree
heuristic tailored to MIG constraints, and phase 3 employs local search via
task moves and swaps. FAR schedules tasks in batches offline, concatenating
their schedules on the fly in an improved way that favors resource reuse.
Excluding reconfiguration costs, the List Scheduling proof shows an
approximation factor of 7/4 on the NVIDIA A30 model. We adapt the technique to
the particular constraints of an NVIDIA A100/H100 to obtain an approximation
factor of 2. Including the reconfiguration cost, our real-world experiments
reveal a makespan with respect to the optimum no worse than 1.22x for a
well-known suite of benchmarks, and 1.10x for synthetic inputs inspired by real
kernels. We obtain good experimental results for each batch of tasks, but also
in the concatenation of batches, with large improvements over the
state-of-the-art and proposals without GPU reconfiguration. Beyond the
algorithm, the paper demonstrates the research potential of the MIG technology
and suggests useful metrics, workload characterizations and evaluation
techniques for future work in this field.

</details>


### [3] [DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training](https://arxiv.org/abs/2507.13833)
*Zhixin Wang,Tianyi Zhou,Liming Liu,Ao Li,Jiarui Hu,Dian Yang,Jinlong Hou,Siyuan Feng,Yuan Cheng,Yuan Qi*

Main category: cs.DC

TL;DR: DistFlow 是一种新型全分布式强化学习框架，通过多控制器架构解决扩展瓶颈，实现高效、灵活的大规模训练。


<details>
  <summary>Details</summary>
Motivation: 大规模强化学习中微小的负载不平衡会导致显著的瓶颈，限制系统扩展性。

Method: 采用多控制器范式，将数据传输和执行任务分配给所有工作节点，消除中心节点，使每个工作节点能独立运行。

Result: DistFlow 实现了近线性的扩展性，端到端吞吐量比现有最佳框架提高了7倍。

Conclusion: DistFlow 通过其全分布式架构有效解决了大规模强化学习中的负载均衡问题，实现了近线性的扩展性和显著的效率提升。

Abstract: Reinforcement learning (RL) has become the pivotal post-training technique
for large language model. Effectively scaling reinforcement learning is now the
key to unlocking advanced reasoning capabilities and ensuring safe,
goal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually
employ a hybrid-controller architecture where a single-controller dispatches
the overall execution logic and manages overall data transfer and the
multi-controller executes distributed computation. For large-scale
reinforcement learning, minor load imbalances can introduce significant
bottlenecks, ultimately constraining the scalability of the system. To address
this limitation, we introduce DistFlow, a novel, fully distributed RL framework
designed to break scaling barrier. We adopt a multi-controller paradigm that
dispatches data transfer and execution tasks to all workers, which eliminates
the centralized node. This allows each worker to operate independently, leading
to near-linear scalability up to thousands of GPUs and dramatic efficiency
gains. Furthermore, our architecture decouples resource configuration from
execution logic, allowing each worker to have a unique execution flow, offering
significant flexibility for rapid and cost-effective algorithmic
experimentation. Extensive experiments show that DistFlow achieves excellent
linear scalability and up to a 7x end-to-end throughput improvement over
state-of-the-art (SOTA) frameworks.

</details>


### [4] [Edge Intelligence with Spiking Neural Networks](https://arxiv.org/abs/2507.14069)
*Shuiguang Deng,Di Yu,Changze Lv,Xin Du,Linshan Jiang,Xiaofan Zhao,Wentao Tong,Xiaoqing Zheng,Weijia Fang,Peng Zhao,Gang Pan,Schahram Dustdar,Albert Y. Zomaya*

Main category: cs.DC

TL;DR: 本文是关于基于脉冲神经网络（SNNs）的边缘智能（EdgeSNNs）的首篇全面综述，探讨了其在设备上学习、推理和安全性方面的潜力，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 人工智能与边缘计算的融合激发了对资源受限设备上直接提供智能服务的兴趣。传统的深度学习模型存在延迟、带宽消耗和隐私问题，而脑启发计算（如SNNs）通过模拟生物神经元动态提供了一种低功耗、事件驱动的计算替代方案。

Method: 本文提供了基于SNNs的边缘智能（EdgeSNNs）的系统分类，包括神经元模型、学习算法和支持的硬件平台，并深入讨论了三个实际考虑因素：轻量级SNN模型的设备上推理、非平稳数据条件下的资源感知训练和更新，以及安全和隐私保护问题。

Result: 本文介绍了EdgeSNNs的潜力，并提出了双轨基准测试策略以支持公平比较和硬件感知优化。

Conclusion: 这篇综述旨在弥合脑启发学习与实际边缘部署之间的差距，为研究人员和从业者提供了关于EdgeSNNs当前进展、开放挑战和未来研究方向的重要参考。

Abstract: The convergence of artificial intelligence and edge computing has spurred
growing interest in enabling intelligent services directly on
resource-constrained devices. While traditional deep learning models require
significant computational resources and centralized data management, the
resulting latency, bandwidth consumption, and privacy concerns have exposed
critical limitations in cloud-centric paradigms. Brain-inspired computing,
particularly Spiking Neural Networks (SNNs), offers a promising alternative by
emulating biological neuronal dynamics to achieve low-power, event-driven
computation. This survey provides a comprehensive overview of Edge Intelligence
based on SNNs (EdgeSNNs), examining their potential to address the challenges
of on-device learning, inference, and security in edge scenarios. We present a
systematic taxonomy of EdgeSNN foundations, encompassing neuron models,
learning algorithms, and supporting hardware platforms. Three representative
practical considerations of EdgeSNN are discussed in depth: on-device inference
using lightweight SNN models, resource-aware training and updating under
non-stationary data conditions, and secure and privacy-preserving issues.
Furthermore, we highlight the limitations of evaluating EdgeSNNs on
conventional hardware and introduce a dual-track benchmarking strategy to
support fair comparisons and hardware-aware optimization. Through this study,
we aim to bridge the gap between brain-inspired learning and practical edge
deployment, offering insights into current advancements, open challenges, and
future research directions. To the best of our knowledge, this is the first
dedicated and comprehensive survey on EdgeSNNs, providing an essential
reference for researchers and practitioners working at the intersection of
neuromorphic computing and edge intelligence.

</details>


### [5] [Shipwright: Proving liveness of distributed systems with Byzantine participants](https://arxiv.org/abs/2507.14080)
*Derek Leung,Nickolai Zeldovich,Frans Kaashoek*

Main category: cs.DC

TL;DR: Shipwright框架通过三种新技术验证了PBFT的活性，成功实现并测试了单个日志条目协议的可执行版本。


<details>
  <summary>Details</summary>
Motivation: 在去中心化系统中确保活性（如PBFT）至关重要，但现有工作未能验证可执行PBFT实现的活性。

Method: Shipwright引入了三种技术：支持恶意参与者的形式化推理、模块化分解系统与证明、以及支持加密签名的合理推理。

Result: 通过Shipwright框架，实现了PBFT中单个日志条目协议的验证，并转化为Go语言可执行实现。

Conclusion: Shipwright成功验证了PBFT中关于单个日志条目协议的初始原型，并展示了其在常见情况和故障场景下的操作和活性。

Abstract: Ensuring liveness in a decentralized system, such as PBFT, is critical,
because there may not be any single administrator that can restart the system
if it encounters a liveness bug. At the same time, liveness is challenging to
achieve because any single participant could be malicious, and yet the overall
system must make forward progress. While verification is a promising approach
for ensuring the absence of bugs, no prior work has been able to verify
liveness for an executable implementation of PBFT.
  Shipwright is a verification framework for proving correctness and liveness
of distributed systems where some participants might be malicious. Shipwright
introduces three techniques that enable formal reasoning about decentralized
settings with malicious participants, allow developers to decompose their
system and proof in a modular fashion into sub-protocols and sub-proofs, and
support sound reasoning about cryptographic signatures that may be embedded in
messages. We used Shipwright to implement and verify an initial prototype of
agreement on a single log entry in PBFT (with a few limitations) and translate
it to an executable implementation in Go. We experimentally demonstrate its
operation and liveness both in the common case and in several failure
scenarios.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [6] [Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica](https://arxiv.org/abs/2507.13476)
*Jaber Daneshamooz,Jessica Nguyen,William Chen,Sanjay Chandrasekaran,Satyandra Guthula,Ankit Gupta,Arpit Gupta,Walter Willinger*

Main category: cs.NI

TL;DR: NetReplica通过生成真实且可控的训练数据，显著提升网络机器学习模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在网络领域中因环境变化导致的领域适应问题。

Method: NetReplica将网络建模为具有特定属性的瓶颈链路集合，利用生产网络痕迹实现真实性，并通过细粒度的链路属性控制实现可控性。

Result: 使用NetReplica增强的数据集训练的模型在挑战性网络条件下传输时间预测误差降低了47%。

Conclusion: NetReplica通过生成具有真实性和可控性的训练数据集，显著提升了机器学习模型在网络领域的泛化能力，为解决领域适应问题迈出了重要一步。

Abstract: Machine learning models in networking suffer from the domain adaptation
problem; models trained in one domain often fail when deployed in different
production environments. This paper presents the design and implementation of
NetReplica, a system that addresses this challenge by generating training
datasets with two critical properties: realism in protocol dynamics and
controllability of network conditions. NetReplica models networks as
collections of bottleneck links with specific attributes, achieves realism by
leveraging production network traces, and enables controllability through fine
grained control knobs for each link attribute. Our evaluation using Puffer
demonstrates that NetReplica not only matches existing data characteristics but
generates realistic samples that are underrepresented in or absent from Puffer
data. Models trained on NetReplica augmented datasets show substantially
improved generalizability, reducing transmission time prediction error by up to
47% for challenging network conditions compared to models trained solely on
Puffer data. This work represents a significant step toward solving the domain
adaptation problem that has limited the effectiveness of ML based networking
systems.

</details>


### [7] [CARTS: Cooperative and Adaptive Resource Triggering and Stitching for 5G ISAC](https://arxiv.org/abs/2507.13676)
*Cheng Jiang,Yihe Yan,Yanxiang Wang,Jiawei Hu,Chun Tung Chou,Wen Hu*

Main category: cs.NI

TL;DR: CARTS是一种自适应5G上行感知方案，通过融合DMRS和SRS的CSI流，提升ISAC性能，支持更多用户且无需额外资源。


<details>
  <summary>Details</summary>
Motivation: 现代5G网络中，上行CSI来自DMRS和SRS，但当前基站实现将它们视为独立信息流，限制了CSI更新频率和感知机会。CARTS旨在融合这两个CSI流以提升性能。

Method: CARTS 提出了一种新颖的信道拼接和补偿方法，整合了来自DMRS和SRS的异步CSI估计，以及实时SRS触发算法。

Result: CARTS显著提升了可扩展性，实现了0.167的信道估计误差和85厘米的UE跟踪精度，支持的用户数是仅使用周期性SRS基准的两倍。

Conclusion: CARTS 提供了一种符合标准的实用解决方案，通过融合DMRS和SRS的CSI流，提高了ISAC的CSI可用性，且无需额外的无线资源。

Abstract: This paper presents CARTS, an adaptive 5G uplink sensing scheme designed to
provide Integrated Sensing and Communication (ISAC) services. The performance
of both communication and sensing fundamentally depends on the availability of
accurate and up-to-date channel state information (CSI). In modern 5G networks,
uplink CSI is derived from two reference signals: the demodulation reference
signal (DMRS) and the sounding reference signal (SRS). However, current base
station implementations treat these CSI measurements as separate information
streams. The key innovation of CARTS is to fuse these two CSI streams, thereby
increasing the frequency of CSI updates and extending sensing opportunities to
more users. CARTS addresses two key challenges: (i) a novel channel stitching
and compensation method that integrates asynchronous CSI estimates from DMRS
and SRS, despite their different time and frequency allocations, and (ii) a
real-time SRS triggering algorithm that complements the inherently
uncontrollable DMRS schedule, ensuring sufficient and non-redundant sensing
opportunities for all users. Our trace-driven evaluation shows that CARTS
significantly improves scalability, achieving a channel estimation error (NMSE)
of 0.167 and UE tracking accuracy of 85 cm while supporting twice the number of
users as a periodic SRS-only baseline with similar performance. By
opportunistically combining DMRS and SRS, CARTS therefore provides a practical,
standard-compliant solution to improve CSI availability for ISAC without
requiring additional radio resources.

</details>


### [8] [ATRO: A Fast Solver-Free Algorithm for Topology and Routing Optimization of Reconfigurable Datacenter Networks](https://arxiv.org/abs/2507.13717)
*Yingming Mao,Qiaozhu Zhai,Zhen Yao,Xia Zhu,Ximeng Liu,Xinchi Han*

Main category: cs.NI

TL;DR: ATRO是一个无需求解器的框架，通过交替优化拓扑和路由，显著提升可重构数据中心网络的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 面对可重构数据中心网络（DCNs）的规模和复杂性增加，现有方法难以平衡解的质量和运行效率。

Method: ATRO框架通过交替进行拓扑优化（TO）和路由优化（RO），利用加速二分搜索方法（ABSM）高效解决TO子问题，并使用现有流量工程加速器解决RO。

Result: ATRO在单跳和多跳场景中均表现出色，显著降低了最大链路利用率（MLU），并在多样化的DCNs中验证了其可扩展性和鲁棒性。

Conclusion: ATRO在单跳场景下达到全局最优，并在多跳设置中显著优于基线方法，表现出良好的可扩展性和鲁棒性。

Abstract: The growing scale and complexity of reconfigurable data center networks
(DCNs) demand more scalable and efficient algorithms for computing logical
topologies and routing. Reconfigurable DCNs typically operate in two modes:
one-hop configurations that require frequent topology optimization (TO), and
multi-hop scenarios that involve joint topology and routing optimization (TRO).
In both cases, the combinatorial nature of topology decisions makes it
difficult for existing methods to balance solution quality and runtime
efficiency. To address this, we introduce Alternating Topology and Routing
Optimization (ATRO), a solver-free framework that alternates between TO and
routing optimization (RO). This decomposition exploits two key insights: first,
each alternating update step monotonically reduces maximum link utilization
(MLU), ensuring consistent performance improvement across iterations; second,
the TO subproblem, equivalent to one-hop optimization, exhibits a monotonic
structure that enables optimal solutions via an efficient Accelerated Binary
Search Method (ABSM). To preserve the solver-free design, RO is solved using
existing Traffic Engineering accelerators. ATRO attains the global optimum in
one-hop scenarios and significantly outperforms baselines in multi-hop settings
in terms of both runtime and solution quality. Evaluations confirm its
scalability and robustness across diverse DCNs.

</details>


### [9] [On the Trade-Off Between Sum-Rate and Energy Efficiency through the Convergence of HAPS and Active RIS Technologies](https://arxiv.org/abs/2507.13889)
*Bilal Karaman,Ilhan Basturk,Ferdi Kara,Metin Ozturk,Sezai Taskin,Halil Yanikomeroglu*

Main category: cs.NI

TL;DR: Active RIS with HAPS enhances NTN performance by addressing path loss and double fading, outperforming passive RIS in QoS and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: The severe path loss and double fading in long-distance HAPS links make passive RIS architectures less effective, prompting the investigation of active RIS due to its inherent signal amplification capabilities.

Method: The study formulates a sum-rate maximization problem to jointly optimize power allocation and RIS element assignment for ground UEs supported by a HAPS-based active RIS-assisted communication system. It also explores sub-connected active RIS architectures to reduce power consumption and hardware complexity.

Result: Simulation results show that active RIS configurations significantly outperform passive RIS in QoS, with sub-connected schemes demonstrating superior energy efficiency under practical power constraints, despite fully-connected architectures achieving the highest throughput.

Conclusion: This paper highlights the potential of active RIS-enabled HAPS systems to meet the demands of beyond-cellular coverage and green networking, demonstrating superior energy efficiency under practical power constraints.

Abstract: This paper investigates the integration of active reconfigurable intelligent
surfaces (RIS) relay with high-altitude platform stations (HAPS) to enhance
non-terrestrial network (NTN) performance in next-generation wireless systems.
While prior studies focused on passive RIS architectures, the severe path loss
and double fading in long-distance HAPS links make active RIS a more suitable
alternative due to its inherent signal amplification capabilities. We formulate
a sum-rate maximization problem to jointly optimize power allocation and RIS
element assignment for ground user equipments (UEs) supported by a HAPS-based
active RIS-assisted communication system. To reduce power consumption and
hardware complexity, several sub-connected active RIS architectures are also
explored. Simulation results reveal that active RIS configurations
significantly outperform passive RIS in terms of quality of service (QoS).
Moreover, although fully-connected architectures achieve the highest
throughput, sub-connected schemes demonstrate superior energy efficiency under
practical power constraints. These findings highlight the potential of active
RIS-enabled HAPS systems to meet the growing demands of beyond-cellular
coverage and green networking.

</details>


### [10] [Preprint: Did I Just Browse A Website Written by LLMs?](https://arxiv.org/abs/2507.13933)
*Sichang "Steven" He,Ramesh Govindan,Harsha V. Madhyastha*

Main category: cs.NI

TL;DR: 本文提出了一种检测LLM-dominant网站的高效方法，并在真实数据中验证了其100%的准确率，发现此类网站在网络中的占比和排名正在上升。


<details>
  <summary>Details</summary>
Motivation: 由于LLM生成的内容可能不可靠且不道德，且网站很少披露此类内容，人类读者难以区分，因此需要开发可靠的LLM-dominant内容检测器。

Method: 提出了一种高度可靠、可扩展的流程，通过基于多个散文式页面的LLM文本检测器输出对整个网站进行分类。

Result: 在收集的两个不同真实数据集中（共120个网站），检测器达到了100%的准确率。在实际应用中，检测到搜索引擎结果和Common Crawl存档中各1万个网站中有相当一部分是LLM-dominant。

Conclusion: LLM-dominant内容在搜索引擎结果和Common Crawl存档中占据相当比例，且增长迅速，排名靠前，这对终端用户和整个网络生态系统的影响值得关注。

Abstract: Increasingly, web content is automatically generated by large language models
(LLMs) with little human input. We call this "LLM-dominant" content. Since LLMs
plagiarize and hallucinate, LLM-dominant content can be unreliable and
unethical. Yet, websites rarely disclose such content, and human readers
struggle to distinguish it. Thus, we must develop reliable detectors for
LLM-dominant content. However, state-of-the-art LLM detectors are insufficient,
because they perform well mainly on clean, prose-like text, while web content
has complex markup and diverse genres.
  We propose a highly reliable, scalable pipeline that classifies entire
websites. Instead of naively classifying text extracted from each page, we
classify each site based on an LLM text detector's outputs of multiple
prose-like pages. We train and evaluate our detector by collecting 2 distinct
ground truth datasets totaling 120 sites, and obtain 100% accuracies testing
across them. In the wild, we detect a sizable portion of sites as LLM-dominant
among 10k sites in search engine results and 10k in Common Crawl archives. We
find LLM-dominant sites are growing in prevalence and rank highly in search
results, raising questions about their impact on end users and the overall Web
ecosystem.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination](https://arxiv.org/abs/2507.13511)
*Nabil Abdelaziz Ferhat Taleb,Abdolazim Rezaei,Raj Atulkumar Patel,Mehdi Sookhak*

Main category: cs.AI

TL;DR: GraphTrafficGPT是一种基于图的LLM交通管理架构，通过并行任务处理和动态资源分配，显著提升效率和扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有链式系统（如TrafficGPT）存在顺序任务执行、高token消耗和扩展性差的问题，难以应对复杂的现实交通场景。

Method: 提出GraphTrafficGPT，一种基于图的任务协调架构，通过Brain Agent分解查询、构建依赖图，并协调多个专业代理进行数据处理。

Result: 实验显示，GraphTrafficGPT相比TrafficGPT在token消耗减少50.2%，响应延迟降低19.0%，多查询效率提升23.0%。

Conclusion: GraphTrafficGPT通过图基架构显著提升了LLM在交通管理中的效率，减少了50.2%的token消耗和19.0%的响应延迟，同时支持多查询并行处理。

Abstract: Large Language Models (LLMs) offer significant promise for intelligent
traffic management; however, current chain-based systems like TrafficGPT are
hindered by sequential task execution, high token usage, and poor scalability,
making them inefficient for complex, real-world scenarios. To address these
limitations, we propose GraphTrafficGPT, a novel graph-based architecture,
which fundamentally redesigns the task coordination process for LLM-driven
traffic applications. GraphTrafficGPT represents tasks and their dependencies
as nodes and edges in a directed graph, enabling efficient parallel execution
and dynamic resource allocation. The main idea behind the proposed model is a
Brain Agent that decomposes user queries, constructs optimized dependency
graphs, and coordinates a network of specialized agents for data retrieval,
analysis, visualization, and simulation. By introducing advanced context-aware
token management and supporting concurrent multi-query processing, the proposed
architecture handles interdependent tasks typical of modern urban mobility
environments. Experimental results demonstrate that GraphTrafficGPT reduces
token consumption by 50.2% and average response latency by 19.0% compared to
TrafficGPT, while supporting simultaneous multi-query execution with up to
23.0% improvement in efficiency.

</details>


### [12] [PrefPalette: Personalized Preference Modeling with Latent Attributes](https://arxiv.org/abs/2507.13541)
*Shuyue Stella Li,Melanie Sclar,Hunter Lang,Ansong Ni,Jacqueline He,Puxin Xu,Andrew Cohen,Chan Young Park,Yulia Tsvetkov,Asli Celikyilmaz*

Main category: cs.AI

TL;DR: PrefPalette是一个分解偏好为属性维度并基于社区动态加权的框架，显著提升预测准确率并提供可解释的社区偏好洞察。


<details>
  <summary>Details</summary>
Motivation: 当前偏好模型通常将人类判断视为黑箱，而个性化AI系统需要理解用户偏好的原因。PrefPalette旨在分解偏好为属性维度，并以人类可解释的方式为不同社交社区定制偏好预测。

Method: PrefPalette采用多属性决策认知科学原理，通过（1）可扩展的反事实属性合成步骤生成合成训练数据以隔离单个属性效应，和（2）基于注意力的偏好建模，学习不同社交社区如何动态加权这些属性。

Result: 在Reddit的45个社交社区评估中，PrefPalette的平均预测准确率比GPT-4o高出46.6%，并揭示了社区特定的偏好特征。

Conclusion: PrefPalette通过建模人类判断的属性中介结构，不仅提供了更优的偏好预测，还提供了透明、可解释的洞察，为更值得信赖、价值感知的个性化应用迈出了第一步。

Abstract: Personalizing AI systems requires understanding not just what users prefer,
but the reasons that underlie those preferences - yet current preference models
typically treat human judgment as a black box. We introduce PrefPalette, a
framework that decomposes preferences into attribute dimensions and tailors its
preference prediction to distinct social community values in a
human-interpretable manner. PrefPalette operationalizes a cognitive science
principle known as multi-attribute decision making in two ways: (1) a scalable
counterfactual attribute synthesis step that involves generating synthetic
training data to isolate for individual attribute effects (e.g., formality,
humor, cultural values), and (2) attention-based preference modeling that
learns how different social communities dynamically weight these attributes.
This approach moves beyond aggregate preference modeling to capture the diverse
evaluation frameworks that drive human judgment. When evaluated on 45 social
communities from the online platform Reddit, PrefPalette outperforms GPT-4o by
46.6% in average prediction accuracy. Beyond raw predictive improvements,
PrefPalette also shed light on intuitive, community-specific profiles:
scholarly communities prioritize verbosity and stimulation, conflict-oriented
communities value sarcasm and directness, and support-based communities
emphasize empathy. By modeling the attribute-mediated structure of human
judgment, PrefPalette delivers both superior preference modeling and
transparent, interpretable insights, and serves as a first step toward more
trustworthy, value-aware personalized applications.

</details>


### [13] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: CUDA-L1是一个基于强化学习的自动化CUDA优化框架，显著提升性能并展示跨GPU架构的可移植性，无需人类干预。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的快速发展，GPU计算资源需求呈指数级增长，亟需自动化CUDA优化策略。

Method: 引入CUDA-L1，一个基于强化学习的自动化框架，用于CUDA优化。

Result: 在NVIDIA A100上训练，CUDA-L1在KernelBench的250个CUDA内核上平均加速17.7倍，峰值加速达449倍，并展示了出色的跨GPU架构可移植性。

Conclusion: CUDA-L1通过强化学习成功将性能不佳的大型语言模型转化为有效的CUDA优化器，仅基于加速奖励信号，无需人类专业知识。这一范式为CUDA操作的自动化优化开辟了可能性，有望显著提升GPU效率并缓解GPU计算资源的压力。

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>


### [14] [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
*Eduardo C. Garrido-Merchán,Cristina Puente*

Main category: cs.AI

TL;DR: 本文提出了一种结合LLMs和符号系统的透明方法，通过结构化提示和专家验证生成可靠的知识库，为敏感领域的AI应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成看似连贯信息时存在幻觉或自信生成错误事实的问题，需要一种可控、透明的方法来开发专家系统。

Method: 通过限制领域并采用结构化的提示提取方法，将知识转化为Prolog符号表示，并由人类专家验证和修正。

Result: 定量和定性实验表明，生成的知识库在事实准确性和语义连贯性方面表现优异。

Conclusion: 本文提出了一种结合大型语言模型（LLMs）和符号系统的透明混合解决方案，为敏感领域提供了可靠的人工智能应用基础。

Abstract: The development of large language models (LLMs) has successfully transformed
knowledge-based systems such as open domain question nswering, which can
automatically produce vast amounts of seemingly coherent information. Yet,
those models have several disadvantages like hallucinations or confident
generation of incorrect or unverifiable facts. In this paper, we introduce a
new approach to the development of expert systems using LLMs in a controlled
and transparent way. By limiting the domain and employing a well-structured
prompt-based extraction approach, we produce a symbolic representation of
knowledge in Prolog, which can be validated and corrected by human experts.
This approach also guarantees interpretability, scalability and reliability of
the developed expert systems. Via quantitative and qualitative experiments with
Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic
coherence on our generated knowledge bases. We present a transparent hybrid
solution that combines the recall capacity of LLMs with the precision of
symbolic systems, thereby laying the foundation for dependable AI applications
in sensitive domains.

</details>


### [15] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 论文探讨了关系型学习在AI中的重要性，指出其当前受限的原因，并呼吁加强研究以实现其潜力。


<details>
  <summary>Details</summary>
Motivation: 当前AI技术主要关注像素和文字等感知数据的建模，而忽视了实体及其关系的直接建模，这与现实世界的构成方式不符。论文旨在揭示这一差距并推动关系型学习的发展。

Method: 通过分析现有AI技术的局限性（如主要处理文本和图像数据）和关系型学习的潜力，论文探讨了关系型学习未能广泛应用的原因。

Result: 论文指出关系型学习在受限关系中已有成功应用，但要实现其广泛普及，还需解决现有挑战。

Conclusion: 该论文呼吁应更重视关系型学习的研究与应用，以弥补当前AI领域在处理实体及其关系上的不足，并提出了推动其发展的必要措施。

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [16] [BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety](https://arxiv.org/abs/2507.13625)
*Yuxin Zhang,Xi Wang,Mo Hu,Zhenyu Zhang*

Main category: cs.AI

TL;DR: BifrostRAG通过双图混合检索机制显著提升了复杂法规文本的多跳问答性能。


<details>
  <summary>Details</summary>
Motivation: 安全法规的信息检索和问答对自动化施工合规性检查至关重要，但法规文本的语言和结构复杂性阻碍了传统RAG系统处理多跳查询。

Method: 引入BifrostRAG：一种双图RAG集成系统，通过实体网络图建模语言关系，通过文档导航图建模文档结构，结合图遍历和向量语义搜索的混合检索机制。

Result: 在多跳问题数据集上的评估显示，BifrostRAG的精确度为92.8%，召回率为85.5%，F1得分为87.3%，显著优于当前主流方法。

Conclusion: BifrostRAG的双图混合检索机制为知识密集型工程领域中的复杂技术文档导航提供了可转移的蓝图。

Abstract: Information retrieval and question answering from safety regulations are
essential for automated construction compliance checking but are hindered by
the linguistic and structural complexity of regulatory text. Many
compliance-related queries are multi-hop, requiring synthesis of information
across interlinked clauses. This poses a challenge for traditional
retrieval-augmented generation (RAG) systems. To overcome this, we introduce
BifrostRAG: a dual-graph RAG-integrated system that explicitly models both
linguistic relationships (via an Entity Network Graph) and document structure
(via a Document Navigator Graph). This architecture powers a hybrid retrieval
mechanism that combines graph traversal with vector-based semantic search,
enabling large language models to reason over both the meaning and the
structure of the text. Evaluation on a multi-hop question dataset shows that
BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1
score of 87.3 percent. These results significantly outperform vector-only and
graph-only RAG baselines that represent current leading approaches. Error
analysis further highlights the comparative advantages of our hybrid method
over single-modality RAGs. These findings establish BifrostRAG as a robust
knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid
retrieval mechanism offers a transferable blueprint for navigating complex
technical documents across knowledge-intensive engineering domains.

</details>


### [17] [Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks](https://arxiv.org/abs/2507.13651)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 研究验证了基于最终答案的自动错误诊断方法在智能辅导系统中的有效性，能显著减少组合爆炸问题，诊断准确率高达97%。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统在学生逐步完成任务时提供支持，但当学生将多个步骤合并为一个步骤时，可能的路径组合爆炸，使错误诊断变得困难。基于最终答案的诊断可以减少组合爆炸的问题。

Method: 研究设计了一个服务，用于在学生合并多个步骤时提供错误规则诊断。该方法通过自动完成任务并根据任务解决策略诊断解决方案来诊断中间输入。

Result: 在二次方程求解的数据集（n=1939）中，最终答案评估可诊断29.4%的步骤。与教师诊断的对比（n=115）显示，97%的情况下诊断结果一致。

Conclusion: 该研究探索了基于最终答案的自动错误诊断方法的潜力，并验证了其有效性，为后续研究奠定了基础。

Abstract: Many intelligent tutoring systems can support a student in solving a stepwise
task. When a student combines several steps in one step, the number of possible
paths connecting consecutive inputs may be very large. This combinatorial
explosion makes error diagnosis hard. Using a final answer to diagnose a
combination of steps can mitigate the combinatorial explosion, because there
are generally fewer possible (erroneous) final answers than (erroneous)
solution paths. An intermediate input for a task can be diagnosed by
automatically completing it according to the task solution strategy and
diagnosing this solution. This study explores the potential of automated error
diagnosis based on a final answer. We investigate the design of a service that
provides a buggy rule diagnosis when a student combines several steps. To
validate the approach, we apply the service to an existing dataset (n=1939) of
unique student steps when solving quadratic equations, which could not be
diagnosed by a buggy rule service that tries to connect consecutive inputs with
a single rule. Results show that final answer evaluation can diagnose 29,4% of
these steps. Moreover, a comparison of the generated diagnoses with teacher
diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the
cases. These results can be considered a basis for further exploration of the
approach.

</details>


### [18] [Combining model tracing and constraint-based modeling for multistep strategy diagnoses](https://arxiv.org/abs/2507.13652)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 结合模型追踪和约束建模，提出新方法以诊断学生多步组合输入，实验验证其与教师判断完全一致。


<details>
  <summary>Details</summary>
Motivation: 解决学生在多步任务中组合步骤时传统方法（模型追踪和约束建模）无法有效诊断的问题。

Method: 提出了一个结合模型追踪和约束建模的方法，通过将约束定义为学生输入与策略步骤共有的属性，实现对多步组合输入的诊断。

Result: 在2136个二次方程求解步骤的数据集上验证，系统诊断与教师编码在140个随机样本中完全一致。

Conclusion: 系统诊断与教师编码在所有140个学生步骤中完全一致，验证了该方法的有效性。

Abstract: Model tracing and constraint-based modeling are two approaches to diagnose
student input in stepwise tasks. Model tracing supports identifying consecutive
problem-solving steps taken by a student, whereas constraint-based modeling
supports student input diagnosis even when several steps are combined into one
step. We propose an approach that merges both paradigms. By defining
constraints as properties that a student input has in common with a step of a
strategy, it is possible to provide a diagnosis when a student deviates from a
strategy even when the student combines several steps. In this study we explore
the design of a system for multistep strategy diagnoses, and evaluate these
diagnoses. As a proof of concept, we generate diagnoses for an existing dataset
containing steps students take when solving quadratic equations (n=2136). To
compare with human diagnoses, two teachers coded a random sample of deviations
(n=70) and applications of the strategy (n=70). Results show that that the
system diagnosis aligned with the teacher coding in all of the 140 student
steps.

</details>


### [19] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian,Xiaoyuan Ren,Zihao Wang,Onat Gungor,Xiaofan Yu,Tajana Rosing*

Main category: cs.AI

TL;DR: DailyLLM 是一种轻量级 LLM 框架，通过整合多维度上下文信息，显著提升活动日志生成的精度和速度。


<details>
  <summary>Details</summary>
Motivation: 现有的活动日志生成方法在准确性、效率和语义丰富度方面存在显著局限性，而大型语言模型（LLM）的语义理解和生成能力为解决这些问题提供了新机会。

Method: DailyLLM 引入了一种轻量级的基于大型语言模型（LLM）的框架，通过结构化提示和高效特征提取实现高级活动理解。

Result: 实验表明，DailyLLM 在日志生成 BERTScore 精度上比 70B 参数的 SOTA 基线提高了 17%，推理速度提升了近 10 倍，且可在个人计算机和 Raspberry Pi 上高效部署。

Conclusion: DailyLLM 是一种创新的日志生成和摘要系统，通过综合整合位置、运动、环境和生理四个维度的上下文活动信息，显著提升了日志生成的准确性、效率和语义丰富度。

Abstract: Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [20] [OntView: What you See is What you Meant](https://arxiv.org/abs/2507.13759)
*Carlos Bobed,Carlota Quintana,Eduardo Mena,Jorge Bobed,Fernando Bobillo*

Main category: cs.AI

TL;DR: OntView是一个开源本体查看器，通过直观可视化和简化视图功能，解决了现有工具在大型本体表示中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有本体编辑器和查看器在图形化表示本体结构方面存在不足，限制了用户对大型本体框架中依赖关系和属性的理解。

Method: OntView基于DL推理器，采用“所见即所得”范式，可视化通用概念包含（GCI），并提供简化视图功能，如本体摘要、TBox元素聚焦和动态分支隐藏/显示。

Result: OntView成功实现了直观的本体可视化，支持GCI展示，并通过简化视图功能避免了信息过载，已作为开源工具发布。

Conclusion: OntView是一个开源的、用户友好的本体查看器，通过直观的可视化展示和简化视图功能，有效解决了现有工具在大型本体框架中表示和理解的局限性。

Abstract: In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.

</details>


### [21] [From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning](https://arxiv.org/abs/2507.13768)
*Renato Ghisellini,Remo Pareschi,Marco Pedroni,Giovanni Battista Raggi*

Main category: cs.AI

TL;DR: 提出了一种混合架构，通过启发式提取、语义激活和组合合成实现战略推理，能够融合冲突的启发式规则，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统决策引擎通常选择最佳规则，而本文提出了一种能够融合冲突启发式规则的新方法，以生成更连贯且情境敏感的叙述。

Method: 结合启发式提取、语义激活和组合合成的方法，利用语义互动建模和修辞框架来融合冲突的启发式规则。

Result: 通过Meta vs. FTC案例研究展示了该框架的有效性，并通过语义指标进行了初步验证。

Conclusion: 论文提出了一个混合架构，通过启发式提取、语义激活和组合合成，实现了代理增强的战略推理。该架构能够融合冲突的启发式规则，生成连贯且情境敏感的叙述，并通过案例研究和语义指标进行了初步验证。

Abstract: We present a hybrid architecture for agent-augmented strategic reasoning,
combining heuristic extraction, semantic activation, and compositional
synthesis. Drawing on sources ranging from classical military theory to
contemporary corporate strategy, our model activates and composes multiple
heuristics through a process of semantic interdependence inspired by research
in quantum cognition. Unlike traditional decision engines that select the best
rule, our system fuses conflicting heuristics into coherent and
context-sensitive narratives, guided by semantic interaction modeling and
rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,
with preliminary validation through semantic metrics. Limitations and
extensions (e.g., dynamic interference tuning) are discussed.

</details>


### [22] [When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction](https://arxiv.org/abs/2507.13825)
*Haoyang Li,Yuming Xu,Yiming Li,Hanmo Liu,Darian Li,Chen Jason Zhang,Lei Chen,Qing Li*

Main category: cs.AI

TL;DR: EAGLE是一种轻量级框架，结合短期时间和长期结构模式，显著提升了动态图时间链接预测的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有的T-GNN模型虽然成功但存在计算开销大、效率低的问题，因此需要一种轻量级框架来平衡时间与结构依赖性。

Method: EAGLE框架包含时间感知模块和结构感知模块，通过自适应加权机制动态调整两者的贡献，避免了复杂的多跳消息传递或内存密集型机制。

Result: 在七个真实世界时间图上，EAGLE在效果和效率上均优于现有T-GNN模型，速度提升超过50倍。

Conclusion: EAGLE框架通过整合短期时间临近性和长期全局结构模式，显著提升了动态图中时间链接预测的效率和效果，且在多个真实世界数据集上验证了其优越性。

Abstract: Temporal link prediction in dynamic graphs is a critical task with
applications in diverse domains such as social networks, recommendation
systems, and e-commerce platforms. While existing Temporal Graph Neural
Networks (T-GNNs) have achieved notable success by leveraging complex
architectures to model temporal and structural dependencies, they often suffer
from scalability and efficiency challenges due to high computational overhead.
In this paper, we propose EAGLE, a lightweight framework that integrates
short-term temporal recency and long-term global structural patterns. EAGLE
consists of a time-aware module that aggregates information from a node's most
recent neighbors to reflect its immediate preferences, and a structure-aware
module that leverages temporal personalized PageRank to capture the influence
of globally important nodes. To balance these attributes, EAGLE employs an
adaptive weighting mechanism to dynamically adjust their contributions based on
data characteristics. Also, EAGLE eliminates the need for complex multi-hop
message passing or memory-intensive mechanisms, enabling significant
improvements in efficiency. Extensive experiments on seven real-world temporal
graphs demonstrate that EAGLE consistently achieves superior performance
against state-of-the-art T-GNNs in both effectiveness and efficiency,
delivering more than a 50x speedup over effective transformer-based T-GNNs.

</details>


### [23] [Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.13846)
*Kathrin Korte,Christian Medeiros Adriano,Sona Ghahremani,Holger Giese*

Main category: cs.AI

TL;DR: 提出因果知识转移框架，通过零样本转移恢复动作宏提升多智能体在非平稳环境中的适应性，减少再训练需求。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳环境中多智能体强化学习知识转移的挑战，减少再训练成本并提升适应性。

Method: 引入因果知识转移框架，通过建模碰撞为因果干预，生成恢复动作宏（macro）并在智能体间零样本转移。

Result: 智能体在异构目标下能缩小随机探索与完全再训练策略之间约一半的差距，且因果转移效果受环境复杂性和目标异质性交互影响。

Conclusion: 因果知识转移框架在非平稳环境中有效提升了多智能体强化学习的适应性，减少了再训练需求，同时揭示了环境复杂性与智能体目标异质性之间的交互影响。

Abstract: [Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.

</details>


### [24] [Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery](https://arxiv.org/abs/2507.13874)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.AI

TL;DR: 提出了一种模型无关的潜在空间创意框架，无需手工规则即可实现可控创意生成，初步结果验证了其通用性和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在生成新颖且相关创意时的局限性，避免依赖领域特定启发式或结构化提示管道的脆弱性。

Method: 采用了一种模型无关的潜在空间创意框架，通过导航连续的嵌入空间来实现创意生成，无需手工规则，且能适应不同领域和任务。

Result: 初步结果表明，该框架在无需手工规则的情况下，能够适应不同领域和输入格式，展现出作为通用创意协作工具的潜力。

Conclusion: 论文提出了一个模型无关的潜在空间创意框架，展示了其在促进可控、可扩展的创造力方面的潜力，为人类-AI协作中的创意生成提供了新的可能性。

Abstract: Innovative idea generation remains a core challenge in AI, as large language
models (LLMs) often struggle to produce outputs that are both novel and
relevant. Despite their fluency, LLMs tend to replicate patterns seen during
training, limiting their ability to diverge creatively without extensive prompt
engineering. Prior work has addressed this through domain-specific heuristics
and structured prompting pipelines, but such solutions are brittle and
difficult to generalize. In this paper, we propose a model-agnostic
latent-space ideation framework that enables controlled, scalable creativity by
navigating the continuous embedding space of ideas. Unlike prior methods, our
framework requires no handcrafted rules and adapts easily to different domains,
input formats, and creative tasks. This paper introduces an early-stage
prototype of our method, outlining the conceptual framework and preliminary
results highlighting its potential as a general-purpose co-ideator for human-AI
collaboration.

</details>


### [25] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: ADPC框架结合视觉-语言因果干预和LLM，有效消除混杂因素，显著提升AD诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 早期识别和干预MCI可以有效减缓AD进展，但当前诊断方法因多模态数据的选择偏差和变量间复杂关系而面临挑战。

Method: 研究提出了一个名为ADPC的视觉-语言因果干预框架，利用大型语言模型（LLM）总结临床数据，并结合MRI和fMRI图像数据，通过因果干预消除混杂因素。

Result: ADPC框架在区分CN/MCI/AD病例方面表现出色，在大多数评估指标上达到了最先进的性能。

Conclusion: 该研究展示了将因果推理与多模态学习相结合在神经疾病诊断中的潜力，特别是在区分认知正常（CN）、轻度认知障碍（MCI）和阿尔茨海默病（AD）方面取得了最先进的性能。

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [26] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar,Martín Diéguez,François Olivier,Torsten Schaub,Igor Stéphan*

Main category: cs.AI

TL;DR: 提出了一种结合时态和非单调推理的ASP扩展，用于高分辨率动态系统建模。


<details>
  <summary>Details</summary>
Motivation: 解决基于逻辑的方法（如ASP）在细粒度时态和数值分辨率下对动态系统推理的挑战。

Method: 通过结合线性时间的Here-and-There逻辑和带约束的Here-and-There逻辑，实现了非单调时态推理与数值约束的直接集成与操作。

Result: 提出了首个专为ASP设计的非单调时态推理与约束结合的扩展系统。

Conclusion: 本文提出了一种新颖的时态和约束基础的逻辑扩展，为ASP范式下的高分辨率动态系统建模提供了基础逻辑框架。

Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [27] [KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models](https://arxiv.org/abs/2507.14032)
*Lam Nguyen,Erika Barcelos,Roger French,Yinghui Wu*

Main category: cs.AI

TL;DR: KROMA是一个利用LLM和RAG流程的本体匹配框架，通过优化技术显著提升性能，同时控制通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有本体匹配系统依赖手工规则或适应性有限的专用模型，KROMA旨在利用大型语言模型（LLMs）提升语义互操作性。

Method: KROMA采用检索增强生成（RAG）流程，结合双相似度概念匹配和轻量级本体细化步骤，动态丰富本体匹配任务的语义上下文。

Result: 实验表明，KROMA在多个基准数据集上优于经典本体匹配系统和前沿的LLM方法。

Conclusion: KROMA框架通过结合知识检索和上下文增强的LLM，显著提升了本体匹配的性能，同时保持了通信开销的可比性，证明了所提出的优化技术在大规模本体匹配中的可行性和优势。

Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.

</details>


### [28] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau,Baiying Lu,Yanjun Cui*

Main category: cs.AI

TL;DR: Glucose-ML是一个包含10个公开糖尿病数据集的集合，旨在加速透明、可复现的AI解决方案开发。研究发现不同数据集对算法性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 大型高质量数据集的获取困难阻碍了稳健AI解决方案的开发，因此需要公开可用的数据集来加速研究。

Method: 通过收集和整合10个公开可用的糖尿病数据集（Glucose-ML），并进行比较分析和案例研究（血糖预测任务），评估不同数据集对算法性能的影响。

Result: 研究发现，同一算法在不同数据集上的预测结果存在显著差异，这为开发稳健AI解决方案提供了重要参考。

Conclusion: 研究强调了高质量数据集在开发稳健AI解决方案中的重要性，并提出了Glucose-ML数据集集合，以支持透明、可复现和稳健的AI解决方案开发。

Abstract: Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


### [29] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer,Neel Macwan,Atharva Jitendra Hude,Heejin Jeong,Shenghan Guo*

Main category: cs.AI

TL;DR: G-AI-HMS利用生成式AI提升工业运动模拟保真度，通过文本到动作模型和计算机视觉验证，在多项任务中优于人工描述。


<details>
  <summary>Details</summary>
Motivation: 现有的人体运动模拟方法在运动保真度上存在不足，影响了工业任务中工人行为、安全和生产效率的评估效果。

Method: 研究采用生成式AI技术，结合大型语言模型（如MotionGPT）和计算机视觉技术，将任务描述转化为动作感知语言，并通过姿态估计算法和运动相似性度量验证AI生成动作与真实人类动作的一致性。

Result: 在八项任务的案例研究中，AI增强动作在大多数场景下表现优于人工描述，具体在六项任务的空间准确性、四项任务的姿态归一化对齐和七项任务的整体时间相似性上表现更佳。统计显示AI提示显著降低了关节误差和时间错位（p < 0.0001）。

Conclusion: G-AI-HMS通过整合文本到文本和文本到动作模型，显著提升了工业任务中人体运动模拟的保真度，并在多个任务中表现出优于人工描述的性能。

Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


### [30] [Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment](https://arxiv.org/abs/2507.14107)
*Viraj Nishesh Darji,Callie C. Liao,Duoduo Liao*

Main category: cs.AI

TL;DR: LLM能高效解释NDE数据，ChatGPT-4和Claude 3.5 Sonnet表现最佳，为桥梁维护提供快速准确的分析支持。


<details>
  <summary>Details</summary>
Motivation: 传统NDE数据分析耗时且依赖专业知识，LLM的进步为自动化分析提供了新可能，旨在提升桥梁检查效率与决策速度。

Method: 研究探索了多种LLM模型，通过特定设计的提示词解释五种NDE等高线图，评估模型生成详细描述、识别缺陷、提供可操作建议及整体准确性的能力。

Result: 九个模型中有四个在图像描述上表现更好，ChatGPT-4和Claude 3.5 Sonnet生成的摘要更有效。LLM辅助分析可提升桥梁维护效率与准确性。

Conclusion: LLMs，特别是ChatGPT-4和Claude 3.5 Sonnet，在解释NDE等高线图和生成桥梁状况综合分析方面表现出色，能够显著提高效率而不牺牲准确性。

Abstract: Bridge maintenance and safety are essential for transportation authorities,
and Non-Destructive Evaluation (NDE) techniques are critical to assessing
structural integrity. However, interpreting NDE data can be time-consuming and
requires expertise, potentially delaying decision-making. Recent advancements
in Large Language Models (LLMs) offer new ways to automate and improve this
analysis. This pilot study introduces a holistic assessment of LLM capabilities
for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in
providing detailed bridge condition analyses. It establishes a framework for
integrating LLMs into bridge inspection workflows, indicating that LLM-assisted
analysis can enhance efficiency without compromising accuracy. In this study,
several LLMs are explored with prompts specifically designed to enhance the
quality of image descriptions, which are applied to interpret five different
NDE contour maps obtained through technologies for assessing bridge conditions.
Each LLM model is evaluated based on its ability to produce detailed
descriptions, identify defects, provide actionable recommendations, and
demonstrate overall accuracy. The research indicates that four of the nine
models provide better image descriptions, effectively covering a wide range of
topics related to the bridge's condition. The outputs from these four models
are summarized using five different LLMs to form a comprehensive overview of
the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more
effective summaries. The findings suggest that LLMs have the potential to
significantly improve efficiency and accuracy. This pilot study presents an
innovative approach that leverages LLMs for image captioning in parallel and
summarization, enabling faster decision-making in bridge maintenance and
enhancing infrastructure management and safety assessments.

</details>
