<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 5]
- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica](https://arxiv.org/abs/2507.13476)
*Jaber Daneshamooz,Jessica Nguyen,William Chen,Sanjay Chandrasekaran,Satyandra Guthula,Ankit Gupta,Arpit Gupta,Walter Willinger*

Main category: cs.NI

TL;DR: NetReplica enhances ML model generalizability in networking by generating realistic, controllable training data, reducing prediction errors by up to 47%.


<details>
  <summary>Details</summary>
Motivation: Machine learning models in networking often fail when deployed in different environments due to the domain adaptation problem. NetReplica aims to solve this by generating datasets that ensure realism and controllability.

Method: NetReplica generates realistic and controllable training datasets by modeling networks as bottleneck links with specific attributes, leveraging production traces, and providing fine-grained control knobs.

Result: Evaluation with Puffer shows NetReplica matches existing data characteristics and generates underrepresented realistic samples, improving model generalizability.

Conclusion: NetReplica significantly improves the generalizability of machine learning models in networking by addressing the domain adaptation problem, reducing prediction errors by up to 47%.

Abstract: Machine learning models in networking suffer from the domain adaptation
problem; models trained in one domain often fail when deployed in different
production environments. This paper presents the design and implementation of
NetReplica, a system that addresses this challenge by generating training
datasets with two critical properties: realism in protocol dynamics and
controllability of network conditions. NetReplica models networks as
collections of bottleneck links with specific attributes, achieves realism by
leveraging production network traces, and enables controllability through fine
grained control knobs for each link attribute. Our evaluation using Puffer
demonstrates that NetReplica not only matches existing data characteristics but
generates realistic samples that are underrepresented in or absent from Puffer
data. Models trained on NetReplica augmented datasets show substantially
improved generalizability, reducing transmission time prediction error by up to
47% for challenging network conditions compared to models trained solely on
Puffer data. This work represents a significant step toward solving the domain
adaptation problem that has limited the effectiveness of ML based networking
systems.

</details>


### [2] [CARTS: Cooperative and Adaptive Resource Triggering and Stitching for 5G ISAC](https://arxiv.org/abs/2507.13676)
*Cheng Jiang,Yihe Yan,Yanxiang Wang,Jiawei Hu,Chun Tung Chou,Wen Hu*

Main category: cs.NI

TL;DR: CARTS 是一种自适应5G上行链路感知方案，通过融合 DMRS 和 SRS 的 CSI 流，显著提升 ISAC 性能，支持更多用户且无需额外资源。


<details>
  <summary>Details</summary>
Motivation: 现代5G网络中，上行链路的 CSI 来自 DMRS 和 SRS，但当前基站将它们视为独立信息流。CARTS 旨在融合这两者，提高 CSI 更新频率并为更多用户提供感知机会。

Method: CARTS 通过两种创新方法实现：一是新颖的信道拼接和补偿方法，整合异步 CSI 估计；二是实时 SRS 触发算法，确保所有用户有足够且非冗余的感知机会。

Result: CARTS 显著提升了可扩展性，实现了 0.167 的 NMSE 和 85 cm 的 UE 跟踪精度，同时支持两倍于基线 SRS 的用户数。

Conclusion: CARTS 提供了一种实用的、符合标准的解决方案，通过融合 DMRS 和 SRS 的 CSI 流，显著提高了 ISAC 服务的 CSI 可用性，且无需额外的无线电资源。

Abstract: This paper presents CARTS, an adaptive 5G uplink sensing scheme designed to
provide Integrated Sensing and Communication (ISAC) services. The performance
of both communication and sensing fundamentally depends on the availability of
accurate and up-to-date channel state information (CSI). In modern 5G networks,
uplink CSI is derived from two reference signals: the demodulation reference
signal (DMRS) and the sounding reference signal (SRS). However, current base
station implementations treat these CSI measurements as separate information
streams. The key innovation of CARTS is to fuse these two CSI streams, thereby
increasing the frequency of CSI updates and extending sensing opportunities to
more users. CARTS addresses two key challenges: (i) a novel channel stitching
and compensation method that integrates asynchronous CSI estimates from DMRS
and SRS, despite their different time and frequency allocations, and (ii) a
real-time SRS triggering algorithm that complements the inherently
uncontrollable DMRS schedule, ensuring sufficient and non-redundant sensing
opportunities for all users. Our trace-driven evaluation shows that CARTS
significantly improves scalability, achieving a channel estimation error (NMSE)
of 0.167 and UE tracking accuracy of 85 cm while supporting twice the number of
users as a periodic SRS-only baseline with similar performance. By
opportunistically combining DMRS and SRS, CARTS therefore provides a practical,
standard-compliant solution to improve CSI availability for ISAC without
requiring additional radio resources.

</details>


### [3] [ATRO: A Fast Solver-Free Algorithm for Topology and Routing Optimization of Reconfigurable Datacenter Networks](https://arxiv.org/abs/2507.13717)
*Yingming Mao,Qiaozhu Zhai,Zhen Yao,Xia Zhu,Ximeng Liu,Xinchi Han*

Main category: cs.NI

TL;DR: ATRO是一种无求解器框架，通过交替进行拓扑和路由优化，显著提升了可重构DCN的解决方案质量和运行效率。


<details>
  <summary>Details</summary>
Motivation: 可重构数据中心网络（DCNs）的规模和复杂性不断增加，需要更高效、可扩展的算法来优化逻辑拓扑和路由。现有方法难以在解决方案质量和运行效率之间取得平衡。

Method: 提出了交替拓扑和路由优化（ATRO）框架，通过交替进行拓扑优化（TO）和路由优化（RO）来分解问题。TO子问题通过加速二进制搜索方法（ABSM）高效求解，RO则利用现有的流量工程加速器。

Result: ATRO在单跳场景中达到全局最优，并在多跳场景中显著优于基线方法，表现出更高的运行效率和更好的解决方案质量。

Conclusion: ATRO在单跳场景下达到全局最优，并在多跳场景中显著优于基线方法，在运行时间和解决方案质量上均表现出色。评估证实了其在各种DCN中的可扩展性和鲁棒性。

Abstract: The growing scale and complexity of reconfigurable data center networks
(DCNs) demand more scalable and efficient algorithms for computing logical
topologies and routing. Reconfigurable DCNs typically operate in two modes:
one-hop configurations that require frequent topology optimization (TO), and
multi-hop scenarios that involve joint topology and routing optimization (TRO).
In both cases, the combinatorial nature of topology decisions makes it
difficult for existing methods to balance solution quality and runtime
efficiency. To address this, we introduce Alternating Topology and Routing
Optimization (ATRO), a solver-free framework that alternates between TO and
routing optimization (RO). This decomposition exploits two key insights: first,
each alternating update step monotonically reduces maximum link utilization
(MLU), ensuring consistent performance improvement across iterations; second,
the TO subproblem, equivalent to one-hop optimization, exhibits a monotonic
structure that enables optimal solutions via an efficient Accelerated Binary
Search Method (ABSM). To preserve the solver-free design, RO is solved using
existing Traffic Engineering accelerators. ATRO attains the global optimum in
one-hop scenarios and significantly outperforms baselines in multi-hop settings
in terms of both runtime and solution quality. Evaluations confirm its
scalability and robustness across diverse DCNs.

</details>


### [4] [On the Trade-Off Between Sum-Rate and Energy Efficiency through the Convergence of HAPS and Active RIS Technologies](https://arxiv.org/abs/2507.13889)
*Bilal Karaman,Ilhan Basturk,Ferdi Kara,Metin Ozturk,Sezai Taskin,Halil Yanikomeroglu*

Main category: cs.NI

TL;DR: Active RIS with HAPS enhances NTN performance by overcoming passive RIS limitations, optimizing power and RIS elements, with sub-connected schemes offering better energy efficiency.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the limitations of passive RIS architectures in long-distance HAPS links, where severe path loss and double fading make active RIS, with its signal amplification capabilities, a more suitable alternative to enhance NTN performance.

Method: The paper formulates a sum-rate maximization problem to jointly optimize power allocation and RIS element assignment for ground UEs in a HAPS-based active RIS-assisted system, while also exploring sub-connected architectures to reduce power consumption and hardware complexity.

Result: Simulation results show that active RIS configurations significantly outperform passive RIS in QoS, with fully-connected architectures achieving the highest throughput but sub-connected schemes demonstrating better energy efficiency under practical power constraints.

Conclusion: The findings highlight the potential of active RIS-enabled HAPS systems to address the demands of beyond-cellular coverage and green networking, with sub-connected schemes offering superior energy efficiency under practical constraints.

Abstract: This paper investigates the integration of active reconfigurable intelligent
surfaces (RIS) relay with high-altitude platform stations (HAPS) to enhance
non-terrestrial network (NTN) performance in next-generation wireless systems.
While prior studies focused on passive RIS architectures, the severe path loss
and double fading in long-distance HAPS links make active RIS a more suitable
alternative due to its inherent signal amplification capabilities. We formulate
a sum-rate maximization problem to jointly optimize power allocation and RIS
element assignment for ground user equipments (UEs) supported by a HAPS-based
active RIS-assisted communication system. To reduce power consumption and
hardware complexity, several sub-connected active RIS architectures are also
explored. Simulation results reveal that active RIS configurations
significantly outperform passive RIS in terms of quality of service (QoS).
Moreover, although fully-connected architectures achieve the highest
throughput, sub-connected schemes demonstrate superior energy efficiency under
practical power constraints. These findings highlight the potential of active
RIS-enabled HAPS systems to meet the growing demands of beyond-cellular
coverage and green networking.

</details>


### [5] [Preprint: Did I Just Browse A Website Written by LLMs?](https://arxiv.org/abs/2507.13933)
*Sichang "Steven" He,Ramesh Govindan,Harsha V. Madhyastha*

Main category: cs.NI

TL;DR: Proposed a reliable pipeline to detect LLM-dominant websites, achieving 100% accuracy on test datasets and identifying many such sites in real-world web data.


<details>
  <summary>Details</summary>
Motivation: The rise of LLM-dominant content, which is often unreliable and unethical, poses challenges as websites rarely disclose such content, making it hard for human readers to distinguish. Existing LLM detectors are insufficient for web content due to its complexity.

Method: A highly reliable, scalable pipeline that classifies entire websites based on an LLM text detector's outputs from multiple prose-like pages. The method involves training and evaluating the detector using two distinct ground truth datasets totaling 120 sites.

Result: The detector achieved 100% accuracy across the ground truth datasets. In real-world testing, a sizable portion of sites among 10k search engine results and 10k Common Crawl archives were identified as LLM-dominant.

Conclusion: LLM-dominant websites are becoming more prevalent and rank highly in search results, raising concerns about their impact on users and the web ecosystem. The proposed pipeline effectively detects such sites with high accuracy.

Abstract: Increasingly, web content is automatically generated by large language models
(LLMs) with little human input. We call this "LLM-dominant" content. Since LLMs
plagiarize and hallucinate, LLM-dominant content can be unreliable and
unethical. Yet, websites rarely disclose such content, and human readers
struggle to distinguish it. Thus, we must develop reliable detectors for
LLM-dominant content. However, state-of-the-art LLM detectors are insufficient,
because they perform well mainly on clean, prose-like text, while web content
has complex markup and diverse genres.
  We propose a highly reliable, scalable pipeline that classifies entire
websites. Instead of naively classifying text extracted from each page, we
classify each site based on an LLM text detector's outputs of multiple
prose-like pages. We train and evaluate our detector by collecting 2 distinct
ground truth datasets totaling 120 sites, and obtain 100% accuracies testing
across them. In the wild, we detect a sizable portion of sites as LLM-dominant
among 10k sites in search engine results and 10k in Common Crawl archives. We
find LLM-dominant sites are growing in prevalence and rank highly in search
results, raising questions about their impact on end users and the overall Web
ecosystem.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Checkmate: Zero-Overhead Model Checkpointing via Network Gradient Replication](https://arxiv.org/abs/2507.13522)
*Ankit Bhardwaj,Weiyang Wang,Jeremy Carin,Adam Belay,Manya Ghobadi*

Main category: cs.DC

TL;DR: Checkmate introduces per-iteration checkpointing in DNN training without slowdown, using gradients to maintain checkpoints, outperforming existing systems in frequency and throughput.


<details>
  <summary>Details</summary>
Motivation: Traditional checkpointing methods require training pauses, creating a tradeoff between checkpoint frequency and failure cost. Checkmate aims to eliminate this tradeoff by leveraging existing gradient information in data-parallel training.

Method: Utilizes a new multicast abstraction to deliver gradients to a CPU-based shadow cluster, which maintains checkpoints by applying gradients to a model copy.

Result: Checkmate achieves 5 to 34.5x more frequent checkpointing, 80% to 97.1% reduction in repeated work per failure, and 1.3x to 6.5x higher throughput compared to state-of-the-art systems.

Conclusion: Checkmate successfully enables per-iteration checkpointing in DNN training without training slowdown, significantly reducing repeated work per failure and outperforming existing systems in throughput.

Abstract: This paper presents Checkmate, a system that enables per-iteration
checkpointing in DNN training without any training slowdown. The traditional
approach to checkpointing requires a pause in training to copy model states to
a separate location, allowing the state to be restored in the event of failure.
This approach fundamentally has a tradeoff between the frequency of checkpoints
and the cost of a failure. We avoid this tradeoff; our key insight is that in
data-parallel training, all information necessary to create a checkpoint
already exists in the network as gradients. Our core contribution is a new
multicast abstraction that simultaneously delivers gradients to a separate
CPU-based shadow cluster. The shadow maintains a checkpoint by applying those
gradients to a copy of the model. Our evaluation shows that Checkmate performs
per-iteration checkpointing with training throughput comparable to an ideal
no-checkpoint baseline. Checkmate achieves 5 to 34.5x more frequent
checkpointing compared to state-of-the-art checkpointing systems, resulting in
80% to 97.1% reduction in repeated work per failure. At the same checkpointing
frequency, Checkmate delivers 1.3x to 6.5x throughput compared to other
systems.

</details>


### [7] [Leveraging Multi-Instance GPUs through moldable task scheduling](https://arxiv.org/abs/2507.13601)
*Jorge Villarrubia,Luis Costero,Francisco D. Igual,Katzalin Olcoz*

Main category: cs.DC

TL;DR: FAR algorithm optimizes task scheduling on NVIDIA MIG with dynamic reconfigurations, achieving near-optimal makespan and significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: To highlight the untapped potential of NVIDIA MIG through moldable task scheduling with dynamic reconfigurations, addressing the makespan minimization problem for multi-task execution under MIG constraints.

Method: The paper proposes FAR, a 3-phase algorithm: Phase 1 uses a classical task moldability method, Phase 2 combines Longest Processing Time First and List Scheduling with a novel repartitioning tree heuristic, and Phase 3 employs local search via task moves and swaps. FAR schedules tasks in batches offline, concatenating their schedules dynamically to favor resource reuse.

Result: FAR achieves an approximation factor of 7/4 on NVIDIA A30 and 2 on NVIDIA A100/H100. Real-world experiments show makespan no worse than 1.22x for benchmarks and 1.10x for synthetic inputs, with large improvements over state-of-the-art methods.

Conclusion: The paper demonstrates the research potential of NVIDIA MIG technology and suggests useful metrics, workload characterizations, and evaluation techniques for future work. FAR algorithm shows significant improvements over state-of-the-art methods, even when including reconfiguration costs.

Abstract: NVIDIA MIG (Multi-Instance GPU) allows partitioning a physical GPU into
multiple logical instances with fully-isolated resources, which can be
dynamically reconfigured. This work highlights the untapped potential of MIG
through moldable task scheduling with dynamic reconfigurations. Specifically,
we propose a makespan minimization problem for multi-task execution under MIG
constraints. Our profiling shows that assuming monotonicity in task work with
respect to resources is not viable, as is usual in multicore scheduling.
Relying on a state-of-the-art proposal that does not require such an
assumption, we present FAR, a 3-phase algorithm to solve the problem. Phase 1
of FAR builds on a classical task moldability method, phase 2 combines Longest
Processing Time First and List Scheduling with a novel repartitioning tree
heuristic tailored to MIG constraints, and phase 3 employs local search via
task moves and swaps. FAR schedules tasks in batches offline, concatenating
their schedules on the fly in an improved way that favors resource reuse.
Excluding reconfiguration costs, the List Scheduling proof shows an
approximation factor of 7/4 on the NVIDIA A30 model. We adapt the technique to
the particular constraints of an NVIDIA A100/H100 to obtain an approximation
factor of 2. Including the reconfiguration cost, our real-world experiments
reveal a makespan with respect to the optimum no worse than 1.22x for a
well-known suite of benchmarks, and 1.10x for synthetic inputs inspired by real
kernels. We obtain good experimental results for each batch of tasks, but also
in the concatenation of batches, with large improvements over the
state-of-the-art and proposals without GPU reconfiguration. Beyond the
algorithm, the paper demonstrates the research potential of the MIG technology
and suggests useful metrics, workload characterizations and evaluation
techniques for future work in this field.

</details>


### [8] [DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training](https://arxiv.org/abs/2507.13833)
*Zhixin Wang,Tianyi Zhou,Liming Liu,Ao Li,Jiarui Hu,Dian Yang,Jinlong Hou,Siyuan Feng,Yuan Cheng,Yuan Qi*

Main category: cs.DC

TL;DR: DistFlow 是一种新型分布式强化学习框架，通过消除中心节点实现近线性扩展，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 大规模强化学习中，轻微的负载不平衡会导致显著的瓶颈，限制了系统的扩展性。

Method: 采用多控制器范式，将数据传输和执行任务分配给所有工作节点，消除中心节点，使每个工作节点独立运行。

Result: DistFlow 实现了优异的线性扩展性，端到端吞吐量比现有技术框架提高了7倍。

Conclusion: DistFlow 成功解决了大规模强化学习中的扩展瓶颈，实现了近线性扩展和显著的效率提升。

Abstract: Reinforcement learning (RL) has become the pivotal post-training technique
for large language model. Effectively scaling reinforcement learning is now the
key to unlocking advanced reasoning capabilities and ensuring safe,
goal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually
employ a hybrid-controller architecture where a single-controller dispatches
the overall execution logic and manages overall data transfer and the
multi-controller executes distributed computation. For large-scale
reinforcement learning, minor load imbalances can introduce significant
bottlenecks, ultimately constraining the scalability of the system. To address
this limitation, we introduce DistFlow, a novel, fully distributed RL framework
designed to break scaling barrier. We adopt a multi-controller paradigm that
dispatches data transfer and execution tasks to all workers, which eliminates
the centralized node. This allows each worker to operate independently, leading
to near-linear scalability up to thousands of GPUs and dramatic efficiency
gains. Furthermore, our architecture decouples resource configuration from
execution logic, allowing each worker to have a unique execution flow, offering
significant flexibility for rapid and cost-effective algorithmic
experimentation. Extensive experiments show that DistFlow achieves excellent
linear scalability and up to a 7x end-to-end throughput improvement over
state-of-the-art (SOTA) frameworks.

</details>


### [9] [Edge Intelligence with Spiking Neural Networks](https://arxiv.org/abs/2507.14069)
*Shuiguang Deng,Di Yu,Changze Lv,Xin Du,Linshan Jiang,Xiaofan Zhao,Wentao Tong,Xiaoqing Zheng,Weijia Fang,Peng Zhao,Gang Pan,Schahram Dustdar,Albert Y. Zomaya*

Main category: cs.DC

TL;DR: 本文综述了基于SNNs的边缘智能（EdgeSNNs），探讨其在设备上学习、推理和边缘场景安全中的潜力，提供了系统性分类和实际应用讨论，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 人工智能与边缘计算的融合激发了在资源受限设备上直接实现智能服务的兴趣。传统深度学习模型需要大量计算资源和集中式数据管理，导致延迟、带宽消耗和隐私问题，暴露了以云为中心范式的关键限制。

Method: 本文提供了基于SNNs的边缘智能（EdgeSNNs）的系统性分类，包括神经元模型、学习算法和支持硬件平台。深入讨论了EdgeSNN的三个实际考虑：使用轻量级SNN模型进行设备上推理、非静态数据条件下的资源感知训练和更新，以及安全和隐私保护问题。

Result: 本文介绍了双轨基准测试策略，以支持公平比较和硬件感知优化，并突出了在传统硬件上评估EdgeSNNs的局限性。

Conclusion: 本文旨在弥合脑启发学习与实际边缘部署之间的差距，提供了对当前进展、开放挑战和未来研究方向的见解。这是第一份专门且全面的EdgeSNNs调查，为在神经形态计算和边缘智能交叉领域工作的研究人员和实践者提供了重要参考。

Abstract: The convergence of artificial intelligence and edge computing has spurred
growing interest in enabling intelligent services directly on
resource-constrained devices. While traditional deep learning models require
significant computational resources and centralized data management, the
resulting latency, bandwidth consumption, and privacy concerns have exposed
critical limitations in cloud-centric paradigms. Brain-inspired computing,
particularly Spiking Neural Networks (SNNs), offers a promising alternative by
emulating biological neuronal dynamics to achieve low-power, event-driven
computation. This survey provides a comprehensive overview of Edge Intelligence
based on SNNs (EdgeSNNs), examining their potential to address the challenges
of on-device learning, inference, and security in edge scenarios. We present a
systematic taxonomy of EdgeSNN foundations, encompassing neuron models,
learning algorithms, and supporting hardware platforms. Three representative
practical considerations of EdgeSNN are discussed in depth: on-device inference
using lightweight SNN models, resource-aware training and updating under
non-stationary data conditions, and secure and privacy-preserving issues.
Furthermore, we highlight the limitations of evaluating EdgeSNNs on
conventional hardware and introduce a dual-track benchmarking strategy to
support fair comparisons and hardware-aware optimization. Through this study,
we aim to bridge the gap between brain-inspired learning and practical edge
deployment, offering insights into current advancements, open challenges, and
future research directions. To the best of our knowledge, this is the first
dedicated and comprehensive survey on EdgeSNNs, providing an essential
reference for researchers and practitioners working at the intersection of
neuromorphic computing and edge intelligence.

</details>


### [10] [Shipwright: Proving liveness of distributed systems with Byzantine participants](https://arxiv.org/abs/2507.14080)
*Derek Leung,Nickolai Zeldovich,Frans Kaashoek*

Main category: cs.DC

TL;DR: Shipwright verifies liveness and correctness in decentralized systems, demonstrated via a PBFT prototype with modular and cryptographic support.


<details>
  <summary>Details</summary>
Motivation: Ensuring liveness in decentralized systems like PBFT is critical yet challenging due to potential malicious participants and the absence of a central administrator.

Method: Shipwright introduces three techniques for formal reasoning in decentralized settings, modular decomposition, and cryptographic signature handling.

Result: Shipwright enables verification of liveness for an executable PBFT implementation, with experimental validation in common and failure scenarios.

Conclusion: Shipwright successfully verifies liveness and correctness in decentralized systems with malicious participants, demonstrated through a PBFT prototype.

Abstract: Ensuring liveness in a decentralized system, such as PBFT, is critical,
because there may not be any single administrator that can restart the system
if it encounters a liveness bug. At the same time, liveness is challenging to
achieve because any single participant could be malicious, and yet the overall
system must make forward progress. While verification is a promising approach
for ensuring the absence of bugs, no prior work has been able to verify
liveness for an executable implementation of PBFT.
  Shipwright is a verification framework for proving correctness and liveness
of distributed systems where some participants might be malicious. Shipwright
introduces three techniques that enable formal reasoning about decentralized
settings with malicious participants, allow developers to decompose their
system and proof in a modular fashion into sub-protocols and sub-proofs, and
support sound reasoning about cryptographic signatures that may be embedded in
messages. We used Shipwright to implement and verify an initial prototype of
agreement on a single log entry in PBFT (with a few limitations) and translate
it to an executable implementation in Go. We experimentally demonstrate its
operation and liveness both in the common case and in several failure
scenarios.

</details>
