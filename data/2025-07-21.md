<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 5]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Checkmate: Zero-Overhead Model Checkpointing via Network Gradient Replication](https://arxiv.org/abs/2507.13522)
*Ankit Bhardwaj,Weiyang Wang,Jeremy Carin,Adam Belay,Manya Ghobadi*

Main category: cs.DC

TL;DR: Checkmate 通过梯度多播实现无减速的每次迭代检查点，显著提升检查点频率和训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统检查点方法需要在训练暂停时复制模型状态，存在检查点频率与故障成本之间的权衡，Checkmate 旨在避免这种权衡。

Method: 利用数据并行训练中的梯度信息，通过多播抽象将梯度同时传递到基于 CPU 的影子集群，影子集群通过应用梯度维护模型副本。

Result: Checkmate 实现了每次迭代检查点，训练吞吐量与无检查点基线相当，检查点频率比现有系统高 5 至 34.5 倍，重复工作减少 80% 至 97.1%。

Conclusion: Checkmate 系统通过创新的多播抽象实现了无训练减速的每次迭代检查点，显著提高了检查点频率并减少了故障时重复工作。

Abstract: This paper presents Checkmate, a system that enables per-iteration
checkpointing in DNN training without any training slowdown. The traditional
approach to checkpointing requires a pause in training to copy model states to
a separate location, allowing the state to be restored in the event of failure.
This approach fundamentally has a tradeoff between the frequency of checkpoints
and the cost of a failure. We avoid this tradeoff; our key insight is that in
data-parallel training, all information necessary to create a checkpoint
already exists in the network as gradients. Our core contribution is a new
multicast abstraction that simultaneously delivers gradients to a separate
CPU-based shadow cluster. The shadow maintains a checkpoint by applying those
gradients to a copy of the model. Our evaluation shows that Checkmate performs
per-iteration checkpointing with training throughput comparable to an ideal
no-checkpoint baseline. Checkmate achieves 5 to 34.5x more frequent
checkpointing compared to state-of-the-art checkpointing systems, resulting in
80% to 97.1% reduction in repeated work per failure. At the same checkpointing
frequency, Checkmate delivers 1.3x to 6.5x throughput compared to other
systems.

</details>


### [2] [Leveraging Multi-Instance GPUs through moldable task scheduling](https://arxiv.org/abs/2507.13601)
*Jorge Villarrubia,Luis Costero,Francisco D. Igual,Katzalin Olcoz*

Main category: cs.DC

TL;DR: 本文提出FAR算法，通过三阶段方法优化NVIDIA MIG技术的任务调度，显著提高了任务完成效率，并展示了MIG技术的巨大研究潜力。


<details>
  <summary>Details</summary>
Motivation: 探索NVIDIA MIG技术的未开发潜力，通过动态重新配置实现可塑性任务调度，以最小化多任务执行的总完成时间。

Method: 本文提出了FAR算法，一个三阶段方法来解决MIG约束下的任务调度问题。第一阶段基于经典的任务可塑性方法，第二阶段结合了最长处理时间优先和列表调度，并引入了针对MIG约束的新颖重新分区树启发式方法，第三阶段通过任务移动和交换进行局部搜索。

Result: 实验结果显示，在不考虑重新配置成本的情况下，FAR算法在NVIDIA A30模型上的近似因子为7/4，在NVIDIA A100/H100上的近似因子为2。包括重新配置成本后，实际实验表明总完成时间相对于最优解不超过1.22倍（基准测试）和1.10倍（合成输入）。

Conclusion: 本文展示了MIG技术的研究潜力，并提出了有用的指标、工作负载特征和评估技术，为未来在这一领域的工作提供了参考。

Abstract: NVIDIA MIG (Multi-Instance GPU) allows partitioning a physical GPU into
multiple logical instances with fully-isolated resources, which can be
dynamically reconfigured. This work highlights the untapped potential of MIG
through moldable task scheduling with dynamic reconfigurations. Specifically,
we propose a makespan minimization problem for multi-task execution under MIG
constraints. Our profiling shows that assuming monotonicity in task work with
respect to resources is not viable, as is usual in multicore scheduling.
Relying on a state-of-the-art proposal that does not require such an
assumption, we present FAR, a 3-phase algorithm to solve the problem. Phase 1
of FAR builds on a classical task moldability method, phase 2 combines Longest
Processing Time First and List Scheduling with a novel repartitioning tree
heuristic tailored to MIG constraints, and phase 3 employs local search via
task moves and swaps. FAR schedules tasks in batches offline, concatenating
their schedules on the fly in an improved way that favors resource reuse.
Excluding reconfiguration costs, the List Scheduling proof shows an
approximation factor of 7/4 on the NVIDIA A30 model. We adapt the technique to
the particular constraints of an NVIDIA A100/H100 to obtain an approximation
factor of 2. Including the reconfiguration cost, our real-world experiments
reveal a makespan with respect to the optimum no worse than 1.22x for a
well-known suite of benchmarks, and 1.10x for synthetic inputs inspired by real
kernels. We obtain good experimental results for each batch of tasks, but also
in the concatenation of batches, with large improvements over the
state-of-the-art and proposals without GPU reconfiguration. Beyond the
algorithm, the paper demonstrates the research potential of the MIG technology
and suggests useful metrics, workload characterizations and evaluation
techniques for future work in this field.

</details>


### [3] [DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training](https://arxiv.org/abs/2507.13833)
*Zhixin Wang,Tianyi Zhou,Liming Liu,Ao Li,Jiarui Hu,Dian Yang,Jinlong Hou,Siyuan Feng,Yuan Cheng,Yuan Qi*

Main category: cs.DC

TL;DR: DistFlow 是一个全分布式强化学习框架，通过消除中心节点和独立工作节点设计，解决了扩展瓶颈，实现了高效的大规模强化学习。


<details>
  <summary>Details</summary>
Motivation: 大规模强化学习中，轻微的负载不平衡会引入显著的瓶颈，限制系统的扩展性。

Method: 采用多控制器范式，将数据传输和执行任务分配给所有工作节点，消除中心节点，实现每个工作节点的独立运行。

Result: DistFlow 实现了近线性的扩展性，端到端吞吐量比现有最佳框架提高了7倍。

Conclusion: DistFlow 通过全分布式架构解决了大规模强化学习中的扩展瓶颈，实现了近线性的扩展性和显著的效率提升。

Abstract: Reinforcement learning (RL) has become the pivotal post-training technique
for large language model. Effectively scaling reinforcement learning is now the
key to unlocking advanced reasoning capabilities and ensuring safe,
goal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually
employ a hybrid-controller architecture where a single-controller dispatches
the overall execution logic and manages overall data transfer and the
multi-controller executes distributed computation. For large-scale
reinforcement learning, minor load imbalances can introduce significant
bottlenecks, ultimately constraining the scalability of the system. To address
this limitation, we introduce DistFlow, a novel, fully distributed RL framework
designed to break scaling barrier. We adopt a multi-controller paradigm that
dispatches data transfer and execution tasks to all workers, which eliminates
the centralized node. This allows each worker to operate independently, leading
to near-linear scalability up to thousands of GPUs and dramatic efficiency
gains. Furthermore, our architecture decouples resource configuration from
execution logic, allowing each worker to have a unique execution flow, offering
significant flexibility for rapid and cost-effective algorithmic
experimentation. Extensive experiments show that DistFlow achieves excellent
linear scalability and up to a 7x end-to-end throughput improvement over
state-of-the-art (SOTA) frameworks.

</details>


### [4] [Edge Intelligence with Spiking Neural Networks](https://arxiv.org/abs/2507.14069)
*Shuiguang Deng,Di Yu,Changze Lv,Xin Du,Linshan Jiang,Xiaofan Zhao,Wentao Tong,Xiaoqing Zheng,Weijia Fang,Peng Zhao,Gang Pan,Schahram Dustdar,Albert Y. Zomaya*

Main category: cs.DC

TL;DR: 本文全面调查了基于SNNs的边缘智能（EdgeSNNs），探讨其在设备上学习、推理和安全性方面的潜力，并提出了双轨基准测试策略以优化评估。


<details>
  <summary>Details</summary>
Motivation: 人工智能与边缘计算的融合激发了对在资源受限设备上直接启用智能服务的兴趣。传统深度学习模型需要大量计算资源和集中式数据管理，导致延迟、带宽消耗和隐私问题。脑启发计算，特别是脉冲神经网络（SNNs），通过模拟生物神经元动力学实现低功耗、事件驱动的计算，提供了一种有前景的替代方案。

Method: 本研究提供了基于SNNs的边缘智能（EdgeSNNs）的全面概述，包括神经元模型、学习算法和支持硬件平台的系统分类。深入讨论了EdgeSNN的三个实际考虑因素：使用轻量级SNN模型进行设备上推理、非平稳数据条件下的资源感知训练和更新，以及安全和隐私保护问题。

Result: 本研究提出了双轨基准测试策略，以支持公平比较和硬件感知优化。

Conclusion: 本研究旨在弥合脑启发学习与实际边缘部署之间的差距，提供了对当前进展、开放挑战和未来研究方向的见解。这是第一份专门且全面的EdgeSNNs调查，为在神经形态计算和边缘智能交叉领域工作的研究人员和从业者提供了重要参考。

Abstract: The convergence of artificial intelligence and edge computing has spurred
growing interest in enabling intelligent services directly on
resource-constrained devices. While traditional deep learning models require
significant computational resources and centralized data management, the
resulting latency, bandwidth consumption, and privacy concerns have exposed
critical limitations in cloud-centric paradigms. Brain-inspired computing,
particularly Spiking Neural Networks (SNNs), offers a promising alternative by
emulating biological neuronal dynamics to achieve low-power, event-driven
computation. This survey provides a comprehensive overview of Edge Intelligence
based on SNNs (EdgeSNNs), examining their potential to address the challenges
of on-device learning, inference, and security in edge scenarios. We present a
systematic taxonomy of EdgeSNN foundations, encompassing neuron models,
learning algorithms, and supporting hardware platforms. Three representative
practical considerations of EdgeSNN are discussed in depth: on-device inference
using lightweight SNN models, resource-aware training and updating under
non-stationary data conditions, and secure and privacy-preserving issues.
Furthermore, we highlight the limitations of evaluating EdgeSNNs on
conventional hardware and introduce a dual-track benchmarking strategy to
support fair comparisons and hardware-aware optimization. Through this study,
we aim to bridge the gap between brain-inspired learning and practical edge
deployment, offering insights into current advancements, open challenges, and
future research directions. To the best of our knowledge, this is the first
dedicated and comprehensive survey on EdgeSNNs, providing an essential
reference for researchers and practitioners working at the intersection of
neuromorphic computing and edge intelligence.

</details>


### [5] [Shipwright: Proving liveness of distributed systems with Byzantine participants](https://arxiv.org/abs/2507.14080)
*Derek Leung,Nickolai Zeldovich,Frans Kaashoek*

Main category: cs.DC

TL;DR: Shipwright框架通过创新技术验证了PBFT在恶意环境下的活性与正确性，并实验展示了其可行性。


<details>
  <summary>Details</summary>
Motivation: 在去中心化系统中确保活性（如PBFT）至关重要，但传统方法无法验证可执行PBFT实现的活性。

Method: Shipwright引入了三种技术：支持恶意参与者的形式化推理、模块化分解系统与证明、以及对嵌入消息中的加密签名进行合理推理。

Result: Shipwright成功实现并验证了PBFT单日志条目的协议原型，并将其转化为可执行的Go实现，实验验证了其在常见及故障场景下的活性。

Conclusion: Shipwright成功验证了PBFT协议在存在恶意参与者情况下的正确性和活性，并通过实验证明了其在实际操作中的有效性。

Abstract: Ensuring liveness in a decentralized system, such as PBFT, is critical,
because there may not be any single administrator that can restart the system
if it encounters a liveness bug. At the same time, liveness is challenging to
achieve because any single participant could be malicious, and yet the overall
system must make forward progress. While verification is a promising approach
for ensuring the absence of bugs, no prior work has been able to verify
liveness for an executable implementation of PBFT.
  Shipwright is a verification framework for proving correctness and liveness
of distributed systems where some participants might be malicious. Shipwright
introduces three techniques that enable formal reasoning about decentralized
settings with malicious participants, allow developers to decompose their
system and proof in a modular fashion into sub-protocols and sub-proofs, and
support sound reasoning about cryptographic signatures that may be embedded in
messages. We used Shipwright to implement and verify an initial prototype of
agreement on a single log entry in PBFT (with a few limitations) and translate
it to an executable implementation in Go. We experimentally demonstrate its
operation and liveness both in the common case and in several failure
scenarios.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [6] [Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica](https://arxiv.org/abs/2507.13476)
*Jaber Daneshamooz,Jessica Nguyen,William Chen,Sanjay Chandrasekaran,Satyandra Guthula,Ankit Gupta,Arpit Gupta,Walter Willinger*

Main category: cs.NI

TL;DR: NetReplica通过生成逼真且可控的训练数据，显著提升机器学习模型在网络环境中的泛化能力，减少预测误差达47%。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在网络领域中因域适应问题导致的性能下降，即模型在一个环境中训练后在其他生产环境中表现不佳的问题。

Method: NetReplica将网络建模为具有特定属性的瓶颈链路集合，利用生产网络痕迹实现真实性，并通过精细控制每个链路属性的旋钮实现可控性。

Result: 评估显示，NetReplica不仅匹配现有数据特征，还能生成Puffer数据中代表性不足或缺失的逼真样本。使用NetReplica增强数据集训练的模型在挑战性网络条件下，传输时间预测误差最高减少了47%。

Conclusion: NetReplica通过生成具有真实协议动态和可控网络条件的训练数据集，显著提高了机器学习模型在不同网络环境中的泛化能力，为解决网络领域中的域适应问题迈出了重要一步。

Abstract: Machine learning models in networking suffer from the domain adaptation
problem; models trained in one domain often fail when deployed in different
production environments. This paper presents the design and implementation of
NetReplica, a system that addresses this challenge by generating training
datasets with two critical properties: realism in protocol dynamics and
controllability of network conditions. NetReplica models networks as
collections of bottleneck links with specific attributes, achieves realism by
leveraging production network traces, and enables controllability through fine
grained control knobs for each link attribute. Our evaluation using Puffer
demonstrates that NetReplica not only matches existing data characteristics but
generates realistic samples that are underrepresented in or absent from Puffer
data. Models trained on NetReplica augmented datasets show substantially
improved generalizability, reducing transmission time prediction error by up to
47% for challenging network conditions compared to models trained solely on
Puffer data. This work represents a significant step toward solving the domain
adaptation problem that has limited the effectiveness of ML based networking
systems.

</details>


### [7] [CARTS: Cooperative and Adaptive Resource Triggering and Stitching for 5G ISAC](https://arxiv.org/abs/2507.13676)
*Cheng Jiang,Yihe Yan,Yanxiang Wang,Jiawei Hu,Chun Tung Chou,Wen Hu*

Main category: cs.NI

TL;DR: CARTS是一种自适应5G上行链路感知方案，通过融合DMRS和SRS信号提升CSI更新频率和感知机会，显著提高了可扩展性和用户支持数量。


<details>
  <summary>Details</summary>
Motivation: 现代5G网络中，上行CSI由DMRS和SRS两种参考信号生成，但当前基站实现将它们视为独立信息流，限制了CSI的更新频率和感知机会。CARTS旨在通过融合这两种信号流，提升ISAC服务的性能。

Method: CARTS采用新颖的信道拼接和补偿方法整合异步CSI估计，并设计实时SRS触发算法补充不可控的DMRS调度，确保所有用户有足够且非冗余的感知机会。

Result: 实验证明CARTS显著提升了可扩展性，支持的用户数量是基线方案的两倍，同时保持较低的通道估计误差（NMSE 0.167）和较高的UE跟踪精度（85 cm）。

Conclusion: CARTS通过智能结合DMRS和SRS，提供了一种无需额外无线电资源的实用、标准兼容方案，显著提升了ISAC的CSI可用性。

Abstract: This paper presents CARTS, an adaptive 5G uplink sensing scheme designed to
provide Integrated Sensing and Communication (ISAC) services. The performance
of both communication and sensing fundamentally depends on the availability of
accurate and up-to-date channel state information (CSI). In modern 5G networks,
uplink CSI is derived from two reference signals: the demodulation reference
signal (DMRS) and the sounding reference signal (SRS). However, current base
station implementations treat these CSI measurements as separate information
streams. The key innovation of CARTS is to fuse these two CSI streams, thereby
increasing the frequency of CSI updates and extending sensing opportunities to
more users. CARTS addresses two key challenges: (i) a novel channel stitching
and compensation method that integrates asynchronous CSI estimates from DMRS
and SRS, despite their different time and frequency allocations, and (ii) a
real-time SRS triggering algorithm that complements the inherently
uncontrollable DMRS schedule, ensuring sufficient and non-redundant sensing
opportunities for all users. Our trace-driven evaluation shows that CARTS
significantly improves scalability, achieving a channel estimation error (NMSE)
of 0.167 and UE tracking accuracy of 85 cm while supporting twice the number of
users as a periodic SRS-only baseline with similar performance. By
opportunistically combining DMRS and SRS, CARTS therefore provides a practical,
standard-compliant solution to improve CSI availability for ISAC without
requiring additional radio resources.

</details>


### [8] [ATRO: A Fast Solver-Free Algorithm for Topology and Routing Optimization of Reconfigurable Datacenter Networks](https://arxiv.org/abs/2507.13717)
*Yingming Mao,Qiaozhu Zhai,Zhen Yao,Xia Zhu,Ximeng Liu,Xinchi Han*

Main category: cs.NI

TL;DR: ATRO是一种无求解器框架，通过交替进行拓扑和路由优化，解决了可重构DCNs中的优化问题，在单跳和多跳场景下均表现出色。


<details>
  <summary>Details</summary>
Motivation: 可重构数据中心网络（DCNs）的规模和复杂性不断增加，需要更高效和可扩展的算法来计算逻辑拓扑和路由。现有方法难以在解决方案质量和运行效率之间取得平衡。

Method: 引入了交替拓扑和路由优化（ATRO）框架，通过交替进行拓扑优化（TO）和路由优化（RO）来分解问题。TO子问题通过高效的加速二分搜索方法（ABSM）解决，RO则利用现有流量工程加速器。

Result: ATRO在单跳场景下达到全局最优，在多跳场景下显著优于基线方法，表现出良好的可扩展性和鲁棒性。

Conclusion: ATRO在单跳场景下达到全局最优，在多跳场景下在运行时间和解决方案质量上显著优于基线方法，证明了其在不同DCN中的可扩展性和鲁棒性。

Abstract: The growing scale and complexity of reconfigurable data center networks
(DCNs) demand more scalable and efficient algorithms for computing logical
topologies and routing. Reconfigurable DCNs typically operate in two modes:
one-hop configurations that require frequent topology optimization (TO), and
multi-hop scenarios that involve joint topology and routing optimization (TRO).
In both cases, the combinatorial nature of topology decisions makes it
difficult for existing methods to balance solution quality and runtime
efficiency. To address this, we introduce Alternating Topology and Routing
Optimization (ATRO), a solver-free framework that alternates between TO and
routing optimization (RO). This decomposition exploits two key insights: first,
each alternating update step monotonically reduces maximum link utilization
(MLU), ensuring consistent performance improvement across iterations; second,
the TO subproblem, equivalent to one-hop optimization, exhibits a monotonic
structure that enables optimal solutions via an efficient Accelerated Binary
Search Method (ABSM). To preserve the solver-free design, RO is solved using
existing Traffic Engineering accelerators. ATRO attains the global optimum in
one-hop scenarios and significantly outperforms baselines in multi-hop settings
in terms of both runtime and solution quality. Evaluations confirm its
scalability and robustness across diverse DCNs.

</details>


### [9] [On the Trade-Off Between Sum-Rate and Energy Efficiency through the Convergence of HAPS and Active RIS Technologies](https://arxiv.org/abs/2507.13889)
*Bilal Karaman,Ilhan Basturk,Ferdi Kara,Metin Ozturk,Sezai Taskin,Halil Yanikomeroglu*

Main category: cs.NI

TL;DR: Active RIS with HAPS enhances NTN performance, outperforming passive RIS in QoS and offering energy-efficient sub-connected architectures.


<details>
  <summary>Details</summary>
Motivation: The severe path loss and double fading in long-distance HAPS links make passive RIS architectures less effective, prompting the investigation of active RIS with signal amplification capabilities to enhance NTN performance in next-generation wireless systems.

Method: The paper formulates a sum-rate maximization problem to jointly optimize power allocation and RIS element assignment for ground UEs supported by a HAPS-based active RIS-assisted communication system. It also explores sub-connected active RIS architectures to reduce power consumption and hardware complexity.

Result: Simulation results show that active RIS configurations significantly outperform passive RIS in QoS, with fully-connected architectures achieving the highest throughput but sub-connected schemes demonstrating superior energy efficiency under practical power constraints.

Conclusion: Active RIS-enabled HAPS systems demonstrate significant potential in meeting the demands of beyond-cellular coverage and green networking, with sub-connected architectures offering a balance between performance and energy efficiency.

Abstract: This paper investigates the integration of active reconfigurable intelligent
surfaces (RIS) relay with high-altitude platform stations (HAPS) to enhance
non-terrestrial network (NTN) performance in next-generation wireless systems.
While prior studies focused on passive RIS architectures, the severe path loss
and double fading in long-distance HAPS links make active RIS a more suitable
alternative due to its inherent signal amplification capabilities. We formulate
a sum-rate maximization problem to jointly optimize power allocation and RIS
element assignment for ground user equipments (UEs) supported by a HAPS-based
active RIS-assisted communication system. To reduce power consumption and
hardware complexity, several sub-connected active RIS architectures are also
explored. Simulation results reveal that active RIS configurations
significantly outperform passive RIS in terms of quality of service (QoS).
Moreover, although fully-connected architectures achieve the highest
throughput, sub-connected schemes demonstrate superior energy efficiency under
practical power constraints. These findings highlight the potential of active
RIS-enabled HAPS systems to meet the growing demands of beyond-cellular
coverage and green networking.

</details>


### [10] [Preprint: Did I Just Browse A Website Written by LLMs?](https://arxiv.org/abs/2507.13933)
*Sichang "Steven" He,Ramesh Govindan,Harsha V. Madhyastha*

Main category: cs.NI

TL;DR: 该论文提出了一种检测LLM-dominant网站的高效方法，解决了现有检测器在复杂网页内容上的不足，并在实际应用中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 由于LLM生成的内容可能不可靠且不道德，且网站很少披露此类内容，人类读者难以区分，因此需要开发可靠的LLM-dominant内容检测器。

Method: 提出了一种高度可靠、可扩展的管道方法，通过对多个散文类页面的LLM文本检测器输出进行分类，来对整个网站进行分类。

Result: 在两个不同的真实数据集上测试获得了100%的准确率，并在实际网络环境中检测到了大量LLM-dominant网站。

Conclusion: LLM-dominant网站的普及和搜索引擎排名上升，引发了对终端用户和整个网络生态系统影响的担忧。

Abstract: Increasingly, web content is automatically generated by large language models
(LLMs) with little human input. We call this "LLM-dominant" content. Since LLMs
plagiarize and hallucinate, LLM-dominant content can be unreliable and
unethical. Yet, websites rarely disclose such content, and human readers
struggle to distinguish it. Thus, we must develop reliable detectors for
LLM-dominant content. However, state-of-the-art LLM detectors are insufficient,
because they perform well mainly on clean, prose-like text, while web content
has complex markup and diverse genres.
  We propose a highly reliable, scalable pipeline that classifies entire
websites. Instead of naively classifying text extracted from each page, we
classify each site based on an LLM text detector's outputs of multiple
prose-like pages. We train and evaluate our detector by collecting 2 distinct
ground truth datasets totaling 120 sites, and obtain 100% accuracies testing
across them. In the wild, we detect a sizable portion of sites as LLM-dominant
among 10k sites in search engine results and 10k in Common Crawl archives. We
find LLM-dominant sites are growing in prevalence and rank highly in search
results, raising questions about their impact on end users and the overall Web
ecosystem.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination](https://arxiv.org/abs/2507.13511)
*Nabil Abdelaziz Ferhat Taleb,Abdolazim Rezaei,Raj Atulkumar Patel,Mehdi Sookhak*

Main category: cs.AI

TL;DR: GraphTrafficGPT是一种图基架构，优化LLM在交通管理中的任务协调，显著提升效率并支持并行处理。


<details>
  <summary>Details</summary>
Motivation: 当前基于链的系统如TrafficGPT存在顺序任务执行、高令牌使用和扩展性差的问题，无法满足复杂实际场景需求。

Method: 提出GraphTrafficGPT，一种基于图的架构，通过Brain Agent分解用户查询、构建优化依赖图，并协调多个专业代理进行数据检索、分析、可视化和模拟。

Result: 实验显示，GraphTrafficGPT令牌消耗减少50.2%，平均响应延迟降低19.0%，并行多查询效率提升23.0%。

Conclusion: GraphTrafficGPT通过图基架构显著提升了LLM在交通管理中的效率，减少了令牌消耗和响应延迟，支持并行多查询处理。

Abstract: Large Language Models (LLMs) offer significant promise for intelligent
traffic management; however, current chain-based systems like TrafficGPT are
hindered by sequential task execution, high token usage, and poor scalability,
making them inefficient for complex, real-world scenarios. To address these
limitations, we propose GraphTrafficGPT, a novel graph-based architecture,
which fundamentally redesigns the task coordination process for LLM-driven
traffic applications. GraphTrafficGPT represents tasks and their dependencies
as nodes and edges in a directed graph, enabling efficient parallel execution
and dynamic resource allocation. The main idea behind the proposed model is a
Brain Agent that decomposes user queries, constructs optimized dependency
graphs, and coordinates a network of specialized agents for data retrieval,
analysis, visualization, and simulation. By introducing advanced context-aware
token management and supporting concurrent multi-query processing, the proposed
architecture handles interdependent tasks typical of modern urban mobility
environments. Experimental results demonstrate that GraphTrafficGPT reduces
token consumption by 50.2% and average response latency by 19.0% compared to
TrafficGPT, while supporting simultaneous multi-query execution with up to
23.0% improvement in efficiency.

</details>


### [12] [PrefPalette: Personalized Preference Modeling with Latent Attributes](https://arxiv.org/abs/2507.13541)
*Shuyue Stella Li,Melanie Sclar,Hunter Lang,Ansong Ni,Jacqueline He,Puxin Xu,Andrew Cohen,Chan Young Park,Yulia Tsvetkov,Asli Celikyilmaz*

Main category: cs.AI

TL;DR: PrefPalette是一个分解偏好为属性维度的框架，通过反事实属性合成和注意力建模，显著提升预测准确性，并揭示社区特定偏好特征。


<details>
  <summary>Details</summary>
Motivation: 当前偏好模型通常将人类判断视为黑箱，而理解用户偏好的底层原因对于个性化AI系统至关重要。PrefPalette旨在分解偏好为属性维度，并以人类可解释的方式针对不同社交社区价值观进行预测。

Method: PrefPalette采用两种方式操作多属性决策认知科学原理：(1) 可扩展的反事实属性合成步骤，生成合成训练数据以隔离单个属性效应；(2) 基于注意力的偏好建模，学习不同社交社区如何动态加权这些属性。

Result: 在Reddit的45个社交社区中评估时，PrefPalette的平均预测准确率比GPT-4o高出46.6%。此外，PrefPalette揭示了直观的社区特定偏好特征。

Conclusion: PrefPalette通过建模人类判断的属性介导结构，不仅提供了更优的偏好预测，还提供了透明且可解释的洞察，是迈向更值得信赖、价值感知个性化应用的第一步。

Abstract: Personalizing AI systems requires understanding not just what users prefer,
but the reasons that underlie those preferences - yet current preference models
typically treat human judgment as a black box. We introduce PrefPalette, a
framework that decomposes preferences into attribute dimensions and tailors its
preference prediction to distinct social community values in a
human-interpretable manner. PrefPalette operationalizes a cognitive science
principle known as multi-attribute decision making in two ways: (1) a scalable
counterfactual attribute synthesis step that involves generating synthetic
training data to isolate for individual attribute effects (e.g., formality,
humor, cultural values), and (2) attention-based preference modeling that
learns how different social communities dynamically weight these attributes.
This approach moves beyond aggregate preference modeling to capture the diverse
evaluation frameworks that drive human judgment. When evaluated on 45 social
communities from the online platform Reddit, PrefPalette outperforms GPT-4o by
46.6% in average prediction accuracy. Beyond raw predictive improvements,
PrefPalette also shed light on intuitive, community-specific profiles:
scholarly communities prioritize verbosity and stimulation, conflict-oriented
communities value sarcasm and directness, and support-based communities
emphasize empathy. By modeling the attribute-mediated structure of human
judgment, PrefPalette delivers both superior preference modeling and
transparent, interpretable insights, and serves as a first step toward more
trustworthy, value-aware personalized applications.

</details>


### [13] [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
*Eduardo C. Garrido-Merchán,Cristina Puente*

Main category: cs.AI

TL;DR: 论文提出了一种结合LLMs和符号系统的混合方法，通过结构化提示和专家验证，提升专家系统的可靠性和透明度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在幻觉或自信生成不可验证事实的缺点，需要一种可控、透明的方法来开发专家系统。

Method: 通过限制领域并使用结构化的提示提取方法，将知识转化为Prolog符号表示，并由人类专家验证和修正。

Result: 定量和定性实验表明，使用Claude Sonnet 3.7和GPT-4.1生成的知识库在事实一致性和语义连贯性上表现良好。

Conclusion: 本研究提出了一种透明、可控的混合方法，结合大型语言模型的召回能力和符号系统的精确性，为敏感领域的可靠AI应用奠定了基础。

Abstract: The development of large language models (LLMs) has successfully transformed
knowledge-based systems such as open domain question nswering, which can
automatically produce vast amounts of seemingly coherent information. Yet,
those models have several disadvantages like hallucinations or confident
generation of incorrect or unverifiable facts. In this paper, we introduce a
new approach to the development of expert systems using LLMs in a controlled
and transparent way. By limiting the domain and employing a well-structured
prompt-based extraction approach, we produce a symbolic representation of
knowledge in Prolog, which can be validated and corrected by human experts.
This approach also guarantees interpretability, scalability and reliability of
the developed expert systems. Via quantitative and qualitative experiments with
Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic
coherence on our generated knowledge bases. We present a transparent hybrid
solution that combines the recall capacity of LLMs with the precision of
symbolic systems, thereby laying the foundation for dependable AI applications
in sensitive domains.

</details>


### [14] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 本文探讨了关系学习在AI中的重要性及其当前应用的局限性，呼吁更多研究以推动其发展。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要关注像素和文本等表面数据，而忽略了实体及其关系的建模，这与现实世界的数据结构不符。关系学习能更好地处理这些数据，但尚未得到广泛应用。

Method: 通过分析现有数据形式和机器学习方法，探讨关系学习在AI领域的应用现状和局限性。

Result: 关系学习在特定受限关系场景中表现良好，但在广泛应用中仍面临挑战。

Conclusion: 本文认为关系学习应成为AI领域的主流方法，但当前由于技术限制尚未实现其潜力，需要进一步研究以提升其影响力。

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [15] [BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety](https://arxiv.org/abs/2507.13625)
*Yuxin Zhang,Xi Wang,Mo Hu,Zhenyu Zhang*

Main category: cs.AI

TL;DR: BifrostRAG 是一种双图 RAG 系统，通过混合检索机制显著提高了多跳问题回答的性能，适用于复杂技术文档的导航。


<details>
  <summary>Details</summary>
Motivation: 信息检索和问答在自动化的建筑合规检查中至关重要，但受到法规文本的语言和结构复杂性的阻碍。许多合规相关的查询是多跳的，需要跨相互链接的条款综合信息，这对传统的检索增强生成（RAG）系统提出了挑战。

Method: BifrostRAG 是一种双图 RAG 集成系统，明确建模了语言关系（通过实体网络图）和文档结构（通过文档导航图）。这种架构支持了一种混合检索机制，结合了图遍历和基于向量的语义搜索。

Result: 评估显示，BifrostRAG 在多跳问题数据集上达到了 92.8% 的精确率、85.5% 的召回率和 87.3% 的 F1 分数，显著优于仅基于向量或图的 RAG 基线方法。

Conclusion: BifrostRAG 被确立为一种强大的知识引擎，适用于基于大型语言模型的合规检查。其双图和混合检索机制为跨知识密集型工程领域的复杂技术文档导航提供了可转移的蓝图。

Abstract: Information retrieval and question answering from safety regulations are
essential for automated construction compliance checking but are hindered by
the linguistic and structural complexity of regulatory text. Many
compliance-related queries are multi-hop, requiring synthesis of information
across interlinked clauses. This poses a challenge for traditional
retrieval-augmented generation (RAG) systems. To overcome this, we introduce
BifrostRAG: a dual-graph RAG-integrated system that explicitly models both
linguistic relationships (via an Entity Network Graph) and document structure
(via a Document Navigator Graph). This architecture powers a hybrid retrieval
mechanism that combines graph traversal with vector-based semantic search,
enabling large language models to reason over both the meaning and the
structure of the text. Evaluation on a multi-hop question dataset shows that
BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1
score of 87.3 percent. These results significantly outperform vector-only and
graph-only RAG baselines that represent current leading approaches. Error
analysis further highlights the comparative advantages of our hybrid method
over single-modality RAGs. These findings establish BifrostRAG as a robust
knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid
retrieval mechanism offers a transferable blueprint for navigating complex
technical documents across knowledge-intensive engineering domains.

</details>


### [16] [Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks](https://arxiv.org/abs/2507.13651)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: A study explores using final answers to diagnose errors in student steps for quadratic equations, achieving 29.4% diagnosis rate and 97% alignment with teacher assessments.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge of combinatorial explosion in error diagnosis when students combine multiple steps in one, making it hard to diagnose errors. Using final answers can mitigate this by focusing on fewer possible erroneous final answers rather than numerous solution paths.

Method: The study investigates the design of a service for buggy rule diagnosis by automating error diagnosis based on final answers. It validates the approach using an existing dataset of student steps in solving quadratic equations.

Result: Results show that the final answer evaluation method diagnoses 29.4% of previously undiagnosed steps, with 97% alignment with teacher diagnoses on a subset of the data.

Conclusion: The study concludes that using final answer evaluation for error diagnosis can effectively diagnose 29.4% of previously undiagnosed steps in solving quadratic equations, with a high alignment (97%) with teacher diagnoses. This serves as a foundation for further exploration of the approach.

Abstract: Many intelligent tutoring systems can support a student in solving a stepwise
task. When a student combines several steps in one step, the number of possible
paths connecting consecutive inputs may be very large. This combinatorial
explosion makes error diagnosis hard. Using a final answer to diagnose a
combination of steps can mitigate the combinatorial explosion, because there
are generally fewer possible (erroneous) final answers than (erroneous)
solution paths. An intermediate input for a task can be diagnosed by
automatically completing it according to the task solution strategy and
diagnosing this solution. This study explores the potential of automated error
diagnosis based on a final answer. We investigate the design of a service that
provides a buggy rule diagnosis when a student combines several steps. To
validate the approach, we apply the service to an existing dataset (n=1939) of
unique student steps when solving quadratic equations, which could not be
diagnosed by a buggy rule service that tries to connect consecutive inputs with
a single rule. Results show that final answer evaluation can diagnose 29,4% of
these steps. Moreover, a comparison of the generated diagnoses with teacher
diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the
cases. These results can be considered a basis for further exploration of the
approach.

</details>


### [17] [Combining model tracing and constraint-based modeling for multistep strategy diagnoses](https://arxiv.org/abs/2507.13652)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 提出结合模型追踪和基于约束的建模方法，有效诊断学生多步策略，实证验证与教师诊断一致。


<details>
  <summary>Details</summary>
Motivation: 解决模型追踪在组合步骤诊断中的局限性，提供更灵活的学生输入诊断方法。

Method: 结合模型追踪和基于约束的建模，定义约束为学生输入与策略步骤共有的属性，以支持多步策略诊断。

Result: 系统诊断与教师编码在140个学生步骤中完全一致。

Conclusion: 该系统诊断与教师编码在所有140个学生步骤中完全一致，验证了方法的有效性。

Abstract: Model tracing and constraint-based modeling are two approaches to diagnose
student input in stepwise tasks. Model tracing supports identifying consecutive
problem-solving steps taken by a student, whereas constraint-based modeling
supports student input diagnosis even when several steps are combined into one
step. We propose an approach that merges both paradigms. By defining
constraints as properties that a student input has in common with a step of a
strategy, it is possible to provide a diagnosis when a student deviates from a
strategy even when the student combines several steps. In this study we explore
the design of a system for multistep strategy diagnoses, and evaluate these
diagnoses. As a proof of concept, we generate diagnoses for an existing dataset
containing steps students take when solving quadratic equations (n=2136). To
compare with human diagnoses, two teachers coded a random sample of deviations
(n=70) and applications of the strategy (n=70). Results show that that the
system diagnosis aligned with the teacher coding in all of the 140 student
steps.

</details>


### [18] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian,Xiaoyuan Ren,Zihao Wang,Onat Gungor,Xiaofan Yu,Tajana Rosing*

Main category: cs.AI

TL;DR: DailyLLM 是一种基于轻量级 LLM 的活动日志生成系统，整合多维度上下文信息，显著提升生成精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有活动日志生成方法在准确性、效率和语义丰富度方面存在明显不足，DailyLLM 旨在解决这些问题。

Method: DailyLLM 提出了一种基于轻量级 LLM 的框架，结合结构化提示和高效特征提取，实现了高级活动理解。

Result: DailyLLM 在日志生成的 BERTScore 精度上比 70B 参数的 SOTA 基线提高了 17%，推理速度提升了近 10 倍，且可在个人电脑和树莓派上高效部署。

Conclusion: DailyLLM 是一种高效、轻量级的日志生成与摘要系统，通过整合四个维度的上下文活动信息，显著提升了日志生成的准确性和语义丰富度，同时保持了较高的运行效率。

Abstract: Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [19] [OntView: What you See is What you Meant](https://arxiv.org/abs/2507.13759)
*Carlos Bobed,Carlota Quintana,Eduardo Mena,Jorge Bobed,Fernando Bobillo*

Main category: cs.AI

TL;DR: OntView是一个开源的本体可视化工具，通过DL推理机和简化视图功能，直观展示本体概念及其关系，解决了现有工具的不足。


<details>
  <summary>Details</summary>
Motivation: 现有本体可视化工具无法以直观且非压倒性的方式图形化表示本体结构，限制了用户对大型本体框架中依赖关系和属性的理解。

Method: OntView基于DL推理机，采用“所见即所意”范式，可视化通用概念包含（GCI），并提供简化视图功能（如本体摘要、TBox元素聚焦和动态分支隐藏/显示）。

Result: OntView成功实现了直观的本体概念可视化，特别是GCI的可视化，并通过简化视图功能避免了信息过载。

Conclusion: OntView通过开源许可证发布，为整个社区提供了一个直观的可视化工具，有效解决了现有本体可视化工具的不足。

Abstract: In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.

</details>


### [20] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: CUDA-L1是一种自动化强化学习框架，用于优化CUDA性能，无需人类干预，显著提升GPU效率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，对GPU计算资源的需求呈指数级增长，亟需自动化的CUDA优化策略。尽管LLMs在代码生成方面显示出潜力，但当前最先进的模型在提升CUDA速度方面成功率较低。

Method: CUDA-L1是一个用于CUDA优化的自动化强化学习框架，通过在NVIDIA A100上进行训练，实现了对250个CUDA内核的平均加速17.7倍，峰值加速达到449倍。

Result: CUDA-L1在CUDA优化任务中实现了性能提升：在NVIDIA A100上训练后，对KernelBench的250个CUDA内核实现了平均17.7倍的加速，峰值加速达到449倍。此外，该模型还展示了出色的跨GPU架构可移植性。

Conclusion: CUDA-L1展示了强化学习如何仅通过基于加速的奖励信号，将初始性能较差的LLM转变为有效的CUDA优化器，无需人类专业知识或领域知识。更重要的是，训练后的RL模型能够将获得的推理能力扩展到新内核。这一范式为CUDA操作的自动化优化开辟了可能性，并有望显著提升GPU效率，缓解GPU计算资源的日益增长的压力。

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>


### [21] [From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning](https://arxiv.org/abs/2507.13768)
*Renato Ghisellini,Remo Pareschi,Marco Pedroni,Giovanni Battista Raggi*

Main category: cs.AI

TL;DR: 论文提出了一种混合架构，通过启发式提取、语义激活和组合合成增强代理的战略推理能力。该系统融合冲突启发式方法，生成连贯叙述，并通过案例研究初步验证。


<details>
  <summary>Details</summary>
Motivation: 传统决策引擎通常选择最佳规则，而忽略了启发式方法之间的冲突和上下文敏感性。论文旨在通过量子认知研究的启发，开发一种能够融合冲突启发式方法并生成连贯叙述的模型，以增强代理的战略推理能力。

Method: 论文提出了一种混合架构，结合了启发式提取、语义激活和组合合成的方法。该方法通过语义相互依赖的过程激活和组合多种启发式方法，不同于传统决策引擎选择最佳规则的方式，而是通过语义交互建模和修辞框架将冲突的启发式方法融合为连贯的叙述。

Result: 论文通过Meta vs. FTC的案例研究展示了该框架的初步有效性，并通过语义指标进行了验证。结果表明，该系统能够生成连贯且上下文敏感的叙述，尽管存在一些局限性（如动态干扰调谐）。

Conclusion: 论文总结了一种结合启发式提取、语义激活和组合合成的混合架构，用于增强代理的战略推理能力。通过量子认知研究的启发，该系统能够融合冲突的启发式方法，形成连贯且上下文敏感的叙述。尽管存在局限性（如动态干扰调谐），但通过案例研究和语义指标的初步验证，展示了其潜力。

Abstract: We present a hybrid architecture for agent-augmented strategic reasoning,
combining heuristic extraction, semantic activation, and compositional
synthesis. Drawing on sources ranging from classical military theory to
contemporary corporate strategy, our model activates and composes multiple
heuristics through a process of semantic interdependence inspired by research
in quantum cognition. Unlike traditional decision engines that select the best
rule, our system fuses conflicting heuristics into coherent and
context-sensitive narratives, guided by semantic interaction modeling and
rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,
with preliminary validation through semantic metrics. Limitations and
extensions (e.g., dynamic interference tuning) are discussed.

</details>


### [22] [When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction](https://arxiv.org/abs/2507.13825)
*Haoyang Li,Yuming Xu,Yiming Li,Hanmo Liu,Darian Li,Chen Jason Zhang,Lei Chen,Qing Li*

Main category: cs.AI

TL;DR: EAGLE是一个轻量级框架，结合短期时间邻近性和长期全局结构模式，通过自适应权重机制显著提升动态图时间链接预测的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有T-GNNs虽然能建模时间和结构依赖，但因计算开销大而面临可扩展性和效率挑战。

Method: EAGLE通过时间感知模块和结构感知模块分别捕捉短期时间邻近性和长期全局结构模式，并采用自适应权重机制动态调整两者贡献。

Result: 在七个真实世界时序图上的实验表明，EAGLE在效果和效率上均优于最先进的T-GNNs。

Conclusion: EAGLE框架在动态图的时间链接预测任务中表现出色，不仅在效果上优于现有T-GNNs，还显著提升了计算效率，实现了50倍以上的加速。

Abstract: Temporal link prediction in dynamic graphs is a critical task with
applications in diverse domains such as social networks, recommendation
systems, and e-commerce platforms. While existing Temporal Graph Neural
Networks (T-GNNs) have achieved notable success by leveraging complex
architectures to model temporal and structural dependencies, they often suffer
from scalability and efficiency challenges due to high computational overhead.
In this paper, we propose EAGLE, a lightweight framework that integrates
short-term temporal recency and long-term global structural patterns. EAGLE
consists of a time-aware module that aggregates information from a node's most
recent neighbors to reflect its immediate preferences, and a structure-aware
module that leverages temporal personalized PageRank to capture the influence
of globally important nodes. To balance these attributes, EAGLE employs an
adaptive weighting mechanism to dynamically adjust their contributions based on
data characteristics. Also, EAGLE eliminates the need for complex multi-hop
message passing or memory-intensive mechanisms, enabling significant
improvements in efficiency. Extensive experiments on seven real-world temporal
graphs demonstrate that EAGLE consistently achieves superior performance
against state-of-the-art T-GNNs in both effectiveness and efficiency,
delivering more than a 50x speedup over effective transformer-based T-GNNs.

</details>


### [23] [Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.13846)
*Kathrin Korte,Christian Medeiros Adriano,Sona Ghahremani,Holger Giese*

Main category: cs.AI

TL;DR: 本文提出了一种因果知识转移框架，帮助智能体在非平稳环境中零样本共享恢复策略，减少重新训练需求。


<details>
  <summary>Details</summary>
Motivation: 解决传统知识转移方法在非平稳环境中泛化能力不足的问题，减少智能体适应新环境时的重新训练成本。

Method: 本文提出了一种因果知识转移框架，通过建模碰撞为因果干预，并生成恢复动作宏，实现零样本知识转移。

Result: 实验表明，该方法能使异质目标智能体在新环境中适应时，填补随机探索与完全重新训练策略之间约一半的差距。

Conclusion: 因果知识转移框架在多智能体强化学习中显示出潜力，特别是在非平稳环境中，能够减少重新训练的需求并提高适应性。

Abstract: [Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.

</details>


### [24] [Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery](https://arxiv.org/abs/2507.13874)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.AI

TL;DR: 论文提出了一种模型无关的潜在空间创意框架，无需手工规则即可实现可控创造力，初步结果展示了其在人类-AI协作中的潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成新颖且相关的内容方面存在局限性，现有解决方案依赖领域特定启发式方法，泛化能力差。

Method: 通过导航连续嵌入空间中的创意，框架无需手工规则，并能轻松适应不同领域、输入格式和创意任务。

Result: 初步结果显示该框架作为一种通用共创工具具有潜力。

Conclusion: 该论文提出了一个模型无关的潜在空间创意框架，能够在不同领域和任务中实现可控且可扩展的创造力，为人类-AI协作提供了一个通用工具。

Abstract: Innovative idea generation remains a core challenge in AI, as large language
models (LLMs) often struggle to produce outputs that are both novel and
relevant. Despite their fluency, LLMs tend to replicate patterns seen during
training, limiting their ability to diverge creatively without extensive prompt
engineering. Prior work has addressed this through domain-specific heuristics
and structured prompting pipelines, but such solutions are brittle and
difficult to generalize. In this paper, we propose a model-agnostic
latent-space ideation framework that enables controlled, scalable creativity by
navigating the continuous embedding space of ideas. Unlike prior methods, our
framework requires no handcrafted rules and adapts easily to different domains,
input formats, and creative tasks. This paper introduces an early-stage
prototype of our method, outlining the conceptual framework and preliminary
results highlighting its potential as a general-purpose co-ideator for human-AI
collaboration.

</details>


### [25] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: ADPC是一种结合视觉和语言数据的因果干预框架，用于阿尔茨海默病早期诊断，显著提升了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 早期识别和干预轻度认知障碍（MCI）可以有效减缓其进展为痴呆，但由于多模态数据的选择偏差和变量间复杂关系，阿尔茨海默病（AD）的诊断仍具挑战性。

Method: 研究提出了一种名为ADPC的新型视觉-语言因果干预框架，利用大型语言模型（LLM）总结临床数据，并结合MRI和fMRI图像进行多模态分类。

Result: ADPC框架通过因果干预隐式消除混杂因素，在CN/MCI/AD分类任务中表现优异，达到了最先进的评估指标。

Conclusion: 该研究展示了将因果推理与多模态学习相结合在神经疾病诊断中的潜力，特别是在区分认知正常、轻度认知障碍和阿尔茨海默病方面取得了最先进的性能。

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [26] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar,Martín Diéguez,François Olivier,Torsten Schaub,Igor Stéphan*

Main category: cs.AI

TL;DR: 论文提出了一种结合线性时间Here-and-There逻辑和约束逻辑的新方法，为ASP范式下的高分辨率动态系统推理提供了首个非单调时序约束解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决逻辑方法（如ASP）在细粒度时间和数值分辨率的动态系统推理中的挑战。

Method: 通过结合Here-and-There逻辑的线性时间版本（提供非单调时序推理能力）和约束逻辑（支持数值约束的直接集成与操作），提出了一种新颖的时序和约束扩展。

Result: 提出了一种专为ASP设计的非单调时序约束推理方法，填补了该领域的空白。

Conclusion: 该论文建立了一个新的逻辑框架，结合了Here-and-There逻辑的线性时间版本和约束逻辑，为ASP范式下的高分辨率动态系统提供了基础支持。

Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [27] [KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models](https://arxiv.org/abs/2507.14032)
*Lam Nguyen,Erika Barcelos,Roger French,Yinghui Wu*

Main category: cs.AI

TL;DR: KROMA是一个新颖的本体匹配框架，结合LLMs和RAG管道，通过优化技术显著提升匹配性能，同时控制通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决现有本体匹配系统依赖手工规则或专用模型、适应性有限的问题。

Method: KROMA利用检索增强生成（RAG）管道中的大型语言模型（LLMs），动态丰富本体匹配任务的语义上下文，并结合双相似性概念匹配和轻量级本体细化步骤来优化性能和效率。

Result: 在多个基准数据集上的实验显示，KROMA在性能上超越了经典本体匹配系统和前沿的LLM-based方法。

Conclusion: KROMA框架通过结合知识检索与上下文增强的LLMs，显著提升了本体匹配的性能，同时保持了可比的通信开销，证明了所提出的优化技术在大规模本体匹配中的可行性和益处。

Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.

</details>


### [28] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau,Baiying Lu,Yanjun Cui*

Main category: cs.AI

TL;DR: Glucose-ML是一个包含10个公开糖尿病数据集的集合，旨在解决AI开发中的数据获取问题，并通过案例研究展示了数据集差异对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决糖尿病管理领域AI开发中高质量数据集获取的障碍，促进透明、可重复和鲁棒的AI解决方案发展。

Method: 提出了Glucose-ML，一个包含10个公开糖尿病数据集的集合，并进行了数据选择和血糖预测的案例研究。

Result: 展示了相同算法在不同数据集上的预测结果差异显著，并提供了数据选择和算法开发的建议。

Conclusion: 本研究通过Glucose-ML数据集集合和案例研究，为糖尿病管理领域的AI开发者提供了数据选择和算法开发的实用建议，强调了数据集差异对AI模型性能的影响。

Abstract: Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


### [29] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer,Neel Macwan,Atharva Jitendra Hude,Heejin Jeong,Shenghan Guo*

Main category: cs.AI

TL;DR: G-AI-HMS利用生成式AI提升工业任务中人类运动模拟的保真度，通过文本到动作模型和计算机视觉验证，显著减少误差。


<details>
  <summary>Details</summary>
Motivation: 现有的人类运动模拟方法存在运动保真度低的问题，影响了工人行为、安全和生产效率的评估。

Method: 研究采用生成式AI技术，结合大型语言模型和MotionGPT的训练词汇，将任务描述转换为运动感知语言，并通过计算机视觉验证AI增强动作与真实人类运动的一致性。

Result: 在八项任务的案例研究中，AI增强的动作在大多数场景中表现出比人类创建描述更低的误差，尤其在空间准确性、姿势归一化后的对齐和整体时间相似性方面表现更优。

Conclusion: G-AI-HMS通过整合文本到文本和文本到动作模型，显著提升了工业任务中人类运动模拟的质量，减少了关节误差和时间错位。

Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


### [30] [Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment](https://arxiv.org/abs/2507.14107)
*Viraj Nishesh Darji,Callie C. Liao,Duoduo Liao*

Main category: cs.AI

TL;DR: LLM可高效解读桥梁NDE数据，ChatGPT-4和Claude 3.5 Sonnet表现突出，提升维护决策效率。


<details>
  <summary>Details</summary>
Motivation: 传统NDE数据分析耗时且依赖专家，LLM的进步为自动化分析提供了新可能。

Method: 研究评估了九种LLM模型，通过设计的提示词生成图像描述，并基于描述的详细程度、缺陷识别、建议提供和准确性进行评分。

Result: 九种模型中有四种表现优异，能生成全面描述，而ChatGPT-4和Claude 3.5 Sonnet的总结效果最佳。

Conclusion: LLMs，特别是ChatGPT-4和Claude 3.5 Sonnet，在解释NDE等高线图方面表现出色，显著提高了桥梁维护决策的效率和准确性。

Abstract: Bridge maintenance and safety are essential for transportation authorities,
and Non-Destructive Evaluation (NDE) techniques are critical to assessing
structural integrity. However, interpreting NDE data can be time-consuming and
requires expertise, potentially delaying decision-making. Recent advancements
in Large Language Models (LLMs) offer new ways to automate and improve this
analysis. This pilot study introduces a holistic assessment of LLM capabilities
for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in
providing detailed bridge condition analyses. It establishes a framework for
integrating LLMs into bridge inspection workflows, indicating that LLM-assisted
analysis can enhance efficiency without compromising accuracy. In this study,
several LLMs are explored with prompts specifically designed to enhance the
quality of image descriptions, which are applied to interpret five different
NDE contour maps obtained through technologies for assessing bridge conditions.
Each LLM model is evaluated based on its ability to produce detailed
descriptions, identify defects, provide actionable recommendations, and
demonstrate overall accuracy. The research indicates that four of the nine
models provide better image descriptions, effectively covering a wide range of
topics related to the bridge's condition. The outputs from these four models
are summarized using five different LLMs to form a comprehensive overview of
the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more
effective summaries. The findings suggest that LLMs have the potential to
significantly improve efficiency and accuracy. This pilot study presents an
innovative approach that leverages LLMs for image captioning in parallel and
summarization, enabling faster decision-making in bridge maintenance and
enhancing infrastructure management and safety assessments.

</details>
