{"id": "2507.13476", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.13476", "abs": "https://arxiv.org/abs/2507.13476", "authors": ["Jaber Daneshamooz", "Jessica Nguyen", "William Chen", "Sanjay Chandrasekaran", "Satyandra Guthula", "Ankit Gupta", "Arpit Gupta", "Walter Willinger"], "title": "Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica", "comment": null, "summary": "Machine learning models in networking suffer from the domain adaptation\nproblem; models trained in one domain often fail when deployed in different\nproduction environments. This paper presents the design and implementation of\nNetReplica, a system that addresses this challenge by generating training\ndatasets with two critical properties: realism in protocol dynamics and\ncontrollability of network conditions. NetReplica models networks as\ncollections of bottleneck links with specific attributes, achieves realism by\nleveraging production network traces, and enables controllability through fine\ngrained control knobs for each link attribute. Our evaluation using Puffer\ndemonstrates that NetReplica not only matches existing data characteristics but\ngenerates realistic samples that are underrepresented in or absent from Puffer\ndata. Models trained on NetReplica augmented datasets show substantially\nimproved generalizability, reducing transmission time prediction error by up to\n47% for challenging network conditions compared to models trained solely on\nPuffer data. This work represents a significant step toward solving the domain\nadaptation problem that has limited the effectiveness of ML based networking\nsystems.", "AI": {"tldr": "NetReplica enhances ML model generalizability in networking by generating realistic, controllable training data, reducing prediction errors by up to 47%.", "motivation": "Machine learning models in networking often fail when deployed in different environments due to the domain adaptation problem. NetReplica aims to solve this by generating datasets that ensure realism and controllability.", "method": "NetReplica generates realistic and controllable training datasets by modeling networks as bottleneck links with specific attributes, leveraging production traces, and providing fine-grained control knobs.", "result": "Evaluation with Puffer shows NetReplica matches existing data characteristics and generates underrepresented realistic samples, improving model generalizability.", "conclusion": "NetReplica significantly improves the generalizability of machine learning models in networking by addressing the domain adaptation problem, reducing prediction errors by up to 47%."}}
{"id": "2507.13676", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13676", "abs": "https://arxiv.org/abs/2507.13676", "authors": ["Cheng Jiang", "Yihe Yan", "Yanxiang Wang", "Jiawei Hu", "Chun Tung Chou", "Wen Hu"], "title": "CARTS: Cooperative and Adaptive Resource Triggering and Stitching for 5G ISAC", "comment": null, "summary": "This paper presents CARTS, an adaptive 5G uplink sensing scheme designed to\nprovide Integrated Sensing and Communication (ISAC) services. The performance\nof both communication and sensing fundamentally depends on the availability of\naccurate and up-to-date channel state information (CSI). In modern 5G networks,\nuplink CSI is derived from two reference signals: the demodulation reference\nsignal (DMRS) and the sounding reference signal (SRS). However, current base\nstation implementations treat these CSI measurements as separate information\nstreams. The key innovation of CARTS is to fuse these two CSI streams, thereby\nincreasing the frequency of CSI updates and extending sensing opportunities to\nmore users. CARTS addresses two key challenges: (i) a novel channel stitching\nand compensation method that integrates asynchronous CSI estimates from DMRS\nand SRS, despite their different time and frequency allocations, and (ii) a\nreal-time SRS triggering algorithm that complements the inherently\nuncontrollable DMRS schedule, ensuring sufficient and non-redundant sensing\nopportunities for all users. Our trace-driven evaluation shows that CARTS\nsignificantly improves scalability, achieving a channel estimation error (NMSE)\nof 0.167 and UE tracking accuracy of 85 cm while supporting twice the number of\nusers as a periodic SRS-only baseline with similar performance. By\nopportunistically combining DMRS and SRS, CARTS therefore provides a practical,\nstandard-compliant solution to improve CSI availability for ISAC without\nrequiring additional radio resources.", "AI": {"tldr": "CARTS \u662f\u4e00\u79cd\u81ea\u9002\u5e945G\u4e0a\u884c\u94fe\u8def\u611f\u77e5\u65b9\u6848\uff0c\u901a\u8fc7\u878d\u5408 DMRS \u548c SRS \u7684 CSI \u6d41\uff0c\u663e\u8457\u63d0\u5347 ISAC \u6027\u80fd\uff0c\u652f\u6301\u66f4\u591a\u7528\u6237\u4e14\u65e0\u9700\u989d\u5916\u8d44\u6e90\u3002", "motivation": "\u73b0\u4ee35G\u7f51\u7edc\u4e2d\uff0c\u4e0a\u884c\u94fe\u8def\u7684 CSI \u6765\u81ea DMRS \u548c SRS\uff0c\u4f46\u5f53\u524d\u57fa\u7ad9\u5c06\u5b83\u4eec\u89c6\u4e3a\u72ec\u7acb\u4fe1\u606f\u6d41\u3002CARTS \u65e8\u5728\u878d\u5408\u8fd9\u4e24\u8005\uff0c\u63d0\u9ad8 CSI \u66f4\u65b0\u9891\u7387\u5e76\u4e3a\u66f4\u591a\u7528\u6237\u63d0\u4f9b\u611f\u77e5\u673a\u4f1a\u3002", "method": "CARTS \u901a\u8fc7\u4e24\u79cd\u521b\u65b0\u65b9\u6cd5\u5b9e\u73b0\uff1a\u4e00\u662f\u65b0\u9896\u7684\u4fe1\u9053\u62fc\u63a5\u548c\u8865\u507f\u65b9\u6cd5\uff0c\u6574\u5408\u5f02\u6b65 CSI \u4f30\u8ba1\uff1b\u4e8c\u662f\u5b9e\u65f6 SRS \u89e6\u53d1\u7b97\u6cd5\uff0c\u786e\u4fdd\u6240\u6709\u7528\u6237\u6709\u8db3\u591f\u4e14\u975e\u5197\u4f59\u7684\u611f\u77e5\u673a\u4f1a\u3002", "result": "CARTS \u663e\u8457\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u5b9e\u73b0\u4e86 0.167 \u7684 NMSE \u548c 85 cm \u7684 UE \u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u540c\u65f6\u652f\u6301\u4e24\u500d\u4e8e\u57fa\u7ebf SRS \u7684\u7528\u6237\u6570\u3002", "conclusion": "CARTS \u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u7b26\u5408\u6807\u51c6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u878d\u5408 DMRS \u548c SRS \u7684 CSI \u6d41\uff0c\u663e\u8457\u63d0\u9ad8\u4e86 ISAC \u670d\u52a1\u7684 CSI \u53ef\u7528\u6027\uff0c\u4e14\u65e0\u9700\u989d\u5916\u7684\u65e0\u7ebf\u7535\u8d44\u6e90\u3002"}}
{"id": "2507.13717", "categories": ["cs.NI", "C.2.3"], "pdf": "https://arxiv.org/pdf/2507.13717", "abs": "https://arxiv.org/abs/2507.13717", "authors": ["Yingming Mao", "Qiaozhu Zhai", "Zhen Yao", "Xia Zhu", "Ximeng Liu", "Xinchi Han"], "title": "ATRO: A Fast Solver-Free Algorithm for Topology and Routing Optimization of Reconfigurable Datacenter Networks", "comment": null, "summary": "The growing scale and complexity of reconfigurable data center networks\n(DCNs) demand more scalable and efficient algorithms for computing logical\ntopologies and routing. Reconfigurable DCNs typically operate in two modes:\none-hop configurations that require frequent topology optimization (TO), and\nmulti-hop scenarios that involve joint topology and routing optimization (TRO).\nIn both cases, the combinatorial nature of topology decisions makes it\ndifficult for existing methods to balance solution quality and runtime\nefficiency. To address this, we introduce Alternating Topology and Routing\nOptimization (ATRO), a solver-free framework that alternates between TO and\nrouting optimization (RO). This decomposition exploits two key insights: first,\neach alternating update step monotonically reduces maximum link utilization\n(MLU), ensuring consistent performance improvement across iterations; second,\nthe TO subproblem, equivalent to one-hop optimization, exhibits a monotonic\nstructure that enables optimal solutions via an efficient Accelerated Binary\nSearch Method (ABSM). To preserve the solver-free design, RO is solved using\nexisting Traffic Engineering accelerators. ATRO attains the global optimum in\none-hop scenarios and significantly outperforms baselines in multi-hop settings\nin terms of both runtime and solution quality. Evaluations confirm its\nscalability and robustness across diverse DCNs.", "AI": {"tldr": "ATRO\u662f\u4e00\u79cd\u65e0\u6c42\u89e3\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u62d3\u6251\u548c\u8def\u7531\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u91cd\u6784DCN\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u8fd0\u884c\u6548\u7387\u3002", "motivation": "\u53ef\u91cd\u6784\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\uff08DCNs\uff09\u7684\u89c4\u6a21\u548c\u590d\u6742\u6027\u4e0d\u65ad\u589e\u52a0\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u7b97\u6cd5\u6765\u4f18\u5316\u903b\u8f91\u62d3\u6251\u548c\u8def\u7531\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u8fd0\u884c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4ea4\u66ff\u62d3\u6251\u548c\u8def\u7531\u4f18\u5316\uff08ATRO\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u62d3\u6251\u4f18\u5316\uff08TO\uff09\u548c\u8def\u7531\u4f18\u5316\uff08RO\uff09\u6765\u5206\u89e3\u95ee\u9898\u3002TO\u5b50\u95ee\u9898\u901a\u8fc7\u52a0\u901f\u4e8c\u8fdb\u5236\u641c\u7d22\u65b9\u6cd5\uff08ABSM\uff09\u9ad8\u6548\u6c42\u89e3\uff0cRO\u5219\u5229\u7528\u73b0\u6709\u7684\u6d41\u91cf\u5de5\u7a0b\u52a0\u901f\u5668\u3002", "result": "ATRO\u5728\u5355\u8df3\u573a\u666f\u4e2d\u8fbe\u5230\u5168\u5c40\u6700\u4f18\uff0c\u5e76\u5728\u591a\u8df3\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8fd0\u884c\u6548\u7387\u548c\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "conclusion": "ATRO\u5728\u5355\u8df3\u573a\u666f\u4e0b\u8fbe\u5230\u5168\u5c40\u6700\u4f18\uff0c\u5e76\u5728\u591a\u8df3\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u8fd0\u884c\u65f6\u95f4\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u5176\u5728\u5404\u79cdDCN\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.13889", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13889", "abs": "https://arxiv.org/abs/2507.13889", "authors": ["Bilal Karaman", "Ilhan Basturk", "Ferdi Kara", "Metin Ozturk", "Sezai Taskin", "Halil Yanikomeroglu"], "title": "On the Trade-Off Between Sum-Rate and Energy Efficiency through the Convergence of HAPS and Active RIS Technologies", "comment": "accepted in PIMRC2025", "summary": "This paper investigates the integration of active reconfigurable intelligent\nsurfaces (RIS) relay with high-altitude platform stations (HAPS) to enhance\nnon-terrestrial network (NTN) performance in next-generation wireless systems.\nWhile prior studies focused on passive RIS architectures, the severe path loss\nand double fading in long-distance HAPS links make active RIS a more suitable\nalternative due to its inherent signal amplification capabilities. We formulate\na sum-rate maximization problem to jointly optimize power allocation and RIS\nelement assignment for ground user equipments (UEs) supported by a HAPS-based\nactive RIS-assisted communication system. To reduce power consumption and\nhardware complexity, several sub-connected active RIS architectures are also\nexplored. Simulation results reveal that active RIS configurations\nsignificantly outperform passive RIS in terms of quality of service (QoS).\nMoreover, although fully-connected architectures achieve the highest\nthroughput, sub-connected schemes demonstrate superior energy efficiency under\npractical power constraints. These findings highlight the potential of active\nRIS-enabled HAPS systems to meet the growing demands of beyond-cellular\ncoverage and green networking.", "AI": {"tldr": "Active RIS with HAPS enhances NTN performance by overcoming passive RIS limitations, optimizing power and RIS elements, with sub-connected schemes offering better energy efficiency.", "motivation": "The study is motivated by the limitations of passive RIS architectures in long-distance HAPS links, where severe path loss and double fading make active RIS, with its signal amplification capabilities, a more suitable alternative to enhance NTN performance.", "method": "The paper formulates a sum-rate maximization problem to jointly optimize power allocation and RIS element assignment for ground UEs in a HAPS-based active RIS-assisted system, while also exploring sub-connected architectures to reduce power consumption and hardware complexity.", "result": "Simulation results show that active RIS configurations significantly outperform passive RIS in QoS, with fully-connected architectures achieving the highest throughput but sub-connected schemes demonstrating better energy efficiency under practical power constraints.", "conclusion": "The findings highlight the potential of active RIS-enabled HAPS systems to address the demands of beyond-cellular coverage and green networking, with sub-connected schemes offering superior energy efficiency under practical constraints."}}
{"id": "2507.13522", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.13522", "abs": "https://arxiv.org/abs/2507.13522", "authors": ["Ankit Bhardwaj", "Weiyang Wang", "Jeremy Carin", "Adam Belay", "Manya Ghobadi"], "title": "Checkmate: Zero-Overhead Model Checkpointing via Network Gradient Replication", "comment": "18 pages, 11 figures", "summary": "This paper presents Checkmate, a system that enables per-iteration\ncheckpointing in DNN training without any training slowdown. The traditional\napproach to checkpointing requires a pause in training to copy model states to\na separate location, allowing the state to be restored in the event of failure.\nThis approach fundamentally has a tradeoff between the frequency of checkpoints\nand the cost of a failure. We avoid this tradeoff; our key insight is that in\ndata-parallel training, all information necessary to create a checkpoint\nalready exists in the network as gradients. Our core contribution is a new\nmulticast abstraction that simultaneously delivers gradients to a separate\nCPU-based shadow cluster. The shadow maintains a checkpoint by applying those\ngradients to a copy of the model. Our evaluation shows that Checkmate performs\nper-iteration checkpointing with training throughput comparable to an ideal\nno-checkpoint baseline. Checkmate achieves 5 to 34.5x more frequent\ncheckpointing compared to state-of-the-art checkpointing systems, resulting in\n80% to 97.1% reduction in repeated work per failure. At the same checkpointing\nfrequency, Checkmate delivers 1.3x to 6.5x throughput compared to other\nsystems.", "AI": {"tldr": "Checkmate introduces per-iteration checkpointing in DNN training without slowdown, using gradients to maintain checkpoints, outperforming existing systems in frequency and throughput.", "motivation": "Traditional checkpointing methods require training pauses, creating a tradeoff between checkpoint frequency and failure cost. Checkmate aims to eliminate this tradeoff by leveraging existing gradient information in data-parallel training.", "method": "Utilizes a new multicast abstraction to deliver gradients to a CPU-based shadow cluster, which maintains checkpoints by applying gradients to a model copy.", "result": "Checkmate achieves 5 to 34.5x more frequent checkpointing, 80% to 97.1% reduction in repeated work per failure, and 1.3x to 6.5x higher throughput compared to state-of-the-art systems.", "conclusion": "Checkmate successfully enables per-iteration checkpointing in DNN training without training slowdown, significantly reducing repeated work per failure and outperforming existing systems in throughput."}}
{"id": "2507.13933", "categories": ["cs.NI", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13933", "abs": "https://arxiv.org/abs/2507.13933", "authors": ["Sichang \"Steven\" He", "Ramesh Govindan", "Harsha V. Madhyastha"], "title": "Preprint: Did I Just Browse A Website Written by LLMs?", "comment": "In submission. 2 pages. 3 figures", "summary": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem.", "AI": {"tldr": "Proposed a reliable pipeline to detect LLM-dominant websites, achieving 100% accuracy on test datasets and identifying many such sites in real-world web data.", "motivation": "The rise of LLM-dominant content, which is often unreliable and unethical, poses challenges as websites rarely disclose such content, making it hard for human readers to distinguish. Existing LLM detectors are insufficient for web content due to its complexity.", "method": "A highly reliable, scalable pipeline that classifies entire websites based on an LLM text detector's outputs from multiple prose-like pages. The method involves training and evaluating the detector using two distinct ground truth datasets totaling 120 sites.", "result": "The detector achieved 100% accuracy across the ground truth datasets. In real-world testing, a sizable portion of sites among 10k search engine results and 10k Common Crawl archives were identified as LLM-dominant.", "conclusion": "LLM-dominant websites are becoming more prevalent and rank highly in search results, raising concerns about their impact on users and the web ecosystem. The proposed pipeline effectively detects such sites with high accuracy."}}
{"id": "2507.13601", "categories": ["cs.DC", "cs.ET", "cs.PF", "90B36, 90C27, 68M14, 68W40", "C.1.2; C.1.4; C.3.1; D.1.3; G.1.6"], "pdf": "https://arxiv.org/pdf/2507.13601", "abs": "https://arxiv.org/abs/2507.13601", "authors": ["Jorge Villarrubia", "Luis Costero", "Francisco D. Igual", "Katzalin Olcoz"], "title": "Leveraging Multi-Instance GPUs through moldable task scheduling", "comment": null, "summary": "NVIDIA MIG (Multi-Instance GPU) allows partitioning a physical GPU into\nmultiple logical instances with fully-isolated resources, which can be\ndynamically reconfigured. This work highlights the untapped potential of MIG\nthrough moldable task scheduling with dynamic reconfigurations. Specifically,\nwe propose a makespan minimization problem for multi-task execution under MIG\nconstraints. Our profiling shows that assuming monotonicity in task work with\nrespect to resources is not viable, as is usual in multicore scheduling.\nRelying on a state-of-the-art proposal that does not require such an\nassumption, we present FAR, a 3-phase algorithm to solve the problem. Phase 1\nof FAR builds on a classical task moldability method, phase 2 combines Longest\nProcessing Time First and List Scheduling with a novel repartitioning tree\nheuristic tailored to MIG constraints, and phase 3 employs local search via\ntask moves and swaps. FAR schedules tasks in batches offline, concatenating\ntheir schedules on the fly in an improved way that favors resource reuse.\nExcluding reconfiguration costs, the List Scheduling proof shows an\napproximation factor of 7/4 on the NVIDIA A30 model. We adapt the technique to\nthe particular constraints of an NVIDIA A100/H100 to obtain an approximation\nfactor of 2. Including the reconfiguration cost, our real-world experiments\nreveal a makespan with respect to the optimum no worse than 1.22x for a\nwell-known suite of benchmarks, and 1.10x for synthetic inputs inspired by real\nkernels. We obtain good experimental results for each batch of tasks, but also\nin the concatenation of batches, with large improvements over the\nstate-of-the-art and proposals without GPU reconfiguration. Beyond the\nalgorithm, the paper demonstrates the research potential of the MIG technology\nand suggests useful metrics, workload characterizations and evaluation\ntechniques for future work in this field.", "AI": {"tldr": "FAR algorithm optimizes task scheduling on NVIDIA MIG with dynamic reconfigurations, achieving near-optimal makespan and significant improvements over existing methods.", "motivation": "To highlight the untapped potential of NVIDIA MIG through moldable task scheduling with dynamic reconfigurations, addressing the makespan minimization problem for multi-task execution under MIG constraints.", "method": "The paper proposes FAR, a 3-phase algorithm: Phase 1 uses a classical task moldability method, Phase 2 combines Longest Processing Time First and List Scheduling with a novel repartitioning tree heuristic, and Phase 3 employs local search via task moves and swaps. FAR schedules tasks in batches offline, concatenating their schedules dynamically to favor resource reuse.", "result": "FAR achieves an approximation factor of 7/4 on NVIDIA A30 and 2 on NVIDIA A100/H100. Real-world experiments show makespan no worse than 1.22x for benchmarks and 1.10x for synthetic inputs, with large improvements over state-of-the-art methods.", "conclusion": "The paper demonstrates the research potential of NVIDIA MIG technology and suggests useful metrics, workload characterizations, and evaluation techniques for future work. FAR algorithm shows significant improvements over state-of-the-art methods, even when including reconfiguration costs."}}
{"id": "2507.13833", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.13833", "abs": "https://arxiv.org/abs/2507.13833", "authors": ["Zhixin Wang", "Tianyi Zhou", "Liming Liu", "Ao Li", "Jiarui Hu", "Dian Yang", "Jinlong Hou", "Siyuan Feng", "Yuan Cheng", "Yuan Qi"], "title": "DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training", "comment": null, "summary": "Reinforcement learning (RL) has become the pivotal post-training technique\nfor large language model. Effectively scaling reinforcement learning is now the\nkey to unlocking advanced reasoning capabilities and ensuring safe,\ngoal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually\nemploy a hybrid-controller architecture where a single-controller dispatches\nthe overall execution logic and manages overall data transfer and the\nmulti-controller executes distributed computation. For large-scale\nreinforcement learning, minor load imbalances can introduce significant\nbottlenecks, ultimately constraining the scalability of the system. To address\nthis limitation, we introduce DistFlow, a novel, fully distributed RL framework\ndesigned to break scaling barrier. We adopt a multi-controller paradigm that\ndispatches data transfer and execution tasks to all workers, which eliminates\nthe centralized node. This allows each worker to operate independently, leading\nto near-linear scalability up to thousands of GPUs and dramatic efficiency\ngains. Furthermore, our architecture decouples resource configuration from\nexecution logic, allowing each worker to have a unique execution flow, offering\nsignificant flexibility for rapid and cost-effective algorithmic\nexperimentation. Extensive experiments show that DistFlow achieves excellent\nlinear scalability and up to a 7x end-to-end throughput improvement over\nstate-of-the-art (SOTA) frameworks.", "AI": {"tldr": "DistFlow \u662f\u4e00\u79cd\u65b0\u578b\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6d88\u9664\u4e2d\u5fc3\u8282\u70b9\u5b9e\u73b0\u8fd1\u7ebf\u6027\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u8f7b\u5fae\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u74f6\u9888\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u591a\u63a7\u5236\u5668\u8303\u5f0f\uff0c\u5c06\u6570\u636e\u4f20\u8f93\u548c\u6267\u884c\u4efb\u52a1\u5206\u914d\u7ed9\u6240\u6709\u5de5\u4f5c\u8282\u70b9\uff0c\u6d88\u9664\u4e2d\u5fc3\u8282\u70b9\uff0c\u4f7f\u6bcf\u4e2a\u5de5\u4f5c\u8282\u70b9\u72ec\u7acb\u8fd0\u884c\u3002", "result": "DistFlow \u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u7ebf\u6027\u6269\u5c55\u6027\uff0c\u7aef\u5230\u7aef\u541e\u5410\u91cf\u6bd4\u73b0\u6709\u6280\u672f\u6846\u67b6\u63d0\u9ad8\u4e867\u500d\u3002", "conclusion": "DistFlow \u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6269\u5c55\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u8fd1\u7ebf\u6027\u6269\u5c55\u548c\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2507.14069", "categories": ["cs.DC", "cs.AI", "cs.ET", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.14069", "abs": "https://arxiv.org/abs/2507.14069", "authors": ["Shuiguang Deng", "Di Yu", "Changze Lv", "Xin Du", "Linshan Jiang", "Xiaofan Zhao", "Wentao Tong", "Xiaoqing Zheng", "Weijia Fang", "Peng Zhao", "Gang Pan", "Schahram Dustdar", "Albert Y. Zomaya"], "title": "Edge Intelligence with Spiking Neural Networks", "comment": "This work has been submitted to Proceeding of IEEE for possible\n  publication", "summary": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8eSNNs\u7684\u8fb9\u7f18\u667a\u80fd\uff08EdgeSNNs\uff09\uff0c\u63a2\u8ba8\u5176\u5728\u8bbe\u5907\u4e0a\u5b66\u4e60\u3001\u63a8\u7406\u548c\u8fb9\u7f18\u573a\u666f\u5b89\u5168\u4e2d\u7684\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u5206\u7c7b\u548c\u5b9e\u9645\u5e94\u7528\u8ba8\u8bba\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u4e0e\u8fb9\u7f18\u8ba1\u7b97\u7684\u878d\u5408\u6fc0\u53d1\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u76f4\u63a5\u5b9e\u73b0\u667a\u80fd\u670d\u52a1\u7684\u5174\u8da3\u3002\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u96c6\u4e2d\u5f0f\u6570\u636e\u7ba1\u7406\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u3001\u5e26\u5bbd\u6d88\u8017\u548c\u9690\u79c1\u95ee\u9898\uff0c\u66b4\u9732\u4e86\u4ee5\u4e91\u4e3a\u4e2d\u5fc3\u8303\u5f0f\u7684\u5173\u952e\u9650\u5236\u3002", "method": "\u672c\u6587\u63d0\u4f9b\u4e86\u57fa\u4e8eSNNs\u7684\u8fb9\u7f18\u667a\u80fd\uff08EdgeSNNs\uff09\u7684\u7cfb\u7edf\u6027\u5206\u7c7b\uff0c\u5305\u62ec\u795e\u7ecf\u5143\u6a21\u578b\u3001\u5b66\u4e60\u7b97\u6cd5\u548c\u652f\u6301\u786c\u4ef6\u5e73\u53f0\u3002\u6df1\u5165\u8ba8\u8bba\u4e86EdgeSNN\u7684\u4e09\u4e2a\u5b9e\u9645\u8003\u8651\uff1a\u4f7f\u7528\u8f7b\u91cf\u7ea7SNN\u6a21\u578b\u8fdb\u884c\u8bbe\u5907\u4e0a\u63a8\u7406\u3001\u975e\u9759\u6001\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u8d44\u6e90\u611f\u77e5\u8bad\u7ec3\u548c\u66f4\u65b0\uff0c\u4ee5\u53ca\u5b89\u5168\u548c\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u3002", "result": "\u672c\u6587\u4ecb\u7ecd\u4e86\u53cc\u8f68\u57fa\u51c6\u6d4b\u8bd5\u7b56\u7565\uff0c\u4ee5\u652f\u6301\u516c\u5e73\u6bd4\u8f83\u548c\u786c\u4ef6\u611f\u77e5\u4f18\u5316\uff0c\u5e76\u7a81\u51fa\u4e86\u5728\u4f20\u7edf\u786c\u4ef6\u4e0a\u8bc4\u4f30EdgeSNNs\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u5f25\u5408\u8111\u542f\u53d1\u5b66\u4e60\u4e0e\u5b9e\u9645\u8fb9\u7f18\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u4f9b\u4e86\u5bf9\u5f53\u524d\u8fdb\u5c55\u3001\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u7684\u89c1\u89e3\u3002\u8fd9\u662f\u7b2c\u4e00\u4efd\u4e13\u95e8\u4e14\u5168\u9762\u7684EdgeSNNs\u8c03\u67e5\uff0c\u4e3a\u5728\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u548c\u8fb9\u7f18\u667a\u80fd\u4ea4\u53c9\u9886\u57df\u5de5\u4f5c\u7684\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2507.14080", "categories": ["cs.DC", "D.2.4; C.2.4"], "pdf": "https://arxiv.org/pdf/2507.14080", "abs": "https://arxiv.org/abs/2507.14080", "authors": ["Derek Leung", "Nickolai Zeldovich", "Frans Kaashoek"], "title": "Shipwright: Proving liveness of distributed systems with Byzantine participants", "comment": "14 pages, 13 figures", "summary": "Ensuring liveness in a decentralized system, such as PBFT, is critical,\nbecause there may not be any single administrator that can restart the system\nif it encounters a liveness bug. At the same time, liveness is challenging to\nachieve because any single participant could be malicious, and yet the overall\nsystem must make forward progress. While verification is a promising approach\nfor ensuring the absence of bugs, no prior work has been able to verify\nliveness for an executable implementation of PBFT.\n  Shipwright is a verification framework for proving correctness and liveness\nof distributed systems where some participants might be malicious. Shipwright\nintroduces three techniques that enable formal reasoning about decentralized\nsettings with malicious participants, allow developers to decompose their\nsystem and proof in a modular fashion into sub-protocols and sub-proofs, and\nsupport sound reasoning about cryptographic signatures that may be embedded in\nmessages. We used Shipwright to implement and verify an initial prototype of\nagreement on a single log entry in PBFT (with a few limitations) and translate\nit to an executable implementation in Go. We experimentally demonstrate its\noperation and liveness both in the common case and in several failure\nscenarios.", "AI": {"tldr": "Shipwright verifies liveness and correctness in decentralized systems, demonstrated via a PBFT prototype with modular and cryptographic support.", "motivation": "Ensuring liveness in decentralized systems like PBFT is critical yet challenging due to potential malicious participants and the absence of a central administrator.", "method": "Shipwright introduces three techniques for formal reasoning in decentralized settings, modular decomposition, and cryptographic signature handling.", "result": "Shipwright enables verification of liveness for an executable PBFT implementation, with experimental validation in common and failure scenarios.", "conclusion": "Shipwright successfully verifies liveness and correctness in decentralized systems with malicious participants, demonstrated through a PBFT prototype."}}
