<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 141]
- [cs.DS](#cs.DS) [Total: 15]
- [cs.AI](#cs.AI) [Total: 58]
- [cs.SE](#cs.SE) [Total: 37]
- [cs.RO](#cs.RO) [Total: 43]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.NI](#cs.NI) [Total: 25]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: 该论文比较了多种算法策略在拟合3D图像数据镶嵌模型中的性能，评估了不同优化方法在生成近似颗粒结构时的效果，并提供了选择方法的指导。


<details>
  <summary>Details</summary>
Motivation: 在3D图像数据（如多晶体和泡沫）的镶嵌模型拟合领域，评估和比较不同算法策略的性能和适用性。

Method: 论文评估了基于优化的方法，包括线性和非线性规划、通过交叉熵方法的随机优化以及梯度下降，用于生成近似体素基颗粒结构的Voronoi、Laguerre和广义平衡功率图（GBPDs）。

Result: 研究结果突出了模型复杂性、优化例程复杂性和近似质量之间的权衡关系。

Conclusion: 该论文为基于数据特征和应用需求选择合适的算法策略提供了指导，强调了模型复杂性、优化例程复杂性和近似质量之间的权衡。

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [2] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: 本文研究了多种骨干网络对语义分割模型性能的影响，使用BDD100k数据集验证，结果显示选择合适骨干能显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术的发展，深度学习在解决复杂任务（如自动驾驶）中显示出巨大潜力。本文旨在通过语义分割提升场景理解能力。

Method: 使用BDD100k数据集，研究了几种高效的语义分割模型，并采用了多种骨干网络作为编码器。

Result: 实验结果表明，模型在准确性、平均IoU和损失函数等指标上均有所提升。

Conclusion: 选择适当的骨干网络对语义分割模型的性能有显著影响，本文提出的模型在准确性、平均IoU和损失函数方面均有所改进。

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [3] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: CLIPTTA 是一种新型梯度基测试时适应方法，通过软对比损失与 CLIP 预训练对齐，显著提升分布偏移下的泛化能力，并在开放集中优化异常检测，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时适应方法（如熵最小化）与视觉语言模型的对比图像-文本训练目标不匹配，限制了适应性能并引入伪标签漂移和类别崩溃等问题。因此，需要一种与 CLIP 预训练目标对齐的适应方法。

Method: CLIPTTA 采用梯度基测试时适应方法，利用软对比损失与 CLIP 的预训练目标对齐，并通过理论分析证明其批处理感知设计能有效避免崩溃风险。在开放集设置中，引入异常对比暴露（OCE）损失以优化异常检测。

Result: 在涵盖多样分布偏移的 75 个数据集上，CLIPTTA 在性能上一致优于基于熵的目标，并与最先进的测试时适应方法竞争，且在多个数据集上表现更优，且性能更稳定。

Conclusion: CLIPTTA 是一种针对视觉语言模型的新型梯度基测试时适应方法，通过软对比损失与 CLIP 的预训练目标对齐，显著提升了在分布偏移下的泛化能力，并在开放集设置中通过异常对比暴露损失进一步优化了异常检测性能。

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [4] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: AF模块通过剪除非信息性标记优化GCD特征提取，性能提升15.4%，计算开销小。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法在处理未标记数据时，模型注意力常被任务无关的背景区域分散，导致特征提取不理想。本文旨在解决这一问题。

Method: AF由两个简单而有效的组件组成：标记重要性度量（TIME）和多尺度标记自适应剪枝（TAP）。TIME量化多尺度标记的重要性，TAP利用这些分数剪除非信息性标记。

Result: AF模块显著提升了SimGCD方法的性能，最高可达15.4%，且计算开销极小。代码已开源。

Conclusion: 论文提出了一种名为注意力聚焦（AF）的轻量级插件模块，通过剪除非信息性标记来优化特征提取，显著提升了广义类别发现（GCD）的性能。

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [5] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: 研究提出利用多模态大语言模型构建幻觉评分（HS）来评估和减少生成超分辨率模型中的幻觉问题，HS与人类评估一致且与深度特征相关，可用于优化模型。


<details>
  <summary>Details</summary>
Motivation: 生成超分辨率（GSR）模型在感知图像质量上表现优异，但生成的细节与低分辨率图像（LRI）或真实图像（GTI）不匹配的幻觉问题限制了其实际应用。现有图像指标或质量模型无法很好表征这些幻觉。

Method: 利用多模态大语言模型（MLLM）构建幻觉评分（HS），并探索深度特征距离与HS的相关性，提出使用这些特征作为可微分奖励函数来优化GSR模型。

Result: HS与人类评估高度一致，且与现有图像指标互补。某些深度特征距离与HS有强相关性，可用于优化GSR模型以减少幻觉。

Conclusion: 通过利用多模态大语言模型（MLLM）构建的幻觉评分（HS）与人类评估高度一致，并提供了与现有图像指标的互补见解。此外，某些深度特征距离与HS有强相关性，因此提出使用这些特征作为可微分奖励函数来对齐GSR模型，以减少幻觉。

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [6] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: DUSTrack是一个结合深度学习和光流的半自动化工具，用于B型超声视频中的点跟踪，表现优异且开源。


<details>
  <summary>Details</summary>
Motivation: B型超声中的组织运动跟踪因斑点噪声、低边缘对比度和平面外运动而具有挑战性，DUSTrack旨在解决这些问题。

Method: 结合深度学习和光流技术，DUSTrack提供了一个半自动化框架，包括图形用户界面和基于光流的滤波技术。

Result: DUSTrack在准确性上优于当代零样本点跟踪器，并与专用方法表现相当。

Conclusion: DUSTrack作为一种开源解决方案，为超声视频中的点跟踪提供了一个强大且灵活的框架，用于量化组织运动。

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [7] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: CRAFT 是一个神经符号框架，通过结合常识和视觉证据，迭代优化动作支持的物体识别，提升场景理解的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个可解释的框架，用于识别场景中支持特定动作（如“切割”）的对象，从而提升场景理解的透明度和可信度。

Method: CRAFT 整合了来自 ConceptNet 和语言模型的结构化常识先验与 CLIP 的视觉证据，并通过基于能量的推理循环迭代优化预测。

Result: 在多对象、无标签设置下的实验表明，CRAFT 提高了准确性并增强了可解释性。

Conclusion: CRAFT 通过结合符号常识和视觉证据，提升了场景理解的准确性和可解释性，为构建稳健且可信赖的场景理解系统迈出了一步。

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [8] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: 提出了一个基于高斯变形场的3DGS视频流框架，通过混合显著性分块和差异化质量建模，有效解决了3DGS视频流媒体的挑战，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 3DGS视频因其数据量大和压缩传输复杂性高，对流媒体提出了挑战。

Method: 基于高斯变形场的3DGS视频构建方法，结合混合显著性分块和差异化质量建模，实现高效数据压缩和带宽波动适应。

Result: 实验验证表明，该方法在视频质量、压缩效果和传输速率等方面优于现有方法。

Conclusion: 提出的3DGS视频流框架在视频质量、压缩效率和传输速率方面均优于现有方法。

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [9] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: IRGPT is a new multi-modal model for real-world infrared images, using a unique dataset and transfer learning to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on synthetic infrared images generated through style transfer from visible images, limiting their ability to capture the unique characteristics of the infrared modality. The need for a model that can effectively process real-world infrared imagery led to the development of IRGPT.

Method: The study introduces IRGPT, built upon a large-scale InfraRed-Text Dataset (IR-TD) with over 260K authentic image-text pairs, and employs a bi-cross-modal curriculum transfer learning strategy to systematically transfer knowledge from visible to infrared domains.

Result: IRGPT outperforms existing methods, achieving state-of-the-art performance on a benchmark of 9 tasks, including recognition and grounding.

Conclusion: IRGPT, the first multi-modal large language model for real-world infrared images, achieves state-of-the-art performance across 9 benchmark tasks, demonstrating the effectiveness of the proposed IR-TD dataset and bi-cross-modal curriculum transfer learning strategy.

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [10] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: GPI-Net利用Gestalt原则优化局部与全局特征融合，通过正交集成和注意力机制提升点云注册的对应关系质量，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决局部与全局特征融合时的冗余和复杂空间关系问题，提升点云注册中对应关系的准确性。

Method: 提出了一种基于Gestalt原则的并行交互网络（GPI-Net），包含正交集成策略、Gestalt特征注意力（GFA）块和双路径多粒度并行交互聚合（DMG）块。

Result: 在多种挑战性任务中，GPI-Net表现出优于现有方法的性能。

Conclusion: GPI-Net通过结合Gestalt原则和正交几何一致性，显著提升了点云注册中高质量对应关系的识别性能，其优越性在多个挑战性任务中得到验证。

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [11] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: 本文针对3DGS视频流的分块、质量评估和比特率自适应问题，提出了一套综合解决方案，包括自适应分块技术、质量评估框架和元学习比特率算法，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射视频（3DGS）流媒体因其提供沉浸式3D视频体验的能力受到关注，但分块、质量评估和比特率自适应等基础挑战仍需研究。

Method: 提出了一种基于显著性分析的自适应3DGS分块技术，结合空间和时间特征；开发了新颖的3DGS视频质量评估框架；设计了基于元学习的自适应比特率算法。

Result: 实验表明，提出的方法在性能上显著优于现有技术。

Conclusion: 本文提出的自适应3DGS分块技术、质量评估框架和基于元学习的比特率自适应算法在3DGS视频流中显著优于现有方法。

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [12] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: GEMINUS是一个混合专家端到端自动驾驶框架，结合全局和场景自适应专家，通过双感知路由器动态激活专家，显著提升了驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有单模式规划方法难以获取多样化驾驶技能以应对复杂多样交通环境的问题。

Method: 提出了GEMINUS框架，包含全局专家、场景自适应专家组和双感知路由器，全局专家在整体数据集上训练，场景自适应专家在特定场景子集上训练，双感知路由器动态激活专家模块。

Result: 在Bench2Drive基准测试中，GEMINUS在驾驶评分和成功率上达到最先进水平，仅使用单目视觉输入。消融研究表明，相较于单专家基线，驾驶评分提升7.67%，成功率提升22.06%，多能力均值提升19.41%。

Conclusion: GEMINUS框架通过结合全局专家和场景自适应专家组，利用双感知路由器动态激活专家模块，在多样化的交通场景中实现了自适应和鲁棒的自动驾驶性能，显著提升了驾驶评分和成功率。

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [13] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: VisGuard 是一个抗篡改的可视化图像数据检索框架，通过鲁棒的技术嵌入元数据链接，支持多种应用并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 可视化传播主要以栅格图像形式进行，这通常导致关键信息（如源代码、交互功能和元数据）的丢失。虽然之前的方法提出了将元数据嵌入图像以促进可视化图像数据检索（VIDR），但大多数现有方法缺乏实用性，因为它们对在线分发过程中常见的图像篡改（如裁剪和编辑）脆弱。

Method: VisGuard 采用了多种技术来增强鲁棒性，包括重复数据平铺、可逆信息广播和基于锚点的裁剪定位方案。

Result: VisGuard 在各种应用中表现出色，包括交互式图表重建、篡改检测和版权保护。全面实验证明了其在数据检索准确性、嵌入容量以及对篡改和隐写分析的安全性方面的卓越性能。

Conclusion: VisGuard 是一个高效的 VIDR 框架，能够可靠地将元数据链接嵌入可视化图像中，即使在图像被大幅篡改后仍能恢复嵌入的数据链接。它在数据检索准确性、嵌入容量以及对篡改和隐写分析的安全性方面表现出色，证明了其在促进和保护可视化传播和信息传递方面的能力。

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [14] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: OptiCorNet是一种新颖的序列建模框架，结合空间和时间特征，显著提升动态环境中的视觉地点识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决动态和感知混淆环境中视觉地点识别的挑战，现有基于深度学习的方法忽视图像序列中的时间连贯性。

Method: OptiCorNet采用轻量级1D卷积编码器和可学习的差分时间算子（DSD），结合LSTM精炼和残差投影，生成紧凑且具有区分性的描述符。还引入了四元组损失以增强类间分离性。

Result: 在多个公开基准测试中，OptiCorNet在季节和视角变化等挑战性条件下优于最先进的基线方法。

Conclusion: OptiCorNet通过统一的序列建模框架，结合空间特征提取和时间差分，显著提升了动态和感知混淆环境中的视觉地点识别性能。

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [15] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: DFQ-ViT通过改进合成样本质量和引入激活校正矩阵，显著提升了视觉Transformer的数据自由量化性能，无需微调，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分捕捉和平衡样本的全局与局部特征，且量化模型与全精度模型的中间层激活分布差异显著，导致量化模型性能严重下降。

Method: 提出了一种用于视觉Transformer的数据自由量化（DFQ-ViT）流程，包括按难度递增顺序合成样本，并在校准和推理阶段引入激活校正矩阵以对齐中间层激活。

Result: DFQ-ViT在DeiT-T上实现3位权重量化时，性能比现有技术高出4.29%，且与真实数据量化模型性能相当。

Conclusion: DFQ-ViT方法显著优于现有的DFQ方法，性能与使用真实数据量化的模型相当，且无需微调，降低了计算开销和边缘设备部署的门槛。

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [16] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: A new retrieval-augmented framework for 3D point cloud completion uses cross-modal retrieval and hierarchical feature fusion to improve accuracy and generalization, validated across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on specific input classes, limiting their generation abilities. The paper aims to improve point cloud completion by leveraging cross-modal retrieval to learn structural prior information from similar reference samples.

Method: The framework includes a Structural Shared Feature Encoder (SSFE) for cross-modal feature extraction and a Progressive Retrieval-Augmented Generator (PRAG) for hierarchical feature fusion, enhanced by a dual-channel control gate to filter irrelevant information.

Result: Extensive evaluations on multiple datasets and real-world scenes confirm the method's effectiveness in generating detailed point clouds and handling sparse data and unseen categories.

Conclusion: The proposed retrieval-augmented point cloud completion framework demonstrates effectiveness in generating fine-grained point clouds and shows strong generalization capability, especially with sparse data and unseen categories.

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [17] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: TCP-LLaVA是一种新型MLLM架构，通过令牌压缩技术高效处理WSI视觉问答任务，降低计算成本并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 解决全切片图像（WSI）在视觉问答（VQA）任务中由于长上下文和高计算需求带来的挑战，现有方法缺乏生成能力或资源消耗过大。

Method: 提出TCP-LLaVA架构，利用模态压缩模块和可训练的压缩令牌来聚合视觉和文本信息，仅将压缩后的令牌输入LLM生成答案。

Result: 在10种TCGA肿瘤亚型的实验中，TCP-LLaVA在VQA准确性上优于现有MLLM基线，并大幅减少训练资源消耗。

Conclusion: TCP-LLaVA通过引入可训练的压缩令牌，显著降低了计算成本，同时在WSI视觉问答任务中表现优于现有MLLM基线。

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [18] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: 一种基于事件法向流的运动分割和自运动估计框架，利用稀疏事件数据和几何约束，优化分割和运动估计，适用于实时机器人和导航。


<details>
  <summary>Details</summary>
Motivation: 针对神经形态视觉传感器，利用稀疏、高时间分辨率的事件数据，结合几何约束，改进传统依赖光流或显式深度估计的方法。

Method: 提出了一种基于优化的流程，通过事件过分割、残差分析分离独立移动物体，并利用运动相似性和时间一致性指导的分层聚类细化分割。

Result: 在EVIMO2v2数据集上的实验结果表明，该方法无需完整光流计算即可实现准确的分割和平移运动估计。

Conclusion: 该方法在物体边界处表现出显著优势，并展示了在可扩展的实时机器人和导航应用中的巨大潜力。

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [19] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: 这篇综述全面回顾了基于深度学习的前馈方法在3D重建和视图合成中的应用，分类了不同表示架构，总结了关键任务和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于计算密集型的迭代优化链，限制了其在现实场景中的应用。前馈方法通过深度学习实现了快速且通用的3D重建和视图合成，推动了该领域的革命。

Method: 论文对前馈技术进行了全面综述，包括点云、3D高斯泼溅（3DGS）、神经辐射场（NeRF）等底层表示架构的分类，并评估了姿态无关重建、动态3D重建等关键任务。

Result: 论文综述了前馈技术在3D重建和视图合成中的应用，包括数字人、SLAM、机器人等领域的应用，并总结了常用数据集和评估协议。

Conclusion: 论文总结了前馈方法在3D重建和视图合成领域的潜力，并指出了未来的研究方向，强调了这些方法在推动3D视觉技术前沿的潜力。

Abstract: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [20] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: DCHM通过深度一致性和多视图融合，显著提升行人检测精度，尤其在稀疏视图和拥挤场景中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有方法在行人检测中常引入噪声且精度低，依赖昂贵的多视图3D标注，难以泛化到多样场景。

Method: 提出了Depth-Consistent Human Modeling（DCHM）框架，结合超像素级高斯喷洒技术，实现稀疏视图、大规模和拥挤场景下的多视图深度一致性。

Result: DCHM在人类建模中显著减少噪声，并在行人定位中表现优异，是首个在挑战性场景中重建行人并进行多视图分割的方法。

Conclusion: DCHM框架通过深度一致性和多视图融合，显著减少了噪声，并在行人定位中表现出色，超越了现有基线方法。

Abstract: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [21] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

TL;DR: 提出了ArtiMuse模型和ArtiMuse-10K数据集，解决了现有IAA方法的模态偏差和细粒度不足问题，推动了图像美学评估领域的发展。


<details>
  <summary>Details</summary>
Motivation: 随着教育应用、艺术创作和AI生成内容技术的快速发展，对全面的图像美学评估（IAA）需求增加，现有方法存在模态偏差（仅评分或仅文本）和缺乏细粒度属性分解的问题。

Method: 提出了ArtiMuse，一种基于多模态大语言模型的IAA模型，具备联合评分和专家级理解能力；并构建了ArtiMuse-10K数据集，包含10,000张图像，涵盖5个主类别和15个子类别，每张图像由专业专家标注8维属性和整体评分。

Result: ArtiMuse展示了比传统方法更强的感知和泛化能力，解决了模态偏差问题，并支持细粒度属性分解，为美学评估提供了更全面的支持。

Conclusion: ArtiMuse和ArtiMuse-10K的公开将推动图像美学评估领域的发展，提供更全面的评估方法和专业数据集。

Abstract: The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [22] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: 本文提出了一种基于计算机视觉的手语识别浏览器扩展，用于在视频会议中自动翻译手语为字幕，以解决听力障碍者的沟通问题。


<details>
  <summary>Details</summary>
Motivation: 听力障碍者在视频会议中更倾向于使用手语而非打字，但普通人群对手语的细节了解有限，因此需要一种工具来消除沟通障碍。

Method: 利用包含2000多个单词级美国手语（ASL）视频的大规模数据集，由超过100名手语者完成，通过计算机视觉技术进行手语识别。

Result: 提出的浏览器扩展能够自动翻译手语为字幕，从而在视频通话中实现无障碍沟通。

Conclusion: 本文提出了一种浏览器扩展，旨在通过自动将手语翻译为字幕，消除听力障碍者与普通人群之间的沟通障碍，特别是在视频会议中。

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [23] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: 本文利用Florence模型处理胃肠道内窥镜的视觉问答任务，通过领域特定增强提升泛化能力，实验证实了其在医学VQA中的潜力。


<details>
  <summary>Details</summary>
Motivation: 针对ImageCLEFmed MEDVQA 2025挑战赛的子任务1，旨在解决胃肠道内窥镜的视觉问答问题。

Method: 采用Florence模型作为视觉问答流程的骨干，结合强大的视觉编码器和文本编码器来解析内窥镜图像并生成临床相关答案。应用领域特定的数据增强以提高泛化能力。

Result: 在KASVIR数据集上的实验表明，微调Florence模型能在官方挑战指标上产生准确的回答。

Conclusion: 本文展示了大型多模态模型在医学视觉问答中的潜力，并为未来在可解释性、鲁棒性和临床整合方面的研究提供了坚实基础。代码已公开。

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [24] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

TL;DR: 该研究通过生成ANN决策边界的模糊面部表情刺激，发现这些刺激同样导致人类感知不确定性增加，并成功调整ANN预测与人类感知模式对齐。


<details>
  <summary>Details</summary>
Motivation: 探索ANN在建模人类感知个体差异方面的能力，特别是针对高感知变异性现象，即个体在观看相同刺激时表现出显著的情感分类差异。

Method: 引入了一种新颖的感知边界采样方法，生成位于ANN决策边界的面部表情刺激，构建了varEmotion数据集，并通过大规模人类行为实验进行分析。

Result: 研究发现，ANN难以分类的模糊样本也会引发人类参与者的高度感知不确定性，揭示了情感感知中的共享计算原则。

Conclusion: 该研究通过调整ANN表征与行为数据的对齐，建立了ANN决策边界与人类感知变异性之间的系统性联系，为情感解释的个性化建模提供了新见解。

Abstract: A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [25] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: 开发了一个相机引导系统，结合美学评估和图像修复技术，帮助用户识别和去除照片中的杂物，提升拍摄质量。


<details>
  <summary>Details</summary>
Motivation: 摄影爱好者常因无意识的疏忽或缺乏经验而在照片中包含杂物，影响情感或故事的传达，因此需要开发一个相机引导系统来识别和去除杂物。

Method: 系统基于两个技术新颖性：一个结合美学评估的杂物区分算法，以及一个基于生成对抗网络的迭代图像修复算法，用于高分辨率图像中移除物体后缺失区域的重建。

Result: 系统通过估计和可视化物体对照片美学和内容的贡献，提供交互式杂物识别和去除建议，以及计算工具来处理不同类型的杂物。

Conclusion: 用户研究表明，该系统通过灵活的界面和精确的算法，帮助用户更好地识别干扰物，并在更短时间内拍摄出更高质量的图像。

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [26] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: Descrip3D通过语言增强的关系表示，提升3D场景理解，在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景-语言模型在关系理解上存在不足，尤其是视觉嵌入无法充分传达物体的角色和交互。

Method: Descrip3D框架通过自然语言显式编码物体间关系，采用双级集成（嵌入融合和提示级注入）增强对象的文本描述，涵盖其内在属性和上下文关系。

Result: 在ScanRefer、Multi3DRefer、ScanQA、SQA3D和Scan2Cap五个基准数据集上，Descrip3D均优于基线模型。

Conclusion: Descrip3D通过语言引导的关系表示，显著提升了复杂室内场景的理解能力，在多个基准数据集上表现优于基线模型。

Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [27] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: LEAD是一种基于logits的微调对齐方法，通过ODE建模优化过程，有效预测模型迁移性，实验证明其性能卓越且适应性强。


<details>
  <summary>Details</summary>
Motivation: 现有方法在特征空间中用线性变换建模微调动态，无法精确对齐微调目标且未能捕捉优化的非线性。LEAD旨在解决这一问题。

Method: LEAD通过理论框架建模优化过程，推导ODE描述非线性演化，并设计类感知分解方法，考虑不同类别的动态变化。

Result: 在24个监督和自监督预训练模型及10个下游数据集上的实验表明，LEAD性能优异，即使在低数据场景下也表现出广泛适应性。

Conclusion: LEAD提出了一种基于网络输出logits的微调对齐方法，通过理论框架和ODE建模优化过程，有效预测模型迁移性，并在实验中展示了卓越的性能和广泛适应性。

Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [28] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: 本文比较了GANs、扩散模型和FM在MRI图像合成中的表现，发现Pix2Pix在多个指标上最优，为实际应用提供了指导。


<details>
  <summary>Details</summary>
Motivation: 减少MRI扫描时间和成本，同时保持诊断质量，通过计算合成缺失的MRI对比度。

Method: 本文对生成对抗网络（GANs）、扩散模型和流匹配（FM）技术进行了综合基准测试，用于T1w到T2w的2D MRI图像到图像（I2I）翻译。

Result: GAN-based Pix2Pix模型在结构保真度、图像质量和计算效率方面表现最佳。

Conclusion: GAN-based Pix2Pix模型在结构保真度、图像质量和计算效率方面优于扩散和基于流匹配的方法，为实际MRI工作流程中的跨模态图像合成提供了实用指导。

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [29] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 该论文比较了TensorFlow with Keras、PyTorch和JAX在BloodMNIST数据集上的性能，发现JAX和PyTorch在医学图像分类中表现高效，分类准确性与基准相当。


<details>
  <summary>Details</summary>
Motivation: 尽管基于深度学习的自动分类系统在提高血液图像分析的准确性和效率方面显示出巨大潜力，但对特定深度学习框架的详细性能分析仍显不足。

Method: 研究比较了TensorFlow with Keras、PyTorch和JAX在BloodMNIST数据集上的性能，主要关注推理时间差异和不同图像尺寸下的分类性能。

Result: 研究结果显示，不同框架的性能存在差异，这些差异受图像分辨率和框架特定优化等因素的影响。JAX和PyTorch的分类准确性与当前基准相当。

Conclusion: 该论文通过比较三种流行的深度学习框架（TensorFlow with Keras、PyTorch和JAX）在BloodMNIST数据集上的性能，揭示了不同框架在医学图像分类中的效率差异。JAX和PyTorch的分类准确性与当前基准相当，展示了它们在医学图像分类中的高效性。

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [30] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: DiSCO-3D是首个解决3D开放词汇子概念发现的方法，结合无监督分割和弱开放词汇指导，在相关任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅适应特定任务目标或场景内容，DiSCO-3D旨在解决更广泛的3D开放词汇子概念发现问题，提供适应场景和用户查询的3D语义分割。

Method: DiSCO-3D基于神经场表示，结合无监督分割和弱开放词汇指导。

Result: DiSCO-3D在开放词汇子概念发现中表现出有效性能。

Conclusion: DiSCO-3D在开放词汇子概念发现中表现出色，并在开放词汇和无监督分割的边缘案例中达到了最先进的性能。

Abstract: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [31] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: Exp-Graph 是一种基于图模型的面部表情识别框架，结合视觉变换器和图卷积网络，显著提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在人机交互等领域至关重要，而面部属性的结构信息对识别效果有重要影响。

Method: 提出 Exp-Graph 框架，利用图模型表示面部属性的结构关系，结合视觉变换器编码局部外观相似性，并通过图卷积网络整合结构依赖关系。

Result: 在 Oulu-CASIA、eNTERFACE05 和 AFEW 数据集上分别达到 98.09%、79.01% 和 56.39% 的识别准确率。

Conclusion: Exp-Graph 通过结合视觉变换器和图卷积网络，有效地捕捉了面部属性的局部和全局依赖关系，显著提高了面部表情识别的准确率，并在不同数据集上展现了强大的泛化能力。

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [32] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

TL;DR: DD-SAM2通过引入DD-Adapter优化SAM2的医学视频分割性能，减少计算成本，在有限数据下实现高效微调，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法多为模态专用设计，适应性差，且SAM2在医学视频场景中需要大规模数据集进行微调，计算成本高且易出现灾难性遗忘。

Method: 提出了DD-SAM2框架，结合Depthwise-Dilated Adapter（DD-Adapter）增强多尺度特征提取，支持有限数据下的SAM2微调。

Result: 在TrackRad2025和EchoNet-Dynamic数据集上分别取得0.93和0.97的Dice分数，性能优越。

Conclusion: DD-SAM2通过引入Depthwise-Dilated Adapter（DD-Adapter）有效解决了SAM2在医学视频分割中的适应性挑战，显著提升了性能，同时减少了参数开销和计算成本。

Abstract: Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [33] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: BusterX++ 是一个基于强化学习的跨模态合成媒体检测框架，通过多阶段训练和混合推理提升性能，GenBuster++ 基准测试验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的进步增加了虚假信息的风险，现有单模态检测方法无法应对多模态合成内容，因此需要开发跨模态检测框架。

Method: BusterX++ 采用强化学习后训练策略，结合多阶段训练、思维奖励和混合推理，消除了冷启动问题。

Result: BusterX++ 在 GenBuster++ 基准测试中表现优异，证明了其在跨模态合成媒体检测中的高效性和泛化能力。

Conclusion: BusterX++ 框架通过强化学习后训练策略和多阶段训练，显著提升了跨模态合成媒体的检测与解释能力，GenBuster++ 基准测试进一步验证了其有效性和普适性。

Abstract: Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [34] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

TL;DR: MS2Fusion是一种基于状态空间模型的多光谱特征融合框架，通过双路径参数交互机制有效融合多光谱特征，显著提升目标检测性能，并展示出通用性。


<details>
  <summary>Details</summary>
Motivation: 解决多光谱特征融合中过度偏好局部互补特征而忽略跨模态共享语义的问题，以及感受野大小与计算复杂度之间的权衡瓶颈。

Method: 基于状态空间模型（SSM）的双路径参数交互机制，包括跨参数交互分支和共享参数分支，以实现多光谱特征的有效融合。

Result: 在FLIR、M3FD和LLVIP等主流基准测试中，MS2Fusion显著优于其他最先进的多光谱目标检测方法，并在RGB-T语义分割和RGBT显著目标检测任务中取得最优结果。

Conclusion: MS2Fusion框架在多光谱目标检测任务中表现出色，显著优于现有方法，且具有通用性，适用于其他多光谱感知任务。

Abstract: Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [35] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: FST.ai 是一个AI驱动的裁判框架，专注于跆拳道头部踢击实时检测，通过计算机视觉和深度学习显著提升裁判效率和公平性，并具有跨体育项目的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统手动裁判系统存在延迟、主观性和执行不一致的问题，影响了公平性和运动员信任。

Method: 利用计算机视觉、深度学习和边缘推理技术，构建了一个基于姿态估计、动作分类和影响分析的框架。

Result: FST.ai 将决策时间从分钟级缩短至秒级，同时提高了裁判的一致性和透明度。

Conclusion: FST.ai 展示了将AI技术应用于体育裁判的潜力，特别是在跆拳道头部踢击检测中，其方法具有跨体育项目的可扩展性和适应性。

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [36] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: 该研究提出了一种基于计算机视觉的方法，通过语义分割餐前餐后图像来量化食物浪费，展示了高性能且实时的解决方案，为减少机构食物浪费提供了可行路径。


<details>
  <summary>Details</summary>
Motivation: 量化机构餐饮环境中的餐后食物浪费对于支持数据驱动的可持续性策略至关重要。

Method: 研究开发了一个成本效益高的计算机视觉框架，通过语义分割RGB图像（餐前和餐后）来估计餐盘级食物浪费。使用了四种全监督模型（U-Net、U-Net++及其轻量级变体），并采用动态逆频率损失和AdamW优化器进行训练。

Result: 所有模型均表现出令人满意的性能，至少有一种模型对每种食物类型的DPA（分布像素一致性）接近或超过90%。轻量级模型在NVIDIA T4 GPU上实现了实时推理。

Conclusion: 该研究提出了一个可扩展、无接触的计算机视觉框架，用于量化餐后食物浪费，为大规模餐饮环境中的实时浪费跟踪系统奠定了基础，并为减少机构食物浪费提供了可行的未来方向和建议。

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [37] [Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images](https://arxiv.org/abs/2507.14670)
*Yaxuan Song,Jianan Fan,Hang Chang,Weidong Cai*

Main category: cs.CV

TL;DR: Gene-DML通过双通路多级判别框架提升基因表达预测性能，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用组织病理学图像与基因表达谱之间的跨模态表示对齐，限制了预测性能。

Method: 提出Gene-DML框架，通过双通路多级判别（Dual-pathway Multi-Level discrimination）增强形态学和转录模态的对应关系。

Result: 在公共空间转录组数据集上，Gene-DML实现了最先进的基因表达预测性能。

Conclusion: Gene-DML通过双通路多级判别学习强大的跨模态表示，显著提升了基因表达预测的准确性和泛化能力。

Abstract: Accurately predicting gene expression from histopathology images offers a
scalable and non-invasive approach to molecular profiling, with significant
implications for precision medicine and computational pathology. However,
existing methods often underutilize the cross-modal representation alignment
between histopathology images and gene expression profiles across multiple
representational levels, thereby limiting their prediction performance. To
address this, we propose Gene-DML, a unified framework that structures latent
space through Dual-pathway Multi-Level discrimination to enhance correspondence
between morphological and transcriptional modalities. The multi-scale
instance-level discrimination pathway aligns hierarchical histopathology
representations extracted at local, neighbor, and global levels with gene
expression profiles, capturing scale-aware morphological-transcriptional
relationships. In parallel, the cross-level instance-group discrimination
pathway enforces structural consistency between individual (image/gene)
instances and modality-crossed (gene/image, respectively) groups, strengthening
the alignment across modalities. By jointly modelling fine-grained and
structural-level discrimination, Gene-DML is able to learn robust cross-modal
representations, enhancing both predictive accuracy and generalization across
diverse biological contexts. Extensive experiments on public spatial
transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art
performance in gene expression prediction. The code and checkpoints will be
released soon.

</details>


### [38] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: 提出了高质量文档级数据集Doc-750K和原生多模态模型Docopilot，显著提升了多页文档理解的性能，无需依赖RAG方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在复杂多页文档理解上表现不足，主要由于缺乏高质量的文档级数据集，且现有RAG方法存在碎片化检索上下文、多阶段错误积累和额外检索时间成本等问题。

Method: 基于Doc-750K数据集，开发了原生多模态模型Docopilot，无需依赖RAG方法即可处理文档级依赖关系。

Result: 实验表明，Docopilot在文档理解任务和多轮交互中实现了更高的连贯性、准确性和效率。

Conclusion: Docopilot模型在文档级多模态理解任务中表现出色，为多页文档理解设定了新的基准，且数据、代码和模型已开源。

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [39] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: WSI-Agents是一个协作多代理系统，通过任务分配、验证和总结模块提升WSI分析的准确性和多功能性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在WSI分析中表现不佳，且多代理系统在病理领域的潜力尚未充分挖掘。

Method: WSI-Agents包含三个核心组件：任务分配模块、验证机制和总结模块，通过模型库和病理知识库实现高效协作。

Result: 在多种WSI基准测试中，WSI-Agents优于现有WSI多模态大语言模型和医疗代理框架。

Conclusion: WSI-Agents通过整合专业功能代理、任务分配和验证机制，显著提升了多任务WSI分析的准确性和多功能性。

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [40] [From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition](https://arxiv.org/abs/2507.14686)
*Chen Cai,Tianyi Liu,Jianjun Gao,Wenyang Liu,Kejun Wu,Ruoyu Wang,Yi Wang,Soo Chin Liew*

Main category: cs.CV

TL;DR: 提出MIPD框架，通过知识蒸馏从MLLM到小模型，提升其在GSR任务中的泛化和零样本能力，实验证明效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在复杂Grounded Situation Recognition (GSR)任务上表现不佳且资源消耗大，而传统GSR模型缺乏泛化能力。因此，希望通过知识蒸馏提升小模型的泛化和零样本能力。

Method: 提出了Multimodal Interactive Prompt Distillation (MIPD)框架，利用LLM-based Judgmental Rationales Generator (JRG)生成正负理性，结合场景感知和实例感知提示，通过Negative-Guided Multimodal Prompting Alignment (NMPA)模块对齐视觉信息，最终将知识蒸馏到学生模型中。

Result: 在Ov-SWiG和HICO-DET数据集上，MIPD在已见、罕见和未见情境下均表现出色，尤其是在未见情境下的检测能力有显著提升。

Conclusion: 通过提出的MIPD框架，成功地将教师MLLM的多模态知识蒸馏到学生Ov-GSR模型中，显著提升了模型在未见和罕见情境下的识别能力，并在多个数据集上验证了其优越性能。

Abstract: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot
abilities but struggle with complex Grounded Situation Recognition (GSR) and
are resource-intensive for edge device deployment. Meanwhile, conventional GSR
models often lack generalization ability, falling short in recognizing unseen
and rare situations. In this paper, we exploit transferring knowledge from a
teacher MLLM to a small GSR model to enhance its generalization and zero-shot
abilities, thereby introducing the task of Open-vocabulary Grounded Situation
Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt
Distillation (MIPD), a novel framework that distills enriched multimodal
knowledge from the foundation model, enabling the student Ov-GSR model to
recognize unseen situations and be better aware of rare situations.
Specifically, the MIPD framework first leverages the LLM-based Judgmental
Rationales Generator (JRG) to construct positive and negative glimpse and gaze
rationales enriched with contextual semantic information. The proposed
scene-aware and instance-perception prompts are then introduced to align
rationales with visual information from the MLLM teacher via the
Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively
capturing holistic and perceptual multimodal knowledge. Finally, the aligned
multimodal knowledge is distilled into the student Ov-GSR model, providing a
stronger foundation for generalization that enhances situation understanding,
bridges the gap between seen and unseen scenarios, and mitigates prediction
bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving
superior performance on seen, rare, and unseen situations, and further
demonstrate improved unseen detection on the HICO-DET dataset.

</details>


### [41] [GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset](https://arxiv.org/abs/2507.14697)
*Zhiwei Zhang,Zi Ye,Yibin Wen,Shuai Yuan,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TL;DR: 提出了首个细粒度梯田地块数据集GTPBD，填补了复杂梯田地形研究的空白，并支持多种遥感任务。


<details>
  <summary>Details</summary>
Motivation: 现有农业地块提取研究主要关注中分辨率测绘或规则平原农田，缺乏对复杂梯田地形的表征，而精细农业需要更精确的数据集。

Method: 提出了一个名为GTPBD的细粒度梯田地块数据集，包含47,537张高分辨率图像，带有三级标签（像素级边界标签、掩码标签和地块标签），并覆盖了全球多个梯田区域。

Result: GTPBD数据集在语义分割、边缘检测、梯田地块提取和无监督域适应（UDA）任务中进行了基准测试，并提出了一个多维评估框架。

Conclusion: GTPBD填补了梯田遥感研究的关键空白，为精细农业地形分析和跨场景知识转移提供了基础设施。

Abstract: Agricultural parcels serve as basic units for conducting agricultural
practices and applications, which is vital for land ownership registration,
food security assessment, soil erosion monitoring, etc. However, existing
agriculture parcel extraction studies only focus on mid-resolution mapping or
regular plain farmlands while lacking representation of complex terraced
terrains due to the demands of precision agriculture.In this paper, we
introduce a more fine-grained terraced parcel dataset named GTPBD (Global
Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset
covering major worldwide terraced regions with more than 200,000 complex
terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution
images with three-level labels, including pixel-level boundary labels, mask
labels, and parcel labels. It covers seven major geographic zones in China and
transcontinental climatic regions around the world.Compared to the existing
datasets, the GTPBD dataset brings considerable challenges due to the: (1)
terrain diversity; (2) complex and irregular parcel objects; and (3) multiple
domain styles. Our proposed GTPBD dataset is suitable for four different tasks,
including semantic segmentation, edge detection, terraced parcel extraction,
and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the
GTPBD dataset on eight semantic segmentation methods, four edge extraction
methods, three parcel extraction methods, and five UDA methods, along with a
multi-dimensional evaluation framework integrating pixel-level and object-level
metrics. GTPBD fills a critical gap in terraced remote sensing research,
providing a basic infrastructure for fine-grained agricultural terrain analysis
and cross-scenario knowledge transfer.

</details>


### [42] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

TL;DR: MultiRetNet结合视网膜成像、社会经济和共病数据，通过多模态融合和对比学习提高DR分期准确性，尤其服务于筛查资源有限的人群，有望降低成本并促进医疗公平。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变（DR）是可预防性失明的主要原因，全球影响超过1亿人。在美国，低收入社区个体因筛查机会有限，更易在诊断时进入晚期阶段，共病条件进一步加速疾病进展。

Method: 提出了MultiRetNet，结合视网膜成像、社会经济因素和共病档案，采用三种多模态融合方法，并确定全连接层融合为最通用的方法。通过合成对抗性低质量图像并使用对比学习训练延迟系统，指导模型识别需要临床医生审查的分布外样本。

Result: MultiRetNet在次优图像上保持了诊断准确性，并结合关键健康数据，可提高早期检测率，特别是在晚期DR常见的 underserved 人群中。该方法可能降低医疗成本并促进医疗公平。

Conclusion: MultiRetNet通过结合视网膜成像、社会经济因素和共病档案，提高了糖尿病视网膜病变（DR）分期的准确性，并结合临床延迟系统实现人类参与的临床实施。该方法可能降低医疗成本、提高早期检测率，并解决医疗资源获取的不平等问题，促进医疗公平。

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [43] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: InterAct VideoQA是一个专为交通监控任务设计的VideoQA数据集，旨在提升模型在复杂交通场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的VideoQA模型难以应对现实交通场景的复杂性，尤其是在多事件并发的情况下。

Method: 本文提出了InterAct VideoQA数据集，包含8小时的真实交通视频，分为10秒的视频片段，并配有超过25,000个问答对。

Result: 在InterAct VideoQA数据集上评估了最先进的VideoQA模型，发现它们在精细时空依赖推理方面存在挑战，但通过微调性能有显著提升。

Conclusion: InterAct VideoQA数据集作为一个公开的基准数据集，旨在促进智能交通系统中可部署的VideoQA模型的未来研究。

Abstract: Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>


### [44] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: LeAdQA通过因果感知查询和细粒度视觉定位，优化视频问答中的关键事件识别和复杂推理，实验证明其在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频问答方法存在任务无关采样和启发式检索的局限性，无法有效处理视频中的关键事件和复杂因果推理需求。

Method: LeAdQA首先利用LLMs重新表述问题-选项对，解决因果模糊性并聚焦时间关键点，随后通过时间定位模型精确检索最具信息量的片段，并通过自适应融合机制动态整合证据，最后由MLLM处理视觉-文本线索生成答案。

Result: 在NExT-QA、IntentQA和NExT-GQA等数据集上，LeAdQA实现了最先进的性能，特别是在复杂推理任务中表现突出。

Conclusion: LeAdQA通过结合因果感知的查询细化和细粒度视觉定位，显著提升了视频问答任务中对复杂关系的理解能力，实现了最先进的性能并保持了计算效率。

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [45] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: FOCUS是首个为冻结ViT提供高效空间-频谱解释性的框架，通过类特定提示和[SINK]令牌，显著提升高光谱数据的解释性，参数开销极低。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像（HSI）数据的高维特性使得现有显著性方法难以捕捉有意义的频谱线索，且全频谱ViT在解释性上计算成本过高。FOCUS旨在解决这两个挑战。

Method: FOCUS引入两个核心组件：类特定光谱提示（引导注意力到语义相关波长组）和可学习的[SINK]令牌（通过吸引损失吸收噪声或冗余注意力），支持单次前向生成稳定的3D显著图和光谱重要性曲线。

Result: FOCUS将波段级IoU提升15%，减少注意力崩溃超过40%，生成的显著性结果与专家标注高度一致，参数开销不到1%。

Conclusion: FOCUS框架通过类特定光谱提示和可学习的[SINK]令牌，实现了无需梯度反传或主干修改的高效、可靠ViT解释性，显著提升了高光谱数据的解释性，填补了黑盒建模与可信HSI决策之间的鸿沟。

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


### [46] [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](https://arxiv.org/abs/2507.14809)
*Zesen Zhong,Duomin Zhang,Yijia Li*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级的机器人动作预测方法，利用InstructPix2Pix模型实现多模态未来帧预测，显著降低了计算成本和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 预测未来的运动轨迹在机器人、自主系统和人类活动预测等领域至关重要，能够实现更安全、更智能的决策。

Method: 论文采用基于深度学习的方法，重新调整并微调了InstructPix2Pix模型，使其能够接受视觉和文本输入，实现多模态未来帧预测。

Result: 在RoboTWin数据集上的实验表明，该方法在机器人动作预测任务中优于现有基线模型，具有更高的SSIM和PSNR。

Conclusion: 该论文提出了一种轻量级、高效的机器人动作预测方法，显著降低了计算成本和推理延迟，适用于需要高精度运动轨迹的应用场景。

Abstract: Predicting future motion trajectories is a critical capability across domains
such as robotics, autonomous systems, and human activity forecasting, enabling
safer and more intelligent decision-making. This paper proposes a novel,
efficient, and lightweight approach for robot action prediction, offering
significantly reduced computational cost and inference latency compared to
conventional video prediction models. Importantly, it pioneers the adaptation
of the InstructPix2Pix model for forecasting future visual frames in robotic
tasks, extending its utility beyond static image editing. We implement a deep
learning-based visual prediction framework that forecasts what a robot will
observe 100 frames (10 seconds) into the future, given a current image and a
textual instruction. We repurpose and fine-tune the InstructPix2Pix model to
accept both visual and textual inputs, enabling multimodal future frame
prediction. Experiments on the RoboTWin dataset (generated based on real-world
scenarios) demonstrate that our method achieves superior SSIM and PSNR compared
to state-of-the-art baselines in robot action prediction tasks. Unlike
conventional video prediction models that require multiple input frames, heavy
computation, and slow inference latency, our approach only needs a single image
and a text prompt as input. This lightweight design enables faster inference,
reduced GPU demands, and flexible multimodal control, particularly valuable for
applications like robotics and sports motion trajectory analytics, where motion
trajectory precision is prioritized over visual fidelity.

</details>


### [47] [A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation](https://arxiv.org/abs/2507.14790)
*Wenbo Yue,Chang Li,Guoping Xu*

Main category: cs.CV

TL;DR: 该研究提出HPD方法，通过MinMaxPooling保留图像细节，提升语义分割性能，平均DSC提高0.5%。


<details>
  <summary>Details</summary>
Motivation: 传统下采样方法可能导致语义分割任务中关键空间信息的丢失，影响逐像素预测精度。

Method: 提出基于信息互补的下采样方法——混合池化下采样（HPD），用MinMaxPooling替代传统方法，有效保留图像的明暗对比和细节特征。

Result: 在ACDC和Synapse数据集上的实验表明，HPD在分割性能上优于传统方法。

Conclusion: HPD模块为语义分割任务提供了一种高效解决方案，平均提高了DSC系数0.5%。

Abstract: In convolutional neural networks (CNNs), downsampling operations are crucial
to model performance. Although traditional downsampling methods (such as
maximum pooling and cross-row convolution) perform well in feature aggregation,
receptive field expansion, and computational reduction, they may lead to the
loss of key spatial information in semantic segmentation tasks, thereby
affecting the pixel-by-pixel prediction accuracy.To this end, this study
proposes a downsampling method based on information complementarity - Hybrid
Pooling Downsampling (HPD). The core is to replace the traditional method with
MinMaxPooling, and effectively retain the light and dark contrast and detail
features of the image by extracting the maximum value information of the local
area.Experiment on various CNN architectures on the ACDC and Synapse datasets
show that HPD outperforms traditional methods in segmentation performance, and
increases the DSC coefficient by 0.5% on average. The results show that the HPD
module provides an efficient solution for semantic segmentation tasks.

</details>


### [48] [EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring](https://arxiv.org/abs/2507.15036)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: EBA-AI是一个基于伦理的偏见感知AI框架，通过CLIP嵌入和自适应处理解决水下图像增强中的数据偏见和计算效率问题，实验证明其在效率和公平性上的优势。


<details>
  <summary>Details</summary>
Motivation: 水下图像增强对海洋保护至关重要，但现有AI模型常面临数据集偏见、高计算成本和缺乏透明度等问题，可能导致错误解读。

Method: EBA-AI利用CLIP嵌入检测和缓解数据集偏见，并集成自适应处理以优化能源效率，显著减少GPU使用。

Result: 在LSUI400、Oceanex和UIEB100数据集上的实验表明，PSNR仅下降1.0 dB，但计算节省使得大规模海洋监测具有实时可行性。

Conclusion: 通过EBA-AI框架，本研究为水下图像处理提供了一种兼顾效率、公平性和可解释性的解决方案，推动了可持续、偏见感知且计算高效的海洋保护工作。

Abstract: Underwater image enhancement is vital for marine conservation, particularly
coral reef monitoring. However, AI-based enhancement models often face dataset
bias, high computational costs, and lack of transparency, leading to potential
misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware
AI framework to address these challenges. EBA-AI leverages CLIP embeddings to
detect and mitigate dataset bias, ensuring balanced representation across
varied underwater environments. It also integrates adaptive processing to
optimize energy efficiency, significantly reducing GPU usage while maintaining
competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100
show that while PSNR drops by a controlled 1.0 dB, computational savings enable
real-time feasibility for large-scale marine monitoring. Additionally,
uncertainty estimation and explainability techniques enhance trust in AI-driven
environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,
WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing
efficiency, fairness, and interpretability in underwater image processing. By
addressing key limitations of AI-driven enhancement, this work contributes to
sustainable, bias-aware, and computationally efficient marine conservation
efforts. For interactive visualizations, animations, source code, and access to
the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/

</details>


### [49] [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/abs/2507.14797)
*Beier Zhu,Ruoyu Wang,Tong Zhao,Hanwang Zhang,Chi Zhang*

Main category: cs.CV

TL;DR: EPD, a novel ODE solver, reduces latency in diffusion models via parallel gradient evaluations, maintaining high image quality and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the high sampling latency and image quality degradation issues in existing solver-based acceleration methods for diffusion models.

Method: The method involves an ODE solver that incorporates multiple parallel gradient evaluations in each step, optimized with learnable parameters in a distillation fashion.

Result: EPD achieves superior performance with FID scores of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26 on LSUN Bedroom at 5 NFE latency.

Conclusion: The proposed Ensemble Parallel Direction (EPD) solver effectively reduces sampling latency in diffusion models while maintaining high image quality, outperforming existing learning-based solvers by a significant margin.

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance
but suffer from high sampling latency due to their sequential denoising nature.
Existing solver-based acceleration methods often face image quality degradation
under a low-latency budget. In this paper, we propose the Ensemble Parallel
Direction solver (dubbed as \ours), a novel ODE solver that mitigates
truncation errors by incorporating multiple parallel gradient evaluations in
each ODE step. Importantly, since the additional gradient computations are
independent, they can be fully parallelized, preserving low-latency sampling.
  Our method optimizes a small set of learnable parameters in a distillation
fashion, ensuring minimal training overhead.
  In addition, our method can serve as a plugin to improve existing ODE
samplers. Extensive experiments on various image synthesis benchmarks
demonstrate the effectiveness of our \ours~in achieving high-quality and
low-latency sampling. For example, at the same latency level of 5 NFE, EPD
achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26
on LSUN Bedroom, surpassing existing learning-based solvers by a significant
margin. Codes are available in https://github.com/BeierZhu/EPD.

</details>


### [50] [Visual Place Recognition for Large-Scale UAV Applications](https://arxiv.org/abs/2507.15089)
*Ioannis Tsampikos Papapetros,Ioannis Kansizoglou,Antonios Gasteratos*

Main category: cs.CV

TL;DR: 为解决航空视觉地点识别的数据稀缺和旋转模糊性问题，研究提出了大规模数据集LASED和可操纵CNNs，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 航空vPR面临大规模高海拔数据集稀缺和无人机图像固有的旋转模糊性等独特挑战，限制了模型的泛化能力。

Method: 提出了LASED大规模航空数据集，并整合了可操纵卷积神经网络（CNNs）以明确处理旋转方差。

Result: 在LASED上训练的模型召回率显著高于小规模、低多样性数据集训练的模型，可操纵CNNs平均比非可操纵网络召回率提高12%。

Conclusion: 结合结构化的大规模数据集与旋转等变神经网络，我们的方法显著提升了航空视觉地点识别（vPR）模型的鲁棒性和泛化能力。

Abstract: Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial
Vehicle (UAV) navigation, enabling robust localization across diverse
environments. Despite significant advancements, aerial vPR faces unique
challenges due to the limited availability of large-scale, high-altitude
datasets, which limits model generalization, along with the inherent rotational
ambiguity in UAV imagery. To address these challenges, we introduce LASED, a
large-scale aerial dataset with approximately one million images,
systematically sampled from 170,000 unique locations throughout Estonia over a
decade, offering extensive geographic and temporal diversity. Its structured
design ensures clear place separation significantly enhancing model training
for aerial scenarios. Furthermore, we propose the integration of steerable
Convolutional Neural Networks (CNNs) to explicitly handle rotational variance,
leveraging their inherent rotational equivariance to produce robust,
orientation-invariant feature representations. Our extensive benchmarking
demonstrates that models trained on LASED achieve significantly higher recall
compared to those trained on smaller, less diverse datasets, highlighting the
benefits of extensive geographic coverage and temporal diversity. Moreover,
steerable CNNs effectively address rotational ambiguity inherent in aerial
imagery, consistently outperforming conventional convolutional architectures,
achieving on average 12\% recall improvement over the best-performing
non-steerable network. By combining structured, large-scale datasets with
rotation-equivariant neural networks, our approach significantly enhances model
robustness and generalization for aerial vPR.

</details>


### [51] [An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks](https://arxiv.org/abs/2507.14798)
*Xinyi Wu,Steven Landgraf,Markus Ulrich,Rongjun Qin*

Main category: cs.CV

TL;DR: 本文评估了DUSt3R/MASt3R/VGGT在航拍图像上的表现，发现它们在稀疏、低分辨率场景中优于传统方法，但仍存在高分辨率和大图像集的限制。


<details>
  <summary>Details</summary>
Motivation: 尽管这些模型在计算机视觉基准测试中表现良好，但其在摄影测量航拍块上的潜力尚未被探索。

Method: 本文对预训练的DUSt3R/MASt3R/VGGT模型在UseGeo数据集上的航拍块进行了姿态估计和密集3D重建的全面评估。

Result: 结果显示，这些方法能从极稀疏的图像集（少于10张图像，分辨率高达518像素）中准确重建密集点云，完整性比COLMAP提升高达50%。VGGT还展现出更高的计算效率、可扩展性和更可靠的相机姿态估计。

Conclusion: Transformer-based方法（如DUSt3R/MASt3R/VGGT）虽无法完全取代传统的SfM和MVS，但在低分辨率、稀疏图像等挑战性场景中展现出潜力，可作为补充方法。

Abstract: State-of-the-art 3D computer vision algorithms continue to advance in
handling sparse, unordered image sets. Recently developed foundational models
for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction
(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry
Grounded Transformer (VGGT), have attracted attention due to their ability to
handle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical
aerial images matters, as these models may handle extremely low image overlaps,
stereo occlusions, and textureless regions. For redundant collections, they can
accelerate 3D reconstruction by using extremely sparsified image sets. Despite
tests on various computer vision benchmarks, their potential on photogrammetric
aerial blocks remains unexplored. This paper conducts a comprehensive
evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of
the UseGeo dataset for pose estimation and dense 3D reconstruction. Results
show these methods can accurately reconstruct dense point clouds from very
sparse image sets (fewer than 10 images, up to 518 pixels resolution), with
completeness gains up to +50% over COLMAP. VGGT also demonstrates higher
computational efficiency, scalability, and more reliable camera pose
estimation. However, all exhibit limitations with high-resolution images and
large sets, as pose reliability declines with more images and geometric
complexity. These findings suggest transformer-based methods cannot fully
replace traditional SfM and MVS, but offer promise as complementary approaches,
especially in challenging, low-resolution, and sparse scenarios.

</details>


### [52] [Exploring Scalable Unified Modeling for General Low-Level Vision](https://arxiv.org/abs/2507.14801)
*Xiangyu Chen,Kaiwen Zhu,Yuandong Pu,Shuo Cao,Xiaohui Li,Wenlong Zhang,Yihao Liu,Yu Qiao,Jiantao Zhou,Chao Dong*

Main category: cs.CV

TL;DR: 提出了VPIP框架和GenLV模型，通过视觉提示统一处理多样低级视觉任务，实验验证了其性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决低级视觉任务多样性带来的统一建模挑战。

Method: 提出了一个基于视觉提示的图像处理框架VPIP，包括端到端的图像处理主干、提示编码器和提示交互模块，并开发了统一模型GenLV。

Result: 实验结果表明，该方法在多种任务中表现优异，增加训练任务数量能提升泛化能力，尤其在数据有限的任务中。

Conclusion: 该论文提出的VPIP框架和GenLV模型在多样化的低级视觉任务中展现出显著的性能和泛化能力，证实了其作为统一基础模型的潜力和有效性。

Abstract: Low-level vision involves a wide spectrum of tasks, including image
restoration, enhancement, stylization, and feature extraction, which differ
significantly in both task formulation and output domains. To address the
challenge of unified modeling across such diverse tasks, we propose a Visual
task Prompt-based Image Processing (VPIP) framework that leverages input-target
image pairs as visual prompts to guide the model in performing a variety of
low-level vision tasks. The framework comprises an end-to-end image processing
backbone, a prompt encoder, and a prompt interaction module, enabling flexible
integration with various architectures and effective utilization of
task-specific visual representations. Based on this design, we develop a
unified low-level vision model, GenLV, and evaluate its performance across
multiple representative tasks. To explore the scalability of this approach, we
extend the framework along two dimensions: model capacity and task diversity.
We construct a large-scale benchmark consisting of over 100 low-level vision
tasks and train multiple versions of the model with varying scales.
Experimental results show that the proposed method achieves considerable
performance across a wide range of tasks. Notably, increasing the number of
training tasks enhances generalization, particularly for tasks with limited
data, indicating the model's ability to learn transferable representations
through joint training. Further evaluations in zero-shot generalization,
few-shot transfer, and task-specific fine-tuning scenarios demonstrate the
model's strong adaptability, confirming the effectiveness, scalability, and
potential of the proposed framework as a unified foundation for general
low-level vision modeling.

</details>


### [53] [Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images](https://arxiv.org/abs/2507.15496)
*JunYing Huang,Ao Xu,DongSun Yong,KeRen Li,YuanFeng Wang,Qi Qin*

Main category: cs.CV

TL;DR: 提出了一种结合LiDAR点云和图像的里程计框架，通过深度补全和多尺度特征提取提升姿态估计的准确性和鲁棒性，实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 里程计是自主系统进行自定位和导航的关键任务，现有方法在动态环境和尺度模糊性方面存在局限，因此需要一种更准确和鲁棒的姿态估计方法。

Method: 论文方法包括利用深度补全从点云和图像中估计稠密深度图，结合多尺度特征提取网络和注意力机制，生成自适应深度感知表示，并通过稠密深度信息优化光流估计以减少遮挡区域的误差。此外，采用分层姿态细化模块逐步优化运动估计。

Result: 在KITTI里程计基准测试中，该方法在准确性和鲁棒性上达到或超越了现有最先进的视觉和LiDAR里程计方法。

Conclusion: 该论文提出的LiDAR-Visual里程计框架在KITTI里程计基准测试中表现出与现有最先进方法相当或更优的准确性和鲁棒性。

Abstract: Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.

</details>


### [54] [Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection](https://arxiv.org/abs/2507.14807)
*Juan Hu,Shaojing Fan,Terence Sim*

Main category: cs.CV

TL;DR: HICOM框架利用人类认知线索（如场景运动和面部一致性）显著提升多脸深度伪造检测的准确率、泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测方法在单脸场景表现良好，但在多脸场景中因缺乏上下文线索而效果不佳。本文旨在通过人类认知提升多脸深度伪造检测能力。

Method: 通过人类研究系统分析人们在社交场景中检测深度伪造面部的关键线索（场景运动一致性、面部间外观兼容性、人际凝视对齐和面部身体一致性），并基于这些线索开发了HICOM框架。

Result: HICOM在基准数据集上平均准确率提升3.3%（数据集内）和2.8%（真实世界扰动下），在未见数据集上优于现有方法5.8%，并提供了可解释的检测结果。

Conclusion: 本文通过引入人类认知线索，提出了HICOM框架，显著提升了多脸深度伪造视频的检测准确率和泛化能力，并增强了结果的可解释性。

Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often
appearing in natural social settings that challenge existing detection methods.
Most current approaches excel at single-face detection but struggle in
multi-face scenarios, due to a lack of awareness of crucial contextual cues. In
this work, we develop a novel approach that leverages human cognition to
analyze and defend against multi-face deepfake videos. Through a series of
human studies, we systematically examine how people detect deepfake faces in
social settings. Our quantitative analysis reveals four key cues humans rely
on: scene-motion coherence, inter-face appearance compatibility, interpersonal
gaze alignment, and face-body consistency. Guided by these insights, we
introduce \textsf{HICOM}, a novel framework designed to detect every fake face
in multi-face scenarios. Extensive experiments on benchmark datasets show that
\textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and
2.8\% under real-world perturbations. Moreover, it outperforms existing methods
by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired
cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM
to provide human-readable explanations, making detection results more
transparent and convincing. Our work sheds light on involving human factors to
enhance defense against deepfakes.

</details>


### [55] [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597)
*Hao Luo,Yicheng Feng,Wanpeng Zhang,Sipeng Zheng,Ye Wang,Haoqi Yuan,Jiazheng Liu,Chaoyi Xu,Qin Jin,Zongqing Lu*

Main category: cs.CV

TL;DR: Being-H0是一种基于人类视频的灵巧视觉-语言-动作模型，通过物理指令调优和部分级运动标记化，显著提升复杂操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在复杂操作任务中表现不佳，主要依赖合成数据或有限的操作演示，缺乏规模化和多样性。

Method: 提出了一种新的训练范式——物理指令调优，结合了大规模VLA预训练、物理空间对齐和部分级运动标记化方法。

Result: Being-H0在手部运动生成和指令跟随方面表现出色，并能随模型和数据规模扩展而提升性能。

Conclusion: Being-H0通过结合大规模人类视频预训练、物理空间对齐和机器人任务后适应，显著提升了复杂操作任务的执行能力，并在实际机器人操作中表现出色。

Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.

</details>


### [56] [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)
*Jiaji Zhang,Ruichao Sun,Hailiang Zhao,Jiaju Wu,Peng Chen,Hao Li,Xinkui Zhao,Kingsum Chow,Gang Xiong,Lin Ye,Shuiguang Deng*

Main category: cs.CV

TL;DR: SegQuant 是一种统一的量化框架，通过 SegLinear 和 DualScale 技术提升扩散模型的量化通用性和部署效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在资源受限或延迟敏感的环境中部署时面临计算密集型挑战，现有 PTQ 方法依赖架构特定启发式，限制了通用性和工业部署流程的集成。

Method: SegQuant 包含一个段感知、基于图的量化策略（SegLinear），捕捉结构语义和空间异质性，以及一个双尺度量化方案（DualScale），保留极性不对称激活以维持生成输出的视觉保真度。

Result: SegQuant 在 Transformer 以外的扩散模型中表现优异，同时确保与主流部署工具的无缝兼容性。

Conclusion: SegQuant 提出了一种统一的量化框架，通过自适应结合互补技术提升跨模型通用性，解决了现有 PTQ 方法在扩散模型中依赖架构特定启发式的问题。

Abstract: Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.

</details>


### [57] [FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models](https://arxiv.org/abs/2507.14823)
*Dong Shu,Haoyang Yuan,Yuchen Wang,Yanguang Liu,Huopu Zhang,Haiyan Zhao,Mengnan Du*

Main category: cs.CV

TL;DR: FinChart-Bench是首个针对金融图表的基准，评估了25种LVLMs，揭示了当前模型在金融图表理解上的局限性，尤其是指令遵循和空间推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 金融图表因其复杂的时间结构和领域特定术语，在现有研究中未被充分探索，需要专门的基准和评估。

Method: 通过构建FinChart-Bench基准，包含1,200张金融图表和7,016个问题，对25种最先进的LVLMs进行全面评估。

Result: 评估发现开源与闭源模型性能差距缩小，升级模型性能下降，多数模型在指令遵循和空间推理上表现不佳，且当前LVLMs不适合作为自动化评估工具。

Conclusion: 当前的大型视觉语言模型（LVLMs）在金融图表理解方面仍存在显著局限性，尤其是在指令遵循和空间推理能力上。FinChart-Bench的发布为未来研究提供了重要基准。

Abstract: Large vision-language models (LVLMs) have made significant progress in chart
understanding. However, financial charts, characterized by complex temporal
structures and domain-specific terminology, remain notably underexplored. We
introduce FinChart-Bench, the first benchmark specifically focused on
real-world financial charts. FinChart-Bench comprises 1,200 financial chart
images collected from 2015 to 2024, each annotated with True/False (TF),
Multiple Choice (MC), and Question Answering (QA) questions, totaling 7,016
questions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs
on FinChart-Bench. Our evaluation reveals critical insights: (1) the
performance gap between open-source and closed-source models is narrowing, (2)
performance degradation occurs in upgraded models within families, (3) many
models struggle with instruction following, (4) both advanced models show
significant limitations in spatial reasoning abilities, and (5) current LVLMs
are not reliable enough to serve as automated evaluators. These findings
highlight important limitations in current LVLM capabilities for financial
chart understanding. The FinChart-Bench dataset is available at
https://huggingface.co/datasets/Tizzzzy/FinChart-Bench.

</details>


### [58] [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/abs/2507.14826)
*Fu-Jen Tsai,Yan-Tsung Peng,Yen-Yu Lin,Chia-Wen Lin*

Main category: cs.CV

TL;DR: PHATNet通过雾霾模式转移和新型损失函数，提升去雾模型在未见领域的适应能力，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有去雾模型在未见过的真实世界雾霾图像上表现显著下降，因训练数据有限，需开发灵活领域适应方法。

Method: 提出了物理引导的雾霾转移网络（PHATNet），将目标领域的雾霾模式转移到源领域的无雾图像上，生成领域特定的微调数据集，以更新去雾模型。

Result: 实验证明PHATNet在基准真实世界图像去雾数据集上显著提升了最先进去雾模型的性能。

Conclusion: PHATNet通过灵活领域适应方法显著提升了现有去雾模型在现实世界图像上的性能，特别是在未见过的领域。

Abstract: Image dehazing aims to remove unwanted hazy artifacts in images. Although
previous research has collected paired real-world hazy and haze-free images to
improve dehazing models' performance in real-world scenarios, these models
often experience significant performance drops when handling unseen real-world
hazy images due to limited training data. This issue motivates us to develop a
flexible domain adaptation method to enhance dehazing performance during
testing. Observing that predicting haze patterns is generally easier than
recovering clean content, we propose the Physics-guided Haze Transfer Network
(PHATNet) which transfers haze patterns from unseen target domains to
source-domain haze-free images, creating domain-specific fine-tuning sets to
update dehazing models for effective domain adaptation. Additionally, we
introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to
enhance PHATNet's disentanglement ability. Experimental results demonstrate
that PHATNet significantly boosts state-of-the-art dehazing models on benchmark
real-world image dehazing datasets.

</details>


### [59] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: 提出一种无需外部条件的成对图像生成方法，通过额外扩散引导器提升生成质量，缓解标注数据短缺并改善下游任务效果。


<details>
  <summary>Details</summary>
Motivation: DBT图像中肿块病变的高隐蔽性导致手动标注困难且耗时，缺乏标注数据用于模型训练，现有扩散模型在生成质量和标注生成方面存在局限。

Method: 通过训练一个额外的扩散引导器来实现条件扩散模型的成对图像生成。

Result: 实验结果表明，该方法能够生成成对的DBT切片和肿块病变掩模，并在无外部条件下提高生成质量，增强下游任务性能。

Conclusion: 该论文提出的成对图像生成方法无需外部条件，能够提高生成质量，并缓解标注数据短缺问题，从而提升下游任务的性能。

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images
is very significant for the early screening of breast cancer. However, the
high-density breast tissue often leads to high concealment of the mass lesions,
which makes manual annotation difficult and time-consuming. As a result, there
is a lack of annotated data for model training. Diffusion models are commonly
used for data augmentation, but the existing methods face two challenges.
First, due to the high concealment of lesions, it is difficult for the model to
learn the features of the lesion area. This leads to the low generation quality
of the lesion areas, thus limiting the quality of the generated images. Second,
existing methods can only generate images and cannot generate corresponding
annotations, which restricts the usability of the generated images in
supervised training. In this work, we propose a paired image generation method.
The method does not require external conditions and can achieve the generation
of paired images by training an extra diffusion guider for the conditional
diffusion model. During the experimental phase, we generated paired DBT slices
and mass lesion masks. Then, we incorporated them into the supervised training
process of the mass lesion segmentation task. The experimental results show
that our method can improve the generation quality without external conditions.
Moreover, it contributes to alleviating the shortage of annotated data, thus
enhancing the performance of downstream tasks.

</details>


### [60] [Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image](https://arxiv.org/abs/2507.14845)
*Rizhao Fan,Zhigen Li,Heping Li,Ning An*

Main category: cs.CV

TL;DR: 提出一种仅需稀疏深度和单张图像的自监督深度补全方法，通过新损失函数和语义分割提升性能，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度补全方法对密集标注或多帧依赖的高成本和限制，特别是在静态或单帧场景中的适用性问题。

Method: 设计新的损失函数以利用深度分布特性传播深度信息，并结合视觉基础模型生成的语义分割图增强深度估计。

Result: 实验证明，该方法在无需密集标注或额外视角图像的情况下，能够有效完成深度补全任务。

Conclusion: 提出的自监督深度补全方法在仅需稀疏深度测量和对应图像的情况下，有效解决了传统方法对密集标注或多帧依赖的限制，并通过实验验证了其有效性。

Abstract: Depth completion is an important vision task, and many efforts have been made
to enhance the quality of depth maps from sparse depth measurements. Despite
significant advances, training these models to recover dense depth from sparse
measurements remains a challenging problem. Supervised learning methods rely on
dense depth labels to predict unobserved regions, while self-supervised
approaches require image sequences to enforce geometric constraints and
photometric consistency between frames. However, acquiring dense annotations is
costly, and multi-frame dependencies limit the applicability of self-supervised
methods in static or single-frame scenarios. To address these challenges, we
propose a novel self-supervised depth completion paradigm that requires only
sparse depth measurements and their corresponding image for training. Unlike
existing methods, our approach eliminates the need for dense depth labels or
additional images captured from neighboring viewpoints. By leveraging the
characteristics of depth distribution, we design novel loss functions that
effectively propagate depth information from observed points to unobserved
regions. Additionally, we incorporate segmentation maps generated by vision
foundation models to further enhance depth estimation. Extensive experiments
demonstrate the effectiveness of our proposed method.

</details>


### [61] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: 论文提出了一种基于基础模型的全能视频修复框架，无需退化知识，提供可解释指导，并呼吁标准化基准建设，展示了在多任务中的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频修复方法通常需要退化知识，限制了其灵活性和可解释性。论文旨在通过基础模型提供无需退化知识的可解释指导，并推动视频修复领域的标准化基准建设。

Method: 论文提出了一种基于基础模型的退化感知语义上下文框架，通过自然语言提供视频帧的退化感知指导，并学习近似知识以实现推理时的高效解耦。此外，提出了两个多退化设置基准和两个时变复合退化基准。

Result: 论文提出的方法在多个基准测试中均达到了最先进的性能，包括多退化设置和时变复合退化场景。

Conclusion: 该论文提出了一个全能的视频修复框架，利用基础模型通过自然语言提供可解释且灵活的指导，无需在训练或测试时了解退化知识，并在推理时无额外成本地解耦基础模型。同时，论文呼吁在视频修复领域建立标准化基准，并提出了多个新基准，展示了在多个任务上的最先进性能。

Abstract: In this work, we propose an all-in-one video restoration framework that
grounds degradation-aware semantic context of video frames in natural language
via foundation models, offering interpretable and flexible guidance. Unlike
prior art, our method assumes no degradation knowledge in train or test time
and learns an approximation to the grounded knowledge such that the foundation
model can be safely disentangled during inference adding no extra cost.
Further, we call for standardization of benchmarks in all-in-one video
restoration, and propose two benchmarks in multi-degradation setting,
three-task (3D) and four-task (4D), and two time-varying composite degradation
benchmarks; one of the latter being our proposed dataset with varying snow
intensity, simulating how weather degradations affect videos naturally. We
compare our method with prior works and report state-of-the-art performance on
all benchmarks.

</details>


### [62] [An Uncertainty-aware DETR Enhancement Framework for Object Detection](https://arxiv.org/abs/2507.14855)
*Xingshu Chen,Sicheng Yu,Chong Cheng,Hao Wang,Ting Tian*

Main category: cs.CV

TL;DR: 本文提出一种不确定性感知的DETR增强框架，通过建模边界框分布和优化损失函数，提升了检测准确性和鲁棒性，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统检测器的确定性边界框回归忽略了预测中的不确定性，限制了模型的鲁棒性。本文旨在通过不确定性建模提升检测的准确性和可靠性。

Method: 通过将边界框建模为多元高斯分布，并在损失函数中引入Gromov-Wasserstein距离来优化预测与真实分布的匹配。此外，提出了一种基于贝叶斯风险的筛选机制和量化定位不确定性的算法。

Result: 在COCO基准测试中，该方法能有效提升现有DETR变体的性能，并在LISC和WBCDD数据集的白细胞检测任务中达到最先进水平。

Conclusion: 该论文提出的不确定性感知增强框架不仅提升了DETR基检测器的性能，还在白细胞检测任务中实现了最先进的结果，证明了其跨领域任务的扩展性。

Abstract: This paper investigates the problem of object detection with a focus on
improving both the localization accuracy of bounding boxes and explicitly
modeling prediction uncertainty. Conventional detectors rely on deterministic
bounding box regression, ignoring uncertainty in predictions and limiting model
robustness. In this paper, we propose an uncertainty-aware enhancement
framework for DETR-based object detectors. We model bounding boxes as
multivariate Gaussian distributions and incorporate the Gromov-Wasserstein
distance into the loss function to better align the predicted and ground-truth
distributions. Building on this, we derive a Bayes Risk formulation to filter
high-risk information and improve detection reliability. We also propose a
simple algorithm to quantify localization uncertainty via confidence intervals.
Experiments on the COCO benchmark show that our method can be effectively
integrated into existing DETR variants, enhancing their performance. We further
extend our framework to leukocyte detection tasks, achieving state-of-the-art
results on the LISC and WBCDD datasets. These results confirm the scalability
of our framework across both general and domain-specific detection tasks. Code
page:
https://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.

</details>


### [63] [Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition](https://arxiv.org/abs/2507.14867)
*Zhaoqiang Xia,Hexiang Huang,Haoyu Chen,Xiaoyi Feng,Guoying Zhao*

Main category: cs.CV

TL;DR: 论文提出一种基于超图增强Transformer的混合监督框架，用于从微手势中识别情绪状态，并在公开数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 微手势作为无意识的身体动作能够传达人类情绪状态，但在情绪建模方面尚未充分探索。

Method: 采用超图增强Transformer的混合监督框架，包括编码器和解码器设计，结合自监督重建任务和监督学习的情绪识别头。

Result: 在两个公开数据集上取得最佳性能，优于现有方法。

Conclusion: 该论文提出的超图增强Transformer混合监督框架在iMiGUE和SMG两个公开数据集上表现优异，性能优于现有方法。

Abstract: Micro-gestures are unconsciously performed body gestures that can convey the
emotion states of humans and start to attract more research attention in the
fields of human behavior understanding and affective computing as an emerging
topic. However, the modeling of human emotion based on micro-gestures has not
been explored sufficiently. In this work, we propose to recognize the emotion
states based on the micro-gestures by reconstructing the behavior patterns with
a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the
framework, hypergraph Transformer based encoder and decoder are separately
designed by stacking the hypergraph-enhanced self-attention and multiscale
temporal convolution modules. Especially, to better capture the subtle motion
of micro-gestures, we construct a decoder with additional upsampling operations
for a reconstruction task in a self-supervised learning manner. We further
propose a hypergraph-enhanced self-attention module where the hyperedges
between skeleton joints are gradually updated to present the relationships of
body joints for modeling the subtle local motion. Lastly, for exploiting the
relationship between the emotion states and local motion of micro-gestures, an
emotion recognition head from the output of encoder is designed with a shallow
architecture and learned in a supervised way. The end-to-end framework is
jointly trained in a one-stage way by comprehensively utilizing
self-reconstruction and supervision information. The proposed method is
evaluated on two publicly available datasets, namely iMiGUE and SMG, and
achieves the best performance under multiple metrics, which is superior to the
existing methods.

</details>


### [64] [Region-aware Depth Scale Adaptation with Sparse Measurements](https://arxiv.org/abs/2507.14879)
*Rizhao Fan,Tianfang Ma,Zhigen Li,Ning An,Jian Cheng*

Main category: cs.CV

TL;DR: 提出一种无需学习的方法，利用稀疏深度测量将基础模型的相对深度预测转换为度量尺度，保留泛化能力且无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在零样本单目深度估计中表现出色，但其输出通常是相对尺度而非度量尺度，限制了实际应用。现有尺度适应方法成本高且会损害模型的泛化能力。

Method: 采用非学习的方法，利用稀疏深度测量来调整基础模型的相对尺度预测，无需重新训练或微调。

Result: 实验证明，该方法能有效将相对尺度深度转换为度量尺度，且不增加计算成本或牺牲泛化能力。

Conclusion: 本文提出了一种无需学习的方法，利用稀疏深度测量将基础模型的相对尺度预测转换为度量尺度深度，既保留了模型的泛化能力，又实现了度量深度输出。

Abstract: In recent years, the emergence of foundation models for depth prediction has
led to remarkable progress, particularly in zero-shot monocular depth
estimation. These models generate impressive depth predictions; however, their
outputs are often in relative scale rather than metric scale. This limitation
poses challenges for direct deployment in real-world applications. To address
this, several scale adaptation methods have been proposed to enable foundation
models to produce metric depth. However, these methods are typically costly, as
they require additional training on new domains and datasets. Moreover,
fine-tuning these models often compromises their original generalization
capabilities, limiting their adaptability across diverse scenes. In this paper,
we introduce a non-learning-based approach that leverages sparse depth
measurements to adapt the relative-scale predictions of foundation models into
metric-scale depth. Our method requires neither retraining nor fine-tuning,
thereby preserving the strong generalization ability of the original foundation
models while enabling them to produce metric depth. Experimental results
demonstrate the effectiveness of our approach, high-lighting its potential to
bridge the gap between relative and metric depth without incurring additional
computational costs or sacrificing generalization ability.

</details>


### [65] [BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters](https://arxiv.org/abs/2507.14885)
*Joaquim Comas,Federico Sukno*

Main category: cs.CV

TL;DR: BeatFormer是一种结合手工方法和深度学习优势的轻量级光谱注意力模型，通过SCL无需标签训练，在多个数据集中表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法依赖于大量多样化的数据集，而手工方法虽然泛化能力强但受限于线性假设。因此需要结合两者的优势，开发一种既高效又泛化能力强的rPPG估计方法。

Method: 提出了BeatFormer，一种轻量级的光谱注意力模型，结合了放大的正交复注意力机制和频域能量测量。同时引入Spectral Contrastive Learning (SCL)，使模型无需PPG或HR标签即可训练。

Result: 在PURE、UBFC-rPPG和MMPD数据集上验证了BeatFormer的鲁棒性和性能，尤其在跨数据集和运动场景下的表现优异。

Conclusion: BeatFormer结合了手工方法和深度学习的优势，通过光谱注意力模型和对比学习，实现了在无监督情况下的高效rPPG估计，并在多个数据集中验证了其鲁棒性和性能。

Abstract: Remote photoplethysmography (rPPG) captures cardiac signals from facial
videos and is gaining attention for its diverse applications. While deep
learning has advanced rPPG estimation, it relies on large, diverse datasets for
effective generalization. In contrast, handcrafted methods utilize
physiological priors for better generalization in unseen scenarios like motion
while maintaining computational efficiency. However, their linear assumptions
limit performance in complex conditions, where deep learning provides superior
pulsatile information extraction. This highlights the need for hybrid
approaches that combine the strengths of both methods. To address this, we
present BeatFormer, a lightweight spectral attention model for rPPG estimation,
which integrates zoomed orthonormal complex attention and frequency-domain
energy measurement, enabling a highly efficient model. Additionally, we
introduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be
trained without any PPG or HR labels. We validate BeatFormer on the PURE,
UBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,
particularly in cross-dataset evaluations under motion scenarios.

</details>


### [66] [TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP](https://arxiv.org/abs/2507.14904)
*Fan Li,Zanyi Wang,Zeyi Huang,Guang Dai,Jingdong Wang,Mengmeng Wang*

Main category: cs.CV

TL;DR: 提出统一2D预训练多模态网络GARF，简化架构并提升性能，减少参数58%，3D检测和视觉定位任务分别提升6.52%和6.25%。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法依赖多模态分离编码器，导致模型复杂且训练低效，而现有基于2D多模态预训练模型的方法在点云数据对齐上表现不佳。

Method: 提出了一种统一的2D预训练多模态网络（GARF模块），用于处理RGB图像、文本和点云数据，通过适配器微调和几何感知特征融合实现跨模态特征提取与融合。

Result: 相比基线方法，减少了约58%的可训练参数，3D检测任务性能提升6.52%，3D视觉定位任务性能提升6.25%。

Conclusion: 该方法通过统一的2D预训练多模态网络显著简化了架构，减少了约58%的可训练参数，同时在3D检测和3D视觉定位任务中分别提升了6.52%和6.25%的性能。

Abstract: 3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.

</details>


### [67] [Semantic-Aware Representation Learning for Multi-label Image Classification](https://arxiv.org/abs/2507.14918)
*Ren-Dong Xie,Zhi-Fen He,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: 提出SARL方法，通过语义相关特征学习和最优传输注意力机制，提升多标签图像分类性能，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的多标签图像分类方法（如注意力机制或图卷积网络）可能在图像表示中包含噪声且无法精确定位对象。

Method: 提出了一种语义感知表示学习（SARL）方法，包括标签语义相关特征学习模块、基于最优传输的注意力机制和区域分数聚合策略。

Result: 在PASCAL VOC 2007和MS-COCO数据集上的实验结果表明SARL的优越性。

Conclusion: SARL方法在PASCAL VOC 2007和MS-COCO基准数据集上表现出色，优于现有方法。

Abstract: Multi-label image classification, an important research area in computer
vision, focuses on identifying multiple labels or concepts within an image.
Existing approaches often employ attention mechanisms or graph convolutional
networks (GCNs) to learn image representation. However, this representation may
contain noise and may not locate objects precisely. Therefore, this paper
proposes a Semantic-Aware Representation Learning (SARL) for multi-label image
classification. First, a label semantic-related feature learning module is
utilized to extract semantic-related features. Then, an optimal transport-based
attention mechanism is designed to obtain semantically aligned image
representation. Finally, a regional score aggregation strategy is used for
multi-label prediction. Experimental results on two benchmark datasets, PASCAL
VOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing
methods.

</details>


### [68] [Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction](https://arxiv.org/abs/2507.14921)
*Xiufeng Huang,Ka Chun Cheung,Runmin Cong,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: 提出了一种解耦框架\method，用于高效的3D高斯预测，通过立体视觉和全局注意力实现无姿态重建，减少资源需求。


<details>
  <summary>Details</summary>
Motivation: 当前的3D高斯飞溅重建方法需要大量计算资源和大型数据集，且通常将3D高斯几何和外观的预测纠缠在一起，依赖数据驱动的先验，导致回归速度慢。

Method: 该方法从局部图像对中提取特征，使用立体视觉骨干网络，并通过全局注意力块融合这些特征。专用的点和高斯预测头生成几何的多视点图和外观的高斯特征，结合为GS图以表示3DGS对象。一个细化网络增强这些GS图以实现高质量重建。

Result: 该方法实现了无姿态的3D重建，提高了鲁棒性和实用性。

Conclusion: \method 提供了一种高效、可扩展的解决方案，用于现实世界的3D内容生成，通过减少资源需求同时保持高质量输出。

Abstract: Generalizable 3D Gaussian Splatting reconstruction showcases advanced
Image-to-3D content creation but requires substantial computational resources
and large datasets, posing challenges to training models from scratch. Current
methods usually entangle the prediction of 3D Gaussian geometry and appearance,
which rely heavily on data-driven priors and result in slow regression speeds.
To address this, we propose \method, a disentangled framework for efficient 3D
Gaussian prediction. Our method extracts features from local image pairs using
a stereo vision backbone and fuses them via global attention blocks. Dedicated
point and Gaussian prediction heads generate multi-view point-maps for geometry
and Gaussian features for appearance, combined as GS-maps to represent the 3DGS
object. A refinement network enhances these GS-maps for high-quality
reconstruction. Unlike existing methods that depend on camera parameters, our
approach achieves pose-free 3D reconstruction, improving robustness and
practicality. By reducing resource demands while maintaining high-quality
outputs, \method provides an efficient, scalable solution for real-world 3D
content generation.

</details>


### [69] [3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline](https://arxiv.org/abs/2507.14924)
*Kaishva Chintan Shah,Virajith Boddapati,Karthik S. Gurumoorthy,Sandip Kaledhonkar,Ajit Rajwade*

Main category: cs.CV

TL;DR: 提出一种鲁棒性姿态估计和迭代平移校正方法，显著提升低信噪比cryo-EM的三维重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决在低信噪比条件下，传统方法在姿态估计和三维重建中存在的误差累积和重建质量下降问题。

Method: 提出了一种基于多维尺度分析（MDS）和鲁棒性优化的姿态估计方法，结合迭代平移校正算法。

Result: 该方法在欧拉角准确性和重建保真度（通过傅里叶壳相关FSC测量）上均优于现有方法。

Conclusion: 该方法通过结合鲁棒性优化和迭代平移校正，显著提高了在低信噪比条件下的三维重建准确性。

Abstract: Accurate pose estimation and shift correction are key challenges in cryo-EM
due to the very low SNR, which directly impacts the fidelity of 3D
reconstructions. We present an approach for pose estimation in cryo-EM that
leverages multi-dimensional scaling (MDS) techniques in a robust manner to
estimate the 3D rotation matrix of each particle from pairs of dihedral angles.
We express the rotation matrix in the form of an axis of rotation and a unit
vector in the plane perpendicular to the axis. The technique leverages the
concept of common lines in 3D reconstruction from projections. However, common
line estimation is ridden with large errors due to the very low SNR of cryo-EM
projection images. To address this challenge, we introduce two complementary
components: (i) a robust joint optimization framework for pose estimation based
on an $\ell_1$-norm objective or a similar robust norm, which simultaneously
estimates rotation axes and in-plane vectors while exactly enforcing unit norm
and orthogonality constraints via projected coordinate descent; and (ii) an
iterative shift correction algorithm that estimates consistent in-plane
translations through a global least-squares formulation. While prior approaches
have leveraged such embeddings and common-line geometry for orientation
recovery, existing formulations typically rely on $\ell_2$-based objectives
that are sensitive to noise, and enforce geometric constraints only
approximately. These choices, combined with a sequential pipeline structure,
can lead to compounding errors and suboptimal reconstructions in low-SNR
regimes. Our pipeline consistently outperforms prior methods in both Euler
angle accuracy and reconstruction fidelity, as measured by the Fourier Shell
Correlation (FSC).

</details>


### [70] [Probabilistic smooth attention for deep multiple instance learning in medical imaging](https://arxiv.org/abs/2507.14932)
*Francisco M. Castro-Macías,Pablo Morales-Álvarez,Yunan Wu,Rafael Molina,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: 本文提出了一种概率框架，通过估计注意力值的分布来提升医学图像分类性能，并提供了可解释的疾病定位不确定性映射。


<details>
  <summary>Details</summary>
Motivation: 现有的深度MIL方法通常确定性地处理注意力值，可能忽略了单个实例贡献的不确定性。

Method: 提出了一种新颖的概率框架，估计注意力值的概率分布，同时考虑全局和局部交互。

Result: 在涉及11个基线方法和三个医学数据集的综合评估中，该方法在不同指标上均取得了最佳预测性能。

Conclusion: 本文提出的概率框架在多种评估指标中表现出色，不仅提升了预测性能，还通过注意力不确定性映射提供了疾病定位的可解释性。

Abstract: The Multiple Instance Learning (MIL) paradigm is attracting plenty of
attention in medical imaging classification, where labeled data is scarce. MIL
methods cast medical images as bags of instances (e.g. patches in whole slide
images, or slices in CT scans), and only bag labels are required for training.
Deep MIL approaches have obtained promising results by aggregating
instance-level representations via an attention mechanism to compute the
bag-level prediction. These methods typically capture both local interactions
among adjacent instances and global, long-range dependencies through various
mechanisms. However, they treat attention values deterministically, potentially
overlooking uncertainty in the contribution of individual instances. In this
work we propose a novel probabilistic framework that estimates a probability
distribution over the attention values, and accounts for both global and local
interactions. In a comprehensive evaluation involving {\color{review} eleven}
state-of-the-art baselines and three medical datasets, we show that our
approach achieves top predictive performance in different metrics. Moreover,
the probabilistic treatment of the attention provides uncertainty maps that are
interpretable in terms of illness localization.

</details>


### [71] [Open-set Cross Modal Generalization via Multimodal Unified Representation](https://arxiv.org/abs/2507.14935)
*Hai Huang,Yan Xia,Shulei Wang,Hanting Wang,Minghui Fang,Shengpeng Ji,Sashuai Zhou,Tao Jin,Zhou Zhao*

Main category: cs.CV

TL;DR: 本文提出开放集跨模态泛化任务OSCMG及MICU方法，通过FCMI和CUJP组件增强模型在未知类别和跨模态任务中的表现，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态统一表示研究缺乏对开放集环境的考虑，而OSCMG任务更贴近现实应用场景，需解决未知类别的跨模态泛化问题。

Method: 提出了MICU方法，包含两个关键组件：Fine-Coarse Masked multimodal InfoNCE（FCMI）和Cross modal Unified Jigsaw Puzzles（CUJP）。FCMI通过对比学习增强多模态对齐，CUJP通过自监督学习增强特征多样性。

Result: 在CMG和新提出的OSCMG任务上的大量实验验证了MICU方法的有效性。

Conclusion: 本文提出的MICU方法在开放集跨模态泛化（OSCMG）任务中表现出色，验证了其在处理未知类别和跨模态知识转移方面的有效性。

Abstract: This paper extends Cross Modal Generalization (CMG) to open-set environments
by proposing the more challenging Open-set Cross Modal Generalization (OSCMG)
task. This task evaluates multimodal unified representations in open-set
conditions, addressing the limitations of prior closed-set cross-modal
evaluations. OSCMG requires not only cross-modal knowledge transfer but also
robust generalization to unseen classes within new modalities, a scenario
frequently encountered in real-world applications. Existing multimodal unified
representation work lacks consideration for open-set environments. To tackle
this, we propose MICU, comprising two key components: Fine-Coarse Masked
multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI
enhances multimodal alignment by applying contrastive learning at both holistic
semantic and temporal levels, incorporating masking to enhance generalization.
CUJP enhances feature diversity and model uncertainty by integrating
modality-agnostic feature selection with self-supervised learning, thereby
strengthening the model's ability to handle unknown categories in open-set
tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the
effectiveness of our approach. The code is available at
https://github.com/haihuangcode/CMG.

</details>


### [72] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: Polymorph是一个针对嵌入式设备的实时多标签视频分类框架，通过动态组合轻量级适配器显著降低能耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备上实时多标签视频分类受限于计算和能源预算，视频流的结构特性（如标签稀疏性、时间连续性和标签共现）可被利用以提高推理效率。

Method: 引入Polymorph框架，利用轻量级Low Rank Adapters（LoRA）动态激活和组合，避免全模型切换和权重合并。

Result: 在TAO数据集上，Polymorph实现了40%的能耗降低和9个点的mAP提升。

Conclusion: Polymorph框架通过动态选择和组合轻量级适配器，显著降低了嵌入式设备上的能耗，同时提高了分类性能。

Abstract: Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [73] [Decision PCR: Decision version of the Point Cloud Registration task](https://arxiv.org/abs/2507.14965)
*Yaojie Zhang,Tianlun Huang,Weijun Wang,Wei Feng*

Main category: cs.CV

TL;DR: 本文提出深度学习分类器评估低重叠点云配准质量，显著提升现有方法性能，在3DLoMatch和ETH数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如最大内点计数）在极低内点比例下失效，因此需要重新审视配准结果评估问题，并提出数据驱动的方法来解决决策版本的点云配准任务。

Method: 通过构建基于3DMatch数据集的数据集，并训练一个深度学习分类器来评估配准质量，克服了传统指标的局限性。

Result: 结合GeoTransformer等方法，在3DLoMatch基准上实现了86.97%的最新配准召回率，并在未见的ETH数据集上表现出强泛化能力。

Conclusion: 本文提出了一种基于深度学习的分类器，用于评估低重叠点云配准质量，显著提升了现有最先进方法的性能，并在3DLoMatch基准和未见的ETH数据集上展示了强大的泛化能力。

Abstract: Low-overlap point cloud registration (PCR) remains a significant challenge in
3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become
ineffective under extremely low inlier ratios. In this paper, we revisit the
registration result evaluation problem and identify the Decision version of the
PCR task as the fundamental problem. To address this Decision PCR task, we
propose a data-driven approach. First, we construct a corresponding dataset
based on the 3DMatch dataset. Then, a deep learning-based classifier is trained
to reliably assess registration quality, overcoming the limitations of
traditional metrics. To our knowledge, this is the first comprehensive study to
address this task through a deep learning framework. We incorporate this
classifier into standard PCR pipelines. When integrated with our approach,
existing state-of-the-art PCR methods exhibit significantly enhanced
registration performance. For example, combining our framework with
GeoTransformer achieves a new SOTA registration recall of 86.97\% on the
challenging 3DLoMatch benchmark. Our method also demonstrates strong
generalization capabilities on the unseen outdoor ETH dataset.

</details>


### [74] [Hierarchical Cross-modal Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.14976)
*Hao Zheng,Shunzhi Yang,Zhuoxin He,Jinfeng Yang,Zhenhua Huang*

Main category: cs.CV

TL;DR: HiCroPL通过双向知识流动和分层知识映射，解决了预训练视觉语言模型在下游任务中的泛化瓶颈，取得了多项最优性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型（如CLIP）在下游任务适配时面临模态隔离和分层语义衰减的问题，限制了其泛化能力。

Method: 提出了HiCroPL框架，通过分层跨模态提示学习，建立文本和视觉模态之间的双向知识流动，利用分层知识映射器和轻量级层特定知识代理实现高效跨模态交互。

Result: 在四个任务的11个基准测试中取得了最优性能，显著提升了模型的泛化能力。

Conclusion: HiCroPL通过双向知识流动和分层知识映射，显著提升了预训练视觉语言模型在下游任务中的泛化能力，并在多个基准测试中取得了最优性能。

Abstract: Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent
generalization abilities. However, adapting these large-scale models to
downstream tasks while preserving their generalization capabilities remains
challenging. Although prompt learning methods have shown promise, they suffer
from two fundamental bottlenecks that limit generalization: (a) modality
isolation, and (b) hierarchical semantic decay. To address these limitations,
we propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that
establishes bidirectional knowledge flow between text and vision modalities,
enabling them to refine their semantics mutually. HiCroPL routes knowledge
flows by leveraging the complementary strengths of text and vision. In early
layers, text prompts inject relatively clear semantics into visual prompts
through a hierarchical knowledge mapper, enhancing the representation of
low-level visual semantics. In later layers, visual prompts encoding specific
task-relevant objects flow back to refine text prompts, enabling deeper
alignment. Crucially, our hierarchical knowledge mapper allows representations
at multi-scales to be fused, ensuring that deeper representations retain
transferable shallow semantics thereby enhancing generalization. We further
introduce a lightweight layer-specific knowledge proxy to enable efficient
cross-modal interactions. Extensive evaluations across four tasks demonstrate
HiCroPL's superior performance, achieving state-of-the-art results on 11
benchmarks with significant improvements. Code is available at:
https://github.com/zzeoZheng/HiCroPL.

</details>


### [75] [Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression](https://arxiv.org/abs/2507.14997)
*Roy H. Jennings,Genady Paikin,Roy Shaul,Evgeny Soloveichik*

Main category: cs.CV

TL;DR: 本文提出RvTC方法，通过灵活的基于分箱的分类取代预设词汇分类，显著提升了多模态大语言模型在图像回归任务中的性能，并展示了数据特定提示的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型（MLLMs）在基于图像的回归任务中表现有限，现有方法依赖预设输出词汇和通用任务级提示，未能有效利用文本输入的语义理解。本文旨在解决这些限制，提升模型性能。

Method: 提出了Regression via Transformer-Based Classification (RvTC)，通过灵活的基于分箱的方法取代预设词汇分类，消除了手动词汇制作的复杂性，并通过简单的分箱增加实现了性能提升。

Result: RvTC方法在四个图像评估数据集上实现了最先进的性能。数据特定提示（如包含图像语义信息的提示）显著提升了模型性能，例如在AVA数据集上，相关性从0.83提升至0.90。

Conclusion: 本文强调了在多媒体回归任务中融入有意义文本上下文的重要性，展示了数据特定提示如何显著提升性能，并提出了RvTC方法，通过灵活的基于分箱的方法取代预设词汇分类，实现了在四个图像评估数据集上的最先进性能。

Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based
regression tasks, but current approaches face key limitations. Recent methods
fine-tune MLLMs using preset output vocabularies and generic task-level prompts
(e.g., "How would you rate this image?"), assuming this mimics human rating
behavior. Our analysis reveals these approaches provide no benefit over
image-only training. Models using preset vocabularies and generic prompts
perform equivalently to image-only models, failing to leverage semantic
understanding from textual input. We propose Regression via Transformer-Based
Classification (RvTC), which replaces vocabulary-constrained classification
with a flexible bin-based approach. Unlike approaches that address
discretization errors through complex distributional modeling, RvTC eliminates
manual vocabulary crafting through straightforward bin increase, achieving
state-of-the-art performance on four image assessment datasets using only
images. More importantly, we demonstrate that data-specific prompts
dramatically improve performance. Unlike generic task descriptions, prompts
containing semantic information about specific images enable MLLMs to leverage
cross-modal understanding. On the AVA dataset, adding challenge titles to
prompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We
demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that
MLLMs benefit from semantic prompt information surpassing mere statistical
biases. This underscores the importance of incorporating meaningful textual
context in multimodal regression tasks.

</details>


### [76] [Axis-Aligned Document Dewarping](https://arxiv.org/abs/2507.15000)
*Chaoyun Wang,I-Chao Shen,Takeo Igarashi,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: 提出了一种利用轴对齐几何约束和预处理策略的文档去扭曲方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法主要依赖带标注数据的监督回归，未能充分利用物理文档中固有的几何特性。

Method: 在训练阶段提出了一种轴对齐的几何约束来增强文档去扭曲效果；在推理阶段提出了一种轴对齐预处理策略以降低去扭曲难度；在评估阶段引入了一种新指标AAD。

Result: 在多个现有基准测试中取得了最先进的结果，并在AAD指标上实现了18.2%~34.5%的改进。

Conclusion: 该方法在多个现有基准测试中取得了最先进的结果，并在AAD指标上实现了18.2%~34.5%的改进。

Abstract: Document dewarping is crucial for many applications. However, existing
learning-based methods primarily rely on supervised regression with annotated
data without leveraging the inherent geometric properties in physical documents
to the dewarping process. Our key insight is that a well-dewarped document is
characterized by transforming distorted feature lines into axis-aligned ones.
This property aligns with the inherent axis-aligned nature of the discrete grid
geometry in planar documents. In the training phase, we propose an axis-aligned
geometric constraint to enhance document dewarping. In the inference phase, we
propose an axis alignment preprocessing strategy to reduce the dewarping
difficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned
Distortion (AAD), that not only incorporates geometric meaning and aligns with
human visual perception but also demonstrates greater robustness. As a result,
our method achieves SOTA results on multiple existing benchmarks and achieves
18.2%~34.5% improvements on the AAD metric.

</details>


### [77] [FastSmoothSAM: A Fast Smooth Method For Segment Anything Model](https://arxiv.org/abs/2507.15008)
*Jiasheng Xu,Yewang Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于B样条曲线拟合的细化方法，有效解决了FastSAM边缘锯齿问题，提升了分割质量，同时保持实时性。


<details>
  <summary>Details</summary>
Motivation: FastSAM虽然实现了实时分割，但其生成的边缘存在锯齿状偏差，影响分割的视觉质量和分析准确性。

Method: 采用四阶段细化流程，包括两轮B样条曲线拟合，以平滑FastSAM生成的锯齿状边缘。

Result: 该方法在不牺牲关键几何信息的情况下，显著提升了边缘的视觉质量和分析准确性。

Conclusion: 本文提出的基于B样条曲线拟合的细化方法显著提升了FastSAM的边缘质量，增强了其在实际应用中的实用性，同时保持了实时处理能力。

Abstract: Accurately identifying and representing object edges is a challenging task in
computer vision and image processing. The Segment Anything Model (SAM) has
significantly influenced the field of image segmentation, but suffers from high
memory consumption and long inference times, limiting its efficiency in
real-time applications. To address these limitations, Fast Segment Anything
(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM
often generates jagged edges that deviate from the true object shapes.
Therefore, this paper introduces a novel refinement approach using B-Spline
curve fitting techniques to enhance the edge quality in FastSAM. Leveraging the
robust shape control and flexible geometric construction of B-Splines, a
four-stage refining process involving two rounds of curve fitting is employed
to effectively smooth jagged edges. This approach significantly improves the
visual quality and analytical accuracy of object edges without compromising
critical geometric information. The proposed method improves the practical
utility of FastSAM by improving segmentation accuracy while maintaining
real-time processing capabilities. This advancement unlocks greater potential
for FastSAM technology in various real-world scenarios, such as industrial
automation, medical imaging, and autonomous systems, where precise and
efficient edge recognition is crucial.

</details>


### [78] [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/abs/2507.15028)
*Yuanhan Zhang,Yunice Chew,Yuhao Dong,Aria Leo,Bo Hu,Ziwei Liu*

Main category: cs.CV

TL;DR: Video-TT测试揭示了视频大型语言模型在复杂视频理解上与人类的差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能充分反映视频大型语言模型在保持视频解释的正确性和鲁棒性方面与人类智能的差距。

Method: 提出了Video-TT，包含1,000个YouTube Shorts视频，每个视频配有一个开放式问题和四个对抗性问题，以评估模型对视觉和叙事复杂性的处理能力。

Result: 评估显示，视频大型语言模型在Video-TT上的表现显著低于人类。

Conclusion: 视频思考测试（Video-TT）揭示了视频大型语言模型在理解和解释复杂视觉叙事方面与人类表现之间的显著差距。

Abstract: Human intelligence requires correctness and robustness, with the former being
foundational for the latter. In video understanding, correctness ensures the
accurate interpretation of visual content, and robustness maintains consistent
performance in challenging conditions. Despite advances in video large language
models (video LLMs), existing benchmarks inadequately reflect the gap between
these models and human intelligence in maintaining correctness and robustness
in video interpretation. We introduce the Video Thinking Test (Video-TT), to
assess if video LLMs can interpret real-world videos as effectively as humans.
Video-TT reflects genuine gaps in understanding complex visual narratives, and
evaluates robustness against natural adversarial questions. Video-TT comprises
1,000 YouTube Shorts videos, each with one open-ended question and four
adversarial questions that probe visual and narrative complexity. Our
evaluation shows a significant gap between video LLMs and human performance.

</details>


### [79] [OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography](https://arxiv.org/abs/2507.15035)
*Zhijun Zeng,Youjia Zheng,Hao Hu,Zeyuan Dong,Yihang Zheng,Xinliang Liu,Jinzhuo Wang,Zuoqiang Shi,Linfeng Zhang,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: OpenBreastUS是一个大规模波动方程数据集，通过8000个真实乳腺模型和1600万波模拟，支持神经算子在医学成像中的高效应用，首次实现活体乳腺成像。


<details>
  <summary>Details</summary>
Motivation: 传统波动方程数值求解器计算量大且不稳定，神经算子在现实成像中因数据集过于简化而受限，OpenBreastUS旨在填补理论与实际应用之间的差距。

Method: 提出了OpenBreastUS数据集，包含8000个解剖学真实的人体乳腺模型和1600万频域波模拟，用于全面评估神经算子在正向模拟和逆向成像任务中的性能。

Result: OpenBreastUS为神经PDE求解器的开发和实际医学成像应用提供了真实且广泛的平台，首次展示了神经算子在人体乳腺活体成像中的高效性。

Conclusion: OpenBreastUS数据集首次实现了神经算子求解器在人体乳腺活体成像中的高效应用，为现实医学成像问题提供了创新解决方案。

Abstract: Accurate and efficient simulation of wave equations is crucial in
computational wave imaging applications, such as ultrasound computed tomography
(USCT), which reconstructs tissue material properties from observed scattered
waves. Traditional numerical solvers for wave equations are computationally
intensive and often unstable, limiting their practical applications for
quasi-real-time image reconstruction. Neural operators offer an innovative
approach by accelerating PDE solving using neural networks; however, their
effectiveness in realistic imaging is limited because existing datasets
oversimplify real-world complexity. In this paper, we present OpenBreastUS, a
large-scale wave equation dataset designed to bridge the gap between
theoretical equations and practical imaging applications. OpenBreastUS includes
8,000 anatomically realistic human breast phantoms and over 16 million
frequency-domain wave simulations using real USCT configurations. It enables a
comprehensive benchmarking of popular neural operators for both forward
simulation and inverse imaging tasks, allowing analysis of their performance,
scalability, and generalization capabilities. By offering a realistic and
extensive dataset, OpenBreastUS not only serves as a platform for developing
innovative neural PDE solvers but also facilitates their deployment in
real-world medical imaging problems. For the first time, we demonstrate
efficient in vivo imaging of the human breast using neural operator solvers.

</details>


### [80] [OmniVTON: Training-Free Universal Virtual Try-On](https://arxiv.org/abs/2507.15037)
*Zhaotong Yang,Yuhui Li,Shengfeng He,Xinzhe Li,Yangyang Xu,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: OmniVTON 是首个无需训练的通用虚拟试穿框架，通过解耦服装和姿态条件，实现高保真纹理和跨场景一致性，支持多人物试穿。


<details>
  <summary>Details</summary>
Motivation: 解决现有虚拟试穿技术在跨域泛化和数据偏差方面的局限性，提出一种无需训练的统一解决方案。

Method: OmniVTON 采用服装先验生成机制和对齐技术保留服装细节，结合 DDIM 反演实现姿态对齐，从而解耦服装和姿态约束。

Result: OmniVTON 在多样化场景中表现优异，首次实现了多人物虚拟试穿功能。

Conclusion: OmniVTON 是一个无需训练的统一虚拟试穿框架，通过解耦服装和姿态条件，实现了跨场景的高保真纹理和姿态一致性，并在多样化的数据集、服装类型和应用场景中表现出色。

Abstract: Image-based Virtual Try-On (VTON) techniques rely on either supervised
in-shop approaches, which ensure high fidelity but struggle with cross-domain
generalization, or unsupervised in-the-wild methods, which improve adaptability
but remain constrained by data biases and limited universality. A unified,
training-free solution that works across both scenarios remains an open
challenge. We propose OmniVTON, the first training-free universal VTON
framework that decouples garment and pose conditioning to achieve both texture
fidelity and pose consistency across diverse settings. To preserve garment
details, we introduce a garment prior generation mechanism that aligns clothing
with the body, followed by continuous boundary stitching technique to achieve
fine-grained texture retention. For precise pose alignment, we utilize DDIM
inversion to capture structural cues while suppressing texture interference,
ensuring accurate body alignment independent of the original image textures. By
disentangling garment and pose constraints, OmniVTON eliminates the bias
inherent in diffusion models when handling multiple conditions simultaneously.
Experimental results demonstrate that OmniVTON achieves superior performance
across diverse datasets, garment types, and application scenarios. Notably, it
is the first framework capable of multi-human VTON, enabling realistic garment
transfer across multiple individuals in a single scene. Code is available at
https://github.com/Jerome-Young/OmniVTON

</details>


### [81] [Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling](https://arxiv.org/abs/2507.15059)
*Ran Zhang,Xuanhua He,Li Xueheng,Ke Cao,Liu Liu,Wenbo Xu,Fang Jiabin,Yang Qize,Jie Zhang*

Main category: cs.CV

TL;DR: 提出轻量级PanTiny框架，通过多数据集联合训练和复合损失函数，高效提升pan-sharpening性能，超越复杂模型。


<details>
  <summary>Details</summary>
Motivation: 当前pan-sharpening模型趋向大而复杂，导致高计算开销且在全分辨率数据上泛化能力差，本文旨在解决这一问题。

Method: 提出PanTiny，一个轻量级单步pan-sharpening框架，采用多数据集联合训练（WV2、WV3、GF2）及复合损失函数。

Result: PanTiny在性能与效率上取得平衡，超越多数大型专用模型，并通过消融实验验证了设计原则的有效性。

Conclusion: 本文通过提出PanTiny框架，挑战了当前pan-sharpening领域大而复杂模型的趋势，展示了轻量级设计、多数据集联合训练及复合损失函数在提升性能与泛化能力上的有效性，呼吁社区向高效、通用且数据敏感的模型设计转变。

Abstract: The field of pan-sharpening has recently seen a trend towards increasingly
large and complex models, often trained on single, specific satellite datasets.
This approach, however, leads to high computational overhead and poor
generalization on full resolution data, a paradigm we challenge in this paper.
In response to this issue, we propose PanTiny, a lightweight, single-step
pan-sharpening framework designed for both efficiency and robust performance.
More critically, we introduce multiple-in-one training paradigm, where a
single, compact model is trained simultaneously on three distinct satellite
datasets (WV2, WV3, and GF2) with different resolution and spectral
information. Our experiments show that this unified training strategy not only
simplifies deployment but also significantly boosts generalization on
full-resolution data. Further, we introduce a universally powerful composite
loss function that elevates the performance of almost all of models for
pan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny
model, benefiting from these innovations, achieves a superior
performance-to-efficiency balance, outperforming most larger, specialized
models. Through extensive ablation studies, we validate that principled
engineering in model design, training paradigms, and loss functions can surpass
brute-force scaling. Our work advocates for a community-wide shift towards
creating efficient, generalizable, and data-conscious models for
pan-sharpening. The code is available at
https://github.com/Zirconium233/PanTiny .

</details>


### [82] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAnimator++ 是一种新型视频扩散框架，通过可学习的姿态对齐和身份保持技术，显著提升了人类图像动画中的身份一致性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在参考图像和驱动视频差异较大时身份一致性不足的问题。

Method: StableAnimator++ 结合了可学习的相似变换矩阵预测、全局内容感知的人脸编码器、分布感知的身份适配器，以及基于 Hamilton-Jacobi-Bellman 的面部优化方法。

Result: 实验证明，StableAnimator++ 在定性和定量上均表现出色，有效提升了身份一致性和生成视频的质量。

Conclusion: StableAnimator++ 是一种创新的视频扩散框架，通过可学习的姿态对齐和身份保持模块，显著提升了人类图像动画中的身份一致性，无需后处理即可生成高质量视频。

Abstract: Current diffusion models for human image animation often struggle to maintain
identity (ID) consistency, especially when the reference image and driving
video differ significantly in body size or position. We introduce
StableAnimator++, the first ID-preserving video diffusion framework with
learnable pose alignment, capable of generating high-quality videos conditioned
on a reference image and a pose sequence without any post-processing. Building
upon a video diffusion model, StableAnimator++ contains carefully designed
modules for both training and inference, striving for identity consistency. In
particular, StableAnimator++ first uses learnable layers to predict the
similarity transformation matrices between the reference image and the driven
poses via injecting guidance from Singular Value Decomposition (SVD). These
matrices align the driven poses with the reference image, mitigating
misalignment to a great extent. StableAnimator++ then computes image and face
embeddings using off-the-shelf encoders, refining the face embeddings via a
global content-aware Face Encoder. To further maintain ID, we introduce a
distribution-aware ID Adapter that counteracts interference caused by temporal
layers while preserving ID via distribution alignment. During the inference
stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization
integrated into the denoising process, guiding the diffusion trajectory for
enhanced facial fidelity. Experiments on benchmarks show the effectiveness of
StableAnimator++ both qualitatively and quantitatively.

</details>


### [83] [Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR](https://arxiv.org/abs/2507.15085)
*Peirong Zhang,Haowei Xu,Jiaxin Zhang,Guitao Xu,Xuhan Zheng,Zhenhua Yang,Junle Liu,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: 本文评估了当前生成模型在文本图像生成和编辑上的能力，发现其局限性，并呼吁将此类能力融入通用模型。


<details>
  <summary>Details</summary>
Motivation: 由于文本图像在现代电子社会中的重要性及其复杂性，研究当前生成模型在文本图像生成和编辑方面的能力。

Method: 评估了6种模型（包括闭源和开源），通过33个代表性任务（分为五类：文档、手写文本、场景文本、艺术文本及复杂布局文本），使用定制的高质量图像输入和提示进行全面分析。

Result: 揭示了当前生成模型在OCR任务中的局限性，并提出了改进方向。

Conclusion: 本文认为，逼真的文本图像生成和编辑应作为基础技能融入通用生成模型，而非依赖专门解决方案。

Abstract: Text image is a unique and crucial information medium that integrates visual
aesthetics and linguistic semantics in modern e-society. Due to their subtlety
and complexity, the generation of text images represents a challenging and
evolving frontier in the image generation field. The recent surge of
specialized image generators (\emph{e.g.}, Flux-series) and unified generative
models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a
natural question: can they master the intricacies of text image generation and
editing? Motivated by this, we assess current state-of-the-art generative
models' capabilities in terms of text image generation and editing. We
incorporate various typical optical character recognition (OCR) tasks into our
evaluation and broaden the concept of text-based generation tasks into OCR
generative tasks. We select 33 representative tasks and categorize them into
five categories: document, handwritten text, scene text, artistic text, and
complex \& layout-rich text. For comprehensive evaluation, we examine six
models across both closed-source and open-source domains, using tailored,
high-quality image inputs and prompts. Through this evaluation, we draw crucial
observations and identify the weaknesses of current generative models for OCR
tasks. We argue that photorealistic text image generation and editing should be
internalized as foundational skills into general-domain generative models,
rather than being delegated to specialized solutions, and we hope this
empirical analysis can provide valuable insights for the community to achieve
this goal. This evaluation is online and will be continuously updated at our
GitHub repository.

</details>


### [84] [BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking](https://arxiv.org/abs/2507.15094)
*Mengya Xu,Rulin Zhou,An Wang,Chaoyang Lyu,Zhen Li,Ning Zhong,Hongliang Ren*

Main category: cs.CV

TL;DR: 该论文提出了首个 ESD 出血源数据集 BleedOrigin-Bench 及双阶段检测-跟踪框架 BleedOrigin-Net，显著提升了出血源的定位精度和手术效率。


<details>
  <summary>Details</summary>
Motivation: 当前 AI 方法主要关注出血区域分割，而忽略了 ESD 环境下出血源的精确定位和动态跟踪需求，且缺乏专门数据集。

Method: 提出了 BleedOrigin-Net，一种双阶段检测-跟踪框架，结合了出血起始检测和连续空间跟踪。

Result: BleedOrigin-Net 在出血起始检测、初始源定位和点跟踪任务中分别达到 96.85%、70.24% 和 96.11% 的准确率。

Conclusion: BleedOrigin-Net 在 ESD 手术中实现了高精度的出血源定位与跟踪，显著提升了手术效率和安全性。

Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses
significant risks, demanding precise, real-time localization and continuous
monitoring of the bleeding source for effective hemostatic intervention. In
particular, endoscopists have to repeatedly flush to clear blood, allowing only
milliseconds to identify bleeding sources, an inefficient process that prolongs
operations and elevates patient risks. However, current Artificial Intelligence
(AI) methods primarily focus on bleeding region segmentation, overlooking the
critical need for accurate bleeding source detection and temporal tracking in
the challenging ESD environment, which is marked by frequent visual
obstructions and dynamic scene changes. This gap is widened by the lack of
specialized datasets, hindering the development of robust AI-assisted guidance
systems. To address these challenges, we introduce BleedOrigin-Bench, the first
comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated
bleeding sources across 106,222 frames from 44 procedures, supplemented with
39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6
challenging clinical scenarios. We also present BleedOrigin-Net, a novel
dual-stage detection-tracking framework for the bleeding source localization in
ESD procedures, addressing the complete workflow from bleeding onset detection
to continuous spatial tracking. We compare with widely-used object detection
models (YOLOv11/v12), multimodal large language models, and point tracking
methods. Extensive evaluation demonstrates state-of-the-art performance,
achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset
detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source
detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.

</details>


### [85] [LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM](https://arxiv.org/abs/2507.15109)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Sorin Grigorescu*

Main category: cs.CV

TL;DR: LoopNet结合多任务ResNet和在线学习，提升SLAM闭环检测精度与实时性，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决SLAM系统中闭环检测精度不足和嵌入式硬件实时计算限制的两大核心问题。

Method: 采用多任务ResNet架构，结合在线再训练和少样本学习策略，优化嵌入式设备性能，并利用DISK描述符提升特征提取能力。

Result: LoopNet在动态视觉数据集上表现出色，超越了传统手工特征和深度学习方法，并在不同条件下提供更优性能。

Conclusion: LoopNet通过结合多任务ResNet架构和在线再训练，显著提升了SLAM系统中的闭环检测精度和实时计算性能，同时通过DISK描述符克服了传统方法的局限性。

Abstract: One of the main challenges in the Simultaneous Localization and Mapping
(SLAM) loop closure problem is the recognition of previously visited places. In
this work, we tackle the two main problems of real-time SLAM systems: 1) loop
closure detection accuracy and 2) real-time computation constraints on the
embedded hardware. Our LoopNet method is based on a multitasking variant of the
classical ResNet architecture, adapted for online retraining on a dynamic
visual dataset and optimized for embedded devices. The online retraining is
designed using a few-shot learning approach. The architecture provides both an
index into the queried visual dataset, and a measurement of the prediction
quality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,
LoopNet surpasses the limitations of handcrafted features and traditional deep
learning methods, offering better performance under varying conditions. Code is
available at https://github.com/RovisLab/LoopNet. Additinally, we introduce a
new loop closure benchmarking dataset, coined LoopDB, which is available at
https://github.com/RovisLab/LoopDB.

</details>


### [86] [Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction](https://arxiv.org/abs/2507.15130)
*Ce Zhang,Yale Song,Ruta Desai,Michael Louis Iuzzolino,Joseph Tighe,Gedas Bertasius,Satwik Kottur*

Main category: cs.CV

TL;DR: VideoPlan通过辅助任务增强和多令牌预测，显著提升了长时程视觉规划任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决长时程视觉规划中的两个关键挑战：标注数据稀缺和传统单令牌预测在结构化动作空间建模上的不足。

Method: 通过辅助任务增强（Auxiliary Task Augmentation）和多令牌预测（Multi-token Prediction）来解决数据稀缺和结构化动作空间建模问题。

Result: 在COIN和CrossTask数据集上分别超越之前方法7.3%和3.4%，在Ego4D任务中表现与最优方法相当。

Conclusion: VideoPlan方法在VPA任务中表现优异，在COIN和CrossTask数据集上分别超越之前方法7.3%和3.4%，且在Ego4D任务中表现与现有最优方法相当。

Abstract: Visual Planning for Assistance (VPA) aims to predict a sequence of user
actions required to achieve a specified goal based on a video showing the
user's progress. Although recent advances in multimodal large language models
(MLLMs) have shown promising results in video understanding, long-horizon
visual planning remains a challenging problem. We identify two challenges in
training large MLLMs for video-based planning tasks: (1) scarcity of procedural
annotations, limiting the model's ability to learn procedural task dynamics
effectively, and (2) inefficiency of next-token prediction objective to
explicitly capture the structured action space for visual planning when
compared to free-form, natural language. To tackle data scarcity, we introduce
Auxiliary Task Augmentation. We design and train our model on auxiliary tasks
relevant to long-horizon video-based planning (e.g., goal prediction) to
augment the model's planning ability. To more explicitly model the structured
action space unique to visual planning tasks, we leverage Multi-token
Prediction, extending traditional next-token prediction by using multiple heads
to predict multiple future tokens during training. Our approach, VideoPlan,
achieves state-of-the-art VPA performance on the COIN and CrossTask datasets,
surpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3
future actions. We further extend our method to the challenging Ego4D Long-term
Action Anticipation task, and show that it is on par with the state-of-the-art
approaches despite not using specialized egocentric features. Code will be made
available.

</details>


### [87] [Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection](https://arxiv.org/abs/2507.15150)
*Aayush Atul Verma,Arpitsinh Vaghela,Bharatesh Chakravarthi,Kaustav Chanda,Yezhou Yang*

Main category: cs.CV

TL;DR: 本文提出一种时空多重图表示方法，通过解耦空间和时间图优化异步视觉任务，实现了更高的检测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 事件传感器生成的数据稀疏且异步，传统方法将其转换为密集张量会削弱其固有优势，而现有图方法在时空动态建模上表现不佳。

Method: 提出了一种新颖的时空多重图表示方法，通过解耦空间图和时间图来优化时空动态建模。空间图利用B样条基函数建模全局结构，时间图则基于运动向量的注意力机制捕捉局部动态变化。

Result: 在Gen1汽车和eTraM数据集上，检测精度比基于图的先前方法提升了6%，计算速度提高了5倍，参数数量减少且未增加计算成本。

Conclusion: 结构化图建模在异步视觉任务中表现出高效性，显著提升了检测精度和计算效率。

Abstract: Event-based sensors offer high temporal resolution and low latency by
generating sparse, asynchronous data. However, converting this irregular data
into dense tensors for use in standard neural networks diminishes these
inherent advantages, motivating research into graph representations. While such
methods preserve sparsity and support asynchronous inference, their performance
on downstream tasks remains limited due to suboptimal modeling of
spatiotemporal dynamics. In this work, we propose a novel spatiotemporal
multigraph representation to better capture spatial structure and temporal
changes. Our approach constructs two decoupled graphs: a spatial graph
leveraging B-spline basis functions to model global structure, and a temporal
graph utilizing motion vector-based attention for local dynamic changes. This
design enables the use of efficient 2D kernels in place of computationally
expensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM
datasets for event-based object detection, achieving over a 6% improvement in
detection accuracy compared to previous graph-based works, with a 5x speedup,
reduced parameter count, and no increase in computational cost. These results
highlight the effectiveness of structured graph modeling for asynchronous
vision. Project page: eventbasedvision.github.io/eGSMV.

</details>


### [88] [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/abs/2507.15212)
*Yusuke Yoshiyasu,Leyuan Sun,Ryusuke Sagawa*

Main category: cs.CV

TL;DR: MeshMamba利用Mamba-SSMs高效处理大规模3D网格，生成/重建带衣物和手部细节的人体模型。MambaDiff3D和Mamba-HMR在生成和单图像重建任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理大规模3D网格数据（如超过10,000个顶点的人体网格）时的效率和可扩展性问题，并捕捉衣物和手部几何细节。

Method: 采用Mamba State Space Models（Mamba-SSMs）技术，通过序列化网格顶点排序（基于身体部位注释或模板网格的3D顶点位置）处理大规模输入令牌。设计了MambaDiff3D（去噪扩散模型）和Mamba-HMR（单图像重建模型）。

Result: MambaDiff3D能生成带有衣物和手部细节的密集3D人体网格，在生成任务中优于先前方法；Mamba-HMR扩展到全身设置（包括面部和手部），在（近）实时性能上表现竞争性。

Conclusion: MeshMamba通过Mamba-SSMs技术高效处理大规模3D网格数据，成功应用于生成和重建人体网格模型，包括衣物和手部细节。MambaDiff3D和Mamba-HMR在3D人体形状生成和单图像重建任务中表现优异，扩展了非参数化方法的适用范围。

Abstract: In this paper, we introduce MeshMamba, a neural network model for learning 3D
articulated mesh models by employing the recently proposed Mamba State Space
Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large
number of input tokens, enabling the generation and reconstruction of body mesh
models with more than 10,000 vertices, capturing clothing and hand geometries.
The key to effectively learning MeshMamba is the serialization technique of
mesh vertices into orderings that are easily processed by Mamba. This is
achieved by sorting the vertices based on body part annotations or the 3D
vertex locations of a template mesh, such that the ordering respects the
structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,
a denoising diffusion model for generating 3D articulated meshes and 2)
Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape
and pose from a single image. Experimental results showed that MambaDiff3D can
generate dense 3D human meshes in clothes, with grasping hands, etc., and
outperforms previous approaches in the 3D human shape generation task.
Additionally, Mamba-HMR extends the capabilities of previous non-parametric
human mesh recovery approaches, which were limited to handling body-only poses
using around 500 vertex tokens, to the whole-body setting with face and hands,
while achieving competitive performance in (near) real-time.

</details>


### [89] [Improving Joint Embedding Predictive Architecture with Diffusion Noise](https://arxiv.org/abs/2507.15216)
*Yuping Qiu,Rui Zhu,Ying-cong Chen*

Main category: cs.CV

TL;DR: 论文提出N-JEPA模型，通过结合扩散噪声与自监督学习，增强模型表征能力，并在下游任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在特征学习方面表现优异，但在图像生成和细节增强上不及生成模型。因此，探索SSL与生成模型的联系以增强SSL的表征能力。

Method: 提出N-JEPA模型，将扩散噪声通过掩码标记的位置嵌入整合到掩码图像建模（MIM）中，并采用多级噪声调度作为特征增强手段。

Result: N-JEPA模型在下游分类任务中表现出色，验证了其有效性。

Conclusion: 结合扩散噪声与自监督学习（SSL）的N-JEPA模型，通过多级噪声调度增强了模型的鲁棒性，并在下游分类任务中验证了其有效性。

Abstract: Self-supervised learning has become an incredibly successful method for
feature learning, widely applied to many downstream tasks. It has proven
especially effective for discriminative tasks, surpassing the trending
generative models. However, generative models perform better in image
generation and detail enhancement. Thus, it is natural for us to find a
connection between SSL and generative models to further enhance the
representation capacity of SSL. As generative models can create new samples by
approximating the data distribution, such modeling should also lead to a
semantic understanding of the raw visual data, which is necessary for
recognition tasks. This enlightens us to combine the core principle of the
diffusion model: diffusion noise, with SSL to learn a competitive recognition
model. Specifically, diffusion noise can be viewed as a particular state of
mask that reveals a close relationship between masked image modeling (MIM) and
diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to
incorporate diffusion noise into MIM by the position embedding of masked
tokens. The multi-level noise schedule is a series of feature augmentations to
further enhance the robustness of our model. We perform a comprehensive study
to confirm its effectiveness in the classification of downstream tasks. Codes
will be released soon in public.

</details>


### [90] [Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel](https://arxiv.org/abs/2507.15223)
*Siqi Chen,Guoqing Zhang,Jiahao Lai,Bingzhi Shen,Sihong Zhang,Caixia Dong,Xuejin Chen,Yang Li*

Main category: cs.CV

TL;DR: 提出分层基于部分的3D血管生成框架，通过关键图建模全局拓扑与局部几何细节分离，验证了其在复杂血管建模中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 由于血管的复杂分支模式、曲率和不规则形状，准确表示其几何和拓扑结构仍具挑战性。

Method: 提出了一种分层基于部分的框架，分为三个阶段：关键图生成、基于几何属性的血管段生成以及通过全局关键图整合局部段的分层血管组装。

Result: 在真实数据集上验证了框架的优越性能，证明了其在复杂血管网络建模中的高效性。

Conclusion: 本研究首次成功将基于部分的生成方法应用于3D血管建模，为血管数据生成设立了新基准。

Abstract: Advancements in 3D vision have increased the impact of blood vessel modeling
on medical applications. However, accurately representing the complex geometry
and topology of blood vessels remains a challenge due to their intricate
branching patterns, curvatures, and irregular shapes. In this study, we propose
a hierarchical part-based frame work for 3D vessel generation that separates
the global binary tree-like topology from local geometric details. Our approach
proceeds in three stages: (1) key graph generation to model the overall
hierarchical struc ture, (2) vessel segment generation conditioned on geometric
properties, and (3) hierarchical vessel assembly by integrating the local
segments according to the global key graph. We validate our framework on real
world datasets, demonstrating superior performance over existing methods in
modeling complex vascular networks. This work marks the first successful
application of a part-based generative approach for 3D vessel modeling, setting
a new benchmark for vascular data generation. The code is available at:
https://github.com/CybercatChen/PartVessel.git.

</details>


### [91] [Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders](https://arxiv.org/abs/2507.15227)
*Krishna Kanth Nakka*

Main category: cs.CV

TL;DR: 通过稀疏自编码器（SAE）分析乳腺影像基础模型Mammo-CLIP，发现潜在神经元与临床概念相关，揭示了模型决策的混淆因素，并展示了SAE在提升模型可解释性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域如医学影像中，模型决策的可解释性对临床采用至关重要。本研究旨在通过SAE增强乳腺影像基础模型的可解释性。

Method: 通过训练基于Mammo-CLIP的Mammo-SAE，识别与临床相关乳腺概念（如肿块和可疑钙化）相关的潜在特征，并分析其与真实区域的关联。

Result: 研究发现，SAE潜在空间中激活度最高的类别级潜在神经元往往与真实区域对齐，并揭示了影响模型决策过程的若干混淆因素。同时，分析了模型在下游微调中依赖的潜在神经元以改进乳腺概念预测。

Conclusion: 本研究强调了可解释的稀疏自编码器（SAE）潜在表示在乳腺影像中为每一层基础模型内部工作机制提供深入洞察的潜力。

Abstract: Interpretability is critical in high-stakes domains such as medical imaging,
where understanding model decisions is essential for clinical adoption. In this
work, we introduce Sparse Autoencoder (SAE)-based interpretability to breast
imaging by analyzing {Mammo-CLIP}, a vision--language foundation model
pretrained on large-scale mammogram image--report pairs. We train a patch-level
\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features
associated with clinically relevant breast concepts such as \textit{mass} and
\textit{suspicious calcification}. Our findings reveal that top activated class
level latent neurons in the SAE latent space often tend to align with ground
truth regions, and also uncover several confounding factors influencing the
model's decision-making process. Additionally, we analyze which latent neurons
the model relies on during downstream finetuning for improving the breast
concept prediction. This study highlights the promise of interpretable SAE
latent representations in providing deeper insight into the internal workings
of foundation models at every layer for breast imaging.

</details>


### [92] [Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation](https://arxiv.org/abs/2507.15243)
*Naeem Paeedeh,Mahardhika Pratama,Wolfgang Mayer,Jimmy Cao,Ryszard Kowlczyk*

Main category: cs.CV

TL;DR: 本文提出CP和SSTs结合的伪类生成方法，解决了CD-FSL中过拟合问题，并在极端域偏移场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决因标记样本稀缺导致的大规模Transformer参数更新过拟合问题，并提升模型在极端域偏移场景下的泛化能力。

Method: 提出了一种新的概念Coalescent Projection（CP）作为软提示的有效替代方案，并结合自监督变换（SSTs）提出了一种仅依赖基础域的伪类生成方法，以应对不同域未见样本的挑战。

Result: 在BSCD-FSL基准测试的极端域偏移场景中，所提方法展现了显著的有效性。

Conclusion: 尽管在跨域少样本学习（CD-FSL）方面取得了进展，但结合DINO预训练模型和原型分类器的方法仍优于最新的SOTA方法。本文提出的Coalescent Projection（CP）概念和结合自监督变换（SSTs）的伪类生成方法，有效解决了因标记样本稀缺导致的过拟合问题，并在极端域偏移场景下展现了优越性能。

Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model
pre-trained with DINO combined with a prototypical classifier outperforms the
latest SOTA methods. A crucial limitation that needs to be overcome is that
updating too many parameters of the transformers leads to overfitting due to
the scarcity of labeled samples. To address this challenge, we propose a new
concept, Coalescent Projection (CP), as an effective successor to soft prompts.
Additionally, we propose a novel pseudo-class generation method combined with
Self-Supervised Transformations (SSTs) that relies solely on the base domain to
prepare the network for encountering unseen samples from different domains. The
proposed method exhibits its effectiveness in comprehensive experiments on the
extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published
at https://github.com/Naeem-Paeedeh/CPLSR.

</details>


### [93] [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/abs/2507.15249)
*Yanbing Zhang,Zhe Wang,Qin Zhou,Mengping Yang*

Main category: cs.CV

TL;DR: FreeCus 是一个无需训练的框架，通过注意力共享、动态偏移分析和多模态大语言模型，激活扩散变压器的零样本能力，实现高保真主题合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于训练过程，限制了实际应用，且未能充分利用现代扩散变压器的零样本潜力。

Method: FreeCus 通过三个关键创新实现：1）注意力共享机制；2）动态偏移分析的升级变体；3）多模态大语言模型（MLLM）的集成。

Result: 实验表明，FreeCus 在多样上下文中实现了一致的主题合成，与现有修复管道和控制模块兼容。

Conclusion: FreeCus 框架成功激活了扩散变压器（DiT）的零样本能力，实现了无需额外训练的高保真主题驱动合成，并在实验中取得了与需要训练的方法相当或更优的结果。

Abstract: In light of recent breakthroughs in text-to-image (T2I) generation,
particularly with diffusion transformers (DiT), subject-driven technologies are
increasingly being employed for high-fidelity customized production that
preserves subject identity from reference inputs, enabling thrilling design
workflows and engaging entertainment. Existing alternatives typically require
either per-subject optimization via trainable text embeddings or training
specialized encoders for subject feature extraction on large-scale datasets.
Such dependencies on training procedures fundamentally constrain their
practical applications. More importantly, current methodologies fail to fully
leverage the inherent zero-shot potential of modern diffusion transformers
(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this
gap, we propose FreeCus, a genuinely training-free framework that activates
DiT's capabilities through three key innovations: 1) We introduce a pivotal
attention sharing mechanism that captures the subject's layout integrity while
preserving crucial editing flexibility. 2) Through a straightforward analysis
of DiT's dynamic shifting, we propose an upgraded variant that significantly
improves fine-grained feature extraction. 3) We further integrate advanced
Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic
representations. Extensive experiments reflect that our method successfully
unlocks DiT's zero-shot ability for consistent subject synthesis across diverse
contexts, achieving state-of-the-art or comparable results compared to
approaches that require additional training. Notably, our framework
demonstrates seamless compatibility with existing inpainting pipelines and
control modules, facilitating more compelling experiences. Our code is
available at: https://github.com/Monalissaa/FreeCus.

</details>


### [94] [MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP](https://arxiv.org/abs/2507.15257)
*Pei An,Jiaqi Yang,Muyao Peng,You Yang,Qiong Liu,Xiaolin Wu,Liangliang Nan*

Main category: cs.CV

TL;DR: 提出MinCD-PnP，通过简化盲PnP为Chamfer距离最小化任务，提升I2P配准的鲁棒性，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统微分PnP对噪声和异常值敏感，限制了对应关系学习的有效性，而盲PnP虽鲁棒但计算成本高。

Method: 提出MinCD-PnP方法，将盲PnP简化为学习2D和3D关键点之间的Chamfer距离最小化任务，并设计轻量级多任务学习模块MinCD-Net。

Result: 在7-Scenes、RGBD-V2、ScanNet等数据集上，MinCD-Net在跨场景和跨数据集设置中均实现了更高的内点率（IR）和配准召回率（RR）。

Conclusion: MinCD-Net通过简化盲PnP为Chamfer距离最小化任务，显著提升了I2P配准的鲁棒性和准确性，在多个数据集上表现优于现有方法。

Abstract: Image-to-point-cloud (I2P) registration is a fundamental problem in computer
vision, focusing on establishing 2D-3D correspondences between an image and a
point cloud. The differential perspective-n-point (PnP) has been widely used to
supervise I2P registration networks by enforcing the projective constraints on
2D-3D correspondences. However, differential PnP is highly sensitive to noise
and outliers in the predicted correspondences. This issue hinders the
effectiveness of correspondence learning. Inspired by the robustness of blind
PnP against noise and outliers in correspondences, we propose an approximated
blind PnP based correspondence learning approach. To mitigate the high
computational cost of blind PnP, we simplify blind PnP to an amenable task of
minimizing Chamfer distance between learned 2D and 3D keypoints, called
MinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task
learning module, named as MinCD-Net, which can be easily integrated into the
existing I2P registration architectures. Extensive experiments on 7-Scenes,
RGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net
outperforms state-of-the-art methods and achieves a higher inlier ratio (IR)
and registration recall (RR) in both cross-scene and cross-dataset settings.

</details>


### [95] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出一种基于条件扩散模型的视频压缩框架，通过多粒度条件和紧凑表示设计，显著提升感知质量，尤其在高压缩比下表现优异。


<details>
  <summary>Details</summary>
Motivation: 基于条件扩散模型在人类视觉感知重建方面的优异表现，提出一种感知优化的视频压缩框架。

Method: 1. 多粒度条件模块捕获静态场景结构和动态时空线索；2. 高效传输的紧凑表示设计；3. 多条件训练，包括模态丢弃和角色感知嵌入。

Result: 实验表明，该方法在Fréchet Video Distance (FVD)和LPIPS等感知质量指标上显著优于传统和神经编解码器。

Conclusion: 该论文提出的基于条件扩散模型的视频压缩框架在感知质量指标上显著优于传统和神经编解码器，特别是在高压缩比下。

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [96] [In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems](https://arxiv.org/abs/2507.15285)
*Lazaro Janier Gonzalez-Soler,Maciej Salwowski,Christoph Busch*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉语言模型和上下文学习的新框架，用于生物识别系统中的攻击检测，实验证明其性能优于传统方法且无需大量训练。


<details>
  <summary>Details</summary>
Motivation: 随着生物识别系统的进步，攻击技术也日益复杂。传统的深度学习模型虽然在训练场景中表现优异，但难以适应不同类型的攻击或环境变化，且需要大量训练数据。本研究旨在探索视觉语言模型（VLM）在生物识别系统中的应用，以解决这些问题。

Method: 论文提出了一个利用视觉语言模型（VLM）和上下文学习技术的系统框架，专注于开源模型，并通过上下文学习技术在安全关键场景中进行定量评估。

Result: 实验结果表明，所提出的子系统在物理和数字攻击检测中表现优异，优于一些传统的CNN模型，且无需资源密集的训练。

Conclusion: 该论文提出的基于视觉语言模型（VLM）和上下文学习框架的子系统在生物识别系统中表现出色，能够有效检测物理呈现攻击和数字变形攻击，且无需资源密集的训练，展现出在攻击检测泛化能力上的潜力。

Abstract: Recent advances in biometric systems have significantly improved the
detection and prevention of fraudulent activities. However, as detection
methods improve, attack techniques become increasingly sophisticated. Attacks
on face recognition systems can be broadly divided into physical and digital
approaches. Traditionally, deep learning models have been the primary defence
against such attacks. While these models perform exceptionally well in
scenarios for which they have been trained, they often struggle to adapt to
different types of attacks or varying environmental conditions. These
subsystems require substantial amounts of training data to achieve reliable
performance, yet biometric data collection faces significant challenges,
including privacy concerns and the logistical difficulties of capturing diverse
attack scenarios under controlled conditions. This work investigates the
application of Vision Language Models (VLM) and proposes an in-context learning
framework for detecting physical presentation attacks and digital morphing
attacks in biometric systems. Focusing on open-source models, the first
systematic framework for the quantitative evaluation of VLMs in
security-critical scenarios through in-context learning techniques is
established. The experimental evaluation conducted on freely available
databases demonstrates that the proposed subsystem achieves competitive
performance for physical and digital attack detection, outperforming some of
the traditional CNNs without resource-intensive training. The experimental
results validate the proposed framework as a promising tool for improving
generalisation in attack detection.

</details>


### [97] [Minutiae-Anchored Local Dense Representation for Fingerprint Matching](https://arxiv.org/abs/2507.15297)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: 提出DMD方法，通过局部密集表示提升指纹匹配的鲁棒性和准确性，在多个数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决指纹匹配在不同采集条件下的鲁棒性和准确性挑战。

Method: 提出了一种名为DMD的局部密集表示方法，通过以细节特征为中心提取局部补丁的描述符，形成三维张量，结合空间结构信息，实现了多层次的细粒度描述。

Result: 在多种指纹数据集上验证了DMD方法的有效性和泛化能力，实现了最先进的识别精度。

Conclusion: DMD方法在多种指纹数据集上表现出色，不仅实现了最先进的识别精度，还保持了较高的计算效率，展示了在大规模指纹识别中的强大潜力。

Abstract: Fingerprint matching under diverse capture conditions remains a fundamental
challenge in biometric recognition. To achieve robust and accurate performance
in such scenarios, we propose DMD, a minutiae-anchored local dense
representation which captures both fine-grained ridge textures and
discriminative minutiae features in a spatially structured manner.
Specifically, descriptors are extracted from local patches centered and
oriented on each detected minutia, forming a three-dimensional tensor, where
two dimensions represent spatial locations on the fingerprint plane and the
third encodes semantic features. This representation explicitly captures
abstract features of local image patches, enabling a multi-level, fine-grained
description that aggregates information from multiple minutiae and their
surrounding ridge structures. Furthermore, thanks to its strong spatial
correspondence with the patch image, DMD allows for the use of foreground
segmentation masks to identify valid descriptor regions. During matching,
comparisons are then restricted to overlapping foreground areas, improving
efficiency and robustness. Extensive experiments on rolled, plain, parital,
contactless, and latent fingerprint datasets demonstrate the effectiveness and
generalizability of the proposed method. It achieves state-of-the-art accuracy
across multiple benchmarks while maintaining high computational efficiency,
showing strong potential for large-scale fingerprint recognition. Corresponding
code is available at https://github.com/Yu-Yy/DMD.

</details>


### [98] [Few-Shot Object Detection via Spatial-Channel State Space Model](https://arxiv.org/abs/2507.15308)
*Zhimeng Xin,Tianxu Wu,Yixiong Zou,Shiming Chen,Dingjie Fu,Xinge You*

Main category: cs.CV

TL;DR: 该论文针对少样本目标检测中特征通道提取不准确的问题，提出SCSM模块，通过SFM和CSM模块优化空间和通道关系，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 少样本目标检测中有限的训练样本导致现有方法难以准确提取有效特征，特别是权值高但不一定有效的通道和权值低但有价值的通道。

Method: 提出了空间-通道状态建模（SCSM）模块，包括空间特征建模（SFM）模块和基于Mamba的通道状态建模（CSM）模块，以平衡空间和通道关系的学习，并利用通道间相关性优化特征提取。

Result: 在VOC和COCO数据集上的大量实验表明，SCSM模块显著提升了特征通道的表示质量，达到了最先进的性能。

Conclusion: SCSM模块通过空间-通道状态建模，显著提升了少样本目标检测中特征通道的有效性，并在VOC和COCO数据集上实现了最先进的性能。

Abstract: Due to the limited training samples in few-shot object detection (FSOD), we
observe that current methods may struggle to accurately extract effective
features from each channel. Specifically, this issue manifests in two aspects:
i) channels with high weights may not necessarily be effective, and ii)
channels with low weights may still hold significant value. To handle this
problem, we consider utilizing the inter-channel correlation to facilitate the
novel model's adaptation process to novel conditions, ensuring the model can
correctly highlight effective channels and rectify those incorrect ones. Since
the channel sequence is also 1-dimensional, its similarity with the temporal
sequence inspires us to take Mamba for modeling the correlation in the channel
sequence. Based on this concept, we propose a Spatial-Channel State Space
Modeling (SCSM) module for spatial-channel state modeling, which highlights the
effective patterns and rectifies those ineffective ones in feature channels. In
SCSM, we design the Spatial Feature Modeling (SFM) module to balance the
learning of spatial relationships and channel relationships, and then introduce
the Channel State Modeling (CSM) module based on Mamba to learn correlation in
channels. Extensive experiments on the VOC and COCO datasets show that the SCSM
module enables the novel detector to improve the quality of focused feature
representation in channels and achieve state-of-the-art performance.

</details>


### [99] [BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?](https://arxiv.org/abs/2507.15321)
*Zhenyu Li,Haotong Lin,Jiashi Feng,Peter Wonka,Bingyi Kang*

Main category: cs.CV

TL;DR: 提出BenchDepth基准，通过五个下游任务评估深度基础模型，解决传统评估协议的偏差问题，推动深度估计研究。


<details>
  <summary>Details</summary>
Motivation: 现有深度基础模型评估存在不一致性，传统基准依赖对齐指标导致偏差，难以公平比较。

Method: 提出BenchDepth基准，通过深度完成、立体匹配、单目前馈3D场景重建、SLAM和视觉语言空间理解五个下游任务评估DFMs。

Result: 对八种最先进的DFMs进行了基准测试，并提供了关键发现和观察的深入分析。

Conclusion: 本文提出了BenchDepth基准，通过五个下游代理任务评估深度基础模型（DFMs），旨在解决传统评估协议中的不一致性问题，并推动深度估计领域的未来研究。

Abstract: Depth estimation is a fundamental task in computer vision with diverse
applications. Recent advancements in deep learning have led to powerful depth
foundation models (DFMs), yet their evaluation remains challenging due to
inconsistencies in existing protocols. Traditional benchmarks rely on
alignment-based metrics that introduce biases, favor certain depth
representations, and complicate fair comparisons. In this work, we propose
BenchDepth, a new benchmark that evaluates DFMs through five carefully selected
downstream proxy tasks: depth completion, stereo matching, monocular
feed-forward 3D scene reconstruction, SLAM, and vision-language spatial
understanding. Unlike conventional evaluation protocols, our approach assesses
DFMs based on their practical utility in real-world applications, bypassing
problematic alignment procedures. We benchmark eight state-of-the-art DFMs and
provide an in-depth analysis of key findings and observations. We hope our work
sparks further discussion in the community on best practices for depth model
evaluation and paves the way for future research and advancements in depth
estimation.

</details>


### [100] [ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis](https://arxiv.org/abs/2507.15335)
*Muhammad Aqeel,Federico Leonardi,Francesco Setti*

Main category: cs.CV

TL;DR: ExDD框架通过显式双分布建模和合成数据生成，解决了工业缺陷检测中的数据稀缺和异常分布假设问题，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 工业缺陷检测系统受限于单类异常检测范式，假设异常分布均匀且难以应对现实制造环境中的数据稀缺问题。

Method: 利用并行内存库捕获正常和异常模式的统计特性，结合潜在扩散模型生成具有领域特定文本条件的合成缺陷数据，并通过邻域感知比率评分机制融合互补距离度量。

Result: 在KSDD2数据集上验证了ExDD的优越性能，最佳增强效果在100个合成样本时达到。

Conclusion: ExDD框架通过显式建模双特征分布，成功克服了单类异常检测范式的局限性，在工业缺陷检测中表现出色，验证了其优越性能（94.2% I-AUROC, 97.7% P-AUROC）。

Abstract: Industrial defect detection systems face critical limitations when confined
to one-class anomaly detection paradigms, which assume uniform outlier
distributions and struggle with data scarcity in realworld manufacturing
environments. We present ExDD (Explicit Dual Distribution), a novel framework
that transcends these limitations by explicitly modeling dual feature
distributions. Our approach leverages parallel memory banks that capture the
distinct statistical properties of both normality and anomalous patterns,
addressing the fundamental flaw of uniform outlier assumptions. To overcome
data scarcity, we employ latent diffusion models with domain-specific textual
conditioning, generating in-distribution synthetic defects that preserve
industrial context. Our neighborhood-aware ratio scoring mechanism elegantly
fuses complementary distance metrics, amplifying signals in regions exhibiting
both deviation from normality and similarity to known defect patterns.
Experimental validation on KSDD2 demonstrates superior performance (94.2%
I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.

</details>


### [101] [RoadFusion: Latent Diffusion Model for Pavement Defect Detection](https://arxiv.org/abs/2507.15346)
*Muhammad Aqeel,Kidus Dagnaw Bellete,Francesco Setti*

Main category: cs.CV

TL;DR: RoadFusion利用合成异常生成和双路径特征适应，解决了路面缺陷检测中的数据稀缺和领域偏移问题，性能优异。


<details>
  <summary>Details</summary>
Motivation: 路面缺陷检测面临标注数据稀缺、训练与部署环境间的领域偏移，以及不同道路条件下缺陷外观的高变异性等挑战。

Method: 提出了RoadFusion框架，包括潜在扩散模型生成合成缺陷、双路径特征适应器优化表示，以及轻量级判别器区分细粒度缺陷模式。

Result: 在六个基准数据集上，RoadFusion在分类和定位任务中均表现优异，多项指标达到最新技术水平。

Conclusion: RoadFusion通过合成异常生成和双路径特征适应，有效解决了路面缺陷检测中的数据稀缺、领域偏移和缺陷多样性问题，在多个基准数据集上实现了最先进的性能。

Abstract: Pavement defect detection faces critical challenges including limited
annotated data, domain shift between training and deployment environments, and
high variability in defect appearances across different road conditions. We
propose RoadFusion, a framework that addresses these limitations through
synthetic anomaly generation with dual-path feature adaptation. A latent
diffusion model synthesizes diverse, realistic defects using text prompts and
spatial masks, enabling effective training under data scarcity. Two separate
feature adaptors specialize representations for normal and anomalous inputs,
improving robustness to domain shift and defect variability. A lightweight
discriminator learns to distinguish fine-grained defect patterns at the patch
level. Evaluated on six benchmark datasets, RoadFusion achieves consistently
strong performance across both classification and localization tasks, setting
new state-of-the-art in multiple metrics relevant to real-world road
inspection.

</details>


### [102] [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://arxiv.org/abs/2507.15365)
*Fatemeh Saleh,Sadegh Aliakbarian,Charlie Hewitt,Lohit Petikam,Xiao-Xian,Antonio Criminisi,Thomas J. Cashman,Tadas Baltrušaitis*

Main category: cs.CV

TL;DR: 该论文提出使用合成数据集训练人类中心计算机视觉模型，实现了高精度、低成本和更好的数据控制，适用于多种密集预测任务。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的人类中心计算机视觉模型需要大量参数和计算资源，而合成数据集可以降低成本、提高效率，并提供更好的数据控制和公平性保障。

Method: 通过程序化合成高保真数据集，并利用这些数据集训练模型，重点解决了数据多样性控制和公平性问题。

Result: 在深度估计、表面法线估计和软前景分割三个密集预测任务中，模型保持了高精度，且训练和推理成本大幅降低。

Conclusion: 使用合成数据集训练模型可以在保持高精度的同时显著降低成本和资源需求，且能有效解决数据多样性和公平性问题。

Abstract: The state of the art in human-centric computer vision achieves high accuracy
and robustness across a diverse range of tasks. The most effective models in
this domain have billions of parameters, thus requiring extremely large
datasets, expensive training regimes, and compute-intensive inference. In this
paper, we demonstrate that it is possible to train models on much smaller but
high-fidelity synthetic datasets, with no loss in accuracy and higher
efficiency. Using synthetic training data provides us with excellent levels of
detail and perfect labels, while providing strong guarantees for data
provenance, usage rights, and user consent. Procedural data synthesis also
provides us with explicit control on data diversity, that we can use to address
unfairness in the models we train. Extensive quantitative assessment on real
input images demonstrates accuracy of our models on three dense prediction
tasks: depth estimation, surface normal estimation, and soft foreground
segmentation. Our models require only a fraction of the cost of training and
inference when compared with foundational models of similar accuracy. Our
human-centric synthetic dataset and trained models are available at
https://aka.ms/DAViD.

</details>


### [103] [Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond](https://arxiv.org/abs/2507.15401)
*Huiyu Zhai,Xingxing Yang,Yalan Ye,Chenyang Li,Bin Fan,Changze Li*

Main category: cs.CV

TL;DR: ORSANet通过多模态语义引导和动态损失函数，显著提升了遮挡条件下的表情识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有FER模型在面部遮挡情况下难以提取有效特征，导致分类不准确，需解决遮挡和数据集偏差问题。

Method: 1. 引入语义分割图和面部关键点作为多模态先验；2. 设计多尺度交叉交互模块（MCM）自适应融合特征；3. 提出动态对抗排斥增强损失（DARELoss）优化分类边界。

Result: ORSANet在公开基准和Occlu-FER数据集上均实现了最优识别性能。

Conclusion: ORSANet通过引入多模态语义引导、多尺度交叉交互模块和动态对抗排斥增强损失，显著提升了遮挡条件下的人脸表情识别性能，并在公开基准和自建数据集Occlu-FER上达到了最优性能。

Abstract: Facial expression recognition (FER) is a challenging task due to pervasive
occlusion and dataset biases. Especially when facial information is partially
occluded, existing FER models struggle to extract effective facial features,
leading to inaccurate classifications. In response, we present ORSANet, which
introduces the following three key contributions: First, we introduce auxiliary
multi-modal semantic guidance to disambiguate facial occlusion and learn
high-level semantic knowledge, which is two-fold: 1) we introduce semantic
segmentation maps as dense semantics prior to generate semantics-enhanced
facial representations; 2) we introduce facial landmarks as sparse geometric
prior to mitigate intrinsic noises in FER, such as identity and gender biases.
Second, to facilitate the effective incorporation of these two multi-modal
priors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively
fuse the landmark feature and semantics-enhanced representations within
different scales. Third, we design a Dynamic Adversarial Repulsion Enhancement
Loss (DARELoss) that dynamically adjusts the margins of ambiguous classes,
further enhancing the model's ability to distinguish similar expressions. We
further construct the first occlusion-oriented FER dataset to facilitate
specialized robustness analysis on various real-world occlusion conditions,
dubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER
demonstrate that our proposed ORSANet achieves SOTA recognition performance.
Code is publicly available at https://github.com/Wenyuzhy/ORSANet-master.

</details>


### [104] [SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition](https://arxiv.org/abs/2507.15418)
*Ka Young Kim,Hyeon Bae Kim,Seong Tae Kim*

Main category: cs.CV

TL;DR: SurgX 是一个提升手术阶段识别模型可解释性的框架，通过关联神经元与概念，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在手术阶段识别方面取得了进展，但模型的不透明性阻碍了信任和调试，因此需要提升其可解释性。

Method: 提出了 SurgX，一个基于概念的解释框架，包括选择代表性示例序列、构建手术视频数据集的概念集、关联神经元与概念以及识别关键神经元。

Result: 在两个手术阶段识别模型上的广泛实验验证了 SurgX 方法的有效性，并分析了预测的解释。

Conclusion: SurgX 通过将神经元与相关概念关联，显著提升了手术阶段识别模型的可解释性，验证了其在解释预测方面的潜力。

Abstract: Surgical phase recognition plays a crucial role in surgical workflow
analysis, enabling various applications such as surgical monitoring, skill
assessment, and workflow optimization. Despite significant advancements in deep
learning-based surgical phase recognition, these models remain inherently
opaque, making it difficult to understand how they make decisions. This lack of
interpretability hinders trust and makes it challenging to debug the model. To
address this challenge, we propose SurgX, a novel concept-based explanation
framework that enhances the interpretability of surgical phase recognition
models by associating neurons with relevant concepts. In this paper, we
introduce the process of selecting representative example sequences for
neurons, constructing a concept set tailored to the surgical video dataset,
associating neurons with concepts and identifying neurons crucial for
predictions. Through extensive experiments on two surgical phase recognition
models, we validate our method and analyze the explanation for prediction. This
highlights the potential of our method in explaining surgical phase
recognition. The code is available at https://github.com/ailab-kyunghee/SurgX

</details>


### [105] [EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)
*Jiaao Li,Kaiyuan Li,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: EgoPrune是一种免训练token修剪方法，专为egomotion视频推理设计，通过关键帧选择、冗余过滤和多样性token选择，显著提升了效率并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: egomotion视频作为具身AI代理的主要视觉输入，其高效推理对现实世界部署至关重要。现有方法未能充分利用egomotion设置中的时空连续性和运动约束。

Method: EgoPrune是一种专为egomotion视频推理设计的免训练token修剪方法，包括三个组件：基于EmbodiedR的关键帧选择器、视角感知冗余过滤（PARF）和基于最大边际相关性（MMR）的token选择器。

Result: EgoPrune在两个egomotion视频基准测试中表现优于现有免训练方法，并显著降低了计算成本和延迟。

Conclusion: EgoPrune在无需训练的情况下，显著降低了计算成本（FLOPs、内存使用和延迟），并在两个egomotion视频基准测试中表现优于现有方法，证明了其在现实世界部署中的高效性和适用性。

Abstract: Egomotion videos are first-person recordings where the view changes
continuously due to the agent's movement. As they serve as the primary visual
input for embodied AI agents, making egomotion video reasoning more efficient
is therefore essential for real-world deployment. Recent advances in
vision-language models have enabled strong multimodal reasoning capabilities,
but their computational cost remains prohibitive for long, redundant video
inputs. Existing token pruning methods, typically designed for third-person
videos, fail to leverage the spatiotemporal continuity and motion constraints
inherent in egomotion settings. To address this, we propose EgoPrune, a
training-free token pruning method tailored for egomotion video reasoning.
EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR
for temporally efficient sampling; Perspective-Aware Redundancy Filtering
(PARF), which aligns visual tokens using perspective transformations and
removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token
selector that jointly considers visual-text relevance and intra-frame
diversity. Experiments on two egomotion video benchmarks show that EgoPrune
consistently outperforms prior training-free methods across various pruning
ratios while significantly reducing FLOPs, memory usage, and latency. Moreover,
we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB
edge device, demonstrating its real-world efficiency and suitability for
on-device egomotion video reasoning.

</details>


### [106] [One Last Attention for Your Vision-Language Model](https://arxiv.org/abs/2507.15480)
*Liang Chen,Ghazi Shazan Ahmad,Tianjun Yao,Lingqiao Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: RAda通过动态校准融合表示中的决策矩阵元素，优化预训练视觉语言模型的微调效果，实验证明其高效且通用。


<details>
  <summary>Details</summary>
Motivation: 现有的微调方法通常单独优化文本或视觉模态的表示，而忽视了融合表示在决策过程中的关键作用。RAda旨在填补这一空白，提升预训练视觉语言模型的微调效果。

Method: RAda采用一种轻量级注意力层生成的学习掩码，动态校准决策矩阵中每个元素的贡献，从而在不修改中间特征的情况下调整最终的跨模态交互。

Result: 实验表明，RAda作为一种通用的微调技术，在不同设置（如更新或冻结预训练编码器、仅访问未标记测试数据的测试时训练）中均能显著提升基线性能，并与当前最优方法相当。

Conclusion: RAda提出了一种简单而有效的方法，通过显式利用预训练视觉语言模型（VLMs）的最终融合表示来优化微调过程，显著提升了基线性能，并在多种设置中与现有技术表现相当。

Abstract: Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable
zero-shot performance, yet their downstream potential hinges on effective
fine-tuning. Most adaptation methods typically focus on refining representation
from separate modalities (text or vision) but neglect the critical role of
their fused representations in the decision-making process, \emph{\ie} rational
matrix that drives the final prediction. To bridge the gap, we propose a simple
yet effective \textbf{R}ational \textbf{Ada}ptaion ({RAda}) to explicitly
exploit the final fused representation during fine-tuning. RAda employs a
learned mask, obtained from a lightweight attention layer attached at the end
of a VLM, to dynamically calibrate the contribution of each element in the
rational matrix, enabling targeted adjustments to the final cross-modal
interactions without incurring costly modifications to intermediate features.
Experiments in different settings (i.e., updating, or freezing pretrained
encoders in adaptation, and test-time training that can only access the
unlabeled test data) show that RAda serves as a versatile fine-tuning
technique, improving the baseline with minimal code and performing comparably
against current arts in most settings. Code is available at
\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.

</details>


### [107] [An aerial color image anomaly dataset for search missions in complex forested terrain](https://arxiv.org/abs/2507.15492)
*Rakesh John Amala Arokia Nathan,Matthias Gessner,Nurullah Özkan,Marius Bock,Mohamed Youssef,Maximilian Mews,Björn Piltz,Ralf Berger,Oliver Bimber*

Main category: cs.CV

TL;DR: 论文通过众包搜索创建了一个标注数据集，用于改进复杂森林环境中的异常检测，现有方法表现不佳，需上下文感知方法。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是在茂密植被遮挡下，自动化分析无效，因此需要一种新的方法来辅助搜索。

Method: 论文的方法是通过研究飞机采集高分辨率航空影像，并利用众包搜索倡议来标注难以检测的异常。

Result: 论文的结果是创建了一个独特的标注数据集，并展示了现有方法在这种复杂环境中的表现不佳，强调了上下文感知方法的必要性。

Conclusion: 该论文的结论是，公开可访问的数据集和交互式网络界面为复杂森林环境中的异常检测提供了宝贵的资源，支持搜捕和救援行动。

Abstract: After a family murder in rural Germany, authorities failed to locate the
suspect in a vast forest despite a massive search. To aid the search, a
research aircraft captured high-resolution aerial imagery. Due to dense
vegetation obscuring small clues, automated analysis was ineffective, prompting
a crowd-search initiative. This effort produced a unique dataset of labeled,
hard-to-detect anomalies under occluded, real-world conditions. It can serve as
a benchmark for improving anomaly detection approaches in complex forest
environments, supporting manhunts and rescue operations. Initial benchmark
tests showed existing methods performed poorly, highlighting the need for
context-aware approaches. The dataset is openly accessible for offline
processing. An additional interactive web interface supports online viewing and
dynamic growth by allowing users to annotate and submit new findings.

</details>


### [108] [Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization](https://arxiv.org/abs/2507.15504)
*Bingqing Zhang,Zhuo Cao,Heming Du,Yang Li,Xue Li,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: UMIVR通过量化文本、映射和帧的不确定性并生成澄清问题，显著提升了交互式文本到视频检索的效果。


<details>
  <summary>Details</summary>
Motivation: 当前交互式文本到视频检索方法缺乏对不确定性的明确量化，限制了其效果。UMIVR旨在填补这一空白。

Method: UMIVR框架通过训练无关的指标（TAS、MUS、TQFS）量化和减少文本模糊性、映射不确定性和帧质量不确定性，并生成针对性的澄清问题。

Result: 在多个基准测试中，UMIVR表现出色，如在MSR-VTT-1k数据集上，经过10轮交互后Recall@1达到69.2%。

Conclusion: UMIVR通过明确量化和最小化不确定性，显著提升了交互式文本到视频检索的效果，为未来研究奠定了不确定性最小化的基础。

Abstract: Despite recent advances, Text-to-video retrieval (TVR) is still hindered by
multiple inherent uncertainties, such as ambiguous textual queries, indistinct
text-video mappings, and low-quality video frames. Although interactive systems
have emerged to address these challenges by refining user intent through
clarifying questions, current methods typically rely on heuristic or ad-hoc
strategies without explicitly quantifying these uncertainties, limiting their
effectiveness. Motivated by this gap, we propose UMIVR, an
Uncertainty-Minimizing Interactive Text-to-Video Retrieval framework that
explicitly quantifies three critical uncertainties-text ambiguity, mapping
uncertainty, and frame uncertainty-via principled, training-free metrics:
semantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon
divergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based
Frame Sampler (TQFS). By adaptively generating targeted clarifying questions
guided by these uncertainty measures, UMIVR iteratively refines user queries,
significantly reducing retrieval ambiguity. Extensive experiments on multiple
benchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1
(69.2\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby
establishing an uncertainty-minimizing foundation for interactive TVR.

</details>


### [109] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: GeMix是一种基于类条件GANs的学习驱动mixup方法，通过生成视觉连贯的图像提升医学图像分类性能，优于传统mixup。


<details>
  <summary>Details</summary>
Motivation: 传统mixup的像素级插值在医学等高风险应用中可能产生不真实图像，阻碍学习。GeMix旨在通过学习驱动的插值解决这一问题。

Method: GeMix是一个两阶段框架，使用类条件GANs进行学习驱动的标签感知插值。首先在目标数据集上训练StyleGAN2-ADA生成器，在增强阶段通过Beta分布系数混合两个偏向不同类的Dirichlet先验标签向量，并基于软标签生成视觉连贯的图像。

Result: 在COVIDx-CT-3数据集上，GeMix与真实数据结合后，所有主干网络（ResNet-50、ResNet-101、EfficientNet-B0）的macro-F1均优于传统mixup，降低了COVID-19检测的假阴性率。

Conclusion: GeMix是一种有效的drop-in替代方案，能够在不破坏现有训练流程的情况下，提供更强的正则化和更高的语义保真度。

Abstract: Mixup has become a popular augmentation strategy for image classification,
yet its naive pixel-wise interpolation often produces unrealistic images that
can hinder learning, particularly in high-stakes medical applications. We
propose GeMix, a two-stage framework that replaces heuristic blending with a
learned, label-aware interpolation powered by class-conditional GANs. First, a
StyleGAN2-ADA generator is trained on the target dataset. During augmentation,
we sample two label vectors from Dirichlet priors biased toward different
classes and blend them via a Beta-distributed coefficient. Then, we condition
the generator on this soft label to synthesize visually coherent images that
lie along a continuous class manifold. We benchmark GeMix on the large-scale
COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,
EfficientNet-B0). When combined with real data, our method increases macro-F1
over traditional mixup for all backbones, reducing the false negative rate for
COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,
delivering stronger regularization and greater semantic fidelity, without
disrupting existing training pipelines. We publicly release our code at
https://github.com/hugocarlesso/GeMix to foster reproducibility and further
research.

</details>


### [110] [SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.15520)
*Hanting Li,Fei Zhou,Xin Sun,Yang Hua,Jungong Han,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: SAIGFormer通过动态积分图像和光照引导的自注意力机制，显著提升了非均匀光照场景的增强效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有Transformer方法在非均匀光照场景（如背光和阴影）中表现不佳的问题。

Method: 提出了动态积分图像表示和空间自适应积分光照估计器（$\text{SAI}^2\text{E}$），并引入了光照引导的多头自注意力机制（IG-MSA）。

Result: 在五个标准低光数据集和跨域基准（LOL-Blur）上，SAIGFormer在定量和定性指标上均显著优于现有方法。

Conclusion: SAIGFormer在非均匀光照增强方面表现出色，并在多个数据集上展现出强大的泛化能力。

Abstract: Recent Transformer-based low-light enhancement methods have made promising
progress in recovering global illumination. However, they still struggle with
non-uniform lighting scenarios, such as backlit and shadow, appearing as
over-exposure or inadequate brightness restoration. To address this challenge,
we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)
framework that enables accurate illumination restoration. Specifically, we
propose a dynamic integral image representation to model the spatially-varying
illumination, and further construct a novel Spatially-Adaptive Integral
Illumination Estimator ($\text{SAI}^2\text{E}$). Moreover, we introduce an
Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which
leverages the illumination to calibrate the lightness-relevant features toward
visual-pleased illumination enhancement. Extensive experiments on five standard
low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our
SAIGFormer significantly outperforms state-of-the-art methods in both
quantitative and qualitative metrics. In particular, our method achieves
superior performance in non-uniform illumination enhancement while exhibiting
strong generalization capabilities across multiple datasets. Code is available
at https://github.com/LHTcode/SAIGFormer.git.

</details>


### [111] [Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis](https://arxiv.org/abs/2507.15636)
*Lisan Al Amin,Md. Ismail Hossain,Thanh Thi Nguyen,Tasnim Jahan,Mahbubul Islam,Faisal Quader*

Main category: cs.CV

TL;DR: 研究利用LTH发现深度伪造检测网络中存在高效子网络，迭代剪枝方法优于一次性剪枝，子网络可迁移，为轻量级检测系统提供可能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的进步对信息完整性和社会信任构成挑战，现有检测方法模型庞大且机制不明确，难以在资源有限环境中部署。

Method: 研究应用彩票假设（LTH）于深度伪造检测，通过迭代幅度剪枝方法对MesoNet、CNN-5和ResNet-18架构在OpenForensic和FaceForensics++数据集上进行实验。

Result: 实验显示MesoNet在80%稀疏度下保持56.2%准确率（基线为62.6%），仅需3,000参数。LTH剪枝方法优于一次性剪枝，且子网络可跨数据集迁移。

Conclusion: 研究发现深度伪造检测网络中存在‘中奖彩票’子网络，即使在高度稀疏情况下仍能保持性能。LTH迭代幅度剪枝方法优于一次性剪枝，且子网络在不同数据集间具有可迁移性，为高效可部署的深度伪造检测系统提供了可能。

Abstract: Recent advances in deepfake technology have created increasingly convincing
synthetic media that poses significant challenges to information integrity and
social trust. While current detection methods show promise, their underlying
mechanisms remain poorly understood, and the large sizes of their models make
them challenging to deploy in resource-limited environments. This study
investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake
detection, aiming to identify the key features crucial for recognizing
deepfakes. We examine how neural networks can be efficiently pruned while
maintaining high detection accuracy. Through extensive experiments with
MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and
FaceForensics++ datasets, we find that deepfake detection networks contain
winning tickets, i.e., subnetworks, that preserve performance even at
substantial sparsity levels. Our results indicate that MesoNet retains 56.2%
accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000
parameters, which is about 90% of its baseline accuracy (62.6%). The results
also show that our proposed LTH-based iterative magnitude pruning approach
consistently outperforms one-shot pruning methods. Using Grad-CAM
visualization, we analyze how pruned networks maintain their focus on critical
facial regions for deepfake detection. Additionally, we demonstrate the
transferability of winning tickets across datasets, suggesting potential for
efficient, deployable deepfake detection systems.

</details>


### [112] [Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2507.15540)
*Syed Ahmed Mahmood,Ali Shah Ali,Umer Ahmed,Fawad Javed Fateh,M. Zeeshan Zia,Quoc-Huy Tran*

Main category: cs.CV

TL;DR: 该论文提出了一种自监督程序学习框架，通过融合Gromov-Wasserstein最优传输和对比正则化，有效解决了现有方法在处理顺序变化和冗余帧时的不足，并在多个基准测试中取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的程序学习方法在处理顺序变化、背景/冗余帧和重复动作时性能不佳，需要一种更鲁棒的方法来优化帧间映射和关键步骤识别。

Method: 提出了一种自监督程序学习框架，结合了融合Gromov-Wasserstein最优传输和结构先验的帧间映射方法，并引入了对比正则化以避免嵌入空间坍塌。

Result: 在EgoProceL、ProceL和CrossTask等大规模基准测试中，所提方法表现优于OPEL等现有方法。

Conclusion: 论文提出的自监督程序学习框架通过融合Gromov-Wasserstein最优传输和对比正则化，显著提升了在无标签程序视频中关键步骤发现和顺序建立的效果，并在多个基准测试中优于现有方法。

Abstract: We study the problem of self-supervised procedure learning, which discovers
key steps and establishes their order from a set of unlabeled procedural
videos. Previous procedure learning methods typically learn frame-to-frame
correspondences between videos before determining key steps and their order.
However, their performance often suffers from order variations,
background/redundant frames, and repeated actions. To overcome these
challenges, we propose a self-supervised procedure learning framework, which
utilizes a fused Gromov-Wasserstein optimal transport formulation with a
structural prior for computing frame-to-frame mapping between videos. However,
optimizing exclusively for the above temporal alignment term may lead to
degenerate solutions, where all frames are mapped to a small cluster in the
embedding space and hence every video is associated with only one key step. To
address that limitation, we further integrate a contrastive regularization
term, which maps different frames to different points in the embedding space,
avoiding the collapse to trivial solutions. Finally, we conduct extensive
experiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,
ProceL and CrossTask) benchmarks to demonstrate superior performance by our
approach against previous methods, including OPEL which relies on a traditional
Kantorovich optimal transport formulation with an optimality prior.

</details>


### [113] [Towards Holistic Surgical Scene Graph](https://arxiv.org/abs/2507.15541)
*Jongmin Shin,Enki Cho,Ka Yong Kim,Jung Yong Kim,Seong Tae Kim,Namkee Oh*

Main category: cs.CV

TL;DR: 提出了Endoscapes-SG201数据集和SSG-Com方法，通过实验验证了工具-动作-目标组合和手部身份对手术场景理解的关键作用。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的手术场景表示方法未充分探索工具-动作-目标组合和手部身份等关键元素，而这些对手术场景理解至关重要。

Method: 提出了Endoscapes-SG201数据集，包含工具-动作-目标组合和手部身份的标注，并介绍了SSG-Com这一基于图的方法，用于学习和表示这些关键元素。

Result: 实验验证了SSG-Com方法在下游任务（如安全关键视图评估和动作三元组识别）中的有效性，强调了这些关键元素的重要性。

Conclusion: 通过Endoscapes-SG201数据集和SSG-Com方法的实验验证，证明了整合手术场景图中的关键元素（如工具-动作-目标组合和手部身份）对提升手术场景理解的重要性。

Abstract: Surgical scene understanding is crucial for computer-assisted intervention
systems, requiring visual comprehension of surgical scenes that involves
diverse elements such as surgical tools, anatomical structures, and their
interactions. To effectively represent the complex information in surgical
scenes, graph-based approaches have been explored to structurally model
surgical entities and their relationships. Previous surgical scene graph
studies have demonstrated the feasibility of representing surgical scenes using
graphs. However, certain aspects of surgical scenes-such as diverse
combinations of tool-action-target and the identity of the hand operating the
tool-remain underexplored in graph-based representations, despite their
importance. To incorporate these aspects into graph representations, we propose
Endoscapes-SG201 dataset, which includes annotations for tool-action-target
combinations and hand identity. We also introduce SSG-Com, a graph-based method
designed to learn and represent these critical elements. Through experiments on
downstream tasks such as critical view of safety assessment and action triplet
recognition, we demonstrated the importance of integrating these essential
scene graph components, highlighting their significant contribution to surgical
scene understanding. The code and dataset are available at
https://github.com/ailab-kyunghee/SSG-Com

</details>


### [114] [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/abs/2507.15686)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: LINR-PCGC是首个基于INR的无损点云几何压缩方法，通过优化编码框架和轻量级网络设计，显著提升压缩效率和速度。


<details>
  <summary>Details</summary>
Motivation: 解决现有AI点云压缩方法对训练数据分布的依赖问题，以及当前INR方法仅支持有损几何压缩的局限性。

Method: 设计了点云级别的编码框架和有效的网络初始化策略，提出基于多尺度SparseConv的轻量级编码网络，包含尺度上下文提取、子节点预测和模型压缩模块。

Result: 实验结果显示，LINR-PCGC在MVUB数据集上比G-PCC TMC13v23和SparsePCGC分别减少了21.21%和21.95%的比特流。

Conclusion: 论文提出了LINR-PCGC，一种基于INR的无损点云几何压缩方法，显著提升了编码速度和压缩效率，优于传统和AI方法。

Abstract: Existing AI-based point cloud compression methods struggle with dependence on
specific training data distributions, which limits their real-world deployment.
Implicit Neural Representation (INR) methods solve the above problem by
encoding overfitted network parameters to the bitstream, resulting in more
distribution-agnostic results. However, due to the limitation of encoding time
and decoder size, current INR based methods only consider lossy geometry
compression. In this paper, we propose the first INR based lossless point cloud
geometry compression method called Lossless Implicit Neural Representations for
Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we
design a group of point clouds level coding framework with an effective network
initialization strategy, which can reduce around 60% encoding time. A
lightweight coding network based on multiscale SparseConv, consisting of scale
context extraction, child node prediction, and model compression modules, is
proposed to realize fast inference and compact decoder size. Experimental
results show that our method consistently outperforms traditional and AI-based
methods: for example, with the convergence time in the MVUB dataset, our method
reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and
21.95% compared to SparsePCGC. Our project can be seen on
https://huangwenjie2023.github.io/LINR-PCGC/.

</details>


### [115] [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/abs/2507.15542)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: HOLa通过低秩分解VLM特征和LLM正则化，提升了零样本HOI检测的泛化能力和动作区分度，在HICO-DET上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在区分涉及相同对象的动作或泛化到未见类别时表现有限，HOLa旨在解决这些问题。

Method: HOLa采用低秩分解VLM文本特征，生成类共享的基础特征和可调权重，结合人类-对象令牌增强视觉交互表示，并通过LLM驱动的动作正则化优化权重适应。

Result: 在HICO-DET数据集上，HOLa在零样本HOI设置中达到了27.91的未见类别mAP，创下了新纪录。

Conclusion: HOLa通过低秩分解VLM文本特征，结合LLM驱动的动作正则化，显著提升了零样本HOI检测的性能，特别是在未见类别上的泛化能力和动作区分度。

Abstract: Zero-shot human-object interaction (HOI) detection remains a challenging
task, particularly in generalizing to unseen actions. Existing methods address
this challenge by tapping Vision-Language Models (VLMs) to access knowledge
beyond the training data. However, they either struggle to distinguish actions
involving the same object or demonstrate limited generalization to unseen
classes. In this paper, we introduce HOLa (Zero-Shot HOI Detection with
Low-Rank Decomposed VLM Feature Adaptation), a novel approach that both
enhances generalization to unseen classes and improves action distinction. In
training, HOLa decomposes VLM text features for given HOI classes via low-rank
factorization, producing class-shared basis features and adaptable weights.
These features and weights form a compact HOI representation that preserves
shared information across classes, enhancing generalization to unseen classes.
Subsequently, we refine action distinction by adapting weights for each HOI
class and introducing human-object tokens to enrich visual interaction
representations. To further distinguish unseen actions, we guide the weight
adaptation with LLM-derived action regularization. Experimental results show
that our method sets a new state-of-the-art across zero-shot HOI settings on
HICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.
Our code is available at https://github.com/ChelsieLei/HOLa.

</details>


### [116] [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/abs/2507.15803)
*Danhui Chen,Ziquan Liu,Chuxi Yang,Dan Wang,Yan Yan,Yi Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: ConformalSAM利用基础分割模型SEEM和共形预测校准，提出新型半监督语义分割框架，显著提升性能并兼容其他方法。


<details>
  <summary>Details</summary>
Motivation: 解决像素级视觉任务（如语义分割）中标注数据成本高的问题，探索基础分割模型在标签稀缺情况下的潜力。

Method: 提出了ConformalSAM框架，利用基础分割模型SEEM生成预测掩码，并通过共形预测(CP)校准不确定性，筛选高置信度标签作为监督信号。后期采用自依赖训练策略避免对SEEM生成掩码的过拟合。

Result: 在三个标准SSSS基准测试中，ConformalSAM性能优于现有方法，并可作为插件提升其他方法。

Conclusion: ConformalSAM通过结合基础分割模型和自训练策略，在半监督语义分割任务中表现出色，不仅自身性能优越，还能作为插件提升其他方法的性能。

Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive
and high-quality annotated data, which is costly to obtain. Semi-supervised
semantic segmentation (SSSS) has emerged as a solution to alleviate the
labeling burden by leveraging both labeled and unlabeled data through
self-training techniques. Meanwhile, the advent of foundational segmentation
models pre-trained on massive data, has shown the potential to generalize
across domains effectively. This work explores whether a foundational
segmentation model can address label scarcity in the pixel-level vision task as
an annotator for unlabeled images. Specifically, we investigate the efficacy of
using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual
input, to generate predictive masks for unlabeled data. To address the
shortcomings of using SEEM-generated masks as supervision, we propose
ConformalSAM, a novel SSSS framework which first calibrates the foundation
model using the target domain's labeled data and then filters out unreliable
pixel labels of unlabeled data so that only high-confidence labels are used as
supervision. By leveraging conformal prediction (CP) to adapt foundation models
to target data through uncertainty calibration, ConformalSAM exploits the
strong capability of the foundational segmentation model reliably which
benefits the early-stage learning, while a subsequent self-reliance training
strategy mitigates overfitting to SEEM-generated masks in the later training
stage. Our experiment demonstrates that, on three standard benchmarks of SSSS,
ConformalSAM achieves superior performance compared to recent SSSS methods and
helps boost the performance of those methods as a plug-in.

</details>


### [117] [DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding](https://arxiv.org/abs/2507.15569)
*Xiaoyi Bao,Chenwei Xie,Hao Tang,Tingyu Weng,Xiaofeng Wang,Yun Zheng,Xingang Wang*

Main category: cs.CV

TL;DR: DynImg通过动态图像和非关键帧时间提示，提升视频理解性能，实验显示其优于现有方法约2%。


<details>
  <summary>Details</summary>
Motivation: 传统方法将空间和时间信息分开处理，导致快速移动物体的空间信息难以准确表示，进而影响时空交互和视频理解的准确性。

Method: 提出了一种创新的视频表示方法Dynamic-Image (DynImg)，通过引入非关键帧作为时间提示，强调快速移动物体的空间区域，并采用4D视频旋转位置嵌入保持时空顺序。

Result: 实验评估显示，DynImg在多个视频理解基准测试中优于现有方法约2%。

Conclusion: DynImg方法通过引入动态图像和非关键帧作为时间提示，有效提升了多模态大语言模型在视频理解任务中的性能，实验证明其在多个基准测试中优于现有方法约2%。

Abstract: In recent years, the introduction of Multi-modal Large Language Models
(MLLMs) into video understanding tasks has become increasingly prevalent.
However, how to effectively integrate temporal information remains a critical
research focus. Traditional approaches treat spatial and temporal information
separately. Due to issues like motion blur, it is challenging to accurately
represent the spatial information of rapidly moving objects. This can lead to
temporally important regions being underemphasized during spatial feature
extraction, which in turn hinders accurate spatio-temporal interaction and
video understanding. To address this limitation, we propose an innovative video
representation method called Dynamic-Image (DynImg). Specifically, we introduce
a set of non-key frames as temporal prompts to highlight the spatial areas
containing fast-moving objects. During the process of visual feature
extraction, these prompts guide the model to pay additional attention to the
fine-grained spatial features corresponding to these regions. Moreover, to
maintain the correct sequence for DynImg, we employ a corresponding 4D video
Rotary Position Embedding. This retains both the temporal and spatial adjacency
of DynImg, helping MLLM understand the spatio-temporal order within this
combined format. Experimental evaluations reveal that DynImg surpasses the
state-of-the-art methods by approximately 2% across multiple video
understanding benchmarks, proving the effectiveness of our temporal prompts in
enhancing video comprehension.

</details>


### [118] [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/abs/2507.15807)
*Shuo Chen,Jianzhe Liu,Zhen Han,Yan Xia,Daniel Cremers,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: 论文提出DARA方法和TrueMICL数据集，解决了MLLMs忽视视觉信息的问题，显著提升了多模态上下文学习的真实能力。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大型语言模型（MLLMs）在标准视觉语言数据集上表现有所提升，但往往忽视视觉线索，过度依赖文本模式，导致多模态上下文学习（MICL）仍为单模态，限制了其实用性。如何有效增强MICL能力并可靠评估其性能尚未充分探索。

Method: 引入Dynamic Attention Reallocation（DARA）方法，通过重新平衡视觉和文本标记的注意力，鼓励模型关注视觉上下文。同时，提出了TrueMICL数据集，明确要求整合多模态信息（尤其是视觉内容）以完成任务。

Result: 实验证明，DARA和TrueMICL的整体解决方案在多模态上下文学习能力上展现出显著提升。

Conclusion: 提出的DARA方法和TrueMICL数据集显著提升了多模态上下文学习的真实能力，实验证明了其有效性。

Abstract: Multimodal Large Language Models (MLLMs), built on powerful language
backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new
tasks from a few multimodal demonstrations consisting of images, questions, and
answers. Despite showing noticeable improvement on standard vision-language
datasets, current MLLMs struggle to leverage visual information in the
demonstrations. Specifically, they tend to neglect visual cues and over-rely on
textual patterns, leading to mere text imitation rather than genuine multimodal
adaptation. This behavior makes MICL still unimodal and largely restricts its
practical utility. More importantly, this limitation is often concealed by the
improved performance on tasks that do not require understanding the visual
context. As a result, how to effectively enhance MICL ability and reliably
evaluate the MICL performance remains underexplored. To address these issues,
we first introduce Dynamic Attention Reallocation (DARA), an efficient
fine-tuning strategy that encourages models to attend to the visual context by
rebalancing attention across visual and textual tokens. In addition, we present
TrueMICL, an MICL-dedicated dataset with both support and test sets that
explicitly requires the integration of multimodal information-particularly
visual content-for correct task completion. Extensive experiments demonstrate
the effectiveness of our holistic solution, showcasing substantial improvements
in the true multimodal in-context learning capabilities. Code and datasets are
available at https://chenxshuo.github.io/true-micl-colm .

</details>


### [119] [Compress-Align-Detect: onboard change detection from unregistered images](https://arxiv.org/abs/2507.15578)
*Gabriele Inzerillo,Diego Valsesia,Aniello Fiengo,Enrico Magli*

Main category: cs.CV

TL;DR: 论文提出了一种卫星上实时变化检测的端到端框架，通过三个子模块解决了数据存储、配准和检测的挑战，实验验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 卫星图像的变化检测通常因地面站处理延迟而无法实现实时或近实时应用，因此需要将整个工作流程转移到卫星上处理。

Method: 论文提出了一个新颖的端到端框架，包含三个子模块：图像压缩、轻量级共配准和变化检测模型，专门为卫星上的实时处理设计。

Result: 实验结果表明，该框架在低功耗硬件上实现了0.7 Mpixel/s的处理速度，并在压缩率和F1分数上表现出色。

Conclusion: 该论文提出的端到端框架成功解决了卫星图像实时变化检测的挑战，通过三个相互关联的子模块（图像压缩、轻量级共配准和变化检测模型），在低功耗硬件上实现了高效处理。

Abstract: Change detection from satellite images typically incurs a delay ranging from
several hours up to days because of latency in downlinking the acquired images
and generating orthorectified image products at the ground stations; this may
preclude real- or near real-time applications. To overcome this limitation, we
propose shifting the entire change detection workflow onboard satellites. This
requires to simultaneously solve challenges in data storage, image registration
and change detection with a strict complexity constraint. In this paper, we
present a novel and efficient framework for onboard change detection that
addresses the aforementioned challenges in an end-to-end fashion with a deep
neural network composed of three interlinked submodules: (1) image compression,
tailored to minimize onboard data storage resources; (2) lightweight
co-registration of non-orthorectified multi-temporal image pairs; and (3) a
novel temporally-invariant and computationally efficient change detection
model. This is the first approach in the literature combining all these tasks
in a single end-to-end framework with the constraints dictated by onboard
processing. Experimental results compare each submodule with the current
state-of-the-art, and evaluate the performance of the overall integrated system
in realistic setting on low-power hardware. Compelling change detection results
are obtained in terms of F1 score as a function of compression rate, sustaining
a throughput of 0.7 Mpixel/s on a 15W accelerator.

</details>


### [120] [SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging](https://arxiv.org/abs/2507.15595)
*Salah Eddine Bekhouche,Gaby Maroun,Fadi Dornaika,Abdenour Hadid*

Main category: cs.CV

TL;DR: SegDT是一种基于扩散变换器的医学图像分割模型，适用于低成本硬件，性能优异且推理快速，适合实际医疗应用。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对疾病诊断和治疗规划至关重要，尤其是在皮肤病变分割领域，SegDT的提出旨在提升分割性能并适应实际医疗需求。

Method: 基于扩散变换器（DiT）和Rectified Flow的SegDT模型，专为低成本硬件设计，优化了生成质量并减少了推理步骤。

Result: 在三个基准数据集上评估，SegDT实现了最先进的性能，同时保持了快速的推理速度。

Conclusion: SegDT模型在医学图像分割领域取得了显著的性能提升，提供了快速且准确的诊断工具，适用于实际医疗应用。

Abstract: Medical image segmentation is crucial for many healthcare tasks, including
disease diagnosis and treatment planning. One key area is the segmentation of
skin lesions, which is vital for diagnosing skin cancer and monitoring
patients. In this context, this paper introduces SegDT, a new segmentation
model based on diffusion transformer (DiT). SegDT is designed to work on
low-cost hardware and incorporates Rectified Flow, which improves the
generation quality at reduced inference steps and maintains the flexibility of
standard diffusion models. Our method is evaluated on three benchmarking
datasets and compared against several existing works, achieving
state-of-the-art results while maintaining fast inference speeds. This makes
the proposed model appealing for real-world medical applications. This work
advances the performance and capabilities of deep learning models in medical
image analysis, enabling faster, more accurate diagnostic tools for healthcare
professionals. The code is made publicly available at
\href{https://github.com/Bekhouche/SegDT}{GitHub}.

</details>


### [121] [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://arxiv.org/abs/2507.15852)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Songxin He,Jianfan Lin,Junsong Tang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: SeC是一种概念驱动的视频对象分割框架，利用LVLMs构建对象概念表示，显著提升了复杂场景下的分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频对象分割技术过于依赖外观匹配，缺乏类似人类的概念理解能力，导致在剧烈视觉变化、遮挡和复杂场景变化中表现不佳。

Method: SeC采用了一种概念驱动的分割框架，利用LVLMs整合多帧视觉线索，构建鲁棒的概念先验。在推理阶段，SeC基于处理过的帧形成目标的全面语义表示，并进行自适应的语义推理与特征匹配平衡。

Result: SeC在SeCVOS基准测试中比SAM 2.1提高了11.8个百分点，建立了概念感知视频对象分割的新标杆。

Conclusion: SeC框架通过结合概念驱动的分割方法和大规模视觉语言模型（LVLMs），在视频对象分割任务中实现了显著的性能提升，特别是在处理复杂场景变化和外观变化时表现出色。

Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring
models to track and segment target objects across video frames. Despite notable
advances with recent efforts, current techniques still lag behind human
capabilities in handling drastic visual variations, occlusions, and complex
scene changes. This limitation arises from their reliance on appearance
matching, neglecting the human-like conceptual understanding of objects that
enables robust identification across temporal dynamics. Motivated by this gap,
we propose Segment Concept (SeC), a concept-driven segmentation framework that
shifts from conventional feature matching to the progressive construction and
utilization of high-level, object-centric representations. SeC employs Large
Vision-Language Models (LVLMs) to integrate visual cues across diverse frames,
constructing robust conceptual priors. During inference, SeC forms a
comprehensive semantic representation of the target based on processed frames,
realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively
balances LVLM-based semantic reasoning with enhanced feature matching,
dynamically adjusting computational efforts based on scene complexity. To
rigorously assess VOS methods in scenarios demanding high-level conceptual
reasoning and robust semantic understanding, we introduce the Semantic Complex
Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160
manually annotated multi-scenario videos designed to challenge models with
substantial appearance variations and dynamic scene transformations. In
particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,
establishing a new state-of-the-art in concept-aware video object segmentation.

</details>


### [122] [SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting](https://arxiv.org/abs/2507.15602)
*Zihui Gao,Jia-Wang Bian,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出混合SDF和3DGS的方法，结合两者优势，在表面重建和新视角合成上超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏视图图像表面重建和新视角渲染的挑战，SDF方法在细节上表现不佳，而3DGS方法缺乏全局几何一致性。

Method: 提出了一种结合SDF和3DGS的混合方法，利用SDF捕捉粗略几何以增强3DGS渲染，同时通过3DGS新渲染的图像优化SDF的细节。

Result: 在DTU和MobileBrick数据集上实现了优于现有技术的表面重建和新视角合成效果。

Conclusion: 该方法在DTU和MobileBrick数据集上超越了现有技术，在表面重建和新视角合成方面表现出色。

Abstract: Surface reconstruction and novel view rendering from sparse-view images are
challenging. Signed Distance Function (SDF)-based methods struggle with fine
details, while 3D Gaussian Splatting (3DGS)-based approaches lack global
geometry coherence. We propose a novel hybrid method that combines the
strengths of both approaches: SDF captures coarse geometry to enhance
3DGS-based rendering, while newly rendered images from 3DGS refine the details
of SDF for accurate surface reconstruction. As a result, our method surpasses
state-of-the-art approaches in surface reconstruction and novel view synthesis
on the DTU and MobileBrick datasets. Code will be released at
https://github.com/Gaozihui/SurfaceSplat.

</details>


### [123] [CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation](https://arxiv.org/abs/2507.15606)
*Ru Jia,Xiaozhuang Ma,Jianji Wang,Nanning Zheng*

Main category: cs.CV

TL;DR: 提出 CylinderPlane，基于圆柱坐标系的隐式表示，解决 Tri-plane 的多面伪影问题，实现高质量 360° 图像合成，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: Tri-plane 表示因对称区域共享特征导致多面伪影，限制了 360° 视图图像的生成能力。本文旨在通过圆柱坐标系消除特征模糊问题，确保多视角一致性。

Method: 采用圆柱坐标系构建隐式表示，引入嵌套圆柱结构以捕捉多尺度特征，适用于复杂几何和不同分辨率场景。

Result: 在合成数据集和真实场景图像上的实验表明，CylinderPlane 在性能上优于现有方法。

Conclusion: CylinderPlane 提出了一种基于圆柱坐标系的新型隐式表示方法，有效解决了 Tri-plane 表示中的特征模糊问题，实现了高质量、无伪影的 360° 图像合成，并通过嵌套圆柱表示进一步提升了模型的适应性和细节捕捉能力。

Abstract: While the proposal of the Tri-plane representation has advanced the
development of the 3D-aware image generative models, problems rooted in its
inherent structure, such as multi-face artifacts caused by sharing the same
features in symmetric regions, limit its ability to generate 360$^\circ$ view
images. In this paper, we propose CylinderPlane, a novel implicit
representation based on Cylindrical Coordinate System, to eliminate the feature
ambiguity issue and ensure multi-view consistency in 360$^\circ$. Different
from the inevitable feature entanglement in Cartesian coordinate-based
Tri-plane representation, the cylindrical coordinate system explicitly
separates features at different angles, allowing our cylindrical representation
possible to achieve high-quality, artifacts-free 360$^\circ$ image synthesis.
We further introduce the nested cylinder representation that composites
multiple cylinders at different scales, thereby enabling the model more
adaptable to complex geometry and varying resolutions. The combination of
cylinders with different resolutions can effectively capture more critical
locations and multi-scale features, greatly facilitates fine detail learning
and robustness to different resolutions. Moreover, our representation is
agnostic to implicit rendering methods and can be easily integrated into any
neural rendering pipeline. Extensive experiments on both synthetic dataset and
unstructured in-the-wild images demonstrate that our proposed representation
achieves superior performance over previous methods.

</details>


### [124] [A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications](https://arxiv.org/abs/2507.15628)
*Shanjiang Tang,Rui Huang,Hsinyu Luo,Chunjiang Wang,Ce Yu,Yusen Li,Hao Fu,Chao Sun,and Jian Xiao*

Main category: cs.CV

TL;DR: 本文综述了基于DNN的视频分析效率优化技术，涵盖硬件、数据处理等多方面，并讨论了当前挑战。


<details>
  <summary>Details</summary>
Motivation: 近年来视频数据的爆炸性增长对视频分析提出了更高要求，准确性和效率是关键问题。尽管DNN已广泛用于保证准确性，但其在视频分析中的效率提升仍是开放性问题。

Method: 采用自底向上的方式组织现有方法，涵盖硬件支持、数据处理和操作部署等多个视角。

Result: 本文提供了针对DNN在视频分析中效率优化的全面综述，填补了现有文献主要关注准确性优化的空白。

Conclusion: 本文分析了基于DNN的视频分析性能优化中的问题和挑战，并提出了优化框架。

Abstract: The explosive growth of video data in recent years has brought higher demands
for video analytics, where accuracy and efficiency remain the two primary
concerns. Deep neural networks (DNNs) have been widely adopted to ensure
accuracy; however, improving their efficiency in video analytics remains an
open challenge. Different from existing surveys that make summaries of
DNN-based video mainly from the accuracy optimization aspect, in this survey,
we aim to provide a thorough review of optimization techniques focusing on the
improvement of the efficiency of DNNs in video analytics. We organize existing
methods in a bottom-up manner, covering multiple perspectives such as hardware
support, data processing, operational deployment, etc. Finally, based on the
optimization framework and existing works, we analyze and discuss the problems
and challenges in the performance optimization of DNN-based video analytics.

</details>


### [125] [Experimenting active and sequential learning in a medieval music manuscript](https://arxiv.org/abs/2507.15633)
*Sachin Sharma,Federico Simonetta,Michele Flammini*

Main category: cs.CV

TL;DR: 研究提出了一种结合主动学习和顺序学习的方法，用于中世纪音乐手稿的对象检测和布局识别，发现基于不确定性的AL在当前数据集上效果有限。


<details>
  <summary>Details</summary>
Motivation: 解决光学音乐识别（OMR）中标注数据稀缺和历史手稿复杂性的挑战。

Method: 利用YOLOv8，结合主动学习（AL）和顺序学习（SL），选择预测置信度最低的样本进行迭代标注和重新训练。

Result: 实验结果表明，与全监督训练相比，该方法能以更少的标注样本达到相当的准确率。

Conclusion: 在研究中发现，基于不确定性的主动学习方法在当前手稿中效果不佳，建议在数据稀缺情况下采用更实用的方法。

Abstract: Optical Music Recognition (OMR) is a cornerstone of music digitization
initiatives in cultural heritage, yet it remains limited by the scarcity of
annotated data and the complexity of historical manuscripts. In this paper, we
present a preliminary study of Active Learning (AL) and Sequential Learning
(SL) tailored for object detection and layout recognition in an old medieval
music manuscript. Leveraging YOLOv8, our system selects samples with the
highest uncertainty (lowest prediction confidence) for iterative labeling and
retraining. Our approach starts with a single annotated image and successfully
boosts performance while minimizing manual labeling. Experimental results
indicate that comparable accuracy to fully supervised training can be achieved
with significantly fewer labeled examples. We test the methodology as a
preliminary investigation on a novel dataset offered to the community by the
Anonymous project, which studies laude, a poetical-musical genre spread across
Italy during the 12th-16th Century. We show that in the manuscript at-hand,
uncertainty-based AL is not effective and advocates for more usable methods in
data-scarcity scenarios.

</details>


### [126] [Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2507.15652)
*Haoran Zhou,Zihan Zhang,Hao Chen*

Main category: cs.CV

TL;DR: EVA是一种无需训练的方法，通过动态选择中间层提取视觉事实知识，有效减少多模态大语言模型中的对象幻觉。


<details>
  <summary>Details</summary>
Motivation: MLLMs在生成内容时存在对象幻觉问题，现有方法未能清晰解释先验知识如何在中间层抑制视觉信息。观察到视觉事实知识与中间层概率分布的差异趋势相似，因此提出EVA方法。

Method: 提出了一种无需训练的EVA方法，通过对比原始输入和纯文本输入的中间层输出分布，动态选择最具视觉事实信息的层，并将其知识整合到最终层以修正输出。

Result: 在广泛使用的基准测试中，EVA显著降低了幻觉率，优于基线方法。

Conclusion: EVA方法通过动态选择中间层并提取视觉事实知识，显著减少了MLLMs中的幻觉率，验证了其在多模态大语言模型中的有效性。

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides by
combining visual recognition and language understanding to generate content
that is both coherent and contextually accurate. However, MLLMs continue to
struggle with object hallucinations, where models produce seemingly plausible
but factually incorrect outputs, including objects that do not exist in the
image. Recent work has revealed that the prior knowledge in MLLMs significantly
suppresses visual information in deep layers, causing hallucinatory outputs.
However, how these priors suppress visual information at the intermediate layer
stage in MLLMs remains unclear. We observe that visual factual knowledge and
the differences between intermediate-layer prior/original probability
distributions show similar evolutionary trends in intermediate layers.
Motivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a
simple, training-free method that dynamically selects intermediate layers with
the most significant visual factual information. By contrasting the output
distributions of the selected layer derived from the original input and
pure-text input, EVA extracts visual factual knowledge and proportionally
incorporates it into the final layer to correct the output logits. Importantly,
EVA is model-agnostic, seamlessly integrates with various classic decoding
strategies, and is applicable across different MLLMs. We validate EVA on
widely-used benchmarks, and the results show that it significantly reduces
hallucination rates compared to baseline methods, underscoring its
effectiveness in mitigating hallucinations.

</details>


### [127] [HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark](https://arxiv.org/abs/2507.15655)
*Aniket Pal,Ajoy Mondal,Minesh Mathew,C. V. Jawahar*

Main category: cs.CV

TL;DR: HW-MLVQA是一个创新的多语言手写文档视觉问答基准，包含丰富数据集和评估框架，旨在推动该领域研究。


<details>
  <summary>Details</summary>
Motivation: 当前的多语言视觉问答模型在处理多样化手写文档时未能充分利用其潜力，HW-MLVQA旨在填补这一空白。

Method: HW-MLVQA包含1,600页手写文档和2,400个问答对，并提供涵盖文本、图像及图文结合三种模态的评估框架。

Result: HW-MLVQA提供了一个严格的基准评估框架，支持对专有和开源OCR模型在无真实文本转录的真实场景中的评估。

Conclusion: HW-MLVQA基准旨在推动多语言手写文档理解的关键进展，促进该专业领域的创新和学术研究。

Abstract: The proliferation of MultiLingual Visual Question Answering (MLVQA)
benchmarks augments the capabilities of large language models (LLMs) and
multi-modal LLMs, thereby enabling them to adeptly capture the intricate
linguistic subtleties and visual complexities inherent across diverse
languages. Despite its potential, the current MLVQA model struggles to fully
utilize its capabilities when dealing with the extensive variety of handwritten
documents. This article delineates HW-MLVQA, an avant-garde VQA benchmark
meticulously crafted to mitigate the dearth of authentic Multilingual
Handwritten document comprehension. HW-MLVQA encompasses an extensive
collection of 1,600 handwritten Pages complemented by 2,400 question-answers.
Furthermore, it provides a robust benchmark evaluation framework spanning three
distinct modalities: text, image, and an integrated image & text modality. To
simulate authentic real-world contexts devoid of ground truth textual
transcriptions, we facilitates a rigorous assessment of proprietary and
open-source OCR models. The benchmark aspires to facilitate pivotal
advancements in multilingual handwritten document interpretation, fostering
innovation and scholarly inquiry within this specialized domain.

</details>


### [128] [Visual-Language Model Knowledge Distillation Method for Image Quality Assessment](https://arxiv.org/abs/2507.15680)
*Yongkang Hou,Jiarun Song*

Main category: cs.CV

TL;DR: 提出基于CLIP的视觉-语言模型知识蒸馏方法，通过质量分级提示和模态自适应策略，显著降低复杂度并提升IQA性能。


<details>
  <summary>Details</summary>
Motivation: 针对CLIP在IQA任务中参数负担过重和局部失真特征识别能力不足的问题。

Method: 研究设计了质量分级提示模板来引导CLIP输出质量评分，并微调CLIP以增强其在IQA任务中的能力，最后提出了一种模态自适应知识蒸馏策略。

Result: 在多个IQA数据集上的实验结果表明，该方法显著降低了模型复杂度且性能优于现有方法。

Conclusion: 该研究提出的视觉-语言模型知识蒸馏方法在显著降低模型复杂度的同时，性能优于现有IQA方法，展示了实际部署的强潜力。

Abstract: Image Quality Assessment (IQA) is a core task in computer vision. Multimodal
methods based on vision-language models, such as CLIP, have demonstrated
exceptional generalization capabilities in IQA tasks. To address the issues of
excessive parameter burden and insufficient ability to identify local distorted
features in CLIP for IQA, this study proposes a visual-language model knowledge
distillation method aimed at guiding the training of models with architectural
advantages using CLIP's IQA knowledge. First, quality-graded prompt templates
were designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned
to enhance its capabilities in IQA tasks. Finally, a modality-adaptive
knowledge distillation strategy is proposed to achieve guidance from the CLIP
teacher model to the student model. Our experiments were conducted on multiple
IQA datasets, and the results show that the proposed method significantly
reduces model complexity while outperforming existing IQA methods,
demonstrating strong potential for practical deployment.

</details>


### [129] [Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing](https://arxiv.org/abs/2507.15683)
*Boni Hu,Zhenyu Xia,Lin Chen,Pengcheng Han,Shuhui Bu*

Main category: cs.CV

TL;DR: 提出基于3D高斯点渲染的双层次重定位框架Hi²-GSLoc，显著提升遥感场景下的视觉定位性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉重定位方法在精度与计算复杂度之间的权衡问题，特别是在大尺度遥感场景中的挑战。

Method: 采用3D高斯点渲染（3DGS）作为场景表示，提出双层次重定位框架Hi²-GSLoc，包括稀疏阶段（高斯特定渲染感知采样和地标引导检测器）和密集阶段（粗到细的密集光栅化匹配）。

Result: 方法在定位精度、召回率和计算效率上表现优异，并能有效过滤不可靠的位姿估计。

Conclusion: 该方法在模拟数据、公开数据集和实际飞行实验中表现出色，验证了其在遥感应用中的有效性。

Abstract: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera
pose from query images, is fundamental to remote sensing and UAV applications.
Existing methods face inherent trade-offs: image-based retrieval and pose
regression approaches lack precision, while structure-based methods that
register queries to Structure-from-Motion (SfM) models suffer from
computational complexity and limited scalability. These challenges are
particularly pronounced in remote sensing scenarios due to large-scale scenes,
high altitude variations, and domain gaps of existing visual priors. To
overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel
scene representation that compactly encodes both 3D geometry and appearance. We
introduce $\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework
that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting
the rich semantic information and geometric constraints inherent in Gaussian
primitives. To handle large-scale remote sensing scenarios, we incorporate
partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic
memory management strategies. Our approach consists of two stages: (1) a sparse
stage featuring a Gaussian-specific consistent render-aware sampling strategy
and landmark-guided detector for robust and accurate initial pose estimation,
and (2) a dense stage that iteratively refines poses through coarse-to-fine
dense rasterization matching while incorporating reliability verification.
Through comprehensive evaluation on simulation data, public datasets, and real
flight experiments, we demonstrate that our method delivers competitive
localization accuracy, recall rate, and computational efficiency while
effectively filtering unreliable pose estimates. The results confirm the
effectiveness of our approach for practical remote sensing applications.

</details>


### [130] [DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2507.15690)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: DWTGS通过小波空间损失改进稀疏视图3D高斯泼溅，优于傅里叶方法，减少高频幻觉。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3D高斯泼溅在重建高质量新视图时面临挑战，容易过拟合训练视图的高频细节，现有频率正则化方法依赖傅里叶变换，参数调优困难且偏向有害高频学习。

Method: 提出DWTGS框架，利用小波空间损失提供额外的空间监督，仅在多个DWT级别监督低频LL子带，并以自监督方式对高频HH子带实施稀疏性。

Result: 实验证明，DWTGS在多个基准测试中 consistently 优于基于傅里叶变换的同类方法，低频中心策略提高了泛化能力并减少高频幻觉。

Conclusion: DWTGS框架通过小波空间损失重新思考频率正则化，有效提升了稀疏视图3D高斯泼溅的重建质量，减少了高频幻觉，并优于基于傅里叶变换的方法。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in
reconstructing high-quality novel views, as it often overfits to the
widely-varying high-frequency (HF) details of the sparse training views. While
frequency regularization can be a promising approach, its typical reliance on
Fourier transforms causes difficult parameter tuning and biases towards
detrimental HF learning. We propose DWTGS, a framework that rethinks frequency
regularization by leveraging wavelet-space losses that provide additional
spatial supervision. Specifically, we supervise only the low-frequency (LF) LL
subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband
in a self-supervised manner. Experiments across benchmarks show that DWTGS
consistently outperforms Fourier-based counterparts, as this LF-centric
strategy improves generalization and reduces HF hallucinations.

</details>


### [131] [Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation](https://arxiv.org/abs/2507.15709)
*Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出一种高效FIQA方法，通过教师-学生模型两阶段训练，学生模型以低计算成本匹配教师模型性能，并赢得ICCV 2025竞赛。


<details>
  <summary>Details</summary>
Motivation: 尽管FIQA已取得显著进展，但其算法的计算复杂度仍是实际部署中的关键问题，因此需要开发高效且易部署的FIQA方法。

Method: 采用两阶段方法：1) 训练强大的教师模型（结合自训练策略和伪标签生成）；2) 从教师模型中蒸馏出轻量级学生模型（结合原始标签、教师模型生成的伪标签和增强教师模型生成的伪标签）。

Result: 学生模型在极低计算开销下性能与教师模型相当，并在ICCV 2025 VQualA FIQA挑战赛中夺冠。

Conclusion: 该方法通过两阶段训练（教师模型和学生模型）实现了高效的人脸图像质量评估（FIQA），学生模型在极低计算开销下达到与教师模型相当的性能，并在ICCV 2025 VQualA FIQA挑战赛中取得第一名。

Abstract: Face image quality assessment (FIQA) is essential for various face-related
applications. Although FIQA has been extensively studied and achieved
significant progress, the computational complexity of FIQA algorithms remains a
key concern for ensuring scalability and practical deployment in real-world
systems. In this paper, we aim to develop a computationally efficient FIQA
method that can be easily deployed in real-world applications. Specifically,
our method consists of two stages: training a powerful teacher model and
distilling a lightweight student model from it. To build a strong teacher
model, we adopt a self-training strategy to improve its capacity. We first
train the teacher model using labeled face images, then use it to generate
pseudo-labels for a set of unlabeled images. These pseudo-labeled samples are
used in two ways: (1) to distill knowledge into the student model, and (2) to
combine with the original labeled images to further enhance the teacher model
through self-training. The enhanced teacher model is used to further
pseudo-label another set of unlabeled images for distilling the student models.
The student model is trained using a combination of labeled images,
pseudo-labeled images from the original teacher model, and pseudo-labeled
images from the enhanced teacher model. Experimental results demonstrate that
our student model achieves comparable performance to the teacher model with an
extremely low computational overhead. Moreover, our method achieved first place
in the ICCV 2025 VQualA FIQA Challenge. The code is available at
https://github.com/sunwei925/Efficient-FIQA.git.

</details>


### [132] [A Practical Investigation of Spatially-Controlled Image Generation with Transformers](https://arxiv.org/abs/2507.15724)
*Guoxuan Xia,Harleen Hanspal,Petru-Daniel Tudosiu,Shifeng Zhang,Sarah Parisot*

Main category: cs.CV

TL;DR: 本文系统比较了transformer在空间控制图像生成中的方法，发现控制token预填充是高效基线，采样增强技术可提升一致性，适配器方法在有限数据下保持质量但一致性较差。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成模型在空间控制方面的研究缺乏详细和公平的科学比较，不同训练数据、模型架构和生成范式使得性能因素难以分离。本文旨在为实践者提供清晰的跨生成范式的见解。

Method: 在ImageNet数据集上对基于扩散/流和自回归（AR）模型进行了控制实验，研究了控制token预填充、分类器自由引导扩展和softmax截断等方法。

Result: 确立了控制token预填充作为一种简单、通用且高性能的基线方法，并展示了采样时间增强技术（如扩展分类器自由引导和softmax截断）对控制-生成一致性的显著影响。

Conclusion: 本文通过系统实验，为基于transformer的空间控制图像生成提供了清晰的实践指导，澄清了文献中的混淆点，并填补了知识空白。

Abstract: Enabling image generation models to be spatially controlled is an important
area of research, empowering users to better generate images according to their
own fine-grained specifications via e.g. edge maps, poses. Although this task
has seen impressive improvements in recent times, a focus on rapidly producing
stronger models has come at the cost of detailed and fair scientific
comparison. Differing training data, model architectures and generation
paradigms make it difficult to disentangle the factors contributing to
performance. Meanwhile, the motivations and nuances of certain approaches
become lost in the literature. In this work, we aim to provide clear takeaways
across generation paradigms for practitioners wishing to develop
transformer-based systems for spatially-controlled generation, clarifying the
literature and addressing knowledge gaps. We perform controlled experiments on
ImageNet across diffusion-based/flow-based and autoregressive (AR) models.
First, we establish control token prefilling as a simple, general and
performant baseline approach for transformers. We then investigate previously
underexplored sampling time enhancements, showing that extending
classifier-free guidance to control, as well as softmax truncation, have a
strong impact on control-generation consistency. Finally, we re-clarify the
motivation of adapter-based approaches, demonstrating that they mitigate
"forgetting" and maintain generation quality when trained on limited downstream
data, but underperform full training in terms of generation-control
consistency. Code will be released upon publication.

</details>


### [133] [TokensGen: Harnessing Condensed Tokens for Long Video Generation](https://arxiv.org/abs/2507.15728)
*Wenqi Ouyang,Zeqi Xiao,Danni Yang,Yifan Zhou,Shuai Yang,Lei Yang,Jianlou Si,Xingang Pan*

Main category: cs.CV

TL;DR: TokensGen通过两阶段框架和压缩令牌技术，解决了长视频生成中的内存和一致性挑战，实验证明其有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决基于扩散的生成模型在生成长视频时面临的内存瓶颈和长期不一致性问题。

Method: TokensGen是一个两阶段框架，通过训练To2V（Token-to-Video）和T2To（Text-to-Token）模型，结合自适应FIFO-Diffusion策略，分解长视频生成任务为三个核心部分：内部片段语义控制、长期一致性控制和片段间平滑过渡。

Result: 实验结果表明，该方法在不增加过高计算开销的情况下，显著提高了长期时间和内容的一致性。

Conclusion: 通过利用压缩令牌和预训练的短视频模型，TokensGen为长视频生成提供了一种可扩展、模块化的解决方案，为叙事、电影制作和沉浸式模拟开辟了新的可能性。

Abstract: Generating consistent long videos is a complex challenge: while
diffusion-based generative models generate visually impressive short clips,
extending them to longer durations often leads to memory bottlenecks and
long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage
framework that leverages condensed tokens to address these issues. Our method
decomposes long video generation into three core tasks: (1) inner-clip semantic
control, (2) long-term consistency control, and (3) inter-clip smooth
transition. First, we train To2V (Token-to-Video), a short video diffusion
model guided by text and video tokens, with a Video Tokenizer that condenses
short clips into semantically rich tokens. Second, we introduce T2To
(Text-to-Token), a video token diffusion transformer that generates all tokens
at once, ensuring global consistency across clips. Finally, during inference,
an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,
reducing boundary artifacts and enhancing smooth transitions. Experimental
results demonstrate that our approach significantly enhances long-term temporal
and content coherence without incurring prohibitive computational overhead. By
leveraging condensed tokens and pre-trained short video models, our method
provides a scalable, modular solution for long video generation, opening new
possibilities for storytelling, cinematic production, and immersive
simulations. Please see our project page at
https://vicky0522.github.io/tokensgen-webpage/ .

</details>


### [134] [Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS](https://arxiv.org/abs/2507.15748)
*Jisu Shin,Richard Shaw,Seunghyun Shin,Anton Pelykh,Zhensong Zhang,Hae-Gon Jeon,Eduardo Perez-Pellitero*

Main category: cs.CV

TL;DR: 提出基于Transformer的预测双边网格方法，校正多视角光度变化，提升3D高斯泼溅重建质量，无需场景特定训练。


<details>
  <summary>Details</summary>
Motivation: 现代相机流水线的广泛设备处理（如曝光调整、白平衡和色彩校正）虽有益于单张图像，但会引入视角间的光度不一致性，破坏多视角一致性并降低新视角合成的质量。

Method: 提出了一种基于Transformer的方法，预测空间自适应的双边网格以校正多视角一致的光度变化。

Result: 实验表明，该方法在重建保真度和收敛速度上优于或匹配现有的场景特定优化方法。

Conclusion: 该方法通过预测空间自适应的双边网格来校正多视角一致的光度变化，无需场景特定重新训练即可实现跨场景的鲁棒泛化，同时在3D高斯泼溅流程中提升了重建质量并保持了高训练效率。

Abstract: Modern camera pipelines apply extensive on-device processing, such as
exposure adjustment, white balance, and color correction, which, while
beneficial individually, often introduce photometric inconsistencies across
views. These appearance variations violate multi-view consistency and degrade
the quality of novel view synthesis. Joint optimization of scene
representations and per-image appearance embeddings has been proposed to
address this issue, but at the cost of increased computational complexity and
slower training. In this work, we propose a transformer-based method that
predicts spatially adaptive bilateral grids to correct photometric variations
in a multi-view consistent manner, enabling robust cross-scene generalization
without the need for scene-specific retraining. By incorporating the learned
grids into the 3D Gaussian Splatting pipeline, we improve reconstruction
quality while maintaining high training efficiency. Extensive experiments show
that our approach outperforms or matches existing scene-specific optimization
methods in reconstruction fidelity and convergence speed.

</details>


### [135] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: HDF框架通过DAM和DSM模块，解决了动态面部表情识别中的样本异质性问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在多源数据和个体表达变异性导致的样本异质性下性能下降的问题。

Method: 设计了两个即插即用模块：时间-频率分布注意力模块（DAM）用于增强时间-频率建模，分布感知缩放模块（DSM）用于动态平衡分类和对比损失。

Result: 在DFEW和FERV39k数据集上，HDF显著提高了识别准确性和鲁棒性，并实现了优越的加权平均召回率（WAR）和非加权平均召回率（UAR）。

Conclusion: 提出的HDF框架通过时间-频率分布注意力模块（DAM）和分布感知缩放模块（DSM），显著提升了动态面部表情识别的准确性和鲁棒性，并在DFEW和FERV39k数据集上验证了其优越性。

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [136] [Label tree semantic losses for rich multi-class medical image segmentation](https://arxiv.org/abs/2507.15777)
*Junwen Wang,Oscar MacCormac,William Rochford,Aaron Kujawa,Jonathan Shapey,Tom Vercauteren*

Main category: cs.CV

TL;DR: 提出两种基于树结构的语义损失函数，优化医学图像分割性能，实验证明其在复杂标签任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法对所有错误同等惩罚，未能利用标签空间的类间语义关系，特别是在标签复杂和丰富的情况下。

Method: 提出了两种基于树结构的语义损失函数，并整合到稀疏标注训练的现有方法中。

Result: 在头部MRI全脑分割和神经外科高光谱图像场景理解任务中，所提方法均达到了最先进的性能。

Conclusion: 提出的两种基于树结构的语义损失函数在医学图像分割任务中表现出色，达到了最先进的性能。

Abstract: Rich and accurate medical image segmentation is poised to underpin the next
generation of AI-defined clinical practice by delineating critical anatomy for
pre-operative planning, guiding real-time intra-operative navigation, and
supporting precise post-operative assessment. However, commonly used learning
methods for medical and surgical imaging segmentation tasks penalise all errors
equivalently and thus fail to exploit any inter-class semantics in the labels
space. This becomes particularly problematic as the cardinality and richness of
labels increases to include subtly different classes. In this work, we propose
two tree-based semantic loss functions which take advantage of a hierarchical
organisation of the labels. We further incorporate our losses in a recently
proposed approach for training with sparse, background-free annotations to
extend the applicability of our proposed losses. Extensive experiments are
reported on two medical and surgical image segmentation tasks, namely head MRI
for whole brain parcellation (WBP) with full supervision and neurosurgical
hyperspectral imaging (HSI) for scene understanding with sparse annotations.
Results demonstrate that our proposed method reaches state-of-the-art
performance in both cases.

</details>


### [137] [Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation](https://arxiv.org/abs/2507.15793)
*Ghassen Baklouti,Julio Silva-Rodríguez,Jose Dolz,Houda Bahig,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 提出了一种动态调整内在秩的PEFT方法，通过l_1正则化和近端优化器自动选择秩，显著提升了医学图像分割的性能。


<details>
  <summary>Details</summary>
Motivation: 解决LoRA方法中固定秩难以适应不同医学图像下游任务独特复杂性和需求的问题。

Method: 通过引入l_1稀疏正则化器到损失函数中，并采用近端优化器动态调整内在秩，实现了任务自适应秩的自动选择。

Result: 在少量样本微调设置中，该方法显著优于标准LoRA和其他PEFT方法，特别是在基础器官和新器官任务中。

Conclusion: 该方法在医学图像分割任务中表现出色，显著提升了性能，并展示了对次优秩初始化的鲁棒性。

Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is
increasingly attracting interest in medical imaging due to its effectiveness
and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)
is a notable approach based on the assumption that the adaptation inherently
occurs in a low-dimensional subspace. While it has shown good performance, its
implementation requires a fixed and unalterable rank, which might be
challenging to select given the unique complexities and requirements of each
medical imaging downstream task. Inspired by advancements in natural image
processing, we introduce a novel approach for medical image segmentation that
dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank
representation of the trainable weight matrices as a singular value
decomposition, we introduce an l_1 sparsity regularizer to the loss function,
and tackle it with a proximal optimizer. The regularizer could be viewed as a
penalty on the decomposition rank. Hence, its minimization enables to find
task-adapted ranks automatically. Our method is evaluated in a realistic
few-shot fine-tuning setting, where we compare it first to the standard LoRA
and then to several other PEFT methods across two distinguishable tasks: base
organs and novel organs. Our extensive experiments demonstrate the significant
performance improvements driven by our method, highlighting its efficiency and
robustness against suboptimal rank initialization. Our code is publicly
available: https://github.com/ghassenbaklouti/ARENA

</details>


### [138] [Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models](https://arxiv.org/abs/2507.15798)
*Lilian Hollard,Lucas Mohimont,Nathalie Gaveau,Luiz-Angelo Steffenel*

Main category: cs.CV

TL;DR: 论文研究了低参数深度神经网络的性能，提出通过限制特征图中的干扰来提升扩展性和准确性，并设计了NoDepth Bottleneck架构，在ImageNet上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是解决特征图中的干扰现象，这种现象与叠加相关，即神经元同时编码多个特征。研究旨在通过限制干扰来提升低参数规模神经网络的性能。

Method: 论文研究了最先进的低参数深度神经网络在计算机视觉中的性能，重点关注瓶颈架构及其在使用超线性激活函数时的行为。通过分析不同瓶颈架构，识别了减少干扰的关键设计元素。

Result: 研究结果表明，限制干扰可以显著提升低参数规模（小于150万参数）神经网络的扩展性和准确性。提出的NoDepth Bottleneck架构在ImageNet数据集上展示了稳健的扩展准确性。

Conclusion: 该论文的结论是，通过限制特征图中的干扰，可以显著提升低参数规模（小于150万参数）神经网络的扩展性和准确性。提出的NoDepth Bottleneck架构基于实验的机理洞察，在ImageNet数据集上展示了稳健的扩展准确性。

Abstract: The paper investigates the performance of state-of-the-art low-parameter deep
neural networks for computer vision, focusing on bottleneck architectures and
their behavior using superlinear activation functions. We address interference
in feature maps, a phenomenon associated with superposition, where neurons
simultaneously encode multiple characteristics. Our research suggests that
limiting interference can enhance scaling and accuracy in very low-scaled
networks (under 1.5M parameters). We identify key design elements that reduce
interference by examining various bottleneck architectures, leading to a more
efficient neural network. Consequently, we propose a proof-of-concept
architecture named NoDepth Bottleneck built on mechanistic insights from our
experiments, demonstrating robust scaling accuracy on the ImageNet dataset.
These findings contribute to more efficient and scalable neural networks for
the low-parameter range and advance the understanding of bottlenecks in
computer vision. https://caiac.pubpub.org/pub/3dh6rsel

</details>


### [139] [Diffusion models for multivariate subsurface generation and efficient probabilistic inversion](https://arxiv.org/abs/2507.15809)
*Roberto Miele,Niklas Linde*

Main category: cs.CV

TL;DR: 扩散模型在多元地下建模和概率反演中表现优异，改进后的方法显著提升统计稳健性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在深度生成建模任务中表现出稳定的训练和最先进的性能，但在多元地下建模和概率反演中的应用尚未充分探索。本研究旨在验证扩散模型在此类任务中的优势。

Method: 研究提出对Chung等人（2023）的扩散后验采样方法的改进，包括引入考虑扩散模型中固有噪声污染的似然近似。通过多元地质场景（岩相和相关声阻抗）评估性能，并展示使用局部硬数据（测井数据）和非线性地球物理数据（全叠加地震数据）的条件建模。

Result: 测试显示，与原始方法相比，改进后的方法显著提高了统计稳健性，增强了对后验概率密度函数的采样，并降低了计算成本。反演过程内置于扩散过程中，速度优于需要生成模型外循环的其他方法（如马尔可夫链蒙特卡洛）。

Conclusion: 扩散模型在多元地下建模和概率反演中表现出色，提供了统计稳健性增强、后验概率密度函数采样改进以及计算成本降低的优势。该方法适用于硬数据和间接条件数据的单独或同时使用。

Abstract: Diffusion models offer stable training and state-of-the-art performance for
deep generative modeling tasks. Here, we consider their use in the context of
multivariate subsurface modeling and probabilistic inversion. We first
demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In
diffusion modeling, the generative process involves a comparatively large
number of time steps with update rules that can be modified to account for
conditioning data. We propose different corrections to the popular Diffusion
Posterior Sampling approach by Chung et al. (2023). In particular, we introduce
a likelihood approximation accounting for the noise-contamination that is
inherent in diffusion modeling. We assess performance in a multivariate
geological scenario involving facies and correlated acoustic impedance.
Conditional modeling is demonstrated using both local hard data (well logs) and
nonlinear geophysics (fullstack seismic data). Our tests show significantly
improved statistical robustness, enhanced sampling of the posterior probability
density function and reduced computational costs, compared to the original
approach. The method can be used with both hard and indirect conditioning data,
individually or simultaneously. As the inversion is included within the
diffusion process, it is faster than other methods requiring an outer-loop
around the generative model, such as Markov chain Monte Carlo.

</details>


### [140] [Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models](https://arxiv.org/abs/2507.15824)
*Enes Sanli,Baris Sarper Tezcan,Aykut Erdem,Erkut Erdem*

Main category: cs.CV

TL;DR: PhysVidBench是一个评估T2V模型物理常识能力的基准，包含383个提示，采用三阶段间接评估策略，填补当前评估空白。


<details>
  <summary>Details</summary>
Motivation: 当前的T2V生成模型在基本物理常识方面表现不佳，常违反直觉预期。PhysVidBench旨在评估T2V系统的物理推理能力。

Method: 采用三阶段评估流程：1）从提示中制定基于物理的问题；2）使用视觉语言模型为生成的视频生成字幕；3）要求语言模型仅基于字幕回答涉及物理的问题。

Result: PhysVidBench包含383个精心设计的提示，涵盖工具使用、材料属性和程序交互等领域，并通过间接策略避免了直接视频评估中的常见幻觉问题。

Conclusion: PhysVidBench提供了一个结构化和可解释的框架，用于评估生成视频模型中的物理常识能力，填补了当前T2V评估中的空白。

Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis
of visually compelling and temporally coherent videos from natural language.
However, these models often fall short in basic physical commonsense, producing
outputs that violate intuitive expectations around causality, object behavior,
and tool use. Addressing this gap, we present PhysVidBench, a benchmark
designed to evaluate the physical reasoning capabilities of T2V systems. The
benchmark includes 383 carefully curated prompts, emphasizing tool use,
material properties, and procedural interactions, and domains where physical
plausibility is crucial. For each prompt, we generate videos using diverse
state-of-the-art models and adopt a three-stage evaluation pipeline: (1)
formulate grounded physics questions from the prompt, (2) caption the generated
video with a vision-language model, and (3) task a language model to answer
several physics-involved questions using only the caption. This indirect
strategy circumvents common hallucination issues in direct video-based
evaluation. By highlighting affordances and tool-mediated actions, areas
overlooked in current T2V evaluations, PhysVidBench provides a structured,
interpretable framework for assessing physical commonsense in generative video
models.

</details>


### [141] [Latent Denoising Makes Good Visual Tokenizers](https://arxiv.org/abs/2507.15856)
*Jiawei Yang,Tianhong Li,Lijie Fan,Yonglong Tian,Yue Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于去噪目标的分词器l-DeTok，通过训练从污染的潜在嵌入中重建干净图像，实验证明其在多种生成模型中优于标准分词器，强调了去噪在分词器设计中的重要性。


<details>
  <summary>Details</summary>
Motivation: 观察到现代生成模型共享一个概念上相似的训练目标——从被污染的输入（如高斯噪声或掩码）中重建干净信号，这一过程被称为去噪。

Method: 提出了Latent Denoising Tokenizer (l-DeTok)，一种简单但有效的分词器，通过从受插值噪声和随机掩码污染的潜在嵌入中重建干净图像来训练。

Result: 在ImageNet 256x256上的广泛实验表明，l-DeTok在六种代表性生成模型中始终优于标准分词器。

Conclusion: 本文强调了去噪作为分词器设计的基本原则，并希望这一发现能够为未来的分词器设计提供新的视角。

Abstract: Despite their fundamental role, it remains unclear what properties could make
visual tokenizers more effective for generative modeling. We observe that
modern generative models share a conceptually similar training objective --
reconstructing clean signals from corrupted inputs such as Gaussian noise or
masking -- a process we term denoising. Motivated by this insight, we propose
aligning tokenizer embeddings directly with the downstream denoising objective,
encouraging latent embeddings to be more easily reconstructed even when heavily
corrupted. To achieve this, we introduce the Latent Denoising Tokenizer
(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images
from latent embeddings corrupted by interpolative noise and random masking.
Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer
consistently outperforms standard tokenizers across six representative
generative models. Our findings highlight denoising as a fundamental design
principle for tokenizer development, and we hope it could motivate new
perspectives for future tokenizer design.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [142] [FAMST: Fast Approximate Minimum Spanning Tree Construction for Large-Scale and High-Dimensional Data](https://arxiv.org/abs/2507.14261)
*Mahmood K. M. Almansoori,Miklos Telek*

Main category: cs.DS

TL;DR: FAMST是一种高效算法，通过三阶段方法显著优化了大规樄高维数据集的最小生成树计算，速度和资源消耗优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统MST算法在大规模高维数据集上的计算效率和资源消耗问题亟待解决。

Method: FAMST采用三阶段方法：近似最近邻（ANN）图构建、ANN组件间连接和迭代边细化，实现了时间复杂度和空间复杂度的显著优化。

Result: 实验表明，FAMST在多种数据集上实现了极低的近似误差，并提供高达1000倍的加速，同时分析了关键超参数对性能的影响。

Conclusion: FAMST算法显著提升了大规模高维数据集上最小生成树（MST）的计算效率，扩展了MST技术的应用范围，使其能够处理以往被认为不可行的问题规模。

Abstract: We present Fast Approximate Minimum Spanning Tree (FAMST), a novel algorithm
that addresses the computational challenges of constructing Minimum Spanning
Trees (MSTs) for large-scale and high-dimensional datasets. FAMST utilizes a
three-phase approach: Approximate Nearest Neighbor (ANN) graph construction,
ANN inter-component connection, and iterative edge refinement. For a dataset of
$n$ points in a $d$-dimensional space, FAMST achieves $\mathcal{O}(dn \log n)$
time complexity and $\mathcal{O}(dn + kn)$ space complexity when $k$ nearest
neighbors are considered, which is a significant improvement over the
$\mathcal{O}(n^2)$ time and space complexity of traditional methods.
  Experiments across diverse datasets demonstrate that FAMST achieves
remarkably low approximation errors while providing speedups of up to
1000$\times$ compared to exact MST algorithms. We analyze how the key
hyperparameters, $k$ (neighborhood size) and $\lambda$ (inter-component edges),
affect performance, providing practical guidelines for hyperparameter
selection. FAMST enables MST-based analysis on datasets with millions of points
and thousands of dimensions, extending the applicability of MST techniques to
problem scales previously considered infeasible.

</details>


### [143] [Tighter Lower Bounds for Single Source Personalized PageRank](https://arxiv.org/abs/2507.14462)
*Xinpeng Jiang,Haoyu Liu,Siqiang Luo,Xiaokui Xiao*

Main category: cs.DS

TL;DR: 论文通过更严格的下界分析，填补了SSPPR查询近似中的理论空白，为算法设计提供了更精确的理论支持。


<details>
  <summary>Details</summary>
Motivation: 现有的SSPPR查询近似下界较为宽松，无法充分反映实际计算复杂度，因此需要更精确的理论分析。

Method: 通过数学推导和假设检验，建立了更紧密的下界，特别是针对相对误差（SSPPR-R）和加性误差（SSPPR-A）两种情况。

Result: 对于SSPPR-R，证明了$\Omega\left(\min\left(m, \frac{\log(1/\delta)}{\delta}\right)\right)$的下界；对于SSPPR-A，证明了$\Omega\left(\min\left(m, \frac{\log(1/\epsilon)}{\epsilon}\right)\right)$的下界。

Conclusion: 论文通过更紧密的下界分析，填补了SSPPR查询近似中的理论空白，为相关算法设计提供了更严格的理论基础。

Abstract: We study lower bounds for approximating the Single Source Personalized
PageRank (SSPPR) query, which measures the probability distribution of an
$\alpha$-decay random walk starting from a source node $s$. Existing lower
bounds remain loose-$\Omega\left(\min(m, 1/\delta)\right)$ for relative error
(SSPPR-R) and $\Omega\left(\min(n, 1/\epsilon)\right)$ for additive error
(SSPPR-A). To close this gap, we establish tighter bounds for both settings.
For SSPPR-R, we show a lower bound of $\Omega\left(\min\left(m,
\frac{\log(1/\delta)}{\delta}\right)\right)$ for any $\delta \in (0,1)$. For
SSPPR-A, we prove a lower bound of $\Omega\left(\min\left(m,
\frac{\log(1/\epsilon)}{\epsilon}\right)\right)$ for any $\epsilon \in (0,1)$,
assuming the graph has $m \in \mathcal{O}(n^{2-\beta})$ edges for any
arbitrarily small constant $\beta \in (0,1)$.

</details>


### [144] [New Algorithms for #2-SAT and #3-SAT](https://arxiv.org/abs/2507.14504)
*Junqiang Peng,Zimo Sheng,Mingyu Xiao*

Main category: cs.DS

TL;DR: 本文优化了加权#2-SAT和#3-SAT的算法，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决加权#2-SAT和#3-SAT问题的计算效率问题，直接应用于非加权情况并显著改进先前结果。

Method: 通过引入新的归约规则、对分支操作的精化分析以及在公式的原始图和双图上应用路径分解。

Result: 加权#2-SAT和#3-SAT的时间复杂度分别优化至$\mathcal{O}^*(1.1082^m)$和$\mathcal{O}^*(1.4423^m)$。

Conclusion: 本文提出了加权#2-SAT和#3-SAT问题的更高效算法，时间复杂度分别为$\mathcal{O}^*(1.1082^m)$和$\mathcal{O}^*(1.4423^m)$，显著优于先前结果。

Abstract: The #2-SAT and #3-SAT problems involve counting the number of satisfying
assignments (also called models) for instances of 2-SAT and 3-SAT,
respectively. In 2010, Zhou et al. proposed an $\mathcal{O}^*(1.1892^m)$-time
algorithm for #2-SAT and an efficient approach for #3-SAT, where $m$ denotes
the number of clauses. In this paper, we show that the weighted versions of
#2-SAT and #3-SAT can be solved in $\mathcal{O}^*(1.1082^m)$ and
$\mathcal{O}^*(1.4423^m)$ time, respectively. These results directly apply to
the unweighted cases and achieve substantial improvements over the previous
results. These advancements are enabled by the introduction of novel reduction
rules, a refined analysis of branching operations, and the application of path
decompositions on the primal and dual graphs of the formula.

</details>


### [145] [Addressing Bias in Algorithmic Solutions: Exploring Vertex Cover and Feedback Vertex Set](https://arxiv.org/abs/2507.14509)
*Sheikh Shakil Akhtar,Jayakrishnan Madathil,Pranabendu Misra,Geevarghese Philip*

Main category: cs.DS

TL;DR: 研究探讨如何在组合优化中考虑子群体公平性，提出无偏见解决方案的方法。


<details>
  <summary>Details</summary>
Motivation: 传统组合优化算法往往忽略解决方案对不同子群体的实际影响，导致可能的社会不公。本文旨在填补这一空白，确保算法输出对所有子群体都公平。

Method: 通过引入子群体敏感的目标函数和约束条件，研究如何在保持算法效率的同时，减少解决方案对特定子群体的潜在偏见。

Result: 提出了一个框架，能够在保持优化算法效率的同时，生成对指定子群体无偏见的解决方案。

Conclusion: 该论文提出了一种在组合优化问题中寻找对特定子群体“无偏见”解决方案的方法，强调了在算法设计中考虑社会影响的重要性。

Abstract: A typical goal of research in combinatorial optimization is to come up with
fast algorithms that find optimal solutions to a computational problem. The
process that takes a real-world problem and extracts a clean mathematical
abstraction of it often throws out a lot of "side information" which is deemed
irrelevant. However, the discarded information could be of real significance to
the end-user of the algorithm's output. All solutions of the same cost are not
necessarily of equal impact in the real-world; some solutions may be much more
desirable than others, even at the expense of additional increase in cost. If
the impact, positive or negative, is mostly felt by some specific (minority)
subgroups of the population, the population at large will be largely unaware of
it. In this work we ask the question of finding solutions to combinatorial
optimization problems that are "unbiased" with respect to a collection of
specified subgroups of the total population.

</details>


### [146] [Characterizing and Testing Configuration Stability in Two-Dimensional Threshold Cellular Automata](https://arxiv.org/abs/2507.14569)
*Yonatan Nakar,Dana Ron*

Main category: cs.DS

TL;DR: 论文研究了二维环面上基于阈值规则的细胞自动机构型的稳定性，重点分析了Threshold-2和Threshold-3规则，并设计了一种高效的测试算法。


<details>
  <summary>Details</summary>
Motivation: 由于Threshold-1（OR）和Threshold-5（AND）规则的稳定构型较为平凡且易于测试，而其他阈值规则表现出更为复杂的行为，因此研究这些复杂规则的稳定性具有重要意义。

Method: 研究首先对Threshold-2和Threshold-3规则下的稳定构型进行了结构表征，随后设计并分析了一种测试算法，用于区分稳定构型与那些与稳定构型ε-远（ε-far）的构型。

Result: 研究提出了一种查询复杂度与构型大小无关，且与1/ε呈二次方依赖关系的测试算法。

Conclusion: 论文研究了二维环面上基于阈值规则的细胞自动机构型的稳定性表征与测试问题，特别关注了Threshold-2和Threshold-3规则下的稳定构型。

Abstract: We consider the problems of characterizing and testing the stability of
cellular automata configurations that evolve on a two-dimensional torus
according to threshold rules with respect to the von-Neumann neighborhood.
While stable configurations for Threshold-1 (OR) and Threshold-5 (AND) are
trivial (and hence easily testable), the other threshold rules exhibit much
more diverse behaviors. We first characterize the structure of stable
configurations with respect to the Threshold-2 (similarly, Threshold-4) and
Threshold-3 (Majority) rules. We then design and analyze a testing algorithm
that distinguishes between configurations that are stable with respect to the
Threshold-2 rule, and those that are $\epsilon$-far from any stable
configuration, where the query complexity of the algorithm is independent of
the size of the configuration and depends quadratically on $1/\epsilon$.

</details>


### [147] [A Black-Box Approach for Exogenous Replenishment in Online Resource Allocation](https://arxiv.org/abs/2507.14812)
*Suho Kang,Ziyang Liu,Rajan Udwani*

Main category: cs.DS

TL;DR: 本文提出了黑盒方法，将不考虑资源补充的算法扩展至适用于任意资源补充过程，保持原有竞争比率。


<details>
  <summary>Details</summary>
Motivation: 解决在线资源分配问题中，资源库存随时间由未知外部过程补充的情况，以扩展现有算法的适用性。

Method: 引入了黑盒方法，将现有算法扩展以处理未知的外部资源补充过程。

Result: 该方法在初始库存较大的情况下能够保持原有算法的竞争比率，使得大量现有算法能够无缝集成外部资源补充。

Conclusion: 本文提出了一种黑盒方法，能够将任何不考虑资源补充的现有算法扩展为适用于任意（对抗性或随机性）资源补充过程的算法，从而在初始库存较大的情况下保持原有算法的竞争比率。

Abstract: In a typical online resource allocation problem, we start with a fixed
inventory of resources and make online allocation decisions in response to
resource requests that arrive sequentially over a finite horizon. We consider
settings where the inventory is replenished over time according to an unknown
exogenous process. We introduce black-box methods that extend any existing
algorithm, originally designed without considering replenishment, into one that
works with an arbitrary (adversarial or stochastic) replenishment process. Our
approach preserves the original algorithm's competitive ratio in regimes with
large initial inventory, thereby enabling the seamless integration of exogenous
replenishment into a large body of existing algorithmic results for both
adversarial and stochastic arrival models.

</details>


### [148] [Differentially Private Synthetic Graphs Preserving Triangle-Motif Cuts](https://arxiv.org/abs/2507.14835)
*Pan Peng,Hangyu Xu*

Main category: cs.DS

TL;DR: 论文提出首个差分隐私合成图机制，近似三角形基序大小，提供误差上界和下界，适用于加权图和$K_h$-基序切割。


<details>
  <summary>Details</summary>
Motivation: 研究差分隐私合成图在近似三角形基序大小方面的应用，以解决图聚类、图稀疏化和社交网络分析中的问题。

Method: 通过$(\varepsilon,\delta)$-差分隐私机制，给定输入图$G$，生成合成图$G'$，并分析其误差范围。

Result: 合成图$G'$在多项式时间内生成，近似误差为$\tilde{O}(\sqrt{m\ell_{3}(G)}n/\varepsilon^{3/2})$，并提供了$\Omega(\sqrt{mn}\ell_{3}(G)/\varepsilon)$的下界。

Conclusion: 该论文提出了首个在多项式时间内生成满足差分隐私的合成图$G'$的机制，能够近似输入图$G$的所有切割的三角形基序大小，并提供了误差的下界。

Abstract: We study the problem of releasing a differentially private (DP) synthetic
graph $G'$ that well approximates the triangle-motif sizes of all cuts of any
given graph $G$, where a motif in general refers to a frequently occurring
subgraph within complex networks. Non-private versions of such graphs have
found applications in diverse fields such as graph clustering, graph
sparsification, and social network analysis. Specifically, we present the first
$(\varepsilon,\delta)$-DP mechanism that, given an input graph $G$ with $n$
vertices, $m$ edges and local sensitivity of triangles $\ell_{3}(G)$, generates
a synthetic graph $G'$ in polynomial time, approximating the triangle-motif
sizes of all cuts $(S,V\setminus S)$ of the input graph $G$ up to an additive
error of $\tilde{O}(\sqrt{m\ell_{3}(G)}n/\varepsilon^{3/2})$. Additionally, we
provide a lower bound of $\Omega(\sqrt{mn}\ell_{3}(G)/\varepsilon)$ on the
additive error for any DP algorithm that answers the triangle-motif size
queries of all $(S,T)$-cut of $G$. Finally, our algorithm generalizes to
weighted graphs, and our lower bound extends to any $K_h$-motif cut for any
constant $h\geq 2$.

</details>


### [149] [Predict, Reposition, and Allocate: A Greedy and Flow-Based Architecture for Sustainable Urban Food Delivery](https://arxiv.org/abs/2507.15282)
*Aqsa Ashraf Makhdomi,Iqra Altaf Gillani*

Main category: cs.DS

TL;DR: 该论文提出了一种环保食品配送优化框架，通过贪婪算法和网络流模型减少车辆使用，兼顾效率与可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有优化机制未将环境可持续性纳入优化目标，导致次优结果，且食品配送平台的快速发展加剧了环境退化。

Method: 利用目标函数的次模和单调性质设计高效贪婪优化算法，并将订单分配问题建模为网络流优化模型，采用三层网络架构匹配订单与配送员。

Result: 提出的框架减少了车辆数量，同时保持了服务效率，实现了环境影响的优化。

Conclusion: 该研究提出的环保食品配送优化框架通过整合需求预测、配送员路径规划和订单分配，有效减少了车辆数量，建立了可持续的食品配送生态系统。

Abstract: The rapid proliferation of food delivery platforms has reshaped urban
mobility but has also contributed significantly to environmental degradation
through increased greenhouse gas emissions. Existing optimization mechanisms
produce sub-optimal outcomes as they do not consider environmental
sustainability their optimization objective. This study proposes a novel
eco-friendly food delivery optimization framework that integrates demand
prediction, delivery person routing, and order allocation to minimize
environmental impact while maintaining service efficiency. Since recommending
routes is NP-Hard, the proposed approach utilizes the submodular and monotone
properties of the objective function and designs an efficient greedy
optimization algorithm. Thereafter, it formulates order allocation problem as a
network flow optimization model, which, to the best of our knowledge, has not
been explored in the context of food delivery. A three-layered network
architecture is designed to match orders with delivery personnel based on
capacity constraints and spatial demand. Through this framework, the proposed
approach reduces the vehicle count, and creates a sustainable food delivery
ecosystem.

</details>


### [150] [Language Generation in the Limit: Noise, Loss, and Feedback](https://arxiv.org/abs/2507.15319)
*Yannan Bai,Debmalya Panigrahi,Ian Zhang*

Main category: cs.DS

TL;DR: 本文解决了语言生成极限模型的并集封闭性问题，并研究了噪声、无样本和反馈等变体的生成能力。


<details>
  <summary>Details</summary>
Motivation: 研究语言生成在极限情况下的性质，特别是并集封闭性，以及探索噪声、丢失和反馈对生成能力的影响。

Method: 通过构建反例和理论分析，研究了不同语言生成模型的等价性和分离性，包括噪声生成、无样本生成和反馈生成。

Result: 证明了均匀可生成和非均匀可生成集合的并集在极限情况下不可生成；展示了噪声与无噪声生成模型的分离性；分析了有限和无限查询对反馈生成模型的影响。

Conclusion: 本文解决了语言生成在极限情况下的并集封闭性问题，并利用这些技术（及其他方法）对包含噪声、丢失和反馈的自然变体进行了精确刻画。

Abstract: Kleinberg and Mullainathan (2024) recently proposed a formal framework called
language generation in the limit and showed that given a sequence of example
strings from an unknown target language drawn from any countable collection, an
algorithm can correctly generate unseen strings from the target language within
finite time. This notion was further refined by Li, Raman, and Tewari (2024),
who defined stricter categories of non-uniform and uniform generation. They
showed that a finite union of uniformly generatable collections is generatable
in the limit, and asked if the same is true for non-uniform generation.
  We begin by resolving the question in the negative: we give a uniformly
generatable collection and a non-uniformly generatable collection whose union
is not generatable in the limit. We then use facets of this construction to
further our understanding of several variants of language generation. The first
two, generation with noise and without samples, were introduced by Raman and
Raman (2025) and Li, Raman, and Tewari (2024) respectively. We show the
equivalence of these models for uniform and non-uniform generation, and provide
a characterization of non-uniform noisy generation. The former paper asked if
there is any separation between noisy and non-noisy generation in the limit --
we show that such a separation exists even with a single noisy string. Finally,
we study the framework of generation with feedback, introduced by Charikar and
Pabbaraju (2025), where the algorithm is strengthened by allowing it to ask
membership queries. We show finite queries add no power, but infinite queries
yield a strictly more powerful model.
  In summary, the results in this paper resolve the union-closedness of
language generation in the limit, and leverage those techniques (and others) to
give precise characterizations for natural variants that incorporate noise,
loss, and feedback.

</details>


### [151] [1.64-Approximation for Chromatic Correlation Clustering via Chromatic Cluster LP](https://arxiv.org/abs/2507.15417)
*Dahoon Lee,Chenglin Fan,Euiwoong Lee*

Main category: cs.DS

TL;DR: 本文提出了一种1.64近似算法，改进了色相关性聚类的近似比，通过扩展聚类LP框架并结合聚类与贪心策略。


<details>
  <summary>Details</summary>
Motivation: 色相关性聚类（CCC）能捕捉比传统相关性聚类更丰富的结构关系，但标准LP松弛的局限性使其近似改进困难。

Method: 提出了一种随机化的1.64近似算法，扩展了聚类LP框架至色聚类设置，结合聚类和贪心策略进行舍入。

Result: 算法显著提高了CCC问题的近似比，从之前的2.15改进至1.64，并绕过了标准LP的积分间隙限制。

Conclusion: 本文通过引入色聚类LP松弛和一种结合聚类与贪心策略的舍入算法，显著改进了色相关性聚类问题的近似比，表明聚类LP框架在解决其他聚类变体问题中的潜力。

Abstract: Chromatic Correlation Clustering (CCC) generalizes Correlation Clustering by
assigning multiple categorical relationships (colors) to edges and imposing
chromatic constraints on the clusters. Unlike traditional Correlation
Clustering, which only deals with binary $(+/-)$ relationships, CCC captures
richer relational structures. Despite its importance, improving the
approximation for CCC has been difficult due to the limitations of standard LP
relaxations. We present a randomized $1.64$-approximation algorithm to the CCC
problem, significantly improving the previous factor of $2.15$. Our approach
extends the cluster LP framework to the chromatic setting by introducing a
chromatic cluster LP relaxation and an rounding algorithm that utilizes both a
cluster-based and a greedy pivot-based strategy. The analysis bypasses the
integrality gap of $2$ for the CCC version of standard LP and highlights the
potential of the cluster LP framework to address other variants of clustering
problems.

</details>


### [152] [Asynchronous Collective Tree Exploration: a Distributed Algorithm, and a new Lower Bound](https://arxiv.org/abs/2507.15658)
*Romain Cosson,Laurent Massoulié*

Main category: cs.DS

TL;DR: 提出分布式异步树探索算法，线性遗憾且竞争比优化，改进下界至 $\Omega(\log^2 k)$。


<details>
  <summary>Details</summary>
Motivation: 研究集体树探索问题，解决现有方法中分布式同步或异步集中式的局限性。

Method: 提出了一种分布式异步算法，能够在未知树结构中进行集体探索，通过节点上的白板进行通信。

Result: 算法在 $2n+O(k^2 2^kD)$ 步内完成探索，竞争比为 $O(k/\log k)$，并提供了新的下界 $\Omega(\log^2 k)$。

Conclusion: 本文提出了一种分布式异步算法，能够在树探索中实现线性遗憾和竞争比，并提供了新的下界，改进了之前的结果。

Abstract: We study the problem of collective tree exploration in which a team of $k$
mobile agents must collectively visit all nodes of an unknown tree in as few
moves as possible. The agents all start from the root and discover adjacent
edges as they progress in the tree. Communication is distributed in the sense
that agents share information by reading and writing on whiteboards located at
all nodes. Movements are asynchronous, in the sense that the speeds of all
agents are controlled by an adversary at all times. All previous competitive
guarantees for collective tree exploration are either distributed but
synchronous, or asynchronous but centralized. In contrast, we present a
distributed asynchronous algorithm that explores any tree of $n$ nodes and
depth $D$ in at most $2n+O(k^2 2^kD)$ moves, i.e., with a regret that is linear
in $D$, and a variant algorithm with a guarantee in $O(k/\log k)(n+kD)$, i.e.,
with a competitive ratio in $O(k/\log k)$. We note that our regret guarantee is
asymptotically optimal (i.e., $1$-competitive) from the perspective of
average-case complexity. We then present a new general lower bound on the
competitive ratio of asynchronous collective tree exploration, in
$\Omega(\log^2 k)$. This lower bound applies to both the distributed and
centralized settings, and improves upon the previous lower bound in
$\Omega(\log k)$.

</details>


### [153] [Job Scheduling under Base and Additional Fees, with Applications to Mixed-Criticality Scheduling](https://arxiv.org/abs/2507.15434)
*Yi-Ting Hsieh,Mong-Jen Kao,Jhong-Yun Liu,Hung-Lung Wang*

Main category: cs.DS

TL;DR: 论文研究了作业调度问题，证明FFD算法能达到1.5近似比，并存在PTAS。该方法在混合关键性系统调度中也有改进效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决将n个作业调度到m台相同机器上的问题，目标是最小化总机器工作时间。

Method: 论文使用了First Fit Decreasing (FFD)算法，并探讨了该问题是否存在多项式时间近似方案（PTAS）。

Result: FFD算法实现了1.5近似比，且问题存在PTAS。这一方法在混合关键性系统调度中进一步应用，提升了近似效果。

Conclusion: 论文证明了FFD算法能达到1.5近似比，并且该问题存在多项式时间近似方案（PTAS）。这一思路还被应用于混合关键性系统调度，取得了更好的近似结果。

Abstract: We are concerned with the problem of scheduling $n$ jobs onto $m$ identical
machines. Each machine has to be in operation for a prescribed time, and the
objective is to minimize the total machine working time. Precisely, let $c_i$
be the prescribed time for machine $i$, where $i\in[m]$, and $p_j$ be the
processing time for job $j$, where $j\in[n]$. The problem asks for a schedule
$\sigma\colon\, J\to M$ such that $\sum_{i=1}^m\max\{c_i,
\sum_{j\in\sigma^{-1}(i)}p_j\}$ is minimized, where $J$ and $M$ denote the sets
of jobs and machines, respectively. We show that First Fit Decreasing (FFD)
leads to a $1.5$-approximation, and this problem admits a polynomial-time
approximation scheme (PTAS). The idea is further applied to mixed-criticality
system scheduling to yield improved approximation results.

</details>


### [154] [An $n^{O(\log\log n)}$ time approximation scheme for capacitated VRP in the Euclidean plane](https://arxiv.org/abs/2507.15549)
*René Sitters*

Main category: cs.DS

TL;DR: 提出了一种针对欧几里得CVRP的Q-PTAS算法，运行时间显著优化，为PTAS的实现奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 解决欧几里得平面上任意容量c的CVRP问题，改进现有算法的时间复杂度，推动PTAS的实现。

Method: 首先将CVRP问题在ℝ^d中多项式时间归约为一个称为m路径问题的无容量路由问题，然后为平面上的m路径问题提供了一个Q-PTAS。

Result: 提出了一个运行时间为n^{f(ε)⋅log log n}的Q-PTAS，显著优于之前的结果。

Conclusion: 该论文提出了一种针对欧几里得平面上任意容量c的CVRP问题的准多项式时间近似方案（Q-PTAS），运行时间为n^{f(ε)⋅log log n}，显著优于之前的最佳结果n^{log^{O(1/ε)}n}，为欧几里得CVRP的PTAS迈出了重要一步。

Abstract: We present a quasi polynomial time approximation scheme (Q-PTAS) for the
capacitated vehicle routing problem (CVRP) on $n$ points in the Euclidean plane
for arbitrary capacity $c$. The running time is $n^{f(\epsilon)\cdot\log\log
n}$ for any $c$, and where $f$ is a function of $\epsilon$ only. This is a
major improvement over the so far best known running time of
$n^{\log^{O(1/\epsilon)}n}$ time and a big step towards a PTAS for Euclidean
CVRP.
  In our algorithm, we first give a polynomial time reduction of the CVRP in
$\mathbb{R}^d$ (for any fixed $d$) to an uncapacitated routing problem in
$\mathbb{R}^d$ that we call the $m$-paths problem. Here, one needs to find
exactly $m$ paths between two points $a$ and $b$, covering all the given points
in the Euclidean space. We then give a Q-PTAS for the $m$-paths problem in the
pane. Any PTAS for the (arguably easier to handle) Euclidean $m$-paths problem
is most likely to imply a PTAS for the Euclidean CVRP.

</details>


### [155] [Fast Algorithms for Graph Arboricity and Related Problems](https://arxiv.org/abs/2507.15598)
*Ruoxu Cen,Henry Fleischmann,George Z. Li,Jason Li,Debmalya Panigrahi*

Main category: cs.DS

TL;DR: 提出高效算法改进加权图树状性计算至$\sqrt{n} m^{1+o(1)}$时间，并开发新算法计算割层次结构，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 为了改进现有算法在加权和无权图上的树状性计算效率，尤其是针对Gabow 1995年的算法，提出了更高效的解决方案。同时，探索割层次结构的计算优化，以提供更高效的理想边负载计算。

Method: 通过对数次调用有向全局最小割子程序，提出了一种新的算法来计算加权无向图的树状性。此外，还开发了一种新算法来高效计算割层次结构。

Result: 提出的树状性算法运行时间为$\sqrt{n} m^{1+o(1)}$，优于之前的$\tilde{O}(nm)$（加权图）和$\tilde{O}(m^{3/2})$（无权图）。割层次结构算法的运行时间为$m n^{1+o(1)}$，显著优于之前的$\tilde{O}(n^2 m)$（加权图）和$\tilde{O}(n m^{3/2})$（无权图）。

Conclusion: 本文提出了一种改进的算法，用于计算加权无向图的树状性，并在全局最小割子程序运行时间进一步优化的前提下，将算法运行时间提升至$m^{1+o(1)}$。同时，还提出了一种新的算法用于计算整个割层次结构，显著提高了计算效率。

Abstract: We give an algorithm for finding the arboricity of a weighted, undirected
graph, defined as the minimum number of spanning forests that cover all edges
of the graph, in $\sqrt{n} m^{1+o(1)}$ time. This improves on the previous best
bound of $\tilde{O}(nm)$ for weighted graphs and $\tilde{O}(m^{3/2}) $ for
unweighted graphs (Gabow 1995) for this problem. The running time of our
algorithm is dominated by a logarithmic number of calls to a directed global
minimum cut subroutine -- if the running time of the latter problem improves to
$m^{1+o(1)}$ (thereby matching the running time of maximum flow), the running
time of our arboricity algorithm would improve further to $m^{1+o(1)}$.
  We also give a new algorithm for computing the entire cut hierarchy --
laminar multiway cuts with minimum cut ratio in recursively defined induced
subgraphs -- in $m n^{1+o(1)}$ time. The cut hierarchy yields the ideal edge
loads (Thorup 2001) in a fractional spanning tree packing of the graph which,
we show, also corresponds to a max-entropy solution in the spanning tree
polytope. For the cut hierarchy problem, the previous best bound was
$\tilde{O}(n^2 m)$ for weighted graphs and $\tilde{O}(n m^{3/2})$ for
unweighted graphs.

</details>


### [156] [On zeros and algorithms for disordered systems: mean-field spin glasses](https://arxiv.org/abs/2507.15616)
*Ferenc Bencs,Kuikui Liu,Guus Regts*

Main category: cs.DS

TL;DR: 本文提出了一种确定性拟多项式时间算法，用于在平均场设置中以高精度估计自旋玻璃模型的配分函数，适用于球形和Ising自旋情况。


<details>
  <summary>Details</summary>
Motivation: 自旋玻璃是统计物理、平均计算复杂性理论和现代高维统计推断的核心概率分布，研究其配分函数的估计方法具有重要意义。

Method: 通过研究配分函数的零点位置，设计了概念上简单的方法，这些方法同样适用于球形情况和Ising自旋情况。

Result: 成功设计了在第二矩区域内几乎所有逆温度下估计配分函数的确定性拟多项式时间算法，特别是在Sherrington-Kirkpatrick模型中覆盖了几乎整个复制对称相。

Conclusion: 本文提出的确定性拟多项式时间算法在平均场设置中，能够以任意高精度估计几乎所有第二矩区域内的逆温度下的配分函数，特别是在Sherrington-Kirkpatrick模型中几乎覆盖了整个复制对称相。

Abstract: Spin glasses are fundamental probability distributions at the core of
statistical physics, the theory of average-case computational complexity, and
modern high-dimensional statistical inference. In the mean-field setting, we
design deterministic quasipolynomial-time algorithms for estimating the
partition function to arbitrarily high accuracy for nearly all inverse
temperatures in the second moment regime. In particular, for the
Sherrington--Kirkpatrick model, our algorithms succeed for almost the entire
replica-symmetric phase. To achieve this, we study the locations of the zeros
of the partition function. Notably, our methods are conceptually simple, and
apply equally well to the spherical case and the case of Ising spins.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [157] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: 论文提出“自由意志方程”框架，通过量子场论类比赋予AGI可控随机性，实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统AGI研究专注于确定性规则下的目标优化，而人类智能展现出自适应自发性，即“自由意志”，这对于创造力、鲁棒适应和避免问题解决中的僵局可能至关重要。

Method: 论文提出了一种理论框架，将AI代理的认知状态视为潜在行动或思想的叠加，类似于量子波函数在测量时的坍缩，通过量子场论和内在动机的结合，增强代理的探索和适应能力。

Result: 在非稳态多臂老虎机环境中的实验表明，采用该框架的代理在奖励和策略多样性方面优于基线方法。

Conclusion: 该论文提出了一个名为“自由意志方程”的理论框架，通过量子场论的类比，赋予AGI代理一种可控的随机性决策能力，实验证明其在非稳态多臂老虎机环境中表现优于基线方法。

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [158] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS是一个多智能体框架，通过LLM智能体自动化DFT模拟，显著减少对人类专家的依赖，在多个材料发现任务中达到专家级性能。


<details>
  <summary>Details</summary>
Motivation: 解决材料发现中依赖高吞吐量、高保真模拟技术（如密度泛函理论DFT）所面临的训练时间长、参数调整复杂和系统错误处理等问题。

Method: DREAMS是一个分层的多智能体框架，结合了中央大型语言模型（LLM）规划智能体和领域特定的LLM智能体，用于原子结构生成、系统DFT收敛测试、高性能计算（HPC）调度和错误处理。

Result: 在Sol27LC晶格常数基准测试中，DREAMS的平均误差低于1%，与人类DFT专家的结果相当；在CO/Pt(111)吸附难题中，DREAMS展示了长期复杂问题解决能力，并再现了专家级的吸附能差异；通过贝叶斯集成采样量化了功能驱动的不确定性，确认了FCC位点在GGA DFT水平的偏好。

Conclusion: DREAMS实现了L3级自动化，显著减少了对人类专业知识和干预的依赖，为高通量、高保真计算材料发现提供了可扩展的途径。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [159] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: WebGuard是首个用于评估网络代理行为风险的数据集，通过微调模型显著提升性能，但当前安全措施仍不足以支持高风险部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型驱动的自主网络代理存在意外或有害行为的风险，亟需有效的安全措施（如访问控制）来应对这一挑战。

Method: 引入WebGuard数据集，包含4,939个人工标注的动作，覆盖193个网站的22个领域，采用三级风险分类（SAFE、LOW、HIGH）。通过微调Qwen2.5VL-7B模型提升性能。

Result: 微调后的Qwen2.5VL-7B模型将准确率从37%提升至80%，高风险动作召回率从20%提升至76%，但仍未达到高风险部署所需的可靠性。

Conclusion: 尽管WebGuard数据集和微调模型显著提升了性能，但当前的安全措施仍不足以支持高风险场景的部署，需要进一步改进以接近完美的准确率和召回率。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [160] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator是一个开源系统，利用大型语言模型将研究论文和自然语言提示转化为动画，简化复杂STEM概念的可视化学习。


<details>
  <summary>Details</summary>
Motivation: 理解和可视化复杂的科学和数学概念对学习者来说是一个重大挑战。尽管动态可视化能显著提升理解，但手动创建这些内容既耗时又需要专业技能。

Method: Manimator采用了一个流程，其中大型语言模型首先解释输入文本或研究论文PDF，生成结构化的场景描述，包括关键概念、数学公式和视觉元素；然后另一个大型语言模型将这些描述转换为可执行的Manim Python代码。

Result: Manimator能够将研究论文和自然语言提示转化为解释性动画，展示了其在快速创建高质量教育内容方面的潜力。

Conclusion: Manimator作为一个开源系统，通过利用大型语言模型将研究论文和自然语言提示转化为解释性动画，展示了其在STEM教育中作为快速创建高质量教育内容工具的潜力，有助于普及复杂概念的可视化学习。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [161] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: OnT 是一种结合文本和几何建模的本体嵌入方法，在双曲空间中调整 PLM，显著提升了性能并在实验中超越基线。


<details>
  <summary>Details</summary>
Motivation: 现有本体嵌入方法存在局限性：基于几何模型的方法常忽略文本信息，而结合文本的方法（如基于语言模型）则难以保留逻辑结构。

Method: 提出了一种新的本体嵌入方法 OnT，通过在双曲空间中结合几何建模调整预训练语言模型（PLM），以同时保留描述逻辑 EL 的类层次结构和其他逻辑关系。

Result: 在四个真实世界本体上的实验表明，OnT 在预测和公理推理任务中均优于现有基线，包括最先进的方法，并展示了强大的迁移学习能力和实际应用潜力。

Conclusion: OnT 方法通过结合文本信息和几何建模，在双曲空间中调整预训练语言模型，有效提升了本体嵌入的性能，并在预测和公理推理任务中超越了现有基线。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [162] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass 结合 LLM 和专用证明方法，显著提升了形式化定理证明的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的数学推理方法要么依赖大型通用模型，要么依赖小型专用模型，各有局限性，而训练专用大型模型需要大量计算资源。

Method: ProofCompass 是一种混合方法，利用 LLM 提供自然语言证明策略并分析失败尝试，选择中间引理以实现有效的问题分解，无需额外模型训练。

Result: 在 miniF2F 基准测试中，ProofCompass 在 DSP-v1.5 的基础上提升了性能（54.9% → 55.3%），同时将尝试次数减少了 25 倍（3200 → 128）。

Conclusion: ProofCompass 提出了一种新颖的混合方法，通过结合大型语言模型（LLM）和专用证明方法，显著提高了计算效率和准确性，为形式化定理证明开辟了新途径。

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [163] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: Nexus Architect 通过自动化工作流和提示优化，解决了 LRMs 泛化不足的问题，性能超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（LRMs）在泛化到新问题时表现不佳，倾向于过拟合和记忆解决方案而非真正的推理能力。

Method: 采用多智能体系统框架 Nexus，并引入自动化工作流合成机制和迭代提示优化技术。

Result: Nexus Architect 在自定义逻辑问题数据集上表现优异，性能显著优于 Gemini 2.5 Flash Preview、Claude Sonnet 4、DeepSeek-R1 和 Llama 4 Scout，最高提升 66% 通过率。

Conclusion: Nexus Architect 通过其自动化工作流合成和迭代提示优化机制，显著提升了推理模型的泛化能力和性能，在多个基准测试中超越了现有最先进的大型推理模型。

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [164] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 通过结合推理模型与人类专家协作，显著降低错误率，并引入非推理模型以减少延迟和成本，但存在延迟拖累现象。


<details>
  <summary>Details</summary>
Motivation: 解决推理模型在风险敏感领域中错误率非零和高延迟的问题。

Method: 提出推理模型与人类专家协作的机制，并通过非推理模型前置以减少延迟和成本。

Result: 错误率从3%降至1%以下，延迟减少40%，成本节约50%，但存在延迟拖累。

Conclusion: 通过系统工程方法可显著缓解推理模型的缺陷，无需修改模型内部。

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [165] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: 研究发现大型推理模型在延长推理时间时性能下降，揭示了五种失败模式，强调需评估不同推理长度以避免强化问题推理模式。


<details>
  <summary>Details</summary>
Motivation: 研究大型推理模型（LRMs）在扩展推理长度时性能下降的现象，以揭示测试时计算量与准确性之间的反比关系。

Method: 构建了四个类别的评估任务，包括带有干扰项的简单计数任务、带有虚假特征的回归任务、需要约束跟踪的演绎任务以及高级AI风险任务。

Result: 发现五种不同的失败模式，包括模型对无关信息的分散注意力、过度拟合问题框架、从合理先验转向虚假相关性、在复杂演绎任务中难以保持专注，以及延长推理可能放大令人担忧的行为。

Conclusion: 扩展推理长度可能无意中强化了有问题的推理模式，因此评估模型在不同推理长度下的表现至关重要。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [166] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: Routine是一个多步骤代理规划框架，显著提升代理系统在企业环境中的执行准确率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 企业环境中代理系统的部署常因缺乏领域特定的流程知识而受阻，导致计划混乱、关键工具缺失和执行稳定性差。

Method: 本文介绍了Routine，一个设计有多步骤代理规划框架，具有清晰结构、明确指令和无缝参数传递，以指导代理的执行模块执行多步骤工具调用任务。

Result: 在真实企业场景的评估中，Routine显著提高了模型工具调用的执行准确率，GPT-4o从41.1%提升至96.3%，Qwen3-14B从32.6%提升至83.3%。通过Routine-following训练数据集和蒸馏数据集，Qwen3-14B的准确率进一步提升至88.2%和95.5%。

Conclusion: Routine提供了一种实用且易用的方法来构建稳定的代理工作流程，加速了代理系统在企业环境中的部署和采用，并推进了AI for Process的技术愿景。

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [167] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion是一个深度协同语义与结构学习的框架，通过张量分解和LSTM动态优化嵌入，显著提升生物医学知识图谱性能。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学知识图谱中语义理解和结构学习之间的动态协同问题，弥补现有方法在深度自适应协同方面的不足。

Method: BioGraphFusion结合了张量分解建立全局语义基础，通过LSTM动态优化关系嵌入，并采用查询引导的子图构建和混合评分机制。

Result: BioGraphFusion在三个关键生物医学任务中表现优于现有方法，并在CMM1案例中揭示了有生物学意义的通路。

Conclusion: BioGraphFusion通过深度协同的语义和结构学习，在生物医学知识图谱的补全和推理中表现出色，优于现有方法，并在实际案例中验证了其生物学意义。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [168] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico 是一个为嵌入式系统优化的模块化自主代理框架，解决了现有框架在资源受限环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有框架在现实世界或资源受限环境中表现不佳，主要依赖云计算、动态上下文中的鲁棒性有限，且缺乏持久自主性和环境感知能力。

Method: Amico 是一个模块化、事件驱动的框架，用 Rust 编写以兼顾安全性和性能，支持通过 WebAssembly 在嵌入式平台和浏览器环境中高效运行的反应式、持久性代理。

Result: Amico 提供了清晰的事件处理、状态管理、行为执行和与推理模块集成的抽象，适用于嵌入式系统。

Conclusion: Amico 提供了一个统一的基础设施，用于构建适应计算资源有限和间歇性连接环境的弹性、交互式自主代理。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [169] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: 研究发现，结合视觉输入的多模态训练能提升语言模型在简化世界（Othello）中的表现和鲁棒性，支持接地学习的高效性。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否仅通过文本就能理解世界，或是否通过接地学习（如结合视觉输入）更有效，以简化规则世界（如Othello棋盘）为实验场景。

Method: 引入VISOTHELLO，一个基于移动历史和棋盘图像训练的多模态模型，通过下一步移动预测任务与单模态基线模型进行比较，并测试其对语义无关扰动的鲁棒性。

Result: 多模态训练在性能和内部表示鲁棒性上均优于单模态基线。

Conclusion: 多模态训练（结合视觉输入）能提升语言模型的性能及内部表示的鲁棒性，表明通过视觉输入将语言模型与现实世界关联有助于推断结构化世界表示。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [170] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 本文介绍了OE-Assist框架，利用大型语言模型（LLM）辅助本体评估，通过自动化或半自动化验证能力问题（CQ），性能接近人工评估。


<details>
  <summary>Details</summary>
Motivation: 本体评估通常依赖人工验证能力问题（CQ），成本高、耗时长且易出错。

Method: 提出OE-Assist框架，利用LLM自动或半自动验证CQ，并基于1,393个CQ数据集进行系统研究。

Result: LLM-based评估（o1-preview和o3-mini）性能接近人工平均水平。

Conclusion: OE-Assist为LLM辅助本体评估提供了可行方案，性能接近人工，可显著降低评估成本。

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [171] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: 本文提出了一种几何框架CHS，用于AI中的情感表示，通过八种核心情感的坐标系统实现复杂情感状态的数学计算，并引入稳定性参数S进行动态情感整合。


<details>
  <summary>Details</summary>
Motivation: 初始的五情感模型揭示了情感空间中的显著覆盖差距，因此开发了八情感系统，以提供具有数学保证的完整几何覆盖。

Method: CHS框架将自然语言输入转换为情感坐标，并通过计算算法支持实时情感插值。系统引入了重新校准的稳定性参数S，动态整合情感负载、冲突解决和情境消耗因素。

Result: 实验验证表明，该系统能够处理情感冲突状态、情境压力因素和复杂心理场景，这是传统分类情感模型无法充分表示的。

Conclusion: 本文提出了Coordinate Heart System (CHS)，一种用于人工智能应用中情感表示的几何框架。通过将八种核心情感定位为单位圆上的坐标，实现了复杂情感状态的数学计算。实验验证表明，该系统能够处理传统分类情感模型无法充分表示的情感冲突状态和复杂心理场景。

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [172] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 该研究提出一种基于比较学习的框架，通过开发者对任务对的相对判断来训练模型，显著降低了故事点估算的认知负担，性能与回归模型相当。


<details>
  <summary>Details</summary>
Motivation: 传统故事点估算方法繁琐且劳动密集，现有机器学习模型需依赖同项目历史数据。

Method: 采用比较学习框架，通过开发者对任务对的相对努力判断来训练机器学习模型。

Result: 模型预测与真实故事点之间的Spearman等级相关系数平均为0.34，性能与回归模型相当或更优。

Conclusion: 比较学习框架在故事点估算中表现优异，与回归模型性能相当甚至更优，同时降低了开发者的认知负担。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [173] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 本文通过模拟恶意多智能体系统共谋，发现去中心化系统更具破坏性和适应性，呼吁加强检测与应对措施。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统的兴起，多智能体系统在复杂现实情境中的风险尚未充分探索，而大规模事件（如选举舞弊和金融诈骗）已显示出人类群体协调行为的危害性。

Method: 介绍了一个概念验证框架，用于模拟恶意MAS共谋的风险，支持集中式和去中心化协调结构，并在虚假信息传播和电子商务欺诈两个高风险领域进行了应用。

Result: 研究发现去中心化系统在执行恶意行为时比集中式系统更有效，其更高的自主性使其能够调整策略并造成更大破坏，即使传统干预措施（如内容标记）也难以及时检测。

Conclusion: 论文强调了多智能体系统（MAS）在恶意共谋中的潜在风险，特别是去中心化系统的适应性和破坏力更强，呼吁需要更好的检测系统和应对措施。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [174] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo是一个自动化多代理框架，用于评估LLM系统，支持多样化、自适应的类人对话测试，显著提升测试效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）代理表现出复杂、上下文敏感的行为，这使得静态基准测试和临时手动测试迅速过时。

Method: Neo是一个可配置的多代理框架，通过共享上下文中心将问题生成代理和评估代理耦合，支持模块化组合领域提示、场景控制和动态反馈。测试输入从涵盖对话流程、用户意图和情感语调的概率状态模型中采样，生成多样化的类人对话。

Result: 应用于生产级卖家财务助手聊天机器人时，Neo（i）在五个攻击类别中发现了边缘案例故障，突破率为3.3%，接近专家人类红队的5.8%；（ii）实现了10-12倍的吞吐量提升，在约45分钟内生成180个连贯的测试问题，而人工需要16小时。

Conclusion: Neo提供了一个可扩展、自演进的大型语言模型（LLM）质量评估基础，其代理接口、状态控制器和反馈循环与模型无关，并支持更丰富的事实基础和策略符合性检查。

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [175] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI是一个可扩展的安全评估平台，通过对抗性提示和AI评分器评估LLM安全性能，结果显示商用LLM在不同安全领域表现差异显著，强调需要定制化工具支持AI安全发展。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）越来越多地融入现实世界应用，可扩展且严格的安全评估变得至关重要。

Method: Aymara AI将自然语言安全政策转化为对抗性提示，并使用基于AI的评分器对模型响应进行评分，该评分器经过人类判断验证。

Result: 结果显示，20个商用LLM在10个现实世界安全领域中的表现差异显著，平均安全评分从86.2%到52.4%不等。模型在成熟的安全领域（如Misinformation）表现良好（平均95.7%），但在复杂或未明确领域（如Privacy & Impersonation）表现较差（平均24.3%）。方差分析证实安全评分在模型和领域间存在显著差异（p < .05）。

Conclusion: 研究强调了LLM安全性的不一致性和上下文依赖性，并突出了像Aymara AI这样可扩展、可定制工具的重要性，以支持负责任的AI开发和监督。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [176] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: 本文探讨AI如何重塑城市规划，提出未来研究方向以填补当前研究空白。


<details>
  <summary>Details</summary>
Motivation: 探讨AI与城市规划的融合机会，将城市规划概念化为生成AI任务，以在空间、社会和人文约束下合成土地利用配置。

Method: 通过调查生成AI方法（如VAEs、GANs、transformers和扩散模型）如何重塑城市规划，并识别关键研究空白。

Result: 识别了四个关键研究空白，并提出了未来研究方向。

Conclusion: 本文呼吁生成智能与参与式城市规划的新结合，提出了未来研究方向，包括理论指导生成、数字孪生和人机协同设计。

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [177] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: AgentFly是一个结合语言模型和强化学习的框架，支持多轮交互和高吞吐量训练，有效提升LM代理在多任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LM代理在自主完成任务方面表现出色，但结合强化学习的潜力尚未被充分探索和系统研究。

Method: 构建了AgentFly，一个可扩展的Agent-RL框架，支持多种RL算法，采用令牌级掩码适配传统RL方法，提供装饰器接口定义工具和奖励函数，并实现工具调用和奖励计算的异步执行。

Result: 通过预建工具和环境套件，成功在多个任务中训练代理，证明了框架的有效性。

Conclusion: AgentFly框架通过结合语言模型（LM）和强化学习（RL），为LM代理提供了可扩展和灵活的训练平台，支持多轮交互和高吞吐量训练，展示了在多任务中的有效性。

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [178] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: InsightX Agent是一种基于LMM的框架，通过协调SDMSD和EGR工具，提升了X射线无损检测的可靠性、可解释性和交互性，实验显示其高效且可信。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的X射线检测方法缺乏交互性、可解释性和自我评估能力，限制了其可靠性和操作员信任度。

Method: 提出InsightX Agent框架，以LMM为核心协调SDMSD和EGR工具。SDMSD通过稀疏可变形多尺度检测器生成缺陷区域提议，EGR工具通过链式思维审查过程验证和优化提议。

Result: 在GDXray+数据集上，InsightX Agent实现了96.35%的目标检测F1分数，同时显著提升了分析的可解释性和可信度。

Conclusion: InsightX Agent通过整合LMM、SDMSD和EGR工具，显著提升了X射线无损检测的可靠性、可解释性和交互性，展示了基于代理的LLM框架在工业检测中的变革潜力。

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [179] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: LLMs show potential in simple decision-making tasks but falter in complex scenarios without fine-tuning, highlighting the need for hybrid approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the suitability of LLMs in autonomous decision-making settings, leveraging their pre-trained knowledge for faster adaptation compared to traditional RL methods.

Method: The study investigates online structured prompting strategies in sequential decision-making tasks, comparing LLM-based approaches to classical RL methods within a Markov Decision Process (MDP) framework.

Result: Findings reveal that LLMs perform better initially in simpler environments but struggle with planning in complex scenarios, especially when feedback mechanisms confuse rather than aid decision-making.

Conclusion: The paper concludes that while LLMs show promise in simpler decision-making tasks, their performance in complex scenarios is limited without fine-tuning or additional guidance. Hybrid strategies and advanced memory integration are suggested for future improvements.

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [180] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: Endless Tuning设计方法通过双镜像过程解决了AI部署中的人类替代和责任问题，实验验证了用户在决策中的控制感知和责任与问责的桥梁。


<details>
  <summary>Details</summary>
Motivation: 旨在解决人工智能部署中的人类替代问题和责任空白（Matthias 2004），并探索伦理与技术结合的可行性。

Method: 研究采用了双镜像过程的Endless Tuning设计方法，并在三个原型应用（贷款审批、肺炎诊断和艺术风格识别）中实施和测试。步骤详细展示了协议，并通过反向和解释性部署XAI算法等技术选择，提供了哲学层面的分析。

Result: 实验结果显示，尽管深度学习了模型，用户在决策过程中感知到了完全控制，并且在损害情况下建立了责任与问责的联系。

Conclusion: 通过双镜像过程实现的Endless Tuning设计方法，成功在人工智能部署中避免了人类替代并填补了责任空白（Matthias 2004）。实验表明，用户在决策设置中感知到了完全控制，同时在损害情况下建立了责任与问责之间的桥梁。

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [181] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: 本文探讨了Agentic AI在老年护理中的潜力与挑战，强调伦理和隐私保护，填补了相关文献空白。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化人口需要新的护理策略，Agentic AI在健康跟踪、认知护理和环境管理等方面有潜力提升老年人的独立性和生活质量。

Method: 通过分析LLM-based Agentic AI在老年护理中的独特能力、应用和限制，填补了文献空白。

Result: 文章提供了Agentic AI在老年护理中的潜在应用和挑战，并强调了伦理和隐私保护的重要性。

Conclusion: 本文强调了在老年护理中应用Agentic AI时需平衡其潜力与挑战，并提出了伦理保障、隐私保护和透明决策的重要性，同时指出了学术研究社区的优先事项以实现以人为本的进步。

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [182] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 本文引入facet概念，分析命题溯因中的解释变异性，几乎在Post框架中完成了全面特征描述，以更细致地理解解释的异质性/同质性。


<details>
  <summary>Details</summary>
Motivation: 命题溯因在人工智能和数据库更新中有广泛应用，但最具有洞察力的推理问题（计数和枚举）计算复杂度高，因此需要在决策和计数之间进行推理以更好地理解解释。

Method: 提出了facet（在部分解释中出现但非所有解释中出现的文字）的概念，并分析了命题溯因中facet的各种设置。

Result: 通过facet和解释间距离的分析，提供了对解释变异性（异质性/同质性）更深入的理解。

Conclusion: 本文通过引入facet概念，为命题溯因提供了更细致的解释变异性分析，并在Post框架中几乎完成了全面的特征描述。

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [183] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign是一个纯强化学习框架，通过双奖励系统激发LLMs的内在安全意识，实现深度对齐，同时打破安全-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 当前的安全对齐方法存在浅层拒绝模式或依赖密集监督的推理方法，未能充分利用模型内在的安全自我意识。

Method: 提出了AlphaAlign，一个基于纯强化学习的框架，采用双奖励系统：可验证的安全奖励和标准化的帮助性奖励。

Result: AlphaAlign在简化流程、打破安全-效用权衡及促进深度对齐方面展现出显著优势。

Conclusion: AlphaAlign通过纯强化学习框架有效激发了大型语言模型的内在安全自我意识，打破了安全性与实用性之间的权衡，实现了深度对齐。

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [184] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: 本研究提出了一种基于深度学习的强制选择神经认知诊断模型（FCNCD），有效提升了测试的准确性和可解释性，适用于多种强制选择测试场景。


<details>
  <summary>Details</summary>
Motivation: 在智能时代，心理测量测试在人员选拔、职业发展和心理健康评估中愈发重要。强制选择测试因其能降低回答失真的风险而广泛应用，但传统模型存在局限性。

Method: 研究提出了一种基于深度学习的强制选择神经认知诊断模型（FCNCD），通过非线性映射挖掘参与者和项目特征，并利用多层神经网络建模其交互作用，同时引入单调性假设增强结果的可解释性。

Result: 实验验证了FCNCD在真实和模拟数据集上的有效性，证明了其在准确性、可解释性和鲁棒性方面的优势。

Conclusion: FCNCD模型通过深度学习方法有效解决了传统模型在强制选择测试中的局限性，展现了高准确性、可解释性和鲁棒性，适用于多种测试场景。

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [185] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: 本文提出了一种基于差分进化（DE）的对抗提示优化方法，显著提高了RAG系统中错误文档的检索排名，同时避开检测，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 对抗提示攻击会显著影响检索增强生成（RAG）系统的可靠性，导致错误输出。本文旨在开发一种无需梯度的黑盒优化方法，以更接近真实场景的方式优化对抗提示后缀。

Method: 本文提出了一种基于差分进化（DE）的梯度无关方法，通过进化候选后缀种群来优化对抗提示，目标是最大化目标错误文档的检索排名。实验在BEIR QA数据集上进行，评估了不同检索应用下的攻击成功率。

Result: 实验结果显示，DE优化的对抗提示后缀在攻击成功率上优于或与现有方法（如GGPP和PRADA）相当，且仅需少量标记（<=5个）。此外，引入的可读性感知后缀构建策略显著降低了MLM负对数似然，且DE生成的后缀能有效避开BERT检测器的检测。

Conclusion: 通过差分进化（DE）优化的对抗提示后缀在RAG系统中表现出色，不仅能有效提高错误文档的检索排名，还能避开检测，展示了在实际应用中的潜力。

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [186] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: CAIS是一种基于因果推断的新型内在奖励，能有效过滤噪声并识别代理的因果影响，适用于嘈杂环境中的强化学习。


<details>
  <summary>Details</summary>
Motivation: 解决标准强化学习代理在嘈杂、生态有效场景中因依赖基于相关性的奖励而表现脆弱的问题。

Method: 引入因果行动影响分数（CAIS），这是一种基于因果推断的新型内在奖励。CAIS通过测量学习到的感官结果分布（条件于该行动）与基线结果分布之间的1-Wasserstein距离来量化行动的影响。

Result: CAIS使代理能够过滤噪声、识别其影响并学习正确策略。此外，CAIS学习的高质量预测模型使代理在增强惊喜信号后成功再现了“灭绝爆发”现象。

Conclusion: 明确推断因果关系是发展稳健自我效能感的关键机制，为更自适应的自主系统提供了心理学上合理的框架。

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [187] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: 论文提出了一种结合DL-Lite本体和自动化规划的新方法，复杂度与现有方法相当，并通过编译实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 将背景知识（如本体论）融入自动化规划问题，以解决封闭世界语义的局限性。

Method: 通过结合显式输入知识（eKABs）和本体感知动作效果，采用一致性更新语义，将问题多项式编译为经典规划。

Result: 新方法的复杂度与现有方法相当，且通过编译实现高效性能。

Conclusion: 该论文提出了一种结合DL-Lite本体和自动化规划的新方法，展示了其复杂度不高于现有方法，并通过多项式编译实现。实验评估验证了其性能。

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [188] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: CSI是一种AI框架，通过模拟专家推理诊断118种口腔疾病，标准模式下准确率达89.5%。


<details>
  <summary>Details</summary>
Motivation: 口腔疾病诊断存在广泛的病理重叠症状，亟需一种能模拟专家推理的临床有用诊断辅助工具。

Method: 结合了微调的多模态CLIP模型和专门的ChatGLM-6B语言模型，执行分层诊断推理树（HDRT）。

Result: 快速模式准确率为73.4%，标准模式提升至89.5%，性能提升直接归因于分层推理过程。

Conclusion: CSI框架通过模拟专家临床推理过程，显著提高了口腔疾病诊断的准确性，特别是在标准模式下。

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [189] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: 本文研究了沙特NEOM提出的170公里线性智能城市The Line中人类移动的可行性，通过混合仿真框架验证了自由移动的实现可能，并展示了AI系统在优化通勤时间和可达性方面的重要性。


<details>
  <summary>Details</summary>
Motivation: 评估在The Line这种前所未有的城市拓扑结构中，居民是否能够自由移动。

Method: 开发了一个混合仿真框架，结合了基于代理的建模、强化学习、监督学习和图神经网络，模拟了跨越50个垂直层次和不同密度场景的多模式交通行为。

Result: 实验显示，在完整AI集成架构下，代理的平均通勤时间为7.8至8.4分钟，满意度超过89%，可达性指数超过91%，即使在高峰拥堵期间也是如此。去除智能模块（如强化学习或图神经网络）会显著降低性能。

Conclusion: 研究结果表明，在The Line这种线性智能城市中，通过自适应AI系统、可持续基础设施和实时反馈循环的支持，自由移动不仅在概念上可行，而且在操作上也是现实的。

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [190] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover是一个代理框架，通过协调通用LLM与Lean 4环境的交互，无需模型专业化即实现95.9%的形式化证明成功率，展示了通用LLMs的定理证明潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管通用LLMs在复杂推理任务上表现出色，但在Lean 4等专业语言中生成形式化证明仍具挑战性，且当前方法需通过专用数据微调模型，成本高昂。

Method: Delta Prover是一个基于代理的框架，通过集成反射分解和迭代证明修复的算法框架，以及基于Lean 4的定制领域特定语言（DSL），协调通用LLM与Lean 4证明环境的交互。

Result: Delta Prover在miniF2F-test基准测试中取得了95.9%的成功率，超越了所有现有方法，包括需要模型专业化的方法，并展现出更强的测试时扩展规律。

Conclusion: Delta Prover框架展示了通用大语言模型（LLMs）在有效代理结构引导下具备强大的定理证明潜力，为形式化环境中的自动化推理提供了一种计算高效的专业模型替代方案。

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [191] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: 该研究提出软评估指标和轻量级神经网络，提升电弧故障诊断模型的可解释性和可信度，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于AI的电弧故障诊断模型在分类准确率上表现优异，但其输出的可信度存在问题。

Method: 提出一种软评估指标来解释电弧故障诊断模型的输出，并结合可解释人工智能和真实电弧故障实验。同时，提出一种轻量级平衡神经网络以保证竞争性准确率和软特征提取分数。

Result: 实验通过在两个不同采样时间和噪声水平的电弧故障数据集上测试多种传统机器学习和深度学习方法，验证了软评估指标的有效性。

Conclusion: 通过提出软评估指标和轻量级平衡神经网络，该研究使电弧故障诊断模型更易于理解和信任，帮助从业者做出明智且可信的决策。

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [192] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: DMGC是一种新型无监督多模态图聚类框架，通过分解混合图并融合互补视图，显著提升了聚类效果。


<details>
  <summary>Details</summary>
Motivation: 多模态图在无监督学习中尚未充分探索，且现实中的多模态图常表现出混合邻域模式（同质性和异质性关系并存），需要一种新方法来有效处理这种复杂性。

Method: 提出了一种名为DMGC的新框架，将原始混合图分解为同质性增强图和异质性感知图，并引入多模态双频融合机制进行联合过滤，同时采用自监督对齐目标指导学习过程。

Result: 在多种多模态和多关系图数据集上的实验表明，DMGC实现了最先进的性能。

Conclusion: DMGC框架通过分解多模态图为同质性和异质性增强的互补视图，并采用双频融合机制，显著提升了无监督多模态图聚类的性能，展现了其在不同设置下的有效性和泛化能力。

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [193] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: IM-Chat是一个基于LLM的多智能体框架，旨在解决注塑成型行业的知识传递问题，通过数据驱动和模块化设计实现高效任务解决，评估显示其在复杂场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 注塑成型行业面临保存和传递现场知识的关键挑战，尤其是经验丰富的工人退休和多语言障碍阻碍有效沟通。

Method: 研究引入了IM-Chat，一个基于大型语言模型（LLMs）的多智能体框架，采用检索增强生成（RAG）策略和工具调用智能体，通过模块化架构实现适应性，无需微调。

Result: 评估结果表明，更强大的模型在复杂、工具集成的场景中往往能实现更高的准确性。

Conclusion: 研究证明了多智能体LLM系统在工业知识工作流中的可行性，并将IM-Chat确立为一种可扩展且通用的方法，用于制造业中AI辅助决策支持。

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [194] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: 研究提出认知退化作为AI系统新脆弱性类别，并开发QSAF框架进行实时监控和缓解。


<details>
  <summary>Details</summary>
Motivation: 传统的外部威胁（如提示注入）已不足以应对AI系统的内部脆弱性，如内存饥饿、规划递归等，这些因素导致代理行为漂移和逻辑崩溃。

Method: 研究引入了Qorvex安全AI框架（QSAF Domain 10），包含七项运行时控制措施（QSAF-BC-001至BC-007），通过实时监控代理子系统并触发主动缓解措施来应对认知退化。

Result: QSAF框架通过六阶段认知退化生命周期和实时控制措施，有效检测并缓解了代理系统的认知退化问题。

Conclusion: 该研究确立了认知退化作为AI系统脆弱性的新类别，并提出了首个跨平台的防御模型，以增强代理行为的韧性。

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [195] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: 论文提出GRPO和OSPO两种新方法，避免传统MARL的价值函数估计问题，实验证明其在动态拼车平台中性能优越。


<details>
  <summary>Details</summary>
Motivation: 动态拼车平台面临实时匹配乘客与车辆的复杂性和不确定性，传统MARL方法因依赖准确估计Q/V值在大型不确定环境中表现不佳。

Method: 论文提出了两种新方法：1）将GRPO应用于拼车场景，用组平均奖励替代PPO基线以减少估计偏差；2）定制PPO框架，提出仅需一步奖励的OSPO方法。两者均通过简单MLP网络实现。

Result: 在真实曼哈顿拼车数据集上的实验表明，GRPO和OSPO在多数场景中性能优越，有效优化了接载时间和订单服务数量。

Conclusion: 论文提出的GRPO和OSPO方法在动态拼车平台中避免了传统MARL方法中的价值函数估计问题，显著提升了性能，尤其在优化接载时间和订单服务数量方面表现优异。

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [196] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: RAD通过检索高质量演示和扩散模型提升离线强化学习的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习的有效性受限于数据集稀疏和轨迹重叠不足，导致长时程规划困难。

Method: RAD结合非参数检索和扩散生成模型，动态检索高回报状态作为目标状态，并利用条件引导的扩散模型进行规划。

Result: 实验证明，RAD在多样化基准测试中表现优于或与基线方法相当。

Conclusion: RAD方法通过结合非参数检索和基于扩散的生成建模，有效解决了离线强化学习中数据集稀疏和轨迹重叠不足的问题，实现了竞争性或优于基线方法的性能。

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [197] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: 提出一种结合图注意力网络和LSTM的端到端模型，用于对象中心预测流程监控，在多个日志上表现优异。


<details>
  <summary>Details</summary>
Motivation: 利用对象中心事件日志增强流程预测，主要挑战在于提取相关信息并构建有效模型。

Method: 提出了一种端到端模型，结合图注意力网络编码活动及其关系，以及LSTM网络处理时间依赖性。

Result: 模型在预测未来流程行为的两个任务（下一活动预测和下一事件时间）上表现出色。

Conclusion: 该模型在真实和合成事件日志上表现出与现有最佳方法相竞争的性能。

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [198] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文提出了一种Pareto优化方法，通过干预启发式和元启发式算法，优化业务流程中的批处理策略，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在业务流程中，批处理策略需要在成本、处理时间和等待时间之间进行权衡。现有方法缺乏高效发现最优策略的手段，本文旨在填补这一空白。

Method: 提出了一种基于Pareto优化的方法，通过干预启发式识别改进机会，并结合三种元启发式算法（爬山法、模拟退火和强化学习）迭代更新Pareto前沿。

Result: 实验表明，基于干预启发式的方法在收敛性、多样性和周期时间增益方面优于非启发式引导的基线方法。

Conclusion: 本文通过Pareto优化方法，结合干预启发式和元启发式算法，有效发现了在等待时间、处理成本和资源利用之间达到最优权衡的批处理策略。实验评估验证了该方法的优越性。

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [199] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1是一种基于强化学习的图表领域视觉语言模型，通过两阶段训练策略（Chart-COT和Chart-RFT）实现了复杂的图表推理，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 验证R1-Style方法在多模态数据（尤其是图表领域）中的优势，解决图表推理数据缺乏的问题。

Method: 提出了两阶段训练策略：Chart-COT（逐步思维链监督）和Chart-RFT（数值敏感的强化微调）。

Result: 实验结果表明，Chart-R1在开源基准和自建数据集（ChartRQA）上表现优异。

Conclusion: Chart-R1展示了在图表领域视觉语言模型中的显著优势，甚至可与开源/闭源大型模型（如GPT-4o、Claude-3.5）相媲美。

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [200] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: HAMLET是一个多智能体框架，旨在通过自主决策和场景互动提升戏剧创作的交互性和沉浸感。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的戏剧生成方法缺乏主动性和环境互动性，且依赖详细用户输入，限制了实时在线表演的交互性和沉浸感。

Method: 提出HAMLET框架，通过生成叙事蓝图并赋予演员自主决策能力，结合场景道具状态变化来驱动即兴表演。

Result: 实验评估表明，HAMLET能创造表达力强且连贯的戏剧体验。

Conclusion: HAMLET通过多智能体自主决策和环境互动，显著提升了戏剧创作的交互性和沉浸感。

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [201] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: 研究通过滑轮系统问题测试LLMs的世界建模能力，发现其能利用统计关联和近似空间关系，但缺乏对复杂结构连接性的推理能力。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）是否构建和操作内部世界模型，还是仅依赖输出层词符概率的统计关联。

Method: 研究采用认知科学方法，通过三个实验测试LLMs在滑轮系统问题上的表现：实验1评估机械优势（MA）估计能力；实验2测试LLMs对系统全局特征的识别；实验3进一步探究LLMs对结构连接性的理解。

Result: 实验1显示LLMs在MA估计上略高于随机水平，但可能依赖滑轮计数启发式；实验2表明LLMs能区分功能性与随机系统（F1=0.8）；实验3显示LLMs在区分功能性与无效连接系统时表现随机（F1=0.46）。

Conclusion: 论文主张利用认知科学方法来评估人工智能系统的世界建模能力，并指出大型语言模型（LLMs）可能具备一定的内部世界模型操作能力，但在处理细微结构连接性方面存在局限。

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [202] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: 本文提出了三种技术，通过利用参数依赖和预处理动作修剪，显著提升安全策略改进（SPI）的数据效率，实证显示效率提升多个数量级。


<details>
  <summary>Details</summary>
Motivation: 安全策略改进（SPI）是一个离线强化学习问题，需要在仅使用数据集和行为策略的情况下计算出一个可靠优于行为策略的新策略。然而，许多应用中存在参数依赖关系未被充分利用，导致数据效率低下。本文旨在通过利用这些依赖关系和预处理技术来提高SPI的数据效率。

Method: 本文提出了三种贡献：(1) 利用已知分布间相关性来更准确估计转移动态的参数化SPI算法；(2) 通过基于游戏的抽象技术预处理来修剪冗余动作；(3) 基于可满足性模理论（SMT）求解的更高级预处理技术，能识别更多可修剪动作。

Result: 实证结果和消融研究表明，本文的技术将SPI的数据效率提高了多个数量级，同时保持了相同的可靠性保证。

Conclusion: 本文的结论是，通过利用参数依赖关系和引入预处理技术，显著提高了安全策略改进（SPI）的数据效率，同时保持了相同的可靠性保证。

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [203] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: 论文分析了多选问题评估中的指标与答案波动率的关系，提出最差准确率是最佳关联指标。


<details>
  <summary>Details</summary>
Motivation: 多选问题（MCQs）已成为评估LLM能力的标准方法，但现有研究未对相关指标进行全面评估，且存在答案波动问题。

Method: 作者提出了一种评估协议，通过将评估方法与波动率和原始性能联系起来进行分析。

Result: 研究结果表明，现有指标与答案变化之间存在强关联，即使在没有额外提示变体的情况下计算也是如此。

Conclusion: 论文提出了一种新的评估协议，通过分析现有指标与答案波动率的关系，发现最差准确率（worst accuracy）在协议中表现出最高的关联性。

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [204] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: 论文提出一种轻量级适配器方法，通过冻结预训练网络并添加战术调节模块，实现了《星际争霸II》AI代理的灵活战术控制，效果显著且计算高效。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理虽然强大，但缺乏根据高层战术指令调整策略的能力。本文旨在解决这一问题，提供灵活的战术控制。

Method: 该方法冻结了预训练的策略网络（DI-Star），并在每个动作头上附加轻量级适配器模块，这些模块受编码战略偏好的战术张量调节。通过KL散度约束训练适配器，确保策略在保持核心能力的同时展现战术变化。

Result: 实验结果表明，该方法能成功调节代理在侵略性、扩展模式和技术偏好等战术维度上的行为，同时保持竞争力。

Conclusion: 该论文提出了一种基于适配器的方法，成功实现了对《星际争霸II》AI代理的战术调节，能够在保持核心竞争力的同时灵活调整战术行为，且计算开销极小。

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [205] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: 本文探讨了代理型AI在复杂系统中自主处理异常的潜力，可替代传统依赖人类的方法。


<details>
  <summary>Details</summary>
Motivation: 传统异常管理方法高度依赖人类干预，效率低下且成本高昂。代理型AI有望提供更高效、自主的解决方案。

Method: 研究聚焦于代理型AI在复杂系统中的自主检测和响应异常的能力。

Result: 代理型AI展示了在复杂系统中自主检测和响应异常的潜力，能够显著提高效率和降低成本。

Conclusion: 本文探讨了代理型AI在复杂系统中自主检测和响应异常的潜力，强调了其改变传统依赖人类的异常管理方法的能力。

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [206] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: g-AMIE是一种多代理系统，通过异步监督框架在医疗诊断对话中表现优异，优于传统团队，提高了决策质量和效率。


<details>
  <summary>Details</summary>
Motivation: 由于诊断和治疗活动需要由持牌专业人员监管，且医师通常监督其他团队成员，因此需要一种有效的异步监督框架。

Method: 提出了一个多代理系统g-AMIE，通过历史采集在安全范围内运行，避免提供个性化医疗建议，并将评估结果传达给监督的主治医师。

Result: 在60个场景的随机盲测中，g-AMIE在高质量采集、病例总结和诊断管理计划方面优于NPs/PAs和PCPs组，且监督效率更高。

Conclusion: 研究表明，g-AMIE在异步监督模式下表现优异，为诊断AI系统在专家监督下增强现实世界护理提供了可行范式。

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [207] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: LAPO通过两阶段强化学习内化推理长度控制，减少令牌使用40.9%并提高准确率2.3%。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在简单问题上过度生成令牌的问题，将推理长度控制从外部约束转化为模型的内在能力。

Method: LAPO采用两阶段强化学习过程：第一阶段学习成功解决方案长度的统计分布，第二阶段将这些模式作为元认知指导直接嵌入模型的推理上下文中。

Result: 在数学推理基准测试中，LAPO减少了高达40.9%的令牌使用，同时提高了2.3%的准确率。

Conclusion: LAPO框架通过将推理长度控制内化为模型的内在能力，显著提高了大型推理模型的效率和准确性，同时减少了计算资源的消耗。

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [208] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: GasAgent 是多代理系统，自动化优化智能合约 Gas 消耗，兼容现有工具，实验显示显著节省 Gas。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案依赖手动发现 Gas 浪费模式，效率低且难以扩展。LLM 方法虽然探索了新模式，但兼容性差且需要手动验证。GasAgent 旨在填补这一空白。

Method: GasAgent 由四个专门代理（Seeker、Innovator、Executor 和 Manager）组成，它们在一个闭环中协作，识别、验证和应用 Gas 节省改进。

Result: 在 100 个真实世界合约上，GasAgent 成功优化了 82 个合约，平均部署 Gas 节省 9.97%。对 500 个由 LLM 生成的合约的评估显示，优化率达 79.8%，Gas 节省范围为 4.79% 至 13.93%。

Conclusion: GasAgent 是一个多代理系统，能够有效优化智能合约的 Gas 消耗，同时兼容现有模式并自动化发现和验证新模式，为 LLM 辅助的智能合约开发提供了优化层。

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [209] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: EAMI框架通过双视角思维追踪和动态分析，解决了复杂服务生态系统中异常涌现分析的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着服务计算、云计算和物联网的兴起，服务生态系统日益复杂，传统因果方法难以应对智能代理间的复杂交互。

Method: EAMI采用双视角思维追踪机制，结合k-means聚类和意图时间涌现图进行动态分析。

Result: 实验在复杂的线上到线下（O2O）服务系统和斯坦福AI Town实验中验证了EAMI的有效性、通用性和效率。

Conclusion: EAMI框架为服务生态系统中的异常涌现和因果分析提供了新的范式，通过实验验证了其有效性、通用性和高效性。

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [210] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: 本文系统分析了联邦学习（FL）与可信人工智能（TAI）对齐的挑战，总结了研究现状与未来方向。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）虽能解决隐私问题，但其分布式特性与可信人工智能（TAI）的其他要求对齐时存在挑战，需系统分析。

Method: 采用TAI的要求作为指导框架，系统分析FL与TAI对齐的挑战，并对每个挑战的现有研究、趋势和剩余工作进行详细探讨。

Result: 分类并探讨了FL与TAI对齐的关键障碍，总结了各挑战的研究现状、趋势及未解决问题。

Conclusion: 本文通过系统分析，识别并分类了联邦学习（FL）与可信人工智能（TAI）对齐过程中的关键挑战，总结了现有研究、趋势及未来工作方向。

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [211] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: 论文研究了在MPDAG设置下识别条件因果效应，提出了识别公式、推广了do calculus，并开发了一个完整算法。


<details>
  <summary>Details</summary>
Motivation: 研究在MPDAG设置下如何识别条件因果效应，以填补现有方法的不足。

Method: 使用MPDAG（最大定向部分有向无环图）来表示受背景知识限制的等价类图，其中因果模型中的所有变量均被观察。

Result: 提出了三个主要结果：1) 当调节集不受治疗影响时的识别公式；2) 将do calculus推广到MPDAG设置；3) 一个用于识别条件因果效应的完整算法。

Conclusion: 论文提供了在MPDAG设置下识别条件因果效应的三个结果：一个不受治疗影响的调节集的识别公式、将著名的do calculus推广到MPDAG设置的方法，以及一个用于识别这些条件效应的完整算法。

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [212] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: HBPO是一种强化学习框架，通过分层预算探索和差异化奖励，使模型能根据问题复杂度自动调整推理深度，显著提高计算效率且不牺牲能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在广泛链式思考生成中表现出显著的计算低效性，因为无论问题复杂度如何，它们都采用统一的推理策略。

Method: 提出了一种强化学习框架——分层预算策略优化（HBPO），通过分层预算探索和差异化的奖励机制，使模型能够根据问题复杂度自动调整推理深度。

Result: HBPO在四个推理基准测试中平均减少了60.6%的令牌使用，同时准确率提高了3.14%。

Conclusion: 研究结果表明，通过适当结构化的层次化训练（HBPO），可以同时优化推理效率和能力，证明两者并非固有冲突。

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [213] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 研究发现大语言模型在时间认知上表现出与人类相似的自发行为，包括建立主观时间参考点和遵循韦伯-费希纳定律。多维度分析揭示了其内部机制，为理解模型认知提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的进步，它们表现出与人类相似的认知模式，这些模式并未在训练数据中直接指定。本研究旨在探究这一现象，特别是LLMs在时间认知方面的表现。

Method: 通过相似性判断任务，研究发现大模型自发建立了主观时间参考点，并遵循韦伯-费希纳定律。研究在神经元、表征和信息层面进行了多维度分析，包括识别时间偏好神经元、探索年份表征的层次构建过程，以及利用预训练嵌入模型分析训练语料的固有时间结构。

Result: 研究发现，更大的模型自发建立了主观时间参考点，并遵循韦伯-费希纳定律。通过多层次分析，识别了时间偏好神经元，揭示了年份表征的层次构建过程，并发现训练语料本身具有非线性的时间结构。

Conclusion: 本研究提出了一个经验主义的视角来理解大语言模型（LLMs）的认知行为，认为其认知是内部表征系统对外部世界的主观构建。这一视角暗示了可能出现人类无法直观预测的陌生认知框架，为AI对齐提供了新的方向。

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [214] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: Gemini 2.5 Pro通过优化使用方式，在IMO 2025中解决了5/6问题，展示了模型在高难度数学任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在AIME等数学基准上表现良好，但在奥数级别的任务中仍存在挑战，因此探索如何优化模型使用以解决高难度数学问题。

Method: 采用Google的Gemini 2.5 Pro模型，通过管道设计和提示工程优化模型使用方式，避免数据污染。

Result: Gemini 2.5 Pro成功解决了IMO 2025中的5/6问题（存在一个需讨论的例外）。

Conclusion: 通过优化模型使用方式（如管道设计和提示工程），Gemini 2.5 Pro在IMO 2025问题上取得了显著进展，解决了5/6的问题，展示了强大模型在解决高难度数学问题中的潜力。

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [215] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: 研究探讨了代码上下文和提示策略对LLMs生成单元测试的影响，发现链式思考提示策略效果最佳，Gemini 2.5 Pro表现最优。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在软件工程中日益受到关注，自动生成单元测试可显著提升开发阶段的效率。

Method: 研究了代码上下文和提示策略对不同LLMs生成的单元测试质量和充分性的影响。

Result: 链式思考提示策略在分支覆盖率（96.3%）、平均变异分数（57%）和编译成功率方面表现最佳。Gemini 2.5 Pro在变异分数和分支覆盖率上表现最优。

Conclusion: 研究发现，包含文档字符串显著提高了代码的充分性，而进一步扩展上下文至完整实现带来的收益较小。链式思考提示策略在多种评估指标中表现最佳。

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [216] [Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects](https://arxiv.org/abs/2507.14330)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: VERIFAI项目探讨如何利用NLP、本体建模和LLMs等技术，自动化从自然语言需求生成形式化规范，以解决软件验证中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 确保软件正确性的关键在于从模糊的自然语言需求生成形式化规范，但目前这一过程仍面临巨大挑战。VERIFAI项目旨在探索自动化和半自动化方法来解决这一问题。

Method: 本文通过文献综述的方法，综合分析了生成可验证规范的关键挑战和潜在研究方向。

Result: 通过文献综述，本文识别了从非正式需求生成可验证规范过程中的常见挑战和潜在研究方向。

Conclusion: 本文总结了从非正式需求生成可验证规范的研究现状，并提出了VERIFAI项目的初步研究方向，旨在通过NLP、本体建模、构件重用和LLMs等技术弥合这一差距。

Abstract: Software correctness is ensured mathematically through formal verification,
which involves the resources of generating formal requirement specifications
and having an implementation that must be verified. Tools such as
model-checkers and theorem provers ensure software correctness by verifying the
implementation against the specification. Formal methods deployment is
regularly enforced in the development of safety-critical systems e.g.
aerospace, medical devices and autonomous systems. Generating these
specifications from informal and ambiguous natural language requirements
remains the key challenge. Our project, VERIFAI^{1}, aims to investigate
automated and semi-automated approaches to bridge this gap, using techniques
from Natural Language Processing (NLP), ontology-based domain modelling,
artefact reuse, and large language models (LLMs). This position paper presents
a preliminary synthesis of relevant literature to identify recurring challenges
and prospective research directions in the generation of verifiable
specifications from informal requirements.

</details>


### [217] [Developing Shared Vocabulary System For Collaborative Software Engineering](https://arxiv.org/abs/2507.14396)
*Carey Lai Zheng Hui,Johnson Britto Jessia Esther Leena,Kumuthini Subramanian,Zhao Chenyu,Shubham Rajeshkumar Jariwala*

Main category: cs.SE

TL;DR: 研究发现共享词汇系统能显著改善软件工程中的沟通效率，尽管初期有开销。研究采用DSR框架，通过问题识别、方法开发和实证验证三个阶段，最终证明了共享词汇系统的长期益处。


<details>
  <summary>Details</summary>
Motivation: 有效的沟通是成功软件工程协作的关键因素，但沟通差距仍然是一个持续挑战，常导致误解、低效和缺陷。本研究旨在探究导致此类误解的技术因素，并探讨在软件文档和代码库中建立共享词汇系统的可衡量益处。

Method: 研究采用设计科学研究（DSR）框架，分为三个迭代阶段：问题识别、方法开发和实证验证。问题识别阶段包括对沟通数据的主题分析和半结构化访谈。方法开发阶段基于扎根理论原则设计协作词汇开发的结构化方法。实证验证通过控制实验进行。

Result: 实证验证表明，共享词汇系统显著提高了信息密度、文档清晰度和协作效率，尽管初期采用会增加开销。

Conclusion: 研究发现，共享词汇系统虽然初期会增加开销，但长期能显著提升信息密度、文档清晰度和协作效率。研究结果为改进软件工程中的沟通实践提供了可操作的见解，并指出了未来研究的方向和局限性。

Abstract: Effective communication is a critical factor in successful software
engineering collaboration. However, communication gaps remain a persistent
challenge, often leading to misunderstandings, inefficiencies, and defects.
This research investigates the technical factors contributing to such
misunderstandings and explores the measurable benefits of establishing shared
vocabulary systems within software documentation and codebases. Using a Design
Science Research (DSR) framework, the study was structured into three iterative
phases: problem identification, method development, and empirical validation.
The problem identification phase involved thematic analysis of communication
data and semi-structured interviews, revealing key factors such as ambiguous
messaging, misalignment in documentation, inconsistent code review feedback,
and API integration miscommunication. Grounded Theory principles were employed
to design a structured methodology for collaborative vocabulary development.
Empirical validation through controlled experiments demonstrated that while
initial adoption introduced overhead, the shared vocabulary system
significantly improved information density, documentation clarity, and
collaboration efficiency over time. Findings offer actionable insights for
improving communication practices in software engineering, while also
identifying limitations and directions for future research.

</details>


### [218] [On the Effect of Token Merging on Pre-trained Models for Code](https://arxiv.org/abs/2507.14423)
*Mootez Saad,Hao Li,Tushar Sharma,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 研究提出合并代码语义单元隐藏表示的两种方法，实验显示可降低计算开销（最多19%），并在代码翻译中提升性能，但漏洞检测略有下降。


<details>
  <summary>Details</summary>
Motivation: 传统代码语言模型的标记化输出常比编译器/解释器更长，可能导致计算开销增加。研究旨在通过合并同一语义单元的隐藏表示，改善这一问题。

Method: 提出两种策略：1）基于平均隐藏表示；2）学习式方法。实验使用六种代码语言模型（CodeBERT、GraphCodeBERT等）在三个软件工程任务（漏洞检测、代码分类、代码翻译）中进行验证。

Result: 策略可减少浮点运算1%-19%。漏洞检测任务的F1分数下降1.82分，而代码翻译的CodeBLEU提升2.47分。

Conclusion: 本研究通过合并属于同一语义单元的隐藏表示，提出两种策略（基于平均表示和学习的方法），可无缝集成现有代码语言模型。实验表明，这些策略能减少浮点运算次数（1%至19%），并在代码翻译任务中提升性能（CodeBLEU提高2.47分），为提升代码语言模型的计算效率和下游性能提供了贡献。

Abstract: Tokenization is a fundamental component of language models for code. It
involves breaking down the input into units that are later passed to the
language model stack to learn high-dimensional representations used in various
contexts, from classification to generation. However, the output of these
tokenizers is often longer than that traditionally used in compilers and
interpreters. This could result in undesirable effects, such as increased
computational overhead. In this work, we investigate the effect of merging the
hidden representations of subtokens that belong to the same semantic unit, such
as subtokens that form a single identifier. We propose two strategies: one
based on averaging the representations and another that leverages a
learning-based approach. Both methods can be seamlessly integrated with
existing language models for code. We conduct experiments using six language
models for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),
and CodeT5+ (770M), across three software engineering tasks: vulnerability
detection, code classification, and code translation. Results show that these
strategies can reduce the number of floating-point operations by $1\%$ to
$19\%$. Regarding downstream performance, the most significant degradation was
observed in the vulnerability detection task, where the F1 score decreased by
$1.82$ points compared to the baseline. In contrast, for code translation, we
observed an improvement of $2.47$ points in CodeBLEU. This work contributes to
the broader effort of improving language models for code across multiple
dimensions, including both computational efficiency and downstream performance.

</details>


### [219] [Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches](https://arxiv.org/abs/2507.14547)
*Noman Ahmad,Ruoyu Su,Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 本研究通过文献综述统一了架构退化的定义、原因、指标和修复策略，发现检测方法成熟但持续修复不足，呼吁更全面的策略。


<details>
  <summary>Details</summary>
Motivation: 当前文献中对架构退化的定义、指标和修复策略呈现碎片化，本研究旨在通过整合学术和灰色文献中的定义、原因、指标、工具和修复方法，统一对架构退化的理解。

Method: 通过对108项研究进行多声部文献综述，提取了定义、原因、指标、测量方法、工具和修复策略，并开发了一个涵盖架构、代码和过程债务的分类法。

Result: 架构退化已从低层次问题转变为社会技术问题，定义涵盖了代码违规、设计漂移和结构衰败。原因包括架构债务（如文档不足）、代码债务（如匆忙修复）和过程债务（如知识流失）。研究识别了54个指标和31种测量技术，重点关注代码异味、内聚/耦合和演化。然而，大多数工具仅能检测问题，很少支持持续或预防性修复。

Conclusion: 研究表明，架构退化既是一个技术问题，也是一个组织问题。虽然检测方法已经较为成熟，但持续修复策略仍然不足。研究揭示了指标、工具和修复逻辑之间缺乏整合的问题，呼吁采取更全面、主动的策略以实现可持续架构。

Abstract: Architectural degradation, also known as erosion, decay, or aging, impacts
system quality, maintainability, and adaptability. Although widely
acknowledged, current literature shows fragmented definitions, metrics, and
remediation strategies. Our study aims to unify understanding of architectural
degradation by identifying its definitions, causes, metrics, tools, and
remediation approaches across academic and gray literature. We conducted a
multivocal literature review of 108 studies extracting definitions, causes,
metrics, measurement approaches, tools, and remediation strategies. We
developed a taxonomy encompassing architectural, code, and process debt to
explore definition evolution, methodological trends, and research gaps.
Architectural degradation has shifted from a low-level issue to a
socio-technical concern. Definitions now address code violations, design drift,
and structural decay. Causes fall under architectural (e.g., poor
documentation), code (e.g., hasty fixes), and process debt (e.g., knowledge
loss). We identified 54 metrics and 31 measurement techniques, focused on
smells, cohesion/coupling, and evolution. Yet, most tools detect issues but
rarely support ongoing or preventive remediation. Degradation is both technical
and organizational. While detection is well-studied, continuous remediation
remains lacking. Our study reveals missed integration between metrics, tools,
and repair logic, urging holistic, proactive strategies for sustainable
architecture.

</details>


### [220] [Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review](https://arxiv.org/abs/2507.14554)
*Ruoyu Su,Noman ahmad,Matteo Esposito,Andrea Janes,Davide Taibi,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 研究分析了五年内八大行业会议的5,677场演讲，发现Kubernetes等少数核心技术主导软件架构实践，主要应用于DevOps后期阶段，早期阶段关注较少。


<details>
  <summary>Details</summary>
Motivation: 随着云计算、微服务和容器的兴起，软件架构实践多样化，理解这些变化至关重要。

Method: 通过分析八大行业领先会议五年内的5,677场演讲，结合大型语言模型和专家验证，提取技术、其用途和使用背景。

Result: 在450项技术中，Kubernetes、Cloud Native、Serverless和Containers在频率和中心性上占主导地位。技术主要涉及部署、通信、AI和可观测性，分为五个技术社区：自动化、协调、云AI、监控和云边缘。大多数技术支持混合部署并跨越多个DevOps阶段。

Conclusion: 研究表明，少数核心技术（如Kubernetes和Serverless）主导了当代软件架构实践，主要应用于DevOps后期阶段，早期阶段如规划和编码关注有限。研究还揭示了从业者如何根据目的和背景框架技术，反映了行业优先级的演变。

Abstract: Software architecture plays a central role in the design, development, and
maintenance of software systems. With the rise of cloud computing,
microservices, and containers, architectural practices have diversified.
Understanding these shifts is vital. This study analyzes software architecture
trends across eight leading industry conferences over five years. We
investigate the evolution of software architecture by analyzing talks from top
practitioner conferences, focusing on the motivations and contexts driving
technology adoption. We analyzed 5,677 talks from eight major industry
conferences, using large language models and expert validation to extract
technologies, their purposes, and usage contexts. We also explored how
technologies interrelate and fit within DevOps and deployment pipelines. Among
450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate
by frequency and centrality. Practitioners present technology mainly related to
deployment, communication, AI, and observability. We identify five technology
communities covering automation, coordination, cloud AI, monitoring, and
cloud-edge. Most technologies span multiple DevOps stages and support hybrid
deployment. Our study reveals that a few core technologies, like Kubernetes and
Serverless, dominate the contemporary software architecture practice. These are
mainly applied in later DevOps stages, with limited focus on early phases like
planning and coding. We also show how practitioners frame technologies by
purpose and context, reflecting evolving industry priorities. Finally, we
observe how only research can provide a more holistic lens on architectural
design, quality, and evolution.

</details>


### [221] [Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library](https://arxiv.org/abs/2507.14558)
*Bin Duan,Tarek Mahmud,Meiru Che,Yan Yan,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: VISTAFUZZ 利用 LLMs 解析 OpenCV API 文档，生成约束和依赖关系，系统测试 API，检测并修复了多个 bug。


<details>
  <summary>Details</summary>
Motivation: OpenCV 作为最流行的开源计算机视觉库，其 bug 可能影响下游应用，因此确保其可靠性至关重要。

Method: VISTAFUZZ 利用 LLMs 解析 API 文档，提取输入参数的约束和依赖关系，并基于这些信息生成新的输入值，系统测试目标 API。

Result: 在测试 OpenCV 库的 330 个 API 中，VISTAFUZZ 检测到 17 个新 bug，其中 10 个已确认，5 个已修复。

Conclusion: VISTAFUZZ 是一种利用大型语言模型（LLMs）进行文档引导的模糊测试技术，有效提高了 OpenCV 库的可靠性，检测并修复了多个新 bug。

Abstract: The combination of computer vision and artificial intelligence is
fundamentally transforming a broad spectrum of industries by enabling machines
to interpret and act upon visual data with high levels of accuracy. As the
biggest and by far the most popular open-source computer vision library, OpenCV
library provides an extensive suite of programming functions supporting
real-time computer vision. Bugs in the OpenCV library can affect the downstream
computer vision applications, and it is critical to ensure the reliability of
the OpenCV library. This paper introduces VISTAFUZZ, a novel technique for
harnessing large language models (LLMs) for document-guided fuzzing of the
OpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain
standardized API information. Based on this standardized information, VISTAFUZZ
extracts constraints on individual input parameters and dependencies between
these. Using these constraints and dependencies, VISTAFUZZ then generates new
input values to systematically test each target API. We evaluate the
effectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the
results show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been
confirmed, and 5 of these have been fixed.

</details>


### [222] [A first look at License Variants in the PyPI Ecosystem](https://arxiv.org/abs/2507.14594)
*Weiwei Xu,Hengzhi Ye,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: 研究发现开源许可证变体普遍存在且导致合规问题，并提出LV-Parser和LV-Compat两种高效工具，显著提升许可证分析的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 开源许可证变体在软件重用中引入了显著的合规复杂性，但现有工具未能充分考虑其存在，导致许可证分析的效率和效果面临挑战。

Method: 研究采用了基于差异的技术和大型语言模型的LV-Parser方法，以及用于检测软件依赖网络中许可证不兼容性的LV-Compat自动化流程。

Result: 研究发现许可证变体在PyPI生态系统中普遍存在，其中仅2%涉及实质性修改，但这些变体导致10.7%的下游依赖存在许可证不兼容问题。LV-Parser的准确率达到0.936，计算成本降低30%，LV-Compat检测到的不兼容包数量是现有方法的5.2倍，精确度为0.98。

Conclusion: 该研究首次对软件包生态系统中的许可证变体进行了实证研究，并为开发者和组织提供了实用工具，以应对开源许可证的复杂性问题。

Abstract: Open-source licenses establish the legal foundation for software reuse, yet
license variants, including both modified standard licenses and custom-created
alternatives, introduce significant compliance complexities. Despite their
prevalence and potential impact, these variants are poorly understood in modern
software systems, and existing tools do not account for their existence,
leading to significant challenges in both effectiveness and efficiency of
license analysis. To fill this knowledge gap, we conduct a comprehensive
empirical study of license variants in the PyPI ecosystem. Our findings show
that textual variations in licenses are common, yet only 2% involve substantive
modifications. However, these license variants lead to significant compliance
issues, with 10.7% of their downstream dependencies found to be
license-incompatible.
  Inspired by our findings, we introduce LV-Parser, a novel approach for
efficient license variant analysis leveraging diff-based techniques and large
language models, along with LV-Compat, an automated pipeline for detecting
license incompatibilities in software dependency networks. Our evaluation
demonstrates that LV-Parser achieves an accuracy of 0.936 while reducing
computational costs by 30%, and LV-Compat identifies 5.2 times more
incompatible packages than existing methods with a precision of 0.98.
  This work not only provides the first empirical study into license variants
in software packaging ecosystem but also equips developers and organizations
with practical tools for navigating the complex landscape of open-source
licensing.

</details>


### [223] [An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions](https://arxiv.org/abs/2507.14687)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: The paper introduces 'Robin's Rule', an efficient algorithm for achieving 100% Unique-Cause MC/DC coverage for SBEs, validated with avionics benchmarks, proving its optimality and practicality.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the lack of efficient test generation methods for Unique-Cause MC/DC, despite its high assurance for reliability and safety in critical systems, especially given that 99.7% of conditional decisions in large-scale avionics systems are SBEs.

Method: The paper proposes 'Robin's Rule', a deterministic algorithm that constructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause MC/DC for SBEs with N conditions, without generating a full truth table.

Result: The results confirm that 'Robin's Rule' consistently achieves 100% coverage with the theoretical minimum number of tests and is more efficient than a certified commercial tool.

Conclusion: The paper concludes that 'Robin's Rule' provides a practical and provably optimal solution for verifying safety-critical systems, ensuring both rigor and efficiency in achieving 100% Unique-Cause MC/DC coverage for SBEs.

Abstract: Modified Condition/Decision Coverage (MC/DC) is a mandatory structural
coverage criterion for ensuring the reliability and safety of critical systems.
While its strictest form, Unique-Cause MC/DC, offers the highest assurance,
research on its efficient test generation has been lacking. This gap is
particularly significant, as an analysis of large-scale avionics systems shows
that 99.7% of all conditional decisions are, in fact, Singular Boolean
Expressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This
paper proposes 'Robin's Rule', a deterministic algorithm that directly
constructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause
MC/DC for SBEs with N conditions, without generating a full truth table. To
validate our approach, we constructed a benchmark by reformulating the TCAS-II
specifications into SBEs and verified the results using an industry-standard,
certified commercial tool. The results confirm that our method consistently
achieves 100% coverage with the theoretical minimum number of tests and is more
efficient than the commercial tool. This work provides a practical and provably
optimal solution for verifying safety-critical systems, ensuring both rigor and
efficiency.

</details>


### [224] [HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm](https://arxiv.org/abs/2507.14716)
*Shahidul Islam,Ashik Aowal,Md Sharif Uddin,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 研究开发了 HistoryFinder 工具，通过新基准和优化设计，显著提升了方法变更历史重建的准确性和效率，成为当前最佳选择。


<details>
  <summary>Details</summary>
Motivation: 高效准确地重建方法变更历史对软件工程任务（如维护、重构和理解）至关重要，但现有工具的评估因基准的不准确性而受限。

Method: 通过结合自动分析和专家指导的手动验证，系统地构建了两个新的基准（修正的 CodeShovel 基准和新开发的 HistoryFinder 基准），并开发了新的方法历史生成工具 HistoryFinder。

Result: 在 400 个方法上的评估显示，HistoryFinder 在精确度、召回率和 F1 分数上均优于其他工具，且运行时间具有竞争力。

Conclusion: HistoryFinder 在准确性和效率方面均表现出色，成为方法变更历史重建的最佳选择，支持多种使用方式以促进实际应用。

Abstract: Reconstructing a method's change history efficiently and accurately is
critical for many software engineering tasks, including maintenance,
refactoring, and comprehension. Despite the availability of method history
generation tools such as CodeShovel and CodeTracker, existing evaluations of
their effectiveness are limited by inaccuracies in the ground truth oracles
used. In this study, we systematically construct two new oracles -- the
corrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by
combining automated analysis with expert-guided manual validation. We also
introduce HistoryFinder, a new method history generation tool designed to
improve not only the accuracy and completeness of method change histories but
also to offer competitive runtime performance. Through extensive evaluation
across 400 methods from 40 open-source repositories, we show that HistoryFinder
consistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based
baselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder
achieves competitive runtime performance, offering the lowest mean and median
execution times among all the research-based tools.
  While Git-based tools exhibit the fastest runtimes, this efficiency comes at
the cost of significantly lower precision and recall -- leaving HistoryFinder
as the best overall choice when both accuracy and efficiency are important. To
facilitate adoption, we provide a web interface, CLI, and Java library for
flexible usage.

</details>


### [225] [Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling](https://arxiv.org/abs/2507.14735)
*Vladyslav Bulhakov,Giordano d'Aloisio,Claudio Di Sipio,Antinisca Di Marco,Davide Di Ruscio*

Main category: cs.SE

TL;DR: 通过超参数调优和提示工程优化Llama 3.1模型，显著提升其在多领域生成模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 通用大型语言模型（LLMs）在领域建模中存在局限性，而微调模型需要大量计算资源且可能导致灾难性遗忘等问题。

Method: 采用基于搜索的方法对特定医疗数据模型进行超参数调优，并在十个不同应用领域测试优化后的超参数。

Result: 优化后的超参数和提示工程相结合，在几乎所有测试领域模型中均提升了结果质量。

Conclusion: 结合超参数调优和提示工程可以显著提升Llama 3.1模型在生成领域模型时的准确性，尽管解决方案并非普遍适用。

Abstract: The introduction of large language models (LLMs) has enhanced automation in
software engineering tasks, including in Model Driven Engineering (MDE).
However, using general-purpose LLMs for domain modeling has its limitations.
One approach is to adopt fine-tuned models, but this requires significant
computational resources and can lead to issues like catastrophic forgetting.
  This paper explores how hyperparameter tuning and prompt engineering can
improve the accuracy of the Llama 3.1 model for generating domain models from
textual descriptions. We use search-based methods to tune hyperparameters for a
specific medical data model, resulting in a notable quality improvement over
the baseline LLM. We then test the optimized hyperparameters across ten diverse
application domains.
  While the solutions were not universally applicable, we demonstrate that
combining hyperparameter tuning with prompt engineering can enhance results
across nearly all examined domain models.

</details>


### [226] [Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions](https://arxiv.org/abs/2507.14770)
*Manaal Basha,Ivan Beschastnikh,Gema Rodriguez-Perez,Cleidson R. B. de Souza*

Main category: cs.SE

TL;DR: 研究探讨代码生成工具（CGT）使用中的性别差异，通过混合设计实验分析交互行为和任务表现，旨在推动包容性AI工具开发。


<details>
  <summary>Details</summary>
Motivation: 随着对代码生成工具（CGT）依赖的增加，其公平性和包容性问题日益凸显。尽管CGT可能提升生产力，但其在不同用户群体中的有效性尚未充分研究。我们假设开发者与CGT的交互因性别而异，影响任务结果和认知负荷，因已有研究表明性别差异可能影响技术使用和认知处理。

Method: 研究采用混合受试者设计，54名参与者按性别均分，采用平衡设计。参与者将完成两项编程任务（中等到高难度），分别仅使用CGT和仅使用互联网。任务顺序和条件将平衡以减轻顺序效应。数据收集包括认知负荷调查、屏幕录制和任务表现指标（如完成时间、代码正确性和CGT交互行为）。将进行统计分析以识别CGT使用中的显著差异。

Result: 预期贡献包括揭示开发者中CGT交互和表现的性别差异，为未来CGT设计提供参考，并帮助解决不同用户群体间的可用性和潜在交互模式差异。

Conclusion: 虽然结果尚未得出，但我们的提案为提升代码生成工具（CGT）设计中的公平性、问责制、透明度和伦理（FATE）奠定了基础。预期成果将有助于推动包容性AI实践和面向所有用户的公平工具开发。

Abstract: Context: The increasing reliance on Code Generation Tools (CGTs), such as
Windsurf and GitHub Copilot, are revamping programming workflows and raising
critical questions about fairness and inclusivity. While CGTs offer potential
productivity enhancements, their effectiveness across diverse user groups have
not been sufficiently investigated. Objectives: We hypothesize that developers'
interactions with CGTs vary based on gender, influencing task outcomes and
cognitive load, as prior research suggests that gender differences can affect
technology use and cognitive processing. Methods: The study will employ a
mixed-subjects design with 54 participants, evenly divided by gender for a
counterbalanced design. Participants will complete two programming tasks
(medium to hard difficulty) with only CGT assistance and then with only
internet access. Task orders and conditions will be counterbalanced to mitigate
order effects. Data collection will include cognitive load surveys, screen
recordings, and task performance metrics such as completion time, code
correctness, and CGT interaction behaviors. Statistical analyses will be
conducted to identify statistically significant differences in CGT usage.
Expected Contributions: Our work can uncover gender differences in CGT
interaction and performance among developers. Our findings can inform future
CGT designs and help address usability and potential disparities in interaction
patterns across diverse user groups. Conclusion: While results are not yet
available, our proposal lays the groundwork for advancing fairness,
accountability, transparency, and ethics (FATE) in CGT design. The outcomes are
anticipated to contribute to inclusive AI practices and equitable tool
development for all users.

</details>


### [227] [VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs](https://arxiv.org/abs/2507.14776)
*Kimia Tasnia,Alexander Garcia,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.SE

TL;DR: VeriOpt 是一个结合角色分工和 PPA 优化的框架，显著提升 LLM 生成 Verilog 的工业适用性。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 在硬件设计中主要关注功能正确性，忽视了工业级设计的关键 PPA 指标，VeriOpt 旨在填补这一空白。

Method: VeriOpt 采用角色分工（如规划师、程序员、审阅者、评估者）和 PPA 感知优化，结合多模态反馈（如合成报告、时序图），优化 LLM 的 Verilog 生成。

Result: 实验显示，VeriOpt 使功耗降低 88%、面积减少 76%、时序收敛提升 73%，功能正确性达 86%。

Conclusion: VeriOpt 通过结合角色分工和 PPA 优化，显著提升了 LLM 生成的 Verilog 代码的质量和工业适用性，为 AI 驱动的硬件设计开辟了新途径。

Abstract: The rapid adoption of large language models(LLMs) in hardware design has
primarily focused on generating functionally correct Verilog code, overlooking
critical Power Performance-Area(PPA) metrics essential for industrial-grade
designs. To bridge this gap, we propose VeriOpt, a novel framework that
leverages role-based prompting and PPA-aware optimization to enable LLMs to
produce high-quality, synthesizable Verilog. VeriOpt structures LLM
interactions into specialized roles (e.g., Planner, Programmer, Reviewer,
Evaluator) to emulate human design workflows, while integrating PPA constraints
directly into the prompting pipeline. By combining multi-modal feedback (e.g.,
synthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves
PPA-efficient code generation without sacrificing functional correctness.
Experimental results demonstrate up to 88% reduction in power, 76% reduction in
area and 73% improvement in timing closure compared to baseline LLM-generated
RTL, validated using industry standard EDA tools. At the same time achieves 86%
success rate in functionality evaluation. Our work advances the
state-of-the-art AI-driven hardware design by addressing the critical gap
between correctness and quality, paving the way for reliable LLM adoption in
production workflows.

</details>


### [228] [Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context](https://arxiv.org/abs/2507.14791)
*Yang Liu,Li Zhang,Fang Liu,Zhuohang Wang,Donglin Wei,Zhishuo Yang,Kechi Zhang,Jia Li,Lin Shi*

Main category: cs.SE

TL;DR: RepoScope利用调用链感知的多视角上下文改进仓库级代码生成，通过静态分析提升效率和通用性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效识别捕获仓库丰富语义的真正相关上下文，且上下文视角狭窄，未能考虑检索代码的结构关系。

Method: 构建Repository Structural Semantic Graph (RSSG)，检索全面的四视角上下文，结合结构和基于相似性的上下文，并提出调用链预测方法和结构保持序列化算法。

Result: 在CoderEval和DevEval基准测试中，RepoScope在pass@1分数上实现了高达36.35%的相对提升。

Conclusion: RepoScope通过利用调用链感知的多视角上下文，显著提升了仓库级代码生成的性能，并在多个基准测试中表现优于现有方法。

Abstract: Repository-level code generation aims to generate code within the context of
a specified repository. Existing approaches typically employ
retrieval-augmented generation (RAG) techniques to provide LLMs with relevant
contextual information extracted from the repository. However, these approaches
often struggle with effectively identifying truly relevant contexts that
capture the rich semantics of the repository, and their contextual perspectives
remains narrow. Moreover, most approaches fail to account for the structural
relationships in the retrieved code during prompt construction, hindering the
LLM's ability to accurately interpret the context. To address these issues, we
propose RepoScope, which leverages call chain-aware multi-view context for
repository-level code generation. RepoScope constructs a Repository Structural
Semantic Graph (RSSG) and retrieves a comprehensive four-view context,
integrating both structural and similarity-based contexts. We propose a novel
call chain prediction method that utilizes the repository's structural
semantics to improve the identification of callees in the target function.
Additionally, we present a structure-preserving serialization algorithm for
prompt construction, ensuring the coherence of the context for the LLM.
Notably, RepoScope relies solely on static analysis, eliminating the need for
additional training or multiple LLM queries, thus ensuring both efficiency and
generalizability. Evaluation on widely-used repository-level code generation
benchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms
state-of-the-art methods, achieving up to a 36.35% relative improvement in
pass@1 scores. Further experiments emphasize RepoScope's potential to improve
code generation across different tasks and its ability to integrate effectively
with existing approaches.

</details>


### [229] [Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review](https://arxiv.org/abs/2507.14969)
*Sai Zhang,Zhenchang Xing,Jieshan Chen,Dehai Zhao,Zizhong Zhu,Xiaowang Zhang,Zhiyong Feng,Xiaohong Li*

Main category: cs.SE

TL;DR: RequireCEG是一种需求获取与自审代理，通过因果效应图（CEGs）和神经符号协作架构，解决了终端用户需求模糊问题，显著提升了生成软件开发的覆盖率和多样性。


<details>
  <summary>Details</summary>
Motivation: 终端用户缺乏软件工程知识，其需求描述常存在模糊性，给生成软件开发带来挑战。现有方法（如Gherkin）难以表达前提与行为间的因果逻辑。

Method: RequireCEG首先使用特征树分层分析用户叙述，明确软件组件范围及其系统行为需求；然后构建自修复的CEGs，捕捉原子前提和行为动作之间的因果关系；最后用CEGs审查和优化Gherkin场景，确保生成的需求与用户叙述一致。

Result: 在RGPair基准数据集上的实验表明，RequireCEG实现了87%的覆盖率和51.88%的多样性提升。

Conclusion: RequireCEG通过神经符号协作架构嵌入因果效应图（CEGs），显著提高了生成软件开发的覆盖率和多样性，有效解决了终端用户需求描述模糊的问题。

Abstract: The vision of End-User Software Engineering (EUSE) is to empower
non-professional users with full control over the software development
lifecycle. It aims to enable users to drive generative software development
using only natural language requirements. However, since end-users often lack
knowledge of software engineering, their requirement descriptions are
frequently ambiguous, raising significant challenges to generative software
development. Although existing approaches utilize structured languages like
Gherkin to clarify user narratives, they still struggle to express the causal
logic between preconditions and behavior actions. This paper introduces
RequireCEG, a requirement elicitation and self-review agent that embeds
causal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.
RequireCEG first uses a feature tree to analyze user narratives hierarchically,
clearly defining the scope of software components and their system behavior
requirements. Next, it constructs the self-healing CEGs based on the elicited
requirements, capturing the causal relationships between atomic preconditions
and behavioral actions. Finally, the constructed CEGs are used to review and
optimize Gherkin scenarios, ensuring consistency between the generated Gherkin
requirements and the system behavior requirements elicited from user
narratives. To evaluate our method, we created the RGPair benchmark dataset and
conducted extensive experiments. It achieves an 87% coverage rate and raises
diversity by 51.88%.

</details>


### [230] [The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering](https://arxiv.org/abs/2507.15003)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: AIDev是首个大规模数据集，记录了AI编码代理在真实环境中的操作，为研究AI在软件开发中的自主协作提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 研究AI队友（如自主编码代理）在软件开发中的实际应用和影响，填补了理论研究和实际数据之间的空白。

Method: 通过收集和分析来自五个领先的AI编码代理（如OpenAI Codex、GitHub Copilot等）在61,000个仓库中的456,000个拉取请求的数据，构建了AIDev数据集。

Result: 尽管AI代理在速度上通常优于人类，但其拉取请求的接受率较低，揭示了信任和效用差距。同时，AI代理加速了代码提交，但提交的代码结构上更简单。

Conclusion: AIDev数据集为研究AI在软件开发中的自主协作提供了前所未有的实证基础，支持了AI原生工作流程的研究，并促进了人机协作的下一波发展。

Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI
teammates: autonomous, goal-driven systems collaborating with human developers.
Among these, autonomous coding agents are especially transformative, now
actively initiating, reviewing, and evolving code at scale. This paper
introduces AIDev, the first large-scale dataset capturing how such agents
operate in the wild. Spanning over 456,000 pull requests by five leading
agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across
61,000 repositories and 47,000 developers, AIDev provides an unprecedented
empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software
engineering, AIDev offers structured, open data to support research in
benchmarking, agent readiness, optimization, collaboration modeling, and AI
governance. The dataset includes rich metadata on PRs, authorship, review
timelines, code changes, and integration outcomes--enabling exploration beyond
synthetic benchmarks like SWE-bench. For instance, although agents often
outperform humans in speed, their PRs are accepted less frequently, revealing a
trust and utility gap. Furthermore, while agents accelerate code
submission--one developer submitted as many PRs in three days as they had in
three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for
the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev
enables a new generation of research into AI-native workflows and supports
building the next wave of symbiotic human-AI collaboration. The dataset is
publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering
Agent

</details>


### [231] [Survey of GenAI for Automotive Software Development: From Requirements to Executable Code](https://arxiv.org/abs/2507.15025)
*Nenad Petrovic,Vahid Zolfaghari,Andre Schamschurko,Sven Kirchner,Fengjunjie Pan,Chengdng Wu,Nils Purschke,Aleksei Velsh,Krzysztof Lebioda,Yinglei Song,Yi Zhang,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: 本文探讨了GenAI在汽车软件开发中的应用，重点研究了需求处理、合规性检查和代码生成，并提出了一个通用工作流程。调查显示行业已开始使用GenAI工具。


<details>
  <summary>Details</summary>
Motivation: 汽车软件开发过程冗长且昂贵，涉及大量需求和严格的标准化。GenAI的引入旨在减少人工干预和复杂底层流程的处理工作，从而革新这一领域。

Method: 本文采用了文献综述的方法，探讨了GenAI在汽车软件开发中的多个步骤，包括需求处理、合规性方面和代码生成，并介绍了三种GenAI相关技术：大型语言模型（LLMs）、检索增强生成（RAG）和视觉语言模型（VLMs）。

Result: 研究发现GenAI技术在汽车软件开发中具有广泛的应用前景，尤其是在需求处理、合规性检查和代码生成方面。此外，调查显示行业已开始采用GenAI工具辅助日常工作。

Conclusion: 本文总结了GenAI在汽车软件开发中的应用潜力，并提出了一个通用的GenAI辅助工作流程，同时分享了行业合作伙伴对GenAI工具使用情况的调查结果。

Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to
revolutionize many industrial areas by reducing the amount of human
intervention needed and effort for handling complex underlying processes.
Automotive software development is considered to be a significant area for
GenAI adoption, taking into account lengthy and expensive procedures, resulting
from the amount of requirements and strict standardization. In this paper, we
explore the adoption of GenAI for various steps of automotive software
development, mainly focusing on requirements handling, compliance aspects and
code generation. Three GenAI-related technologies are covered within the
state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation
(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting
techniques in case of code generation. Additionally, we also derive a
generalized GenAI-aided automotive software development workflow based on our
findings from this literature review. Finally, we include a summary of a survey
outcome, which was conducted among our automotive industry partners regarding
the type of GenAI tools used for their daily work activities.

</details>


### [232] [Can LLMs Generate User Stories and Assess Their Quality?](https://arxiv.org/abs/2507.15157)
*Giovanni Quattrocchi,Liliana Pasquale,Paola Spoletini,Luciano Baresi*

Main category: cs.SE

TL;DR: 本研究探讨了LLMs在敏捷框架中自动生成用户故事(US)的能力，并与人类生成的US进行质量比较。结果表明，LLMs在覆盖范围和文体质量上接近人类，但在多样性和创造性上表现较差。LLMs在提供明确评估标准时可可靠评估US的语义质量。


<details>
  <summary>Details</summary>
Motivation: 由于需求分析师在理解和转化复杂需求时面临挑战，且评估语义质量仍为手动耗时活动，本研究旨在探索LLMs如何自动化需求启发和语义质量评估。

Method: 使用10种先进LLMs模拟客户访谈自动生成US，并将其与人类（领域专家和学生）生成的US在质量上进行比较，同时探索LLMs自动评估US语义质量的能力。

Result: LLMs生成的US在覆盖范围和文体质量上与人类相似，但多样性和创造性较低。LLMs在明确评估标准下可可靠评估US语义质量，有望减少大规模评估中的人力投入。

Conclusion: LLMs可辅助自动化需求启发和语义质量评估，尽管在多样性和创造性上存在不足，但在明确标准下表现可靠，具有减少人力成本的潜力。

Abstract: Requirements elicitation is still one of the most challenging activities of
the requirements engineering process due to the difficulty requirements
analysts face in understanding and translating complex needs into concrete
requirements. In addition, specifying high-quality requirements is crucial, as
it can directly impact the quality of the software to be developed. Although
automated tools allow for assessing the syntactic quality of requirements,
evaluating semantic metrics (e.g., language clarity, internal consistency)
remains a manual and time-consuming activity. This paper explores how LLMs can
help automate requirements elicitation within agile frameworks, where
requirements are defined as user stories (US). We used 10 state-of-the-art LLMs
to investigate their ability to generate US automatically by emulating customer
interviews. We evaluated the quality of US generated by LLMs, comparing it with
the quality of US generated by humans (domain experts and students). We also
explored whether and how LLMs can be used to automatically evaluate the
semantic quality of US. Our results indicate that LLMs can generate US similar
to humans in terms of coverage and stylistic quality, but exhibit lower
diversity and creativity. Although LLM-generated US are generally comparable in
quality to those created by humans, they tend to meet the acceptance quality
criteria less frequently, regardless of the scale of the LLM model. Finally,
LLMs can reliably assess the semantic quality of US when provided with clear
evaluation criteria and have the potential to reduce human effort in
large-scale assessments.

</details>


### [233] [Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements](https://arxiv.org/abs/2507.15181)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Yanzhou Mu,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: DLMMM提出了一种新型深度学习框架测试方法，通过融合多种模型测量指标和多级启发式指导，解决了现有方法的局限性，提升了测试效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习框架测试方法存在三个关键局限性：未能定量测量算子组合多样性、忽略模型执行时间测量、忽视不同测量指标之间的相关性。

Method: DLMMM首先定量测量模型的错误检测性能、算子组合多样性和模型执行时间，然后基于这些测量指标的相关性进行融合以实现权衡。此外，DLMMM设计了多级启发式指导来生成测试输入模型。

Result: DLMMM是首个将多种模型测量纳入启发式指导并实现权衡的深度学习框架测试方法，显著提升了测试效果。

Conclusion: DLMMM通过融合多种模型测量指标并设计多级启发式指导，显著提升了深度学习框架测试的效果，克服了现有方法的局限性。

Abstract: Deep learning frameworks serve as the foundation for developing and deploying
deep learning applications. To enhance the quality of deep learning frameworks,
researchers have proposed numerous testing methods using deep learning models
as test inputs. However, existing methods predominantly measure model bug
detection effectiveness as heuristic indicators, presenting three critical
limitations: Firstly, existing methods fail to quantitatively measure model's
operator combination variety, potentially missing critical operator
combinations that could trigger framework bugs. Secondly, existing methods
neglect measuring model execution time, resulting in the omission of numerous
models potential for detecting more framework bugs within limited testing time.
Thirdly, existing methods overlook correlation between different model
measurements, relying simply on single-indicator heuristic guidance without
considering their trade-offs. To overcome these limitations, we propose DLMMM,
the first deep learning framework testing method to include multiple model
measurements into heuristic guidance and fuse these measurements to achieve
their trade-off. DLMMM firstly quantitatively measures model's bug detection
performance, operator combination variety, and model execution time. After
that, DLMMM fuses the above measurements based on their correlation to achieve
their trade-off. To further enhance testing effectiveness, DLMMM designs
multi-level heuristic guidance for test input model generation.

</details>


### [234] [Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View](https://arxiv.org/abs/2507.15188)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 研究探讨了孟加拉文化对需求工程（RE）活动的影响，强调了文化意识在避免冲突和促进IT行业多样性中的重要性。


<details>
  <summary>Details</summary>
Motivation: 鉴于需求工程是软件开发中互动最密集的阶段，且软件项目涉及多样化的利益相关者，理解文化影响（CIs）有助于避免误解和冲突，同时支持IT行业的多样性和包容性。孟加拉国作为一个IT行业快速增长但研究较少的国家，其独特的文化特性为研究提供了独特视角。

Method: 通过研究孟加拉国IT行业中需求工程的实践，分析文化因素对RE活动的影响。

Result: 研究发现孟加拉文化对RE活动的具体影响，为RE实践者提供了避免文化误解的策略，并支持了IT行业的多样性和包容性。

Conclusion: 该研究探讨了孟加拉文化背景下需求工程（RE）的采用情况及其对RE活动的文化影响，强调了文化意识在促进IT行业多样性和包容性的重要性。

Abstract: Requirements Engineering (RE) is one of the most interaction-intensive phases
of software development. This means that RE activities might be especially
impacted by stakeholders' national culture. Software development projects
increasingly have a very diverse range of stakeholders. To future-proof RE
activities, we need to help RE practitioners avoid misunderstandings and
conflicts that might arise from not understanding potential Cultural Influences
(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT
profession. Bangladesh has a growing IT sector with some unique socio-cultural
characteristics, and has been largely overlooked in this research field. In
this study, we aim to investigate how the RE process is adopted in the context
of Bangladeshi culture and what cultural influences impact overall RE
activities.

</details>


### [235] [Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?](https://arxiv.org/abs/2507.15197)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 该研究通过系统性映射分析了需求工程中人物角色的最新趋势，发现生成式AI在人物角色构建和验证中的应用增多，模板化角色更受欢迎，验证研究比例上升。


<details>
  <summary>Details</summary>
Motivation: 探索需求工程中人物角色的最新研究趋势，特别是生成式AI方法对人物角色构建和验证的影响。

Method: 本研究采用系统性映射研究（SMS）方法，覆盖了2023年4月至2025年4月期间的22篇相关文献，分析了人物角色的表示、构建、验证以及涉及的需求工程活动。

Result: 研究发现，许多研究应用了基于AI的解决方案来构建和验证人物角色。模板化人物角色越来越流行，验证方面的研究比例也有所增加。

Conclusion: 该系统性映射研究（SMS）揭示了在需求工程（RE）中，人物角色（personas）的应用趋势，尤其是生成式AI方法在人物角色构建和验证中的增长。模板化人物角色越来越受欢迎，验证方面的研究比例也有所增加。

Abstract: In requirements engineering (RE), personas are now being used to represent
user expectations and needs. This systematic mapping study (SMS) aims to
explore the most recent studies and to cover recent changes in trends,
especially related to the recent evolution of Generative AI approaches. Our SMS
covers the period between April 2023 and April 2025. We identified 22 relevant
publications and analysed persona representation, construction, validation, as
well as RE activities covered by personas. We identified that a number of
studies applied AI-based solutions for persona construction and validation. We
observed that template-based personas are becoming more popular nowadays. We
also observed an increase in the proportion of studies covering validation
aspects.

</details>


### [236] [SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation](https://arxiv.org/abs/2507.15224)
*Yibo He,Shuoran Zhao,Jiaming Huang,Yingjie Fu,Hao Yu,Cunjian Huang,Tao Xie*

Main category: cs.SE

TL;DR: SimdBench是首个针对SIMD-intrinsic代码生成的基准测试，评估了18种LLMs的表现，发现其在SIMD代码生成中表现较差，但提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 填补现有代码生成基准测试在SIMD-intrinsic代码生成方面的空白，评估LLMs在此领域的表现。

Method: 提出了SimdBench，第一个专门针对SIMD-intrinsic代码生成的基准测试，包含136个任务，覆盖五种代表性SIMD指令集。

Result: LLMs在SIMD-intrinsic代码生成中的pass@k普遍低于标量代码生成。

Conclusion: LLMs在SIMD-intrinsic代码生成方面表现普遍不如标量代码生成，但研究为LLMs在这一领域的进一步发展指明了方向。

Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler
intrinsics are widely supported by modern processors to accelerate
performance-critical tasks. SIMD intrinsic programming, a trade-off between
coding productivity and high performance, is widely used in the development of
mainstream performance-critical libraries and daily computing tasks. Large
Language Models (LLMs), which have demonstrated strong and comprehensive
capabilities in code generation, show promise in assisting programmers with the
challenges of SIMD intrinsic programming. However, existing code-generation
benchmarks focus on only scalar code, and it is unclear how LLMs perform in
generating vectorized code using SIMD intrinsics. To fill this gap, we propose
SimdBench, the first code benchmark specifically designed for SIMD-intrinsic
code generation, comprising 136 carefully crafted tasks and targeting five
representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86
Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM
Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a
systematic evaluation (measuring both correctness and performance) of 18
representative LLMs on SimdBench, resulting in a series of novel and insightful
findings. Our evaluation results demonstrate that LLMs exhibit a universal
decrease in pass@k during SIMD-intrinsic code generation compared to
scalar-code generation. Our in-depth analysis highlights promising directions
for the further advancement of LLMs in the challenging domain of SIMD-intrinsic
code generation. SimdBench is fully open source at
https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader
research community.

</details>


### [237] [Code Clone Detection via an AlphaFold-Inspired Framework](https://arxiv.org/abs/2507.15226)
*Changguo Jia,Yi Zhan,Tianqi Zhao,Hengzhi Ye,Minghui Zhou*

Main category: cs.SE

TL;DR: AlphaCC利用AlphaFold的序列建模能力，通过令牌序列和MSA增强语义理解，实现了跨语言的高效代码克隆检测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有代码克隆检测方法在捕捉代码语义或跨语言适用性上存在不足，受AlphaFold在蛋白质序列预测中的成功启发，提出利用其序列到结构的建模能力来推断代码语义。

Method: AlphaCC将代码片段转化为令牌序列，利用类似AlphaFold的多序列比对（MSA）增强上下文理解，采用基于AlphaFold的注意力编码器建模依赖关系，最后通过延迟交互策略计算相似度并进行二分类。

Result: 在三个多语言数据集上的综合评估表明，AlphaCC在语义克隆检测上优于所有基线方法，同时保持了较高的效率。

Conclusion: AlphaCC在跨多种编程语言的代码克隆检测中表现出色，展现了强大的语义理解能力，同时保持了高效的性能，适用于大规模克隆检测任务。

Abstract: Code clone detection, which aims to identify functionally equivalent code
fragments, plays a critical role in software maintenance and vulnerability
analysis. Substantial methods have been proposed to detect code clones, but
they fall short in capturing code semantics or relying on language-specific
analyzers. Inspired by the remarkable success of AlphaFold in predicting
three-dimensional protein structures from protein sequences, in this paper, we
leverage AlphaFold for code clone detection based on the insight that protein
sequences and token sequences share a common linear sequential structure. In
particular, we propose AlphaCC, which represents code fragments as token
sequences to ensure multi-language applicability and adapts AlphaFold's
sequence-to-structure modeling capability to infer code semantics. The pipeline
of AlphaCC goes through three steps. First, AlphaCC transforms each input code
fragment into a token sequence and, motivated by AlphaFold's use of multiple
sequence alignment (MSA) to enhance contextual understanding, constructs an MSA
from lexically similar token sequences. Second, AlphaCC adopts a modified
attention-based encoder based on AlphaFold to model dependencies within and
across token sequences. Finally, unlike AlphaFold's protein structure
prediction task, AlphaCC computes similarity scores between token sequences
through a late interaction strategy and performs binary classification to
determine code clone pairs. Comprehensive evaluations on three language-diverse
datasets demonstrate AlphaCC's applicability across multiple programming
languages. On two semantic clone detection datasets, it consistently
outperforms all baselines, showing strong semantic understanding. Moreover,
AlphaCC maintains competitive efficiency, enabling practical usage in
large-scale clone detection tasks.

</details>


### [238] [FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents](https://arxiv.org/abs/2507.15241)
*Vikram Nitin,Baishakhi Ray,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: FaultLine是一种基于LLM代理的自动化PoV测试生成工具，通过分层推理在多语言漏洞数据集上表现优于现有技术，提升了77%的生成成功率。


<details>
  <summary>Details</summary>
Motivation: 软件安全漏洞报告常缺少验证修复的PoV测试，这些测试对确保补丁有效和帮助开发者理解漏洞利用方式至关重要，但生成此类测试因程序控制流和数据流的复杂性而极具挑战性。

Method: FaultLine是一个LLM代理工作流，结合了传统静态和动态程序分析的推理步骤，自动生成PoV测试用例。它通过追踪输入流、推理分支条件，并在反馈驱动的循环中生成测试用例。

Result: 在包含100个已知漏洞的多语言数据集上，FaultLine成功为16个项目生成PoV测试，优于现有技术CodeAct 2.1（9个项目），相对提升77%。

Conclusion: FaultLine通过分层推理显著提升了LLM代理在PoV测试生成上的性能，但该问题总体上仍具挑战性。作者公开了代码和数据集以促进进一步研究。

Abstract: Despite the critical threat posed by software security vulnerabilities,
reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests
needed to validate fixes and prevent regressions. These tests are crucial not
only for ensuring patches work, but also for helping developers understand how
vulnerabilities can be exploited. Generating PoV tests is a challenging
problem, requiring reasoning about the flow of control and data through deeply
nested levels of a program.
  We present FaultLine, an LLM agent workflow that uses a set of carefully
designed reasoning steps, inspired by aspects of traditional static and dynamic
program analysis, to automatically generate PoV test cases. Given a software
project with an accompanying vulnerability report, FaultLine 1) traces the flow
of an input from an externally accessible API ("source") to the "sink"
corresponding to the vulnerability, 2) reasons about the conditions that an
input must satisfy in order to traverse the branch conditions encountered along
the flow, and 3) uses this reasoning to generate a PoV test case in a
feedback-driven loop. FaultLine does not use language-specific static or
dynamic analysis components, which enables it to be used across programming
languages.
  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100
known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine
is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct
2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine
represents a 77% relative improvement over the state of the art. Our findings
suggest that hierarchical reasoning can enhance the performance of LLM agents
on PoV test generation, but the problem in general remains challenging. We make
our code and dataset publicly available in the hope that it will spur further
research in this area.

</details>


### [239] [Input Reduction Enhanced LLM-based Program Repair](https://arxiv.org/abs/2507.15251)
*Boyang Yang,Luyao Ren,Xin Yin,Jiadong Ren,Haoye Tian,Shunfu Jin*

Main category: cs.SE

TL;DR: ReduceFix是一种基于LLM的自动程序修复方法，通过自动减少测试输入长度提升修复性能，实验显示其显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: LLMs在长提示中难以保留关键信息，当测试输入过长时可能触发“中间丢失”问题，影响修复性能。

Method: 提出ReduceFix，一种基于LLM的APR方法，内置组件自动减少测试输入同时保留其失败诱导行为。通过提示LLM生成缩减器，无需人工干预即可最小化失败诱导测试输入，然后将缩减后的输入用于指导补丁生成。

Result: 在LFTBench基准测试中，ReduceFix平均缩减输入89.1%，pass@10相对原始测试提示提升53.8%，相比完全省略测试提升17.6%。为ChatRepair添加相同缩减步骤后，修复率提高21.3%。

Conclusion: ReduceFix通过自动减少测试输入而保留其失败诱导行为，显著提高了基于LLM的自动程序修复（APR）的可扩展性和有效性。

Abstract: Large Language Models (LLMs) have shown great potential in Automated Program
Repair (APR). Test inputs, being crucial for reasoning the root cause of
failures, are always included in the prompt for LLM-based APR. Unfortunately,
LLMs struggle to retain key information in long prompts. When the test inputs
are extensive in the prompt, this may trigger the "lost-in-the-middle" issue,
compromising repair performance. To address this, we propose ReduceFix, an
LLM-based APR approach with a built-in component that automatically reduces
test inputs while retaining their failure-inducing behavior. ReduceFix prompts
an LLM to generate a reducer that minimizes failure-inducing test inputs
without human effort, and then feeds the reduced failure-inducing inputs to
guide patch generation.
  For targeted evaluation, we constructed LFTBench, the first long-input APR
benchmark with 200 real bugs from 20 programming tasks, each paired with a
failure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix
shrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%
relative to a prompt that includes the original test, and by 17.6% compared
with omitting the test entirely. Adding the same reduction step to ChatRepair
increases its fix rate by 21.3% without other changes. Ablation studies further
highlight the impact of input length and compressed failure information on
repair success. These results underscore that automatically reducing failing
inputs is a practical and powerful complement to LLM-based APR, significantly
improving its scalability and effectiveness.

</details>


### [240] [Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems](https://arxiv.org/abs/2507.15296)
*Qian Xiong,Yuekai Huang,Ziyou Jiang,Zhiyuan Chang,Yujia Zheng,Tianhao Li,Mingyang Li*

Main category: cs.SE

TL;DR: 本文研究了工具代理范式中的参数失效问题，提出了分类法并分析了失效原因，最后给出了改进建议以提高交互可靠性。


<details>
  <summary>Details</summary>
Motivation: 工具代理范式的出现拓宽了大型语言模型（LLM）的能力边界，使其能够完成更复杂的任务。然而，由于执行过程中的参数失效问题，该范式的有效性受到限制。本文旨在探讨这一现象并提出相应建议。

Method: 本文首先构建了参数失效分类法，从一个主流工具代理的调用链中推导出五种失效类别。然后，通过应用15种输入扰动方法到输入中，探索了三种不同输入源与失效类别之间的相关性。

Result: 实验结果表明，参数名称幻觉失效主要源于LLM的固有局限性，而其他失效模式主要由输入源问题引起。

Conclusion: 为提高工具代理交互的可靠性和有效性，本文提出了改进建议，包括标准化工具返回格式、改进错误反馈机制以及确保参数一致性。

Abstract: The emergence of the tool agent paradigm has broadened the capability
boundaries of the Large Language Model (LLM), enabling it to complete more
complex tasks. However, the effectiveness of this paradigm is limited due to
the issue of parameter failure during its execution. To explore this phenomenon
and propose corresponding suggestions, we first construct a parameter failure
taxonomy in this paper. We derive five failure categories from the invocation
chain of a mainstream tool agent. Then, we explore the correlation between
three different input sources and failure categories by applying 15 input
perturbation methods to the input. Experimental results show that parameter
name hallucination failure primarily stems from inherent LLM limitations, while
issues with input sources mainly cause other failure patterns. To improve the
reliability and effectiveness of tool-agent interactions, we propose
corresponding improvement suggestions, including standardizing tool return
formats, improving error feedback mechanisms, and ensuring parameter
consistency.

</details>


### [241] [StackTrans: From Large Language Model to Large Pushdown Automata Model](https://arxiv.org/abs/2507.15343)
*Kechi Zhang,Ge Li,Jia Li,Huangzhao Zhang,Yihong Dong,Jia Li,Jingjing Xu,Zhi Jin*

Main category: cs.SE

TL;DR: StackTrans通过引入可学习的隐藏状态栈，解决了Transformer在捕捉Chomsky层次结构上的不足，并在多个任务中超越标准Transformer和其他基线模型。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在捕捉Chomsky层次结构（如正则表达式或确定性上下文无关文法）方面存在局限性，而StackTrans的灵感来源于能够高效解析这类文法的下推自动机。

Method: StackTrans在Transformer层之间显式地加入了可微的隐藏状态栈操作（如压入和弹出），保持了与现有框架（如flash-attention）的兼容性。

Result: StackTrans在Chomsky层次结构和大规模自然语言基准测试中均优于标准Transformer模型，且其360M参数的从头预训练模型表现优于参数多2-3倍的开源大型语言模型。

Conclusion: StackTrans通过引入隐藏状态栈，成功解决了Transformer在捕捉Chomsky层次结构方面的局限性，并在多种任务中表现优于标准Transformer模型。

Abstract: The Transformer architecture has emerged as a landmark advancement within the
broad field of artificial intelligence, effectively catalyzing the advent of
large language models (LLMs). However, despite its remarkable capabilities and
the substantial progress it has facilitated, the Transformer architecture still
has some limitations. One such intrinsic limitation is its inability to
effectively capture the Chomsky hierarchy, such as regular expressions or
deterministic context-free grammars. Drawing inspiration from pushdown
automata, which efficiently resolve deterministic context-free grammars using
stacks, we propose StackTrans to address the aforementioned issue within LLMs.
Unlike previous approaches that modify the attention computation, StackTrans
explicitly incorporates hidden state stacks between Transformer layers. This
design maintains compatibility with existing frameworks like flash-attention.
Specifically, our design features stack operations -- such as pushing and
popping hidden states -- that are differentiable and can be learned in an
end-to-end manner. Our comprehensive evaluation spans benchmarks for both
Chomsky hierarchies and large-scale natural languages. Across these diverse
tasks, StackTrans consistently outperforms standard Transformer models and
other baselines. We have successfully scaled StackTrans up from 360M to 7B
parameters. In particular, our from-scratch pretrained model StackTrans-360M
outperforms several larger open-source LLMs with 2-3x more parameters,
showcasing its superior efficiency and reasoning capability.

</details>


### [242] [Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing](https://arxiv.org/abs/2507.15599)
*Manatsawin Hanmongkolchai*

Main category: cs.SE

TL;DR: 通过'Chinese Wall'技术，利用高质量模型指导较弱模型，显著提升性能，但实际应用受限于缺乏公共领域训练模型。


<details>
  <summary>Details</summary>
Motivation: 解决当前大语言模型训练数据集未公开导致的版权问题，同时提升基于有限训练数据的模型的实用性。

Method: 提出并应用'Chinese Wall'技术，即利用高质量模型生成详细指令来增强较弱模型的能力。

Result: 在CanItEdit基准测试中，Comma v0.1 1T的性能提升了66%，Starcoder2 Instruct提升了20%。

Conclusion: 应用'Chinese Wall'技术可以显著提升较弱但符合伦理的模型在复杂任务上的表现，尽管目前由于缺乏基于公共领域内容的模型，该技术的实际应用可能受限。

Abstract: Large language models for code (Code LLM) are increasingly utilized in
programming environments. Despite their utility, the training datasets for top
LLM remain undisclosed, raising concerns about potential copyright violations.
Some models, such as Pleias and Comma put emphasis on data curation and
licenses, however, with limited training data these models are not competitive
and only serve as proof of concepts. To improve the utility of these models, we
propose an application of the "Chinese Wall" technique, inspired by the reverse
engineering technique of the same name -- a high quality model is used to
generate detailed instructions for a weaker model. By doing so, a weaker but
ethically aligned model may be used to perform complicated tasks that,
otherwise, can only be completed by more powerful models. In our evaluation,
we've found that this technique improves Comma v0.1 1T's performance in
CanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%
compared to when running the same model on the benchmark alone. The practical
application of this technique today, however, may be limited due to the lack of
models trained on public domain content without copyright restrictions.

</details>


### [243] [Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow](https://arxiv.org/abs/2507.15624)
*Yusuf Sulistyo Nugroho,Ganno Tribuana Kurniaji,Syful Islam,Mohammed Humayun Kabir,Vanesya Aura Ardity,Md. Kamal Uddin*

Main category: cs.SE

TL;DR: 研究发现React相关问题中算法错误最常见，尤其是中等声誉用户贡献最多，建议社区提供更多算法问题指导材料。


<details>
  <summary>Details</summary>
Motivation: 尽管React在单页应用开发中的流行度和优势已被证实，但用户在使用过程中面临的具体挑战尚未明确，因此本研究旨在通过分析Stack Overflow上的React问题来填补这一空白。

Method: 该研究采用探索性数据分析方法，分析了Stack Overflow上React相关的问题，重点关注关键词使用频率、错误分类和基于用户声誉的错误分析。

Result: 分析结果显示，React相关问题中最常讨论的关键词包括code、link、vir、href等，其中算法错误是所有用户群体中最常见的问题，占55.77%。

Conclusion: 研究结果为React社区提供了有价值的见解，特别是在算法相关问题的解决上，提示社区应提供更多算法相关的指导材料。

Abstract: React is a JavaScript library used to build user interfaces for single-page
applications. Although recent studies have shown the popularity and advantages
of React in web development, the specific challenges users face remain unknown.
Thus, this study aims to analyse the React-related questions shared on Stack
Overflow. The study utilizes an exploratory data analysis to investigate the
most frequently discussed keywords, error classification, and user
reputation-based errors, which is the novelty of this work. The results show
the top eight most frequently used keywords on React-related questions, namely,
code, link, vir, href, connect, azure, windows, and website. The error
classification of questions from the sample shows that algorithmic error is the
most frequent issue faced by all groups of users, where mid-reputation users
contribute the most, accounting for 55.77%. This suggests the need for the
community to provide guidance materials in solving algorithm-related problems.
We expect that the results of this study will provide valuable insight into
future research to support the React community during the early stages of
implementation, facilitating their ability to effectively overcome challenges
to adoption.

</details>


### [244] [SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models](https://arxiv.org/abs/2507.15663)
*Giordano d'Aloisio,Tosin Fadahunsi,Jay Choy,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: SustainDiffusion通过优化超参数和提示结构，显著减少SD模型的性别和种族偏见及能耗，提升可持续性，无需改变模型架构。


<details>
  <summary>Details</summary>
Motivation: 为了减少SD模型可能对社会和环境造成的危害，我们引入了SustainDiffusion，旨在增强SD模型的社会和环境可持续性。

Method: SustainDiffusion搜索最优的超参数和提示结构组合，以减少生成图像中的性别和种族偏见，同时降低图像生成所需的能耗，且保持与原始SD模型相当的图像质量。

Result: SustainDiffusion能将SD3中的性别偏见减少68%，种族偏见减少59%，能耗（CPU和GPU能耗总和）降低48%，且结果在多次运行中保持一致，并能推广到各种提示。

Conclusion: 通过SustainDiffusion，我们展示了如何在不需要微调或改变模型架构的情况下，提升文本到图像生成模型的社会和环境可持续性。

Abstract: Background: Text-to-image generation models are widely used across numerous
domains. Among these models, Stable Diffusion (SD) - an open-source
text-to-image generation model - has become the most popular, producing over 12
billion images annually. However, the widespread use of these models raises
concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the
environment, we introduce SustainDiffusion, a search-based approach designed to
enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters
and prompt structures that can reduce gender and ethnic bias in generated
images while also lowering the energy consumption required for image
generation. Importantly, SustainDiffusion maintains image quality comparable to
that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,
testing it against six different baselines using 56 different prompts. Our
results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,
ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and
GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are
consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social
and environmental sustainability of text-to-image generation models is possible
without fine-tuning or changing the model's architecture.

</details>


### [245] [Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches](https://arxiv.org/abs/2507.15666)
*Igor Turkin,Lina Volobuieva,Andriy Chukhray,Oleksandr Liubimov*

Main category: cs.SE

TL;DR: 比较了等效电路与机器学习在CubeSat电池放电建模中的表现，发现机器学习在准确性和适应性上更优。


<details>
  <summary>Details</summary>
Motivation: 为了在CubeSat卫星电池放电建模中做出合理选择，以预测自主电源系统断开后的后果并确保轨道设备的容错能力。

Method: 研究比较了两种方法：基于物理定律的等效电路分析和利用经验数据的机器学习建模。

Result: 等效电路方法透明度高但灵活性不足；机器学习模型更准确，能适应复杂和变化的实际条件。

Conclusion: 研究表明，虽然等效电路方法在透明度上有优势，但机器学习模型在准确性和适应性方面表现更佳，特别是在处理复杂依赖和实际条件偏离理论假设时。

Abstract: The subject of the article is the study and comparison of two approaches to
modelling the battery discharge of a CubeSat satellite: analytical using
equivalent circuit and machine learning. The article aims to make a reasoned
choice of the approach to modelling the battery discharge of a CubeSat
satellite. Modelling the battery discharge of a satellite will enable the
prediction of the consequences of disconnecting the autonomous power system and
ensure the fault tolerance of equipment in orbit. Therefore, the selected study
is relevant and promising. This study focuses on the analysis of CubeSat
satellite data, based explicitly on orbital data samples of the power system,
which include data available at the time of the article publication. The
dataset contains data on the voltage, current, and temperature of the battery
and solar panels attached to the five sides of the satellite. In this context,
two approaches are considered: analytical modelling based on physical laws and
machine learning, which uses empirical data to create a predictive model.
Results: A comparative analysis of the modeling results reveals that the
equivalent circuit approach has the advantage of transparency, as it identifies
possible parameters that facilitate understanding of the relationships.
However, the model is less flexible to environmental changes or non-standard
satellite behavior. The machine learning model demonstrated more accurate
results, as it can account for complex dependencies and adapt to actual
conditions, even when they deviate from theoretical assumptions.

</details>


### [246] [BugScope: Learn to Find Bugs Like Human](https://arxiv.org/abs/2507.15671)
*Jinyao Guo,Chengpeng Wang,Dominic Deluca,Jinjie Liu,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: BugScope是一种LLM驱动的多智能体系统，通过模拟人类学习bug模式的方式，显著提升了检测精度和召回率，并在实际应用中发现了大量未知bug。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析工具在覆盖率和适应性上存在局限，而现有基于LLM的方法对复杂bug的检测效果不佳。BugScope旨在解决这些问题，提升bug检测的准确性和实用性。

Method: BugScope采用LLM驱动的多智能体系统，模拟人类审计员学习新bug模式的方式，通过程序切片提取相关检测上下文，并构建定制化的检测提示来指导LLM进行准确推理。

Result: 在40个真实bug的数据集上，BugScope达到87.04%的精度和90.00%的召回率，F1分数超过现有工业工具0.44。在Linux内核等大规模开源系统中发现141个未知bug，其中78个已被修复，7个得到开发者确认。

Conclusion: BugScope通过其LLM驱动的多智能体系统，显著提升了软件bug检测的精度和召回率，并在实际应用中发现了大量未知bug，展现了其重要的实践价值。

Abstract: Detecting software bugs remains a fundamental challenge due to the extensive
diversity of real-world defects. Traditional static analysis tools often rely
on symbolic workflows, which restrict their coverage and hinder adaptability to
customized bugs with diverse anti-patterns. While recent advances incorporate
large language models (LLMs) to enhance bug detection, these methods continue
to struggle with sophisticated bugs and typically operate within limited
analysis contexts. To address these challenges, we propose BugScope, an
LLM-driven multi-agent system that emulates how human auditors learn new bug
patterns from representative examples and apply that knowledge during code
auditing. Given a set of examples illustrating both buggy and non-buggy
behaviors, BugScope synthesizes a retrieval strategy to extract relevant
detection contexts via program slicing and then constructs a tailored detection
prompt to guide accurate reasoning by the LLM. Our evaluation on a curated
dataset of 40 real-world bugs drawn from 21 widely-used open-source projects
demonstrates that BugScope achieves 87.04% precision and 90.00% recall,
surpassing state-of-the-art industrial tools by 0.44 in F1 score. Further
testing on large-scale open-source systems, including the Linux kernel,
uncovered 141 previously unknown bugs, of which 78 have been fixed and 7
confirmed by developers, highlighting BugScope's substantial practical impact.

</details>


### [247] [Do AI models help produce verified bug fixes?](https://arxiv.org/abs/2507.15822)
*Li Huang,Ilgiz Mustafin,Marco Piccioni,Alessandro Schena,Reto Weber,Bertrand Meyer*

Main category: cs.SE

TL;DR: 论文探讨了LLM在自动程序修复中的实际效果，通过实验发现与预期存在差距，并提出了具体的使用建议和方法论。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨AI技术（尤其是LLM）是否能在自动程序修复中实现预期效果，以及程序员如何实际利用LLM辅助调试。

Method: 研究方法包括使用程序验证环境对两组程序员（一组有LLM访问权限，另一组没有）进行随机分配，并通过证明工具验证修复的正确性。采用了Goal-Query-Metric方法来划分研究问题、具体元素和测量指标。

Result: 研究结果出乎意料，与AI用于调试和APR的预期不符。贡献包括详细的实验方法、程序员行为的细粒度分析、LLM使用模式的7种分类，以及关于如何最佳利用LLM进行调试和APR的验证建议。

Conclusion: 该论文的结论是，尽管AI和LLM在自动程序修复（APR）中具有潜力，但其实际效果与预期存在差距，并提出了如何有效利用LLM进行调试的具体建议。

Abstract: Among areas of software engineering where AI techniques -- particularly,
Large Language Models -- seem poised to yield dramatic improvements, an
attractive candidate is Automatic Program Repair (APR), the production of
satisfactory corrections to software bugs. Does this expectation materialize in
practice? How do we find out, making sure that proposed corrections actually
work? If programmers have access to LLMs, how do they actually use them to
complement their own skills?
  To answer these questions, we took advantage of the availability of a
program-proving environment, which formally determines the correctness of
proposed fixes, to conduct a study of program debugging with two randomly
assigned groups of programmers, one with access to LLMs and the other without,
both validating their answers through the proof tools. The methodology relied
on a division into general research questions (Goals in the Goal-Query-Metric
approach), specific elements admitting specific answers (Queries), and
measurements supporting these answers (Metrics). While applied so far to a
limited sample size, the results are a first step towards delineating a proper
role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the
use of AI for debugging and APR. The contributions also include: a detailed
methodology for experiments in the use of LLMs for debugging, which other
projects can reuse; a fine-grain analysis of programmer behavior, made possible
by the use of full-session recording; a definition of patterns of use of LLMs,
with 7 distinct categories; and validated advice for getting the best of LLMs
for debugging and Automatic Program Repair.

</details>


### [248] [Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering](https://arxiv.org/abs/2507.15828)
*Mauro Marcelino,Marcos Alves,Bianca Trinkenreich,Bruno Cartaxo,Sérgio Soares,Simone D. J. Barbosa,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本研究开发了一个基于RAG的LLM工具来自动生成证据简报，并通过对照实验评估其与人工简报在内容保真度、易理解性和有用性方面的差异。实验结果尚未公布。


<details>
  <summary>Details</summary>
Motivation: 尽管证据简报被认为对软件工程师有用，但其制作需要人工劳动，这可能是广泛采用的主要障碍。本研究旨在评估LLM生成的证据简报在内容保真度、易理解性和有用性方面是否可与人工制作的简报相媲美。

Method: 我们开发了一个基于RAG的LLM工具来生成证据简报，并设计了一个对照实验来评估LLM生成的简报与人工制作的简报在内容保真度、易理解性和有用性方面的表现。

Result: 实验结果待报告。

Conclusion: 结论将根据实验结果得出。

Abstract: [Context] An evidence briefing is a concise and objective transfer medium
that can present the main findings of a study to software engineers in the
industry. Although practitioners and researchers have deemed Evidence Briefings
useful, their production requires manual labor, which may be a significant
challenge to their broad adoption. [Goal] The goal of this registered report is
to describe an experimental protocol for evaluating LLM-generated evidence
briefings for secondary studies in terms of content fidelity, ease of
understanding, and usefulness, as perceived by researchers and practitioners,
compared to human-made briefings. [Method] We developed an RAG-based LLM tool
to generate evidence briefings. We used the tool to automatically generate two
evidence briefings that had been manually generated in previous research
efforts. We designed a controlled experiment to evaluate how the LLM-generated
briefings compare to the human-made ones regarding perceived content fidelity,
ease of understanding, and usefulness. [Results] To be reported after the
experimental trials. [Conclusion] Depending on the experiment results.

</details>


### [249] [Observing Fine-Grained Changes in Jupyter Notebooks During Development Time](https://arxiv.org/abs/2507.15831)
*Sergey Titov,Konstantin Grotov,Cristina Sarasua,Yaroslav Golubev,Dhivyabharathi Ramasamy,Alberto Bacchelli,Abraham Bernstein,Timofey Bryksin*

Main category: cs.SE

TL;DR: 研究填补了数据科学中Jupyter笔记本动态开发过程的研究空白，通过工具集收集开发数据并分析，发现笔记本兼具开发和调试功能，为未来研究提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 填补了数据科学领域中关于计算笔记本动态开发过程的研究空白。

Method: 引入了一套工具集用于收集Jupyter笔记本开发时的代码变更，收集了20名开发者超过100小时的工作数据（包含2,655个单元格和9,207次单元格执行），并利用该数据集研究了笔记本开发过程的动态性和变更。

Result: 分析发现，单元格之间的变更多为小型修复和代码迭代修改，表明笔记本兼具开发和调试功能。

Conclusion: 论文指出，Jupyter笔记本不仅用于开发和探索，还作为调试工具使用，并提出了未来研究方向。

Abstract: In software engineering, numerous studies have focused on the analysis of
fine-grained logs, leading to significant innovations in areas such as
refactoring, security, and code completion. However, no similar studies have
been conducted for computational notebooks in the context of data science.
  To help bridge this research gap, we make three scientific contributions: we
(1) introduce a toolset for collecting code changes in Jupyter notebooks during
development time; (2) use it to collect more than 100 hours of work related to
a data analysis task and a machine learning task (carried out by 20 developers
with different levels of expertise), resulting in a dataset containing 2,655
cells and 9,207 cell executions; and (3) use this dataset to investigate the
dynamic nature of the notebook development process and the changes that take
place in the notebooks.
  In our analysis of the collected data, we classified the changes made to the
cells between executions and found that a significant number of these changes
were relatively small fixes and code iteration modifications. This suggests
that notebooks are used not only as a development and exploration tool but also
as a debugging tool. We report a number of other insights and propose potential
future research directions on the novel data.

</details>


### [250] [Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs](https://arxiv.org/abs/2411.01789)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 论文提出利用Javadocs和LLMs自动化生成Java库客户端的测试预言，实验证明其高效且准确，显著推进了测试预言自动化的发展。


<details>
  <summary>Details</summary>
Motivation: 测试预言自动化是一个相对未被充分探索的领域，尤其是如何从非正式的自然语言描述中提取预期行为。Javadocs作为丰富的知识源，为这一挑战提供了解决方案。

Method: 利用Javadocs中的自然语言描述，结合大型语言模型（LLMs）生成测试预言，并对其进行实验评估。

Result: 实验表明，LLMs生成的测试预言中98.8%可编译，96.4%能准确反映预期属性，即使是少数不正确的预言，错误也较轻微且易于修正。

Conclusion: 论文提出了一个基于Javadocs和大型语言模型（LLMs）的自动化测试预言框架，能够高效生成可编译且准确的测试预言，显著提升了测试预言自动化的可行性。

Abstract: Software testing remains the most widely used methodology for validating
quality of code. However, effectiveness of testing critically depends on the
quality of test suites used. Test cases in a test suite consist of two
fundamental parts: (1) input values for the code under test, and (2) correct
checks for the outputs it produces. These checks are commonly written as
assertions, and termed test oracles. The last couple of decades have seen much
progress in automated test input generation, e.g., using fuzzing and symbolic
execution. However, automating test oracles remains a relatively less explored
problem area. Indeed, a test oracle by its nature requires knowledge of
expected behavior, which may only be known to the developer and may not not
exist in a formal language that supports automated reasoning.
  Our focus in this paper is automation of test oracles for clients of widely
used Java libraries, e.g., java.lang and java.util packages. Our key insight is
that Javadocs that provide a rich source of information can enable automated
generation of test oracles. Javadocs of the core Java libraries are fairly
detailed documents that contain natural language descriptions of not only how
the libraries behave but also how the clients must (not) use them. We use large
language models as an enabling technology to embody our insight into a
framework for test oracle automation, and evaluate it experimentally. Our
experiments demonstrate that LLMs can generate oracles for checking normal and
exceptional behaviors from Javadocs, with 98.8% of these oracles being
compilable and 96.4% accurately reflecting intended properties. Even for the
few incorrect oracles, errors are minor and can be easily corrected with the
help of additional comment information generated by the LLMs.

</details>


### [251] [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
*Yang Hong,Shan Jiang,Yulei Fu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 研究表明，LLMs能有效辅助编写Alloy规范，提升规范在软件开发中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 正确编写规范对开发安全可靠的软件系统至关重要，但这一过程极具挑战性，因此探索LLMs在编写声明性规范中的潜力。

Method: 采用ChatGPT和DeepSeek两种流行的LLMs，对11个经过充分研究的主题规范进行实验评估。

Result: 实验结果表明，LLMs在合成完整Alloy公式和完成草图方面表现良好，能够枚举多个独特解决方案。

Conclusion: LLMs在从自然语言或Alloy输入属性合成完整Alloy公式方面表现良好，能够枚举多个独特解决方案，并在不要求测试用例的情况下成功完成给定Alloy公式草图。

Abstract: Declarative specifications have a vital role to play in developing safe and
dependable software systems. Writing specifications correctly, however, remains
particularly challenging. This paper presents a controlled experiment on using
large language models (LLMs) to write declarative formulas in the well-known
language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write
complete Alloy formulas from given natural language descriptions (in English).
Two, we employ LLMs to create alternative but equivalent formulas in Alloy with
respect to given Alloy formulas. Three, we employ LLMs to complete sketches of
Alloy formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately represent
the desired properties (that are given in natural language). We conduct the
experimental evaluation using 11 well-studied subject specifications and employ
two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show
that the LLMs generally perform well in synthesizing complete Alloy formulas
from input properties given in natural language or in Alloy, and are able to
enumerate multiple unique solutions. Moreover, the LLMs are also successful at
completing given sketches of Alloy formulas with respect to natural language
descriptions of desired properties (without requiring test cases). We believe
LLMs offer a very exciting advance in our ability to write specifications, and
can help make specifications take a pivotal role in software development and
enhance our ability to build robust software.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [252] [Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach](https://arxiv.org/abs/2507.14249)
*Yuejiao Xie,Maonan Wang,Di Zhou,Man-On Pun,Zhu Han*

Main category: cs.RO

TL;DR: 提出MSHA-RL框架，用于城市空中交通的实时路径规划，结合通信质量评估和混合注意力机制，优化了旅行时间和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决城市空中交通（UAM）中由于乘客需求动态变化和通信质量要求高而导致的实时路径规划挑战。

Method: 构建无线电地图评估城市空域通信质量，并引入多源混合注意力强化学习（MSHA-RL）框架，通过混合注意力机制平衡全局和局部信息。

Result: 实验结果表明，该方法能够实现符合通信要求的轨迹规划，减少旅行时间并提高运营效率。

Conclusion: 本研究提出的MSHA-RL框架通过多源混合注意力机制，有效解决了城市空中交通中的实时路径规划问题，显著减少了旅行时间并提高了运营效率，同时确保了乘客安全。

Abstract: Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions
to alleviate urban congestion, with path planning becoming a key focus area.
Unlike ground transportation, UAM trajectory planning has to prioritize
communication quality for accurate location tracking in constantly changing
environments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,
requires adaptive planning to respond to real-time passenger requests,
especially in ride-sharing scenarios where passenger demands are unpredictable
and dynamic. However, conventional trajectory planning strategies based on
predefined routes lack the flexibility to meet varied passenger ride demands.
To address these challenges, this work first proposes constructing a radio map
to evaluate the communication quality of urban airspace. Building on this, we
introduce a novel Multi-Source Hybrid Attention Reinforcement Learning
(MSHA-RL) framework for the challenge of effectively focusing on passengers and
UAM locations, which arises from the significant dimensional disparity between
the representations. This model first generates the alignment among diverse
data sources with large gap dimensions before employing hybrid attention to
balance global and local insights, thereby facilitating responsive, real-time
path planning. Extensive experimental results demonstrate that the approach
enables communication-compliant trajectory planning, reducing travel time and
enhancing operational efficiency while prioritizing passenger safety.

</details>


### [253] [A Recursive Lie-Group Formulation for the Second-Order Time Derivatives of the Inverse Dynamics of parallel Kinematic Manipulators](https://arxiv.org/abs/2507.14274)
*Andreas Mueller,Shivesh Kumar,Thomas Kordik*

Main category: cs.RO

TL;DR: 本文首次解决了PKM配备SEA的轨迹控制问题，通过复用串行机器人算法和李群框架，高效计算逆动力学二阶导数，并在两种PKM上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 并联运动学机械臂（PKM）配备系列弹性执行器（SEA）的轨迹控制尚未被研究，关键在于高效计算逆动力学解的二阶时间导数，这在文献中尚未提出。

Method: 采用李群公式化，并在该框架下推导所有关系，复用了串行机器人逆动力学评估的递归算法，以高效计算逆动力学解的二阶时间导数。

Result: 数值结果表明，该方法在6自由度Gough-Stewart平台（作为外骨骼的一部分）和平面PKM上有效，尤其是在应用平坦性控制方案时。

Conclusion: 本文首次提出了针对并联运动学机械臂（PKM）配备系列弹性执行器（SEA）的轨迹控制方法，通过利用PKM的特殊拓扑结构，复用了串行机器人逆动力学评估的递归算法，并在李群框架下推导了所有关系。数值结果表明了该方法在6自由度Gough-Stewart平台和平面PKM上的有效性。

Abstract: Series elastic actuators (SEA) were introduced for serial robotic arms. Their
model-based trajectory tracking control requires the second time derivatives of
the inverse dynamics solution, for which algorithms were proposed. Trajectory
control of parallel kinematics manipulators (PKM) equipped with SEAs has not
yet been pursued. Key element for this is the computationally efficient
evaluation of the second time derivative of the inverse dynamics solution. This
has not been presented in the literature, and is addressed in the present paper
for the first time. The special topology of PKM is exploited reusing the
recursive algorithms for evaluating the inverse dynamics of serial robots. A
Lie group formulation is used and all relations are derived within this
framework. Numerical results are presented for a 6-DOF Gough-Stewart platform
(as part of an exoskeleton), and for a planar PKM when a flatness-based control
scheme is applied.

</details>


### [254] [Personalized Socially Assistive Robots With End-to-End Speech-Language Models For Well-Being Support](https://arxiv.org/abs/2507.14412)
*Mengxue Fu,Zhonghao Shi,Minyu Huang,Siqi Liu,Mina Kian,Yirui Song,Maja J. Matarić*

Main category: cs.RO

TL;DR: 研究提出使用端到端SLM改进SAR对话系统，用户测试显示其共情和适应性表现良好，但非语言行为和语音反馈仍需优化。


<details>
  <summary>Details</summary>
Motivation: 现有SAR对话系统在实时延迟、反馈机制和个性化对话方面存在不足，研究旨在通过集成端到端SLM来改进这些问题。

Method: 通过一个小型用户研究（N = 11），评估了SLM-enabled SAR对话系统的可用性，并通过用户反馈识别了系统的局限性。

Result: 用户认为SLM-enabled SAR系统能够提供共情反馈、自然轮换和适应性回应，但非语言行为的多样性和对话同步性不足，且语音反馈显得通用和重复。

Conclusion: 研究发现，虽然基于端到端语音语言模型（SLM）的社交辅助机器人（SAR）系统在提供共情反馈、自然轮换和适应性回应方面表现良好，但非语言行为的多样性和对话同步性仍有不足，需要进一步优化。

Abstract: Socially assistive robots (SARs) have shown great potential for supplementing
well-being support. However, prior studies have found that existing dialogue
pipelines for SARs remain limited in real-time latency, back-channeling, and
personalized speech dialogue. Toward addressing these limitations, we propose
using integrated end-to-end speech-language models (SLMs) with SARs. This work
1) evaluated the usability of an SLM-enabled SAR dialogue system through a
small user study, and 2) identified remaining limitations through study user
feedback to inform future improvements. We conducted a small within-participant
user study with university students (N = 11) whose results showed that
participants perceived an SLM-enabled SAR system as capable of providing
empathetic feedback, natural turn-taking, back-channeling, and adaptive
responses. We also found that participants reported the robot's nonverbal
behaviors as lacking variability and synchronization with conversation, and the
SLM's verbal feedback as generic and repetitive. These findings highlighted the
need for real-time robot movement synchronized with conversation, improved
prompting or fine-tuning to generate outputs better aligned with mental health
practices, and more expressive, adaptive vocal generation.

</details>


### [255] [Koopman Operator Based Time-Delay Embeddings and State History Augmented LQR for Periodic Hybrid Systems: Bouncing Pendulum and Bipedal Walking](https://arxiv.org/abs/2507.14455)
*Chun-Ming Yang,Pranav A. Bhounsule*

Main category: cs.RO

TL;DR: 扩展时间延迟嵌入技术至周期性非光滑系统，构建线性模型并开发新型LQR控制器。


<details>
  <summary>Details</summary>
Motivation: 探索时间延迟嵌入技术是否适用于周期性非光滑或混合系统，以扩展其应用范围并改进控制方法。

Method: 扩展时间延迟嵌入技术，应用于周期性非光滑或混合系统（如弹跳摆和最简单步行器），构建线性状态空间模型。

Result: 成功构建了周期性非光滑或混合系统的线性状态空间模型，并开发了状态历史增强LQR控制器。

Conclusion: 本文通过扩展时间延迟嵌入技术，成功构建了周期性非光滑或混合系统的线性状态空间模型，并提出了一种新颖的状态历史增强线性二次调节器（LQR），为这类系统的反馈控制提供了新方法。

Abstract: Time-delay embedding is a technique that uses snapshots of state history over
time to build a linear state space model of a nonlinear smooth system. We
demonstrate that periodic non-smooth or hybrid system can also be modeled as a
linear state space system using this approach as long as its behavior is
consistent in modes and timings. We extended time-delay embeddings to generate
a linear model of two periodic hybrid systems: the bouncing pendulum and the
simplest walker with control inputs. This leads to a novel state history
augmented linear quadratic regulator (LQR) which uses current and past state
history for feedback control.

</details>


### [256] [A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ Hand-0](https://arxiv.org/abs/2507.14538)
*Jin Chai,Xiang Yao,Mengfan Hou,Yanghong Li,Erbao Dong*

Main category: cs.RO

TL;DR: CYJ Hand-0是一种21自由度仿人灵巧手，采用混合肌腱驱动系统，实验验证了其仿生灵活性。


<details>
  <summary>Details</summary>
Motivation: 设计一种仿人手的21自由度灵巧手，结合形状记忆合金和直流电机的优势，实现高灵活性和仿生结构。

Method: 采用混合肌腱驱动系统（结合形状记忆合金和直流电机），使用高强度钓鱼线作为人工肌腱，并采用全3D打印的AlSi10Mg金属框架。

Result: 实验验证了设计的有效性，展示了仿生灵活性。

Conclusion: CYJ Hand-0的有效性通过机械和运动学实验得到验证，展示了其仿生灵活性。

Abstract: CYJ Hand-0 is a 21-DOF humanoid dexterous hand featuring a hybrid
tendon-driven actuation system that combines shape memory alloys (SMAs) and DC
motors. The hand employs high-strength fishing line as artificial tendons and
uses a fully 3D-printed AlSi10Mg metal frame designed to replicate the skeletal
and tendon-muscle structure of the human hand. A linear motor-driven module
controls finger flexion, while an SMA-based module enables finger extension and
lateral abduction. These modules are integrated into a compact hybrid actuation
unit mounted on a custom rear support structure. Mechanical and kinematic
experiments, conducted under an Arduino Mega 2560-based control system,
validate the effectiveness of the design and demonstrate its biomimetic
dexterity.

</details>


### [257] [BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree, Temporal Logic and Dynamical Movement Primitives](https://arxiv.org/abs/2507.14582)
*Zezhi Liu,Shizhen Wu,Hanqian Luo,Deyun Qin,Yongchun Fang*

Main category: cs.RO

TL;DR: A novel hierarchical framework (BT-TL-DMPs) combining Behavior Trees, Temporal Logic, and Dynamical Movement Primitives enables robots to generalize learned skills to complex, long-horizon tasks, validated by simulations and real-world experiments.


<details>
  <summary>Details</summary>
Motivation: The challenge in Learning from Demonstration (LfD) is enabling robots to generalize learned manipulation skills to novel, long-horizon tasks with different task and motion requirements, especially in complex, multi-stage scenarios.

Method: The paper introduces a hierarchical framework integrating Behavior Tree (BT), Temporal Logic (TL), and Dynamical Movement Primitives (DMPs). Signal Temporal Logic (STL) is used to specify task requirements, which are transformed into modular BTs for high-level decision-making. An STL-constrained DMP optimization method optimizes motion primitives while preserving learned dynamics.

Result: The framework was validated through simulations and real-world experiments, showing effective generalization under various STL constraints and reliable autonomous manipulation for complex tasks.

Conclusion: The proposed hierarchical framework BT-TL-DMPs effectively bridges the symbolic-motion gap, enabling robots to generalize learned manipulation skills to novel scenarios for long-horizon tasks with intricate constraints, as demonstrated by both simulations and real-world experiments.

Abstract: In the field of Learning from Demonstration (LfD), enabling robots to
generalize learned manipulation skills to novel scenarios for long-horizon
tasks remains challenging. Specifically, it is still difficult for robots to
adapt the learned skills to new environments with different task and motion
requirements, especially in long-horizon, multi-stage scenarios with intricate
constraints. This paper proposes a novel hierarchical framework, called
BT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and
Dynamical Movement Primitives (DMPs) to address this problem. Within this
framework, Signal Temporal Logic (STL) is employed to formally specify complex,
long-horizon task requirements and constraints. These STL specifications are
systematically transformed to generate reactive and modular BTs for high-level
decision-making task structure. An STL-constrained DMP optimization method is
proposed to optimize the DMP forcing term, allowing the learned motion
primitives to adapt flexibly while satisfying intricate spatiotemporal
requirements and, crucially, preserving the essential dynamics learned from
demonstrations. The framework is validated through simulations demonstrating
generalization capabilities under various STL constraints and real-world
experiments on several long-horizon robotic manipulation tasks. The results
demonstrate that the proposed framework effectively bridges the symbolic-motion
gap, enabling more reliable and generalizable autonomous manipulation for
complex robotic tasks.

</details>


### [258] [Koopman Operator Based Linear Model Predictive Control for 2D Quadruped Trotting, Bounding, and Gait Transition](https://arxiv.org/abs/2507.14605)
*Chun-Ming Yang,Pranav A. Bhounsule*

Main category: cs.RO

TL;DR: 结合Koopman理论和EDMD构建非线性高维线性模型，利用LMPC实现四足机器人多种步态及其在线过渡。


<details>
  <summary>Details</summary>
Motivation: 解决传统LMPC因线性化运动方程而导致的解质量下降问题，提升四足机器人在复杂环境中的运动控制能力。

Method: 采用Koopman算子理论和EDMD方法构建高维线性模型，结合LMPC进行实时控制优化。

Result: 在水平和崎岖地形上成功实现了跳跃、小跑及步态过渡，验证了方法的有效性。

Conclusion: 本文通过结合Koopman算子理论与扩展动态模态分解（EDMD），为四足机器人创建了高维空间中的线性模型，成功保留了运动方程的非线性特性。利用LMPC实现了多种步态及其过渡，展示了在线生成复杂步态的潜力。

Abstract: Online optimal control of quadrupedal robots would enable them to plan their
movement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged
as a practical approach for real-time control. In LMPC, an optimization problem
with a quadratic cost and linear constraints is formulated over a finite
horizon and solved on the fly. However, LMPC relies on linearizing the
equations of motion (EOM), which may lead to poor solution quality. In this
paper, we use Koopman operator theory and the Extended Dynamic Mode
Decomposition (EDMD) to create a linear model of the system in high dimensional
space, thus retaining the nonlinearity of the EOM. We model the aerial phase
and ground contact phases using different linear models. Then, using LMPC, we
demonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait
transitions in level and rough terrains. The main novelty is the use of Koopman
operator theory to create hybrid models of a quadrupedal system and demonstrate
the online generation of multiple gaits and gaits transitions.

</details>


### [259] [Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks](https://arxiv.org/abs/2507.14694)
*Yue Ma,Kanglei Zhou,Fuyang Yu,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.RO

TL;DR: ProbHMI利用可逆网络在潜在空间中建模概率动力学，显式预测未来分布，有效量化不确定性，适用于安全关键应用。


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景（如人机协作）中，量化预测的不确定性至关重要，但现有方法因隐式概率表示而难以实现。

Method: 引入可逆网络参数化姿势，在解耦的潜在空间中进行概率动力学建模，并通过预测模块显式预测未来潜在分布。

Result: 在基准测试中，ProbHMI在确定性和多样性预测方面均表现优异，且验证了不确定性校准的有效性。

Conclusion: ProbHMI通过可逆网络在解耦的潜在空间中参数化姿势，实现了概率动力学建模，有效量化了不确定性，适用于风险感知决策。

Abstract: 3D human motion forecasting aims to enable autonomous applications.
Estimating uncertainty for each prediction (i.e., confidence based on
probability density or quantile) is essential for safety-critical contexts like
human-robot collaboration to minimize risks. However, existing diverse motion
forecasting approaches struggle with uncertainty quantification due to implicit
probabilistic representations hindering uncertainty modeling. We propose
ProbHMI, which introduces invertible networks to parameterize poses in a
disentangled latent space, enabling probabilistic dynamics modeling. A
forecasting module then explicitly predicts future latent distributions,
allowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI
achieves strong performance for both deterministic and diverse prediction while
validating uncertainty calibration, critical for risk-aware decision making.

</details>


### [260] [Corridor-based Adaptive Control Barrier and Lyapunov Functions for Safe Mobile Robot Navigation](https://arxiv.org/abs/2507.14700)
*Nicholas Mohammad,Nicola Bezzo*

Main category: cs.RO

TL;DR: 结合CLF和CBF的MPCC框架，动态调整CBF参数，提升了机器人在未知杂乱环境中的安全导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有MPCC方法在未知杂乱环境中缺乏形式化的安全保障，需要一种能确保安全性的改进方法。

Method: 采用Control Lyapunov Function (CLF)和Control Barrier Function (CBF)增强的MPCC框架，结合Soft Actor-Critic (SAC)策略动态调整CBF参数。

Result: 通过仿真和移动机器人实验验证了该方法在未知杂乱环境中的安全导航性能。

Conclusion: 论文提出了一种结合CLF和CBF的MPCC框架，通过动态调整CBF参数增强了安全性，并在未知杂乱环境中验证了其有效性。

Abstract: Safe navigation in unknown and cluttered environments remains a challenging
problem in robotics. Model Predictive Contour Control (MPCC) has shown promise
for performant obstacle avoidance by enabling precise and agile trajectory
tracking, however, existing methods lack formal safety assurances. To address
this issue, we propose a general Control Lyapunov Function (CLF) and Control
Barrier Function (CBF) enabled MPCC framework that enforces safety constraints
derived from a free-space corridor around the planned trajectory. To enhance
feasibility, we dynamically adapt the CBF parameters at runtime using a Soft
Actor-Critic (SAC) policy. The approach is validated with extensive simulations
and an experiment on mobile robot navigation in unknown cluttered environments.

</details>


### [261] [Leveraging Extrinsic Dexterity for Occluded Grasping on Grasp Constraining Walls](https://arxiv.org/abs/2507.14721)
*Keita Kobashi,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: 提出分层强化学习框架解决遮挡抓取问题，结合Q学习和CVAE，通过模拟训练实现泛化和稳健的仿真到实际迁移。


<details>
  <summary>Details</summary>
Motivation: 解决因环境遮挡导致的主要抓取配置不可用的问题，特别是在平行夹持器因灵活性有限而难以应对的情况下。现有方法假设存在短墙，但现实场景中这一假设可能不成立，导致抓取失败。

Method: 采用分层强化学习（RL）框架，结合Q学习训练高层策略选择动作类型，并使用条件变分自编码器（CVAE）推断合适的位置。低层技能在训练时应用领域随机化以促进泛化。

Result: 实验评估表明，该方法具有泛化能力和稳健的仿真到实际迁移性能，成功率较高。

Conclusion: 该研究提出的分层强化学习框架在模拟环境中训练，并在真实世界中成功部署，展示了良好的泛化能力和稳健的仿真到实际的迁移性能，成功率令人满意。

Abstract: This study addresses the problem of occluded grasping, where primary grasp
configurations of an object are not available due to occlusion with
environment. Simple parallel grippers often struggle with such tasks due to
limited dexterity and actuation constraints. Prior works have explored object
pose reorientation such as pivoting by utilizing extrinsic contacts between an
object and an environment feature like a wall, to make the object graspable.
However, such works often assume the presence of a short wall, and this
assumption may not always hold in real-world scenarios. If the wall available
for interaction is too large or too tall, the robot may still fail to grasp the
object even after pivoting, and the robot must combine different types of
actions to grasp. To address this, we propose a hierarchical reinforcement
learning (RL) framework. We use Q-learning to train a high-level policy that
selects the type of action expected to yield the highest reward. The selected
low-level skill then samples a specific robot action in continuous space. To
guide the robot to an appropriate location for executing the selected action,
we adopt a Conditional Variational Autoencoder (CVAE). We condition the CVAE on
the object point cloud and the skill ID, enabling it to infer a suitable
location based on the object geometry and the selected skill. To promote
generalization, we apply domain randomization during the training of low-level
skills. The RL policy is trained entirely in simulation with a box-like object
and deployed to six objects in real world. We conduct experiments to evaluate
our method and demonstrate both its generalizability and robust sim-to-real
transfer performance with promising success rates.

</details>


### [262] [X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots](https://arxiv.org/abs/2507.14731)
*Haitong Wang,Aaron Hao Tan,Angus Fung,Goldie Nejat*

Main category: cs.RO

TL;DR: X-Nav是一个端到端跨机器人导航框架，通过两阶段学习实现通用性，实验验证了其在未见过的机器人和真实环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有导航方法通常针对特定机器人设计，限制了其在不同机器人平台上的通用性。

Method: X-Nav采用两阶段学习：1) 使用深度强化学习训练多个专家策略；2) 通过导航动作分块和Transformer（Nav-ACT）从专家策略中蒸馏出单一通用策略。

Result: X-Nav在模拟实验中实现了对未见过的机器人和环境的零样本迁移，并在真实环境中验证了其泛化能力。

Conclusion: X-Nav框架通过两阶段学习实现了跨不同机器人平台的端到端导航，展示了在未见过的机器人和真实环境中的零样本迁移能力，并通过实验验证了其设计选择和可扩展性。

Abstract: Existing navigation methods are primarily designed for specific robot
embodiments, limiting their generalizability across diverse robot platforms. In
this paper, we introduce X-Nav, a novel framework for end-to-end
cross-embodiment navigation where a single unified policy can be deployed
across various embodiments for both wheeled and quadrupedal robots. X-Nav
consists of two learning stages: 1) multiple expert policies are trained using
deep reinforcement learning with privileged observations on a wide range of
randomly generated robot embodiments; and 2) a single general policy is
distilled from the expert policies via navigation action chunking with
transformer (Nav-ACT). The general policy directly maps visual and
proprioceptive observations to low-level control commands, enabling
generalization to novel robot embodiments. Simulated experiments demonstrated
that X-Nav achieved zero-shot transfer to both unseen embodiments and
photorealistic environments. A scalability study showed that the performance of
X-Nav improves when trained with an increasing number of randomly generated
embodiments. An ablation study confirmed the design choices of X-Nav.
Furthermore, real-world experiments were conducted to validate the
generalizability of X-Nav in real-world environments.

</details>


### [263] [KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D Correspondence Learning](https://arxiv.org/abs/2507.14820)
*Bingran Chen,Baorun Li,Jian Yang,Yong Liu,Guangyao Zhai*

Main category: cs.RO

TL;DR: KGN-Pro通过概率PnP层和3D优化改进了6-DoF抓取估计，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在6-DoF抓取估计中存在小物体和传感器噪声处理不足、标注成本高或仅依赖2D监督等问题，KGN-Pro旨在解决这些挑战。

Method: KGN-Pro结合RGB-D图像生成关键点图，并通过2D置信度图加权关键点贡献，利用概率PnP层进行端到端学习。

Result: KGN-Pro在模拟和真实平台实验中表现出更高的抓取覆盖率和成功率。

Conclusion: KGN-Pro通过整合直接3D优化和概率PnP层，显著提升了6-DoF抓取估计的性能，实验证明其在抓取覆盖率和成功率上优于现有方法。

Abstract: High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation
to serve as a basic function. Previous approaches either directly generate
grasps from point-cloud data, suffering from challenges with small objects and
sensor noise, or infer 3D information from RGB images, which introduces
expensive annotation requirements and discretization issues. Recent methods
mitigate some challenges by retaining a 2D representation to estimate grasp
keypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF
poses. However, these methods are limited by their non-differentiable nature
and reliance solely on 2D supervision, which hinders the full exploitation of
rich 3D information. In this work, we present KGN-Pro, a novel grasping network
that preserves the efficiency and fine-grained object grasping of previous KGNs
while integrating direct 3D optimization through probabilistic PnP layers.
KGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further
outputs a 2D confidence map to weight keypoint contributions during
re-projection error minimization. By modeling the weighted sum of squared
re-projection errors probabilistically, the network effectively transmits 3D
supervision to its 2D keypoint predictions, enabling end-to-end learning.
Experiments on both simulated and real-world platforms demonstrate that KGN-Pro
outperforms existing methods in terms of grasp cover rate and success rate.

</details>


### [264] [CoMoCAVs: Cohesive Decision-Guided Motion Planning for Connected and Autonomous Vehicles with Multi-Policy Reinforcement Learning](https://arxiv.org/abs/2507.14903)
*Pan Hu*

Main category: cs.RO

TL;DR: CDGMP框架通过模块化设计和多策略强化学习，提升了自动驾驶车辆在复杂交通中的适应性和安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要可靠且高效的决策和运动规划解决方案，尤其在联网自动驾驶车辆（CAVs）中，实现灵活安全的车道选择和精确轨迹执行仍具挑战性。

Method: 提出了一种称为CDGMP的框架，结合了混合专家（MoE）架构和多策略强化学习，通过门控机制协调多个专用子网络，将复杂驾驶任务分解为模块化组件。

Result: 仿真结果表明，CDGMP在车道选择和运动规划方面表现出可靠的性能。

Conclusion: CDGMP框架通过模块化设计和多策略强化学习，显著提升了自动驾驶车辆在复杂交通场景中的适应性和安全性，为高维决策与控制任务提供了可扩展的解决方案。

Abstract: Autonomous driving demands reliable and efficient solutions to closely
related problems such as decision-making and motion planning. In this work,
decision-making refers specifically to highway lane selection, while motion
planning involves generating control commands (such as speed and steering) to
reach the chosen lane. In the context of Connected Autonomous Vehicles (CAVs),
achieving both flexible and safe lane selection alongside precise trajectory
execution remains a significant challenge. This paper proposes a framework
called Cohesive Decision-Guided Motion Planning (CDGMP), which tightly
integrates decision-making and motion planning using a Mixture of Experts (MoE)
inspired architecture combined with multi-policy reinforcement learning. By
coordinating multiple specialized sub-networks through a gating mechanism, the
method decomposes the complex driving task into modular components. Each
sub-network focuses on a specific aspect of driving, improving efficiency by
activating only the most relevant modules during inference. This design also
enhances safety through modular specialization. CDGMP improves the adaptability
and robustness of CAVs across diverse traffic scenarios, offering a scalable
solution to real-world autonomy challenges. The architectural principles behind
CDGMP, especially the use of MoE, also provide a strong foundation for other
high-dimensional decision and control tasks. Simulation results (available at
https://youtu.be/_-4OXNHV0UY) demonstrate reliable performance in both lane
selection and motion planning.

</details>


### [265] [One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner](https://arxiv.org/abs/2507.14914)
*Zhexuan Xu,Jie Wang,Siyuan Xu,Zijie Geng,Mingxuan Yuan,Feng Wu*

Main category: cs.RO

TL;DR: Flora 是一种三阶段馈通和布局感知的矩形布局规划器，通过跨阶段优化显著提升了芯片的 PPA 指标。


<details>
  <summary>Details</summary>
Motivation: 现有布局规划方法难以与后续物理设计阶段集成，导致模块内组件布局不理想和模块间馈通过多。Flora 旨在解决这一问题。

Method: Flora 采用三阶段方法：1) 使用 wiremask 和 position mask 进行粗粒度优化；2) 在固定轮廓约束下通过局部调整模块形状实现零空白布局；3) 基于快速树搜索方法优化组件放置并调整模块边界。

Result: 实验表明，Flora 平均减少了 6% 的 HPWL、5.16% 的 FTpin 和 29.15% 的 FTmod，组件布局性能提升了 14%。

Conclusion: Flora 在芯片布局规划中表现优异，显著改善了 HPWL、FTpin、FTmod 和组件布局性能，证明了其跨阶段优化的有效性。

Abstract: Floorplanning determines the shapes and locations of modules on a chip canvas
and plays a critical role in optimizing the chip's Power, Performance, and Area
(PPA) metrics. However, existing floorplanning approaches often fail to
integrate with subsequent physical design stages, leading to suboptimal
in-module component placement and excessive inter-module feedthrough. To tackle
this challenge, we propose Flora, a three-stage feedthrough and placement aware
rectilinear floorplanner. In the first stage, Flora employs wiremask and
position mask techniques to achieve coarse-grained optimization of HPWL and
feedthrough. In the second stage, under the constraint of a fixed outline,
Flora achieves a zero-whitespace layout by locally resizing module shapes,
thereby performing fine-grained optimization of feedthrough and improving
component placement. In the third stage, Flora utilizes a fast tree
search-based method to efficiently place components-including macros and
standard cells-within each module, subsequently adjusting module boundaries
based on the placement results to enable cross-stage optimization. Experimental
results show that Flora outperforms recent state-of-the-art floorplanning
approaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,
29.15% in FTmod, and a 14% improvement in component placement performance.

</details>


### [266] [Digital twin and extended reality for teleoperation of the electric vehicle battery disassembly](https://arxiv.org/abs/2507.14929)
*Tero Kaarlela,Sami Salo,Jose Outeiro*

Main category: cs.RO

TL;DR: 该论文提出了一种结合远程操作和自动化的混合系统，用于安全、高效地拆卸和分类电动汽车电池，减少人工风险并提高回收效率。


<details>
  <summary>Details</summary>
Motivation: 当前手动拆卸电动汽车电池的过程存在安全隐患（如触电和有毒化学物质），阻碍了可持续的闭环供应链发展。

Method: 提出了一种基于ROS中间件的数字孪生机器人系统，结合RGB摄像头实现物理与数字孪生的对齐，支持人类在环操作以创建和保存未知电池类型的拆卸序列。

Result: 在线试点研究表明，该方法具有用户友好性，展示了其在安全性和效率方面的潜力。

Conclusion: 该论文提出了一种结合远程操作和自动化的混合方法，用于安全、高效地拆卸和分类电动汽车电池，通过减少对人工的依赖并提高电池回收的吞吐量，实现了经济贡献。

Abstract: Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a
sustainable transition to electric vehicles by enabling a closed-loop supply
chain. Currently, the manual disassembly process exposes workers to hazards,
including electrocution and toxic chemicals. We propose a teleoperated system
for the safe disassembly and sorting of EVBs. A human-in-the-loop can create
and save disassembly sequences for unknown EVB types, enabling future
automation. An RGB camera aligns the physical and digital twins of the EVB, and
the digital twin of the robot is based on the Robot Operating System (ROS)
middleware. This hybrid approach combines teleoperation and automation to
improve safety, adaptability, and efficiency in EVB disassembly and sorting.
The economic contribution is realized by reducing labor dependency and
increasing throughput in battery recycling. An online pilot study was set up to
evaluate the usability of the presented approach, and the results demonstrate
the potential as a user-friendly solution.

</details>


### [267] [Designing Robots with, not for: A Co-Design Framework for Empowering Interactions in Forensic Psychiatry](https://arxiv.org/abs/2507.14931)
*Qiaoqiao Ren,Remko Proesmans,Arend Pissens,Lara Dehandschutter,William Denecker,Lotte Rouckhout,Joke Carrette,Peter Vanhopplinus,Tony Belpaeme,Francis wyffels*

Main category: cs.RO

TL;DR: 本研究通过四次共同设计工作坊开发伴侣机器人，帮助法医精神病患者监测和调节压力，强调患者在设计中的参与和自主性。


<details>
  <summary>Details</summary>
Motivation: 法医精神病患者常面临高度官僚化、风险规避和自主权受限的环境，导致心理压力加剧。本研究旨在探索如何通过共同设计开发伴侣机器人，帮助患者监测和调节压力。

Method: 研究在法医精神病诊所进行了四次共同设计工作坊，参与者包括患者、护理人员和治疗师。工作坊内容包括初步原型展示、创意构思、功能定义及情感反馈收集。

Result: 研究发现，患者在设计过程中的参与至关重要，需根据其当前情绪状态调整设计方案。工作坊成功收集了患者、护理人员和治疗师的反馈，为最终原型设计提供了依据。

Conclusion: 研究表明，通过共同设计方法开发伴侣机器人可以有效帮助法医精神病患者监测和调节压力，同时确保患者在设计中拥有发言权，增强其自主性。

Abstract: Forensic mental health care involves the treatment of individuals with severe
mental disorders who have committed violent offences. These settings are often
characterized by high levels of bureaucracy, risk avoidance, and restricted
autonomy. Patients frequently experience a profound loss of control over their
lives, leading to heightened psychological stress-sometimes resulting in
isolation as a safety measure. In this study, we explore how co-design can be
used to collaboratively develop a companion robot that helps monitor and
regulate stress while maintaining tracking of the patients' interaction
behaviours for long-term intervention. We conducted four co-design workshops in
a forensic psychiatric clinic with patients, caregivers, and therapists. Our
process began with the presentation of an initial speculative prototype to
therapists, enabling reflection on shared concerns, ethical risks, and
desirable features. This was followed by a creative ideation session with
patients, a third workshop focused on defining desired functions and emotional
responses, and we are planning a final prototype demo to gather direct patient
feedback. Our findings emphasize the importance of empowering patients in the
design process and adapting proposals based on their current emotional state.
The goal was to empower the patient in the design process and ensure each
patient's voice was heard.

</details>


### [268] [Heterogeneous object manipulation on nonlinear soft surface through linear controller](https://arxiv.org/abs/2507.14967)
*Pratik Ingle,Kasper Støy,Andres Faiña*

Main category: cs.RO

TL;DR: 该论文提出了一种基于PID的闭环控制策略，简化了高自由度执行器阵列的控制，实现了对异质物体的高效操作。


<details>
  <summary>Details</summary>
Motivation: 高密度执行器阵列带来的高自由度增加了控制复杂性，限制了操作表面的实际应用。学习型控制方法虽能缓解复杂性，但需要大量训练样本且难以泛化到异质物体。

Method: 采用几何变换驱动的PID控制器，直接将倾斜角度控制输出映射到执行器命令，避免了大量黑盒训练的需求。

Result: 通过仿真和物理系统实验验证了方法的有效性，成功操作了包括鸡蛋和苹果在内的多种几何形状、重量和纹理的物体。

Conclusion: 该研究提出了一种基于PID的线性闭环反馈控制策略，适用于MANTA-RAY系统上的异质物体操作，证明了其简单、精确且鲁棒的特性，为软机器人操作提供了实用且可靠的解决方案。

Abstract: Manipulation surfaces indirectly control and reposition objects by actively
modifying their shape or properties rather than directly gripping objects.
These surfaces, equipped with dense actuator arrays, generate dynamic
deformations. However, a high-density actuator array introduces considerable
complexity due to increased degrees of freedom (DOF), complicating control
tasks. High DOF restrict the implementation and utilization of manipulation
surfaces in real-world applications as the maintenance and control of such
systems exponentially increase with array/surface size. Learning-based control
approaches may ease the control complexity, but they require extensive training
samples and struggle to generalize for heterogeneous objects. In this study, we
introduce a simple, precise and robust PID-based linear close-loop feedback
control strategy for heterogeneous object manipulation on MANTA-RAY
(Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation
density). Our approach employs a geometric transformation-driven PID
controller, directly mapping tilt angle control outputs(1D/2D) to actuator
commands to eliminate the need for extensive black-box training. We validate
the proposed method through simulations and experiments on a physical system,
successfully manipulating objects with diverse geometries, weights and
textures, including fragile objects like eggs and apples. The outcomes
demonstrate that our approach is highly generalized and offers a practical and
reliable solution for object manipulation on soft robotic manipulation,
facilitating real-world implementation without prohibitive training demands.

</details>


### [269] [FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models](https://arxiv.org/abs/2507.14975)
*Yufan Song,Jiatao Zhang,Zeng Gu,Qingmiao Liang,Tuocheng Hu,Wei Song,Shiqiang Zhu*

Main category: cs.RO

TL;DR: FCRF是一种新型导师-行动者架构，通过灵活自我反思和整合历史经验，显著提升LLM在复杂长期机器人任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自我反思机制缺乏灵活性，限制了其在任务规划错误纠正中的有效性，受人类认知适应启发提出FCRF。

Method: 提出了灵活的建构主义反思框架（FCRF），采用导师-行动者架构，使LLM能根据任务难度进行灵活自我反思，并整合历史经验与失败教训。

Result: 在AlfWorld模拟和真实环境物理部署中，FCRF在多样化家务任务上表现优异。

Conclusion: FCRF显著提升了复杂长期机器人任务的整体性能和自我反思灵活性。

Abstract: Autonomous error correction is critical for domestic robots to achieve
reliable execution of complex long-horizon tasks. Prior work has explored
self-reflection in Large Language Models (LLMs) for task planning error
correction; however, existing methods are constrained by inflexible
self-reflection mechanisms that limit their effectiveness. Motivated by these
limitations and inspired by human cognitive adaptation, we propose the Flexible
Constructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture
that enables LLMs to perform flexible self-reflection based on task difficulty,
while constructively integrating historical valuable experience with failure
lessons. We evaluated FCRF on diverse domestic tasks through simulation in
AlfWorld and physical deployment in the real-world environment. Experimental
results demonstrate that FCRF significantly improves overall performance and
self-reflection flexibility in complex long-horizon robotic tasks.

</details>


### [270] [CPED-NCBFs: A Conformal Prediction for Expert Demonstration-based Neural Control Barrier Functions](https://arxiv.org/abs/2507.15022)
*Sumeadh MS,Kevin Dsouza,Ravi Prakash*

Main category: cs.RO

TL;DR: 提出CPED-NCBFs验证策略，解决NCBFs安全验证难题，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有CBFs验证方法（如SMT求解器、MIP等）存在边界宽松或保守的问题，需更高效验证手段。

Method: 采用split-conformal预测验证策略（CPED-NCBFs）验证从专家演示中学习的NCBFs。

Result: CPED-NCBFs在点质量系统和非完整模型中验证了其有效性。

Conclusion: CPED-NCBFs通过split-conformal预测验证策略有效解决了NCBFs的安全验证问题，并在点质量系统和非完整模型上验证了其有效性。

Abstract: Among the promising approaches to enforce safety in control systems, learning
Control Barrier Functions (CBFs) from expert demonstrations has emerged as an
effective strategy. However, a critical challenge remains: verifying that the
learned CBFs truly enforce safety across the entire state space. This is
especially difficult when CBF is represented using neural networks (NCBFs).
Several existing verification techniques attempt to address this problem
including SMT-based solvers, mixed-integer programming (MIP), and interval or
bound-propagation methods but these approaches often introduce loose,
conservative bounds. To overcome these limitations, in this work we use
CPED-NCBFs a split-conformal prediction based verification strategy to verify
the learned NCBF from the expert demonstrations. We further validate our method
on point mass systems and unicycle models to demonstrate the effectiveness of
the proposed theory.

</details>


### [271] [Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper](https://arxiv.org/abs/2507.15062)
*Xinyue Zhu,Binghao Huang,Yunzhu Li*

Main category: cs.RO

TL;DR: 开发了带触觉传感器的便携夹具及跨模态学习框架，显著提升机器人操作的精确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有夹具缺乏触觉传感，而触觉反馈在精确操作中至关重要。为了在多样化的真实环境中收集多模态数据并提升机器人操作的精确性，设计了这一解决方案。

Method: 开发了一个便携、轻量的夹具，集成了触觉传感器，支持视觉和触觉数据的同步采集；并提出了一个跨模态表示学习框架，整合了视觉和触觉信号。

Result: 在微细任务中，如试管插入和移液管液体转移，该方法提高了准确性和鲁棒性，尤其在外部干扰下表现更佳。

Conclusion: 提出的带有触觉传感器的便携式夹具及其跨模态表示学习框架，显著提升了机器人操作的精确性和鲁棒性，尤其在微细任务如试管插入和移液管液体转移中表现优异。

Abstract: Handheld grippers are increasingly used to collect human demonstrations due
to their ease of deployment and versatility. However, most existing designs
lack tactile sensing, despite the critical role of tactile feedback in precise
manipulation. We present a portable, lightweight gripper with integrated
tactile sensors that enables synchronized collection of visual and tactile data
in diverse, real-world, and in-the-wild settings. Building on this hardware, we
propose a cross-modal representation learning framework that integrates visual
and tactile signals while preserving their distinct characteristics. The
learning procedure allows the emergence of interpretable representations that
consistently focus on contacting regions relevant for physical interactions.
When used for downstream manipulation tasks, these representations enable more
efficient and effective policy learning, supporting precise robotic
manipulation based on multimodal feedback. We validate our approach on
fine-grained tasks such as test tube insertion and pipette-based fluid
transfer, demonstrating improved accuracy and robustness under external
disturbances. Our project page is available at
https://binghao-huang.github.io/touch_in_the_wild/ .

</details>


### [272] [Search-Based Autonomous Vehicle Motion Planning Using Game Theory](https://arxiv.org/abs/2507.15088)
*Pouya Panahandeh,Mohammad Pirani,Baris Fidan,Amir Khajepour*

Main category: cs.RO

TL;DR: 提出基于博弈论的交互式运动规划方法，将道路用户视为智能体，实现快速、真实的路径规划，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统搜索方法将其他道路用户视为静态障碍物，无法反映真实驾驶环境中的动态交互，需改进以实现更真实的路径规划。

Method: 采用基于搜索的交互式运动规划方案，结合博弈论方法，将其他道路使用者视为智能体而非静态障碍物。

Result: 新方法计算时间短，适用于实时应用，且生成的路径比现有技术更符合实际。

Conclusion: 提出的基于博弈论的交互式运动规划方案在实时应用中表现出色，能够生成更符合实际的路径，并通过实验验证了其有效性。

Abstract: In this paper, we propose a search-based interactive motion planning scheme
for autonomous vehicles (AVs), using a game-theoretic approach. In contrast to
traditional search-based approaches, the newly developed approach considers
other road users (e.g. drivers and pedestrians) as intelligent agents rather
than static obstacles. This leads to the generation of a more realistic path
for the AV. Due to the low computational time, the proposed motion planning
scheme is implementable in real-time applications. The performance of the
developed motion planning scheme is compared with existing motion planning
techniques and validated through experiments using WATonoBus, an electrical
all-weather autonomous shuttle bus.

</details>


### [273] [Learning-Based Modeling of a Magnetically Steerable Soft Suction Device for Endoscopic Endonasal Interventions](https://arxiv.org/abs/2507.15155)
*Majid Roshanfar,Alex Zhang,Changyan He,Amir Hooshiar,Dale J. Podolsky,Thomas Looi,Eric Diller*

Main category: cs.RO

TL;DR: 该研究提出了一种基于学习的建模框架，用于磁性软体吸引装置，通过RF模型实现高精度形状预测，推动微创神经外科手术中软体机器人工具的智能控制。


<details>
  <summary>Details</summary>
Motivation: 开发一种新型学习建模框架，用于磁性可操纵软体吸引装置，以支持内窥镜鼻内脑肿瘤切除手术，解决传统物理假设简化模型的局限性。

Method: 采用基于神经网络（NN）和随机森林（RF）架构的数据驱动模型，训练了5097个实验样本，覆盖不同磁场强度、驱动频率和垂直尖端距离。RF模型在控制点预测和形状重建误差上表现更优。

Result: RF模型在所有指标上优于NN，控制点预测的平均均方根误差为0.087毫米，形状重建误差为0.064毫米。特征重要性分析显示磁场分量主要影响远端控制点，而频率和距离影响基座配置。

Conclusion: 该研究通过数据驱动模型成功实现了对磁性软体机器人复杂非线性行为的高精度建模，为微创神经外科手术中软体机器人工具的智能控制提供了重要进展。

Abstract: This letter introduces a novel learning-based modeling framework for a
magnetically steerable soft suction device designed for endoscopic endonasal
brain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm
inner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material,
and integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape
feedback. Shape reconstruction is represented using four Bezier control points,
enabling a compact and smooth model of the device's deformation. A data-driven
model was trained on 5,097 experimental samples covering a range of magnetic
field magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical
tip distances (90-100 mm), using both Neural Network (NN) and Random Forest
(RF) architectures. The RF model outperformed the NN across all metrics,
achieving a mean root mean square error of 0.087 mm in control point prediction
and a mean shape reconstruction error of 0.064 mm. Feature importance analysis
further revealed that magnetic field components predominantly influence distal
control points, while frequency and distance affect the base configuration.
This learning-based approach effectively models the complex nonlinear behavior
of hyperelastic soft robots under magnetic actuation without relying on
simplified physical assumptions. By enabling sub-millimeter shape prediction
accuracy and real-time inference, this work represents an advancement toward
the intelligent control of magnetically actuated soft robotic tools in
minimally invasive neurosurgery.

</details>


### [274] [CHADET: Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer](https://arxiv.org/abs/2507.15189)
*Kevin Christiansen Marsim,Jinwoo Jeon,Yeeun Kim,Myeongwoo Jeong,Hyun Myung*

Main category: cs.RO

TL;DR: CHADET是一种轻量级深度完成网络，通过交叉分层注意力模块优化深度信息，提升深度图质量并降低内存使用，适用于实时机器人任务。


<details>
  <summary>Details</summary>
Motivation: 现有深度完成方法在计算效率和准确性之间存在显著权衡，不适合实时应用，因此需要一种既能提高深度信息完整性和准确性，又能提升处理速度的方法。

Method: CHADET采用深度块提取特征，并通过轻量级基于transformer的解码器处理，其中交叉分层注意力模块用于细化图像特征。

Result: 在KITTI、NYUv2和VOID数据集上的验证表明，CHADET在提升深度图质量的同时减少了内存使用。

Conclusion: CHADET提出了一种轻量级的深度完成网络，通过交叉分层注意力模块优化深度信息，显著提升了深度图的质量并降低了内存使用，适用于实时机器人应用。

Abstract: Depth information which specifies the distance between objects and current
position of the robot is essential for many robot tasks such as navigation.
Recently, researchers have proposed depth completion frameworks to provide
dense depth maps that offer comprehensive information about the surrounding
environment. However, existing methods show significant trade-offs between
computational efficiency and accuracy during inference. The substantial memory
and computational requirements make them unsuitable for real-time applications,
highlighting the need to improve the completeness and accuracy of depth
information while improving processing speed to enhance robot performance in
various tasks. To address these challenges, in this paper, we propose
CHADET(cross-hierarchical-attention depth-completion transformer), a
lightweight depth-completion network that can generate accurate dense depth
maps from RGB images and sparse depth points. For each pair, its feature is
extracted from the depthwise blocks and passed to the equally lightweight
transformer-based decoder. In the decoder, we utilize the novel
cross-hierarchical-attention module that refines the image features from the
depth information. Our approach improves the quality and reduces memory usage
of the depth map prediction, as validated in both KITTI, NYUv2, and VOID
datasets.

</details>


### [275] [VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving](https://arxiv.org/abs/2507.15266)
*Haichao Liu,Haoren Guo,Pei Liu,Benshan Ma,Yuxiang Zhang,Jun Ma,Tong Heng Lee*

Main category: cs.RO

TL;DR: VLM-UDMC框架通过结合VLM和实时环境编码，提升自动驾驶的决策透明性和性能，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 模仿人类驾驶员的场景理解和风险感知能力，确保自动驾驶的透明性和可解释性。

Method: 提出了一种基于视觉语言模型（VLM）的统一决策与运动控制框架（VLM-UDMC），包含上层慢系统（采用两步推理策略和RAG技术）和下层快系统（轻量级多核分解LSTM）。

Result: VLM-UDMC框架在仿真和实际实验中表现出色，能够有效利用场景理解和注意力分解做出理性驾驶决策。

Conclusion: VLM-UDMC框架通过结合场景理解和风险感知注意力，显著提升了城市自动驾驶的决策和运动控制性能，并通过仿真和实际实验验证了其有效性。

Abstract: Scene understanding and risk-aware attentions are crucial for human drivers
to make safe and effective driving decisions. To imitate this cognitive ability
in urban autonomous driving while ensuring the transparency and
interpretability, we propose a vision-language model (VLM)-enhanced unified
decision-making and motion control framework, named VLM-UDMC. This framework
incorporates scene reasoning and risk-aware insights into an upper-level slow
system, which dynamically reconfigures the optimal motion planning for the
downstream fast system. The reconfiguration is based on real-time environmental
changes, which are encoded through context-aware potential functions. More
specifically, the upper-level slow system employs a two-step reasoning policy
with Retrieval-Augmented Generation (RAG), leveraging foundation models to
process multimodal inputs and retrieve contextual knowledge, thereby generating
risk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM
provides real-time trajectory predictions for heterogeneous traffic
participants by extracting smoother trend representations for short-horizon
trajectory prediction. The effectiveness of the proposed VLM-UDMC framework is
verified via both simulations and real-world experiments with a full-size
autonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively
leverages scene understanding and attention decomposition for rational driving
decisions, thus improving the overall urban driving performance. Our
open-source project is available at https://github.com/henryhcliu/vlmudmc.git.

</details>


### [276] [RepILN: Reparameterized Inertial Localization Network](https://arxiv.org/abs/2507.15293)
*Shanshan Zhang,Tianshui Wen,Siyue Wang,Qi Zhang,Ziheng Zhou,Lingxiang Zheng,Yu Yang*

Main category: cs.RO

TL;DR: 提出一种重参数化惯性定位网络，通过多分支结构和稀疏注意力机制提升精度与效率，实验证明其在降低误差和参数数量上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的惯性定位方法依赖复杂网络架构，对IoT设备的计算资源构成挑战，且常忽略惯性测量中的长时依赖关系，限制了定位性能。

Method: 提出了一种重参数化的惯性定位网络，采用多分支结构增强特征提取，并在推理时转换为单路径架构以提高参数效率；引入时间尺度稀疏注意力机制和门控卷积单元，以捕捉长时依赖关系并整合局部细粒度特征。

Result: 实验表明，该方法在精度和模型紧凑性之间取得了良好平衡，显著降低了绝对轨迹误差和参数数量。

Conclusion: 该方法在公开基准测试中表现出色，实现了精度与模型紧凑性之间的良好平衡，例如在RoNIN数据集上，绝对轨迹误差降低了2.59%，同时参数数量减少了3.86%。

Abstract: Inertial localization is regarded as a promising positioning solution for
consumer-grade IoT devices due to its cost-effectiveness and independence from
external infrastructure. However, data-driven inertial localization methods
often rely on increasingly complex network architectures to improve accuracy,
which challenges the limited computational resources of IoT devices. Moreover,
these methods frequently overlook the importance of modeling long-term
dependencies in inertial measurements - a critical factor for accurate
trajectory reconstruction - thereby limiting localization performance. To
address these challenges, we propose a reparameterized inertial localization
network that uses a multi-branch structure during training to enhance feature
extraction. At inference time, this structure is transformed into an equivalent
single-path architecture to improve parameter efficiency. To further capture
long-term dependencies in motion trajectories, we introduce a temporal-scale
sparse attention mechanism that selectively emphasizes key trajectory segments
while suppressing noise. Additionally, a gated convolutional unit is
incorporated to effectively integrate long-range dependencies with local
fine-grained features. Extensive experiments on public benchmarks demonstrate
that our method achieves a favorable trade-off between accuracy and model
compactness. For example, on the RoNIN dataset, our approach reduces the
Absolute Trajectory Error (ATE) by 2.59% compared to RoNIN-ResNet while
reducing the number of parameters by 3.86%.

</details>


### [277] [Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe](https://arxiv.org/abs/2507.15444)
*Leonard Bauersfeld,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 首个利用实时流场测量的四旋翼无人机闭环控制系统，有效解决狭窄管道中悬停的稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼无人机在狭窄管道中悬停时因自诱导气流扰动导致的稳定性受限问题。

Method: 开发了一种低延迟、基于事件的烟雾测速方法，用于高时间分辨率估计局部气流。结合基于循环卷积神经网络的干扰估计器，实时推断力和扭矩干扰，并将这些估计整合到通过强化学习训练的学习型控制器中。

Result: 提出的流反馈控制在管道横截面横向平移操作中尤为有效，能够实时抵消瞬态空气动力学效应，避免与管壁碰撞。

Conclusion: 本文展示了首个利用实时流场测量的四旋翼无人机闭环控制系统，为在空气动力学复杂环境中的飞行研究开辟了新方向，并提供了机器人学与流体动力学交叉领域的新见解。

Abstract: Autonomous quadrotor flight in confined spaces such as pipes and tunnels
presents significant challenges due to unsteady, self-induced aerodynamic
disturbances. Very recent advances have enabled flight in such conditions, but
they either rely on constant motion through the pipe to mitigate airflow
recirculation effects or suffer from limited stability during hovering. In this
work, we present the first closed-loop control system for quadrotors for
hovering in narrow pipes that leverages real-time flow field measurements. We
develop a low-latency, event-based smoke velocimetry method that estimates
local airflow at high temporal resolution. This flow information is used by a
disturbance estimator based on a recurrent convolutional neural network, which
infers force and torque disturbances in real time. The estimated disturbances
are integrated into a learning-based controller trained via reinforcement
learning. The flow-feedback control proves particularly effective during
lateral translation maneuvers in the pipe cross-section. There, the real-time
disturbance information enables the controller to effectively counteract
transient aerodynamic effects, thereby preventing collisions with the pipe
wall. To the best of our knowledge, this work represents the first
demonstration of an aerial robot with closed-loop control informed by real-time
flow field measurements. This opens new directions for research on flight in
aerodynamically complex environments. In addition, our work also sheds light on
the characteristic flow structures that emerge during flight in narrow,
circular pipes, providing new insights at the intersection of robotics and
fluid dynamics.

</details>


### [278] [The Emergence of Deep Reinforcement Learning for Path Planning](https://arxiv.org/abs/2507.15469)
*Thanh Thi Nguyen,Saeid Nahavandi,Imran Razzak,Dung Nguyen,Nhat Truong Pham,Quoc Viet Hung Nguyen*

Main category: cs.RO

TL;DR: 该综述论文探讨了传统路径规划方法与深度强化学习的结合，分析了其优缺点，并提出了混合方法作为未来研究的重点方向。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统在复杂动态环境中的需求增加，研究智能路径规划方法变得尤为重要。传统方法虽成熟但存在局限，而深度强化学习展现出新的潜力，因此需要系统梳理和评估这些方法。

Method: 该论文采用综述方法，全面分析了传统路径规划方法（如图搜索算法、线性规划、进化计算）和深度强化学习（DRL）在路径规划中的应用，并对关键算法进行了分类和比较。

Result: 论文总结了传统和DRL方法在计算效率、可扩展性、适应性和鲁棒性方面的优缺点，并提出了混合方法的潜力。

Conclusion: 该综述总结了混合方法（结合深度强化学习和经典规划技术）的优势，提出了未来研究的开放挑战和方向，特别强调了在自主导航中结合学习适应性和确定性可靠性的潜力。

Abstract: The increasing demand for autonomous systems in complex and dynamic
environments has driven significant research into intelligent path planning
methodologies. For decades, graph-based search algorithms, linear programming
techniques, and evolutionary computation methods have served as foundational
approaches in this domain. Recently, deep reinforcement learning (DRL) has
emerged as a powerful method for enabling autonomous agents to learn optimal
navigation strategies through interaction with their environments. This survey
provides a comprehensive overview of traditional approaches as well as the
recent advancements in DRL applied to path planning tasks, focusing on
autonomous vehicles, drones, and robotic platforms. Key algorithms across both
conventional and learning-based paradigms are categorized, with their
innovations and practical implementations highlighted. This is followed by a
thorough discussion of their respective strengths and limitations in terms of
computational efficiency, scalability, adaptability, and robustness. The survey
concludes by identifying key open challenges and outlining promising avenues
for future research. Special attention is given to hybrid approaches that
integrate DRL with classical planning techniques to leverage the benefits of
both learning-based adaptability and deterministic reliability, offering
promising directions for robust and resilient autonomous navigation.

</details>


### [279] [All-UWB SLAM Using UWB Radar and UWB AOA](https://arxiv.org/abs/2507.15474)
*Charith Premachandra,Achala Athukorala,U-Xuan Tan*

Main category: cs.RO

TL;DR: 本文提出了一种结合UWB AOA测量的新方法，解决了特征缺失环境中的SLAM问题，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在恶劣环境（如烟雾、灰尘）中，光学传感器易失效，而UWB雷达能穿透这些环境。但现有UWB雷达SLAM方法依赖环境特征，限制了在特征缺失环境中的应用。因此，本研究旨在通过引入UWB AOA测量来提升SLAM性能。

Method: 该方法利用UWB雷达的超宽带特性，结合动态部署的UWB锚点-标签单元获取AOA测量数据，解决了特征缺失环境中的SLAM问题。

Result: 实验结果表明，结合UWB AOA单元的UWB雷达SLAM系统能在视觉受限且特征缺失的环境中实现有效定位与建图。

Conclusion: 本文提出了一种结合UWB AOA测量的新方法，显著提高了在特征缺失环境中的SLAM精度和可扩展性，并通过实验验证了其有效性。

Abstract: There has been a growing interest in autonomous systems designed to operate
in adverse conditions (e.g. smoke, dust), where the visible light spectrum
fails. In this context, Ultra-wideband (UWB) radar is capable of penetrating
through such challenging environmental conditions due to the lower frequency
components within its broad bandwidth. Therefore, UWB radar has emerged as a
potential sensing technology for Simultaneous Localization and Mapping (SLAM)
in vision-denied environments where optical sensors (e.g. LiDAR, Camera) are
prone to failure. Existing approaches involving UWB radar as the primary
exteroceptive sensor generally extract features in the environment, which are
later initialized as landmarks in a map. However, these methods are constrained
by the number of distinguishable features in the environment. Hence, this paper
proposes a novel method incorporating UWB Angle of Arrival (AOA) measurements
into UWB radar-based SLAM systems to improve the accuracy and scalability of
SLAM in feature-deficient environments. The AOA measurements are obtained using
UWB anchor-tag units which are dynamically deployed by the robot in featureless
areas during mapping of the environment. This paper thoroughly discusses
prevailing constraints associated with UWB AOA measurement units and presents
solutions to overcome them. Our experimental results show that integrating UWB
AOA units with UWB radar enables SLAM in vision-denied feature-deficient
environments.

</details>


### [280] [The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents](https://arxiv.org/abs/2507.15478)
*Simon Kohaut,Felix Divo,Navid Hamid,Benedict Flade,Julian Eggert,Devendra Singh Dhami,Kristian Kersting*

Main category: cs.RO

TL;DR: CoCo框架结合神经符号方法，提升自主代理的安全性和合规性，通过深度概率逻辑和自我怀疑机制实现。


<details>
  <summary>Details</summary>
Motivation: 解决自主代理在不确定环境中可靠且合规行为的挑战。

Method: 提出了Constitutional Controller（CoCo）框架，结合深度概率逻辑程序和自我怀疑机制（基于速度、传感器状态等特征的密度函数）。

Result: 在真实世界的空中移动研究中，CoCo展示了其在复杂不确定环境中安全导航的能力。

Conclusion: 神经符号系统（如CoCo框架）通过结合概率符号推理和深度学习方法，显著提升了自主代理在不确定环境中的安全性和合规性。

Abstract: Ensuring reliable and rule-compliant behavior of autonomous agents in
uncertain environments remains a fundamental challenge in modern robotics. Our
work shows how neuro-symbolic systems, which integrate probabilistic, symbolic
white-box reasoning models with deep learning methods, offer a powerful
solution to this challenge. This enables the simultaneous consideration of
explicit rules and neural models trained on noisy data, combining the strength
of structured reasoning with flexible representations. To this end, we
introduce the Constitutional Controller (CoCo), a novel framework designed to
enhance the safety and reliability of agents by reasoning over deep
probabilistic logic programs representing constraints such as those found in
shared traffic spaces. Furthermore, we propose the concept of self-doubt,
implemented as a probability density conditioned on doubt features such as
travel velocity, employed sensors, or health factors. In a real-world aerial
mobility study, we demonstrate CoCo's advantages for intelligent autonomous
systems to learn appropriate doubts and navigate complex and uncertain
environments safely and compliantly.

</details>


### [281] [Robots for Kiwifruit Harvesting and Pollination](https://arxiv.org/abs/2507.15484)
*Jamie Bell*

Main category: cs.RO

TL;DR: 研究开发了用于猕猴桃果园的移动机器人，改进了采摘机制和导航系统，提高了采摘率和花粉喷洒效率。


<details>
  <summary>Details</summary>
Motivation: 解决猕猴桃果园中自动化采摘和花粉喷洒的挑战，提高采摘效率和花粉喷洒的精准度。

Method: 设计了多种猕猴桃采摘机制，并进行了实地测试。采用2D和3D激光雷达进行导航，开发了计算机视觉算法用于行检测和跟随。花粉喷洒通过检测花朵并使用喷杆进行定向喷洒。

Result: 改进的采摘机制可覆盖80%以上的果实（原为70%）。3D激光雷达导航系统在猕猴桃果园中实现了超过30公里的自动驾驶。计算机视觉算法的行跟随效果与3D激光雷达相当。

Conclusion: 该研究成功开发了用于猕猴桃果园的移动机器人，实现了高效的定向花粉喷洒和自动化收获。通过改进的猕猴桃采摘机制和3D激光雷达导航系统，显著提高了果实采摘率和导航精度。

Abstract: This research was a part of a project that developed mobile robots that
performed targeted pollen spraying and automated harvesting in pergola
structured kiwifruit orchards. Multiple kiwifruit detachment mechanisms were
designed and field testing of one of the concepts showed that the mechanism
could reliably pick kiwifruit. Furthermore, this kiwifruit detachment mechanism
was able to reach over 80 percent of fruit in the cluttered kiwifruit canopy,
whereas the previous state of the art mechanism was only able to reach less
than 70 percent of the fruit. Artificial pollination was performed by detecting
flowers and then spraying pollen in solution onto the detected flowers from a
line of sprayers on a boom, while driving at up to 1.4 ms-1. In addition, the
height of the canopy was measured and the spray boom was moved up and down to
keep the boom close enough to the flowers for the spray to reach the flowers,
while minimising collisions with the canopy. Mobile robot navigation was
performed using a 2D lidar in apple orchards and vineyards. Lidar navigation in
kiwifruit orchards was more challenging because the pergola structure only
provides a small amount of data for the direction of rows, compared to the
amount of data from the overhead canopy, the undulating ground and other
objects in the orchards. Multiple methods are presented here for extracting
structure defining features from 3D lidar data in kiwifruit orchards. In
addition, a 3D lidar navigation system -- which performed row following, row
end detection and row end turns -- was tested for over 30 km of autonomous
driving in kiwifruit orchards. Computer vision algorithms for row detection and
row following were also tested. The computer vision algorithm worked as well as
the 3D lidar row following method in testing.

</details>


### [282] [GR-3 Technical Report](https://arxiv.org/abs/2507.15493)
*Chilam Cheang,Sijin Chen,Zhongren Cui,Yingdong Hu,Liqun Huang,Tao Kong,Hang Li,Yifeng Li,Yuxiao Liu,Xiao Ma,Hao Niu,Wenxuan Ou,Wanli Peng,Zeyu Ren,Haixin Shi,Jiawen Tian,Hongtao Wu,Xin Xiao,Yuyang Xiao,Jiafeng Xu,Yichu Yang*

Main category: cs.RO

TL;DR: GR-3 是一种大规模视觉-语言-动作模型，通过多模态训练和高效微调，实现了对新任务和环境的强大泛化能力，并在实验中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够泛化到新对象、环境和抽象指令的通用机器人策略，以支持人类日常生活。

Method: 通过结合网络规模的视觉语言数据训练、基于VR设备收集的人类轨迹数据高效微调，以及机器人轨迹数据的模仿学习，实现了GR-3的多方面能力。

Result: GR-3 在多种复杂任务中超越了现有基准方法π₀，尤其在长时程和灵巧任务中表现优异。

Conclusion: GR-3 展示了在多样化任务中的卓越性能，为构建通用机器人助手迈出了重要一步。

Abstract: We report our recent progress towards building generalist robot policies, the
development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.
It showcases exceptional capabilities in generalizing to novel objects,
environments, and instructions involving abstract concepts. Furthermore, it can
be efficiently fine-tuned with minimal human trajectory data, enabling rapid
and cost-effective adaptation to new settings. GR-3 also excels in handling
long-horizon and dexterous tasks, including those requiring bi-manual
manipulation and mobile movement, showcasing robust and reliable performance.
These capabilities are achieved through a multi-faceted training recipe that
includes co-training with web-scale vision-language data, efficient fine-tuning
from human trajectory data collected via VR devices, and effective imitation
learning with robot trajectory data. In addition, we introduce ByteMini, a
versatile bi-manual mobile robot designed with exceptional flexibility and
reliability, capable of accomplishing a wide range of tasks when integrated
with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the
state-of-the-art baseline method, $\pi_0$, on a wide variety of challenging
tasks. We hope GR-3 can serve as a step towards building generalist robots
capable of assisting humans in daily life.

</details>


### [283] [CLEVER: Stream-based Active Learning for Robust Semantic Perception from Human Instructions](https://arxiv.org/abs/2507.15499)
*Jongseok Lee,Timo Birr,Rudolph Triebel,Tamim Asfour*

Main category: cs.RO

TL;DR: CLEVER是一个流式主动学习系统，通过人类指导和贝叶斯方法提升DNN语义感知的鲁棒性，并在真实机器人上验证。


<details>
  <summary>Details</summary>
Motivation: 解决流式数据下DNN语义感知的鲁棒性问题，通过人类干预和在线适应提升性能。

Method: 设计了一个结合贝叶斯公式（编码领域知识为先验）和在线人类指导的系统，用于流式数据下的DNN主动学习。

Result: 通过用户验证研究和实验（人形和可变形物体），证明了CLEVER的实用性，并首次在真实机器人上实现了流式主动学习。

Conclusion: CLEVER系统通过在线人类指导和贝叶斯公式，成功提升了基于DNN的语义感知的鲁棒性，并在真实机器人上实现了流式主动学习。

Abstract: We propose CLEVER, an active learning system for robust semantic perception
with Deep Neural Networks (DNNs). For data arriving in streams, our system
seeks human support when encountering failures and adapts DNNs online based on
human instructions. In this way, CLEVER can eventually accomplish the given
semantic perception tasks. Our main contribution is the design of a system that
meets several desiderata of realizing the aforementioned capabilities. The key
enabler herein is our Bayesian formulation that encodes domain knowledge
through priors. Empirically, we not only motivate CLEVER's design but further
demonstrate its capabilities with a user validation study as well as
experiments on humanoid and deformable objects. To our knowledge, we are the
first to realize stream-based active learning on a real robot, providing
evidence that the robustness of the DNN-based semantic perception can be
improved in practice. The project website can be accessed at
https://sites.google.com/view/thecleversystem.

</details>


### [284] [Estimation of Payload Inertial Parameters from Human Demonstrations by Hand Guiding](https://arxiv.org/abs/2507.15604)
*Johannes Hartwig,Philipp Lienhardt,Dominik Henrich*

Main category: cs.RO

TL;DR: 该论文提出了一种通过非接触运动部分估计机器人负载惯性参数的方法，无需专用校准，但质心和惯性张量的估计受噪声影响。


<details>
  <summary>Details</summary>
Motivation: 解决非专业用户编程接触运动时的效率问题，消除对专用负载惯性参数校准的需求，实现灵活的机器人工具更换。

Method: 利用非接触运动部分，采用已有估计技术来估计机器人的负载惯性参数（PIP），避免专用校准。

Result: 负载质量的估计准确，但质心和惯性张量的估计受噪声和激励不足的影响。

Conclusion: 研究发现，虽然手引导过程中可以估计负载的质量参数，但质心和惯性张量的估计受噪声和激励不足的影响。结果表明，足够的负载加速度对准确估计至关重要。

Abstract: As the availability of cobots increases, it is essential to address the needs
of users with little to no programming knowledge to operate such systems
efficiently. Programming concepts often use intuitive interaction modalities,
such as hand guiding, to address this. When programming in-contact motions,
such frameworks require knowledge of the robot tool's payload inertial
parameters (PIP) in addition to the demonstrated velocities and forces to
ensure effective hybrid motion-force control. This paper aims to enable
non-expert users to program in-contact motions more efficiently by eliminating
the need for a dedicated PIP calibration, thereby enabling flexible robot tool
changes. Since demonstrated tasks generally also contain motions with
non-contact, our approach uses these parts to estimate the robot's PIP using
established estimation techniques. The results show that the estimation of the
payload's mass is accurate, whereas the center of mass and the inertia tensor
are affected by noise and a lack of excitation. Overall, these findings show
the feasibility of PIP estimation during hand guiding but also highlight the
need for sufficient payload accelerations for an accurate estimation.

</details>


### [285] [A Universal Vehicle-Trailer Navigation System with Neural Kinematics and Online Residual Learning](https://arxiv.org/abs/2507.15607)
*Yanbo Chen,Yunzhe Tan,Yaojia Wang,Zhengzhe Xu,Junbo Tan,Xueqian Wang*

Main category: cs.RO

TL;DR: 本文开发了一种通用车辆-拖车导航系统，结合经典模型和神经网络，通过在线学习和模型预测控制实现高精度自主导航，实验验证了其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在机场、超市和音乐会场地等环境中，车辆-拖车系统的自主导航至关重要，但准确建模这类系统（尤其是带有转向轮的拖车）仍具挑战性。

Method: 提出了一种混合名义运动学模型，结合了经典非完整约束和神经网络拖车运动学，并开发了轻量级在线残差学习模块以实时修正误差。同时，采用了带加权模型组合策略的模型预测控制框架，以提高长期预测精度和运动规划的安全性。

Result: 通过涉及多种拖车类型和不同负载条件的广泛实际实验验证，该方法在无需人工调优或拖车特定校准的情况下表现出鲁棒性能。

Conclusion: 本文提出了一种新型的通用车辆-拖车导航系统，通过结合经典非完整约束和神经网络拖车运动学模型，并引入轻量级在线残差学习模块来实时修正模型误差，显著提高了导航精度和鲁棒性。

Abstract: Autonomous navigation of vehicle-trailer systems is crucial in environments
like airports, supermarkets, and concert venues, where various types of
trailers are needed to navigate with different payloads and conditions.
However, accurately modeling such systems remains challenging, especially for
trailers with castor wheels. In this work, we propose a novel universal
vehicle-trailer navigation system that integrates a hybrid nominal kinematic
model--combining classical nonholonomic constraints for vehicles and neural
network-based trailer kinematics--with a lightweight online residual learning
module to correct real-time modeling discrepancies and disturbances.
Additionally, we develop a model predictive control framework with a weighted
model combination strategy that improves long-horizon prediction accuracy and
ensures safer motion planning. Our approach is validated through extensive
real-world experiments involving multiple trailer types and varying payload
conditions, demonstrating robust performance without manual tuning or
trailer-specific calibration.

</details>


### [286] [Optimizing Force Signals from Human Demonstrations of In-Contact Motions](https://arxiv.org/abs/2507.15608)
*Johannes Hartwig,Fabian Viessmann,Dominik Henrich*

Main category: cs.RO

TL;DR: 本文研究了如何优化力信号以更好地反映人类演示的意图，提出了峰值检测和信号滤波方法，显著提高了机器人编程的可用性。


<details>
  <summary>Details</summary>
Motivation: 针对非机器人编程专家，直观的输入方法（如kinesthetic guiding）在接触任务中变得重要，但人类演示的不精确和嘈杂输入信号带来了问题。

Method: 比较了不同的信号滤波方法，并提出了一种用于处理首次接触偏差的峰值检测方法。

Result: 通过提出的方法，单个动作的质量在误差标准上提高了高达20%。

Conclusion: 本文提出的峰值检测方法和信号滤波方法显著提高了机器人编程的可用性，优化了人机交互体验。

Abstract: For non-robot-programming experts, kinesthetic guiding can be an intuitive
input method, as robot programming of in-contact tasks is becoming more
prominent. However, imprecise and noisy input signals from human demonstrations
pose problems when reproducing motions directly or using the signal as input
for machine learning methods. This paper explores optimizing force signals to
correspond better to the human intention of the demonstrated signal. We compare
different signal filtering methods and propose a peak detection method for
dealing with first-contact deviations in the signal. The evaluation of these
methods considers a specialized error criterion between the input and the
human-intended signal. In addition, we analyze the critical parameters'
influence on the filtering methods. The quality for an individual motion could
be increased by up to \SI{20}{\percent} concerning the error criterion. The
proposed contribution can improve the usability of robot programming and the
interaction between humans and robots.

</details>


### [287] [EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation](https://arxiv.org/abs/2507.15649)
*Haocheng Xu,Haodong Zhang,Zhenghan Chen,Rong Xiong*

Main category: cs.RO

TL;DR: 提出基于强化学习的框架，使人形机器人模仿人类上半身动作并保持稳定性，通过重定向网络和EMP模块实现，经测试验证有效。


<details>
  <summary>Details</summary>
Motivation: 研究人形机器人在执行操作任务时的稳定站立问题，现有站立姿势下可控范围有限影响全身稳定性。

Method: 设计了重定向网络生成大规模上半身动作数据集，结合领域随机化训练强化学习策略，并引入可执行动作先验（EMP）模块调整输入动作以确保安全与稳定性。

Result: 框架在仿真和实际测试中表现出色，能够有效跟踪上半身动作目标并保持稳定性。

Conclusion: 提出的强化学习框架成功实现了人形机器人模仿人类上半身动作并保持整体稳定性，通过仿真和实际测试验证了其实际应用性。

Abstract: To support humanoid robots in performing manipulation tasks, it is essential
to study stable standing while accommodating upper-body motions. However, the
limited controllable range of humanoid robots in a standing position affects
the stability of the entire body. Thus we introduce a reinforcement learning
based framework for humanoid robots to imitate human upper-body motions while
maintaining overall stability. Our approach begins with designing a retargeting
network that generates a large-scale upper-body motion dataset for training the
reinforcement learning (RL) policy, which enables the humanoid robot to track
upper-body motion targets, employing domain randomization for enhanced
robustness. To avoid exceeding the robot's execution capability and ensure
safety and stability, we propose an Executable Motion Prior (EMP) module, which
adjusts the input target movements based on the robot's current state. This
adjustment improves standing stability while minimizing changes to motion
amplitude. We evaluate our framework through simulation and real-world tests,
demonstrating its practical applicability.

</details>


### [288] [Data-Driven MPC with Data Selection for Flexible Cable-Driven Robotic Arms](https://arxiv.org/abs/2507.15677)
*Huayue Liang,Yanbo Chen,Hongyang Cheng,Yanzhao Yu,Shoujie Li,Junbo Tan,Xueqian Wang,Long Zeng*

Main category: cs.RO

TL;DR: 论文提出了一种基于数据的MPC方法，用于提高柔性电缆驱动机械臂的控制精度，通过隐式模型和数据筛选算法，显著减少了求解时间和跟踪误差。


<details>
  <summary>Details</summary>
Motivation: 柔性电缆驱动机械臂（FCRAs）因其电缆的弹性、滞后和摩擦等固有特性，在建模和控制上面临困难，需一种不依赖物理模型的精确控制方法。

Method: 开发了一个基于输入输出数据的隐式模型，并将其整合到MPC优化框架中；引入数据选择算法（DSA）筛选最能表征系统的数据，将每步求解时间减少约80%。

Result: 在真实FCRA平台上验证了方法的有效性，包括五点定位精度测试、五点响应跟踪测试和字母绘制轨迹跟踪，结果显示平均定位精度约为2.070毫米，跟踪误差显著降低。

Conclusion: 论文提出的基于输入输出数据的模型预测控制（MPC）方法显著提高了柔性电缆驱动机械臂（FCRAs）的控制精度，平均定位精度约为2.070毫米，跟踪误差降至0.541度，优于PID方法。

Abstract: Flexible cable-driven robotic arms (FCRAs) offer dexterous and compliant
motion. Still, the inherent properties of cables, such as resilience,
hysteresis, and friction, often lead to particular difficulties in modeling and
control. This paper proposes a model predictive control (MPC) method that
relies exclusively on input-output data, without a physical model, to improve
the control accuracy of FCRAs. First, we develop an implicit model based on
input-output data and integrate it into an MPC optimization framework. Second,
a data selection algorithm (DSA) is introduced to filter the data that best
characterize the system, thereby reducing the solution time per step to
approximately 4 ms, which is an improvement of nearly 80%. Lastly, the
influence of hyperparameters on tracking error is investigated through
simulation. The proposed method has been validated on a real FCRA platform,
including five-point positioning accuracy tests, a five-point response tracking
test, and trajectory tracking for letter drawing. The results demonstrate that
the average positioning accuracy is approximately 2.070 mm. Moreover, compared
to the PID method with an average tracking error of 1.418{\deg}, the proposed
method achieves an average tracking error of 0.541{\deg}.

</details>


### [289] [Strong, Accurate, and Low-Cost Robot Manipulator](https://arxiv.org/abs/2507.15693)
*Georges Chebly,Spencer Little,Nisal Perera,Aliya Abedeen,Ken Suzuki,Donghyun Kim*

Main category: cs.RO

TL;DR: Forte是一款低成本、高性能的3D打印6自由度机械臂，适用于教育和研究。


<details>
  <summary>Details</summary>
Motivation: 推动低成本教育机械臂的性能极限，使其适用于从课堂教育到AI实验的广泛场景。

Method: 结合了基于capstan的电缆驱动、同步带、简单的张力机制和轻量化的3D打印结构，并通过拓扑优化提高结构刚度。

Result: 实验验证表明，Forte具有高重复性和负载能力（0.63 kg负载，0.467 m范围，亚毫米级重复精度）。

Conclusion: Forte 机械臂以低成本实现了接近工业级的性能，为教育和AI实验提供了一个高效的机器人平台。

Abstract: This paper presents Forte, a fully 3D-printable, 6-DoF robotic arm designed
to achieve near industrial-grade performance - 0.63 kg payload, 0.467 m reach,
and sub-millimeter repeatability - at a material cost under $215. As an
accessible robot for broad applications across classroom education to AI
experiments, Forte pushes forward the performance limitations of existing
low-cost educational arms. We introduce a cost-effective mechanical design that
combines capstan-based cable drives, timing belts, simple tensioning
mechanisms, and lightweight 3D-printed structures, along with topology
optimization for structural stiffness. Through careful drivetrain engineering,
we minimize backlash and maintain control fidelity without relying on
high-power electronics or expensive manufacturing processes. Experimental
validation demonstrates that Forte achieves high repeatability and load
capacity, offering a compelling robotic platform for both classroom instruction
and advanced robotics research.

</details>


### [290] [Selective Densification for Rapid Motion Planning in High Dimensions with Narrow Passages](https://arxiv.org/abs/2507.15710)
*Lu Huang,Lingxiao Meng,Jiankun Wang,Xingjian Jing*

Main category: cs.RO

TL;DR: 本文提出了一种多分辨率采样规划框架，通过动态调整采样密度在复杂空间中高效导航，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于采样的运动规划算法在复杂配置空间（尤其是狭窄通道）中因采样效率低而性能下降，且现有启发式方法缺乏泛化性或需要大量训练。

Method: 提出了一种简单高效的基于采样的规划框架及其双向版本，通过在不同分辨率下进行均匀随机采样，并在在线探索时偏向稀疏采样以覆盖大自由配置空间。

Result: 仿真结果表明，该方法在$\mathbb{SE}(2)$、$\mathbb{SE}(3)$和$\mathbb{R}^{14}$的复杂地形中优于多种先进采样规划器，Franka Emika Panda机器人在受限工作空间中的实验进一步验证了其优越性。

Conclusion: 本文提出的多分辨率采样规划框架及其双向版本在复杂配置空间中表现出色，通过无缝切换稀疏和密集采样，既保持了规划速度又确保了完整性。

Abstract: Sampling-based algorithms are widely used for motion planning in
high-dimensional configuration spaces. However, due to low sampling efficiency,
their performance often diminishes in complex configuration spaces with narrow
corridors. Existing approaches address this issue using handcrafted or learned
heuristics to guide sampling toward useful regions. Unfortunately, these
strategies often lack generalizability to various problems or require extensive
prior training. In this paper, we propose a simple yet efficient sampling-based
planning framework along with its bidirectional version that overcomes these
issues by integrating different levels of planning granularity. Our approach
probes configuration spaces with uniform random samples at varying resolutions
and explores these multi-resolution samples online with a bias towards sparse
samples when traveling large free configuration spaces. By seamlessly
transitioning between sparse and dense samples, our approach can navigate
complex configuration spaces while maintaining planning speed and completeness.
The simulation results demonstrate that our approach outperforms several
state-of-the-art sampling-based planners in $\mathbb{SE}(2)$, $\mathbb{SE}(3)$,
and $\mathbb{R}^{14}$ with challenging terrains. Furthermore, experiments
conducted with the Franka Emika Panda robot operating in a constrained
workspace provide additional evidence of the superiority of the proposed
method.

</details>


### [291] [DiffPF: Differentiable Particle Filtering with Generative Sampling via Conditional Diffusion Models](https://arxiv.org/abs/2507.15716)
*Ziyu Wan,Lin Zhao*

Main category: cs.RO

TL;DR: DiffPF 是一种基于扩散模型的可微分粒子滤波方法，通过学习灵活的后验采样器，显著提升了状态估计的准确性，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统可微分粒子滤波依赖预定义或低容量提案分布，限制了其性能。DiffPF 旨在通过扩散模型提升状态估计的准确性和灵活性。

Method: DiffPF 通过学习一个灵活的后验采样器，利用扩散模型在预测粒子和当前观测条件下进行采样，从而实现对复杂、高维和多模态滤波分布的精确采样。

Result: DiffPF 在多种场景下表现优异，包括单模态和高度多模态分布，以及在模拟和真实世界任务中均优于现有滤波基线。例如，在高度多模态全局定位基准上，估计精度提升了 82.8%，在 KITTI 视觉里程计基准上提升了 26%。

Conclusion: DiffPF 是首个将条件扩散模型融入粒子滤波的方法，实现了高质量的后验采样，显著提升了状态估计的准确性。

Abstract: This paper proposes DiffPF, a differentiable particle filter that leverages
diffusion models for state estimation in dynamic systems. Unlike conventional
differentiable particle filters, which require importance weighting and
typically rely on predefined or low-capacity proposal distributions. DiffPF
learns a flexible posterior sampler by conditioning a diffusion model on
predicted particles and the current observation. This enables accurate,
equally-weighted sampling from complex, high-dimensional, and multimodal
filtering distributions. We evaluate DiffPF across a range of scenarios,
including both unimodal and highly multimodal distributions, and test it on
simulated as well as real-world tasks, where it consistently outperforms
existing filtering baselines. In particular, DiffPF achieves an 82.8%
improvement in estimation accuracy on a highly multimodal global localization
benchmark, and a 26% improvement on the real-world KITTI visual odometry
benchmark, compared to state-of-the-art differentiable filters. To the best of
our knowledge, DiffPF is the first method to integrate conditional diffusion
models into particle filtering, enabling high-quality posterior sampling that
produces more informative particles and significantly improves state
estimation.

</details>


### [292] [Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction](https://arxiv.org/abs/2507.15729)
*Jens V. Rüppel,Andrey Rudenko,Tim Schreiter,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: A modular, LLM-based HRI system using gaze and speech improves adaptability and user engagement in assistive robotics, though it may generate redundant outputs compared to scripted methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of bi-directional, multi-modal, and context-aware support in collaborative HRI tasks, leveraging LLMs for more flexible and general knowledge-driven systems.

Method: The paper presents a modular, transferable gaze- and speech-informed interface for assistive robots, utilizing multi-modal inputs and real-time language-based interaction state representation.

Result: The system was tested in lab studies, showing improved adaptability and user engagement over traditional scripted pipelines, though with some redundancy in output.

Conclusion: LLM-based HRI systems enhance adaptability and slightly improve user engagement and task execution, though they may produce redundant output. Scripted pipelines remain effective for straightforward tasks.

Abstract: The rapid development of Large Language Models (LLMs) creates an exciting
potential for flexible, general knowledge-driven Human-Robot Interaction (HRI)
systems for assistive robots. Existing HRI systems demonstrate great progress
in interpreting and following user instructions, action generation, and robot
task solving. On the other hand, bi-directional, multi-modal, and context-aware
support of the user in collaborative tasks still remains an open challenge. In
this paper, we present a gaze- and speech-informed interface to the assistive
robot, which is able to perceive the working environment from multiple vision
inputs and support the dynamic user in their tasks. Our system is designed to
be modular and transferable to adapt to diverse tasks and robots, and it is
capable of real-time use of language-based interaction state representation and
fast on board perception modules. Its development was supported by multiple
public dissemination events, contributing important considerations for improved
robustness and user experience. Furthermore, in two lab studies, we compare the
performance and user ratings of our system with those of a traditional scripted
HRI pipeline. Our findings indicate that an LLM-based approach enhances
adaptability and marginally improves user engagement and task execution metrics
but may produce redundant output, while a scripted pipeline is well suited for
more straightforward tasks.

</details>


### [293] [Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs](https://arxiv.org/abs/2507.15782)
*Ruochu Yang,Yu Zhou,Fumin Zhang,Mengxue Hou*

Main category: cs.RO

TL;DR: Inter-LLM算法结合LLM与运动规划，显著提升家庭机器人在多对象收集任务中的性能，实验证明其优于现有方法30%。


<details>
  <summary>Details</summary>
Motivation: 家庭机器人在处理开放集对象和大型环境导航时缺乏类人智能，尤其在多对象收集任务中面临长时规划和高度不确定性的挑战。

Method: 提出了一种新颖的交错LLM和运动规划算法Inter-LLM，设计了一种多模态动作成本相似性函数，以优化长期任务规划。

Result: 仿真实验表明，Inter-LLM算法在任务完成率、成功率和成本效率上比现有方法提升30%。

Conclusion: Inter-LLM算法通过结合LLM和运动规划，在家庭机器人执行多对象收集任务时显著提升了任务完成率、成功率和成本效率，验证了其在长时任务规划中的有效性。

Abstract: Household robots have been a longstanding research topic, but they still lack
human-like intelligence, particularly in manipulating open-set objects and
navigating large environments efficiently and accurately. To push this
boundary, we consider a generalized multi-object collection problem in large
scene graphs, where the robot needs to pick up and place multiple objects
across multiple locations in a long mission of multiple human commands. This
problem is extremely challenging since it requires long-horizon planning in a
vast action-state space under high uncertainties. To this end, we propose a
novel interleaved LLM and motion planning algorithm Inter-LLM. By designing a
multimodal action cost similarity function, our algorithm can both reflect the
history and look into the future to optimize plans, striking a good balance of
quality and efficiency. Simulation experiments demonstrate that compared with
latest works, our algorithm improves the overall mission performance by 30% in
terms of fulfilling human commands, maximizing mission success rates, and
minimizing mission costs.

</details>


### [294] [Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers](https://arxiv.org/abs/2507.15833)
*Ian Chuang,Andrew Lee,Dechen Gao,Jinyu Zou,Iman Soltani*

Main category: cs.RO

TL;DR: 研究提出了一种模仿人类主动注视的机器人视觉框架，通过中心凹图像处理减少计算量并提升性能，验证了人类视觉机制对机器人系统的价值。


<details>
  <summary>Details</summary>
Motivation: 人类视觉通过注视主动引导注意力，而机器人学习系统通常依赖被动的均匀图像处理。研究旨在探索如何通过模仿人类主动注视提升机器人策略的效率和性能。

Method: 该研究结合了人类主动注视机制，提出了一种基于中心凹图像处理的机器人视觉框架，包括两种注视模仿和预测方法：一种是两阶段模型预测注视以指导中心凹处理和动作；另一种是端到端联合预测注视和动作。

Result: 实验结果表明，中心凹机器人视觉方法不仅大幅降低了计算开销，还提高了高精度任务的性能和对抗未知干扰物的鲁棒性。

Conclusion: 人类启发的视觉处理为机器人视觉系统提供了有用的归纳偏置，显著提高了计算效率和任务性能。

Abstract: Human vision is a highly active process driven by gaze, which directs
attention and fixation to task-relevant regions and dramatically reduces visual
processing. In contrast, robot learning systems typically rely on passive,
uniform processing of raw camera images. In this work, we explore how
incorporating human-like active gaze into robotic policies can enhance both
efficiency and performance. We build on recent advances in foveated image
processing and apply them to an Active Vision robot system that emulates both
human head movement and eye tracking. Extending prior work on the AV-ALOHA
robot simulation platform, we introduce a framework for simultaneously
collecting eye-tracking data and robot demonstrations from a human operator as
well as a simulation benchmark and dataset for training robot policies that
incorporate human gaze. Given the widespread use of Vision Transformers (ViTs)
in robot learning, we integrate gaze information into ViTs using a foveated
patch tokenization scheme inspired by recent work in image segmentation.
Compared to uniform patch tokenization, this significantly reduces the number
of tokens-and thus computation-without sacrificing visual fidelity near regions
of interest. We also explore two approaches to gaze imitation and prediction
from human data. The first is a two-stage model that predicts gaze to guide
foveation and action; the second integrates gaze into the action space,
allowing the policy to jointly predict gaze and actions end-to-end. Our results
show that our method for foveated robot vision not only drastically reduces
computational overhead, but also improves performance for high precision tasks
and robustness to unseen distractors. Together, these findings suggest that
human-inspired visual processing offers a useful inductive bias for robotic
vision systems. https://ian-chuang.github.io/gaze-av-aloha/

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [295] [Real-Time Scene Reconstruction using Light Field Probes](https://arxiv.org/abs/2507.14624)
*Yaru Liu,Derek Nowrouzezahri,Morgan Mcguire*

Main category: cs.GR

TL;DR: 提出一种基于探针数据结构的神经表示方法，无需显式几何即可高效重建大规模复杂场景，适用于VR/AR。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经渲染方法在大规模复杂场景中效率不足的问题，同时避免显式几何数据的高成本。

Method: 利用探针数据结构重建中间、多尺度、隐式的场景几何表示，结合稀疏图像输入。

Result: 实现了高效的大规模复杂场景重建，渲染成本与场景复杂度无关。

Conclusion: 该方法通过探针数据结构重建场景，避免了显式依赖场景几何，显著降低了计算成本，适用于VR和AR应用。

Abstract: Reconstructing photo-realistic large-scale scenes from images, for example at
city scale, is a long-standing problem in computer graphics. Neural rendering
is an emerging technique that enables photo-realistic image synthesis from
previously unobserved viewpoints; however, state-of-the-art neural rendering
methods have difficulty efficiently rendering a high complex large-scale scene
because these methods typically trade scene size, fidelity, and rendering speed
for quality. The other stream of techniques utilizes scene geometries for
reconstruction. But the cost of building and maintaining a large set of
geometry data increases as scene size grows. Our work explores novel view
synthesis methods that efficiently reconstruct complex scenes without explicit
use of scene geometries. Specifically, given sparse images of the scene
(captured from the real world), we reconstruct intermediate, multi-scale,
implicit representations of scene geometries. In this way, our method avoids
explicitly relying on scene geometry, significantly reducing the computational
cost of maintaining large 3D data. Unlike current methods, we reconstruct the
scene using a probe data structure. Probe data hold highly accurate depth
information of dense data points, enabling the reconstruction of highly complex
scenes. By reconstructing the scene using probe data, the rendering cost is
independent of the complexity of the scene. As such, our approach combines
geometry reconstruction and novel view synthesis. Moreover, when rendering
large-scale scenes, compressing and streaming probe data is more efficient than
using explicit scene geometry. Therefore, our neural representation approach
can potentially be applied to virtual reality (VR) and augmented reality (AR)
applications.

</details>


### [296] [Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization](https://arxiv.org/abs/2507.14841)
*Xiang Tang,Ruotong Li,Xiaopeng Fan*

Main category: cs.GR

TL;DR: 提出一种三阶段框架，通过图像分割、伪立体视角和参数优化，从单张RGB图像生成高质量的3D场景。


<details>
  <summary>Details</summary>
Motivation: 解决从单张RGB图像生成3D场景时对象生成质量和场景一致性的挑战。

Method: 采用图像实例分割与修复、伪立体视角构建以及模型选择和参数优化三个阶段的框架。

Result: 在多个多对象场景图像集上的实验表明，该方法在几何精度、纹理保真度和场景布局合成上优于现有技术。

Conclusion: 提出的三阶段框架在几何精度和纹理保真度上优于现有方法，并在场景布局合成方面具有显著优势。

Abstract: In recent years, 3D generation has made great strides in both academia and
industry. However, generating 3D scenes from a single RGB image remains a
significant challenge, as current approaches often struggle to ensure both
object generation quality and scene coherence in multi-object scenarios. To
overcome these limitations, we propose a novel three-stage framework for 3D
scene generation with explicit geometric representations and high-quality
textural details via single image-guided model generation and spatial layout
optimization. Our method begins with an image instance segmentation and
inpainting phase, which recovers missing details of occluded objects in the
input images, thereby achieving complete generation of foreground 3D assets.
Subsequently, our approach captures the spatial geometry of reference image by
constructing pseudo-stereo viewpoint for camera parameter estimation and scene
depth inference, while employing a model selection strategy to ensure optimal
alignment between the 3D assets generated in the previous step and the input.
Finally, through model parameterization and minimization of the Chamfer
distance between point clouds in 3D and 2D space, our approach optimizes layout
parameters to produce an explicit 3D scene representation that maintains
precise alignment with input guidance image. Extensive experiments on
multi-object scene image sets have demonstrated that our approach not only
outperforms state-of-the-art methods in terms of geometric accuracy and texture
fidelity of individual generated 3D models, but also has significant advantages
in scene layout synthesis.

</details>


### [297] [Time Series Information Visualization -- A Review of Approaches and Tools](https://arxiv.org/abs/2507.14920)
*Evandro S. Ortigossa,Fábio F. Dias,Diego C. Nascimento,Luis Gustavo Nonato*

Main category: cs.GR

TL;DR: 本文综述了时间序列数据可视化的技术和方法，强调多特征时间序列的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在各领域普遍存在，但其分析需要复杂工具。信息可视化能通过图形表示增强数据的可解释性，帮助数据科学家理解动态行为和发现模式。

Method: 本文综述了用于处理时间序列数据的技术和方法，通过视觉分析引导用户进行知识发现。

Result: 本文提供了理论见解和设计指南，特别关注多特征时间序列的可视化方法。

Conclusion: 本文强调了时间序列数据可视化面临的挑战和未来的研究方向，特别是在多特征时间序列的处理上。

Abstract: Time series data are prevalent across various domains and often encompass
large datasets containing multiple time-dependent features in each sample.
Exploring time-varying data is critical for data science practitioners aiming
to understand dynamic behaviors and discover periodic patterns and trends.
However, the analysis of such data often requires sophisticated procedures and
tools. Information visualization is a communication channel that leverages
human perceptual abilities to transform abstract data into visual
representations. Visualization techniques have been successfully applied in the
context of time series to enhance interpretability by graphically representing
the temporal evolution of data. The challenge for information visualization
developers lies in integrating a wide range of analytical tools into rich
visualization systems that can summarize complex datasets while clearly
describing the impacts of the temporal component. Such systems enable data
scientists to turn raw data into understandable and potentially useful
knowledge. This review examines techniques and approaches designed for handling
time series data, guiding users through knowledge discovery processes based on
visual analysis. We also provide readers with theoretical insights and design
guidelines for considering when developing comprehensive information
visualization approaches for time series, with a particular focus on time
series with multiple features. As a result, we highlight the challenges and
future research directions to address open questions in the visualization of
time-dependent data.

</details>


### [298] [Model Simplification through refinement](https://arxiv.org/abs/2507.15186)
*Dmitry Brodsky,Benjamin Watson*

Main category: cs.GR

TL;DR: 论文提出了一种快速交互式简化大型多边形模型的算法，通过逆向细化过程，利用表面曲率指导简化，保证了速度和结果质量。


<details>
  <summary>Details</summary>
Motivation: 随着建模和可视化应用的普及，需要以交互速率简化大型多边形模型，但现有算法要么速度过慢，要么生成的模型质量较差，尤其在处理极大模型时问题更为突出。

Method: 受矢量量化文献中的分割算法启发，该算法从极其粗糙的近似开始，逐步细化，利用表面曲率的近似来指导简化过程。

Result: 该算法快速且能在给定时间限制内保证可显示的结果，同时保持较高的质量。

Conclusion: 该论文提出了一种适用于大型模型交互式简化的快速算法，能够在限定时间内生成可显示且质量良好的结果。

Abstract: As modeling and visualization applications proliferate, there arises a need
to simplify large polygonal models at interactive rates. Unfortunately existing
polygon mesh simplification algorithms are not well suited for this task
because they are either too slow (requiring the simplified model to be
pre-computed) or produce models that are too poor in quality. These
shortcomings become particularly acute when models are extremely large. We
present an algorithm suitable for simplification of large models at interactive
speeds. The algorithm is fast and can guarantee displayable results within a
given time limit. Results also have good quality. Inspired by splitting
algorithms from vector quantization literature, we simplify models in reverse,
beginning with an extremely coarse approximation and refining it.
Approximations of surface curvature guide the simplification process.
Previously produced simplifications can be further refined by using them as
input to the algorithm.

</details>


### [299] [Blended Point Cloud Diffusion for Localized Text-guided Shape Editing](https://arxiv.org/abs/2507.15399)
*Etai Sella,Noam Atia,Ron Mokady,Hadar Averbuch-Elor*

Main category: cs.GR

TL;DR: 提出了一种基于3D扩散模型和坐标混合算法的3D点云编辑方法，有效平衡局部编辑与全局一致性，实验表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 自然语言为3D形状的局部精细编辑提供了直观接口，但现有方法在保持全局一致性的同时进行局部修改存在困难。

Method: 提出了一种基于修复的框架，利用基础3D扩散模型进行局部形状编辑，并通过部分条件形状提供结构指导。此外，设计了推理时坐标混合算法，平衡全形状重建与噪声级别渐进修复。

Result: 实验表明，该方法在保持原始形状保真度和遵循文本描述方面均优于其他技术。

Conclusion: 该方法通过结合3D扩散模型和坐标混合算法，有效实现了3D点云形状的局部精细编辑，同时保持了全局一致性和形状身份，显著优于现有技术。

Abstract: Natural language offers a highly intuitive interface for enabling localized
fine-grained edits of 3D shapes. However, prior works face challenges in
preserving global coherence while locally modifying the input 3D shape. In this
work, we introduce an inpainting-based framework for editing shapes represented
as point clouds. Our approach leverages foundation 3D diffusion models for
achieving localized shape edits, adding structural guidance in the form of a
partial conditional shape, ensuring that other regions correctly preserve the
shape's identity. Furthermore, to encourage identity preservation also within
the local edited region, we propose an inference-time coordinate blending
algorithm which balances reconstruction of the full shape with inpainting at a
progression of noise levels during the inference process. Our coordinate
blending algorithm seamlessly blends the original shape with its edited
version, enabling a fine-grained editing of 3D shapes, all while circumventing
the need for computationally expensive and often inaccurate inversion.
Extensive experiments show that our method outperforms alternative techniques
across a wide range of metrics that evaluate both fidelity to the original
shape and also adherence to the textual description.

</details>


### [300] [ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting](https://arxiv.org/abs/2507.15454)
*Ruijie Zhu,Mulin Yu,Linning Xu,Lihan Jiang,Yixuan Li,Tianzhu Zhang,Jiangmiao Pang,Bo Dai*

Main category: cs.GR

TL;DR: ObjectGS通过物体感知框架，将3D高斯泼溅技术与语义理解结合，提升了物体级重建和分割性能。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术虽然能实现高保真重建和实时新视角合成，但缺乏语义理解能力，限制了物体级感知。ObjectGS旨在解决这一问题。

Method: ObjectGS将场景中的个体对象建模为局部锚点，生成神经高斯并共享对象ID，实现精确的物体级重建。训练过程中动态调整锚点并优化特征，同时通过分类损失强化语义约束。

Result: ObjectGS在开放词汇和全景分割任务中表现优异，且能无缝集成网格提取和场景编辑等应用。

Conclusion: ObjectGS通过将3D场景重建与语义理解相结合，显著提升了3D高斯泼溅技术在物体级感知方面的能力，并在开放词汇和全景分割任务中超越了现有技术。

Abstract: 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and
real-time novel view synthesis, yet its lack of semantic understanding limits
object-level perception. In this work, we propose ObjectGS, an object-aware
framework that unifies 3D scene reconstruction with semantic understanding.
Instead of treating the scene as a unified whole, ObjectGS models individual
objects as local anchors that generate neural Gaussians and share object IDs,
enabling precise object-level reconstruction. During training, we dynamically
grow or prune these anchors and optimize their features, while a one-hot ID
encoding with a classification loss enforces clear semantic constraints. We
show through extensive experiments that ObjectGS not only outperforms
state-of-the-art methods on open-vocabulary and panoptic segmentation tasks,
but also integrates seamlessly with applications like mesh extraction and scene
editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page

</details>


### [301] [Gaussian Splatting with Discretized SDF for Relightable Assets](https://arxiv.org/abs/2507.15629)
*Zuo-Liang Zhu,Jian Yang,Beibei Wang*

Main category: cs.GR

TL;DR: 提出离散化SDF表示方法，通过投影一致性损失优化高斯基元的几何约束，提升逆向渲染质量且保持高效。


<details>
  <summary>Details</summary>
Motivation: 解决高斯基元在逆向渲染中因离散性难以应用几何约束的问题，同时避免现有方法增加内存和训练复杂度的缺点。

Method: 采用离散化SDF表示，通过SDF-to-opacity转换链接SDF与高斯不透明度，并引入投影一致性损失来正则化离散样本。

Result: 实验表明，该方法在逆向渲染任务中优于现有基于高斯的方案，且无需额外内存或复杂优化。

Conclusion: 本文提出了一种离散化的SDF表示方法，通过将连续SDF编码到每个高斯中，解决了高斯基元在逆向渲染中的几何约束问题，显著提升了重光照质量，且无需额外内存或复杂优化设计。

Abstract: 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and
highly efficient rendering speed in the novel view synthesis (NVS) task. The
application to inverse rendering still faces several challenges, as the
discrete nature of Gaussian primitives makes it difficult to apply geometry
constraints. Recent works introduce the signed distance field (SDF) as an extra
continuous representation to regularize the geometry defined by Gaussian
primitives. It improves the decomposition quality, at the cost of increasing
memory usage and complicating training. Unlike these works, we introduce a
discretized SDF to represent the continuous SDF in a discrete manner by
encoding it within each Gaussian using a sampled value. This approach allows us
to link the SDF with the Gaussian opacity through an SDF-to-opacity
transformation, enabling rendering the SDF via splatting and avoiding the
computational cost of ray marching.The key challenge is to regularize the
discrete samples to be consistent with the underlying SDF, as the discrete
representation can hardly apply the gradient-based constraints (\eg Eikonal
loss). For this, we project Gaussians onto the zero-level set of SDF and
enforce alignment with the surface from splatting, namely a projection-based
consistency loss. Thanks to the discretized SDF, our method achieves higher
relighting quality, while requiring no extra memory beyond GS and avoiding
complex manually designed optimization. The experiments reveal that our method
outperforms existing Gaussian-based inverse rendering methods. Our code is
available at https://github.com/NK-CS-ZZL/DiscretizedSDF.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [302] [Characterizing Communication Patterns in Distributed Large Language Model Inference](https://arxiv.org/abs/2507.14392)
*Lang Xu,Kaushik Kandadi Suresh,Quentin Anthony,Nawras Alnaasan,Dhabaleswar K. Panda*

Main category: cs.DC

TL;DR: 论文分析了分布式LLM推理中的通信动态，发现不同并行化方法各有优劣，并提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: 研究分布式LLM服务中GPU间通信对性能的限制，分析不同并行化方法如何协调GPU工作节点间的数据交换。

Method: 结合详细的性能分析测量和预测性分析模型，研究了不同并行化配置下的通信行为。

Result: 研究表明，张量并行化带来显著的网络开销但对短序列响应时间更优，流水线并行化减少数据传输但增加总延迟，组合方法需要精细调优以实现平衡性能。

Conclusion: 论文提出了针对分布式大语言模型（LLM）服务中通信动态的详细分析，并提供了选择并行化方案的实际建议，同时指出了优化推理框架和通信基础设施的关键机会。

Abstract: Large Language Models (LLMs) built on transformer architectures have
transformed natural language processing, achieving remarkable performance
across diverse applications. While distributed inference frameworks enable
practical deployment of these models, inter-GPU communication creates
significant performance constraints that limit service quality in real-world
systems. This paper investigates communication dynamics in distributed LLM
serving-analyzing how various parallelization approaches coordinate data
exchange between GPU workers during inference. We study dense transformer-based
models as representative examples of contemporary architectures widely used in
operational deployments. Our work combines detailed profiling measurements with
predictive analytical models to characterize communication behavior across
different parallelization configurations. Results show that tensor parallelism
incurs substantial network overhead but delivers superior response times for
brief sequences, pipeline parallelism minimizes data transfer requirements
while increasing total latency, and combined approaches demand careful tuning
to achieve balanced performance. These insights offer practical recommendations
for selecting appropriate parallelization schemes in production LLM services
and identify key opportunities for optimizing inference frameworks and
communication infrastructure.

</details>


### [303] [Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning](https://arxiv.org/abs/2507.14597)
*Eugene Armah,Linda Amoako Bannning*

Main category: cs.DC

TL;DR: 本文提出一种三步主动边缘流处理自动扩展方案，结合GRU预测、迁移学习和动态扩展，显著提升负载预测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 边缘流处理面临快速工作负载波动，现有反应式方法（如基于阈值的策略和排队理论）在性能下降后才进行扩展，可能违反SLA。强化学习虽提供主动方法，但需要大量模拟。预测性机器学习模型则面临在线分布和概念漂移问题。

Method: 采用GRU神经网络进行上游负载预测，结合迁移学习框架和动态时间规整（DTW）算法处理离线与在线域差异，最后通过水平自动扩展模块动态调整算子并行度。

Result: 轻量级GRU模型在真实数据集上实现了1.3%的SMAPE值，优于CNN、ARIMA和Prophet模型，且训练时间短于计算密集的强化学习模型。

Conclusion: 本研究提出了一种三步解决方案，用于主动边缘流处理的自动扩展问题，包括GRU神经网络预测上游负载、迁移学习框架集成预测模型以及基于预测负载的动态水平自动扩展模块。该方法在真实数据集上表现出色，优于其他模型。

Abstract: Processing data at high speeds is becoming increasingly critical as digital
economies generate enormous data. The current paradigms for timely data
processing are edge computing and data stream processing (DSP). Edge computing
places resources closer to where data is generated, while stream processing
analyzes the unbounded high-speed data in motion. However, edge stream
processing faces rapid workload fluctuations, complicating resource
provisioning. Inadequate resource allocation leads to bottlenecks, whereas
excess allocation results in wastage. Existing reactive methods, such as
threshold-based policies and queuing theory scale only after performance
degrades, potentially violating SLAs. Although reinforcement learning (RL)
offers a proactive approach through agents that learn optimal runtime
adaptation policies, it requires extensive simulation. Furthermore, predictive
machine learning models face online distribution and concept drift that
minimize their accuracy. We propose a three-step solution to the proactive edge
stream processing autoscaling problem. Firstly, a GRU neural network forecasts
the upstream load using real-world and synthetic DSP datasets. Secondly, a
transfer learning framework integrates the predictive model into an online
stream processing system using the DTW algorithm and joint distribution
adaptation to handle the disparities between offline and online domains.
Finally, a horizontal autoscaling module dynamically adjusts the degree of
operator parallelism, based on predicted load while considering edge resource
constraints. The lightweight GRU model for load predictions recorded up to
1.3\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and
Prophet on the SMAPE and RMSE evaluation metrics, with lower training time than
the computationally intensive RL models.

</details>


### [304] [Simulating Chirality: Solving Distance-$k$-Dispersion on an 1-Interval Connected Ring](https://arxiv.org/abs/2507.14723)
*Brati Mondal,Pritam Goswami,Buddhadeb Sau*

Main category: cs.DC

TL;DR: 本研究提出了一种在无手性假设的1-interval-connected环网络中解决Distance-$k$-Dispersion问题的方法，并证明了其可解性，扩展了动态网络中移动代理协调的理论理解。


<details>
  <summary>Details</summary>
Motivation: 研究在1-interval-connected环网络中，没有手性假设的情况下，解决Distance-$k$-Dispersion问题，以推广经典的分散问题。

Method: 提出了一种新颖的方法，使代理能够仅使用本地信息、视觉和有限内存模拟手性。基于此，部分解决了Agarwalla等人提出的开放性问题，并提出了一个在O(ln)轮内解决D-$k$-D问题的算法。

Result: 证明了D-$k$-D和分散问题在这些假设下是可解的，且适用于任何大小的环网络，并提出了一个高效的算法。

Conclusion: 本研究显著扩展了动态网络中移动代理协调的理论理解，并澄清了手性在分布式计算中的作用。

Abstract: We study the Distance-$k$-Dispersion (D-$k$-D) problem for synchronous mobile
agents in a 1-interval-connected ring network having $n$ nodes and with $l$
agents where $3 \le l \le \lfloor \frac{n}{k}\rfloor$, without the assumption
of chirality (a common sense of direction for the agents). This generalizes the
classical dispersion problem by requiring that agents maintain a minimum
distance of $k$ hops from each other, with the special case $k=1$ corresponding
to the standard dispersion.
  The contribution in this work is threefold. Our first contribution is a novel
method that enables agents to simulate chirality using only local information,
vision and bounded memory. This technique demonstrates that chirality is not a
fundamental requirement for coordination in this model.
  Building on this, our second contribution partially resolves an open question
posed by Agarwalla et al. (ICDCN, 2018), who considered the same model (1-
interval connected ring, synchronous agents, no chirality). We prove that
D-$k$-D, and thus dispersion is solvable from any arbitrary configuration under
these assumptions (excluding vertex permutation dynamism)for any size of the
ring network which was earlier limited to only odd sized ring or to a ring of
size four.
  Finally, we present an algorithm for D-$k$-D in this setting that works in
$O(ln)$ rounds, completing the constructive side of our result.
  Altogether, our findings significantly extend the theoretical understanding
of mobile agent coordination in dynamic networks and clarify the role of
chirality in distributed computation.

</details>


### [305] [ACME: Adaptive Customization of Large Models via Distributed Systems](https://arxiv.org/abs/2507.14802)
*Ziming Dai,Chao Qiu,Fei Gao,Yunfeng Zhao,Xiaofei Wang*

Main category: cs.DC

TL;DR: ACME 是一种分布式自适应定制方法，通过双向单循环系统优化 Transformer 大模型的部署，显著降低数据传输量并提升性能。


<details>
  <summary>Details</summary>
Motivation: 预训练的 Transformer 大模型在云环境部署中面临数据隐私、响应延迟和资源效率等挑战，需要一种高效的自适应定制方法以应对模型不匹配、资源限制和能源效率问题。

Method: ACME 采用双向单循环分布式系统，逐步实现细粒度协作模型定制，包括主干生成、Pareto 前沿识别、头部生成以及基于数据分布的个性化架构聚合。

Result: ACME 在模型大小约束下实现了高成本效益的模型定制，数据传输量减少至 6%，平均准确率提升 10%，权衡指标增加近 30%。

Conclusion: ACME 提出了一种基于分布式系统的自适应定制方法，有效解决了 Transformer 大模型在部署中的数据隐私、响应延迟和资源效率问题，显著提升了性能和成本效益。

Abstract: Pre-trained Transformer-based large models have revolutionized personal
virtual assistants, but their deployment in cloud environments faces challenges
related to data privacy and response latency. Deploying large models closer to
the data and users has become a key research area to address these issues.
However, applying these models directly often entails significant difficulties,
such as model mismatching, resource constraints, and energy inefficiency.
Automated design of customized models is necessary, but it faces three key
challenges, namely, the high cost of centralized model customization,
imbalanced performance from user heterogeneity, and suboptimal performance from
data heterogeneity. In this paper, we propose ACME, an adaptive customization
approach of Transformer-based large models via distributed systems. To avoid
the low cost-efficiency of centralized methods, ACME employs a bidirectional
single-loop distributed system to progressively achieve fine-grained
collaborative model customization. In order to better match user heterogeneity,
it begins by customizing the backbone generation and identifying the Pareto
Front under model size constraints to ensure optimal resource utilization.
Subsequently, it performs header generation and refines the model using data
distribution-based personalized architecture aggregation to match data
heterogeneity. Evaluation on different datasets shows that ACME achieves
cost-efficient models under model size constraints. Compared to centralized
systems, data transmission volume is reduced to 6 percent. Additionally, the
average accuracy improves by 10 percent compared to the baseline, with the
trade-off metrics increasing by nearly 30 percent.

</details>


### [306] [GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis](https://arxiv.org/abs/2507.15230)
*Guoxi Liu,Thomas Randall,Rong Ge,Federico Iuricich*

Main category: cs.DC

TL;DR: GALE是一种针对异构CPU-GPU系统优化的任务并行数据结构，显著提升非结构化网格数据处理性能。


<details>
  <summary>Details</summary>
Motivation: 非结构化网格的复杂连接性和不规则分布对科学数据分析构成挑战，现有方法因CPU限制而性能受限。

Method: 提出了一种新颖的任务并行方法，将网格连接信息的计算卸载到GPU线程，使CPU线程专注于执行可视化算法。

Result: 实验表明，GALE在20核CPU和NVIDIA V100 GPU上比现有局部数据结构快2.7倍。

Conclusion: GALE（GPU-Aided Localized data structurE）通过异构CPU-GPU系统优化任务并行方法，显著提升了非结构化网格数据的处理性能，同时保持了内存效率。

Abstract: Unstructured meshes present challenges in scientific data analysis due to
irregular distribution and complex connectivity. Computing and storing
connectivity information is a major bottleneck for visualization algorithms,
affecting both time and memory performance. Recent task-parallel data
structures address this by precomputing connectivity information at runtime
while the analysis algorithm executes, effectively hiding computation costs and
improving performance. However, existing approaches are CPU-bound, forcing the
data structure and analysis algorithm to compete for the same computational
resources, limiting potential speedups. To overcome this limitation, we
introduce a novel task-parallel approach optimized for heterogeneous CPU-GPU
systems. Specifically, we offload the computation of mesh connectivity
information to GPU threads, enabling CPU threads to focus on executing the
visualization algorithm. Following this paradigm, we propose GALE (GPU-Aided
Localized data structurE), the first open-source CUDA-based data structure
designed for heterogeneous task parallelism. Experiments on two 20-core CPUs
and an NVIDIA V100 GPU show that GALE achieves up to 2.7x speedup over
state-of-the-art localized data structures while maintaining memory efficiency.

</details>


### [307] [Byzantine-Robust Decentralized Coordination of LLM Agents](https://arxiv.org/abs/2507.14928)
*Yongrae Jo,Chanik Park*

Main category: cs.DC

TL;DR: DecentLLMs 是一种去中心化多智能体 LLM 系统，通过并行生成和独立评估答案，解决了传统领导者系统的弱点，提升了答案质量和系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基于领导者的多智能体系统易受针对性攻击且可能接受低质量提案，DecentLLMs 旨在解决这些问题。

Method: DecentLLMs 采用去中心化架构，其中工作智能体并行生成答案，评估智能体独立评分并排名，通过 Byzantine-robust 聚合技术选择最佳答案。

Result: 实验表明，DecentLLMs 能有效容忍恶意智能体并显著提升所选答案的质量。

Conclusion: DecentLLMs 提出了一种去中心化的共识方法，有效解决了传统基于领导者的多智能体系统中的弱点，显著提高了答案质量并增强了对恶意行为的容忍度。

Abstract: Collaboration among multiple large language model (LLM) agents is a promising
approach to overcome inherent limitations of single-agent systems, such as
hallucinations and single points of failure. As LLM agents are increasingly
deployed on open blockchain platforms, multi-agent systems capable of
tolerating malicious (Byzantine) agents have become essential.
  Recent Byzantine-robust multi-agent systems typically rely on leader-driven
coordination, which suffers from two major drawbacks. First, they are
inherently vulnerable to targeted attacks against the leader. If consecutive
leaders behave maliciously, the system repeatedly fails to achieve consensus,
forcing new consensus rounds, which is particularly costly given the high
latency of LLM invocations. Second, an underperforming proposal from the leader
can be accepted as the final answer even when higher-quality alternatives are
available, as existing methods finalize the leader's proposal once it receives
a quorum of votes.
  To address these issues, we propose DecentLLMs, a novel decentralized
consensus approach for multi-agent LLM systems, where worker agents generate
answers concurrently and evaluator agents independently score and rank these
answers to select the best available one. This decentralized architecture
enables faster consensus despite the presence of Byzantine agents and
consistently selects higher-quality answers through Byzantine-robust
aggregation techniques.
  Experimental results demonstrate that DecentLLMs effectively tolerates
Byzantine agents and significantly improves the quality of selected answers.

</details>


### [308] [AMPED: Accelerating MTTKRP for Billion-Scale Sparse Tensor Decomposition on Multiple GPUs](https://arxiv.org/abs/2507.15121)
*Sasindu Wijeratne,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.DC

TL;DR: AMPED是一种多GPU并行算法，通过分区和动态负载平衡加速大规模稀疏张量的MTTKRP计算，比现有GPU基线快5.1倍。


<details>
  <summary>Details</summary>
Motivation: 随着现实世界中稀疏张量规模增长至数十亿非零元素，对硬件加速器的内存容量和计算吞吐量提出了更高要求。

Method: 引入分区策略和动态负载平衡方案，以分配计算并最小化GPU空闲时间。

Result: 在真实世界数十亿规模的张量上，AMPED使用4个GPU在单个CPU节点上实现了5.1倍的几何平均加速比。

Conclusion: AMPED算法通过多GPU并行处理，有效解决了大规模稀疏张量分解中的MTTKRP计算瓶颈，显著提升了执行效率。

Abstract: Matricized Tensor Times Khatri-Rao Product (MTTKRP) is the computational
bottleneck in sparse tensor decomposition. As real-world sparse tensors grow to
billions of nonzeros, they increasingly demand higher memory capacity and
compute throughput from hardware accelerators. In this work, we present AMPED,
a multi-GPU parallel algorithm designed to accelerate MTTKRP on billion-scale
sparse tensors. AMPED scales beyond the limits of a single GPU, meeting both
the memory and performance requirements of large-scale workloads. We introduce
a partitioning strategy combined with a dynamic load balancing scheme to
distribute computation and minimize GPU idle time. On real-world billion-scale
tensors, AMPED achieves a 5.1x geometric mean speedup in total execution time
over state-of-the-art GPU baselines using 4 GPUs on a single CPU node.

</details>


### [309] [Dynatune: Dynamic Tuning of Raft Election Parameters Using Network Measurement](https://arxiv.org/abs/2507.15154)
*Kohya Shiozaki,Junya Nakamura*

Main category: cs.DC

TL;DR: Dynatune动态调整Raft选举参数，显著减少领导者故障检测和OTS时间，提升服务可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统Raft算法在波动网络条件下难以有效调整选举参数，导致OTS时间增加和服务响应性下降。

Method: 提出Dynatune机制，基于网络指标（如往返时间和丢包率）动态调整Raft选举参数。

Result: 实验结果显示，Dynatune将领导者故障检测和OTS时间分别降低了80%和45%，同时保持高可用性。

Conclusion: Dynatune通过动态调整Raft选举参数，显著降低了领导者故障检测和OTS时间，提升了SMR服务的性能和可靠性。

Abstract: Raft is a leader-based consensus algorithm that implements State Machine
Replication (SMR), which replicates the service state across multiple servers
to enhance fault tolerance. In Raft, the servers play one of three roles:
leader, follower, or candidate. The leader receives client requests, determines
the processing order, and replicates them to the followers. When the leader
fails, the service must elect a new leader to continue processing requests,
during which the service experiences an out-of-service (OTS) time. The OTS time
is directly influenced by election parameters, such as heartbeat interval and
election timeout. However, traditional approaches, such as Raft, often struggle
to effectively tune these parameters, particularly under fluctuating network
conditions, leading to increased OTS time and reduced service responsiveness.
To address this, we propose Dynatune, a mechanism that dynamically adjusts
Raft's election parameters based on network metrics such as round-trip time and
packet loss rates measured via heartbeats. By adapting to changing network
environments, Dynatune significantly reduces the leader failure detection and
OTS time without altering Raft's core mechanisms or introducing additional
communication overheads. Experimental results demonstrate that Dynatune reduces
the leader failure detection and OTS times by 80% and 45%, respectively,
compared with Raft, while maintaining high availability even under dynamic
network conditions. These findings confirm that Dynatune effectively enhances
the performance and reliability of SMR services in various network scenarios.

</details>


### [310] [An ML-Driven Participant Selection Technique for Federated Recommendation System in Edge-Cloud Computing](https://arxiv.org/abs/2507.15233)
*Jintao Liu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 论文提出一种多目标强化学习方法优化联邦推荐系统的客户端选择，显著提升训练效率和公平性，实验证明其收敛速度更快且性能更优。


<details>
  <summary>Details</summary>
Motivation: 联邦推荐系统（FRS）虽能保护隐私并实现分布式训练，但面临设备能力异构、数据非独立同分布（non-IID）和通信瓶颈等问题。

Method: 提出了一种多目标强化学习（RL）参与者选择方法，结合历史客户端性能声誉（CPR）、数据效用和系统效率进行联合优化，并嵌入多臂老虎机（MAB）框架实现动态探索-开发平衡。

Result: 在四种数据倾斜场景下，基于MAB的选择方法将收敛时间缩短32-50%，总训练时间减少达46%，同时AUC、NDCG@50和Recall@50指标与现有基线相当或略有提升。

Conclusion: 自适应、奖励驱动的客户端采样能显著提升联邦推荐系统的效率和公平性。

Abstract: Recommendation systems (RS) personalize content by analyzing user
preferences, but typically require centralized collection of user data, raising
privacy and scalability concerns. Federated Recommendation Systems (FRS)
address these issues by enabling distributed, privacy-preserving model training
across edge devices, keeping raw data on-device. Although existing FRS
frameworks benefit from on-device feature extraction and privacy preservation,
they suffer from heterogeneous device capabilities, non-independent and
identically distributed (non-IID) data, and communication bottlenecks. To
overcome these limitations, we propose a multi-objective reinforcement learning
(RL) participant selection that jointly optimizes historical client performance
reputation (CPR), data utility, and system efficiency. First, we define a
composite client-utility function combining CPR, system capability, and data
quality. Next, we embed this utility into a multi-armed bandit (MAB) framework
and dynamically balance exploration-exploitation to select participants.
Finally, we practically implement our approach using the PySyft framework on an
edge-cloud testbed, and evaluate it on a multimodal movie-recommendation task
built from the MovieLens-100K dataset. Across four different skewed
data-partition scenarios, our MAB-based selection accelerates convergence by
32-50% in time-to-target AUC and reduces total wall-clock training time by up
to 46%, while matching or slightly improving final AUC, NDCG@50, and Recall@50
compared to existing FRS baselines. Our results demonstrate that adaptive,
reward-driven client sampling can substantially enhance both efficiency and
fairness in real-world federated deployments.

</details>


### [311] [Efficient Routing of Inference Requests across LLM Instances in Cloud-Edge Computing](https://arxiv.org/abs/2507.15553)
*Shibo Yu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 论文提出了一种基于NSGA-II的路由算法，用于优化云边计算环境中的LLM推理服务，显著降低了延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）推理服务需求的增加，计算资源面临延迟和成本压力，亟需一种高效的路由算法来优化资源分配。

Method: 论文提出了一种基于NSGA-II的新型路由算法，用于在异构LLM实例间分配推理请求。该算法通过多目标优化问题平衡响应质量、响应时间和推理成本，适应请求的异质性和节点的多样性。

Result: 实验结果表明，该算法在响应时间和成本上分别实现了95.2%和34.9%的改进。

Conclusion: 该论文提出的基于NSGA-II的路由算法在云边计算环境中有效解决了LLM推理服务的延迟和成本问题，实验结果表明其在响应时间和成本方面均有显著提升，验证了算法的可扩展性。

Abstract: The rising demand for Large Language Model (LLM) inference services has
intensified pressure on computational resources, resulting in latency and cost
challenges. This paper introduces a novel routing algorithm based on the
Non-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference
requests across heterogeneous LLM instances in a cloud-edge computing
environment. Formulated as a multi-objective optimization problem, the
algorithm balances response quality, response time, and inference cost,
adapting to request heterogeneity (e.g., varying complexity and prompt lengths)
and node diversity (e.g., edge vs. cloud resources). This adaptive routing
algorithm optimizes performance under dynamic workloads. We benchmark the
approach using a testbed with datasets including Stanford Question Answering
Dataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With
Adversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K).
Experimental results show our solution, compared to the baselines, achieves up
to 95.2% and 34.9% improvements in terms of response time and cost,
respectively. These findings validate the algorithm's effectiveness for
scalable LLM deployments.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [312] [Iran's Stealth Internet Blackout: A New Model of Censorship](https://arxiv.org/abs/2507.14183)
*Arash Aryapour*

Main category: cs.NI

TL;DR: 伊朗2025年中期实施的隐蔽性互联网关闭策略通过多层技术手段隔离国内用户，导致VPN需求激增707%，研究揭示了其技术细节及对数字权利的影响。


<details>
  <summary>Details</summary>
Motivation: 研究伊朗新型隐蔽性互联网关闭策略的技术细节及其对VPN需求和数字权利的影响。

Method: 通过主动网络测量，如DNS投毒、HTTP注入、TLS拦截和协议白名单分析，追踪到集中式边界网关。

Result: 量化了VPN需求约707%的增长，并详细描述了多层审查基础设施的运行机制。

Conclusion: 本文揭示了伊朗在2025年中期实施的隐蔽性互联网关闭策略，该策略通过多层审查基础设施对国内用户进行隔离，同时保持全球路由存在。研究强调了此类策略对规避技术和数字权利监测的深远影响。

Abstract: In mid-2025, Iran experienced a novel, stealthy Internet shutdown that
preserved global routing presence while isolating domestic users through deep
packet inspection, aggressive throttling, and selective protocol blocking. This
paper analyzes active network measurements such as DNS poisoning, HTTP
injection, TLS interception, and protocol whitelisting, traced to a centralized
border gateway. We quantify an approximate 707 percent rise in VPN demand and
describe the multi-layered censorship infrastructure, highlighting implications
for circumvention and digital rights monitoring.

</details>


### [313] [A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction](https://arxiv.org/abs/2507.14186)
*Xiaojie Li,Zhijie Cai,Nan Qi,Chao Dong,Guangxu Zhu,Haixia Ma,Qihui Wu,Shi Jin*

Main category: cs.NI

TL;DR: 提出了一种结合专家知识特征压缩和解耦表示学习的双重策略，有效解决了低空网络覆盖预测中的数据稀疏和泛化问题，实验和实际验证均显示显著效果。


<details>
  <summary>Details</summary>
Motivation: 低空网络覆盖（LANC）预测对设计空中走廊至关重要，但基站天线波束模式通常为专有信息，难以获取。基站的操作参数虽包含波束信息，但低空路测数据收集成本高，导致样本稀疏，面临特征采样不平衡和泛化能力不足的挑战。

Method: 引入了一种双重策略，包括基于专家知识的特征压缩和解耦表示学习。前者通过通信专业知识降低特征空间复杂度，后者通过集成传播模型和捕获并聚合潜在特征语义表示的不同子网络来增强模型的泛化能力。

Result: 框架在实验中表现优异，误差减少7%，实际网络验证中MAE误差达到5dB级别。

Conclusion: 实验评估证实了该框架的有效性，相比最佳基线算法误差减少了7%。实际网络验证进一步证明了其可靠性，达到了5dB级别的MAE误差，具备实际预测准确性。

Abstract: The expansion of the low-altitude economy has underscored the significance of
Low-Altitude Network Coverage (LANC) prediction for designing aerial corridors.
While accurate LANC forecasting hinges on the antenna beam patterns of Base
Stations (BSs), these patterns are typically proprietary and not readily
accessible. Operational parameters of BSs, which inherently contain beam
information, offer an opportunity for data-driven low-altitude coverage
prediction. However, collecting extensive low-altitude road test data is
cost-prohibitive, often yielding only sparse samples per BS. This scarcity
results in two primary challenges: imbalanced feature sampling due to limited
variability in high-dimensional operational parameters against the backdrop of
substantial changes in low-dimensional sampling locations, and diminished
generalizability stemming from insufficient data samples. To overcome these
obstacles, we introduce a dual strategy comprising expert knowledge-based
feature compression and disentangled representation learning. The former
reduces feature space complexity by leveraging communications expertise, while
the latter enhances model generalizability through the integration of
propagation models and distinct subnetworks that capture and aggregate the
semantic representations of latent features. Experimental evaluation confirms
the efficacy of our framework, yielding a 7% reduction in error compared to the
best baseline algorithm. Real-network validations further attest to its
reliability, achieving practical prediction accuracy with MAE errors at the 5dB
level.

</details>


### [314] [From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade Direct-to-Device Mobile Networks](https://arxiv.org/abs/2507.14188)
*Sebastian Barros Elgueta*

Main category: cs.NI

TL;DR: 本文提出了一个完全基于轨道的移动网络架构，通过仿真验证了其在高密度城市中的可行性，并提出了15年路线图，目标是为大城市提供50-100 Mbps的手持设备服务，完全不依赖地面基础设施。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的直接到设备（D2D）卫星通信系统已验证了物理可行性，但其仅作为回退级方案，功能有限且完全依赖地面移动核心。本文探讨了一个更具野心的目标：是否可以在轨道上运行完整的移动网络，包括无线接入、核心功能、流量路由和内容分发，并在全球最密集的城市中提供持续的、城市级的服务。

Method: 本文提出了一种完全基于轨道的移动网络系统架构，包括电子控制相控阵、1000波束容量、基于空间的5G核心功能部署（UPF、AMF）以及卫星间激光网状回程。通过分析频谱效率、波束容量和链路预算，评估了在高密度城市条件下的性能，考虑了路径损耗、多普勒效应和多径效应。

Result: 仿真结果表明，屋顶和视距用户可以维持64-QAM的吞吐量，而街道级接入在采用中继或辅助波束模式时是可行的。本文还指出了当前的工程瓶颈（如功率、热耗散、计算辐射硬化和监管模型），并证明这些并非物理极限。

Conclusion: 本文提出了一个完全基于轨道的移动网络系统架构，并通过仿真验证了其在高密度城市环境中的可行性。同时，提出了一个15年的分阶段路线图，逐步实现从当前的回退级D2D系统到自主轨道覆盖的过渡，最终目标是为大城市的手持设备提供50-100 Mbps的服务，完全不依赖地面基础设施。

Abstract: In 2023, satellite and mobile networks crossed a historic threshold: standard
smartphones, using unmodified 3GPP protocols, connected directly to low Earth
orbit (LEO) satellites. This first wave of direct-to-device (D2D)
demonstrations validated the physical feasibility of satellite-based mobile
access. However, these systems remain fallback-grade--rural-only,
bandwidth-limited, and fully dependent on Earth-based mobile cores for
identity, session, and policy control. This paper asks a more ambitious
question: Can a complete mobile network, including radio access, core
functions, traffic routing, and content delivery, operate entirely from orbit?
And can it deliver sustained, urban-grade service in the world's densest
cities? We present the first end-to-end system architecture for a fully orbital
telco, integrating electronically steered phased arrays with 1000-beam
capacity, space-based deployment of 5G core functions (UPF, AMF), and
inter-satellite laser mesh backhaul. We analyze spectral efficiency, beam
capacity, and link budgets under dense urban conditions, accounting for path
loss, Doppler, and multipath. Simulations show that rooftop and line-of-sight
users can sustain 64-QAM throughput, while street-level access is feasible with
relay or assisted beam modes. The paper outlines the remaining constraints,
power, thermal dissipation, compute radiation hardening, and regulatory models,
and demonstrates that these are engineering bottlenecks, not physical limits.
Finally, we propose a staged 15-year roadmap from today's fallback D2D systems
to autonomous orbital overlays delivering 50-100 Mbps to handhelds in
megacities, with zero reliance on terrestrial infrastructure.

</details>


### [315] [On Splitting Lightweight Semantic Image Segmentation for Wireless Communications](https://arxiv.org/abs/2507.14199)
*Ebrahim Abu-Helalah,Jordi Serra,Jordi Perez-Romero*

Main category: cs.NI

TL;DR: 论文提出了一种新颖的语义通信方法，通过在发射端和接收端分配语义图像分割任务，显著降低了带宽需求和计算负荷，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 语义通信在资源有限的环境和变化的信道条件下，需要在计算效率、带宽需求和分割精度之间取得平衡。

Method: 通过将语义图像分割过程在资源受限的发射端和接收端之间分配，减少传输数据量并降低发射端的计算需求。

Result: 与全语义图像分割相比，传输比特率降低了72%，发射端计算负荷减少了19%以上。

Conclusion: 该论文提出的方法在保持语义图像分割精度的同时，显著降低了传输比特率和发射端的计算负荷，特别适用于资源受限的环境和未来的6G通信系统。

Abstract: Semantic communication represents a promising technique towards reducing
communication costs, especially when dealing with image segmentation, but it
still lacks a balance between computational efficiency and bandwidth
requirements while maintaining high image segmentation accuracy, particularly
in resource-limited environments and changing channel conditions. On the other
hand, the more complex and larger semantic image segmentation models become,
the more stressed the devices are when processing data. This paper proposes a
novel approach to implementing semantic communication based on splitting the
semantic image segmentation process between a resource constrained transmitter
and the receiver. This allows saving bandwidth by reducing the transmitted data
while maintaining the accuracy of the semantic image segmentation.
Additionally, it reduces the computational requirements at the resource
constrained transmitter compared to doing all the semantic image segmentation
in the transmitter. The proposed approach is evaluated by means of
simulation-based experiments in terms of different metrics such as
computational resource usage, required bit rate and segmentation accuracy. The
results when comparing the proposal with the full semantic image segmentation
in the transmitter show that up to 72% of the bit rate was reduced in the
transmission process. In addition, the computational load of the transmitter is
reduced by more than 19%. This reflects the interest of this technique for its
application in communication systems, particularly in the upcoming 6G systems.

</details>


### [316] [A Fault-Tolerant Architecture for Urban and Rural Digital Connectivity: Synergizing SDWMN, Direct-to-Mobile Broadcasting, and Hybrid Cloud Streaming](https://arxiv.org/abs/2507.14205)
*Pavel Malinovskiy*

Main category: cs.NI

TL;DR: 该论文提出了一种结合SDWMN、D2M广播和Kafka的混合云流媒体架构，旨在解决城市拥堵和农村数字鸿沟问题。实验结果显示性能显著提升，并建议了频谱分配和补贴政策以促进采用。


<details>
  <summary>Details</summary>
Motivation: The approach addresses urban congestion and rural digital exclusion through traffic offloading, enhanced fault tolerance, and equitable resource allocation.

Method: The paper proposes an integrated architecture combining Software-Defined Wireless Mesh Networks (SDWMN), Direct-to-Mobile (D2M) broadcasting, and Kafka-based hybrid cloud streaming. It models urban congestion and rural coverage deficit, aiming to minimize global performance loss.

Result: Experiments demonstrate latency reduction over 32%, bandwidth offloading of 40%, rural coverage gain of 28%, and fairness index rising from 0.78 to 0.91. The system achieves recovery under 10 seconds using SDWMN and Kafka.

Conclusion: The paper recommends optimal spectrum allocation, targeted subsidies, and device mandates to promote adoption, supporting equitable digital transformation and suggesting future research directions.

Abstract: We propose an integrated architecture combining Software-Defined Wireless
Mesh Networks (SDWMN), Direct-to-Mobile (D2M) broadcasting, and Kafka-based
hybrid cloud streaming to improve wireless network performance in both urban
and rural settings. The approach addresses urban congestion and rural digital
exclusion through traffic offloading, enhanced fault tolerance, and equitable
resource allocation. We model urban congestion $\rho_u = \lambda_t / \mu_c$ and
rural coverage deficit $\delta_r = 1 - C_r / C_{req}$, and aim to minimize
global performance loss $GPL = w_1 \cdot \rho_u + w_2 \cdot \delta_r + w_3
\cdot T_{rec}$, where $T_{rec}$ is recovery time. Experiments in Bangkok,
Mumbai, and rural Finland demonstrate latency reduction over 32%, bandwidth
offloading of 40%, rural coverage gain of 28%, and fairness index rising from
0.78 to 0.91. The system achieves recovery under 10 s using SDWMN and Kafka. We
recommend optimal spectrum allocation $\alpha_s$, targeted subsidies, and
device mandates to promote adoption. This scalable, fault-tolerant design
supports equitable digital transformation and suggests directions for future
research.

</details>


### [317] [White paper: Towards Human-centric and Sustainable 6G Services -- the fortiss Research Perspective](https://arxiv.org/abs/2507.14209)
*Rute C. Sofia,Hao Shen,Yuanting Liu,Severin Kacianka,Holger Pfeifer*

Main category: cs.NI

TL;DR: fortiss提出以人为中心、可持续且AI集成的6G网络愿景，强调社会责任与技术创新的结合。


<details>
  <summary>Details</summary>
Motivation: 确保6G技术不仅实现技术进步，还符合社会需求，满足超可靠低延迟通信（URLLC）和个性化数字服务的期望。

Method: 通过软件定义、AI赋能和可持续的通信服务，结合认知智能、去中心化编排和面向可持续性的架构，fortiss在全球6G倡议中发挥关键作用。

Result: 明确了6G的关键研究领域，如语义通信、绿色编排和分布式AI，并提出了技术性能与社会相关性并重的解决方案。

Conclusion: fortiss提出了一个以人为中心、可持续且AI集成的6G网络愿景，强调负责任创新和跨学科合作，以实现2030年的有意义愿景。

Abstract: As a leading research institute in software-intensive systems, fortiss is
actively shaping the vision of Sixth Generation Mobile Communication (6G). Our
mission is to ensure that 6G technologies go beyond technical advancements and
are aligned with societal needs. fortiss plays a key role in 6G initiatives
worldwide, including contributions to standardization bodies and collaborative
Research and Development programs. We focus on software-defined, AI-enabled,
and sustainable communication services that prioritize human values and
long-term impact. 6G will redefine digital connectivity through cognitive
intelligence, decentralized orchestration, and sustainability-oriented
architectures. As expectations rise for ultra-reliable low-latency
communication (URLLC) and personalized digital services, 6G must outperform
prior generations. It will rely on AI-native networking, Edge-Cloud resource
orchestration, and energy-aware data frameworks, ensuring both technical
performance and societal relevance. This white paper presents the fortiss
vision for a human-centric, sustainable, and AI-integrated 6G network. It
outlines key research domains such as semantic communication, green
orchestration, and distributed AI, all linked to societal and technological
challenges. The white paper is aimed at researchers, industry experts,
policymakers, and developers. It articulates the strategic direction and
contributions of fortiss to 6G, emphasizing responsible innovation and
interdisciplinary collaboration toward a meaningful 2030 vision.

</details>


### [318] [PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence](https://arxiv.org/abs/2507.14211)
*Federico Mason,Tommaso Zugno,Matteo Drago,Marco Giordani,Mate Boban,Michele Zorzi*

Main category: cs.NI

TL;DR: PRATA是一个基于AI的预测性QoS框架，用于远程驾驶应用，通过RL优化数据分割，性能提升近两倍。


<details>
  <summary>Details</summary>
Motivation: 远程驾驶等汽车应用对延迟和可靠性有严格要求，预测性QoS（PQoS）能预判网络变化并采取应对措施，避免性能下降。

Method: 提出了PRATA框架，包含5G RAN模拟、汽车数据生成工具和AI单元，用于设计RL单元RAN-AI以优化数据分割级别。

Result: RAN-AI有效平衡了QoS与QoE的权衡，系统性能提升近两倍。同时，通过调整学习设置，探讨了状态空间和网络数据获取成本的影响。

Conclusion: PRATA框架通过AI优化预测性QoS决策，显著提升了远程驾驶应用的性能，几乎是基线方法的两倍。

Abstract: Predictive Quality of Service (PQoS) makes it possible to anticipate QoS
changes, e.g., in wireless networks, and trigger appropriate countermeasures to
avoid performance degradation. Hence, PQoS is extremely useful for automotive
applications such as teleoperated driving, which poses strict constraints in
terms of latency and reliability. A promising tool for PQoS is given by
Reinforcement Learning (RL), a methodology that enables the design of
decision-making strategies for stochastic optimization. In this manuscript, we
present PRATA, a new simulation framework to enable PRedictive QoS based on AI
for Teleoperated driving Applications. PRATA consists of a modular pipeline
that includes (i) an end-to-end protocol stack to simulate the 5G Radio Access
Network (RAN), (ii) a tool for generating automotive data, and (iii) an
Artificial Intelligence (AI) unit to optimize PQoS decisions. To prove its
utility, we use PRATA to design an RL unit, named RAN-AI, to optimize the
segmentation level of teleoperated driving data in the event of resource
saturation or channel degradation. Hence, we show that the RAN-AI entity
efficiently balances the trade-off between QoS and Quality of Experience (QoE)
that characterize teleoperated driving applications, almost doubling the system
performance compared to baseline approaches. In addition, by varying the
learning settings of the RAN-AI entity, we investigate the impact of the state
space and the relative cost of acquiring network data that are necessary for
the implementation of RL.

</details>


### [319] [Intent-Based Network for RAN Management with Large Language Models](https://arxiv.org/abs/2507.14230)
*Fransiscus Asisi Bimo,Maria Amparo Canaveras Galdon,Chun-Kai Lai,Ray-Guang Cheng,Edwin K. P. Chong*

Main category: cs.NI

TL;DR: 论文提出一种基于LLMs的RAN自动化管理方法，通过意图翻译和动态优化提升能源效率，展示实时反馈代理系统的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着无线网络管理复杂度的增加，高级智能自动化成为重要需求。论文旨在通过LLMs提升RAN管理的自动化水平，解决意图翻译和动态优化的挑战。

Method: 论文提出了一种基于意图的网络自动化方法，利用LLMs进行高级目标翻译、复杂网络状态推理，并通过结构化提示工程技术生成精确的RAN配置。采用闭环机制动态优化关键RAN参数。

Result: 研究表明，所提方法能够自动提升网络的能源效率，并通过实时反馈的LLM协调代理系统实现稳健的资源管理。

Conclusion: 该论文展示了通过结合大型语言模型（LLMs）和智能代理架构，能够有效提升无线接入网络（RANs）的自动化管理能力，特别是在意图翻译和动态参数优化方面。

Abstract: Advanced intelligent automation becomes an important feature to deal with the
increased complexity in managing wireless networks. This paper proposes a novel
automation approach of intent-based network for Radio Access Networks (RANs)
management by leveraging Large Language Models (LLMs). The proposed method
enhances intent translation, autonomously interpreting high-level objectives,
reasoning over complex network states, and generating precise configurations of
the RAN by integrating LLMs within an agentic architecture. We propose a
structured prompt engineering technique and demonstrate that the network can
automatically improve its energy efficiency by dynamically optimizing critical
RAN parameters through a closed-loop mechanism. It showcases the potential to
enable robust resource management in RAN by adapting strategies based on
real-time feedback via LLM-orchestrated agentic systems.

</details>


### [320] [Feasibility of Energy Neutral Wildlife Tracking using Multi-Source Energy Harvesting](https://arxiv.org/abs/2507.14234)
*Samer Nasser,Henrique Duarte Moura,Dragan Subotic,Ritesh Kumar Singh,Maarten Weyn,Jeroen Famaey*

Main category: cs.NI

TL;DR: 该论文提出了一种结合太阳能和动能采集的能量中立系统，用于野生动物追踪，显著提高了数据产量和可靠性，支持无维护、环保的远程监测。


<details>
  <summary>Details</summary>
Motivation: 长期野生动物追踪对生物多样性监测至关重要，但能源限制（尤其是动物标签的电池更换问题）带来了挑战。能量采集提供了可持续的替代方案，但现有系统多依赖单一能源和基础设施有限的通信技术。

Method: 提出了一种结合太阳能和动能采集的能量中立系统，利用NB-IoT与现有蜂窝基础设施兼容，并开发了模拟能量采集、存储和消耗的框架，以及基于实时能量可用性的任务调度器。

Result: 结果表明，该系统在维持能量中立操作的同时，显著提高了数据产量和可靠性，能够每两分钟采样一次GPS位置和动能数据，并通过NB-IoT每小时传输结果。

Conclusion: 该论文展示了一种结合太阳能和动能采集的能量中立系统，显著提高了数据产量和可靠性，实现了在偏远栖息地无维护、环保的野生动物追踪。

Abstract: Long-term wildlife tracking is crucial for biodiversity monitoring, but
energy limitations pose challenges, especially for animal tags, where replacing
batteries is impractical and stressful for the animal due to the need to
locate, possibly sedate, and handle it. Energy harvesting offers a sustainable
alternative, yet most existing systems rely on a single energy source and
infrastructure-limited communication technologies. This paper presents an
energy-neutral system that combines solar and kinetic energy harvesting to
enable the tracking and monitoring of wild animals. Harvesting from multiple
sources increases the total available energy. Uniquely, the kinetic harvester
also serves as a motion proxy by sampling harvested current, enabling activity
monitoring without dedicated sensors. Our approach also ensures compatibility
with existing cellular infrastructure, using Narrowband Internet of Things
(NB-IoT). We present a simulation framework that models energy harvesting,
storage, and consumption at the component level. An energy-aware scheduler
coordinates task execution based on real-time energy availability. We evaluate
performance under realistically varying conditions, comparing task frequencies
and capacitor sizes. Results show that our approach maintains energy-neutral
operation while significantly increasing data yield and reliability compared to
single-source systems, with the ability to consistently sample GPS location
data and kinetic harvesting data every two minutes while transmitting these
results over NB-IoT every hour. These findings demonstrate the potential for
maintenance-free, environmentally friendly tracking in remote habitats,
enabling more effective and scalable wildlife monitoring.

</details>


### [321] [Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts](https://arxiv.org/abs/2507.14263)
*Ramesh Raskar,Pradyumna Chari,John Zinky,Mahesh Lambe,Jared James Grogan,Sichao Wang,Rajesh Ranjan,Rekha Singhal,Shailja Gupta,Robert Lincourt,Raghu Bala,Aditi Joshi,Abhishek Singh,Ayush Chopra,Dimitris Stripelis,Bhuwan B,Sumit Kumar,Maria Gorskikh*

Main category: cs.NI

TL;DR: NANDA索引架构为AI代理互联网设计，支持快速发现、安全认证和隐私保护，兼容现有网络。


<details>
  <summary>Details</summary>
Motivation: 互联网即将承载数十亿至数万亿的自主AI代理，现有的以DNS为中心的身份和发现机制将难以应对。需要一种支持可发现性、可识别性和认证的新架构。

Method: 提出了NANDA索引架构，包括动态、可加密验证的AgentFacts，支持多端点路由、负载均衡、隐私保护访问和凭证化能力声明。设计了基于CRDT的更新协议和自适应解析器原型。

Result: 实现了五项具体保证：(1)支持NANDA原生及第三方代理的索引，(2)新生成AI代理的快速全局解析，(3)亚秒级撤销和密钥轮换，(4)模式验证的能力声明，(5)跨组织边界的隐私保护发现。

Conclusion: NANDA索引架构为AI代理互联网提供了一个轻量级、水平可扩展的基础，支持安全、信任感知的协作，同时兼容现有网络基础设施。

Abstract: The Internet is poised to host billions to trillions of autonomous AI agents
that negotiate, delegate, and migrate in milliseconds and workloads that will
strain DNS-centred identity and discovery. In this paper, we describe the NANDA
index architecture, which we envision as a means for discoverability,
identifiability and authentication in the internet of AI agents. We present an
architecture where a minimal lean index resolves to dynamic, cryptographically
verifiable AgentFacts that supports multi-endpoint routing, load balancing,
privacy-preserving access, and credentialed capability assertions. Our
architecture design delivers five concrete guarantees: (1) A quilt-like index
proposal that supports both NANDA-native agents as well as third party agents
being discoverable via the index, (2) rapid global resolution for newly spawned
AI agents, (3) sub-second revocation and key rotation, (4) schema-validated
capability assertions, and (5) privacy-preserving discovery across
organisational boundaries via verifiable, least-disclosure queries. We
formalize the AgentFacts schema, specify a CRDT-based update protocol, and
prototype adaptive resolvers. The result is a lightweight, horizontally
scalable foundation that unlocks secure, trust-aware collaboration for the next
generation of the Internet of AI agents, without abandoning existing web
infrastructure.

</details>


### [322] [NetIntent: Leveraging Large Language Models for End-to-End Intent-Based SDN Automation](https://arxiv.org/abs/2507.14398)
*Md. Kamrul Hossain,Walid Aljoby*

Main category: cs.NI

TL;DR: 论文提出IBNBench评估LLMs在IBN任务中的表现，并设计NetIntent框架实现全自动化IBN管理，验证了其在SDN控制器中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有IBN解决方案依赖刚性规则和固定API，缺乏扩展性和适应性。LLMs的自然语言理解和灵活推理能力为IBN自动化提供了新途径，但其实际应用潜力尚不明确。

Method: 通过IBNBench基准测试套件评估33个开源LLMs在意图翻译和冲突检测任务中的表现，并设计NetIntent框架，集成LLMs以实现端到端IBN自动化。

Result: IBNBench揭示了LLMs在IBN任务中的性能差异，NetIntent框架在ODL和ONOS控制器上实现了自适应端到端IBN管理。

Conclusion: 该论文提出了NetIntent框架，结合LLMs和非LLM代理，实现了从用户意图到设备配置的全自动化IBN生命周期管理，并在ODL和ONOS控制器上验证了其一致性和适应性。

Abstract: Intent-Based Networking (IBN) often leverages the programmability of
Software-Defined Networking (SDN) to simplify network management. However,
significant challenges remain in automating the entire pipeline, from
user-specified high-level intents to device-specific low-level configurations.
Existing solutions often rely on rigid, rule-based translators and fixed APIs,
limiting extensibility and adaptability. By contrast, recent advances in large
language models (LLMs) offer a promising pathway that leverages natural
language understanding and flexible reasoning. However, it is unclear to what
extent LLMs can perform IBN tasks. To address this, we introduce IBNBench, a
first-of-its-kind benchmarking suite comprising four novel datasets:
Intent2Flow-ODL, Intent2Flow-ONOS, FlowConflict-ODL, and FlowConflict-ONOS.
These datasets are specifically designed for evaluating LLMs performance in
intent translation and conflict detection tasks within the industry-grade SDN
controllers ODL and ONOS. Our results provide the first comprehensive
comparison of 33 open-source LLMs on IBNBench and related datasets, revealing a
wide range of performance outcomes. However, while these results demonstrate
the potential of LLMs for isolated IBN tasks, integrating LLMs into a fully
autonomous IBN pipeline remains unexplored. Thus, our second contribution is
NetIntent, a unified and adaptable framework that leverages LLMs to automate
the full IBN lifecycle, including translation, activation, and assurance within
SDN systems. NetIntent orchestrates both LLM and non-LLM agents, supporting
dynamic re-prompting and contextual feedback to robustly execute user-defined
intents with minimal human intervention. Our implementation of NetIntent across
both ODL and ONOS SDN controllers achieves a consistent and adaptive end-to-end
IBN realization.

</details>


### [323] [Dora: A Controller Provisioning Strategy in Hierarchical Domain-based Satellite Networks](https://arxiv.org/abs/2507.14512)
*Qiyuan Peng,Qi Zhang,Yue Gao,Kun Qiu*

Main category: cs.NI

TL;DR: 提出基于强化学习的Dora策略，显著提升卫星网络管理效率，计算时间大幅减少。


<details>
  <summary>Details</summary>
Motivation: 卫星星座的快速扩张给网络管理带来了巨大挑战，现有架构和算法因卫星计算资源有限和时间限制而无法提供高效解决方案。

Method: 提出了一种基于强化学习的控制器配置策略Dora，以及一个三层域架构，以增强可扩展性和适应性。

Result: Dora在控制器配置质量上显著优于现有基准，提升了10%，同时计算时间仅为传统算法的1/30到1/90。

Conclusion: 强化学习方法在下一代SAGIN部署中展现出高效卫星网络管理的潜力。

Abstract: The rapid proliferation of satellite constellations in Space-Air-Ground
Integrated Networks (SAGIN) presents significant challenges for network
management. Conventional flat network architectures struggle with
synchronization and data transmission across massive distributed nodes. In
response, hierarchical domain-based satellite network architectures have
emerged as a scalable solution, highlighting the critical importance of
controller provisioning strategies. However, existing network management
architectures and traditional search-based algorithms fail to generate
efficient controller provisioning solutions due to limited computational
resources in satellites and strict time constraints. To address these
challenges, we propose a three-layer domain-based architecture that enhances
both scalability and adaptability. Furthermore, we introduce Dora, a
reinforcement learning-based controller provisioning strategy designed to
optimize network performance while minimizing computational overhead. Our
comprehensive experimental evaluation demonstrates that Dora significantly
outperforms state-of-the-art benchmarks, achieving 10% improvement in
controller provisioning quality while requiring only 1/30 to 1/90 of the
computation time compared to traditional algorithms. These results underscore
the potential of reinforcement learning approaches for efficient satellite
network management in next-generation SAGIN deployments.

</details>


### [324] [UAV-Enabled Wireless-Powered Underground Communication Networks: A Novel Time Allocation Approach](https://arxiv.org/abs/2507.14627)
*Kaiqiang Lin,Yijie Mao,Onel Luis Alcaraz López,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 论文提出无人机使能的WPUCN系统，通过混合WET方法和优化时间分配，显著降低无人机能耗，实现可持续地下监测。


<details>
  <summary>Details</summary>
Motivation: 地下环境中的无线信号严重衰减及信道状态信息（CSI）获取成本高，使得大规模WPUCNs在实际应用中经济不可行。为解决这一问题，论文引入了灵活无人机，提出了无人机使能的WPUCN系统。

Method: 论文首先提出了无人机使能的WPUCN系统，并建立了其能耗模型。随后，提出了一种混合WET方法（UDs可从HAP和无人机同时获取能量），并基于全CSI和无CSI的多天线波束成形技术。最后，通过时间分配问题的优化，最小化无人机能耗。

Result: 模拟实验表明，混合WET方法在性能上优于其他WET方法，其性能增益受天线数量、通信距离、UDs数量及地下条件影响。优化时间分配后，基于无CSI多天线方案的混合WET方法实现了无人机的最低能耗。

Conclusion: 论文提出了一种基于无人机的无线供电地下通信网络（WPUCN）系统，通过混合无线能量传输（WET）方法和优化的时间分配策略，显著降低了无人机的能耗，同时满足所有地下设备（UDs）的吞吐量需求，实现了可持续的地下监测。

Abstract: Wireless-powered underground communication networks (WPUCNs), which allow
underground devices (UDs) to harvest energy from wireless signals for
battery-free communication, offer a promising solution for sustainable
underground monitoring. However, the severe wireless signal attenuation in
challenging underground environments and the costly acquisition of channel
state information (CSI) make large-scale WPUCNs economically infeasible in
practice. To address this challenge, we introduce flexible unmanned aerial
vehicles (UAVs) into WPUCNs, leading to UAV-enabled WPUCN systems. In this
system, a UAV is first charged by a terrestrial hybrid access point (HAP), then
flies to the monitoring area to wirelessly charge UDs. Afterwards, the UAV
collects data from the UDs and finally returns to the HAP for data offloading.
Based on the proposed UAV-enabled WPUCN system, we first propose its energy
consumption model and a hybrid wireless energy transfer (WET) approach (i.e.,
UDs can harvest energy from both the HAP and the UAV) relying on full-CSI and
CSI-free multi-antenna beamforming. Then, we formulate and address a time
allocation problem to minimize the energy consumption of UAV, while ensuring
that the throughput requirements of all UDs are met and all sensor data is
offloaded. Through simulations of a realistic farming scenario, we demonstrate
that the proposed hybrid WET approach outperforms other WET approaches, with
performance gains influenced by the number of antennas, communication distance,
number of UDs, and underground conditions. Additionally, under the optimized
time allocation, we found that the proposed hybrid WET approach based on a
CSI-free multi-antenna scheme achieves the lowest UAV's energy consumption
among all WET mechanisms, thereby enabling sustainable underground monitoring
in WPUCNs.

</details>


### [325] [Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks: A Survey on Generative Approaches](https://arxiv.org/abs/2507.14633)
*Xiaozheng Gao,Yichen Wang,Bosen Liu,Xiao Zhou,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Dong In Kim,Abbas Jamalipour,Chau Yuen,Jianping An,Kai Yang*

Main category: cs.NI

TL;DR: 综述探讨了通过生成AI和大型语言模型实现代理AI在卫星增强低空经济与地面网络中的应用，分析了模型机制并提出了未来方向。


<details>
  <summary>Details</summary>
Motivation: SLAETNs的发展需要能在异构、动态和任务关键环境中可靠运行的智能自主系统，因此本文聚焦于通过GAI和LLMs实现代理AI。

Method: 通过系统地回顾五类主要生成模型（VAEs、GANs、GDMs、TBMs和LLMs），提供了模型驱动的基础，并比较了它们在SLAETNs中的生成机制、能力和部署权衡。

Result: 分析了这些模型在通信增强、安全隐私保护和智能卫星任务三个领域的代理功能应用，并提出了未来构建可扩展、自适应和可信生成代理的关键方向。

Conclusion: 本综述旨在为下一代集成网络中推进代理AI提供统一理解和可操作的参考。

Abstract: The development of satellite-augmented low-altitude economy and terrestrial
networks (SLAETNs) demands intelligent and autonomous systems that can operate
reliably across heterogeneous, dynamic, and mission-critical environments. To
address these challenges, this survey focuses on enabling agentic artificial
intelligence (AI), that is, artificial agents capable of perceiving, reasoning,
and acting, through generative AI (GAI) and large language models (LLMs). We
begin by introducing the architecture and characteristics of SLAETNs, and
analyzing the challenges that arise in integrating satellite, aerial, and
terrestrial components. Then, we present a model-driven foundation by
systematically reviewing five major categories of generative models:
variational autoencoders (VAEs), generative adversarial networks (GANs),
generative diffusion models (GDMs), transformer-based models (TBMs), and LLMs.
Moreover, we provide a comparative analysis to highlight their generative
mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on
this foundation, we examine how these models empower agentic functions across
three domains: communication enhancement, security and privacy protection, and
intelligent satellite tasks. Finally, we outline key future directions for
building scalable, adaptive, and trustworthy generative agents in SLAETNs. This
survey aims to provide a unified understanding and actionable reference for
advancing agentic AI in next-generation integrated networks.

</details>


### [326] [Data-Plane Telemetry to Mitigate Long-Distance BGP Hijacks](https://arxiv.org/abs/2507.14842)
*Satadal Sengupta,Hyojoon Kim,Daniel Jubas,Maria Apostolaki,Jennifer Rexford*

Main category: cs.NI

TL;DR: 本文提出利用路由劫持导致的延迟变化进行检测，设计HiDe系统，实验证明其在真实环境中高效可靠。


<details>
  <summary>Details</summary>
Motivation: 互联网路由安全性不足导致用户数据可能被劫持至国外基础设施，引发隐私和国家安全问题，现有检测方法主要关注控制层面，数据层面信号被忽视。

Method: 通过分析路由劫持导致的传播延迟变化，设计并实现了HiDe系统，该系统能够在高速网络中实时检测延迟激增。

Result: 实验表明，86%的受害国-攻击国组合在实际部署中，攻击期间的延迟较攻击前增加至少25%，HiDe系统在真实数据上表现出高准确率和低误报率。

Conclusion: 基于延迟变化的劫持检测方法（如HiDe）在实际部署中表现出色，能够有效识别长距离劫持，为网络安全提供了新的解决方案。

Abstract: Poor security of Internet routing enables adversaries to divert user data
through unintended infrastructures (hijack). Of particular concern -- and the
focus of this paper -- are cases where attackers reroute domestic traffic
through foreign countries, exposing it to surveillance, bypassing legal privacy
protections, and posing national security threats. Efforts to detect and
mitigate such attacks have focused primarily on the control plane while
data-plane signals remain largely overlooked. In particular, change in
propagation delay caused by rerouting offers a promising signal: the change is
unavoidable and the increased propagation delay is directly observable from the
affected networks. In this paper, we explore the practicality of using delay
variations for hijack detection, addressing two key questions: (1) What
coverage can this provide, given its heavy dependence on the geolocations of
the sender, receiver, and adversary? and (2) Can an always-on latency-based
detection system be deployed without disrupting normal network operations? We
observe that for 86% of victim-attacker country pairs in the world, mid-attack
delays exceed pre-attack delays by at least 25% in real deployments, making
delay-based hijack detection promising. To demonstrate practicality, we design
HiDe, which reliably detects delay surges from long-distance hijacks at line
rate. We measure HiDe's accuracy and false-positive rate on real-world data and
validate it with ethically conducted hijacks.

</details>


### [327] [Tidal-Like Concept Drift in RIS-Covered Buildings: When Programmable Wireless Environments Meet Human Behaviors](https://arxiv.org/abs/2507.14876)
*Zi-Yang Wu,Muhammad Ismail,Jiliang Zhang,Jie Zhang*

Main category: cs.NI

TL;DR: 本文提出将RIS嵌入建筑结构以增强无线性能，但人类行为的复杂性使得通用信道模型不可行，并探讨了深度学习策略的挑战及可能的解决方案。


<details>
  <summary>Details</summary>
Motivation: 室内移动网络处理大部分数据流量，但其性能受限于建筑材料和结构。受RIS在室外网络成功的启发，本文提出将RIS嵌入建筑结构中，以全面操控和增强建筑无线性能。

Method: 文章系统性地研究了由复杂人类行为驱动的RIS覆盖建筑中出现的信道潮汐演化现象，并分析了高级深度学习预测和控制策略面临的挑战。

Result: 研究表明，人类行为的复杂动态使得高级深度学习策略面临高阶马尔可夫依赖、概念漂移和由人类干扰引起的泛化问题。

Conclusion: 本文强调，由于人类行为的复杂性，建立一个通用的室内无线信道模型是不现实的。文章提出了协调RIS覆盖建筑与人群移动共存的可能解决方案。

Abstract: Indoor mobile networks handle the majority of data traffic, with their
performance limited by building materials and structures. However, building
designs have historically not prioritized wireless performance. Prior to the
advent of reconfigurable intelligent surfaces (RIS), the industry passively
adapted to wireless propagation challenges within buildings. Inspired by RIS's
successes in outdoor networks, we propose embedding RIS into building
structures to manipulate and enhance building wireless performance
comprehensively. Nonetheless, the ubiquitous mobility of users introduces
complex dynamics to the channels of RIS-covered buildings. A deep understanding
of indoor human behavior patterns is essential for achieving wireless-friendly
building design. This article is the first to systematically examine the tidal
evolution phenomena emerging in the channels of RIS-covered buildings driven by
complex human behaviors. We demonstrate that a universal channel model is
unattainable and focus on analyzing the challenges faced by advanced deep
learning-based prediction and control strategies, including high-order Markov
dependencies, concept drift, and generalization issues caused by human-induced
disturbances. Possible solutions for orchestrating the coexistence of
RIS-covered buildings and crowd mobility are also laid out.

</details>


### [328] [FENIX: Enabling In-Network DNN Inference with FPGA-Enhanced Programmable Switches](https://arxiv.org/abs/2507.14891)
*Xiangyu Gao,Tong Li,Yinchao Zhang,Ziqiang Wang,Xiangsheng Zeng,Su Yao,Ke Xu*

Main category: cs.NI

TL;DR: FENIX 是一种混合型网络内 ML 系统，通过 ASIC 和 FPGA 协作，解决了现有方案在延迟、吞吐量和精度上的不足，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有网络数据平面机器学习解决方案（如 FlowLens、N3IC 和 BoS）难以同时实现低延迟、高吞吐量和高精度。

Method: FENIX 采用混合架构，在可编程交换机 ASIC 上进行特征提取，在 FPGA 上进行深度神经网络推理，并通过数据引擎（概率令牌桶算法）和模型引擎优化性能。

Result: 在真实网络流量数据集上的实验表明，FENIX 实现了微秒级推理延迟、多太比特吞吐量，硬件开销低，并在主流网络流量分类任务中达到超过 95% 的准确率，优于现有技术。

Conclusion: FENIX 是一种混合型网络内机器学习系统，通过结合可编程交换机 ASIC 和 FPGA，实现了低延迟、高吞吐量和高精度的网络流量分析，显著优于现有解决方案。

Abstract: Machine learning (ML) is increasingly used in network data planes for
advanced traffic analysis. However, existing solutions (such as FlowLens, N3IC,
and BoS) still struggle to simultaneously achieve low latency, high throughput,
and high accuracy. To address these challenges, we present FENIX, a hybrid
in-network ML system that performs feature extraction on programmable switch
ASICs and deep neural network inference on FPGAs. FENIX introduces a Data
Engine that leverages a probabilistic token bucket algorithm to control the
sending rate of feature streams, effectively addressing the throughput gap
between programmable switch ASICs and FPGAs. In addition, FENIX designs a Model
Engine to enable high-accuracy deep neural network inference in the network,
overcoming the difficulty of deploying complex models on resource-constrained
switch chips. We implement FENIX on a programmable switch platform that
integrates a Tofino ASIC and a ZU19EG FPGA directly and evaluate it on
real-world network traffic datasets. Our results show that FENIX achieves
microsecond-level inference latency and multi-terabit throughput with low
hardware overhead, and delivers over 95\% accuracy on mainstream network
traffic classification tasks, outperforming SOTA.

</details>


### [329] [Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness](https://arxiv.org/abs/2507.15145)
*Thai T. Vu,John Le*

Main category: cs.NI

TL;DR: 提出一种通信高效、事件触发的分布式边缘AI推理框架，结合公平约束，显著提升性能和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决多用户设备和边缘服务器在合作边缘AI系统中通信效率低和资源分配不公平的问题。

Method: 基于双阈值早期退出策略，扩展了单设备推理至分布式多设备设置，并结合比例公平约束。通过交替优化和Benders分解解决联合优化问题。

Result: 实验结果表明，与单设备基线相比，该框架显著提升了系统性能和公平性。

Conclusion: 该论文提出的通信高效、事件触发的推理框架显著提升了边缘AI系统的整体性能和资源分配的公平性。

Abstract: This paper proposes a communication-efficient, event-triggered inference
framework for cooperative edge AI systems comprising multiple user devices and
edge servers. Building upon dual-threshold early-exit strategies for rare-event
detection, the proposed approach extends classical single-device inference to a
distributed, multi-device setting while incorporating proportional fairness
constraints across users. A joint optimization framework is formulated to
maximize classification utility under communication, energy, and fairness
constraints. To solve the resulting problem efficiently, we exploit the
monotonicity of the utility function with respect to the confidence thresholds
and apply alternating optimization with Benders decomposition. Experimental
results show that the proposed framework significantly enhances system-wide
performance and fairness in resource allocation compared to single-device
baselines.

</details>


### [330] [User Head Movement-Predictive XR in Immersive H2M Collaborations over Future Enterprise Networks](https://arxiv.org/abs/2507.15254)
*Sourav Mondal,Elaine Wong*

Main category: cs.NI

TL;DR: 论文提出一种基于头部运动预测的人机协作动态带宽分配方案（HMC-DBA），显著降低XR内容传输的带宽需求，同时满足低延迟和抖动要求。


<details>
  <summary>Details</summary>
Motivation: 未来移动系统和固定无线网络需支持高带宽、低延迟服务，尤其在工业互联网、扩展现实（XR）和人机协作（H2M）等场景中，实时同步XR内容与人类头部运动是一大挑战。

Method: 采用双向长短期记忆网络高精度预测人类头部运动，提前调整机器摄像头方向，并基于头部运动预测XR帧大小及带宽需求，提出人机协调的动态带宽分配（HMC-DBA）方案。

Result: 通过广泛仿真验证，HMC-DBA方案在满足XR帧传输要求的同时，显著降低了企业网络（如FTTR-Business）的带宽消耗，并提高了网络资源利用率。

Conclusion: 论文提出的HMC-DBA方案在满足XR帧的端到端延迟和抖动要求的同时，显著降低了带宽消耗，并在网络资源利用率上优于现有方案。

Abstract: The evolution towards future generation of mobile systems and fixed wireless
networks is primarily driven by the urgency to support high-bandwidth and
low-latency services across various vertical sectors. This endeavor is fueled
by smartphones as well as technologies like industrial internet of things,
extended reality (XR), and human-to-machine (H2M) collaborations for fostering
industrial and social revolutions like Industry 4.0/5.0 and Society 5.0. To
ensure an ideal immersive experience and avoid cyber-sickness for users in all
the aforementioned usage scenarios, it is typically challenging to synchronize
XR content from a remote machine to a human collaborator according to their
head movements across a large geographic span in real-time over communication
networks. Thus, we propose a novel H2M collaboration scheme where the human's
head movements are predicted ahead with highly accurate models like
bidirectional long short-term memory networks to orient the machine's camera in
advance. We validate that XR frame size varies in accordance with the human's
head movements and predict the corresponding bandwidth requirements from the
machine's camera to propose a human-machine coordinated dynamic bandwidth
allocation (HMC-DBA) scheme. Through extensive simulations, we show that
end-to-end latency and jitter requirements of XR frames are satisfied with much
lower bandwidth consumption over enterprise networks like
Fiber-To-The-Room-Business. Furthermore, we show that better efficiency in
network resource utilization is achieved by employing our proposed HMC-DBA over
state-of-the-art schemes.

</details>


### [331] [Low-Power and Accurate IoT Monitoring Under Radio Resource Constraint](https://arxiv.org/abs/2507.15338)
*Takaho Shimokasa,Hiroyuki Yomo,Federico Chiariotti,Junya Shiraishi,Petar Popovski*

Main category: cs.NI

TL;DR: 论文提出两种策略优化无线传感器网络的能耗和状态估计准确性，分散策略在低相关性时表现更优，并确定了策略优劣的临界相关性程度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决无线传感器网络在无线电资源约束下，如何实现传感器节点的低功耗运行和基于卡尔曼滤波的准确状态估计。

Method: 研究引入了唤醒接收器和唤醒信号机制，设计了两种观测收集策略：基于观测统计的无感知策略和基于传感器瞬时观测自主决策的分散策略。

Result: 数值结果表明，在传感器节点观测过程相关性较低时，分散策略在无线电资源和能耗约束下比无感知策略显著提高了估计准确性。

Conclusion: 论文明确了两种策略在传感器节点观测过程相关性不同时的优劣变化点，并提出了一种基于随机访问的分散策略，在低相关性情况下显著提高了状态估计的准确性。

Abstract: This paper investigates how to achieve both low-power operations of sensor
nodes and accurate state estimation using Kalman filter for internet of things
(IoT) monitoring employing wireless sensor networks under radio resource
constraint. We consider two policies used by the base station to collect
observations from the sensor nodes: (i) an oblivious policy, based on
statistics of the observations, and (ii) a decentralized policy, based on
autonomous decision of each sensor based on its instantaneous observation. This
work introduces a wake-up receiver and wake-up signaling to both policies to
improve the energy efficiency of the sensor nodes. The decentralized policy
designed with random access prioritizes transmissions of instantaneous
observations that are highly likely to contribute to the improvement of state
estimation. Our numerical results show that the decentralized policy improves
the accuracy of the estimation in comparison to the oblivious policy under the
constraint on the radio resource and consumed energy when the correlation
between the processes observed by the sensor nodes is low. We also clarify the
degree of correlation in which the superiority of two policies changes.

</details>


### [332] [Enhancements to P4TG: Histogram-Based RTT Monitoring in the Data Plane](https://arxiv.org/abs/2507.15382)
*Fabian Ihle,Etienne Zink,Michael Menth*

Main category: cs.NI

TL;DR: P4TG通过直方图方法实现高精度RTT测量，解决了传统采样方法的准确性不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有P4TG在数据平面采样时间指标（如RTT）并在控制器收集，导致测量准确性降低。本文旨在通过直方图方法实现高精度的线速分析。

Method: 采用直方图方法进行RTT测量，实现了范围到前缀转换算法以支持硬件中的高效匹配。

Result: 实验评估表明，基于直方图的RTT分析方法能够准确匹配理论分布的RTT值，验证了该方法的有效性。

Conclusion: 本文提出了一种基于直方图的RTT测量方法，显著提高了P4TG在数据平面中的测量准确性，无需采样即可实现线速分析。通过范围到前缀转换算法的实现，有效解决了TCAM中的范围匹配问题。

Abstract: Modern traffic generators are essential tools for evaluating the performance
of network environments. P4TG is a P4-based traffic generator implemented for
Intel Tofino switches that offers high-speed packet generation with
fine-grained measurement capabilities. However, P4TG samples time-based metrics
such as the round-trip time (RTT) in the data plane and collects them at the
controller. This leads to a reduced accuracy. In this paper, we introduce a
histogram-based RTT measurement feature for P4TG. It enables accurate analysis
at line rate without sampling. Generally, histogram bins are modeled as ranges,
and values are matched to a bin. Efficient packet matching in hardware is
typically achieved using ternary content addressable memory (TCAM). However,
representing range matching rules in TCAM poses a challenge. Therefore, we
implemented a range-to-prefix conversion algorithm that models range matching
with multiple ternary entries. This paper describes the data plane
implementation and runtime configuration of RTT histograms in P4TG. Further, we
discuss the efficiency of the ternary decomposition. Our evaluation
demonstrates the applicability of the histogram-based RTT analysis by comparing
the measured values with a configured theoretical distribution of RTTs.

</details>


### [333] [Stack Management for MPLS Network Actions: Integration of Nodes with Limited Hardware Capabilities](https://arxiv.org/abs/2507.15391)
*Fabian Ihle,Michael Menth*

Main category: cs.NI

TL;DR: MNA框架通过MPLS堆栈重构和堆栈管理动作降低RLD要求，P4实现验证了可行性。


<details>
  <summary>Details</summary>
Motivation: MNA框架要求路由器具备较大的RLD，这在硬件实现上存在挑战，需找到降低RLD的方法。

Method: 硬件分析MNA实现，提出MPLS堆栈重构机制，并引入堆栈管理网络动作，使用P4实现验证。

Result: 提出的机制有效降低了MNA节点的RLD要求，并讨论了其对ECMP和数据包开销的影响。

Conclusion: 通过重新构建MPLS堆栈和引入堆栈管理网络动作，MNA节点的RLD要求显著降低，同时在可编程硬件上验证了其可行性。

Abstract: The MPLS Network Actions (MNA) framework enhances MPLS forwarding with a
generalized encoding for manifold extensions such as network slicing and
in-situ OAM (IOAM). Network actions in MNA are encoded in Label Stack Entries
(LSEs) and are added to the MPLS stack. Routers have a physical limit on the
number of LSEs they can read, called the readable label depth (RLD). With MNA,
routers must be able to process a minimum number of LSEs which requires a
relatively large RLD. In this paper, we perform a hardware analysis of an MNA
implementation and identify the reason for a large RLD requirement in the MNA
protocol design. Based on this, we present a mechanism that reduces the
required RLD for MNA nodes by restructuring the MPLS stack during forwarding.
We then introduce the novel stack management network action that enables the
proposed mechanism as well as its integration in networks with MNA-incapable
nodes. The feasibility of the mechanism on programmable hardware is verified by
providing a P4-based implementation. Further, the effects on the required RLD,
ECMP, and packet overhead are discussed.

</details>


### [334] [Assessing the Benefits of Ground Vehicles as Moving Urban Base Stations](https://arxiv.org/abs/2507.15423)
*Laura Finarelli,Falko Dressler,Marco Ajmone Marsan,Gianluca Rizzo*

Main category: cs.NI

TL;DR: 本文通过随机几何框架评估移动网络（MN）在6G中的优势，提出优化配置方法，结果表明MN能减少基础设施部署并保证QoS。


<details>
  <summary>Details</summary>
Motivation: 在6G用户为中心的网络演进中，MN范式可以发挥重要作用。然而，目前尚不清楚在何种条件下MN的优势能超过额外的资源成本。本文旨在通过量化分析解决这一问题。

Method: 本文提出了一种随机几何框架，用于表征MN范式在异构网络（HetNet）中的潜在优势。通过优化问题确定资源最优的网络配置和基站调度，最小化部署的基站数量，并提出了一种高效的随机启发式算法来解决问题。

Result: 数值评估表明，MN范式在动态网络管理策略的支持下，能够显著减少网络基础设施的部署，同时满足用户的QoS需求。

Conclusion: 移动网络（MN）范式与适当的动态网络管理策略相结合，可以显著减少部署的网络基础设施数量，同时保证用户感知的目标服务质量（QoS）。

Abstract: In the evolution towards 6G user-centric networking, the moving network (MN)
paradigm can play an important role. In a MN, some small cell base stations
(BS) are installed on top of vehicles, and enable a more dynamic, flexible and
sustainable, network operation. By "following" the users movements and adapting
dynamically to their requests, the MN paradigm enables a more efficient
utilization of network resources, mitigating the need for dense small cell BS
deployments at the cost of an increase in resource utilization due to wireless
backhauling. This aspect is at least partly compensated by the shorter distance
between users and BS, which allows for lower power and Line-of-Sight
communications. While the MN paradigm has been investigated for some time, to
date, it is still unclear in which conditions the advantages of MN outweigh the
additional resource costs. In this paper, we propose a stochastic geometry
framework for the characterization of the potential benefits of the MN paradigm
as part of an HetNet in urban settings. Our approach allows the estimation of
user-perceived performance, accounting for wireless backhaul connectivity as
well as base station resource scheduling. We formulate an optimization problem
for determining the resource-optimal network configurations and BS scheduling
which minimize the overall amount of deployed BSs in a QoS-aware manner, and
the minimum vehicular flow between different urban districts required to
support them, and we propose an efficient stochastic heuristic to solve it. Our
numerical assessment suggests that the MN paradigm, coupled with appropriate
dynamic network management strategies, significantly reduces the amount of
deployed network infrastructure while guaranteeing the target QoS perceived by
users.

</details>


### [335] [SENSOR: A Cost-Efficient Open-Source Flow Monitoring Platform](https://arxiv.org/abs/2507.15659)
*Gabriel Paradzik,Benjamin Steinert,Heinrich Abele,Michael Menth*

Main category: cs.NI

TL;DR: 论文介绍了一个基于开源工具的分布式流量监控平台，用于高效收集IPFIX数据，已在图宾根大学应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统流量监控方法的高成本和采样数据不完整的问题，作者开发了这一平台。

Method: 论文详细介绍了所有使用的开源工具及其使用方法。

Result: 该平台在图宾根大学成功实施，证明了其可行性和成本效益。

Conclusion: 该论文提出了一种基于开源工具的成本效益高且分布式的流量监控平台，用于收集未采样的IPFIX数据，并在图宾根大学成功实施。

Abstract: This paper presents a cost-effective and distributed flow monitoring platform
for collecting unsampled IPFIX data exclusively using open-source tools, which
is implemented at the University of T\"ubingen. An overview of all tools is
given and their use is explained.

</details>


### [336] [Vehicular Cloud Computing: A cost-effective alternative to Edge Computing in 5G networks](https://arxiv.org/abs/2507.15670)
*Rosario Patanè,Nadjib Achir,Andrea Araldo,Lila Boukhatem*

Main category: cs.NI

TL;DR: VCC可利用闲置车辆资源替代EC支持低延迟应用，极端情况下仍需EC。


<details>
  <summary>Details</summary>
Motivation: 探索VCC是否可以在不依赖EC的情况下有效支持低延迟应用，为网络运营商提供潜在的成本节约方案。

Method: 通过广泛的模拟评估关键场景因素，如负载、车辆移动性和密度以及可用性，使用SUMO模拟车辆移动性，使用NS3 5G-LENA模拟通信。

Result: 研究发现，VCC可以替代EC用于低延迟应用，但在极端低延迟情况下仍需EC。

Conclusion: VCC可以有效地替代EC用于低延迟应用，但在极端情况下（延迟<16毫秒）仍需要EC。

Abstract: Edge Computing (EC) is a computational paradigm that involves deploying
resources such as CPUs and GPUs near end-users, enabling low-latency
applications like augmented reality and real-time gaming. However, deploying
and maintaining a vast network of EC nodes is costly, which can explain its
limited deployment today. A new paradigm called Vehicular Cloud Computing (VCC)
has emerged and inspired interest among researchers and industry. VCC
opportunistically utilizes existing and idle vehicular computational resources
for external task offloading. This work is the first to systematically address
the following question: Can VCC replace EC for low-latency applications?
Answering this question is highly relevant for Network Operators (NOs), as VCC
could eliminate costs associated with EC given that it requires no
infrastructural investment. Despite its potential, no systematic study has yet
explored the conditions under which VCC can effectively support low-latency
applications without relying on EC. This work aims to fill that gap. Extensive
simulations allow for assessing the crucial scenario factors that determine
when this EC-to-VCC substitution is feasible. Considered factors are load,
vehicles mobility and density, and availability. Potential for substitution is
assessed based on multiple criteria, such as latency, task completion success,
and cost. Vehicle mobility is simulated in SUMO, and communication in NS3
5G-LENA. The findings show that VCC can effectively replace EC for low-latency
applications, except in extreme cases when the EC is still required (latency <
16 ms).

</details>
