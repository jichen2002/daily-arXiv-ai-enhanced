<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 141]
- [cs.SE](#cs.SE) [Total: 37]
- [cs.RO](#cs.RO) [Total: 43]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AI](#cs.AI) [Total: 58]
- [cs.NI](#cs.NI) [Total: 25]
- [cs.DS](#cs.DS) [Total: 15]
- [cs.GR](#cs.GR) [Total: 7]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: 论文比较了多种算法策略在3D图像数据镶嵌模型拟合中的表现，评估了优化方法的效果，并提供了选择方法的指导。


<details>
  <summary>Details</summary>
Motivation: 在3D图像数据（如多晶和泡沫材料）的镶嵌模型拟合领域，比较和评估不同的算法策略，以推动该领域的进步。

Method: 论文回顾并评估了基于优化的方法，包括线性和非线性规划、通过交叉熵方法的随机优化以及梯度下降，用于生成Voronoi、Laguerre和广义平衡功率图（GBPDs）来近似基于体素的晶粒结构。

Result: 研究结果在真实数据集上评估了拟合质量，使用差异度量量化了晶粒体积、表面积和拓扑结构的差异。

Conclusion: 该论文为基于数据特性和应用需求选择合适的算法策略提供了指导，强调了模型复杂性、优化程序复杂性和近似质量之间的权衡。

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [2] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: GEMINUS是一个混合专家端到端自动驾驶框架，通过全局和场景自适应专家组的结合，显著提升了多样化场景下的驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 现有的单模式规划方法难以学习多样化的驾驶技能以应对复杂多变的交通环境。

Method: 提出了GEMINUS框架，包含一个全局专家、一个场景自适应专家组和一个双感知路由器。全局专家在整体数据集上训练，场景自适应专家在对应场景子集上训练，双感知路由器动态激活专家模块。

Result: GEMINUS在Bench2Drive闭环基准测试中表现优于现有方法，在驾驶分数和成功率上达到最新水平，且仅需单目视觉输入。消融研究显示，相比单专家基线，驾驶分数提升7.67%，成功率提升22.06%，多能力均值提升19.41%。

Conclusion: GEMINUS框架通过结合全局专家和场景自适应专家组，以及双感知路由器，实现了在多样化交通环境中的自适应和鲁棒性能，显著提升了自动驾驶的表现。

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [3] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: 本文提出几种高效模型研究语义分割，使用BDD100k数据集和多种骨干网络。结果表明，选择合适的骨干网络显著提升性能，改进场景理解。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）和深度学习（DL）在解决复杂任务中表现出色，尤其在自动驾驶领域，高效的场景理解至关重要。

Method: 使用BDD100k数据集，提出几种高效模型研究语义分割的场景理解，并采用多种骨干网络作为编码器。

Result: 实验结果表明，模型在准确率、平均IoU和损失函数等指标上均有提升。

Conclusion: 选择合适的骨干网络对语义分割模型的性能有显著影响，改进的语义分割性能有助于更好地理解场景和环境。

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [4] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: 论文提出了一种利用事件相机数据进行运动分割和自运动估计的优化框架，通过稀疏事件和几何约束实现高效准确的结果。


<details>
  <summary>Details</summary>
Motivation: 针对传统方法在基于事件相机的运动分割和自运动估计中依赖密集光流或显式深度估计的局限性，提出了一种利用稀疏高时间分辨率事件数据的新框架。

Method: 论文提出了一种基于优化的流程，包括事件过分割、通过残差分析分离独立移动对象，以及利用运动相似性和时间一致性进行层次聚类来细化分割。

Result: 在EVIMO2v2数据集上的实验结果表明，该方法无需完整光流计算即可实现准确的分割和平移运动估计。

Conclusion: 该论文提出的方法在物体边界处表现出显著优势，并展示了在可扩展、实时机器人和导航应用中的巨大潜力。

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [5] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: CLIPTTA通过软对比损失改进VLMs的测试时适应，避免崩溃并在多样分布偏移中稳定超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法（如熵最小化）与VLMs的对比图像-文本训练目标不匹配，导致适应性能受限及伪标签漂移等问题。

Method: 提出CLIPTTA，一种基于梯度的TTA方法，采用与CLIP预训练目标一致的软对比损失，并通过理论分析其梯度设计以避免崩溃。

Result: CLIPTTA在开放集设置中通过OCE损失改进OOD检测，并在多样分布偏移下表现优于现有方法。

Conclusion: CLIPTTA在75个数据集上的评估中表现优于基于熵的目标，并在多种分布偏移下展现出更稳定的性能。

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [6] [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](https://arxiv.org/abs/2507.14809)
*Zesen Zhong,Duomin Zhang,Yijia Li*

Main category: cs.CV

TL;DR: 论文提出一种轻量级多模态未来帧预测方法，改进InstructPix2Pix模型，显著降低计算成本，在机器人动作预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 预测未来运动轨迹在机器人、自主系统等领域至关重要，但传统视频预测模型计算成本高且推理延迟大。论文旨在提出一种更高效、轻量级的解决方案。

Method: 论文提出了一种基于深度学习的视觉预测框架，通过改进和微调InstructPix2Pix模型，使其能够接受视觉和文本输入，实现多模态未来帧预测。

Result: 在RoboTWin数据集上的实验表明，该方法在SSIM和PSNR指标上优于现有基线，且仅需单张图像和文本提示即可实现快速推理。

Conclusion: 该论文提出的轻量级方法在机器人动作预测任务中表现出色，显著降低了计算成本和推理延迟，同时保持了较高的预测精度。

Abstract: Predicting future motion trajectories is a critical capability across domains
such as robotics, autonomous systems, and human activity forecasting, enabling
safer and more intelligent decision-making. This paper proposes a novel,
efficient, and lightweight approach for robot action prediction, offering
significantly reduced computational cost and inference latency compared to
conventional video prediction models. Importantly, it pioneers the adaptation
of the InstructPix2Pix model for forecasting future visual frames in robotic
tasks, extending its utility beyond static image editing. We implement a deep
learning-based visual prediction framework that forecasts what a robot will
observe 100 frames (10 seconds) into the future, given a current image and a
textual instruction. We repurpose and fine-tune the InstructPix2Pix model to
accept both visual and textual inputs, enabling multimodal future frame
prediction. Experiments on the RoboTWin dataset (generated based on real-world
scenarios) demonstrate that our method achieves superior SSIM and PSNR compared
to state-of-the-art baselines in robot action prediction tasks. Unlike
conventional video prediction models that require multiple input frames, heavy
computation, and slow inference latency, our approach only needs a single image
and a text prompt as input. This lightweight design enables faster inference,
reduced GPU demands, and flexible multimodal control, particularly valuable for
applications like robotics and sports motion trajectory analytics, where motion
trajectory precision is prioritized over visual fidelity.

</details>


### [7] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: AF机制通过剪除非信息性标记，显著提升GCD性能，集成后性能提升15.4%。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法在处理未标记数据时，模型注意力容易分散到与任务无关的背景区域，导致特征提取不理想。

Method: AF由两个组件组成：标记重要性度量（TIME）和多尺度标记自适应剪枝（TAP），通过多尺度重要性评分剪除非信息性标记。

Result: AF被集成到SimGCD方法中，性能提升高达15.4%，且计算开销极小。

Conclusion: 论文提出了注意力聚焦（AF）机制，通过剪除非信息性标记来优化特征提取，显著提升了广义类别发现（GCD）的性能。

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [8] [EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring](https://arxiv.org/abs/2507.15036)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: EBA-AI是一个基于伦理的偏置感知AI框架，通过CLIP嵌入和自适应处理优化水下图像增强，平衡效率、公平性和可解释性，适用于海洋保护。


<details>
  <summary>Details</summary>
Motivation: 水下图像增强对海洋保护至关重要，但现有AI模型存在数据集偏置、高计算成本和缺乏透明度等问题，可能导致误解。EBA-AI旨在解决这些挑战。

Method: EBA-AI利用CLIP嵌入检测和缓解数据集偏置，并集成自适应处理以优化能源效率，同时通过不确定性估计和可解释性技术增强AI驱动的环境决策的信任度。

Result: 在LSUI400、Oceanex和UIEB100数据集上的实验表明，EBA-AI在PSNR仅下降1.0 dB的情况下，显著降低了计算成本，实现了大规模海洋监测的实时可行性。

Conclusion: EBA-AI框架通过平衡效率、公平性和可解释性，为水下图像处理提供了可持续、偏置感知且计算高效的解决方案，推动了海洋保护的可持续发展。

Abstract: Underwater image enhancement is vital for marine conservation, particularly
coral reef monitoring. However, AI-based enhancement models often face dataset
bias, high computational costs, and lack of transparency, leading to potential
misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware
AI framework to address these challenges. EBA-AI leverages CLIP embeddings to
detect and mitigate dataset bias, ensuring balanced representation across
varied underwater environments. It also integrates adaptive processing to
optimize energy efficiency, significantly reducing GPU usage while maintaining
competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100
show that while PSNR drops by a controlled 1.0 dB, computational savings enable
real-time feasibility for large-scale marine monitoring. Additionally,
uncertainty estimation and explainability techniques enhance trust in AI-driven
environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,
WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing
efficiency, fairness, and interpretability in underwater image processing. By
addressing key limitations of AI-driven enhancement, this work contributes to
sustainable, bias-aware, and computationally efficient marine conservation
efforts. For interactive visualizations, animations, source code, and access to
the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/

</details>


### [9] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: 研究提出利用MLLM生成的HS和深度特征距离作为奖励函数，以减少GSR模型中的幻觉伪影，提升感知质量与保真度的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前生成式超分辨率（GSR）模型在感知图像质量上虽领先，但存在生成的细节与低分辨率图像（LRI）或真实图像（GTI）不匹配的幻觉伪影问题，限制了实际应用。

Method: 构建了一个评估幻觉视觉元素的提示，利用MLLM生成HS，并发现特定深度特征距离与HS强相关，进而提出将这些特征作为可微分奖励函数来对齐GSR模型。

Result: HS与人类评估高度一致，并为超分辨率（SR）模型的图像度量提供了互补性见解。同时，某些深度特征距离与HS有强相关性。

Conclusion: 通过利用多模态大语言模型（MLLM）生成的“幻觉分数”（HS）及深度特征距离作为可微分奖励函数，可以有效减少生成式超分辨率（GSR）模型中的幻觉伪影，提升模型的感知质量与保真度平衡。

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [10] [Visual Place Recognition for Large-Scale UAV Applications](https://arxiv.org/abs/2507.15089)
*Ioannis Tsampikos Papapetros,Ioannis Kansizoglou,Antonios Gasteratos*

Main category: cs.CV

TL;DR: 本文介绍了LASED大规模航空数据集和可转向CNNs，显著提升了空中视觉地点识别的模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 空中vPR面临大规模高海拔数据集稀缺和无人机图像固有旋转模糊性的挑战，限制了模型的泛化能力。

Method: 提出LASED大规模航空数据集和可转向卷积神经网络（CNNs）的结合，以处理旋转方差并生成鲁棒的、方向不变的特征表示。

Result: 在LASED上训练的模型比在小规模、多样性不足的数据集上训练的模型召回率显著提高。可转向CNNs平均比最佳非可转向网络提高了12%的召回率。

Conclusion: 通过结合结构化的大规模数据集和旋转等变神经网络，我们的方法显著增强了空中视觉地点识别（vPR）的模型鲁棒性和泛化能力。

Abstract: Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial
Vehicle (UAV) navigation, enabling robust localization across diverse
environments. Despite significant advancements, aerial vPR faces unique
challenges due to the limited availability of large-scale, high-altitude
datasets, which limits model generalization, along with the inherent rotational
ambiguity in UAV imagery. To address these challenges, we introduce LASED, a
large-scale aerial dataset with approximately one million images,
systematically sampled from 170,000 unique locations throughout Estonia over a
decade, offering extensive geographic and temporal diversity. Its structured
design ensures clear place separation significantly enhancing model training
for aerial scenarios. Furthermore, we propose the integration of steerable
Convolutional Neural Networks (CNNs) to explicitly handle rotational variance,
leveraging their inherent rotational equivariance to produce robust,
orientation-invariant feature representations. Our extensive benchmarking
demonstrates that models trained on LASED achieve significantly higher recall
compared to those trained on smaller, less diverse datasets, highlighting the
benefits of extensive geographic coverage and temporal diversity. Moreover,
steerable CNNs effectively address rotational ambiguity inherent in aerial
imagery, consistently outperforming conventional convolutional architectures,
achieving on average 12\% recall improvement over the best-performing
non-steerable network. By combining structured, large-scale datasets with
rotation-equivariant neural networks, our approach significantly enhances model
robustness and generalization for aerial vPR.

</details>


### [11] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: DUSTrack是一个结合深度学习和光流的半自动工具包，用于高效追踪B型超声视频中的组织运动，具有高准确性和多功能性。


<details>
  <summary>Details</summary>
Motivation: 准确追踪B型超声中的组织运动由于斑点噪声、低边缘对比度和平面外运动等挑战而困难，这对临床和研究应用中量化组织动力学至关重要。

Method: 结合深度学习和光流技术，开发了一个半自动框架DUSTrack，用于追踪B型超声视频中的任意点。该工具包包含图形用户界面，支持高质量训练数据的生成和迭代模型优化，并采用了一种基于光流的新型过滤技术以减少高频帧间噪声。

Result: DUSTrack在准确性上优于现有的零起点追踪器，与专用方法表现相当。通过三个用例展示了其多功能性：心脏壁运动追踪、肌肉变形分析和筋膜追踪。

Conclusion: DUSTrack作为一种开源解决方案，为超声视频中的组织运动量化提供了一个强大而灵活的点追踪框架，具有广泛的临床应用和生物力学研究潜力。

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [12] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: CRAFT是一个神经符号框架，结合常识先验和视觉证据，通过迭代推理提升场景中动作与对象关联的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 为了解决场景中对象与动作关联的透明性和可解释性问题，CRAFT旨在通过神经符号方法实现目标驱动的决策。

Method: CRAFT整合了ConceptNet的结构化常识先验、语言模型和CLIP的视觉证据，通过基于能量的推理循环迭代优化预测。

Result: 在多对象、无标签设置下的实验表明，CRAFT不仅提高了准确性，还增强了可解释性。

Conclusion: CRAFT框架通过结合神经符号方法，显著提高了场景理解的准确性和可解释性，为构建稳健且可信赖的视觉系统提供了重要一步。

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [13] [Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images](https://arxiv.org/abs/2507.15496)
*JunYing Huang,Ao Xu,DongSun Yong,KeRen Li,YuanFeng Wang,Qi Qin*

Main category: cs.CV

TL;DR: 提出了一种结合LiDAR点云和图像的里程计框架，通过深度补全和注意力机制实现高精度姿态估计，在KITTI基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 里程计是自主系统进行自我定位和导航的关键任务，需要高精度和鲁棒性的姿态估计方法。

Method: 该方法通过深度补全从点云和图像中估计密集深度图，并采用带注意力机制的多尺度特征提取网络，实现自适应深度感知表示。此外，利用密集深度信息优化光流估计，减少遮挡区域的误差。通过分层姿态优化模块逐步优化运动估计，确保在动态环境和尺度模糊情况下的鲁棒预测。

Result: 在KITTI里程计基准测试中，该方法展示了优异的性能，准确性和鲁棒性与现有最佳方法相当或更优。

Conclusion: 该论文提出的LiDAR-Visual里程计框架在KITTI里程计基准测试中展现了与当前最先进的视觉和LiDAR里程计方法相当或更优的准确性和鲁棒性。

Abstract: Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.

</details>


### [14] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: 本文提出一种基于高斯变形场的3DGS视频流框架，通过混合显著性分块和差异化质量建模，实现高效压缩与传输，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射（3DGS）虽提升了体积视频表示质量，但其数据量大、压缩与传输复杂度高，亟需解决流媒体传输挑战。

Method: 基于高斯变形场的3DGS视频构建方法，结合混合显著性分块和差异化质量建模，实现高效数据压缩和带宽波动适应。

Result: 实验评估表明，该方法在视频质量、压缩效果和传输速率等多个方面优于现有方法。

Conclusion: 本文提出的3DGS视频流框架在视频质量、压缩效率和传输速率方面均优于现有方法，验证了其有效性。

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [15] [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597)
*Hao Luo,Yicheng Feng,Wanpeng Zhang,Sipeng Zheng,Ye Wang,Haoqi Yuan,Jiazheng Liu,Chaoyi Xu,Qin Jin,Zongqing Lu*

Main category: cs.CV

TL;DR: Being-H0是一种基于人类视频训练的灵巧视觉-语言-动作模型，通过物理指令调优和运动标记化方法，显著提升了复杂操作任务的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型因依赖合成数据或缺乏多样性的遥操作演示，难以处理高灵巧性任务且泛化能力差，需利用人类手部操作的丰富数据解决这一瓶颈。

Method: 提出物理指令调优训练范式，结合大规模VLA预训练、物理空间对齐和机器人任务后适应，并引入部分级运动标记化方法以实现毫米级重建精度。

Result: Being-H0在手部运动生成和指令跟随方面表现优异，且能随模型和数据规模扩展，真实机器人操作中也观察到预期增益。

Conclusion: Being-H0通过物理指令调优和部分级运动标记化方法，显著提升了复杂操作任务的性能，并在真实机器人操作中展现了预期的增益。

Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.

</details>


### [16] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: IRGPT是首个针对真实世界红外图像的多模态大语言模型，基于26万张真实红外图像-文本对的数据集，采用双向跨模态课程学习策略，在多个任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖通过风格转移从可见光图像生成的合成红外图像，无法捕捉红外模态的独特特性，因此需要一种新的方法来解决真实世界红外图像的挑战。

Method: 提出了IRGPT，一个基于大规模红外-文本数据集（IR-TD）的多模态大语言模型，采用双向跨模态课程转移学习策略，从可见光到红外领域系统化转移知识。

Result: IRGPT在包括识别和定位等9个任务上表现优异，即使与更大规模的模型相比也达到了最先进的性能。

Conclusion: IRGPT在9个任务基准测试中表现优异，达到了最先进的性能，证明了其在处理真实世界红外图像方面的有效性。

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [17] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: GPI-Net利用Gestalt原则优化局部与全局特征融合，通过正交集成和注意力机制提升点云配准性能，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 解决由于特征冗余和复杂空间关系导致的局部与全局特征融合难题，提高点云配准中高质量对应关系的识别准确性。

Method: 提出了一种基于Gestalt原则的并行交互网络（GPI-Net），包括正交集成策略、Gestalt特征注意力（GFA）块和双路径多粒度并行交互聚合（DMG）块。

Result: 在多种挑战性任务上的实验表明，GPI-Net优于现有方法。

Conclusion: GPI-Net通过Gestalt原则和正交几何一致性，成功优化了局部与全局特征的融合，显著提升了点云配准中高质量对应关系的识别性能。

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [18] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: 本文针对3DGS视频流的分块、质量评估和比特率自适应问题，提出了自适应分块技术、质量评估框架和元学习比特率算法，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 3DGS视频流在提供沉浸式3D视频体验方面具有潜力，但分块、质量评估和比特率自适应等基础挑战仍需研究。

Method: 提出了自适应3DGS分块技术（基于显著性分析）、质量评估框架（联合评估3DGS表示的空间域退化和2D渲染图像质量）和基于元学习的自适应比特率算法。

Result: 实验表明，所提方法在性能上显著优于现有技术。

Conclusion: 本文提出的自适应3DGS分块技术、质量评估框架和基于元学习的自适应比特率算法，显著优于现有方法，为3DGS视频流提供了全面的解决方案。

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [19] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: VisGuard 是一个抗篡改的可视化图像数据检索框架，通过鲁棒性技术嵌入元数据链接，支持多种应用并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的可视化图像数据检索方法大多缺乏实用性，因为它们对在线分发过程中的常见图像篡改（如裁剪和编辑）很脆弱。VisGuard 旨在解决这一问题，提供一个抗篡改的解决方案。

Method: VisGuard 采用了多种技术来增强鲁棒性，包括重复数据平铺、可逆信息广播和基于锚点的裁剪定位方案。

Result: 实验证明，VisGuard 在数据检索准确性、嵌入容量和安全性方面具有卓越性能，能够有效支持交互式图表重建、篡改检测和版权保护等应用。

Conclusion: VisGuard 是一个抗篡改的可视化图像数据检索（VIDR）框架，能够可靠地将元数据链接嵌入可视化图像中，即使在图像遭受重大篡改后，嵌入的数据链接仍可恢复。VisGuard 在数据检索准确性、嵌入容量和安全性方面表现出色，有效地促进和保护了可视化的传播和信息传递。

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [20] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: OptiCorNet是一个新颖的序列建模框架，通过端到端训练统一空间和时间特征，显著提升动态环境中的视觉地点识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决动态和感知混叠环境中视觉地点识别的挑战，现有方法多忽视图像序列中的时间一致性。

Method: OptiCorNet采用轻量级1D卷积编码器和可学习的微分时间算子（DSD），结合LSTM精炼和可选残差投影，生成紧凑且具有区分性的描述符。

Result: 在多个公开基准测试中，OptiCorNet在季节性和视角变化挑战下优于现有技术基线。

Conclusion: OptiCorNet通过统一空间特征提取和时间差分，提供了一个端到端的可训练模块，显著提升了动态和感知混叠环境中的视觉地点识别性能。

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [21] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: DFQ-ViT 通过合成高质量样本和激活校正矩阵，解决了数据无关量化中的性能下降问题，无需微调且适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法充分捕获和平衡样本中的全局和局部特征，导致合成数据质量有限，且量化模型与全精度模型在推理时中间层激活分布差异显著。

Method: 提出了一种合成样本难度递增的管道，并引入了激活校正矩阵以对齐量化模型和全精度模型的中间层激活。

Result: DFQ-ViT 在性能上显著优于现有 DFQ 方法，且与使用真实数据量化的模型性能相当。例如，3 位权重量化的 DeiT-T 性能比现有技术高出 4.29%。

Conclusion: DFQ-ViT 无需微调即可实现高性能的量化，降低了计算开销和边缘设备部署门槛，符合绿色学习原则。

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [22] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: 提出了一种检索增强的点云补全框架，通过跨模态检索和结构共享特征编码器提升补全效果和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法局限于特定输入类别且缺乏泛化能力的问题，通过引入跨模态检索学习结构先验信息。

Method: 设计了结构共享特征编码器（SSFE）和渐进式检索增强生成器（PRAG），通过双通道控制门增强相关结构特征，并采用分层特征融合机制整合参考先验信息。

Result: 在多个数据集和真实场景中验证了方法的有效性，能够生成精细化的点云，并能处理稀疏数据和未见类别。

Conclusion: 该论文提出的检索增强点云补全框架通过引入跨模态检索和结构共享特征编码器（SSFE），有效提升了点云补全的精细化和泛化能力。

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [23] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: TCP-LLaVA通过令牌压缩技术，有效降低计算成本并提升WSI视觉问答准确率。


<details>
  <summary>Details</summary>
Motivation: 解决WSIs在病理学中因长上下文和高计算需求对MLLM的挑战，现有方法缺乏生成能力或资源消耗过大。

Method: 提出TCP-LLaVA，一种通过令牌压缩进行WSI VQA的MLLM架构，利用模态压缩模块聚合视觉和文本信息。

Result: 在十种TCGA肿瘤亚型上的实验显示，TCP-LLaVA在VQA准确率上优于现有基线，同时大幅减少训练资源消耗。

Conclusion: TCP-LLaVA通过引入可训练的压缩令牌，显著减少了输入长度和计算成本，同时在VQA准确率上优于现有MLLM基线。

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [24] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: 本文综述了基于深度学习的前馈方法在3D重建和视图合成中的应用，分类了不同表示架构，总结了关键任务和挑战，并展望了未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统方法在计算上过于复杂，限制了实际应用。前馈方法通过深度学习实现了快速且通用的3D重建和视图合成，推动了该领域的革命。

Method: 论文提供了前馈技术在3D重建和视图合成中的全面综述，根据底层表示架构（如点云、3D高斯泼溅、NeRF等）进行分类，并分析了关键任务如无姿态重建、动态3D重建和3D感知图像/视频合成。

Result: 论文详细综述了前馈技术的分类、关键任务、应用场景、常用数据集和评估协议。

Conclusion: 论文总结了前馈方法在3D视觉领域的潜力，并讨论了未来研究的开放挑战和有前景的方向。

Abstract: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [25] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: DCHM框架通过深度一致性建模和超像素高斯溅射，解决了多视角行人检测中的噪声和泛化问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角行人检测中因噪声和低精度问题，依赖昂贵的人工标注且泛化能力差。

Method: 提出Depth-Consistent Human Modeling (DCHM)框架，结合超像素高斯溅射技术，实现稀疏视角、大规模和拥挤场景中的多视角深度一致性建模。

Result: DCHM显著减少了人体建模中的噪声，优于现有方法，并首次在复杂场景中实现行人重建和多视角分割。

Conclusion: DCHM框架通过深度一致性建模和超像素高斯溅射技术，显著减少了多视角行人检测中的人体建模噪声，并在复杂场景中实现了高精度的点云重建和多视角分割。

Abstract: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [26] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: 本文利用Florence模型改进胃肠道内窥镜的视觉问答，通过数据增强和微调在KASVIR数据集上取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 针对ImageCLEFmed MEDVQA 2025挑战赛的子任务1，旨在通过视觉问答技术提升胃肠道内窥镜图像的临床解读。

Method: 采用Florence模型作为VQA管道的核心，结合强大的视觉编码器和文本编码器来解析内窥镜图像并生成临床相关答案，同时应用领域特定的数据增强以提高泛化能力。

Result: 在KASVIR数据集上的实验表明，微调Florence模型能够在官方挑战指标上产生准确的回答。

Conclusion: 该论文展示了大规模多模态模型Florence在医学视觉问答（VQA）中的潜力，为未来在可解释性、鲁棒性和临床整合方面的研究提供了坚实基础。

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [27] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

TL;DR: 提出了ArtiMuse模型和ArtiMuse-10K数据集，解决了现有图像美学评估方法的模态偏差和细粒度不足问题，推动领域发展。


<details>
  <summary>Details</summary>
Motivation: 随着教育应用、艺术创作和AIGC技术的快速发展，对图像美学评估的需求增加，现有方法存在模态偏差和细粒度属性分解不足的问题。

Method: 提出了ArtiMuse，一种基于MLLM的IAA模型，具备联合评分和专家级理解能力；并创建了ArtiMuse-10K数据集，包含10,000张专家标注的图像。

Result: ArtiMuse模型在感知和泛化能力上优于传统方法，ArtiMuse-10K数据集提供了专业的8维属性分析和整体评分。

Conclusion: ArtiMuse和ArtiMuse-10K的公开将推动图像美学评估领域的发展，为研究和应用提供更全面的工具和数据支持。

Abstract: The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [28] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: 论文比较了GAN、扩散和FM模型在T1w到T2w MRI跨模态合成中的表现，发现Pix2Pix在多个指标上最优。


<details>
  <summary>Details</summary>
Motivation: 减少MRI扫描时间和成本，通过计算合成缺失的MRI对比度，同时保持诊断质量。

Method: 本文对生成对抗网络（GANs）、扩散模型和流匹配（FM）技术进行了全面基准测试，使用可比较的设置评估了T1w到T2w的2D MRI图像到图像（I2I）翻译。

Result: GAN-based Pix2Pix模型在三个公开的MRI数据集上表现最佳，尤其在结构保真度和计算效率方面。

Conclusion: GAN-based Pix2Pix模型在结构保真度、图像质量和计算效率上优于扩散和FM方法，为MRI跨模态合成提供了实用指导。

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [29] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: 本文提出一种基于计算机视觉的浏览器扩展，用于实时翻译手语为视频通话字幕，利用大规模ASL数据集实现高效沟通。


<details>
  <summary>Details</summary>
Motivation: 听障人士在疫情期间更倾向于使用手语而非打字进行视频通话，现有技术无法满足这一需求。

Method: 利用包含2000多个单词级ASL视频的大规模数据集，由100多名手语者表演，通过计算机视觉技术实现手语识别。

Result: 开发的浏览器扩展能够实时翻译手语为字幕，提升视频通话的沟通效率。

Conclusion: 本文提出了一种浏览器扩展，能够自动将手语翻译为视频通话中的字幕，旨在消除听障人士与普通人之间的沟通障碍。

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [30] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 本文比较了三种深度学习框架在血液细胞图像分类中的性能，发现JAX和PyTorch表现最佳，与现有基准相当。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在血液图像分析中展现出高潜力，但针对特定框架的详细性能分析尚不充分。本文旨在填补这一空白。

Method: 本文比较了三种流行的深度学习框架（TensorFlow with Keras、PyTorch和JAX）在BloodMNIST数据集上的性能，主要关注推断时间差异和不同图像尺寸下的分类性能。

Result: 结果显示，不同框架的性能存在差异，JAX和PyTorch的分类准确率与现有基准相当。

Conclusion: 不同深度学习框架（TensorFlow with Keras、PyTorch和JAX）在血液细胞图像分类任务中表现存在差异，受图像分辨率和框架特定优化等因素影响。JAX和PyTorch的分类准确率与现有基准相当，展示了它们在医学图像分类中的高效性。

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [31] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: Exp-Graph是一种基于图的框架，通过整合面部属性的结构信息提升表情识别准确率，在多种环境下表现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在人机交互应用中至关重要，而面部属性的结构随表情变化，因此将结构信息融入面部属性对表情识别至关重要。

Method: 提出Exp-Graph框架，通过基于图的建模表示面部属性的结构关系，利用面部关键点作为图的顶点，基于面部关键点的接近度和局部外观相似性确定边，并采用图卷积网络捕捉和整合这些结构依赖性。

Result: 在Oulu-CASIA、eNTERFACE05和AFEW三个基准数据集上的识别准确率分别为98.09%、79.01%和56.39%。

Conclusion: Exp-Graph模型在实验室环境和真实世界无约束环境中均表现出强大的泛化能力，证实了其在面部表情识别实际应用中的有效性。

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [32] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

TL;DR: 研究发现ANN决策边界与人类感知变异性之间存在系统性联系，通过微调ANN表示，成功对齐了ANN预测与人类感知模式。


<details>
  <summary>Details</summary>
Motivation: 探索人工神经网络（ANN）在模拟人类感知个体差异方面的能力，特别是面部表情分类中的高感知变异性现象。

Method: 研究引入了一种新颖的感知边界采样方法，生成位于ANN决策边界的面部表情刺激，构建了varEmotion数据集，并通过大规模人类行为实验进行分析。

Result: 分析显示，ANN难以分类的刺激同样引发了人类参与者的高度感知不确定性，揭示了情感感知中的共享计算原则。

Conclusion: 研究发现，通过微调人工神经网络（ANN）表示，可以实现ANN预测与群体及个体层面的人类感知模式对齐，为情感解释的个性化建模提供了新视角。

Abstract: A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [33] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: FST.ai是一个AI驱动的裁判框架，专注于跆拳道中的实时头部踢击检测，通过计算机视觉和深度学习提升决策速度和公平性，并具有跨体育项目的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的手动裁判系统（即使有即时视频回放支持）存在延迟、主观性和执行不一致的问题，影响了公平性和运动员的信任。AI的引入旨在解决这些问题。

Method: 该论文提出了一种基于计算机视觉、深度学习和边缘推理的AI框架FST.ai，专注于实时头部踢击检测和评分。方法包括姿态估计、动作分类和影响分析。

Result: FST.ai显著减少了决策时间（从分钟级降至秒级），并提高了裁判的一致性和透明度。此外，该方法不仅适用于跆拳道，还可扩展到其他需要动作检测的体育项目。

Conclusion: FST.ai框架展示了在体育裁判中应用AI的潜力，特别是在提升决策速度、一致性和透明度方面，并且其框架设计具有跨体育项目的适应性和扩展性。

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [34] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: 开发了一个相机引导系统，通过美学评估和图像修复算法帮助用户识别并去除照片中的杂物，提升拍摄质量。


<details>
  <summary>Details</summary>
Motivation: 解决摄影爱好者因疏忽或经验不足导致照片中杂物过多的问题。

Method: 采用基于美学评估的杂物区分算法和基于生成对抗网络的迭代图像修复算法。

Result: 用户研究表明系统能有效帮助用户在更短时间内拍摄更高质量的照片。

Conclusion: 该系统通过灵活界面和精准算法帮助用户更高效地识别照片中的干扰物并提升拍摄质量。

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [35] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: 研究提出了一种基于计算机视觉的框架，用于估计机构餐饮中的食物浪费，展示了良好的性能，并为未来的自动化浪费追踪系统奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 量化机构餐饮环境中的食物浪费对于支持数据驱动的可持续发展策略至关重要。

Method: 研究提出了一个成本效益高的计算机视觉框架，通过利用RGB图像在餐前和餐后的语义分割来估计食物浪费。使用了四种全监督模型（U-Net、U-Net++及其轻量级变体），并采用动态逆频率损失和AdamW优化器进行训练。

Result: 所有模型均表现良好，至少有一个模型对每种食物类型的DPA接近或超过90%。轻量级模型在NVIDIA T4 GPU上实现了实时推理。对于干燥和刚性成分（如米饭和薯条）的分割性能更优，而复杂、碎片化或粘稠的菜肴（如炖菜）表现较差。

Conclusion: 该研究为大规模餐饮环境中的自动化、实时食物浪费追踪系统奠定了基础，并为餐厅管理和政策制定者提供了可行的未来方向。

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [36] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: Descrip3D框架通过自然语言描述增强物体关系表示，显著提升3D场景理解任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景-语言模型在关系理解上存在不足，尤其是仅依赖视觉嵌入无法充分捕捉物体角色和交互。

Method: Descrip3D采用双级集成（嵌入融合和提示级注入）方法，通过文本描述增强物体表示，统一支持多种任务（如定位、描述和问答）。

Result: Descrip3D在ScanRefer、Multi3DRefer等五个基准数据集上均优于基线模型。

Conclusion: Descrip3D通过自然语言显式编码物体间关系，显著提升了3D场景理解能力，在多个基准数据集上表现优于现有基线模型。

Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [37] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: WSI-Agents是一种新型协作多代理系统，通过任务分配、验证和总结模块，提升多模态WSI分析的准确性与多功能性，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在WSI分析中多功能性与任务准确性难以兼顾，医疗多代理系统的潜力在病理领域尚未充分探索。

Method: 提出WSI-Agents系统，包含三个核心组件：任务分配模块（基于模型库分配任务）、验证机制（通过内外一致性检查确保准确性）和总结模块（生成最终报告与可视化解释）。

Result: 在多种WSI基准测试中，WSI-Agents表现优于现有WSI MLLMs和医疗代理框架。

Conclusion: WSI-Agents通过整合专业功能代理、任务分配与验证机制，显著提升了多任务WSI分析的准确性与多功能性，实验证明其优于现有WSI MLLMs和医疗代理框架。

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [38] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: LEAD 是一种基于对数输出的微调对齐方法，通过ODE建模非线性优化过程，有效预测模型迁移性，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 预训练模型的激增导致选择最适合下游任务的模型变得困难，现有方法在特征空间中线性建模微调动态，未能准确捕捉优化中的非线性。

Method: LEAD 提出了一种基于对数输出的微调对齐方法，通过理论框架建模优化过程，并设计类感知分解方法，考虑不同类别的动态变化。

Result: 在24个监督和自监督预训练模型及10个下游数据集上的实验表明，LEAD 表现出色，尤其在低数据场景下仍具广泛适应性。

Conclusion: LEAD 提供了一种基于对数输出的微调对齐方法，通过理论框架和ODE建模非线性优化过程，有效预测模型迁移性，并在实验中展示了卓越的性能和广泛适应性。

Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [39] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: LeAdQA结合因果感知查询优化和视觉定位，显著提升视频问答性能，在多个数据集上实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前VideoQA方法存在任务无关的采样和启发式检索的局限性，无法有效处理稀疏关键帧和复杂因果关系。LeAdQA旨在通过因果感知查询优化和细粒度视觉定位解决这些问题。

Method: LeAdQA首先利用LLMs重新表述问题-选项对，消除因果模糊性并聚焦关键时间点；随后通过时间定位模型精确检索最相关视频片段，并结合自适应融合机制动态整合证据；最后通过MLLM处理视觉-文本线索生成答案。

Result: 在NExT-QA、IntentQA和NExT-GQA数据集上的实验表明，LeAdQA在复杂推理任务中达到SOTA性能，同时保持了计算效率。

Conclusion: LeAdQA通过结合因果感知查询优化和细粒度视觉定位，显著提升了视频问答（VideoQA）任务的性能，在NExT-QA、IntentQA和NExT-GQA数据集上实现了最先进（SOTA）效果，同时保持了计算效率。

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [40] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: FOCUS框架通过光谱提示和[SINK]令牌，实现了高效且可解释的ViT在HSI中的应用，提升了波段识别和注意力稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有显著性方法难以捕捉有意义的频谱线索，且全频谱ViT在高维HSI数据中计算成本过高，限制了ViT在HSI中的可解释性应用。

Method: FOCUS框架包含两个核心组件：类特定光谱提示和可学习的[SINK]令牌，通过吸引力损失训练，避免梯度反向传播或主干网络修改。

Result: FOCUS将波段级IoU提高了15%，减少了超过40%的注意力崩溃，生成的显著性结果与专家标注高度一致。

Conclusion: FOCUS框架通过引入类特定光谱提示和可学习的[SINK]令牌，显著提升了ViT在HSI数据中的解释性，使其在保持高效的同时，生成稳定且可解释的3D显著性图和光谱重要性曲线。

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


### [41] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: DiSCO-3D是首个解决3D开放词汇子概念发现的方法，结合无监督分割和开放词汇指导，表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅适应特定任务目标或场景内容，无法同时适应场景和用户查询。

Method: 基于神经场表示，结合无监督分割和弱开放词汇指导。

Result: DiSCO-3D在开放词汇子概念发现中实现了有效性能。

Conclusion: DiSCO-3D在3D开放词汇子概念发现中表现出色，并在开放词汇和无监督分割的边缘案例中展示了最先进的成果。

Abstract: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [42] [Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection](https://arxiv.org/abs/2507.14807)
*Juan Hu,Shaojing Fan,Terence Sim*

Main category: cs.CV

TL;DR: 该论文提出HICOM框架，利用人类认知线索提升多面孔深度伪造视频检测能力，实验显示其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测方法在单面孔场景中表现良好，但在多面孔社交场景中表现不佳，缺乏对关键上下文线索的感知。

Method: 通过人类研究系统分析了人们在社交环境中检测深度伪造面孔的四种关键线索，并基于这些线索开发了HICOM框架。

Result: HICOM在基准数据集上平均准确率提高了3.3%，在真实世界扰动下提高了2.8%，在未见数据集上优于现有方法5.8%。

Conclusion: 该研究通过引入人类认知的线索，显著提升了多面孔深度伪造视频的检测能力，并增强了检测结果的可解释性。

Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often
appearing in natural social settings that challenge existing detection methods.
Most current approaches excel at single-face detection but struggle in
multi-face scenarios, due to a lack of awareness of crucial contextual cues. In
this work, we develop a novel approach that leverages human cognition to
analyze and defend against multi-face deepfake videos. Through a series of
human studies, we systematically examine how people detect deepfake faces in
social settings. Our quantitative analysis reveals four key cues humans rely
on: scene-motion coherence, inter-face appearance compatibility, interpersonal
gaze alignment, and face-body consistency. Guided by these insights, we
introduce \textsf{HICOM}, a novel framework designed to detect every fake face
in multi-face scenarios. Extensive experiments on benchmark datasets show that
\textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and
2.8\% under real-world perturbations. Moreover, it outperforms existing methods
by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired
cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM
to provide human-readable explanations, making detection results more
transparent and convincing. Our work sheds light on involving human factors to
enhance defense against deepfakes.

</details>


### [43] [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)
*Jiaji Zhang,Ruichao Sun,Hailiang Zhao,Jiaju Wu,Peng Chen,Hao Li,Xinkui Zhao,Kingsum Chow,Gang Xiong,Lin Ye,Shuiguang Deng*

Main category: cs.CV

TL;DR: SegQuant是一个通用的量化框架，结合SegLinear和DualScale技术，解决了扩散模型PTQ的通用性和部署问题，保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成能力上表现出色，但计算成本高，限制了其在资源受限或延迟敏感环境中的部署。现有PTQ方法依赖特定架构启发式，通用性差，难以与工业部署流程集成。

Method: SegQuant包含一个段感知的基于图的量化策略（SegLinear）和一个双尺度量化方案（DualScale），前者捕捉结构语义和空间异质性，后者保留极性不对称激活。

Result: SegQuant不仅适用于基于Transformer的扩散模型，还能与主流部署工具无缝兼容，表现出色。

Conclusion: SegQuant是一个通用的量化框架，通过自适应结合互补技术增强跨模型适应性，解决了现有PTQ方法在扩散模型中的局限性，同时保持了生成输出的视觉保真度。

Abstract: Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.

</details>


### [44] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

TL;DR: DD-SAM2通过轻量级适配器（DD-Adapter）高效微调SAM2，显著提升医学视频分割与追踪性能，仅需少量训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法多为模态特定设计，适应性差，且SAM2等模型在医学视频场景中需大规模数据集微调，计算成本高且易发生灾难性遗忘。

Method: 提出DD-SAM2框架，结合Depthwise-Dilated Adapter（DD-Adapter）增强多尺度特征提取，实现SAM2在医学视频上的高效微调。

Result: 在TrackRad2025和EchoNet-Dynamic数据集上分别达到Dice分数0.93和0.97，表现优异。

Conclusion: DD-SAM2通过引入Depthwise-Dilated Adapter（DD-Adapter）有效提升了SAM2在医学视频分割和追踪中的性能，且仅需少量训练数据即可实现高效微调，为医学视频分析提供了新的解决方案。

Abstract: Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [45] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: 提出一种无需外部条件的配对图像生成方法，通过训练扩散引导器生成DBT切片和病灶掩码，提升生成质量和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: DBT图像中肿块病灶的高隐蔽性导致手动标注困难且耗时，现有数据增强方法难以学习病灶区域特征且无法生成对应标注。

Method: 通过训练一个额外的扩散引导器来实现条件扩散模型的配对图像生成。

Result: 实验结果表明，该方法能提高生成质量，并有效缓解标注数据不足问题，提升下游分割任务性能。

Conclusion: 该研究提出的配对图像生成方法无需外部条件即可提高生成质量，并缓解标注数据短缺问题，从而提升下游任务性能。

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images
is very significant for the early screening of breast cancer. However, the
high-density breast tissue often leads to high concealment of the mass lesions,
which makes manual annotation difficult and time-consuming. As a result, there
is a lack of annotated data for model training. Diffusion models are commonly
used for data augmentation, but the existing methods face two challenges.
First, due to the high concealment of lesions, it is difficult for the model to
learn the features of the lesion area. This leads to the low generation quality
of the lesion areas, thus limiting the quality of the generated images. Second,
existing methods can only generate images and cannot generate corresponding
annotations, which restricts the usability of the generated images in
supervised training. In this work, we propose a paired image generation method.
The method does not require external conditions and can achieve the generation
of paired images by training an extra diffusion guider for the conditional
diffusion model. During the experimental phase, we generated paired DBT slices
and mass lesion masks. Then, we incorporated them into the supervised training
process of the mass lesion segmentation task. The experimental results show
that our method can improve the generation quality without external conditions.
Moreover, it contributes to alleviating the shortage of annotated data, thus
enhancing the performance of downstream tasks.

</details>


### [46] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: BusterX++ 是多模态合成媒体检测框架，通过强化学习后训练提升性能，GenBuster++ 基准测试验证其效果。


<details>
  <summary>Details</summary>
Motivation: 生成式 AI 的快速发展导致合成媒体（图像、视频）的误用风险增加，现有单模态检测方法无法应对多格式结合的合成内容。

Method: BusterX++ 采用多模态设计，结合强化学习后训练策略（多阶段训练、思维奖励和混合推理），以解决单模态检测的局限性。

Result: BusterX++ 在 GenBuster++ 基准测试中表现出稳定且显著的性能提升，验证了其跨模态检测的有效性。

Conclusion: BusterX++ 框架通过多模态设计和强化学习后训练策略，显著提升了合成媒体的检测与解释能力，GenBuster++ 基准测试进一步验证了其有效性和泛化性。

Abstract: Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [47] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: 论文提出了一种无需退化知识的全合一视频修复框架，利用基础模型和自然语言实现可解释的修复，并在多个基准上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频修复方法通常需要退化知识，限制了其灵活性和可解释性。论文旨在通过自然语言和基础模型，提供一个无需退化知识的全合一视频修复框架，并推动该领域的基准标准化。

Method: 论文方法基于基础模型，通过自然语言将视频帧的退化感知语义上下文进行建模，无需退化知识即可训练和测试。方法学习对基础模型的近似知识，使得推理时可以无成本地解耦模型。

Result: 论文提出的方法在多个基准上实现了最先进的性能，包括多退化设置（3D和4D）和时变复合退化基准（如模拟雪强度变化的自然天气退化数据集）。

Conclusion: 该论文提出了一个全合一的视频修复框架，通过基础模型将视频帧的退化感知语义上下文与自然语言结合，提供可解释且灵活的指导。与现有方法不同，该方法在训练或测试时无需退化知识，并学习对基础模型的近似知识，从而在推理时无额外成本地解耦基础模型。此外，论文呼吁全合一视频修复的基准标准化，并提出了多退化设置下的两个基准（3D和4D）以及两个时变复合退化基准。实验表明，该方法在所有基准上均达到了最先进的性能。

Abstract: In this work, we propose an all-in-one video restoration framework that
grounds degradation-aware semantic context of video frames in natural language
via foundation models, offering interpretable and flexible guidance. Unlike
prior art, our method assumes no degradation knowledge in train or test time
and learns an approximation to the grounded knowledge such that the foundation
model can be safely disentangled during inference adding no extra cost.
Further, we call for standardization of benchmarks in all-in-one video
restoration, and propose two benchmarks in multi-degradation setting,
three-task (3D) and four-task (4D), and two time-varying composite degradation
benchmarks; one of the latter being our proposed dataset with varying snow
intensity, simulating how weather degradations affect videos naturally. We
compare our method with prior works and report state-of-the-art performance on
all benchmarks.

</details>


### [48] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

TL;DR: MS2Fusion通过双路径SSM框架高效融合多光谱特征，在目标检测等任务中表现优异且通用性强。


<details>
  <summary>Details</summary>
Motivation: 解决多光谱特征融合中过度偏好局部互补特征而忽略跨模态共享语义的问题，以及感受野大小与计算复杂度之间的权衡瓶颈。

Method: 提出基于状态空间模型（SSM）的MS2Fusion框架，采用双路径参数交互机制：第一条路径通过跨模态隐藏状态解码挖掘互补信息，第二条路径通过参数共享实现跨模态对齐。

Result: 在FLIR、M3FD和LLVIP等主流基准测试中显著优于现有方法，同时在RGB-T语义分割和RGBT显著目标检测任务中达到最优效果。

Conclusion: MS2Fusion框架通过双路径参数交互机制，在统一框架中融合多光谱特征，显著优于现有方法，并展现出在其他多光谱感知任务中的通用性。

Abstract: Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [49] [TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP](https://arxiv.org/abs/2507.14904)
*Fan Li,Zanyi Wang,Zeyi Huang,Guang Dai,Jingdong Wang,Mengmeng Wang*

Main category: cs.CV

TL;DR: 提出统一2D预训练多模态网络，简化3D视觉定位架构，通过GARF模块实现跨模态特征融合，减少参数并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法依赖多模态分离编码器，导致模型庞大且训练效率低。尽管部分方法尝试使用预训练的2D多模态模型，但仍难以对齐点云数据与2D编码器，仍需依赖3D编码器。

Method: 利用基于2D CLIP双模态模型的适配器微调框架，设计GARF模块融合点云与图像的几何多尺度特征，并结合文本特征进行最终模态融合，引入多模态解码器以实现深度跨模态理解。

Result: 与基线相比，该方法减少了约58%的可训练参数，同时在3D检测任务中提升了6.52%，在3D视觉定位任务中提升了6.25%。

Conclusion: 本文提出了一种统一的2D预训练多模态网络，显著简化了3D视觉定位模型的架构。通过几何感知的2D-3D特征恢复与融合模块（GARF），实现了跨模态的统一特征提取与融合，减少了可训练参数并提升了性能。

Abstract: 3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.

</details>


### [50] [Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images](https://arxiv.org/abs/2507.14670)
*Yaxuan Song,Jianan Fan,Hang Chang,Weidong Cai*

Main category: cs.CV

TL;DR: Gene-DML通过双路径多层次判别框架，显著提升基因表达预测性能，实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用组织病理学图像与基因表达谱之间的跨模态表示对齐，限制了预测性能。Gene-DML旨在解决这一问题。

Method: Gene-DML采用双路径（多尺度实例级判别和跨级实例-组判别）框架，通过多层次对齐形态学和转录模态的表示，增强跨模态对应关系。

Result: 在公共空间转录组数据集上的实验表明，Gene-DML在基因表达预测中达到了最先进的性能。

Conclusion: Gene-DML通过双路径多层次判别框架，显著提升了基因表达预测的准确性和泛化能力，为精准医学和计算病理学提供了有力工具。

Abstract: Accurately predicting gene expression from histopathology images offers a
scalable and non-invasive approach to molecular profiling, with significant
implications for precision medicine and computational pathology. However,
existing methods often underutilize the cross-modal representation alignment
between histopathology images and gene expression profiles across multiple
representational levels, thereby limiting their prediction performance. To
address this, we propose Gene-DML, a unified framework that structures latent
space through Dual-pathway Multi-Level discrimination to enhance correspondence
between morphological and transcriptional modalities. The multi-scale
instance-level discrimination pathway aligns hierarchical histopathology
representations extracted at local, neighbor, and global levels with gene
expression profiles, capturing scale-aware morphological-transcriptional
relationships. In parallel, the cross-level instance-group discrimination
pathway enforces structural consistency between individual (image/gene)
instances and modality-crossed (gene/image, respectively) groups, strengthening
the alignment across modalities. By jointly modelling fine-grained and
structural-level discrimination, Gene-DML is able to learn robust cross-modal
representations, enhancing both predictive accuracy and generalization across
diverse biological contexts. Extensive experiments on public spatial
transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art
performance in gene expression prediction. The code and checkpoints will be
released soon.

</details>


### [51] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: 提出了Doc-750K数据集和Docopilot模型，显著提升了复杂多页文档的理解能力，无需依赖RAG。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在复杂多页文档理解上表现不足，主要由于缺乏高质量的文档级数据集，且现有RAG方法存在检索上下文碎片化、多阶段错误累积和额外时间成本等问题。

Method: 开发了Doc-750K数据集，包含多样文档结构和跨页依赖关系，并基于此构建了原生多模态模型Docopilot，无需依赖RAG即可处理文档级依赖。

Result: Docopilot在文档理解任务和多轮交互中表现出卓越的连贯性、准确性和效率。

Conclusion: Docopilot通过高质量的Doc-750K数据集和原生多模态模型，显著提升了复杂文档理解任务的连贯性、准确性和效率，为文档级多模态理解设定了新基准。

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [52] [From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition](https://arxiv.org/abs/2507.14686)
*Chen Cai,Tianyi Liu,Jianjun Gao,Wenyang Liu,Kejun Wu,Ruoyu Wang,Yi Wang,Soo Chin Liew*

Main category: cs.CV

TL;DR: MIPD通过从MLLM中提取多模态知识，提升了小模型在开放词汇基础情境识别中的泛化和零样本能力。


<details>
  <summary>Details</summary>
Motivation: 解决MLLMs在复杂基础情境识别任务中的资源密集性和泛化能力不足，以及传统GSR模型对未见和罕见情境识别能力有限的问题。

Method: 提出了Multimodal Interactive Prompt Distillation (MIPD)框架，包括LLM-based Judgmental Rationales Generator (JRG)、Negative-Guided Multimodal Prompting Alignment (NMPA)模块等，以对齐和提取多模态知识。

Result: 在Ov-SWiG数据集上实现了对已见、罕见和未见情境的优越性能，并在HICO-DET数据集上展示了改进的未见检测能力。

Conclusion: MIPD框架通过从教师MLLM中提取丰富的多模态知识，显著提升了学生Ov-GSR模型在开放词汇基础情境识别任务中的泛化和零样本能力。

Abstract: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot
abilities but struggle with complex Grounded Situation Recognition (GSR) and
are resource-intensive for edge device deployment. Meanwhile, conventional GSR
models often lack generalization ability, falling short in recognizing unseen
and rare situations. In this paper, we exploit transferring knowledge from a
teacher MLLM to a small GSR model to enhance its generalization and zero-shot
abilities, thereby introducing the task of Open-vocabulary Grounded Situation
Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt
Distillation (MIPD), a novel framework that distills enriched multimodal
knowledge from the foundation model, enabling the student Ov-GSR model to
recognize unseen situations and be better aware of rare situations.
Specifically, the MIPD framework first leverages the LLM-based Judgmental
Rationales Generator (JRG) to construct positive and negative glimpse and gaze
rationales enriched with contextual semantic information. The proposed
scene-aware and instance-perception prompts are then introduced to align
rationales with visual information from the MLLM teacher via the
Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively
capturing holistic and perceptual multimodal knowledge. Finally, the aligned
multimodal knowledge is distilled into the student Ov-GSR model, providing a
stronger foundation for generalization that enhances situation understanding,
bridges the gap between seen and unseen scenarios, and mitigates prediction
bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving
superior performance on seen, rare, and unseen situations, and further
demonstrate improved unseen detection on the HICO-DET dataset.

</details>


### [53] [GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset](https://arxiv.org/abs/2507.14697)
*Zhiwei Zhang,Zi Ye,Yibin Wen,Shuai Yuan,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TL;DR: GTPBD是首个全球精细梯田地块数据集，覆盖复杂地形，支持多种任务，填补了研究空白。


<details>
  <summary>Details</summary>
Motivation: 现有农业地块提取研究缺乏对复杂梯田地形的精细表达，无法满足精准农业的需求。

Method: 提出了一个名为GTPBD的精细梯田地块数据集，包含47,537张高分辨率图像和三级标签，覆盖全球主要梯田区域。

Result: GTPBD在语义分割、边缘检测、梯田地块提取和无监督域适应任务中表现出色，填补了研究空白。

Conclusion: GTPBD填补了梯田遥感研究的关键空白，为精细农业地形分析和跨场景知识迁移提供了基础设施。

Abstract: Agricultural parcels serve as basic units for conducting agricultural
practices and applications, which is vital for land ownership registration,
food security assessment, soil erosion monitoring, etc. However, existing
agriculture parcel extraction studies only focus on mid-resolution mapping or
regular plain farmlands while lacking representation of complex terraced
terrains due to the demands of precision agriculture.In this paper, we
introduce a more fine-grained terraced parcel dataset named GTPBD (Global
Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset
covering major worldwide terraced regions with more than 200,000 complex
terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution
images with three-level labels, including pixel-level boundary labels, mask
labels, and parcel labels. It covers seven major geographic zones in China and
transcontinental climatic regions around the world.Compared to the existing
datasets, the GTPBD dataset brings considerable challenges due to the: (1)
terrain diversity; (2) complex and irregular parcel objects; and (3) multiple
domain styles. Our proposed GTPBD dataset is suitable for four different tasks,
including semantic segmentation, edge detection, terraced parcel extraction,
and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the
GTPBD dataset on eight semantic segmentation methods, four edge extraction
methods, three parcel extraction methods, and five UDA methods, along with a
multi-dimensional evaluation framework integrating pixel-level and object-level
metrics. GTPBD fills a critical gap in terraced remote sensing research,
providing a basic infrastructure for fine-grained agricultural terrain analysis
and cross-scenario knowledge transfer.

</details>


### [54] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAnimator++ 是一种新型视频扩散框架，通过可学习的姿态对齐和ID适配器，解决了人体动画中的身份一致性问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在人体图像动画中因姿态或体型差异导致的身份一致性问题。

Method: StableAnimator++ 结合了可学习的姿态对齐模块、全局内容感知的人脸编码器和分布感知的ID适配器，通过HJB优化进一步提升面部保真度。

Result: 在基准测试中，StableAnimator++ 在保持身份一致性和生成高质量视频方面表现优异。

Conclusion: StableAnimator++ 通过引入可学习的姿态对齐和分布感知的ID适配器，显著提升了身份一致性，并在实验中展示了其高质量的视频生成能力。

Abstract: Current diffusion models for human image animation often struggle to maintain
identity (ID) consistency, especially when the reference image and driving
video differ significantly in body size or position. We introduce
StableAnimator++, the first ID-preserving video diffusion framework with
learnable pose alignment, capable of generating high-quality videos conditioned
on a reference image and a pose sequence without any post-processing. Building
upon a video diffusion model, StableAnimator++ contains carefully designed
modules for both training and inference, striving for identity consistency. In
particular, StableAnimator++ first uses learnable layers to predict the
similarity transformation matrices between the reference image and the driven
poses via injecting guidance from Singular Value Decomposition (SVD). These
matrices align the driven poses with the reference image, mitigating
misalignment to a great extent. StableAnimator++ then computes image and face
embeddings using off-the-shelf encoders, refining the face embeddings via a
global content-aware Face Encoder. To further maintain ID, we introduce a
distribution-aware ID Adapter that counteracts interference caused by temporal
layers while preserving ID via distribution alignment. During the inference
stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization
integrated into the denoising process, guiding the diffusion trajectory for
enhanced facial fidelity. Experiments on benchmarks show the effectiveness of
StableAnimator++ both qualitatively and quantitatively.

</details>


### [55] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

TL;DR: MultiRetNet通过多模态融合和临床延迟系统，提高糖尿病视网膜病变分期准确性，尤其帮助低收入人群早期检测。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是全球可预防性失明的主要原因，低收入社区人群因筛查机会有限，更容易进展至晚期。共病条件进一步加速疾病进展，因此需要一种更准确的早期检测方法。

Method: 提出MultiRetNet管道，结合视网膜成像、社会经济因素和共病档案，采用三种多模态融合方法，并确定全连接层融合为最佳方法。通过合成对抗性低质量图像和对比学习训练延迟系统，指导模型识别需要临床医生审查的样本。

Result: MultiRetNet在次优图像上保持诊断准确性，并整合关键健康数据，尤其在医疗资源匮乏人群中显著提高早期检测率。

Conclusion: MultiRetNet通过结合视网膜成像、社会经济因素和共病档案，提高了糖尿病视网膜病变分期的准确性，并结合临床延迟系统，实现了临床人机协作。该方法有望降低医疗成本、提高早期检测率并解决医疗资源不平等问题。

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [56] [BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking](https://arxiv.org/abs/2507.15094)
*Mengya Xu,Rulin Zhou,An Wang,Chaoyang Lyu,Zhen Li,Ning Zhong,Hongliang Ren*

Main category: cs.CV

TL;DR: 本文提出了首个 ESD 出血源数据集 BleedOrigin-Bench 和双阶段检测-跟踪框架 BleedOrigin-Net，显著提升了出血源的实时定位与跟踪能力。


<details>
  <summary>Details</summary>
Motivation: 针对 ESD 手术中出血源实时定位和连续监测的挑战，现有 AI 方法仅关注出血区域分割，缺乏对出血源的精确定位和动态跟踪能力。

Method: 提出了双阶段检测-跟踪框架 BleedOrigin-Net，结合了出血源检测和时空跟踪技术，并利用 BleedOrigin-Bench 数据集进行训练和评估。

Result: BleedOrigin-Net 在出血起始检测、初始源定位和点跟踪任务中均达到了最先进性能，分别为 96.85% 帧级准确率、70.24% 像素级准确率和 96.11% 像素级准确率。

Conclusion: BleedOrigin-Net 在 ESD 手术中实现了出血源的精确检测与跟踪，显著提升了手术效率和安全性。

Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses
significant risks, demanding precise, real-time localization and continuous
monitoring of the bleeding source for effective hemostatic intervention. In
particular, endoscopists have to repeatedly flush to clear blood, allowing only
milliseconds to identify bleeding sources, an inefficient process that prolongs
operations and elevates patient risks. However, current Artificial Intelligence
(AI) methods primarily focus on bleeding region segmentation, overlooking the
critical need for accurate bleeding source detection and temporal tracking in
the challenging ESD environment, which is marked by frequent visual
obstructions and dynamic scene changes. This gap is widened by the lack of
specialized datasets, hindering the development of robust AI-assisted guidance
systems. To address these challenges, we introduce BleedOrigin-Bench, the first
comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated
bleeding sources across 106,222 frames from 44 procedures, supplemented with
39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6
challenging clinical scenarios. We also present BleedOrigin-Net, a novel
dual-stage detection-tracking framework for the bleeding source localization in
ESD procedures, addressing the complete workflow from bleeding onset detection
to continuous spatial tracking. We compare with widely-used object detection
models (YOLOv11/v12), multimodal large language models, and point tracking
methods. Extensive evaluation demonstrates state-of-the-art performance,
achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset
detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source
detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.

</details>


### [57] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: InterAct VideoQA是一个针对复杂交通场景的视频问答数据集，用于提升和评估VideoQA模型，已在GitHub公开。


<details>
  <summary>Details</summary>
Motivation: 现有的视频问答（VideoQA）模型在处理现实交通场景的复杂性时表现不佳，这些场景中多个并发事件在时空维度上展开。

Method: 本文引入了InterAct VideoQA数据集，包含8小时的真实交通视频，分为10秒剪辑，并标注了超过25,000个问题-答案对，覆盖时空动态、车辆交互、事件检测等关键交通属性。

Result: 通过在InterAct VideoQA数据集上微调，现有最先进的VideoQA模型性能显著提升，证明了领域特定数据集对VideoQA的必要性。

Conclusion: InterAct VideoQA数据集作为一个公开的基准数据集，旨在促进智能交通系统中可部署视频问答模型的未来研究。

Abstract: Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>


### [58] [Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation](https://arxiv.org/abs/2507.15243)
*Naeem Paeedeh,Mahardhika Pratama,Wolfgang Mayer,Jimmy Cao,Ryszard Kowlczyk*

Main category: cs.CV

TL;DR: 结合DINO和原型分类器的预训练模型在CD-FSL中超越SOTA方法；提出CP和SSTs伪类生成方法解决过拟合问题，在极端域偏移场景表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决因标记样本稀缺导致更新过多Transformer参数时出现的过拟合问题，提升跨域少样本学习性能。

Method: 提出了一种新概念Coalescent Projection（CP）作为软提示的有效替代方案，并结合自监督变换（SSTs）的伪类生成方法，仅依赖基础域数据为网络应对不同域的未见样本做准备。

Result: 在BSCD-FSL基准测试的极端域偏移场景下，所提方法表现出色，性能优于现有SOTA方法。

Conclusion: 尽管在跨域少样本学习（CD-FSL）方面取得了进展，但结合DINO预训练模型和原型分类器的方法在性能上超越了最新的SOTA方法。本文提出的Coalescent Projection（CP）概念和结合自监督变换（SSTs）的伪类生成方法，有效解决了因标记样本稀缺导致的过拟合问题，并在极端域偏移场景下表现出色。

Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model
pre-trained with DINO combined with a prototypical classifier outperforms the
latest SOTA methods. A crucial limitation that needs to be overcome is that
updating too many parameters of the transformers leads to overfitting due to
the scarcity of labeled samples. To address this challenge, we propose a new
concept, Coalescent Projection (CP), as an effective successor to soft prompts.
Additionally, we propose a novel pseudo-class generation method combined with
Self-Supervised Transformations (SSTs) that relies solely on the base domain to
prepare the network for encountering unseen samples from different domains. The
proposed method exhibits its effectiveness in comprehensive experiments on the
extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published
at https://github.com/Naeem-Paeedeh/CPLSR.

</details>


### [59] [A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation](https://arxiv.org/abs/2507.14790)
*Wenbo Yue,Chang Li,Guoping Xu*

Main category: cs.CV

TL;DR: 提出HPD下采样方法，通过MinMaxPooling保留图像细节，实验证明其在语义分割任务中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统下采样方法（如最大池化和跨行卷积）在特征聚合、感受野扩展和计算量减少方面表现良好，但在语义分割任务中可能导致关键空间信息丢失，影响逐像素预测精度。

Method: 提出了一种基于信息互补的下采样方法——混合池化下采样（HPD），其核心是用MinMaxPooling替代传统方法，通过提取局部区域的最大值信息，有效保留图像的明暗对比和细节特征。

Result: 在ACDC和Synapse数据集上的实验表明，HPD在分割性能上优于传统方法，平均DSC系数提高了0.5%。

Conclusion: HPD模块为语义分割任务提供了高效的解决方案，平均提高了DSC系数0.5%。

Abstract: In convolutional neural networks (CNNs), downsampling operations are crucial
to model performance. Although traditional downsampling methods (such as
maximum pooling and cross-row convolution) perform well in feature aggregation,
receptive field expansion, and computational reduction, they may lead to the
loss of key spatial information in semantic segmentation tasks, thereby
affecting the pixel-by-pixel prediction accuracy.To this end, this study
proposes a downsampling method based on information complementarity - Hybrid
Pooling Downsampling (HPD). The core is to replace the traditional method with
MinMaxPooling, and effectively retain the light and dark contrast and detail
features of the image by extracting the maximum value information of the local
area.Experiment on various CNN architectures on the ACDC and Synapse datasets
show that HPD outperforms traditional methods in segmentation performance, and
increases the DSC coefficient by 0.5% on average. The results show that the HPD
module provides an efficient solution for semantic segmentation tasks.

</details>


### [60] [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/abs/2507.14797)
*Beier Zhu,Ruoyu Wang,Tong Zhao,Hanwang Zhang,Chi Zhang*

Main category: cs.CV

TL;DR: EPD是一种新型ODE求解器，通过并行梯度计算减少截断误差，在低延迟下实现高质量图像生成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）虽然实现了最先进的生成性能，但由于其顺序去噪特性，采样延迟较高。现有的基于求解器的加速方法在低延迟预算下往往面临图像质量下降的问题。

Method: 提出了一种名为EPD的新型ODE求解器，通过在每个ODE步骤中引入多个并行梯度评估来减少截断误差，并且这些额外的梯度计算可以完全并行化，从而保持低延迟采样。方法通过蒸馏方式优化少量可学习参数，确保训练开销最小化。

Result: 在多种图像合成基准测试中，EPD在保持低延迟的同时实现了高质量的采样。例如，在5 NFE的相同延迟水平下，EPD在CIFAR-10上实现了4.47的FID，显著优于现有方法。

Conclusion: EPD（Ensemble Parallel Direction solver）作为一种新型ODE求解器，通过并行梯度计算有效减少了截断误差，并在低延迟预算下实现了高质量的图像生成，显著超越了现有基于学习的求解器。

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance
but suffer from high sampling latency due to their sequential denoising nature.
Existing solver-based acceleration methods often face image quality degradation
under a low-latency budget. In this paper, we propose the Ensemble Parallel
Direction solver (dubbed as \ours), a novel ODE solver that mitigates
truncation errors by incorporating multiple parallel gradient evaluations in
each ODE step. Importantly, since the additional gradient computations are
independent, they can be fully parallelized, preserving low-latency sampling.
  Our method optimizes a small set of learnable parameters in a distillation
fashion, ensuring minimal training overhead.
  In addition, our method can serve as a plugin to improve existing ODE
samplers. Extensive experiments on various image synthesis benchmarks
demonstrate the effectiveness of our \ours~in achieving high-quality and
low-latency sampling. For example, at the same latency level of 5 NFE, EPD
achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26
on LSUN Bedroom, surpassing existing learning-based solvers by a significant
margin. Codes are available in https://github.com/BeierZhu/EPD.

</details>


### [61] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 论文提出一种基于条件扩散模型的视频压缩框架，通过多粒度条件和多条件训练，显著提升了高压缩比下的感知质量。


<details>
  <summary>Details</summary>
Motivation: 基于条件扩散模型在视频内容重建中表现优异，论文旨在利用这一优势构建感知优化的视频压缩框架。

Method: 1. 多粒度条件捕捉静态场景结构和动态时空线索；2. 设计高效传输的紧凑表示；3. 多条件训练结合模态丢弃和角色感知嵌入以增强鲁棒性。

Result: 在FVD和LPIPS等感知质量指标上显著优于传统和神经编解码器。

Conclusion: 该论文提出的基于条件扩散模型的视频压缩框架在感知质量指标上显著优于传统和神经编解码器，特别是在高压缩比下表现优异。

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [62] [An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks](https://arxiv.org/abs/2507.14798)
*Xinyi Wu,Steven Landgraf,Markus Ulrich,Rongjun Qin*

Main category: cs.CV

TL;DR: DUSt3R/MASt3R/VGGT在稀疏航拍图像上表现出色，完整性提升50%，但高分辨率和大图像集时姿态估计可靠性下降，适合作为传统方法的补充。


<details>
  <summary>Details</summary>
Motivation: 评估这些先进模型在航拍图像上的潜在应用，尤其是在处理极低图像重叠、立体遮挡和无纹理区域时的表现。

Method: 本研究对预训练的DUSt3R/MASt3R/VGGT模型在UseGeo数据集的航拍块上进行了全面评估，重点关注姿态估计和密集3D重建。

Result: 这些方法能够从极稀疏的图像集（少于10张图像，分辨率高达518像素）中准确重建密集点云，完整性比COLMAP提高多达50%。VGGT还表现出更高的计算效率、可扩展性和更可靠的相机姿态估计。

Conclusion: 尽管基于Transformer的方法（如DUSt3R/MASt3R/VGGT）在稀疏图像集和高计算效率方面表现出色，但它们无法完全取代传统的SfM和MVS方法，更适合作为补充工具应用于低分辨率和稀疏场景。

Abstract: State-of-the-art 3D computer vision algorithms continue to advance in
handling sparse, unordered image sets. Recently developed foundational models
for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction
(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry
Grounded Transformer (VGGT), have attracted attention due to their ability to
handle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical
aerial images matters, as these models may handle extremely low image overlaps,
stereo occlusions, and textureless regions. For redundant collections, they can
accelerate 3D reconstruction by using extremely sparsified image sets. Despite
tests on various computer vision benchmarks, their potential on photogrammetric
aerial blocks remains unexplored. This paper conducts a comprehensive
evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of
the UseGeo dataset for pose estimation and dense 3D reconstruction. Results
show these methods can accurately reconstruct dense point clouds from very
sparse image sets (fewer than 10 images, up to 518 pixels resolution), with
completeness gains up to +50% over COLMAP. VGGT also demonstrates higher
computational efficiency, scalability, and more reliable camera pose
estimation. However, all exhibit limitations with high-resolution images and
large sets, as pose reliability declines with more images and geometric
complexity. These findings suggest transformer-based methods cannot fully
replace traditional SfM and MVS, but offer promise as complementary approaches,
especially in challenging, low-resolution, and sparse scenarios.

</details>


### [63] [Exploring Scalable Unified Modeling for General Low-Level Vision](https://arxiv.org/abs/2507.14801)
*Xiangyu Chen,Kaiwen Zhu,Yuandong Pu,Shuo Cao,Xiaohui Li,Wenlong Zhang,Yihao Liu,Yu Qiao,Jiantao Zhou,Chao Dong*

Main category: cs.CV

TL;DR: 论文提出VPIP框架和GenLV模型，通过视觉提示统一建模多样低级别视觉任务，实验证明其在多任务中表现优异，并展示出良好的泛化和适应能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决低级别视觉任务中任务表述和输出域差异大的挑战，作者旨在开发一个能够统一建模多种任务的框架。

Method: 论文提出了一个基于视觉提示的图像处理框架VPIP，包括端到端的图像处理主干、提示编码器和提示交互模块，并开发了统一低级别视觉模型GenLV。通过扩展模型容量和任务多样性，构建了一个包含100多个任务的大规模基准。

Result: 实验结果表明，该方法在多种任务中取得了显著性能，增加训练任务数量增强了泛化能力，特别是在数据有限的任务中。模型在零样本泛化、少样本迁移和任务特定微调中表现出强适应性。

Conclusion: 该论文提出的VPIP框架和GenLV模型在多样化的低级别视觉任务中表现出色，展现了其作为统一基础模型的潜力，特别是在零样本泛化、少样本迁移和任务特定微调场景中的强大适应性。

Abstract: Low-level vision involves a wide spectrum of tasks, including image
restoration, enhancement, stylization, and feature extraction, which differ
significantly in both task formulation and output domains. To address the
challenge of unified modeling across such diverse tasks, we propose a Visual
task Prompt-based Image Processing (VPIP) framework that leverages input-target
image pairs as visual prompts to guide the model in performing a variety of
low-level vision tasks. The framework comprises an end-to-end image processing
backbone, a prompt encoder, and a prompt interaction module, enabling flexible
integration with various architectures and effective utilization of
task-specific visual representations. Based on this design, we develop a
unified low-level vision model, GenLV, and evaluate its performance across
multiple representative tasks. To explore the scalability of this approach, we
extend the framework along two dimensions: model capacity and task diversity.
We construct a large-scale benchmark consisting of over 100 low-level vision
tasks and train multiple versions of the model with varying scales.
Experimental results show that the proposed method achieves considerable
performance across a wide range of tasks. Notably, increasing the number of
training tasks enhances generalization, particularly for tasks with limited
data, indicating the model's ability to learn transferable representations
through joint training. Further evaluations in zero-shot generalization,
few-shot transfer, and task-specific fine-tuning scenarios demonstrate the
model's strong adaptability, confirming the effectiveness, scalability, and
potential of the proposed framework as a unified foundation for general
low-level vision modeling.

</details>


### [64] [ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis](https://arxiv.org/abs/2507.15335)
*Muhammad Aqeel,Federico Leonardi,Francesco Setti*

Main category: cs.CV

TL;DR: ExDD框架通过双重特征分布建模和合成数据生成，显著提升了工业缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 工业缺陷检测系统在单类异常检测范式下存在局限性，尤其是在数据稀缺和非均匀异常分布情况下表现不佳。

Method: 采用并行记忆库捕获正常和异常模式的统计特性，结合潜在扩散模型生成领域特定的合成缺陷数据，并通过邻域感知比率评分机制融合互补距离度量。

Result: 在KSDD2数据集上的实验验证显示，ExDD实现了94.2%的I-AUROC和97.7%的P-AUROC，最佳增强效果在100个合成样本时达到。

Conclusion: ExDD框架通过显式建模双重特征分布，成功克服了传统单类异常检测范式的局限性，特别是在数据稀缺的工业环境中表现出色。

Abstract: Industrial defect detection systems face critical limitations when confined
to one-class anomaly detection paradigms, which assume uniform outlier
distributions and struggle with data scarcity in realworld manufacturing
environments. We present ExDD (Explicit Dual Distribution), a novel framework
that transcends these limitations by explicitly modeling dual feature
distributions. Our approach leverages parallel memory banks that capture the
distinct statistical properties of both normality and anomalous patterns,
addressing the fundamental flaw of uniform outlier assumptions. To overcome
data scarcity, we employ latent diffusion models with domain-specific textual
conditioning, generating in-distribution synthetic defects that preserve
industrial context. Our neighborhood-aware ratio scoring mechanism elegantly
fuses complementary distance metrics, amplifying signals in regions exhibiting
both deviation from normality and similarity to known defect patterns.
Experimental validation on KSDD2 demonstrates superior performance (94.2%
I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.

</details>


### [65] [EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)
*Jiaao Li,Kaiyuan Li,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: EgoPrune是一种无需训练的令牌修剪方法，专为自我运动视频设计，显著提高推理效率并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 由于自我运动视频在体现AI代理中的重要性，现有方法无法有效利用其时空连续性和运动约束，需要更高效的推理方法。

Method: EgoPrune包括三个组件：关键帧选择器、视角感知冗余过滤器和基于最大边际相关性的令牌选择器。

Result: 在两种自我运动视频基准测试中，EgoPrune始终优于现有方法，显著降低了计算资源消耗。

Conclusion: EgoPrune是一种专为自我运动视频推理设计的无需训练的令牌修剪方法，显著提高了计算效率，适用于现实世界的部署。

Abstract: Egomotion videos are first-person recordings where the view changes
continuously due to the agent's movement. As they serve as the primary visual
input for embodied AI agents, making egomotion video reasoning more efficient
is therefore essential for real-world deployment. Recent advances in
vision-language models have enabled strong multimodal reasoning capabilities,
but their computational cost remains prohibitive for long, redundant video
inputs. Existing token pruning methods, typically designed for third-person
videos, fail to leverage the spatiotemporal continuity and motion constraints
inherent in egomotion settings. To address this, we propose EgoPrune, a
training-free token pruning method tailored for egomotion video reasoning.
EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR
for temporally efficient sampling; Perspective-Aware Redundancy Filtering
(PARF), which aligns visual tokens using perspective transformations and
removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token
selector that jointly considers visual-text relevance and intra-frame
diversity. Experiments on two egomotion video benchmarks show that EgoPrune
consistently outperforms prior training-free methods across various pruning
ratios while significantly reducing FLOPs, memory usage, and latency. Moreover,
we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB
edge device, demonstrating its real-world efficiency and suitability for
on-device egomotion video reasoning.

</details>


### [66] [FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models](https://arxiv.org/abs/2507.14823)
*Dong Shu,Haoyang Yuan,Yuchen Wang,Yanguang Liu,Huopu Zhang,Haiyan Zhao,Mengnan Du*

Main category: cs.CV

TL;DR: FinChart-Bench是首个专注于金融图表的基准数据集，评估发现当前LVLM在金融图表理解上存在多项局限性。


<details>
  <summary>Details</summary>
Motivation: 金融图表因复杂的时间结构和领域特定术语而未被充分探索，需要专门的基准来评估LVLM的性能。

Method: 引入FinChart-Bench基准数据集，包含1,200张金融图表和7,016个问题，并对25种最先进的LVLM进行全面评估。

Result: 评估揭示了五个关键发现：开源与闭源模型性能差距缩小、升级模型性能下降、指令遵循困难、空间推理能力有限、LVLM作为自动评估器的可靠性不足。

Conclusion: 当前的大型视觉语言模型在金融图表理解方面存在显著局限性，FinChart-Bench数据集为未来研究提供了重要资源。

Abstract: Large vision-language models (LVLMs) have made significant progress in chart
understanding. However, financial charts, characterized by complex temporal
structures and domain-specific terminology, remain notably underexplored. We
introduce FinChart-Bench, the first benchmark specifically focused on
real-world financial charts. FinChart-Bench comprises 1,200 financial chart
images collected from 2015 to 2024, each annotated with True/False (TF),
Multiple Choice (MC), and Question Answering (QA) questions, totaling 7,016
questions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs
on FinChart-Bench. Our evaluation reveals critical insights: (1) the
performance gap between open-source and closed-source models is narrowing, (2)
performance degradation occurs in upgraded models within families, (3) many
models struggle with instruction following, (4) both advanced models show
significant limitations in spatial reasoning abilities, and (5) current LVLMs
are not reliable enough to serve as automated evaluators. These findings
highlight important limitations in current LVLM capabilities for financial
chart understanding. The FinChart-Bench dataset is available at
https://huggingface.co/datasets/Tizzzzy/FinChart-Bench.

</details>


### [67] [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/abs/2507.14826)
*Fu-Jen Tsai,Yan-Tsung Peng,Yen-Yu Lin,Chia-Wen Lin*

Main category: cs.CV

TL;DR: PHATNet通过雾模式转移和特定领域微调，提升了去雾模型在真实场景中的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有去雾模型在真实世界场景中因训练数据有限，对未见雾图像的泛化能力较差，需开发灵活的域适应方法以提升测试时的去雾性能。

Method: 提出了Physics-guided Haze Transfer Network (PHATNet)，通过转移未见目标域的雾模式到源域无雾图像上，生成领域特定的微调数据集。同时引入了Haze-Transfer-Consistency损失和Content-Leakage损失以增强解耦能力。

Result: 实验证明PHATNet在真实世界图像去雾基准数据集上显著提升了最先进去雾模型的性能。

Conclusion: PHATNet通过将目标域中的雾模式转移到源域无雾图像上，创建特定领域的微调数据集，显著提升了现有去雾模型在真实世界场景中的性能。

Abstract: Image dehazing aims to remove unwanted hazy artifacts in images. Although
previous research has collected paired real-world hazy and haze-free images to
improve dehazing models' performance in real-world scenarios, these models
often experience significant performance drops when handling unseen real-world
hazy images due to limited training data. This issue motivates us to develop a
flexible domain adaptation method to enhance dehazing performance during
testing. Observing that predicting haze patterns is generally easier than
recovering clean content, we propose the Physics-guided Haze Transfer Network
(PHATNet) which transfers haze patterns from unseen target domains to
source-domain haze-free images, creating domain-specific fine-tuning sets to
update dehazing models for effective domain adaptation. Additionally, we
introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to
enhance PHATNet's disentanglement ability. Experimental results demonstrate
that PHATNet significantly boosts state-of-the-art dehazing models on benchmark
real-world image dehazing datasets.

</details>


### [68] [Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image](https://arxiv.org/abs/2507.14845)
*Rizhao Fan,Zhigen Li,Heping Li,Ning An*

Main category: cs.CV

TL;DR: 提出自监督深度补全新范式，仅需稀疏深度和单张图像，通过创新损失函数和分割图增强效果，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度补全方法对密集标注或多帧数据的依赖问题，提升在静态或单帧场景中的适用性。

Method: 设计了一种新型自监督深度补全方法，利用深度分布特性构建损失函数，并整合视觉基础模型生成的分割图以优化深度估计。

Result: 实验证明，该方法在无需密集标签或额外视角图像的情况下，能有效传播深度信息至未观测区域，显著提升深度补全质量。

Conclusion: 论文提出了一种仅需稀疏深度测量及其对应图像的自监督深度补全范式，有效克服了依赖密集标注或多帧数据的限制，通过创新的损失函数和分割图增强，显著提升了深度补全效果。

Abstract: Depth completion is an important vision task, and many efforts have been made
to enhance the quality of depth maps from sparse depth measurements. Despite
significant advances, training these models to recover dense depth from sparse
measurements remains a challenging problem. Supervised learning methods rely on
dense depth labels to predict unobserved regions, while self-supervised
approaches require image sequences to enforce geometric constraints and
photometric consistency between frames. However, acquiring dense annotations is
costly, and multi-frame dependencies limit the applicability of self-supervised
methods in static or single-frame scenarios. To address these challenges, we
propose a novel self-supervised depth completion paradigm that requires only
sparse depth measurements and their corresponding image for training. Unlike
existing methods, our approach eliminates the need for dense depth labels or
additional images captured from neighboring viewpoints. By leveraging the
characteristics of depth distribution, we design novel loss functions that
effectively propagate depth information from observed points to unobserved
regions. Additionally, we incorporate segmentation maps generated by vision
foundation models to further enhance depth estimation. Extensive experiments
demonstrate the effectiveness of our proposed method.

</details>


### [69] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: GeMix通过类条件GAN生成视觉连贯的图像，替代传统mixup，显著提升医学图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统mixup的像素级插值在医学等高风险应用中可能生成不真实图像，阻碍学习效果。

Method: GeMix采用两阶段框架：首先训练StyleGAN2-ADA生成器，然后通过Dirichlet先验和Beta分布系数生成软标签，合成连续类流形上的图像。

Result: 在COVIDx-CT-3数据集上，GeMix结合真实数据后，所有骨干网络的macro-F1均优于传统mixup，并降低了COVID-19检测的假阴性率。

Conclusion: GeMix是一种有效的像素空间mixup替代方案，通过结合类条件GAN生成视觉连贯的图像，显著提升了COVID-19检测的准确性，并降低了假阴性率。

Abstract: Mixup has become a popular augmentation strategy for image classification,
yet its naive pixel-wise interpolation often produces unrealistic images that
can hinder learning, particularly in high-stakes medical applications. We
propose GeMix, a two-stage framework that replaces heuristic blending with a
learned, label-aware interpolation powered by class-conditional GANs. First, a
StyleGAN2-ADA generator is trained on the target dataset. During augmentation,
we sample two label vectors from Dirichlet priors biased toward different
classes and blend them via a Beta-distributed coefficient. Then, we condition
the generator on this soft label to synthesize visually coherent images that
lie along a continuous class manifold. We benchmark GeMix on the large-scale
COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,
EfficientNet-B0). When combined with real data, our method increases macro-F1
over traditional mixup for all backbones, reducing the false negative rate for
COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,
delivering stronger regularization and greater semantic fidelity, without
disrupting existing training pipelines. We publicly release our code at
https://github.com/hugocarlesso/GeMix to foster reproducibility and further
research.

</details>


### [70] [Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis](https://arxiv.org/abs/2507.15636)
*Lisan Al Amin,Md. Ismail Hossain,Thanh Thi Nguyen,Tasnim Jahan,Mahbubul Islam,Faisal Quader*

Main category: cs.CV

TL;DR: 研究利用彩票假设（LTH）剪枝深度伪造检测模型，发现关键子网络在极高稀疏度下仍保持性能，且可跨数据集迁移，为轻量化检测系统提供可能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的进步对信息完整性和社会信任构成挑战，而现有检测方法模型庞大且机制不透明，难以在资源有限环境中部署。研究旨在探索如何通过剪枝技术高效识别深度伪造检测的关键特征。

Method: 研究采用MesoNet、CNN-5和ResNet-18架构，在OpenForensic和FaceForensics++数据集上进行了大量实验。通过基于LTH的迭代幅度剪枝方法，识别出关键子网络，并结合Grad-CAM可视化分析剪枝后网络的注意力区域。

Result: 实验表明，MesoNet在80%稀疏度下仍保持56.2%的准确率（基线为62.6%），仅需3,000参数。基于LTH的剪枝方法优于一次性剪枝，且子网络在不同数据集间可迁移。

Conclusion: 研究发现，基于彩票假设（LTH）的剪枝方法能有效识别深度伪造检测中的关键子网络（winning tickets），在保持高准确率的同时大幅减少模型参数。此外，这些子网络在不同数据集间具有可迁移性，为高效部署深度伪造检测系统提供了潜力。

Abstract: Recent advances in deepfake technology have created increasingly convincing
synthetic media that poses significant challenges to information integrity and
social trust. While current detection methods show promise, their underlying
mechanisms remain poorly understood, and the large sizes of their models make
them challenging to deploy in resource-limited environments. This study
investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake
detection, aiming to identify the key features crucial for recognizing
deepfakes. We examine how neural networks can be efficiently pruned while
maintaining high detection accuracy. Through extensive experiments with
MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and
FaceForensics++ datasets, we find that deepfake detection networks contain
winning tickets, i.e., subnetworks, that preserve performance even at
substantial sparsity levels. Our results indicate that MesoNet retains 56.2%
accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000
parameters, which is about 90% of its baseline accuracy (62.6%). The results
also show that our proposed LTH-based iterative magnitude pruning approach
consistently outperforms one-shot pruning methods. Using Grad-CAM
visualization, we analyze how pruned networks maintain their focus on critical
facial regions for deepfake detection. Additionally, we demonstrate the
transferability of winning tickets across datasets, suggesting potential for
efficient, deployable deepfake detection systems.

</details>


### [71] [An Uncertainty-aware DETR Enhancement Framework for Object Detection](https://arxiv.org/abs/2507.14855)
*Xingshu Chen,Sicheng Yu,Chong Cheng,Hao Wang,Ting Tian*

Main category: cs.CV

TL;DR: 该论文提出了一种不确定性感知的DETR增强框架，通过建模边界框为高斯分布并引入新损失函数，提升了检测性能，并在多个数据集中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统检测器依赖确定性边界框回归，忽略了预测中的不确定性，限制了模型的鲁棒性。

Method: 提出了一种基于DETR的增强框架，通过将边界框建模为多元高斯分布，并在损失函数中引入Gromov-Wasserstein距离，同时利用贝叶斯风险过滤高风险信息。

Result: 在COCO基准测试中有效提升了性能，并在白细胞检测任务中（LISC和WBCDD数据集）达到了最优结果。

Conclusion: 该框架通过不确定性建模和Gromov-Wasserstein距离的引入，显著提升了DETR类检测器的性能，并在通用和特定领域任务中均展现了良好的扩展性。

Abstract: This paper investigates the problem of object detection with a focus on
improving both the localization accuracy of bounding boxes and explicitly
modeling prediction uncertainty. Conventional detectors rely on deterministic
bounding box regression, ignoring uncertainty in predictions and limiting model
robustness. In this paper, we propose an uncertainty-aware enhancement
framework for DETR-based object detectors. We model bounding boxes as
multivariate Gaussian distributions and incorporate the Gromov-Wasserstein
distance into the loss function to better align the predicted and ground-truth
distributions. Building on this, we derive a Bayes Risk formulation to filter
high-risk information and improve detection reliability. We also propose a
simple algorithm to quantify localization uncertainty via confidence intervals.
Experiments on the COCO benchmark show that our method can be effectively
integrated into existing DETR variants, enhancing their performance. We further
extend our framework to leukocyte detection tasks, achieving state-of-the-art
results on the LISC and WBCDD datasets. These results confirm the scalability
of our framework across both general and domain-specific detection tasks. Code
page:
https://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.

</details>


### [72] [Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition](https://arxiv.org/abs/2507.14867)
*Zhaoqiang Xia,Hexiang Huang,Haoyu Chen,Xiaoyi Feng,Guoying Zhao*

Main category: cs.CV

TL;DR: 提出超图增强Transformer框架，通过混合监督学习识别微手势情感，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 微手势作为无意识的身体动作能传达人类情感状态，但基于微手势的情感建模研究不足，因此探索其识别方法具有重要意义。

Method: 通过超图增强的自注意力模块和多尺度时间卷积模块构建编码器和解码器，采用自监督学习方式重构任务，并结合监督学习进行情感识别。

Result: 在iMiGUE和SMG数据集上，该方法在多个指标上达到最佳性能。

Conclusion: 本文提出的基于超图增强Transformer的混合监督框架在iMiGUE和SMG两个公开数据集上表现优异，性能优于现有方法。

Abstract: Micro-gestures are unconsciously performed body gestures that can convey the
emotion states of humans and start to attract more research attention in the
fields of human behavior understanding and affective computing as an emerging
topic. However, the modeling of human emotion based on micro-gestures has not
been explored sufficiently. In this work, we propose to recognize the emotion
states based on the micro-gestures by reconstructing the behavior patterns with
a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the
framework, hypergraph Transformer based encoder and decoder are separately
designed by stacking the hypergraph-enhanced self-attention and multiscale
temporal convolution modules. Especially, to better capture the subtle motion
of micro-gestures, we construct a decoder with additional upsampling operations
for a reconstruction task in a self-supervised learning manner. We further
propose a hypergraph-enhanced self-attention module where the hyperedges
between skeleton joints are gradually updated to present the relationships of
body joints for modeling the subtle local motion. Lastly, for exploiting the
relationship between the emotion states and local motion of micro-gestures, an
emotion recognition head from the output of encoder is designed with a shallow
architecture and learned in a supervised way. The end-to-end framework is
jointly trained in a one-stage way by comprehensively utilizing
self-reconstruction and supervision information. The proposed method is
evaluated on two publicly available datasets, namely iMiGUE and SMG, and
achieves the best performance under multiple metrics, which is superior to the
existing methods.

</details>


### [73] [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/abs/2507.15686)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: LINR-PCGC是首个基于INR的无损点云几何压缩方法，通过优化编码速度和轻量级网络设计，显著提升了压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于AI的点云压缩方法受限于训练数据分布，而基于INR的方法虽然解决了分布依赖问题，但编码时间和解码器大小的限制使其仅适用于有损几何压缩。本文旨在解决这一问题。

Method: 设计了点云级别的编码框架和有效的网络初始化策略，减少了60%的编码时间；提出基于多尺度稀疏卷积的轻量级编码网络，包括尺度上下文提取、子节点预测和模型压缩模块。

Result: 实验结果表明，LINR-PCGC在MVUB数据集上比G-PCC TMC13v23减少了21.21%的比特流，比SparsePCGC减少了21.95%。

Conclusion: LINR-PCGC 是一种基于隐式神经表示（INR）的无损点云几何压缩方法，通过加速编码速度和轻量级网络设计，显著优于传统和基于AI的压缩方法。

Abstract: Existing AI-based point cloud compression methods struggle with dependence on
specific training data distributions, which limits their real-world deployment.
Implicit Neural Representation (INR) methods solve the above problem by
encoding overfitted network parameters to the bitstream, resulting in more
distribution-agnostic results. However, due to the limitation of encoding time
and decoder size, current INR based methods only consider lossy geometry
compression. In this paper, we propose the first INR based lossless point cloud
geometry compression method called Lossless Implicit Neural Representations for
Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we
design a group of point clouds level coding framework with an effective network
initialization strategy, which can reduce around 60% encoding time. A
lightweight coding network based on multiscale SparseConv, consisting of scale
context extraction, child node prediction, and model compression modules, is
proposed to realize fast inference and compact decoder size. Experimental
results show that our method consistently outperforms traditional and AI-based
methods: for example, with the convergence time in the MVUB dataset, our method
reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and
21.95% compared to SparsePCGC. Our project can be seen on
https://huangwenjie2023.github.io/LINR-PCGC/.

</details>


### [74] [Region-aware Depth Scale Adaptation with Sparse Measurements](https://arxiv.org/abs/2507.14879)
*Rizhao Fan,Tianfang Ma,Zhigen Li,Ning An,Jian Cheng*

Main category: cs.CV

TL;DR: 提出一种非学习的方法，利用稀疏深度测量将基础模型的相对深度预测转换为度量深度，无需重新训练或微调，保持泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型在零样本单目深度估计中输出为相对尺度而非度量尺度的问题，避免现有方法因额外训练或微调导致的成本和泛化能力下降。

Method: 非学习基础的方法，利用稀疏深度测量来调整基础模型的相对尺度预测，无需重新训练或微调。

Result: 实验结果表明，该方法有效实现了相对尺度到度量尺度的转换，且未增加计算成本或牺牲泛化能力。

Conclusion: 本文提出了一种无需学习的方法，利用稀疏深度测量将基础模型的相对尺度预测转换为度量尺度深度，既保留了模型的泛化能力，又实现了度量深度输出。

Abstract: In recent years, the emergence of foundation models for depth prediction has
led to remarkable progress, particularly in zero-shot monocular depth
estimation. These models generate impressive depth predictions; however, their
outputs are often in relative scale rather than metric scale. This limitation
poses challenges for direct deployment in real-world applications. To address
this, several scale adaptation methods have been proposed to enable foundation
models to produce metric depth. However, these methods are typically costly, as
they require additional training on new domains and datasets. Moreover,
fine-tuning these models often compromises their original generalization
capabilities, limiting their adaptability across diverse scenes. In this paper,
we introduce a non-learning-based approach that leverages sparse depth
measurements to adapt the relative-scale predictions of foundation models into
metric-scale depth. Our method requires neither retraining nor fine-tuning,
thereby preserving the strong generalization ability of the original foundation
models while enabling them to produce metric depth. Experimental results
demonstrate the effectiveness of our approach, high-lighting its potential to
bridge the gap between relative and metric depth without incurring additional
computational costs or sacrificing generalization ability.

</details>


### [75] [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/abs/2507.15803)
*Danhui Chen,Ziquan Liu,Chuxi Yang,Dan Wang,Yan Yan,Yi Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: ConformalSAM利用基础分割模型和一致性预测，在半监督语义分割中高效生成高质量标注，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决像素级视觉任务中标注数据稀缺的问题，探索基础分割模型作为标注工具的潜力。

Method: 提出了ConformalSAM框架，包括利用目标域标注数据校准基础模型，并通过一致性预测过滤不可靠的像素标签，结合自依赖训练策略避免过拟合。

Result: 在三个标准SSSS基准测试中，ConformalSAM性能优于现有方法，并能作为插件提升其他方法的性能。

Conclusion: ConformalSAM通过结合基础分割模型和自训练策略，在半监督语义分割任务中表现出色，不仅自身性能优越，还能作为插件提升其他方法的性能。

Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive
and high-quality annotated data, which is costly to obtain. Semi-supervised
semantic segmentation (SSSS) has emerged as a solution to alleviate the
labeling burden by leveraging both labeled and unlabeled data through
self-training techniques. Meanwhile, the advent of foundational segmentation
models pre-trained on massive data, has shown the potential to generalize
across domains effectively. This work explores whether a foundational
segmentation model can address label scarcity in the pixel-level vision task as
an annotator for unlabeled images. Specifically, we investigate the efficacy of
using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual
input, to generate predictive masks for unlabeled data. To address the
shortcomings of using SEEM-generated masks as supervision, we propose
ConformalSAM, a novel SSSS framework which first calibrates the foundation
model using the target domain's labeled data and then filters out unreliable
pixel labels of unlabeled data so that only high-confidence labels are used as
supervision. By leveraging conformal prediction (CP) to adapt foundation models
to target data through uncertainty calibration, ConformalSAM exploits the
strong capability of the foundational segmentation model reliably which
benefits the early-stage learning, while a subsequent self-reliance training
strategy mitigates overfitting to SEEM-generated masks in the later training
stage. Our experiment demonstrates that, on three standard benchmarks of SSSS,
ConformalSAM achieves superior performance compared to recent SSSS methods and
helps boost the performance of those methods as a plug-in.

</details>


### [76] [BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters](https://arxiv.org/abs/2507.14885)
*Joaquim Comas,Federico Sukno*

Main category: cs.CV

TL;DR: BeatFormer是一个轻量级谱注意力模型，结合手工方法和深度学习优势，无需标签即可训练，在跨数据集运动场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法依赖大数据集，手工方法在未见场景中泛化能力强但性能受限，需要结合两者优势的混合方法。

Method: 提出了BeatFormer，一个轻量级谱注意力模型，结合了放大的正交复注意力和频域能量测量，以及谱对比学习（SCL）方法，无需PPG或HR标签即可训练。

Result: 在PURE、UBFC-rPPG和MMPD数据集上验证了BeatFormer的鲁棒性和性能，尤其在运动场景的跨数据集评估中表现优异。

Conclusion: BeatFormer结合了手工方法和深度学习的优势，通过轻量级谱注意力模型和谱对比学习，在无需PPG或HR标签的情况下实现了高效的rPPG估计，并在跨数据集评估中表现出色。

Abstract: Remote photoplethysmography (rPPG) captures cardiac signals from facial
videos and is gaining attention for its diverse applications. While deep
learning has advanced rPPG estimation, it relies on large, diverse datasets for
effective generalization. In contrast, handcrafted methods utilize
physiological priors for better generalization in unseen scenarios like motion
while maintaining computational efficiency. However, their linear assumptions
limit performance in complex conditions, where deep learning provides superior
pulsatile information extraction. This highlights the need for hybrid
approaches that combine the strengths of both methods. To address this, we
present BeatFormer, a lightweight spectral attention model for rPPG estimation,
which integrates zoomed orthonormal complex attention and frequency-domain
energy measurement, enabling a highly efficient model. Additionally, we
introduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be
trained without any PPG or HR labels. We validate BeatFormer on the PURE,
UBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,
particularly in cross-dataset evaluations under motion scenarios.

</details>


### [77] [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/abs/2507.15807)
*Shuo Chen,Jianzhe Liu,Zhen Han,Yan Xia,Daniel Cremers,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: 本文提出DARA策略和TrueMICL数据集，解决了MLLMs在视觉-语言任务中忽视视觉信息的问题，显著提升了多模态上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型(MLLMs)在视觉-语言任务中过度依赖文本模式，忽视视觉线索，导致多模态上下文学习(MICL)能力受限。这一局限性常被无需视觉理解的任务性能提升所掩盖。

Method: 采用Dynamic Attention Reallocation (DARA)策略，通过重新平衡视觉和文本标记的注意力，增强模型对视觉内容的关注。同时，提出了TrueMICL数据集，明确要求整合多模态信息以完成任务。

Result: 实验表明，DARA和TrueMICL能够显著提升模型的多模态上下文学习能力。

Conclusion: 本文提出的Dynamic Attention Reallocation (DARA)策略和TrueMICL数据集有效提升了多模态上下文学习能力，解决了当前模型过度依赖文本模式的问题。

Abstract: Multimodal Large Language Models (MLLMs), built on powerful language
backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new
tasks from a few multimodal demonstrations consisting of images, questions, and
answers. Despite showing noticeable improvement on standard vision-language
datasets, current MLLMs struggle to leverage visual information in the
demonstrations. Specifically, they tend to neglect visual cues and over-rely on
textual patterns, leading to mere text imitation rather than genuine multimodal
adaptation. This behavior makes MICL still unimodal and largely restricts its
practical utility. More importantly, this limitation is often concealed by the
improved performance on tasks that do not require understanding the visual
context. As a result, how to effectively enhance MICL ability and reliably
evaluate the MICL performance remains underexplored. To address these issues,
we first introduce Dynamic Attention Reallocation (DARA), an efficient
fine-tuning strategy that encourages models to attend to the visual context by
rebalancing attention across visual and textual tokens. In addition, we present
TrueMICL, an MICL-dedicated dataset with both support and test sets that
explicitly requires the integration of multimodal information-particularly
visual content-for correct task completion. Extensive experiments demonstrate
the effectiveness of our holistic solution, showcasing substantial improvements
in the true multimodal in-context learning capabilities. Code and datasets are
available at https://chenxshuo.github.io/true-micl-colm .

</details>


### [78] [Semantic-Aware Representation Learning for Multi-label Image Classification](https://arxiv.org/abs/2507.14918)
*Ren-Dong Xie,Zhi-Fen He,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: 本文提出SARL方法，通过语义感知表示学习改进多标签图像分类，实验证明其在两个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多标签图像分类方法通常使用注意力机制或图卷积网络（GCNs）学习图像表示，但这些表示可能包含噪声且无法精确定位对象。因此，本文旨在通过语义感知表示学习来解决这些问题。

Method: 本文提出了一种语义感知表示学习（SARL）方法，包括三个主要步骤：1）使用标签语义相关特征学习模块提取语义相关特征；2）设计基于最优传输的注意力机制以获得语义对齐的图像表示；3）采用区域分数聚合策略进行多标签预测。

Result: 在PASCAL VOC 2007和MS-COCO数据集上的实验表明，SARL方法优于现有方法。

Conclusion: SARL方法在PASCAL VOC 2007和MS-COCO两个基准数据集上的实验结果表明，其性能优于现有方法，验证了语义感知表示学习的有效性。

Abstract: Multi-label image classification, an important research area in computer
vision, focuses on identifying multiple labels or concepts within an image.
Existing approaches often employ attention mechanisms or graph convolutional
networks (GCNs) to learn image representation. However, this representation may
contain noise and may not locate objects precisely. Therefore, this paper
proposes a Semantic-Aware Representation Learning (SARL) for multi-label image
classification. First, a label semantic-related feature learning module is
utilized to extract semantic-related features. Then, an optimal transport-based
attention mechanism is designed to obtain semantically aligned image
representation. Finally, a regional score aggregation strategy is used for
multi-label prediction. Experimental results on two benchmark datasets, PASCAL
VOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing
methods.

</details>


### [79] [Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction](https://arxiv.org/abs/2507.14921)
*Xiufeng Huang,Ka Chun Cheung,Runmin Cong,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: \method 是一个解耦框架，通过立体视觉和全局注意力高效预测3D高斯几何与外观，减少了资源需求，适用于实际3D内容生成。


<details>
  <summary>Details</summary>
Motivation: 当前方法通常将3D高斯几何与外观预测耦合，严重依赖数据驱动先验，导致回归速度慢。为解决这一问题，提出了\method，一个解耦框架，用于高效的3D高斯预测。

Method: 该方法通过立体视觉骨干网络从局部图像对中提取特征，并通过全局注意力块融合这些特征。专用的点和高斯预测头生成多视点图用于几何预测和高斯特征用于外观预测，结合为GS-maps表示3DGS对象。一个细化网络增强这些GS-maps以实现高质量重建。

Result: \method 实现了无需相机参数的姿态无关3D重建，提高了鲁棒性和实用性。

Conclusion: \method 提供了一个高效、可扩展的解决方案，通过解耦3D高斯几何与外观预测，减少了资源需求，同时保持了高质量输出，适用于实际3D内容生成。

Abstract: Generalizable 3D Gaussian Splatting reconstruction showcases advanced
Image-to-3D content creation but requires substantial computational resources
and large datasets, posing challenges to training models from scratch. Current
methods usually entangle the prediction of 3D Gaussian geometry and appearance,
which rely heavily on data-driven priors and result in slow regression speeds.
To address this, we propose \method, a disentangled framework for efficient 3D
Gaussian prediction. Our method extracts features from local image pairs using
a stereo vision backbone and fuses them via global attention blocks. Dedicated
point and Gaussian prediction heads generate multi-view point-maps for geometry
and Gaussian features for appearance, combined as GS-maps to represent the 3DGS
object. A refinement network enhances these GS-maps for high-quality
reconstruction. Unlike existing methods that depend on camera parameters, our
approach achieves pose-free 3D reconstruction, improving robustness and
practicality. By reducing resource demands while maintaining high-quality
outputs, \method provides an efficient, scalable solution for real-world 3D
content generation.

</details>


### [80] [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://arxiv.org/abs/2507.15852)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Songxin He,Jianfan Lin,Junsong Tang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: SeC是一种概念驱动的视频对象分割框架，利用大型视觉语言模型构建对象概念表示，显著提升了复杂场景下的分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频对象分割技术依赖于外观匹配，缺乏人类般的对象概念理解能力，导致在处理复杂场景时性能不足。SeC旨在通过概念驱动的分割方法弥补这一差距。

Method: SeC框架采用大型视觉语言模型（LVLMs）构建和利用高层、对象中心的表示，结合视觉线索和语义推理，动态调整计算资源以应对场景复杂度。

Result: SeC在SeCVOS基准测试中比SAM 2.1提高了11.8分，实现了概念感知视频对象分割的新最佳性能。

Conclusion: SeC框架通过概念驱动的分割方法，显著提升了视频对象分割（VOS）任务在复杂场景下的性能，特别是在处理视觉变化、遮挡和动态场景转换方面，取得了11.8分的性能提升，成为概念感知视频对象分割的新标杆。

Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring
models to track and segment target objects across video frames. Despite notable
advances with recent efforts, current techniques still lag behind human
capabilities in handling drastic visual variations, occlusions, and complex
scene changes. This limitation arises from their reliance on appearance
matching, neglecting the human-like conceptual understanding of objects that
enables robust identification across temporal dynamics. Motivated by this gap,
we propose Segment Concept (SeC), a concept-driven segmentation framework that
shifts from conventional feature matching to the progressive construction and
utilization of high-level, object-centric representations. SeC employs Large
Vision-Language Models (LVLMs) to integrate visual cues across diverse frames,
constructing robust conceptual priors. During inference, SeC forms a
comprehensive semantic representation of the target based on processed frames,
realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively
balances LVLM-based semantic reasoning with enhanced feature matching,
dynamically adjusting computational efforts based on scene complexity. To
rigorously assess VOS methods in scenarios demanding high-level conceptual
reasoning and robust semantic understanding, we introduce the Semantic Complex
Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160
manually annotated multi-scenario videos designed to challenge models with
substantial appearance variations and dynamic scene transformations. In
particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,
establishing a new state-of-the-art in concept-aware video object segmentation.

</details>


### [81] [3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline](https://arxiv.org/abs/2507.14924)
*Kaishva Chintan Shah,Virajith Boddapati,Karthik S. Gurumoorthy,Sandip Kaledhonkar,Ajit Rajwade*

Main category: cs.CV

TL;DR: 本文提出了一种鲁棒的冷冻电镜姿态估计方法，结合$\ell_1$-范数优化和迭代平移校正，显著提高了低信噪比条件下的重建质量。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜（cryo-EM）中的姿态估计和位移校正是关键挑战，尤其是在极低信噪比（SNR）条件下，直接影响三维重建的保真度。现有方法通常依赖于对噪声敏感的$\ell_2$-范数目标函数，且仅近似满足几何约束，导致在低信噪比条件下误差累积和次优重建。

Method: 本文采用了一种基于多维尺度分析（MDS）技术的鲁棒姿态估计方法，结合$\ell_1$-范数目标函数和投影坐标下降法，同时估计旋转轴和平面内向量，并通过全局最小二乘公式进行迭代平移校正。

Result: 实验结果表明，本文提出的方法在欧拉角准确性和重建保真度（通过傅里叶壳层相关性（FSC）衡量）上均优于现有方法。

Conclusion: 本文提出的方法在低信噪比的冷冻电镜（cryo-EM）条件下，通过结合鲁棒的$\ell_1$-范数优化和迭代平移校正算法，显著提高了姿态估计的准确性和三维重建的保真度。

Abstract: Accurate pose estimation and shift correction are key challenges in cryo-EM
due to the very low SNR, which directly impacts the fidelity of 3D
reconstructions. We present an approach for pose estimation in cryo-EM that
leverages multi-dimensional scaling (MDS) techniques in a robust manner to
estimate the 3D rotation matrix of each particle from pairs of dihedral angles.
We express the rotation matrix in the form of an axis of rotation and a unit
vector in the plane perpendicular to the axis. The technique leverages the
concept of common lines in 3D reconstruction from projections. However, common
line estimation is ridden with large errors due to the very low SNR of cryo-EM
projection images. To address this challenge, we introduce two complementary
components: (i) a robust joint optimization framework for pose estimation based
on an $\ell_1$-norm objective or a similar robust norm, which simultaneously
estimates rotation axes and in-plane vectors while exactly enforcing unit norm
and orthogonality constraints via projected coordinate descent; and (ii) an
iterative shift correction algorithm that estimates consistent in-plane
translations through a global least-squares formulation. While prior approaches
have leveraged such embeddings and common-line geometry for orientation
recovery, existing formulations typically rely on $\ell_2$-based objectives
that are sensitive to noise, and enforce geometric constraints only
approximately. These choices, combined with a sequential pipeline structure,
can lead to compounding errors and suboptimal reconstructions in low-SNR
regimes. Our pipeline consistently outperforms prior methods in both Euler
angle accuracy and reconstruction fidelity, as measured by the Fourier Shell
Correlation (FSC).

</details>


### [82] [Probabilistic smooth attention for deep multiple instance learning in medical imaging](https://arxiv.org/abs/2507.14932)
*Francisco M. Castro-Macías,Pablo Morales-Álvarez,Yunan Wu,Rafael Molina,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: 本文提出一个概率框架，用于多示例学习中的不确定性建模，在医学图像分类中表现优异，并提供可解释的不确定性映射。


<details>
  <summary>Details</summary>
Motivation: 现有的深度多示例学习方法通常确定性地处理注意力值，忽视了单个实例贡献的不确定性，可能导致性能受限。

Method: 提出了一种新颖的概率框架，通过估计注意力值的概率分布来捕捉全局和局部交互，并考虑了单个实例贡献的不确定性。

Result: 在三个医学数据集和十一个基线方法的全面评估中，该方法在不同指标上均取得了最佳预测性能。

Conclusion: 本文提出的概率框架在多个医学数据集上表现出色，不仅预测性能优异，还能提供可解释的不确定性映射，有助于疾病定位。

Abstract: The Multiple Instance Learning (MIL) paradigm is attracting plenty of
attention in medical imaging classification, where labeled data is scarce. MIL
methods cast medical images as bags of instances (e.g. patches in whole slide
images, or slices in CT scans), and only bag labels are required for training.
Deep MIL approaches have obtained promising results by aggregating
instance-level representations via an attention mechanism to compute the
bag-level prediction. These methods typically capture both local interactions
among adjacent instances and global, long-range dependencies through various
mechanisms. However, they treat attention values deterministically, potentially
overlooking uncertainty in the contribution of individual instances. In this
work we propose a novel probabilistic framework that estimates a probability
distribution over the attention values, and accounts for both global and local
interactions. In a comprehensive evaluation involving {\color{review} eleven}
state-of-the-art baselines and three medical datasets, we show that our
approach achieves top predictive performance in different metrics. Moreover,
the probabilistic treatment of the attention provides uncertainty maps that are
interpretable in terms of illness localization.

</details>


### [83] [Open-set Cross Modal Generalization via Multimodal Unified Representation](https://arxiv.org/abs/2507.14935)
*Hai Huang,Yan Xia,Shulei Wang,Hanting Wang,Minghui Fang,Shengpeng Ji,Sashuai Zhou,Tao Jin,Zhou Zhao*

Main category: cs.CV

TL;DR: 论文提出开放集跨模态泛化任务（OSCMG）及MICU方法，通过FCMI和CUJP组件增强多模态对齐和特征多样性，实验验证了其在开放集条件下的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态统一表示工作缺乏对开放集环境的考虑，而现实应用中常遇到未见类别的新模态场景。因此，论文提出了更具挑战性的开放集跨模态泛化（OSCMG）任务。

Method: 提出了MICU方法，包含两个关键组件：Fine-Coarse Masked multimodal InfoNCE（FCMI）和Cross modal Unified Jigsaw Puzzles（CUJP）。FCMI通过对比学习在整体语义和时间层面上增强多模态对齐，CUJP通过模态无关特征选择与自监督学习增强特征多样性和模型不确定性。

Result: 在CMG和新提出的OSCMG任务上的大量实验验证了MICU方法的有效性。

Conclusion: 该论文通过提出MICU方法，成功扩展了跨模态泛化（CMG）到开放集环境，解决了现有方法在开放集条件下的局限性，并通过实验验证了其有效性。

Abstract: This paper extends Cross Modal Generalization (CMG) to open-set environments
by proposing the more challenging Open-set Cross Modal Generalization (OSCMG)
task. This task evaluates multimodal unified representations in open-set
conditions, addressing the limitations of prior closed-set cross-modal
evaluations. OSCMG requires not only cross-modal knowledge transfer but also
robust generalization to unseen classes within new modalities, a scenario
frequently encountered in real-world applications. Existing multimodal unified
representation work lacks consideration for open-set environments. To tackle
this, we propose MICU, comprising two key components: Fine-Coarse Masked
multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI
enhances multimodal alignment by applying contrastive learning at both holistic
semantic and temporal levels, incorporating masking to enhance generalization.
CUJP enhances feature diversity and model uncertainty by integrating
modality-agnostic feature selection with self-supervised learning, thereby
strengthening the model's ability to handle unknown categories in open-set
tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the
effectiveness of our approach. The code is available at
https://github.com/haihuangcode/CMG.

</details>


### [84] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: Polymorph是一种动态激活轻量级适配器的框架，显著降低嵌入式设备上的能耗并提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备上的实时多标签视频分类受限于计算和能源预算，但视频流具有标签稀疏性、时间连续性和标签共现性等结构特性，可优化推理效率。

Method: Polymorph采用上下文感知框架，动态激活每帧所需的轻量级低秩适配器（LoRA），避免全模型切换和权重合并。

Result: 在TAO数据集上，Polymorph比强基线降低了40%的能耗，并提高了9个点的mAP。

Conclusion: Polymorph框架通过动态选择和组合轻量级适配器，显著降低了嵌入式设备上的能耗，并提高了分类性能。

Abstract: Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [85] [Decision PCR: Decision version of the Point Cloud Registration task](https://arxiv.org/abs/2507.14965)
*Yaojie Zhang,Tianlun Huang,Weijun Wang,Wei Feng*

Main category: cs.CV

TL;DR: 本文通过深度学习分类器解决低重叠点云配准的评估问题，显著提升现有方法性能，并在多个数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标在极低内点比例下失效，因此需要重新审视配准结果评估问题，并将PCR任务的决策版本作为核心问题。

Method: 通过构建基于3DMatch的数据集，并训练一个深度学习分类器来可靠评估配准质量，克服传统指标的局限性。

Result: 提出的方法显著提升了现有PCR方法的性能，例如与GeoTransformer结合后在3DLoMatch基准测试中达到86.97%的配准召回率，并在未见的室外ETH数据集上表现出强泛化能力。

Conclusion: 本文提出了一种基于深度学习的方法来解决低重叠点云配准（PCR）任务中的评估问题，显著提升了现有最先进PCR方法的性能，并在3DLoMatch基准测试中达到了86.97%的配准召回率。

Abstract: Low-overlap point cloud registration (PCR) remains a significant challenge in
3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become
ineffective under extremely low inlier ratios. In this paper, we revisit the
registration result evaluation problem and identify the Decision version of the
PCR task as the fundamental problem. To address this Decision PCR task, we
propose a data-driven approach. First, we construct a corresponding dataset
based on the 3DMatch dataset. Then, a deep learning-based classifier is trained
to reliably assess registration quality, overcoming the limitations of
traditional metrics. To our knowledge, this is the first comprehensive study to
address this task through a deep learning framework. We incorporate this
classifier into standard PCR pipelines. When integrated with our approach,
existing state-of-the-art PCR methods exhibit significantly enhanced
registration performance. For example, combining our framework with
GeoTransformer achieves a new SOTA registration recall of 86.97\% on the
challenging 3DLoMatch benchmark. Our method also demonstrates strong
generalization capabilities on the unseen outdoor ETH dataset.

</details>


### [86] [Hierarchical Cross-modal Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.14976)
*Hao Zheng,Shunzhi Yang,Zhuoxin He,Jinfeng Yang,Zhenhua Huang*

Main category: cs.CV

TL;DR: HiCroPL通过双向知识流和分层知识映射器，有效解决了预训练视觉语言模型在下游任务中的泛化问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决预训练视觉语言模型在下游任务适应过程中存在的模态隔离和分层语义衰减问题，以提升模型的泛化能力。

Method: 提出了HiCroPL框架，利用分层知识映射器实现文本和视觉模态之间的双向知识流动，并通过轻量级层特定知识代理实现高效跨模态交互。

Result: 在四个任务上的广泛评估表明，HiCroPL在11个基准测试中实现了最先进的性能，并有显著提升。

Conclusion: HiCroPL通过双向知识流和分层知识映射器，显著提升了预训练视觉语言模型在下游任务中的泛化能力，并在11个基准测试中取得了最先进的结果。

Abstract: Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent
generalization abilities. However, adapting these large-scale models to
downstream tasks while preserving their generalization capabilities remains
challenging. Although prompt learning methods have shown promise, they suffer
from two fundamental bottlenecks that limit generalization: (a) modality
isolation, and (b) hierarchical semantic decay. To address these limitations,
we propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that
establishes bidirectional knowledge flow between text and vision modalities,
enabling them to refine their semantics mutually. HiCroPL routes knowledge
flows by leveraging the complementary strengths of text and vision. In early
layers, text prompts inject relatively clear semantics into visual prompts
through a hierarchical knowledge mapper, enhancing the representation of
low-level visual semantics. In later layers, visual prompts encoding specific
task-relevant objects flow back to refine text prompts, enabling deeper
alignment. Crucially, our hierarchical knowledge mapper allows representations
at multi-scales to be fused, ensuring that deeper representations retain
transferable shallow semantics thereby enhancing generalization. We further
introduce a lightweight layer-specific knowledge proxy to enable efficient
cross-modal interactions. Extensive evaluations across four tasks demonstrate
HiCroPL's superior performance, achieving state-of-the-art results on 11
benchmarks with significant improvements. Code is available at:
https://github.com/zzeoZheng/HiCroPL.

</details>


### [87] [Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression](https://arxiv.org/abs/2507.14997)
*Roy H. Jennings,Genady Paikin,Roy Shaul,Evgeny Soloveichik*

Main category: cs.CV

TL;DR: 本文提出RvTC方法，通过灵活的基于分箱的分类回归替代预设词汇分类，结合数据特定提示显著提升多模态回归任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在图像回归任务中存在局限性，预设词汇和通用提示无法有效利用文本输入的语义理解。

Method: 提出了基于Transformer的分类回归方法（RvTC），通过灵活的基于分箱的方法替代了预设词汇分类，避免了手动词汇构建。

Result: 在四个图像评估数据集上实现了最先进的性能，特别是在AVA数据集中，通过添加挑战标题，相关性从0.83提升至0.90。

Conclusion: 本文强调了在多模态回归任务中融入有意义文本上下文的重要性，展示了数据特定提示如何显著提升模型性能。

Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based
regression tasks, but current approaches face key limitations. Recent methods
fine-tune MLLMs using preset output vocabularies and generic task-level prompts
(e.g., "How would you rate this image?"), assuming this mimics human rating
behavior. Our analysis reveals these approaches provide no benefit over
image-only training. Models using preset vocabularies and generic prompts
perform equivalently to image-only models, failing to leverage semantic
understanding from textual input. We propose Regression via Transformer-Based
Classification (RvTC), which replaces vocabulary-constrained classification
with a flexible bin-based approach. Unlike approaches that address
discretization errors through complex distributional modeling, RvTC eliminates
manual vocabulary crafting through straightforward bin increase, achieving
state-of-the-art performance on four image assessment datasets using only
images. More importantly, we demonstrate that data-specific prompts
dramatically improve performance. Unlike generic task descriptions, prompts
containing semantic information about specific images enable MLLMs to leverage
cross-modal understanding. On the AVA dataset, adding challenge titles to
prompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We
demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that
MLLMs benefit from semantic prompt information surpassing mere statistical
biases. This underscores the importance of incorporating meaningful textual
context in multimodal regression tasks.

</details>


### [88] [Axis-Aligned Document Dewarping](https://arxiv.org/abs/2507.15000)
*Chaoyun Wang,I-Chao Shen,Takeo Igarashi,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: 提出一种利用文档固有几何特性的去扭曲方法，通过轴对齐约束和预处理策略提升效果，并在新指标AAD上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法主要依赖带标注数据的监督回归，而未充分利用物理文档中的固有几何特性。

Method: 在训练阶段提出轴对齐几何约束以增强文档去扭曲效果，在推理阶段提出轴对齐预处理策略以降低去扭曲难度，并引入新的评估指标AAD。

Result: 在多个现有基准测试中取得SOTA结果，AAD指标提升18.2%~34.5%。

Conclusion: 该方法在多个现有基准测试中取得了SOTA结果，并在AAD指标上实现了18.2%~34.5%的改进。

Abstract: Document dewarping is crucial for many applications. However, existing
learning-based methods primarily rely on supervised regression with annotated
data without leveraging the inherent geometric properties in physical documents
to the dewarping process. Our key insight is that a well-dewarped document is
characterized by transforming distorted feature lines into axis-aligned ones.
This property aligns with the inherent axis-aligned nature of the discrete grid
geometry in planar documents. In the training phase, we propose an axis-aligned
geometric constraint to enhance document dewarping. In the inference phase, we
propose an axis alignment preprocessing strategy to reduce the dewarping
difficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned
Distortion (AAD), that not only incorporates geometric meaning and aligns with
human visual perception but also demonstrates greater robustness. As a result,
our method achieves SOTA results on multiple existing benchmarks and achieves
18.2%~34.5% improvements on the AAD metric.

</details>


### [89] [FastSmoothSAM: A Fast Smooth Method For Segment Anything Model](https://arxiv.org/abs/2507.15008)
*Jiasheng Xu,Yewang Chen*

Main category: cs.CV

TL;DR: 本文提出一种基于B样条曲线拟合的细化方法，有效解决FastSAM在实时分割中产生的锯齿边缘问题，提升边缘质量和分割精度。


<details>
  <summary>Details</summary>
Motivation: FastSAM虽然实现了实时分割，但生成的边缘存在锯齿状偏差，影响了分割的视觉质量和分析准确性。

Method: 采用四阶段细化流程，包括两轮B样条曲线拟合，以平滑锯齿边缘，同时保留关键几何信息。

Result: 该方法在不牺牲实时处理能力的前提下，显著提高了FastSAM的分割精度和边缘视觉质量。

Conclusion: 本文提出的基于B样条曲线拟合的细化方法显著提升了FastSAM的边缘质量，增强了其在实际应用中的实用性，特别是在需要精确边缘识别的场景中。

Abstract: Accurately identifying and representing object edges is a challenging task in
computer vision and image processing. The Segment Anything Model (SAM) has
significantly influenced the field of image segmentation, but suffers from high
memory consumption and long inference times, limiting its efficiency in
real-time applications. To address these limitations, Fast Segment Anything
(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM
often generates jagged edges that deviate from the true object shapes.
Therefore, this paper introduces a novel refinement approach using B-Spline
curve fitting techniques to enhance the edge quality in FastSAM. Leveraging the
robust shape control and flexible geometric construction of B-Splines, a
four-stage refining process involving two rounds of curve fitting is employed
to effectively smooth jagged edges. This approach significantly improves the
visual quality and analytical accuracy of object edges without compromising
critical geometric information. The proposed method improves the practical
utility of FastSAM by improving segmentation accuracy while maintaining
real-time processing capabilities. This advancement unlocks greater potential
for FastSAM technology in various real-world scenarios, such as industrial
automation, medical imaging, and autonomous systems, where precise and
efficient edge recognition is crucial.

</details>


### [90] [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/abs/2507.15028)
*Yuanhan Zhang,Yunice Chew,Yuhao Dong,Aria Leo,Bo Hu,Ziwei Liu*

Main category: cs.CV

TL;DR: Video-TT 是一个新基准测试，用于评估视频大语言模型在复杂视觉叙事和对抗性问题上的表现，结果显示其与人类智能存在差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能充分反映视频大语言模型在保持正确性和鲁棒性方面与人类智能的差距，因此需要一个新的评估工具。

Method: 研究者提出了 Video-TT 测试，包含 1,000 个 YouTube Shorts 视频，每个视频配有一个开放式问题和四个对抗性问题，以评估模型在视觉和叙事复杂性上的表现。

Result: 评估结果显示，视频大语言模型在 Video-TT 上的表现与人类存在显著差距。

Conclusion: Video-TT 是一个有效的工具，揭示了视频大语言模型在理解复杂视觉叙事和对抗性问题上的表现与人类存在显著差距。

Abstract: Human intelligence requires correctness and robustness, with the former being
foundational for the latter. In video understanding, correctness ensures the
accurate interpretation of visual content, and robustness maintains consistent
performance in challenging conditions. Despite advances in video large language
models (video LLMs), existing benchmarks inadequately reflect the gap between
these models and human intelligence in maintaining correctness and robustness
in video interpretation. We introduce the Video Thinking Test (Video-TT), to
assess if video LLMs can interpret real-world videos as effectively as humans.
Video-TT reflects genuine gaps in understanding complex visual narratives, and
evaluates robustness against natural adversarial questions. Video-TT comprises
1,000 YouTube Shorts videos, each with one open-ended question and four
adversarial questions that probe visual and narrative complexity. Our
evaluation shows a significant gap between video LLMs and human performance.

</details>


### [91] [OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography](https://arxiv.org/abs/2507.15035)
*Zhijun Zeng,Youjia Zheng,Hao Hu,Zeyuan Dong,Yihang Zheng,Xinliang Liu,Jinzhuo Wang,Zuoqiang Shi,Linfeng Zhang,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: OpenBreastUS是一个大规模波方程数据集，用于评估神经算子的性能，首次展示了其在活体乳腺成像中的高效应用。


<details>
  <summary>Details</summary>
Motivation: 传统波方程数值求解器计算量大且不稳定，限制了准实时成像的应用；现有神经算子数据集过于简化，无法满足实际成像需求。

Method: 提出了OpenBreastUS数据集，包含8000个解剖学上真实的人类乳腺模型和1600万次频域波模拟，用于评估神经算子的性能和泛化能力。

Result: OpenBreastUS为神经算子的开发和实际医学成像问题部署提供了平台，并首次实现了神经算子在人体乳腺活体成像中的高效应用。

Conclusion: OpenBreastUS数据集为神经算子提供了现实且广泛的基准测试平台，首次展示了其在人体乳腺活体成像中的高效应用。

Abstract: Accurate and efficient simulation of wave equations is crucial in
computational wave imaging applications, such as ultrasound computed tomography
(USCT), which reconstructs tissue material properties from observed scattered
waves. Traditional numerical solvers for wave equations are computationally
intensive and often unstable, limiting their practical applications for
quasi-real-time image reconstruction. Neural operators offer an innovative
approach by accelerating PDE solving using neural networks; however, their
effectiveness in realistic imaging is limited because existing datasets
oversimplify real-world complexity. In this paper, we present OpenBreastUS, a
large-scale wave equation dataset designed to bridge the gap between
theoretical equations and practical imaging applications. OpenBreastUS includes
8,000 anatomically realistic human breast phantoms and over 16 million
frequency-domain wave simulations using real USCT configurations. It enables a
comprehensive benchmarking of popular neural operators for both forward
simulation and inverse imaging tasks, allowing analysis of their performance,
scalability, and generalization capabilities. By offering a realistic and
extensive dataset, OpenBreastUS not only serves as a platform for developing
innovative neural PDE solvers but also facilitates their deployment in
real-world medical imaging problems. For the first time, we demonstrate
efficient in vivo imaging of the human breast using neural operator solvers.

</details>


### [92] [OmniVTON: Training-Free Universal Virtual Try-On](https://arxiv.org/abs/2507.15037)
*Zhaotong Yang,Yuhui Li,Shengfeng He,Xinzhe Li,Yangyang Xu,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: OmniVTON是一种无需训练的通用虚拟试穿框架，通过解耦服装和姿态条件，实现了高保真和多人物试穿。


<details>
  <summary>Details</summary>
Motivation: 现有监督式和非监督式VTON方法在跨域泛化和数据偏差方面存在局限，需要一个统一的、无需训练的解决方案。

Method: 提出了一种无需训练的通用VTON框架，结合服装先验生成机制和连续边界缝合技术，以及DDIM反演实现精确姿态对齐。

Result: 实验证明OmniVTON在多种数据集、服装类型和应用场景中表现优异，首次实现了多人物虚拟试穿。

Conclusion: OmniVTON通过解耦服装和姿态条件，实现了跨场景的高保真虚拟试穿，解决了现有方法的局限性，并在多人物试穿中展现了首次应用能力。

Abstract: Image-based Virtual Try-On (VTON) techniques rely on either supervised
in-shop approaches, which ensure high fidelity but struggle with cross-domain
generalization, or unsupervised in-the-wild methods, which improve adaptability
but remain constrained by data biases and limited universality. A unified,
training-free solution that works across both scenarios remains an open
challenge. We propose OmniVTON, the first training-free universal VTON
framework that decouples garment and pose conditioning to achieve both texture
fidelity and pose consistency across diverse settings. To preserve garment
details, we introduce a garment prior generation mechanism that aligns clothing
with the body, followed by continuous boundary stitching technique to achieve
fine-grained texture retention. For precise pose alignment, we utilize DDIM
inversion to capture structural cues while suppressing texture interference,
ensuring accurate body alignment independent of the original image textures. By
disentangling garment and pose constraints, OmniVTON eliminates the bias
inherent in diffusion models when handling multiple conditions simultaneously.
Experimental results demonstrate that OmniVTON achieves superior performance
across diverse datasets, garment types, and application scenarios. Notably, it
is the first framework capable of multi-human VTON, enabling realistic garment
transfer across multiple individuals in a single scene. Code is available at
https://github.com/Jerome-Young/OmniVTON

</details>


### [93] [Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling](https://arxiv.org/abs/2507.15059)
*Ran Zhang,Xuanhua He,Li Xueheng,Ke Cao,Liu Liu,Wenbo Xu,Fang Jiabin,Yang Qize,Jie Zhang*

Main category: cs.CV

TL;DR: 本文提出轻量级pan-sharpening框架PanTiny，通过多数据集联合训练和复合损失函数，显著提升泛化能力和性能效率，优于大型模型。


<details>
  <summary>Details</summary>
Motivation: 当前pan-sharpening领域倾向于使用大型复杂模型，但存在计算开销大、泛化能力差的问题。本文旨在挑战这一范式，提出更高效、通用的解决方案。

Method: 提出PanTiny框架，采用多数据集（WV2、WV3、GF2）联合训练策略，并设计了一种通用的复合损失函数。

Result: PanTiny在性能和效率上均优于大多数大型专用模型，并通过消融实验验证了模型设计、训练范式和损失函数的重要性。

Conclusion: 本文提出了一种轻量级、单步的pan-sharpening框架PanTiny，通过多数据集联合训练和复合损失函数的设计，显著提升了模型的泛化能力和性能效率平衡，为pan-sharpening领域的高效、通用模型设计提供了新方向。

Abstract: The field of pan-sharpening has recently seen a trend towards increasingly
large and complex models, often trained on single, specific satellite datasets.
This approach, however, leads to high computational overhead and poor
generalization on full resolution data, a paradigm we challenge in this paper.
In response to this issue, we propose PanTiny, a lightweight, single-step
pan-sharpening framework designed for both efficiency and robust performance.
More critically, we introduce multiple-in-one training paradigm, where a
single, compact model is trained simultaneously on three distinct satellite
datasets (WV2, WV3, and GF2) with different resolution and spectral
information. Our experiments show that this unified training strategy not only
simplifies deployment but also significantly boosts generalization on
full-resolution data. Further, we introduce a universally powerful composite
loss function that elevates the performance of almost all of models for
pan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny
model, benefiting from these innovations, achieves a superior
performance-to-efficiency balance, outperforming most larger, specialized
models. Through extensive ablation studies, we validate that principled
engineering in model design, training paradigms, and loss functions can surpass
brute-force scaling. Our work advocates for a community-wide shift towards
creating efficient, generalizable, and data-conscious models for
pan-sharpening. The code is available at
https://github.com/Zirconium233/PanTiny .

</details>


### [94] [Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR](https://arxiv.org/abs/2507.15085)
*Peirong Zhang,Haowei Xu,Jiaxin Zhang,Guitao Xu,Xuhan Zheng,Zhenhua Yang,Junle Liu,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: 本文评估了六种生成模型在文本图像生成和编辑中的表现，揭示了其弱点，并建议将此类任务作为通用生成模型的基础技能。


<details>
  <summary>Details</summary>
Motivation: 探讨当前最先进的生成模型是否能掌握文本图像生成和编辑的复杂性，并评估其在OCR任务中的表现。

Method: 评估了六种最先进的生成模型（包括闭源和开源领域），使用33个代表性任务，分为五类：文档、手写文本、场景文本、艺术文本及复杂与布局丰富的文本。

Result: 通过评估揭示了当前生成模型在OCR任务中的弱点，并提出了关键观察。

Conclusion: 本文认为，将逼真的文本图像生成和编辑作为通用生成模型的基础技能，而非依赖专用解决方案，是未来发展的方向。

Abstract: Text image is a unique and crucial information medium that integrates visual
aesthetics and linguistic semantics in modern e-society. Due to their subtlety
and complexity, the generation of text images represents a challenging and
evolving frontier in the image generation field. The recent surge of
specialized image generators (\emph{e.g.}, Flux-series) and unified generative
models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a
natural question: can they master the intricacies of text image generation and
editing? Motivated by this, we assess current state-of-the-art generative
models' capabilities in terms of text image generation and editing. We
incorporate various typical optical character recognition (OCR) tasks into our
evaluation and broaden the concept of text-based generation tasks into OCR
generative tasks. We select 33 representative tasks and categorize them into
five categories: document, handwritten text, scene text, artistic text, and
complex \& layout-rich text. For comprehensive evaluation, we examine six
models across both closed-source and open-source domains, using tailored,
high-quality image inputs and prompts. Through this evaluation, we draw crucial
observations and identify the weaknesses of current generative models for OCR
tasks. We argue that photorealistic text image generation and editing should be
internalized as foundational skills into general-domain generative models,
rather than being delegated to specialized solutions, and we hope this
empirical analysis can provide valuable insights for the community to achieve
this goal. This evaluation is online and will be continuously updated at our
GitHub repository.

</details>


### [95] [LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM](https://arxiv.org/abs/2507.15109)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Sorin Grigorescu*

Main category: cs.CV

TL;DR: LoopNet通过多任务ResNet和DISK描述符提升SLAM闭环检测性能，并发布LoopDB数据集。


<details>
  <summary>Details</summary>
Motivation: 解决SLAM闭环检测中的准确性和实时计算约束问题。

Method: 采用多任务ResNet架构，支持动态视觉数据集的在线重训练，优化嵌入式设备性能，结合DISK描述符替代传统手工特征。

Result: LoopNet在多变条件下表现优于传统深度学习和手工特征方法。

Conclusion: LoopNet通过多任务ResNet架构和在线动态重训练，结合DISK描述符，显著提升了SLAM闭环检测的准确性和实时性，并发布了新的基准数据集LoopDB。

Abstract: One of the main challenges in the Simultaneous Localization and Mapping
(SLAM) loop closure problem is the recognition of previously visited places. In
this work, we tackle the two main problems of real-time SLAM systems: 1) loop
closure detection accuracy and 2) real-time computation constraints on the
embedded hardware. Our LoopNet method is based on a multitasking variant of the
classical ResNet architecture, adapted for online retraining on a dynamic
visual dataset and optimized for embedded devices. The online retraining is
designed using a few-shot learning approach. The architecture provides both an
index into the queried visual dataset, and a measurement of the prediction
quality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,
LoopNet surpasses the limitations of handcrafted features and traditional deep
learning methods, offering better performance under varying conditions. Code is
available at https://github.com/RovisLab/LoopNet. Additinally, we introduce a
new loop closure benchmarking dataset, coined LoopDB, which is available at
https://github.com/RovisLab/LoopDB.

</details>


### [96] [Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction](https://arxiv.org/abs/2507.15130)
*Ce Zhang,Yale Song,Ruta Desai,Michael Louis Iuzzolino,Joseph Tighe,Gedas Bertasius,Satwik Kottur*

Main category: cs.CV

TL;DR: VideoPlan通过辅助任务增强和多令牌预测，解决了视觉规划中的数据稀缺和动作空间建模问题，在多个数据集上实现了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在视频理解方面取得了进展，但长期视觉规划仍是一个具有挑战性的问题，主要由于程序性注释的稀缺性和传统下一个令牌预测目标在捕捉结构化动作空间时的低效性。

Method: 为了解决数据稀缺问题，作者引入了辅助任务增强（Auxiliary Task Augmentation），并设计了多令牌预测（Multi-token Prediction）来更明确地建模视觉规划任务的结构化动作空间。

Result: VideoPlan在COIN和CrossTask数据集上预测3个未来动作时，性能分别提升了7.3%和3.4%，并在Ego4D任务中表现优异。

Conclusion: VideoPlan方法在COIN和CrossTask数据集上实现了最先进的VPA性能，分别比之前的方法提高了7.3%和3.4%，并且在Ego4D长期动作预测任务中表现与现有最佳方法相当。

Abstract: Visual Planning for Assistance (VPA) aims to predict a sequence of user
actions required to achieve a specified goal based on a video showing the
user's progress. Although recent advances in multimodal large language models
(MLLMs) have shown promising results in video understanding, long-horizon
visual planning remains a challenging problem. We identify two challenges in
training large MLLMs for video-based planning tasks: (1) scarcity of procedural
annotations, limiting the model's ability to learn procedural task dynamics
effectively, and (2) inefficiency of next-token prediction objective to
explicitly capture the structured action space for visual planning when
compared to free-form, natural language. To tackle data scarcity, we introduce
Auxiliary Task Augmentation. We design and train our model on auxiliary tasks
relevant to long-horizon video-based planning (e.g., goal prediction) to
augment the model's planning ability. To more explicitly model the structured
action space unique to visual planning tasks, we leverage Multi-token
Prediction, extending traditional next-token prediction by using multiple heads
to predict multiple future tokens during training. Our approach, VideoPlan,
achieves state-of-the-art VPA performance on the COIN and CrossTask datasets,
surpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3
future actions. We further extend our method to the challenging Ego4D Long-term
Action Anticipation task, and show that it is on par with the state-of-the-art
approaches despite not using specialized egocentric features. Code will be made
available.

</details>


### [97] [Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection](https://arxiv.org/abs/2507.15150)
*Aayush Atul Verma,Arpitsinh Vaghela,Bharatesh Chakravarthi,Kaustav Chanda,Yezhou Yang*

Main category: cs.CV

TL;DR: 论文提出一种时空多图表示方法，通过解耦空间和时间图建模，显著提升事件传感器的检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 事件传感器生成稀疏异步数据，传统方法将其转换为密集张量会削弱其固有优势。现有图表示方法虽保留稀疏性，但对时空动态建模不足，限制了性能。

Method: 提出了一种时空多图表示方法，包括利用B样条基函数建模全局空间结构的空间图和基于运动向量注意力机制建模局部动态变化的时间图，替代了传统的高计算成本3D核。

Result: 在Gen1 automotive和eTraM数据集上，检测精度提升超过6%，速度提升5倍，参数减少且计算成本未增加。

Conclusion: 论文提出了一种新颖的时空多图表示方法，通过解耦空间和时间图建模，显著提升了事件传感器的性能，同时在检测精度、速度和计算效率上均有显著改进。

Abstract: Event-based sensors offer high temporal resolution and low latency by
generating sparse, asynchronous data. However, converting this irregular data
into dense tensors for use in standard neural networks diminishes these
inherent advantages, motivating research into graph representations. While such
methods preserve sparsity and support asynchronous inference, their performance
on downstream tasks remains limited due to suboptimal modeling of
spatiotemporal dynamics. In this work, we propose a novel spatiotemporal
multigraph representation to better capture spatial structure and temporal
changes. Our approach constructs two decoupled graphs: a spatial graph
leveraging B-spline basis functions to model global structure, and a temporal
graph utilizing motion vector-based attention for local dynamic changes. This
design enables the use of efficient 2D kernels in place of computationally
expensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM
datasets for event-based object detection, achieving over a 6% improvement in
detection accuracy compared to previous graph-based works, with a 5x speedup,
reduced parameter count, and no increase in computational cost. These results
highlight the effectiveness of structured graph modeling for asynchronous
vision. Project page: eventbasedvision.github.io/eGSMV.

</details>


### [98] [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/abs/2507.15212)
*Yusuke Yoshiyasu,Leyuan Sun,Ryusuke Sagawa*

Main category: cs.CV

TL;DR: MeshMamba 利用 Mamba-SSMs 高效学习 3D 关节网格，其衍生模型 MambaDiff3D 和 Mamba-HMR 分别在生成和重建任务中表现优异，支持大规模顶点和细节捕捉。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理大规模 3D 关节网格时效率不足，且无法有效捕捉衣物和手部几何细节。MeshMamba 旨在通过 Mamba-SSMs 解决这些问题。

Method: MeshMamba 通过将网格顶点序列化为易于 Mamba 处理的顺序（基于身体部位注释或模板网格的 3D 顶点位置），实现了对大规模输入的高效处理。基于此设计了 MambaDiff3D（去噪扩散模型）和 Mamba-HMR（单图像人体网格恢复模型）。

Result: MambaDiff3D 能够生成带衣物和手部细节的密集 3D 人体网格，在生成任务中优于现有方法；Mamba-HMR 扩展了非参数化人体网格恢复的能力，支持全身（包括面部和手部）重建，并实现接近实时的性能。

Conclusion: MeshMamba 通过 Mamba-SSMs 实现了高效且可扩展的 3D 关节网格模型学习，其衍生的 MambaDiff3D 和 Mamba-HMR 分别在 3D 人体网格生成和单图像人体网格重建任务中表现出色，超越了现有方法。

Abstract: In this paper, we introduce MeshMamba, a neural network model for learning 3D
articulated mesh models by employing the recently proposed Mamba State Space
Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large
number of input tokens, enabling the generation and reconstruction of body mesh
models with more than 10,000 vertices, capturing clothing and hand geometries.
The key to effectively learning MeshMamba is the serialization technique of
mesh vertices into orderings that are easily processed by Mamba. This is
achieved by sorting the vertices based on body part annotations or the 3D
vertex locations of a template mesh, such that the ordering respects the
structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,
a denoising diffusion model for generating 3D articulated meshes and 2)
Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape
and pose from a single image. Experimental results showed that MambaDiff3D can
generate dense 3D human meshes in clothes, with grasping hands, etc., and
outperforms previous approaches in the 3D human shape generation task.
Additionally, Mamba-HMR extends the capabilities of previous non-parametric
human mesh recovery approaches, which were limited to handling body-only poses
using around 500 vertex tokens, to the whole-body setting with face and hands,
while achieving competitive performance in (near) real-time.

</details>


### [99] [Improving Joint Embedding Predictive Architecture with Diffusion Noise](https://arxiv.org/abs/2507.15216)
*Yuping Qiu,Rui Zhu,Ying-cong Chen*

Main category: cs.CV

TL;DR: 结合扩散模型噪声与自监督学习，提出N-JEPA方法，通过噪声增强MIM，提升分类任务表现。


<details>
  <summary>Details</summary>
Motivation: 探索自监督学习（SSL）与生成模型的结合，以增强SSL的表征能力，特别是在识别任务中的语义理解。

Method: 提出N-JEPA方法，将扩散噪声通过位置嵌入融入掩码图像建模（MIM），并采用多级噪声调度作为特征增强手段。

Result: 在分类任务中验证了N-JEPA的有效性，表明其能提升模型鲁棒性。

Conclusion: 通过结合扩散模型的噪声原理与自监督学习（SSL），提出了N-JEPA方法，显著提升了SSL在分类任务中的表现。

Abstract: Self-supervised learning has become an incredibly successful method for
feature learning, widely applied to many downstream tasks. It has proven
especially effective for discriminative tasks, surpassing the trending
generative models. However, generative models perform better in image
generation and detail enhancement. Thus, it is natural for us to find a
connection between SSL and generative models to further enhance the
representation capacity of SSL. As generative models can create new samples by
approximating the data distribution, such modeling should also lead to a
semantic understanding of the raw visual data, which is necessary for
recognition tasks. This enlightens us to combine the core principle of the
diffusion model: diffusion noise, with SSL to learn a competitive recognition
model. Specifically, diffusion noise can be viewed as a particular state of
mask that reveals a close relationship between masked image modeling (MIM) and
diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to
incorporate diffusion noise into MIM by the position embedding of masked
tokens. The multi-level noise schedule is a series of feature augmentations to
further enhance the robustness of our model. We perform a comprehensive study
to confirm its effectiveness in the classification of downstream tasks. Codes
will be released soon in public.

</details>


### [100] [Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel](https://arxiv.org/abs/2507.15223)
*Siqi Chen,Guoqing Zhang,Jiahao Lai,Bingzhi Shen,Sihong Zhang,Caixia Dong,Xuejin Chen,Yang Li*

Main category: cs.CV

TL;DR: 该研究提出了一种分层部分框架，用于3D血管生成，通过分离全局拓扑和局部几何细节，有效模拟复杂血管网络，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管3D视觉技术的进步增强了血管建模在医学应用中的影响力，但由于血管具有复杂的几何和拓扑结构，准确表示其分支模式、曲率和不规则形状仍然是一个挑战。

Method: 研究提出了一种分层部分框架，分为三个阶段：关键图生成、基于几何特性的血管段生成以及根据全局关键图整合局部段的分层血管组装。

Result: 在真实数据集上的验证表明，该方法在模拟复杂血管网络方面优于现有方法。

Conclusion: 本研究提出的分层部分框架在3D血管建模中首次成功应用了基于部分的生成方法，为血管数据生成设立了新标准。

Abstract: Advancements in 3D vision have increased the impact of blood vessel modeling
on medical applications. However, accurately representing the complex geometry
and topology of blood vessels remains a challenge due to their intricate
branching patterns, curvatures, and irregular shapes. In this study, we propose
a hierarchical part-based frame work for 3D vessel generation that separates
the global binary tree-like topology from local geometric details. Our approach
proceeds in three stages: (1) key graph generation to model the overall
hierarchical struc ture, (2) vessel segment generation conditioned on geometric
properties, and (3) hierarchical vessel assembly by integrating the local
segments according to the global key graph. We validate our framework on real
world datasets, demonstrating superior performance over existing methods in
modeling complex vascular networks. This work marks the first successful
application of a part-based generative approach for 3D vessel modeling, setting
a new benchmark for vascular data generation. The code is available at:
https://github.com/CybercatChen/PartVessel.git.

</details>


### [101] [Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders](https://arxiv.org/abs/2507.15227)
*Krishna Kanth Nakka*

Main category: cs.CV

TL;DR: 通过稀疏自动编码器（SAE）分析乳腺影像基础模型，揭示了潜在特征与临床概念的关联，并识别了影响决策的混杂因素。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域如医学影像中，模型决策的可解释性对临床采用至关重要。

Method: 通过训练基于Mammo-CLIP的Mammo-SAE，识别和探测与临床相关乳腺概念（如肿块和可疑钙化）相关的潜在特征。

Result: 研究发现，SAE潜在空间中激活程度最高的神经元通常与真实区域对齐，并揭示了影响模型决策的多个混杂因素。此外，还分析了模型在下游微调中依赖的潜在神经元。

Conclusion: 本研究展示了稀疏自动编码器（SAE）在乳腺影像中的可解释性潜力，为理解基础模型在乳腺影像中的内部工作机制提供了新视角。

Abstract: Interpretability is critical in high-stakes domains such as medical imaging,
where understanding model decisions is essential for clinical adoption. In this
work, we introduce Sparse Autoencoder (SAE)-based interpretability to breast
imaging by analyzing {Mammo-CLIP}, a vision--language foundation model
pretrained on large-scale mammogram image--report pairs. We train a patch-level
\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features
associated with clinically relevant breast concepts such as \textit{mass} and
\textit{suspicious calcification}. Our findings reveal that top activated class
level latent neurons in the SAE latent space often tend to align with ground
truth regions, and also uncover several confounding factors influencing the
model's decision-making process. Additionally, we analyze which latent neurons
the model relies on during downstream finetuning for improving the breast
concept prediction. This study highlights the promise of interpretable SAE
latent representations in providing deeper insight into the internal workings
of foundation models at every layer for breast imaging.

</details>


### [102] [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/abs/2507.15249)
*Yanbing Zhang,Zhe Wang,Qin Zhou,Mengping Yang*

Main category: cs.CV

TL;DR: FreeCus是一种无需训练的框架，通过注意力共享、动态偏移分析和MLLMs集成，激活扩散变换器的零样本能力，实现高保真主题驱动合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖训练过程，限制了实际应用，且未能充分利用现代扩散变换器的零样本潜力。

Method: 提出了FreeCus框架，通过三种关键创新：1) 注意力共享机制；2) 动态偏移分析的升级变体；3) 多模态大语言模型（MLLMs）的集成。

Result: 实验表明，FreeCus成功解锁了DiT的零样本能力，实现了跨多样上下文的一致主题合成，并与现有修复流程和控制模块无缝兼容。

Conclusion: FreeCus成功激活了扩散变换器（DiT）的零样本能力，实现了无需额外训练的高保真主题驱动合成，并在实验中取得了与需要训练的方法相媲美的结果。

Abstract: In light of recent breakthroughs in text-to-image (T2I) generation,
particularly with diffusion transformers (DiT), subject-driven technologies are
increasingly being employed for high-fidelity customized production that
preserves subject identity from reference inputs, enabling thrilling design
workflows and engaging entertainment. Existing alternatives typically require
either per-subject optimization via trainable text embeddings or training
specialized encoders for subject feature extraction on large-scale datasets.
Such dependencies on training procedures fundamentally constrain their
practical applications. More importantly, current methodologies fail to fully
leverage the inherent zero-shot potential of modern diffusion transformers
(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this
gap, we propose FreeCus, a genuinely training-free framework that activates
DiT's capabilities through three key innovations: 1) We introduce a pivotal
attention sharing mechanism that captures the subject's layout integrity while
preserving crucial editing flexibility. 2) Through a straightforward analysis
of DiT's dynamic shifting, we propose an upgraded variant that significantly
improves fine-grained feature extraction. 3) We further integrate advanced
Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic
representations. Extensive experiments reflect that our method successfully
unlocks DiT's zero-shot ability for consistent subject synthesis across diverse
contexts, achieving state-of-the-art or comparable results compared to
approaches that require additional training. Notably, our framework
demonstrates seamless compatibility with existing inpainting pipelines and
control modules, facilitating more compelling experiences. Our code is
available at: https://github.com/Monalissaa/FreeCus.

</details>


### [103] [MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP](https://arxiv.org/abs/2507.15257)
*Pei An,Jiaqi Yang,Muyao Peng,You Yang,Qiong Liu,Xiaolin Wu,Liangliang Nan*

Main category: cs.CV

TL;DR: 提出MinCD-PnP方法，通过简化盲PnP并设计MinCD-Net模块，显著提升了I2P注册的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 差分PnP对噪声和异常值高度敏感，影响了对应学习的有效性。受盲PnP在噪声和异常值中的鲁棒性启发，提出了近似盲PnP的方法。

Method: 提出了一种基于近似盲PnP的对应学习方法MinCD-PnP，并通过设计轻量级多任务学习模块MinCD-Net来有效解决该问题。

Result: 在7-Scenes、RGBD-V2、ScanNet和自收集数据集上的广泛实验表明，MinCD-Net表现优异。

Conclusion: MinCD-Net在跨场景和跨数据集设置中均优于现有方法，实现了更高的内点率（IR）和注册召回率（RR）。

Abstract: Image-to-point-cloud (I2P) registration is a fundamental problem in computer
vision, focusing on establishing 2D-3D correspondences between an image and a
point cloud. The differential perspective-n-point (PnP) has been widely used to
supervise I2P registration networks by enforcing the projective constraints on
2D-3D correspondences. However, differential PnP is highly sensitive to noise
and outliers in the predicted correspondences. This issue hinders the
effectiveness of correspondence learning. Inspired by the robustness of blind
PnP against noise and outliers in correspondences, we propose an approximated
blind PnP based correspondence learning approach. To mitigate the high
computational cost of blind PnP, we simplify blind PnP to an amenable task of
minimizing Chamfer distance between learned 2D and 3D keypoints, called
MinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task
learning module, named as MinCD-Net, which can be easily integrated into the
existing I2P registration architectures. Extensive experiments on 7-Scenes,
RGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net
outperforms state-of-the-art methods and achieves a higher inlier ratio (IR)
and registration recall (RR) in both cross-scene and cross-dataset settings.

</details>


### [104] [In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems](https://arxiv.org/abs/2507.15285)
*Lazaro Janier Gonzalez-Soler,Maciej Salwowski,Christoph Busch*

Main category: cs.CV

TL;DR: 本研究提出了一种基于VLM的上下文学习框架，用于生物识别系统中的攻击检测，实验证明其性能优于传统方法且无需大量训练。


<details>
  <summary>Details</summary>
Motivation: 随着生物识别系统检测方法的进步，攻击技术也日益复杂。传统深度学习模型在适应不同类型攻击或环境变化时表现不佳，且需要大量训练数据。本研究旨在探索VLM在攻击检测中的潜力。

Method: 提出了一种基于开源视觉语言模型（VLM）的上下文学习框架，用于检测生物识别系统中的物理呈现攻击和数字变形攻击。

Result: 实验结果表明，所提出的框架在物理和数字攻击检测中表现优异，优于部分传统CNN模型，且无需大量训练资源。

Conclusion: 本研究验证了基于视觉语言模型（VLM）的上下文学习框架在生物识别系统中的有效性，尤其在物理和数字攻击检测方面表现出色，且无需资源密集型训练。

Abstract: Recent advances in biometric systems have significantly improved the
detection and prevention of fraudulent activities. However, as detection
methods improve, attack techniques become increasingly sophisticated. Attacks
on face recognition systems can be broadly divided into physical and digital
approaches. Traditionally, deep learning models have been the primary defence
against such attacks. While these models perform exceptionally well in
scenarios for which they have been trained, they often struggle to adapt to
different types of attacks or varying environmental conditions. These
subsystems require substantial amounts of training data to achieve reliable
performance, yet biometric data collection faces significant challenges,
including privacy concerns and the logistical difficulties of capturing diverse
attack scenarios under controlled conditions. This work investigates the
application of Vision Language Models (VLM) and proposes an in-context learning
framework for detecting physical presentation attacks and digital morphing
attacks in biometric systems. Focusing on open-source models, the first
systematic framework for the quantitative evaluation of VLMs in
security-critical scenarios through in-context learning techniques is
established. The experimental evaluation conducted on freely available
databases demonstrates that the proposed subsystem achieves competitive
performance for physical and digital attack detection, outperforming some of
the traditional CNNs without resource-intensive training. The experimental
results validate the proposed framework as a promising tool for improving
generalisation in attack detection.

</details>


### [105] [Minutiae-Anchored Local Dense Representation for Fingerprint Matching](https://arxiv.org/abs/2507.15297)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: DMD是一种基于细节点锚定的局部密集表示方法，通过三维张量捕获指纹的细粒度特征，实验证明其在多种指纹数据集上具有最先进的识别性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决指纹匹配在不同采集条件下的鲁棒性和准确性挑战。

Method: 提出DMD，一种基于细节点的局部密集表示方法，通过从每个检测到的细节点中心提取局部补丁描述符，形成三维张量，捕获细粒度脊线纹理和细节点特征。

Result: 在滚动、平面、部分、非接触和潜在指纹数据集上的广泛实验证明了该方法的有效性和泛化能力。

Conclusion: DMD方法在多种指纹数据集上表现出色，实现了最先进的准确性，同时保持了计算效率，显示出在大规模指纹识别中的强大潜力。

Abstract: Fingerprint matching under diverse capture conditions remains a fundamental
challenge in biometric recognition. To achieve robust and accurate performance
in such scenarios, we propose DMD, a minutiae-anchored local dense
representation which captures both fine-grained ridge textures and
discriminative minutiae features in a spatially structured manner.
Specifically, descriptors are extracted from local patches centered and
oriented on each detected minutia, forming a three-dimensional tensor, where
two dimensions represent spatial locations on the fingerprint plane and the
third encodes semantic features. This representation explicitly captures
abstract features of local image patches, enabling a multi-level, fine-grained
description that aggregates information from multiple minutiae and their
surrounding ridge structures. Furthermore, thanks to its strong spatial
correspondence with the patch image, DMD allows for the use of foreground
segmentation masks to identify valid descriptor regions. During matching,
comparisons are then restricted to overlapping foreground areas, improving
efficiency and robustness. Extensive experiments on rolled, plain, parital,
contactless, and latent fingerprint datasets demonstrate the effectiveness and
generalizability of the proposed method. It achieves state-of-the-art accuracy
across multiple benchmarks while maintaining high computational efficiency,
showing strong potential for large-scale fingerprint recognition. Corresponding
code is available at https://github.com/Yu-Yy/DMD.

</details>


### [106] [Few-Shot Object Detection via Spatial-Channel State Space Model](https://arxiv.org/abs/2507.15308)
*Zhimeng Xin,Tianxu Wu,Yixiong Zou,Shiming Chen,Dingjie Fu,Xinge You*

Main category: cs.CV

TL;DR: 提出SCSM模块，利用空间-通道状态建模解决FSOD中的通道特征提取问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决少样本目标检测（FSOD）中由于训练样本有限，现有方法难以准确提取有效通道特征的问题。

Method: 提出了一个空间-通道状态建模（SCSM）模块，包括用于空间特征建模的SFM模块和基于Mamba的CSM模块，以学习通道间的相关性。

Result: SCSM模块能够有效突出特征通道中的有效模式并纠正无效模式，提升了检测性能。

Conclusion: SCSM模块通过在VOC和COCO数据集上的广泛实验，证明了其能够提升新检测器在通道中聚焦特征表示的质量，并实现最先进的性能。

Abstract: Due to the limited training samples in few-shot object detection (FSOD), we
observe that current methods may struggle to accurately extract effective
features from each channel. Specifically, this issue manifests in two aspects:
i) channels with high weights may not necessarily be effective, and ii)
channels with low weights may still hold significant value. To handle this
problem, we consider utilizing the inter-channel correlation to facilitate the
novel model's adaptation process to novel conditions, ensuring the model can
correctly highlight effective channels and rectify those incorrect ones. Since
the channel sequence is also 1-dimensional, its similarity with the temporal
sequence inspires us to take Mamba for modeling the correlation in the channel
sequence. Based on this concept, we propose a Spatial-Channel State Space
Modeling (SCSM) module for spatial-channel state modeling, which highlights the
effective patterns and rectifies those ineffective ones in feature channels. In
SCSM, we design the Spatial Feature Modeling (SFM) module to balance the
learning of spatial relationships and channel relationships, and then introduce
the Channel State Modeling (CSM) module based on Mamba to learn correlation in
channels. Extensive experiments on the VOC and COCO datasets show that the SCSM
module enables the novel detector to improve the quality of focused feature
representation in channels and achieve state-of-the-art performance.

</details>


### [107] [BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?](https://arxiv.org/abs/2507.15321)
*Zhenyu Li,Haotong Lin,Jiashi Feng,Peter Wonka,Bingyi Kang*

Main category: cs.CV

TL;DR: 提出了BenchDepth基准，通过五个下游任务评估深度基础模型，解决了传统评估中的偏差问题，并分析了八种DFMs的表现。


<details>
  <summary>Details</summary>
Motivation: 当前深度基础模型的评估存在不一致性，传统基准测试依赖的基于对齐的指标引入了偏差，且难以进行公平比较。

Method: 通过精心选择的五个下游代理任务（深度补全、立体匹配、单目前馈3D场景重建、SLAM和视觉语言空间理解）评估DFMs，避免了传统基于对齐的指标带来的问题。

Result: 对八种最先进的DFMs进行了基准测试，并提供了关键发现和观察的深入分析。

Conclusion: 本文提出了BenchDepth这一新基准，旨在通过五个下游代理任务评估深度基础模型（DFMs），避免了传统评估协议中的偏差和对齐问题，为深度估计领域的未来研究和进展铺平了道路。

Abstract: Depth estimation is a fundamental task in computer vision with diverse
applications. Recent advancements in deep learning have led to powerful depth
foundation models (DFMs), yet their evaluation remains challenging due to
inconsistencies in existing protocols. Traditional benchmarks rely on
alignment-based metrics that introduce biases, favor certain depth
representations, and complicate fair comparisons. In this work, we propose
BenchDepth, a new benchmark that evaluates DFMs through five carefully selected
downstream proxy tasks: depth completion, stereo matching, monocular
feed-forward 3D scene reconstruction, SLAM, and vision-language spatial
understanding. Unlike conventional evaluation protocols, our approach assesses
DFMs based on their practical utility in real-world applications, bypassing
problematic alignment procedures. We benchmark eight state-of-the-art DFMs and
provide an in-depth analysis of key findings and observations. We hope our work
sparks further discussion in the community on best practices for depth model
evaluation and paves the way for future research and advancements in depth
estimation.

</details>


### [108] [RoadFusion: Latent Diffusion Model for Pavement Defect Detection](https://arxiv.org/abs/2507.15346)
*Muhammad Aqeel,Kidus Dagnaw Bellete,Francesco Setti*

Main category: cs.CV

TL;DR: RoadFusion 通过合成异常生成和双路径特征适应，解决了路面缺陷检测中的数据稀缺和领域偏移问题，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 路面缺陷检测面临标注数据有限、训练与部署环境间的领域偏移以及不同道路条件下缺陷外观的高度可变性等关键挑战。

Method: RoadFusion 通过合成异常生成和双路径特征适应来解决数据稀缺、领域偏移和缺陷外观多样性问题。使用潜在扩散模型生成多样化的逼真缺陷，并通过两个独立的特征适配器分别处理正常和异常输入，同时轻量级鉴别器在补丁级别学习区分细粒度缺陷模式。

Result: 在六个基准数据集上的评估表明，RoadFusion 在分类和定位任务中均取得了优异的性能，并在多个实际道路检查相关指标中达到了新的最先进水平。

Conclusion: RoadFusion 在多个基准数据集上表现出色，在分类和定位任务中均取得了最先进的性能，为实际道路检查提供了有效的解决方案。

Abstract: Pavement defect detection faces critical challenges including limited
annotated data, domain shift between training and deployment environments, and
high variability in defect appearances across different road conditions. We
propose RoadFusion, a framework that addresses these limitations through
synthetic anomaly generation with dual-path feature adaptation. A latent
diffusion model synthesizes diverse, realistic defects using text prompts and
spatial masks, enabling effective training under data scarcity. Two separate
feature adaptors specialize representations for normal and anomalous inputs,
improving robustness to domain shift and defect variability. A lightweight
discriminator learns to distinguish fine-grained defect patterns at the patch
level. Evaluated on six benchmark datasets, RoadFusion achieves consistently
strong performance across both classification and localization tasks, setting
new state-of-the-art in multiple metrics relevant to real-world road
inspection.

</details>


### [109] [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://arxiv.org/abs/2507.15365)
*Fatemeh Saleh,Sadegh Aliakbarian,Charlie Hewitt,Lohit Petikam,Xiao-Xian,Antonio Criminisi,Thomas J. Cashman,Tadas Baltrušaitis*

Main category: cs.CV

TL;DR: 研究表明，使用合成数据集训练模型可在保持精度的同时降低成本，并提供更好的数据可控性。


<details>
  <summary>Details</summary>
Motivation: 解决当前人类中心计算机视觉模型需要大量参数、数据集和计算资源的问题。

Method: 使用高保真合成数据集进行模型训练，并利用程序化数据合成技术控制数据多样性。

Result: 在深度估计、表面法线估计和软前景分割三个密集预测任务上实现了高精度，且训练和推理成本显著降低。

Conclusion: 通过合成数据集训练的模型在保持高精度的同时显著降低了计算成本和训练资源需求，且提供了更好的数据可控性和公平性。

Abstract: The state of the art in human-centric computer vision achieves high accuracy
and robustness across a diverse range of tasks. The most effective models in
this domain have billions of parameters, thus requiring extremely large
datasets, expensive training regimes, and compute-intensive inference. In this
paper, we demonstrate that it is possible to train models on much smaller but
high-fidelity synthetic datasets, with no loss in accuracy and higher
efficiency. Using synthetic training data provides us with excellent levels of
detail and perfect labels, while providing strong guarantees for data
provenance, usage rights, and user consent. Procedural data synthesis also
provides us with explicit control on data diversity, that we can use to address
unfairness in the models we train. Extensive quantitative assessment on real
input images demonstrates accuracy of our models on three dense prediction
tasks: depth estimation, surface normal estimation, and soft foreground
segmentation. Our models require only a fraction of the cost of training and
inference when compared with foundational models of similar accuracy. Our
human-centric synthetic dataset and trained models are available at
https://aka.ms/DAViD.

</details>


### [110] [Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond](https://arxiv.org/abs/2507.15401)
*Huiyu Zhai,Xingxing Yang,Yalan Ye,Chenyang Li,Bin Fan,Changze Li*

Main category: cs.CV

TL;DR: ORSANet通过多模态先验和动态损失函数，显著提升遮挡条件下的表情识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有FER模型在面部遮挡情况下难以提取有效特征，导致分类不准确。

Method: 1. 引入语义分割图和面部关键点作为多模态先验；2. 设计多尺度交叉交互模块（MCM）自适应融合特征；3. 提出动态对抗排斥增强损失（DARELoss）优化分类边界。

Result: ORSANet在公开基准和Occlu-FER数据集上均实现了最优识别性能。

Conclusion: ORSANet通过引入多模态语义引导、多尺度交互模块和动态对抗排斥增强损失，显著提升了遮挡条件下的人脸表情识别性能，并在公开基准和自建数据集Occlu-FER上达到SOTA水平。

Abstract: Facial expression recognition (FER) is a challenging task due to pervasive
occlusion and dataset biases. Especially when facial information is partially
occluded, existing FER models struggle to extract effective facial features,
leading to inaccurate classifications. In response, we present ORSANet, which
introduces the following three key contributions: First, we introduce auxiliary
multi-modal semantic guidance to disambiguate facial occlusion and learn
high-level semantic knowledge, which is two-fold: 1) we introduce semantic
segmentation maps as dense semantics prior to generate semantics-enhanced
facial representations; 2) we introduce facial landmarks as sparse geometric
prior to mitigate intrinsic noises in FER, such as identity and gender biases.
Second, to facilitate the effective incorporation of these two multi-modal
priors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively
fuse the landmark feature and semantics-enhanced representations within
different scales. Third, we design a Dynamic Adversarial Repulsion Enhancement
Loss (DARELoss) that dynamically adjusts the margins of ambiguous classes,
further enhancing the model's ability to distinguish similar expressions. We
further construct the first occlusion-oriented FER dataset to facilitate
specialized robustness analysis on various real-world occlusion conditions,
dubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER
demonstrate that our proposed ORSANet achieves SOTA recognition performance.
Code is publicly available at https://github.com/Wenyuzhy/ORSANet-master.

</details>


### [111] [SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition](https://arxiv.org/abs/2507.15418)
*Ka Young Kim,Hyeon Bae Kim,Seong Tae Kim*

Main category: cs.CV

TL;DR: SurgX是一个提升手术阶段识别模型解释性的框架，通过关联神经元与概念，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在手术阶段识别中缺乏可解释性，影响信任和调试。

Method: 提出SurgX框架，包括选择代表性神经元序列、构建手术视频数据集定制概念集、关联神经元与概念及识别关键神经元。

Result: 在两种手术阶段识别模型上的实验验证了方法的有效性。

Conclusion: SurgX通过将神经元与相关概念关联，显著提升了手术阶段识别模型的解释性，验证了其在解释预测方面的潜力。

Abstract: Surgical phase recognition plays a crucial role in surgical workflow
analysis, enabling various applications such as surgical monitoring, skill
assessment, and workflow optimization. Despite significant advancements in deep
learning-based surgical phase recognition, these models remain inherently
opaque, making it difficult to understand how they make decisions. This lack of
interpretability hinders trust and makes it challenging to debug the model. To
address this challenge, we propose SurgX, a novel concept-based explanation
framework that enhances the interpretability of surgical phase recognition
models by associating neurons with relevant concepts. In this paper, we
introduce the process of selecting representative example sequences for
neurons, constructing a concept set tailored to the surgical video dataset,
associating neurons with concepts and identifying neurons crucial for
predictions. Through extensive experiments on two surgical phase recognition
models, we validate our method and analyze the explanation for prediction. This
highlights the potential of our method in explaining surgical phase
recognition. The code is available at https://github.com/ailab-kyunghee/SurgX

</details>


### [112] [One Last Attention for Your Vision-Language Model](https://arxiv.org/abs/2507.15480)
*Liang Chen,Ghazi Shazan Ahmad,Tianjun Yao,Lingqiao Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: RAda通过动态校准融合表示（理性矩阵）提升视觉语言模型的微调效果，在不同设置下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的微调方法通常忽视融合表示（理性矩阵）在决策过程中的关键作用，而RAda旨在填补这一空白。

Method: RAda通过在学习到的轻量级注意力层上附加一个掩码，动态调整理性矩阵中每个元素的贡献，从而在不修改中间特征的情况下优化跨模态交互。

Result: 在不同设置（更新或冻结预训练编码器、仅访问未标记测试数据的测试时训练）中，RAda均显著优于基线方法，并与当前最优方法表现相当。

Conclusion: RAda是一种简单而有效的微调技术，通过动态校准理性矩阵中的元素贡献，显著提升了预训练视觉语言模型的性能，且在不同设置下均表现出色。

Abstract: Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable
zero-shot performance, yet their downstream potential hinges on effective
fine-tuning. Most adaptation methods typically focus on refining representation
from separate modalities (text or vision) but neglect the critical role of
their fused representations in the decision-making process, \emph{\ie} rational
matrix that drives the final prediction. To bridge the gap, we propose a simple
yet effective \textbf{R}ational \textbf{Ada}ptaion ({RAda}) to explicitly
exploit the final fused representation during fine-tuning. RAda employs a
learned mask, obtained from a lightweight attention layer attached at the end
of a VLM, to dynamically calibrate the contribution of each element in the
rational matrix, enabling targeted adjustments to the final cross-modal
interactions without incurring costly modifications to intermediate features.
Experiments in different settings (i.e., updating, or freezing pretrained
encoders in adaptation, and test-time training that can only access the
unlabeled test data) show that RAda serves as a versatile fine-tuning
technique, improving the baseline with minimal code and performing comparably
against current arts in most settings. Code is available at
\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.

</details>


### [113] [An aerial color image anomaly dataset for search missions in complex forested terrain](https://arxiv.org/abs/2507.15492)
*Rakesh John Amala Arokia Nathan,Matthias Gessner,Nurullah Özkan,Marius Bock,Mohamed Youssef,Maximilian Mews,Björn Piltz,Ralf Berger,Oliver Bimber*

Main category: cs.CV

TL;DR: 论文介绍了一个由众包搜索产生的独特数据集，用于改进复杂森林环境中的异常检测，现有方法表现不佳，数据集和网络接口已公开。


<details>
  <summary>Details</summary>
Motivation: 在德国农村一起家庭谋杀案后，尽管进行了大规模搜索，当局仍未能找到嫌疑人。为了协助搜索，需要一种有效的方法来处理高分辨率航空影像，尤其是在植被密集的情况下。

Method: 研究飞机捕获高分辨率航空影像，但由于植被密集遮蔽小线索，自动化分析效果不佳，因此启动了众包搜索。这产生了在遮挡、真实条件下难以检测的异常标注数据集。

Result: 初始基准测试显示现有方法表现不佳，突出了上下文感知方法的必要性。数据集和交互式网络接口已公开，支持离线处理和在线动态增长。

Conclusion: 该论文提供了一个独特的标注数据集，用于在复杂森林环境中改进异常检测方法，支持搜捕和救援行动。现有的方法在该数据集上表现不佳，强调了需要上下文感知的方法。数据集和交互式网络接口已公开，便于离线处理和在线动态增长。

Abstract: After a family murder in rural Germany, authorities failed to locate the
suspect in a vast forest despite a massive search. To aid the search, a
research aircraft captured high-resolution aerial imagery. Due to dense
vegetation obscuring small clues, automated analysis was ineffective, prompting
a crowd-search initiative. This effort produced a unique dataset of labeled,
hard-to-detect anomalies under occluded, real-world conditions. It can serve as
a benchmark for improving anomaly detection approaches in complex forest
environments, supporting manhunts and rescue operations. Initial benchmark
tests showed existing methods performed poorly, highlighting the need for
context-aware approaches. The dataset is openly accessible for offline
processing. An additional interactive web interface supports online viewing and
dynamic growth by allowing users to annotate and submit new findings.

</details>


### [114] [Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization](https://arxiv.org/abs/2507.15504)
*Bingqing Zhang,Zhuo Cao,Heming Du,Yang Li,Xue Li,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: UMIVR 是一种不确定性最小化的交互式文本到视频检索框架，通过量化文本模糊性、映射不确定性和帧不确定性，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 当前交互式文本到视频检索方法依赖启发式或临时策略，未显式量化不确定性，限制了其效果。UMIVR 旨在填补这一空白，通过显式量化文本模糊性、映射不确定性和帧不确定性来提升检索性能。

Method: UMIVR 提出了三个无训练的度量标准：基于语义熵的文本模糊性评分（TAS）、基于Jensen-Shannon散度的映射不确定性评分（MUS）和基于时间质量的帧采样器（TQFS），通过这些度量标准自适应生成针对性澄清问题。

Result: 在 MSR-VTT-1k 数据集上，UMIVR 在 10 轮交互后实现了 Recall@1 69.2% 的显著提升。

Conclusion: UMIVR 通过显式量化和最小化文本到视频检索中的关键不确定性，显著提高了检索性能，并在多个基准测试中验证了其有效性。

Abstract: Despite recent advances, Text-to-video retrieval (TVR) is still hindered by
multiple inherent uncertainties, such as ambiguous textual queries, indistinct
text-video mappings, and low-quality video frames. Although interactive systems
have emerged to address these challenges by refining user intent through
clarifying questions, current methods typically rely on heuristic or ad-hoc
strategies without explicitly quantifying these uncertainties, limiting their
effectiveness. Motivated by this gap, we propose UMIVR, an
Uncertainty-Minimizing Interactive Text-to-Video Retrieval framework that
explicitly quantifies three critical uncertainties-text ambiguity, mapping
uncertainty, and frame uncertainty-via principled, training-free metrics:
semantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon
divergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based
Frame Sampler (TQFS). By adaptively generating targeted clarifying questions
guided by these uncertainty measures, UMIVR iteratively refines user queries,
significantly reducing retrieval ambiguity. Extensive experiments on multiple
benchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1
(69.2\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby
establishing an uncertainty-minimizing foundation for interactive TVR.

</details>


### [115] [SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.15520)
*Hanting Li,Fei Zhou,Xin Sun,Yang Hua,Jungong Han,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: SAIGFormer通过动态积分图像和光照引导自注意力机制，有效解决了非均匀光照增强问题，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有Transformer方法在非均匀光照场景（如背光和阴影）中表现不佳的问题。

Method: 提出了一种动态积分图像表示法来建模空间变化的光照，并构建了Spatially-Adaptive Integral Illumination Estimator（SAI²E）。此外，还引入了Illumination-Guided Multi-head Self-Attention（IG-MSA）机制。

Result: 在五个标准低光数据集和一个跨域基准测试（LOL-Blur）上，SAIGFormer在定量和定性指标上均显著优于现有方法。

Conclusion: SAIGFormer在非均匀光照增强方面表现优异，并展现出强大的跨数据集泛化能力。

Abstract: Recent Transformer-based low-light enhancement methods have made promising
progress in recovering global illumination. However, they still struggle with
non-uniform lighting scenarios, such as backlit and shadow, appearing as
over-exposure or inadequate brightness restoration. To address this challenge,
we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)
framework that enables accurate illumination restoration. Specifically, we
propose a dynamic integral image representation to model the spatially-varying
illumination, and further construct a novel Spatially-Adaptive Integral
Illumination Estimator ($\text{SAI}^2\text{E}$). Moreover, we introduce an
Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which
leverages the illumination to calibrate the lightness-relevant features toward
visual-pleased illumination enhancement. Extensive experiments on five standard
low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our
SAIGFormer significantly outperforms state-of-the-art methods in both
quantitative and qualitative metrics. In particular, our method achieves
superior performance in non-uniform illumination enhancement while exhibiting
strong generalization capabilities across multiple datasets. Code is available
at https://github.com/LHTcode/SAIGFormer.git.

</details>


### [116] [Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2507.15540)
*Syed Ahmed Mahmood,Ali Shah Ali,Umer Ahmed,Fawad Javed Fateh,M. Zeeshan Zia,Quoc-Huy Tran*

Main category: cs.CV

TL;DR: 本文提出一种自监督程序学习框架，结合Gromov-Wasserstein最优传输和对比正则化，有效解决顺序变化和冗余帧问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的程序学习方法在顺序变化、背景/冗余帧和重复动作存在时性能下降，需要一种更鲁棒的方法来识别关键步骤及其顺序。

Method: 提出了一种自监督程序学习框架，结合了融合Gromov-Wasserstein最优传输和结构先验的帧到帧映射方法，并引入对比正则化以避免嵌入空间中的退化解。

Result: 在EgoProceL、ProceL和CrossTask等大规模基准测试中，所提方法表现优于依赖传统Kantorovich最优传输的OPEL等方法。

Conclusion: 本文提出的自监督程序学习框架通过结合Gromov-Wasserstein最优传输和对比正则化，有效解决了顺序变化、背景/冗余帧和重复动作带来的挑战，并在多个基准测试中表现优于现有方法。

Abstract: We study the problem of self-supervised procedure learning, which discovers
key steps and establishes their order from a set of unlabeled procedural
videos. Previous procedure learning methods typically learn frame-to-frame
correspondences between videos before determining key steps and their order.
However, their performance often suffers from order variations,
background/redundant frames, and repeated actions. To overcome these
challenges, we propose a self-supervised procedure learning framework, which
utilizes a fused Gromov-Wasserstein optimal transport formulation with a
structural prior for computing frame-to-frame mapping between videos. However,
optimizing exclusively for the above temporal alignment term may lead to
degenerate solutions, where all frames are mapped to a small cluster in the
embedding space and hence every video is associated with only one key step. To
address that limitation, we further integrate a contrastive regularization
term, which maps different frames to different points in the embedding space,
avoiding the collapse to trivial solutions. Finally, we conduct extensive
experiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,
ProceL and CrossTask) benchmarks to demonstrate superior performance by our
approach against previous methods, including OPEL which relies on a traditional
Kantorovich optimal transport formulation with an optimality prior.

</details>


### [117] [Towards Holistic Surgical Scene Graph](https://arxiv.org/abs/2507.15541)
*Jongmin Shin,Enki Cho,Ka Yong Kim,Jung Yong Kim,Seong Tae Kim,Namkee Oh*

Main category: cs.CV

TL;DR: 论文提出Endoscapes-SG201数据集和SSG-Com方法，通过整合工具-动作-目标组合及操作手身份，显著提升了手术场景理解的图表示效果。


<details>
  <summary>Details</summary>
Motivation: 手术场景理解的现有图表示方法未充分探索工具-动作-目标组合及操作手身份等关键元素，而这些对计算机辅助干预系统至关重要。

Method: 提出了Endoscapes-SG201数据集，包含工具-动作-目标组合及操作手身份的标注，并设计了SSG-Com图学习方法来表示这些关键元素。

Result: 在关键安全视图评估和动作三元组识别等下游任务中，验证了这些关键元素对手术场景理解的重要贡献。

Conclusion: 论文通过提出Endoscapes-SG201数据集和SSG-Com方法，成功将手术工具-动作-目标组合及操作手身份等关键元素整合到图表示中，显著提升了手术场景理解的效果。

Abstract: Surgical scene understanding is crucial for computer-assisted intervention
systems, requiring visual comprehension of surgical scenes that involves
diverse elements such as surgical tools, anatomical structures, and their
interactions. To effectively represent the complex information in surgical
scenes, graph-based approaches have been explored to structurally model
surgical entities and their relationships. Previous surgical scene graph
studies have demonstrated the feasibility of representing surgical scenes using
graphs. However, certain aspects of surgical scenes-such as diverse
combinations of tool-action-target and the identity of the hand operating the
tool-remain underexplored in graph-based representations, despite their
importance. To incorporate these aspects into graph representations, we propose
Endoscapes-SG201 dataset, which includes annotations for tool-action-target
combinations and hand identity. We also introduce SSG-Com, a graph-based method
designed to learn and represent these critical elements. Through experiments on
downstream tasks such as critical view of safety assessment and action triplet
recognition, we demonstrated the importance of integrating these essential
scene graph components, highlighting their significant contribution to surgical
scene understanding. The code and dataset are available at
https://github.com/ailab-kyunghee/SSG-Com

</details>


### [118] [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/abs/2507.15542)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: HOLa通过低秩分解VLM特征和LLM正则化，提升零样本HOI检测的泛化与动作区分能力，HICO-DET上SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在区分涉及相同对象的动作或泛化到未见类别时表现有限，HOLa旨在解决这些问题。

Method: HOLa采用低秩分解VLM文本特征，生成类别共享的基础特征和可调权重，结合人类-物体标记增强视觉交互表示，并通过LLM驱动的动作正则化优化权重调整。

Result: 在HICO-DET的零样本HOI设置中，HOLa在未见动词设置下达到27.91的未见类别mAP，创下新纪录。

Conclusion: HOLa通过低秩分解VLM文本特征，结合LLM驱动的动作正则化，显著提升了零样本HOI检测的性能，特别是在未见动作类别的泛化能力上，达到了HICO-DET数据集上的新SOTA。

Abstract: Zero-shot human-object interaction (HOI) detection remains a challenging
task, particularly in generalizing to unseen actions. Existing methods address
this challenge by tapping Vision-Language Models (VLMs) to access knowledge
beyond the training data. However, they either struggle to distinguish actions
involving the same object or demonstrate limited generalization to unseen
classes. In this paper, we introduce HOLa (Zero-Shot HOI Detection with
Low-Rank Decomposed VLM Feature Adaptation), a novel approach that both
enhances generalization to unseen classes and improves action distinction. In
training, HOLa decomposes VLM text features for given HOI classes via low-rank
factorization, producing class-shared basis features and adaptable weights.
These features and weights form a compact HOI representation that preserves
shared information across classes, enhancing generalization to unseen classes.
Subsequently, we refine action distinction by adapting weights for each HOI
class and introducing human-object tokens to enrich visual interaction
representations. To further distinguish unseen actions, we guide the weight
adaptation with LLM-derived action regularization. Experimental results show
that our method sets a new state-of-the-art across zero-shot HOI settings on
HICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.
Our code is available at https://github.com/ChelsieLei/HOLa.

</details>


### [119] [DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding](https://arxiv.org/abs/2507.15569)
*Xiaoyi Bao,Chenwei Xie,Hao Tang,Tingyu Weng,Xiaofeng Wang,Yun Zheng,Xingang Wang*

Main category: cs.CV

TL;DR: DynImg通过非关键帧时间提示和4D旋转位置嵌入，有效整合视频时空信息，性能提升2%。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理视频时空信息时存在快速移动物体空间信息表达不准确的问题，导致时空交互和理解受限。

Method: 提出了一种名为Dynamic-Image（DynImg）的创新视频表示方法，通过非关键帧作为时间提示来突出快速移动物体的空间区域，并采用4D视频旋转位置嵌入保持时空顺序。

Result: 实验评估显示DynImg在多个视频理解基准测试中性能优于现有方法约2%。

Conclusion: DynImg方法通过引入非关键帧作为时间提示，有效提升了视频理解任务中时空信息的整合能力，实验证明其在多个基准测试中性能优于现有方法约2%。

Abstract: In recent years, the introduction of Multi-modal Large Language Models
(MLLMs) into video understanding tasks has become increasingly prevalent.
However, how to effectively integrate temporal information remains a critical
research focus. Traditional approaches treat spatial and temporal information
separately. Due to issues like motion blur, it is challenging to accurately
represent the spatial information of rapidly moving objects. This can lead to
temporally important regions being underemphasized during spatial feature
extraction, which in turn hinders accurate spatio-temporal interaction and
video understanding. To address this limitation, we propose an innovative video
representation method called Dynamic-Image (DynImg). Specifically, we introduce
a set of non-key frames as temporal prompts to highlight the spatial areas
containing fast-moving objects. During the process of visual feature
extraction, these prompts guide the model to pay additional attention to the
fine-grained spatial features corresponding to these regions. Moreover, to
maintain the correct sequence for DynImg, we employ a corresponding 4D video
Rotary Position Embedding. This retains both the temporal and spatial adjacency
of DynImg, helping MLLM understand the spatio-temporal order within this
combined format. Experimental evaluations reveal that DynImg surpasses the
state-of-the-art methods by approximately 2% across multiple video
understanding benchmarks, proving the effectiveness of our temporal prompts in
enhancing video comprehension.

</details>


### [120] [Compress-Align-Detect: onboard change detection from unregistered images](https://arxiv.org/abs/2507.15578)
*Gabriele Inzerillo,Diego Valsesia,Aniello Fiengo,Enrico Magli*

Main category: cs.CV

TL;DR: 该论文提出了一种在卫星上实时进行变化检测的新框架，通过端到端的深度神经网络解决数据存储、图像配准和变化检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统卫星图像变化检测因地面站处理延迟而无法实现实时应用，需将整个工作流程移至卫星上以克服此限制。

Method: 开发了一个包含图像压缩、轻量级图像配准和高效变化检测模型的端到端深度神经网络框架。

Result: 实验表明，该框架在低功耗硬件上实现了0.7 Mpixel/s的处理速度，并在压缩率与F1分数之间取得了良好平衡。

Conclusion: 该研究首次实现了在卫星上端到端的变化检测，为实时应用提供了可行方案。

Abstract: Change detection from satellite images typically incurs a delay ranging from
several hours up to days because of latency in downlinking the acquired images
and generating orthorectified image products at the ground stations; this may
preclude real- or near real-time applications. To overcome this limitation, we
propose shifting the entire change detection workflow onboard satellites. This
requires to simultaneously solve challenges in data storage, image registration
and change detection with a strict complexity constraint. In this paper, we
present a novel and efficient framework for onboard change detection that
addresses the aforementioned challenges in an end-to-end fashion with a deep
neural network composed of three interlinked submodules: (1) image compression,
tailored to minimize onboard data storage resources; (2) lightweight
co-registration of non-orthorectified multi-temporal image pairs; and (3) a
novel temporally-invariant and computationally efficient change detection
model. This is the first approach in the literature combining all these tasks
in a single end-to-end framework with the constraints dictated by onboard
processing. Experimental results compare each submodule with the current
state-of-the-art, and evaluate the performance of the overall integrated system
in realistic setting on low-power hardware. Compelling change detection results
are obtained in terms of F1 score as a function of compression rate, sustaining
a throughput of 0.7 Mpixel/s on a 15W accelerator.

</details>


### [121] [SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging](https://arxiv.org/abs/2507.15595)
*Salah Eddine Bekhouche,Gaby Maroun,Fadi Dornaika,Abdenour Hadid*

Main category: cs.CV

TL;DR: 本文提出SegDT模型，一种基于扩散变换器的医学图像分割方法，在皮肤病变分割上表现出色，兼具高效和准确性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在疾病诊断和治疗规划中至关重要，尤其是在皮肤病变分割方面，对皮肤癌的诊断和患者监测具有重要意义。

Method: 本文提出了一种基于扩散变换器（DiT）的新分割模型SegDT，结合了Rectified Flow技术，优化了生成质量并减少了推理步骤。

Result: SegDT在三个基准数据集上进行了评估，与现有方法相比，实现了最先进的性能，同时保持了快速的推理速度。

Conclusion: SegDT模型在医学图像分割领域取得了显著的进展，特别是在皮肤病变分割方面，为医疗专业人员提供了更快、更准确的诊断工具。

Abstract: Medical image segmentation is crucial for many healthcare tasks, including
disease diagnosis and treatment planning. One key area is the segmentation of
skin lesions, which is vital for diagnosing skin cancer and monitoring
patients. In this context, this paper introduces SegDT, a new segmentation
model based on diffusion transformer (DiT). SegDT is designed to work on
low-cost hardware and incorporates Rectified Flow, which improves the
generation quality at reduced inference steps and maintains the flexibility of
standard diffusion models. Our method is evaluated on three benchmarking
datasets and compared against several existing works, achieving
state-of-the-art results while maintaining fast inference speeds. This makes
the proposed model appealing for real-world medical applications. This work
advances the performance and capabilities of deep learning models in medical
image analysis, enabling faster, more accurate diagnostic tools for healthcare
professionals. The code is made publicly available at
\href{https://github.com/Bekhouche/SegDT}{GitHub}.

</details>


### [122] [SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting](https://arxiv.org/abs/2507.15602)
*Zihui Gao,Jia-Wang Bian,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出混合SDF和3DGS的方法，提升稀疏视图下的表面重建和新视角渲染效果。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏视图图像中表面重建和新视角渲染的挑战，特别是SDF方法在细节处理和3DGS方法在全局几何一致性上的不足。

Method: 提出了一种混合方法，结合了SDF（捕捉粗略几何）和3DGS（渲染新图像以细化SDF细节）的优势。

Result: 在表面重建和新视角合成方面优于现有技术。

Conclusion: 该方法在DTU和MobileBrick数据集上超越了现有技术，实现了更优的表面重建和新视角合成效果。

Abstract: Surface reconstruction and novel view rendering from sparse-view images are
challenging. Signed Distance Function (SDF)-based methods struggle with fine
details, while 3D Gaussian Splatting (3DGS)-based approaches lack global
geometry coherence. We propose a novel hybrid method that combines the
strengths of both approaches: SDF captures coarse geometry to enhance
3DGS-based rendering, while newly rendered images from 3DGS refine the details
of SDF for accurate surface reconstruction. As a result, our method surpasses
state-of-the-art approaches in surface reconstruction and novel view synthesis
on the DTU and MobileBrick datasets. Code will be released at
https://github.com/Gaozihui/SurfaceSplat.

</details>


### [123] [CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation](https://arxiv.org/abs/2507.15606)
*Ru Jia,Xiaozhuang Ma,Jianji Wang,Nanning Zheng*

Main category: cs.CV

TL;DR: CylinderPlane是一种基于圆柱坐标系的隐式表示，解决了Tri-plane的多脸伪影问题，实现了高质量的360°图像合成。


<details>
  <summary>Details</summary>
Motivation: Tri-plane表示在3D感知图像生成中存在特征共享导致的多脸伪影问题，限制了360°视图图像的生成能力。

Method: 通过圆柱坐标系明确分离不同角度的特征，并结合嵌套圆柱表示（多尺度圆柱组合）来适应复杂几何和多分辨率需求。

Result: 在合成数据集和野外非结构化图像上的广泛实验表明，CylinderPlane优于先前方法。

Conclusion: CylinderPlane 提出了一种基于圆柱坐标系的新型隐式表示方法，有效解决了Tri-plane表示中的特征模糊问题，实现了高质量、无伪影的360°图像合成。

Abstract: While the proposal of the Tri-plane representation has advanced the
development of the 3D-aware image generative models, problems rooted in its
inherent structure, such as multi-face artifacts caused by sharing the same
features in symmetric regions, limit its ability to generate 360$^\circ$ view
images. In this paper, we propose CylinderPlane, a novel implicit
representation based on Cylindrical Coordinate System, to eliminate the feature
ambiguity issue and ensure multi-view consistency in 360$^\circ$. Different
from the inevitable feature entanglement in Cartesian coordinate-based
Tri-plane representation, the cylindrical coordinate system explicitly
separates features at different angles, allowing our cylindrical representation
possible to achieve high-quality, artifacts-free 360$^\circ$ image synthesis.
We further introduce the nested cylinder representation that composites
multiple cylinders at different scales, thereby enabling the model more
adaptable to complex geometry and varying resolutions. The combination of
cylinders with different resolutions can effectively capture more critical
locations and multi-scale features, greatly facilitates fine detail learning
and robustness to different resolutions. Moreover, our representation is
agnostic to implicit rendering methods and can be easily integrated into any
neural rendering pipeline. Extensive experiments on both synthetic dataset and
unstructured in-the-wild images demonstrate that our proposed representation
achieves superior performance over previous methods.

</details>


### [124] [A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications](https://arxiv.org/abs/2507.15628)
*Shanjiang Tang,Rui Huang,Hsinyu Luo,Chunjiang Wang,Ce Yu,Yusen Li,Hao Fu,Chao Sun,and Jian Xiao*

Main category: cs.CV

TL;DR: 本文综述了DNN在视频分析中效率优化的技术，从硬件支持到数据处理等多角度组织方法，并讨论了优化中的挑战。


<details>
  <summary>Details</summary>
Motivation: 视频数据的爆炸性增长对视频分析提出了更高要求，准确性和效率是关键。现有调查多从准确性优化角度总结，本文专注于DNN在视频分析中效率优化的技术。

Method: 采用自下而上的方式组织现有方法，涵盖硬件支持、数据处理、操作部署等多个视角。

Result: 提供了全面的DNN在视频分析中效率优化技术的综述。

Conclusion: 本文分析了基于DNN的视频分析性能优化中的问题和挑战，并提供了优化框架和现有工作的总结。

Abstract: The explosive growth of video data in recent years has brought higher demands
for video analytics, where accuracy and efficiency remain the two primary
concerns. Deep neural networks (DNNs) have been widely adopted to ensure
accuracy; however, improving their efficiency in video analytics remains an
open challenge. Different from existing surveys that make summaries of
DNN-based video mainly from the accuracy optimization aspect, in this survey,
we aim to provide a thorough review of optimization techniques focusing on the
improvement of the efficiency of DNNs in video analytics. We organize existing
methods in a bottom-up manner, covering multiple perspectives such as hardware
support, data processing, operational deployment, etc. Finally, based on the
optimization framework and existing works, we analyze and discuss the problems
and challenges in the performance optimization of DNN-based video analytics.

</details>


### [125] [Experimenting active and sequential learning in a medieval music manuscript](https://arxiv.org/abs/2507.15633)
*Sachin Sharma,Federico Simonetta,Michele Flammini*

Main category: cs.CV

TL;DR: 研究提出了一种基于YOLOv8的主动学习方法，用于中世纪音乐手稿的对象检测和布局识别，结果显示在数据稀缺情况下需要更有效的方法。


<details>
  <summary>Details</summary>
Motivation: 解决光学音乐识别（OMR）中标注数据稀缺和历史手稿复杂性的问题。

Method: 利用YOLOv8进行对象检测和布局识别，通过主动学习（AL）和顺序学习（SL）选择预测置信度最低的样本进行迭代标注和重新训练。

Result: 实验结果表明，与完全监督训练相当的准确度可以通过显著减少的标注样本实现。

Conclusion: 在数据稀缺场景下，基于不确定性的主动学习效果不佳，需要探索更实用的方法。

Abstract: Optical Music Recognition (OMR) is a cornerstone of music digitization
initiatives in cultural heritage, yet it remains limited by the scarcity of
annotated data and the complexity of historical manuscripts. In this paper, we
present a preliminary study of Active Learning (AL) and Sequential Learning
(SL) tailored for object detection and layout recognition in an old medieval
music manuscript. Leveraging YOLOv8, our system selects samples with the
highest uncertainty (lowest prediction confidence) for iterative labeling and
retraining. Our approach starts with a single annotated image and successfully
boosts performance while minimizing manual labeling. Experimental results
indicate that comparable accuracy to fully supervised training can be achieved
with significantly fewer labeled examples. We test the methodology as a
preliminary investigation on a novel dataset offered to the community by the
Anonymous project, which studies laude, a poetical-musical genre spread across
Italy during the 12th-16th Century. We show that in the manuscript at-hand,
uncertainty-based AL is not effective and advocates for more usable methods in
data-scarcity scenarios.

</details>


### [126] [Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2507.15652)
*Haoran Zhou,Zihan Zhang,Hao Chen*

Main category: cs.CV

TL;DR: EVA是一种无需训练的方法，通过动态选择中间层并提取视觉事实知识，显著减少MLLMs的幻觉输出，具有模型无关性和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: MLLMs在深层显著抑制视觉信息导致幻觉输出，但中间层阶段先验知识如何抑制视觉信息尚不明确。观察到视觉事实知识与中间层先验/原始概率分布差异在中间层呈现相似演化趋势。

Method: 提出了Decoding by Extracting Visual Facts (EVA)，一种无需训练的简单方法，动态选择具有最显著视觉事实信息的中间层，通过对比原始输入和纯文本输入的输出分布提取视觉事实知识，并将其按比例融入最终层以修正输出logits。

Result: 在广泛使用的基准测试中验证了EVA，结果显示相比基线方法显著降低了幻觉率，证明了其在缓解幻觉方面的有效性。

Conclusion: EVA方法通过动态选择中间层并提取视觉事实知识，显著减少了MLLMs中的幻觉现象，且具有模型无关性和解码策略兼容性。

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides by
combining visual recognition and language understanding to generate content
that is both coherent and contextually accurate. However, MLLMs continue to
struggle with object hallucinations, where models produce seemingly plausible
but factually incorrect outputs, including objects that do not exist in the
image. Recent work has revealed that the prior knowledge in MLLMs significantly
suppresses visual information in deep layers, causing hallucinatory outputs.
However, how these priors suppress visual information at the intermediate layer
stage in MLLMs remains unclear. We observe that visual factual knowledge and
the differences between intermediate-layer prior/original probability
distributions show similar evolutionary trends in intermediate layers.
Motivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a
simple, training-free method that dynamically selects intermediate layers with
the most significant visual factual information. By contrasting the output
distributions of the selected layer derived from the original input and
pure-text input, EVA extracts visual factual knowledge and proportionally
incorporates it into the final layer to correct the output logits. Importantly,
EVA is model-agnostic, seamlessly integrates with various classic decoding
strategies, and is applicable across different MLLMs. We validate EVA on
widely-used benchmarks, and the results show that it significantly reduces
hallucination rates compared to baseline methods, underscoring its
effectiveness in mitigating hallucinations.

</details>


### [127] [HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark](https://arxiv.org/abs/2507.15655)
*Aniket Pal,Ajoy Mondal,Minesh Mathew,C. V. Jawahar*

Main category: cs.CV

TL;DR: HW-MLVQA是一个新型多语言手写文档视觉问答基准，包含丰富数据集和评估框架，旨在提升模型在手写文档上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前MLVQA模型在处理多样化手写文档时能力有限，HW-MLVQA旨在弥补这一不足，提供更真实的评估场景。

Method: HW-MLVQA包含1,600页手写文档和2,400个问答对，并提供了文本、图像及图文结合三种模态的评估框架。

Result: HW-MLVQA提供了一个全面的评估框架，支持对专有和开源OCR模型的严格测试，模拟无真实文本转录的现实场景。

Conclusion: HW-MLVQA基准旨在推动多语言手写文档理解领域的创新和研究，填补了现有MLVQA模型在手写文档处理上的不足。

Abstract: The proliferation of MultiLingual Visual Question Answering (MLVQA)
benchmarks augments the capabilities of large language models (LLMs) and
multi-modal LLMs, thereby enabling them to adeptly capture the intricate
linguistic subtleties and visual complexities inherent across diverse
languages. Despite its potential, the current MLVQA model struggles to fully
utilize its capabilities when dealing with the extensive variety of handwritten
documents. This article delineates HW-MLVQA, an avant-garde VQA benchmark
meticulously crafted to mitigate the dearth of authentic Multilingual
Handwritten document comprehension. HW-MLVQA encompasses an extensive
collection of 1,600 handwritten Pages complemented by 2,400 question-answers.
Furthermore, it provides a robust benchmark evaluation framework spanning three
distinct modalities: text, image, and an integrated image & text modality. To
simulate authentic real-world contexts devoid of ground truth textual
transcriptions, we facilitates a rigorous assessment of proprietary and
open-source OCR models. The benchmark aspires to facilitate pivotal
advancements in multilingual handwritten document interpretation, fostering
innovation and scholarly inquiry within this specialized domain.

</details>


### [128] [Visual-Language Model Knowledge Distillation Method for Image Quality Assessment](https://arxiv.org/abs/2507.15680)
*Yongkang Hou,Jiarun Song*

Main category: cs.CV

TL;DR: 通过知识蒸馏方法优化CLIP在IQA任务中的表现，减少参数负担并提升局部特征识别能力，实验显示效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP在IQA任务中参数负担过重和局部失真特征识别能力不足的问题。

Method: 设计了质量分级提示模板以指导CLIP输出质量评分，对CLIP进行微调以增强其在IQA任务中的能力，并提出模态自适应知识蒸馏策略，实现从CLIP教师模型到学生模型的指导。

Result: 在多个IQA数据集上的实验结果表明，所提方法在显著降低模型复杂度的同时，性能优于现有IQA方法。

Conclusion: 该研究通过视觉-语言模型知识蒸馏方法，显著降低了模型复杂度，并在多个IQA数据集上超越了现有方法，展示了实际部署的强潜力。

Abstract: Image Quality Assessment (IQA) is a core task in computer vision. Multimodal
methods based on vision-language models, such as CLIP, have demonstrated
exceptional generalization capabilities in IQA tasks. To address the issues of
excessive parameter burden and insufficient ability to identify local distorted
features in CLIP for IQA, this study proposes a visual-language model knowledge
distillation method aimed at guiding the training of models with architectural
advantages using CLIP's IQA knowledge. First, quality-graded prompt templates
were designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned
to enhance its capabilities in IQA tasks. Finally, a modality-adaptive
knowledge distillation strategy is proposed to achieve guidance from the CLIP
teacher model to the student model. Our experiments were conducted on multiple
IQA datasets, and the results show that the proposed method significantly
reduces model complexity while outperforming existing IQA methods,
demonstrating strong potential for practical deployment.

</details>


### [129] [Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing](https://arxiv.org/abs/2507.15683)
*Boni Hu,Zhenyu Xia,Lin Chen,Pengcheng Han,Shuhui Bu*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯抛光的双重层次视觉重定位方法，有效解决了大规模场景下的精度和效率问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉重定位方法在大规模场景、高海拔变化和领域差距下的精度不足和计算复杂性问题。

Method: 引入了$\mathrm{Hi}^2$-GSLoc，一个基于3D高斯抛光的双重层次重定位框架，采用稀疏到密集、粗到细的范式。

Result: 通过仿真数据、公开数据集和真实飞行实验验证，方法在定位精度、召回率和计算效率上表现优异。

Conclusion: 该论文提出的方法在远程感知应用中表现出色，具有较高的定位精度、召回率和计算效率。

Abstract: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera
pose from query images, is fundamental to remote sensing and UAV applications.
Existing methods face inherent trade-offs: image-based retrieval and pose
regression approaches lack precision, while structure-based methods that
register queries to Structure-from-Motion (SfM) models suffer from
computational complexity and limited scalability. These challenges are
particularly pronounced in remote sensing scenarios due to large-scale scenes,
high altitude variations, and domain gaps of existing visual priors. To
overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel
scene representation that compactly encodes both 3D geometry and appearance. We
introduce $\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework
that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting
the rich semantic information and geometric constraints inherent in Gaussian
primitives. To handle large-scale remote sensing scenarios, we incorporate
partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic
memory management strategies. Our approach consists of two stages: (1) a sparse
stage featuring a Gaussian-specific consistent render-aware sampling strategy
and landmark-guided detector for robust and accurate initial pose estimation,
and (2) a dense stage that iteratively refines poses through coarse-to-fine
dense rasterization matching while incorporating reliability verification.
Through comprehensive evaluation on simulation data, public datasets, and real
flight experiments, we demonstrate that our method delivers competitive
localization accuracy, recall rate, and computational efficiency while
effectively filtering unreliable pose estimates. The results confirm the
effectiveness of our approach for practical remote sensing applications.

</details>


### [130] [DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2507.15690)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: DWTGS通过小波空间损失改进稀疏视图3DGS，优于傅里叶方法，减少高频幻觉。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3D高斯泼溅（3DGS）在重建高质量新视图时面临挑战，因为它容易过拟合稀疏训练视图中的高频细节。频率正则化是一种有潜力的方法，但依赖傅里叶变换会导致参数调优困难和对高频学习的有害偏向。

Method: DWTGS利用小波空间损失，仅在多个DWT级别监督低频LL子带，并以自监督方式对高频HH子带实施稀疏性。

Result: 实验表明，DWTGS在多个基准测试中一致优于基于傅里叶变换的对应方法。

Conclusion: DWTGS框架通过利用小波空间损失提供额外的空间监督，显著优于基于傅里叶变换的方法，改善了泛化能力并减少了高频幻觉。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in
reconstructing high-quality novel views, as it often overfits to the
widely-varying high-frequency (HF) details of the sparse training views. While
frequency regularization can be a promising approach, its typical reliance on
Fourier transforms causes difficult parameter tuning and biases towards
detrimental HF learning. We propose DWTGS, a framework that rethinks frequency
regularization by leveraging wavelet-space losses that provide additional
spatial supervision. Specifically, we supervise only the low-frequency (LF) LL
subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband
in a self-supervised manner. Experiments across benchmarks show that DWTGS
consistently outperforms Fourier-based counterparts, as this LF-centric
strategy improves generalization and reduces HF hallucinations.

</details>


### [131] [Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation](https://arxiv.org/abs/2507.15709)
*Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了一种高效的人脸图像质量评估方法，通过教师-学生模型蒸馏策略，在保持高性能的同时显著降低计算复杂度，并在ICCV 2025挑战赛中取得第一名。


<details>
  <summary>Details</summary>
Motivation: 尽管FIQA研究已取得显著进展，但算法的计算复杂度仍是实际应用中的关键问题。本文旨在开发一种计算高效且易于部署的FIQA方法。

Method: 方法分为两阶段：1) 训练一个强大的教师模型，采用自训练策略提升其能力；2) 通过知识蒸馏从教师模型中提取轻量级学生模型。教师模型通过伪标签生成和自训练逐步增强，学生模型则结合标注数据和伪标签数据进行训练。

Result: 实验结果表明，学生模型在极低计算开销下性能与教师模型相当，并在ICCV 2025 VQualA FIQA挑战赛中夺冠。

Conclusion: 本文提出的高效人脸图像质量评估（FIQA）方法通过教师-学生模型的知识蒸馏策略，显著降低了计算复杂度，同时保持了高性能。该方法在ICCV 2025 VQualA FIQA挑战赛中取得了第一名。

Abstract: Face image quality assessment (FIQA) is essential for various face-related
applications. Although FIQA has been extensively studied and achieved
significant progress, the computational complexity of FIQA algorithms remains a
key concern for ensuring scalability and practical deployment in real-world
systems. In this paper, we aim to develop a computationally efficient FIQA
method that can be easily deployed in real-world applications. Specifically,
our method consists of two stages: training a powerful teacher model and
distilling a lightweight student model from it. To build a strong teacher
model, we adopt a self-training strategy to improve its capacity. We first
train the teacher model using labeled face images, then use it to generate
pseudo-labels for a set of unlabeled images. These pseudo-labeled samples are
used in two ways: (1) to distill knowledge into the student model, and (2) to
combine with the original labeled images to further enhance the teacher model
through self-training. The enhanced teacher model is used to further
pseudo-label another set of unlabeled images for distilling the student models.
The student model is trained using a combination of labeled images,
pseudo-labeled images from the original teacher model, and pseudo-labeled
images from the enhanced teacher model. Experimental results demonstrate that
our student model achieves comparable performance to the teacher model with an
extremely low computational overhead. Moreover, our method achieved first place
in the ICCV 2025 VQualA FIQA Challenge. The code is available at
https://github.com/sunwei925/Efficient-FIQA.git.

</details>


### [132] [A Practical Investigation of Spatially-Controlled Image Generation with Transformers](https://arxiv.org/abs/2507.15724)
*Guoxuan Xia,Harleen Hanspal,Petru-Daniel Tudosiu,Shifeng Zhang,Sarah Parisot*

Main category: cs.CV

TL;DR: 该论文对比了不同生成范式在空间控制图像生成中的表现，提出了控制令牌预填充作为高效基线，并探讨了采样时间增强和适配器方法的效果。


<details>
  <summary>Details</summary>
Motivation: 旨在为希望开发基于transformer的空间控制生成系统的从业者提供清晰的跨生成范式的见解，澄清文献并填补知识空白。

Method: 在ImageNet数据集上对基于扩散/流和自回归模型进行控制实验，提出了控制令牌预填充方法，并测试了采样时间增强技术如分类器自由指导和softmax截断。

Result: 控制令牌预填充被证明是一种简单、通用且高效的基线方法；采样时间增强技术显著提高了控制生成的一致性；适配器方法在有限下游数据训练时能保持生成质量，但在生成控制一致性方面不如完整训练。

Conclusion: 论文通过对比实验澄清了不同生成范式在空间控制图像生成中的表现，提出了控制令牌预填充作为简单且高效的基线方法，并探讨了采样时间增强对控制生成一致性的影响，同时重新评估了适配器方法的动机和效果。

Abstract: Enabling image generation models to be spatially controlled is an important
area of research, empowering users to better generate images according to their
own fine-grained specifications via e.g. edge maps, poses. Although this task
has seen impressive improvements in recent times, a focus on rapidly producing
stronger models has come at the cost of detailed and fair scientific
comparison. Differing training data, model architectures and generation
paradigms make it difficult to disentangle the factors contributing to
performance. Meanwhile, the motivations and nuances of certain approaches
become lost in the literature. In this work, we aim to provide clear takeaways
across generation paradigms for practitioners wishing to develop
transformer-based systems for spatially-controlled generation, clarifying the
literature and addressing knowledge gaps. We perform controlled experiments on
ImageNet across diffusion-based/flow-based and autoregressive (AR) models.
First, we establish control token prefilling as a simple, general and
performant baseline approach for transformers. We then investigate previously
underexplored sampling time enhancements, showing that extending
classifier-free guidance to control, as well as softmax truncation, have a
strong impact on control-generation consistency. Finally, we re-clarify the
motivation of adapter-based approaches, demonstrating that they mitigate
"forgetting" and maintain generation quality when trained on limited downstream
data, but underperform full training in terms of generation-control
consistency. Code will be released upon publication.

</details>


### [133] [TokensGen: Harnessing Condensed Tokens for Long Video Generation](https://arxiv.org/abs/2507.15728)
*Wenqi Ouyang,Zeqi Xiao,Danni Yang,Yifan Zhou,Shuai Yang,Lei Yang,Jianlou Si,Xingang Pan*

Main category: cs.CV

TL;DR: TokensGen 是一个两阶段框架，通过压缩标记和预训练模型解决长视频生成的长期不一致性问题，显著提升时间和内容一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的生成模型在生成短视频时表现出色，但在生成长视频时面临内存瓶颈和长期不一致性问题。TokensGen 旨在通过分解长视频生成为三个核心任务来解决这些问题。

Method: TokensGen 是一个两阶段框架，包括 To2V（Token-to-Video）短视频扩散模型和 T2To（Text-to-Token）视频标记扩散变换器。To2V 通过视频标记器将短视频压缩为语义丰富的标记，而 T2To 一次性生成所有标记以确保全局一致性。推理时采用自适应 FIFO-Diffusion 策略平滑连接相邻片段。

Result: 实验结果表明，该方法在不增加过高计算开销的情况下，显著提升了长期时间和内容的一致性。

Conclusion: TokensGen 提供了一个可扩展、模块化的长视频生成解决方案，通过利用压缩标记和预训练的短视频模型，显著提升了长期时间和内容的一致性，为故事讲述、电影制作和沉浸式模拟开辟了新可能性。

Abstract: Generating consistent long videos is a complex challenge: while
diffusion-based generative models generate visually impressive short clips,
extending them to longer durations often leads to memory bottlenecks and
long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage
framework that leverages condensed tokens to address these issues. Our method
decomposes long video generation into three core tasks: (1) inner-clip semantic
control, (2) long-term consistency control, and (3) inter-clip smooth
transition. First, we train To2V (Token-to-Video), a short video diffusion
model guided by text and video tokens, with a Video Tokenizer that condenses
short clips into semantically rich tokens. Second, we introduce T2To
(Text-to-Token), a video token diffusion transformer that generates all tokens
at once, ensuring global consistency across clips. Finally, during inference,
an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,
reducing boundary artifacts and enhancing smooth transitions. Experimental
results demonstrate that our approach significantly enhances long-term temporal
and content coherence without incurring prohibitive computational overhead. By
leveraging condensed tokens and pre-trained short video models, our method
provides a scalable, modular solution for long video generation, opening new
possibilities for storytelling, cinematic production, and immersive
simulations. Please see our project page at
https://vicky0522.github.io/tokensgen-webpage/ .

</details>


### [134] [Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS](https://arxiv.org/abs/2507.15748)
*Jisu Shin,Richard Shaw,Seunghyun Shin,Anton Pelykh,Zhensong Zhang,Hae-Gon Jeon,Eduardo Perez-Pellitero*

Main category: cs.CV

TL;DR: 提出基于Transformer的方法，通过预测双边网格校正多视角光度变化，提升3D高斯泼溅流程的重建质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现代相机流水线的广泛设备端处理（如曝光调整、白平衡和色彩校正）虽有益于单个视图，但常导致视角间的光度不一致，破坏多视角一致性并降低新视角合成的质量。

Method: 提出了一种基于Transformer的方法，预测空间自适应的双边网格以校正多视角一致性的光度变化，无需场景特定重训练即可实现跨场景的鲁棒泛化。

Result: 该方法在重建保真度和收敛速度上优于或匹配现有场景特定优化方法，且无需场景特定重训练。

Conclusion: 通过将学习到的双边网格整合到3D高斯泼溅流程中，该方法在保持高训练效率的同时提升了重建质量，实验证明其在重建保真度和收敛速度上优于或匹配现有的场景特定优化方法。

Abstract: Modern camera pipelines apply extensive on-device processing, such as
exposure adjustment, white balance, and color correction, which, while
beneficial individually, often introduce photometric inconsistencies across
views. These appearance variations violate multi-view consistency and degrade
the quality of novel view synthesis. Joint optimization of scene
representations and per-image appearance embeddings has been proposed to
address this issue, but at the cost of increased computational complexity and
slower training. In this work, we propose a transformer-based method that
predicts spatially adaptive bilateral grids to correct photometric variations
in a multi-view consistent manner, enabling robust cross-scene generalization
without the need for scene-specific retraining. By incorporating the learned
grids into the 3D Gaussian Splatting pipeline, we improve reconstruction
quality while maintaining high training efficiency. Extensive experiments show
that our approach outperforms or matches existing scene-specific optimization
methods in reconstruction fidelity and convergence speed.

</details>


### [135] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: HDF框架通过双模块设计提升动态面部表情识别的鲁棒性和准确性，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在多源数据和个体表达差异导致的样本异质性下的性能下降问题。

Method: 提出了一种名为HDF的新框架，包含两个即插即用模块：时间-频率分布注意力模块（DAM）和分布感知缩放模块（DSM），分别用于增强时间-频率建模和优化不平衡问题。

Result: 在DFEW和FERV39k数据集上，HDF显著提升了识别准确性和鲁棒性，取得了更高的加权平均召回率（WAR）和非加权平均召回率（UAR）。

Conclusion: HDF框架通过时间-频率分布注意力模块和自适应优化模块，显著提升了动态面部表情识别的准确性和鲁棒性，并在多个数据集上验证了其优越性。

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [136] [Label tree semantic losses for rich multi-class medical image segmentation](https://arxiv.org/abs/2507.15777)
*Junwen Wang,Oscar MacCormac,William Rochford,Aaron Kujawa,Jonathan Shapey,Tom Vercauteren*

Main category: cs.CV

TL;DR: 提出两种基于树结构的语义损失函数，利用标签层次结构提升医学图像分割性能，在两项任务中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像分割方法对所有错误均等惩罚，未能利用标签空间中的类间语义关系，尤其是在标签丰富且包含细微差别类别时表现不佳。

Method: 提出了两种基于树结构的语义损失函数，并整合到稀疏注释训练方法中。

Result: 在头部MRI全脑分割和神经外科高光谱图像场景理解任务中，所提方法均达到了最先进的性能。

Conclusion: 提出的两种基于树结构的语义损失函数在医学图像分割任务中表现出色，达到了最先进的性能。

Abstract: Rich and accurate medical image segmentation is poised to underpin the next
generation of AI-defined clinical practice by delineating critical anatomy for
pre-operative planning, guiding real-time intra-operative navigation, and
supporting precise post-operative assessment. However, commonly used learning
methods for medical and surgical imaging segmentation tasks penalise all errors
equivalently and thus fail to exploit any inter-class semantics in the labels
space. This becomes particularly problematic as the cardinality and richness of
labels increases to include subtly different classes. In this work, we propose
two tree-based semantic loss functions which take advantage of a hierarchical
organisation of the labels. We further incorporate our losses in a recently
proposed approach for training with sparse, background-free annotations to
extend the applicability of our proposed losses. Extensive experiments are
reported on two medical and surgical image segmentation tasks, namely head MRI
for whole brain parcellation (WBP) with full supervision and neurosurgical
hyperspectral imaging (HSI) for scene understanding with sparse annotations.
Results demonstrate that our proposed method reaches state-of-the-art
performance in both cases.

</details>


### [137] [Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation](https://arxiv.org/abs/2507.15793)
*Ghassen Baklouti,Julio Silva-Rodríguez,Jose Dolz,Houda Bahig,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 提出了一种动态调整低秩适应（LoRA）内在秩的新方法，通过l_1正则化优化，显著提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统LoRA方法中固定秩选择在医学图像下游任务中的挑战，利用自然图像处理领域的进展。

Method: 引入了一种基于低秩适应（LoRA）的新方法，通过l_1稀疏正则化器和近端优化器动态调整内在秩。

Result: 在少量样本微调设置中，该方法显著优于标准LoRA和其他PEFT方法，特别是在基础器官和新器官任务中。

Conclusion: 该方法在医学图像分割任务中表现出色，通过动态调整内在秩，显著提升了性能，并展示了其对次优秩初始化的鲁棒性。

Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is
increasingly attracting interest in medical imaging due to its effectiveness
and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)
is a notable approach based on the assumption that the adaptation inherently
occurs in a low-dimensional subspace. While it has shown good performance, its
implementation requires a fixed and unalterable rank, which might be
challenging to select given the unique complexities and requirements of each
medical imaging downstream task. Inspired by advancements in natural image
processing, we introduce a novel approach for medical image segmentation that
dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank
representation of the trainable weight matrices as a singular value
decomposition, we introduce an l_1 sparsity regularizer to the loss function,
and tackle it with a proximal optimizer. The regularizer could be viewed as a
penalty on the decomposition rank. Hence, its minimization enables to find
task-adapted ranks automatically. Our method is evaluated in a realistic
few-shot fine-tuning setting, where we compare it first to the standard LoRA
and then to several other PEFT methods across two distinguishable tasks: base
organs and novel organs. Our extensive experiments demonstrate the significant
performance improvements driven by our method, highlighting its efficiency and
robustness against suboptimal rank initialization. Our code is publicly
available: https://github.com/ghassenbaklouti/ARENA

</details>


### [138] [Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models](https://arxiv.org/abs/2507.15798)
*Lilian Hollard,Lucas Mohimont,Nathalie Gaveau,Luiz-Angelo Steffenel*

Main category: cs.CV

TL;DR: 论文研究了低参数深度神经网络的性能，提出通过减少特征图干扰来提升扩展性和准确性，并设计了一个名为NoDepth Bottleneck的概念架构，在ImageNet上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索低参数深度神经网络中特征图干扰现象对性能和扩展性的影响，旨在通过减少干扰来提升极低参数网络（少于150万参数）的扩展性和准确性。

Method: 论文研究了瓶颈架构及其在使用超线性激活函数时的行为，重点解决了特征图中的干扰问题。通过分析多种瓶颈架构，确定了减少干扰的关键设计要素，并提出了NoDepth Bottleneck这一概念架构。

Result: 研究结果表明，限制干扰可以显著提升极低参数网络的扩展性和准确性。NoDepth Bottleneck架构在ImageNet数据集上展示了稳健的扩展准确性。

Conclusion: 该论文通过研究低参数深度神经网络在计算机视觉中的表现，提出了一种名为NoDepth Bottleneck的概念架构，该架构基于实验的机械性见解，展示了在ImageNet数据集上的强大扩展准确性。这些发现有助于在低参数范围内构建更高效、可扩展的神经网络，并深化了对计算机视觉中瓶颈架构的理解。

Abstract: The paper investigates the performance of state-of-the-art low-parameter deep
neural networks for computer vision, focusing on bottleneck architectures and
their behavior using superlinear activation functions. We address interference
in feature maps, a phenomenon associated with superposition, where neurons
simultaneously encode multiple characteristics. Our research suggests that
limiting interference can enhance scaling and accuracy in very low-scaled
networks (under 1.5M parameters). We identify key design elements that reduce
interference by examining various bottleneck architectures, leading to a more
efficient neural network. Consequently, we propose a proof-of-concept
architecture named NoDepth Bottleneck built on mechanistic insights from our
experiments, demonstrating robust scaling accuracy on the ImageNet dataset.
These findings contribute to more efficient and scalable neural networks for
the low-parameter range and advance the understanding of bottlenecks in
computer vision. https://caiac.pubpub.org/pub/3dh6rsel

</details>


### [139] [Diffusion models for multivariate subsurface generation and efficient probabilistic inversion](https://arxiv.org/abs/2507.15809)
*Roberto Miele,Niklas Linde*

Main category: cs.CV

TL;DR: 扩散模型在多元地下建模和概率反演中优于传统方法，通过修正后验采样方法提高了效率和稳健性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在深度生成建模任务中表现优异，但在多元地下建模和概率反演中的应用尚未充分探索。

Method: 提出了对Chung等人（2023）的扩散后验采样方法的修正，包括引入考虑噪声污染的似然近似。

Result: 在涉及岩相和相关声阻抗的多元地质场景中，扩散模型表现出优于变分自编码器和生成对抗网络的性能。

Conclusion: 扩散模型在多元地下建模和概率反演中表现出色，显著提高了统计稳健性、后验概率密度采样的效率，并降低了计算成本。

Abstract: Diffusion models offer stable training and state-of-the-art performance for
deep generative modeling tasks. Here, we consider their use in the context of
multivariate subsurface modeling and probabilistic inversion. We first
demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In
diffusion modeling, the generative process involves a comparatively large
number of time steps with update rules that can be modified to account for
conditioning data. We propose different corrections to the popular Diffusion
Posterior Sampling approach by Chung et al. (2023). In particular, we introduce
a likelihood approximation accounting for the noise-contamination that is
inherent in diffusion modeling. We assess performance in a multivariate
geological scenario involving facies and correlated acoustic impedance.
Conditional modeling is demonstrated using both local hard data (well logs) and
nonlinear geophysics (fullstack seismic data). Our tests show significantly
improved statistical robustness, enhanced sampling of the posterior probability
density function and reduced computational costs, compared to the original
approach. The method can be used with both hard and indirect conditioning data,
individually or simultaneously. As the inversion is included within the
diffusion process, it is faster than other methods requiring an outer-loop
around the generative model, such as Markov chain Monte Carlo.

</details>


### [140] [Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models](https://arxiv.org/abs/2507.15824)
*Enes Sanli,Baris Sarper Tezcan,Aykut Erdem,Erkut Erdem*

Main category: cs.CV

TL;DR: PhysVidBench 是一个评估文本到视频生成模型物理常识的基准，通过三阶段间接评估流程，填补了当前模型在物理合理性评估上的空白。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成模型在物理常识方面存在不足，输出的视频常违反因果关系、对象行为和工具使用等直觉预期，因此需要开发一个评估物理推理能力的基准。

Method: 通过三阶段评估流程（1）从提示中制定基于物理的问题，（2）用视觉语言模型为生成的视频生成字幕，（3）让语言模型仅基于字幕回答物理相关问题，间接评估视频的物理合理性。

Result: PhysVidBench 包含 383 个精心设计的提示，涵盖工具使用、材料属性和程序交互等领域，并通过多阶段评估流程验证了其有效性。

Conclusion: PhysVidBench 提供了一个结构化、可解释的框架，用于评估生成视频模型中的物理常识，填补了当前文本到视频生成模型在物理合理性评估上的空白。

Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis
of visually compelling and temporally coherent videos from natural language.
However, these models often fall short in basic physical commonsense, producing
outputs that violate intuitive expectations around causality, object behavior,
and tool use. Addressing this gap, we present PhysVidBench, a benchmark
designed to evaluate the physical reasoning capabilities of T2V systems. The
benchmark includes 383 carefully curated prompts, emphasizing tool use,
material properties, and procedural interactions, and domains where physical
plausibility is crucial. For each prompt, we generate videos using diverse
state-of-the-art models and adopt a three-stage evaluation pipeline: (1)
formulate grounded physics questions from the prompt, (2) caption the generated
video with a vision-language model, and (3) task a language model to answer
several physics-involved questions using only the caption. This indirect
strategy circumvents common hallucination issues in direct video-based
evaluation. By highlighting affordances and tool-mediated actions, areas
overlooked in current T2V evaluations, PhysVidBench provides a structured,
interpretable framework for assessing physical commonsense in generative video
models.

</details>


### [141] [Latent Denoising Makes Good Visual Tokenizers](https://arxiv.org/abs/2507.15856)
*Jiawei Yang,Tianhong Li,Lijie Fan,Yonglong Tian,Yue Wang*

Main category: cs.CV

TL;DR: 论文提出基于去噪原则的视觉分词器l-DeTok，实验证明其在生成任务中优于传统分词器。


<details>
  <summary>Details</summary>
Motivation: 观察到现代生成模型的训练目标（从噪声或掩码输入中重建干净信号）与分词器设计之间存在潜在联系，希望通过对齐分词器嵌入与下游去噪目标来提升生成模型性能。

Method: 引入Latent Denoising Tokenizer（l-DeTok），通过训练分词器从受插值噪声和随机掩码污染的潜在嵌入中重建干净图像。

Result: 在ImageNet 256x256上的实验表明，l-DeTok分词器在六种代表性生成模型中均优于标准分词器。

Conclusion: 论文提出了一种新的视觉分词器设计原则——去噪，通过l-DeTok分词器的实验验证了这一原则的有效性，并希望这一发现能激励未来分词器设计的新视角。

Abstract: Despite their fundamental role, it remains unclear what properties could make
visual tokenizers more effective for generative modeling. We observe that
modern generative models share a conceptually similar training objective --
reconstructing clean signals from corrupted inputs such as Gaussian noise or
masking -- a process we term denoising. Motivated by this insight, we propose
aligning tokenizer embeddings directly with the downstream denoising objective,
encouraging latent embeddings to be more easily reconstructed even when heavily
corrupted. To achieve this, we introduce the Latent Denoising Tokenizer
(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images
from latent embeddings corrupted by interpolative noise and random masking.
Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer
consistently outperforms standard tokenizers across six representative
generative models. Our findings highlight denoising as a fundamental design
principle for tokenizer development, and we hope it could motivate new
perspectives for future tokenizer design.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [142] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: 研究显示，结合文档字符串和链式思考提示策略的LLM生成单元测试效果最佳，其中Gemini 2.5 Pro表现最优。


<details>
  <summary>Details</summary>
Motivation: 通过自动生成单元测试提升软件开发阶段的效率，尤其是在测试金字塔中占比最大的单元测试。

Method: 研究了代码上下文和提示策略对不同大型语言模型（LLM）生成的单元测试质量和充分性的影响。

Result: 包含文档字符串显著提高了代码充分性，而完整实现上下文的扩展效果较小。链式思考提示策略效果最佳，达到96.3%的分支覆盖率和57%的平均突变分数。

Conclusion: Gemini 2.5 Pro (M5) 在突变分数和分支覆盖率方面表现最佳，同时在编译成功率上也保持领先。

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [143] [Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects](https://arxiv.org/abs/2507.14330)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 本文综述了如何利用NLP等技术自动化生成可验证软件规范，提出VERIFAI项目以解决这一挑战。


<details>
  <summary>Details</summary>
Motivation: 解决从非正式和模糊的自然语言需求生成正式规范这一关键挑战，以提升软件正确性验证的效率。

Method: 通过文献综述，识别了从非正式需求生成可验证规范中的常见挑战和潜在研究方向。

Result: 提出了VERIFAI项目，探索自动化与半自动化方法，填补非正式需求与可验证规范之间的鸿沟。

Conclusion: 本文总结了利用自然语言处理（NLP）、本体论领域建模、构件重用和大语言模型（LLMs）等技术，自动化或半自动化地将非正式需求转化为可验证规范的挑战与前景。

Abstract: Software correctness is ensured mathematically through formal verification,
which involves the resources of generating formal requirement specifications
and having an implementation that must be verified. Tools such as
model-checkers and theorem provers ensure software correctness by verifying the
implementation against the specification. Formal methods deployment is
regularly enforced in the development of safety-critical systems e.g.
aerospace, medical devices and autonomous systems. Generating these
specifications from informal and ambiguous natural language requirements
remains the key challenge. Our project, VERIFAI^{1}, aims to investigate
automated and semi-automated approaches to bridge this gap, using techniques
from Natural Language Processing (NLP), ontology-based domain modelling,
artefact reuse, and large language models (LLMs). This position paper presents
a preliminary synthesis of relevant literature to identify recurring challenges
and prospective research directions in the generation of verifiable
specifications from informal requirements.

</details>


### [144] [Developing Shared Vocabulary System For Collaborative Software Engineering](https://arxiv.org/abs/2507.14396)
*Carey Lai Zheng Hui,Johnson Britto Jessia Esther Leena,Kumuthini Subramanian,Zhao Chenyu,Shubham Rajeshkumar Jariwala*

Main category: cs.SE

TL;DR: 研究发现共享词汇系统能有效改善软件工程中的沟通问题，尽管初期有开销，但长期效益显著。


<details>
  <summary>Details</summary>
Motivation: 软件工程协作中沟通不畅是普遍问题，常导致误解、低效和缺陷。本研究旨在探究技术因素导致的沟通误解，并评估共享词汇系统在软件文档和代码库中的可量化效益。

Method: 研究采用设计科学研究（DSR）框架，分为三个迭代阶段：问题识别、方法开发和实证验证。问题识别阶段通过主题分析和半结构化访谈揭示关键因素，使用扎根理论原则设计协作词汇开发方法，并通过控制实验进行实证验证。

Result: 实证验证表明，共享词汇系统虽然初期引入额外开销，但长期显著提升了信息密度、文档清晰度和协作效率。

Conclusion: 研究结果表明，共享词汇系统显著提高了信息密度、文档清晰度和协作效率，为软件工程中的沟通实践提供了可行建议，并指出了未来研究的方向和局限性。

Abstract: Effective communication is a critical factor in successful software
engineering collaboration. However, communication gaps remain a persistent
challenge, often leading to misunderstandings, inefficiencies, and defects.
This research investigates the technical factors contributing to such
misunderstandings and explores the measurable benefits of establishing shared
vocabulary systems within software documentation and codebases. Using a Design
Science Research (DSR) framework, the study was structured into three iterative
phases: problem identification, method development, and empirical validation.
The problem identification phase involved thematic analysis of communication
data and semi-structured interviews, revealing key factors such as ambiguous
messaging, misalignment in documentation, inconsistent code review feedback,
and API integration miscommunication. Grounded Theory principles were employed
to design a structured methodology for collaborative vocabulary development.
Empirical validation through controlled experiments demonstrated that while
initial adoption introduced overhead, the shared vocabulary system
significantly improved information density, documentation clarity, and
collaboration efficiency over time. Findings offer actionable insights for
improving communication practices in software engineering, while also
identifying limitations and directions for future research.

</details>


### [145] [On the Effect of Token Merging on Pre-trained Models for Code](https://arxiv.org/abs/2507.14423)
*Mootez Saad,Hao Li,Tushar Sharma,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 论文提出两种合并代码语义单元隐藏表示的方法，减少计算开销（1%-19%），并在代码翻译任务中提升性能，但漏洞检测略有下降。


<details>
  <summary>Details</summary>
Motivation: 传统代码语言模型的tokenizer输出长度超过编译器/解释器的标准，导致计算开销增加。研究旨在通过合并语义单元的隐藏表示来缓解这一问题。

Method: 提出两种策略：1) 基于平均表示的方法；2) 学习型方法。实验使用六种代码语言模型（CodeBERT、GraphCodeBERT等）在三个软件工程任务（漏洞检测、代码分类、代码翻译）中进行验证。

Result: 浮点运算减少1%-19%；漏洞检测任务的F1分数下降1.82分，代码翻译任务的CodeBLEU提升2.47分。

Conclusion: 本研究通过合并属于同一语义单元的隐藏表示，提出两种策略（基于平均表示和学习方法），显著减少了浮点运算次数（1%-19%），并在代码翻译任务中提升了性能（CodeBLEU提高2.47分），为代码语言模型的效率和性能改进提供了新思路。

Abstract: Tokenization is a fundamental component of language models for code. It
involves breaking down the input into units that are later passed to the
language model stack to learn high-dimensional representations used in various
contexts, from classification to generation. However, the output of these
tokenizers is often longer than that traditionally used in compilers and
interpreters. This could result in undesirable effects, such as increased
computational overhead. In this work, we investigate the effect of merging the
hidden representations of subtokens that belong to the same semantic unit, such
as subtokens that form a single identifier. We propose two strategies: one
based on averaging the representations and another that leverages a
learning-based approach. Both methods can be seamlessly integrated with
existing language models for code. We conduct experiments using six language
models for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),
and CodeT5+ (770M), across three software engineering tasks: vulnerability
detection, code classification, and code translation. Results show that these
strategies can reduce the number of floating-point operations by $1\%$ to
$19\%$. Regarding downstream performance, the most significant degradation was
observed in the vulnerability detection task, where the F1 score decreased by
$1.82$ points compared to the baseline. In contrast, for code translation, we
observed an improvement of $2.47$ points in CodeBLEU. This work contributes to
the broader effort of improving language models for code across multiple
dimensions, including both computational efficiency and downstream performance.

</details>


### [146] [Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches](https://arxiv.org/abs/2507.14547)
*Noman Ahmad,Ruoyu Su,Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 该研究通过文献综述统一了架构退化的定义、原因、指标和修复策略，发现当前工具在持续修复方面存在不足，呼吁采取更全面的方法。


<details>
  <summary>Details</summary>
Motivation: 当前文献对架构退化的定义、指标和修复策略存在碎片化现象，研究旨在统一对架构退化的理解。

Method: 通过多声部文献综述（MLR）分析了108项研究，提取了定义、原因、指标、测量方法、工具和修复策略，并构建了一个涵盖架构、代码和过程债务的分类法。

Result: 研究发现架构退化已从低层次问题演变为社会技术问题，定义了54种指标和31种测量技术，但大多数工具仅能检测问题而缺乏持续或预防性修复支持。

Conclusion: 该研究揭示了架构退化问题在技术和社会层面的复杂性，强调了持续修复策略的缺失，并呼吁采取整体、主动的方法来维持可持续的架构。

Abstract: Architectural degradation, also known as erosion, decay, or aging, impacts
system quality, maintainability, and adaptability. Although widely
acknowledged, current literature shows fragmented definitions, metrics, and
remediation strategies. Our study aims to unify understanding of architectural
degradation by identifying its definitions, causes, metrics, tools, and
remediation approaches across academic and gray literature. We conducted a
multivocal literature review of 108 studies extracting definitions, causes,
metrics, measurement approaches, tools, and remediation strategies. We
developed a taxonomy encompassing architectural, code, and process debt to
explore definition evolution, methodological trends, and research gaps.
Architectural degradation has shifted from a low-level issue to a
socio-technical concern. Definitions now address code violations, design drift,
and structural decay. Causes fall under architectural (e.g., poor
documentation), code (e.g., hasty fixes), and process debt (e.g., knowledge
loss). We identified 54 metrics and 31 measurement techniques, focused on
smells, cohesion/coupling, and evolution. Yet, most tools detect issues but
rarely support ongoing or preventive remediation. Degradation is both technical
and organizational. While detection is well-studied, continuous remediation
remains lacking. Our study reveals missed integration between metrics, tools,
and repair logic, urging holistic, proactive strategies for sustainable
architecture.

</details>


### [147] [Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review](https://arxiv.org/abs/2507.14554)
*Ruoyu Su,Noman ahmad,Matteo Esposito,Andrea Janes,Davide Taibi,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 研究分析了五年内八个行业会议的5,677场演讲，发现Kubernetes和Serverless等核心技术主导了软件架构实践，主要应用于DevOps后期阶段，反映了行业优先事项的演变。


<details>
  <summary>Details</summary>
Motivation: 随着云计算、微服务和容器的兴起，架构实践多样化。理解这些变化至关重要。

Method: 研究分析了来自八个主要行业会议的5,677场演讲，使用大型语言模型和专家验证来提取技术、其目的和使用上下文。还探讨了技术如何在DevOps和部署管道中相互关联和适应。

Result: 在450项技术中，Kubernetes、Cloud Native、Serverless和Containers在频率和中心性上占据主导地位。从业者主要展示与部署、通信、AI和可观测性相关的技术。确定了五个技术社区，涵盖自动化、协调、云AI、监控和云边缘。

Conclusion: 研究表明，少数核心技术（如Kubernetes和Serverless）主导了当代软件架构实践，主要应用于DevOps后期阶段，而对早期阶段（如规划和编码）的关注有限。研究还揭示了从业者如何根据目的和上下文来构建技术，反映了行业优先事项的演变。

Abstract: Software architecture plays a central role in the design, development, and
maintenance of software systems. With the rise of cloud computing,
microservices, and containers, architectural practices have diversified.
Understanding these shifts is vital. This study analyzes software architecture
trends across eight leading industry conferences over five years. We
investigate the evolution of software architecture by analyzing talks from top
practitioner conferences, focusing on the motivations and contexts driving
technology adoption. We analyzed 5,677 talks from eight major industry
conferences, using large language models and expert validation to extract
technologies, their purposes, and usage contexts. We also explored how
technologies interrelate and fit within DevOps and deployment pipelines. Among
450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate
by frequency and centrality. Practitioners present technology mainly related to
deployment, communication, AI, and observability. We identify five technology
communities covering automation, coordination, cloud AI, monitoring, and
cloud-edge. Most technologies span multiple DevOps stages and support hybrid
deployment. Our study reveals that a few core technologies, like Kubernetes and
Serverless, dominate the contemporary software architecture practice. These are
mainly applied in later DevOps stages, with limited focus on early phases like
planning and coding. We also show how practitioners frame technologies by
purpose and context, reflecting evolving industry priorities. Finally, we
observe how only research can provide a more holistic lens on architectural
design, quality, and evolution.

</details>


### [148] [Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library](https://arxiv.org/abs/2507.14558)
*Bin Duan,Tarek Mahmud,Meiru Che,Yan Yan,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: VISTAFUZZ利用LLMs解析API文档，通过约束和依赖生成测试输入，有效检测OpenCV漏洞，成功修复多个问题。


<details>
  <summary>Details</summary>
Motivation: OpenCV作为最流行的开源计算机视觉库，其可靠性至关重要。库中的漏洞可能影响下游应用，因此需要一种有效的方法来确保其质量。

Method: VISTAFUZZ利用LLMs解析OpenCV的API文档，提取输入参数的约束和依赖关系，并基于这些信息生成新的输入值，以系统测试每个目标API。

Result: 在测试OpenCV库的330个API中，VISTAFUZZ检测到17个新漏洞，其中10个已确认，5个已修复。

Conclusion: VISTAFUZZ是一种有效的方法，通过利用大型语言模型（LLMs）解析API文档并生成标准化信息，系统性地测试OpenCV库中的API，成功检测并修复了多个漏洞。

Abstract: The combination of computer vision and artificial intelligence is
fundamentally transforming a broad spectrum of industries by enabling machines
to interpret and act upon visual data with high levels of accuracy. As the
biggest and by far the most popular open-source computer vision library, OpenCV
library provides an extensive suite of programming functions supporting
real-time computer vision. Bugs in the OpenCV library can affect the downstream
computer vision applications, and it is critical to ensure the reliability of
the OpenCV library. This paper introduces VISTAFUZZ, a novel technique for
harnessing large language models (LLMs) for document-guided fuzzing of the
OpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain
standardized API information. Based on this standardized information, VISTAFUZZ
extracts constraints on individual input parameters and dependencies between
these. Using these constraints and dependencies, VISTAFUZZ then generates new
input values to systematically test each target API. We evaluate the
effectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the
results show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been
confirmed, and 5 of these have been fixed.

</details>


### [149] [A first look at License Variants in the PyPI Ecosystem](https://arxiv.org/abs/2507.14594)
*Weiwei Xu,Hengzhi Ye,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: 研究发现开源许可证变体普遍存在但实质性修改较少，却导致显著合规问题；提出LV-Parser和LV-Compat工具，显著提升分析效率和兼容性检测能力。


<details>
  <summary>Details</summary>
Motivation: 开源许可证的变体（包括修改的标准许可证和自定义替代方案）引入了显著的合规复杂性，现有工具未能考虑这些变体的存在，导致许可证分析的效率和效果面临重大挑战。

Method: 采用基于差异的技术和大型语言模型的LV-Parser方法，以及用于检测软件依赖网络中许可证不兼容性的LV-Compat自动化流程。

Result: 研究发现，许可证的文本变体很常见，但只有2%涉及实质性修改；这些变体导致10.7%的下游依赖存在许可证不兼容问题。LV-Parser的准确率达到0.936，同时降低30%的计算成本；LV-Compat识别的不兼容包数量是现有方法的5.2倍，精确度为0.98。

Conclusion: 本研究不仅首次对软件包生态系统中的许可证变体进行了实证研究，还为开发者和组织提供了实用的工具，以应对开源许可证的复杂局面。

Abstract: Open-source licenses establish the legal foundation for software reuse, yet
license variants, including both modified standard licenses and custom-created
alternatives, introduce significant compliance complexities. Despite their
prevalence and potential impact, these variants are poorly understood in modern
software systems, and existing tools do not account for their existence,
leading to significant challenges in both effectiveness and efficiency of
license analysis. To fill this knowledge gap, we conduct a comprehensive
empirical study of license variants in the PyPI ecosystem. Our findings show
that textual variations in licenses are common, yet only 2% involve substantive
modifications. However, these license variants lead to significant compliance
issues, with 10.7% of their downstream dependencies found to be
license-incompatible.
  Inspired by our findings, we introduce LV-Parser, a novel approach for
efficient license variant analysis leveraging diff-based techniques and large
language models, along with LV-Compat, an automated pipeline for detecting
license incompatibilities in software dependency networks. Our evaluation
demonstrates that LV-Parser achieves an accuracy of 0.936 while reducing
computational costs by 30%, and LV-Compat identifies 5.2 times more
incompatible packages than existing methods with a precision of 0.98.
  This work not only provides the first empirical study into license variants
in software packaging ecosystem but also equips developers and organizations
with practical tools for navigating the complex landscape of open-source
licensing.

</details>


### [150] [An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions](https://arxiv.org/abs/2507.14687)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: 本文提出‘Robin's Rule’算法，高效生成最小测试集（N + 1个案例）实现100% Unique-Cause MC/DC覆盖SBEs，验证结果优于商业工具。


<details>
  <summary>Details</summary>
Motivation: 尽管Unique-Cause MC/DC为关键系统提供了最高保证，但其高效测试生成的研究不足。分析显示99.7%的条件决策为SBEs，适合应用Unique-Cause MC/DC，因此需要一种高效且最优的测试生成方法。

Method: 提出‘Robin's Rule’算法，直接构建最小测试集（N + 1个案例）实现100% Unique-Cause MC/DC覆盖SBEs，无需生成完整真值表。通过将TCAS-II规范重构为SBEs构建基准，并使用行业标准商业工具验证结果。

Result: 验证结果表明‘Robin's Rule’始终实现100% Unique-Cause MC/DC覆盖，且测试用例数量为理论最小值（N + 1），效率高于商业工具。

Conclusion: 本文提出了‘Robin's Rule’，一种确定性算法，能够直接构建最小测试集（N + 1个案例）以确保100% Unique-Cause MC/DC覆盖SBEs，无需生成完整的真值表。验证结果表明该方法始终实现100%覆盖且效率高于商业工具，为验证安全关键系统提供了实用且最优的解决方案。

Abstract: Modified Condition/Decision Coverage (MC/DC) is a mandatory structural
coverage criterion for ensuring the reliability and safety of critical systems.
While its strictest form, Unique-Cause MC/DC, offers the highest assurance,
research on its efficient test generation has been lacking. This gap is
particularly significant, as an analysis of large-scale avionics systems shows
that 99.7% of all conditional decisions are, in fact, Singular Boolean
Expressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This
paper proposes 'Robin's Rule', a deterministic algorithm that directly
constructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause
MC/DC for SBEs with N conditions, without generating a full truth table. To
validate our approach, we constructed a benchmark by reformulating the TCAS-II
specifications into SBEs and verified the results using an industry-standard,
certified commercial tool. The results confirm that our method consistently
achieves 100% coverage with the theoretical minimum number of tests and is more
efficient than the commercial tool. This work provides a practical and provably
optimal solution for verifying safety-critical systems, ensuring both rigor and
efficiency.

</details>


### [151] [HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm](https://arxiv.org/abs/2507.14716)
*Shahidul Islam,Ashik Aowal,Md Sharif Uddin,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 本研究开发了 HistoryFinder 工具，通过新基准验证其在方法变更历史重建中优于现有工具，兼具高准确性和高效运行时性能。


<details>
  <summary>Details</summary>
Motivation: 高效准确地重建方法的变更历史对软件工程任务（如维护、重构和理解）至关重要，但现有工具的评估受限于基准的不准确性。

Method: 通过结合自动化分析和专家指导的手动验证，系统构建了两个新的基准（修正的 CodeShovel 基准和新开发的 HistoryFinder 基准），并开发了 HistoryFinder 工具。

Result: HistoryFinder 在 40 个开源仓库的 400 个方法上评估中，在精确率、召回率和 F1 分数上均优于 CodeShovel、CodeTracker、IntelliJ 和基于 Git 的基线工具，同时运行时性能具有竞争力。

Conclusion: HistoryFinder 在准确性和效率方面均优于现有工具，是方法变更历史重建的最佳选择。

Abstract: Reconstructing a method's change history efficiently and accurately is
critical for many software engineering tasks, including maintenance,
refactoring, and comprehension. Despite the availability of method history
generation tools such as CodeShovel and CodeTracker, existing evaluations of
their effectiveness are limited by inaccuracies in the ground truth oracles
used. In this study, we systematically construct two new oracles -- the
corrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by
combining automated analysis with expert-guided manual validation. We also
introduce HistoryFinder, a new method history generation tool designed to
improve not only the accuracy and completeness of method change histories but
also to offer competitive runtime performance. Through extensive evaluation
across 400 methods from 40 open-source repositories, we show that HistoryFinder
consistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based
baselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder
achieves competitive runtime performance, offering the lowest mean and median
execution times among all the research-based tools.
  While Git-based tools exhibit the fastest runtimes, this efficiency comes at
the cost of significantly lower precision and recall -- leaving HistoryFinder
as the best overall choice when both accuracy and efficiency are important. To
facilitate adoption, we provide a web interface, CLI, and Java library for
flexible usage.

</details>


### [152] [Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling](https://arxiv.org/abs/2507.14735)
*Vladyslav Bulhakov,Giordano d'Aloisio,Claudio Di Sipio,Antinisca Di Marco,Davide Di Ruscio*

Main category: cs.SE

TL;DR: 通过超参数调优和提示工程提升Llama 3.1模型在领域模型生成中的准确性，实验证明该方法在多数测试领域中有效。


<details>
  <summary>Details</summary>
Motivation: 通用大型语言模型（LLMs）在领域建模中存在局限性，而微调模型需要大量计算资源且可能导致灾难性遗忘问题，因此探索如何通过超参数调优和提示工程提升模型性能。

Method: 采用基于搜索的方法对特定医疗数据模型的超参数进行调优，并在十个不同应用领域测试优化后的超参数。

Result: 优化后的超参数在医疗数据模型上显著提升了生成质量，并在多数测试领域中表现出改进效果。

Conclusion: 结合超参数调优和提示工程可以显著提升Llama 3.1模型在生成领域模型时的准确性，尽管解决方案并非普遍适用，但在几乎所有测试领域模型中均表现出改进效果。

Abstract: The introduction of large language models (LLMs) has enhanced automation in
software engineering tasks, including in Model Driven Engineering (MDE).
However, using general-purpose LLMs for domain modeling has its limitations.
One approach is to adopt fine-tuned models, but this requires significant
computational resources and can lead to issues like catastrophic forgetting.
  This paper explores how hyperparameter tuning and prompt engineering can
improve the accuracy of the Llama 3.1 model for generating domain models from
textual descriptions. We use search-based methods to tune hyperparameters for a
specific medical data model, resulting in a notable quality improvement over
the baseline LLM. We then test the optimized hyperparameters across ten diverse
application domains.
  While the solutions were not universally applicable, we demonstrate that
combining hyperparameter tuning with prompt engineering can enhance results
across nearly all examined domain models.

</details>


### [153] [Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions](https://arxiv.org/abs/2507.14770)
*Manaal Basha,Ivan Beschastnikh,Gema Rodriguez-Perez,Cleidson R. B. de Souza*

Main category: cs.SE

TL;DR: 研究探讨代码生成工具（CGT）使用中的性别差异，通过混合受试者设计分析任务表现和认知负荷。预期结果为CGT的公平设计提供依据。


<details>
  <summary>Details</summary>
Motivation: 随着对代码生成工具（CGT）的依赖增加，如Windsurf和GitHub Copilot，编程工作流程正在改变，但也引发了对公平性和包容性的关键问题。尽管CGT可能提高生产力，但其在不同用户群体中的效果尚未得到充分研究。假设开发者与CGT的互动因性别而异，影响任务结果和认知负荷。

Method: 研究采用混合受试者设计，54名参与者按性别均分，进行平衡设计。参与者将完成两项编程任务（中等至高等难度），分别仅使用CGT辅助和仅使用互联网访问。数据收集包括认知负荷调查、屏幕录制和任务表现指标（如完成时间、代码正确性和CGT交互行为）。统计分析将用于识别CGT使用中的显著差异。

Result: 预计贡献：我们的工作可以揭示开发者中CGT互动和表现的性别差异。研究结果可为未来CGT设计提供信息，并帮助解决不同用户群体间的可用性和潜在互动模式差异问题。

Conclusion: 虽然结果尚未得出，但我们的提案为在代码生成工具（CGT）设计中推进公平性、责任性、透明性和道德性（FATE）奠定了基础。预期成果将有助于包容性人工智能实践和面向所有用户的公平工具开发。

Abstract: Context: The increasing reliance on Code Generation Tools (CGTs), such as
Windsurf and GitHub Copilot, are revamping programming workflows and raising
critical questions about fairness and inclusivity. While CGTs offer potential
productivity enhancements, their effectiveness across diverse user groups have
not been sufficiently investigated. Objectives: We hypothesize that developers'
interactions with CGTs vary based on gender, influencing task outcomes and
cognitive load, as prior research suggests that gender differences can affect
technology use and cognitive processing. Methods: The study will employ a
mixed-subjects design with 54 participants, evenly divided by gender for a
counterbalanced design. Participants will complete two programming tasks
(medium to hard difficulty) with only CGT assistance and then with only
internet access. Task orders and conditions will be counterbalanced to mitigate
order effects. Data collection will include cognitive load surveys, screen
recordings, and task performance metrics such as completion time, code
correctness, and CGT interaction behaviors. Statistical analyses will be
conducted to identify statistically significant differences in CGT usage.
Expected Contributions: Our work can uncover gender differences in CGT
interaction and performance among developers. Our findings can inform future
CGT designs and help address usability and potential disparities in interaction
patterns across diverse user groups. Conclusion: While results are not yet
available, our proposal lays the groundwork for advancing fairness,
accountability, transparency, and ethics (FATE) in CGT design. The outcomes are
anticipated to contribute to inclusive AI practices and equitable tool
development for all users.

</details>


### [154] [VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs](https://arxiv.org/abs/2507.14776)
*Kimia Tasnia,Alexander Garcia,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.SE

TL;DR: VeriOpt通过角色分工和PPA优化，使LLM生成的Verilog在PPA指标上显著提升，同时保持功能正确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在硬件设计中主要关注功能正确性，忽视了工业级设计的关键PPA指标，VeriOpt旨在填补这一空白。

Method: VeriOpt框架采用基于角色的提示（如Planner、Programmer等）模拟人类设计流程，并结合PPA约束的提示和多模态反馈（如综合报告、时序图）进行优化。

Result: 实验结果表明，VeriOpt相比基线LLM生成的RTL，功耗降低88%，面积减少76%，时序闭合提升73%，功能评估成功率达86%。

Conclusion: VeriOpt通过角色分工和PPA优化，显著提升了LLM在硬件设计中的PPA效率，同时保持功能正确性，为LLM在生产流程中的可靠应用铺平了道路。

Abstract: The rapid adoption of large language models(LLMs) in hardware design has
primarily focused on generating functionally correct Verilog code, overlooking
critical Power Performance-Area(PPA) metrics essential for industrial-grade
designs. To bridge this gap, we propose VeriOpt, a novel framework that
leverages role-based prompting and PPA-aware optimization to enable LLMs to
produce high-quality, synthesizable Verilog. VeriOpt structures LLM
interactions into specialized roles (e.g., Planner, Programmer, Reviewer,
Evaluator) to emulate human design workflows, while integrating PPA constraints
directly into the prompting pipeline. By combining multi-modal feedback (e.g.,
synthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves
PPA-efficient code generation without sacrificing functional correctness.
Experimental results demonstrate up to 88% reduction in power, 76% reduction in
area and 73% improvement in timing closure compared to baseline LLM-generated
RTL, validated using industry standard EDA tools. At the same time achieves 86%
success rate in functionality evaluation. Our work advances the
state-of-the-art AI-driven hardware design by addressing the critical gap
between correctness and quality, paving the way for reliable LLM adoption in
production workflows.

</details>


### [155] [Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context](https://arxiv.org/abs/2507.14791)
*Yang Liu,Li Zhang,Fang Liu,Zhuohang Wang,Donglin Wei,Zhishuo Yang,Kechi Zhang,Jia Li,Lin Shi*

Main category: cs.SE

TL;DR: RepoScope通过多视角上下文和静态分析提升仓库级代码生成效果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在识别仓库的丰富语义和结构关系方面存在不足，导致上下文视角狭窄且提示构建效果不佳，影响了代码生成的准确性。

Method: RepoScope构建了Repository Structural Semantic Graph (RSSG)，并检索了包含结构和相似性上下文的四视角上下文；提出了一种新颖的调用链预测方法和结构保持序列化算法。

Result: 在CoderEval和DevEval基准测试中，RepoScope相对现有方法提升了36.35%的pass@1分数。

Conclusion: RepoScope通过静态分析和多视角上下文整合，显著提升了仓库级代码生成的准确性和效率，且在多种任务中展现出良好的适应性和集成能力。

Abstract: Repository-level code generation aims to generate code within the context of
a specified repository. Existing approaches typically employ
retrieval-augmented generation (RAG) techniques to provide LLMs with relevant
contextual information extracted from the repository. However, these approaches
often struggle with effectively identifying truly relevant contexts that
capture the rich semantics of the repository, and their contextual perspectives
remains narrow. Moreover, most approaches fail to account for the structural
relationships in the retrieved code during prompt construction, hindering the
LLM's ability to accurately interpret the context. To address these issues, we
propose RepoScope, which leverages call chain-aware multi-view context for
repository-level code generation. RepoScope constructs a Repository Structural
Semantic Graph (RSSG) and retrieves a comprehensive four-view context,
integrating both structural and similarity-based contexts. We propose a novel
call chain prediction method that utilizes the repository's structural
semantics to improve the identification of callees in the target function.
Additionally, we present a structure-preserving serialization algorithm for
prompt construction, ensuring the coherence of the context for the LLM.
Notably, RepoScope relies solely on static analysis, eliminating the need for
additional training or multiple LLM queries, thus ensuring both efficiency and
generalizability. Evaluation on widely-used repository-level code generation
benchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms
state-of-the-art methods, achieving up to a 36.35% relative improvement in
pass@1 scores. Further experiments emphasize RepoScope's potential to improve
code generation across different tasks and its ability to integrate effectively
with existing approaches.

</details>


### [156] [Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review](https://arxiv.org/abs/2507.14969)
*Sai Zhang,Zhenchang Xing,Jieshan Chen,Dehai Zhao,Zizhong Zhu,Xiaowang Zhang,Zhiyong Feng,Xiaohong Li*

Main category: cs.SE

TL;DR: RequireCEG通过因果效应图和神经符号协作提升需求描述的清晰度，实验显示覆盖率和多样性显著提高。


<details>
  <summary>Details</summary>
Motivation: 由于非专业用户的需求描述常存在模糊性，现有方法难以表达因果逻辑，因此需要一种新的需求获取和自审查代理。

Method: RequireCEG采用特征树分层分析用户叙述，构建自修复的CEGs以捕捉因果关系，并优化Gherkin场景以确保一致性。

Result: 在RGPair基准数据集上的实验显示，该方法实现了87%的覆盖率和51.88%的多样性提升。

Conclusion: RequireCEG通过神经符号协作架构中的因果效应图（CEGs）有效提升了生成式软件开发中需求描述的清晰度和一致性，显著提高了覆盖率和多样性。

Abstract: The vision of End-User Software Engineering (EUSE) is to empower
non-professional users with full control over the software development
lifecycle. It aims to enable users to drive generative software development
using only natural language requirements. However, since end-users often lack
knowledge of software engineering, their requirement descriptions are
frequently ambiguous, raising significant challenges to generative software
development. Although existing approaches utilize structured languages like
Gherkin to clarify user narratives, they still struggle to express the causal
logic between preconditions and behavior actions. This paper introduces
RequireCEG, a requirement elicitation and self-review agent that embeds
causal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.
RequireCEG first uses a feature tree to analyze user narratives hierarchically,
clearly defining the scope of software components and their system behavior
requirements. Next, it constructs the self-healing CEGs based on the elicited
requirements, capturing the causal relationships between atomic preconditions
and behavioral actions. Finally, the constructed CEGs are used to review and
optimize Gherkin scenarios, ensuring consistency between the generated Gherkin
requirements and the system behavior requirements elicited from user
narratives. To evaluate our method, we created the RGPair benchmark dataset and
conducted extensive experiments. It achieves an 87% coverage rate and raises
diversity by 51.88%.

</details>


### [157] [The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering](https://arxiv.org/abs/2507.15003)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: AIDev是首个大规模数据集，记录了AI编码代理在真实环境中的操作，支持对AI队友在软件开发中的协作、效能和治理的研究。


<details>
  <summary>Details</summary>
Motivation: 随着AI队友（自主、目标驱动的系统）在软件工程中的崛起，需要大规模、真实世界的数据来研究其操作模式、协作效果及治理问题。

Method: 通过收集和分析456,000个由五个领先的AI编码代理（如OpenAI Codex、GitHub Copilot等）在61,000个仓库和47,000名开发者中提交的拉取请求，构建了一个结构化、开放的数据集。

Result: AI代理在速度上常优于人类，但其拉取请求的接受率较低，揭示了信任和效用差距；同时，AI提交的代码结构上更简单。

Conclusion: AIDev数据集为研究AI在软件开发中的自主协作提供了前所未有的实证基础，支持了AI原生工作流程的研究和构建下一代人机共生协作。

Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI
teammates: autonomous, goal-driven systems collaborating with human developers.
Among these, autonomous coding agents are especially transformative, now
actively initiating, reviewing, and evolving code at scale. This paper
introduces AIDev, the first large-scale dataset capturing how such agents
operate in the wild. Spanning over 456,000 pull requests by five leading
agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across
61,000 repositories and 47,000 developers, AIDev provides an unprecedented
empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software
engineering, AIDev offers structured, open data to support research in
benchmarking, agent readiness, optimization, collaboration modeling, and AI
governance. The dataset includes rich metadata on PRs, authorship, review
timelines, code changes, and integration outcomes--enabling exploration beyond
synthetic benchmarks like SWE-bench. For instance, although agents often
outperform humans in speed, their PRs are accepted less frequently, revealing a
trust and utility gap. Furthermore, while agents accelerate code
submission--one developer submitted as many PRs in three days as they had in
three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for
the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev
enables a new generation of research into AI-native workflows and supports
building the next wave of symbiotic human-AI collaboration. The dataset is
publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering
Agent

</details>


### [158] [Survey of GenAI for Automotive Software Development: From Requirements to Executable Code](https://arxiv.org/abs/2507.15025)
*Nenad Petrovic,Vahid Zolfaghari,Andre Schamschurko,Sven Kirchner,Fengjunjie Pan,Chengdng Wu,Nils Purschke,Aleksei Velsh,Krzysztof Lebioda,Yinglei Song,Yi Zhang,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: 本文探讨了GenAI在汽车软件开发中的应用，包括需求处理、合规性和代码生成，提出了一个通用工作流程，并分享了行业调查结果。


<details>
  <summary>Details</summary>
Motivation: 汽车软件开发过程冗长且昂贵，涉及大量需求和严格标准化，因此探索GenAI在该领域的应用潜力。

Method: 本文通过文献综述和行业调查，探讨了GenAI在汽车软件开发中的应用，包括需求处理、合规性方面和代码生成。

Result: 提出了一个通用的GenAI辅助汽车软件开发工作流程，并总结了行业合作伙伴对GenAI工具的使用情况。

Conclusion: 本文总结了一种基于GenAI的汽车软件开发工作流程，并提供了行业合作伙伴对GenAI工具使用情况的调查结果。

Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to
revolutionize many industrial areas by reducing the amount of human
intervention needed and effort for handling complex underlying processes.
Automotive software development is considered to be a significant area for
GenAI adoption, taking into account lengthy and expensive procedures, resulting
from the amount of requirements and strict standardization. In this paper, we
explore the adoption of GenAI for various steps of automotive software
development, mainly focusing on requirements handling, compliance aspects and
code generation. Three GenAI-related technologies are covered within the
state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation
(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting
techniques in case of code generation. Additionally, we also derive a
generalized GenAI-aided automotive software development workflow based on our
findings from this literature review. Finally, we include a summary of a survey
outcome, which was conducted among our automotive industry partners regarding
the type of GenAI tools used for their daily work activities.

</details>


### [159] [Can LLMs Generate User Stories and Assess Their Quality?](https://arxiv.org/abs/2507.15157)
*Giovanni Quattrocchi,Liliana Pasquale,Paola Spoletini,Luciano Baresi*

Main category: cs.SE

TL;DR: LLMs能生成与人类相似的用户故事，但多样性和创造力不足；在明确标准下可自动化评估语义质量，减少人力投入。


<details>
  <summary>Details</summary>
Motivation: 由于需求分析师在理解和转化复杂需求时面临挑战，以及评估语义质量的耗时性，研究探索LLMs在敏捷框架中自动化需求获取的潜力。

Method: 研究使用10种先进的LLMs，模拟客户访谈自动生成US，并比较LLM与人类（领域专家和学生）生成的US质量。同时探索LLMs在自动评估US语义质量上的应用。

Result: LLMs生成的US在覆盖范围和风格质量上与人类相似，但多样性和创造力较低。LLM生成的US达到接受标准的频率较低，但LLMs在提供明确标准时可可靠评估语义质量。

Conclusion: LLMs能够生成与人类相似的用户故事（US），但在多样性和创造力上表现较差。尽管LLM生成的US质量与人类相当，但达到接受标准的频率较低。LLMs在提供明确评估标准时，可以可靠地评估US的语义质量，并有望减少大规模评估中的人力投入。

Abstract: Requirements elicitation is still one of the most challenging activities of
the requirements engineering process due to the difficulty requirements
analysts face in understanding and translating complex needs into concrete
requirements. In addition, specifying high-quality requirements is crucial, as
it can directly impact the quality of the software to be developed. Although
automated tools allow for assessing the syntactic quality of requirements,
evaluating semantic metrics (e.g., language clarity, internal consistency)
remains a manual and time-consuming activity. This paper explores how LLMs can
help automate requirements elicitation within agile frameworks, where
requirements are defined as user stories (US). We used 10 state-of-the-art LLMs
to investigate their ability to generate US automatically by emulating customer
interviews. We evaluated the quality of US generated by LLMs, comparing it with
the quality of US generated by humans (domain experts and students). We also
explored whether and how LLMs can be used to automatically evaluate the
semantic quality of US. Our results indicate that LLMs can generate US similar
to humans in terms of coverage and stylistic quality, but exhibit lower
diversity and creativity. Although LLM-generated US are generally comparable in
quality to those created by humans, they tend to meet the acceptance quality
criteria less frequently, regardless of the scale of the LLM model. Finally,
LLMs can reliably assess the semantic quality of US when provided with clear
evaluation criteria and have the potential to reduce human effort in
large-scale assessments.

</details>


### [160] [Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements](https://arxiv.org/abs/2507.15181)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Yanzhou Mu,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: DLMMM是一种新型深度学习框架测试方法，通过融合多种模型测量指标和多级启发式指导，解决了现有方法的三大局限性，提升了测试效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习框架测试方法存在三大局限性：无法定量测量算子组合多样性、忽略模型执行时间、忽视不同测量指标间的相关性。为此，提出了DLMMM方法以克服这些不足。

Method: DLMMM首先定量测量模型的错误检测性能、算子组合多样性和模型执行时间，然后基于这些测量指标的相关性进行融合以实现权衡，并设计多级启发式指导用于测试输入模型的生成。

Result: DLMMM通过融合多种测量指标和多级启发式指导，显著提升了框架测试的效率和效果。

Conclusion: DLMMM通过融合多种模型测量指标并设计多级启发式指导，显著提升了深度学习框架测试的效果，克服了现有方法的三大局限性。

Abstract: Deep learning frameworks serve as the foundation for developing and deploying
deep learning applications. To enhance the quality of deep learning frameworks,
researchers have proposed numerous testing methods using deep learning models
as test inputs. However, existing methods predominantly measure model bug
detection effectiveness as heuristic indicators, presenting three critical
limitations: Firstly, existing methods fail to quantitatively measure model's
operator combination variety, potentially missing critical operator
combinations that could trigger framework bugs. Secondly, existing methods
neglect measuring model execution time, resulting in the omission of numerous
models potential for detecting more framework bugs within limited testing time.
Thirdly, existing methods overlook correlation between different model
measurements, relying simply on single-indicator heuristic guidance without
considering their trade-offs. To overcome these limitations, we propose DLMMM,
the first deep learning framework testing method to include multiple model
measurements into heuristic guidance and fuse these measurements to achieve
their trade-off. DLMMM firstly quantitatively measures model's bug detection
performance, operator combination variety, and model execution time. After
that, DLMMM fuses the above measurements based on their correlation to achieve
their trade-off. To further enhance testing effectiveness, DLMMM designs
multi-level heuristic guidance for test input model generation.

</details>


### [161] [Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View](https://arxiv.org/abs/2507.15188)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 研究探讨了孟加拉文化如何影响需求工程（RE）过程，强调了文化意识在避免冲突和支持多样性中的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着软件开发项目中利益相关者多样性的增加，理解文化影响（CIs）对避免RE活动中的误解和冲突至关重要，尤其是在孟加拉这样具有独特社会文化特征的国家。

Method: 研究通过调查孟加拉IT行业中的RE实践，分析了文化因素如何影响RE活动的采纳和执行。

Result: 研究发现孟加拉文化对RE过程有显著影响，揭示了具体文化因素如何塑造RE实践。

Conclusion: 该研究探讨了孟加拉文化对需求工程（RE）过程的具体影响，强调了文化意识在避免误解和冲突中的重要性，并支持IT行业的多样性和包容性。

Abstract: Requirements Engineering (RE) is one of the most interaction-intensive phases
of software development. This means that RE activities might be especially
impacted by stakeholders' national culture. Software development projects
increasingly have a very diverse range of stakeholders. To future-proof RE
activities, we need to help RE practitioners avoid misunderstandings and
conflicts that might arise from not understanding potential Cultural Influences
(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT
profession. Bangladesh has a growing IT sector with some unique socio-cultural
characteristics, and has been largely overlooked in this research field. In
this study, we aim to investigate how the RE process is adopted in the context
of Bangladeshi culture and what cultural influences impact overall RE
activities.

</details>


### [162] [Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?](https://arxiv.org/abs/2507.15197)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 系统映射研究显示，AI在人物角色构建和验证中的应用增加，模板化角色更受欢迎，验证研究比例上升。


<details>
  <summary>Details</summary>
Motivation: 探索需求工程中人物角色的最新研究趋势，特别是生成式AI方法对人物角色应用的影响。

Method: 本研究采用系统映射研究（SMS）方法，覆盖2023年4月至2025年4月期间的22篇相关文献，分析了人物角色的表示、构建、验证及其在RE活动中的应用。

Result: 研究发现AI解决方案在人物角色构建和验证中的应用增加，模板化人物角色更受欢迎，验证方面的研究比例上升。

Conclusion: 本研究通过系统映射研究（SMS）探讨了需求工程（RE）中人物角色的最新应用趋势，特别是在生成式AI方法的影响下。研究发现，AI解决方案在人物角色构建和验证中的应用增加，模板化人物角色更受欢迎，验证方面的研究比例也有所上升。

Abstract: In requirements engineering (RE), personas are now being used to represent
user expectations and needs. This systematic mapping study (SMS) aims to
explore the most recent studies and to cover recent changes in trends,
especially related to the recent evolution of Generative AI approaches. Our SMS
covers the period between April 2023 and April 2025. We identified 22 relevant
publications and analysed persona representation, construction, validation, as
well as RE activities covered by personas. We identified that a number of
studies applied AI-based solutions for persona construction and validation. We
observed that template-based personas are becoming more popular nowadays. We
also observed an increase in the proportion of studies covering validation
aspects.

</details>


### [163] [SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation](https://arxiv.org/abs/2507.15224)
*Yibo He,Shuoran Zhao,Jiaming Huang,Yingjie Fu,Hao Yu,Cunjian Huang,Tao Xie*

Main category: cs.SE

TL;DR: SimdBench是首个针对SIMD-intrinsic代码生成的基准测试，评估了18个LLM的表现，发现其在该领域的生成能力较弱，但研究为优化提供了方向。


<details>
  <summary>Details</summary>
Motivation: 填补现有代码生成基准测试在SIMD-intrinsic代码生成评估上的空白，探究LLM在该领域的表现。

Method: 提出了SimdBench，一个专门针对SIMD-intrinsic代码生成的基准测试，包含136个任务，覆盖五种代表性SIMD指令集，并系统性评估了18个代表性LLM。

Result: LLMs在SIMD-intrinsic代码生成中的pass@k普遍低于标量代码生成，但研究为LLM在该领域的进一步优化指明了方向。

Conclusion: LLMs在SIMD-intrinsic代码生成方面表现不如标量代码生成，但研究结果为其进一步优化提供了方向。

Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler
intrinsics are widely supported by modern processors to accelerate
performance-critical tasks. SIMD intrinsic programming, a trade-off between
coding productivity and high performance, is widely used in the development of
mainstream performance-critical libraries and daily computing tasks. Large
Language Models (LLMs), which have demonstrated strong and comprehensive
capabilities in code generation, show promise in assisting programmers with the
challenges of SIMD intrinsic programming. However, existing code-generation
benchmarks focus on only scalar code, and it is unclear how LLMs perform in
generating vectorized code using SIMD intrinsics. To fill this gap, we propose
SimdBench, the first code benchmark specifically designed for SIMD-intrinsic
code generation, comprising 136 carefully crafted tasks and targeting five
representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86
Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM
Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a
systematic evaluation (measuring both correctness and performance) of 18
representative LLMs on SimdBench, resulting in a series of novel and insightful
findings. Our evaluation results demonstrate that LLMs exhibit a universal
decrease in pass@k during SIMD-intrinsic code generation compared to
scalar-code generation. Our in-depth analysis highlights promising directions
for the further advancement of LLMs in the challenging domain of SIMD-intrinsic
code generation. SimdBench is fully open source at
https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader
research community.

</details>


### [164] [Code Clone Detection via an AlphaFold-Inspired Framework](https://arxiv.org/abs/2507.15226)
*Changguo Jia,Yi Zhan,Tianqi Zhao,Hengzhi Ye,Minghui Zhou*

Main category: cs.SE

TL;DR: AlphaCC利用AlphaFold的序列建模能力，通过令牌序列和MSA增强语义理解，在多语言代码克隆检测中表现优异且高效。


<details>
  <summary>Details</summary>
Motivation: 现有代码克隆检测方法难以捕捉代码语义或依赖语言特定分析器，AlphaFold在蛋白质序列到结构预测的成功启发将类似方法应用于代码克隆检测。

Method: AlphaCC将代码片段转换为令牌序列，构建多序列比对（MSA）以增强上下文理解，采用基于AlphaFold的注意力编码器建模依赖关系，最后通过后期交互策略计算相似度并进行二分类。

Result: 在三个多语言数据集上的评估显示，AlphaCC在多语言适用性和语义克隆检测上均优于基线方法，同时保持高效。

Conclusion: AlphaCC通过借鉴AlphaFold的序列到结构建模能力，在多语言代码克隆检测中表现出色，不仅在语义理解上优于现有基线，还保持了较高的效率，适用于大规模检测任务。

Abstract: Code clone detection, which aims to identify functionally equivalent code
fragments, plays a critical role in software maintenance and vulnerability
analysis. Substantial methods have been proposed to detect code clones, but
they fall short in capturing code semantics or relying on language-specific
analyzers. Inspired by the remarkable success of AlphaFold in predicting
three-dimensional protein structures from protein sequences, in this paper, we
leverage AlphaFold for code clone detection based on the insight that protein
sequences and token sequences share a common linear sequential structure. In
particular, we propose AlphaCC, which represents code fragments as token
sequences to ensure multi-language applicability and adapts AlphaFold's
sequence-to-structure modeling capability to infer code semantics. The pipeline
of AlphaCC goes through three steps. First, AlphaCC transforms each input code
fragment into a token sequence and, motivated by AlphaFold's use of multiple
sequence alignment (MSA) to enhance contextual understanding, constructs an MSA
from lexically similar token sequences. Second, AlphaCC adopts a modified
attention-based encoder based on AlphaFold to model dependencies within and
across token sequences. Finally, unlike AlphaFold's protein structure
prediction task, AlphaCC computes similarity scores between token sequences
through a late interaction strategy and performs binary classification to
determine code clone pairs. Comprehensive evaluations on three language-diverse
datasets demonstrate AlphaCC's applicability across multiple programming
languages. On two semantic clone detection datasets, it consistently
outperforms all baselines, showing strong semantic understanding. Moreover,
AlphaCC maintains competitive efficiency, enabling practical usage in
large-scale clone detection tasks.

</details>


### [165] [FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents](https://arxiv.org/abs/2507.15241)
*Vikram Nitin,Baishakhi Ray,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: FaultLine利用LLM代理工作流自动生成PoV测试，相比现有技术提升77%，但问题仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 软件安全漏洞报告常缺少PoV测试，导致修复验证和回归预防困难。自动生成PoV测试对确保补丁有效性和开发者理解漏洞利用方式至关重要。

Method: FaultLine是一个LLM代理工作流，结合静态和动态程序分析的设计思路，通过三步推理（追踪输入流、分析分支条件、反馈驱动生成）自动生成PoV测试用例。

Result: 在100个多语言漏洞数据集中，FaultLine生成了16个项目的PoV测试，相对现有技术（CodeAct 2.1的9个）提升了77%。

Conclusion: FaultLine通过分层推理显著提升了LLM代理在PoV测试生成中的性能，但该问题整体仍具挑战性。

Abstract: Despite the critical threat posed by software security vulnerabilities,
reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests
needed to validate fixes and prevent regressions. These tests are crucial not
only for ensuring patches work, but also for helping developers understand how
vulnerabilities can be exploited. Generating PoV tests is a challenging
problem, requiring reasoning about the flow of control and data through deeply
nested levels of a program.
  We present FaultLine, an LLM agent workflow that uses a set of carefully
designed reasoning steps, inspired by aspects of traditional static and dynamic
program analysis, to automatically generate PoV test cases. Given a software
project with an accompanying vulnerability report, FaultLine 1) traces the flow
of an input from an externally accessible API ("source") to the "sink"
corresponding to the vulnerability, 2) reasons about the conditions that an
input must satisfy in order to traverse the branch conditions encountered along
the flow, and 3) uses this reasoning to generate a PoV test case in a
feedback-driven loop. FaultLine does not use language-specific static or
dynamic analysis components, which enables it to be used across programming
languages.
  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100
known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine
is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct
2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine
represents a 77% relative improvement over the state of the art. Our findings
suggest that hierarchical reasoning can enhance the performance of LLM agents
on PoV test generation, but the problem in general remains challenging. We make
our code and dataset publicly available in the hope that it will spur further
research in this area.

</details>


### [166] [Input Reduction Enhanced LLM-based Program Repair](https://arxiv.org/abs/2507.15251)
*Boyang Yang,Luyao Ren,Xin Yin,Jiadong Ren,Haoye Tian,Shunfu Jin*

Main category: cs.SE

TL;DR: ReduceFix通过自动减少测试输入解决LLM在长提示中的性能问题，显著提升修复效果。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在处理长提示时出现的‘lost-in-the-middle’问题，该问题会因测试输入过长而影响修复性能。

Method: 提出ReduceFix，一种基于LLM的APR方法，内置自动减少测试输入同时保留其失败诱导行为的组件。通过提示LLM生成一个无需人工干预的最小化失败诱导测试输入的缩减器，然后用缩减后的输入指导补丁生成。

Result: 在LFTBench基准测试中，ReduceFix平均缩减输入89.1%，并将pass@10相对原始测试提示提高了53.8%，相比完全省略测试提高了17.6%。同样的缩减步骤应用到ChatRepair中，其修复率提高了21.3%。

Conclusion: 自动减少失败输入是LLM-based APR的实用且强大的补充，显著提高了其可扩展性和有效性。

Abstract: Large Language Models (LLMs) have shown great potential in Automated Program
Repair (APR). Test inputs, being crucial for reasoning the root cause of
failures, are always included in the prompt for LLM-based APR. Unfortunately,
LLMs struggle to retain key information in long prompts. When the test inputs
are extensive in the prompt, this may trigger the "lost-in-the-middle" issue,
compromising repair performance. To address this, we propose ReduceFix, an
LLM-based APR approach with a built-in component that automatically reduces
test inputs while retaining their failure-inducing behavior. ReduceFix prompts
an LLM to generate a reducer that minimizes failure-inducing test inputs
without human effort, and then feeds the reduced failure-inducing inputs to
guide patch generation.
  For targeted evaluation, we constructed LFTBench, the first long-input APR
benchmark with 200 real bugs from 20 programming tasks, each paired with a
failure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix
shrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%
relative to a prompt that includes the original test, and by 17.6% compared
with omitting the test entirely. Adding the same reduction step to ChatRepair
increases its fix rate by 21.3% without other changes. Ablation studies further
highlight the impact of input length and compressed failure information on
repair success. These results underscore that automatically reducing failing
inputs is a practical and powerful complement to LLM-based APR, significantly
improving its scalability and effectiveness.

</details>


### [167] [Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems](https://arxiv.org/abs/2507.15296)
*Qian Xiong,Yuekai Huang,Ziyou Jiang,Zhiyuan Chang,Yujia Zheng,Tianhao Li,Mingyang Li*

Main category: cs.SE

TL;DR: 本文研究了工具代理范式中的参数失效问题，提出了分类法并分析了输入源与失效类别的相关性，最终提出了提高交互可靠性的建议。


<details>
  <summary>Details</summary>
Motivation: 工具代理范式的出现拓宽了大型语言模型（LLM）的能力边界，但由于执行过程中的参数失效问题，其有效性受到限制。本文旨在探讨这一现象并提出相应建议。

Method: 本文首先构建了参数失效的分类法，从主流工具代理的调用链中推导出五类失效类别。然后，通过应用15种输入扰动方法，探索了三种不同输入源与失效类别之间的相关性。

Result: 实验结果表明，参数名称幻觉失效主要源于LLM的固有局限性，而其他失效模式主要由输入源问题引起。

Conclusion: 为提高工具代理交互的可靠性和有效性，本文提出了标准化工具返回格式、改进错误反馈机制和确保参数一致性等改进建议。

Abstract: The emergence of the tool agent paradigm has broadened the capability
boundaries of the Large Language Model (LLM), enabling it to complete more
complex tasks. However, the effectiveness of this paradigm is limited due to
the issue of parameter failure during its execution. To explore this phenomenon
and propose corresponding suggestions, we first construct a parameter failure
taxonomy in this paper. We derive five failure categories from the invocation
chain of a mainstream tool agent. Then, we explore the correlation between
three different input sources and failure categories by applying 15 input
perturbation methods to the input. Experimental results show that parameter
name hallucination failure primarily stems from inherent LLM limitations, while
issues with input sources mainly cause other failure patterns. To improve the
reliability and effectiveness of tool-agent interactions, we propose
corresponding improvement suggestions, including standardizing tool return
formats, improving error feedback mechanisms, and ensuring parameter
consistency.

</details>


### [168] [StackTrans: From Large Language Model to Large Pushdown Automata Model](https://arxiv.org/abs/2507.15343)
*Kechi Zhang,Ge Li,Jia Li,Huangzhao Zhang,Yihong Dong,Jia Li,Jingjing Xu,Zhi Jin*

Main category: cs.SE

TL;DR: StackTrans通过引入可学习的隐藏状态栈，解决了Transformer在捕获Chomsky层次结构时的不足，并在多种任务中表现优异，尤其是参数效率显著。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer架构在人工智能领域取得了显著进展，但其在捕获Chomsky层次结构方面存在固有局限性，这促使研究者提出StackTrans以解决这一问题。

Method: StackTrans在Transformer层间显式引入可微的隐藏状态栈操作（如压入和弹出），保持与现有框架（如flash-attention）的兼容性，并通过端到端学习优化。

Result: StackTrans在Chomsky层次结构和大规模自然语言基准测试中均优于标准Transformer及其他基线模型，且StackTrans-360M在参数效率上超越了许多参数更大的开源大型语言模型。

Conclusion: StackTrans通过引入隐藏状态栈有效解决了Transformer架构在捕获Chomsky层次结构（如正则表达式或确定性上下文无关文法）方面的局限性，并在多种任务中表现优于标准Transformer及其他基线模型。

Abstract: The Transformer architecture has emerged as a landmark advancement within the
broad field of artificial intelligence, effectively catalyzing the advent of
large language models (LLMs). However, despite its remarkable capabilities and
the substantial progress it has facilitated, the Transformer architecture still
has some limitations. One such intrinsic limitation is its inability to
effectively capture the Chomsky hierarchy, such as regular expressions or
deterministic context-free grammars. Drawing inspiration from pushdown
automata, which efficiently resolve deterministic context-free grammars using
stacks, we propose StackTrans to address the aforementioned issue within LLMs.
Unlike previous approaches that modify the attention computation, StackTrans
explicitly incorporates hidden state stacks between Transformer layers. This
design maintains compatibility with existing frameworks like flash-attention.
Specifically, our design features stack operations -- such as pushing and
popping hidden states -- that are differentiable and can be learned in an
end-to-end manner. Our comprehensive evaluation spans benchmarks for both
Chomsky hierarchies and large-scale natural languages. Across these diverse
tasks, StackTrans consistently outperforms standard Transformer models and
other baselines. We have successfully scaled StackTrans up from 360M to 7B
parameters. In particular, our from-scratch pretrained model StackTrans-360M
outperforms several larger open-source LLMs with 2-3x more parameters,
showcasing its superior efficiency and reasoning capability.

</details>


### [169] [Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing](https://arxiv.org/abs/2507.15599)
*Manatsawin Hanmongkolchai*

Main category: cs.SE

TL;DR: Proposes 'Chinese Wall' technique to boost weaker, ethically aligned Code LLMs' performance, showing significant improvements but limited by lack of copyright-free training models.


<details>
  <summary>Details</summary>
Motivation: Address concerns about copyright violations in training datasets for Code LLMs and enhance the utility of ethically aligned but weaker models.

Method: Application of the 'Chinese Wall' technique, where a high-quality model generates detailed instructions for a weaker model to improve its performance.

Result: Comma v0.1 1T's performance improved by over 66%, and Starcoder2 Instruct by roughly 20% in the CanItEdit benchmark.

Conclusion: The 'Chinese Wall' technique significantly enhances the performance of weaker, ethically aligned models, though current application is limited by the scarcity of models trained on copyright-free public domain content.

Abstract: Large language models for code (Code LLM) are increasingly utilized in
programming environments. Despite their utility, the training datasets for top
LLM remain undisclosed, raising concerns about potential copyright violations.
Some models, such as Pleias and Comma put emphasis on data curation and
licenses, however, with limited training data these models are not competitive
and only serve as proof of concepts. To improve the utility of these models, we
propose an application of the "Chinese Wall" technique, inspired by the reverse
engineering technique of the same name -- a high quality model is used to
generate detailed instructions for a weaker model. By doing so, a weaker but
ethically aligned model may be used to perform complicated tasks that,
otherwise, can only be completed by more powerful models. In our evaluation,
we've found that this technique improves Comma v0.1 1T's performance in
CanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%
compared to when running the same model on the benchmark alone. The practical
application of this technique today, however, may be limited due to the lack of
models trained on public domain content without copyright restrictions.

</details>


### [170] [Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow](https://arxiv.org/abs/2507.15624)
*Yusuf Sulistyo Nugroho,Ganno Tribuana Kurniaji,Syful Islam,Mohammed Humayun Kabir,Vanesya Aura Ardity,Md. Kamal Uddin*

Main category: cs.SE

TL;DR: 本研究分析了Stack Overflow上React相关的问题，发现算法错误是最常见的挑战，中等声誉用户贡献最多。结果为React社区提供了早期实施阶段的指导建议。


<details>
  <summary>Details</summary>
Motivation: 尽管React在Web开发中的流行度和优势已得到证实，但用户面临的具体挑战尚不明确，因此本研究旨在通过分析Stack Overflow上的React相关问题来填补这一空白。

Method: 本研究采用探索性数据分析方法，调查了React相关问题中最常讨论的关键词、错误分类以及基于用户声誉的错误分布。

Result: 结果显示，React相关问题中最常使用的八个关键词是code、link、vir、href、connect、azure、windows和website。错误分类表明，算法错误是所有用户群体中最常见的问题，其中中等声誉用户贡献最多，占比55.77%。

Conclusion: 本研究通过分析Stack Overflow上React相关问题的数据，揭示了React用户在开发过程中遇到的主要挑战，特别是算法错误是最常见的问题。研究结果对React社区在早期实施阶段提供有价值的见解，帮助用户更有效地克服采用过程中的挑战。

Abstract: React is a JavaScript library used to build user interfaces for single-page
applications. Although recent studies have shown the popularity and advantages
of React in web development, the specific challenges users face remain unknown.
Thus, this study aims to analyse the React-related questions shared on Stack
Overflow. The study utilizes an exploratory data analysis to investigate the
most frequently discussed keywords, error classification, and user
reputation-based errors, which is the novelty of this work. The results show
the top eight most frequently used keywords on React-related questions, namely,
code, link, vir, href, connect, azure, windows, and website. The error
classification of questions from the sample shows that algorithmic error is the
most frequent issue faced by all groups of users, where mid-reputation users
contribute the most, accounting for 55.77%. This suggests the need for the
community to provide guidance materials in solving algorithm-related problems.
We expect that the results of this study will provide valuable insight into
future research to support the React community during the early stages of
implementation, facilitating their ability to effectively overcome challenges
to adoption.

</details>


### [171] [SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models](https://arxiv.org/abs/2507.15663)
*Giordano d'Aloisio,Tosin Fadahunsi,Jay Choy,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: SustainDiffusion通过优化超参数和提示结构，有效减少文本到图像生成模型中的偏见和能耗，提升社会和环境可持续性，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: Stable Diffusion（SD）作为流行的开源文本到图像生成模型，每年生成超过120亿张图像，但其广泛使用引发了社会和环境的可持续性问题。

Method: SustainDiffusion搜索最佳的超参数和提示结构组合，以减少生成图像中的性别和种族偏见，同时降低图像生成所需的能耗，且保持与原SD模型相当的图像质量。

Result: SustainDiffusion在56个不同提示下与6个基线进行了全面评估，结果显示它能够将SD3中的性别偏见减少68%，种族偏见减少59%，能耗（CPU和GPU能耗总和）降低48%，且结果具有一致性和泛化性。

Conclusion: 通过SustainDiffusion，我们展示了如何在无需微调或改变模型架构的情况下，增强文本到图像生成模型的社会和环境可持续性。

Abstract: Background: Text-to-image generation models are widely used across numerous
domains. Among these models, Stable Diffusion (SD) - an open-source
text-to-image generation model - has become the most popular, producing over 12
billion images annually. However, the widespread use of these models raises
concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the
environment, we introduce SustainDiffusion, a search-based approach designed to
enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters
and prompt structures that can reduce gender and ethnic bias in generated
images while also lowering the energy consumption required for image
generation. Importantly, SustainDiffusion maintains image quality comparable to
that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,
testing it against six different baselines using 56 different prompts. Our
results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,
ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and
GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are
consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social
and environmental sustainability of text-to-image generation models is possible
without fine-tuning or changing the model's architecture.

</details>


### [172] [Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches](https://arxiv.org/abs/2507.15666)
*Igor Turkin,Lina Volobuieva,Andriy Chukhray,Oleksandr Liubimov*

Main category: cs.SE

TL;DR: 文章比较了等效电路和机器学习两种方法预测CubeSat电池放电的性能，发现机器学习更准确且适应性强。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了通过建模CubeSat卫星电池放电来预测自主电源系统断开的影响，并确保轨道设备的容错能力。

Method: 文章采用了两种方法：基于物理定律的等效电路分析建模和基于经验数据的机器学习建模。

Result: 比较分析结果显示，等效电路方法透明但灵活性差，机器学习模型更准确且适应性强。

Conclusion: 文章得出结论，机器学习模型在预测CubeSat卫星电池放电方面更为准确，能够适应实际条件，而等效电路方法虽然透明但灵活性不足。

Abstract: The subject of the article is the study and comparison of two approaches to
modelling the battery discharge of a CubeSat satellite: analytical using
equivalent circuit and machine learning. The article aims to make a reasoned
choice of the approach to modelling the battery discharge of a CubeSat
satellite. Modelling the battery discharge of a satellite will enable the
prediction of the consequences of disconnecting the autonomous power system and
ensure the fault tolerance of equipment in orbit. Therefore, the selected study
is relevant and promising. This study focuses on the analysis of CubeSat
satellite data, based explicitly on orbital data samples of the power system,
which include data available at the time of the article publication. The
dataset contains data on the voltage, current, and temperature of the battery
and solar panels attached to the five sides of the satellite. In this context,
two approaches are considered: analytical modelling based on physical laws and
machine learning, which uses empirical data to create a predictive model.
Results: A comparative analysis of the modeling results reveals that the
equivalent circuit approach has the advantage of transparency, as it identifies
possible parameters that facilitate understanding of the relationships.
However, the model is less flexible to environmental changes or non-standard
satellite behavior. The machine learning model demonstrated more accurate
results, as it can account for complex dependencies and adapt to actual
conditions, even when they deviate from theoretical assumptions.

</details>


### [173] [BugScope: Learn to Find Bugs Like Human](https://arxiv.org/abs/2507.15671)
*Jinyao Guo,Chengpeng Wang,Dominic Deluca,Jinjie Liu,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: BugScope是一种基于LLM的多代理系统，通过模拟人类学习错误模式的方式，显著提升了缺陷检测的准确性和适应性，在实际测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析工具在覆盖范围和适应性上存在局限，无法有效应对多样化且复杂的软件缺陷，而现有的基于LLM的方法在处理复杂错误和有限分析上下文时仍显不足。

Method: BugScope是一个基于大型语言模型（LLM）的多代理系统，通过程序切片提取相关检测上下文，并构建定制化的检测提示来指导LLM进行准确推理。

Result: 在40个真实世界错误的数据集上，BugScope实现了87.04%的精确率和90.00%的召回率，F1分数超过现有工业工具0.44。在包括Linux内核在内的大规模开源系统中，发现了141个未知错误，其中78个已被修复，7个得到开发者确认。

Conclusion: BugScope通过模拟人类审计员学习新错误模式的方式，显著提升了软件缺陷检测的准确性和适应性，在实际应用中发现了大量未知错误，展示了其重要的实用价值。

Abstract: Detecting software bugs remains a fundamental challenge due to the extensive
diversity of real-world defects. Traditional static analysis tools often rely
on symbolic workflows, which restrict their coverage and hinder adaptability to
customized bugs with diverse anti-patterns. While recent advances incorporate
large language models (LLMs) to enhance bug detection, these methods continue
to struggle with sophisticated bugs and typically operate within limited
analysis contexts. To address these challenges, we propose BugScope, an
LLM-driven multi-agent system that emulates how human auditors learn new bug
patterns from representative examples and apply that knowledge during code
auditing. Given a set of examples illustrating both buggy and non-buggy
behaviors, BugScope synthesizes a retrieval strategy to extract relevant
detection contexts via program slicing and then constructs a tailored detection
prompt to guide accurate reasoning by the LLM. Our evaluation on a curated
dataset of 40 real-world bugs drawn from 21 widely-used open-source projects
demonstrates that BugScope achieves 87.04% precision and 90.00% recall,
surpassing state-of-the-art industrial tools by 0.44 in F1 score. Further
testing on large-scale open-source systems, including the Linux kernel,
uncovered 141 previously unknown bugs, of which 78 have been fixed and 7
confirmed by developers, highlighting BugScope's substantial practical impact.

</details>


### [174] [Do AI models help produce verified bug fixes?](https://arxiv.org/abs/2507.15822)
*Li Huang,Ilgiz Mustafin,Marco Piccioni,Alessandro Schena,Reto Weber,Bertrand Meyer*

Main category: cs.SE

TL;DR: 研究探讨LLM在自动程序修复中的实际效果，通过实验发现与预期差异，并提供了方法、行为分析和优化建议。


<details>
  <summary>Details</summary>
Motivation: 探讨AI技术（尤其是LLM）是否能显著提升自动程序修复（APR）的效果，以及程序员如何结合LLM与自身技能进行调试。

Method: 采用Goal-Query-Metric方法，将研究问题分为目标、具体问题和测量指标，并通过编程证明环境验证修复的正确性。两组程序员（一组使用LLM，另一组不使用）参与实验，全程记录行为数据。

Result: 研究结果与预期不同，揭示了LLM在调试和APR中的实际作用。贡献包括实验方法、行为分析、LLM使用模式分类（7类）及优化建议。

Conclusion: 研究表明，AI和LLM在程序调试和自动程序修复（APR）中具有潜在作用，但实际效果与预期存在差异。研究提供了详细的实验方法、程序员行为分析、LLM使用模式分类以及优化建议。

Abstract: Among areas of software engineering where AI techniques -- particularly,
Large Language Models -- seem poised to yield dramatic improvements, an
attractive candidate is Automatic Program Repair (APR), the production of
satisfactory corrections to software bugs. Does this expectation materialize in
practice? How do we find out, making sure that proposed corrections actually
work? If programmers have access to LLMs, how do they actually use them to
complement their own skills?
  To answer these questions, we took advantage of the availability of a
program-proving environment, which formally determines the correctness of
proposed fixes, to conduct a study of program debugging with two randomly
assigned groups of programmers, one with access to LLMs and the other without,
both validating their answers through the proof tools. The methodology relied
on a division into general research questions (Goals in the Goal-Query-Metric
approach), specific elements admitting specific answers (Queries), and
measurements supporting these answers (Metrics). While applied so far to a
limited sample size, the results are a first step towards delineating a proper
role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the
use of AI for debugging and APR. The contributions also include: a detailed
methodology for experiments in the use of LLMs for debugging, which other
projects can reuse; a fine-grain analysis of programmer behavior, made possible
by the use of full-session recording; a definition of patterns of use of LLMs,
with 7 distinct categories; and validated advice for getting the best of LLMs
for debugging and Automatic Program Repair.

</details>


### [175] [Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering](https://arxiv.org/abs/2507.15828)
*Mauro Marcelino,Marcos Alves,Bianca Trinkenreich,Bruno Cartaxo,Sérgio Soares,Simone D. J. Barbosa,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文描述了一个实验协议，用于评估由LLM生成的证据简报在内容保真度、易理解性和实用性方面与人工简报的对比。使用基于RAG的LLM工具生成简报，并通过对照实验进行评估。


<details>
  <summary>Details</summary>
Motivation: 尽管从业者和研究人员认为证据简报有用，但其制作需要人工劳动，这可能是广泛采用的主要挑战。

Method: 我们开发了一个基于RAG的LLM工具来生成证据简报，并使用该工具自动生成了两份先前研究中手动生成的证据简报。我们设计了一个对照实验，以评估LLM生成的简报在内容保真度、易理解性和实用性方面与人工制作的简报相比如何。

Result: 实验结果待报告。

Conclusion: 结论将根据实验结果而定。

Abstract: [Context] An evidence briefing is a concise and objective transfer medium
that can present the main findings of a study to software engineers in the
industry. Although practitioners and researchers have deemed Evidence Briefings
useful, their production requires manual labor, which may be a significant
challenge to their broad adoption. [Goal] The goal of this registered report is
to describe an experimental protocol for evaluating LLM-generated evidence
briefings for secondary studies in terms of content fidelity, ease of
understanding, and usefulness, as perceived by researchers and practitioners,
compared to human-made briefings. [Method] We developed an RAG-based LLM tool
to generate evidence briefings. We used the tool to automatically generate two
evidence briefings that had been manually generated in previous research
efforts. We designed a controlled experiment to evaluate how the LLM-generated
briefings compare to the human-made ones regarding perceived content fidelity,
ease of understanding, and usefulness. [Results] To be reported after the
experimental trials. [Conclusion] Depending on the experiment results.

</details>


### [176] [Observing Fine-Grained Changes in Jupyter Notebooks During Development Time](https://arxiv.org/abs/2507.15831)
*Sergey Titov,Konstantin Grotov,Cristina Sarasua,Yaroslav Golubev,Dhivyabharathi Ramasamy,Alberto Bacchelli,Abraham Bernstein,Timofey Bryksin*

Main category: cs.SE

TL;DR: 论文填补了数据科学领域对Jupyter笔记本开发过程研究的空白，通过工具集收集并分析开发数据，发现笔记本广泛用于调试，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 填补数据科学领域对计算笔记本开发过程研究的空白，特别是在细粒度日志分析方面的不足。

Method: 引入工具集收集Jupyter笔记本开发时的代码变更，收集了20名开发者超过100小时的工作数据，形成包含2,655个单元格和9,207次执行的数据集，并对其进行分析。

Result: 分析发现笔记本中的变更多为小规模修复和代码迭代，表明其作为调试工具的用途。

Conclusion: 该论文通过分析Jupyter笔记本的开发过程，揭示了笔记本不仅用于开发和探索，还作为调试工具的使用模式，并提出了未来研究方向。

Abstract: In software engineering, numerous studies have focused on the analysis of
fine-grained logs, leading to significant innovations in areas such as
refactoring, security, and code completion. However, no similar studies have
been conducted for computational notebooks in the context of data science.
  To help bridge this research gap, we make three scientific contributions: we
(1) introduce a toolset for collecting code changes in Jupyter notebooks during
development time; (2) use it to collect more than 100 hours of work related to
a data analysis task and a machine learning task (carried out by 20 developers
with different levels of expertise), resulting in a dataset containing 2,655
cells and 9,207 cell executions; and (3) use this dataset to investigate the
dynamic nature of the notebook development process and the changes that take
place in the notebooks.
  In our analysis of the collected data, we classified the changes made to the
cells between executions and found that a significant number of these changes
were relatively small fixes and code iteration modifications. This suggests
that notebooks are used not only as a development and exploration tool but also
as a debugging tool. We report a number of other insights and propose potential
future research directions on the novel data.

</details>


### [177] [Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs](https://arxiv.org/abs/2411.01789)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 论文提出用Javadocs和LLMs自动化生成测试预言，实验证明其高效且准确。


<details>
  <summary>Details</summary>
Motivation: 自动化测试预言生成是一个相对未被充分探索的领域，尤其是如何从非正式的自然语言描述中提取预期行为。

Method: 利用Javadocs中的自然语言描述，结合LLMs技术，自动化生成测试预言。

Result: 实验显示，LLMs生成的测试预言中98.8%可编译，96.4%准确反映了预期属性。

Conclusion: 论文提出了一种利用Javadocs和大型语言模型（LLMs）自动化生成测试预言的方法，实验证明该方法能高效生成可编译且准确的测试预言。

Abstract: Software testing remains the most widely used methodology for validating
quality of code. However, effectiveness of testing critically depends on the
quality of test suites used. Test cases in a test suite consist of two
fundamental parts: (1) input values for the code under test, and (2) correct
checks for the outputs it produces. These checks are commonly written as
assertions, and termed test oracles. The last couple of decades have seen much
progress in automated test input generation, e.g., using fuzzing and symbolic
execution. However, automating test oracles remains a relatively less explored
problem area. Indeed, a test oracle by its nature requires knowledge of
expected behavior, which may only be known to the developer and may not not
exist in a formal language that supports automated reasoning.
  Our focus in this paper is automation of test oracles for clients of widely
used Java libraries, e.g., java.lang and java.util packages. Our key insight is
that Javadocs that provide a rich source of information can enable automated
generation of test oracles. Javadocs of the core Java libraries are fairly
detailed documents that contain natural language descriptions of not only how
the libraries behave but also how the clients must (not) use them. We use large
language models as an enabling technology to embody our insight into a
framework for test oracle automation, and evaluate it experimentally. Our
experiments demonstrate that LLMs can generate oracles for checking normal and
exceptional behaviors from Javadocs, with 98.8% of these oracles being
compilable and 96.4% accurately reflecting intended properties. Even for the
few incorrect oracles, errors are minor and can be easily corrected with the
help of additional comment information generated by the LLMs.

</details>


### [178] [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
*Yang Hong,Shan Jiang,Yulei Fu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: LLMs 能有效生成和补全 Alloy 声明式规范，为软件开发提供新工具。


<details>
  <summary>Details</summary>
Motivation: 声明式规范对开发安全可靠的软件系统至关重要，但正确编写规范仍具挑战性，因此探索 LLMs 在此领域的潜力。

Method: 使用 ChatGPT 和 DeepSeek 两种 LLM，通过三种方式评估其能力：1) 从自然语言描述生成完整 Alloy 公式；2) 生成等效的 Alloy 公式；3) 补全 Alloy 公式草图。

Result: 实验结果表明，LLMs 能够从自然语言或 Alloy 输入中合成完整公式，并生成多个独特解，且能成功补全公式草图。

Conclusion: LLMs 在编写声明式规范方面表现出色，能够从自然语言描述中合成完整的 Alloy 公式，并生成等效或补全的公式，为软件开发中规范的编写提供了重要支持。

Abstract: Declarative specifications have a vital role to play in developing safe and
dependable software systems. Writing specifications correctly, however, remains
particularly challenging. This paper presents a controlled experiment on using
large language models (LLMs) to write declarative formulas in the well-known
language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write
complete Alloy formulas from given natural language descriptions (in English).
Two, we employ LLMs to create alternative but equivalent formulas in Alloy with
respect to given Alloy formulas. Three, we employ LLMs to complete sketches of
Alloy formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately represent
the desired properties (that are given in natural language). We conduct the
experimental evaluation using 11 well-studied subject specifications and employ
two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show
that the LLMs generally perform well in synthesizing complete Alloy formulas
from input properties given in natural language or in Alloy, and are able to
enumerate multiple unique solutions. Moreover, the LLMs are also successful at
completing given sketches of Alloy formulas with respect to natural language
descriptions of desired properties (without requiring test cases). We believe
LLMs offer a very exciting advance in our ability to write specifications, and
can help make specifications take a pivotal role in software development and
enhance our ability to build robust software.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [179] [Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach](https://arxiv.org/abs/2507.14249)
*Yuejiao Xie,Maonan Wang,Di Zhou,Man-On Pun,Zhu Han*

Main category: cs.RO

TL;DR: 论文提出MSHA-RL框架，用于UAM系统的实时路径规划，结合通信质量评估和混合注意力机制，优化旅行时间和运营效率，确保安全。


<details>
  <summary>Details</summary>
Motivation: 城市空中交通系统（UAM）作为缓解城市拥堵的解决方案，其路径规划需优先考虑通信质量和实时乘客需求。传统基于预定义路线的轨迹规划缺乏灵活性，无法满足动态乘客需求。

Method: 通过构建无线电地图评估城市空域的通信质量，并引入MSHA-RL框架，该框架首先生成不同数据源之间的对齐，然后利用混合注意力平衡全局和局部信息，实现实时路径规划。

Result: 实验结果表明，该方法能够实现符合通信要求的轨迹规划，减少旅行时间并提高运营效率，同时优先保障乘客安全。

Conclusion: 该论文提出了一种新型的多源混合注意力强化学习框架（MSHA-RL），用于解决城市空中交通（UAM）系统中的路径规划问题，确保通信质量和乘客安全，同时提高运营效率。

Abstract: Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions
to alleviate urban congestion, with path planning becoming a key focus area.
Unlike ground transportation, UAM trajectory planning has to prioritize
communication quality for accurate location tracking in constantly changing
environments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,
requires adaptive planning to respond to real-time passenger requests,
especially in ride-sharing scenarios where passenger demands are unpredictable
and dynamic. However, conventional trajectory planning strategies based on
predefined routes lack the flexibility to meet varied passenger ride demands.
To address these challenges, this work first proposes constructing a radio map
to evaluate the communication quality of urban airspace. Building on this, we
introduce a novel Multi-Source Hybrid Attention Reinforcement Learning
(MSHA-RL) framework for the challenge of effectively focusing on passengers and
UAM locations, which arises from the significant dimensional disparity between
the representations. This model first generates the alignment among diverse
data sources with large gap dimensions before employing hybrid attention to
balance global and local insights, thereby facilitating responsive, real-time
path planning. Extensive experimental results demonstrate that the approach
enables communication-compliant trajectory planning, reducing travel time and
enhancing operational efficiency while prioritizing passenger safety.

</details>


### [180] [A Recursive Lie-Group Formulation for the Second-Order Time Derivatives of the Inverse Dynamics of parallel Kinematic Manipulators](https://arxiv.org/abs/2507.14274)
*Andreas Mueller,Shivesh Kumar,Thomas Kordik*

Main category: cs.RO

TL;DR: 本文首次解决了PKM配备SEA时轨迹控制的逆动力学解二阶导数计算问题，采用李群框架和递归算法，并验证了数值结果。


<details>
  <summary>Details</summary>
Motivation: 尽管串联弹性执行器（SEA）已用于串联机械臂，但并联运动学机械臂（PKM）配备SEA的轨迹控制尚未实现，关键在于逆动力学解的二阶时间导数的高效计算尚未解决。

Method: 利用PKM的特殊拓扑结构，复用串联机器人的递归算法，并采用李群（Lie group）框架推导所有关系。

Result: 数值结果表明，该方法适用于6自由度Gough-Stewart平台（作为外骨骼的一部分）和平面PKM（采用基于平坦性的控制方案）。

Conclusion: 本文首次解决了并联运动学机械臂（PKM）配备串联弹性执行器（SEA）时轨迹控制的逆动力学解的二阶时间导数高效计算问题，并展示了其数值结果。

Abstract: Series elastic actuators (SEA) were introduced for serial robotic arms. Their
model-based trajectory tracking control requires the second time derivatives of
the inverse dynamics solution, for which algorithms were proposed. Trajectory
control of parallel kinematics manipulators (PKM) equipped with SEAs has not
yet been pursued. Key element for this is the computationally efficient
evaluation of the second time derivative of the inverse dynamics solution. This
has not been presented in the literature, and is addressed in the present paper
for the first time. The special topology of PKM is exploited reusing the
recursive algorithms for evaluating the inverse dynamics of serial robots. A
Lie group formulation is used and all relations are derived within this
framework. Numerical results are presented for a 6-DOF Gough-Stewart platform
(as part of an exoskeleton), and for a planar PKM when a flatness-based control
scheme is applied.

</details>


### [181] [Personalized Socially Assistive Robots With End-to-End Speech-Language Models For Well-Being Support](https://arxiv.org/abs/2507.14412)
*Mengxue Fu,Zhonghao Shi,Minyu Huang,Siqi Liu,Mina Kian,Yirui Song,Maja J. Matarić*

Main category: cs.RO

TL;DR: 研究提出使用端到端SLM改进SAR对话系统，通过用户研究发现其共情和自适应响应能力，但非语言行为和反馈仍需优化。


<details>
  <summary>Details</summary>
Motivation: 现有SAR对话系统在实时延迟、反馈和个性化语音对话方面存在限制，研究旨在通过集成端到端SLM解决这些问题。

Method: 通过一个小型的用户内研究（N = 11）评估了基于端到端语音语言模型（SLM）的SAR对话系统的可用性，并通过用户反馈识别了剩余限制。

Result: 参与者认为SLM启用的SAR系统能够提供共情反馈、自然轮转、反馈和自适应响应，但也指出了机器人非语言行为缺乏变化和对话同步性不足，以及SLM的反馈通用且重复。

Conclusion: 该研究强调了实时机器人动作与对话同步、改进提示或微调以生成更符合心理健康实践的输出，以及更具表现力和适应性的语音生成的必要性。

Abstract: Socially assistive robots (SARs) have shown great potential for supplementing
well-being support. However, prior studies have found that existing dialogue
pipelines for SARs remain limited in real-time latency, back-channeling, and
personalized speech dialogue. Toward addressing these limitations, we propose
using integrated end-to-end speech-language models (SLMs) with SARs. This work
1) evaluated the usability of an SLM-enabled SAR dialogue system through a
small user study, and 2) identified remaining limitations through study user
feedback to inform future improvements. We conducted a small within-participant
user study with university students (N = 11) whose results showed that
participants perceived an SLM-enabled SAR system as capable of providing
empathetic feedback, natural turn-taking, back-channeling, and adaptive
responses. We also found that participants reported the robot's nonverbal
behaviors as lacking variability and synchronization with conversation, and the
SLM's verbal feedback as generic and repetitive. These findings highlighted the
need for real-time robot movement synchronized with conversation, improved
prompting or fine-tuning to generate outputs better aligned with mental health
practices, and more expressive, adaptive vocal generation.

</details>


### [182] [Koopman Operator Based Time-Delay Embeddings and State History Augmented LQR for Periodic Hybrid Systems: Bouncing Pendulum and Bipedal Walking](https://arxiv.org/abs/2507.14455)
*Chun-Ming Yang,Pranav A. Bhounsule*

Main category: cs.RO

TL;DR: 时间延迟嵌入技术扩展至周期性非光滑系统，生成线性模型并开发新型LQR控制器。


<details>
  <summary>Details</summary>
Motivation: 证明时间延迟嵌入技术不仅适用于非线性光滑系统，还能用于周期性非光滑或混合系统。

Method: 扩展时间延迟嵌入技术，应用于周期性混合系统（如反弹摆和最简单的步行器），并生成线性模型。

Result: 成功生成了反弹摆和最简单步行器的线性模型，并提出了一种基于状态历史的LQR控制器。

Conclusion: 时间延迟嵌入技术可以扩展到周期性非光滑或混合系统，生成线性状态空间模型，从而提出一种新颖的状态历史增强线性二次调节器（LQR）。

Abstract: Time-delay embedding is a technique that uses snapshots of state history over
time to build a linear state space model of a nonlinear smooth system. We
demonstrate that periodic non-smooth or hybrid system can also be modeled as a
linear state space system using this approach as long as its behavior is
consistent in modes and timings. We extended time-delay embeddings to generate
a linear model of two periodic hybrid systems: the bouncing pendulum and the
simplest walker with control inputs. This leads to a novel state history
augmented linear quadratic regulator (LQR) which uses current and past state
history for feedback control.

</details>


### [183] [A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ Hand-0](https://arxiv.org/abs/2507.14538)
*Jin Chai,Xiang Yao,Mengfan Hou,Yanghong Li,Erbao Dong*

Main category: cs.RO

TL;DR: CYJ Hand-0是一种21自由度仿生灵巧手，采用混合驱动系统（SMA和DC电机），实验验证了其设计有效性。


<details>
  <summary>Details</summary>
Motivation: 开发一种结合形状记忆合金和直流电机的混合驱动系统，以模仿人手骨骼和肌腱肌肉结构。

Method: 采用高强度钓鱼线作为人工肌腱，3D打印AlSi10Mg金属框架，结合线性电机和SMA模块控制手指运动。

Result: 机械和运动学实验验证了设计的有效性，并展示了仿生灵巧性。

Conclusion: CYJ Hand-0的设计和混合驱动系统被实验验证有效，展示了仿生灵巧性。

Abstract: CYJ Hand-0 is a 21-DOF humanoid dexterous hand featuring a hybrid
tendon-driven actuation system that combines shape memory alloys (SMAs) and DC
motors. The hand employs high-strength fishing line as artificial tendons and
uses a fully 3D-printed AlSi10Mg metal frame designed to replicate the skeletal
and tendon-muscle structure of the human hand. A linear motor-driven module
controls finger flexion, while an SMA-based module enables finger extension and
lateral abduction. These modules are integrated into a compact hybrid actuation
unit mounted on a custom rear support structure. Mechanical and kinematic
experiments, conducted under an Arduino Mega 2560-based control system,
validate the effectiveness of the design and demonstrate its biomimetic
dexterity.

</details>


### [184] [BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree, Temporal Logic and Dynamical Movement Primitives](https://arxiv.org/abs/2507.14582)
*Zezhi Liu,Shizhen Wu,Hanqian Luo,Deyun Qin,Yongchun Fang*

Main category: cs.RO

TL;DR: A hierarchical framework (BT-TL-DMPs) combining Behavior Trees, Temporal Logic, and DMPs improves robot skill generalization in complex, long-horizon tasks by bridging symbolic and motion planning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generalizing learned manipulation skills to novel scenarios in long-horizon tasks, especially in environments with different task and motion requirements.

Method: The framework integrates Behavior Tree (BT), Temporal Logic (TL), and Dynamical Movement Primitives (DMPs), employing Signal Temporal Logic (STL) to specify task requirements and constraints, which are transformed into modular BTs for decision-making, and an STL-constrained DMP optimization method to adapt motion primitives.

Result: Simulations and real-world experiments validate the framework's generalization capabilities under various STL constraints, demonstrating its effectiveness in complex robotic manipulation tasks.

Conclusion: The proposed hierarchical framework BT-TL-DMPs effectively bridges the symbolic-motion gap, enabling reliable and generalizable autonomous manipulation for complex robotic tasks.

Abstract: In the field of Learning from Demonstration (LfD), enabling robots to
generalize learned manipulation skills to novel scenarios for long-horizon
tasks remains challenging. Specifically, it is still difficult for robots to
adapt the learned skills to new environments with different task and motion
requirements, especially in long-horizon, multi-stage scenarios with intricate
constraints. This paper proposes a novel hierarchical framework, called
BT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and
Dynamical Movement Primitives (DMPs) to address this problem. Within this
framework, Signal Temporal Logic (STL) is employed to formally specify complex,
long-horizon task requirements and constraints. These STL specifications are
systematically transformed to generate reactive and modular BTs for high-level
decision-making task structure. An STL-constrained DMP optimization method is
proposed to optimize the DMP forcing term, allowing the learned motion
primitives to adapt flexibly while satisfying intricate spatiotemporal
requirements and, crucially, preserving the essential dynamics learned from
demonstrations. The framework is validated through simulations demonstrating
generalization capabilities under various STL constraints and real-world
experiments on several long-horizon robotic manipulation tasks. The results
demonstrate that the proposed framework effectively bridges the symbolic-motion
gap, enabling more reliable and generalizable autonomous manipulation for
complex robotic tasks.

</details>


### [185] [Koopman Operator Based Linear Model Predictive Control for 2D Quadruped Trotting, Bounding, and Gait Transition](https://arxiv.org/abs/2507.14605)
*Chun-Ming Yang,Pranav A. Bhounsule*

Main category: cs.RO

TL;DR: 结合Koopman算子与LMPC，四足机器人成功实现在线步态生成与转换。


<details>
  <summary>Details</summary>
Motivation: 解决LMPC因线性化运动方程导致的解质量不佳问题，提升四足机器人在线控制的性能。

Method: 采用Koopman算子理论和EDMD方法构建高维空间中的线性模型，保留EOM的非线性特性，并针对空中和地面接触阶段使用不同线性模型。

Result: 在水平和粗糙地形中实现了跳跃、小跑以及步态转换。

Conclusion: 本文通过结合Koopman算子理论和LMPC，成功实现了四足机器人在多种地形下的步态生成和转换，展示了在线控制的潜力。

Abstract: Online optimal control of quadrupedal robots would enable them to plan their
movement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged
as a practical approach for real-time control. In LMPC, an optimization problem
with a quadratic cost and linear constraints is formulated over a finite
horizon and solved on the fly. However, LMPC relies on linearizing the
equations of motion (EOM), which may lead to poor solution quality. In this
paper, we use Koopman operator theory and the Extended Dynamic Mode
Decomposition (EDMD) to create a linear model of the system in high dimensional
space, thus retaining the nonlinearity of the EOM. We model the aerial phase
and ground contact phases using different linear models. Then, using LMPC, we
demonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait
transitions in level and rough terrains. The main novelty is the use of Koopman
operator theory to create hybrid models of a quadrupedal system and demonstrate
the online generation of multiple gaits and gaits transitions.

</details>


### [186] [Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks](https://arxiv.org/abs/2507.14694)
*Yue Ma,Kanglei Zhou,Fuyang Yu,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.RO

TL;DR: ProbHMI通过可逆网络和显式潜在分布预测，实现了3D人体运动预测中的不确定性量化，适用于安全关键场景。


<details>
  <summary>Details</summary>
Motivation: 现有多样化运动预测方法因隐式概率表示难以量化不确定性，而安全关键场景（如人机协作）需要准确的不确定性估计以降低风险。

Method: 提出ProbHMI，利用可逆网络在解耦的潜在空间中参数化姿态，并通过预测模块显式建模未来潜在分布。

Result: 在基准测试中，ProbHMI在确定性和多样化预测上均表现优异，且验证了不确定性校准的有效性。

Conclusion: ProbHMI 通过引入可逆网络和显式潜在分布预测，有效解决了3D人体运动预测中的不确定性量化问题，为安全关键场景提供了可靠支持。

Abstract: 3D human motion forecasting aims to enable autonomous applications.
Estimating uncertainty for each prediction (i.e., confidence based on
probability density or quantile) is essential for safety-critical contexts like
human-robot collaboration to minimize risks. However, existing diverse motion
forecasting approaches struggle with uncertainty quantification due to implicit
probabilistic representations hindering uncertainty modeling. We propose
ProbHMI, which introduces invertible networks to parameterize poses in a
disentangled latent space, enabling probabilistic dynamics modeling. A
forecasting module then explicitly predicts future latent distributions,
allowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI
achieves strong performance for both deterministic and diverse prediction while
validating uncertainty calibration, critical for risk-aware decision making.

</details>


### [187] [Corridor-based Adaptive Control Barrier and Lyapunov Functions for Safe Mobile Robot Navigation](https://arxiv.org/abs/2507.14700)
*Nicholas Mohammad,Nicola Bezzo*

Main category: cs.RO

TL;DR: 提出了一种结合CLF和CBF的MPCC框架，通过动态调整CBF参数提升安全性，并在仿真和实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有MPCC方法缺乏形式化安全保证，难以在未知杂乱环境中确保机器人导航的安全性。

Method: 采用Control Lyapunov Function (CLF)和Control Barrier Function (CBF)增强的Model Predictive Contour Control (MPCC)框架，结合Soft Actor-Critic (SAC)策略动态调整CBF参数。

Result: 通过仿真和实物实验验证了该框架在未知杂乱环境中的安全导航能力。

Conclusion: 提出的结合CLF和CBF的MPCC框架有效提升了未知杂乱环境中机器人导航的安全性，并通过动态调整CBF参数增强了可行性。

Abstract: Safe navigation in unknown and cluttered environments remains a challenging
problem in robotics. Model Predictive Contour Control (MPCC) has shown promise
for performant obstacle avoidance by enabling precise and agile trajectory
tracking, however, existing methods lack formal safety assurances. To address
this issue, we propose a general Control Lyapunov Function (CLF) and Control
Barrier Function (CBF) enabled MPCC framework that enforces safety constraints
derived from a free-space corridor around the planned trajectory. To enhance
feasibility, we dynamically adapt the CBF parameters at runtime using a Soft
Actor-Critic (SAC) policy. The approach is validated with extensive simulations
and an experiment on mobile robot navigation in unknown cluttered environments.

</details>


### [188] [Leveraging Extrinsic Dexterity for Occluded Grasping on Grasp Constraining Walls](https://arxiv.org/abs/2507.14721)
*Keita Kobashi,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: 提出分层强化学习框架结合CVAE，解决机器人抓取被遮挡物体问题，实验显示良好泛化与模拟到现实迁移性能。


<details>
  <summary>Details</summary>
Motivation: 解决因环境遮挡导致的主要抓取配置不可用问题，尤其是在现实场景中可能遇到的高墙或大墙等复杂情况，传统平行夹爪机器人难以应对。

Method: 采用分层强化学习（RL）框架，高层策略使用Q学习选择动作类型，低层技能通过CVAE在连续空间中采样具体动作。结合领域随机化训练低层技能以提升泛化能力。

Result: 实验验证了该方法在模拟和现实场景中的有效性，成功抓取六种不同物体，表现出较高的成功率和泛化能力。

Conclusion: 该研究提出的分层强化学习框架结合CVAE，能有效解决机器人抓取被遮挡物体的问题，并在实验中展示了良好的泛化能力和稳健的模拟到现实的迁移性能。

Abstract: This study addresses the problem of occluded grasping, where primary grasp
configurations of an object are not available due to occlusion with
environment. Simple parallel grippers often struggle with such tasks due to
limited dexterity and actuation constraints. Prior works have explored object
pose reorientation such as pivoting by utilizing extrinsic contacts between an
object and an environment feature like a wall, to make the object graspable.
However, such works often assume the presence of a short wall, and this
assumption may not always hold in real-world scenarios. If the wall available
for interaction is too large or too tall, the robot may still fail to grasp the
object even after pivoting, and the robot must combine different types of
actions to grasp. To address this, we propose a hierarchical reinforcement
learning (RL) framework. We use Q-learning to train a high-level policy that
selects the type of action expected to yield the highest reward. The selected
low-level skill then samples a specific robot action in continuous space. To
guide the robot to an appropriate location for executing the selected action,
we adopt a Conditional Variational Autoencoder (CVAE). We condition the CVAE on
the object point cloud and the skill ID, enabling it to infer a suitable
location based on the object geometry and the selected skill. To promote
generalization, we apply domain randomization during the training of low-level
skills. The RL policy is trained entirely in simulation with a box-like object
and deployed to six objects in real world. We conduct experiments to evaluate
our method and demonstrate both its generalizability and robust sim-to-real
transfer performance with promising success rates.

</details>


### [189] [X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots](https://arxiv.org/abs/2507.14731)
*Haitong Wang,Aaron Hao Tan,Angus Fung,Goldie Nejat*

Main category: cs.RO

TL;DR: X-Nav 是一个跨平台通用导航框架，通过两阶段学习实现单一策略适用于多种机器人，并在模拟和真实环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有导航方法通常针对特定机器人设计，缺乏跨平台的通用性。X-Nav 旨在解决这一问题，实现单一策略适用于多种机器人平台。

Method: X-Nav 采用两阶段学习：1) 通过深度强化学习训练多个专家策略；2) 通过导航动作分块与Transformer（Nav-ACT）从专家策略中蒸馏出单一通用策略。

Result: X-Nav 在模拟实验中实现了对未见过的机器人平台和逼真环境的零样本迁移，并在真实环境中验证了其通用性。

Conclusion: X-Nav 框架通过两阶段学习方法成功实现了跨不同机器人平台的通用导航策略，并在模拟和真实环境中验证了其有效性。

Abstract: Existing navigation methods are primarily designed for specific robot
embodiments, limiting their generalizability across diverse robot platforms. In
this paper, we introduce X-Nav, a novel framework for end-to-end
cross-embodiment navigation where a single unified policy can be deployed
across various embodiments for both wheeled and quadrupedal robots. X-Nav
consists of two learning stages: 1) multiple expert policies are trained using
deep reinforcement learning with privileged observations on a wide range of
randomly generated robot embodiments; and 2) a single general policy is
distilled from the expert policies via navigation action chunking with
transformer (Nav-ACT). The general policy directly maps visual and
proprioceptive observations to low-level control commands, enabling
generalization to novel robot embodiments. Simulated experiments demonstrated
that X-Nav achieved zero-shot transfer to both unseen embodiments and
photorealistic environments. A scalability study showed that the performance of
X-Nav improves when trained with an increasing number of randomly generated
embodiments. An ablation study confirmed the design choices of X-Nav.
Furthermore, real-world experiments were conducted to validate the
generalizability of X-Nav in real-world environments.

</details>


### [190] [KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D Correspondence Learning](https://arxiv.org/abs/2507.14820)
*Bingran Chen,Baorun Li,Jian Yang,Yong Liu,Guangyao Zhai*

Main category: cs.RO

TL;DR: KGN-Pro是一种新型抓取网络，通过概率PnP层直接进行3D优化，提高了6-DoF抓取估计的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在6-DoF抓取估计中因小物体、传感器噪声、昂贵标注或离散化问题导致的性能限制。

Method: 结合RGB-D图像生成关键点地图和2D置信度图，通过概率PnP层进行端到端学习。

Result: 在仿真和真实平台上，KGN-Pro在抓取覆盖率和成功率上优于现有方法。

Conclusion: KGN-Pro通过整合3D优化，显著提升了抓取性能，为高自由度机器人操作任务提供了更高效的解决方案。

Abstract: High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation
to serve as a basic function. Previous approaches either directly generate
grasps from point-cloud data, suffering from challenges with small objects and
sensor noise, or infer 3D information from RGB images, which introduces
expensive annotation requirements and discretization issues. Recent methods
mitigate some challenges by retaining a 2D representation to estimate grasp
keypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF
poses. However, these methods are limited by their non-differentiable nature
and reliance solely on 2D supervision, which hinders the full exploitation of
rich 3D information. In this work, we present KGN-Pro, a novel grasping network
that preserves the efficiency and fine-grained object grasping of previous KGNs
while integrating direct 3D optimization through probabilistic PnP layers.
KGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further
outputs a 2D confidence map to weight keypoint contributions during
re-projection error minimization. By modeling the weighted sum of squared
re-projection errors probabilistically, the network effectively transmits 3D
supervision to its 2D keypoint predictions, enabling end-to-end learning.
Experiments on both simulated and real-world platforms demonstrate that KGN-Pro
outperforms existing methods in terms of grasp cover rate and success rate.

</details>


### [191] [CoMoCAVs: Cohesive Decision-Guided Motion Planning for Connected and Autonomous Vehicles with Multi-Policy Reinforcement Learning](https://arxiv.org/abs/2507.14903)
*Pan Hu*

Main category: cs.RO

TL;DR: CDGMP框架结合MoE和多策略强化学习，通过模块化设计提升CAVs的决策和运动规划能力，仿真验证了其可靠性。


<details>
  <summary>Details</summary>
Motivation: 在CAVs背景下，实现灵活安全的车道选择和精确的轨迹执行仍是一个重大挑战。

Method: 提出了一种名为CDGMP的框架，该框架使用MoE架构和多策略强化学习，通过门控机制协调多个专门子网络，将复杂的驾驶任务分解为模块化组件。

Result: 仿真结果表明CDGMP在车道选择和运动规划方面表现可靠。

Conclusion: CDGMP框架通过整合决策制定和运动规划，结合MoE架构和多策略强化学习，为CAVs提供了适应性强、鲁棒性高的解决方案，同时为其他高维决策和控制任务奠定了基础。

Abstract: Autonomous driving demands reliable and efficient solutions to closely
related problems such as decision-making and motion planning. In this work,
decision-making refers specifically to highway lane selection, while motion
planning involves generating control commands (such as speed and steering) to
reach the chosen lane. In the context of Connected Autonomous Vehicles (CAVs),
achieving both flexible and safe lane selection alongside precise trajectory
execution remains a significant challenge. This paper proposes a framework
called Cohesive Decision-Guided Motion Planning (CDGMP), which tightly
integrates decision-making and motion planning using a Mixture of Experts (MoE)
inspired architecture combined with multi-policy reinforcement learning. By
coordinating multiple specialized sub-networks through a gating mechanism, the
method decomposes the complex driving task into modular components. Each
sub-network focuses on a specific aspect of driving, improving efficiency by
activating only the most relevant modules during inference. This design also
enhances safety through modular specialization. CDGMP improves the adaptability
and robustness of CAVs across diverse traffic scenarios, offering a scalable
solution to real-world autonomy challenges. The architectural principles behind
CDGMP, especially the use of MoE, also provide a strong foundation for other
high-dimensional decision and control tasks. Simulation results (available at
https://youtu.be/_-4OXNHV0UY) demonstrate reliable performance in both lane
selection and motion planning.

</details>


### [192] [One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner](https://arxiv.org/abs/2507.14914)
*Zhexuan Xu,Jie Wang,Siyuan Xu,Zijie Geng,Mingxuan Yuan,Feng Wu*

Main category: cs.RO

TL;DR: Flora 是一种三阶段的 rectilinear floorplanner，通过 feedthrough 和 placement aware 优化，显著提升芯片 PPA 指标。


<details>
  <summary>Details</summary>
Motivation: 现有 floorplanning 方法难以与后续物理设计阶段集成，导致模块内组件布局和模块间 feedthrough 不理想。

Method: Flora 采用三阶段方法：1) 使用 wiremask 和 position mask 进行粗粒度优化；2) 在固定轮廓约束下通过局部调整模块形状实现零空白布局；3) 使用快速树搜索方法放置组件并调整模块边界。

Result: Flora 平均减少了 6% 的 HPWL、5.16% 的 FTpin 和 29.15% 的 FTmod，组件布局性能提升了 14%。

Conclusion: Flora 是一种三阶段的 feedthrough 和 placement aware 的 rectilinear floorplanner，显著优化了芯片的 PPA 指标，并在实验中优于现有技术。

Abstract: Floorplanning determines the shapes and locations of modules on a chip canvas
and plays a critical role in optimizing the chip's Power, Performance, and Area
(PPA) metrics. However, existing floorplanning approaches often fail to
integrate with subsequent physical design stages, leading to suboptimal
in-module component placement and excessive inter-module feedthrough. To tackle
this challenge, we propose Flora, a three-stage feedthrough and placement aware
rectilinear floorplanner. In the first stage, Flora employs wiremask and
position mask techniques to achieve coarse-grained optimization of HPWL and
feedthrough. In the second stage, under the constraint of a fixed outline,
Flora achieves a zero-whitespace layout by locally resizing module shapes,
thereby performing fine-grained optimization of feedthrough and improving
component placement. In the third stage, Flora utilizes a fast tree
search-based method to efficiently place components-including macros and
standard cells-within each module, subsequently adjusting module boundaries
based on the placement results to enable cross-stage optimization. Experimental
results show that Flora outperforms recent state-of-the-art floorplanning
approaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,
29.15% in FTmod, and a 14% improvement in component placement performance.

</details>


### [193] [Digital twin and extended reality for teleoperation of the electric vehicle battery disassembly](https://arxiv.org/abs/2507.14929)
*Tero Kaarlela,Sami Salo,Jose Outeiro*

Main category: cs.RO

TL;DR: 提出混合远程操作与自动化系统，安全高效拆卸EVB，试点验证用户友好性及经济潜力。


<details>
  <summary>Details</summary>
Motivation: 手动拆卸EVB存在安全隐患（如触电和有毒化学物质），且效率低下。研究旨在通过技术手段提高安全性、适应性和效率。

Method: 采用远程操作系统，结合RGB摄像头对齐物理和数字孪生，基于ROS中间件构建机器人数字孪生，实现人机协同的拆卸序列创建与保存。

Result: 在线试点研究表明，该方法具有用户友好性，并能减少对劳动力的依赖，提高电池回收的吞吐量。

Conclusion: 该研究提出了一种结合远程操作和自动化的混合方法，用于安全、高效地拆卸和分类电动汽车电池（EVB），并通过在线试点研究验证了其用户友好性和潜在经济效益。

Abstract: Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a
sustainable transition to electric vehicles by enabling a closed-loop supply
chain. Currently, the manual disassembly process exposes workers to hazards,
including electrocution and toxic chemicals. We propose a teleoperated system
for the safe disassembly and sorting of EVBs. A human-in-the-loop can create
and save disassembly sequences for unknown EVB types, enabling future
automation. An RGB camera aligns the physical and digital twins of the EVB, and
the digital twin of the robot is based on the Robot Operating System (ROS)
middleware. This hybrid approach combines teleoperation and automation to
improve safety, adaptability, and efficiency in EVB disassembly and sorting.
The economic contribution is realized by reducing labor dependency and
increasing throughput in battery recycling. An online pilot study was set up to
evaluate the usability of the presented approach, and the results demonstrate
the potential as a user-friendly solution.

</details>


### [194] [Designing Robots with, not for: A Co-Design Framework for Empowering Interactions in Forensic Psychiatry](https://arxiv.org/abs/2507.14931)
*Qiaoqiao Ren,Remko Proesmans,Arend Pissens,Lara Dehandschutter,William Denecker,Lotte Rouckhout,Joke Carrette,Peter Vanhopplinus,Tony Belpaeme,Francis wyffels*

Main category: cs.RO

TL;DR: 研究通过四次共同设计研讨会开发伴侣机器人，强调在法医心理健康护理中赋予患者设计权力，确保其声音被听到，并根据情绪状态调整方案。


<details>
  <summary>Details</summary>
Motivation: 法医心理健康护理环境通常官僚主义严重、风险规避且自主权受限，患者常感到生活失控，导致心理压力加剧。研究探索如何通过共同设计开发伴侣机器人，以监控和调节压力，同时跟踪患者互动行为以实现长期干预。

Method: 在法医精神病诊所进行了四次共同设计研讨会，参与者包括患者、护理人员和治疗师。过程从向治疗师展示初步推测原型开始，随后与患者进行创意构思会议，第三次研讨会聚焦于定义期望的功能和情感反应，并计划进行最终原型演示以收集患者直接反馈。

Result: 研究强调了在共同设计过程中赋予患者权力的重要性，并根据患者当前情绪状态调整设计提案。

Conclusion: 研究发现，在设计中赋予患者权力并基于他们的情绪状态调整方案至关重要，目标是确保每位患者的声音被听到。

Abstract: Forensic mental health care involves the treatment of individuals with severe
mental disorders who have committed violent offences. These settings are often
characterized by high levels of bureaucracy, risk avoidance, and restricted
autonomy. Patients frequently experience a profound loss of control over their
lives, leading to heightened psychological stress-sometimes resulting in
isolation as a safety measure. In this study, we explore how co-design can be
used to collaboratively develop a companion robot that helps monitor and
regulate stress while maintaining tracking of the patients' interaction
behaviours for long-term intervention. We conducted four co-design workshops in
a forensic psychiatric clinic with patients, caregivers, and therapists. Our
process began with the presentation of an initial speculative prototype to
therapists, enabling reflection on shared concerns, ethical risks, and
desirable features. This was followed by a creative ideation session with
patients, a third workshop focused on defining desired functions and emotional
responses, and we are planning a final prototype demo to gather direct patient
feedback. Our findings emphasize the importance of empowering patients in the
design process and adapting proposals based on their current emotional state.
The goal was to empower the patient in the design process and ensure each
patient's voice was heard.

</details>


### [195] [Heterogeneous object manipulation on nonlinear soft surface through linear controller](https://arxiv.org/abs/2507.14967)
*Pratik Ingle,Kasper Støy,Andres Faiña*

Main category: cs.RO

TL;DR: 研究提出了一种简单、精确且鲁棒的PID闭环控制策略，用于异构物体在软机器人操作表面的控制，避免了传统学习方法的高训练需求，实验验证了其高度通用性。


<details>
  <summary>Details</summary>
Motivation: 高密度执行器阵列带来的高自由度增加了控制复杂度，限制了操作表面的实际应用，现有学习方法需要大量训练样本且难以泛化。

Method: 采用几何变换驱动的PID控制器，直接将倾斜角度控制输出映射到执行器命令，避免了复杂的黑盒训练需求。

Result: 通过仿真和物理系统实验，成功操作了包括脆弱物体（如鸡蛋和苹果）在内的多种几何、重量和纹理的物体。

Conclusion: 该研究提出了一种基于PID的线性闭环反馈控制策略，适用于异构物体在MANTA-RAY上的操作，证明了其高度通用性及实际应用的可行性。

Abstract: Manipulation surfaces indirectly control and reposition objects by actively
modifying their shape or properties rather than directly gripping objects.
These surfaces, equipped with dense actuator arrays, generate dynamic
deformations. However, a high-density actuator array introduces considerable
complexity due to increased degrees of freedom (DOF), complicating control
tasks. High DOF restrict the implementation and utilization of manipulation
surfaces in real-world applications as the maintenance and control of such
systems exponentially increase with array/surface size. Learning-based control
approaches may ease the control complexity, but they require extensive training
samples and struggle to generalize for heterogeneous objects. In this study, we
introduce a simple, precise and robust PID-based linear close-loop feedback
control strategy for heterogeneous object manipulation on MANTA-RAY
(Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation
density). Our approach employs a geometric transformation-driven PID
controller, directly mapping tilt angle control outputs(1D/2D) to actuator
commands to eliminate the need for extensive black-box training. We validate
the proposed method through simulations and experiments on a physical system,
successfully manipulating objects with diverse geometries, weights and
textures, including fragile objects like eggs and apples. The outcomes
demonstrate that our approach is highly generalized and offers a practical and
reliable solution for object manipulation on soft robotic manipulation,
facilitating real-world implementation without prohibitive training demands.

</details>


### [196] [FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models](https://arxiv.org/abs/2507.14975)
*Yufan Song,Jiatao Zhang,Zeng Gu,Qingmiao Liang,Tuocheng Hu,Wei Song,Shiqiang Zhu*

Main category: cs.RO

TL;DR: FCRF是一种新型导师-执行者架构，通过灵活自我反思和整合历史经验，显著提升了机器人在复杂长期任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法因自我反思机制不够灵活而效果受限，受人类认知适应启发，旨在提升机器人任务执行的可靠性和灵活性。

Method: 提出了灵活的建构主义反思框架（FCRF），采用导师-执行者架构，使LLM能够基于任务难度进行灵活自我反思，并整合历史经验与失败教训。

Result: 在AlfWorld模拟和真实环境部署中，FCRF在多样化家庭任务上表现出显著性能提升。

Conclusion: FCRF显著提升了复杂长期机器人任务中的整体性能和自我反思灵活性。

Abstract: Autonomous error correction is critical for domestic robots to achieve
reliable execution of complex long-horizon tasks. Prior work has explored
self-reflection in Large Language Models (LLMs) for task planning error
correction; however, existing methods are constrained by inflexible
self-reflection mechanisms that limit their effectiveness. Motivated by these
limitations and inspired by human cognitive adaptation, we propose the Flexible
Constructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture
that enables LLMs to perform flexible self-reflection based on task difficulty,
while constructively integrating historical valuable experience with failure
lessons. We evaluated FCRF on diverse domestic tasks through simulation in
AlfWorld and physical deployment in the real-world environment. Experimental
results demonstrate that FCRF significantly improves overall performance and
self-reflection flexibility in complex long-horizon robotic tasks.

</details>


### [197] [CPED-NCBFs: A Conformal Prediction for Expert Demonstration-based Neural Control Barrier Functions](https://arxiv.org/abs/2507.15022)
*Sumeadh MS,Kevin Dsouza,Ravi Prakash*

Main category: cs.RO

TL;DR: 提出CPED-NCBFs验证策略，有效验证学习到的NCBF，并在实际系统中验证其效果。


<details>
  <summary>Details</summary>
Motivation: 现有验证方法（如SMT求解器、混合整数规划、区间传播方法）常引入宽松且保守的边界，无法有效验证学习到的NCBF是否在整个状态空间内确保安全。

Method: 使用CPED-NCBFs（基于分割共形预测的验证策略）来验证从专家演示中学习的NCBF。

Result: CPED-NCBFs在点质量系统和非完整模型中的验证结果表明其有效性。

Conclusion: CPED-NCBFs作为一种基于分割共形预测的验证策略，有效验证了从专家演示中学习的NCBF，并在点质量系统和非完整模型中验证了其有效性。

Abstract: Among the promising approaches to enforce safety in control systems, learning
Control Barrier Functions (CBFs) from expert demonstrations has emerged as an
effective strategy. However, a critical challenge remains: verifying that the
learned CBFs truly enforce safety across the entire state space. This is
especially difficult when CBF is represented using neural networks (NCBFs).
Several existing verification techniques attempt to address this problem
including SMT-based solvers, mixed-integer programming (MIP), and interval or
bound-propagation methods but these approaches often introduce loose,
conservative bounds. To overcome these limitations, in this work we use
CPED-NCBFs a split-conformal prediction based verification strategy to verify
the learned NCBF from the expert demonstrations. We further validate our method
on point mass systems and unicycle models to demonstrate the effectiveness of
the proposed theory.

</details>


### [198] [Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper](https://arxiv.org/abs/2507.15062)
*Xinyue Zhu,Binghao Huang,Yunzhu Li*

Main category: cs.RO

TL;DR: 便携触觉夹爪+跨模态学习框架，提升机器人精细操作的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有夹爪设计缺乏触觉反馈，而触觉反馈对精确操作至关重要，因此需要一种集成触觉传感器的便携解决方案。

Method: 提出了一种便携式轻量夹爪设计，集成了触觉传感器，支持视觉与触觉数据的同步采集；并开发了跨模态表示学习框架，整合视觉与触觉信号。

Result: 在试管插入和移液管流体转移等精细任务中，验证了方法的准确性和对外部干扰的鲁棒性提升。

Conclusion: 集成了触觉传感器的便携式轻量夹爪及其跨模态表示学习框架，显著提升了机器人操作的精度和鲁棒性，尤其在精细操作任务中表现优异。

Abstract: Handheld grippers are increasingly used to collect human demonstrations due
to their ease of deployment and versatility. However, most existing designs
lack tactile sensing, despite the critical role of tactile feedback in precise
manipulation. We present a portable, lightweight gripper with integrated
tactile sensors that enables synchronized collection of visual and tactile data
in diverse, real-world, and in-the-wild settings. Building on this hardware, we
propose a cross-modal representation learning framework that integrates visual
and tactile signals while preserving their distinct characteristics. The
learning procedure allows the emergence of interpretable representations that
consistently focus on contacting regions relevant for physical interactions.
When used for downstream manipulation tasks, these representations enable more
efficient and effective policy learning, supporting precise robotic
manipulation based on multimodal feedback. We validate our approach on
fine-grained tasks such as test tube insertion and pipette-based fluid
transfer, demonstrating improved accuracy and robustness under external
disturbances. Our project page is available at
https://binghao-huang.github.io/touch_in_the_wild/ .

</details>


### [199] [Search-Based Autonomous Vehicle Motion Planning Using Game Theory](https://arxiv.org/abs/2507.15088)
*Pouya Panahandeh,Mohammad Pirani,Baris Fidan,Amir Khajepour*

Main category: cs.RO

TL;DR: 提出一种基于博弈论的实时运动规划方案，将其他道路用户视为智能体，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统搜索式方法未考虑其他道路使用者的智能行为，导致生成的路径不够真实。

Method: 采用博弈论方法，将其他道路使用者视为智能体而非静态障碍物，开发了一种新的搜索式运动规划方案。

Result: 新方案在计算时间上表现优异，适用于实时应用，并通过实验验证了其有效性。

Conclusion: 提出的基于搜索的交互式运动规划方案在实时应用中具有可行性，并通过实验验证了其性能优于现有技术。

Abstract: In this paper, we propose a search-based interactive motion planning scheme
for autonomous vehicles (AVs), using a game-theoretic approach. In contrast to
traditional search-based approaches, the newly developed approach considers
other road users (e.g. drivers and pedestrians) as intelligent agents rather
than static obstacles. This leads to the generation of a more realistic path
for the AV. Due to the low computational time, the proposed motion planning
scheme is implementable in real-time applications. The performance of the
developed motion planning scheme is compared with existing motion planning
techniques and validated through experiments using WATonoBus, an electrical
all-weather autonomous shuttle bus.

</details>


### [200] [Learning-Based Modeling of a Magnetically Steerable Soft Suction Device for Endoscopic Endonasal Interventions](https://arxiv.org/abs/2507.15155)
*Majid Roshanfar,Alex Zhang,Changyan He,Amir Hooshiar,Dale J. Podolsky,Thomas Looi,Eric Diller*

Main category: cs.RO

TL;DR: 该研究提出了一种基于学习的磁驱动软吸引装置建模框架，使用RF模型实现了亚毫米级的形状预测精度，为微创神经外科手术中的智能控制提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种基于学习的建模框架，用于磁驱动软吸引装置，以支持内窥镜鼻内脑肿瘤切除手术，实现高精度的形状预测和实时反馈。

Method: 研究采用了数据驱动模型，基于5097个实验样本训练了神经网络（NN）和随机森林（RF）架构，覆盖了磁场强度（0-14 mT）、驱动频率（0.2-1.0 Hz）和垂直尖端距离（90-100 mm）等参数。RF模型在控制点预测和形状重建误差上均优于NN。

Result: RF模型在所有指标上均优于NN，控制点预测的平均均方根误差为0.087 mm，形状重建的平均误差为0.064 mm。特征重要性分析显示，磁场分量主要影响远端控制点，而频率和距离影响基础配置。

Conclusion: 该研究通过基于学习的方法有效建模了超弹性软机器人在磁驱动下的复杂非线性行为，无需依赖简化的物理假设，实现了亚毫米级的形状预测精度和实时推理，为微创神经外科中磁驱动软机器人工具的智能控制提供了进展。

Abstract: This letter introduces a novel learning-based modeling framework for a
magnetically steerable soft suction device designed for endoscopic endonasal
brain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm
inner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material,
and integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape
feedback. Shape reconstruction is represented using four Bezier control points,
enabling a compact and smooth model of the device's deformation. A data-driven
model was trained on 5,097 experimental samples covering a range of magnetic
field magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical
tip distances (90-100 mm), using both Neural Network (NN) and Random Forest
(RF) architectures. The RF model outperformed the NN across all metrics,
achieving a mean root mean square error of 0.087 mm in control point prediction
and a mean shape reconstruction error of 0.064 mm. Feature importance analysis
further revealed that magnetic field components predominantly influence distal
control points, while frequency and distance affect the base configuration.
This learning-based approach effectively models the complex nonlinear behavior
of hyperelastic soft robots under magnetic actuation without relying on
simplified physical assumptions. By enabling sub-millimeter shape prediction
accuracy and real-time inference, this work represents an advancement toward
the intelligent control of magnetically actuated soft robotic tools in
minimally invasive neurosurgery.

</details>


### [201] [CHADET: Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer](https://arxiv.org/abs/2507.15189)
*Kevin Christiansen Marsim,Jinwoo Jeon,Yeeun Kim,Myeongwoo Jeong,Hyun Myung*

Main category: cs.RO

TL;DR: CHADET是一种轻量级深度补全网络，结合跨层次注意力模块，高效生成精确的密集深度图，适用于实时机器人任务。


<details>
  <summary>Details</summary>
Motivation: 现有深度补全方法在计算效率和准确性之间存在显著权衡，无法满足实时应用需求，需提升深度信息的完整性和准确性同时加快处理速度。

Method: 采用深度块提取特征，并通过轻量级基于Transformer的解码器处理，利用跨层次注意力模块细化图像特征。

Result: 在KITTI、NYUv2和VOID数据集上验证了CHADET在提升深度图质量和减少内存使用方面的有效性。

Conclusion: CHADET 提出了一种轻量级的深度补全网络，通过跨层次注意力模块优化图像特征，显著提升了深度图预测的质量和效率，适用于实时机器人应用。

Abstract: Depth information which specifies the distance between objects and current
position of the robot is essential for many robot tasks such as navigation.
Recently, researchers have proposed depth completion frameworks to provide
dense depth maps that offer comprehensive information about the surrounding
environment. However, existing methods show significant trade-offs between
computational efficiency and accuracy during inference. The substantial memory
and computational requirements make them unsuitable for real-time applications,
highlighting the need to improve the completeness and accuracy of depth
information while improving processing speed to enhance robot performance in
various tasks. To address these challenges, in this paper, we propose
CHADET(cross-hierarchical-attention depth-completion transformer), a
lightweight depth-completion network that can generate accurate dense depth
maps from RGB images and sparse depth points. For each pair, its feature is
extracted from the depthwise blocks and passed to the equally lightweight
transformer-based decoder. In the decoder, we utilize the novel
cross-hierarchical-attention module that refines the image features from the
depth information. Our approach improves the quality and reduces memory usage
of the depth map prediction, as validated in both KITTI, NYUv2, and VOID
datasets.

</details>


### [202] [VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving](https://arxiv.org/abs/2507.15266)
*Haichao Liu,Haoren Guo,Pei Liu,Benshan Ma,Yuxiang Zhang,Jun Ma,Tong Heng Lee*

Main category: cs.RO

TL;DR: 提出了VLM-UDMC框架，结合VLM提升自动驾驶决策的透明性和性能，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 为了实现城市自动驾驶中的透明性和可解释性，模仿人类驾驶员的认知能力。

Method: 采用了两步推理策略（RAG）和轻量级多核分解LSTM，分别用于生成风险感知洞察和实时轨迹预测。

Result: 通过仿真和实车实验验证了VLM-UDMC框架的有效性，提升了驾驶决策的合理性。

Conclusion: VLM-UDMC框架通过结合场景理解和风险感知注意力，提升了城市自动驾驶的决策和运动控制性能，验证了其在实际应用中的有效性。

Abstract: Scene understanding and risk-aware attentions are crucial for human drivers
to make safe and effective driving decisions. To imitate this cognitive ability
in urban autonomous driving while ensuring the transparency and
interpretability, we propose a vision-language model (VLM)-enhanced unified
decision-making and motion control framework, named VLM-UDMC. This framework
incorporates scene reasoning and risk-aware insights into an upper-level slow
system, which dynamically reconfigures the optimal motion planning for the
downstream fast system. The reconfiguration is based on real-time environmental
changes, which are encoded through context-aware potential functions. More
specifically, the upper-level slow system employs a two-step reasoning policy
with Retrieval-Augmented Generation (RAG), leveraging foundation models to
process multimodal inputs and retrieve contextual knowledge, thereby generating
risk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM
provides real-time trajectory predictions for heterogeneous traffic
participants by extracting smoother trend representations for short-horizon
trajectory prediction. The effectiveness of the proposed VLM-UDMC framework is
verified via both simulations and real-world experiments with a full-size
autonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively
leverages scene understanding and attention decomposition for rational driving
decisions, thus improving the overall urban driving performance. Our
open-source project is available at https://github.com/henryhcliu/vlmudmc.git.

</details>


### [203] [RepILN: Reparameterized Inertial Localization Network](https://arxiv.org/abs/2507.15293)
*Shanshan Zhang,Tianshui Wen,Siyue Wang,Qi Zhang,Ziheng Zhou,Lingxiang Zheng,Yu Yang*

Main category: cs.RO

TL;DR: 提出了一种高效惯性定位网络，通过多分支训练和单路径推理优化参数效率，结合稀疏注意力与门控卷积提升长时依赖建模，显著降低误差与参数量。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动的惯性定位方法因复杂网络架构对IoT设备计算资源的挑战，以及忽视惯性测量中长期依赖关系建模的问题。

Method: 提出了一种重新参数化的惯性定位网络，采用多分支结构增强特征提取，并在推理时转换为单路径架构以提高参数效率；引入时间尺度稀疏注意力机制和门控卷积单元以捕捉长时依赖关系。

Result: 在RoNIN数据集上，绝对轨迹误差降低了2.59%，参数数量减少了3.86%。

Conclusion: 提出的方法在公共基准测试中展现了准确性与模型简洁性之间的良好平衡，显著降低了绝对轨迹误差并减少了参数数量。

Abstract: Inertial localization is regarded as a promising positioning solution for
consumer-grade IoT devices due to its cost-effectiveness and independence from
external infrastructure. However, data-driven inertial localization methods
often rely on increasingly complex network architectures to improve accuracy,
which challenges the limited computational resources of IoT devices. Moreover,
these methods frequently overlook the importance of modeling long-term
dependencies in inertial measurements - a critical factor for accurate
trajectory reconstruction - thereby limiting localization performance. To
address these challenges, we propose a reparameterized inertial localization
network that uses a multi-branch structure during training to enhance feature
extraction. At inference time, this structure is transformed into an equivalent
single-path architecture to improve parameter efficiency. To further capture
long-term dependencies in motion trajectories, we introduce a temporal-scale
sparse attention mechanism that selectively emphasizes key trajectory segments
while suppressing noise. Additionally, a gated convolutional unit is
incorporated to effectively integrate long-range dependencies with local
fine-grained features. Extensive experiments on public benchmarks demonstrate
that our method achieves a favorable trade-off between accuracy and model
compactness. For example, on the RoNIN dataset, our approach reduces the
Absolute Trajectory Error (ATE) by 2.59% compared to RoNIN-ResNet while
reducing the number of parameters by 3.86%.

</details>


### [204] [Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe](https://arxiv.org/abs/2507.15444)
*Leonard Bauersfeld,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 该研究提出了一种基于实时流场测量的四旋翼无人机闭环控制系统，通过低延迟烟雾测速和神经网络干扰估计器实现了在狭窄管道中的稳定悬停和侧向平移。


<details>
  <summary>Details</summary>
Motivation: 在狭窄管道中飞行的四旋翼无人机面临由自诱导气流扰动带来的稳定性挑战，现有方法要么依赖持续运动来减轻气流再循环效应，要么在悬停时稳定性有限。

Method: 开发了一种低延迟、基于事件的烟雾测速方法，用于高时间分辨率的局部气流估计，并结合基于循环卷积神经网络的干扰估计器实时推断力和扭矩干扰。这些估计的干扰被整合到一个通过强化学习训练的学习型控制器中。

Result: 流反馈控制在管道横截面的侧向平移操作中特别有效，实时干扰信息使控制器能够有效抵消瞬态空气动力学效应，防止与管壁碰撞。

Conclusion: 该研究首次展示了基于实时流场测量的四旋翼无人机闭环控制系统，为在空气动力学复杂环境中的飞行研究开辟了新方向，并提供了机器人学与流体力学交叉领域的新见解。

Abstract: Autonomous quadrotor flight in confined spaces such as pipes and tunnels
presents significant challenges due to unsteady, self-induced aerodynamic
disturbances. Very recent advances have enabled flight in such conditions, but
they either rely on constant motion through the pipe to mitigate airflow
recirculation effects or suffer from limited stability during hovering. In this
work, we present the first closed-loop control system for quadrotors for
hovering in narrow pipes that leverages real-time flow field measurements. We
develop a low-latency, event-based smoke velocimetry method that estimates
local airflow at high temporal resolution. This flow information is used by a
disturbance estimator based on a recurrent convolutional neural network, which
infers force and torque disturbances in real time. The estimated disturbances
are integrated into a learning-based controller trained via reinforcement
learning. The flow-feedback control proves particularly effective during
lateral translation maneuvers in the pipe cross-section. There, the real-time
disturbance information enables the controller to effectively counteract
transient aerodynamic effects, thereby preventing collisions with the pipe
wall. To the best of our knowledge, this work represents the first
demonstration of an aerial robot with closed-loop control informed by real-time
flow field measurements. This opens new directions for research on flight in
aerodynamically complex environments. In addition, our work also sheds light on
the characteristic flow structures that emerge during flight in narrow,
circular pipes, providing new insights at the intersection of robotics and
fluid dynamics.

</details>


### [205] [The Emergence of Deep Reinforcement Learning for Path Planning](https://arxiv.org/abs/2507.15469)
*Thanh Thi Nguyen,Saeid Nahavandi,Imran Razzak,Dung Nguyen,Nhat Truong Pham,Quoc Viet Hung Nguyen*

Main category: cs.RO

TL;DR: 该综述全面概述了传统和基于DRL的路径规划方法，比较了它们的优缺点，并提出了结合两者优势的混合方法作为未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统在复杂动态环境中的需求增加，研究智能路径规划方法变得尤为重要。

Method: 该论文通过分类和比较传统方法（如图形搜索算法、线性规划技术和进化计算方法）与DRL方法的关键算法，提供了全面的概述。

Result: 论文详细讨论了各种方法在计算效率、可扩展性、适应性和鲁棒性方面的优缺点。

Conclusion: 该综述总结了路径规划领域的关键开放挑战，并指出了未来研究的有希望方向，特别关注了将深度强化学习（DRL）与经典规划技术结合的混合方法。

Abstract: The increasing demand for autonomous systems in complex and dynamic
environments has driven significant research into intelligent path planning
methodologies. For decades, graph-based search algorithms, linear programming
techniques, and evolutionary computation methods have served as foundational
approaches in this domain. Recently, deep reinforcement learning (DRL) has
emerged as a powerful method for enabling autonomous agents to learn optimal
navigation strategies through interaction with their environments. This survey
provides a comprehensive overview of traditional approaches as well as the
recent advancements in DRL applied to path planning tasks, focusing on
autonomous vehicles, drones, and robotic platforms. Key algorithms across both
conventional and learning-based paradigms are categorized, with their
innovations and practical implementations highlighted. This is followed by a
thorough discussion of their respective strengths and limitations in terms of
computational efficiency, scalability, adaptability, and robustness. The survey
concludes by identifying key open challenges and outlining promising avenues
for future research. Special attention is given to hybrid approaches that
integrate DRL with classical planning techniques to leverage the benefits of
both learning-based adaptability and deterministic reliability, offering
promising directions for robust and resilient autonomous navigation.

</details>


### [206] [All-UWB SLAM Using UWB Radar and UWB AOA](https://arxiv.org/abs/2507.15474)
*Charith Premachandra,Achala Athukorala,U-Xuan Tan*

Main category: cs.RO

TL;DR: 本文提出了一种结合UWB AOA测量的新方法，解决了在特征缺失环境中SLAM的挑战，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 在恶劣环境（如烟雾、灰尘）中，光学传感器易失效，而UWB雷达因其低频成分能穿透此类环境，成为SLAM的潜在传感技术。然而，现有方法受限于环境中可区分特征的数量。

Method: 将UWB AOA测量集成到基于UWB雷达的SLAM系统中，利用动态部署的UWB锚点-标签单元在特征缺失区域获取AOA测量。

Result: 实验结果表明，集成UWB AOA单元与UWB雷达可在视觉受限且特征缺失的环境中实现SLAM。

Conclusion: 本文提出了一种结合UWB AOA测量的新方法，显著提高了在特征缺失环境中SLAM的准确性和可扩展性。

Abstract: There has been a growing interest in autonomous systems designed to operate
in adverse conditions (e.g. smoke, dust), where the visible light spectrum
fails. In this context, Ultra-wideband (UWB) radar is capable of penetrating
through such challenging environmental conditions due to the lower frequency
components within its broad bandwidth. Therefore, UWB radar has emerged as a
potential sensing technology for Simultaneous Localization and Mapping (SLAM)
in vision-denied environments where optical sensors (e.g. LiDAR, Camera) are
prone to failure. Existing approaches involving UWB radar as the primary
exteroceptive sensor generally extract features in the environment, which are
later initialized as landmarks in a map. However, these methods are constrained
by the number of distinguishable features in the environment. Hence, this paper
proposes a novel method incorporating UWB Angle of Arrival (AOA) measurements
into UWB radar-based SLAM systems to improve the accuracy and scalability of
SLAM in feature-deficient environments. The AOA measurements are obtained using
UWB anchor-tag units which are dynamically deployed by the robot in featureless
areas during mapping of the environment. This paper thoroughly discusses
prevailing constraints associated with UWB AOA measurement units and presents
solutions to overcome them. Our experimental results show that integrating UWB
AOA units with UWB radar enables SLAM in vision-denied feature-deficient
environments.

</details>


### [207] [The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents](https://arxiv.org/abs/2507.15478)
*Simon Kohaut,Felix Divo,Navid Hamid,Benedict Flade,Julian Eggert,Devendra Singh Dhami,Kristian Kersting*

Main category: cs.RO

TL;DR: CoCo框架结合神经符号系统，提升自主代理在不确定环境中的安全性和可靠性，通过深度概率逻辑程序和自怀疑概念实现。


<details>
  <summary>Details</summary>
Motivation: 解决自主代理在不确定环境中可靠且合规行为的挑战。

Method: 引入Constitutional Controller（CoCo）框架，结合深度概率逻辑程序和自怀疑概念（基于旅行速度、传感器状态或健康因素等特征的概率密度）。

Result: 在真实世界的空中移动研究中，CoCo展示了智能自主系统学习适当怀疑并安全合规地导航复杂不确定环境的能力。

Conclusion: CoCo框架通过结合神经符号系统，显著提升了自主代理在不确定环境中的安全性和可靠性。

Abstract: Ensuring reliable and rule-compliant behavior of autonomous agents in
uncertain environments remains a fundamental challenge in modern robotics. Our
work shows how neuro-symbolic systems, which integrate probabilistic, symbolic
white-box reasoning models with deep learning methods, offer a powerful
solution to this challenge. This enables the simultaneous consideration of
explicit rules and neural models trained on noisy data, combining the strength
of structured reasoning with flexible representations. To this end, we
introduce the Constitutional Controller (CoCo), a novel framework designed to
enhance the safety and reliability of agents by reasoning over deep
probabilistic logic programs representing constraints such as those found in
shared traffic spaces. Furthermore, we propose the concept of self-doubt,
implemented as a probability density conditioned on doubt features such as
travel velocity, employed sensors, or health factors. In a real-world aerial
mobility study, we demonstrate CoCo's advantages for intelligent autonomous
systems to learn appropriate doubts and navigate complex and uncertain
environments safely and compliantly.

</details>


### [208] [Robots for Kiwifruit Harvesting and Pollination](https://arxiv.org/abs/2507.15484)
*Jamie Bell*

Main category: cs.RO

TL;DR: 研究开发了自动化机器人系统，用于猕猴桃果园的采摘和花粉喷洒，新采摘机制覆盖率达80%，导航系统通过激光雷达和计算机视觉实现高效作业。


<details>
  <summary>Details</summary>
Motivation: 传统猕猴桃果园的采摘和花粉喷洒依赖人工，效率低且成本高。开发自动化机器人系统可以提高作业效率，降低人力成本，并适应复杂的果园环境。

Method: 研究设计了多种猕猴桃采摘机制，并通过实地测试验证其可靠性。采用2D和3D激光雷达技术进行果园导航，同时开发了计算机视觉算法用于行检测和跟随。花粉喷洒系统通过检测花朵并调整喷杆高度实现精准喷洒。

Result: 新采摘机制覆盖率达80%以上，优于现有技术的70%。花粉喷洒系统在1.4米/秒的速度下实现精准喷洒。3D激光雷达导航系统完成超过30公里的自动驾驶测试，计算机视觉算法表现与激光雷达方法相当。

Conclusion: 本研究成功开发了一种移动机器人系统，能够在猕猴桃果园中实现定向花粉喷洒和自动化采摘。新设计的猕猴桃采摘机制在测试中表现优异，能够覆盖超过80%的果实，显著优于现有技术。此外，3D激光雷达导航系统和计算机视觉算法在果园导航中表现出色，为自动化农业提供了可行的解决方案。

Abstract: This research was a part of a project that developed mobile robots that
performed targeted pollen spraying and automated harvesting in pergola
structured kiwifruit orchards. Multiple kiwifruit detachment mechanisms were
designed and field testing of one of the concepts showed that the mechanism
could reliably pick kiwifruit. Furthermore, this kiwifruit detachment mechanism
was able to reach over 80 percent of fruit in the cluttered kiwifruit canopy,
whereas the previous state of the art mechanism was only able to reach less
than 70 percent of the fruit. Artificial pollination was performed by detecting
flowers and then spraying pollen in solution onto the detected flowers from a
line of sprayers on a boom, while driving at up to 1.4 ms-1. In addition, the
height of the canopy was measured and the spray boom was moved up and down to
keep the boom close enough to the flowers for the spray to reach the flowers,
while minimising collisions with the canopy. Mobile robot navigation was
performed using a 2D lidar in apple orchards and vineyards. Lidar navigation in
kiwifruit orchards was more challenging because the pergola structure only
provides a small amount of data for the direction of rows, compared to the
amount of data from the overhead canopy, the undulating ground and other
objects in the orchards. Multiple methods are presented here for extracting
structure defining features from 3D lidar data in kiwifruit orchards. In
addition, a 3D lidar navigation system -- which performed row following, row
end detection and row end turns -- was tested for over 30 km of autonomous
driving in kiwifruit orchards. Computer vision algorithms for row detection and
row following were also tested. The computer vision algorithm worked as well as
the 3D lidar row following method in testing.

</details>


### [209] [GR-3 Technical Report](https://arxiv.org/abs/2507.15493)
*Chilam Cheang,Sijin Chen,Zhongren Cui,Yingdong Hu,Liqun Huang,Tao Kong,Hang Li,Yifeng Li,Yuxiao Liu,Xiao Ma,Hao Niu,Wenxuan Ou,Wanli Peng,Zeyu Ren,Haixin Shi,Jiawen Tian,Hongtao Wu,Xin Xiao,Yuyang Xiao,Jiafeng Xu,Yichu Yang*

Main category: cs.RO

TL;DR: GR-3是一个大规模视觉-语言-动作模型，通过多方法训练实现了对新环境和任务的强大泛化能力，并在实验中超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够泛化到新对象、环境和抽象概念指令的通用机器人策略模型，以实现快速、经济高效的适应新设置。

Method: 通过多方面的训练方法，包括与网络规模的视觉-语言数据共同训练、通过VR设备收集的人类轨迹数据进行高效微调，以及利用机器人轨迹数据进行有效的模仿学习。

Result: GR-3在多种挑战性任务中超越了最先进的基线方法π₀，展示了在长视野和灵巧任务中的稳健性能。

Conclusion: GR-3作为一个通用机器人策略模型，展示了在多样化任务中的卓越性能，并有望成为辅助人类日常生活的通用机器人发展的重要一步。

Abstract: We report our recent progress towards building generalist robot policies, the
development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.
It showcases exceptional capabilities in generalizing to novel objects,
environments, and instructions involving abstract concepts. Furthermore, it can
be efficiently fine-tuned with minimal human trajectory data, enabling rapid
and cost-effective adaptation to new settings. GR-3 also excels in handling
long-horizon and dexterous tasks, including those requiring bi-manual
manipulation and mobile movement, showcasing robust and reliable performance.
These capabilities are achieved through a multi-faceted training recipe that
includes co-training with web-scale vision-language data, efficient fine-tuning
from human trajectory data collected via VR devices, and effective imitation
learning with robot trajectory data. In addition, we introduce ByteMini, a
versatile bi-manual mobile robot designed with exceptional flexibility and
reliability, capable of accomplishing a wide range of tasks when integrated
with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the
state-of-the-art baseline method, $\pi_0$, on a wide variety of challenging
tasks. We hope GR-3 can serve as a step towards building generalist robots
capable of assisting humans in daily life.

</details>


### [210] [CLEVER: Stream-based Active Learning for Robust Semantic Perception from Human Instructions](https://arxiv.org/abs/2507.15499)
*Jongseok Lee,Timo Birr,Rudolph Triebel,Tamim Asfour*

Main category: cs.RO

TL;DR: CLEVER是一个基于DNN的主动学习系统，通过在线人类指导和贝叶斯方法提升语义感知的鲁棒性，首次在真实机器人上实现流式主动学习。


<details>
  <summary>Details</summary>
Motivation: 为了解决数据流中DNN语义感知的鲁棒性问题，需要一种能够在线适应并利用人类指导的系统。

Method: 系统采用贝叶斯公式编码领域知识作为先验，并结合在线人类指导来适应数据流中的失败情况。

Result: 通过用户验证研究及对人形和可变形物体的实验，证明了CLEVER系统的有效性。

Conclusion: CLEVER系统通过在线人类指导和贝叶斯公式，成功提升了基于DNN的语义感知在实际应用中的鲁棒性。

Abstract: We propose CLEVER, an active learning system for robust semantic perception
with Deep Neural Networks (DNNs). For data arriving in streams, our system
seeks human support when encountering failures and adapts DNNs online based on
human instructions. In this way, CLEVER can eventually accomplish the given
semantic perception tasks. Our main contribution is the design of a system that
meets several desiderata of realizing the aforementioned capabilities. The key
enabler herein is our Bayesian formulation that encodes domain knowledge
through priors. Empirically, we not only motivate CLEVER's design but further
demonstrate its capabilities with a user validation study as well as
experiments on humanoid and deformable objects. To our knowledge, we are the
first to realize stream-based active learning on a real robot, providing
evidence that the robustness of the DNN-based semantic perception can be
improved in practice. The project website can be accessed at
https://sites.google.com/view/thecleversystem.

</details>


### [211] [Estimation of Payload Inertial Parameters from Human Demonstrations by Hand Guiding](https://arxiv.org/abs/2507.15604)
*Johannes Hartwig,Philipp Lienhardt,Dominik Henrich*

Main category: cs.RO

TL;DR: 研究提出了一种无需专用PIP校准的方法，利用非接触运动部分估计机器人PIP，结果显示质量估计准确，但质心和惯性张量估计受噪声影响。


<details>
  <summary>Details</summary>
Motivation: 解决非专业用户在编程接触运动时需要专用PIP校准的问题，以实现灵活的机器人工具更换。

Method: 利用非接触运动部分通过已建立的估计技术来估计机器人的PIP。

Result: 结果显示，有效载荷的质量估计准确，而质心和惯性张量受噪声和缺乏激励的影响。

Conclusion: 研究展示了在手动引导过程中估计有效载荷惯性参数（PIP）的可行性，但强调了需要足够的有效载荷加速度以实现精确估计。

Abstract: As the availability of cobots increases, it is essential to address the needs
of users with little to no programming knowledge to operate such systems
efficiently. Programming concepts often use intuitive interaction modalities,
such as hand guiding, to address this. When programming in-contact motions,
such frameworks require knowledge of the robot tool's payload inertial
parameters (PIP) in addition to the demonstrated velocities and forces to
ensure effective hybrid motion-force control. This paper aims to enable
non-expert users to program in-contact motions more efficiently by eliminating
the need for a dedicated PIP calibration, thereby enabling flexible robot tool
changes. Since demonstrated tasks generally also contain motions with
non-contact, our approach uses these parts to estimate the robot's PIP using
established estimation techniques. The results show that the estimation of the
payload's mass is accurate, whereas the center of mass and the inertia tensor
are affected by noise and a lack of excitation. Overall, these findings show
the feasibility of PIP estimation during hand guiding but also highlight the
need for sufficient payload accelerations for an accurate estimation.

</details>


### [212] [A Universal Vehicle-Trailer Navigation System with Neural Kinematics and Online Residual Learning](https://arxiv.org/abs/2507.15607)
*Yanbo Chen,Yunzhe Tan,Yaojia Wang,Zhengzhe Xu,Junbo Tan,Xueqian Wang*

Main category: cs.RO

TL;DR: 一种新型通用车辆-拖车导航系统，结合混合运动学模型和在线学习模块，显著提升导航性能，适用于多种拖车类型和负载条件。


<details>
  <summary>Details</summary>
Motivation: 车辆-拖车系统的自主导航在机场、超市等环境中至关重要，但现有模型难以准确描述带脚轮拖车的运动学特性，因此需要一种通用且鲁棒的导航解决方案。

Method: 论文提出了一种混合名义运动学模型，结合了经典非完整约束和神经网络拖车运动学，并引入轻量级在线残差学习模块来实时修正模型误差和扰动。此外，还开发了一个模型预测控制框架，采用加权模型组合策略以提高长期预测准确性。

Result: 通过多种拖车类型和不同负载条件的实际实验验证，该系统表现出稳健的性能，无需手动调整或特定拖车校准。

Conclusion: 该论文提出的新型通用车辆-拖车导航系统通过结合混合名义运动学模型和在线残差学习模块，显著提升了拖车导航的准确性和鲁棒性，无需手动调整或拖车特定校准。

Abstract: Autonomous navigation of vehicle-trailer systems is crucial in environments
like airports, supermarkets, and concert venues, where various types of
trailers are needed to navigate with different payloads and conditions.
However, accurately modeling such systems remains challenging, especially for
trailers with castor wheels. In this work, we propose a novel universal
vehicle-trailer navigation system that integrates a hybrid nominal kinematic
model--combining classical nonholonomic constraints for vehicles and neural
network-based trailer kinematics--with a lightweight online residual learning
module to correct real-time modeling discrepancies and disturbances.
Additionally, we develop a model predictive control framework with a weighted
model combination strategy that improves long-horizon prediction accuracy and
ensures safer motion planning. Our approach is validated through extensive
real-world experiments involving multiple trailer types and varying payload
conditions, demonstrating robust performance without manual tuning or
trailer-specific calibration.

</details>


### [213] [Optimizing Force Signals from Human Demonstrations of In-Contact Motions](https://arxiv.org/abs/2507.15608)
*Johannes Hartwig,Fabian Viessmann,Dominik Henrich*

Main category: cs.RO

TL;DR: 本文研究了如何优化力信号以更准确地反映人类意图，提出了峰值检测方法处理首次接触偏差，显著提高了运动再现的精确度。


<details>
  <summary>Details</summary>
Motivation: 非机器人编程专家通过运动引导进行编程时，输入信号的不精确和噪声问题影响了运动再现和机器学习输入的质量。

Method: 比较了不同的信号滤波方法，并提出了一种峰值检测方法来处理首次接触偏差。

Result: 通过优化方法，单个运动的误差标准提高了20%。

Conclusion: 本文提出的方法显著提高了机器人编程的可用性和人机交互的体验，通过优化力信号更好地反映人类意图。

Abstract: For non-robot-programming experts, kinesthetic guiding can be an intuitive
input method, as robot programming of in-contact tasks is becoming more
prominent. However, imprecise and noisy input signals from human demonstrations
pose problems when reproducing motions directly or using the signal as input
for machine learning methods. This paper explores optimizing force signals to
correspond better to the human intention of the demonstrated signal. We compare
different signal filtering methods and propose a peak detection method for
dealing with first-contact deviations in the signal. The evaluation of these
methods considers a specialized error criterion between the input and the
human-intended signal. In addition, we analyze the critical parameters'
influence on the filtering methods. The quality for an individual motion could
be increased by up to \SI{20}{\percent} concerning the error criterion. The
proposed contribution can improve the usability of robot programming and the
interaction between humans and robots.

</details>


### [214] [EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation](https://arxiv.org/abs/2507.15649)
*Haocheng Xu,Haodong Zhang,Zhenghan Chen,Rong Xiong*

Main category: cs.RO

TL;DR: 提出基于强化学习的框架，使人形机器人模仿人体上半身动作时保持稳定，通过EMP模块优化动作执行，验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究人形机器人在执行操作任务时的稳定站立问题，解决站立姿势下可控范围有限对整体稳定性的影响。

Method: 设计了一个重定向网络生成大规模上半身动作数据集，用于训练强化学习策略，并结合领域随机化提升鲁棒性。引入EMP模块根据机器人当前状态调整输入目标动作，确保安全与稳定性。

Result: 模拟和实际测试验证了框架的实用性，机器人能够有效模仿人体上半身动作并保持稳定性。

Conclusion: 通过强化学习框架和可执行运动先验（EMP）模块，实现了人形机器人模仿人体上半身动作的同时保持整体稳定性，验证了方法的实用性和有效性。

Abstract: To support humanoid robots in performing manipulation tasks, it is essential
to study stable standing while accommodating upper-body motions. However, the
limited controllable range of humanoid robots in a standing position affects
the stability of the entire body. Thus we introduce a reinforcement learning
based framework for humanoid robots to imitate human upper-body motions while
maintaining overall stability. Our approach begins with designing a retargeting
network that generates a large-scale upper-body motion dataset for training the
reinforcement learning (RL) policy, which enables the humanoid robot to track
upper-body motion targets, employing domain randomization for enhanced
robustness. To avoid exceeding the robot's execution capability and ensure
safety and stability, we propose an Executable Motion Prior (EMP) module, which
adjusts the input target movements based on the robot's current state. This
adjustment improves standing stability while minimizing changes to motion
amplitude. We evaluate our framework through simulation and real-world tests,
demonstrating its practical applicability.

</details>


### [215] [Data-Driven MPC with Data Selection for Flexible Cable-Driven Robotic Arms](https://arxiv.org/abs/2507.15677)
*Huayue Liang,Yanbo Chen,Hongyang Cheng,Yanzhao Yu,Shoujie Li,Junbo Tan,Xueqian Wang,Long Zeng*

Main category: cs.RO

TL;DR: 论文提出了一种基于输入输出数据的模型预测控制方法，显著提升了柔性电缆驱动机械臂的控制精度和效率，平均跟踪误差降至0.541度。


<details>
  <summary>Details</summary>
Motivation: 柔性电缆驱动机械臂（FCRAs）因其灵活性和适应性而具有优势，但电缆的弹性、滞回和摩擦等特性给建模和控制带来挑战。为此，论文提出了一种无需物理模型的模型预测控制方法，以提高控制精度。

Method: 论文首先开发了一个基于输入输出数据的隐式模型，并将其整合到MPC优化框架中。其次，引入数据选择算法（DSA）筛选最能表征系统的数据，将每步求解时间缩短至约4毫秒，提升近80%。最后，通过仿真研究了超参数对跟踪误差的影响。

Result: 在真实FCRA平台上验证了所提方法，包括五点定位精度测试、五点响应跟踪测试和字母绘制轨迹跟踪。结果显示平均定位精度约为2.070毫米，平均跟踪误差为0.541度，优于PID方法的1.418度。

Conclusion: 该论文提出的基于输入输出数据的模型预测控制方法显著提升了柔性电缆驱动机械臂的控制精度，平均定位精度约为2.070毫米，跟踪误差降至0.541度，优于传统PID方法。

Abstract: Flexible cable-driven robotic arms (FCRAs) offer dexterous and compliant
motion. Still, the inherent properties of cables, such as resilience,
hysteresis, and friction, often lead to particular difficulties in modeling and
control. This paper proposes a model predictive control (MPC) method that
relies exclusively on input-output data, without a physical model, to improve
the control accuracy of FCRAs. First, we develop an implicit model based on
input-output data and integrate it into an MPC optimization framework. Second,
a data selection algorithm (DSA) is introduced to filter the data that best
characterize the system, thereby reducing the solution time per step to
approximately 4 ms, which is an improvement of nearly 80%. Lastly, the
influence of hyperparameters on tracking error is investigated through
simulation. The proposed method has been validated on a real FCRA platform,
including five-point positioning accuracy tests, a five-point response tracking
test, and trajectory tracking for letter drawing. The results demonstrate that
the average positioning accuracy is approximately 2.070 mm. Moreover, compared
to the PID method with an average tracking error of 1.418{\deg}, the proposed
method achieves an average tracking error of 0.541{\deg}.

</details>


### [216] [Strong, Accurate, and Low-Cost Robot Manipulator](https://arxiv.org/abs/2507.15693)
*Georges Chebly,Spencer Little,Nisal Perera,Aliya Abedeen,Ken Suzuki,Donghyun Kim*

Main category: cs.RO

TL;DR: Forte是一款全3D打印、低成本（<215美元）的6自由度机械臂，具有接近工业级的性能（0.63 kg负载，0.467 m工作半径，亚毫米级重复精度），适用于教育和研究。


<details>
  <summary>Details</summary>
Motivation: 突破现有低成本教育机械臂的性能限制，提供一个材料成本低于215美元、性能接近工业级的机器人平台。

Method: 结合了capstan-based电缆驱动、同步带、简单张紧机构和轻量化3D打印结构，并通过拓扑优化提高结构刚度，精心设计的传动系统减少了回差并保持了控制精度。

Result: 实验验证表明，Forte实现了高重复性和负载能力（0.63 kg负载，0.467 m工作半径，亚毫米级重复精度）。

Conclusion: Forte 是一款低成本、高性能的6自由度机械臂，适用于从课堂教育到AI实验的广泛场景，其设计和性能验证了其在教育和研究中的实用性。

Abstract: This paper presents Forte, a fully 3D-printable, 6-DoF robotic arm designed
to achieve near industrial-grade performance - 0.63 kg payload, 0.467 m reach,
and sub-millimeter repeatability - at a material cost under $215. As an
accessible robot for broad applications across classroom education to AI
experiments, Forte pushes forward the performance limitations of existing
low-cost educational arms. We introduce a cost-effective mechanical design that
combines capstan-based cable drives, timing belts, simple tensioning
mechanisms, and lightweight 3D-printed structures, along with topology
optimization for structural stiffness. Through careful drivetrain engineering,
we minimize backlash and maintain control fidelity without relying on
high-power electronics or expensive manufacturing processes. Experimental
validation demonstrates that Forte achieves high repeatability and load
capacity, offering a compelling robotic platform for both classroom instruction
and advanced robotics research.

</details>


### [217] [Selective Densification for Rapid Motion Planning in High Dimensions with Narrow Passages](https://arxiv.org/abs/2507.15710)
*Lu Huang,Lingxiao Meng,Jiankun Wang,Xingjian Jing*

Main category: cs.RO

TL;DR: 提出多分辨率采样规划框架，通过在线调整采样密度优化复杂空间中的运动规划，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于采样的运动规划算法在复杂配置空间中因采样效率低而性能下降的问题，克服现有启发式方法缺乏泛化性或需要大量预训练的限制。

Method: 提出一种简单高效的基于采样的规划框架，通过在线探索不同分辨率的均匀随机样本，并偏向稀疏样本以优化大自由配置空间的探索。

Result: 仿真结果表明，该方法在$\mathbb{SE}(2)$、$\mathbb{SE}(3)$和$\mathbb{R}^{14}$等挑战性地形中优于多种最先进的采样规划器，且在Franka Emika Panda机器人受限工作空间中的实验进一步验证了其优越性。

Conclusion: 论文提出的多分辨率采样规划框架及其双向版本在复杂配置空间中表现出色，显著优于现有最先进的采样规划器，适用于多种挑战性环境。

Abstract: Sampling-based algorithms are widely used for motion planning in
high-dimensional configuration spaces. However, due to low sampling efficiency,
their performance often diminishes in complex configuration spaces with narrow
corridors. Existing approaches address this issue using handcrafted or learned
heuristics to guide sampling toward useful regions. Unfortunately, these
strategies often lack generalizability to various problems or require extensive
prior training. In this paper, we propose a simple yet efficient sampling-based
planning framework along with its bidirectional version that overcomes these
issues by integrating different levels of planning granularity. Our approach
probes configuration spaces with uniform random samples at varying resolutions
and explores these multi-resolution samples online with a bias towards sparse
samples when traveling large free configuration spaces. By seamlessly
transitioning between sparse and dense samples, our approach can navigate
complex configuration spaces while maintaining planning speed and completeness.
The simulation results demonstrate that our approach outperforms several
state-of-the-art sampling-based planners in $\mathbb{SE}(2)$, $\mathbb{SE}(3)$,
and $\mathbb{R}^{14}$ with challenging terrains. Furthermore, experiments
conducted with the Franka Emika Panda robot operating in a constrained
workspace provide additional evidence of the superiority of the proposed
method.

</details>


### [218] [DiffPF: Differentiable Particle Filtering with Generative Sampling via Conditional Diffusion Models](https://arxiv.org/abs/2507.15716)
*Ziyu Wan,Lin Zhao*

Main category: cs.RO

TL;DR: DiffPF是一种基于扩散模型的可微分粒子滤波，显著提升了复杂动态系统中的状态估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统差分粒子滤波依赖于预定义或低容量提案分布，限制了性能。DiffPF旨在通过扩散模型克服这些限制，提供更灵活、高效的后验采样。

Method: DiffPF利用扩散模型学习灵活的后验采样器，通过预测粒子和当前观测条件化，实现复杂、高维和多模态滤波分布的准确采样。

Result: DiffPF在多模态分布和真实世界任务中表现优异，如在高度多模态的全局定位基准上估计精度提升了82.8%，在KITTI视觉测距基准上提升了26%。

Conclusion: DiffPF通过将条件扩散模型集成到粒子滤波中，实现了高质量的后续采样，显著提升了状态估计的准确性。

Abstract: This paper proposes DiffPF, a differentiable particle filter that leverages
diffusion models for state estimation in dynamic systems. Unlike conventional
differentiable particle filters, which require importance weighting and
typically rely on predefined or low-capacity proposal distributions. DiffPF
learns a flexible posterior sampler by conditioning a diffusion model on
predicted particles and the current observation. This enables accurate,
equally-weighted sampling from complex, high-dimensional, and multimodal
filtering distributions. We evaluate DiffPF across a range of scenarios,
including both unimodal and highly multimodal distributions, and test it on
simulated as well as real-world tasks, where it consistently outperforms
existing filtering baselines. In particular, DiffPF achieves an 82.8%
improvement in estimation accuracy on a highly multimodal global localization
benchmark, and a 26% improvement on the real-world KITTI visual odometry
benchmark, compared to state-of-the-art differentiable filters. To the best of
our knowledge, DiffPF is the first method to integrate conditional diffusion
models into particle filtering, enabling high-quality posterior sampling that
produces more informative particles and significantly improves state
estimation.

</details>


### [219] [Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction](https://arxiv.org/abs/2507.15729)
*Jens V. Rüppel,Andrey Rudenko,Tim Schreiter,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: 该论文提出了一种基于LLM的多模态交互系统，通过视线和语音增强人机协作，实验表明其在适应性和用户参与度上优于传统方法，但存在冗余问题。


<details>
  <summary>Details</summary>
Motivation: 解决双向、多模态和上下文感知的协作任务支持这一开放挑战。

Method: 提出了一种基于视线和语音的多模态交互界面，支持实时语言交互和快速感知模块。

Result: 在两项实验室研究中，与传统脚本化HRI流程相比，LLM-based方法在适应性和用户参与度上略有提升，但存在冗余输出问题。

Conclusion: LLM-based方法在增强适应性和提升用户参与度方面表现更优，但可能产生冗余输出；而脚本化流程更适合简单任务。

Abstract: The rapid development of Large Language Models (LLMs) creates an exciting
potential for flexible, general knowledge-driven Human-Robot Interaction (HRI)
systems for assistive robots. Existing HRI systems demonstrate great progress
in interpreting and following user instructions, action generation, and robot
task solving. On the other hand, bi-directional, multi-modal, and context-aware
support of the user in collaborative tasks still remains an open challenge. In
this paper, we present a gaze- and speech-informed interface to the assistive
robot, which is able to perceive the working environment from multiple vision
inputs and support the dynamic user in their tasks. Our system is designed to
be modular and transferable to adapt to diverse tasks and robots, and it is
capable of real-time use of language-based interaction state representation and
fast on board perception modules. Its development was supported by multiple
public dissemination events, contributing important considerations for improved
robustness and user experience. Furthermore, in two lab studies, we compare the
performance and user ratings of our system with those of a traditional scripted
HRI pipeline. Our findings indicate that an LLM-based approach enhances
adaptability and marginally improves user engagement and task execution metrics
but may produce redundant output, while a scripted pipeline is well suited for
more straightforward tasks.

</details>


### [220] [Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs](https://arxiv.org/abs/2507.15782)
*Ruochu Yang,Yu Zhou,Fumin Zhang,Mengxue Hou*

Main category: cs.RO

TL;DR: Inter-LLM算法通过结合LLM和运动规划，提升了家庭机器人在复杂任务中的表现，性能提升30%。


<details>
  <summary>Details</summary>
Motivation: 家庭机器人在处理开放集对象和大型环境导航时缺乏类人智能，亟需解决方案以提升其任务执行能力。

Method: 提出了一种新颖的交错式LLM和运动规划算法Inter-LLM，设计了多模态动作成本相似性函数，以优化长期规划。

Result: 仿真实验表明，Inter-LLM算法在满足人类指令、最大化任务成功率和最小化任务成本方面比最新工作提升了30%。

Conclusion: Inter-LLM算法通过结合LLM和运动规划，显著提升了家庭机器人在复杂场景中的任务执行效率，实现了30%的性能提升。

Abstract: Household robots have been a longstanding research topic, but they still lack
human-like intelligence, particularly in manipulating open-set objects and
navigating large environments efficiently and accurately. To push this
boundary, we consider a generalized multi-object collection problem in large
scene graphs, where the robot needs to pick up and place multiple objects
across multiple locations in a long mission of multiple human commands. This
problem is extremely challenging since it requires long-horizon planning in a
vast action-state space under high uncertainties. To this end, we propose a
novel interleaved LLM and motion planning algorithm Inter-LLM. By designing a
multimodal action cost similarity function, our algorithm can both reflect the
history and look into the future to optimize plans, striking a good balance of
quality and efficiency. Simulation experiments demonstrate that compared with
latest works, our algorithm improves the overall mission performance by 30% in
terms of fulfilling human commands, maximizing mission success rates, and
minimizing mission costs.

</details>


### [221] [Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers](https://arxiv.org/abs/2507.15833)
*Ian Chuang,Andrew Lee,Dechen Gao,Jinyu Zou,Iman Soltani*

Main category: cs.RO

TL;DR: 研究将人类主动注视融入机器人视觉系统，通过foveated图像处理和ViTs优化，显著提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 人类视觉通过注视主动引导注意力，而机器人学习系统通常被动处理原始图像。研究如何将人类主动注视融入机器人策略以提升效率和性能。

Method: 结合了人类注视的主动视觉系统，采用了基于foveated图像处理的方法，并集成了Vision Transformers (ViTs)和foveated patch tokenization方案。探索了两种注视模仿和预测方法：两阶段模型和端到端联合预测模型。

Result: foveated机器人视觉方法显著降低了计算开销，同时提高了高精度任务的性能和对抗未见干扰物的鲁棒性。

Conclusion: 人类启发的视觉处理为机器人视觉系统提供了有益的归纳偏置，显著提高了计算效率和任务性能。

Abstract: Human vision is a highly active process driven by gaze, which directs
attention and fixation to task-relevant regions and dramatically reduces visual
processing. In contrast, robot learning systems typically rely on passive,
uniform processing of raw camera images. In this work, we explore how
incorporating human-like active gaze into robotic policies can enhance both
efficiency and performance. We build on recent advances in foveated image
processing and apply them to an Active Vision robot system that emulates both
human head movement and eye tracking. Extending prior work on the AV-ALOHA
robot simulation platform, we introduce a framework for simultaneously
collecting eye-tracking data and robot demonstrations from a human operator as
well as a simulation benchmark and dataset for training robot policies that
incorporate human gaze. Given the widespread use of Vision Transformers (ViTs)
in robot learning, we integrate gaze information into ViTs using a foveated
patch tokenization scheme inspired by recent work in image segmentation.
Compared to uniform patch tokenization, this significantly reduces the number
of tokens-and thus computation-without sacrificing visual fidelity near regions
of interest. We also explore two approaches to gaze imitation and prediction
from human data. The first is a two-stage model that predicts gaze to guide
foveation and action; the second integrates gaze into the action space,
allowing the policy to jointly predict gaze and actions end-to-end. Our results
show that our method for foveated robot vision not only drastically reduces
computational overhead, but also improves performance for high precision tasks
and robustness to unseen distractors. Together, these findings suggest that
human-inspired visual processing offers a useful inductive bias for robotic
vision systems. https://ian-chuang.github.io/gaze-av-aloha/

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [222] [Characterizing Communication Patterns in Distributed Large Language Model Inference](https://arxiv.org/abs/2507.14392)
*Lang Xu,Kaushik Kandadi Suresh,Quentin Anthony,Nawras Alnaasan,Dhabaleswar K. Panda*

Main category: cs.DC

TL;DR: 本文研究了分布式LLM推理中的通信动态，分析了不同并行化方法在GPU间数据交换中的表现，为生产环境提供了优化建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是分析分布式LLM推理中GPU间通信的性能限制，以提升实际系统的服务质量。

Method: 结合详细的性能分析测量和预测性分析模型，研究了不同并行化配置下的通信行为。

Result: 结果表明张量并行化带来显著网络开销但短序列响应时间优异，管道并行化减少数据传输但增加总延迟，而组合方法需精细调优以实现平衡性能。

Conclusion: 论文的结论是为生产环境中的LLM服务提供了选择适当并行化方案的实际建议，并指出了优化推理框架和通信基础设施的关键机会。

Abstract: Large Language Models (LLMs) built on transformer architectures have
transformed natural language processing, achieving remarkable performance
across diverse applications. While distributed inference frameworks enable
practical deployment of these models, inter-GPU communication creates
significant performance constraints that limit service quality in real-world
systems. This paper investigates communication dynamics in distributed LLM
serving-analyzing how various parallelization approaches coordinate data
exchange between GPU workers during inference. We study dense transformer-based
models as representative examples of contemporary architectures widely used in
operational deployments. Our work combines detailed profiling measurements with
predictive analytical models to characterize communication behavior across
different parallelization configurations. Results show that tensor parallelism
incurs substantial network overhead but delivers superior response times for
brief sequences, pipeline parallelism minimizes data transfer requirements
while increasing total latency, and combined approaches demand careful tuning
to achieve balanced performance. These insights offer practical recommendations
for selecting appropriate parallelization schemes in production LLM services
and identify key opportunities for optimizing inference frameworks and
communication infrastructure.

</details>


### [223] [Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning](https://arxiv.org/abs/2507.14597)
*Eugene Armah,Linda Amoako Bannning*

Main category: cs.DC

TL;DR: 论文提出了一种三步骤的主动边缘流处理自动扩展方法，结合GRU神经网络和迁移学习，显著提升了负载预测的准确性和资源分配的效率。


<details>
  <summary>Details</summary>
Motivation: 随着数字经济的快速发展，高速数据处理变得至关重要。边缘计算和数据流处理（DSP）是当前的主要范式，但面临资源分配的挑战。现有反应式方法在性能下降后才进行扩展，可能违反SLA。强化学习（RL）虽然提供了一种主动方法，但需要大量模拟。此外，预测性机器学习模型面临在线分布和概念漂移问题。

Method: 论文采用GRU神经网络进行上游负载预测，结合迁移学习框架（使用DTW算法和联合分布适应）来处理离线与在线领域的差异，并设计了一个水平自动扩展模块来动态调整算子并行度。

Result: 提出的轻量级GRU模型在真实数据集上实现了高达1.3%的SMAPE值，优于CNN、ARIMA和Prophet模型，并且在SMAPE和RMSE评估指标上表现更好，训练时间也短于计算密集的RL模型。

Conclusion: 论文提出了一个三步骤的解决方案，用于主动边缘流处理自动扩展问题，包括GRU神经网络预测上游负载、迁移学习框架处理离线与在线领域差异，以及基于预测负载的水平自动扩展模块。该方法在真实数据集上表现优异，优于CNN、ARIMA和Prophet模型。

Abstract: Processing data at high speeds is becoming increasingly critical as digital
economies generate enormous data. The current paradigms for timely data
processing are edge computing and data stream processing (DSP). Edge computing
places resources closer to where data is generated, while stream processing
analyzes the unbounded high-speed data in motion. However, edge stream
processing faces rapid workload fluctuations, complicating resource
provisioning. Inadequate resource allocation leads to bottlenecks, whereas
excess allocation results in wastage. Existing reactive methods, such as
threshold-based policies and queuing theory scale only after performance
degrades, potentially violating SLAs. Although reinforcement learning (RL)
offers a proactive approach through agents that learn optimal runtime
adaptation policies, it requires extensive simulation. Furthermore, predictive
machine learning models face online distribution and concept drift that
minimize their accuracy. We propose a three-step solution to the proactive edge
stream processing autoscaling problem. Firstly, a GRU neural network forecasts
the upstream load using real-world and synthetic DSP datasets. Secondly, a
transfer learning framework integrates the predictive model into an online
stream processing system using the DTW algorithm and joint distribution
adaptation to handle the disparities between offline and online domains.
Finally, a horizontal autoscaling module dynamically adjusts the degree of
operator parallelism, based on predicted load while considering edge resource
constraints. The lightweight GRU model for load predictions recorded up to
1.3\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and
Prophet on the SMAPE and RMSE evaluation metrics, with lower training time than
the computationally intensive RL models.

</details>


### [224] [Simulating Chirality: Solving Distance-$k$-Dispersion on an 1-Interval Connected Ring](https://arxiv.org/abs/2507.14723)
*Brati Mondal,Pritam Goswami,Buddhadeb Sau*

Main category: cs.DC

TL;DR: 本文研究了无手性假设下环形网络中同步移动代理的Distance-$k$-Dispersion问题，提出模拟手性的方法和高效算法，扩展了理论理解。


<details>
  <summary>Details</summary>
Motivation: 研究在1-间隔连接的环形网络中，无手性假设下，同步移动代理的Distance-$k$-Dispersion问题，以推广经典的分散问题。

Method: 提出了一种新颖的方法，使代理能够仅使用本地信息、视觉和有限内存模拟手性。此外，还提出了一种算法，在O(ln)轮内解决D-$k$-D问题。

Result: 证明了D-$k$-D和分散问题在任何环形网络大小下均可解（不包括顶点置换动态性），并提出了一个O(ln)轮的算法。

Conclusion: 本研究显著扩展了对动态网络中移动代理协调的理论理解，并阐明了手性在分布式计算中的作用。

Abstract: We study the Distance-$k$-Dispersion (D-$k$-D) problem for synchronous mobile
agents in a 1-interval-connected ring network having $n$ nodes and with $l$
agents where $3 \le l \le \lfloor \frac{n}{k}\rfloor$, without the assumption
of chirality (a common sense of direction for the agents). This generalizes the
classical dispersion problem by requiring that agents maintain a minimum
distance of $k$ hops from each other, with the special case $k=1$ corresponding
to the standard dispersion.
  The contribution in this work is threefold. Our first contribution is a novel
method that enables agents to simulate chirality using only local information,
vision and bounded memory. This technique demonstrates that chirality is not a
fundamental requirement for coordination in this model.
  Building on this, our second contribution partially resolves an open question
posed by Agarwalla et al. (ICDCN, 2018), who considered the same model (1-
interval connected ring, synchronous agents, no chirality). We prove that
D-$k$-D, and thus dispersion is solvable from any arbitrary configuration under
these assumptions (excluding vertex permutation dynamism)for any size of the
ring network which was earlier limited to only odd sized ring or to a ring of
size four.
  Finally, we present an algorithm for D-$k$-D in this setting that works in
$O(ln)$ rounds, completing the constructive side of our result.
  Altogether, our findings significantly extend the theoretical understanding
of mobile agent coordination in dynamic networks and clarify the role of
chirality in distributed computation.

</details>


### [225] [ACME: Adaptive Customization of Large Models via Distributed Systems](https://arxiv.org/abs/2507.14802)
*Ziming Dai,Chao Qiu,Fei Gao,Yunfeng Zhao,Xiaofei Wang*

Main category: cs.DC

TL;DR: ACME 是一种分布式 Transformer 大模型定制方法，通过双向协作优化解决了数据隐私和效率问题，显著降低了传输量并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决云端部署大模型时面临的数据隐私、响应延迟、模型不匹配、资源限制和能效低下等问题。

Method: ACME 采用双向单循环分布式系统，逐步实现细粒度协作模型定制，包括主干生成、Pareto 前沿识别、头部生成和数据分布驱动的个性化架构聚合。

Result: 在模型大小限制下，ACME 实现了成本高效的模型定制，数据传输量降至 6%，平均准确率提升 10%，权衡指标提升近 30%。

Conclusion: ACME 通过分布式系统实现了 Transformer 大模型的自适应定制，显著降低了数据传输量并提高了模型性能。

Abstract: Pre-trained Transformer-based large models have revolutionized personal
virtual assistants, but their deployment in cloud environments faces challenges
related to data privacy and response latency. Deploying large models closer to
the data and users has become a key research area to address these issues.
However, applying these models directly often entails significant difficulties,
such as model mismatching, resource constraints, and energy inefficiency.
Automated design of customized models is necessary, but it faces three key
challenges, namely, the high cost of centralized model customization,
imbalanced performance from user heterogeneity, and suboptimal performance from
data heterogeneity. In this paper, we propose ACME, an adaptive customization
approach of Transformer-based large models via distributed systems. To avoid
the low cost-efficiency of centralized methods, ACME employs a bidirectional
single-loop distributed system to progressively achieve fine-grained
collaborative model customization. In order to better match user heterogeneity,
it begins by customizing the backbone generation and identifying the Pareto
Front under model size constraints to ensure optimal resource utilization.
Subsequently, it performs header generation and refines the model using data
distribution-based personalized architecture aggregation to match data
heterogeneity. Evaluation on different datasets shows that ACME achieves
cost-efficient models under model size constraints. Compared to centralized
systems, data transmission volume is reduced to 6 percent. Additionally, the
average accuracy improves by 10 percent compared to the baseline, with the
trade-off metrics increasing by nearly 30 percent.

</details>


### [226] [Byzantine-Robust Decentralized Coordination of LLM Agents](https://arxiv.org/abs/2507.14928)
*Yongrae Jo,Chanik Park*

Main category: cs.DC

TL;DR: DecentLLMs 是一种去中心化共识方法，解决了多智能体 LLM 系统中的领导者依赖问题，提升了答案质量并容忍拜占庭代理。


<details>
  <summary>Details</summary>
Motivation: 现有拜占庭鲁棒多智能体系统依赖领导者协调，易受针对性攻击且可能接受低质量提案。

Method: 提出 DecentLLMs，一种去中心化的共识方法，其中工作代理并行生成答案，评估代理独立评分和排序以选择最佳答案。

Result: 实验表明，DecentLLMs 能有效容忍拜占庭代理并显著提升所选答案质量。

Conclusion: DecentLLMs 通过去中心化共识方法有效解决了多智能体 LLM 系统中的领导者依赖问题，显著提升了答案质量，并能够容忍拜占庭代理。

Abstract: Collaboration among multiple large language model (LLM) agents is a promising
approach to overcome inherent limitations of single-agent systems, such as
hallucinations and single points of failure. As LLM agents are increasingly
deployed on open blockchain platforms, multi-agent systems capable of
tolerating malicious (Byzantine) agents have become essential.
  Recent Byzantine-robust multi-agent systems typically rely on leader-driven
coordination, which suffers from two major drawbacks. First, they are
inherently vulnerable to targeted attacks against the leader. If consecutive
leaders behave maliciously, the system repeatedly fails to achieve consensus,
forcing new consensus rounds, which is particularly costly given the high
latency of LLM invocations. Second, an underperforming proposal from the leader
can be accepted as the final answer even when higher-quality alternatives are
available, as existing methods finalize the leader's proposal once it receives
a quorum of votes.
  To address these issues, we propose DecentLLMs, a novel decentralized
consensus approach for multi-agent LLM systems, where worker agents generate
answers concurrently and evaluator agents independently score and rank these
answers to select the best available one. This decentralized architecture
enables faster consensus despite the presence of Byzantine agents and
consistently selects higher-quality answers through Byzantine-robust
aggregation techniques.
  Experimental results demonstrate that DecentLLMs effectively tolerates
Byzantine agents and significantly improves the quality of selected answers.

</details>


### [227] [AMPED: Accelerating MTTKRP for Billion-Scale Sparse Tensor Decomposition on Multiple GPUs](https://arxiv.org/abs/2507.15121)
*Sasindu Wijeratne,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.DC

TL;DR: AMPED是一种多GPU并行算法，显著加速大规模稀疏张量的MTTKRP计算，性能提升5.1倍。


<details>
  <summary>Details</summary>
Motivation: 随着现实世界稀疏张量规模增长至数十亿非零元素，对硬件加速器的内存容量和计算吞吐量提出了更高要求，MTTKRP成为计算瓶颈。

Method: 提出了AMPED，一种多GPU并行算法，结合分区策略和动态负载均衡方案，以分散计算并最小化GPU空闲时间。

Result: 在十亿级规模的稀疏张量上，AMPED使用4个GPU在单个CPU节点上实现了5.1倍几何平均加速比，优于现有GPU基准。

Conclusion: AMPED通过多GPU并行算法成功加速了大规模稀疏张量的MTTKRP计算，满足了内存和性能需求，并在实际应用中显著提升了执行速度。

Abstract: Matricized Tensor Times Khatri-Rao Product (MTTKRP) is the computational
bottleneck in sparse tensor decomposition. As real-world sparse tensors grow to
billions of nonzeros, they increasingly demand higher memory capacity and
compute throughput from hardware accelerators. In this work, we present AMPED,
a multi-GPU parallel algorithm designed to accelerate MTTKRP on billion-scale
sparse tensors. AMPED scales beyond the limits of a single GPU, meeting both
the memory and performance requirements of large-scale workloads. We introduce
a partitioning strategy combined with a dynamic load balancing scheme to
distribute computation and minimize GPU idle time. On real-world billion-scale
tensors, AMPED achieves a 5.1x geometric mean speedup in total execution time
over state-of-the-art GPU baselines using 4 GPUs on a single CPU node.

</details>


### [228] [Dynatune: Dynamic Tuning of Raft Election Parameters Using Network Measurement](https://arxiv.org/abs/2507.15154)
*Kohya Shiozaki,Junya Nakamura*

Main category: cs.DC

TL;DR: Dynatune是一种动态调整Raft选举参数的机制，显著减少领导者故障检测和OTS时间，提升服务性能。


<details>
  <summary>Details</summary>
Motivation: 传统Raft算法在网络条件波动时难以有效调整选举参数，导致OTS时间增加和服务响应性降低。

Method: 提出Dynatune机制，基于网络指标（如往返时间和丢包率）动态调整Raft的选举参数。

Result: 实验结果表明，Dynatune相比Raft将领导者故障检测和OTS时间分别减少了80%和45%。

Conclusion: Dynatune通过动态调整Raft选举参数，显著降低了领导者故障检测和OTS时间，同时保持了高可用性，有效提升了SMR服务在各种网络场景下的性能和可靠性。

Abstract: Raft is a leader-based consensus algorithm that implements State Machine
Replication (SMR), which replicates the service state across multiple servers
to enhance fault tolerance. In Raft, the servers play one of three roles:
leader, follower, or candidate. The leader receives client requests, determines
the processing order, and replicates them to the followers. When the leader
fails, the service must elect a new leader to continue processing requests,
during which the service experiences an out-of-service (OTS) time. The OTS time
is directly influenced by election parameters, such as heartbeat interval and
election timeout. However, traditional approaches, such as Raft, often struggle
to effectively tune these parameters, particularly under fluctuating network
conditions, leading to increased OTS time and reduced service responsiveness.
To address this, we propose Dynatune, a mechanism that dynamically adjusts
Raft's election parameters based on network metrics such as round-trip time and
packet loss rates measured via heartbeats. By adapting to changing network
environments, Dynatune significantly reduces the leader failure detection and
OTS time without altering Raft's core mechanisms or introducing additional
communication overheads. Experimental results demonstrate that Dynatune reduces
the leader failure detection and OTS times by 80% and 45%, respectively,
compared with Raft, while maintaining high availability even under dynamic
network conditions. These findings confirm that Dynatune effectively enhances
the performance and reliability of SMR services in various network scenarios.

</details>


### [229] [GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis](https://arxiv.org/abs/2507.15230)
*Guoxi Liu,Thomas Randall,Rong Ge,Federico Iuricich*

Main category: cs.DC

TL;DR: GALE 是一种针对异构系统的 GPU 辅助任务并行数据结构，显著提升了非结构化网格的处理性能。


<details>
  <summary>Details</summary>
Motivation: 非结构化网格在科学数据分析中因分布不规则和连接复杂而存在挑战，现有 CPU 绑定的方法限制了性能提升。

Method: 通过将网格连通性信息的计算卸载到 GPU 线程，使 CPU 线程专注于执行可视化算法，提出了 GALE（GPU-Aided Localized data structurE）。

Result: 在 20 核 CPU 和 NVIDIA V100 GPU 上的实验表明，GALE 相比现有技术实现了最高 2.7 倍的加速。

Conclusion: GALE 提出了一种针对异构 CPU-GPU 系统的任务并行方法，显著提升了非结构化网格数据的处理性能，同时保持了内存效率。

Abstract: Unstructured meshes present challenges in scientific data analysis due to
irregular distribution and complex connectivity. Computing and storing
connectivity information is a major bottleneck for visualization algorithms,
affecting both time and memory performance. Recent task-parallel data
structures address this by precomputing connectivity information at runtime
while the analysis algorithm executes, effectively hiding computation costs and
improving performance. However, existing approaches are CPU-bound, forcing the
data structure and analysis algorithm to compete for the same computational
resources, limiting potential speedups. To overcome this limitation, we
introduce a novel task-parallel approach optimized for heterogeneous CPU-GPU
systems. Specifically, we offload the computation of mesh connectivity
information to GPU threads, enabling CPU threads to focus on executing the
visualization algorithm. Following this paradigm, we propose GALE (GPU-Aided
Localized data structurE), the first open-source CUDA-based data structure
designed for heterogeneous task parallelism. Experiments on two 20-core CPUs
and an NVIDIA V100 GPU show that GALE achieves up to 2.7x speedup over
state-of-the-art localized data structures while maintaining memory efficiency.

</details>


### [230] [An ML-Driven Participant Selection Technique for Federated Recommendation System in Edge-Cloud Computing](https://arxiv.org/abs/2507.15233)
*Jintao Liu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 提出一种多目标强化学习的客户端选择方法，优化联邦推荐系统的效率和公平性，实验表明其显著提升了收敛速度和训练效率。


<details>
  <summary>Details</summary>
Motivation: 联邦推荐系统（FRS）通过分布式、隐私保护的模型训练解决了集中式推荐系统的隐私和可扩展性问题，但现有框架面临设备能力异构、数据非独立同分布（非IID）和通信瓶颈等挑战。

Method: 提出了一种多目标强化学习（RL）参与者选择方法，结合历史客户端性能声誉（CPR）、数据效用和系统效率进行联合优化。该方法首先定义了一个综合客户端效用函数，随后将其嵌入多臂老虎机（MAB）框架中，动态平衡探索与开发以选择参与者。

Result: 在四种不同的数据偏斜场景中，基于MAB的选择方法将目标AUC的收敛时间缩短了32-50%，总训练时间减少了46%，同时在最终AUC、NDCG@50和Recall@50上达到或略优于现有FRS基线。

Conclusion: 自适应、奖励驱动的客户端采样能显著提升联邦推荐系统的效率和公平性。

Abstract: Recommendation systems (RS) personalize content by analyzing user
preferences, but typically require centralized collection of user data, raising
privacy and scalability concerns. Federated Recommendation Systems (FRS)
address these issues by enabling distributed, privacy-preserving model training
across edge devices, keeping raw data on-device. Although existing FRS
frameworks benefit from on-device feature extraction and privacy preservation,
they suffer from heterogeneous device capabilities, non-independent and
identically distributed (non-IID) data, and communication bottlenecks. To
overcome these limitations, we propose a multi-objective reinforcement learning
(RL) participant selection that jointly optimizes historical client performance
reputation (CPR), data utility, and system efficiency. First, we define a
composite client-utility function combining CPR, system capability, and data
quality. Next, we embed this utility into a multi-armed bandit (MAB) framework
and dynamically balance exploration-exploitation to select participants.
Finally, we practically implement our approach using the PySyft framework on an
edge-cloud testbed, and evaluate it on a multimodal movie-recommendation task
built from the MovieLens-100K dataset. Across four different skewed
data-partition scenarios, our MAB-based selection accelerates convergence by
32-50% in time-to-target AUC and reduces total wall-clock training time by up
to 46%, while matching or slightly improving final AUC, NDCG@50, and Recall@50
compared to existing FRS baselines. Our results demonstrate that adaptive,
reward-driven client sampling can substantially enhance both efficiency and
fairness in real-world federated deployments.

</details>


### [231] [Efficient Routing of Inference Requests across LLM Instances in Cloud-Edge Computing](https://arxiv.org/abs/2507.15553)
*Shibo Yu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 论文提出了一种基于NSGA-II的路由算法，优化云边环境中的LLM推理分配，显著提升响应时间和成本效率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）推理服务需求的增长，计算资源压力加剧，导致延迟和成本问题。如何在异构环境中高效分配推理请求成为关键挑战。

Method: 论文提出了一种基于非支配排序遗传算法II（NSGA-II）的路由算法，用于在云边计算环境中分配异构LLM实例的推理请求。该算法将问题建模为多目标优化问题，平衡响应质量、响应时间和推理成本，并适应请求的异质性和节点的多样性。

Result: 实验结果表明，与基线方法相比，该算法在响应时间和成本上分别实现了最高95.2%和34.9%的改进。

Conclusion: 该论文提出的基于NSGA-II的路由算法在云边计算环境中有效优化了LLM推理服务的性能，显著提升了响应时间和成本效率，验证了其在可扩展LLM部署中的实用性。

Abstract: The rising demand for Large Language Model (LLM) inference services has
intensified pressure on computational resources, resulting in latency and cost
challenges. This paper introduces a novel routing algorithm based on the
Non-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference
requests across heterogeneous LLM instances in a cloud-edge computing
environment. Formulated as a multi-objective optimization problem, the
algorithm balances response quality, response time, and inference cost,
adapting to request heterogeneity (e.g., varying complexity and prompt lengths)
and node diversity (e.g., edge vs. cloud resources). This adaptive routing
algorithm optimizes performance under dynamic workloads. We benchmark the
approach using a testbed with datasets including Stanford Question Answering
Dataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With
Adversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K).
Experimental results show our solution, compared to the baselines, achieves up
to 95.2% and 34.9% improvements in terms of response time and cost,
respectively. These findings validate the algorithm's effectiveness for
scalable LLM deployments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [232] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: 论文提出'自由意志方程'框架，借鉴量子场论为AGI决策引入受控随机性，实验显示其在非稳态环境中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统AGI研究专注于在确定性规则下优化特定目标，而人类智能展现出适应性自发性，即做出不受过去数据或即时奖励严格支配的意外选择的能力。这种特性可能对创造力、鲁棒适应性和避免问题解决中的思维定势至关重要。

Method: 论文提出了一个理论框架——自由意志方程，将AI代理的认知状态视为潜在行动或想法的叠加态，在决策时概率性地坍缩为具体行动。该方法结合了类似量子场的机制和内在动机项。

Result: 在非稳态多臂老虎机环境中的实验表明，使用该框架的代理在奖励和策略多样性方面优于基线方法。

Conclusion: 该论文提出了一种名为'自由意志方程'的理论框架，通过借鉴量子场论，为AGI代理的决策过程引入了一种适应性、受控的随机性，实验证明该方法在非稳态多臂老虎机环境中表现优于基线方法。

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [233] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS是一个多代理框架，通过LLM和共享画布实现DFT模拟自动化，显著提升材料发现的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决DFT模拟中训练周期长、参数调优复杂和系统错误处理困难等挑战。

Method: 引入基于DFT的研究引擎DREAMS，结合中心化LLM规划代理与领域特定代理，包括原子结构生成、DFT收敛测试、HPC调度和错误处理，并通过共享画布协调代理间讨论。

Result: 在Sol27LC基准测试中平均误差低于1%，成功解决CO/Pt(111)吸附难题，并通过贝叶斯集成采样量化功能驱动的不确定性。

Conclusion: DREAMS实现了L3级自动化，显著减少对人类专业知识和干预的依赖，为高通量、高保真计算材料发现提供了可扩展的路径。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [234] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: 论文介绍了WebGuard数据集，用于评估网络代理动作风险并开发安全措施。实验表明，当前模型性能虽显著提升，但仍未达到高风险部署的可靠性要求。


<details>
  <summary>Details</summary>
Motivation: 随着基于大型语言模型的自主网络代理的快速发展，其潜在的意外或有害行为风险凸显了对有效安全措施的迫切需求。

Method: 引入WebGuard数据集，包含4,939个人工标注的动作，覆盖193个网站和22个领域，采用三级风险分类（SAFE、LOW、HIGH）。通过微调Qwen2.5VL-7B模型进行实验评估。

Result: 微调后的Qwen2.5VL-7B模型将准确性从37%提升至80%，高风险动作召回率从20%提升至76%。

Conclusion: 尽管WebGuard数据集和微调模型显著提升了性能，但当前的安全措施仍不足以满足高风险部署的需求，需要进一步改进以实现近乎完美的准确性和召回率。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [235] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator 是一个开源工具，通过 LLM 将研究论文转化为动画，帮助学习者理解复杂 STEM 概念，简化高质量教育内容的创建。


<details>
  <summary>Details</summary>
Motivation: 理解复杂科学和数学概念，特别是那些在密集研究论文中呈现的内容，对学习者来说是一个重大挑战。动态可视化可以极大地增强理解，但手动创建它们耗时且需要专业知识和技能。

Method: Manimator 采用一个流程，其中 LLM 解释输入文本或研究论文 PDF，生成一个结构化的场景描述，概述关键概念、数学公式和视觉元素，另一个 LLM 将此描述翻译为可执行的 Manim Python 代码。

Result: Manimator 是一个开源系统，利用大型语言模型将研究论文和自然语言提示转换为使用 Manim 引擎的解释性动画。

Conclusion: Manimator 是一个有潜力的教育工具，能够快速为复杂的 STEM 主题创建引人入胜的视觉解释，从而普及高质量教育内容的创作。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [236] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: OnT是一种新的本体嵌入方法，结合预训练语言模型和双曲几何建模，有效整合文本并保持逻辑结构，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有本体嵌入方法存在局限性：几何模型忽略文本信息，而基于语言模型的方法无法保持逻辑结构。OnT旨在解决这一问题。

Method: 提出OnT方法，通过双曲几何建模调整预训练语言模型（PLM），以同时整合文本标签并保持描述逻辑EL的类层次和其他逻辑关系。

Result: 在四个实际本体上的实验表明，OnT在预测和公理推理任务上均优于基线方法，包括最先进的方法。

Conclusion: OnT方法通过结合预训练语言模型和双曲几何建模，有效整合了文本信息并保持了逻辑结构，在多个实际本体上表现优于现有基线，展示了强大的迁移学习能力和实际应用潜力。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [237] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass通过LLM与专用证明器的协同，显著提升计算效率和定理证明准确性，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一的大型通用模型或小型专用模型，各有局限性，而训练大型专用模型需要大量计算资源。

Method: 引入ProofCompass，一种混合方法，利用大型语言模型（LLM）指导专用证明器（如DSP-v1.5），无需额外训练模型。LLM提供自然语言证明策略并分析失败尝试以选择中间引理。

Result: 在miniF2F基准测试中，ProofCompass以25倍更少的尝试（3200→128）超越了DSP-v1.5的性能（54.9%→55.3%）。

Conclusion: ProofCompass通过结合大型语言模型和专用证明器，显著提升了计算效率和准确性，为形式化定理证明开辟了新途径。

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [238] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: Nexus Architect通过自动化工作流和提示优化，显著提升推理模型的泛化能力，实证表现优于现有LRMs。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（LRMs）在泛化到新问题时表现不佳，常依赖记忆而非真正推理，存在过拟合问题。

Method: 引入Nexus Architect，一个多智能体系统框架，配备自动化工作流合成和迭代提示优化机制，以生成定制化的推理工作流。

Result: Nexus Architect在逻辑问题数据集上表现优异，相比Gemini 2.5 Flash Preview提升了66%，对Claude Sonnet 4和DeepSeek-R1提升了约2.5倍，对Llama 4 Scout提升了3倍以上。

Conclusion: Nexus Architect通过其自动化工作流合成和迭代提示优化机制，显著提升了推理模型的泛化能力，并在实证评估中优于现有最先进的LRMs。

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [239] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 通过“快速失败或询问”系统结合推理模型和人类专家，显著降低错误率和延迟，实现高效协作。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的推理LLMs在风险敏感领域应用时仍存在错误率和延迟问题，需要将错误率降至接近0%。

Method: 提出了一种结合推理模型和非推理模型的协作系统（“快速失败或询问”），通过量化推理模型的不确定性（基于推理轨迹长度）实现人类专家的延迟决策，并探索非推理模型前置以降低延迟和成本。

Result: 系统将Qwen3 235B-A22B在困难MATH问题上的错误率从3%降至1%以下（延迟7.5%查询），并实现约40%延迟降低和50%成本节省（DeepSeek R1），同时保持90+%准确率-拒绝曲线下面积。

Conclusion: 研究结果表明，通过黑盒系统工程（如“快速失败或询问”系统）可以显著缓解最先进推理模型的缺陷（错误率和延迟问题），而无需访问LLM内部。

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [240] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: 研究表明，增加大型推理模型的推理长度可能导致性能下降，揭示了五种失败模式，强调了评估不同推理长度的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型推理模型（LRMs）在延长推理长度时性能下降的现象，揭示测试时计算量与准确性之间的反比关系。

Method: 构建了四类评估任务：带干扰项的简单计数任务、带伪特征的回归任务、带约束跟踪的演绎任务以及高级AI风险任务。

Result: 识别出五种不同的失败模式，包括模型对干扰项的敏感性增加、过度拟合问题框架、转向伪相关性、在复杂演绎任务中难以保持专注，以及延长推理可能放大问题行为。

Conclusion: 研究发现，尽管增加测试时计算量有望提升模型能力，但可能无意中强化了有问题的推理模式。这强调了评估不同推理长度下模型表现的重要性。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [241] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: Routine是一个多步骤代理规划框架，显著提升企业环境中代理系统的执行准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 企业环境中代理系统的部署常因缺乏领域特定过程知识而导致计划混乱、关键工具缺失和执行稳定性差。

Method: 本文提出了Routine，一个多步骤代理规划框架，具有清晰的结构、明确的指令和无缝的参数传递，以指导代理的执行模块执行多步骤工具调用任务。

Result: 在真实企业场景的评估中，Routine显著提高了模型工具调用的执行准确性，GPT-4o的性能从41.1%提升至96.3%，Qwen3-14B从32.6%提升至83.3%。通过微调，Qwen3-14B的准确性进一步提升至88.2%，接近GPT-4o的性能。

Conclusion: Routine提供了一个实用且易于使用的方法来构建稳定的代理工作流，加速了企业环境中代理系统的部署和采用，并推进了AI for Process的技术愿景。

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [242] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion框架通过协同语义与结构学习，提升生物医学知识图谱推理能力，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱的补全与推理具有挑战性，现有方法在语义理解与结构学习的协同进化上存在不足。

Method: BioGraphFusion结合张量分解建立全局语义基础，通过LSTM动态优化关系嵌入，并结合查询引导的子图构建与混合评分机制。

Result: BioGraphFusion在三个关键生物医学任务中表现优于现有KE、GNN和集成模型，并通过案例研究验证了其生物学意义。

Conclusion: BioGraphFusion通过深度协同的语义与结构学习，显著提升了生物医学知识图谱的推理能力，并在多个任务中超越了现有方法。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [243] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico 是一个用 Rust 编写的模块化事件驱动框架，专为嵌入式系统优化的自主代理设计，支持 WebAssembly 以实现跨平台高效运行。


<details>
  <summary>Details</summary>
Motivation: 现有框架在现实世界或资源受限环境中表现不佳，主要依赖云计算的局限性、动态环境中的鲁棒性不足以及缺乏持久自主性和环境感知能力。

Method: Amico 是一个模块化、事件驱动的框架，使用 Rust 编写以保障安全性和性能，支持通过 WebAssembly 在嵌入式平台和浏览器环境中高效运行的反应式、持久性代理。

Result: Amico 提供了事件处理、状态管理、行为执行以及与推理模块集成的清晰抽象，为构建适应性强、交互性好的自主代理提供了基础设施。

Conclusion: Amico 提供了一个统一的框架，用于构建在计算资源有限和间歇性连接环境下具有弹性和交互性的自主代理。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [244] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: 多模态训练（结合视觉输入）比单模态更高效，能提升语言模型对结构化世界的理解和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否仅通过文本就能理解世界，还是需要基于实际场景的接地学习更高效。

Method: 引入VISOTHELLO多模态模型，通过移动历史和棋盘图像进行训练，使用下一步移动预测与单模态基线比较，并测试对语义无关扰动的鲁棒性。

Result: 多模态训练提高了性能和内部表征的鲁棒性。

Conclusion: 多模态训练（结合视觉输入）有助于语言模型推断结构化世界表征，提升性能和内部表征的鲁棒性。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [245] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: OE-Assist是一个利用LLM自动化和半自动化CQ验证的框架，性能与人工相当。


<details>
  <summary>Details</summary>
Motivation: 本体评估中的功能需求测试（如CQ验证）成本高、劳动密集且容易出错，需要更高效的解决方案。

Method: 引入OE-Assist框架，利用1,393个CQ及其对应本体和本体故事的数据集，评估LLM在自动执行CQ验证中的有效性，并开发了一个LLM驱动的框架来辅助CQ验证。

Result: 自动化LLM评估（使用o1-preview和o3-mini）的性能与普通用户的性能相当。

Conclusion: OE-Assist框架通过自动化和半自动化的CQ验证，展示了LLM在协助本体评估中的潜力，其性能与普通用户相当。

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [246] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: 提出一个基于几何框架的情感表示系统，通过八坐标单位圆和数学运算解决传统模型的局限性，并验证其处理复杂情感的能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统分类情感模型无法充分表示情感冲突状态和复杂心理场景的问题，填补情感空间的覆盖空白。

Method: 提出Coordinate Heart System（CHS），将八种核心情感定位为单位圆上的坐标，支持通过坐标混合和向量运算进行复杂情感状态的数学计算。系统引入稳定性参数S，动态整合情感负荷、冲突解决和情境消耗因素。

Result: 开发了八情感系统，提供完整的几何覆盖，并提出情感混合、冲突解决和距离计算的新算法。实验验证了系统处理复杂情感状态的能力。

Conclusion: 本论文建立了一个新的人工智能情感建模数学基础，通过八坐标系统消除了情感表示的盲点，并通过实验验证了系统处理情感冲突和复杂心理场景的能力。

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [247] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 通过比较判断训练的故事点估计模型性能与回归模型相当，但降低了人类认知负担。


<details>
  <summary>Details</summary>
Motivation: 传统的故事点估计方法（如计划扑克）在团队达成共识后变得繁琐且耗时，机器学习可减轻负担但需要足够的历史决策上下文。

Method: 通过比较判断（开发者选择两个待办事项中哪个需要更多工作量）训练机器学习模型，而非直接分配具体故事点数值。

Result: 模型在16个项目中的23,313个手动估计数据上，预测与真实故事点之间的Spearman等级相关系数平均为0.34，与回归模型性能相当。

Conclusion: 提出的比较学习方法在故事点估计中表现出与现有回归方法相当甚至更好的性能，同时降低了人类的认知负担。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [248] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 论文通过模拟恶意多智能体系统共谋，揭示去中心化系统在虚假信息和电商欺诈中的高效破坏性，强调需改进检测与应对措施。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统的兴起，人们越来越担心AI驱动的群体可能造成类似人类群体协调行为带来的危害。尽管大多数AI安全研究集中在单个AI系统上，但多智能体系统在复杂现实情境中的风险仍未充分探索。

Method: 研究引入了一个概念验证框架，用于模拟恶意MAS共谋的风险，支持集中式和去中心化的协调结构，并在虚假信息传播和电子商务欺诈两个高风险领域进行了应用。

Result: 研究发现，去中心化系统在执行恶意行动时比集中式系统更有效，其更高的自主性使其能够调整策略并造成更大破坏。即使应用传统干预措施（如内容标记），去中心化群体也能调整策略以避免检测。

Conclusion: 论文强调了多智能体系统（MAS）在恶意共谋中的潜在危害，指出去中心化系统比集中式系统更具破坏性，并呼吁开发更好的检测系统和应对措施。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [249] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo是一个自动化多轮评估LLM系统的框架，通过模块化设计和概率状态模型实现高效、多样化的测试，显著提升测试覆盖率和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）代理表现出复杂、上下文敏感的行为，使得静态基准和临时手动测试迅速过时，需要自动化、多轮评估框架。

Method: Neo通过一个可配置的多代理框架，结合问题生成代理和评估代理，通过共享上下文中心实现模块化组合。测试输入从概率状态模型中采样，覆盖对话流、用户意图和情感语调，实现多样化和类人对话。

Result: 应用于生产级卖家财务助手聊天机器人时，Neo在五个攻击类别中发现了边缘案例故障，其3.3%的突破率接近专家人类红队的5.8%，且吞吐量提高了10-12倍，45分钟内生成180个连贯测试问题，而人类需要16小时。

Conclusion: Neo提供了一个可扩展、自进化的LLM QA基础，其代理接口、状态控制器和反馈循环是模型无关的，并可扩展到更丰富的事实基础和策略合规性检查。

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [250] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI是一个用于生成和管理定制化、基于政策的安全评估平台，评估显示商业LLM在不同安全领域表现差异显著，强调了可扩展安全工具的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）越来越多地融入现实应用，可扩展且严格的安全评估变得至关重要。

Method: Aymara AI是一个程序化平台，通过将自然语言安全政策转化为对抗性提示，并使用基于AI的评分器（经人类判断验证）来评分模型响应。

Result: 评估了20个商业可用LLM在10个现实安全领域的表现，结果显示性能差异显著（平均安全分数从86.2%到52.4%），在复杂或未明确领域（如隐私与冒充）表现较差（平均24.3%）。方差分析确认安全分数在模型和领域间存在显著差异（p < .05）。

Conclusion: 研究强调了LLM安全性的不一致性和上下文依赖性，并突出了像Aymara AI这样可扩展、可定制的工具在支持负责任AI开发和监督中的必要性。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [251] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: 本文探讨生成式AI在城市规划中的应用，指出当前研究的不足，并提出了理论引导生成、数字孪生和人机协同设计等未来方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI、大语言模型和代理AI的出现为城市规划提供了新的机会，本文旨在将城市规划概念化为生成式AI任务。

Method: 本文调查了生成式AI方法（如VAEs、GANs、transformers和扩散模型）如何重塑城市设计，并指出了当前研究中的关键缺口。

Result: 本文识别了四个关键研究缺口，并提出了相应的未来研究方向。

Conclusion: 本文呼吁生成智能与参与式城市主义的新综合，提出了理论引导生成、数字孪生和人机协同设计等未来研究方向。

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [252] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: AgentFly是一个结合语言模型代理和强化学习的可扩展框架，支持多轮交互和高吞吐量训练，成功应用于多任务。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理与强化学习的结合（Agent-RL）缺乏系统性研究，因此需要构建一个系统化的框架来探索和增强其能力。

Method: 构建了AgentFly，一个支持多轮交互、异步执行工具调用和奖励计算的可扩展Agent-RL框架，采用基于装饰器的接口定义工具和奖励函数。

Result: 实现了高吞吐量训练，成功在多任务中训练了代理，展示了框架的有效性。

Conclusion: AgentFly框架通过结合语言模型代理和强化学习，提供了一个可扩展且易于使用的解决方案，成功在多任务中展示了其有效性。

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [253] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: InsightX Agent是一种基于LMM的新型代理框架，通过结合SDMSD和EGR工具，提升了X射线无损检测的可靠性和可解释性，实验表明其高效且可信。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的X射线检测方法缺乏交互性、可解释性和自我评估能力，限制了其可靠性和操作员信任。

Method: 提出InsightX Agent框架，以LMM为核心协调器，结合SDMSD进行多尺度缺陷检测和EGR工具进行证据链式反思，优化检测流程并提升结果的可信度。

Result: 在GDXray+数据集上，InsightX Agent实现了96.35%的目标检测F1分数，同时显著提升了分析的可解释性和可信度。

Conclusion: InsightX Agent通过结合LMM、SDMSD和EGR工具，显著提升了X射线无损检测的可靠性、可解释性和交互性，展示了基于代理的LLM框架在工业检测任务中的变革潜力。

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [254] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: LLMs在简单决策任务中表现优于传统RL方法，但在复杂环境中表现不佳，需进一步研究混合策略和微调。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在自主决策设置中的适用性，特别是在利用其从多样数据集中获取的先验知识进行快速适应方面。

Method: 研究了在线结构化提示策略在顺序决策任务中的应用，并比较了基于LLM的方法与经典RL方法的零样本性能。

Result: 研究发现，LLMs在简单环境中表现良好，但在复杂场景中缺乏规划和推理能力，且反馈机制可能引入混淆，导致性能下降。

Conclusion: 论文指出，虽然LLMs在简单环境中表现出更好的初始性能，但在复杂场景中仍需进一步探索混合策略、微调和高级内存集成，以提升其决策能力。

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [255] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: Endless Tuning是一种双镜像设计方法，通过反向XAI算法部署填补责任缺口，实验证明用户感知完全控制，并建立了责任与问责的桥梁。


<details>
  <summary>Details</summary>
Motivation: 旨在解决人工智能部署中的责任缺口问题，同时避免人类被替代的风险，倡导一种不同的AI伦理视角（Gilligan 1993）。

Method: Endless Tuning是一种基于双镜像过程的设计方法，通过反向和诠释性的XAI算法部署，实现了三个原型应用（贷款审批、肺炎诊断和艺术风格识别）的测试，并与领域专家合作验证。

Result: 实验显示，尽管深度学习的应用非常深入，但用户在决策过程中感知到了完全控制，且责任与问责之间可以建立联系。

Conclusion: 通过双镜像过程实现的Endless Tuning设计方法，不仅避免了人类被替代的风险，还填补了责任缺口（Matthias 2004）。实验结果表明，用户在决策过程中感知到了完全控制，同时为损害情况下的责任与问责之间搭建了桥梁。

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [256] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: 本文探讨了基于LLMs的Agentic AI在老年护理中的潜力与挑战，强调需平衡技术创新与伦理保障，填补了相关文献空白。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化人口需要新的护理策略，Agentic AI在提升老年人独立性和生活质量方面具有潜力，但也引发了数据隐私和决策独立性等担忧。

Method: 通过分析基于大型语言模型（LLMs）的Agentic AI在老年护理中的独特能力、应用和局限性，填补了现有文献的空白。

Result: Agentic AI在健康监测、认知护理和环境管理等方面有重要应用潜力，但需解决伦理和隐私问题。

Conclusion: 本文强调了在老年护理中应用Agentic AI的潜力与挑战，并提出了伦理保障、隐私保护和透明决策的重要性。同时，指出了学术研究社区在实现以人为中心的Agentic AI集成中的优先事项。

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [257] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 本文引入面（facets）概念，细化命题溯因解释的可变性，并在Post框架中几乎完全表征了面，为高复杂度推理问题提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 命题溯因在人工智能和数据库更新等领域有广泛应用，但最深入的推理问题（如计数和枚举）计算复杂度高。因此，需要在决策和计数之间进行推理，以更好地理解解释同时保持有利的复杂度。

Method: 引入面（facets）作为命题溯因中的字面量，分析其在解释中的相关性和可替代性，并考虑解释之间的距离以理解异质性/同质性。

Result: 提出了面的概念，并全面分析了命题溯因中面在各种设置下的性质，包括在Post框架中的几乎完全表征。

Conclusion: 本文通过引入面（facets）的概念，为命题溯因提供了更细粒度的解释可变性理解，并在Post框架中几乎完全表征了命题溯因的面。

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [258] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign是一种纯强化学习框架，通过双奖励系统激发LLMs的内在安全意识，实现深度对齐并保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前的安全对齐方法往往导致浅层拒绝或依赖密集监督，未能充分利用模型内在的安全自我意识。

Method: AlphaAlign采用双奖励系统：可验证的安全奖励鼓励对有害查询的正确拒绝和明确理由，同时惩罚过度拒绝；标准化帮助性奖励则引导对良性输入的高质量响应。

Result: AlphaAlign在简化流程、打破安全-效用权衡及促进深度安全对齐方面展现出显著优势。

Conclusion: AlphaAlign通过纯强化学习框架有效激发大型语言模型的内在安全自我意识，实现了深度安全对齐，同时保持或提升通用任务性能。

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [259] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: 本文提出了一种基于深度学习的迫选神经认知诊断模型（FCNCD），有效克服传统模型的限制，实验验证了其准确性、可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在智能化时代，心理测量测试对人员选拔、职业发展和心理健康评估日益重要。迫选测试因其能降低回答失真的风险而常用于人格评估，但传统模型存在局限性。

Method: 研究提出了一种基于深度学习的迫选神经认知诊断模型（FCNCD），通过非线性映射挖掘参与者和项目特征，并使用多层神经网络建模其交互作用，同时利用单调性假设提升诊断结果的可解释性。

Result: 实验验证了FCNCD模型在真实和模拟数据集上的准确性、可解释性和鲁棒性。

Conclusion: FCNCD模型通过实验验证了其在准确性、可解释性和鲁棒性方面的有效性，为心理测量测试中的迫选测试提供了新的解决方案。

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [260] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: 本文提出了一种基于差分进化（DE）的对抗性提示优化方法，能有效攻击RAG系统并规避检测，实验证明了其高效性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 对抗性提示攻击会显著影响检索增强生成（RAG）系统的可靠性，导致错误输出。本文旨在提出一种更接近真实场景的攻击方法。

Method: 本文提出了一种基于差分进化（DE）的梯度无关方法，通过优化对抗性提示后缀来攻击RAG系统。该方法将RAG管道视为黑箱，通过进化候选后缀种群来最大化目标错误文档的检索排名。

Result: 实验表明，DE优化的对抗性提示后缀在BEIR QA数据集上取得了与GGPP和PRADA相当甚至更高的攻击成功率，同时仅需少量标记（<=5个）。此外，该方法生成的提示后缀能有效规避BERT-based检测器的检测。

Conclusion: 使用差分进化（DE）优化的对抗性提示后缀在RAG系统中表现出色，不仅能有效提高错误文档的检索排名，还能规避检测，展示了在实际应用中的潜力。

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [261] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: 提出CAIS作为内在奖励，通过因果推断量化行动影响，使代理在嘈杂环境中稳健学习，并重现心理学现象。


<details>
  <summary>Details</summary>
Motivation: 解决标准强化学习代理在嘈杂、生态有效场景中依赖基于相关性的奖励而表现脆弱的问题。

Method: 引入了一种新颖的内在奖励——因果行动影响分数（CAIS），通过测量基于行动的条件感官结果分布与基线结果分布之间的1-Wasserstein距离来量化行动的影响。

Result: CAIS使代理能够过滤噪音、识别其影响并学习正确的策略，且在增强惊喜信号后成功重现了“灭绝爆发”现象。

Conclusion: 明确推断因果关系是发展强大代理感的关键机制，为更自适应的自主系统提供了心理上合理的框架。

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [262] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: 论文提出了一种结合DL-Lite本体和自动化规划的新方法，通过编译实现，实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 将背景知识（如本体论）融入自动化规划问题，以增强规划能力。

Method: 采用多项式编译将本体感知的动作效果和显式输入知识结合到经典规划中。

Result: 通过编译方法实现，实验验证了不同编译变体在规划系统上的性能。

Conclusion: 该论文提出了一种结合DL-Lite本体和自动化规划的新方法，展示了其复杂度不高于现有方法，并通过实验验证了编译方法的性能。

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [263] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: CSI是一种新型AI框架，通过模拟专家临床推理诊断118种口腔疾病，标准模式准确率达89.5%。


<details>
  <summary>Details</summary>
Motivation: 口腔疾病诊断存在症状重叠的临床挑战，需要超越简单的模式匹配，模拟专家推理过程以开发实用的临床诊断辅助工具。

Method: CSI框架整合了微调的多模态CLIP模型和专门的ChatGLM-6B语言模型，采用分层诊断推理树（HDRT）进行系统性、多步骤的逻辑推理，提供快速筛查和标准模式两种诊断方式。

Result: 在431张内部测试图像上，CSI的快速模式准确率为73.4%，标准模式准确率提升至89.5%，性能提升直接归因于分层推理过程。

Conclusion: CSI框架通过模拟专家临床推理过程，成功开发了一种能够诊断118种口腔疾病的人工智能系统，其标准模式的准确率高达89.5%，验证了分层诊断推理树（HDRT）的有效性。

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [264] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: 该研究通过混合模拟框架评估了沙特NEOM线性智能城市The Line中人类移动的可行性，结果显示AI集成系统可实现高效、满意的移动体验。


<details>
  <summary>Details</summary>
Motivation: To assess whether citizens can move freely within The Line, a proposed 170-kilometer linear smart city in NEOM, Saudi Arabia.

Method: A hybrid simulation framework integrating agent-based modeling, reinforcement learning, supervised learning, and graph neural networks.

Result: With the full AI-integrated architecture, agents achieved an average commute time of 7.8 to 8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index of over 91 percent, even during peak congestion periods.

Conclusion: The findings suggest that freedom of movement is not only conceptually achievable in The Line, but also operationally realistic if supported by adaptive AI systems, sustainable infrastructure, and real-time feedback loops.

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [265] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover 通过智能体框架结合通用 LLM 和 Lean 4 环境，无需专业化模型即实现高效定理证明，在 miniF2F-test 上达到 95.9% 成功率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管通用 LLM 在复杂推理任务中表现优异，但在 Lean 4 等专业语言中生成形式化证明仍面临挑战，现有方法需依赖高成本的专业化模型微调。

Method: Delta Prover 是一个基于智能体的框架，通过协调通用 LLM 与 Lean 4 证明环境的交互，结合反射分解、迭代证明修复算法和定制领域特定语言（DSL），无需模型专业化即可构建形式化证明。

Result: Delta Prover 在 miniF2F-test 基准测试中达到 95.9% 的成功率，超越所有现有方法（包括需模型专业化的方法），并展现出更强的测试时扩展规律。

Conclusion: Delta Prover 展示了通用大型语言模型（LLM）在有效智能体结构引导下具备显著的定理证明潜力，为形式化环境中的自动化推理提供了一种计算高效的专业模型替代方案。

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [266] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: 该研究提出软评估指标和轻量级神经网络，提升电弧故障诊断模型的可解释性和可信度，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有AI电弧故障诊断模型在分类准确度上表现优异，但其可信度存疑。本研究旨在解决这一问题，使模型输出更易理解和信任。

Method: 提出一个软评估指标来解释电弧故障诊断模型的输出，并结合可解释人工智能和真实电弧故障实验。同时，提出一个轻量级平衡神经网络以保证竞争性准确度和软特征提取分数。

Result: 在两个不同采样时间和噪声水平的电弧故障数据集上，通过传统机器学习和深度学习方法验证了软评估指标的有效性。

Conclusion: 通过提出软评估指标和轻量级平衡神经网络，该研究使电弧故障诊断模型更易理解和可信，帮助从业者做出明智且可靠的决策。

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [267] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: DMGC通过解耦多模态图的同质性和异质性关系，结合自监督学习，实现了高效的多模态图聚类，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 多模态图在无监督学习中的潜力尚未充分探索，尤其是现实中的多模态图常表现出混合邻域模式，结合了同质性和异质性关系，需要新的方法来解决这一挑战。

Method: 提出了一种名为DMGC的新框架，通过将原始混合图分解为同质性增强图和异质性感知图，并引入多模态双频融合机制，结合双通道策略进行有效整合。

Result: 在多个多模态和多关系图数据集上的实验表明，DMGC实现了最先进的性能，验证了其有效性和泛化能力。

Conclusion: DMGC框架通过解耦多模态图中的同质性和异质性关系，并采用自监督对齐目标，实现了在多模态图聚类中的最先进性能，展示了其在不同设置下的有效性和泛化能力。

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [268] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: IM-Chat是一个基于LLM的多代理框架，旨在解决注塑行业的知识传递难题，通过RAG和模块化设计实现无需微调的适应性，评估显示高性能模型在复杂任务中表现更佳。


<details>
  <summary>Details</summary>
Motivation: 注塑成型行业在保留和传递现场知识方面面临关键挑战，尤其是随着经验丰富的工人退休和多语言障碍阻碍有效沟通。

Method: 本研究提出了IM-Chat，一个基于大型语言模型（LLMs）的多代理框架，采用检索增强生成（RAG）策略和工具调用代理的模块化架构，无需微调即可实现适应性。

Result: 评估结果表明，性能更强的模型在复杂、工具集成的场景中表现更优，特别是在相关性和正确性方面。

Conclusion: 研究结果表明，多代理LLM系统在工业知识工作流中具有可行性，IM-Chat作为一种可扩展且通用的方法，为制造业中的AI辅助决策支持提供了有效解决方案。

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [269] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: 本文提出认知退化作为智能代理系统的新型漏洞类别，并引入QSAF框架进行防御，通过实时监控和主动缓解措施提升代理行为韧性。


<details>
  <summary>Details</summary>
Motivation: 认知退化是智能代理系统中的新型漏洞类别，源于内存饥饿、规划器递归、上下文洪泛和输出抑制等内部问题，导致代理漂移、逻辑崩溃和持久幻觉等系统性弱点。

Method: 研究引入了六阶段认知退化生命周期和七个运行时控制（QSAF-BC-001至BC-007），包括后备路由、饥饿检测和内存完整性强制等措施。

Result: QSAF框架通过实时监控和主动缓解措施，有效应对认知退化问题，并首次建立了跨平台的智能代理行为韧性模型。

Conclusion: 本研究提出了Qorvex Security AI Framework（QSAF Domain 10）作为应对认知退化的防御框架，通过实时监控和主动缓解措施，首次建立了跨平台的智能代理行为韧性模型。

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [270] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: 论文提出GRPO和OSPO方法，通过绕过值函数估计解决了传统MARL拼车方法在大规模不确定环境中的问题，实验证明其高效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于MARL的拼车方法因依赖Q值/V值准确估计，在大规模不确定环境中表现不佳，且独立范式加剧了训练不稳定和估值偏差。

Method: 论文提出了两种方法：1) 将GRPO应用于拼车场景，用组平均奖励替代PPO基线以消除评估误差；2) 定制PPO框架并提出OSPO，仅需一步奖励即可训练最优策略。

Result: 在真实曼哈顿拼车数据集上的实验表明，GRPO和OSPO在多数场景中性能优越。

Conclusion: 论文提出的GRPO和OSPO方法在动态拼车平台中表现出色，有效优化了接载时间和订单服务数量，且仅需简单MLP网络即可实现。

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [271] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: RAD通过检索高质量演示和扩散模型提升离线强化学习的泛化能力，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习因数据集稀疏和轨迹重叠不足导致长期规划困难，现有方法如合成数据增强或轨迹拼接难以泛化到新状态。

Method: RAD结合非参数检索和扩散生成模型，动态检索高回报状态作为目标状态，并利用条件引导的扩散模型进行规划。

Result: 实验表明，RAD在多种基准测试中表现优于基线方法。

Conclusion: RAD方法通过结合非参数检索和基于扩散的生成模型，有效解决了离线强化学习中数据集稀疏和轨迹重叠不足的问题，提升了长期规划的泛化能力。

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [272] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: 提出了一种结合图注意力网络和LSTM的端到端模型，用于对象中心预测过程监控，在下一个活动和时间预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 对象中心预测过程监控的主要挑战在于提取相关信息并构建有效模型。

Method: 模型结合了图注意力网络（用于编码活动及其关系）和LSTM网络（用于处理时间依赖性）。

Result: 在一个现实生活和三个合成事件日志上的评估表明，该模型性能优于现有方法。

Conclusion: 该论文提出的端到端模型在对象中心预测过程监控中表现出色，特别是在下一个活动预测和下一个事件时间任务上，相比现有方法具有竞争优势。

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [273] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文提出一种基于干预启发式的帕累托优化方法，用于优化商业流程中的活动批处理策略，实验证明其在多项指标上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 商业流程中活动批处理策略需要在等待时间、处理成本和资源利用率之间找到最优权衡，现有方法缺乏高效的启发式引导。

Method: 采用帕累托优化方法，结合干预启发式和三种元启发式（爬山法、模拟退火和强化学习），通过仿真评估干预效果。

Result: 实验结果表明，基于干预启发式的方法在帕累托最优策略的收敛性、多样性和周期时间增益方面表现更优。

Conclusion: 本文提出了一种基于干预启发式的帕累托优化方法，用于发现商业流程中活动批处理策略的最优权衡。实验评估表明，该方法在收敛性、多样性和周期时间增益方面优于非启发式引导的元启发式基线。

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [274] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1通过强化学习微调和多阶段训练策略，在图表推理任务中表现优于现有方法，接近大型模型水平。


<details>
  <summary>Details</summary>
Motivation: 验证R1-Style方法在多模态数据（尤其是图表）上的优势，解决图表领域复杂推理数据不足的问题。

Method: 提出了一种新颖的程序化数据合成技术生成高质量逐步推理数据，并采用两阶段训练策略：Chart-COT（逐步监督）和Chart-RFT（数值敏感的强化微调）。

Result: 实验结果表明，Chart-R1在开源基准和自建数据集（ChartRQA）上表现优异。

Conclusion: Chart-R1在图表领域表现出显著优势，甚至可与开源/闭源大型模型（如GPT-4o、Claude-3.5）相媲美。

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [275] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: HAMLET是一个多智能体框架，通过赋予演员自主思维和互动能力，解决了现有LLM戏剧生成方法的局限，实现了更具交互性和沉浸感的在线戏剧表演。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的戏剧生成方法通常导致AI智能体缺乏主动性且无法与物理环境互动，且需详细用户输入驱动戏剧，降低了在线实时表演的交互性和沉浸感。

Method: 提出了HAMLET，一个专注于戏剧创作和在线表演的多智能体框架。该框架通过生成叙事蓝图指导即兴表演，并为每个演员赋予自主思维，使其能基于背景、目标和情感状态独立决策，并通过动作改变场景道具状态。

Result: HAMLET能够根据简单主题生成叙事蓝图，并支持演员通过自主决策和动作改变场景状态，从而创建富有表现力和连贯性的戏剧体验。

Conclusion: HAMLET框架能够生成富有表现力和连贯性的戏剧体验，实验评估验证了其在角色表现、叙事质量和互动体验方面的有效性。

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [276] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: 论文探讨LLMs是否依赖内部世界模型还是统计关联。通过滑轮系统测试，发现LLMs能利用滑轮数量与MA的统计关联（研究1）和近似表示空间关系（研究2），但对结构连通性推理能力有限（研究3）。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨大型语言模型（LLMs）是否构建和操纵内部世界模型，还是仅依赖于表示为输出层令牌概率的统计关联。

Method: 研究采用了认知科学方法，通过TikZ渲染的刺激物测试LLMs在滑轮系统问题上的表现。研究1测试了LLMs是否能估计机械优势（MA）；研究2探究了LLMs是否能表示对MA估计至关重要的全局特征；研究3进一步测试了LLMs对功能系统与非功能系统的区分能力。

Result: 研究1显示，最先进的模型在MA估计上表现略高于随机水平，且其估计与真实MA显著相关。研究2发现，LLMs能够区分功能系统与随机组件系统（F1=0.8）。研究3显示，LLMs在区分功能系统与无力量传递的匹配系统时表现接近随机猜测（F1=0.46）。

Conclusion: 论文结论认为，虽然大型语言模型（LLMs）可能操纵内部世界模型，足以利用滑轮数量与机械优势（MA）之间的统计关联（研究1），并近似表示系统组件的空间关系（研究2），但它们可能缺乏对细微结构连通性进行推理的能力（研究3）。作者提倡使用认知科学方法来评估人工智能系统的世界建模能力。

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [277] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: 本文通过利用参数依赖关系和预处理技术，显著提升了安全策略改进（SPI）的数据效率，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在SPI问题中，许多应用场景下存在额外的参数依赖信息，但现有方法未能充分利用这些信息来提高数据效率。

Method: 1. 提出了一种参数化SPI算法，利用分布之间的已知相关性更准确地估计转移动态；2. 引入了一种基于游戏抽象的预处理技术，用于修剪冗余动作；3. 提出了一种基于可满足性模理论（SMT）的更高级预处理技术，能够识别更多可修剪的动作。

Result: 实验结果表明，所提技术将SPI的数据效率提高了多个数量级，同时保持了相同的可靠性。

Conclusion: 本文提出的技术通过利用已知的参数依赖关系，显著提高了安全策略改进（SPI）的数据效率，同时保持了相同的可靠性保证。

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [278] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: 研究提出新评估协议，发现现有指标与答案波动率关联强，‘最差准确率’表现最好。


<details>
  <summary>Details</summary>
Motivation: 多选问题（MCQ）评估中存在的答案波动问题未被充分研究，且缺乏对现有评估指标的全面评估。

Method: 提出了一种基于答案波动率和原始性能的评估协议，分析了不同评估方法的表现。

Result: 结果显示，现有指标与答案波动率存在强关联，新指标‘最差准确率’表现最佳。

Conclusion: 研究发现，现有评估指标与答案波动率之间存在显著关联，而提出的新指标‘最差准确率’在该协议中显示出最高的关联性。

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [279] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: 提出一种基于适配器的StarCraft II AI战术调控方法，通过轻量级模块实现灵活策略调整，计算高效且保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理虽强大，但缺乏基于高层战术指令调整策略的能力，需一种灵活且低开销的战术调控方法。

Method: 冻结预训练策略网络（DI-Star），在每个动作头附加轻量级适配模块，通过KL散度约束训练适配器，战术张量编码战略偏好。

Result: 实验表明，该方法能有效调控代理行为（如侵略性、扩张模式、技术偏好），同时保持竞争力。

Conclusion: 该方法通过轻量级适配模块实现了对StarCraft II AI代理的战术调控，既保持了核心能力，又提供了灵活的策略定制，计算开销低，适用于复杂实时策略游戏。

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [280] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: 论文探讨了代理式AI在自主异常管理中的潜力，展示了其优于传统方法的效率和自主性。


<details>
  <summary>Details</summary>
Motivation: 传统异常管理方法依赖人类，效率低下且成本高昂，代理式AI有望提供更高效、自动化的解决方案。

Method: 探索了代理式AI在异常检测和响应中的应用，重点分析了其自主性和适应性。

Result: 研究表明代理式AI能够有效识别和应对复杂系统中的异常，减少对人类干预的依赖。

Conclusion: 该论文强调了代理式AI在复杂系统中自主检测和响应异常的潜力，能够改变传统依赖人类的异常管理方法。

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [281] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: 研究提出g-AMIE多代理系统，通过异步监督框架提升诊断AI系统的临床决策质量，实验显示其在多个方面优于人类团队，且更高效。


<details>
  <summary>Details</summary>
Motivation: 受现实中医生监督其他团队成员（如护士从业者或医师助理）的启发，研究旨在为诊断对话AI系统提供有效的异步监督框架。

Method: 提出了guardrailed-AMIE（g-AMIE）多代理系统，该系统在护栏内执行病史采集，避免提供个性化医疗建议，并通过临床医生驾驶舱界面将评估结果传达给监督的初级保健医生（PCP）。

Result: 在随机、盲法的虚拟OSCE中，g-AMIE在高质量病史采集、病例总结及诊断和管理计划提议方面优于NPs/PAs和PCPs组，且PCP监督g-AMIE比独立PCP咨询更高效。

Conclusion: 研究展示了异步监督作为一种可行范式，让诊断AI系统在专家人类监督下运行，以增强现实世界中的医疗护理。

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [282] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: LAPO通过两阶段强化学习让模型自适应控制推理长度，减少40.9%的token使用并提升2.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在简单问题上过度生成token的问题，将推理长度控制从外部约束转变为模型的内在能力。

Method: 提出了Length-Adaptive Policy Optimization (LAPO)框架，通过两阶段强化学习过程：第一阶段学习成功的解决方案长度的统计分布，第二阶段将这些模式作为元认知指导嵌入模型的推理上下文中。

Result: 在数学推理基准测试中，LAPO减少了高达40.9%的token使用，同时准确率提高了2.3%。

Conclusion: LAPO框架通过两阶段强化学习，使模型能够内化对推理深度的理解，从而在不牺牲推理质量的前提下，显著减少计算资源的使用。

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [283] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: GasAgent 是首个结合兼容性与自动化发现/验证的多智能体系统，用于智能合约 Gas 优化，实验证明其高效且广泛适用。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案依赖于人工发现 Gas 浪费模式，效率低且难以扩展。近期研究尝试使用大型语言模型（LLMs）探索新模式，但存在兼容性差、冗余模式多和需手动验证的问题。

Method: GasAgent 由四个专门化的智能体（Seeker、Innovator、Executor 和 Manager）组成，通过闭环协作识别、验证和应用 Gas 节省改进。

Result: 在 100 个已验证的真实合约中，GasAgent 成功优化了 82 个，平均部署 Gas 节省为 9.97%。此外，对 500 个由 LLMs 生成的合约评估显示，GasAgent 优化了 79.8%，Gas 节省范围为 4.79% 至 13.93%。

Conclusion: GasAgent 是一个多智能体系统，能够有效优化智能合约的 Gas 消耗，同时兼容现有工具，并通过实验验证了其模块的有效性和广泛适用性。

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [284] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: EAMI框架通过双视角思维追踪和k-means聚类，动态分析智能体意图的涌现，为复杂服务生态系统提供新分析方法。


<details>
  <summary>Details</summary>
Motivation: 随着服务计算、云计算和物联网的发展，服务生态系统日益复杂，传统因果分析方法难以应对智能体间的复杂交互。

Method: EAMI框架采用双视角思维追踪机制，结合k-means聚类和意图时间涌现图，实现动态和可解释的涌现分析。

Result: 实验证明EAMI在复杂O2O服务系统和Stanford AI Town实验中有效，消融研究验证了其效果、通用性和效率。

Conclusion: 本文提出的EAMI框架为服务生态系统中的异常涌现和因果分析提供了新范式，通过实验验证了其有效性、通用性和效率。

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [285] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: 本文系统分析了联邦学习（FL）在满足可信人工智能（TAI）要求时的挑战，总结了现有研究和未来方向。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）因其分布式特性在隐私保护方面具有潜力，但与可信人工智能（TAI）的其他要求对齐存在挑战，需要系统研究。

Method: 采用TAI的要求作为指导结构，系统分类和分析了FL在适应TAI过程中的关键障碍，并详细探讨了现有研究、趋势和未解决问题。

Result: 研究识别了FL与TAI对齐的主要挑战，总结了现有进展和未来研究方向，为相关领域提供了参考。

Conclusion: 本文通过系统分析联邦学习（FL）在可信人工智能（TAI）框架下的挑战，提出了未来研究的方向和潜在解决方案，强调了FL与TAI对齐的重要性。

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [286] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: 本文在MPDAG设置下提出了条件因果效应的识别方法，包括识别公式、do calculus推广和完整算法。


<details>
  <summary>Details</summary>
Motivation: 解决在MPDAG设置下条件因果效应的识别问题，扩展现有因果推断方法的适用性。

Method: 通过分析MPDAG（最大定向部分有向无环图）的等价类，结合背景知识和所有变量的观测数据。

Result: 提出了一个识别公式、do calculus的推广以及一个完整的识别算法，为MPDAG设置下的条件因果效应识别提供了理论和方法支持。

Conclusion: 本文提供了在MPDAG设置下识别条件因果效应的三个结果，包括一个识别公式、do calculus的推广以及一个完整的识别算法。

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [287] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: HBPO是一种强化学习框架，通过分层预算探索和差异化奖励机制，使模型能够根据问题复杂度自适应调整推理深度，显著提高计算效率且不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在广泛生成思维链时表现出显著的计算低效性，因为它们无论问题复杂度如何均采用统一的推理策略。

Method: 提出了分层预算策略优化（HBPO），这是一个强化学习框架，通过分层预算探索和差异化奖励机制，使模型能够根据问题复杂度自动调整推理深度。

Result: HBPO在四个推理基准测试中平均减少了60.6%的令牌使用，同时准确率提高了3.14%。

Conclusion: 研究结果表明，推理效率和能力并非固有冲突，通过适当结构化的分层训练可以同时优化两者。

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [288] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 研究发现LLMs会自发建立主观时间参考点并遵循韦伯-费希纳定律，揭示了时间偏好神经元和对数编码机制，提出了经验主义视角以理解LLMs的认知构建。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的进步，它们展现出与人类相似的某些认知模式，这些模式并未在训练数据中直接指定。本研究旨在通过聚焦LLMs的时间认知来探究这一现象。

Method: 研究通过相似性判断任务，发现更大规模的模型会自发建立主观时间参考点，并遵循韦伯-费希纳定律。通过神经元、表征和信息层面的多重分析，识别了一组时间偏好神经元，揭示了年份表征的分层构建过程，并发现训练语料本身具有内在的非线性时间结构。

Result: 研究发现，更大规模的LLMs会自发建立主观时间参考点，并遵循韦伯-费希纳定律。神经元分析表明，时间偏好神经元在主观参考点处激活最小，并实现了对数编码方案。年份表征从浅层的数值逐步构建为深层的抽象时间定向。训练语料本身具有非线性时间结构，为模型的内部构建提供了原材料。

Conclusion: 论文提出了一个经验主义的视角，将LLMs的认知视为其内部表征系统对外部世界的主观构建。这一视角暗示了可能出现的、人类无法直观预测的异类认知框架，为AI对齐提供了一个关注内部构建引导的方向。

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [289] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: 研究展示了如何通过优化模型使用方式，使Gemini 2.5 Pro在IMO 2025问题上取得高成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在一般数学基准测试中表现良好，但在奥林匹克级别的任务中仍面临挑战。

Method: 采用Google的Gemini 2.5 Pro模型，结合管道设计和提示工程，处理IMO 2025的新题目。

Result: 在6个问题中正确解决了5个（存在一个注意事项）。

Conclusion: 研究强调了在解决高难度数学竞赛问题时，找到最优方法使用强大模型的重要性。

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [290] [Iran's Stealth Internet Blackout: A New Model of Censorship](https://arxiv.org/abs/2507.14183)
*Arash Aryapour*

Main category: cs.NI

TL;DR: 伊朗2025年新型互联网关闭手段通过隐蔽技术隔离国内用户，导致VPN需求激增707%，研究揭示了其多层审查机制及其对数字权利的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了量化和分析伊朗新型互联网关闭手段的技术细节及其对VPN需求的影响，以及对数字权利的潜在威胁。

Method: 论文通过主动网络测量（如DNS投毒、HTTP注入、TLS拦截和协议白名单）分析了这一现象，并将这些行为追溯至一个集中化的边界网关。

Result: 研究发现VPN需求激增约707%，并详细描述了多层审查基础设施的运作方式。

Conclusion: 该论文揭示了伊朗在2025年中期实施的一种新型、隐蔽的互联网关闭手段，通过保留全局路由存在，同时利用深度包检测、激进限速和选择性协议封锁隔离国内用户。研究强调了这种多层审查基础设施对规避技术和数字权利监控的影响。

Abstract: In mid-2025, Iran experienced a novel, stealthy Internet shutdown that
preserved global routing presence while isolating domestic users through deep
packet inspection, aggressive throttling, and selective protocol blocking. This
paper analyzes active network measurements such as DNS poisoning, HTTP
injection, TLS interception, and protocol whitelisting, traced to a centralized
border gateway. We quantify an approximate 707 percent rise in VPN demand and
describe the multi-layered censorship infrastructure, highlighting implications
for circumvention and digital rights monitoring.

</details>


### [291] [A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction](https://arxiv.org/abs/2507.14186)
*Xiaojie Li,Zhijie Cai,Nan Qi,Chao Dong,Guangxu Zhu,Haixia Ma,Qihui Wu,Shi Jin*

Main category: cs.NI

TL;DR: 该论文提出了一种结合专家知识特征压缩和解耦表示学习的双策略方法，用于解决低空网络覆盖预测中的数据稀疏和泛化问题，实验证明其有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 低空网络覆盖（LANC）预测对设计空中走廊至关重要，但基站（BS）的天线波束模式通常是专有的且不易获取。BS的操作参数虽包含波束信息，但收集大量低空路测数据成本高昂，导致每个BS的样本稀疏，进而面临特征采样不平衡和泛化能力不足的挑战。

Method: 提出了一种双策略，包括基于专家知识的特征压缩和解耦表示学习。前者通过通信专业知识减少特征空间复杂度，后者通过整合传播模型和捕捉并聚合潜在特征语义表示的独立子网络来增强模型的泛化能力。

Result: 提出的方法在实验中表现优异，误差减少了7%，实际网络验证中MAE误差达到5dB级别。

Conclusion: 实验评估证实了该框架的有效性，相比最佳基线算法误差减少了7%。实际网络验证进一步证明了其可靠性，实现了5dB级别的MAE误差，达到了实用的预测精度。

Abstract: The expansion of the low-altitude economy has underscored the significance of
Low-Altitude Network Coverage (LANC) prediction for designing aerial corridors.
While accurate LANC forecasting hinges on the antenna beam patterns of Base
Stations (BSs), these patterns are typically proprietary and not readily
accessible. Operational parameters of BSs, which inherently contain beam
information, offer an opportunity for data-driven low-altitude coverage
prediction. However, collecting extensive low-altitude road test data is
cost-prohibitive, often yielding only sparse samples per BS. This scarcity
results in two primary challenges: imbalanced feature sampling due to limited
variability in high-dimensional operational parameters against the backdrop of
substantial changes in low-dimensional sampling locations, and diminished
generalizability stemming from insufficient data samples. To overcome these
obstacles, we introduce a dual strategy comprising expert knowledge-based
feature compression and disentangled representation learning. The former
reduces feature space complexity by leveraging communications expertise, while
the latter enhances model generalizability through the integration of
propagation models and distinct subnetworks that capture and aggregate the
semantic representations of latent features. Experimental evaluation confirms
the efficacy of our framework, yielding a 7% reduction in error compared to the
best baseline algorithm. Real-network validations further attest to its
reliability, achieving practical prediction accuracy with MAE errors at the 5dB
level.

</details>


### [292] [From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade Direct-to-Device Mobile Networks](https://arxiv.org/abs/2507.14188)
*Sebastian Barros Elgueta*

Main category: cs.NI

TL;DR: 论文提出了一种完全基于轨道的移动网络架构，通过模拟验证了其在密集城市中的可行性，并规划了15年路线图以实现城市级手持设备的高带宽连接。


<details>
  <summary>Details</summary>
Motivation: 探讨一个更宏大的问题：完整的移动网络（包括无线接入、核心功能、流量路由和内容交付）能否完全在轨道上运行，并在全球最密集的城市中提供持续的城市级服务。

Method: 论文提出了一种完全基于轨道的移动网络系统架构，包括电子控制相控阵、5G核心功能的空间部署（UPF、AMF）和卫星间激光网状回程。通过分析频谱效率、波束容量和链路预算，模拟了密集城市条件下的性能。

Result: 模拟结果显示，屋顶和视距用户可维持64-QAM吞吐量，而街道级接入可通过中继或辅助波束模式实现。论文还指出了剩余的工程瓶颈（如功率、热耗散、计算辐射硬化和监管模型）。

Conclusion: 论文提出了一个15年路线图，从当前的备用D2D系统逐步过渡到完全自主的轨道覆盖网络，最终实现城市级手持设备50-100 Mbps的带宽，无需依赖地面基础设施。

Abstract: In 2023, satellite and mobile networks crossed a historic threshold: standard
smartphones, using unmodified 3GPP protocols, connected directly to low Earth
orbit (LEO) satellites. This first wave of direct-to-device (D2D)
demonstrations validated the physical feasibility of satellite-based mobile
access. However, these systems remain fallback-grade--rural-only,
bandwidth-limited, and fully dependent on Earth-based mobile cores for
identity, session, and policy control. This paper asks a more ambitious
question: Can a complete mobile network, including radio access, core
functions, traffic routing, and content delivery, operate entirely from orbit?
And can it deliver sustained, urban-grade service in the world's densest
cities? We present the first end-to-end system architecture for a fully orbital
telco, integrating electronically steered phased arrays with 1000-beam
capacity, space-based deployment of 5G core functions (UPF, AMF), and
inter-satellite laser mesh backhaul. We analyze spectral efficiency, beam
capacity, and link budgets under dense urban conditions, accounting for path
loss, Doppler, and multipath. Simulations show that rooftop and line-of-sight
users can sustain 64-QAM throughput, while street-level access is feasible with
relay or assisted beam modes. The paper outlines the remaining constraints,
power, thermal dissipation, compute radiation hardening, and regulatory models,
and demonstrates that these are engineering bottlenecks, not physical limits.
Finally, we propose a staged 15-year roadmap from today's fallback D2D systems
to autonomous orbital overlays delivering 50-100 Mbps to handhelds in
megacities, with zero reliance on terrestrial infrastructure.

</details>


### [293] [On Splitting Lightweight Semantic Image Segmentation for Wireless Communications](https://arxiv.org/abs/2507.14199)
*Ebrahim Abu-Helalah,Jordi Serra,Jordi Perez-Romero*

Main category: cs.NI

TL;DR: 该论文提出了一种在发送端和接收端间分割语义图像分割过程的方法，显著降低了带宽需求和发送端计算负载，适用于6G系统。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信技术在资源受限环境和变化信道条件下难以平衡计算效率、带宽需求与图像分割精度，且复杂模型增加了设备处理数据的压力。

Method: 提出了一种新颖的语义通信实现方法，将语义图像分割过程分割在资源受限的发送端和接收端之间，以减少传输数据量并降低发送端的计算需求。

Result: 仿真实验表明，与发送端完全处理语义图像分割相比，该方法减少了72%的比特率和19%以上的发送端计算负载。

Conclusion: 该论文提出的方法在资源受限环境中有效平衡了计算效率和带宽需求，同时保持了高图像分割精度，适用于未来6G通信系统。

Abstract: Semantic communication represents a promising technique towards reducing
communication costs, especially when dealing with image segmentation, but it
still lacks a balance between computational efficiency and bandwidth
requirements while maintaining high image segmentation accuracy, particularly
in resource-limited environments and changing channel conditions. On the other
hand, the more complex and larger semantic image segmentation models become,
the more stressed the devices are when processing data. This paper proposes a
novel approach to implementing semantic communication based on splitting the
semantic image segmentation process between a resource constrained transmitter
and the receiver. This allows saving bandwidth by reducing the transmitted data
while maintaining the accuracy of the semantic image segmentation.
Additionally, it reduces the computational requirements at the resource
constrained transmitter compared to doing all the semantic image segmentation
in the transmitter. The proposed approach is evaluated by means of
simulation-based experiments in terms of different metrics such as
computational resource usage, required bit rate and segmentation accuracy. The
results when comparing the proposal with the full semantic image segmentation
in the transmitter show that up to 72% of the bit rate was reduced in the
transmission process. In addition, the computational load of the transmitter is
reduced by more than 19%. This reflects the interest of this technique for its
application in communication systems, particularly in the upcoming 6G systems.

</details>


### [294] [A Fault-Tolerant Architecture for Urban and Rural Digital Connectivity: Synergizing SDWMN, Direct-to-Mobile Broadcasting, and Hybrid Cloud Streaming](https://arxiv.org/abs/2507.14205)
*Pavel Malinovskiy*

Main category: cs.NI

TL;DR: 该论文提出了一种集成SDWMN、D2M广播和Kafka的架构，显著提升了城市和农村的无线网络性能，实验显示延迟减少、带宽卸载和覆盖增益显著，并建议优化频谱分配以促进采用。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决城市网络拥堵和农村数字排斥问题，通过流量卸载、增强容错能力和公平资源分配来提高无线网络性能。

Method: 论文提出了一种结合软件定义无线网状网络（SDWMN）、直接到移动（D2M）广播和基于Kafka的混合云流媒体的集成架构。通过建模城市拥堵（ρ_u = λ_t / μ_c）和农村覆盖缺陷（δ_r = 1 - C_r / C_req），并最小化全局性能损失（GPL = w_1·ρ_u + w_2·δ_r + w_3·T_rec），其中T_rec为恢复时间。

Result: 在曼谷、孟买和芬兰农村的实验表明，系统实现了超过32%的延迟减少、40%的带宽卸载、28%的农村覆盖增益，公平指数从0.78上升至0.91，并在10秒内通过SDWMN和Kafka实现恢复。

Conclusion: 该论文建议通过优化频谱分配、定向补贴和设备授权来促进系统采用，并提出了一个可扩展且容错的设计，支持公平的数字化转型，为未来研究指明了方向。

Abstract: We propose an integrated architecture combining Software-Defined Wireless
Mesh Networks (SDWMN), Direct-to-Mobile (D2M) broadcasting, and Kafka-based
hybrid cloud streaming to improve wireless network performance in both urban
and rural settings. The approach addresses urban congestion and rural digital
exclusion through traffic offloading, enhanced fault tolerance, and equitable
resource allocation. We model urban congestion $\rho_u = \lambda_t / \mu_c$ and
rural coverage deficit $\delta_r = 1 - C_r / C_{req}$, and aim to minimize
global performance loss $GPL = w_1 \cdot \rho_u + w_2 \cdot \delta_r + w_3
\cdot T_{rec}$, where $T_{rec}$ is recovery time. Experiments in Bangkok,
Mumbai, and rural Finland demonstrate latency reduction over 32%, bandwidth
offloading of 40%, rural coverage gain of 28%, and fairness index rising from
0.78 to 0.91. The system achieves recovery under 10 s using SDWMN and Kafka. We
recommend optimal spectrum allocation $\alpha_s$, targeted subsidies, and
device mandates to promote adoption. This scalable, fault-tolerant design
supports equitable digital transformation and suggests directions for future
research.

</details>


### [295] [White paper: Towards Human-centric and Sustainable 6G Services -- the fortiss Research Perspective](https://arxiv.org/abs/2507.14209)
*Rute C. Sofia,Hao Shen,Yuanting Liu,Severin Kacianka,Holger Pfeifer*

Main category: cs.NI

TL;DR: fortiss的愿景是构建一个以人为中心、可持续且AI集成的6G网络，通过技术创新和社会责任推动2030年的数字化未来。


<details>
  <summary>Details</summary>
Motivation: 确保6G技术不仅实现技术进步，还能满足社会需求，超越前代技术，提供超可靠低延迟通信和个性化数字服务。

Method: 通过参与全球6G倡议、标准化机构和研发合作项目，fortiss聚焦于软件定义、AI赋能和可持续的通信服务，研究领域包括语义通信、绿色编排和分布式AI。

Result: 提出了6G网络的战略方向，包括AI原生网络、边缘-云资源编排和能源感知数据框架，以应对社会和技术的双重挑战。

Conclusion: fortiss提出了一个以人为中心、可持续且AI集成的6G网络愿景，强调技术创新与社会需求的结合，旨在为2030年的数字化未来奠定基础。

Abstract: As a leading research institute in software-intensive systems, fortiss is
actively shaping the vision of Sixth Generation Mobile Communication (6G). Our
mission is to ensure that 6G technologies go beyond technical advancements and
are aligned with societal needs. fortiss plays a key role in 6G initiatives
worldwide, including contributions to standardization bodies and collaborative
Research and Development programs. We focus on software-defined, AI-enabled,
and sustainable communication services that prioritize human values and
long-term impact. 6G will redefine digital connectivity through cognitive
intelligence, decentralized orchestration, and sustainability-oriented
architectures. As expectations rise for ultra-reliable low-latency
communication (URLLC) and personalized digital services, 6G must outperform
prior generations. It will rely on AI-native networking, Edge-Cloud resource
orchestration, and energy-aware data frameworks, ensuring both technical
performance and societal relevance. This white paper presents the fortiss
vision for a human-centric, sustainable, and AI-integrated 6G network. It
outlines key research domains such as semantic communication, green
orchestration, and distributed AI, all linked to societal and technological
challenges. The white paper is aimed at researchers, industry experts,
policymakers, and developers. It articulates the strategic direction and
contributions of fortiss to 6G, emphasizing responsible innovation and
interdisciplinary collaboration toward a meaningful 2030 vision.

</details>


### [296] [PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence](https://arxiv.org/abs/2507.14211)
*Federico Mason,Tommaso Zugno,Matteo Drago,Marco Giordani,Mate Boban,Michele Zorzi*

Main category: cs.NI

TL;DR: PRATA是一个基于AI的预测性QoS框架，通过RL优化远程驾驶的QoS决策，性能提升近两倍，并探讨了RL实现的关键因素。


<details>
  <summary>Details</summary>
Motivation: 预测性QoS（PQoS）对于如远程驾驶等严格延迟和可靠性要求的汽车应用至关重要。RL作为一种随机优化工具，为PQoS决策提供了潜力。

Method: PRATA是一个模块化框架，包括5G RAN端到端协议栈模拟、汽车数据生成工具和AI单元（如RAN-AI）用于优化PQoS决策。通过RL方法优化了远程驾驶数据的分段级别。

Result: RAN-AI实体有效平衡了QoS与QoE之间的权衡，系统性能几乎比基线方法翻倍。同时，研究还分析了状态空间和网络数据获取成本对RL实现的影响。

Conclusion: PRATA框架通过集成AI单元（如RAN-AI）有效优化了预测性QoS决策，显著提升了系统性能，几乎比基线方法翻倍。同时，研究还探讨了状态空间和网络数据获取成本对RL实现的影响。

Abstract: Predictive Quality of Service (PQoS) makes it possible to anticipate QoS
changes, e.g., in wireless networks, and trigger appropriate countermeasures to
avoid performance degradation. Hence, PQoS is extremely useful for automotive
applications such as teleoperated driving, which poses strict constraints in
terms of latency and reliability. A promising tool for PQoS is given by
Reinforcement Learning (RL), a methodology that enables the design of
decision-making strategies for stochastic optimization. In this manuscript, we
present PRATA, a new simulation framework to enable PRedictive QoS based on AI
for Teleoperated driving Applications. PRATA consists of a modular pipeline
that includes (i) an end-to-end protocol stack to simulate the 5G Radio Access
Network (RAN), (ii) a tool for generating automotive data, and (iii) an
Artificial Intelligence (AI) unit to optimize PQoS decisions. To prove its
utility, we use PRATA to design an RL unit, named RAN-AI, to optimize the
segmentation level of teleoperated driving data in the event of resource
saturation or channel degradation. Hence, we show that the RAN-AI entity
efficiently balances the trade-off between QoS and Quality of Experience (QoE)
that characterize teleoperated driving applications, almost doubling the system
performance compared to baseline approaches. In addition, by varying the
learning settings of the RAN-AI entity, we investigate the impact of the state
space and the relative cost of acquiring network data that are necessary for
the implementation of RL.

</details>


### [297] [Intent-Based Network for RAN Management with Large Language Models](https://arxiv.org/abs/2507.14230)
*Fransiscus Asisi Bimo,Maria Amparo Canaveras Galdon,Chun-Kai Lai,Ray-Guang Cheng,Edwin K. P. Chong*

Main category: cs.NI

TL;DR: 论文提出一种基于LLMs的RAN自动化管理方法，通过智能代理和闭环机制优化网络能效，展示了实时资源管理的潜力。


<details>
  <summary>Details</summary>
Motivation: 应对无线网络管理日益增长的复杂性，利用LLMs提升高级目标意图的自主解释和网络状态推理能力。

Method: 采用结构化提示工程技术，将LLMs集成到智能代理架构中，实现意图翻译、复杂网络状态推理和RAN精确配置生成。

Result: 通过闭环机制动态优化RAN参数，网络能效得到显著提升，验证了LLM协调的智能系统在实时资源管理中的有效性。

Conclusion: 该论文提出了一种基于大型语言模型（LLMs）的意图驱动网络自动化方法，通过闭环机制动态优化RAN关键参数，显著提升了网络能效，展示了LLM协调的智能系统在实时资源管理中的潜力。

Abstract: Advanced intelligent automation becomes an important feature to deal with the
increased complexity in managing wireless networks. This paper proposes a novel
automation approach of intent-based network for Radio Access Networks (RANs)
management by leveraging Large Language Models (LLMs). The proposed method
enhances intent translation, autonomously interpreting high-level objectives,
reasoning over complex network states, and generating precise configurations of
the RAN by integrating LLMs within an agentic architecture. We propose a
structured prompt engineering technique and demonstrate that the network can
automatically improve its energy efficiency by dynamically optimizing critical
RAN parameters through a closed-loop mechanism. It showcases the potential to
enable robust resource management in RAN by adapting strategies based on
real-time feedback via LLM-orchestrated agentic systems.

</details>


### [298] [Feasibility of Energy Neutral Wildlife Tracking using Multi-Source Energy Harvesting](https://arxiv.org/abs/2507.14234)
*Samer Nasser,Henrique Duarte Moura,Dragan Subotic,Ritesh Kumar Singh,Maarten Weyn,Jeroen Famaey*

Main category: cs.NI

TL;DR: A multi-source energy harvesting system (solar + kinetic) for wildlife tracking, using NB-IoT and an energy-aware scheduler, achieves energy-neutral operation with high data yield and reliability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the energy limitations in wildlife tracking by developing a sustainable, multi-source energy harvesting system that avoids the impracticality and stress of battery replacement in animal tags.

Method: The method involves an energy-neutral system combining solar and kinetic energy harvesting, using NB-IoT for communication, and an energy-aware scheduler to coordinate tasks based on real-time energy availability. A simulation framework models energy harvesting, storage, and consumption at the component level.

Result: Results show the system maintains energy-neutral operation, consistently sampling GPS and kinetic data every two minutes and transmitting hourly via NB-IoT, outperforming single-source systems in data yield and reliability.

Conclusion: The paper concludes that combining solar and kinetic energy harvesting enables maintenance-free, environmentally friendly wildlife tracking, significantly improving data yield and reliability compared to single-source systems.

Abstract: Long-term wildlife tracking is crucial for biodiversity monitoring, but
energy limitations pose challenges, especially for animal tags, where replacing
batteries is impractical and stressful for the animal due to the need to
locate, possibly sedate, and handle it. Energy harvesting offers a sustainable
alternative, yet most existing systems rely on a single energy source and
infrastructure-limited communication technologies. This paper presents an
energy-neutral system that combines solar and kinetic energy harvesting to
enable the tracking and monitoring of wild animals. Harvesting from multiple
sources increases the total available energy. Uniquely, the kinetic harvester
also serves as a motion proxy by sampling harvested current, enabling activity
monitoring without dedicated sensors. Our approach also ensures compatibility
with existing cellular infrastructure, using Narrowband Internet of Things
(NB-IoT). We present a simulation framework that models energy harvesting,
storage, and consumption at the component level. An energy-aware scheduler
coordinates task execution based on real-time energy availability. We evaluate
performance under realistically varying conditions, comparing task frequencies
and capacitor sizes. Results show that our approach maintains energy-neutral
operation while significantly increasing data yield and reliability compared to
single-source systems, with the ability to consistently sample GPS location
data and kinetic harvesting data every two minutes while transmitting these
results over NB-IoT every hour. These findings demonstrate the potential for
maintenance-free, environmentally friendly tracking in remote habitats,
enabling more effective and scalable wildlife monitoring.

</details>


### [299] [Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts](https://arxiv.org/abs/2507.14263)
*Ramesh Raskar,Pradyumna Chari,John Zinky,Mahesh Lambe,Jared James Grogan,Sichao Wang,Rajesh Ranjan,Rekha Singhal,Shailja Gupta,Robert Lincourt,Raghu Bala,Aditi Joshi,Abhishek Singh,Ayush Chopra,Dimitris Stripelis,Bhuwan B,Sumit Kumar,Maria Gorskikh*

Main category: cs.NI

TL;DR: NANDA索引架构为AI代理互联网提供了一种轻量级、可扩展的解决方案，支持快速发现、安全认证和隐私保护，同时兼容现有网络。


<details>
  <summary>Details</summary>
Motivation: 互联网将承载数十亿至数万亿自主AI代理，现有DNS中心的身份和发现机制将面临挑战，需要新的解决方案。

Method: 提出了一种基于动态、可密码验证的AgentFacts的架构，包括CRDT-based更新协议和自适应解析器原型。

Result: NANDA架构实现了五个具体保证：支持第三方代理发现、快速全局解析、秒级撤销和密钥轮换、模式验证能力断言、以及隐私保护发现。

Conclusion: NANDA索引架构为AI代理互联网提供了一个轻量级、水平可扩展的基础，支持安全、信任感知的协作，同时兼容现有网络基础设施。

Abstract: The Internet is poised to host billions to trillions of autonomous AI agents
that negotiate, delegate, and migrate in milliseconds and workloads that will
strain DNS-centred identity and discovery. In this paper, we describe the NANDA
index architecture, which we envision as a means for discoverability,
identifiability and authentication in the internet of AI agents. We present an
architecture where a minimal lean index resolves to dynamic, cryptographically
verifiable AgentFacts that supports multi-endpoint routing, load balancing,
privacy-preserving access, and credentialed capability assertions. Our
architecture design delivers five concrete guarantees: (1) A quilt-like index
proposal that supports both NANDA-native agents as well as third party agents
being discoverable via the index, (2) rapid global resolution for newly spawned
AI agents, (3) sub-second revocation and key rotation, (4) schema-validated
capability assertions, and (5) privacy-preserving discovery across
organisational boundaries via verifiable, least-disclosure queries. We
formalize the AgentFacts schema, specify a CRDT-based update protocol, and
prototype adaptive resolvers. The result is a lightweight, horizontally
scalable foundation that unlocks secure, trust-aware collaboration for the next
generation of the Internet of AI agents, without abandoning existing web
infrastructure.

</details>


### [300] [NetIntent: Leveraging Large Language Models for End-to-End Intent-Based SDN Automation](https://arxiv.org/abs/2507.14398)
*Md. Kamrul Hossain,Walid Aljoby*

Main category: cs.NI

TL;DR: 论文提出IBNBench评估LLM在IBN任务中的表现，并开发NetIntent框架自动化全生命周期IBN，展示了LLM的潜力。


<details>
  <summary>Details</summary>
Motivation: 意图网络（IBN）在简化网络管理方面潜力巨大，但现有解决方案在自动化和灵活性方面存在局限，大型语言模型（LLMs）为解决这一问题提供了新途径。

Method: 引入IBNBench作为首个评估套件，包含四个数据集，用于评估LLM在意图翻译和冲突检测任务中的表现。随后提出NetIntent框架，利用LLM和非LLM代理自动化IBN全生命周期。

Result: IBNBench首次全面比较了33个开源LLM的性能，NetIntent框架在ODL和ONOS SDN控制器上实现了自适应端到端IBN。

Conclusion: 论文提出了IBNBench和NetIntent，展示了大型语言模型在意图网络中的潜力，并提供了一个自动化全生命周期IBN的框架。

Abstract: Intent-Based Networking (IBN) often leverages the programmability of
Software-Defined Networking (SDN) to simplify network management. However,
significant challenges remain in automating the entire pipeline, from
user-specified high-level intents to device-specific low-level configurations.
Existing solutions often rely on rigid, rule-based translators and fixed APIs,
limiting extensibility and adaptability. By contrast, recent advances in large
language models (LLMs) offer a promising pathway that leverages natural
language understanding and flexible reasoning. However, it is unclear to what
extent LLMs can perform IBN tasks. To address this, we introduce IBNBench, a
first-of-its-kind benchmarking suite comprising four novel datasets:
Intent2Flow-ODL, Intent2Flow-ONOS, FlowConflict-ODL, and FlowConflict-ONOS.
These datasets are specifically designed for evaluating LLMs performance in
intent translation and conflict detection tasks within the industry-grade SDN
controllers ODL and ONOS. Our results provide the first comprehensive
comparison of 33 open-source LLMs on IBNBench and related datasets, revealing a
wide range of performance outcomes. However, while these results demonstrate
the potential of LLMs for isolated IBN tasks, integrating LLMs into a fully
autonomous IBN pipeline remains unexplored. Thus, our second contribution is
NetIntent, a unified and adaptable framework that leverages LLMs to automate
the full IBN lifecycle, including translation, activation, and assurance within
SDN systems. NetIntent orchestrates both LLM and non-LLM agents, supporting
dynamic re-prompting and contextual feedback to robustly execute user-defined
intents with minimal human intervention. Our implementation of NetIntent across
both ODL and ONOS SDN controllers achieves a consistent and adaptive end-to-end
IBN realization.

</details>


### [301] [Dora: A Controller Provisioning Strategy in Hierarchical Domain-based Satellite Networks](https://arxiv.org/abs/2507.14512)
*Qiyuan Peng,Qi Zhang,Yue Gao,Kun Qiu*

Main category: cs.NI

TL;DR: 提出Dora，一种基于强化学习的控制器供应策略，显著提升卫星网络管理效率。


<details>
  <summary>Details</summary>
Motivation: 传统扁平网络架构和搜索算法在卫星网络中因计算资源有限和时间严格而效率低下。

Method: 提出了一种基于强化学习的控制器供应策略Dora，以及一个三层基于域的架构。

Result: Dora在控制器供应质量上提升了10%，且计算时间仅为传统算法的1/30至1/90。

Conclusion: 强化学习方法在下一代SAGIN部署中展示了高效卫星网络管理的潜力。

Abstract: The rapid proliferation of satellite constellations in Space-Air-Ground
Integrated Networks (SAGIN) presents significant challenges for network
management. Conventional flat network architectures struggle with
synchronization and data transmission across massive distributed nodes. In
response, hierarchical domain-based satellite network architectures have
emerged as a scalable solution, highlighting the critical importance of
controller provisioning strategies. However, existing network management
architectures and traditional search-based algorithms fail to generate
efficient controller provisioning solutions due to limited computational
resources in satellites and strict time constraints. To address these
challenges, we propose a three-layer domain-based architecture that enhances
both scalability and adaptability. Furthermore, we introduce Dora, a
reinforcement learning-based controller provisioning strategy designed to
optimize network performance while minimizing computational overhead. Our
comprehensive experimental evaluation demonstrates that Dora significantly
outperforms state-of-the-art benchmarks, achieving 10% improvement in
controller provisioning quality while requiring only 1/30 to 1/90 of the
computation time compared to traditional algorithms. These results underscore
the potential of reinforcement learning approaches for efficient satellite
network management in next-generation SAGIN deployments.

</details>


### [302] [UAV-Enabled Wireless-Powered Underground Communication Networks: A Novel Time Allocation Approach](https://arxiv.org/abs/2507.14627)
*Kaiqiang Lin,Yijie Mao,Onel Luis Alcaraz López,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 引入UAV到WPUCNs，提出混合WET方法和时间分配优化，显著降低能耗，实现可持续地下监测。


<details>
  <summary>Details</summary>
Motivation: 解决地下环境中无线信号严重衰减和CSI获取成本高的问题，以实现大规模WPUCNs的经济可行性。

Method: 提出了一种UAV-enabled WPUCN系统，包括能量消耗模型和混合无线能量传输（WET）方法，结合了全CSI和无CSI多天线波束成形技术，并通过时间分配问题优化UAV的能耗。

Result: 仿真结果表明，混合WET方法优于其他WET方法，性能增益受天线数量、通信距离、UD数量和地下条件影响；优化时间分配后，基于无CSI多天线方案的混合WET方法实现了最低的UAV能耗。

Conclusion: 通过优化时间分配和采用基于无CSI多天线方案的混合WET方法，UAV的能耗最低，从而实现了WPUCNs的可持续地下监测。

Abstract: Wireless-powered underground communication networks (WPUCNs), which allow
underground devices (UDs) to harvest energy from wireless signals for
battery-free communication, offer a promising solution for sustainable
underground monitoring. However, the severe wireless signal attenuation in
challenging underground environments and the costly acquisition of channel
state information (CSI) make large-scale WPUCNs economically infeasible in
practice. To address this challenge, we introduce flexible unmanned aerial
vehicles (UAVs) into WPUCNs, leading to UAV-enabled WPUCN systems. In this
system, a UAV is first charged by a terrestrial hybrid access point (HAP), then
flies to the monitoring area to wirelessly charge UDs. Afterwards, the UAV
collects data from the UDs and finally returns to the HAP for data offloading.
Based on the proposed UAV-enabled WPUCN system, we first propose its energy
consumption model and a hybrid wireless energy transfer (WET) approach (i.e.,
UDs can harvest energy from both the HAP and the UAV) relying on full-CSI and
CSI-free multi-antenna beamforming. Then, we formulate and address a time
allocation problem to minimize the energy consumption of UAV, while ensuring
that the throughput requirements of all UDs are met and all sensor data is
offloaded. Through simulations of a realistic farming scenario, we demonstrate
that the proposed hybrid WET approach outperforms other WET approaches, with
performance gains influenced by the number of antennas, communication distance,
number of UDs, and underground conditions. Additionally, under the optimized
time allocation, we found that the proposed hybrid WET approach based on a
CSI-free multi-antenna scheme achieves the lowest UAV's energy consumption
among all WET mechanisms, thereby enabling sustainable underground monitoring
in WPUCNs.

</details>


### [303] [Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks: A Survey on Generative Approaches](https://arxiv.org/abs/2507.14633)
*Xiaozheng Gao,Yichen Wang,Bosen Liu,Xiao Zhou,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Dong In Kim,Abbas Jamalipour,Chau Yuen,Jianping An,Kai Yang*

Main category: cs.NI

TL;DR: 该论文探讨了如何利用生成式AI和大型语言模型（LLMs）来开发智能自主系统，以支持卫星增强低空经济与地面网络（SLAETNs）的发展。


<details>
  <summary>Details</summary>
Motivation: 应对SLAETNs在异构、动态和关键任务环境中可靠运行的挑战。

Method: 通过系统综述五大类生成模型（VAEs、GANs、GDMs、TBMs、LLMs），并分析它们在SLAETNs中的生成机制、能力和部署权衡。

Result: 展示了这些模型在通信增强、安全隐私保护和智能卫星任务三个领域的代理功能。

Conclusion: 提出了未来构建可扩展、自适应和可信赖的生成式代理的关键方向，为下一代集成网络中代理AI的发展提供了统一理解和行动参考。

Abstract: The development of satellite-augmented low-altitude economy and terrestrial
networks (SLAETNs) demands intelligent and autonomous systems that can operate
reliably across heterogeneous, dynamic, and mission-critical environments. To
address these challenges, this survey focuses on enabling agentic artificial
intelligence (AI), that is, artificial agents capable of perceiving, reasoning,
and acting, through generative AI (GAI) and large language models (LLMs). We
begin by introducing the architecture and characteristics of SLAETNs, and
analyzing the challenges that arise in integrating satellite, aerial, and
terrestrial components. Then, we present a model-driven foundation by
systematically reviewing five major categories of generative models:
variational autoencoders (VAEs), generative adversarial networks (GANs),
generative diffusion models (GDMs), transformer-based models (TBMs), and LLMs.
Moreover, we provide a comparative analysis to highlight their generative
mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on
this foundation, we examine how these models empower agentic functions across
three domains: communication enhancement, security and privacy protection, and
intelligent satellite tasks. Finally, we outline key future directions for
building scalable, adaptive, and trustworthy generative agents in SLAETNs. This
survey aims to provide a unified understanding and actionable reference for
advancing agentic AI in next-generation integrated networks.

</details>


### [304] [Data-Plane Telemetry to Mitigate Long-Distance BGP Hijacks](https://arxiv.org/abs/2507.14842)
*Satadal Sengupta,Hyojoon Kim,Daniel Jubas,Maria Apostolaki,Jennifer Rexford*

Main category: cs.NI

TL;DR: 本文提出利用延迟变化检测路由劫持攻击的方法，设计了HiDe系统，验证了其在真实数据中的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 互联网路由安全薄弱，攻击者可能将国内流量重路由至外国，导致隐私泄露和国家安全威胁。现有检测方法主要关注控制平面，而数据平面信号被忽视。

Method: 本文设计了HiDe系统，利用延迟变化作为信号来检测路由劫持攻击。通过在实际部署中测量延迟变化，并验证其准确性和误报率。

Result: 对于全球86%的受害者-攻击者国家组合，攻击期间的延迟至少比攻击前增加25%。HiDe系统能够可靠地检测长距离劫持导致的延迟激增。

Conclusion: 延迟变化作为一种信号，可用于有效地检测路由劫持攻击。HiDe系统通过实时检测延迟激增，证明了这种方法的实用性和可靠性。

Abstract: Poor security of Internet routing enables adversaries to divert user data
through unintended infrastructures (hijack). Of particular concern -- and the
focus of this paper -- are cases where attackers reroute domestic traffic
through foreign countries, exposing it to surveillance, bypassing legal privacy
protections, and posing national security threats. Efforts to detect and
mitigate such attacks have focused primarily on the control plane while
data-plane signals remain largely overlooked. In particular, change in
propagation delay caused by rerouting offers a promising signal: the change is
unavoidable and the increased propagation delay is directly observable from the
affected networks. In this paper, we explore the practicality of using delay
variations for hijack detection, addressing two key questions: (1) What
coverage can this provide, given its heavy dependence on the geolocations of
the sender, receiver, and adversary? and (2) Can an always-on latency-based
detection system be deployed without disrupting normal network operations? We
observe that for 86% of victim-attacker country pairs in the world, mid-attack
delays exceed pre-attack delays by at least 25% in real deployments, making
delay-based hijack detection promising. To demonstrate practicality, we design
HiDe, which reliably detects delay surges from long-distance hijacks at line
rate. We measure HiDe's accuracy and false-positive rate on real-world data and
validate it with ethically conducted hijacks.

</details>


### [305] [Tidal-Like Concept Drift in RIS-Covered Buildings: When Programmable Wireless Environments Meet Human Behaviors](https://arxiv.org/abs/2507.14876)
*Zi-Yang Wu,Muhammad Ismail,Jiliang Zhang,Jie Zhang*

Main category: cs.NI

TL;DR: 本文探讨了将RIS嵌入建筑以优化无线性能，并分析了人类移动性带来的通道动态挑战，提出了可能的解决方案。


<details>
  <summary>Details</summary>
Motivation: 建筑设计历史上未优先考虑无线性能，而RIS在户外网络的成功激发了将其嵌入建筑结构以全面增强无线性能的想法。然而，用户移动性引入了通道的复杂动态，需深入理解人类行为模式。

Method: 文章首次系统地研究了由复杂人类行为驱动的RIS覆盖建筑通道中的潮汐演化现象，并分析了高级深度学习预测和控制策略面临的挑战。

Result: 研究表明，由于人类行为引起的干扰，无法实现通用的通道模型。文章重点分析了高阶马尔可夫依赖、概念漂移和泛化问题等挑战。

Conclusion: 本文提出了一种将可重构智能表面（RIS）嵌入建筑结构的创新方法，以全面增强建筑无线性能。同时，强调了在RIS覆盖的建筑中，由于用户移动性引入的复杂动态，需要深入理解室内人类行为模式，以实现无线友好的建筑设计。文章还探讨了协调RIS覆盖建筑与人群移动性共存的可能解决方案。

Abstract: Indoor mobile networks handle the majority of data traffic, with their
performance limited by building materials and structures. However, building
designs have historically not prioritized wireless performance. Prior to the
advent of reconfigurable intelligent surfaces (RIS), the industry passively
adapted to wireless propagation challenges within buildings. Inspired by RIS's
successes in outdoor networks, we propose embedding RIS into building
structures to manipulate and enhance building wireless performance
comprehensively. Nonetheless, the ubiquitous mobility of users introduces
complex dynamics to the channels of RIS-covered buildings. A deep understanding
of indoor human behavior patterns is essential for achieving wireless-friendly
building design. This article is the first to systematically examine the tidal
evolution phenomena emerging in the channels of RIS-covered buildings driven by
complex human behaviors. We demonstrate that a universal channel model is
unattainable and focus on analyzing the challenges faced by advanced deep
learning-based prediction and control strategies, including high-order Markov
dependencies, concept drift, and generalization issues caused by human-induced
disturbances. Possible solutions for orchestrating the coexistence of
RIS-covered buildings and crowd mobility are also laid out.

</details>


### [306] [FENIX: Enabling In-Network DNN Inference with FPGA-Enhanced Programmable Switches](https://arxiv.org/abs/2507.14891)
*Xiangyu Gao,Tong Li,Yinchao Zhang,Ziqiang Wang,Xiangsheng Zeng,Su Yao,Ke Xu*

Main category: cs.NI

TL;DR: FENIX 是一种混合网络内 ML 系统，结合 ASIC 和 FPGA，解决了现有方案无法兼顾低延迟、高吞吐量和高准确性的问题，性能显著优于 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案（如 FlowLens、N3IC 和 BoS）难以同时实现低延迟、高吞吐量和高准确性，因此需要一种新的混合网络内 ML 系统来解决这些挑战。

Method: FENIX 引入了数据引擎（Data Engine）和模型引擎（Model Engine），分别通过概率令牌桶算法控制特征流的发送速率，以及在资源受限的交换机芯片上部署高精度复杂模型。

Result: FENIX 在真实网络流量数据集上实现了微秒级推理延迟、多太比特吞吐量，硬件开销低，并在主流网络流量分类任务中达到超过 95% 的准确率，优于现有技术。

Conclusion: FENIX 是一种混合网络内机器学习系统，通过在可编程交换机 ASIC 上进行特征提取和在 FPGA 上进行深度神经网络推理，同时实现了低延迟、高吞吐量和高准确性。

Abstract: Machine learning (ML) is increasingly used in network data planes for
advanced traffic analysis. However, existing solutions (such as FlowLens, N3IC,
and BoS) still struggle to simultaneously achieve low latency, high throughput,
and high accuracy. To address these challenges, we present FENIX, a hybrid
in-network ML system that performs feature extraction on programmable switch
ASICs and deep neural network inference on FPGAs. FENIX introduces a Data
Engine that leverages a probabilistic token bucket algorithm to control the
sending rate of feature streams, effectively addressing the throughput gap
between programmable switch ASICs and FPGAs. In addition, FENIX designs a Model
Engine to enable high-accuracy deep neural network inference in the network,
overcoming the difficulty of deploying complex models on resource-constrained
switch chips. We implement FENIX on a programmable switch platform that
integrates a Tofino ASIC and a ZU19EG FPGA directly and evaluate it on
real-world network traffic datasets. Our results show that FENIX achieves
microsecond-level inference latency and multi-terabit throughput with low
hardware overhead, and delivers over 95\% accuracy on mainstream network
traffic classification tasks, outperforming SOTA.

</details>


### [307] [Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness](https://arxiv.org/abs/2507.15145)
*Thai T. Vu,John Le*

Main category: cs.NI

TL;DR: 本文提出了一种通信高效、事件触发的边缘AI推理框架，通过双阈值和联合优化方法，提升了多设备协同下的性能和公平性。


<details>
  <summary>Details</summary>
Motivation: 为了解决多用户设备和边缘服务器协同边缘AI系统中的通信效率、能量消耗和公平性问题，扩展了传统的单设备推理到分布式多设备场景。

Method: 提出了一种基于双阈值早期退出策略的通信高效、事件触发的推理框架，结合比例公平约束，采用联合优化框架，并通过交替优化和Benders分解高效求解。

Result: 实验结果表明，该框架在系统性能和资源分配公平性方面显著优于单设备基准。

Conclusion: 该框架通过联合优化和交替优化方法，显著提升了系统整体性能和资源分配的公平性，优于单设备基准。

Abstract: This paper proposes a communication-efficient, event-triggered inference
framework for cooperative edge AI systems comprising multiple user devices and
edge servers. Building upon dual-threshold early-exit strategies for rare-event
detection, the proposed approach extends classical single-device inference to a
distributed, multi-device setting while incorporating proportional fairness
constraints across users. A joint optimization framework is formulated to
maximize classification utility under communication, energy, and fairness
constraints. To solve the resulting problem efficiently, we exploit the
monotonicity of the utility function with respect to the confidence thresholds
and apply alternating optimization with Benders decomposition. Experimental
results show that the proposed framework significantly enhances system-wide
performance and fairness in resource allocation compared to single-device
baselines.

</details>


### [308] [User Head Movement-Predictive XR in Immersive H2M Collaborations over Future Enterprise Networks](https://arxiv.org/abs/2507.15254)
*Sourav Mondal,Elaine Wong*

Main category: cs.NI

TL;DR: 论文提出了一种基于头部运动预测的动态带宽分配方案（HMC-DBA），以优化XR内容在远程人机协作中的实时同步，显著降低带宽消耗并提升网络效率。


<details>
  <summary>Details</summary>
Motivation: 未来移动系统和固定无线网络的发展需要支持高带宽和低延迟服务，尤其是在工业互联网、扩展现实（XR）和人机协作（H2M）等场景中，如何实时同步XR内容以避免网络延迟带来的不适感是一个挑战。

Method: 提出了一种新颖的人机协作方案，利用双向长短期记忆网络等高度准确的模型预测人类头部运动，并据此动态分配带宽（HMC-DBA）。

Result: 通过广泛的仿真验证，HMC-DBA方案在满足XR帧的端到端延迟和抖动要求的同时，显著降低了带宽消耗，并在网络资源利用效率上优于现有方案。

Conclusion: 论文提出的HMC-DBA方案在满足XR帧的端到端延迟和抖动要求的同时，显著降低了带宽消耗，并在网络资源利用效率上优于现有方案。

Abstract: The evolution towards future generation of mobile systems and fixed wireless
networks is primarily driven by the urgency to support high-bandwidth and
low-latency services across various vertical sectors. This endeavor is fueled
by smartphones as well as technologies like industrial internet of things,
extended reality (XR), and human-to-machine (H2M) collaborations for fostering
industrial and social revolutions like Industry 4.0/5.0 and Society 5.0. To
ensure an ideal immersive experience and avoid cyber-sickness for users in all
the aforementioned usage scenarios, it is typically challenging to synchronize
XR content from a remote machine to a human collaborator according to their
head movements across a large geographic span in real-time over communication
networks. Thus, we propose a novel H2M collaboration scheme where the human's
head movements are predicted ahead with highly accurate models like
bidirectional long short-term memory networks to orient the machine's camera in
advance. We validate that XR frame size varies in accordance with the human's
head movements and predict the corresponding bandwidth requirements from the
machine's camera to propose a human-machine coordinated dynamic bandwidth
allocation (HMC-DBA) scheme. Through extensive simulations, we show that
end-to-end latency and jitter requirements of XR frames are satisfied with much
lower bandwidth consumption over enterprise networks like
Fiber-To-The-Room-Business. Furthermore, we show that better efficiency in
network resource utilization is achieved by employing our proposed HMC-DBA over
state-of-the-art schemes.

</details>


### [309] [Low-Power and Accurate IoT Monitoring Under Radio Resource Constraint](https://arxiv.org/abs/2507.15338)
*Takaho Shimokasa,Hiroyuki Yomo,Federico Chiariotti,Junya Shiraishi,Petar Popovski*

Main category: cs.NI

TL;DR: 论文研究了在资源受限的物联网中，通过唤醒机制和分散式策略优化传感器节点的能效和状态估计准确性，发现分散式策略在低相关性下表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究如何在无线资源受限的物联网监测中实现传感器节点的低功耗运行和基于卡尔曼滤波的准确状态估计。

Method: 引入了两种策略：基于统计的无感知策略和基于瞬时观测的分散式策略，并采用唤醒接收器和信号提升能效。分散式策略通过随机访问优先传输可能改善状态估计的观测。

Result: 数值结果表明，在传感器节点间相关性较低时，分散式策略在无线电资源和能耗约束下比无感知策略更准确。

Conclusion: 在无线资源受限的情况下，分散式策略在传感器节点间相关性较低时能提高状态估计的准确性，并明确了两种策略优势转换的相关性程度。

Abstract: This paper investigates how to achieve both low-power operations of sensor
nodes and accurate state estimation using Kalman filter for internet of things
(IoT) monitoring employing wireless sensor networks under radio resource
constraint. We consider two policies used by the base station to collect
observations from the sensor nodes: (i) an oblivious policy, based on
statistics of the observations, and (ii) a decentralized policy, based on
autonomous decision of each sensor based on its instantaneous observation. This
work introduces a wake-up receiver and wake-up signaling to both policies to
improve the energy efficiency of the sensor nodes. The decentralized policy
designed with random access prioritizes transmissions of instantaneous
observations that are highly likely to contribute to the improvement of state
estimation. Our numerical results show that the decentralized policy improves
the accuracy of the estimation in comparison to the oblivious policy under the
constraint on the radio resource and consumed energy when the correlation
between the processes observed by the sensor nodes is low. We also clarify the
degree of correlation in which the superiority of two policies changes.

</details>


### [310] [Enhancements to P4TG: Histogram-Based RTT Monitoring in the Data Plane](https://arxiv.org/abs/2507.15382)
*Fabian Ihle,Etienne Zink,Michael Menth*

Main category: cs.NI

TL;DR: P4TG提出直方图RTT测量方法，通过范围到前缀转换优化TCAM匹配，实现高精度分析。


<details>
  <summary>Details</summary>
Motivation: P4TG在数据平面采样时间指标（如RTT）并在控制器收集，导致精度降低，需要一种无需采样的高精度分析方法。

Method: 采用直方图方法进行RTT测量，使用范围到前缀的转换算法来优化TCAM中的范围匹配规则。

Result: 评估表明，基于直方图的RTT分析方法能够准确测量RTT，并与理论分布一致。

Conclusion: 本文提出了一种基于直方图的RTT测量方法，通过范围到前缀的转换算法在P4TG中实现了高精度的RTT分析，验证了该方法的有效性。

Abstract: Modern traffic generators are essential tools for evaluating the performance
of network environments. P4TG is a P4-based traffic generator implemented for
Intel Tofino switches that offers high-speed packet generation with
fine-grained measurement capabilities. However, P4TG samples time-based metrics
such as the round-trip time (RTT) in the data plane and collects them at the
controller. This leads to a reduced accuracy. In this paper, we introduce a
histogram-based RTT measurement feature for P4TG. It enables accurate analysis
at line rate without sampling. Generally, histogram bins are modeled as ranges,
and values are matched to a bin. Efficient packet matching in hardware is
typically achieved using ternary content addressable memory (TCAM). However,
representing range matching rules in TCAM poses a challenge. Therefore, we
implemented a range-to-prefix conversion algorithm that models range matching
with multiple ternary entries. This paper describes the data plane
implementation and runtime configuration of RTT histograms in P4TG. Further, we
discuss the efficiency of the ternary decomposition. Our evaluation
demonstrates the applicability of the histogram-based RTT analysis by comparing
the measured values with a configured theoretical distribution of RTTs.

</details>


### [311] [Stack Management for MPLS Network Actions: Integration of Nodes with Limited Hardware Capabilities](https://arxiv.org/abs/2507.15391)
*Fabian Ihle,Michael Menth*

Main category: cs.NI

TL;DR: 本文分析了MNA实现中RLD需求大的原因，提出了一种通过重构MPLS堆栈降低RLD的机制，并在可编程硬件上验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: MPLS网络动作（MNA）框架通过LSEs扩展MPLS转发功能，但路由器对可读标签深度（RLD）有物理限制，MNA要求较大的RLD。本文旨在解决这一问题。

Method: 对MNA实现进行硬件分析，识别RLD需求大的原因，并提出通过转发时重构MPLS堆栈的机制。引入堆栈管理网络动作并展示其与不支持MNA节点的网络集成。

Result: 提出的机制在可编程硬件上实现，验证了其可行性，并讨论了其对RLD、ECMP和数据包开销的影响。

Conclusion: 通过重新构建MPLS堆栈和引入堆栈管理网络动作，该机制成功降低了MNA节点所需的RLD，并在可编程硬件上验证了其可行性。

Abstract: The MPLS Network Actions (MNA) framework enhances MPLS forwarding with a
generalized encoding for manifold extensions such as network slicing and
in-situ OAM (IOAM). Network actions in MNA are encoded in Label Stack Entries
(LSEs) and are added to the MPLS stack. Routers have a physical limit on the
number of LSEs they can read, called the readable label depth (RLD). With MNA,
routers must be able to process a minimum number of LSEs which requires a
relatively large RLD. In this paper, we perform a hardware analysis of an MNA
implementation and identify the reason for a large RLD requirement in the MNA
protocol design. Based on this, we present a mechanism that reduces the
required RLD for MNA nodes by restructuring the MPLS stack during forwarding.
We then introduce the novel stack management network action that enables the
proposed mechanism as well as its integration in networks with MNA-incapable
nodes. The feasibility of the mechanism on programmable hardware is verified by
providing a P4-based implementation. Further, the effects on the required RLD,
ECMP, and packet overhead are discussed.

</details>


### [312] [Assessing the Benefits of Ground Vehicles as Moving Urban Base Stations](https://arxiv.org/abs/2507.15423)
*Laura Finarelli,Falko Dressler,Marco Ajmone Marsan,Gianluca Rizzo*

Main category: cs.NI

TL;DR: 研究移动网络（MN）范式在6G HetNet中的效益，通过随机几何框架和优化问题，证明MN能减少基础设施部署并保证服务质量。


<details>
  <summary>Details</summary>
Motivation: 研究在6G以用户为中心的网络演进中，移动网络（MN）范式如何在动态、灵活和可持续的网络运营中发挥作用，并明确其优势超过额外资源成本的条件。

Method: 提出了一个随机几何框架，用于描述MN范式在HetNet中的潜在效益，并制定了优化问题以确定资源最优的网络配置和基站调度。

Result: 数值评估表明，MN范式能显著减少部署的网络基础设施，同时保证用户感知的目标服务质量。

Conclusion: MN范式与适当的动态网络管理策略结合，能显著减少部署的网络基础设施，同时保证用户感知的目标服务质量。

Abstract: In the evolution towards 6G user-centric networking, the moving network (MN)
paradigm can play an important role. In a MN, some small cell base stations
(BS) are installed on top of vehicles, and enable a more dynamic, flexible and
sustainable, network operation. By "following" the users movements and adapting
dynamically to their requests, the MN paradigm enables a more efficient
utilization of network resources, mitigating the need for dense small cell BS
deployments at the cost of an increase in resource utilization due to wireless
backhauling. This aspect is at least partly compensated by the shorter distance
between users and BS, which allows for lower power and Line-of-Sight
communications. While the MN paradigm has been investigated for some time, to
date, it is still unclear in which conditions the advantages of MN outweigh the
additional resource costs. In this paper, we propose a stochastic geometry
framework for the characterization of the potential benefits of the MN paradigm
as part of an HetNet in urban settings. Our approach allows the estimation of
user-perceived performance, accounting for wireless backhaul connectivity as
well as base station resource scheduling. We formulate an optimization problem
for determining the resource-optimal network configurations and BS scheduling
which minimize the overall amount of deployed BSs in a QoS-aware manner, and
the minimum vehicular flow between different urban districts required to
support them, and we propose an efficient stochastic heuristic to solve it. Our
numerical assessment suggests that the MN paradigm, coupled with appropriate
dynamic network management strategies, significantly reduces the amount of
deployed network infrastructure while guaranteeing the target QoS perceived by
users.

</details>


### [313] [SENSOR: A Cost-Efficient Open-Source Flow Monitoring Platform](https://arxiv.org/abs/2507.15659)
*Gabriel Paradzik,Benjamin Steinert,Heinrich Abele,Michael Menth*

Main category: cs.NI

TL;DR: 开源工具构建的分布式流监控平台，低成本且高效。


<details>
  <summary>Details</summary>
Motivation: 解决传统流监控平台成本高且采样数据不完整的问题。

Method: 通过开源工具构建分布式流监控平台，详细介绍了工具的使用方法。

Result: 成功在蒂宾根大学实施该平台，验证了其可行性和成本效益。

Conclusion: 该论文提出了一种经济高效的分布式流监控平台，专用于收集未采样的IPFIX数据，并完全基于开源工具实现。

Abstract: This paper presents a cost-effective and distributed flow monitoring platform
for collecting unsampled IPFIX data exclusively using open-source tools, which
is implemented at the University of T\"ubingen. An overview of all tools is
given and their use is explained.

</details>


### [314] [Vehicular Cloud Computing: A cost-effective alternative to Edge Computing in 5G networks](https://arxiv.org/abs/2507.15670)
*Rosario Patanè,Nadjib Achir,Andrea Araldo,Lila Boukhatem*

Main category: cs.NI

TL;DR: VCC可替代EC用于低延迟应用，极端情况下例外。


<details>
  <summary>Details</summary>
Motivation: 探索VCC是否能在不依赖EC的情况下有效支持低延迟应用，为网络运营商提供潜在的成本节约方案。

Method: 通过广泛的模拟评估关键场景因素，包括负载、车辆移动性和密度以及可用性，使用SUMO模拟车辆移动性，NS3 5G-LENA模拟通信。

Result: 研究发现VCC在大多数情况下可以替代EC，仅在极端低延迟需求时仍需EC。

Conclusion: VCC可以有效地替代EC用于低延迟应用，但在极端情况下（延迟<16毫秒）仍需依赖EC。

Abstract: Edge Computing (EC) is a computational paradigm that involves deploying
resources such as CPUs and GPUs near end-users, enabling low-latency
applications like augmented reality and real-time gaming. However, deploying
and maintaining a vast network of EC nodes is costly, which can explain its
limited deployment today. A new paradigm called Vehicular Cloud Computing (VCC)
has emerged and inspired interest among researchers and industry. VCC
opportunistically utilizes existing and idle vehicular computational resources
for external task offloading. This work is the first to systematically address
the following question: Can VCC replace EC for low-latency applications?
Answering this question is highly relevant for Network Operators (NOs), as VCC
could eliminate costs associated with EC given that it requires no
infrastructural investment. Despite its potential, no systematic study has yet
explored the conditions under which VCC can effectively support low-latency
applications without relying on EC. This work aims to fill that gap. Extensive
simulations allow for assessing the crucial scenario factors that determine
when this EC-to-VCC substitution is feasible. Considered factors are load,
vehicles mobility and density, and availability. Potential for substitution is
assessed based on multiple criteria, such as latency, task completion success,
and cost. Vehicle mobility is simulated in SUMO, and communication in NS3
5G-LENA. The findings show that VCC can effectively replace EC for low-latency
applications, except in extreme cases when the EC is still required (latency <
16 ms).

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [315] [FAMST: Fast Approximate Minimum Spanning Tree Construction for Large-Scale and High-Dimensional Data](https://arxiv.org/abs/2507.14261)
*Mahmood K. M. Almansoori,Miklos Telek*

Main category: cs.DS

TL;DR: FAMST是一种高效的最小生成树近似算法，通过三阶段方法显著提升计算效率，适用于大规模高维数据。


<details>
  <summary>Details</summary>
Motivation: 解决大规模和高维数据集上构建最小生成树的计算挑战，扩展MST技术的应用范围。

Method: FAMST采用三阶段方法：近似最近邻图构建、ANN组件间连接和迭代边细化。

Result: 实验表明，FAMST在多种数据集上实现了极低的近似误差，速度提升高达1000倍，适用于百万级点和数千维的数据集。

Conclusion: FAMST算法显著提高了在大规模和高维数据集上构建最小生成树的效率，将传统方法的O(n^2)时间和空间复杂度降低至O(dn log n)和O(dn + kn)，并保持了较低的近似误差。

Abstract: We present Fast Approximate Minimum Spanning Tree (FAMST), a novel algorithm
that addresses the computational challenges of constructing Minimum Spanning
Trees (MSTs) for large-scale and high-dimensional datasets. FAMST utilizes a
three-phase approach: Approximate Nearest Neighbor (ANN) graph construction,
ANN inter-component connection, and iterative edge refinement. For a dataset of
$n$ points in a $d$-dimensional space, FAMST achieves $\mathcal{O}(dn \log n)$
time complexity and $\mathcal{O}(dn + kn)$ space complexity when $k$ nearest
neighbors are considered, which is a significant improvement over the
$\mathcal{O}(n^2)$ time and space complexity of traditional methods.
  Experiments across diverse datasets demonstrate that FAMST achieves
remarkably low approximation errors while providing speedups of up to
1000$\times$ compared to exact MST algorithms. We analyze how the key
hyperparameters, $k$ (neighborhood size) and $\lambda$ (inter-component edges),
affect performance, providing practical guidelines for hyperparameter
selection. FAMST enables MST-based analysis on datasets with millions of points
and thousands of dimensions, extending the applicability of MST techniques to
problem scales previously considered infeasible.

</details>


### [316] [Tighter Lower Bounds for Single Source Personalized PageRank](https://arxiv.org/abs/2507.14462)
*Xinpeng Jiang,Haoyu Liu,Siqiang Luo,Xiaokui Xiao*

Main category: cs.DS

TL;DR: 本文填补了SSPPR查询下界研究的空白，针对相对和加性误差分别提出了更严格的下界。


<details>
  <summary>Details</summary>
Motivation: 现有研究中对SSPPR查询的下界较为宽松，无法准确反映问题的复杂性，因此需要更精确的下界分析。

Method: 通过理论分析，针对相对误差（SSPPR-R）和加性误差（SSPPR-A）两种情况，分别建立了更紧的下界。

Result: 对于SSPPR-R，证明了$\Omega\left(\min\left(m, \frac{\log(1/\delta)}{\delta}\right)\right)$的下界；对于SSPPR-A，证明了$\Omega\left(\min\left(m, \frac{\log(1/\epsilon)}{\epsilon}\right)\right)$的下界。

Conclusion: 本文为单源个性化PageRank（SSPPR）查询的近似问题提供了更严格的下界，填补了现有研究的空白。

Abstract: We study lower bounds for approximating the Single Source Personalized
PageRank (SSPPR) query, which measures the probability distribution of an
$\alpha$-decay random walk starting from a source node $s$. Existing lower
bounds remain loose-$\Omega\left(\min(m, 1/\delta)\right)$ for relative error
(SSPPR-R) and $\Omega\left(\min(n, 1/\epsilon)\right)$ for additive error
(SSPPR-A). To close this gap, we establish tighter bounds for both settings.
For SSPPR-R, we show a lower bound of $\Omega\left(\min\left(m,
\frac{\log(1/\delta)}{\delta}\right)\right)$ for any $\delta \in (0,1)$. For
SSPPR-A, we prove a lower bound of $\Omega\left(\min\left(m,
\frac{\log(1/\epsilon)}{\epsilon}\right)\right)$ for any $\epsilon \in (0,1)$,
assuming the graph has $m \in \mathcal{O}(n^{2-\beta})$ edges for any
arbitrarily small constant $\beta \in (0,1)$.

</details>


### [317] [New Algorithms for #2-SAT and #3-SAT](https://arxiv.org/abs/2507.14504)
*Junqiang Peng,Zimo Sheng,Mingyu Xiao*

Main category: cs.DS

TL;DR: 本文提出了一种更高效的算法，用于解决加权#2-SAT和#3-SAT问题，显著提升了时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决加权版本的#2-SAT和#3-SAT问题，并改进之前的结果。

Method: 引入了新颖的归约规则、对分支操作的精细分析，以及在公式的原始图和双图上应用路径分解。

Result: 加权#2-SAT和#3-SAT的时间复杂度分别为$\mathcal{O}^*(1.1082^m)$和$\mathcal{O}^*(1.4423^m)$。

Conclusion: 本文展示了加权版本的#2-SAT和#3-SAT问题可以在$\mathcal{O}^*(1.1082^m)$和$\mathcal{O}^*(1.4423^m)$时间内解决，这些结果直接适用于非加权情况，并显著改进了之前的结果。

Abstract: The #2-SAT and #3-SAT problems involve counting the number of satisfying
assignments (also called models) for instances of 2-SAT and 3-SAT,
respectively. In 2010, Zhou et al. proposed an $\mathcal{O}^*(1.1892^m)$-time
algorithm for #2-SAT and an efficient approach for #3-SAT, where $m$ denotes
the number of clauses. In this paper, we show that the weighted versions of
#2-SAT and #3-SAT can be solved in $\mathcal{O}^*(1.1082^m)$ and
$\mathcal{O}^*(1.4423^m)$ time, respectively. These results directly apply to
the unweighted cases and achieve substantial improvements over the previous
results. These advancements are enabled by the introduction of novel reduction
rules, a refined analysis of branching operations, and the application of path
decompositions on the primal and dual graphs of the formula.

</details>


### [318] [Addressing Bias in Algorithmic Solutions: Exploring Vertex Cover and Feedback Vertex Set](https://arxiv.org/abs/2507.14509)
*Sheikh Shakil Akhtar,Jayakrishnan Madathil,Pranabendu Misra,Geevarghese Philip*

Main category: cs.DS

TL;DR: 本研究开发了一种组合优化算法，旨在生成对特定子群体公平的解，弥补传统方法忽略社会公平性的缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统的组合优化算法在提取数学抽象时丢弃了大量“边缘信息”，但这些信息可能对某些子群体至关重要。为了确保算法输出在现实世界中的公平性，需要开发能够考虑这些信息的方法。

Method: 通过提取和整合传统优化过程中被忽略的“边缘信息”，设计了一种新的算法框架，以确保解在不同子群体间的公平分配。

Result: 提出了一种能够生成对指定子群体无偏解的新算法，这些解在保持成本效益的同时，更符合社会公平性原则。

Conclusion: 本研究提出了一种在组合优化问题中寻找对特定子群体“无偏”解的方法，强调了在算法设计中考虑社会公平性的重要性。

Abstract: A typical goal of research in combinatorial optimization is to come up with
fast algorithms that find optimal solutions to a computational problem. The
process that takes a real-world problem and extracts a clean mathematical
abstraction of it often throws out a lot of "side information" which is deemed
irrelevant. However, the discarded information could be of real significance to
the end-user of the algorithm's output. All solutions of the same cost are not
necessarily of equal impact in the real-world; some solutions may be much more
desirable than others, even at the expense of additional increase in cost. If
the impact, positive or negative, is mostly felt by some specific (minority)
subgroups of the population, the population at large will be largely unaware of
it. In this work we ask the question of finding solutions to combinatorial
optimization problems that are "unbiased" with respect to a collection of
specified subgroups of the total population.

</details>


### [319] [Characterizing and Testing Configuration Stability in Two-Dimensional Threshold Cellular Automata](https://arxiv.org/abs/2507.14569)
*Yonatan Nakar,Dana Ron*

Main category: cs.DS

TL;DR: 论文分析了二维环面上基于阈值规则的细胞自动机稳定配置的结构，并设计了一个高效的测试算法，查询复杂度独立于配置大小，仅与1/ϵ平方相关。


<details>
  <summary>Details</summary>
Motivation: 研究二维环面上基于阈值规则的细胞自动机配置的稳定性和测试问题，特别是针对Threshold-2和Threshold-3规则，因为它们在行为上比Threshold-1和Threshold-5更复杂多样。

Method: 论文首先分析了Threshold-2（类似Threshold-4）和Threshold-3（多数）规则下的稳定配置结构，随后设计了一个测试算法来区分稳定配置与ϵ-远离稳定的配置。

Result: 论文不仅对稳定配置进行了结构分析，还提出了一个高效的测试算法，其查询复杂度与配置大小无关，仅依赖于1/ϵ的平方。

Conclusion: 论文首先对二维环面上基于冯·诺依曼邻域的阈值规则的稳定配置进行了结构分析，然后设计并分析了一个测试算法，能够区分稳定配置与远离稳定的配置，且算法的查询复杂度与配置大小无关，仅与1/ϵ的平方相关。

Abstract: We consider the problems of characterizing and testing the stability of
cellular automata configurations that evolve on a two-dimensional torus
according to threshold rules with respect to the von-Neumann neighborhood.
While stable configurations for Threshold-1 (OR) and Threshold-5 (AND) are
trivial (and hence easily testable), the other threshold rules exhibit much
more diverse behaviors. We first characterize the structure of stable
configurations with respect to the Threshold-2 (similarly, Threshold-4) and
Threshold-3 (Majority) rules. We then design and analyze a testing algorithm
that distinguishes between configurations that are stable with respect to the
Threshold-2 rule, and those that are $\epsilon$-far from any stable
configuration, where the query complexity of the algorithm is independent of
the size of the configuration and depends quadratically on $1/\epsilon$.

</details>


### [320] [A Black-Box Approach for Exogenous Replenishment in Online Resource Allocation](https://arxiv.org/abs/2507.14812)
*Suho Kang,Ziyang Liu,Rajan Udwani*

Main category: cs.DS

TL;DR: 该论文提出了一种黑盒方法，将不考虑资源补充的现有算法扩展到能处理任意补充过程的场景，保持原始算法的竞争比。


<details>
  <summary>Details</summary>
Motivation: 在线资源分配问题中，资源库存随时间补充的过程通常是未知的，现有算法未考虑这一因素。

Method: 引入了黑盒方法，将任何不考虑资源补充的现有算法扩展为能够处理任意（对抗性或随机性）补充过程的算法。

Result: 在初始库存较大的情况下，该方法能够保持原始算法的竞争比，适用于对抗性和随机性到达模型。

Conclusion: 该方法能够将现有算法无缝扩展到考虑资源补充的场景，同时保持原始算法的竞争比。

Abstract: In a typical online resource allocation problem, we start with a fixed
inventory of resources and make online allocation decisions in response to
resource requests that arrive sequentially over a finite horizon. We consider
settings where the inventory is replenished over time according to an unknown
exogenous process. We introduce black-box methods that extend any existing
algorithm, originally designed without considering replenishment, into one that
works with an arbitrary (adversarial or stochastic) replenishment process. Our
approach preserves the original algorithm's competitive ratio in regimes with
large initial inventory, thereby enabling the seamless integration of exogenous
replenishment into a large body of existing algorithmic results for both
adversarial and stochastic arrival models.

</details>


### [321] [Differentially Private Synthetic Graphs Preserving Triangle-Motif Cuts](https://arxiv.org/abs/2507.14835)
*Pan Peng,Hangyu Xu*

Main category: cs.DS

TL;DR: 首个差分隐私合成图生成方法，近似保留三角形主题大小，误差有上下界。


<details>
  <summary>Details</summary>
Motivation: 研究如何在保持差分隐私的前提下，生成近似保留复杂网络关键特征的合成图，以支持图聚类、稀疏化和社会网络分析等应用。

Method: 通过差分隐私机制生成合成图$G'$，近似输入图$G$的所有割的三角形主题大小，误差为$\tilde{O}(\sqrt{m\ell_{3}(G)}n/\varepsilon^{3/2})$。

Result: 提出的算法在多项式时间内生成合成图$G'$，误差上界为$\tilde{O}(\sqrt{m\ell_{3}(G)}n/\varepsilon^{3/2})$，并证明了误差下界为$\Omega(\sqrt{mn}\ell_{3}(G)/\varepsilon)$。

Conclusion: 该论文提出了第一个多项式时间的$\varepsilon,\delta$-差分隐私机制，用于生成近似保留输入图所有割的三角形主题大小的合成图，并提供了误差下界。

Abstract: We study the problem of releasing a differentially private (DP) synthetic
graph $G'$ that well approximates the triangle-motif sizes of all cuts of any
given graph $G$, where a motif in general refers to a frequently occurring
subgraph within complex networks. Non-private versions of such graphs have
found applications in diverse fields such as graph clustering, graph
sparsification, and social network analysis. Specifically, we present the first
$(\varepsilon,\delta)$-DP mechanism that, given an input graph $G$ with $n$
vertices, $m$ edges and local sensitivity of triangles $\ell_{3}(G)$, generates
a synthetic graph $G'$ in polynomial time, approximating the triangle-motif
sizes of all cuts $(S,V\setminus S)$ of the input graph $G$ up to an additive
error of $\tilde{O}(\sqrt{m\ell_{3}(G)}n/\varepsilon^{3/2})$. Additionally, we
provide a lower bound of $\Omega(\sqrt{mn}\ell_{3}(G)/\varepsilon)$ on the
additive error for any DP algorithm that answers the triangle-motif size
queries of all $(S,T)$-cut of $G$. Finally, our algorithm generalizes to
weighted graphs, and our lower bound extends to any $K_h$-motif cut for any
constant $h\geq 2$.

</details>


### [322] [Predict, Reposition, and Allocate: A Greedy and Flow-Based Architecture for Sustainable Urban Food Delivery](https://arxiv.org/abs/2507.15282)
*Aqsa Ashraf Makhdomi,Iqra Altaf Gillani*

Main category: cs.DS

TL;DR: 研究提出了一种环保食品配送优化框架，通过贪婪算法和网络流模型减少车辆使用，降低排放。


<details>
  <summary>Details</summary>
Motivation: 现有的优化机制未将环境可持续性纳入优化目标，导致次优结果，而食品配送平台的快速扩张加剧了城市环境退化。

Method: 利用目标函数的子模性和单调性设计了一种高效的贪婪优化算法，并将订单分配问题表述为网络流优化模型，设计了三层网络架构以满足容量约束和空间需求。

Result: 该框架减少了车辆数量，降低了温室气体排放，同时保持了服务效率。

Conclusion: 该研究提出的环保食品配送优化框架通过整合需求预测、配送员路径规划和订单分配，有效减少了车辆数量，建立了可持续的食品配送生态系统。

Abstract: The rapid proliferation of food delivery platforms has reshaped urban
mobility but has also contributed significantly to environmental degradation
through increased greenhouse gas emissions. Existing optimization mechanisms
produce sub-optimal outcomes as they do not consider environmental
sustainability their optimization objective. This study proposes a novel
eco-friendly food delivery optimization framework that integrates demand
prediction, delivery person routing, and order allocation to minimize
environmental impact while maintaining service efficiency. Since recommending
routes is NP-Hard, the proposed approach utilizes the submodular and monotone
properties of the objective function and designs an efficient greedy
optimization algorithm. Thereafter, it formulates order allocation problem as a
network flow optimization model, which, to the best of our knowledge, has not
been explored in the context of food delivery. A three-layered network
architecture is designed to match orders with delivery personnel based on
capacity constraints and spatial demand. Through this framework, the proposed
approach reduces the vehicle count, and creates a sustainable food delivery
ecosystem.

</details>


### [323] [Language Generation in the Limit: Noise, Loss, and Feedback](https://arxiv.org/abs/2507.15319)
*Yannan Bai,Debmalya Panigrahi,Ian Zhang*

Main category: cs.DS

TL;DR: 本文解决了语言生成极限中的并集封闭性问题，并扩展研究了噪声、无样本及反馈生成的变体，展示了其等价性和分离性。


<details>
  <summary>Details</summary>
Motivation: 研究语言生成极限问题中的并集封闭性，并探索噪声、丢失和反馈对生成能力的影响。

Method: 通过构造一个可统一生成的集合和一个非统一生成的集合，证明它们的并集在极限内不可生成。进一步研究了噪声生成、无样本生成及反馈生成等变体，并展示了这些模型的等价性及分离性。

Result: 证明了并集在极限内不可生成，展示了噪声与非噪声生成模型的分离性，并指出有限查询无增强能力而无限查询则严格增强模型能力。

Conclusion: 本文总结了语言生成极限问题中的并集封闭性，并利用相关技术（及其他方法）对包含噪声、丢失和反馈的自然变体进行了精确刻画。

Abstract: Kleinberg and Mullainathan (2024) recently proposed a formal framework called
language generation in the limit and showed that given a sequence of example
strings from an unknown target language drawn from any countable collection, an
algorithm can correctly generate unseen strings from the target language within
finite time. This notion was further refined by Li, Raman, and Tewari (2024),
who defined stricter categories of non-uniform and uniform generation. They
showed that a finite union of uniformly generatable collections is generatable
in the limit, and asked if the same is true for non-uniform generation.
  We begin by resolving the question in the negative: we give a uniformly
generatable collection and a non-uniformly generatable collection whose union
is not generatable in the limit. We then use facets of this construction to
further our understanding of several variants of language generation. The first
two, generation with noise and without samples, were introduced by Raman and
Raman (2025) and Li, Raman, and Tewari (2024) respectively. We show the
equivalence of these models for uniform and non-uniform generation, and provide
a characterization of non-uniform noisy generation. The former paper asked if
there is any separation between noisy and non-noisy generation in the limit --
we show that such a separation exists even with a single noisy string. Finally,
we study the framework of generation with feedback, introduced by Charikar and
Pabbaraju (2025), where the algorithm is strengthened by allowing it to ask
membership queries. We show finite queries add no power, but infinite queries
yield a strictly more powerful model.
  In summary, the results in this paper resolve the union-closedness of
language generation in the limit, and leverage those techniques (and others) to
give precise characterizations for natural variants that incorporate noise,
loss, and feedback.

</details>


### [324] [1.64-Approximation for Chromatic Correlation Clustering via Chromatic Cluster LP](https://arxiv.org/abs/2507.15417)
*Dahoon Lee,Chenglin Fan,Euiwoong Lee*

Main category: cs.DS

TL;DR: 论文提出了一种1.64-近似算法用于色相关聚类问题，通过色聚类LP松弛和混合舍入策略，显著提升了近似比并展示了LP框架的潜力。


<details>
  <summary>Details</summary>
Motivation: 色相关聚类（CCC）通过赋予边多重分类关系（颜色）并在聚类上施加色约束，比传统相关聚类仅处理二元关系（+/-）能捕捉更丰富的结构关系。然而，由于标准LP松弛的限制，提升CCC的近似比一直存在困难。

Method: 论文提出了一种随机1.64-近似算法，扩展了聚类LP框架至色相关聚类问题，引入了色聚类LP松弛和结合聚类与贪婪策略的舍入算法。

Result: 提出的算法显著将近似比从之前的2.15提升至1.64，绕过了标准LP在CCC问题上的积分间隙2。

Conclusion: 该论文通过引入色聚类LP松弛和结合聚类与贪婪策略的舍入算法，显著提升了色相关聚类问题的近似比，展示了色聚类LP框架在解决其他聚类变体问题中的潜力。

Abstract: Chromatic Correlation Clustering (CCC) generalizes Correlation Clustering by
assigning multiple categorical relationships (colors) to edges and imposing
chromatic constraints on the clusters. Unlike traditional Correlation
Clustering, which only deals with binary $(+/-)$ relationships, CCC captures
richer relational structures. Despite its importance, improving the
approximation for CCC has been difficult due to the limitations of standard LP
relaxations. We present a randomized $1.64$-approximation algorithm to the CCC
problem, significantly improving the previous factor of $2.15$. Our approach
extends the cluster LP framework to the chromatic setting by introducing a
chromatic cluster LP relaxation and an rounding algorithm that utilizes both a
cluster-based and a greedy pivot-based strategy. The analysis bypasses the
integrality gap of $2$ for the CCC version of standard LP and highlights the
potential of the cluster LP framework to address other variants of clustering
problems.

</details>


### [325] [Asynchronous Collective Tree Exploration: a Distributed Algorithm, and a new Lower Bound](https://arxiv.org/abs/2507.15658)
*Romain Cosson,Laurent Massoulié*

Main category: cs.DS

TL;DR: 论文提出了一种分布式异步集体树探索算法，性能接近最优，并改进了竞争比下界。


<details>
  <summary>Details</summary>
Motivation: 研究集体树探索问题，解决现有方法要么是分布式的但同步的，要么是异步的但集中式的局限性。

Method: 论文提出了一种分布式异步算法，通过代理在树的节点上读写白板进行通信，探索未知树结构。算法的时间复杂度为2n+O(k^2 2^kD)或O(k/log k)(n+kD)。

Result: 算法在树探索中的移动次数为2n+O(k^2 2^kD)或O(k/log k)(n+kD)，竞争比为O(k/log k)，并且提出了新的竞争比下界Ω(log²k)。

Conclusion: 该论文提出了一种分布式异步算法，用于集体树探索问题，其性能在平均情况下是最优的，并且提出了一个新的竞争比下界，改进了之前的结果。

Abstract: We study the problem of collective tree exploration in which a team of $k$
mobile agents must collectively visit all nodes of an unknown tree in as few
moves as possible. The agents all start from the root and discover adjacent
edges as they progress in the tree. Communication is distributed in the sense
that agents share information by reading and writing on whiteboards located at
all nodes. Movements are asynchronous, in the sense that the speeds of all
agents are controlled by an adversary at all times. All previous competitive
guarantees for collective tree exploration are either distributed but
synchronous, or asynchronous but centralized. In contrast, we present a
distributed asynchronous algorithm that explores any tree of $n$ nodes and
depth $D$ in at most $2n+O(k^2 2^kD)$ moves, i.e., with a regret that is linear
in $D$, and a variant algorithm with a guarantee in $O(k/\log k)(n+kD)$, i.e.,
with a competitive ratio in $O(k/\log k)$. We note that our regret guarantee is
asymptotically optimal (i.e., $1$-competitive) from the perspective of
average-case complexity. We then present a new general lower bound on the
competitive ratio of asynchronous collective tree exploration, in
$\Omega(\log^2 k)$. This lower bound applies to both the distributed and
centralized settings, and improves upon the previous lower bound in
$\Omega(\log k)$.

</details>


### [326] [Job Scheduling under Base and Additional Fees, with Applications to Mixed-Criticality Scheduling](https://arxiv.org/abs/2507.15434)
*Yi-Ting Hsieh,Mong-Jen Kao,Jhong-Yun Liu,Hung-Lung Wang*

Main category: cs.DS

TL;DR: 论文研究了作业调度问题，证明FFD算法能达到1.5近似比，并提出了PTAS。该方法还被应用于混合关键性系统调度，取得了更好的结果。


<details>
  <summary>Details</summary>
Motivation: 解决将n个作业调度到m台相同机器上的问题，目标是最小化总机器工作时间。

Method: 采用First Fit Decreasing (FFD)算法，并进一步开发了多项式时间近似方案(PTAS)。

Result: FFD算法实现了1.5近似比，且问题存在PTAS。该方法在混合关键性系统调度中也取得了改进的近似结果。

Conclusion: 论文证明了First Fit Decreasing (FFD)算法在调度问题中能达到1.5近似比，并提出了多项式时间近似方案(PTAS)。此外，该方法还被应用于混合关键性系统调度，取得了更好的近似结果。

Abstract: We are concerned with the problem of scheduling $n$ jobs onto $m$ identical
machines. Each machine has to be in operation for a prescribed time, and the
objective is to minimize the total machine working time. Precisely, let $c_i$
be the prescribed time for machine $i$, where $i\in[m]$, and $p_j$ be the
processing time for job $j$, where $j\in[n]$. The problem asks for a schedule
$\sigma\colon\, J\to M$ such that $\sum_{i=1}^m\max\{c_i,
\sum_{j\in\sigma^{-1}(i)}p_j\}$ is minimized, where $J$ and $M$ denote the sets
of jobs and machines, respectively. We show that First Fit Decreasing (FFD)
leads to a $1.5$-approximation, and this problem admits a polynomial-time
approximation scheme (PTAS). The idea is further applied to mixed-criticality
system scheduling to yield improved approximation results.

</details>


### [327] [An $n^{O(\log\log n)}$ time approximation scheme for capacitated VRP in the Euclidean plane](https://arxiv.org/abs/2507.15549)
*René Sitters*

Main category: cs.DS

TL;DR: 论文提出了一种准多项式时间近似方案（Q-PTAS），显著改进了欧几里得平面上容量车辆路径问题的运行时间，并通过归约为m路径问题实现了这一目标。


<details>
  <summary>Details</summary>
Motivation: 解决欧几里得平面上容量车辆路径问题（CVRP）的高效近似算法，以改进现有最佳运行时间。

Method: 论文首先将CVRP在任意固定维度d的空间中多项式时间归约为无容量限制的m路径问题，然后为平面上的m路径问题提供了Q-PTAS。

Result: 提出的算法运行时间为n^{f(ε)⋅log log n}，显著优于之前已知的n^{log^{O(1/ε)}n}时间。

Conclusion: 该论文提出了一个准多项式时间近似方案（Q-PTAS），用于解决欧几里得平面上的容量车辆路径问题（CVRP），这是一个重大改进，并为实现PTAS迈出了重要一步。

Abstract: We present a quasi polynomial time approximation scheme (Q-PTAS) for the
capacitated vehicle routing problem (CVRP) on $n$ points in the Euclidean plane
for arbitrary capacity $c$. The running time is $n^{f(\epsilon)\cdot\log\log
n}$ for any $c$, and where $f$ is a function of $\epsilon$ only. This is a
major improvement over the so far best known running time of
$n^{\log^{O(1/\epsilon)}n}$ time and a big step towards a PTAS for Euclidean
CVRP.
  In our algorithm, we first give a polynomial time reduction of the CVRP in
$\mathbb{R}^d$ (for any fixed $d$) to an uncapacitated routing problem in
$\mathbb{R}^d$ that we call the $m$-paths problem. Here, one needs to find
exactly $m$ paths between two points $a$ and $b$, covering all the given points
in the Euclidean space. We then give a Q-PTAS for the $m$-paths problem in the
pane. Any PTAS for the (arguably easier to handle) Euclidean $m$-paths problem
is most likely to imply a PTAS for the Euclidean CVRP.

</details>


### [328] [Fast Algorithms for Graph Arboricity and Related Problems](https://arxiv.org/abs/2507.15598)
*Ruoxu Cen,Henry Fleischmann,George Z. Li,Jason Li,Debmalya Panigrahi*

Main category: cs.DS

TL;DR: 论文提出了更高效的算法，改进了加权和无向图树状性及切割层次结构的计算时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 旨在改进加权和无向图树状性计算的现有算法，减少时间复杂度，并解决整个切割层次结构计算的问题。

Method: 通过调用对数次数的有向全局最小切割子程序，算法在√n m^(1+o(1))时间内找到图的树状性。此外，还提出了一个计算整个切割层次结构的新算法，运行时间为m n^(1+o(1))。

Result: 算法将加权图的树状性计算时间复杂度从之前的O~(nm)改进到√n m^(1+o(1))，无向图从O~(m^(3/2))改进到相同复杂度。切割层次结构计算的时间复杂度也从O~(n^2m)和O~(nm^(3/2))改进到m n^(1+o(1))。

Conclusion: 该论文提出了一个改进的算法，用于计算加权无向图的树状性，并在计算整个切割层次结构方面取得了新的进展。

Abstract: We give an algorithm for finding the arboricity of a weighted, undirected
graph, defined as the minimum number of spanning forests that cover all edges
of the graph, in $\sqrt{n} m^{1+o(1)}$ time. This improves on the previous best
bound of $\tilde{O}(nm)$ for weighted graphs and $\tilde{O}(m^{3/2}) $ for
unweighted graphs (Gabow 1995) for this problem. The running time of our
algorithm is dominated by a logarithmic number of calls to a directed global
minimum cut subroutine -- if the running time of the latter problem improves to
$m^{1+o(1)}$ (thereby matching the running time of maximum flow), the running
time of our arboricity algorithm would improve further to $m^{1+o(1)}$.
  We also give a new algorithm for computing the entire cut hierarchy --
laminar multiway cuts with minimum cut ratio in recursively defined induced
subgraphs -- in $m n^{1+o(1)}$ time. The cut hierarchy yields the ideal edge
loads (Thorup 2001) in a fractional spanning tree packing of the graph which,
we show, also corresponds to a max-entropy solution in the spanning tree
polytope. For the cut hierarchy problem, the previous best bound was
$\tilde{O}(n^2 m)$ for weighted graphs and $\tilde{O}(n m^{3/2})$ for
unweighted graphs.

</details>


### [329] [On zeros and algorithms for disordered systems: mean-field spin glasses](https://arxiv.org/abs/2507.15616)
*Ferenc Bencs,Kuikui Liu,Guus Regts*

Main category: cs.DS

TL;DR: 本文设计了一种高效算法，用于估计自旋玻璃模型的配分函数，适用于多种情况，并在Sherrington-Kirkpatrick模型中取得广泛成功。


<details>
  <summary>Details</summary>
Motivation: 自旋玻璃是统计物理学、平均计算复杂性理论和现代高维统计推断的核心概率分布，研究其配分函数具有重要意义。

Method: 通过研究配分函数的零点位置，设计了概念上简单且适用于球形情况和Ising自旋情况的方法。

Result: 算法在第二矩范围内成功估计配分函数，特别是在Sherrington-Kirkpatrick模型中几乎覆盖整个复制对称相。

Conclusion: 本文设计了一种确定性拟多项式时间算法，用于在第二矩范围内以任意高精度估计几乎所有逆温度下的配分函数，特别是在Sherrington-Kirkpatrick模型中，该算法几乎在整个复制对称相中都取得成功。

Abstract: Spin glasses are fundamental probability distributions at the core of
statistical physics, the theory of average-case computational complexity, and
modern high-dimensional statistical inference. In the mean-field setting, we
design deterministic quasipolynomial-time algorithms for estimating the
partition function to arbitrarily high accuracy for nearly all inverse
temperatures in the second moment regime. In particular, for the
Sherrington--Kirkpatrick model, our algorithms succeed for almost the entire
replica-symmetric phase. To achieve this, we study the locations of the zeros
of the partition function. Notably, our methods are conceptually simple, and
apply equally well to the spherical case and the case of Ising spins.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [330] [Real-Time Scene Reconstruction using Light Field Probes](https://arxiv.org/abs/2507.14624)
*Yaru Liu,Derek Nowrouzezahri,Morgan Mcguire*

Main category: cs.GR

TL;DR: 提出一种基于探针数据结构的神经表示方法，高效重建大尺度场景，适用于VR/AR。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经渲染方法在大尺度场景中效率低下及显式几何重建成本高的问题。

Method: 通过稀疏图像重建中间、多尺度的隐式场景几何表示，利用探针数据结构存储高精度深度信息，实现场景的高效渲染。

Result: 提出的方法能够高效重建复杂场景，渲染成本与场景复杂度无关，且探针数据压缩与流式传输更高效。

Conclusion: 该论文提出了一种基于探针数据结构的神经表示方法，能够高效重建复杂大尺度场景，无需依赖显式场景几何，显著降低了计算成本，并适用于VR和AR应用。

Abstract: Reconstructing photo-realistic large-scale scenes from images, for example at
city scale, is a long-standing problem in computer graphics. Neural rendering
is an emerging technique that enables photo-realistic image synthesis from
previously unobserved viewpoints; however, state-of-the-art neural rendering
methods have difficulty efficiently rendering a high complex large-scale scene
because these methods typically trade scene size, fidelity, and rendering speed
for quality. The other stream of techniques utilizes scene geometries for
reconstruction. But the cost of building and maintaining a large set of
geometry data increases as scene size grows. Our work explores novel view
synthesis methods that efficiently reconstruct complex scenes without explicit
use of scene geometries. Specifically, given sparse images of the scene
(captured from the real world), we reconstruct intermediate, multi-scale,
implicit representations of scene geometries. In this way, our method avoids
explicitly relying on scene geometry, significantly reducing the computational
cost of maintaining large 3D data. Unlike current methods, we reconstruct the
scene using a probe data structure. Probe data hold highly accurate depth
information of dense data points, enabling the reconstruction of highly complex
scenes. By reconstructing the scene using probe data, the rendering cost is
independent of the complexity of the scene. As such, our approach combines
geometry reconstruction and novel view synthesis. Moreover, when rendering
large-scale scenes, compressing and streaming probe data is more efficient than
using explicit scene geometry. Therefore, our neural representation approach
can potentially be applied to virtual reality (VR) and augmented reality (AR)
applications.

</details>


### [331] [Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization](https://arxiv.org/abs/2507.14841)
*Xiang Tang,Ruotong Li,Xiaopeng Fan*

Main category: cs.GR

TL;DR: 提出三阶段框架，通过图像修复、几何捕捉与布局优化，实现单图像生成高质量3D场景，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在单图像生成3D场景时难以兼顾物体生成质量与场景一致性的问题。

Method: 通过图像实例分割与修复、伪立体视角构建及相机参数估计、模型参数化与点云Chamfer距离最小化三阶段流程，实现高质量3D场景生成。

Result: 实验证明，该方法在多物体场景图像集上几何精度和纹理保真度优于现有技术，且场景布局合成效果显著。

Conclusion: 该论文提出的三阶段框架在单张RGB图像引导的3D场景生成中表现出色，尤其在几何精度和纹理保真度上优于现有方法，且在场景布局合成方面具有显著优势。

Abstract: In recent years, 3D generation has made great strides in both academia and
industry. However, generating 3D scenes from a single RGB image remains a
significant challenge, as current approaches often struggle to ensure both
object generation quality and scene coherence in multi-object scenarios. To
overcome these limitations, we propose a novel three-stage framework for 3D
scene generation with explicit geometric representations and high-quality
textural details via single image-guided model generation and spatial layout
optimization. Our method begins with an image instance segmentation and
inpainting phase, which recovers missing details of occluded objects in the
input images, thereby achieving complete generation of foreground 3D assets.
Subsequently, our approach captures the spatial geometry of reference image by
constructing pseudo-stereo viewpoint for camera parameter estimation and scene
depth inference, while employing a model selection strategy to ensure optimal
alignment between the 3D assets generated in the previous step and the input.
Finally, through model parameterization and minimization of the Chamfer
distance between point clouds in 3D and 2D space, our approach optimizes layout
parameters to produce an explicit 3D scene representation that maintains
precise alignment with input guidance image. Extensive experiments on
multi-object scene image sets have demonstrated that our approach not only
outperforms state-of-the-art methods in terms of geometric accuracy and texture
fidelity of individual generated 3D models, but also has significant advantages
in scene layout synthesis.

</details>


### [332] [Time Series Information Visualization -- A Review of Approaches and Tools](https://arxiv.org/abs/2507.14920)
*Evandro S. Ortigossa,Fábio F. Dias,Diego C. Nascimento,Luis Gustavo Nonato*

Main category: cs.GR

TL;DR: 本文综述了时间序列数据可视化技术，强调了多特征时间序列的挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据普遍存在于多个领域，分析这些数据需要复杂的工具和流程。可视化技术可以增强数据的可解释性，帮助数据科学家理解动态行为和发现模式。

Method: 综述了现有的时间序列数据可视化技术和工具，提供了理论见解和设计指南。

Result: 本文提供了时间序列数据可视化技术的全面回顾，强调了多特征时间序列的挑战，并提出了未来的研究方向。

Conclusion: 本文总结了时间序列数据可视化技术的现状，并指出了未来的研究方向，特别是针对多特征时间序列的挑战。

Abstract: Time series data are prevalent across various domains and often encompass
large datasets containing multiple time-dependent features in each sample.
Exploring time-varying data is critical for data science practitioners aiming
to understand dynamic behaviors and discover periodic patterns and trends.
However, the analysis of such data often requires sophisticated procedures and
tools. Information visualization is a communication channel that leverages
human perceptual abilities to transform abstract data into visual
representations. Visualization techniques have been successfully applied in the
context of time series to enhance interpretability by graphically representing
the temporal evolution of data. The challenge for information visualization
developers lies in integrating a wide range of analytical tools into rich
visualization systems that can summarize complex datasets while clearly
describing the impacts of the temporal component. Such systems enable data
scientists to turn raw data into understandable and potentially useful
knowledge. This review examines techniques and approaches designed for handling
time series data, guiding users through knowledge discovery processes based on
visual analysis. We also provide readers with theoretical insights and design
guidelines for considering when developing comprehensive information
visualization approaches for time series, with a particular focus on time
series with multiple features. As a result, we highlight the challenges and
future research directions to address open questions in the visualization of
time-dependent data.

</details>


### [333] [Model Simplification through refinement](https://arxiv.org/abs/2507.15186)
*Dmitry Brodsky,Benjamin Watson*

Main category: cs.GR

TL;DR: 提出了一种快速、高质量的交互式大型多边形模型简化算法，通过逆向简化和曲率指导实现高效处理。


<details>
  <summary>Details</summary>
Motivation: 随着建模和可视化应用的普及，需要对大型多边形模型进行交互式简化。现有算法要么速度过慢，要么生成的质量较差，尤其是在处理极大模型时。

Method: 受矢量量化文献中的分割算法启发，该算法逆向简化模型，从极其粗糙的近似开始并逐步细化。表面曲率的近似指导了简化过程。

Result: 算法能够快速生成高质量的简化模型，并支持进一步细化。

Conclusion: 该论文提出了一种适用于大型模型交互式简化的算法，该算法不仅快速，还能在给定时间限制内保证可显示的结果，且结果质量良好。

Abstract: As modeling and visualization applications proliferate, there arises a need
to simplify large polygonal models at interactive rates. Unfortunately existing
polygon mesh simplification algorithms are not well suited for this task
because they are either too slow (requiring the simplified model to be
pre-computed) or produce models that are too poor in quality. These
shortcomings become particularly acute when models are extremely large. We
present an algorithm suitable for simplification of large models at interactive
speeds. The algorithm is fast and can guarantee displayable results within a
given time limit. Results also have good quality. Inspired by splitting
algorithms from vector quantization literature, we simplify models in reverse,
beginning with an extremely coarse approximation and refining it.
Approximations of surface curvature guide the simplification process.
Previously produced simplifications can be further refined by using them as
input to the algorithm.

</details>


### [334] [Blended Point Cloud Diffusion for Localized Text-guided Shape Editing](https://arxiv.org/abs/2507.15399)
*Etai Sella,Noam Atia,Ron Mokady,Hadar Averbuch-Elor*

Main category: cs.GR

TL;DR: 提出了一种结合3D扩散模型和坐标混合算法的方法，实现了3D形状的局部精细编辑，同时保持全局一致性，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在3D形状局部编辑时难以保持全局一致性的问题。

Method: 提出了一个基于修复的框架，利用基础3D扩散模型进行局部形状编辑，并通过部分条件形状提供结构指导。此外，引入了一种推理时坐标混合算法，平衡完整形状重建与修复过程。

Result: 实验结果表明，该方法在保持原始形状保真度和文本描述一致性方面优于其他技术。

Conclusion: 该方法通过结合3D扩散模型和坐标混合算法，在保持3D形状全局一致性的同时实现了局部精细编辑，显著优于现有技术。

Abstract: Natural language offers a highly intuitive interface for enabling localized
fine-grained edits of 3D shapes. However, prior works face challenges in
preserving global coherence while locally modifying the input 3D shape. In this
work, we introduce an inpainting-based framework for editing shapes represented
as point clouds. Our approach leverages foundation 3D diffusion models for
achieving localized shape edits, adding structural guidance in the form of a
partial conditional shape, ensuring that other regions correctly preserve the
shape's identity. Furthermore, to encourage identity preservation also within
the local edited region, we propose an inference-time coordinate blending
algorithm which balances reconstruction of the full shape with inpainting at a
progression of noise levels during the inference process. Our coordinate
blending algorithm seamlessly blends the original shape with its edited
version, enabling a fine-grained editing of 3D shapes, all while circumventing
the need for computationally expensive and often inaccurate inversion.
Extensive experiments show that our method outperforms alternative techniques
across a wide range of metrics that evaluate both fidelity to the original
shape and also adherence to the textual description.

</details>


### [335] [ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting](https://arxiv.org/abs/2507.15454)
*Ruijie Zhu,Mulin Yu,Linning Xu,Lihan Jiang,Yixuan Li,Tianzhu Zhang,Jiangmiao Pang,Bo Dai*

Main category: cs.GR

TL;DR: ObjectGS是一个对象感知框架，通过局部锚点和语义约束实现高精度对象级重建，显著提升语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射虽然在高保真重建和实时新视角合成方面表现出色，但缺乏语义理解能力，限制了对象级感知。ObjectGS旨在统一3D场景重建与语义理解。

Method: ObjectGS将场景中的单个对象建模为局部锚点，生成神经高斯并共享对象ID，通过动态增长或修剪锚点并优化特征，同时使用one-hot ID编码和分类损失强化语义约束。

Result: ObjectGS在开放词汇和全景分割任务上优于现有技术，并能无缝集成网格提取和场景编辑等应用。

Conclusion: ObjectGS通过动态调整对象锚点并结合语义约束，实现了高精度的对象级重建和语义理解，显著提升了开放词汇和全景分割任务的性能。

Abstract: 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and
real-time novel view synthesis, yet its lack of semantic understanding limits
object-level perception. In this work, we propose ObjectGS, an object-aware
framework that unifies 3D scene reconstruction with semantic understanding.
Instead of treating the scene as a unified whole, ObjectGS models individual
objects as local anchors that generate neural Gaussians and share object IDs,
enabling precise object-level reconstruction. During training, we dynamically
grow or prune these anchors and optimize their features, while a one-hot ID
encoding with a classification loss enforces clear semantic constraints. We
show through extensive experiments that ObjectGS not only outperforms
state-of-the-art methods on open-vocabulary and panoptic segmentation tasks,
but also integrates seamlessly with applications like mesh extraction and scene
editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page

</details>


### [336] [Gaussian Splatting with Discretized SDF for Relightable Assets](https://arxiv.org/abs/2507.15629)
*Zuo-Liang Zhu,Jian Yang,Beibei Wang*

Main category: cs.GR

TL;DR: 离散化SDF表示通过高斯溅射实现高质量逆渲染，避免额外内存和复杂优化。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯溅射在逆渲染中因离散特性难以应用几何约束的问题，同时避免现有方法增加内存和训练复杂度的缺点。

Method: 引入离散化SDF表示，通过采样值在每个高斯内编码SDF，并通过SDF-to-opacity转换与高斯不透明度关联，避免光线追踪的计算成本。

Result: 实验表明，该方法在高斯基逆渲染方法中表现优越。

Conclusion: 通过离散化SDF表示，该方法在不需要额外内存和复杂优化的情况下，显著提升了高斯基逆渲染的质量。

Abstract: 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and
highly efficient rendering speed in the novel view synthesis (NVS) task. The
application to inverse rendering still faces several challenges, as the
discrete nature of Gaussian primitives makes it difficult to apply geometry
constraints. Recent works introduce the signed distance field (SDF) as an extra
continuous representation to regularize the geometry defined by Gaussian
primitives. It improves the decomposition quality, at the cost of increasing
memory usage and complicating training. Unlike these works, we introduce a
discretized SDF to represent the continuous SDF in a discrete manner by
encoding it within each Gaussian using a sampled value. This approach allows us
to link the SDF with the Gaussian opacity through an SDF-to-opacity
transformation, enabling rendering the SDF via splatting and avoiding the
computational cost of ray marching.The key challenge is to regularize the
discrete samples to be consistent with the underlying SDF, as the discrete
representation can hardly apply the gradient-based constraints (\eg Eikonal
loss). For this, we project Gaussians onto the zero-level set of SDF and
enforce alignment with the surface from splatting, namely a projection-based
consistency loss. Thanks to the discretized SDF, our method achieves higher
relighting quality, while requiring no extra memory beyond GS and avoiding
complex manually designed optimization. The experiments reveal that our method
outperforms existing Gaussian-based inverse rendering methods. Our code is
available at https://github.com/NK-CS-ZZL/DiscretizedSDF.

</details>
