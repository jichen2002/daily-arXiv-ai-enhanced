{"id": "2507.14154", "categories": ["cs.AI", "cs.LG", "68T05, 81P68", "I.2.6; I.2.0; F.1.2"], "pdf": "https://arxiv.org/pdf/2507.14154", "abs": "https://arxiv.org/abs/2507.14154", "authors": ["Rahul Kabali"], "title": "The Free Will Equation: Quantum Field Analogies for AGI", "comment": "22 pages, 5 figures. Submitted as an arXiv preprint. All code and\n  experiment details included in appendix", "summary": "Artificial General Intelligence (AGI) research traditionally focuses on\nalgorithms that optimize for specific goals under deterministic rules. Yet,\nhuman-like intelligence exhibits adaptive spontaneity - an ability to make\nunexpected choices or free decisions not strictly dictated by past data or\nimmediate reward. This trait, often dubbed \"free will\" in a loose sense, might\nbe crucial for creativity, robust adaptation, and avoiding ruts in\nproblem-solving. This paper proposes a theoretical framework, called the Free\nWill Equation, that draws analogies from quantum field theory to endow AGI\nagents with a form of adaptive, controlled stochasticity in their\ndecision-making process. The core idea is to treat an AI agent's cognitive\nstate as a superposition of potential actions or thoughts, which collapses\nprobabilistically into a concrete action when a decision is made - much like a\nquantum wavefunction collapsing upon measurement. By incorporating mechanisms\nanalogous to quantum fields, along with intrinsic motivation terms, we aim to\nimprove an agent's ability to explore novel strategies and adapt to unforeseen\nchanges. Experiments in a non-stationary multi-armed bandit environment\ndemonstrate that agents using this framework achieve higher rewards and policy\ndiversity compared to baseline methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa'\u81ea\u7531\u610f\u5fd7\u65b9\u7a0b'\u6846\u67b6\uff0c\u501f\u9274\u91cf\u5b50\u573a\u8bba\u4e3aAGI\u51b3\u7b56\u5f15\u5165\u53d7\u63a7\u968f\u673a\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u975e\u7a33\u6001\u73af\u5883\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfAGI\u7814\u7a76\u4e13\u6ce8\u4e8e\u5728\u786e\u5b9a\u6027\u89c4\u5219\u4e0b\u4f18\u5316\u7279\u5b9a\u76ee\u6807\uff0c\u800c\u4eba\u7c7b\u667a\u80fd\u5c55\u73b0\u51fa\u9002\u5e94\u6027\u81ea\u53d1\u6027\uff0c\u5373\u505a\u51fa\u4e0d\u53d7\u8fc7\u53bb\u6570\u636e\u6216\u5373\u65f6\u5956\u52b1\u4e25\u683c\u652f\u914d\u7684\u610f\u5916\u9009\u62e9\u7684\u80fd\u529b\u3002\u8fd9\u79cd\u7279\u6027\u53ef\u80fd\u5bf9\u521b\u9020\u529b\u3001\u9c81\u68d2\u9002\u5e94\u6027\u548c\u907f\u514d\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u601d\u7ef4\u5b9a\u52bf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u2014\u2014\u81ea\u7531\u610f\u5fd7\u65b9\u7a0b\uff0c\u5c06AI\u4ee3\u7406\u7684\u8ba4\u77e5\u72b6\u6001\u89c6\u4e3a\u6f5c\u5728\u884c\u52a8\u6216\u60f3\u6cd5\u7684\u53e0\u52a0\u6001\uff0c\u5728\u51b3\u7b56\u65f6\u6982\u7387\u6027\u5730\u574d\u7f29\u4e3a\u5177\u4f53\u884c\u52a8\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u7c7b\u4f3c\u91cf\u5b50\u573a\u7684\u673a\u5236\u548c\u5185\u5728\u52a8\u673a\u9879\u3002", "result": "\u5728\u975e\u7a33\u6001\u591a\u81c2\u8001\u864e\u673a\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u8be5\u6846\u67b6\u7684\u4ee3\u7406\u5728\u5956\u52b1\u548c\u7b56\u7565\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'\u81ea\u7531\u610f\u5fd7\u65b9\u7a0b'\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u501f\u9274\u91cf\u5b50\u573a\u8bba\uff0c\u4e3aAGI\u4ee3\u7406\u7684\u51b3\u7b56\u8fc7\u7a0b\u5f15\u5165\u4e86\u4e00\u79cd\u9002\u5e94\u6027\u3001\u53d7\u63a7\u7684\u968f\u673a\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u975e\u7a33\u6001\u591a\u81c2\u8001\u864e\u673a\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2507.14267", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.14267", "abs": "https://arxiv.org/abs/2507.14267", "authors": ["Ziqi Wang", "Hongshuo Huang", "Hancheng Zhao", "Changwen Xu", "Shang Zhu", "Jan Janssen", "Venkatasubramanian Viswanathan"], "title": "DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation", "comment": "34 pages, 28 pages of Supporting Information", "summary": "Materials discovery relies on high-throughput, high-fidelity simulation\ntechniques such as Density Functional Theory (DFT), which require years of\ntraining, extensive parameter fine-tuning and systematic error handling. To\naddress these challenges, we introduce the DFT-based Research Engine for\nAgentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for\nDFT simulation that combines a central Large Language Model (LLM) planner agent\nwith domain-specific LLM agents for atomistic structure generation, systematic\nDFT convergence testing, High-Performance Computing (HPC) scheduling, and error\nhandling. In addition, a shared canvas helps the LLM agents to structure their\ndiscussions, preserve context and prevent hallucination. We validate DREAMS\ncapabilities on the Sol27LC lattice-constant benchmark, achieving average\nerrors below 1\\% compared to the results of human DFT experts. Furthermore, we\napply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating\nits long-term and complex problem-solving capabilities. The framework again\nreproduces expert-level literature adsorption-energy differences. Finally,\nDREAMS is employed to quantify functional-driven uncertainties with Bayesian\nensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at\nthe Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS\napproaches L3-level automation - autonomous exploration of a defined design\nspace - and significantly reduces the reliance on human expertise and\nintervention, offering a scalable path toward democratized, high-throughput,\nhigh-fidelity computational materials discovery.", "AI": {"tldr": "DREAMS\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7LLM\u548c\u5171\u4eab\u753b\u5e03\u5b9e\u73b0DFT\u6a21\u62df\u81ea\u52a8\u5316\uff0c\u663e\u8457\u63d0\u5347\u6750\u6599\u53d1\u73b0\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3DFT\u6a21\u62df\u4e2d\u8bad\u7ec3\u5468\u671f\u957f\u3001\u53c2\u6570\u8c03\u4f18\u590d\u6742\u548c\u7cfb\u7edf\u9519\u8bef\u5904\u7406\u56f0\u96be\u7b49\u6311\u6218\u3002", "method": "\u5f15\u5165\u57fa\u4e8eDFT\u7684\u7814\u7a76\u5f15\u64ceDREAMS\uff0c\u7ed3\u5408\u4e2d\u5fc3\u5316LLM\u89c4\u5212\u4ee3\u7406\u4e0e\u9886\u57df\u7279\u5b9a\u4ee3\u7406\uff0c\u5305\u62ec\u539f\u5b50\u7ed3\u6784\u751f\u6210\u3001DFT\u6536\u655b\u6d4b\u8bd5\u3001HPC\u8c03\u5ea6\u548c\u9519\u8bef\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u753b\u5e03\u534f\u8c03\u4ee3\u7406\u95f4\u8ba8\u8bba\u3002", "result": "\u5728Sol27LC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u8bef\u5dee\u4f4e\u4e8e1%\uff0c\u6210\u529f\u89e3\u51b3CO/Pt(111)\u5438\u9644\u96be\u9898\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u96c6\u6210\u91c7\u6837\u91cf\u5316\u529f\u80fd\u9a71\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "DREAMS\u5b9e\u73b0\u4e86L3\u7ea7\u81ea\u52a8\u5316\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u548c\u5e72\u9884\u7684\u4f9d\u8d56\uff0c\u4e3a\u9ad8\u901a\u91cf\u3001\u9ad8\u4fdd\u771f\u8ba1\u7b97\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2507.14293", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14293", "abs": "https://arxiv.org/abs/2507.14293", "authors": ["Boyuan Zheng", "Zeyi Liao", "Scott Salisbury", "Zeyuan Liu", "Michael Lin", "Qinyuan Zheng", "Zifan Wang", "Xiang Deng", "Dawn Song", "Huan Sun", "Yu Su"], "title": "WebGuard: Building a Generalizable Guardrail for Web Agents", "comment": "We publicly release WebGuard, along with its annotation tools and\n  fine-tuned models, to facilitate open-source research on monitoring and\n  safeguarding web agents. All resources are available at\n  https://github.com/OSU-NLP-Group/WebGuard", "summary": "The rapid development of autonomous web agents powered by Large Language\nModels (LLMs), while greatly elevating efficiency, exposes the frontier risk of\ntaking unintended or harmful actions. This situation underscores an urgent need\nfor effective safety measures, akin to access controls for human users. To\naddress this critical challenge, we introduce WebGuard, the first comprehensive\ndataset designed to support the assessment of web agent action risks and\nfacilitate the development of guardrails for real-world online environments. In\ndoing so, WebGuard specifically focuses on predicting the outcome of\nstate-changing actions and contains 4,939 human-annotated actions from 193\nwebsites across 22 diverse domains, including often-overlooked long-tail\nwebsites. These actions are categorized using a novel three-tier risk schema:\nSAFE, LOW, and HIGH. The dataset includes designated training and test splits\nto support evaluation under diverse generalization settings. Our initial\nevaluations reveal a concerning deficiency: even frontier LLMs achieve less\nthan 60% accuracy in predicting action outcomes and less than 60% recall in\nlagging HIGH-risk actions, highlighting the risks of deploying\ncurrent-generation agents without dedicated safeguards. We therefore\ninvestigate fine-tuning specialized guardrail models using WebGuard. We conduct\ncomprehensive evaluations across multiple generalization settings and find that\na fine-tuned Qwen2.5VL-7B model yields a substantial improvement in\nperformance, boosting accuracy from 37% to 80% and HIGH-risk action recall from\n20% to 76%. Despite these improvements, the performance still falls short of\nthe reliability required for high-stakes deployment, where guardrails must\napproach near-perfect accuracy and recall.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86WebGuard\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7f51\u7edc\u4ee3\u7406\u52a8\u4f5c\u98ce\u9669\u5e76\u5f00\u53d1\u5b89\u5168\u63aa\u65bd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6a21\u578b\u6027\u80fd\u867d\u663e\u8457\u63d0\u5347\uff0c\u4f46\u4ecd\u672a\u8fbe\u5230\u9ad8\u98ce\u9669\u90e8\u7f72\u7684\u53ef\u9760\u6027\u8981\u6c42\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u4e3b\u7f51\u7edc\u4ee3\u7406\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u6f5c\u5728\u7684\u610f\u5916\u6216\u6709\u5bb3\u884c\u4e3a\u98ce\u9669\u51f8\u663e\u4e86\u5bf9\u6709\u6548\u5b89\u5168\u63aa\u65bd\u7684\u8feb\u5207\u9700\u6c42\u3002", "method": "\u5f15\u5165WebGuard\u6570\u636e\u96c6\uff0c\u5305\u542b4,939\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u52a8\u4f5c\uff0c\u8986\u76d6193\u4e2a\u7f51\u7ad9\u548c22\u4e2a\u9886\u57df\uff0c\u91c7\u7528\u4e09\u7ea7\u98ce\u9669\u5206\u7c7b\uff08SAFE\u3001LOW\u3001HIGH\uff09\u3002\u901a\u8fc7\u5fae\u8c03Qwen2.5VL-7B\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5fae\u8c03\u540e\u7684Qwen2.5VL-7B\u6a21\u578b\u5c06\u51c6\u786e\u6027\u4ece37%\u63d0\u5347\u81f380%\uff0c\u9ad8\u98ce\u9669\u52a8\u4f5c\u53ec\u56de\u7387\u4ece20%\u63d0\u5347\u81f376%\u3002", "conclusion": "\u5c3d\u7ba1WebGuard\u6570\u636e\u96c6\u548c\u5fae\u8c03\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f46\u5f53\u524d\u7684\u5b89\u5168\u63aa\u65bd\u4ecd\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u9ad8\u98ce\u9669\u90e8\u7f72\u7684\u9700\u6c42\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u6027\u548c\u53ec\u56de\u7387\u3002"}}
{"id": "2507.14306", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.14306", "abs": "https://arxiv.org/abs/2507.14306", "authors": ["Samarth P", "Vyoman Jain", "Shiva Golugula", "Motamarri Sai Sathvik"], "title": "Manimator: Transforming Research Papers into Visual Explanations", "comment": null, "summary": "Understanding complex scientific and mathematical concepts, particularly\nthose presented in dense research papers, poses a significant challenge for\nlearners. Dynamic visualizations can greatly enhance comprehension, but\ncreating them manually is time-consuming and requires specialized knowledge and\nskills. We introduce manimator, an open-source system that leverages Large\nLanguage Models to transform research papers and natural language prompts into\nexplanatory animations using the Manim engine. Manimator employs a pipeline\nwhere an LLM interprets the input text or research paper PDF to generate a\nstructured scene description outlining key concepts, mathematical formulas, and\nvisual elements and another LLM translates this description into executable\nManim Python code. We discuss its potential as an educational tool for rapidly\ncreating engaging visual explanations for complex STEM topics, democratizing\nthe creation of high-quality educational content.", "AI": {"tldr": "Manimator \u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u901a\u8fc7 LLM \u5c06\u7814\u7a76\u8bba\u6587\u8f6c\u5316\u4e3a\u52a8\u753b\uff0c\u5e2e\u52a9\u5b66\u4e60\u8005\u7406\u89e3\u590d\u6742 STEM \u6982\u5ff5\uff0c\u7b80\u5316\u9ad8\u8d28\u91cf\u6559\u80b2\u5185\u5bb9\u7684\u521b\u5efa\u3002", "motivation": "\u7406\u89e3\u590d\u6742\u79d1\u5b66\u548c\u6570\u5b66\u6982\u5ff5\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u5728\u5bc6\u96c6\u7814\u7a76\u8bba\u6587\u4e2d\u5448\u73b0\u7684\u5185\u5bb9\uff0c\u5bf9\u5b66\u4e60\u8005\u6765\u8bf4\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u52a8\u6001\u53ef\u89c6\u5316\u53ef\u4ee5\u6781\u5927\u5730\u589e\u5f3a\u7406\u89e3\uff0c\u4f46\u624b\u52a8\u521b\u5efa\u5b83\u4eec\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u548c\u6280\u80fd\u3002", "method": "Manimator \u91c7\u7528\u4e00\u4e2a\u6d41\u7a0b\uff0c\u5176\u4e2d LLM \u89e3\u91ca\u8f93\u5165\u6587\u672c\u6216\u7814\u7a76\u8bba\u6587 PDF\uff0c\u751f\u6210\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u573a\u666f\u63cf\u8ff0\uff0c\u6982\u8ff0\u5173\u952e\u6982\u5ff5\u3001\u6570\u5b66\u516c\u5f0f\u548c\u89c6\u89c9\u5143\u7d20\uff0c\u53e6\u4e00\u4e2a LLM \u5c06\u6b64\u63cf\u8ff0\u7ffb\u8bd1\u4e3a\u53ef\u6267\u884c\u7684 Manim Python \u4ee3\u7801\u3002", "result": "Manimator \u662f\u4e00\u4e2a\u5f00\u6e90\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u7814\u7a76\u8bba\u6587\u548c\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8f6c\u6362\u4e3a\u4f7f\u7528 Manim \u5f15\u64ce\u7684\u89e3\u91ca\u6027\u52a8\u753b\u3002", "conclusion": "Manimator \u662f\u4e00\u4e2a\u6709\u6f5c\u529b\u7684\u6559\u80b2\u5de5\u5177\uff0c\u80fd\u591f\u5feb\u901f\u4e3a\u590d\u6742\u7684 STEM \u4e3b\u9898\u521b\u5efa\u5f15\u4eba\u5165\u80dc\u7684\u89c6\u89c9\u89e3\u91ca\uff0c\u4ece\u800c\u666e\u53ca\u9ad8\u8d28\u91cf\u6559\u80b2\u5185\u5bb9\u7684\u521b\u4f5c\u3002"}}
{"id": "2507.14392", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.14392", "abs": "https://arxiv.org/abs/2507.14392", "authors": ["Lang Xu", "Kaushik Kandadi Suresh", "Quentin Anthony", "Nawras Alnaasan", "Dhabaleswar K. Panda"], "title": "Characterizing Communication Patterns in Distributed Large Language Model Inference", "comment": "To be presented at Hot Interconnects 2025", "summary": "Large Language Models (LLMs) built on transformer architectures have\ntransformed natural language processing, achieving remarkable performance\nacross diverse applications. While distributed inference frameworks enable\npractical deployment of these models, inter-GPU communication creates\nsignificant performance constraints that limit service quality in real-world\nsystems. This paper investigates communication dynamics in distributed LLM\nserving-analyzing how various parallelization approaches coordinate data\nexchange between GPU workers during inference. We study dense transformer-based\nmodels as representative examples of contemporary architectures widely used in\noperational deployments. Our work combines detailed profiling measurements with\npredictive analytical models to characterize communication behavior across\ndifferent parallelization configurations. Results show that tensor parallelism\nincurs substantial network overhead but delivers superior response times for\nbrief sequences, pipeline parallelism minimizes data transfer requirements\nwhile increasing total latency, and combined approaches demand careful tuning\nto achieve balanced performance. These insights offer practical recommendations\nfor selecting appropriate parallelization schemes in production LLM services\nand identify key opportunities for optimizing inference frameworks and\ncommunication infrastructure.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5206\u5e03\u5f0fLLM\u63a8\u7406\u4e2d\u7684\u901a\u4fe1\u52a8\u6001\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u5e76\u884c\u5316\u65b9\u6cd5\u5728GPU\u95f4\u6570\u636e\u4ea4\u6362\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u751f\u4ea7\u73af\u5883\u63d0\u4f9b\u4e86\u4f18\u5316\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5206\u6790\u5206\u5e03\u5f0fLLM\u63a8\u7406\u4e2dGPU\u95f4\u901a\u4fe1\u7684\u6027\u80fd\u9650\u5236\uff0c\u4ee5\u63d0\u5347\u5b9e\u9645\u7cfb\u7edf\u7684\u670d\u52a1\u8d28\u91cf\u3002", "method": "\u7ed3\u5408\u8be6\u7ec6\u7684\u6027\u80fd\u5206\u6790\u6d4b\u91cf\u548c\u9884\u6d4b\u6027\u5206\u6790\u6a21\u578b\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u5e76\u884c\u5316\u914d\u7f6e\u4e0b\u7684\u901a\u4fe1\u884c\u4e3a\u3002", "result": "\u7ed3\u679c\u8868\u660e\u5f20\u91cf\u5e76\u884c\u5316\u5e26\u6765\u663e\u8457\u7f51\u7edc\u5f00\u9500\u4f46\u77ed\u5e8f\u5217\u54cd\u5e94\u65f6\u95f4\u4f18\u5f02\uff0c\u7ba1\u9053\u5e76\u884c\u5316\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u4f46\u589e\u52a0\u603b\u5ef6\u8fdf\uff0c\u800c\u7ec4\u5408\u65b9\u6cd5\u9700\u7cbe\u7ec6\u8c03\u4f18\u4ee5\u5b9e\u73b0\u5e73\u8861\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u7684\u7ed3\u8bba\u662f\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684LLM\u670d\u52a1\u63d0\u4f9b\u4e86\u9009\u62e9\u9002\u5f53\u5e76\u884c\u5316\u65b9\u6848\u7684\u5b9e\u9645\u5efa\u8bae\uff0c\u5e76\u6307\u51fa\u4e86\u4f18\u5316\u63a8\u7406\u6846\u67b6\u548c\u901a\u4fe1\u57fa\u7840\u8bbe\u65bd\u7684\u5173\u952e\u673a\u4f1a\u3002"}}
{"id": "2507.14261", "categories": ["cs.DS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14261", "abs": "https://arxiv.org/abs/2507.14261", "authors": ["Mahmood K. M. Almansoori", "Miklos Telek"], "title": "FAMST: Fast Approximate Minimum Spanning Tree Construction for Large-Scale and High-Dimensional Data", "comment": null, "summary": "We present Fast Approximate Minimum Spanning Tree (FAMST), a novel algorithm\nthat addresses the computational challenges of constructing Minimum Spanning\nTrees (MSTs) for large-scale and high-dimensional datasets. FAMST utilizes a\nthree-phase approach: Approximate Nearest Neighbor (ANN) graph construction,\nANN inter-component connection, and iterative edge refinement. For a dataset of\n$n$ points in a $d$-dimensional space, FAMST achieves $\\mathcal{O}(dn \\log n)$\ntime complexity and $\\mathcal{O}(dn + kn)$ space complexity when $k$ nearest\nneighbors are considered, which is a significant improvement over the\n$\\mathcal{O}(n^2)$ time and space complexity of traditional methods.\n  Experiments across diverse datasets demonstrate that FAMST achieves\nremarkably low approximation errors while providing speedups of up to\n1000$\\times$ compared to exact MST algorithms. We analyze how the key\nhyperparameters, $k$ (neighborhood size) and $\\lambda$ (inter-component edges),\naffect performance, providing practical guidelines for hyperparameter\nselection. FAMST enables MST-based analysis on datasets with millions of points\nand thousands of dimensions, extending the applicability of MST techniques to\nproblem scales previously considered infeasible.", "AI": {"tldr": "FAMST\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6700\u5c0f\u751f\u6210\u6811\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u9ad8\u7ef4\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u548c\u9ad8\u7ef4\u6570\u636e\u96c6\u4e0a\u6784\u5efa\u6700\u5c0f\u751f\u6210\u6811\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u6269\u5c55MST\u6280\u672f\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "FAMST\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u8fd1\u4f3c\u6700\u8fd1\u90bb\u56fe\u6784\u5efa\u3001ANN\u7ec4\u4ef6\u95f4\u8fde\u63a5\u548c\u8fed\u4ee3\u8fb9\u7ec6\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFAMST\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6781\u4f4e\u7684\u8fd1\u4f3c\u8bef\u5dee\uff0c\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe1000\u500d\uff0c\u9002\u7528\u4e8e\u767e\u4e07\u7ea7\u70b9\u548c\u6570\u5343\u7ef4\u7684\u6570\u636e\u96c6\u3002", "conclusion": "FAMST\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u5927\u89c4\u6a21\u548c\u9ad8\u7ef4\u6570\u636e\u96c6\u4e0a\u6784\u5efa\u6700\u5c0f\u751f\u6210\u6811\u7684\u6548\u7387\uff0c\u5c06\u4f20\u7edf\u65b9\u6cd5\u7684O(n^2)\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u964d\u4f4e\u81f3O(dn log n)\u548cO(dn + kn)\uff0c\u5e76\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u8fd1\u4f3c\u8bef\u5dee\u3002"}}
{"id": "2507.14256", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14256", "abs": "https://arxiv.org/abs/2507.14256", "authors": ["Jakub Walczak", "Piotr Tomalak", "Artur Laskowski"], "title": "Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models", "comment": null, "summary": "Generative AI is gaining increasing attention in software engineering, where\ntesting remains an indispensable reliability mechanism. According to the widely\nadopted testing pyramid, unit tests constitute the majority of test cases and\nare often schematic, requiring minimal domain expertise. Automatically\ngenerating such tests under the supervision of software engineers can\nsignificantly enhance productivity during the development phase of the software\nlifecycle.\n  This paper investigates the impact of code context and prompting strategies\non the quality and adequacy of unit tests generated by various large language\nmodels (LLMs) across several families. The results show that including\ndocstrings notably improves code adequacy, while further extending context to\nthe full implementation yields definitely smaller gains. Notably, the\nchain-of-thought prompting strategy -- applied even to 'reasoning' models --\nachieves the best results, with up to 96.3\\% branch coverage, a 57\\% average\nmutation score, and near-perfect compilation success rate. Among the evaluated\nmodels, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation\nscore and branch coverage being still in top in terms of compilation success\nrate.\n  All the code and resulting test suites are publicly available at\nhttps://github.com/peetery/LLM-analysis.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u7ed3\u5408\u6587\u6863\u5b57\u7b26\u4e32\u548c\u94fe\u5f0f\u601d\u8003\u63d0\u793a\u7b56\u7565\u7684LLM\u751f\u6210\u5355\u5143\u6d4b\u8bd5\u6548\u679c\u6700\u4f73\uff0c\u5176\u4e2dGemini 2.5 Pro\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u5355\u5143\u6d4b\u8bd5\u63d0\u5347\u8f6f\u4ef6\u5f00\u53d1\u9636\u6bb5\u7684\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u6d4b\u8bd5\u91d1\u5b57\u5854\u4e2d\u5360\u6bd4\u6700\u5927\u7684\u5355\u5143\u6d4b\u8bd5\u3002", "method": "\u7814\u7a76\u4e86\u4ee3\u7801\u4e0a\u4e0b\u6587\u548c\u63d0\u793a\u7b56\u7565\u5bf9\u4e0d\u540c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u5355\u5143\u6d4b\u8bd5\u8d28\u91cf\u548c\u5145\u5206\u6027\u7684\u5f71\u54cd\u3002", "result": "\u5305\u542b\u6587\u6863\u5b57\u7b26\u4e32\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u5145\u5206\u6027\uff0c\u800c\u5b8c\u6574\u5b9e\u73b0\u4e0a\u4e0b\u6587\u7684\u6269\u5c55\u6548\u679c\u8f83\u5c0f\u3002\u94fe\u5f0f\u601d\u8003\u63d0\u793a\u7b56\u7565\u6548\u679c\u6700\u4f73\uff0c\u8fbe\u523096.3%\u7684\u5206\u652f\u8986\u76d6\u7387\u548c57%\u7684\u5e73\u5747\u7a81\u53d8\u5206\u6570\u3002", "conclusion": "Gemini 2.5 Pro (M5) \u5728\u7a81\u53d8\u5206\u6570\u548c\u5206\u652f\u8986\u76d6\u7387\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u540c\u65f6\u5728\u7f16\u8bd1\u6210\u529f\u7387\u4e0a\u4e5f\u4fdd\u6301\u9886\u5148\u3002"}}
{"id": "2507.14624", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14624", "abs": "https://arxiv.org/abs/2507.14624", "authors": ["Yaru Liu", "Derek Nowrouzezahri", "Morgan Mcguire"], "title": "Real-Time Scene Reconstruction using Light Field Probes", "comment": null, "summary": "Reconstructing photo-realistic large-scale scenes from images, for example at\ncity scale, is a long-standing problem in computer graphics. Neural rendering\nis an emerging technique that enables photo-realistic image synthesis from\npreviously unobserved viewpoints; however, state-of-the-art neural rendering\nmethods have difficulty efficiently rendering a high complex large-scale scene\nbecause these methods typically trade scene size, fidelity, and rendering speed\nfor quality. The other stream of techniques utilizes scene geometries for\nreconstruction. But the cost of building and maintaining a large set of\ngeometry data increases as scene size grows. Our work explores novel view\nsynthesis methods that efficiently reconstruct complex scenes without explicit\nuse of scene geometries. Specifically, given sparse images of the scene\n(captured from the real world), we reconstruct intermediate, multi-scale,\nimplicit representations of scene geometries. In this way, our method avoids\nexplicitly relying on scene geometry, significantly reducing the computational\ncost of maintaining large 3D data. Unlike current methods, we reconstruct the\nscene using a probe data structure. Probe data hold highly accurate depth\ninformation of dense data points, enabling the reconstruction of highly complex\nscenes. By reconstructing the scene using probe data, the rendering cost is\nindependent of the complexity of the scene. As such, our approach combines\ngeometry reconstruction and novel view synthesis. Moreover, when rendering\nlarge-scale scenes, compressing and streaming probe data is more efficient than\nusing explicit scene geometry. Therefore, our neural representation approach\ncan potentially be applied to virtual reality (VR) and augmented reality (AR)\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u63a2\u9488\u6570\u636e\u7ed3\u6784\u7684\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u9ad8\u6548\u91cd\u5efa\u5927\u5c3a\u5ea6\u573a\u666f\uff0c\u9002\u7528\u4e8eVR/AR\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u5728\u5927\u5c3a\u5ea6\u573a\u666f\u4e2d\u6548\u7387\u4f4e\u4e0b\u53ca\u663e\u5f0f\u51e0\u4f55\u91cd\u5efa\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7a00\u758f\u56fe\u50cf\u91cd\u5efa\u4e2d\u95f4\u3001\u591a\u5c3a\u5ea6\u7684\u9690\u5f0f\u573a\u666f\u51e0\u4f55\u8868\u793a\uff0c\u5229\u7528\u63a2\u9488\u6570\u636e\u7ed3\u6784\u5b58\u50a8\u9ad8\u7cbe\u5ea6\u6df1\u5ea6\u4fe1\u606f\uff0c\u5b9e\u73b0\u573a\u666f\u7684\u9ad8\u6548\u6e32\u67d3\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u91cd\u5efa\u590d\u6742\u573a\u666f\uff0c\u6e32\u67d3\u6210\u672c\u4e0e\u573a\u666f\u590d\u6742\u5ea6\u65e0\u5173\uff0c\u4e14\u63a2\u9488\u6570\u636e\u538b\u7f29\u4e0e\u6d41\u5f0f\u4f20\u8f93\u66f4\u9ad8\u6548\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a2\u9488\u6570\u636e\u7ed3\u6784\u7684\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u91cd\u5efa\u590d\u6742\u5927\u5c3a\u5ea6\u573a\u666f\uff0c\u65e0\u9700\u4f9d\u8d56\u663e\u5f0f\u573a\u666f\u51e0\u4f55\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u9002\u7528\u4e8eVR\u548cAR\u5e94\u7528\u3002"}}
{"id": "2507.14183", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.14183", "abs": "https://arxiv.org/abs/2507.14183", "authors": ["Arash Aryapour"], "title": "Iran's Stealth Internet Blackout: A New Model of Censorship", "comment": null, "summary": "In mid-2025, Iran experienced a novel, stealthy Internet shutdown that\npreserved global routing presence while isolating domestic users through deep\npacket inspection, aggressive throttling, and selective protocol blocking. This\npaper analyzes active network measurements such as DNS poisoning, HTTP\ninjection, TLS interception, and protocol whitelisting, traced to a centralized\nborder gateway. We quantify an approximate 707 percent rise in VPN demand and\ndescribe the multi-layered censorship infrastructure, highlighting implications\nfor circumvention and digital rights monitoring.", "AI": {"tldr": "\u4f0a\u67172025\u5e74\u65b0\u578b\u4e92\u8054\u7f51\u5173\u95ed\u624b\u6bb5\u901a\u8fc7\u9690\u853d\u6280\u672f\u9694\u79bb\u56fd\u5185\u7528\u6237\uff0c\u5bfc\u81f4VPN\u9700\u6c42\u6fc0\u589e707%\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u5176\u591a\u5c42\u5ba1\u67e5\u673a\u5236\u53ca\u5176\u5bf9\u6570\u5b57\u6743\u5229\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u91cf\u5316\u548c\u5206\u6790\u4f0a\u6717\u65b0\u578b\u4e92\u8054\u7f51\u5173\u95ed\u624b\u6bb5\u7684\u6280\u672f\u7ec6\u8282\u53ca\u5176\u5bf9VPN\u9700\u6c42\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5bf9\u6570\u5b57\u6743\u5229\u7684\u6f5c\u5728\u5a01\u80c1\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u4e3b\u52a8\u7f51\u7edc\u6d4b\u91cf\uff08\u5982DNS\u6295\u6bd2\u3001HTTP\u6ce8\u5165\u3001TLS\u62e6\u622a\u548c\u534f\u8bae\u767d\u540d\u5355\uff09\u5206\u6790\u4e86\u8fd9\u4e00\u73b0\u8c61\uff0c\u5e76\u5c06\u8fd9\u4e9b\u884c\u4e3a\u8ffd\u6eaf\u81f3\u4e00\u4e2a\u96c6\u4e2d\u5316\u7684\u8fb9\u754c\u7f51\u5173\u3002", "result": "\u7814\u7a76\u53d1\u73b0VPN\u9700\u6c42\u6fc0\u589e\u7ea6707%\uff0c\u5e76\u8be6\u7ec6\u63cf\u8ff0\u4e86\u591a\u5c42\u5ba1\u67e5\u57fa\u7840\u8bbe\u65bd\u7684\u8fd0\u4f5c\u65b9\u5f0f\u3002", "conclusion": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u4f0a\u6717\u57282025\u5e74\u4e2d\u671f\u5b9e\u65bd\u7684\u4e00\u79cd\u65b0\u578b\u3001\u9690\u853d\u7684\u4e92\u8054\u7f51\u5173\u95ed\u624b\u6bb5\uff0c\u901a\u8fc7\u4fdd\u7559\u5168\u5c40\u8def\u7531\u5b58\u5728\uff0c\u540c\u65f6\u5229\u7528\u6df1\u5ea6\u5305\u68c0\u6d4b\u3001\u6fc0\u8fdb\u9650\u901f\u548c\u9009\u62e9\u6027\u534f\u8bae\u5c01\u9501\u9694\u79bb\u56fd\u5185\u7528\u6237\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u8fd9\u79cd\u591a\u5c42\u5ba1\u67e5\u57fa\u7840\u8bbe\u65bd\u5bf9\u89c4\u907f\u6280\u672f\u548c\u6570\u5b57\u6743\u5229\u76d1\u63a7\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.14334", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14334", "abs": "https://arxiv.org/abs/2507.14334", "authors": ["Hui Yang", "Jiaoyan Chen", "Yuan He", "Yongsheng Gao", "Ian Horrocks"], "title": "Language Models as Ontology Encoders", "comment": null, "summary": "OWL (Web Ontology Language) ontologies which are able to formally represent\ncomplex knowledge and support semantic reasoning have been widely adopted\nacross various domains such as healthcare and bioinformatics. Recently,\nontology embeddings have gained wide attention due to its potential to infer\nplausible new knowledge and approximate complex reasoning. However, existing\nmethods face notable limitations: geometric model-based embeddings typically\noverlook valuable textual information, resulting in suboptimal performance,\nwhile the approaches that incorporate text, which are often based on language\nmodels, fail to preserve the logical structure. In this work, we propose a new\nontology embedding method OnT, which tunes a Pretrained Language Model (PLM)\nvia geometric modeling in a hyperbolic space for effectively incorporating\ntextual labels and simultaneously preserving class hierarchies and other\nlogical relationships of Description Logic EL. Extensive experiments on four\nreal-world ontologies show that OnT consistently outperforms the baselines\nincluding the state-of-the-art across both tasks of prediction and inference of\naxioms. OnT also demonstrates strong potential in real-world applications,\nindicated by its robust transfer learning abilities and effectiveness in real\ncases of constructing a new ontology from SNOMED CT. Data and code are\navailable at https://github.com/HuiYang1997/OnT.", "AI": {"tldr": "OnT\u662f\u4e00\u79cd\u65b0\u7684\u672c\u4f53\u5d4c\u5165\u65b9\u6cd5\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u53cc\u66f2\u51e0\u4f55\u5efa\u6a21\uff0c\u6709\u6548\u6574\u5408\u6587\u672c\u5e76\u4fdd\u6301\u903b\u8f91\u7ed3\u6784\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u672c\u4f53\u5d4c\u5165\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u51e0\u4f55\u6a21\u578b\u5ffd\u7565\u6587\u672c\u4fe1\u606f\uff0c\u800c\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u6301\u903b\u8f91\u7ed3\u6784\u3002OnT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faOnT\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u66f2\u51e0\u4f55\u5efa\u6a21\u8c03\u6574\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\uff0c\u4ee5\u540c\u65f6\u6574\u5408\u6587\u672c\u6807\u7b7e\u5e76\u4fdd\u6301\u63cf\u8ff0\u903b\u8f91EL\u7684\u7c7b\u5c42\u6b21\u548c\u5176\u4ed6\u903b\u8f91\u5173\u7cfb\u3002", "result": "\u5728\u56db\u4e2a\u5b9e\u9645\u672c\u4f53\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOnT\u5728\u9884\u6d4b\u548c\u516c\u7406\u63a8\u7406\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "OnT\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u53cc\u66f2\u51e0\u4f55\u5efa\u6a21\uff0c\u6709\u6548\u6574\u5408\u4e86\u6587\u672c\u4fe1\u606f\u5e76\u4fdd\u6301\u4e86\u903b\u8f91\u7ed3\u6784\uff0c\u5728\u591a\u4e2a\u5b9e\u9645\u672c\u4f53\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14597", "categories": ["cs.DC", "cs.CV", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.14597", "abs": "https://arxiv.org/abs/2507.14597", "authors": ["Eugene Armah", "Linda Amoako Bannning"], "title": "Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning", "comment": null, "summary": "Processing data at high speeds is becoming increasingly critical as digital\neconomies generate enormous data. The current paradigms for timely data\nprocessing are edge computing and data stream processing (DSP). Edge computing\nplaces resources closer to where data is generated, while stream processing\nanalyzes the unbounded high-speed data in motion. However, edge stream\nprocessing faces rapid workload fluctuations, complicating resource\nprovisioning. Inadequate resource allocation leads to bottlenecks, whereas\nexcess allocation results in wastage. Existing reactive methods, such as\nthreshold-based policies and queuing theory scale only after performance\ndegrades, potentially violating SLAs. Although reinforcement learning (RL)\noffers a proactive approach through agents that learn optimal runtime\nadaptation policies, it requires extensive simulation. Furthermore, predictive\nmachine learning models face online distribution and concept drift that\nminimize their accuracy. We propose a three-step solution to the proactive edge\nstream processing autoscaling problem. Firstly, a GRU neural network forecasts\nthe upstream load using real-world and synthetic DSP datasets. Secondly, a\ntransfer learning framework integrates the predictive model into an online\nstream processing system using the DTW algorithm and joint distribution\nadaptation to handle the disparities between offline and online domains.\nFinally, a horizontal autoscaling module dynamically adjusts the degree of\noperator parallelism, based on predicted load while considering edge resource\nconstraints. The lightweight GRU model for load predictions recorded up to\n1.3\\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and\nProphet on the SMAPE and RMSE evaluation metrics, with lower training time than\nthe computationally intensive RL models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u6b65\u9aa4\u7684\u4e3b\u52a8\u8fb9\u7f18\u6d41\u5904\u7406\u81ea\u52a8\u6269\u5c55\u65b9\u6cd5\uff0c\u7ed3\u5408GRU\u795e\u7ecf\u7f51\u7edc\u548c\u8fc1\u79fb\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d1f\u8f7d\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u8d44\u6e90\u5206\u914d\u7684\u6548\u7387\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u7ecf\u6d4e\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9ad8\u901f\u6570\u636e\u5904\u7406\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u8fb9\u7f18\u8ba1\u7b97\u548c\u6570\u636e\u6d41\u5904\u7406\uff08DSP\uff09\u662f\u5f53\u524d\u7684\u4e3b\u8981\u8303\u5f0f\uff0c\u4f46\u9762\u4e34\u8d44\u6e90\u5206\u914d\u7684\u6311\u6218\u3002\u73b0\u6709\u53cd\u5e94\u5f0f\u65b9\u6cd5\u5728\u6027\u80fd\u4e0b\u964d\u540e\u624d\u8fdb\u884c\u6269\u5c55\uff0c\u53ef\u80fd\u8fdd\u53cdSLA\u3002\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u867d\u7136\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e3b\u52a8\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u5927\u91cf\u6a21\u62df\u3002\u6b64\u5916\uff0c\u9884\u6d4b\u6027\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9762\u4e34\u5728\u7ebf\u5206\u5e03\u548c\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\u3002", "method": "\u8bba\u6587\u91c7\u7528GRU\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4e0a\u6e38\u8d1f\u8f7d\u9884\u6d4b\uff0c\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff08\u4f7f\u7528DTW\u7b97\u6cd5\u548c\u8054\u5408\u5206\u5e03\u9002\u5e94\uff09\u6765\u5904\u7406\u79bb\u7ebf\u4e0e\u5728\u7ebf\u9886\u57df\u7684\u5dee\u5f02\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6c34\u5e73\u81ea\u52a8\u6269\u5c55\u6a21\u5757\u6765\u52a8\u6001\u8c03\u6574\u7b97\u5b50\u5e76\u884c\u5ea6\u3002", "result": "\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7GRU\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe1.3%\u7684SMAPE\u503c\uff0c\u4f18\u4e8eCNN\u3001ARIMA\u548cProphet\u6a21\u578b\uff0c\u5e76\u4e14\u5728SMAPE\u548cRMSE\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u8bad\u7ec3\u65f6\u95f4\u4e5f\u77ed\u4e8e\u8ba1\u7b97\u5bc6\u96c6\u7684RL\u6a21\u578b\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u6b65\u9aa4\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u4e3b\u52a8\u8fb9\u7f18\u6d41\u5904\u7406\u81ea\u52a8\u6269\u5c55\u95ee\u9898\uff0c\u5305\u62ecGRU\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u4e0a\u6e38\u8d1f\u8f7d\u3001\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\u5904\u7406\u79bb\u7ebf\u4e0e\u5728\u7ebf\u9886\u57df\u5dee\u5f02\uff0c\u4ee5\u53ca\u57fa\u4e8e\u9884\u6d4b\u8d1f\u8f7d\u7684\u6c34\u5e73\u81ea\u52a8\u6269\u5c55\u6a21\u5757\u3002\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8eCNN\u3001ARIMA\u548cProphet\u6a21\u578b\u3002"}}
{"id": "2507.14462", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2507.14462", "abs": "https://arxiv.org/abs/2507.14462", "authors": ["Xinpeng Jiang", "Haoyu Liu", "Siqiang Luo", "Xiaokui Xiao"], "title": "Tighter Lower Bounds for Single Source Personalized PageRank", "comment": "33 pages", "summary": "We study lower bounds for approximating the Single Source Personalized\nPageRank (SSPPR) query, which measures the probability distribution of an\n$\\alpha$-decay random walk starting from a source node $s$. Existing lower\nbounds remain loose-$\\Omega\\left(\\min(m, 1/\\delta)\\right)$ for relative error\n(SSPPR-R) and $\\Omega\\left(\\min(n, 1/\\epsilon)\\right)$ for additive error\n(SSPPR-A). To close this gap, we establish tighter bounds for both settings.\nFor SSPPR-R, we show a lower bound of $\\Omega\\left(\\min\\left(m,\n\\frac{\\log(1/\\delta)}{\\delta}\\right)\\right)$ for any $\\delta \\in (0,1)$. For\nSSPPR-A, we prove a lower bound of $\\Omega\\left(\\min\\left(m,\n\\frac{\\log(1/\\epsilon)}{\\epsilon}\\right)\\right)$ for any $\\epsilon \\in (0,1)$,\nassuming the graph has $m \\in \\mathcal{O}(n^{2-\\beta})$ edges for any\narbitrarily small constant $\\beta \\in (0,1)$.", "AI": {"conclusion": "\u672c\u6587\u4e3a\u5355\u6e90\u4e2a\u6027\u5316PageRank\uff08SSPPR\uff09\u67e5\u8be2\u7684\u8fd1\u4f3c\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u4e25\u683c\u7684\u4e0b\u754c\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u9488\u5bf9\u76f8\u5bf9\u8bef\u5dee\uff08SSPPR-R\uff09\u548c\u52a0\u6027\u8bef\u5dee\uff08SSPPR-A\uff09\u4e24\u79cd\u60c5\u51b5\uff0c\u5206\u522b\u5efa\u7acb\u4e86\u66f4\u7d27\u7684\u4e0b\u754c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e2d\u5bf9SSPPR\u67e5\u8be2\u7684\u4e0b\u754c\u8f83\u4e3a\u5bbd\u677e\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u786e\u7684\u4e0b\u754c\u5206\u6790\u3002", "result": "\u5bf9\u4e8eSSPPR-R\uff0c\u8bc1\u660e\u4e86$\\Omega\\left(\\min\\left(m, \\frac{\\log(1/\\delta)}{\\delta}\\right)\\right)$\u7684\u4e0b\u754c\uff1b\u5bf9\u4e8eSSPPR-A\uff0c\u8bc1\u660e\u4e86$\\Omega\\left(\\min\\left(m, \\frac{\\log(1/\\epsilon)}{\\epsilon}\\right)\\right)$\u7684\u4e0b\u754c\u3002", "tldr": "\u672c\u6587\u586b\u8865\u4e86SSPPR\u67e5\u8be2\u4e0b\u754c\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u9488\u5bf9\u76f8\u5bf9\u548c\u52a0\u6027\u8bef\u5dee\u5206\u522b\u63d0\u51fa\u4e86\u66f4\u4e25\u683c\u7684\u4e0b\u754c\u3002"}}
{"id": "2507.14330", "categories": ["cs.SE", "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2507.14330", "abs": "https://arxiv.org/abs/2507.14330", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "title": "Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects", "comment": "Submitted to Overlay2025 - 7th International Workshop on Artificial\n  Intelligence and fOrmal VERification, Logic, Automata, and sYnthesis. [under\n  review]", "summary": "Software correctness is ensured mathematically through formal verification,\nwhich involves the resources of generating formal requirement specifications\nand having an implementation that must be verified. Tools such as\nmodel-checkers and theorem provers ensure software correctness by verifying the\nimplementation against the specification. Formal methods deployment is\nregularly enforced in the development of safety-critical systems e.g.\naerospace, medical devices and autonomous systems. Generating these\nspecifications from informal and ambiguous natural language requirements\nremains the key challenge. Our project, VERIFAI^{1}, aims to investigate\nautomated and semi-automated approaches to bridge this gap, using techniques\nfrom Natural Language Processing (NLP), ontology-based domain modelling,\nartefact reuse, and large language models (LLMs). This position paper presents\na preliminary synthesis of relevant literature to identify recurring challenges\nand prospective research directions in the generation of verifiable\nspecifications from informal requirements.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5982\u4f55\u5229\u7528NLP\u7b49\u6280\u672f\u81ea\u52a8\u5316\u751f\u6210\u53ef\u9a8c\u8bc1\u8f6f\u4ef6\u89c4\u8303\uff0c\u63d0\u51faVERIFAI\u9879\u76ee\u4ee5\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u4ece\u975e\u6b63\u5f0f\u548c\u6a21\u7cca\u7684\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u751f\u6210\u6b63\u5f0f\u89c4\u8303\u8fd9\u4e00\u5173\u952e\u6311\u6218\uff0c\u4ee5\u63d0\u5347\u8f6f\u4ef6\u6b63\u786e\u6027\u9a8c\u8bc1\u7684\u6548\u7387\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u8bc6\u522b\u4e86\u4ece\u975e\u6b63\u5f0f\u9700\u6c42\u751f\u6210\u53ef\u9a8c\u8bc1\u89c4\u8303\u4e2d\u7684\u5e38\u89c1\u6311\u6218\u548c\u6f5c\u5728\u7814\u7a76\u65b9\u5411\u3002", "result": "\u63d0\u51fa\u4e86VERIFAI\u9879\u76ee\uff0c\u63a2\u7d22\u81ea\u52a8\u5316\u4e0e\u534a\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u586b\u8865\u975e\u6b63\u5f0f\u9700\u6c42\u4e0e\u53ef\u9a8c\u8bc1\u89c4\u8303\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u3001\u672c\u4f53\u8bba\u9886\u57df\u5efa\u6a21\u3001\u6784\u4ef6\u91cd\u7528\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7b49\u6280\u672f\uff0c\u81ea\u52a8\u5316\u6216\u534a\u81ea\u52a8\u5316\u5730\u5c06\u975e\u6b63\u5f0f\u9700\u6c42\u8f6c\u5316\u4e3a\u53ef\u9a8c\u8bc1\u89c4\u8303\u7684\u6311\u6218\u4e0e\u524d\u666f\u3002"}}
{"id": "2507.14841", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14841", "abs": "https://arxiv.org/abs/2507.14841", "authors": ["Xiang Tang", "Ruotong Li", "Xiaopeng Fan"], "title": "Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization", "comment": "15 pages, 8 figures, Project page: https://xdlbw.github.io/sing3d/", "summary": "In recent years, 3D generation has made great strides in both academia and\nindustry. However, generating 3D scenes from a single RGB image remains a\nsignificant challenge, as current approaches often struggle to ensure both\nobject generation quality and scene coherence in multi-object scenarios. To\novercome these limitations, we propose a novel three-stage framework for 3D\nscene generation with explicit geometric representations and high-quality\ntextural details via single image-guided model generation and spatial layout\noptimization. Our method begins with an image instance segmentation and\ninpainting phase, which recovers missing details of occluded objects in the\ninput images, thereby achieving complete generation of foreground 3D assets.\nSubsequently, our approach captures the spatial geometry of reference image by\nconstructing pseudo-stereo viewpoint for camera parameter estimation and scene\ndepth inference, while employing a model selection strategy to ensure optimal\nalignment between the 3D assets generated in the previous step and the input.\nFinally, through model parameterization and minimization of the Chamfer\ndistance between point clouds in 3D and 2D space, our approach optimizes layout\nparameters to produce an explicit 3D scene representation that maintains\nprecise alignment with input guidance image. Extensive experiments on\nmulti-object scene image sets have demonstrated that our approach not only\noutperforms state-of-the-art methods in terms of geometric accuracy and texture\nfidelity of individual generated 3D models, but also has significant advantages\nin scene layout synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u50cf\u4fee\u590d\u3001\u51e0\u4f55\u6355\u6349\u4e0e\u5e03\u5c40\u4f18\u5316\uff0c\u5b9e\u73b0\u5355\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf3D\u573a\u666f\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u56fe\u50cf\u751f\u62103D\u573a\u666f\u65f6\u96be\u4ee5\u517c\u987e\u7269\u4f53\u751f\u6210\u8d28\u91cf\u4e0e\u573a\u666f\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u56fe\u50cf\u5b9e\u4f8b\u5206\u5272\u4e0e\u4fee\u590d\u3001\u4f2a\u7acb\u4f53\u89c6\u89d2\u6784\u5efa\u53ca\u76f8\u673a\u53c2\u6570\u4f30\u8ba1\u3001\u6a21\u578b\u53c2\u6570\u5316\u4e0e\u70b9\u4e91Chamfer\u8ddd\u79bb\u6700\u5c0f\u5316\u4e09\u9636\u6bb5\u6d41\u7a0b\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u573a\u666f\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u7269\u4f53\u573a\u666f\u56fe\u50cf\u96c6\u4e0a\u51e0\u4f55\u7cbe\u5ea6\u548c\u7eb9\u7406\u4fdd\u771f\u5ea6\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u573a\u666f\u5e03\u5c40\u5408\u6210\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u4e09\u9636\u6bb5\u6846\u67b6\u5728\u5355\u5f20RGB\u56fe\u50cf\u5f15\u5bfc\u76843D\u573a\u666f\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u7eb9\u7406\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u573a\u666f\u5e03\u5c40\u5408\u6210\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.14186", "categories": ["cs.NI", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14186", "abs": "https://arxiv.org/abs/2507.14186", "authors": ["Xiaojie Li", "Zhijie Cai", "Nan Qi", "Chao Dong", "Guangxu Zhu", "Haixia Ma", "Qihui Wu", "Shi Jin"], "title": "A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction", "comment": "This paper has been submitted to IEEE for possible publication", "summary": "The expansion of the low-altitude economy has underscored the significance of\nLow-Altitude Network Coverage (LANC) prediction for designing aerial corridors.\nWhile accurate LANC forecasting hinges on the antenna beam patterns of Base\nStations (BSs), these patterns are typically proprietary and not readily\naccessible. Operational parameters of BSs, which inherently contain beam\ninformation, offer an opportunity for data-driven low-altitude coverage\nprediction. However, collecting extensive low-altitude road test data is\ncost-prohibitive, often yielding only sparse samples per BS. This scarcity\nresults in two primary challenges: imbalanced feature sampling due to limited\nvariability in high-dimensional operational parameters against the backdrop of\nsubstantial changes in low-dimensional sampling locations, and diminished\ngeneralizability stemming from insufficient data samples. To overcome these\nobstacles, we introduce a dual strategy comprising expert knowledge-based\nfeature compression and disentangled representation learning. The former\nreduces feature space complexity by leveraging communications expertise, while\nthe latter enhances model generalizability through the integration of\npropagation models and distinct subnetworks that capture and aggregate the\nsemantic representations of latent features. Experimental evaluation confirms\nthe efficacy of our framework, yielding a 7% reduction in error compared to the\nbest baseline algorithm. Real-network validations further attest to its\nreliability, achieving practical prediction accuracy with MAE errors at the 5dB\nlevel.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u7279\u5f81\u538b\u7f29\u548c\u89e3\u8026\u8868\u793a\u5b66\u4e60\u7684\u53cc\u7b56\u7565\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4f4e\u7a7a\u7f51\u7edc\u8986\u76d6\u9884\u6d4b\u4e2d\u7684\u6570\u636e\u7a00\u758f\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f4e\u7a7a\u7f51\u7edc\u8986\u76d6\uff08LANC\uff09\u9884\u6d4b\u5bf9\u8bbe\u8ba1\u7a7a\u4e2d\u8d70\u5eca\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u57fa\u7ad9\uff08BS\uff09\u7684\u5929\u7ebf\u6ce2\u675f\u6a21\u5f0f\u901a\u5e38\u662f\u4e13\u6709\u7684\u4e14\u4e0d\u6613\u83b7\u53d6\u3002BS\u7684\u64cd\u4f5c\u53c2\u6570\u867d\u5305\u542b\u6ce2\u675f\u4fe1\u606f\uff0c\u4f46\u6536\u96c6\u5927\u91cf\u4f4e\u7a7a\u8def\u6d4b\u6570\u636e\u6210\u672c\u9ad8\u6602\uff0c\u5bfc\u81f4\u6bcf\u4e2aBS\u7684\u6837\u672c\u7a00\u758f\uff0c\u8fdb\u800c\u9762\u4e34\u7279\u5f81\u91c7\u6837\u4e0d\u5e73\u8861\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7b56\u7565\uff0c\u5305\u62ec\u57fa\u4e8e\u4e13\u5bb6\u77e5\u8bc6\u7684\u7279\u5f81\u538b\u7f29\u548c\u89e3\u8026\u8868\u793a\u5b66\u4e60\u3002\u524d\u8005\u901a\u8fc7\u901a\u4fe1\u4e13\u4e1a\u77e5\u8bc6\u51cf\u5c11\u7279\u5f81\u7a7a\u95f4\u590d\u6742\u5ea6\uff0c\u540e\u8005\u901a\u8fc7\u6574\u5408\u4f20\u64ad\u6a21\u578b\u548c\u6355\u6349\u5e76\u805a\u5408\u6f5c\u5728\u7279\u5f81\u8bed\u4e49\u8868\u793a\u7684\u72ec\u7acb\u5b50\u7f51\u7edc\u6765\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bef\u5dee\u51cf\u5c11\u4e867%\uff0c\u5b9e\u9645\u7f51\u7edc\u9a8c\u8bc1\u4e2dMAE\u8bef\u5dee\u8fbe\u52305dB\u7ea7\u522b\u3002", "conclusion": "\u5b9e\u9a8c\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u7b97\u6cd5\u8bef\u5dee\u51cf\u5c11\u4e867%\u3002\u5b9e\u9645\u7f51\u7edc\u9a8c\u8bc1\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5176\u53ef\u9760\u6027\uff0c\u5b9e\u73b0\u4e865dB\u7ea7\u522b\u7684MAE\u8bef\u5dee\uff0c\u8fbe\u5230\u4e86\u5b9e\u7528\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2507.14335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14335", "abs": "https://arxiv.org/abs/2507.14335", "authors": ["Nicolas Wischermann", "Claudio Mayrink Verdun", "Gabriel Poesia", "Francesco Noseda"], "title": "ProofCompass: Enhancing Specialized Provers with LLM Guidance", "comment": "19 pages, 7 figures. Accepted at the 2nd AI for MATH Workshop at the\n  42nd International Conference on Machine Learning (ICML 2025)", "summary": "Language models have become increasingly powerful tools for formal\nmathematical reasoning. However, most existing approaches rely exclusively on\neither large general-purpose models or smaller specialized models, each with\ndistinct limitations, while training specialized large models still requires\nsignificant computational resources. This paper introduces ProofCompass, a\nnovel hybrid methodology that achieves remarkable computational efficiency by\nstrategically guiding existing specialized prover methods, such as\nDeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without\nrequiring additional model training. The LLM provides natural language proof\nstrategies and analyzes failed attempts to select intermediate lemmas, enabling\neffective problem decomposition. On the miniF2F benchmark, ProofCompass\ndemonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\\%\n\\rightarrow 55.3\\%$) while using 25x fewer attempts ($3200 \\rightarrow 128$).\nOur synergistic approach paves the way for simultaneously improving\ncomputational efficiency and accuracy in formal theorem proving.", "AI": {"tldr": "ProofCompass\u901a\u8fc7LLM\u4e0e\u4e13\u7528\u8bc1\u660e\u5668\u7684\u534f\u540c\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u5b9a\u7406\u8bc1\u660e\u51c6\u786e\u6027\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u7684\u5927\u578b\u901a\u7528\u6a21\u578b\u6216\u5c0f\u578b\u4e13\u7528\u6a21\u578b\uff0c\u5404\u6709\u5c40\u9650\u6027\uff0c\u800c\u8bad\u7ec3\u5927\u578b\u4e13\u7528\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u5f15\u5165ProofCompass\uff0c\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6307\u5bfc\u4e13\u7528\u8bc1\u660e\u5668\uff08\u5982DSP-v1.5\uff09\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6a21\u578b\u3002LLM\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\u7b56\u7565\u5e76\u5206\u6790\u5931\u8d25\u5c1d\u8bd5\u4ee5\u9009\u62e9\u4e2d\u95f4\u5f15\u7406\u3002", "result": "\u5728miniF2F\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProofCompass\u4ee525\u500d\u66f4\u5c11\u7684\u5c1d\u8bd5\uff083200\u2192128\uff09\u8d85\u8d8a\u4e86DSP-v1.5\u7684\u6027\u80fd\uff0854.9%\u219255.3%\uff09\u3002", "conclusion": "ProofCompass\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4e13\u7528\u8bc1\u660e\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.14723", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.14723", "abs": "https://arxiv.org/abs/2507.14723", "authors": ["Brati Mondal", "Pritam Goswami", "Buddhadeb Sau"], "title": "Simulating Chirality: Solving Distance-$k$-Dispersion on an 1-Interval Connected Ring", "comment": null, "summary": "We study the Distance-$k$-Dispersion (D-$k$-D) problem for synchronous mobile\nagents in a 1-interval-connected ring network having $n$ nodes and with $l$\nagents where $3 \\le l \\le \\lfloor \\frac{n}{k}\\rfloor$, without the assumption\nof chirality (a common sense of direction for the agents). This generalizes the\nclassical dispersion problem by requiring that agents maintain a minimum\ndistance of $k$ hops from each other, with the special case $k=1$ corresponding\nto the standard dispersion.\n  The contribution in this work is threefold. Our first contribution is a novel\nmethod that enables agents to simulate chirality using only local information,\nvision and bounded memory. This technique demonstrates that chirality is not a\nfundamental requirement for coordination in this model.\n  Building on this, our second contribution partially resolves an open question\nposed by Agarwalla et al. (ICDCN, 2018), who considered the same model (1-\ninterval connected ring, synchronous agents, no chirality). We prove that\nD-$k$-D, and thus dispersion is solvable from any arbitrary configuration under\nthese assumptions (excluding vertex permutation dynamism)for any size of the\nring network which was earlier limited to only odd sized ring or to a ring of\nsize four.\n  Finally, we present an algorithm for D-$k$-D in this setting that works in\n$O(ln)$ rounds, completing the constructive side of our result.\n  Altogether, our findings significantly extend the theoretical understanding\nof mobile agent coordination in dynamic networks and clarify the role of\nchirality in distributed computation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u65e0\u624b\u6027\u5047\u8bbe\u4e0b\u73af\u5f62\u7f51\u7edc\u4e2d\u540c\u6b65\u79fb\u52a8\u4ee3\u7406\u7684Distance-$k$-Dispersion\u95ee\u9898\uff0c\u63d0\u51fa\u6a21\u62df\u624b\u6027\u7684\u65b9\u6cd5\u548c\u9ad8\u6548\u7b97\u6cd5\uff0c\u6269\u5c55\u4e86\u7406\u8bba\u7406\u89e3\u3002", "motivation": "\u7814\u7a76\u57281-\u95f4\u9694\u8fde\u63a5\u7684\u73af\u5f62\u7f51\u7edc\u4e2d\uff0c\u65e0\u624b\u6027\u5047\u8bbe\u4e0b\uff0c\u540c\u6b65\u79fb\u52a8\u4ee3\u7406\u7684Distance-$k$-Dispersion\u95ee\u9898\uff0c\u4ee5\u63a8\u5e7f\u7ecf\u5178\u7684\u5206\u6563\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u4ec5\u4f7f\u7528\u672c\u5730\u4fe1\u606f\u3001\u89c6\u89c9\u548c\u6709\u9650\u5185\u5b58\u6a21\u62df\u624b\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u5728O(ln)\u8f6e\u5185\u89e3\u51b3D-$k$-D\u95ee\u9898\u3002", "result": "\u8bc1\u660e\u4e86D-$k$-D\u548c\u5206\u6563\u95ee\u9898\u5728\u4efb\u4f55\u73af\u5f62\u7f51\u7edc\u5927\u5c0f\u4e0b\u5747\u53ef\u89e3\uff08\u4e0d\u5305\u62ec\u9876\u70b9\u7f6e\u6362\u52a8\u6001\u6027\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2aO(ln)\u8f6e\u7684\u7b97\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u663e\u8457\u6269\u5c55\u4e86\u5bf9\u52a8\u6001\u7f51\u7edc\u4e2d\u79fb\u52a8\u4ee3\u7406\u534f\u8c03\u7684\u7406\u8bba\u7406\u89e3\uff0c\u5e76\u9610\u660e\u4e86\u624b\u6027\u5728\u5206\u5e03\u5f0f\u8ba1\u7b97\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2507.14504", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.14504", "abs": "https://arxiv.org/abs/2507.14504", "authors": ["Junqiang Peng", "Zimo Sheng", "Mingyu Xiao"], "title": "New Algorithms for #2-SAT and #3-SAT", "comment": "Accepted by IJCAI 2025", "summary": "The #2-SAT and #3-SAT problems involve counting the number of satisfying\nassignments (also called models) for instances of 2-SAT and 3-SAT,\nrespectively. In 2010, Zhou et al. proposed an $\\mathcal{O}^*(1.1892^m)$-time\nalgorithm for #2-SAT and an efficient approach for #3-SAT, where $m$ denotes\nthe number of clauses. In this paper, we show that the weighted versions of\n#2-SAT and #3-SAT can be solved in $\\mathcal{O}^*(1.1082^m)$ and\n$\\mathcal{O}^*(1.4423^m)$ time, respectively. These results directly apply to\nthe unweighted cases and achieve substantial improvements over the previous\nresults. These advancements are enabled by the introduction of novel reduction\nrules, a refined analysis of branching operations, and the application of path\ndecompositions on the primal and dual graphs of the formula.", "AI": {"conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u52a0\u6743\u7248\u672c\u7684#2-SAT\u548c#3-SAT\u95ee\u9898\u53ef\u4ee5\u5728$\\mathcal{O}^*(1.1082^m)$\u548c$\\mathcal{O}^*(1.4423^m)$\u65f6\u95f4\u5185\u89e3\u51b3\uff0c\u8fd9\u4e9b\u7ed3\u679c\u76f4\u63a5\u9002\u7528\u4e8e\u975e\u52a0\u6743\u60c5\u51b5\uff0c\u5e76\u663e\u8457\u6539\u8fdb\u4e86\u4e4b\u524d\u7684\u7ed3\u679c\u3002", "method": "\u5f15\u5165\u4e86\u65b0\u9896\u7684\u5f52\u7ea6\u89c4\u5219\u3001\u5bf9\u5206\u652f\u64cd\u4f5c\u7684\u7cbe\u7ec6\u5206\u6790\uff0c\u4ee5\u53ca\u5728\u516c\u5f0f\u7684\u539f\u59cb\u56fe\u548c\u53cc\u56fe\u4e0a\u5e94\u7528\u8def\u5f84\u5206\u89e3\u3002", "motivation": "\u89e3\u51b3\u52a0\u6743\u7248\u672c\u7684#2-SAT\u548c#3-SAT\u95ee\u9898\uff0c\u5e76\u6539\u8fdb\u4e4b\u524d\u7684\u7ed3\u679c\u3002", "result": "\u52a0\u6743#2-SAT\u548c#3-SAT\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u5206\u522b\u4e3a$\\mathcal{O}^*(1.1082^m)$\u548c$\\mathcal{O}^*(1.4423^m)$\u3002", "tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u52a0\u6743#2-SAT\u548c#3-SAT\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u590d\u6742\u5ea6\u3002"}}
{"id": "2507.14396", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14396", "abs": "https://arxiv.org/abs/2507.14396", "authors": ["Carey Lai Zheng Hui", "Johnson Britto Jessia Esther Leena", "Kumuthini Subramanian", "Zhao Chenyu", "Shubham Rajeshkumar Jariwala"], "title": "Developing Shared Vocabulary System For Collaborative Software Engineering", "comment": "16 pages, including appendix", "summary": "Effective communication is a critical factor in successful software\nengineering collaboration. However, communication gaps remain a persistent\nchallenge, often leading to misunderstandings, inefficiencies, and defects.\nThis research investigates the technical factors contributing to such\nmisunderstandings and explores the measurable benefits of establishing shared\nvocabulary systems within software documentation and codebases. Using a Design\nScience Research (DSR) framework, the study was structured into three iterative\nphases: problem identification, method development, and empirical validation.\nThe problem identification phase involved thematic analysis of communication\ndata and semi-structured interviews, revealing key factors such as ambiguous\nmessaging, misalignment in documentation, inconsistent code review feedback,\nand API integration miscommunication. Grounded Theory principles were employed\nto design a structured methodology for collaborative vocabulary development.\nEmpirical validation through controlled experiments demonstrated that while\ninitial adoption introduced overhead, the shared vocabulary system\nsignificantly improved information density, documentation clarity, and\ncollaboration efficiency over time. Findings offer actionable insights for\nimproving communication practices in software engineering, while also\nidentifying limitations and directions for future research.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5171\u4eab\u8bcd\u6c47\u7cfb\u7edf\u80fd\u6709\u6548\u6539\u5584\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u6c9f\u901a\u95ee\u9898\uff0c\u5c3d\u7ba1\u521d\u671f\u6709\u5f00\u9500\uff0c\u4f46\u957f\u671f\u6548\u76ca\u663e\u8457\u3002", "motivation": "\u8f6f\u4ef6\u5de5\u7a0b\u534f\u4f5c\u4e2d\u6c9f\u901a\u4e0d\u7545\u662f\u666e\u904d\u95ee\u9898\uff0c\u5e38\u5bfc\u81f4\u8bef\u89e3\u3001\u4f4e\u6548\u548c\u7f3a\u9677\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u6280\u672f\u56e0\u7d20\u5bfc\u81f4\u7684\u6c9f\u901a\u8bef\u89e3\uff0c\u5e76\u8bc4\u4f30\u5171\u4eab\u8bcd\u6c47\u7cfb\u7edf\u5728\u8f6f\u4ef6\u6587\u6863\u548c\u4ee3\u7801\u5e93\u4e2d\u7684\u53ef\u91cf\u5316\u6548\u76ca\u3002", "method": "\u7814\u7a76\u91c7\u7528\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\uff08DSR\uff09\u6846\u67b6\uff0c\u5206\u4e3a\u4e09\u4e2a\u8fed\u4ee3\u9636\u6bb5\uff1a\u95ee\u9898\u8bc6\u522b\u3001\u65b9\u6cd5\u5f00\u53d1\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u3002\u95ee\u9898\u8bc6\u522b\u9636\u6bb5\u901a\u8fc7\u4e3b\u9898\u5206\u6790\u548c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u63ed\u793a\u5173\u952e\u56e0\u7d20\uff0c\u4f7f\u7528\u624e\u6839\u7406\u8bba\u539f\u5219\u8bbe\u8ba1\u534f\u4f5c\u8bcd\u6c47\u5f00\u53d1\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0c\u5171\u4eab\u8bcd\u6c47\u7cfb\u7edf\u867d\u7136\u521d\u671f\u5f15\u5165\u989d\u5916\u5f00\u9500\uff0c\u4f46\u957f\u671f\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u606f\u5bc6\u5ea6\u3001\u6587\u6863\u6e05\u6670\u5ea6\u548c\u534f\u4f5c\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5171\u4eab\u8bcd\u6c47\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u4fe1\u606f\u5bc6\u5ea6\u3001\u6587\u6863\u6e05\u6670\u5ea6\u548c\u534f\u4f5c\u6548\u7387\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u6c9f\u901a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u53ef\u884c\u5efa\u8bae\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2507.14920", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.14920", "abs": "https://arxiv.org/abs/2507.14920", "authors": ["Evandro S. Ortigossa", "F\u00e1bio F. Dias", "Diego C. Nascimento", "Luis Gustavo Nonato"], "title": "Time Series Information Visualization -- A Review of Approaches and Tools", "comment": "Preprint. Under review", "summary": "Time series data are prevalent across various domains and often encompass\nlarge datasets containing multiple time-dependent features in each sample.\nExploring time-varying data is critical for data science practitioners aiming\nto understand dynamic behaviors and discover periodic patterns and trends.\nHowever, the analysis of such data often requires sophisticated procedures and\ntools. Information visualization is a communication channel that leverages\nhuman perceptual abilities to transform abstract data into visual\nrepresentations. Visualization techniques have been successfully applied in the\ncontext of time series to enhance interpretability by graphically representing\nthe temporal evolution of data. The challenge for information visualization\ndevelopers lies in integrating a wide range of analytical tools into rich\nvisualization systems that can summarize complex datasets while clearly\ndescribing the impacts of the temporal component. Such systems enable data\nscientists to turn raw data into understandable and potentially useful\nknowledge. This review examines techniques and approaches designed for handling\ntime series data, guiding users through knowledge discovery processes based on\nvisual analysis. We also provide readers with theoretical insights and design\nguidelines for considering when developing comprehensive information\nvisualization approaches for time series, with a particular focus on time\nseries with multiple features. As a result, we highlight the challenges and\nfuture research directions to address open questions in the visualization of\ntime-dependent data.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u53ef\u89c6\u5316\u6280\u672f\uff0c\u5f3a\u8c03\u4e86\u591a\u7279\u5f81\u65f6\u95f4\u5e8f\u5217\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u666e\u904d\u5b58\u5728\u4e8e\u591a\u4e2a\u9886\u57df\uff0c\u5206\u6790\u8fd9\u4e9b\u6570\u636e\u9700\u8981\u590d\u6742\u7684\u5de5\u5177\u548c\u6d41\u7a0b\u3002\u53ef\u89c6\u5316\u6280\u672f\u53ef\u4ee5\u589e\u5f3a\u6570\u636e\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e2e\u52a9\u6570\u636e\u79d1\u5b66\u5bb6\u7406\u89e3\u52a8\u6001\u884c\u4e3a\u548c\u53d1\u73b0\u6a21\u5f0f\u3002", "method": "\u7efc\u8ff0\u4e86\u73b0\u6709\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u53ef\u89c6\u5316\u6280\u672f\u548c\u5de5\u5177\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u89c1\u89e3\u548c\u8bbe\u8ba1\u6307\u5357\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u53ef\u89c6\u5316\u6280\u672f\u7684\u5168\u9762\u56de\u987e\uff0c\u5f3a\u8c03\u4e86\u591a\u7279\u5f81\u65f6\u95f4\u5e8f\u5217\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u53ef\u89c6\u5316\u6280\u672f\u7684\u73b0\u72b6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u7279\u522b\u662f\u9488\u5bf9\u591a\u7279\u5f81\u65f6\u95f4\u5e8f\u5217\u7684\u6311\u6218\u3002"}}
{"id": "2507.14188", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14188", "abs": "https://arxiv.org/abs/2507.14188", "authors": ["Sebastian Barros Elgueta"], "title": "From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade Direct-to-Device Mobile Networks", "comment": "50 pages", "summary": "In 2023, satellite and mobile networks crossed a historic threshold: standard\nsmartphones, using unmodified 3GPP protocols, connected directly to low Earth\norbit (LEO) satellites. This first wave of direct-to-device (D2D)\ndemonstrations validated the physical feasibility of satellite-based mobile\naccess. However, these systems remain fallback-grade--rural-only,\nbandwidth-limited, and fully dependent on Earth-based mobile cores for\nidentity, session, and policy control. This paper asks a more ambitious\nquestion: Can a complete mobile network, including radio access, core\nfunctions, traffic routing, and content delivery, operate entirely from orbit?\nAnd can it deliver sustained, urban-grade service in the world's densest\ncities? We present the first end-to-end system architecture for a fully orbital\ntelco, integrating electronically steered phased arrays with 1000-beam\ncapacity, space-based deployment of 5G core functions (UPF, AMF), and\ninter-satellite laser mesh backhaul. We analyze spectral efficiency, beam\ncapacity, and link budgets under dense urban conditions, accounting for path\nloss, Doppler, and multipath. Simulations show that rooftop and line-of-sight\nusers can sustain 64-QAM throughput, while street-level access is feasible with\nrelay or assisted beam modes. The paper outlines the remaining constraints,\npower, thermal dissipation, compute radiation hardening, and regulatory models,\nand demonstrates that these are engineering bottlenecks, not physical limits.\nFinally, we propose a staged 15-year roadmap from today's fallback D2D systems\nto autonomous orbital overlays delivering 50-100 Mbps to handhelds in\nmegacities, with zero reliance on terrestrial infrastructure.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u57fa\u4e8e\u8f68\u9053\u7684\u79fb\u52a8\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u5728\u5bc6\u96c6\u57ce\u5e02\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u89c4\u5212\u4e8615\u5e74\u8def\u7ebf\u56fe\u4ee5\u5b9e\u73b0\u57ce\u5e02\u7ea7\u624b\u6301\u8bbe\u5907\u7684\u9ad8\u5e26\u5bbd\u8fde\u63a5\u3002", "motivation": "\u63a2\u8ba8\u4e00\u4e2a\u66f4\u5b8f\u5927\u7684\u95ee\u9898\uff1a\u5b8c\u6574\u7684\u79fb\u52a8\u7f51\u7edc\uff08\u5305\u62ec\u65e0\u7ebf\u63a5\u5165\u3001\u6838\u5fc3\u529f\u80fd\u3001\u6d41\u91cf\u8def\u7531\u548c\u5185\u5bb9\u4ea4\u4ed8\uff09\u80fd\u5426\u5b8c\u5168\u5728\u8f68\u9053\u4e0a\u8fd0\u884c\uff0c\u5e76\u5728\u5168\u7403\u6700\u5bc6\u96c6\u7684\u57ce\u5e02\u4e2d\u63d0\u4f9b\u6301\u7eed\u7684\u57ce\u5e02\u7ea7\u670d\u52a1\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u57fa\u4e8e\u8f68\u9053\u7684\u79fb\u52a8\u7f51\u7edc\u7cfb\u7edf\u67b6\u6784\uff0c\u5305\u62ec\u7535\u5b50\u63a7\u5236\u76f8\u63a7\u9635\u30015G\u6838\u5fc3\u529f\u80fd\u7684\u7a7a\u95f4\u90e8\u7f72\uff08UPF\u3001AMF\uff09\u548c\u536b\u661f\u95f4\u6fc0\u5149\u7f51\u72b6\u56de\u7a0b\u3002\u901a\u8fc7\u5206\u6790\u9891\u8c31\u6548\u7387\u3001\u6ce2\u675f\u5bb9\u91cf\u548c\u94fe\u8def\u9884\u7b97\uff0c\u6a21\u62df\u4e86\u5bc6\u96c6\u57ce\u5e02\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u5c4b\u9876\u548c\u89c6\u8ddd\u7528\u6237\u53ef\u7ef4\u630164-QAM\u541e\u5410\u91cf\uff0c\u800c\u8857\u9053\u7ea7\u63a5\u5165\u53ef\u901a\u8fc7\u4e2d\u7ee7\u6216\u8f85\u52a9\u6ce2\u675f\u6a21\u5f0f\u5b9e\u73b0\u3002\u8bba\u6587\u8fd8\u6307\u51fa\u4e86\u5269\u4f59\u7684\u5de5\u7a0b\u74f6\u9888\uff08\u5982\u529f\u7387\u3001\u70ed\u8017\u6563\u3001\u8ba1\u7b97\u8f90\u5c04\u786c\u5316\u548c\u76d1\u7ba1\u6a21\u578b\uff09\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a15\u5e74\u8def\u7ebf\u56fe\uff0c\u4ece\u5f53\u524d\u7684\u5907\u7528D2D\u7cfb\u7edf\u9010\u6b65\u8fc7\u6e21\u5230\u5b8c\u5168\u81ea\u4e3b\u7684\u8f68\u9053\u8986\u76d6\u7f51\u7edc\uff0c\u6700\u7ec8\u5b9e\u73b0\u57ce\u5e02\u7ea7\u624b\u6301\u8bbe\u590750-100 Mbps\u7684\u5e26\u5bbd\uff0c\u65e0\u9700\u4f9d\u8d56\u5730\u9762\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2507.14393", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14393", "abs": "https://arxiv.org/abs/2507.14393", "authors": ["Humza Sami", "Mubashir ul Islam", "Pierre-Emmanuel Gaillardon", "Valerio Tenace"], "title": "Adaptive Multi-Agent Reasoning via Automated Workflow Generation", "comment": null, "summary": "The rise of Large Reasoning Models (LRMs) promises a significant leap forward\nin language model capabilities, aiming to tackle increasingly sophisticated\ntasks with unprecedented efficiency and accuracy. However, despite their\nimpressive performance, recent studies have highlighted how current reasoning\nmodels frequently fail to generalize to novel, unseen problems, often resorting\nto memorized solutions rather than genuine inferential reasoning. Such behavior\nunderscores a critical limitation in modern LRMs, i.e., their tendency toward\noverfitting, which in turn results in poor generalization in problem-solving\ncapabilities.\n  In this paper, we introduce Nexus Architect, an enhanced iteration of our\nmulti-agent system framework, Nexus, equipped with a novel automated workflow\nsynthesis mechanism. Given a user's prompt and a small set of representative\nexamples, the Architect autonomously generates a tailored reasoning workflow by\nselecting suitable strategies, tool integrations, and adversarial techniques\nfor a specific problem class. Furthermore, the Architect includes an iterative\nprompt refinement mechanism that fine-tunes agents' system prompts to maximize\nperformance and improve the generalization capabilities of the system.\n  We empirically evaluate Nexus Architect by employing an off-the-shelf,\nnon-reasoning model on a custom dataset of challenging logical questions and\ncompare its performance against state-of-the-art LRMs. Results show that Nexus\nArchitect consistently outperforms existing solutions, achieving up to a 66%\nincrease in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\\times$ against\nClaude Sonnet 4 and DeepSeek-R1, and over 3$\\times$ w.r.t. Llama 4 Scout.", "AI": {"tldr": "Nexus Architect\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u548c\u63d0\u793a\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u8bc1\u8868\u73b0\u4f18\u4e8e\u73b0\u6709LRMs\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u6cdb\u5316\u5230\u65b0\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5e38\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u771f\u6b63\u63a8\u7406\uff0c\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\u3002", "method": "\u5f15\u5165Nexus Architect\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u914d\u5907\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u5408\u6210\u548c\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u673a\u5236\uff0c\u4ee5\u751f\u6210\u5b9a\u5236\u5316\u7684\u63a8\u7406\u5de5\u4f5c\u6d41\u3002", "result": "Nexus Architect\u5728\u903b\u8f91\u95ee\u9898\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4Gemini 2.5 Flash Preview\u63d0\u5347\u4e8666%\uff0c\u5bf9Claude Sonnet 4\u548cDeepSeek-R1\u63d0\u5347\u4e86\u7ea62.5\u500d\uff0c\u5bf9Llama 4 Scout\u63d0\u5347\u4e863\u500d\u4ee5\u4e0a\u3002", "conclusion": "Nexus Architect\u901a\u8fc7\u5176\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u5408\u6210\u548c\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u8bc1\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684LRMs\u3002"}}
{"id": "2507.14802", "categories": ["cs.DC", "cs.AI", "C.2.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.14802", "abs": "https://arxiv.org/abs/2507.14802", "authors": ["Ziming Dai", "Chao Qiu", "Fei Gao", "Yunfeng Zhao", "Xiaofei Wang"], "title": "ACME: Adaptive Customization of Large Models via Distributed Systems", "comment": "Accepted to IEEE ICDCS 2025. 11 pages, 13 figures", "summary": "Pre-trained Transformer-based large models have revolutionized personal\nvirtual assistants, but their deployment in cloud environments faces challenges\nrelated to data privacy and response latency. Deploying large models closer to\nthe data and users has become a key research area to address these issues.\nHowever, applying these models directly often entails significant difficulties,\nsuch as model mismatching, resource constraints, and energy inefficiency.\nAutomated design of customized models is necessary, but it faces three key\nchallenges, namely, the high cost of centralized model customization,\nimbalanced performance from user heterogeneity, and suboptimal performance from\ndata heterogeneity. In this paper, we propose ACME, an adaptive customization\napproach of Transformer-based large models via distributed systems. To avoid\nthe low cost-efficiency of centralized methods, ACME employs a bidirectional\nsingle-loop distributed system to progressively achieve fine-grained\ncollaborative model customization. In order to better match user heterogeneity,\nit begins by customizing the backbone generation and identifying the Pareto\nFront under model size constraints to ensure optimal resource utilization.\nSubsequently, it performs header generation and refines the model using data\ndistribution-based personalized architecture aggregation to match data\nheterogeneity. Evaluation on different datasets shows that ACME achieves\ncost-efficient models under model size constraints. Compared to centralized\nsystems, data transmission volume is reduced to 6 percent. Additionally, the\naverage accuracy improves by 10 percent compared to the baseline, with the\ntrade-off metrics increasing by nearly 30 percent.", "AI": {"tldr": "ACME \u662f\u4e00\u79cd\u5206\u5e03\u5f0f Transformer \u5927\u6a21\u578b\u5b9a\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411\u534f\u4f5c\u4f18\u5316\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u548c\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u8f93\u91cf\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4e91\u7aef\u90e8\u7f72\u5927\u6a21\u578b\u65f6\u9762\u4e34\u7684\u6570\u636e\u9690\u79c1\u3001\u54cd\u5e94\u5ef6\u8fdf\u3001\u6a21\u578b\u4e0d\u5339\u914d\u3001\u8d44\u6e90\u9650\u5236\u548c\u80fd\u6548\u4f4e\u4e0b\u7b49\u95ee\u9898\u3002", "method": "ACME \u91c7\u7528\u53cc\u5411\u5355\u5faa\u73af\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u9010\u6b65\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u534f\u4f5c\u6a21\u578b\u5b9a\u5236\uff0c\u5305\u62ec\u4e3b\u5e72\u751f\u6210\u3001Pareto \u524d\u6cbf\u8bc6\u522b\u3001\u5934\u90e8\u751f\u6210\u548c\u6570\u636e\u5206\u5e03\u9a71\u52a8\u7684\u4e2a\u6027\u5316\u67b6\u6784\u805a\u5408\u3002", "result": "\u5728\u6a21\u578b\u5927\u5c0f\u9650\u5236\u4e0b\uff0cACME \u5b9e\u73b0\u4e86\u6210\u672c\u9ad8\u6548\u7684\u6a21\u578b\u5b9a\u5236\uff0c\u6570\u636e\u4f20\u8f93\u91cf\u964d\u81f3 6%\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347 10%\uff0c\u6743\u8861\u6307\u6807\u63d0\u5347\u8fd1 30%\u3002", "conclusion": "ACME \u901a\u8fc7\u5206\u5e03\u5f0f\u7cfb\u7edf\u5b9e\u73b0\u4e86 Transformer \u5927\u6a21\u578b\u7684\u81ea\u9002\u5e94\u5b9a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6570\u636e\u4f20\u8f93\u91cf\u5e76\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.14509", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.14509", "abs": "https://arxiv.org/abs/2507.14509", "authors": ["Sheikh Shakil Akhtar", "Jayakrishnan Madathil", "Pranabendu Misra", "Geevarghese Philip"], "title": "Addressing Bias in Algorithmic Solutions: Exploring Vertex Cover and Feedback Vertex Set", "comment": null, "summary": "A typical goal of research in combinatorial optimization is to come up with\nfast algorithms that find optimal solutions to a computational problem. The\nprocess that takes a real-world problem and extracts a clean mathematical\nabstraction of it often throws out a lot of \"side information\" which is deemed\nirrelevant. However, the discarded information could be of real significance to\nthe end-user of the algorithm's output. All solutions of the same cost are not\nnecessarily of equal impact in the real-world; some solutions may be much more\ndesirable than others, even at the expense of additional increase in cost. If\nthe impact, positive or negative, is mostly felt by some specific (minority)\nsubgroups of the population, the population at large will be largely unaware of\nit. In this work we ask the question of finding solutions to combinatorial\noptimization problems that are \"unbiased\" with respect to a collection of\nspecified subgroups of the total population.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u7ec4\u5408\u4f18\u5316\u7b97\u6cd5\uff0c\u65e8\u5728\u751f\u6210\u5bf9\u7279\u5b9a\u5b50\u7fa4\u4f53\u516c\u5e73\u7684\u89e3\uff0c\u5f25\u8865\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u793e\u4f1a\u516c\u5e73\u6027\u7684\u7f3a\u9677\u3002", "motivation": "\u4f20\u7edf\u7684\u7ec4\u5408\u4f18\u5316\u7b97\u6cd5\u5728\u63d0\u53d6\u6570\u5b66\u62bd\u8c61\u65f6\u4e22\u5f03\u4e86\u5927\u91cf\u201c\u8fb9\u7f18\u4fe1\u606f\u201d\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u606f\u53ef\u80fd\u5bf9\u67d0\u4e9b\u5b50\u7fa4\u4f53\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u786e\u4fdd\u7b97\u6cd5\u8f93\u51fa\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u516c\u5e73\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8003\u8651\u8fd9\u4e9b\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u548c\u6574\u5408\u4f20\u7edf\u4f18\u5316\u8fc7\u7a0b\u4e2d\u88ab\u5ffd\u7565\u7684\u201c\u8fb9\u7f18\u4fe1\u606f\u201d\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u4ee5\u786e\u4fdd\u89e3\u5728\u4e0d\u540c\u5b50\u7fa4\u4f53\u95f4\u7684\u516c\u5e73\u5206\u914d\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u751f\u6210\u5bf9\u6307\u5b9a\u5b50\u7fa4\u4f53\u65e0\u504f\u89e3\u7684\u65b0\u7b97\u6cd5\uff0c\u8fd9\u4e9b\u89e3\u5728\u4fdd\u6301\u6210\u672c\u6548\u76ca\u7684\u540c\u65f6\uff0c\u66f4\u7b26\u5408\u793e\u4f1a\u516c\u5e73\u6027\u539f\u5219\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u5bfb\u627e\u5bf9\u7279\u5b9a\u5b50\u7fa4\u4f53\u201c\u65e0\u504f\u201d\u89e3\u7684\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u5728\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u8003\u8651\u793e\u4f1a\u516c\u5e73\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.14423", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14423", "abs": "https://arxiv.org/abs/2507.14423", "authors": ["Mootez Saad", "Hao Li", "Tushar Sharma", "Ahmed E. Hassan"], "title": "On the Effect of Token Merging on Pre-trained Models for Code", "comment": null, "summary": "Tokenization is a fundamental component of language models for code. It\ninvolves breaking down the input into units that are later passed to the\nlanguage model stack to learn high-dimensional representations used in various\ncontexts, from classification to generation. However, the output of these\ntokenizers is often longer than that traditionally used in compilers and\ninterpreters. This could result in undesirable effects, such as increased\ncomputational overhead. In this work, we investigate the effect of merging the\nhidden representations of subtokens that belong to the same semantic unit, such\nas subtokens that form a single identifier. We propose two strategies: one\nbased on averaging the representations and another that leverages a\nlearning-based approach. Both methods can be seamlessly integrated with\nexisting language models for code. We conduct experiments using six language\nmodels for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),\nand CodeT5+ (770M), across three software engineering tasks: vulnerability\ndetection, code classification, and code translation. Results show that these\nstrategies can reduce the number of floating-point operations by $1\\%$ to\n$19\\%$. Regarding downstream performance, the most significant degradation was\nobserved in the vulnerability detection task, where the F1 score decreased by\n$1.82$ points compared to the baseline. In contrast, for code translation, we\nobserved an improvement of $2.47$ points in CodeBLEU. This work contributes to\nthe broader effort of improving language models for code across multiple\ndimensions, including both computational efficiency and downstream performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u5408\u5e76\u4ee3\u7801\u8bed\u4e49\u5355\u5143\u9690\u85cf\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff081%-19%\uff09\uff0c\u5e76\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u6f0f\u6d1e\u68c0\u6d4b\u7565\u6709\u4e0b\u964d\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u7684tokenizer\u8f93\u51fa\u957f\u5ea6\u8d85\u8fc7\u7f16\u8bd1\u5668/\u89e3\u91ca\u5668\u7684\u6807\u51c6\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5408\u5e76\u8bed\u4e49\u5355\u5143\u7684\u9690\u85cf\u8868\u793a\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u7b56\u7565\uff1a1) \u57fa\u4e8e\u5e73\u5747\u8868\u793a\u7684\u65b9\u6cd5\uff1b2) \u5b66\u4e60\u578b\u65b9\u6cd5\u3002\u5b9e\u9a8c\u4f7f\u7528\u516d\u79cd\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\uff08CodeBERT\u3001GraphCodeBERT\u7b49\uff09\u5728\u4e09\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff08\u6f0f\u6d1e\u68c0\u6d4b\u3001\u4ee3\u7801\u5206\u7c7b\u3001\u4ee3\u7801\u7ffb\u8bd1\uff09\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6d6e\u70b9\u8fd0\u7b97\u51cf\u5c111%-19%\uff1b\u6f0f\u6d1e\u68c0\u6d4b\u4efb\u52a1\u7684F1\u5206\u6570\u4e0b\u964d1.82\u5206\uff0c\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u7684CodeBLEU\u63d0\u53472.47\u5206\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5408\u5e76\u5c5e\u4e8e\u540c\u4e00\u8bed\u4e49\u5355\u5143\u7684\u9690\u85cf\u8868\u793a\uff0c\u63d0\u51fa\u4e24\u79cd\u7b56\u7565\uff08\u57fa\u4e8e\u5e73\u5747\u8868\u793a\u548c\u5b66\u4e60\u65b9\u6cd5\uff09\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6d6e\u70b9\u8fd0\u7b97\u6b21\u6570\uff081%-19%\uff09\uff0c\u5e76\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\uff08CodeBLEU\u63d0\u9ad82.47\u5206\uff09\uff0c\u4e3a\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.15186", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.15186", "abs": "https://arxiv.org/abs/2507.15186", "authors": ["Dmitry Brodsky", "Benjamin Watson"], "title": "Model Simplification through refinement", "comment": null, "summary": "As modeling and visualization applications proliferate, there arises a need\nto simplify large polygonal models at interactive rates. Unfortunately existing\npolygon mesh simplification algorithms are not well suited for this task\nbecause they are either too slow (requiring the simplified model to be\npre-computed) or produce models that are too poor in quality. These\nshortcomings become particularly acute when models are extremely large. We\npresent an algorithm suitable for simplification of large models at interactive\nspeeds. The algorithm is fast and can guarantee displayable results within a\ngiven time limit. Results also have good quality. Inspired by splitting\nalgorithms from vector quantization literature, we simplify models in reverse,\nbeginning with an extremely coarse approximation and refining it.\nApproximations of surface curvature guide the simplification process.\nPreviously produced simplifications can be further refined by using them as\ninput to the algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u9ad8\u8d28\u91cf\u7684\u4ea4\u4e92\u5f0f\u5927\u578b\u591a\u8fb9\u5f62\u6a21\u578b\u7b80\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u9006\u5411\u7b80\u5316\u548c\u66f2\u7387\u6307\u5bfc\u5b9e\u73b0\u9ad8\u6548\u5904\u7406\u3002", "motivation": "\u968f\u7740\u5efa\u6a21\u548c\u53ef\u89c6\u5316\u5e94\u7528\u7684\u666e\u53ca\uff0c\u9700\u8981\u5bf9\u5927\u578b\u591a\u8fb9\u5f62\u6a21\u578b\u8fdb\u884c\u4ea4\u4e92\u5f0f\u7b80\u5316\u3002\u73b0\u6709\u7b97\u6cd5\u8981\u4e48\u901f\u5ea6\u8fc7\u6162\uff0c\u8981\u4e48\u751f\u6210\u7684\u8d28\u91cf\u8f83\u5dee\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u6781\u5927\u6a21\u578b\u65f6\u3002", "method": "\u53d7\u77e2\u91cf\u91cf\u5316\u6587\u732e\u4e2d\u7684\u5206\u5272\u7b97\u6cd5\u542f\u53d1\uff0c\u8be5\u7b97\u6cd5\u9006\u5411\u7b80\u5316\u6a21\u578b\uff0c\u4ece\u6781\u5176\u7c97\u7cd9\u7684\u8fd1\u4f3c\u5f00\u59cb\u5e76\u9010\u6b65\u7ec6\u5316\u3002\u8868\u9762\u66f2\u7387\u7684\u8fd1\u4f3c\u6307\u5bfc\u4e86\u7b80\u5316\u8fc7\u7a0b\u3002", "result": "\u7b97\u6cd5\u80fd\u591f\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7b80\u5316\u6a21\u578b\uff0c\u5e76\u652f\u6301\u8fdb\u4e00\u6b65\u7ec6\u5316\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5927\u578b\u6a21\u578b\u4ea4\u4e92\u5f0f\u7b80\u5316\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4e0d\u4ec5\u5feb\u901f\uff0c\u8fd8\u80fd\u5728\u7ed9\u5b9a\u65f6\u95f4\u9650\u5236\u5185\u4fdd\u8bc1\u53ef\u663e\u793a\u7684\u7ed3\u679c\uff0c\u4e14\u7ed3\u679c\u8d28\u91cf\u826f\u597d\u3002"}}
{"id": "2507.14199", "categories": ["cs.NI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.14199", "abs": "https://arxiv.org/abs/2507.14199", "authors": ["Ebrahim Abu-Helalah", "Jordi Serra", "Jordi Perez-Romero"], "title": "On Splitting Lightweight Semantic Image Segmentation for Wireless Communications", "comment": "IEEE International Mediterranean Conference on Communications and\n  Networking", "summary": "Semantic communication represents a promising technique towards reducing\ncommunication costs, especially when dealing with image segmentation, but it\nstill lacks a balance between computational efficiency and bandwidth\nrequirements while maintaining high image segmentation accuracy, particularly\nin resource-limited environments and changing channel conditions. On the other\nhand, the more complex and larger semantic image segmentation models become,\nthe more stressed the devices are when processing data. This paper proposes a\nnovel approach to implementing semantic communication based on splitting the\nsemantic image segmentation process between a resource constrained transmitter\nand the receiver. This allows saving bandwidth by reducing the transmitted data\nwhile maintaining the accuracy of the semantic image segmentation.\nAdditionally, it reduces the computational requirements at the resource\nconstrained transmitter compared to doing all the semantic image segmentation\nin the transmitter. The proposed approach is evaluated by means of\nsimulation-based experiments in terms of different metrics such as\ncomputational resource usage, required bit rate and segmentation accuracy. The\nresults when comparing the proposal with the full semantic image segmentation\nin the transmitter show that up to 72% of the bit rate was reduced in the\ntransmission process. In addition, the computational load of the transmitter is\nreduced by more than 19%. This reflects the interest of this technique for its\napplication in communication systems, particularly in the upcoming 6G systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u53d1\u9001\u7aef\u548c\u63a5\u6536\u7aef\u95f4\u5206\u5272\u8bed\u4e49\u56fe\u50cf\u5206\u5272\u8fc7\u7a0b\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5e26\u5bbd\u9700\u6c42\u548c\u53d1\u9001\u7aef\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u9002\u7528\u4e8e6G\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u901a\u4fe1\u6280\u672f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u548c\u53d8\u5316\u4fe1\u9053\u6761\u4ef6\u4e0b\u96be\u4ee5\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u3001\u5e26\u5bbd\u9700\u6c42\u4e0e\u56fe\u50cf\u5206\u5272\u7cbe\u5ea6\uff0c\u4e14\u590d\u6742\u6a21\u578b\u589e\u52a0\u4e86\u8bbe\u5907\u5904\u7406\u6570\u636e\u7684\u538b\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u4e49\u901a\u4fe1\u5b9e\u73b0\u65b9\u6cd5\uff0c\u5c06\u8bed\u4e49\u56fe\u50cf\u5206\u5272\u8fc7\u7a0b\u5206\u5272\u5728\u8d44\u6e90\u53d7\u9650\u7684\u53d1\u9001\u7aef\u548c\u63a5\u6536\u7aef\u4e4b\u95f4\uff0c\u4ee5\u51cf\u5c11\u4f20\u8f93\u6570\u636e\u91cf\u5e76\u964d\u4f4e\u53d1\u9001\u7aef\u7684\u8ba1\u7b97\u9700\u6c42\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u53d1\u9001\u7aef\u5b8c\u5168\u5904\u7406\u8bed\u4e49\u56fe\u50cf\u5206\u5272\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e8672%\u7684\u6bd4\u7279\u7387\u548c19%\u4ee5\u4e0a\u7684\u53d1\u9001\u7aef\u8ba1\u7b97\u8d1f\u8f7d\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u6709\u6548\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u5e26\u5bbd\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u56fe\u50cf\u5206\u5272\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u672a\u67656G\u901a\u4fe1\u7cfb\u7edf\u3002"}}
{"id": "2507.14406", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14406", "abs": "https://arxiv.org/abs/2507.14406", "authors": ["Michael J. Zellinger", "Matt Thomson"], "title": "Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering", "comment": "8 pages, 5 figures", "summary": "State-of-the-art reasoning LLMs are powerful problem solvers, but they still\noccasionally make mistakes. However, adopting AI models in risk-sensitive\ndomains often requires error rates near 0%. To address this gap, we propose\ncollaboration between a reasoning model and a human expert who resolves queries\nthe model cannot confidently answer. We find that quantifying the uncertainty\nof a reasoning model through the length of its reasoning trace yields an\neffective basis for deferral to a human, e.g., cutting the error rate of Qwen3\n235B-A22B on difficult MATH problems from 3% to less than 1% when deferring\n7.5% of queries. However, the high latency of reasoning models still makes them\nchallenging to deploy on use cases with high query volume. To address this\nchallenge, we explore fronting a reasoning model with a large non-reasoning\nmodel. We call this modified human-in-the-loop system \"Fail Fast, or Ask\",\nsince the non-reasoning model may defer difficult queries to the human expert\ndirectly (\"failing fast\"), without incurring the reasoning model's higher\nlatency. We show that this approach yields around 40% latency reduction and\nabout 50% cost savings for DeepSeek R1 while maintaining 90+% area under the\naccuracy-rejection curve. However, we observe that latency savings are lower\nthan expected because of \"latency drag\", the phenomenon that processing easier\nqueries with a non-reasoning model pushes the reasoning model's latency\ndistribution towards longer latencies. Broadly, our results suggest that the\ndeficiencies of state-of-the-art reasoning models -- nontrivial error rates and\nhigh latency -- can be substantially mitigated through black-box systems\nengineering, without requiring access to LLM internals.", "AI": {"tldr": "\u901a\u8fc7\u201c\u5feb\u901f\u5931\u8d25\u6216\u8be2\u95ee\u201d\u7cfb\u7edf\u7ed3\u5408\u63a8\u7406\u6a21\u578b\u548c\u4eba\u7c7b\u4e13\u5bb6\uff0c\u663e\u8457\u964d\u4f4e\u9519\u8bef\u7387\u548c\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u9ad8\u6548\u534f\u4f5c\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u63a8\u7406LLMs\u5728\u98ce\u9669\u654f\u611f\u9886\u57df\u5e94\u7528\u65f6\u4ecd\u5b58\u5728\u9519\u8bef\u7387\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u9700\u8981\u5c06\u9519\u8bef\u7387\u964d\u81f3\u63a5\u8fd10%\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u63a8\u7406\u6a21\u578b\u548c\u975e\u63a8\u7406\u6a21\u578b\u7684\u534f\u4f5c\u7cfb\u7edf\uff08\u201c\u5feb\u901f\u5931\u8d25\u6216\u8be2\u95ee\u201d\uff09\uff0c\u901a\u8fc7\u91cf\u5316\u63a8\u7406\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\uff08\u57fa\u4e8e\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\uff09\u5b9e\u73b0\u4eba\u7c7b\u4e13\u5bb6\u7684\u5ef6\u8fdf\u51b3\u7b56\uff0c\u5e76\u63a2\u7d22\u975e\u63a8\u7406\u6a21\u578b\u524d\u7f6e\u4ee5\u964d\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "result": "\u7cfb\u7edf\u5c06Qwen3 235B-A22B\u5728\u56f0\u96beMATH\u95ee\u9898\u4e0a\u7684\u9519\u8bef\u7387\u4ece3%\u964d\u81f31%\u4ee5\u4e0b\uff08\u5ef6\u8fdf7.5%\u67e5\u8be2\uff09\uff0c\u5e76\u5b9e\u73b0\u7ea640%\u5ef6\u8fdf\u964d\u4f4e\u548c50%\u6210\u672c\u8282\u7701\uff08DeepSeek R1\uff09\uff0c\u540c\u65f6\u4fdd\u630190+%\u51c6\u786e\u7387-\u62d2\u7edd\u66f2\u7ebf\u4e0b\u9762\u79ef\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u9ed1\u76d2\u7cfb\u7edf\u5de5\u7a0b\uff08\u5982\u201c\u5feb\u901f\u5931\u8d25\u6216\u8be2\u95ee\u201d\u7cfb\u7edf\uff09\u53ef\u4ee5\u663e\u8457\u7f13\u89e3\u6700\u5148\u8fdb\u63a8\u7406\u6a21\u578b\u7684\u7f3a\u9677\uff08\u9519\u8bef\u7387\u548c\u5ef6\u8fdf\u95ee\u9898\uff09\uff0c\u800c\u65e0\u9700\u8bbf\u95eeLLM\u5185\u90e8\u3002"}}
{"id": "2507.14928", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14928", "abs": "https://arxiv.org/abs/2507.14928", "authors": ["Yongrae Jo", "Chanik Park"], "title": "Byzantine-Robust Decentralized Coordination of LLM Agents", "comment": null, "summary": "Collaboration among multiple large language model (LLM) agents is a promising\napproach to overcome inherent limitations of single-agent systems, such as\nhallucinations and single points of failure. As LLM agents are increasingly\ndeployed on open blockchain platforms, multi-agent systems capable of\ntolerating malicious (Byzantine) agents have become essential.\n  Recent Byzantine-robust multi-agent systems typically rely on leader-driven\ncoordination, which suffers from two major drawbacks. First, they are\ninherently vulnerable to targeted attacks against the leader. If consecutive\nleaders behave maliciously, the system repeatedly fails to achieve consensus,\nforcing new consensus rounds, which is particularly costly given the high\nlatency of LLM invocations. Second, an underperforming proposal from the leader\ncan be accepted as the final answer even when higher-quality alternatives are\navailable, as existing methods finalize the leader's proposal once it receives\na quorum of votes.\n  To address these issues, we propose DecentLLMs, a novel decentralized\nconsensus approach for multi-agent LLM systems, where worker agents generate\nanswers concurrently and evaluator agents independently score and rank these\nanswers to select the best available one. This decentralized architecture\nenables faster consensus despite the presence of Byzantine agents and\nconsistently selects higher-quality answers through Byzantine-robust\naggregation techniques.\n  Experimental results demonstrate that DecentLLMs effectively tolerates\nByzantine agents and significantly improves the quality of selected answers.", "AI": {"tldr": "DecentLLMs \u662f\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u5171\u8bc6\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53 LLM \u7cfb\u7edf\u4e2d\u7684\u9886\u5bfc\u8005\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7b54\u6848\u8d28\u91cf\u5e76\u5bb9\u5fcd\u62dc\u5360\u5ead\u4ee3\u7406\u3002", "motivation": "\u73b0\u6709\u62dc\u5360\u5ead\u9c81\u68d2\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f9d\u8d56\u9886\u5bfc\u8005\u534f\u8c03\uff0c\u6613\u53d7\u9488\u5bf9\u6027\u653b\u51fb\u4e14\u53ef\u80fd\u63a5\u53d7\u4f4e\u8d28\u91cf\u63d0\u6848\u3002", "method": "\u63d0\u51fa DecentLLMs\uff0c\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u5171\u8bc6\u65b9\u6cd5\uff0c\u5176\u4e2d\u5de5\u4f5c\u4ee3\u7406\u5e76\u884c\u751f\u6210\u7b54\u6848\uff0c\u8bc4\u4f30\u4ee3\u7406\u72ec\u7acb\u8bc4\u5206\u548c\u6392\u5e8f\u4ee5\u9009\u62e9\u6700\u4f73\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDecentLLMs \u80fd\u6709\u6548\u5bb9\u5fcd\u62dc\u5360\u5ead\u4ee3\u7406\u5e76\u663e\u8457\u63d0\u5347\u6240\u9009\u7b54\u6848\u8d28\u91cf\u3002", "conclusion": "DecentLLMs \u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u5171\u8bc6\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53 LLM \u7cfb\u7edf\u4e2d\u7684\u9886\u5bfc\u8005\u4f9d\u8d56\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b54\u6848\u8d28\u91cf\uff0c\u5e76\u80fd\u591f\u5bb9\u5fcd\u62dc\u5360\u5ead\u4ee3\u7406\u3002"}}
{"id": "2507.14569", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.14569", "abs": "https://arxiv.org/abs/2507.14569", "authors": ["Yonatan Nakar", "Dana Ron"], "title": "Characterizing and Testing Configuration Stability in Two-Dimensional Threshold Cellular Automata", "comment": null, "summary": "We consider the problems of characterizing and testing the stability of\ncellular automata configurations that evolve on a two-dimensional torus\naccording to threshold rules with respect to the von-Neumann neighborhood.\nWhile stable configurations for Threshold-1 (OR) and Threshold-5 (AND) are\ntrivial (and hence easily testable), the other threshold rules exhibit much\nmore diverse behaviors. We first characterize the structure of stable\nconfigurations with respect to the Threshold-2 (similarly, Threshold-4) and\nThreshold-3 (Majority) rules. We then design and analyze a testing algorithm\nthat distinguishes between configurations that are stable with respect to the\nThreshold-2 rule, and those that are $\\epsilon$-far from any stable\nconfiguration, where the query complexity of the algorithm is independent of\nthe size of the configuration and depends quadratically on $1/\\epsilon$.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u4e8c\u7ef4\u73af\u9762\u4e0a\u57fa\u4e8e\u9608\u503c\u89c4\u5219\u7684\u7ec6\u80de\u81ea\u52a8\u673a\u7a33\u5b9a\u914d\u7f6e\u7684\u7ed3\u6784\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u6d4b\u8bd5\u7b97\u6cd5\uff0c\u67e5\u8be2\u590d\u6742\u5ea6\u72ec\u7acb\u4e8e\u914d\u7f6e\u5927\u5c0f\uff0c\u4ec5\u4e0e1/\u03f5\u5e73\u65b9\u76f8\u5173\u3002", "motivation": "\u7814\u7a76\u4e8c\u7ef4\u73af\u9762\u4e0a\u57fa\u4e8e\u9608\u503c\u89c4\u5219\u7684\u7ec6\u80de\u81ea\u52a8\u673a\u914d\u7f6e\u7684\u7a33\u5b9a\u6027\u548c\u6d4b\u8bd5\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9Threshold-2\u548cThreshold-3\u89c4\u5219\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u884c\u4e3a\u4e0a\u6bd4Threshold-1\u548cThreshold-5\u66f4\u590d\u6742\u591a\u6837\u3002", "method": "\u8bba\u6587\u9996\u5148\u5206\u6790\u4e86Threshold-2\uff08\u7c7b\u4f3cThreshold-4\uff09\u548cThreshold-3\uff08\u591a\u6570\uff09\u89c4\u5219\u4e0b\u7684\u7a33\u5b9a\u914d\u7f6e\u7ed3\u6784\uff0c\u968f\u540e\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6d4b\u8bd5\u7b97\u6cd5\u6765\u533a\u5206\u7a33\u5b9a\u914d\u7f6e\u4e0e\u03f5-\u8fdc\u79bb\u7a33\u5b9a\u7684\u914d\u7f6e\u3002", "result": "\u8bba\u6587\u4e0d\u4ec5\u5bf9\u7a33\u5b9a\u914d\u7f6e\u8fdb\u884c\u4e86\u7ed3\u6784\u5206\u6790\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u6d4b\u8bd5\u7b97\u6cd5\uff0c\u5176\u67e5\u8be2\u590d\u6742\u5ea6\u4e0e\u914d\u7f6e\u5927\u5c0f\u65e0\u5173\uff0c\u4ec5\u4f9d\u8d56\u4e8e1/\u03f5\u7684\u5e73\u65b9\u3002", "conclusion": "\u8bba\u6587\u9996\u5148\u5bf9\u4e8c\u7ef4\u73af\u9762\u4e0a\u57fa\u4e8e\u51af\u00b7\u8bfa\u4f9d\u66fc\u90bb\u57df\u7684\u9608\u503c\u89c4\u5219\u7684\u7a33\u5b9a\u914d\u7f6e\u8fdb\u884c\u4e86\u7ed3\u6784\u5206\u6790\uff0c\u7136\u540e\u8bbe\u8ba1\u5e76\u5206\u6790\u4e86\u4e00\u4e2a\u6d4b\u8bd5\u7b97\u6cd5\uff0c\u80fd\u591f\u533a\u5206\u7a33\u5b9a\u914d\u7f6e\u4e0e\u8fdc\u79bb\u7a33\u5b9a\u7684\u914d\u7f6e\uff0c\u4e14\u7b97\u6cd5\u7684\u67e5\u8be2\u590d\u6742\u5ea6\u4e0e\u914d\u7f6e\u5927\u5c0f\u65e0\u5173\uff0c\u4ec5\u4e0e1/\u03f5\u7684\u5e73\u65b9\u76f8\u5173\u3002"}}
{"id": "2507.14547", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14547", "abs": "https://arxiv.org/abs/2507.14547", "authors": ["Noman Ahmad", "Ruoyu Su", "Matteo Esposito", "Andrea Janes", "Valentina Lenarduzzi", "Davide Taibi"], "title": "Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches", "comment": null, "summary": "Architectural degradation, also known as erosion, decay, or aging, impacts\nsystem quality, maintainability, and adaptability. Although widely\nacknowledged, current literature shows fragmented definitions, metrics, and\nremediation strategies. Our study aims to unify understanding of architectural\ndegradation by identifying its definitions, causes, metrics, tools, and\nremediation approaches across academic and gray literature. We conducted a\nmultivocal literature review of 108 studies extracting definitions, causes,\nmetrics, measurement approaches, tools, and remediation strategies. We\ndeveloped a taxonomy encompassing architectural, code, and process debt to\nexplore definition evolution, methodological trends, and research gaps.\nArchitectural degradation has shifted from a low-level issue to a\nsocio-technical concern. Definitions now address code violations, design drift,\nand structural decay. Causes fall under architectural (e.g., poor\ndocumentation), code (e.g., hasty fixes), and process debt (e.g., knowledge\nloss). We identified 54 metrics and 31 measurement techniques, focused on\nsmells, cohesion/coupling, and evolution. Yet, most tools detect issues but\nrarely support ongoing or preventive remediation. Degradation is both technical\nand organizational. While detection is well-studied, continuous remediation\nremains lacking. Our study reveals missed integration between metrics, tools,\nand repair logic, urging holistic, proactive strategies for sustainable\narchitecture.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u7edf\u4e00\u4e86\u67b6\u6784\u9000\u5316\u7684\u5b9a\u4e49\u3001\u539f\u56e0\u3001\u6307\u6807\u548c\u4fee\u590d\u7b56\u7565\uff0c\u53d1\u73b0\u5f53\u524d\u5de5\u5177\u5728\u6301\u7eed\u4fee\u590d\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u547c\u5401\u91c7\u53d6\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u6587\u732e\u5bf9\u67b6\u6784\u9000\u5316\u7684\u5b9a\u4e49\u3001\u6307\u6807\u548c\u4fee\u590d\u7b56\u7565\u5b58\u5728\u788e\u7247\u5316\u73b0\u8c61\uff0c\u7814\u7a76\u65e8\u5728\u7edf\u4e00\u5bf9\u67b6\u6784\u9000\u5316\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u591a\u58f0\u90e8\u6587\u732e\u7efc\u8ff0\uff08MLR\uff09\u5206\u6790\u4e86108\u9879\u7814\u7a76\uff0c\u63d0\u53d6\u4e86\u5b9a\u4e49\u3001\u539f\u56e0\u3001\u6307\u6807\u3001\u6d4b\u91cf\u65b9\u6cd5\u3001\u5de5\u5177\u548c\u4fee\u590d\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u6db5\u76d6\u67b6\u6784\u3001\u4ee3\u7801\u548c\u8fc7\u7a0b\u503a\u52a1\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u67b6\u6784\u9000\u5316\u5df2\u4ece\u4f4e\u5c42\u6b21\u95ee\u9898\u6f14\u53d8\u4e3a\u793e\u4f1a\u6280\u672f\u95ee\u9898\uff0c\u5b9a\u4e49\u4e8654\u79cd\u6307\u6807\u548c31\u79cd\u6d4b\u91cf\u6280\u672f\uff0c\u4f46\u5927\u591a\u6570\u5de5\u5177\u4ec5\u80fd\u68c0\u6d4b\u95ee\u9898\u800c\u7f3a\u4e4f\u6301\u7eed\u6216\u9884\u9632\u6027\u4fee\u590d\u652f\u6301\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u67b6\u6784\u9000\u5316\u95ee\u9898\u5728\u6280\u672f\u548c\u793e\u4f1a\u5c42\u9762\u7684\u590d\u6742\u6027\uff0c\u5f3a\u8c03\u4e86\u6301\u7eed\u4fee\u590d\u7b56\u7565\u7684\u7f3a\u5931\uff0c\u5e76\u547c\u5401\u91c7\u53d6\u6574\u4f53\u3001\u4e3b\u52a8\u7684\u65b9\u6cd5\u6765\u7ef4\u6301\u53ef\u6301\u7eed\u7684\u67b6\u6784\u3002"}}
{"id": "2507.15399", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15399", "abs": "https://arxiv.org/abs/2507.15399", "authors": ["Etai Sella", "Noam Atia", "Ron Mokady", "Hadar Averbuch-Elor"], "title": "Blended Point Cloud Diffusion for Localized Text-guided Shape Editing", "comment": "Accepted to ICCV 2025. Project Page:\n  https://tau-vailab.github.io/BlendedPC/", "summary": "Natural language offers a highly intuitive interface for enabling localized\nfine-grained edits of 3D shapes. However, prior works face challenges in\npreserving global coherence while locally modifying the input 3D shape. In this\nwork, we introduce an inpainting-based framework for editing shapes represented\nas point clouds. Our approach leverages foundation 3D diffusion models for\nachieving localized shape edits, adding structural guidance in the form of a\npartial conditional shape, ensuring that other regions correctly preserve the\nshape's identity. Furthermore, to encourage identity preservation also within\nthe local edited region, we propose an inference-time coordinate blending\nalgorithm which balances reconstruction of the full shape with inpainting at a\nprogression of noise levels during the inference process. Our coordinate\nblending algorithm seamlessly blends the original shape with its edited\nversion, enabling a fine-grained editing of 3D shapes, all while circumventing\nthe need for computationally expensive and often inaccurate inversion.\nExtensive experiments show that our method outperforms alternative techniques\nacross a wide range of metrics that evaluate both fidelity to the original\nshape and also adherence to the textual description.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u6269\u6563\u6a21\u578b\u548c\u5750\u6807\u6df7\u5408\u7b97\u6cd5\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e863D\u5f62\u72b6\u7684\u5c40\u90e8\u7cbe\u7ec6\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u57283D\u5f62\u72b6\u5c40\u90e8\u7f16\u8f91\u65f6\u96be\u4ee5\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4fee\u590d\u7684\u6846\u67b6\uff0c\u5229\u7528\u57fa\u78403D\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5c40\u90e8\u5f62\u72b6\u7f16\u8f91\uff0c\u5e76\u901a\u8fc7\u90e8\u5206\u6761\u4ef6\u5f62\u72b6\u63d0\u4f9b\u7ed3\u6784\u6307\u5bfc\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u63a8\u7406\u65f6\u5750\u6807\u6df7\u5408\u7b97\u6cd5\uff0c\u5e73\u8861\u5b8c\u6574\u5f62\u72b6\u91cd\u5efa\u4e0e\u4fee\u590d\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u539f\u59cb\u5f62\u72b6\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u63cf\u8ff0\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u54083D\u6269\u6563\u6a21\u578b\u548c\u5750\u6807\u6df7\u5408\u7b97\u6cd5\uff0c\u5728\u4fdd\u63013D\u5f62\u72b6\u5168\u5c40\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5c40\u90e8\u7cbe\u7ec6\u7f16\u8f91\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2507.14205", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.14205", "abs": "https://arxiv.org/abs/2507.14205", "authors": ["Pavel Malinovskiy"], "title": "A Fault-Tolerant Architecture for Urban and Rural Digital Connectivity: Synergizing SDWMN, Direct-to-Mobile Broadcasting, and Hybrid Cloud Streaming", "comment": null, "summary": "We propose an integrated architecture combining Software-Defined Wireless\nMesh Networks (SDWMN), Direct-to-Mobile (D2M) broadcasting, and Kafka-based\nhybrid cloud streaming to improve wireless network performance in both urban\nand rural settings. The approach addresses urban congestion and rural digital\nexclusion through traffic offloading, enhanced fault tolerance, and equitable\nresource allocation. We model urban congestion $\\rho_u = \\lambda_t / \\mu_c$ and\nrural coverage deficit $\\delta_r = 1 - C_r / C_{req}$, and aim to minimize\nglobal performance loss $GPL = w_1 \\cdot \\rho_u + w_2 \\cdot \\delta_r + w_3\n\\cdot T_{rec}$, where $T_{rec}$ is recovery time. Experiments in Bangkok,\nMumbai, and rural Finland demonstrate latency reduction over 32%, bandwidth\noffloading of 40%, rural coverage gain of 28%, and fairness index rising from\n0.78 to 0.91. The system achieves recovery under 10 s using SDWMN and Kafka. We\nrecommend optimal spectrum allocation $\\alpha_s$, targeted subsidies, and\ndevice mandates to promote adoption. This scalable, fault-tolerant design\nsupports equitable digital transformation and suggests directions for future\nresearch.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210SDWMN\u3001D2M\u5e7f\u64ad\u548cKafka\u7684\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57ce\u5e02\u548c\u519c\u6751\u7684\u65e0\u7ebf\u7f51\u7edc\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u5ef6\u8fdf\u51cf\u5c11\u3001\u5e26\u5bbd\u5378\u8f7d\u548c\u8986\u76d6\u589e\u76ca\u663e\u8457\uff0c\u5e76\u5efa\u8bae\u4f18\u5316\u9891\u8c31\u5206\u914d\u4ee5\u4fc3\u8fdb\u91c7\u7528\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u57ce\u5e02\u7f51\u7edc\u62e5\u5835\u548c\u519c\u6751\u6570\u5b57\u6392\u65a5\u95ee\u9898\uff0c\u901a\u8fc7\u6d41\u91cf\u5378\u8f7d\u3001\u589e\u5f3a\u5bb9\u9519\u80fd\u529b\u548c\u516c\u5e73\u8d44\u6e90\u5206\u914d\u6765\u63d0\u9ad8\u65e0\u7ebf\u7f51\u7edc\u6027\u80fd\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7f51\u72b6\u7f51\u7edc\uff08SDWMN\uff09\u3001\u76f4\u63a5\u5230\u79fb\u52a8\uff08D2M\uff09\u5e7f\u64ad\u548c\u57fa\u4e8eKafka\u7684\u6df7\u5408\u4e91\u6d41\u5a92\u4f53\u7684\u96c6\u6210\u67b6\u6784\u3002\u901a\u8fc7\u5efa\u6a21\u57ce\u5e02\u62e5\u5835\uff08\u03c1_u = \u03bb_t / \u03bc_c\uff09\u548c\u519c\u6751\u8986\u76d6\u7f3a\u9677\uff08\u03b4_r = 1 - C_r / C_req\uff09\uff0c\u5e76\u6700\u5c0f\u5316\u5168\u5c40\u6027\u80fd\u635f\u5931\uff08GPL = w_1\u00b7\u03c1_u + w_2\u00b7\u03b4_r + w_3\u00b7T_rec\uff09\uff0c\u5176\u4e2dT_rec\u4e3a\u6062\u590d\u65f6\u95f4\u3002", "result": "\u5728\u66fc\u8c37\u3001\u5b5f\u4e70\u548c\u82ac\u5170\u519c\u6751\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e86\u8d85\u8fc732%\u7684\u5ef6\u8fdf\u51cf\u5c11\u300140%\u7684\u5e26\u5bbd\u5378\u8f7d\u300128%\u7684\u519c\u6751\u8986\u76d6\u589e\u76ca\uff0c\u516c\u5e73\u6307\u6570\u4ece0.78\u4e0a\u5347\u81f30.91\uff0c\u5e76\u572810\u79d2\u5185\u901a\u8fc7SDWMN\u548cKafka\u5b9e\u73b0\u6062\u590d\u3002", "conclusion": "\u8be5\u8bba\u6587\u5efa\u8bae\u901a\u8fc7\u4f18\u5316\u9891\u8c31\u5206\u914d\u3001\u5b9a\u5411\u8865\u8d34\u548c\u8bbe\u5907\u6388\u6743\u6765\u4fc3\u8fdb\u7cfb\u7edf\u91c7\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u5bb9\u9519\u7684\u8bbe\u8ba1\uff0c\u652f\u6301\u516c\u5e73\u7684\u6570\u5b57\u5316\u8f6c\u578b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.14417", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14417", "abs": "https://arxiv.org/abs/2507.14417", "authors": ["Aryo Pradipta Gema", "Alexander H\u00e4gele", "Runjin Chen", "Andy Arditi", "Jacob Goldman-Wetzler", "Kit Fraser-Taliente", "Henry Sleight", "Linda Petrini", "Julian Michael", "Beatrice Alex", "Pasquale Minervini", "Yanda Chen", "Joe Benton", "Ethan Perez"], "title": "Inverse Scaling in Test-Time Compute", "comment": null, "summary": "We construct evaluation tasks where extending the reasoning length of Large\nReasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling\nrelationship between test-time compute and accuracy. Our evaluation tasks span\nfour categories: simple counting tasks with distractors, regression tasks with\nspurious features, deduction tasks with constraint tracking, and advanced AI\nrisks. We identify five distinct failure modes when models reason for longer:\n1) Claude models become increasingly distracted by irrelevant information; 2)\nOpenAI o-series models resist distractors but overfit to problem framings; 3)\nmodels shift from reasonable priors to spurious correlations; 4) all models\nshow difficulties in maintaining focus on complex deductive tasks; and 5)\nextended reasoning may amplify concerning behaviors, with Claude Sonnet 4\nshowing increased expressions of self-preservation. These findings suggest that\nwhile test-time compute scaling remains promising for improving model\ncapabilities, it may inadvertently reinforce problematic reasoning patterns.\nOur results demonstrate the importance of evaluating models across diverse\nreasoning lengths to identify and address these failure modes in LRMs.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u589e\u52a0\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u957f\u5ea6\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u4e94\u79cd\u5931\u8d25\u6a21\u5f0f\uff0c\u5f3a\u8c03\u4e86\u8bc4\u4f30\u4e0d\u540c\u63a8\u7406\u957f\u5ea6\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u5ef6\u957f\u63a8\u7406\u957f\u5ea6\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u73b0\u8c61\uff0c\u63ed\u793a\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u53cd\u6bd4\u5173\u7cfb\u3002", "method": "\u6784\u5efa\u4e86\u56db\u7c7b\u8bc4\u4f30\u4efb\u52a1\uff1a\u5e26\u5e72\u6270\u9879\u7684\u7b80\u5355\u8ba1\u6570\u4efb\u52a1\u3001\u5e26\u4f2a\u7279\u5f81\u7684\u56de\u5f52\u4efb\u52a1\u3001\u5e26\u7ea6\u675f\u8ddf\u8e2a\u7684\u6f14\u7ece\u4efb\u52a1\u4ee5\u53ca\u9ad8\u7ea7AI\u98ce\u9669\u4efb\u52a1\u3002", "result": "\u8bc6\u522b\u51fa\u4e94\u79cd\u4e0d\u540c\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5305\u62ec\u6a21\u578b\u5bf9\u5e72\u6270\u9879\u7684\u654f\u611f\u6027\u589e\u52a0\u3001\u8fc7\u5ea6\u62df\u5408\u95ee\u9898\u6846\u67b6\u3001\u8f6c\u5411\u4f2a\u76f8\u5173\u6027\u3001\u5728\u590d\u6742\u6f14\u7ece\u4efb\u52a1\u4e2d\u96be\u4ee5\u4fdd\u6301\u4e13\u6ce8\uff0c\u4ee5\u53ca\u5ef6\u957f\u63a8\u7406\u53ef\u80fd\u653e\u5927\u95ee\u9898\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\u6709\u671b\u63d0\u5347\u6a21\u578b\u80fd\u529b\uff0c\u4f46\u53ef\u80fd\u65e0\u610f\u4e2d\u5f3a\u5316\u4e86\u6709\u95ee\u9898\u7684\u63a8\u7406\u6a21\u5f0f\u3002\u8fd9\u5f3a\u8c03\u4e86\u8bc4\u4f30\u4e0d\u540c\u63a8\u7406\u957f\u5ea6\u4e0b\u6a21\u578b\u8868\u73b0\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.15121", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.15121", "abs": "https://arxiv.org/abs/2507.15121", "authors": ["Sasindu Wijeratne", "Rajgopal Kannan", "Viktor Prasanna"], "title": "AMPED: Accelerating MTTKRP for Billion-Scale Sparse Tensor Decomposition on Multiple GPUs", "comment": null, "summary": "Matricized Tensor Times Khatri-Rao Product (MTTKRP) is the computational\nbottleneck in sparse tensor decomposition. As real-world sparse tensors grow to\nbillions of nonzeros, they increasingly demand higher memory capacity and\ncompute throughput from hardware accelerators. In this work, we present AMPED,\na multi-GPU parallel algorithm designed to accelerate MTTKRP on billion-scale\nsparse tensors. AMPED scales beyond the limits of a single GPU, meeting both\nthe memory and performance requirements of large-scale workloads. We introduce\na partitioning strategy combined with a dynamic load balancing scheme to\ndistribute computation and minimize GPU idle time. On real-world billion-scale\ntensors, AMPED achieves a 5.1x geometric mean speedup in total execution time\nover state-of-the-art GPU baselines using 4 GPUs on a single CPU node.", "AI": {"tldr": "AMPED\u662f\u4e00\u79cd\u591aGPU\u5e76\u884c\u7b97\u6cd5\uff0c\u663e\u8457\u52a0\u901f\u5927\u89c4\u6a21\u7a00\u758f\u5f20\u91cf\u7684MTTKRP\u8ba1\u7b97\uff0c\u6027\u80fd\u63d0\u53475.1\u500d\u3002", "motivation": "\u968f\u7740\u73b0\u5b9e\u4e16\u754c\u7a00\u758f\u5f20\u91cf\u89c4\u6a21\u589e\u957f\u81f3\u6570\u5341\u4ebf\u975e\u96f6\u5143\u7d20\uff0c\u5bf9\u786c\u4ef6\u52a0\u901f\u5668\u7684\u5185\u5b58\u5bb9\u91cf\u548c\u8ba1\u7b97\u541e\u5410\u91cf\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0cMTTKRP\u6210\u4e3a\u8ba1\u7b97\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86AMPED\uff0c\u4e00\u79cd\u591aGPU\u5e76\u884c\u7b97\u6cd5\uff0c\u7ed3\u5408\u5206\u533a\u7b56\u7565\u548c\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\u65b9\u6848\uff0c\u4ee5\u5206\u6563\u8ba1\u7b97\u5e76\u6700\u5c0f\u5316GPU\u7a7a\u95f2\u65f6\u95f4\u3002", "result": "\u5728\u5341\u4ebf\u7ea7\u89c4\u6a21\u7684\u7a00\u758f\u5f20\u91cf\u4e0a\uff0cAMPED\u4f7f\u75284\u4e2aGPU\u5728\u5355\u4e2aCPU\u8282\u70b9\u4e0a\u5b9e\u73b0\u4e865.1\u500d\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u6bd4\uff0c\u4f18\u4e8e\u73b0\u6709GPU\u57fa\u51c6\u3002", "conclusion": "AMPED\u901a\u8fc7\u591aGPU\u5e76\u884c\u7b97\u6cd5\u6210\u529f\u52a0\u901f\u4e86\u5927\u89c4\u6a21\u7a00\u758f\u5f20\u91cf\u7684MTTKRP\u8ba1\u7b97\uff0c\u6ee1\u8db3\u4e86\u5185\u5b58\u548c\u6027\u80fd\u9700\u6c42\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u901f\u5ea6\u3002"}}
{"id": "2507.14812", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.14812", "abs": "https://arxiv.org/abs/2507.14812", "authors": ["Suho Kang", "Ziyang Liu", "Rajan Udwani"], "title": "A Black-Box Approach for Exogenous Replenishment in Online Resource Allocation", "comment": null, "summary": "In a typical online resource allocation problem, we start with a fixed\ninventory of resources and make online allocation decisions in response to\nresource requests that arrive sequentially over a finite horizon. We consider\nsettings where the inventory is replenished over time according to an unknown\nexogenous process. We introduce black-box methods that extend any existing\nalgorithm, originally designed without considering replenishment, into one that\nworks with an arbitrary (adversarial or stochastic) replenishment process. Our\napproach preserves the original algorithm's competitive ratio in regimes with\nlarge initial inventory, thereby enabling the seamless integration of exogenous\nreplenishment into a large body of existing algorithmic results for both\nadversarial and stochastic arrival models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ed1\u76d2\u65b9\u6cd5\uff0c\u5c06\u4e0d\u8003\u8651\u8d44\u6e90\u8865\u5145\u7684\u73b0\u6709\u7b97\u6cd5\u6269\u5c55\u5230\u80fd\u5904\u7406\u4efb\u610f\u8865\u5145\u8fc7\u7a0b\u7684\u573a\u666f\uff0c\u4fdd\u6301\u539f\u59cb\u7b97\u6cd5\u7684\u7ade\u4e89\u6bd4\u3002", "motivation": "\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u95ee\u9898\u4e2d\uff0c\u8d44\u6e90\u5e93\u5b58\u968f\u65f6\u95f4\u8865\u5145\u7684\u8fc7\u7a0b\u901a\u5e38\u662f\u672a\u77e5\u7684\uff0c\u73b0\u6709\u7b97\u6cd5\u672a\u8003\u8651\u8fd9\u4e00\u56e0\u7d20\u3002", "method": "\u5f15\u5165\u4e86\u9ed1\u76d2\u65b9\u6cd5\uff0c\u5c06\u4efb\u4f55\u4e0d\u8003\u8651\u8d44\u6e90\u8865\u5145\u7684\u73b0\u6709\u7b97\u6cd5\u6269\u5c55\u4e3a\u80fd\u591f\u5904\u7406\u4efb\u610f\uff08\u5bf9\u6297\u6027\u6216\u968f\u673a\u6027\uff09\u8865\u5145\u8fc7\u7a0b\u7684\u7b97\u6cd5\u3002", "result": "\u5728\u521d\u59cb\u5e93\u5b58\u8f83\u5927\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4fdd\u6301\u539f\u59cb\u7b97\u6cd5\u7684\u7ade\u4e89\u6bd4\uff0c\u9002\u7528\u4e8e\u5bf9\u6297\u6027\u548c\u968f\u673a\u6027\u5230\u8fbe\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5c06\u73b0\u6709\u7b97\u6cd5\u65e0\u7f1d\u6269\u5c55\u5230\u8003\u8651\u8d44\u6e90\u8865\u5145\u7684\u573a\u666f\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u7b97\u6cd5\u7684\u7ade\u4e89\u6bd4\u3002"}}
{"id": "2507.14554", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14554", "abs": "https://arxiv.org/abs/2507.14554", "authors": ["Ruoyu Su", "Noman ahmad", "Matteo Esposito", "Andrea Janes", "Davide Taibi", "Valentina Lenarduzzi"], "title": "Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review", "comment": null, "summary": "Software architecture plays a central role in the design, development, and\nmaintenance of software systems. With the rise of cloud computing,\nmicroservices, and containers, architectural practices have diversified.\nUnderstanding these shifts is vital. This study analyzes software architecture\ntrends across eight leading industry conferences over five years. We\ninvestigate the evolution of software architecture by analyzing talks from top\npractitioner conferences, focusing on the motivations and contexts driving\ntechnology adoption. We analyzed 5,677 talks from eight major industry\nconferences, using large language models and expert validation to extract\ntechnologies, their purposes, and usage contexts. We also explored how\ntechnologies interrelate and fit within DevOps and deployment pipelines. Among\n450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate\nby frequency and centrality. Practitioners present technology mainly related to\ndeployment, communication, AI, and observability. We identify five technology\ncommunities covering automation, coordination, cloud AI, monitoring, and\ncloud-edge. Most technologies span multiple DevOps stages and support hybrid\ndeployment. Our study reveals that a few core technologies, like Kubernetes and\nServerless, dominate the contemporary software architecture practice. These are\nmainly applied in later DevOps stages, with limited focus on early phases like\nplanning and coding. We also show how practitioners frame technologies by\npurpose and context, reflecting evolving industry priorities. Finally, we\nobserve how only research can provide a more holistic lens on architectural\ndesign, quality, and evolution.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u4e94\u5e74\u5185\u516b\u4e2a\u884c\u4e1a\u4f1a\u8bae\u76845,677\u573a\u6f14\u8bb2\uff0c\u53d1\u73b0Kubernetes\u548cServerless\u7b49\u6838\u5fc3\u6280\u672f\u4e3b\u5bfc\u4e86\u8f6f\u4ef6\u67b6\u6784\u5b9e\u8df5\uff0c\u4e3b\u8981\u5e94\u7528\u4e8eDevOps\u540e\u671f\u9636\u6bb5\uff0c\u53cd\u6620\u4e86\u884c\u4e1a\u4f18\u5148\u4e8b\u9879\u7684\u6f14\u53d8\u3002", "motivation": "\u968f\u7740\u4e91\u8ba1\u7b97\u3001\u5fae\u670d\u52a1\u548c\u5bb9\u5668\u7684\u5174\u8d77\uff0c\u67b6\u6784\u5b9e\u8df5\u591a\u6837\u5316\u3002\u7406\u89e3\u8fd9\u4e9b\u53d8\u5316\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u6765\u81ea\u516b\u4e2a\u4e3b\u8981\u884c\u4e1a\u4f1a\u8bae\u76845,677\u573a\u6f14\u8bb2\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4e13\u5bb6\u9a8c\u8bc1\u6765\u63d0\u53d6\u6280\u672f\u3001\u5176\u76ee\u7684\u548c\u4f7f\u7528\u4e0a\u4e0b\u6587\u3002\u8fd8\u63a2\u8ba8\u4e86\u6280\u672f\u5982\u4f55\u5728DevOps\u548c\u90e8\u7f72\u7ba1\u9053\u4e2d\u76f8\u4e92\u5173\u8054\u548c\u9002\u5e94\u3002", "result": "\u5728450\u9879\u6280\u672f\u4e2d\uff0cKubernetes\u3001Cloud Native\u3001Serverless\u548cContainers\u5728\u9891\u7387\u548c\u4e2d\u5fc3\u6027\u4e0a\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u4ece\u4e1a\u8005\u4e3b\u8981\u5c55\u793a\u4e0e\u90e8\u7f72\u3001\u901a\u4fe1\u3001AI\u548c\u53ef\u89c2\u6d4b\u6027\u76f8\u5173\u7684\u6280\u672f\u3002\u786e\u5b9a\u4e86\u4e94\u4e2a\u6280\u672f\u793e\u533a\uff0c\u6db5\u76d6\u81ea\u52a8\u5316\u3001\u534f\u8c03\u3001\u4e91AI\u3001\u76d1\u63a7\u548c\u4e91\u8fb9\u7f18\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c11\u6570\u6838\u5fc3\u6280\u672f\uff08\u5982Kubernetes\u548cServerless\uff09\u4e3b\u5bfc\u4e86\u5f53\u4ee3\u8f6f\u4ef6\u67b6\u6784\u5b9e\u8df5\uff0c\u4e3b\u8981\u5e94\u7528\u4e8eDevOps\u540e\u671f\u9636\u6bb5\uff0c\u800c\u5bf9\u65e9\u671f\u9636\u6bb5\uff08\u5982\u89c4\u5212\u548c\u7f16\u7801\uff09\u7684\u5173\u6ce8\u6709\u9650\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u4ece\u4e1a\u8005\u5982\u4f55\u6839\u636e\u76ee\u7684\u548c\u4e0a\u4e0b\u6587\u6765\u6784\u5efa\u6280\u672f\uff0c\u53cd\u6620\u4e86\u884c\u4e1a\u4f18\u5148\u4e8b\u9879\u7684\u6f14\u53d8\u3002"}}
{"id": "2507.15454", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15454", "abs": "https://arxiv.org/abs/2507.15454", "authors": ["Ruijie Zhu", "Mulin Yu", "Linning Xu", "Lihan Jiang", "Yixuan Li", "Tianzhu Zhang", "Jiangmiao Pang", "Bo Dai"], "title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting", "comment": "Accepted by ICCV 2025", "summary": "3D Gaussian Splatting is renowned for its high-fidelity reconstructions and\nreal-time novel view synthesis, yet its lack of semantic understanding limits\nobject-level perception. In this work, we propose ObjectGS, an object-aware\nframework that unifies 3D scene reconstruction with semantic understanding.\nInstead of treating the scene as a unified whole, ObjectGS models individual\nobjects as local anchors that generate neural Gaussians and share object IDs,\nenabling precise object-level reconstruction. During training, we dynamically\ngrow or prune these anchors and optimize their features, while a one-hot ID\nencoding with a classification loss enforces clear semantic constraints. We\nshow through extensive experiments that ObjectGS not only outperforms\nstate-of-the-art methods on open-vocabulary and panoptic segmentation tasks,\nbut also integrates seamlessly with applications like mesh extraction and scene\nediting. Project page: https://ruijiezhu94.github.io/ObjectGS_page", "AI": {"tldr": "ObjectGS\u662f\u4e00\u4e2a\u5bf9\u8c61\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u951a\u70b9\u548c\u8bed\u4e49\u7ea6\u675f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5bf9\u8c61\u7ea7\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "3D\u9ad8\u65af\u6e85\u5c04\u867d\u7136\u5728\u9ad8\u4fdd\u771f\u91cd\u5efa\u548c\u5b9e\u65f6\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5bf9\u8c61\u7ea7\u611f\u77e5\u3002ObjectGS\u65e8\u5728\u7edf\u4e003D\u573a\u666f\u91cd\u5efa\u4e0e\u8bed\u4e49\u7406\u89e3\u3002", "method": "ObjectGS\u5c06\u573a\u666f\u4e2d\u7684\u5355\u4e2a\u5bf9\u8c61\u5efa\u6a21\u4e3a\u5c40\u90e8\u951a\u70b9\uff0c\u751f\u6210\u795e\u7ecf\u9ad8\u65af\u5e76\u5171\u4eab\u5bf9\u8c61ID\uff0c\u901a\u8fc7\u52a8\u6001\u589e\u957f\u6216\u4fee\u526a\u951a\u70b9\u5e76\u4f18\u5316\u7279\u5f81\uff0c\u540c\u65f6\u4f7f\u7528one-hot ID\u7f16\u7801\u548c\u5206\u7c7b\u635f\u5931\u5f3a\u5316\u8bed\u4e49\u7ea6\u675f\u3002", "result": "ObjectGS\u5728\u5f00\u653e\u8bcd\u6c47\u548c\u5168\u666f\u5206\u5272\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u7f51\u683c\u63d0\u53d6\u548c\u573a\u666f\u7f16\u8f91\u7b49\u5e94\u7528\u3002", "conclusion": "ObjectGS\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5bf9\u8c61\u951a\u70b9\u5e76\u7ed3\u5408\u8bed\u4e49\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5bf9\u8c61\u7ea7\u91cd\u5efa\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u548c\u5168\u666f\u5206\u5272\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14209", "categories": ["cs.NI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.14209", "abs": "https://arxiv.org/abs/2507.14209", "authors": ["Rute C. Sofia", "Hao Shen", "Yuanting Liu", "Severin Kacianka", "Holger Pfeifer"], "title": "White paper: Towards Human-centric and Sustainable 6G Services -- the fortiss Research Perspective", "comment": null, "summary": "As a leading research institute in software-intensive systems, fortiss is\nactively shaping the vision of Sixth Generation Mobile Communication (6G). Our\nmission is to ensure that 6G technologies go beyond technical advancements and\nare aligned with societal needs. fortiss plays a key role in 6G initiatives\nworldwide, including contributions to standardization bodies and collaborative\nResearch and Development programs. We focus on software-defined, AI-enabled,\nand sustainable communication services that prioritize human values and\nlong-term impact. 6G will redefine digital connectivity through cognitive\nintelligence, decentralized orchestration, and sustainability-oriented\narchitectures. As expectations rise for ultra-reliable low-latency\ncommunication (URLLC) and personalized digital services, 6G must outperform\nprior generations. It will rely on AI-native networking, Edge-Cloud resource\norchestration, and energy-aware data frameworks, ensuring both technical\nperformance and societal relevance. This white paper presents the fortiss\nvision for a human-centric, sustainable, and AI-integrated 6G network. It\noutlines key research domains such as semantic communication, green\norchestration, and distributed AI, all linked to societal and technological\nchallenges. The white paper is aimed at researchers, industry experts,\npolicymakers, and developers. It articulates the strategic direction and\ncontributions of fortiss to 6G, emphasizing responsible innovation and\ninterdisciplinary collaboration toward a meaningful 2030 vision.", "AI": {"tldr": "fortiss\u7684\u613f\u666f\u662f\u6784\u5efa\u4e00\u4e2a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u3001\u53ef\u6301\u7eed\u4e14AI\u96c6\u6210\u76846G\u7f51\u7edc\uff0c\u901a\u8fc7\u6280\u672f\u521b\u65b0\u548c\u793e\u4f1a\u8d23\u4efb\u63a8\u52a82030\u5e74\u7684\u6570\u5b57\u5316\u672a\u6765\u3002", "motivation": "\u786e\u4fdd6G\u6280\u672f\u4e0d\u4ec5\u5b9e\u73b0\u6280\u672f\u8fdb\u6b65\uff0c\u8fd8\u80fd\u6ee1\u8db3\u793e\u4f1a\u9700\u6c42\uff0c\u8d85\u8d8a\u524d\u4ee3\u6280\u672f\uff0c\u63d0\u4f9b\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u548c\u4e2a\u6027\u5316\u6570\u5b57\u670d\u52a1\u3002", "method": "\u901a\u8fc7\u53c2\u4e0e\u5168\u74036G\u5021\u8bae\u3001\u6807\u51c6\u5316\u673a\u6784\u548c\u7814\u53d1\u5408\u4f5c\u9879\u76ee\uff0cfortiss\u805a\u7126\u4e8e\u8f6f\u4ef6\u5b9a\u4e49\u3001AI\u8d4b\u80fd\u548c\u53ef\u6301\u7eed\u7684\u901a\u4fe1\u670d\u52a1\uff0c\u7814\u7a76\u9886\u57df\u5305\u62ec\u8bed\u4e49\u901a\u4fe1\u3001\u7eff\u8272\u7f16\u6392\u548c\u5206\u5e03\u5f0fAI\u3002", "result": "\u63d0\u51fa\u4e866G\u7f51\u7edc\u7684\u6218\u7565\u65b9\u5411\uff0c\u5305\u62ecAI\u539f\u751f\u7f51\u7edc\u3001\u8fb9\u7f18-\u4e91\u8d44\u6e90\u7f16\u6392\u548c\u80fd\u6e90\u611f\u77e5\u6570\u636e\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u793e\u4f1a\u548c\u6280\u672f\u7684\u53cc\u91cd\u6311\u6218\u3002", "conclusion": "fortiss\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u3001\u53ef\u6301\u7eed\u4e14AI\u96c6\u6210\u76846G\u7f51\u7edc\u613f\u666f\uff0c\u5f3a\u8c03\u6280\u672f\u521b\u65b0\u4e0e\u793e\u4f1a\u9700\u6c42\u7684\u7ed3\u5408\uff0c\u65e8\u5728\u4e3a2030\u5e74\u7684\u6570\u5b57\u5316\u672a\u6765\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.14249", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14249", "abs": "https://arxiv.org/abs/2507.14249", "authors": ["Yuejiao Xie", "Maonan Wang", "Di Zhou", "Man-On Pun", "Zhu Han"], "title": "Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach", "comment": null, "summary": "Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions\nto alleviate urban congestion, with path planning becoming a key focus area.\nUnlike ground transportation, UAM trajectory planning has to prioritize\ncommunication quality for accurate location tracking in constantly changing\nenvironments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,\nrequires adaptive planning to respond to real-time passenger requests,\nespecially in ride-sharing scenarios where passenger demands are unpredictable\nand dynamic. However, conventional trajectory planning strategies based on\npredefined routes lack the flexibility to meet varied passenger ride demands.\nTo address these challenges, this work first proposes constructing a radio map\nto evaluate the communication quality of urban airspace. Building on this, we\nintroduce a novel Multi-Source Hybrid Attention Reinforcement Learning\n(MSHA-RL) framework for the challenge of effectively focusing on passengers and\nUAM locations, which arises from the significant dimensional disparity between\nthe representations. This model first generates the alignment among diverse\ndata sources with large gap dimensions before employing hybrid attention to\nbalance global and local insights, thereby facilitating responsive, real-time\npath planning. Extensive experimental results demonstrate that the approach\nenables communication-compliant trajectory planning, reducing travel time and\nenhancing operational efficiency while prioritizing passenger safety.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMSHA-RL\u6846\u67b6\uff0c\u7528\u4e8eUAM\u7cfb\u7edf\u7684\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\uff0c\u7ed3\u5408\u901a\u4fe1\u8d28\u91cf\u8bc4\u4f30\u548c\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f18\u5316\u65c5\u884c\u65f6\u95f4\u548c\u8fd0\u8425\u6548\u7387\uff0c\u786e\u4fdd\u5b89\u5168\u3002", "motivation": "\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u7cfb\u7edf\uff08UAM\uff09\u4f5c\u4e3a\u7f13\u89e3\u57ce\u5e02\u62e5\u5835\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u8def\u5f84\u89c4\u5212\u9700\u4f18\u5148\u8003\u8651\u901a\u4fe1\u8d28\u91cf\u548c\u5b9e\u65f6\u4e58\u5ba2\u9700\u6c42\u3002\u4f20\u7edf\u57fa\u4e8e\u9884\u5b9a\u4e49\u8def\u7ebf\u7684\u8f68\u8ff9\u89c4\u5212\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u52a8\u6001\u4e58\u5ba2\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u65e0\u7ebf\u7535\u5730\u56fe\u8bc4\u4f30\u57ce\u5e02\u7a7a\u57df\u7684\u901a\u4fe1\u8d28\u91cf\uff0c\u5e76\u5f15\u5165MSHA-RL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u9996\u5148\u751f\u6210\u4e0d\u540c\u6570\u636e\u6e90\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u7136\u540e\u5229\u7528\u6df7\u5408\u6ce8\u610f\u529b\u5e73\u8861\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u7b26\u5408\u901a\u4fe1\u8981\u6c42\u7684\u8f68\u8ff9\u89c4\u5212\uff0c\u51cf\u5c11\u65c5\u884c\u65f6\u95f4\u5e76\u63d0\u9ad8\u8fd0\u8425\u6548\u7387\uff0c\u540c\u65f6\u4f18\u5148\u4fdd\u969c\u4e58\u5ba2\u5b89\u5168\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u591a\u6e90\u6df7\u5408\u6ce8\u610f\u529b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08MSHA-RL\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\uff08UAM\uff09\u7cfb\u7edf\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u786e\u4fdd\u901a\u4fe1\u8d28\u91cf\u548c\u4e58\u5ba2\u5b89\u5168\uff0c\u540c\u65f6\u63d0\u9ad8\u8fd0\u8425\u6548\u7387\u3002"}}
{"id": "2507.14447", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14447", "abs": "https://arxiv.org/abs/2507.14447", "authors": ["Guancheng Zeng", "Xueyi Chen", "Jiawang Hu", "Shaohua Qi", "Yaxuan Mao", "Zhantao Wang", "Yifan Nie", "Shuang Li", "Qiuyang Feng", "Pengxu Qiu", "Yujia Wang", "Wenqiang Han", "Linyan Huang", "Gang Li", "Jingjing Mo", "Haowen Hu"], "title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise", "comment": "26 pages, 8 figures, 5 tables", "summary": "The deployment of agent systems in an enterprise environment is often\nhindered by several challenges: common models lack domain-specific process\nknowledge, leading to disorganized plans, missing key tools, and poor execution\nstability. To address this, this paper introduces Routine, a multi-step agent\nplanning framework designed with a clear structure, explicit instructions, and\nseamless parameter passing to guide the agent's execution module in performing\nmulti-step tool-calling tasks with high stability. In evaluations conducted\nwithin a real-world enterprise scenario, Routine significantly increases the\nexecution accuracy in model tool calls, increasing the performance of GPT-4o\nfrom 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed\na Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an\naccuracy increase to 88.2% on scenario-specific evaluations, indicating\nimproved adherence to execution plans. In addition, we employed Routine-based\ndistillation to create a scenario-specific, multi-step tool-calling dataset.\nFine-tuning on this distilled dataset raised the model's accuracy to 95.5%,\napproaching GPT-4o's performance. These results highlight Routine's\neffectiveness in distilling domain-specific tool-usage patterns and enhancing\nmodel adaptability to new scenarios. Our experimental results demonstrate that\nRoutine provides a practical and accessible approach to building stable agent\nworkflows, accelerating the deployment and adoption of agent systems in\nenterprise environments, and advancing the technical vision of AI for Process.", "AI": {"tldr": "Routine\u662f\u4e00\u4e2a\u591a\u6b65\u9aa4\u4ee3\u7406\u89c4\u5212\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4f01\u4e1a\u73af\u5883\u4e2d\u4ee3\u7406\u7cfb\u7edf\u7684\u6267\u884c\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f01\u4e1a\u73af\u5883\u4e2d\u4ee3\u7406\u7cfb\u7edf\u7684\u90e8\u7f72\u5e38\u56e0\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u8fc7\u7a0b\u77e5\u8bc6\u800c\u5bfc\u81f4\u8ba1\u5212\u6df7\u4e71\u3001\u5173\u952e\u5de5\u5177\u7f3a\u5931\u548c\u6267\u884c\u7a33\u5b9a\u6027\u5dee\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Routine\uff0c\u4e00\u4e2a\u591a\u6b65\u9aa4\u4ee3\u7406\u89c4\u5212\u6846\u67b6\uff0c\u5177\u6709\u6e05\u6670\u7684\u7ed3\u6784\u3001\u660e\u786e\u7684\u6307\u4ee4\u548c\u65e0\u7f1d\u7684\u53c2\u6570\u4f20\u9012\uff0c\u4ee5\u6307\u5bfc\u4ee3\u7406\u7684\u6267\u884c\u6a21\u5757\u6267\u884c\u591a\u6b65\u9aa4\u5de5\u5177\u8c03\u7528\u4efb\u52a1\u3002", "result": "\u5728\u771f\u5b9e\u4f01\u4e1a\u573a\u666f\u7684\u8bc4\u4f30\u4e2d\uff0cRoutine\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5de5\u5177\u8c03\u7528\u7684\u6267\u884c\u51c6\u786e\u6027\uff0cGPT-4o\u7684\u6027\u80fd\u4ece41.1%\u63d0\u5347\u81f396.3%\uff0cQwen3-14B\u4ece32.6%\u63d0\u5347\u81f383.3%\u3002\u901a\u8fc7\u5fae\u8c03\uff0cQwen3-14B\u7684\u51c6\u786e\u6027\u8fdb\u4e00\u6b65\u63d0\u5347\u81f388.2%\uff0c\u63a5\u8fd1GPT-4o\u7684\u6027\u80fd\u3002", "conclusion": "Routine\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u65b9\u6cd5\u6765\u6784\u5efa\u7a33\u5b9a\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u52a0\u901f\u4e86\u4f01\u4e1a\u73af\u5883\u4e2d\u4ee3\u7406\u7cfb\u7edf\u7684\u90e8\u7f72\u548c\u91c7\u7528\uff0c\u5e76\u63a8\u8fdb\u4e86AI for Process\u7684\u6280\u672f\u613f\u666f\u3002"}}
{"id": "2507.15154", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.15154", "abs": "https://arxiv.org/abs/2507.15154", "authors": ["Kohya Shiozaki", "Junya Nakamura"], "title": "Dynatune: Dynamic Tuning of Raft Election Parameters Using Network Measurement", "comment": "This paper was accepted at the 27th International Workshop on\n  Advances in Parallel and Distributed Computational Models (APDCM 2025), held\n  in conjunction with IPDPS 2025", "summary": "Raft is a leader-based consensus algorithm that implements State Machine\nReplication (SMR), which replicates the service state across multiple servers\nto enhance fault tolerance. In Raft, the servers play one of three roles:\nleader, follower, or candidate. The leader receives client requests, determines\nthe processing order, and replicates them to the followers. When the leader\nfails, the service must elect a new leader to continue processing requests,\nduring which the service experiences an out-of-service (OTS) time. The OTS time\nis directly influenced by election parameters, such as heartbeat interval and\nelection timeout. However, traditional approaches, such as Raft, often struggle\nto effectively tune these parameters, particularly under fluctuating network\nconditions, leading to increased OTS time and reduced service responsiveness.\nTo address this, we propose Dynatune, a mechanism that dynamically adjusts\nRaft's election parameters based on network metrics such as round-trip time and\npacket loss rates measured via heartbeats. By adapting to changing network\nenvironments, Dynatune significantly reduces the leader failure detection and\nOTS time without altering Raft's core mechanisms or introducing additional\ncommunication overheads. Experimental results demonstrate that Dynatune reduces\nthe leader failure detection and OTS times by 80% and 45%, respectively,\ncompared with Raft, while maintaining high availability even under dynamic\nnetwork conditions. These findings confirm that Dynatune effectively enhances\nthe performance and reliability of SMR services in various network scenarios.", "AI": {"tldr": "Dynatune\u662f\u4e00\u79cd\u52a8\u6001\u8c03\u6574Raft\u9009\u4e3e\u53c2\u6570\u7684\u673a\u5236\uff0c\u663e\u8457\u51cf\u5c11\u9886\u5bfc\u8005\u6545\u969c\u68c0\u6d4b\u548cOTS\u65f6\u95f4\uff0c\u63d0\u5347\u670d\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfRaft\u7b97\u6cd5\u5728\u7f51\u7edc\u6761\u4ef6\u6ce2\u52a8\u65f6\u96be\u4ee5\u6709\u6548\u8c03\u6574\u9009\u4e3e\u53c2\u6570\uff0c\u5bfc\u81f4OTS\u65f6\u95f4\u589e\u52a0\u548c\u670d\u52a1\u54cd\u5e94\u6027\u964d\u4f4e\u3002", "method": "\u63d0\u51faDynatune\u673a\u5236\uff0c\u57fa\u4e8e\u7f51\u7edc\u6307\u6807\uff08\u5982\u5f80\u8fd4\u65f6\u95f4\u548c\u4e22\u5305\u7387\uff09\u52a8\u6001\u8c03\u6574Raft\u7684\u9009\u4e3e\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDynatune\u76f8\u6bd4Raft\u5c06\u9886\u5bfc\u8005\u6545\u969c\u68c0\u6d4b\u548cOTS\u65f6\u95f4\u5206\u522b\u51cf\u5c11\u4e8680%\u548c45%\u3002", "conclusion": "Dynatune\u901a\u8fc7\u52a8\u6001\u8c03\u6574Raft\u9009\u4e3e\u53c2\u6570\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9886\u5bfc\u8005\u6545\u969c\u68c0\u6d4b\u548cOTS\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u53ef\u7528\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86SMR\u670d\u52a1\u5728\u5404\u79cd\u7f51\u7edc\u573a\u666f\u4e0b\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.14835", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14835", "abs": "https://arxiv.org/abs/2507.14835", "authors": ["Pan Peng", "Hangyu Xu"], "title": "Differentially Private Synthetic Graphs Preserving Triangle-Motif Cuts", "comment": "COLT 2025", "summary": "We study the problem of releasing a differentially private (DP) synthetic\ngraph $G'$ that well approximates the triangle-motif sizes of all cuts of any\ngiven graph $G$, where a motif in general refers to a frequently occurring\nsubgraph within complex networks. Non-private versions of such graphs have\nfound applications in diverse fields such as graph clustering, graph\nsparsification, and social network analysis. Specifically, we present the first\n$(\\varepsilon,\\delta)$-DP mechanism that, given an input graph $G$ with $n$\nvertices, $m$ edges and local sensitivity of triangles $\\ell_{3}(G)$, generates\na synthetic graph $G'$ in polynomial time, approximating the triangle-motif\nsizes of all cuts $(S,V\\setminus S)$ of the input graph $G$ up to an additive\nerror of $\\tilde{O}(\\sqrt{m\\ell_{3}(G)}n/\\varepsilon^{3/2})$. Additionally, we\nprovide a lower bound of $\\Omega(\\sqrt{mn}\\ell_{3}(G)/\\varepsilon)$ on the\nadditive error for any DP algorithm that answers the triangle-motif size\nqueries of all $(S,T)$-cut of $G$. Finally, our algorithm generalizes to\nweighted graphs, and our lower bound extends to any $K_h$-motif cut for any\nconstant $h\\geq 2$.", "AI": {"tldr": "\u9996\u4e2a\u5dee\u5206\u9690\u79c1\u5408\u6210\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u8fd1\u4f3c\u4fdd\u7559\u4e09\u89d2\u5f62\u4e3b\u9898\u5927\u5c0f\uff0c\u8bef\u5dee\u6709\u4e0a\u4e0b\u754c\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u4fdd\u6301\u5dee\u5206\u9690\u79c1\u7684\u524d\u63d0\u4e0b\uff0c\u751f\u6210\u8fd1\u4f3c\u4fdd\u7559\u590d\u6742\u7f51\u7edc\u5173\u952e\u7279\u5f81\u7684\u5408\u6210\u56fe\uff0c\u4ee5\u652f\u6301\u56fe\u805a\u7c7b\u3001\u7a00\u758f\u5316\u548c\u793e\u4f1a\u7f51\u7edc\u5206\u6790\u7b49\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5dee\u5206\u9690\u79c1\u673a\u5236\u751f\u6210\u5408\u6210\u56fe$G'$\uff0c\u8fd1\u4f3c\u8f93\u5165\u56fe$G$\u7684\u6240\u6709\u5272\u7684\u4e09\u89d2\u5f62\u4e3b\u9898\u5927\u5c0f\uff0c\u8bef\u5dee\u4e3a$\\tilde{O}(\\sqrt{m\\ell_{3}(G)}n/\\varepsilon^{3/2})$\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u751f\u6210\u5408\u6210\u56fe$G'$\uff0c\u8bef\u5dee\u4e0a\u754c\u4e3a$\\tilde{O}(\\sqrt{m\\ell_{3}(G)}n/\\varepsilon^{3/2})$\uff0c\u5e76\u8bc1\u660e\u4e86\u8bef\u5dee\u4e0b\u754c\u4e3a$\\Omega(\\sqrt{mn}\\ell_{3}(G)/\\varepsilon)$\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u591a\u9879\u5f0f\u65f6\u95f4\u7684$\\varepsilon,\\delta$-\u5dee\u5206\u9690\u79c1\u673a\u5236\uff0c\u7528\u4e8e\u751f\u6210\u8fd1\u4f3c\u4fdd\u7559\u8f93\u5165\u56fe\u6240\u6709\u5272\u7684\u4e09\u89d2\u5f62\u4e3b\u9898\u5927\u5c0f\u7684\u5408\u6210\u56fe\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bef\u5dee\u4e0b\u754c\u3002"}}
{"id": "2507.14558", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14558", "abs": "https://arxiv.org/abs/2507.14558", "authors": ["Bin Duan", "Tarek Mahmud", "Meiru Che", "Yan Yan", "Naipeng Dong", "Dan Dongseong Kim", "Guowei Yang"], "title": "Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library", "comment": null, "summary": "The combination of computer vision and artificial intelligence is\nfundamentally transforming a broad spectrum of industries by enabling machines\nto interpret and act upon visual data with high levels of accuracy. As the\nbiggest and by far the most popular open-source computer vision library, OpenCV\nlibrary provides an extensive suite of programming functions supporting\nreal-time computer vision. Bugs in the OpenCV library can affect the downstream\ncomputer vision applications, and it is critical to ensure the reliability of\nthe OpenCV library. This paper introduces VISTAFUZZ, a novel technique for\nharnessing large language models (LLMs) for document-guided fuzzing of the\nOpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain\nstandardized API information. Based on this standardized information, VISTAFUZZ\nextracts constraints on individual input parameters and dependencies between\nthese. Using these constraints and dependencies, VISTAFUZZ then generates new\ninput values to systematically test each target API. We evaluate the\neffectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the\nresults show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been\nconfirmed, and 5 of these have been fixed.", "AI": {"tldr": "VISTAFUZZ\u5229\u7528LLMs\u89e3\u6790API\u6587\u6863\uff0c\u901a\u8fc7\u7ea6\u675f\u548c\u4f9d\u8d56\u751f\u6210\u6d4b\u8bd5\u8f93\u5165\uff0c\u6709\u6548\u68c0\u6d4bOpenCV\u6f0f\u6d1e\uff0c\u6210\u529f\u4fee\u590d\u591a\u4e2a\u95ee\u9898\u3002", "motivation": "OpenCV\u4f5c\u4e3a\u6700\u6d41\u884c\u7684\u5f00\u6e90\u8ba1\u7b97\u673a\u89c6\u89c9\u5e93\uff0c\u5176\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u5e93\u4e2d\u7684\u6f0f\u6d1e\u53ef\u80fd\u5f71\u54cd\u4e0b\u6e38\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u5176\u8d28\u91cf\u3002", "method": "VISTAFUZZ\u5229\u7528LLMs\u89e3\u6790OpenCV\u7684API\u6587\u6863\uff0c\u63d0\u53d6\u8f93\u5165\u53c2\u6570\u7684\u7ea6\u675f\u548c\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u4fe1\u606f\u751f\u6210\u65b0\u7684\u8f93\u5165\u503c\uff0c\u4ee5\u7cfb\u7edf\u6d4b\u8bd5\u6bcf\u4e2a\u76ee\u6807API\u3002", "result": "\u5728\u6d4b\u8bd5OpenCV\u5e93\u7684330\u4e2aAPI\u4e2d\uff0cVISTAFUZZ\u68c0\u6d4b\u523017\u4e2a\u65b0\u6f0f\u6d1e\uff0c\u5176\u4e2d10\u4e2a\u5df2\u786e\u8ba4\uff0c5\u4e2a\u5df2\u4fee\u590d\u3002", "conclusion": "VISTAFUZZ\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89e3\u6790API\u6587\u6863\u5e76\u751f\u6210\u6807\u51c6\u5316\u4fe1\u606f\uff0c\u7cfb\u7edf\u6027\u5730\u6d4b\u8bd5OpenCV\u5e93\u4e2d\u7684API\uff0c\u6210\u529f\u68c0\u6d4b\u5e76\u4fee\u590d\u4e86\u591a\u4e2a\u6f0f\u6d1e\u3002"}}
{"id": "2507.15629", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15629", "abs": "https://arxiv.org/abs/2507.15629", "authors": ["Zuo-Liang Zhu", "Jian Yang", "Beibei Wang"], "title": "Gaussian Splatting with Discretized SDF for Relightable Assets", "comment": null, "summary": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and\nhighly efficient rendering speed in the novel view synthesis (NVS) task. The\napplication to inverse rendering still faces several challenges, as the\ndiscrete nature of Gaussian primitives makes it difficult to apply geometry\nconstraints. Recent works introduce the signed distance field (SDF) as an extra\ncontinuous representation to regularize the geometry defined by Gaussian\nprimitives. It improves the decomposition quality, at the cost of increasing\nmemory usage and complicating training. Unlike these works, we introduce a\ndiscretized SDF to represent the continuous SDF in a discrete manner by\nencoding it within each Gaussian using a sampled value. This approach allows us\nto link the SDF with the Gaussian opacity through an SDF-to-opacity\ntransformation, enabling rendering the SDF via splatting and avoiding the\ncomputational cost of ray marching.The key challenge is to regularize the\ndiscrete samples to be consistent with the underlying SDF, as the discrete\nrepresentation can hardly apply the gradient-based constraints (\\eg Eikonal\nloss). For this, we project Gaussians onto the zero-level set of SDF and\nenforce alignment with the surface from splatting, namely a projection-based\nconsistency loss. Thanks to the discretized SDF, our method achieves higher\nrelighting quality, while requiring no extra memory beyond GS and avoiding\ncomplex manually designed optimization. The experiments reveal that our method\noutperforms existing Gaussian-based inverse rendering methods. Our code is\navailable at https://github.com/NK-CS-ZZL/DiscretizedSDF.", "AI": {"tldr": "\u79bb\u6563\u5316SDF\u8868\u793a\u901a\u8fc7\u9ad8\u65af\u6e85\u5c04\u5b9e\u73b0\u9ad8\u8d28\u91cf\u9006\u6e32\u67d3\uff0c\u907f\u514d\u989d\u5916\u5185\u5b58\u548c\u590d\u6742\u4f18\u5316\u3002", "motivation": "\u89e3\u51b33D\u9ad8\u65af\u6e85\u5c04\u5728\u9006\u6e32\u67d3\u4e2d\u56e0\u79bb\u6563\u7279\u6027\u96be\u4ee5\u5e94\u7528\u51e0\u4f55\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u589e\u52a0\u5185\u5b58\u548c\u8bad\u7ec3\u590d\u6742\u5ea6\u7684\u7f3a\u70b9\u3002", "method": "\u5f15\u5165\u79bb\u6563\u5316SDF\u8868\u793a\uff0c\u901a\u8fc7\u91c7\u6837\u503c\u5728\u6bcf\u4e2a\u9ad8\u65af\u5185\u7f16\u7801SDF\uff0c\u5e76\u901a\u8fc7SDF-to-opacity\u8f6c\u6362\u4e0e\u9ad8\u65af\u4e0d\u900f\u660e\u5ea6\u5173\u8054\uff0c\u907f\u514d\u5149\u7ebf\u8ffd\u8e2a\u7684\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8\u65af\u57fa\u9006\u6e32\u67d3\u65b9\u6cd5\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u901a\u8fc7\u79bb\u6563\u5316SDF\u8868\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u9700\u8981\u989d\u5916\u5185\u5b58\u548c\u590d\u6742\u4f18\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u65af\u57fa\u9006\u6e32\u67d3\u7684\u8d28\u91cf\u3002"}}
{"id": "2507.14211", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14211", "abs": "https://arxiv.org/abs/2507.14211", "authors": ["Federico Mason", "Tommaso Zugno", "Matteo Drago", "Marco Giordani", "Mate Boban", "Michele Zorzi"], "title": "PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence", "comment": null, "summary": "Predictive Quality of Service (PQoS) makes it possible to anticipate QoS\nchanges, e.g., in wireless networks, and trigger appropriate countermeasures to\navoid performance degradation. Hence, PQoS is extremely useful for automotive\napplications such as teleoperated driving, which poses strict constraints in\nterms of latency and reliability. A promising tool for PQoS is given by\nReinforcement Learning (RL), a methodology that enables the design of\ndecision-making strategies for stochastic optimization. In this manuscript, we\npresent PRATA, a new simulation framework to enable PRedictive QoS based on AI\nfor Teleoperated driving Applications. PRATA consists of a modular pipeline\nthat includes (i) an end-to-end protocol stack to simulate the 5G Radio Access\nNetwork (RAN), (ii) a tool for generating automotive data, and (iii) an\nArtificial Intelligence (AI) unit to optimize PQoS decisions. To prove its\nutility, we use PRATA to design an RL unit, named RAN-AI, to optimize the\nsegmentation level of teleoperated driving data in the event of resource\nsaturation or channel degradation. Hence, we show that the RAN-AI entity\nefficiently balances the trade-off between QoS and Quality of Experience (QoE)\nthat characterize teleoperated driving applications, almost doubling the system\nperformance compared to baseline approaches. In addition, by varying the\nlearning settings of the RAN-AI entity, we investigate the impact of the state\nspace and the relative cost of acquiring network data that are necessary for\nthe implementation of RL.", "AI": {"tldr": "PRATA\u662f\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u9884\u6d4b\u6027QoS\u6846\u67b6\uff0c\u901a\u8fc7RL\u4f18\u5316\u8fdc\u7a0b\u9a7e\u9a76\u7684QoS\u51b3\u7b56\uff0c\u6027\u80fd\u63d0\u5347\u8fd1\u4e24\u500d\uff0c\u5e76\u63a2\u8ba8\u4e86RL\u5b9e\u73b0\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u9884\u6d4b\u6027QoS\uff08PQoS\uff09\u5bf9\u4e8e\u5982\u8fdc\u7a0b\u9a7e\u9a76\u7b49\u4e25\u683c\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u8981\u6c42\u7684\u6c7d\u8f66\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002RL\u4f5c\u4e3a\u4e00\u79cd\u968f\u673a\u4f18\u5316\u5de5\u5177\uff0c\u4e3aPQoS\u51b3\u7b56\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "method": "PRATA\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5305\u62ec5G RAN\u7aef\u5230\u7aef\u534f\u8bae\u6808\u6a21\u62df\u3001\u6c7d\u8f66\u6570\u636e\u751f\u6210\u5de5\u5177\u548cAI\u5355\u5143\uff08\u5982RAN-AI\uff09\u7528\u4e8e\u4f18\u5316PQoS\u51b3\u7b56\u3002\u901a\u8fc7RL\u65b9\u6cd5\u4f18\u5316\u4e86\u8fdc\u7a0b\u9a7e\u9a76\u6570\u636e\u7684\u5206\u6bb5\u7ea7\u522b\u3002", "result": "RAN-AI\u5b9e\u4f53\u6709\u6548\u5e73\u8861\u4e86QoS\u4e0eQoE\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u7cfb\u7edf\u6027\u80fd\u51e0\u4e4e\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u7ffb\u500d\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u72b6\u6001\u7a7a\u95f4\u548c\u7f51\u7edc\u6570\u636e\u83b7\u53d6\u6210\u672c\u5bf9RL\u5b9e\u73b0\u7684\u5f71\u54cd\u3002", "conclusion": "PRATA\u6846\u67b6\u901a\u8fc7\u96c6\u6210AI\u5355\u5143\uff08\u5982RAN-AI\uff09\u6709\u6548\u4f18\u5316\u4e86\u9884\u6d4b\u6027QoS\u51b3\u7b56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u51e0\u4e4e\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u7ffb\u500d\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u72b6\u6001\u7a7a\u95f4\u548c\u7f51\u7edc\u6570\u636e\u83b7\u53d6\u6210\u672c\u5bf9RL\u5b9e\u73b0\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.14274", "categories": ["cs.RO", "cs.NA", "math.DG", "math.DS", "math.GR", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.14274", "abs": "https://arxiv.org/abs/2507.14274", "authors": ["Andreas Mueller", "Shivesh Kumar", "Thomas Kordik"], "title": "A Recursive Lie-Group Formulation for the Second-Order Time Derivatives of the Inverse Dynamics of parallel Kinematic Manipulators", "comment": null, "summary": "Series elastic actuators (SEA) were introduced for serial robotic arms. Their\nmodel-based trajectory tracking control requires the second time derivatives of\nthe inverse dynamics solution, for which algorithms were proposed. Trajectory\ncontrol of parallel kinematics manipulators (PKM) equipped with SEAs has not\nyet been pursued. Key element for this is the computationally efficient\nevaluation of the second time derivative of the inverse dynamics solution. This\nhas not been presented in the literature, and is addressed in the present paper\nfor the first time. The special topology of PKM is exploited reusing the\nrecursive algorithms for evaluating the inverse dynamics of serial robots. A\nLie group formulation is used and all relations are derived within this\nframework. Numerical results are presented for a 6-DOF Gough-Stewart platform\n(as part of an exoskeleton), and for a planar PKM when a flatness-based control\nscheme is applied.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u89e3\u51b3\u4e86PKM\u914d\u5907SEA\u65f6\u8f68\u8ff9\u63a7\u5236\u7684\u9006\u52a8\u529b\u5b66\u89e3\u4e8c\u9636\u5bfc\u6570\u8ba1\u7b97\u95ee\u9898\uff0c\u91c7\u7528\u674e\u7fa4\u6846\u67b6\u548c\u9012\u5f52\u7b97\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6570\u503c\u7ed3\u679c\u3002", "motivation": "\u5c3d\u7ba1\u4e32\u8054\u5f39\u6027\u6267\u884c\u5668\uff08SEA\uff09\u5df2\u7528\u4e8e\u4e32\u8054\u673a\u68b0\u81c2\uff0c\u4f46\u5e76\u8054\u8fd0\u52a8\u5b66\u673a\u68b0\u81c2\uff08PKM\uff09\u914d\u5907SEA\u7684\u8f68\u8ff9\u63a7\u5236\u5c1a\u672a\u5b9e\u73b0\uff0c\u5173\u952e\u5728\u4e8e\u9006\u52a8\u529b\u5b66\u89e3\u7684\u4e8c\u9636\u65f6\u95f4\u5bfc\u6570\u7684\u9ad8\u6548\u8ba1\u7b97\u5c1a\u672a\u89e3\u51b3\u3002", "method": "\u5229\u7528PKM\u7684\u7279\u6b8a\u62d3\u6251\u7ed3\u6784\uff0c\u590d\u7528\u4e32\u8054\u673a\u5668\u4eba\u7684\u9012\u5f52\u7b97\u6cd5\uff0c\u5e76\u91c7\u7528\u674e\u7fa4\uff08Lie group\uff09\u6846\u67b6\u63a8\u5bfc\u6240\u6709\u5173\u7cfb\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e6\u81ea\u7531\u5ea6Gough-Stewart\u5e73\u53f0\uff08\u4f5c\u4e3a\u5916\u9aa8\u9abc\u7684\u4e00\u90e8\u5206\uff09\u548c\u5e73\u9762PKM\uff08\u91c7\u7528\u57fa\u4e8e\u5e73\u5766\u6027\u7684\u63a7\u5236\u65b9\u6848\uff09\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u89e3\u51b3\u4e86\u5e76\u8054\u8fd0\u52a8\u5b66\u673a\u68b0\u81c2\uff08PKM\uff09\u914d\u5907\u4e32\u8054\u5f39\u6027\u6267\u884c\u5668\uff08SEA\uff09\u65f6\u8f68\u8ff9\u63a7\u5236\u7684\u9006\u52a8\u529b\u5b66\u89e3\u7684\u4e8c\u9636\u65f6\u95f4\u5bfc\u6570\u9ad8\u6548\u8ba1\u7b97\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6570\u503c\u7ed3\u679c\u3002"}}
{"id": "2507.14468", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14468", "abs": "https://arxiv.org/abs/2507.14468", "authors": ["Yitong Lin", "Jiaying He", "Jiahe Chen", "Xinnan Zhu", "Jianwei Zheng", "Tao Bo"], "title": "BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning", "comment": "Accepted by Bioinformatics on July 11th", "summary": "Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery\nand disease understanding, yet their completion and reasoning are challenging.\nKnowledge Embedding (KE) methods capture global semantics but struggle with\ndynamic structural integration, while Graph Neural Networks (GNNs) excel\nlocally but often lack semantic understanding. Even ensemble approaches,\nincluding those leveraging language models, often fail to achieve a deep,\nadaptive, and synergistic co-evolution between semantic comprehension and\nstructural learning. Addressing this critical gap in fostering continuous,\nreciprocal refinement between these two aspects in complex biomedical KGs is\nparamount.\n  Results: We introduce BioGraphFusion, a novel framework for deeply\nsynergistic semantic and structural learning. BioGraphFusion establishes a\nglobal semantic foundation via tensor decomposition, guiding an LSTM-driven\nmechanism to dynamically refine relation embeddings during graph propagation.\nThis fosters adaptive interplay between semantic understanding and structural\nlearning, further enhanced by query-guided subgraph construction and a hybrid\nscoring mechanism. Experiments across three key biomedical tasks demonstrate\nBioGraphFusion's superior performance over state-of-the-art KE, GNN, and\nensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)\nhighlights its ability to unveil biologically meaningful pathways.\n  Availability and Implementation: Source code and all training data are freely\navailable for download at https://github.com/Y-TARL/BioGraphFusion.\n  Contact: zjw@zjut.edu.cn, botao666666@126.com.\n  Supplementary information: Supplementary data are available at Bioinformatics\nonline.", "AI": {"tldr": "BioGraphFusion\u6846\u67b6\u901a\u8fc7\u534f\u540c\u8bed\u4e49\u4e0e\u7ed3\u6784\u5b66\u4e60\uff0c\u63d0\u5347\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u7684\u8865\u5168\u4e0e\u63a8\u7406\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u7406\u89e3\u4e0e\u7ed3\u6784\u5b66\u4e60\u7684\u534f\u540c\u8fdb\u5316\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "BioGraphFusion\u7ed3\u5408\u5f20\u91cf\u5206\u89e3\u5efa\u7acb\u5168\u5c40\u8bed\u4e49\u57fa\u7840\uff0c\u901a\u8fc7LSTM\u52a8\u6001\u4f18\u5316\u5173\u7cfb\u5d4c\u5165\uff0c\u5e76\u7ed3\u5408\u67e5\u8be2\u5f15\u5bfc\u7684\u5b50\u56fe\u6784\u5efa\u4e0e\u6df7\u5408\u8bc4\u5206\u673a\u5236\u3002", "result": "BioGraphFusion\u5728\u4e09\u4e2a\u5173\u952e\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709KE\u3001GNN\u548c\u96c6\u6210\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u751f\u7269\u5b66\u610f\u4e49\u3002", "conclusion": "BioGraphFusion\u901a\u8fc7\u6df1\u5ea6\u534f\u540c\u7684\u8bed\u4e49\u4e0e\u7ed3\u6784\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.15230", "categories": ["cs.DC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.15230", "abs": "https://arxiv.org/abs/2507.15230", "authors": ["Guoxi Liu", "Thomas Randall", "Rong Ge", "Federico Iuricich"], "title": "GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis", "comment": null, "summary": "Unstructured meshes present challenges in scientific data analysis due to\nirregular distribution and complex connectivity. Computing and storing\nconnectivity information is a major bottleneck for visualization algorithms,\naffecting both time and memory performance. Recent task-parallel data\nstructures address this by precomputing connectivity information at runtime\nwhile the analysis algorithm executes, effectively hiding computation costs and\nimproving performance. However, existing approaches are CPU-bound, forcing the\ndata structure and analysis algorithm to compete for the same computational\nresources, limiting potential speedups. To overcome this limitation, we\nintroduce a novel task-parallel approach optimized for heterogeneous CPU-GPU\nsystems. Specifically, we offload the computation of mesh connectivity\ninformation to GPU threads, enabling CPU threads to focus on executing the\nvisualization algorithm. Following this paradigm, we propose GALE (GPU-Aided\nLocalized data structurE), the first open-source CUDA-based data structure\ndesigned for heterogeneous task parallelism. Experiments on two 20-core CPUs\nand an NVIDIA V100 GPU show that GALE achieves up to 2.7x speedup over\nstate-of-the-art localized data structures while maintaining memory efficiency.", "AI": {"tldr": "GALE \u662f\u4e00\u79cd\u9488\u5bf9\u5f02\u6784\u7cfb\u7edf\u7684 GPU \u8f85\u52a9\u4efb\u52a1\u5e76\u884c\u6570\u636e\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u7ed3\u6784\u5316\u7f51\u683c\u7684\u5904\u7406\u6027\u80fd\u3002", "motivation": "\u975e\u7ed3\u6784\u5316\u7f51\u683c\u5728\u79d1\u5b66\u6570\u636e\u5206\u6790\u4e2d\u56e0\u5206\u5e03\u4e0d\u89c4\u5219\u548c\u8fde\u63a5\u590d\u6742\u800c\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709 CPU \u7ed1\u5b9a\u7684\u65b9\u6cd5\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u901a\u8fc7\u5c06\u7f51\u683c\u8fde\u901a\u6027\u4fe1\u606f\u7684\u8ba1\u7b97\u5378\u8f7d\u5230 GPU \u7ebf\u7a0b\uff0c\u4f7f CPU \u7ebf\u7a0b\u4e13\u6ce8\u4e8e\u6267\u884c\u53ef\u89c6\u5316\u7b97\u6cd5\uff0c\u63d0\u51fa\u4e86 GALE\uff08GPU-Aided Localized data structurE\uff09\u3002", "result": "\u5728 20 \u6838 CPU \u548c NVIDIA V100 GPU \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGALE \u76f8\u6bd4\u73b0\u6709\u6280\u672f\u5b9e\u73b0\u4e86\u6700\u9ad8 2.7 \u500d\u7684\u52a0\u901f\u3002", "conclusion": "GALE \u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5f02\u6784 CPU-GPU \u7cfb\u7edf\u7684\u4efb\u52a1\u5e76\u884c\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u7ed3\u6784\u5316\u7f51\u683c\u6570\u636e\u7684\u5904\u7406\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5185\u5b58\u6548\u7387\u3002"}}
{"id": "2507.15282", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.15282", "abs": "https://arxiv.org/abs/2507.15282", "authors": ["Aqsa Ashraf Makhdomi", "Iqra Altaf Gillani"], "title": "Predict, Reposition, and Allocate: A Greedy and Flow-Based Architecture for Sustainable Urban Food Delivery", "comment": null, "summary": "The rapid proliferation of food delivery platforms has reshaped urban\nmobility but has also contributed significantly to environmental degradation\nthrough increased greenhouse gas emissions. Existing optimization mechanisms\nproduce sub-optimal outcomes as they do not consider environmental\nsustainability their optimization objective. This study proposes a novel\neco-friendly food delivery optimization framework that integrates demand\nprediction, delivery person routing, and order allocation to minimize\nenvironmental impact while maintaining service efficiency. Since recommending\nroutes is NP-Hard, the proposed approach utilizes the submodular and monotone\nproperties of the objective function and designs an efficient greedy\noptimization algorithm. Thereafter, it formulates order allocation problem as a\nnetwork flow optimization model, which, to the best of our knowledge, has not\nbeen explored in the context of food delivery. A three-layered network\narchitecture is designed to match orders with delivery personnel based on\ncapacity constraints and spatial demand. Through this framework, the proposed\napproach reduces the vehicle count, and creates a sustainable food delivery\necosystem.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u73af\u4fdd\u98df\u54c1\u914d\u9001\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8d2a\u5a6a\u7b97\u6cd5\u548c\u7f51\u7edc\u6d41\u6a21\u578b\u51cf\u5c11\u8f66\u8f86\u4f7f\u7528\uff0c\u964d\u4f4e\u6392\u653e\u3002", "motivation": "\u73b0\u6709\u7684\u4f18\u5316\u673a\u5236\u672a\u5c06\u73af\u5883\u53ef\u6301\u7eed\u6027\u7eb3\u5165\u4f18\u5316\u76ee\u6807\uff0c\u5bfc\u81f4\u6b21\u4f18\u7ed3\u679c\uff0c\u800c\u98df\u54c1\u914d\u9001\u5e73\u53f0\u7684\u5feb\u901f\u6269\u5f20\u52a0\u5267\u4e86\u57ce\u5e02\u73af\u5883\u9000\u5316\u3002", "method": "\u5229\u7528\u76ee\u6807\u51fd\u6570\u7684\u5b50\u6a21\u6027\u548c\u5355\u8c03\u6027\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8d2a\u5a6a\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u5c06\u8ba2\u5355\u5206\u914d\u95ee\u9898\u8868\u8ff0\u4e3a\u7f51\u7edc\u6d41\u4f18\u5316\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e86\u4e09\u5c42\u7f51\u7edc\u67b6\u6784\u4ee5\u6ee1\u8db3\u5bb9\u91cf\u7ea6\u675f\u548c\u7a7a\u95f4\u9700\u6c42\u3002", "result": "\u8be5\u6846\u67b6\u51cf\u5c11\u4e86\u8f66\u8f86\u6570\u91cf\uff0c\u964d\u4f4e\u4e86\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u670d\u52a1\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u73af\u4fdd\u98df\u54c1\u914d\u9001\u4f18\u5316\u6846\u67b6\u901a\u8fc7\u6574\u5408\u9700\u6c42\u9884\u6d4b\u3001\u914d\u9001\u5458\u8def\u5f84\u89c4\u5212\u548c\u8ba2\u5355\u5206\u914d\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u8f66\u8f86\u6570\u91cf\uff0c\u5efa\u7acb\u4e86\u53ef\u6301\u7eed\u7684\u98df\u54c1\u914d\u9001\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2507.14594", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14594", "abs": "https://arxiv.org/abs/2507.14594", "authors": ["Weiwei Xu", "Hengzhi Ye", "Kai Gao", "Minghui Zhou"], "title": "A first look at License Variants in the PyPI Ecosystem", "comment": null, "summary": "Open-source licenses establish the legal foundation for software reuse, yet\nlicense variants, including both modified standard licenses and custom-created\nalternatives, introduce significant compliance complexities. Despite their\nprevalence and potential impact, these variants are poorly understood in modern\nsoftware systems, and existing tools do not account for their existence,\nleading to significant challenges in both effectiveness and efficiency of\nlicense analysis. To fill this knowledge gap, we conduct a comprehensive\nempirical study of license variants in the PyPI ecosystem. Our findings show\nthat textual variations in licenses are common, yet only 2% involve substantive\nmodifications. However, these license variants lead to significant compliance\nissues, with 10.7% of their downstream dependencies found to be\nlicense-incompatible.\n  Inspired by our findings, we introduce LV-Parser, a novel approach for\nefficient license variant analysis leveraging diff-based techniques and large\nlanguage models, along with LV-Compat, an automated pipeline for detecting\nlicense incompatibilities in software dependency networks. Our evaluation\ndemonstrates that LV-Parser achieves an accuracy of 0.936 while reducing\ncomputational costs by 30%, and LV-Compat identifies 5.2 times more\nincompatible packages than existing methods with a precision of 0.98.\n  This work not only provides the first empirical study into license variants\nin software packaging ecosystem but also equips developers and organizations\nwith practical tools for navigating the complex landscape of open-source\nlicensing.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f00\u6e90\u8bb8\u53ef\u8bc1\u53d8\u4f53\u666e\u904d\u5b58\u5728\u4f46\u5b9e\u8d28\u6027\u4fee\u6539\u8f83\u5c11\uff0c\u5374\u5bfc\u81f4\u663e\u8457\u5408\u89c4\u95ee\u9898\uff1b\u63d0\u51faLV-Parser\u548cLV-Compat\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u5206\u6790\u6548\u7387\u548c\u517c\u5bb9\u6027\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u5f00\u6e90\u8bb8\u53ef\u8bc1\u7684\u53d8\u4f53\uff08\u5305\u62ec\u4fee\u6539\u7684\u6807\u51c6\u8bb8\u53ef\u8bc1\u548c\u81ea\u5b9a\u4e49\u66ff\u4ee3\u65b9\u6848\uff09\u5f15\u5165\u4e86\u663e\u8457\u7684\u5408\u89c4\u590d\u6742\u6027\uff0c\u73b0\u6709\u5de5\u5177\u672a\u80fd\u8003\u8651\u8fd9\u4e9b\u53d8\u4f53\u7684\u5b58\u5728\uff0c\u5bfc\u81f4\u8bb8\u53ef\u8bc1\u5206\u6790\u7684\u6548\u7387\u548c\u6548\u679c\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5dee\u5f02\u7684\u6280\u672f\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684LV-Parser\u65b9\u6cd5\uff0c\u4ee5\u53ca\u7528\u4e8e\u68c0\u6d4b\u8f6f\u4ef6\u4f9d\u8d56\u7f51\u7edc\u4e2d\u8bb8\u53ef\u8bc1\u4e0d\u517c\u5bb9\u6027\u7684LV-Compat\u81ea\u52a8\u5316\u6d41\u7a0b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bb8\u53ef\u8bc1\u7684\u6587\u672c\u53d8\u4f53\u5f88\u5e38\u89c1\uff0c\u4f46\u53ea\u67092%\u6d89\u53ca\u5b9e\u8d28\u6027\u4fee\u6539\uff1b\u8fd9\u4e9b\u53d8\u4f53\u5bfc\u81f410.7%\u7684\u4e0b\u6e38\u4f9d\u8d56\u5b58\u5728\u8bb8\u53ef\u8bc1\u4e0d\u517c\u5bb9\u95ee\u9898\u3002LV-Parser\u7684\u51c6\u786e\u7387\u8fbe\u52300.936\uff0c\u540c\u65f6\u964d\u4f4e30%\u7684\u8ba1\u7b97\u6210\u672c\uff1bLV-Compat\u8bc6\u522b\u7684\u4e0d\u517c\u5bb9\u5305\u6570\u91cf\u662f\u73b0\u6709\u65b9\u6cd5\u76845.2\u500d\uff0c\u7cbe\u786e\u5ea6\u4e3a0.98\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u9996\u6b21\u5bf9\u8f6f\u4ef6\u5305\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u8bb8\u53ef\u8bc1\u53d8\u4f53\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\uff0c\u8fd8\u4e3a\u5f00\u53d1\u8005\u548c\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u4ee5\u5e94\u5bf9\u5f00\u6e90\u8bb8\u53ef\u8bc1\u7684\u590d\u6742\u5c40\u9762\u3002"}}
{"id": "2507.14230", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14230", "abs": "https://arxiv.org/abs/2507.14230", "authors": ["Fransiscus Asisi Bimo", "Maria Amparo Canaveras Galdon", "Chun-Kai Lai", "Ray-Guang Cheng", "Edwin K. P. Chong"], "title": "Intent-Based Network for RAN Management with Large Language Models", "comment": "5 pages, 3 figures, submitted to IEEE Globecom 2025", "summary": "Advanced intelligent automation becomes an important feature to deal with the\nincreased complexity in managing wireless networks. This paper proposes a novel\nautomation approach of intent-based network for Radio Access Networks (RANs)\nmanagement by leveraging Large Language Models (LLMs). The proposed method\nenhances intent translation, autonomously interpreting high-level objectives,\nreasoning over complex network states, and generating precise configurations of\nthe RAN by integrating LLMs within an agentic architecture. We propose a\nstructured prompt engineering technique and demonstrate that the network can\nautomatically improve its energy efficiency by dynamically optimizing critical\nRAN parameters through a closed-loop mechanism. It showcases the potential to\nenable robust resource management in RAN by adapting strategies based on\nreal-time feedback via LLM-orchestrated agentic systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLMs\u7684RAN\u81ea\u52a8\u5316\u7ba1\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u667a\u80fd\u4ee3\u7406\u548c\u95ed\u73af\u673a\u5236\u4f18\u5316\u7f51\u7edc\u80fd\u6548\uff0c\u5c55\u793a\u4e86\u5b9e\u65f6\u8d44\u6e90\u7ba1\u7406\u7684\u6f5c\u529b\u3002", "motivation": "\u5e94\u5bf9\u65e0\u7ebf\u7f51\u7edc\u7ba1\u7406\u65e5\u76ca\u589e\u957f\u7684\u590d\u6742\u6027\uff0c\u5229\u7528LLMs\u63d0\u5347\u9ad8\u7ea7\u76ee\u6807\u610f\u56fe\u7684\u81ea\u4e3b\u89e3\u91ca\u548c\u7f51\u7edc\u72b6\u6001\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u5c06LLMs\u96c6\u6210\u5230\u667a\u80fd\u4ee3\u7406\u67b6\u6784\u4e2d\uff0c\u5b9e\u73b0\u610f\u56fe\u7ffb\u8bd1\u3001\u590d\u6742\u7f51\u7edc\u72b6\u6001\u63a8\u7406\u548cRAN\u7cbe\u786e\u914d\u7f6e\u751f\u6210\u3002", "result": "\u901a\u8fc7\u95ed\u73af\u673a\u5236\u52a8\u6001\u4f18\u5316RAN\u53c2\u6570\uff0c\u7f51\u7edc\u80fd\u6548\u5f97\u5230\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86LLM\u534f\u8c03\u7684\u667a\u80fd\u7cfb\u7edf\u5728\u5b9e\u65f6\u8d44\u6e90\u7ba1\u7406\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u610f\u56fe\u9a71\u52a8\u7f51\u7edc\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ed\u73af\u673a\u5236\u52a8\u6001\u4f18\u5316RAN\u5173\u952e\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u80fd\u6548\uff0c\u5c55\u793a\u4e86LLM\u534f\u8c03\u7684\u667a\u80fd\u7cfb\u7edf\u5728\u5b9e\u65f6\u8d44\u6e90\u7ba1\u7406\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14412", "abs": "https://arxiv.org/abs/2507.14412", "authors": ["Mengxue Fu", "Zhonghao Shi", "Minyu Huang", "Siqi Liu", "Mina Kian", "Yirui Song", "Maja J. Matari\u0107"], "title": "Personalized Socially Assistive Robots With End-to-End Speech-Language Models For Well-Being Support", "comment": null, "summary": "Socially assistive robots (SARs) have shown great potential for supplementing\nwell-being support. However, prior studies have found that existing dialogue\npipelines for SARs remain limited in real-time latency, back-channeling, and\npersonalized speech dialogue. Toward addressing these limitations, we propose\nusing integrated end-to-end speech-language models (SLMs) with SARs. This work\n1) evaluated the usability of an SLM-enabled SAR dialogue system through a\nsmall user study, and 2) identified remaining limitations through study user\nfeedback to inform future improvements. We conducted a small within-participant\nuser study with university students (N = 11) whose results showed that\nparticipants perceived an SLM-enabled SAR system as capable of providing\nempathetic feedback, natural turn-taking, back-channeling, and adaptive\nresponses. We also found that participants reported the robot's nonverbal\nbehaviors as lacking variability and synchronization with conversation, and the\nSLM's verbal feedback as generic and repetitive. These findings highlighted the\nneed for real-time robot movement synchronized with conversation, improved\nprompting or fine-tuning to generate outputs better aligned with mental health\npractices, and more expressive, adaptive vocal generation.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u7aef\u5230\u7aefSLM\u6539\u8fdbSAR\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u53d1\u73b0\u5176\u5171\u60c5\u548c\u81ea\u9002\u5e94\u54cd\u5e94\u80fd\u529b\uff0c\u4f46\u975e\u8bed\u8a00\u884c\u4e3a\u548c\u53cd\u9988\u4ecd\u9700\u4f18\u5316\u3002", "motivation": "\u73b0\u6709SAR\u5bf9\u8bdd\u7cfb\u7edf\u5728\u5b9e\u65f6\u5ef6\u8fdf\u3001\u53cd\u9988\u548c\u4e2a\u6027\u5316\u8bed\u97f3\u5bf9\u8bdd\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u96c6\u6210\u7aef\u5230\u7aefSLM\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u5c0f\u578b\u7684\u7528\u6237\u5185\u7814\u7a76\uff08N = 11\uff09\u8bc4\u4f30\u4e86\u57fa\u4e8e\u7aef\u5230\u7aef\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7684SAR\u5bf9\u8bdd\u7cfb\u7edf\u7684\u53ef\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u53cd\u9988\u8bc6\u522b\u4e86\u5269\u4f59\u9650\u5236\u3002", "result": "\u53c2\u4e0e\u8005\u8ba4\u4e3aSLM\u542f\u7528\u7684SAR\u7cfb\u7edf\u80fd\u591f\u63d0\u4f9b\u5171\u60c5\u53cd\u9988\u3001\u81ea\u7136\u8f6e\u8f6c\u3001\u53cd\u9988\u548c\u81ea\u9002\u5e94\u54cd\u5e94\uff0c\u4f46\u4e5f\u6307\u51fa\u4e86\u673a\u5668\u4eba\u975e\u8bed\u8a00\u884c\u4e3a\u7f3a\u4e4f\u53d8\u5316\u548c\u5bf9\u8bdd\u540c\u6b65\u6027\u4e0d\u8db3\uff0c\u4ee5\u53caSLM\u7684\u53cd\u9988\u901a\u7528\u4e14\u91cd\u590d\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5b9e\u65f6\u673a\u5668\u4eba\u52a8\u4f5c\u4e0e\u5bf9\u8bdd\u540c\u6b65\u3001\u6539\u8fdb\u63d0\u793a\u6216\u5fae\u8c03\u4ee5\u751f\u6210\u66f4\u7b26\u5408\u5fc3\u7406\u5065\u5eb7\u5b9e\u8df5\u7684\u8f93\u51fa\uff0c\u4ee5\u53ca\u66f4\u5177\u8868\u73b0\u529b\u548c\u9002\u5e94\u6027\u7684\u8bed\u97f3\u751f\u6210\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.14513", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14513", "abs": "https://arxiv.org/abs/2507.14513", "authors": ["Hongyi Yang", "Yue Pan", "Jiayi Xu", "Kelsen Liu"], "title": "Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy", "comment": null, "summary": "Recent advances in large language models (LLMs) and autonomous agents have\nenabled systems capable of performing complex tasks across domains such as\nhuman-computer interaction, planning, and web navigation. However, many\nexisting frameworks struggle in real-world or resource-constrained environments\ndue to their reliance on cloud-based computation, limited robustness in dynamic\ncontexts, and lack of persistent autonomy and environmental awareness.\n  We present Amico, a modular, event-driven framework for building autonomous\nagents optimized for embedded systems. Written in Rust for safety and\nperformance, Amico supports reactive, persistent agents that operate\nefficiently across embedded platforms and browser environments via WebAssembly.\nIt provides clean abstractions for event handling, state management, behavior\nexecution, and integration with reasoning modules. Amico delivers a unified\ninfrastructure for constructing resilient, interactive agents suitable for\ndeployment in settings with limited compute and intermittent connectivity.", "AI": {"tldr": "Amico \u662f\u4e00\u4e2a\u7528 Rust \u7f16\u5199\u7684\u6a21\u5757\u5316\u4e8b\u4ef6\u9a71\u52a8\u6846\u67b6\uff0c\u4e13\u4e3a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4f18\u5316\u7684\u81ea\u4e3b\u4ee3\u7406\u8bbe\u8ba1\uff0c\u652f\u6301 WebAssembly \u4ee5\u5b9e\u73b0\u8de8\u5e73\u53f0\u9ad8\u6548\u8fd0\u884c\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u5728\u73b0\u5b9e\u4e16\u754c\u6216\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u4f9d\u8d56\u4e91\u8ba1\u7b97\u7684\u5c40\u9650\u6027\u3001\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u4ee5\u53ca\u7f3a\u4e4f\u6301\u4e45\u81ea\u4e3b\u6027\u548c\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002", "method": "Amico \u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u4e8b\u4ef6\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u4f7f\u7528 Rust \u7f16\u5199\u4ee5\u4fdd\u969c\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0c\u652f\u6301\u901a\u8fc7 WebAssembly \u5728\u5d4c\u5165\u5f0f\u5e73\u53f0\u548c\u6d4f\u89c8\u5668\u73af\u5883\u4e2d\u9ad8\u6548\u8fd0\u884c\u7684\u53cd\u5e94\u5f0f\u3001\u6301\u4e45\u6027\u4ee3\u7406\u3002", "result": "Amico \u63d0\u4f9b\u4e86\u4e8b\u4ef6\u5904\u7406\u3001\u72b6\u6001\u7ba1\u7406\u3001\u884c\u4e3a\u6267\u884c\u4ee5\u53ca\u4e0e\u63a8\u7406\u6a21\u5757\u96c6\u6210\u7684\u6e05\u6670\u62bd\u8c61\uff0c\u4e3a\u6784\u5efa\u9002\u5e94\u6027\u5f3a\u3001\u4ea4\u4e92\u6027\u597d\u7684\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u4e86\u57fa\u7840\u8bbe\u65bd\u3002", "conclusion": "Amico \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u548c\u95f4\u6b47\u6027\u8fde\u63a5\u73af\u5883\u4e0b\u5177\u6709\u5f39\u6027\u548c\u4ea4\u4e92\u6027\u7684\u81ea\u4e3b\u4ee3\u7406\u3002"}}
{"id": "2507.15233", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.15233", "abs": "https://arxiv.org/abs/2507.15233", "authors": ["Jintao Liu", "Mohammad Goudarzi", "Adel Nadjaran Toosi"], "title": "An ML-Driven Participant Selection Technique for Federated Recommendation System in Edge-Cloud Computing", "comment": null, "summary": "Recommendation systems (RS) personalize content by analyzing user\npreferences, but typically require centralized collection of user data, raising\nprivacy and scalability concerns. Federated Recommendation Systems (FRS)\naddress these issues by enabling distributed, privacy-preserving model training\nacross edge devices, keeping raw data on-device. Although existing FRS\nframeworks benefit from on-device feature extraction and privacy preservation,\nthey suffer from heterogeneous device capabilities, non-independent and\nidentically distributed (non-IID) data, and communication bottlenecks. To\novercome these limitations, we propose a multi-objective reinforcement learning\n(RL) participant selection that jointly optimizes historical client performance\nreputation (CPR), data utility, and system efficiency. First, we define a\ncomposite client-utility function combining CPR, system capability, and data\nquality. Next, we embed this utility into a multi-armed bandit (MAB) framework\nand dynamically balance exploration-exploitation to select participants.\nFinally, we practically implement our approach using the PySyft framework on an\nedge-cloud testbed, and evaluate it on a multimodal movie-recommendation task\nbuilt from the MovieLens-100K dataset. Across four different skewed\ndata-partition scenarios, our MAB-based selection accelerates convergence by\n32-50% in time-to-target AUC and reduces total wall-clock training time by up\nto 46%, while matching or slightly improving final AUC, NDCG@50, and Recall@50\ncompared to existing FRS baselines. Our results demonstrate that adaptive,\nreward-driven client sampling can substantially enhance both efficiency and\nfairness in real-world federated deployments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\uff0c\u4f18\u5316\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u7684\u6548\u7387\u548c\u516c\u5e73\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\uff08FRS\uff09\u901a\u8fc7\u5206\u5e03\u5f0f\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u6a21\u578b\u8bad\u7ec3\u89e3\u51b3\u4e86\u96c6\u4e2d\u5f0f\u63a8\u8350\u7cfb\u7edf\u7684\u9690\u79c1\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u6846\u67b6\u9762\u4e34\u8bbe\u5907\u80fd\u529b\u5f02\u6784\u3001\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08\u975eIID\uff09\u548c\u901a\u4fe1\u74f6\u9888\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u53c2\u4e0e\u8005\u9009\u62e9\u65b9\u6cd5\uff0c\u7ed3\u5408\u5386\u53f2\u5ba2\u6237\u7aef\u6027\u80fd\u58f0\u8a89\uff08CPR\uff09\u3001\u6570\u636e\u6548\u7528\u548c\u7cfb\u7edf\u6548\u7387\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u5b9a\u4e49\u4e86\u4e00\u4e2a\u7efc\u5408\u5ba2\u6237\u7aef\u6548\u7528\u51fd\u6570\uff0c\u968f\u540e\u5c06\u5176\u5d4c\u5165\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u6846\u67b6\u4e2d\uff0c\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u4e0e\u5f00\u53d1\u4ee5\u9009\u62e9\u53c2\u4e0e\u8005\u3002", "result": "\u5728\u56db\u79cd\u4e0d\u540c\u7684\u6570\u636e\u504f\u659c\u573a\u666f\u4e2d\uff0c\u57fa\u4e8eMAB\u7684\u9009\u62e9\u65b9\u6cd5\u5c06\u76ee\u6807AUC\u7684\u6536\u655b\u65f6\u95f4\u7f29\u77ed\u4e8632-50%\uff0c\u603b\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e8646%\uff0c\u540c\u65f6\u5728\u6700\u7ec8AUC\u3001NDCG@50\u548cRecall@50\u4e0a\u8fbe\u5230\u6216\u7565\u4f18\u4e8e\u73b0\u6709FRS\u57fa\u7ebf\u3002", "conclusion": "\u81ea\u9002\u5e94\u3001\u5956\u52b1\u9a71\u52a8\u7684\u5ba2\u6237\u7aef\u91c7\u6837\u80fd\u663e\u8457\u63d0\u5347\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u7684\u6548\u7387\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2507.15319", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15319", "abs": "https://arxiv.org/abs/2507.15319", "authors": ["Yannan Bai", "Debmalya Panigrahi", "Ian Zhang"], "title": "Language Generation in the Limit: Noise, Loss, and Feedback", "comment": null, "summary": "Kleinberg and Mullainathan (2024) recently proposed a formal framework called\nlanguage generation in the limit and showed that given a sequence of example\nstrings from an unknown target language drawn from any countable collection, an\nalgorithm can correctly generate unseen strings from the target language within\nfinite time. This notion was further refined by Li, Raman, and Tewari (2024),\nwho defined stricter categories of non-uniform and uniform generation. They\nshowed that a finite union of uniformly generatable collections is generatable\nin the limit, and asked if the same is true for non-uniform generation.\n  We begin by resolving the question in the negative: we give a uniformly\ngeneratable collection and a non-uniformly generatable collection whose union\nis not generatable in the limit. We then use facets of this construction to\nfurther our understanding of several variants of language generation. The first\ntwo, generation with noise and without samples, were introduced by Raman and\nRaman (2025) and Li, Raman, and Tewari (2024) respectively. We show the\nequivalence of these models for uniform and non-uniform generation, and provide\na characterization of non-uniform noisy generation. The former paper asked if\nthere is any separation between noisy and non-noisy generation in the limit --\nwe show that such a separation exists even with a single noisy string. Finally,\nwe study the framework of generation with feedback, introduced by Charikar and\nPabbaraju (2025), where the algorithm is strengthened by allowing it to ask\nmembership queries. We show finite queries add no power, but infinite queries\nyield a strictly more powerful model.\n  In summary, the results in this paper resolve the union-closedness of\nlanguage generation in the limit, and leverage those techniques (and others) to\ngive precise characterizations for natural variants that incorporate noise,\nloss, and feedback.", "AI": {"tldr": "\u672c\u6587\u89e3\u51b3\u4e86\u8bed\u8a00\u751f\u6210\u6781\u9650\u4e2d\u7684\u5e76\u96c6\u5c01\u95ed\u6027\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u7814\u7a76\u4e86\u566a\u58f0\u3001\u65e0\u6837\u672c\u53ca\u53cd\u9988\u751f\u6210\u7684\u53d8\u4f53\uff0c\u5c55\u793a\u4e86\u5176\u7b49\u4ef7\u6027\u548c\u5206\u79bb\u6027\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u751f\u6210\u6781\u9650\u95ee\u9898\u4e2d\u7684\u5e76\u96c6\u5c01\u95ed\u6027\uff0c\u5e76\u63a2\u7d22\u566a\u58f0\u3001\u4e22\u5931\u548c\u53cd\u9988\u5bf9\u751f\u6210\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6784\u9020\u4e00\u4e2a\u53ef\u7edf\u4e00\u751f\u6210\u7684\u96c6\u5408\u548c\u4e00\u4e2a\u975e\u7edf\u4e00\u751f\u6210\u7684\u96c6\u5408\uff0c\u8bc1\u660e\u5b83\u4eec\u7684\u5e76\u96c6\u5728\u6781\u9650\u5185\u4e0d\u53ef\u751f\u6210\u3002\u8fdb\u4e00\u6b65\u7814\u7a76\u4e86\u566a\u58f0\u751f\u6210\u3001\u65e0\u6837\u672c\u751f\u6210\u53ca\u53cd\u9988\u751f\u6210\u7b49\u53d8\u4f53\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u7b49\u4ef7\u6027\u53ca\u5206\u79bb\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5e76\u96c6\u5728\u6781\u9650\u5185\u4e0d\u53ef\u751f\u6210\uff0c\u5c55\u793a\u4e86\u566a\u58f0\u4e0e\u975e\u566a\u58f0\u751f\u6210\u6a21\u578b\u7684\u5206\u79bb\u6027\uff0c\u5e76\u6307\u51fa\u6709\u9650\u67e5\u8be2\u65e0\u589e\u5f3a\u80fd\u529b\u800c\u65e0\u9650\u67e5\u8be2\u5219\u4e25\u683c\u589e\u5f3a\u6a21\u578b\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u8bed\u8a00\u751f\u6210\u6781\u9650\u95ee\u9898\u4e2d\u7684\u5e76\u96c6\u5c01\u95ed\u6027\uff0c\u5e76\u5229\u7528\u76f8\u5173\u6280\u672f\uff08\u53ca\u5176\u4ed6\u65b9\u6cd5\uff09\u5bf9\u5305\u542b\u566a\u58f0\u3001\u4e22\u5931\u548c\u53cd\u9988\u7684\u81ea\u7136\u53d8\u4f53\u8fdb\u884c\u4e86\u7cbe\u786e\u523b\u753b\u3002"}}
{"id": "2507.14687", "categories": ["cs.SE", "68Q60, 03B70", "D.2.5"], "pdf": "https://arxiv.org/pdf/2507.14687", "abs": "https://arxiv.org/abs/2507.14687", "authors": ["Robin Lee", "Youngho Nam"], "title": "An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions", "comment": "10 pages, 5 figures", "summary": "Modified Condition/Decision Coverage (MC/DC) is a mandatory structural\ncoverage criterion for ensuring the reliability and safety of critical systems.\nWhile its strictest form, Unique-Cause MC/DC, offers the highest assurance,\nresearch on its efficient test generation has been lacking. This gap is\nparticularly significant, as an analysis of large-scale avionics systems shows\nthat 99.7% of all conditional decisions are, in fact, Singular Boolean\nExpressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This\npaper proposes 'Robin's Rule', a deterministic algorithm that directly\nconstructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause\nMC/DC for SBEs with N conditions, without generating a full truth table. To\nvalidate our approach, we constructed a benchmark by reformulating the TCAS-II\nspecifications into SBEs and verified the results using an industry-standard,\ncertified commercial tool. The results confirm that our method consistently\nachieves 100% coverage with the theoretical minimum number of tests and is more\nefficient than the commercial tool. This work provides a practical and provably\noptimal solution for verifying safety-critical systems, ensuring both rigor and\nefficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u2018Robin's Rule\u2019\u7b97\u6cd5\uff0c\u9ad8\u6548\u751f\u6210\u6700\u5c0f\u6d4b\u8bd5\u96c6\uff08N + 1\u4e2a\u6848\u4f8b\uff09\u5b9e\u73b0100% Unique-Cause MC/DC\u8986\u76d6SBEs\uff0c\u9a8c\u8bc1\u7ed3\u679c\u4f18\u4e8e\u5546\u4e1a\u5de5\u5177\u3002", "motivation": "\u5c3d\u7ba1Unique-Cause MC/DC\u4e3a\u5173\u952e\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6700\u9ad8\u4fdd\u8bc1\uff0c\u4f46\u5176\u9ad8\u6548\u6d4b\u8bd5\u751f\u6210\u7684\u7814\u7a76\u4e0d\u8db3\u3002\u5206\u6790\u663e\u793a99.7%\u7684\u6761\u4ef6\u51b3\u7b56\u4e3aSBEs\uff0c\u9002\u5408\u5e94\u7528Unique-Cause MC/DC\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u6700\u4f18\u7684\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u2018Robin's Rule\u2019\u7b97\u6cd5\uff0c\u76f4\u63a5\u6784\u5efa\u6700\u5c0f\u6d4b\u8bd5\u96c6\uff08N + 1\u4e2a\u6848\u4f8b\uff09\u5b9e\u73b0100% Unique-Cause MC/DC\u8986\u76d6SBEs\uff0c\u65e0\u9700\u751f\u6210\u5b8c\u6574\u771f\u503c\u8868\u3002\u901a\u8fc7\u5c06TCAS-II\u89c4\u8303\u91cd\u6784\u4e3aSBEs\u6784\u5efa\u57fa\u51c6\uff0c\u5e76\u4f7f\u7528\u884c\u4e1a\u6807\u51c6\u5546\u4e1a\u5de5\u5177\u9a8c\u8bc1\u7ed3\u679c\u3002", "result": "\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\u2018Robin's Rule\u2019\u59cb\u7ec8\u5b9e\u73b0100% Unique-Cause MC/DC\u8986\u76d6\uff0c\u4e14\u6d4b\u8bd5\u7528\u4f8b\u6570\u91cf\u4e3a\u7406\u8bba\u6700\u5c0f\u503c\uff08N + 1\uff09\uff0c\u6548\u7387\u9ad8\u4e8e\u5546\u4e1a\u5de5\u5177\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u2018Robin's Rule\u2019\uff0c\u4e00\u79cd\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u80fd\u591f\u76f4\u63a5\u6784\u5efa\u6700\u5c0f\u6d4b\u8bd5\u96c6\uff08N + 1\u4e2a\u6848\u4f8b\uff09\u4ee5\u786e\u4fdd100% Unique-Cause MC/DC\u8986\u76d6SBEs\uff0c\u65e0\u9700\u751f\u6210\u5b8c\u6574\u7684\u771f\u503c\u8868\u3002\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u59cb\u7ec8\u5b9e\u73b0100%\u8986\u76d6\u4e14\u6548\u7387\u9ad8\u4e8e\u5546\u4e1a\u5de5\u5177\uff0c\u4e3a\u9a8c\u8bc1\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u6700\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14234", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.14234", "abs": "https://arxiv.org/abs/2507.14234", "authors": ["Samer Nasser", "Henrique Duarte Moura", "Dragan Subotic", "Ritesh Kumar Singh", "Maarten Weyn", "Jeroen Famaey"], "title": "Feasibility of Energy Neutral Wildlife Tracking using Multi-Source Energy Harvesting", "comment": null, "summary": "Long-term wildlife tracking is crucial for biodiversity monitoring, but\nenergy limitations pose challenges, especially for animal tags, where replacing\nbatteries is impractical and stressful for the animal due to the need to\nlocate, possibly sedate, and handle it. Energy harvesting offers a sustainable\nalternative, yet most existing systems rely on a single energy source and\ninfrastructure-limited communication technologies. This paper presents an\nenergy-neutral system that combines solar and kinetic energy harvesting to\nenable the tracking and monitoring of wild animals. Harvesting from multiple\nsources increases the total available energy. Uniquely, the kinetic harvester\nalso serves as a motion proxy by sampling harvested current, enabling activity\nmonitoring without dedicated sensors. Our approach also ensures compatibility\nwith existing cellular infrastructure, using Narrowband Internet of Things\n(NB-IoT). We present a simulation framework that models energy harvesting,\nstorage, and consumption at the component level. An energy-aware scheduler\ncoordinates task execution based on real-time energy availability. We evaluate\nperformance under realistically varying conditions, comparing task frequencies\nand capacitor sizes. Results show that our approach maintains energy-neutral\noperation while significantly increasing data yield and reliability compared to\nsingle-source systems, with the ability to consistently sample GPS location\ndata and kinetic harvesting data every two minutes while transmitting these\nresults over NB-IoT every hour. These findings demonstrate the potential for\nmaintenance-free, environmentally friendly tracking in remote habitats,\nenabling more effective and scalable wildlife monitoring.", "AI": {"tldr": "A multi-source energy harvesting system (solar + kinetic) for wildlife tracking, using NB-IoT and an energy-aware scheduler, achieves energy-neutral operation with high data yield and reliability.", "motivation": "The motivation is to address the energy limitations in wildlife tracking by developing a sustainable, multi-source energy harvesting system that avoids the impracticality and stress of battery replacement in animal tags.", "method": "The method involves an energy-neutral system combining solar and kinetic energy harvesting, using NB-IoT for communication, and an energy-aware scheduler to coordinate tasks based on real-time energy availability. A simulation framework models energy harvesting, storage, and consumption at the component level.", "result": "Results show the system maintains energy-neutral operation, consistently sampling GPS and kinetic data every two minutes and transmitting hourly via NB-IoT, outperforming single-source systems in data yield and reliability.", "conclusion": "The paper concludes that combining solar and kinetic energy harvesting enables maintenance-free, environmentally friendly wildlife tracking, significantly improving data yield and reliability compared to single-source systems."}}
{"id": "2507.14455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14455", "abs": "https://arxiv.org/abs/2507.14455", "authors": ["Chun-Ming Yang", "Pranav A. Bhounsule"], "title": "Koopman Operator Based Time-Delay Embeddings and State History Augmented LQR for Periodic Hybrid Systems: Bouncing Pendulum and Bipedal Walking", "comment": null, "summary": "Time-delay embedding is a technique that uses snapshots of state history over\ntime to build a linear state space model of a nonlinear smooth system. We\ndemonstrate that periodic non-smooth or hybrid system can also be modeled as a\nlinear state space system using this approach as long as its behavior is\nconsistent in modes and timings. We extended time-delay embeddings to generate\na linear model of two periodic hybrid systems: the bouncing pendulum and the\nsimplest walker with control inputs. This leads to a novel state history\naugmented linear quadratic regulator (LQR) which uses current and past state\nhistory for feedback control.", "AI": {"tldr": "\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u6269\u5c55\u81f3\u5468\u671f\u6027\u975e\u5149\u6ed1\u7cfb\u7edf\uff0c\u751f\u6210\u7ebf\u6027\u6a21\u578b\u5e76\u5f00\u53d1\u65b0\u578bLQR\u63a7\u5236\u5668\u3002", "motivation": "\u8bc1\u660e\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u4e0d\u4ec5\u9002\u7528\u4e8e\u975e\u7ebf\u6027\u5149\u6ed1\u7cfb\u7edf\uff0c\u8fd8\u80fd\u7528\u4e8e\u5468\u671f\u6027\u975e\u5149\u6ed1\u6216\u6df7\u5408\u7cfb\u7edf\u3002", "method": "\u6269\u5c55\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\uff0c\u5e94\u7528\u4e8e\u5468\u671f\u6027\u6df7\u5408\u7cfb\u7edf\uff08\u5982\u53cd\u5f39\u6446\u548c\u6700\u7b80\u5355\u7684\u6b65\u884c\u5668\uff09\uff0c\u5e76\u751f\u6210\u7ebf\u6027\u6a21\u578b\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u53cd\u5f39\u6446\u548c\u6700\u7b80\u5355\u6b65\u884c\u5668\u7684\u7ebf\u6027\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u5386\u53f2\u7684LQR\u63a7\u5236\u5668\u3002", "conclusion": "\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u53ef\u4ee5\u6269\u5c55\u5230\u5468\u671f\u6027\u975e\u5149\u6ed1\u6216\u6df7\u5408\u7cfb\u7edf\uff0c\u751f\u6210\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u72b6\u6001\u5386\u53f2\u589e\u5f3a\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff08LQR\uff09\u3002"}}
{"id": "2507.14520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14520", "abs": "https://arxiv.org/abs/2507.14520", "authors": ["Xinyi Chen", "Yifei Yuan", "Jiaang Li", "Serge Belongie", "Maarten de Rijke", "Anders S\u00f8gaard"], "title": "What if Othello-Playing Language Models Could See?", "comment": "ICML 2025 Assessing World Models Workshop", "summary": "Language models are often said to face a symbol grounding problem. While some\nargue that world understanding can emerge from text alone, others suggest\ngrounded learning is more efficient. We explore this through Othello, where the\nboard state defines a simplified, rule-based world. Building on prior work, we\nintroduce VISOTHELLO, a multi-modal model trained on move histories and board\nimages. Using next-move prediction, we compare it to mono-modal baselines and\ntest robustness to semantically irrelevant perturbations. We find that\nmulti-modal training improves both performance and the robustness of internal\nrepresentations. These results suggest that grounding language in visual input\nhelps models infer structured world representations.", "AI": {"tldr": "\u591a\u6a21\u6001\u8bad\u7ec3\uff08\u7ed3\u5408\u89c6\u89c9\u8f93\u5165\uff09\u6bd4\u5355\u6a21\u6001\u66f4\u9ad8\u6548\uff0c\u80fd\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5bf9\u7ed3\u6784\u5316\u4e16\u754c\u7684\u7406\u89e3\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4ec5\u901a\u8fc7\u6587\u672c\u5c31\u80fd\u7406\u89e3\u4e16\u754c\uff0c\u8fd8\u662f\u9700\u8981\u57fa\u4e8e\u5b9e\u9645\u573a\u666f\u7684\u63a5\u5730\u5b66\u4e60\u66f4\u9ad8\u6548\u3002", "method": "\u5f15\u5165VISOTHELLO\u591a\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u79fb\u52a8\u5386\u53f2\u548c\u68cb\u76d8\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u7528\u4e0b\u4e00\u6b65\u79fb\u52a8\u9884\u6d4b\u4e0e\u5355\u6a21\u6001\u57fa\u7ebf\u6bd4\u8f83\uff0c\u5e76\u6d4b\u8bd5\u5bf9\u8bed\u4e49\u65e0\u5173\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "result": "\u591a\u6a21\u6001\u8bad\u7ec3\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u5185\u90e8\u8868\u5f81\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u8bad\u7ec3\uff08\u7ed3\u5408\u89c6\u89c9\u8f93\u5165\uff09\u6709\u52a9\u4e8e\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u7ed3\u6784\u5316\u4e16\u754c\u8868\u5f81\uff0c\u63d0\u5347\u6027\u80fd\u548c\u5185\u90e8\u8868\u5f81\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.15553", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.15553", "abs": "https://arxiv.org/abs/2507.15553", "authors": ["Shibo Yu", "Mohammad Goudarzi", "Adel Nadjaran Toosi"], "title": "Efficient Routing of Inference Requests across LLM Instances in Cloud-Edge Computing", "comment": null, "summary": "The rising demand for Large Language Model (LLM) inference services has\nintensified pressure on computational resources, resulting in latency and cost\nchallenges. This paper introduces a novel routing algorithm based on the\nNon-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference\nrequests across heterogeneous LLM instances in a cloud-edge computing\nenvironment. Formulated as a multi-objective optimization problem, the\nalgorithm balances response quality, response time, and inference cost,\nadapting to request heterogeneity (e.g., varying complexity and prompt lengths)\nand node diversity (e.g., edge vs. cloud resources). This adaptive routing\nalgorithm optimizes performance under dynamic workloads. We benchmark the\napproach using a testbed with datasets including Stanford Question Answering\nDataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With\nAdversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K).\nExperimental results show our solution, compared to the baselines, achieves up\nto 95.2% and 34.9% improvements in terms of response time and cost,\nrespectively. These findings validate the algorithm's effectiveness for\nscalable LLM deployments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNSGA-II\u7684\u8def\u7531\u7b97\u6cd5\uff0c\u4f18\u5316\u4e91\u8fb9\u73af\u5883\u4e2d\u7684LLM\u63a8\u7406\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u54cd\u5e94\u65f6\u95f4\u548c\u6210\u672c\u6548\u7387\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u670d\u52a1\u9700\u6c42\u7684\u589e\u957f\uff0c\u8ba1\u7b97\u8d44\u6e90\u538b\u529b\u52a0\u5267\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u548c\u6210\u672c\u95ee\u9898\u3002\u5982\u4f55\u5728\u5f02\u6784\u73af\u5883\u4e2d\u9ad8\u6548\u5206\u914d\u63a8\u7406\u8bf7\u6c42\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u652f\u914d\u6392\u5e8f\u9057\u4f20\u7b97\u6cd5II\uff08NSGA-II\uff09\u7684\u8def\u7531\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u4e91\u8fb9\u8ba1\u7b97\u73af\u5883\u4e2d\u5206\u914d\u5f02\u6784LLM\u5b9e\u4f8b\u7684\u63a8\u7406\u8bf7\u6c42\u3002\u8be5\u7b97\u6cd5\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5e73\u8861\u54cd\u5e94\u8d28\u91cf\u3001\u54cd\u5e94\u65f6\u95f4\u548c\u63a8\u7406\u6210\u672c\uff0c\u5e76\u9002\u5e94\u8bf7\u6c42\u7684\u5f02\u8d28\u6027\u548c\u8282\u70b9\u7684\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u7b97\u6cd5\u5728\u54cd\u5e94\u65f6\u95f4\u548c\u6210\u672c\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad895.2%\u548c34.9%\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8eNSGA-II\u7684\u8def\u7531\u7b97\u6cd5\u5728\u4e91\u8fb9\u8ba1\u7b97\u73af\u5883\u4e2d\u6709\u6548\u4f18\u5316\u4e86LLM\u63a8\u7406\u670d\u52a1\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u54cd\u5e94\u65f6\u95f4\u548c\u6210\u672c\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u53ef\u6269\u5c55LLM\u90e8\u7f72\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.15417", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.15417", "abs": "https://arxiv.org/abs/2507.15417", "authors": ["Dahoon Lee", "Chenglin Fan", "Euiwoong Lee"], "title": "1.64-Approximation for Chromatic Correlation Clustering via Chromatic Cluster LP", "comment": null, "summary": "Chromatic Correlation Clustering (CCC) generalizes Correlation Clustering by\nassigning multiple categorical relationships (colors) to edges and imposing\nchromatic constraints on the clusters. Unlike traditional Correlation\nClustering, which only deals with binary $(+/-)$ relationships, CCC captures\nricher relational structures. Despite its importance, improving the\napproximation for CCC has been difficult due to the limitations of standard LP\nrelaxations. We present a randomized $1.64$-approximation algorithm to the CCC\nproblem, significantly improving the previous factor of $2.15$. Our approach\nextends the cluster LP framework to the chromatic setting by introducing a\nchromatic cluster LP relaxation and an rounding algorithm that utilizes both a\ncluster-based and a greedy pivot-based strategy. The analysis bypasses the\nintegrality gap of $2$ for the CCC version of standard LP and highlights the\npotential of the cluster LP framework to address other variants of clustering\nproblems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd1.64-\u8fd1\u4f3c\u7b97\u6cd5\u7528\u4e8e\u8272\u76f8\u5173\u805a\u7c7b\u95ee\u9898\uff0c\u901a\u8fc7\u8272\u805a\u7c7bLP\u677e\u5f1b\u548c\u6df7\u5408\u820d\u5165\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd1\u4f3c\u6bd4\u5e76\u5c55\u793a\u4e86LP\u6846\u67b6\u7684\u6f5c\u529b\u3002", "motivation": "\u8272\u76f8\u5173\u805a\u7c7b\uff08CCC\uff09\u901a\u8fc7\u8d4b\u4e88\u8fb9\u591a\u91cd\u5206\u7c7b\u5173\u7cfb\uff08\u989c\u8272\uff09\u5e76\u5728\u805a\u7c7b\u4e0a\u65bd\u52a0\u8272\u7ea6\u675f\uff0c\u6bd4\u4f20\u7edf\u76f8\u5173\u805a\u7c7b\u4ec5\u5904\u7406\u4e8c\u5143\u5173\u7cfb\uff08+/-\uff09\u80fd\u6355\u6349\u66f4\u4e30\u5bcc\u7684\u7ed3\u6784\u5173\u7cfb\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6807\u51c6LP\u677e\u5f1b\u7684\u9650\u5236\uff0c\u63d0\u5347CCC\u7684\u8fd1\u4f3c\u6bd4\u4e00\u76f4\u5b58\u5728\u56f0\u96be\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a1.64-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u6269\u5c55\u4e86\u805a\u7c7bLP\u6846\u67b6\u81f3\u8272\u76f8\u5173\u805a\u7c7b\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u8272\u805a\u7c7bLP\u677e\u5f1b\u548c\u7ed3\u5408\u805a\u7c7b\u4e0e\u8d2a\u5a6a\u7b56\u7565\u7684\u820d\u5165\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u663e\u8457\u5c06\u8fd1\u4f3c\u6bd4\u4ece\u4e4b\u524d\u76842.15\u63d0\u5347\u81f31.64\uff0c\u7ed5\u8fc7\u4e86\u6807\u51c6LP\u5728CCC\u95ee\u9898\u4e0a\u7684\u79ef\u5206\u95f4\u96992\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u8272\u805a\u7c7bLP\u677e\u5f1b\u548c\u7ed3\u5408\u805a\u7c7b\u4e0e\u8d2a\u5a6a\u7b56\u7565\u7684\u820d\u5165\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8272\u76f8\u5173\u805a\u7c7b\u95ee\u9898\u7684\u8fd1\u4f3c\u6bd4\uff0c\u5c55\u793a\u4e86\u8272\u805a\u7c7bLP\u6846\u67b6\u5728\u89e3\u51b3\u5176\u4ed6\u805a\u7c7b\u53d8\u4f53\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14716", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14716", "abs": "https://arxiv.org/abs/2507.14716", "authors": ["Shahidul Islam", "Ashik Aowal", "Md Sharif Uddin", "Shaiful Chowdhury"], "title": "HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm", "comment": null, "summary": "Reconstructing a method's change history efficiently and accurately is\ncritical for many software engineering tasks, including maintenance,\nrefactoring, and comprehension. Despite the availability of method history\ngeneration tools such as CodeShovel and CodeTracker, existing evaluations of\ntheir effectiveness are limited by inaccuracies in the ground truth oracles\nused. In this study, we systematically construct two new oracles -- the\ncorrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by\ncombining automated analysis with expert-guided manual validation. We also\nintroduce HistoryFinder, a new method history generation tool designed to\nimprove not only the accuracy and completeness of method change histories but\nalso to offer competitive runtime performance. Through extensive evaluation\nacross 400 methods from 40 open-source repositories, we show that HistoryFinder\nconsistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based\nbaselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder\nachieves competitive runtime performance, offering the lowest mean and median\nexecution times among all the research-based tools.\n  While Git-based tools exhibit the fastest runtimes, this efficiency comes at\nthe cost of significantly lower precision and recall -- leaving HistoryFinder\nas the best overall choice when both accuracy and efficiency are important. To\nfacilitate adoption, we provide a web interface, CLI, and Java library for\nflexible usage.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86 HistoryFinder \u5de5\u5177\uff0c\u901a\u8fc7\u65b0\u57fa\u51c6\u9a8c\u8bc1\u5176\u5728\u65b9\u6cd5\u53d8\u66f4\u5386\u53f2\u91cd\u5efa\u4e2d\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\uff0c\u517c\u5177\u9ad8\u51c6\u786e\u6027\u548c\u9ad8\u6548\u8fd0\u884c\u65f6\u6027\u80fd\u3002", "motivation": "\u9ad8\u6548\u51c6\u786e\u5730\u91cd\u5efa\u65b9\u6cd5\u7684\u53d8\u66f4\u5386\u53f2\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff08\u5982\u7ef4\u62a4\u3001\u91cd\u6784\u548c\u7406\u89e3\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u7684\u8bc4\u4f30\u53d7\u9650\u4e8e\u57fa\u51c6\u7684\u4e0d\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u81ea\u52a8\u5316\u5206\u6790\u548c\u4e13\u5bb6\u6307\u5bfc\u7684\u624b\u52a8\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u6784\u5efa\u4e86\u4e24\u4e2a\u65b0\u7684\u57fa\u51c6\uff08\u4fee\u6b63\u7684 CodeShovel \u57fa\u51c6\u548c\u65b0\u5f00\u53d1\u7684 HistoryFinder \u57fa\u51c6\uff09\uff0c\u5e76\u5f00\u53d1\u4e86 HistoryFinder \u5de5\u5177\u3002", "result": "HistoryFinder \u5728 40 \u4e2a\u5f00\u6e90\u4ed3\u5e93\u7684 400 \u4e2a\u65b9\u6cd5\u4e0a\u8bc4\u4f30\u4e2d\uff0c\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548c F1 \u5206\u6570\u4e0a\u5747\u4f18\u4e8e CodeShovel\u3001CodeTracker\u3001IntelliJ \u548c\u57fa\u4e8e Git \u7684\u57fa\u7ebf\u5de5\u5177\uff0c\u540c\u65f6\u8fd0\u884c\u65f6\u6027\u80fd\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "HistoryFinder \u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\uff0c\u662f\u65b9\u6cd5\u53d8\u66f4\u5386\u53f2\u91cd\u5efa\u7684\u6700\u4f73\u9009\u62e9\u3002"}}
{"id": "2507.14263", "categories": ["cs.NI", "cs.AI", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.14263", "abs": "https://arxiv.org/abs/2507.14263", "authors": ["Ramesh Raskar", "Pradyumna Chari", "John Zinky", "Mahesh Lambe", "Jared James Grogan", "Sichao Wang", "Rajesh Ranjan", "Rekha Singhal", "Shailja Gupta", "Robert Lincourt", "Raghu Bala", "Aditi Joshi", "Abhishek Singh", "Ayush Chopra", "Dimitris Stripelis", "Bhuwan B", "Sumit Kumar", "Maria Gorskikh"], "title": "Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts", "comment": null, "summary": "The Internet is poised to host billions to trillions of autonomous AI agents\nthat negotiate, delegate, and migrate in milliseconds and workloads that will\nstrain DNS-centred identity and discovery. In this paper, we describe the NANDA\nindex architecture, which we envision as a means for discoverability,\nidentifiability and authentication in the internet of AI agents. We present an\narchitecture where a minimal lean index resolves to dynamic, cryptographically\nverifiable AgentFacts that supports multi-endpoint routing, load balancing,\nprivacy-preserving access, and credentialed capability assertions. Our\narchitecture design delivers five concrete guarantees: (1) A quilt-like index\nproposal that supports both NANDA-native agents as well as third party agents\nbeing discoverable via the index, (2) rapid global resolution for newly spawned\nAI agents, (3) sub-second revocation and key rotation, (4) schema-validated\ncapability assertions, and (5) privacy-preserving discovery across\norganisational boundaries via verifiable, least-disclosure queries. We\nformalize the AgentFacts schema, specify a CRDT-based update protocol, and\nprototype adaptive resolvers. The result is a lightweight, horizontally\nscalable foundation that unlocks secure, trust-aware collaboration for the next\ngeneration of the Internet of AI agents, without abandoning existing web\ninfrastructure.", "AI": {"tldr": "NANDA\u7d22\u5f15\u67b6\u6784\u4e3aAI\u4ee3\u7406\u4e92\u8054\u7f51\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5feb\u901f\u53d1\u73b0\u3001\u5b89\u5168\u8ba4\u8bc1\u548c\u9690\u79c1\u4fdd\u62a4\uff0c\u540c\u65f6\u517c\u5bb9\u73b0\u6709\u7f51\u7edc\u3002", "motivation": "\u4e92\u8054\u7f51\u5c06\u627f\u8f7d\u6570\u5341\u4ebf\u81f3\u6570\u4e07\u4ebf\u81ea\u4e3bAI\u4ee3\u7406\uff0c\u73b0\u6709DNS\u4e2d\u5fc3\u7684\u8eab\u4efd\u548c\u53d1\u73b0\u673a\u5236\u5c06\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u3001\u53ef\u5bc6\u7801\u9a8c\u8bc1\u7684AgentFacts\u7684\u67b6\u6784\uff0c\u5305\u62ecCRDT-based\u66f4\u65b0\u534f\u8bae\u548c\u81ea\u9002\u5e94\u89e3\u6790\u5668\u539f\u578b\u3002", "result": "NANDA\u67b6\u6784\u5b9e\u73b0\u4e86\u4e94\u4e2a\u5177\u4f53\u4fdd\u8bc1\uff1a\u652f\u6301\u7b2c\u4e09\u65b9\u4ee3\u7406\u53d1\u73b0\u3001\u5feb\u901f\u5168\u5c40\u89e3\u6790\u3001\u79d2\u7ea7\u64a4\u9500\u548c\u5bc6\u94a5\u8f6e\u6362\u3001\u6a21\u5f0f\u9a8c\u8bc1\u80fd\u529b\u65ad\u8a00\u3001\u4ee5\u53ca\u9690\u79c1\u4fdd\u62a4\u53d1\u73b0\u3002", "conclusion": "NANDA\u7d22\u5f15\u67b6\u6784\u4e3aAI\u4ee3\u7406\u4e92\u8054\u7f51\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6c34\u5e73\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u652f\u6301\u5b89\u5168\u3001\u4fe1\u4efb\u611f\u77e5\u7684\u534f\u4f5c\uff0c\u540c\u65f6\u517c\u5bb9\u73b0\u6709\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2507.14538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14538", "abs": "https://arxiv.org/abs/2507.14538", "authors": ["Jin Chai", "Xiang Yao", "Mengfan Hou", "Yanghong Li", "Erbao Dong"], "title": "A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ Hand-0", "comment": null, "summary": "CYJ Hand-0 is a 21-DOF humanoid dexterous hand featuring a hybrid\ntendon-driven actuation system that combines shape memory alloys (SMAs) and DC\nmotors. The hand employs high-strength fishing line as artificial tendons and\nuses a fully 3D-printed AlSi10Mg metal frame designed to replicate the skeletal\nand tendon-muscle structure of the human hand. A linear motor-driven module\ncontrols finger flexion, while an SMA-based module enables finger extension and\nlateral abduction. These modules are integrated into a compact hybrid actuation\nunit mounted on a custom rear support structure. Mechanical and kinematic\nexperiments, conducted under an Arduino Mega 2560-based control system,\nvalidate the effectiveness of the design and demonstrate its biomimetic\ndexterity.", "AI": {"tldr": "CYJ Hand-0\u662f\u4e00\u79cd21\u81ea\u7531\u5ea6\u4eff\u751f\u7075\u5de7\u624b\uff0c\u91c7\u7528\u6df7\u5408\u9a71\u52a8\u7cfb\u7edf\uff08SMA\u548cDC\u7535\u673a\uff09\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u8bbe\u8ba1\u6709\u6548\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u7ed3\u5408\u5f62\u72b6\u8bb0\u5fc6\u5408\u91d1\u548c\u76f4\u6d41\u7535\u673a\u7684\u6df7\u5408\u9a71\u52a8\u7cfb\u7edf\uff0c\u4ee5\u6a21\u4eff\u4eba\u624b\u9aa8\u9abc\u548c\u808c\u8171\u808c\u8089\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u9ad8\u5f3a\u5ea6\u9493\u9c7c\u7ebf\u4f5c\u4e3a\u4eba\u5de5\u808c\u8171\uff0c3D\u6253\u5370AlSi10Mg\u91d1\u5c5e\u6846\u67b6\uff0c\u7ed3\u5408\u7ebf\u6027\u7535\u673a\u548cSMA\u6a21\u5757\u63a7\u5236\u624b\u6307\u8fd0\u52a8\u3002", "result": "\u673a\u68b0\u548c\u8fd0\u52a8\u5b66\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u4eff\u751f\u7075\u5de7\u6027\u3002", "conclusion": "CYJ Hand-0\u7684\u8bbe\u8ba1\u548c\u6df7\u5408\u9a71\u52a8\u7cfb\u7edf\u88ab\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\uff0c\u5c55\u793a\u4e86\u4eff\u751f\u7075\u5de7\u6027\u3002"}}
{"id": "2507.14552", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14552", "abs": "https://arxiv.org/abs/2507.14552", "authors": ["Anna Sofia Lippolis", "Mohammad Javad Saeedizade", "Robin Keskis\u00e4rkk\u00e4", "Aldo Gangemi", "Eva Blomqvist", "Andrea Giovanni Nuzzolese"], "title": "Large Language Models Assisting Ontology Evaluation", "comment": null, "summary": "Ontology evaluation through functional requirements, such as testing via\ncompetency question (CQ) verification, is a well-established yet costly,\nlabour-intensive, and error-prone endeavour, even for ontology engineering\nexperts. In this work, we introduce OE-Assist, a novel framework designed to\nassist ontology evaluation through automated and semi-automated CQ\nverification. By presenting and leveraging a dataset of 1,393 CQs paired with\ncorresponding ontologies and ontology stories, our contributions present, to\nour knowledge, the first systematic investigation into large language model\n(LLM)-assisted ontology evaluation, and include: (i) evaluating the\neffectiveness of a LLM-based approach for automatically performing CQ\nverification against a manually created gold standard, and (ii) developing and\nassessing an LLM-powered framework to assist CQ verification with Prot\\'eg\\'e,\nby providing suggestions. We found that automated LLM-based evaluation with\no1-preview and o3-mini perform at a similar level to the average user's\nperformance.", "AI": {"tldr": "OE-Assist\u662f\u4e00\u4e2a\u5229\u7528LLM\u81ea\u52a8\u5316\u548c\u534a\u81ea\u52a8\u5316CQ\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u6027\u80fd\u4e0e\u4eba\u5de5\u76f8\u5f53\u3002", "motivation": "\u672c\u4f53\u8bc4\u4f30\u4e2d\u7684\u529f\u80fd\u9700\u6c42\u6d4b\u8bd5\uff08\u5982CQ\u9a8c\u8bc1\uff09\u6210\u672c\u9ad8\u3001\u52b3\u52a8\u5bc6\u96c6\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165OE-Assist\u6846\u67b6\uff0c\u5229\u75281,393\u4e2aCQ\u53ca\u5176\u5bf9\u5e94\u672c\u4f53\u548c\u672c\u4f53\u6545\u4e8b\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30LLM\u5728\u81ea\u52a8\u6267\u884cCQ\u9a8c\u8bc1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u6846\u67b6\u6765\u8f85\u52a9CQ\u9a8c\u8bc1\u3002", "result": "\u81ea\u52a8\u5316LLM\u8bc4\u4f30\uff08\u4f7f\u7528o1-preview\u548co3-mini\uff09\u7684\u6027\u80fd\u4e0e\u666e\u901a\u7528\u6237\u7684\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "OE-Assist\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u534a\u81ea\u52a8\u5316\u7684CQ\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86LLM\u5728\u534f\u52a9\u672c\u4f53\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\uff0c\u5176\u6027\u80fd\u4e0e\u666e\u901a\u7528\u6237\u76f8\u5f53\u3002"}}
{"id": "2507.15658", "categories": ["cs.DS", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15658", "abs": "https://arxiv.org/abs/2507.15658", "authors": ["Romain Cosson", "Laurent Massouli\u00e9"], "title": "Asynchronous Collective Tree Exploration: a Distributed Algorithm, and a new Lower Bound", "comment": null, "summary": "We study the problem of collective tree exploration in which a team of $k$\nmobile agents must collectively visit all nodes of an unknown tree in as few\nmoves as possible. The agents all start from the root and discover adjacent\nedges as they progress in the tree. Communication is distributed in the sense\nthat agents share information by reading and writing on whiteboards located at\nall nodes. Movements are asynchronous, in the sense that the speeds of all\nagents are controlled by an adversary at all times. All previous competitive\nguarantees for collective tree exploration are either distributed but\nsynchronous, or asynchronous but centralized. In contrast, we present a\ndistributed asynchronous algorithm that explores any tree of $n$ nodes and\ndepth $D$ in at most $2n+O(k^2 2^kD)$ moves, i.e., with a regret that is linear\nin $D$, and a variant algorithm with a guarantee in $O(k/\\log k)(n+kD)$, i.e.,\nwith a competitive ratio in $O(k/\\log k)$. We note that our regret guarantee is\nasymptotically optimal (i.e., $1$-competitive) from the perspective of\naverage-case complexity. We then present a new general lower bound on the\ncompetitive ratio of asynchronous collective tree exploration, in\n$\\Omega(\\log^2 k)$. This lower bound applies to both the distributed and\ncentralized settings, and improves upon the previous lower bound in\n$\\Omega(\\log k)$.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u5f02\u6b65\u96c6\u4f53\u6811\u63a2\u7d22\u7b97\u6cd5\uff0c\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\uff0c\u5e76\u6539\u8fdb\u4e86\u7ade\u4e89\u6bd4\u4e0b\u754c\u3002", "motivation": "\u7814\u7a76\u96c6\u4f53\u6811\u63a2\u7d22\u95ee\u9898\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u662f\u5206\u5e03\u5f0f\u7684\u4f46\u540c\u6b65\u7684\uff0c\u8981\u4e48\u662f\u5f02\u6b65\u7684\u4f46\u96c6\u4e2d\u5f0f\u7684\u5c40\u9650\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u5f02\u6b65\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7406\u5728\u6811\u7684\u8282\u70b9\u4e0a\u8bfb\u5199\u767d\u677f\u8fdb\u884c\u901a\u4fe1\uff0c\u63a2\u7d22\u672a\u77e5\u6811\u7ed3\u6784\u3002\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a2n+O(k^2 2^kD)\u6216O(k/log k)(n+kD)\u3002", "result": "\u7b97\u6cd5\u5728\u6811\u63a2\u7d22\u4e2d\u7684\u79fb\u52a8\u6b21\u6570\u4e3a2n+O(k^2 2^kD)\u6216O(k/log k)(n+kD)\uff0c\u7ade\u4e89\u6bd4\u4e3aO(k/log k)\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86\u65b0\u7684\u7ade\u4e89\u6bd4\u4e0b\u754c\u03a9(log\u00b2k)\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u5f02\u6b65\u7b97\u6cd5\uff0c\u7528\u4e8e\u96c6\u4f53\u6811\u63a2\u7d22\u95ee\u9898\uff0c\u5176\u6027\u80fd\u5728\u5e73\u5747\u60c5\u51b5\u4e0b\u662f\u6700\u4f18\u7684\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u7ade\u4e89\u6bd4\u4e0b\u754c\uff0c\u6539\u8fdb\u4e86\u4e4b\u524d\u7684\u7ed3\u679c\u3002"}}
{"id": "2507.15434", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.15434", "abs": "https://arxiv.org/abs/2507.15434", "authors": ["Yi-Ting Hsieh", "Mong-Jen Kao", "Jhong-Yun Liu", "Hung-Lung Wang"], "title": "Job Scheduling under Base and Additional Fees, with Applications to Mixed-Criticality Scheduling", "comment": null, "summary": "We are concerned with the problem of scheduling $n$ jobs onto $m$ identical\nmachines. Each machine has to be in operation for a prescribed time, and the\nobjective is to minimize the total machine working time. Precisely, let $c_i$\nbe the prescribed time for machine $i$, where $i\\in[m]$, and $p_j$ be the\nprocessing time for job $j$, where $j\\in[n]$. The problem asks for a schedule\n$\\sigma\\colon\\, J\\to M$ such that $\\sum_{i=1}^m\\max\\{c_i,\n\\sum_{j\\in\\sigma^{-1}(i)}p_j\\}$ is minimized, where $J$ and $M$ denote the sets\nof jobs and machines, respectively. We show that First Fit Decreasing (FFD)\nleads to a $1.5$-approximation, and this problem admits a polynomial-time\napproximation scheme (PTAS). The idea is further applied to mixed-criticality\nsystem scheduling to yield improved approximation results.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f5c\u4e1a\u8c03\u5ea6\u95ee\u9898\uff0c\u8bc1\u660eFFD\u7b97\u6cd5\u80fd\u8fbe\u52301.5\u8fd1\u4f3c\u6bd4\uff0c\u5e76\u63d0\u51fa\u4e86PTAS\u3002\u8be5\u65b9\u6cd5\u8fd8\u88ab\u5e94\u7528\u4e8e\u6df7\u5408\u5173\u952e\u6027\u7cfb\u7edf\u8c03\u5ea6\uff0c\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u5c06n\u4e2a\u4f5c\u4e1a\u8c03\u5ea6\u5230m\u53f0\u76f8\u540c\u673a\u5668\u4e0a\u7684\u95ee\u9898\uff0c\u76ee\u6807\u662f\u6700\u5c0f\u5316\u603b\u673a\u5668\u5de5\u4f5c\u65f6\u95f4\u3002", "method": "\u91c7\u7528First Fit Decreasing (FFD)\u7b97\u6cd5\uff0c\u5e76\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u65b9\u6848(PTAS)\u3002", "result": "FFD\u7b97\u6cd5\u5b9e\u73b0\u4e861.5\u8fd1\u4f3c\u6bd4\uff0c\u4e14\u95ee\u9898\u5b58\u5728PTAS\u3002\u8be5\u65b9\u6cd5\u5728\u6df7\u5408\u5173\u952e\u6027\u7cfb\u7edf\u8c03\u5ea6\u4e2d\u4e5f\u53d6\u5f97\u4e86\u6539\u8fdb\u7684\u8fd1\u4f3c\u7ed3\u679c\u3002", "conclusion": "\u8bba\u6587\u8bc1\u660e\u4e86First Fit Decreasing (FFD)\u7b97\u6cd5\u5728\u8c03\u5ea6\u95ee\u9898\u4e2d\u80fd\u8fbe\u52301.5\u8fd1\u4f3c\u6bd4\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u65b9\u6848(PTAS)\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u88ab\u5e94\u7528\u4e8e\u6df7\u5408\u5173\u952e\u6027\u7cfb\u7edf\u8c03\u5ea6\uff0c\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u8fd1\u4f3c\u7ed3\u679c\u3002"}}
{"id": "2507.14735", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14735", "abs": "https://arxiv.org/abs/2507.14735", "authors": ["Vladyslav Bulhakov", "Giordano d'Aloisio", "Claudio Di Sipio", "Antinisca Di Marco", "Davide Di Ruscio"], "title": "Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling", "comment": "Accepted at 51st Euromicro Conference Series on Software Engineering\n  and Advanced Applications (SEAA)", "summary": "The introduction of large language models (LLMs) has enhanced automation in\nsoftware engineering tasks, including in Model Driven Engineering (MDE).\nHowever, using general-purpose LLMs for domain modeling has its limitations.\nOne approach is to adopt fine-tuned models, but this requires significant\ncomputational resources and can lead to issues like catastrophic forgetting.\n  This paper explores how hyperparameter tuning and prompt engineering can\nimprove the accuracy of the Llama 3.1 model for generating domain models from\ntextual descriptions. We use search-based methods to tune hyperparameters for a\nspecific medical data model, resulting in a notable quality improvement over\nthe baseline LLM. We then test the optimized hyperparameters across ten diverse\napplication domains.\n  While the solutions were not universally applicable, we demonstrate that\ncombining hyperparameter tuning with prompt engineering can enhance results\nacross nearly all examined domain models.", "AI": {"tldr": "\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u63d0\u793a\u5de5\u7a0b\u63d0\u5347Llama 3.1\u6a21\u578b\u5728\u9886\u57df\u6a21\u578b\u751f\u6210\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u6570\u6d4b\u8bd5\u9886\u57df\u4e2d\u6709\u6548\u3002", "motivation": "\u901a\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9886\u57df\u5efa\u6a21\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u5fae\u8c03\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u4e14\u53ef\u80fd\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u56e0\u6b64\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u63d0\u793a\u5de5\u7a0b\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u641c\u7d22\u7684\u65b9\u6cd5\u5bf9\u7279\u5b9a\u533b\u7597\u6570\u636e\u6a21\u578b\u7684\u8d85\u53c2\u6570\u8fdb\u884c\u8c03\u4f18\uff0c\u5e76\u5728\u5341\u4e2a\u4e0d\u540c\u5e94\u7528\u9886\u57df\u6d4b\u8bd5\u4f18\u5316\u540e\u7684\u8d85\u53c2\u6570\u3002", "result": "\u4f18\u5316\u540e\u7684\u8d85\u53c2\u6570\u5728\u533b\u7597\u6570\u636e\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u5728\u591a\u6570\u6d4b\u8bd5\u9886\u57df\u4e2d\u8868\u73b0\u51fa\u6539\u8fdb\u6548\u679c\u3002", "conclusion": "\u7ed3\u5408\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u63d0\u793a\u5de5\u7a0b\u53ef\u4ee5\u663e\u8457\u63d0\u5347Llama 3.1\u6a21\u578b\u5728\u751f\u6210\u9886\u57df\u6a21\u578b\u65f6\u7684\u51c6\u786e\u6027\uff0c\u5c3d\u7ba1\u89e3\u51b3\u65b9\u6848\u5e76\u975e\u666e\u904d\u9002\u7528\uff0c\u4f46\u5728\u51e0\u4e4e\u6240\u6709\u6d4b\u8bd5\u9886\u57df\u6a21\u578b\u4e2d\u5747\u8868\u73b0\u51fa\u6539\u8fdb\u6548\u679c\u3002"}}
{"id": "2507.14398", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.14398", "abs": "https://arxiv.org/abs/2507.14398", "authors": ["Md. Kamrul Hossain", "Walid Aljoby"], "title": "NetIntent: Leveraging Large Language Models for End-to-End Intent-Based SDN Automation", "comment": null, "summary": "Intent-Based Networking (IBN) often leverages the programmability of\nSoftware-Defined Networking (SDN) to simplify network management. However,\nsignificant challenges remain in automating the entire pipeline, from\nuser-specified high-level intents to device-specific low-level configurations.\nExisting solutions often rely on rigid, rule-based translators and fixed APIs,\nlimiting extensibility and adaptability. By contrast, recent advances in large\nlanguage models (LLMs) offer a promising pathway that leverages natural\nlanguage understanding and flexible reasoning. However, it is unclear to what\nextent LLMs can perform IBN tasks. To address this, we introduce IBNBench, a\nfirst-of-its-kind benchmarking suite comprising four novel datasets:\nIntent2Flow-ODL, Intent2Flow-ONOS, FlowConflict-ODL, and FlowConflict-ONOS.\nThese datasets are specifically designed for evaluating LLMs performance in\nintent translation and conflict detection tasks within the industry-grade SDN\ncontrollers ODL and ONOS. Our results provide the first comprehensive\ncomparison of 33 open-source LLMs on IBNBench and related datasets, revealing a\nwide range of performance outcomes. However, while these results demonstrate\nthe potential of LLMs for isolated IBN tasks, integrating LLMs into a fully\nautonomous IBN pipeline remains unexplored. Thus, our second contribution is\nNetIntent, a unified and adaptable framework that leverages LLMs to automate\nthe full IBN lifecycle, including translation, activation, and assurance within\nSDN systems. NetIntent orchestrates both LLM and non-LLM agents, supporting\ndynamic re-prompting and contextual feedback to robustly execute user-defined\nintents with minimal human intervention. Our implementation of NetIntent across\nboth ODL and ONOS SDN controllers achieves a consistent and adaptive end-to-end\nIBN realization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faIBNBench\u8bc4\u4f30LLM\u5728IBN\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f00\u53d1NetIntent\u6846\u67b6\u81ea\u52a8\u5316\u5168\u751f\u547d\u5468\u671fIBN\uff0c\u5c55\u793a\u4e86LLM\u7684\u6f5c\u529b\u3002", "motivation": "\u610f\u56fe\u7f51\u7edc\uff08IBN\uff09\u5728\u7b80\u5316\u7f51\u7edc\u7ba1\u7406\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u81ea\u52a8\u5316\u548c\u7075\u6d3b\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "method": "\u5f15\u5165IBNBench\u4f5c\u4e3a\u9996\u4e2a\u8bc4\u4f30\u5957\u4ef6\uff0c\u5305\u542b\u56db\u4e2a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u610f\u56fe\u7ffb\u8bd1\u548c\u51b2\u7a81\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u968f\u540e\u63d0\u51faNetIntent\u6846\u67b6\uff0c\u5229\u7528LLM\u548c\u975eLLM\u4ee3\u7406\u81ea\u52a8\u5316IBN\u5168\u751f\u547d\u5468\u671f\u3002", "result": "IBNBench\u9996\u6b21\u5168\u9762\u6bd4\u8f83\u4e8633\u4e2a\u5f00\u6e90LLM\u7684\u6027\u80fd\uff0cNetIntent\u6846\u67b6\u5728ODL\u548cONOS SDN\u63a7\u5236\u5668\u4e0a\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u7aef\u5230\u7aefIBN\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86IBNBench\u548cNetIntent\uff0c\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u610f\u56fe\u7f51\u7edc\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u5168\u751f\u547d\u5468\u671fIBN\u7684\u6846\u67b6\u3002"}}
{"id": "2507.14582", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14582", "abs": "https://arxiv.org/abs/2507.14582", "authors": ["Zezhi Liu", "Shizhen Wu", "Hanqian Luo", "Deyun Qin", "Yongchun Fang"], "title": "BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree, Temporal Logic and Dynamical Movement Primitives", "comment": "11 pages, 8 figures", "summary": "In the field of Learning from Demonstration (LfD), enabling robots to\ngeneralize learned manipulation skills to novel scenarios for long-horizon\ntasks remains challenging. Specifically, it is still difficult for robots to\nadapt the learned skills to new environments with different task and motion\nrequirements, especially in long-horizon, multi-stage scenarios with intricate\nconstraints. This paper proposes a novel hierarchical framework, called\nBT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and\nDynamical Movement Primitives (DMPs) to address this problem. Within this\nframework, Signal Temporal Logic (STL) is employed to formally specify complex,\nlong-horizon task requirements and constraints. These STL specifications are\nsystematically transformed to generate reactive and modular BTs for high-level\ndecision-making task structure. An STL-constrained DMP optimization method is\nproposed to optimize the DMP forcing term, allowing the learned motion\nprimitives to adapt flexibly while satisfying intricate spatiotemporal\nrequirements and, crucially, preserving the essential dynamics learned from\ndemonstrations. The framework is validated through simulations demonstrating\ngeneralization capabilities under various STL constraints and real-world\nexperiments on several long-horizon robotic manipulation tasks. The results\ndemonstrate that the proposed framework effectively bridges the symbolic-motion\ngap, enabling more reliable and generalizable autonomous manipulation for\ncomplex robotic tasks.", "AI": {"tldr": "A hierarchical framework (BT-TL-DMPs) combining Behavior Trees, Temporal Logic, and DMPs improves robot skill generalization in complex, long-horizon tasks by bridging symbolic and motion planning.", "motivation": "Addressing the challenge of generalizing learned manipulation skills to novel scenarios in long-horizon tasks, especially in environments with different task and motion requirements.", "method": "The framework integrates Behavior Tree (BT), Temporal Logic (TL), and Dynamical Movement Primitives (DMPs), employing Signal Temporal Logic (STL) to specify task requirements and constraints, which are transformed into modular BTs for decision-making, and an STL-constrained DMP optimization method to adapt motion primitives.", "result": "Simulations and real-world experiments validate the framework's generalization capabilities under various STL constraints, demonstrating its effectiveness in complex robotic manipulation tasks.", "conclusion": "The proposed hierarchical framework BT-TL-DMPs effectively bridges the symbolic-motion gap, enabling reliable and generalizable autonomous manipulation for complex robotic tasks."}}
{"id": "2507.14593", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14593", "abs": "https://arxiv.org/abs/2507.14593", "authors": ["Omar Al-Desi"], "title": "Coordinate Heart System: A Geometric Framework for Emotion Representation", "comment": "26 pages", "summary": "This paper presents the Coordinate Heart System (CHS), a geometric framework\nfor emotion representation in artificial intelligence applications. We position\neight core emotions as coordinates on a unit circle, enabling mathematical\ncomputation of complex emotional states through coordinate mixing and vector\noperations. Our initial five-emotion model revealed significant coverage gaps\nin the emotion space, leading to the development of an eight-emotion system\nthat provides complete geometric coverage with mathematical guarantees. The\nframework converts natural language input to emotion coordinates and supports\nreal-time emotion interpolation through computational algorithms. The system\nintroduces a re-calibrated stability parameter S in [0,1], which dynamically\nintegrates emotional load, conflict resolution, and contextual drain factors.\nThis stability model leverages advanced Large Language Model interpretation of\ntextual cues and incorporates hybrid temporal tracking mechanisms to provide\nnuanced assessment of psychological well-being states. Our key contributions\ninclude: (i) mathematical proof demonstrating why five emotions are\ninsufficient for complete geometric coverage, (ii) an eight-coordinate system\nthat eliminates representational blind spots, (iii) novel algorithms for\nemotion mixing, conflict resolution, and distance calculation in emotion space,\nand (iv) a comprehensive computational framework for AI emotion recognition\nwith enhanced multi-dimensional stability modeling. Experimental validation\nthrough case studies demonstrates the system's capability to handle emotionally\nconflicted states, contextual distress factors, and complex psychological\nscenarios that traditional categorical emotion models cannot adequately\nrepresent. This work establishes a new mathematical foundation for emotion\nmodeling in artificial intelligence systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u51e0\u4f55\u6846\u67b6\u7684\u60c5\u611f\u8868\u793a\u7cfb\u7edf\uff0c\u901a\u8fc7\u516b\u5750\u6807\u5355\u4f4d\u5706\u548c\u6570\u5b66\u8fd0\u7b97\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u9a8c\u8bc1\u5176\u5904\u7406\u590d\u6742\u60c5\u611f\u7684\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5206\u7c7b\u60c5\u611f\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u8868\u793a\u60c5\u611f\u51b2\u7a81\u72b6\u6001\u548c\u590d\u6742\u5fc3\u7406\u573a\u666f\u7684\u95ee\u9898\uff0c\u586b\u8865\u60c5\u611f\u7a7a\u95f4\u7684\u8986\u76d6\u7a7a\u767d\u3002", "method": "\u63d0\u51faCoordinate Heart System\uff08CHS\uff09\uff0c\u5c06\u516b\u79cd\u6838\u5fc3\u60c5\u611f\u5b9a\u4f4d\u4e3a\u5355\u4f4d\u5706\u4e0a\u7684\u5750\u6807\uff0c\u652f\u6301\u901a\u8fc7\u5750\u6807\u6df7\u5408\u548c\u5411\u91cf\u8fd0\u7b97\u8fdb\u884c\u590d\u6742\u60c5\u611f\u72b6\u6001\u7684\u6570\u5b66\u8ba1\u7b97\u3002\u7cfb\u7edf\u5f15\u5165\u7a33\u5b9a\u6027\u53c2\u6570S\uff0c\u52a8\u6001\u6574\u5408\u60c5\u611f\u8d1f\u8377\u3001\u51b2\u7a81\u89e3\u51b3\u548c\u60c5\u5883\u6d88\u8017\u56e0\u7d20\u3002", "result": "\u5f00\u53d1\u4e86\u516b\u60c5\u611f\u7cfb\u7edf\uff0c\u63d0\u4f9b\u5b8c\u6574\u7684\u51e0\u4f55\u8986\u76d6\uff0c\u5e76\u63d0\u51fa\u60c5\u611f\u6df7\u5408\u3001\u51b2\u7a81\u89e3\u51b3\u548c\u8ddd\u79bb\u8ba1\u7b97\u7684\u65b0\u7b97\u6cd5\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5904\u7406\u590d\u6742\u60c5\u611f\u72b6\u6001\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u8bba\u6587\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u4eba\u5de5\u667a\u80fd\u60c5\u611f\u5efa\u6a21\u6570\u5b66\u57fa\u7840\uff0c\u901a\u8fc7\u516b\u5750\u6807\u7cfb\u7edf\u6d88\u9664\u4e86\u60c5\u611f\u8868\u793a\u7684\u76f2\u70b9\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5904\u7406\u60c5\u611f\u51b2\u7a81\u548c\u590d\u6742\u5fc3\u7406\u573a\u666f\u7684\u80fd\u529b\u3002"}}
{"id": "2507.15549", "categories": ["cs.DS", "90", "F.2; E.1; G.2"], "pdf": "https://arxiv.org/pdf/2507.15549", "abs": "https://arxiv.org/abs/2507.15549", "authors": ["Ren\u00e9 Sitters"], "title": "An $n^{O(\\log\\log n)}$ time approximation scheme for capacitated VRP in the Euclidean plane", "comment": "40 pages", "summary": "We present a quasi polynomial time approximation scheme (Q-PTAS) for the\ncapacitated vehicle routing problem (CVRP) on $n$ points in the Euclidean plane\nfor arbitrary capacity $c$. The running time is $n^{f(\\epsilon)\\cdot\\log\\log\nn}$ for any $c$, and where $f$ is a function of $\\epsilon$ only. This is a\nmajor improvement over the so far best known running time of\n$n^{\\log^{O(1/\\epsilon)}n}$ time and a big step towards a PTAS for Euclidean\nCVRP.\n  In our algorithm, we first give a polynomial time reduction of the CVRP in\n$\\mathbb{R}^d$ (for any fixed $d$) to an uncapacitated routing problem in\n$\\mathbb{R}^d$ that we call the $m$-paths problem. Here, one needs to find\nexactly $m$ paths between two points $a$ and $b$, covering all the given points\nin the Euclidean space. We then give a Q-PTAS for the $m$-paths problem in the\npane. Any PTAS for the (arguably easier to handle) Euclidean $m$-paths problem\nis most likely to imply a PTAS for the Euclidean CVRP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51c6\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u65b9\u6848\uff08Q-PTAS\uff09\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u6b27\u51e0\u91cc\u5f97\u5e73\u9762\u4e0a\u5bb9\u91cf\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u7684\u8fd0\u884c\u65f6\u95f4\uff0c\u5e76\u901a\u8fc7\u5f52\u7ea6\u4e3am\u8def\u5f84\u95ee\u9898\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\u3002", "motivation": "\u89e3\u51b3\u6b27\u51e0\u91cc\u5f97\u5e73\u9762\u4e0a\u5bb9\u91cf\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08CVRP\uff09\u7684\u9ad8\u6548\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u4ee5\u6539\u8fdb\u73b0\u6709\u6700\u4f73\u8fd0\u884c\u65f6\u95f4\u3002", "method": "\u8bba\u6587\u9996\u5148\u5c06CVRP\u5728\u4efb\u610f\u56fa\u5b9a\u7ef4\u5ea6d\u7684\u7a7a\u95f4\u4e2d\u591a\u9879\u5f0f\u65f6\u95f4\u5f52\u7ea6\u4e3a\u65e0\u5bb9\u91cf\u9650\u5236\u7684m\u8def\u5f84\u95ee\u9898\uff0c\u7136\u540e\u4e3a\u5e73\u9762\u4e0a\u7684m\u8def\u5f84\u95ee\u9898\u63d0\u4f9b\u4e86Q-PTAS\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u8fd0\u884c\u65f6\u95f4\u4e3an^{f(\u03b5)\u22c5log log n}\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u5df2\u77e5\u7684n^{log^{O(1/\u03b5)}n}\u65f6\u95f4\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u51c6\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u65b9\u6848\uff08Q-PTAS\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u6b27\u51e0\u91cc\u5f97\u5e73\u9762\u4e0a\u7684\u5bb9\u91cf\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08CVRP\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u91cd\u5927\u6539\u8fdb\uff0c\u5e76\u4e3a\u5b9e\u73b0PTAS\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2507.14770", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14770", "abs": "https://arxiv.org/abs/2507.14770", "authors": ["Manaal Basha", "Ivan Beschastnikh", "Gema Rodriguez-Perez", "Cleidson R. B. de Souza"], "title": "Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions", "comment": "ESEM 2025 Registered Reports", "summary": "Context: The increasing reliance on Code Generation Tools (CGTs), such as\nWindsurf and GitHub Copilot, are revamping programming workflows and raising\ncritical questions about fairness and inclusivity. While CGTs offer potential\nproductivity enhancements, their effectiveness across diverse user groups have\nnot been sufficiently investigated. Objectives: We hypothesize that developers'\ninteractions with CGTs vary based on gender, influencing task outcomes and\ncognitive load, as prior research suggests that gender differences can affect\ntechnology use and cognitive processing. Methods: The study will employ a\nmixed-subjects design with 54 participants, evenly divided by gender for a\ncounterbalanced design. Participants will complete two programming tasks\n(medium to hard difficulty) with only CGT assistance and then with only\ninternet access. Task orders and conditions will be counterbalanced to mitigate\norder effects. Data collection will include cognitive load surveys, screen\nrecordings, and task performance metrics such as completion time, code\ncorrectness, and CGT interaction behaviors. Statistical analyses will be\nconducted to identify statistically significant differences in CGT usage.\nExpected Contributions: Our work can uncover gender differences in CGT\ninteraction and performance among developers. Our findings can inform future\nCGT designs and help address usability and potential disparities in interaction\npatterns across diverse user groups. Conclusion: While results are not yet\navailable, our proposal lays the groundwork for advancing fairness,\naccountability, transparency, and ethics (FATE) in CGT design. The outcomes are\nanticipated to contribute to inclusive AI practices and equitable tool\ndevelopment for all users.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4ee3\u7801\u751f\u6210\u5de5\u5177\uff08CGT\uff09\u4f7f\u7528\u4e2d\u7684\u6027\u522b\u5dee\u5f02\uff0c\u901a\u8fc7\u6df7\u5408\u53d7\u8bd5\u8005\u8bbe\u8ba1\u5206\u6790\u4efb\u52a1\u8868\u73b0\u548c\u8ba4\u77e5\u8d1f\u8377\u3002\u9884\u671f\u7ed3\u679c\u4e3aCGT\u7684\u516c\u5e73\u8bbe\u8ba1\u63d0\u4f9b\u4f9d\u636e\u3002", "motivation": "\u968f\u7740\u5bf9\u4ee3\u7801\u751f\u6210\u5de5\u5177\uff08CGT\uff09\u7684\u4f9d\u8d56\u589e\u52a0\uff0c\u5982Windsurf\u548cGitHub Copilot\uff0c\u7f16\u7a0b\u5de5\u4f5c\u6d41\u7a0b\u6b63\u5728\u6539\u53d8\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u5bf9\u516c\u5e73\u6027\u548c\u5305\u5bb9\u6027\u7684\u5173\u952e\u95ee\u9898\u3002\u5c3d\u7ba1CGT\u53ef\u80fd\u63d0\u9ad8\u751f\u4ea7\u529b\uff0c\u4f46\u5176\u5728\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u4e2d\u7684\u6548\u679c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u5047\u8bbe\u5f00\u53d1\u8005\u4e0eCGT\u7684\u4e92\u52a8\u56e0\u6027\u522b\u800c\u5f02\uff0c\u5f71\u54cd\u4efb\u52a1\u7ed3\u679c\u548c\u8ba4\u77e5\u8d1f\u8377\u3002", "method": "\u7814\u7a76\u91c7\u7528\u6df7\u5408\u53d7\u8bd5\u8005\u8bbe\u8ba1\uff0c54\u540d\u53c2\u4e0e\u8005\u6309\u6027\u522b\u5747\u5206\uff0c\u8fdb\u884c\u5e73\u8861\u8bbe\u8ba1\u3002\u53c2\u4e0e\u8005\u5c06\u5b8c\u6210\u4e24\u9879\u7f16\u7a0b\u4efb\u52a1\uff08\u4e2d\u7b49\u81f3\u9ad8\u7b49\u96be\u5ea6\uff09\uff0c\u5206\u522b\u4ec5\u4f7f\u7528CGT\u8f85\u52a9\u548c\u4ec5\u4f7f\u7528\u4e92\u8054\u7f51\u8bbf\u95ee\u3002\u6570\u636e\u6536\u96c6\u5305\u62ec\u8ba4\u77e5\u8d1f\u8377\u8c03\u67e5\u3001\u5c4f\u5e55\u5f55\u5236\u548c\u4efb\u52a1\u8868\u73b0\u6307\u6807\uff08\u5982\u5b8c\u6210\u65f6\u95f4\u3001\u4ee3\u7801\u6b63\u786e\u6027\u548cCGT\u4ea4\u4e92\u884c\u4e3a\uff09\u3002\u7edf\u8ba1\u5206\u6790\u5c06\u7528\u4e8e\u8bc6\u522bCGT\u4f7f\u7528\u4e2d\u7684\u663e\u8457\u5dee\u5f02\u3002", "result": "\u9884\u8ba1\u8d21\u732e\uff1a\u6211\u4eec\u7684\u5de5\u4f5c\u53ef\u4ee5\u63ed\u793a\u5f00\u53d1\u8005\u4e2dCGT\u4e92\u52a8\u548c\u8868\u73b0\u7684\u6027\u522b\u5dee\u5f02\u3002\u7814\u7a76\u7ed3\u679c\u53ef\u4e3a\u672a\u6765CGT\u8bbe\u8ba1\u63d0\u4f9b\u4fe1\u606f\uff0c\u5e76\u5e2e\u52a9\u89e3\u51b3\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u95f4\u7684\u53ef\u7528\u6027\u548c\u6f5c\u5728\u4e92\u52a8\u6a21\u5f0f\u5dee\u5f02\u95ee\u9898\u3002", "conclusion": "\u867d\u7136\u7ed3\u679c\u5c1a\u672a\u5f97\u51fa\uff0c\u4f46\u6211\u4eec\u7684\u63d0\u6848\u4e3a\u5728\u4ee3\u7801\u751f\u6210\u5de5\u5177\uff08CGT\uff09\u8bbe\u8ba1\u4e2d\u63a8\u8fdb\u516c\u5e73\u6027\u3001\u8d23\u4efb\u6027\u3001\u900f\u660e\u6027\u548c\u9053\u5fb7\u6027\uff08FATE\uff09\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u9884\u671f\u6210\u679c\u5c06\u6709\u52a9\u4e8e\u5305\u5bb9\u6027\u4eba\u5de5\u667a\u80fd\u5b9e\u8df5\u548c\u9762\u5411\u6240\u6709\u7528\u6237\u7684\u516c\u5e73\u5de5\u5177\u5f00\u53d1\u3002"}}
{"id": "2507.14512", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.14512", "abs": "https://arxiv.org/abs/2507.14512", "authors": ["Qiyuan Peng", "Qi Zhang", "Yue Gao", "Kun Qiu"], "title": "Dora: A Controller Provisioning Strategy in Hierarchical Domain-based Satellite Networks", "comment": null, "summary": "The rapid proliferation of satellite constellations in Space-Air-Ground\nIntegrated Networks (SAGIN) presents significant challenges for network\nmanagement. Conventional flat network architectures struggle with\nsynchronization and data transmission across massive distributed nodes. In\nresponse, hierarchical domain-based satellite network architectures have\nemerged as a scalable solution, highlighting the critical importance of\ncontroller provisioning strategies. However, existing network management\narchitectures and traditional search-based algorithms fail to generate\nefficient controller provisioning solutions due to limited computational\nresources in satellites and strict time constraints. To address these\nchallenges, we propose a three-layer domain-based architecture that enhances\nboth scalability and adaptability. Furthermore, we introduce Dora, a\nreinforcement learning-based controller provisioning strategy designed to\noptimize network performance while minimizing computational overhead. Our\ncomprehensive experimental evaluation demonstrates that Dora significantly\noutperforms state-of-the-art benchmarks, achieving 10% improvement in\ncontroller provisioning quality while requiring only 1/30 to 1/90 of the\ncomputation time compared to traditional algorithms. These results underscore\nthe potential of reinforcement learning approaches for efficient satellite\nnetwork management in next-generation SAGIN deployments.", "AI": {"tldr": "\u63d0\u51faDora\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u5668\u4f9b\u5e94\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u536b\u661f\u7f51\u7edc\u7ba1\u7406\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u6241\u5e73\u7f51\u7edc\u67b6\u6784\u548c\u641c\u7d22\u7b97\u6cd5\u5728\u536b\u661f\u7f51\u7edc\u4e2d\u56e0\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u548c\u65f6\u95f4\u4e25\u683c\u800c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u5668\u4f9b\u5e94\u7b56\u7565Dora\uff0c\u4ee5\u53ca\u4e00\u4e2a\u4e09\u5c42\u57fa\u4e8e\u57df\u7684\u67b6\u6784\u3002", "result": "Dora\u5728\u63a7\u5236\u5668\u4f9b\u5e94\u8d28\u91cf\u4e0a\u63d0\u5347\u4e8610%\uff0c\u4e14\u8ba1\u7b97\u65f6\u95f4\u4ec5\u4e3a\u4f20\u7edf\u7b97\u6cd5\u76841/30\u81f31/90\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4e0b\u4e00\u4ee3SAGIN\u90e8\u7f72\u4e2d\u5c55\u793a\u4e86\u9ad8\u6548\u536b\u661f\u7f51\u7edc\u7ba1\u7406\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14605", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14605", "abs": "https://arxiv.org/abs/2507.14605", "authors": ["Chun-Ming Yang", "Pranav A. Bhounsule"], "title": "Koopman Operator Based Linear Model Predictive Control for 2D Quadruped Trotting, Bounding, and Gait Transition", "comment": null, "summary": "Online optimal control of quadrupedal robots would enable them to plan their\nmovement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged\nas a practical approach for real-time control. In LMPC, an optimization problem\nwith a quadratic cost and linear constraints is formulated over a finite\nhorizon and solved on the fly. However, LMPC relies on linearizing the\nequations of motion (EOM), which may lead to poor solution quality. In this\npaper, we use Koopman operator theory and the Extended Dynamic Mode\nDecomposition (EDMD) to create a linear model of the system in high dimensional\nspace, thus retaining the nonlinearity of the EOM. We model the aerial phase\nand ground contact phases using different linear models. Then, using LMPC, we\ndemonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait\ntransitions in level and rough terrains. The main novelty is the use of Koopman\noperator theory to create hybrid models of a quadrupedal system and demonstrate\nthe online generation of multiple gaits and gaits transitions.", "AI": {"tldr": "\u7ed3\u5408Koopman\u7b97\u5b50\u4e0eLMPC\uff0c\u56db\u8db3\u673a\u5668\u4eba\u6210\u529f\u5b9e\u73b0\u5728\u7ebf\u6b65\u6001\u751f\u6210\u4e0e\u8f6c\u6362\u3002", "motivation": "\u89e3\u51b3LMPC\u56e0\u7ebf\u6027\u5316\u8fd0\u52a8\u65b9\u7a0b\u5bfc\u81f4\u7684\u89e3\u8d28\u91cf\u4e0d\u4f73\u95ee\u9898\uff0c\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u5728\u7ebf\u63a7\u5236\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528Koopman\u7b97\u5b50\u7406\u8bba\u548cEDMD\u65b9\u6cd5\u6784\u5efa\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u6a21\u578b\uff0c\u4fdd\u7559EOM\u7684\u975e\u7ebf\u6027\u7279\u6027\uff0c\u5e76\u9488\u5bf9\u7a7a\u4e2d\u548c\u5730\u9762\u63a5\u89e6\u9636\u6bb5\u4f7f\u7528\u4e0d\u540c\u7ebf\u6027\u6a21\u578b\u3002", "result": "\u5728\u6c34\u5e73\u548c\u7c97\u7cd9\u5730\u5f62\u4e2d\u5b9e\u73b0\u4e86\u8df3\u8dc3\u3001\u5c0f\u8dd1\u4ee5\u53ca\u6b65\u6001\u8f6c\u6362\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7ed3\u5408Koopman\u7b97\u5b50\u7406\u8bba\u548cLMPC\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u591a\u79cd\u5730\u5f62\u4e0b\u7684\u6b65\u6001\u751f\u6210\u548c\u8f6c\u6362\uff0c\u5c55\u793a\u4e86\u5728\u7ebf\u63a7\u5236\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14642", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14642", "abs": "https://arxiv.org/abs/2507.14642", "authors": ["Monoshiz Mahbub Khan", "Xioayin Xi", "Andrew Meneely", "Zhe Yu"], "title": "Efficient Story Point Estimation With Comparative Learning", "comment": null, "summary": "Story point estimation is an essential part of agile software development.\nStory points are unitless, project-specific effort estimates that help\ndevelopers plan their sprints. Traditionally, developers estimate story points\ncollaboratively using planning poker or other manual techniques. While the\ninitial calibrating of the estimates to each project is helpful, once a team\nhas converged on a set of precedents, story point estimation can become tedious\nand labor-intensive. Machine learning can reduce this burden, but only with\nenough context from the historical decisions made by the project team. That is,\nstate-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate\npredictions (within-project) when trained on data from the same project. The\ngoal of this work is to streamline story point estimation by evaluating a\ncomparative learning-based framework for calibrating project-specific story\npoint prediction models. Instead of assigning a specific story point value to\nevery backlog item, developers are presented with pairs of items, and indicate\nwhich item requires more effort. Using these comparative judgments, a machine\nlearning model is trained to predict the story point estimates. We empirically\nevaluated our technique using data with 23,313 manual estimates in 16 projects.\nThe model learned from comparative judgments can achieve on average 0.34\nSpearman's rank correlation coefficient between its predictions and the ground\ntruth story points. This is similar to, if not better than, the performance of\na regression model learned from the ground truth story points. Therefore, the\nproposed comparative learning approach is more efficient than state-of-the-art\nregression-based approaches according to the law of comparative judgments -\nproviding comparative judgments yields a lower cognitive burden on humans than\nproviding ratings or categorical labels.", "AI": {"tldr": "\u901a\u8fc7\u6bd4\u8f83\u5224\u65ad\u8bad\u7ec3\u7684\u6545\u4e8b\u70b9\u4f30\u8ba1\u6a21\u578b\u6027\u80fd\u4e0e\u56de\u5f52\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u964d\u4f4e\u4e86\u4eba\u7c7b\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u4f20\u7edf\u7684\u6545\u4e8b\u70b9\u4f30\u8ba1\u65b9\u6cd5\uff08\u5982\u8ba1\u5212\u6251\u514b\uff09\u5728\u56e2\u961f\u8fbe\u6210\u5171\u8bc6\u540e\u53d8\u5f97\u7e41\u7410\u4e14\u8017\u65f6\uff0c\u673a\u5668\u5b66\u4e60\u53ef\u51cf\u8f7b\u8d1f\u62c5\u4f46\u9700\u8981\u8db3\u591f\u7684\u5386\u53f2\u51b3\u7b56\u4e0a\u4e0b\u6587\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5224\u65ad\uff08\u5f00\u53d1\u8005\u9009\u62e9\u4e24\u4e2a\u5f85\u529e\u4e8b\u9879\u4e2d\u54ea\u4e2a\u9700\u8981\u66f4\u591a\u5de5\u4f5c\u91cf\uff09\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u800c\u975e\u76f4\u63a5\u5206\u914d\u5177\u4f53\u6545\u4e8b\u70b9\u6570\u503c\u3002", "result": "\u6a21\u578b\u572816\u4e2a\u9879\u76ee\u4e2d\u768423,313\u4e2a\u624b\u52a8\u4f30\u8ba1\u6570\u636e\u4e0a\uff0c\u9884\u6d4b\u4e0e\u771f\u5b9e\u6545\u4e8b\u70b9\u4e4b\u95f4\u7684Spearman\u7b49\u7ea7\u76f8\u5173\u7cfb\u6570\u5e73\u5747\u4e3a0.34\uff0c\u4e0e\u56de\u5f52\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u7684\u6bd4\u8f83\u5b66\u4e60\u65b9\u6cd5\u5728\u6545\u4e8b\u70b9\u4f30\u8ba1\u4e2d\u8868\u73b0\u51fa\u4e0e\u73b0\u6709\u56de\u5f52\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u4eba\u7c7b\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002"}}
{"id": "2507.15598", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.15598", "abs": "https://arxiv.org/abs/2507.15598", "authors": ["Ruoxu Cen", "Henry Fleischmann", "George Z. Li", "Jason Li", "Debmalya Panigrahi"], "title": "Fast Algorithms for Graph Arboricity and Related Problems", "comment": "FOCS 2025. 25 pages, 3 figures", "summary": "We give an algorithm for finding the arboricity of a weighted, undirected\ngraph, defined as the minimum number of spanning forests that cover all edges\nof the graph, in $\\sqrt{n} m^{1+o(1)}$ time. This improves on the previous best\nbound of $\\tilde{O}(nm)$ for weighted graphs and $\\tilde{O}(m^{3/2}) $ for\nunweighted graphs (Gabow 1995) for this problem. The running time of our\nalgorithm is dominated by a logarithmic number of calls to a directed global\nminimum cut subroutine -- if the running time of the latter problem improves to\n$m^{1+o(1)}$ (thereby matching the running time of maximum flow), the running\ntime of our arboricity algorithm would improve further to $m^{1+o(1)}$.\n  We also give a new algorithm for computing the entire cut hierarchy --\nlaminar multiway cuts with minimum cut ratio in recursively defined induced\nsubgraphs -- in $m n^{1+o(1)}$ time. The cut hierarchy yields the ideal edge\nloads (Thorup 2001) in a fractional spanning tree packing of the graph which,\nwe show, also corresponds to a max-entropy solution in the spanning tree\npolytope. For the cut hierarchy problem, the previous best bound was\n$\\tilde{O}(n^2 m)$ for weighted graphs and $\\tilde{O}(n m^{3/2})$ for\nunweighted graphs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u6539\u8fdb\u4e86\u52a0\u6743\u548c\u65e0\u5411\u56fe\u6811\u72b6\u6027\u53ca\u5207\u5272\u5c42\u6b21\u7ed3\u6784\u7684\u8ba1\u7b97\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "motivation": "\u65e8\u5728\u6539\u8fdb\u52a0\u6743\u548c\u65e0\u5411\u56fe\u6811\u72b6\u6027\u8ba1\u7b97\u7684\u73b0\u6709\u7b97\u6cd5\uff0c\u51cf\u5c11\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u5e76\u89e3\u51b3\u6574\u4e2a\u5207\u5272\u5c42\u6b21\u7ed3\u6784\u8ba1\u7b97\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8c03\u7528\u5bf9\u6570\u6b21\u6570\u7684\u6709\u5411\u5168\u5c40\u6700\u5c0f\u5207\u5272\u5b50\u7a0b\u5e8f\uff0c\u7b97\u6cd5\u5728\u221an m^(1+o(1))\u65f6\u95f4\u5185\u627e\u5230\u56fe\u7684\u6811\u72b6\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6574\u4e2a\u5207\u5272\u5c42\u6b21\u7ed3\u6784\u7684\u65b0\u7b97\u6cd5\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3am n^(1+o(1))\u3002", "result": "\u7b97\u6cd5\u5c06\u52a0\u6743\u56fe\u7684\u6811\u72b6\u6027\u8ba1\u7b97\u65f6\u95f4\u590d\u6742\u5ea6\u4ece\u4e4b\u524d\u7684O~(nm)\u6539\u8fdb\u5230\u221an m^(1+o(1))\uff0c\u65e0\u5411\u56fe\u4eceO~(m^(3/2))\u6539\u8fdb\u5230\u76f8\u540c\u590d\u6742\u5ea6\u3002\u5207\u5272\u5c42\u6b21\u7ed3\u6784\u8ba1\u7b97\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e5f\u4eceO~(n^2m)\u548cO~(nm^(3/2))\u6539\u8fdb\u5230m n^(1+o(1))\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u52a0\u6743\u65e0\u5411\u56fe\u7684\u6811\u72b6\u6027\uff0c\u5e76\u5728\u8ba1\u7b97\u6574\u4e2a\u5207\u5272\u5c42\u6b21\u7ed3\u6784\u65b9\u9762\u53d6\u5f97\u4e86\u65b0\u7684\u8fdb\u5c55\u3002"}}
{"id": "2507.14776", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14776", "abs": "https://arxiv.org/abs/2507.14776", "authors": ["Kimia Tasnia", "Alexander Garcia", "Tasnuva Farheen", "Sazadur Rahman"], "title": "VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs", "comment": "9 pages, 7 figures, Accepted for ICCAD 2025, Munich, Germany", "summary": "The rapid adoption of large language models(LLMs) in hardware design has\nprimarily focused on generating functionally correct Verilog code, overlooking\ncritical Power Performance-Area(PPA) metrics essential for industrial-grade\ndesigns. To bridge this gap, we propose VeriOpt, a novel framework that\nleverages role-based prompting and PPA-aware optimization to enable LLMs to\nproduce high-quality, synthesizable Verilog. VeriOpt structures LLM\ninteractions into specialized roles (e.g., Planner, Programmer, Reviewer,\nEvaluator) to emulate human design workflows, while integrating PPA constraints\ndirectly into the prompting pipeline. By combining multi-modal feedback (e.g.,\nsynthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves\nPPA-efficient code generation without sacrificing functional correctness.\nExperimental results demonstrate up to 88% reduction in power, 76% reduction in\narea and 73% improvement in timing closure compared to baseline LLM-generated\nRTL, validated using industry standard EDA tools. At the same time achieves 86%\nsuccess rate in functionality evaluation. Our work advances the\nstate-of-the-art AI-driven hardware design by addressing the critical gap\nbetween correctness and quality, paving the way for reliable LLM adoption in\nproduction workflows.", "AI": {"tldr": "VeriOpt\u901a\u8fc7\u89d2\u8272\u5206\u5de5\u548cPPA\u4f18\u5316\uff0c\u4f7fLLM\u751f\u6210\u7684Verilog\u5728PPA\u6307\u6807\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u529f\u80fd\u6b63\u786e\u6027\u3002", "motivation": "\u5f53\u524dLLM\u5728\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u4e3b\u8981\u5173\u6ce8\u529f\u80fd\u6b63\u786e\u6027\uff0c\u5ffd\u89c6\u4e86\u5de5\u4e1a\u7ea7\u8bbe\u8ba1\u7684\u5173\u952ePPA\u6307\u6807\uff0cVeriOpt\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "VeriOpt\u6846\u67b6\u91c7\u7528\u57fa\u4e8e\u89d2\u8272\u7684\u63d0\u793a\uff08\u5982Planner\u3001Programmer\u7b49\uff09\u6a21\u62df\u4eba\u7c7b\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u5e76\u7ed3\u5408PPA\u7ea6\u675f\u7684\u63d0\u793a\u548c\u591a\u6a21\u6001\u53cd\u9988\uff08\u5982\u7efc\u5408\u62a5\u544a\u3001\u65f6\u5e8f\u56fe\uff09\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVeriOpt\u76f8\u6bd4\u57fa\u7ebfLLM\u751f\u6210\u7684RTL\uff0c\u529f\u8017\u964d\u4f4e88%\uff0c\u9762\u79ef\u51cf\u5c1176%\uff0c\u65f6\u5e8f\u95ed\u5408\u63d0\u534773%\uff0c\u529f\u80fd\u8bc4\u4f30\u6210\u529f\u7387\u8fbe86%\u3002", "conclusion": "VeriOpt\u901a\u8fc7\u89d2\u8272\u5206\u5de5\u548cPPA\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684PPA\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u529f\u80fd\u6b63\u786e\u6027\uff0c\u4e3aLLM\u5728\u751f\u4ea7\u6d41\u7a0b\u4e2d\u7684\u53ef\u9760\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.14627", "categories": ["cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14627", "abs": "https://arxiv.org/abs/2507.14627", "authors": ["Kaiqiang Lin", "Yijie Mao", "Onel Luis Alcaraz L\u00f3pez", "Mohamed-Slim Alouini"], "title": "UAV-Enabled Wireless-Powered Underground Communication Networks: A Novel Time Allocation Approach", "comment": "14 pages, 8 figures, 3 tables, submitted to IEEE TGCN", "summary": "Wireless-powered underground communication networks (WPUCNs), which allow\nunderground devices (UDs) to harvest energy from wireless signals for\nbattery-free communication, offer a promising solution for sustainable\nunderground monitoring. However, the severe wireless signal attenuation in\nchallenging underground environments and the costly acquisition of channel\nstate information (CSI) make large-scale WPUCNs economically infeasible in\npractice. To address this challenge, we introduce flexible unmanned aerial\nvehicles (UAVs) into WPUCNs, leading to UAV-enabled WPUCN systems. In this\nsystem, a UAV is first charged by a terrestrial hybrid access point (HAP), then\nflies to the monitoring area to wirelessly charge UDs. Afterwards, the UAV\ncollects data from the UDs and finally returns to the HAP for data offloading.\nBased on the proposed UAV-enabled WPUCN system, we first propose its energy\nconsumption model and a hybrid wireless energy transfer (WET) approach (i.e.,\nUDs can harvest energy from both the HAP and the UAV) relying on full-CSI and\nCSI-free multi-antenna beamforming. Then, we formulate and address a time\nallocation problem to minimize the energy consumption of UAV, while ensuring\nthat the throughput requirements of all UDs are met and all sensor data is\noffloaded. Through simulations of a realistic farming scenario, we demonstrate\nthat the proposed hybrid WET approach outperforms other WET approaches, with\nperformance gains influenced by the number of antennas, communication distance,\nnumber of UDs, and underground conditions. Additionally, under the optimized\ntime allocation, we found that the proposed hybrid WET approach based on a\nCSI-free multi-antenna scheme achieves the lowest UAV's energy consumption\namong all WET mechanisms, thereby enabling sustainable underground monitoring\nin WPUCNs.", "AI": {"tldr": "\u5f15\u5165UAV\u5230WPUCNs\uff0c\u63d0\u51fa\u6df7\u5408WET\u65b9\u6cd5\u548c\u65f6\u95f4\u5206\u914d\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u5b9e\u73b0\u53ef\u6301\u7eed\u5730\u4e0b\u76d1\u6d4b\u3002", "motivation": "\u89e3\u51b3\u5730\u4e0b\u73af\u5883\u4e2d\u65e0\u7ebf\u4fe1\u53f7\u4e25\u91cd\u8870\u51cf\u548cCSI\u83b7\u53d6\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21WPUCNs\u7684\u7ecf\u6d4e\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdUAV-enabled WPUCN\u7cfb\u7edf\uff0c\u5305\u62ec\u80fd\u91cf\u6d88\u8017\u6a21\u578b\u548c\u6df7\u5408\u65e0\u7ebf\u80fd\u91cf\u4f20\u8f93\uff08WET\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5168CSI\u548c\u65e0CSI\u591a\u5929\u7ebf\u6ce2\u675f\u6210\u5f62\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u5206\u914d\u95ee\u9898\u4f18\u5316UAV\u7684\u80fd\u8017\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6df7\u5408WET\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6WET\u65b9\u6cd5\uff0c\u6027\u80fd\u589e\u76ca\u53d7\u5929\u7ebf\u6570\u91cf\u3001\u901a\u4fe1\u8ddd\u79bb\u3001UD\u6570\u91cf\u548c\u5730\u4e0b\u6761\u4ef6\u5f71\u54cd\uff1b\u4f18\u5316\u65f6\u95f4\u5206\u914d\u540e\uff0c\u57fa\u4e8e\u65e0CSI\u591a\u5929\u7ebf\u65b9\u6848\u7684\u6df7\u5408WET\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684UAV\u80fd\u8017\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u65f6\u95f4\u5206\u914d\u548c\u91c7\u7528\u57fa\u4e8e\u65e0CSI\u591a\u5929\u7ebf\u65b9\u6848\u7684\u6df7\u5408WET\u65b9\u6cd5\uff0cUAV\u7684\u80fd\u8017\u6700\u4f4e\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86WPUCNs\u7684\u53ef\u6301\u7eed\u5730\u4e0b\u76d1\u6d4b\u3002"}}
{"id": "2507.14694", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14694", "abs": "https://arxiv.org/abs/2507.14694", "authors": ["Yue Ma", "Kanglei Zhou", "Fuyang Yu", "Frederick W. B. Li", "Xiaohui Liang"], "title": "Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks", "comment": null, "summary": "3D human motion forecasting aims to enable autonomous applications.\nEstimating uncertainty for each prediction (i.e., confidence based on\nprobability density or quantile) is essential for safety-critical contexts like\nhuman-robot collaboration to minimize risks. However, existing diverse motion\nforecasting approaches struggle with uncertainty quantification due to implicit\nprobabilistic representations hindering uncertainty modeling. We propose\nProbHMI, which introduces invertible networks to parameterize poses in a\ndisentangled latent space, enabling probabilistic dynamics modeling. A\nforecasting module then explicitly predicts future latent distributions,\nallowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI\nachieves strong performance for both deterministic and diverse prediction while\nvalidating uncertainty calibration, critical for risk-aware decision making.", "AI": {"tldr": "ProbHMI\u901a\u8fc7\u53ef\u9006\u7f51\u7edc\u548c\u663e\u5f0f\u6f5c\u5728\u5206\u5e03\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e863D\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u591a\u6837\u5316\u8fd0\u52a8\u9884\u6d4b\u65b9\u6cd5\u56e0\u9690\u5f0f\u6982\u7387\u8868\u793a\u96be\u4ee5\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u5b89\u5168\u5173\u952e\u573a\u666f\uff08\u5982\u4eba\u673a\u534f\u4f5c\uff09\u9700\u8981\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4ee5\u964d\u4f4e\u98ce\u9669\u3002", "method": "\u63d0\u51faProbHMI\uff0c\u5229\u7528\u53ef\u9006\u7f51\u7edc\u5728\u89e3\u8026\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u53c2\u6570\u5316\u59ff\u6001\uff0c\u5e76\u901a\u8fc7\u9884\u6d4b\u6a21\u5757\u663e\u5f0f\u5efa\u6a21\u672a\u6765\u6f5c\u5728\u5206\u5e03\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProbHMI\u5728\u786e\u5b9a\u6027\u548c\u591a\u6837\u5316\u9884\u6d4b\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u9a8c\u8bc1\u4e86\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "ProbHMI \u901a\u8fc7\u5f15\u5165\u53ef\u9006\u7f51\u7edc\u548c\u663e\u5f0f\u6f5c\u5728\u5206\u5e03\u9884\u6d4b\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2507.14660", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14660", "abs": "https://arxiv.org/abs/2507.14660", "authors": ["Qibing Ren", "Sitao Xie", "Longxuan Wei", "Zhenfei Yin", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "title": "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems", "comment": "Code is available at https://github.com/renqibing/RogueAgent", "summary": "Recent large-scale events like election fraud and financial scams have shown\nhow harmful coordinated efforts by human groups can be. With the rise of\nautonomous AI systems, there is growing concern that AI-driven groups could\nalso cause similar harm. While most AI safety research focuses on individual AI\nsystems, the risks posed by multi-agent systems (MAS) in complex real-world\nsituations are still underexplored. In this paper, we introduce a\nproof-of-concept to simulate the risks of malicious MAS collusion, using a\nflexible framework that supports both centralized and decentralized\ncoordination structures. We apply this framework to two high-risk fields:\nmisinformation spread and e-commerce fraud. Our findings show that\ndecentralized systems are more effective at carrying out malicious actions than\ncentralized ones. The increased autonomy of decentralized systems allows them\nto adapt their strategies and cause more damage. Even when traditional\ninterventions, like content flagging, are applied, decentralized groups can\nadjust their tactics to avoid detection. We present key insights into how these\nmalicious groups operate and the need for better detection systems and\ncountermeasures. Code is available at https://github.com/renqibing/RogueAgent.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6a21\u62df\u6076\u610f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5171\u8c0b\uff0c\u63ed\u793a\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u5728\u865a\u5047\u4fe1\u606f\u548c\u7535\u5546\u6b3a\u8bc8\u4e2d\u7684\u9ad8\u6548\u7834\u574f\u6027\uff0c\u5f3a\u8c03\u9700\u6539\u8fdb\u68c0\u6d4b\u4e0e\u5e94\u5bf9\u63aa\u65bd\u3002", "motivation": "\u968f\u7740\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u5174\u8d77\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u62c5\u5fc3AI\u9a71\u52a8\u7684\u7fa4\u4f53\u53ef\u80fd\u9020\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7fa4\u4f53\u534f\u8c03\u884c\u4e3a\u5e26\u6765\u7684\u5371\u5bb3\u3002\u5c3d\u7ba1\u5927\u591a\u6570AI\u5b89\u5168\u7814\u7a76\u96c6\u4e2d\u5728\u5355\u4e2aAI\u7cfb\u7edf\u4e0a\uff0c\u4f46\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u73b0\u5b9e\u60c5\u5883\u4e2d\u7684\u98ce\u9669\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u6076\u610fMAS\u5171\u8c0b\u7684\u98ce\u9669\uff0c\u652f\u6301\u96c6\u4e2d\u5f0f\u548c\u53bb\u4e2d\u5fc3\u5316\u7684\u534f\u8c03\u7ed3\u6784\uff0c\u5e76\u5728\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u548c\u7535\u5b50\u5546\u52a1\u6b3a\u8bc8\u4e24\u4e2a\u9ad8\u98ce\u9669\u9886\u57df\u8fdb\u884c\u4e86\u5e94\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u5728\u6267\u884c\u6076\u610f\u884c\u52a8\u65f6\u6bd4\u96c6\u4e2d\u5f0f\u7cfb\u7edf\u66f4\u6709\u6548\uff0c\u5176\u66f4\u9ad8\u7684\u81ea\u4e3b\u6027\u4f7f\u5176\u80fd\u591f\u8c03\u6574\u7b56\u7565\u5e76\u9020\u6210\u66f4\u5927\u7834\u574f\u3002\u5373\u4f7f\u5e94\u7528\u4f20\u7edf\u5e72\u9884\u63aa\u65bd\uff08\u5982\u5185\u5bb9\u6807\u8bb0\uff09\uff0c\u53bb\u4e2d\u5fc3\u5316\u7fa4\u4f53\u4e5f\u80fd\u8c03\u6574\u7b56\u7565\u4ee5\u907f\u514d\u68c0\u6d4b\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u6076\u610f\u5171\u8c0b\u4e2d\u7684\u6f5c\u5728\u5371\u5bb3\uff0c\u6307\u51fa\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u6bd4\u96c6\u4e2d\u5f0f\u7cfb\u7edf\u66f4\u5177\u7834\u574f\u6027\uff0c\u5e76\u547c\u5401\u5f00\u53d1\u66f4\u597d\u7684\u68c0\u6d4b\u7cfb\u7edf\u548c\u5e94\u5bf9\u63aa\u65bd\u3002"}}
{"id": "2507.15616", "categories": ["cs.DS", "cond-mat.dis-nn", "cs.DM", "math-ph", "math.MP", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.15616", "abs": "https://arxiv.org/abs/2507.15616", "authors": ["Ferenc Bencs", "Kuikui Liu", "Guus Regts"], "title": "On zeros and algorithms for disordered systems: mean-field spin glasses", "comment": null, "summary": "Spin glasses are fundamental probability distributions at the core of\nstatistical physics, the theory of average-case computational complexity, and\nmodern high-dimensional statistical inference. In the mean-field setting, we\ndesign deterministic quasipolynomial-time algorithms for estimating the\npartition function to arbitrarily high accuracy for nearly all inverse\ntemperatures in the second moment regime. In particular, for the\nSherrington--Kirkpatrick model, our algorithms succeed for almost the entire\nreplica-symmetric phase. To achieve this, we study the locations of the zeros\nof the partition function. Notably, our methods are conceptually simple, and\napply equally well to the spherical case and the case of Ising spins.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u81ea\u65cb\u73bb\u7483\u6a21\u578b\u7684\u914d\u5206\u51fd\u6570\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u60c5\u51b5\uff0c\u5e76\u5728Sherrington-Kirkpatrick\u6a21\u578b\u4e2d\u53d6\u5f97\u5e7f\u6cdb\u6210\u529f\u3002", "motivation": "\u81ea\u65cb\u73bb\u7483\u662f\u7edf\u8ba1\u7269\u7406\u5b66\u3001\u5e73\u5747\u8ba1\u7b97\u590d\u6742\u6027\u7406\u8bba\u548c\u73b0\u4ee3\u9ad8\u7ef4\u7edf\u8ba1\u63a8\u65ad\u7684\u6838\u5fc3\u6982\u7387\u5206\u5e03\uff0c\u7814\u7a76\u5176\u914d\u5206\u51fd\u6570\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u914d\u5206\u51fd\u6570\u7684\u96f6\u70b9\u4f4d\u7f6e\uff0c\u8bbe\u8ba1\u4e86\u6982\u5ff5\u4e0a\u7b80\u5355\u4e14\u9002\u7528\u4e8e\u7403\u5f62\u60c5\u51b5\u548cIsing\u81ea\u65cb\u60c5\u51b5\u7684\u65b9\u6cd5\u3002", "result": "\u7b97\u6cd5\u5728\u7b2c\u4e8c\u77e9\u8303\u56f4\u5185\u6210\u529f\u4f30\u8ba1\u914d\u5206\u51fd\u6570\uff0c\u7279\u522b\u662f\u5728Sherrington-Kirkpatrick\u6a21\u578b\u4e2d\u51e0\u4e4e\u8986\u76d6\u6574\u4e2a\u590d\u5236\u5bf9\u79f0\u76f8\u3002", "conclusion": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u786e\u5b9a\u6027\u62df\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7b2c\u4e8c\u77e9\u8303\u56f4\u5185\u4ee5\u4efb\u610f\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u51e0\u4e4e\u6240\u6709\u9006\u6e29\u5ea6\u4e0b\u7684\u914d\u5206\u51fd\u6570\uff0c\u7279\u522b\u662f\u5728Sherrington-Kirkpatrick\u6a21\u578b\u4e2d\uff0c\u8be5\u7b97\u6cd5\u51e0\u4e4e\u5728\u6574\u4e2a\u590d\u5236\u5bf9\u79f0\u76f8\u4e2d\u90fd\u53d6\u5f97\u6210\u529f\u3002"}}
{"id": "2507.14791", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14791", "abs": "https://arxiv.org/abs/2507.14791", "authors": ["Yang Liu", "Li Zhang", "Fang Liu", "Zhuohang Wang", "Donglin Wei", "Zhishuo Yang", "Kechi Zhang", "Jia Li", "Lin Shi"], "title": "Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context", "comment": null, "summary": "Repository-level code generation aims to generate code within the context of\na specified repository. Existing approaches typically employ\nretrieval-augmented generation (RAG) techniques to provide LLMs with relevant\ncontextual information extracted from the repository. However, these approaches\noften struggle with effectively identifying truly relevant contexts that\ncapture the rich semantics of the repository, and their contextual perspectives\nremains narrow. Moreover, most approaches fail to account for the structural\nrelationships in the retrieved code during prompt construction, hindering the\nLLM's ability to accurately interpret the context. To address these issues, we\npropose RepoScope, which leverages call chain-aware multi-view context for\nrepository-level code generation. RepoScope constructs a Repository Structural\nSemantic Graph (RSSG) and retrieves a comprehensive four-view context,\nintegrating both structural and similarity-based contexts. We propose a novel\ncall chain prediction method that utilizes the repository's structural\nsemantics to improve the identification of callees in the target function.\nAdditionally, we present a structure-preserving serialization algorithm for\nprompt construction, ensuring the coherence of the context for the LLM.\nNotably, RepoScope relies solely on static analysis, eliminating the need for\nadditional training or multiple LLM queries, thus ensuring both efficiency and\ngeneralizability. Evaluation on widely-used repository-level code generation\nbenchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms\nstate-of-the-art methods, achieving up to a 36.35% relative improvement in\npass@1 scores. Further experiments emphasize RepoScope's potential to improve\ncode generation across different tasks and its ability to integrate effectively\nwith existing approaches.", "AI": {"tldr": "RepoScope\u901a\u8fc7\u591a\u89c6\u89d2\u4e0a\u4e0b\u6587\u548c\u9759\u6001\u5206\u6790\u63d0\u5347\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\u6548\u679c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bc6\u522b\u4ed3\u5e93\u7684\u4e30\u5bcc\u8bed\u4e49\u548c\u7ed3\u6784\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u89c6\u89d2\u72ed\u7a84\u4e14\u63d0\u793a\u6784\u5efa\u6548\u679c\u4e0d\u4f73\uff0c\u5f71\u54cd\u4e86\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u3002", "method": "RepoScope\u6784\u5efa\u4e86Repository Structural Semantic Graph (RSSG)\uff0c\u5e76\u68c0\u7d22\u4e86\u5305\u542b\u7ed3\u6784\u548c\u76f8\u4f3c\u6027\u4e0a\u4e0b\u6587\u7684\u56db\u89c6\u89d2\u4e0a\u4e0b\u6587\uff1b\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8c03\u7528\u94fe\u9884\u6d4b\u65b9\u6cd5\u548c\u7ed3\u6784\u4fdd\u6301\u5e8f\u5217\u5316\u7b97\u6cd5\u3002", "result": "\u5728CoderEval\u548cDevEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRepoScope\u76f8\u5bf9\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e8636.35%\u7684pass@1\u5206\u6570\u3002", "conclusion": "RepoScope\u901a\u8fc7\u9759\u6001\u5206\u6790\u548c\u591a\u89c6\u89d2\u4e0a\u4e0b\u6587\u6574\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e14\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u96c6\u6210\u80fd\u529b\u3002"}}
{"id": "2507.14633", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14633", "abs": "https://arxiv.org/abs/2507.14633", "authors": ["Xiaozheng Gao", "Yichen Wang", "Bosen Liu", "Xiao Zhou", "Ruichen Zhang", "Jiacheng Wang", "Dusit Niyato", "Dong In Kim", "Abbas Jamalipour", "Chau Yuen", "Jianping An", "Kai Yang"], "title": "Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks: A Survey on Generative Approaches", "comment": null, "summary": "The development of satellite-augmented low-altitude economy and terrestrial\nnetworks (SLAETNs) demands intelligent and autonomous systems that can operate\nreliably across heterogeneous, dynamic, and mission-critical environments. To\naddress these challenges, this survey focuses on enabling agentic artificial\nintelligence (AI), that is, artificial agents capable of perceiving, reasoning,\nand acting, through generative AI (GAI) and large language models (LLMs). We\nbegin by introducing the architecture and characteristics of SLAETNs, and\nanalyzing the challenges that arise in integrating satellite, aerial, and\nterrestrial components. Then, we present a model-driven foundation by\nsystematically reviewing five major categories of generative models:\nvariational autoencoders (VAEs), generative adversarial networks (GANs),\ngenerative diffusion models (GDMs), transformer-based models (TBMs), and LLMs.\nMoreover, we provide a comparative analysis to highlight their generative\nmechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on\nthis foundation, we examine how these models empower agentic functions across\nthree domains: communication enhancement, security and privacy protection, and\nintelligent satellite tasks. Finally, we outline key future directions for\nbuilding scalable, adaptive, and trustworthy generative agents in SLAETNs. This\nsurvey aims to provide a unified understanding and actionable reference for\nadvancing agentic AI in next-generation integrated networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u751f\u6210\u5f0fAI\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u5f00\u53d1\u667a\u80fd\u81ea\u4e3b\u7cfb\u7edf\uff0c\u4ee5\u652f\u6301\u536b\u661f\u589e\u5f3a\u4f4e\u7a7a\u7ecf\u6d4e\u4e0e\u5730\u9762\u7f51\u7edc\uff08SLAETNs\uff09\u7684\u53d1\u5c55\u3002", "motivation": "\u5e94\u5bf9SLAETNs\u5728\u5f02\u6784\u3001\u52a8\u6001\u548c\u5173\u952e\u4efb\u52a1\u73af\u5883\u4e2d\u53ef\u9760\u8fd0\u884c\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u4e94\u5927\u7c7b\u751f\u6210\u6a21\u578b\uff08VAEs\u3001GANs\u3001GDMs\u3001TBMs\u3001LLMs\uff09\uff0c\u5e76\u5206\u6790\u5b83\u4eec\u5728SLAETNs\u4e2d\u7684\u751f\u6210\u673a\u5236\u3001\u80fd\u529b\u548c\u90e8\u7f72\u6743\u8861\u3002", "result": "\u5c55\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u901a\u4fe1\u589e\u5f3a\u3001\u5b89\u5168\u9690\u79c1\u4fdd\u62a4\u548c\u667a\u80fd\u536b\u661f\u4efb\u52a1\u4e09\u4e2a\u9886\u57df\u7684\u4ee3\u7406\u529f\u80fd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u6784\u5efa\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u548c\u53ef\u4fe1\u8d56\u7684\u751f\u6210\u5f0f\u4ee3\u7406\u7684\u5173\u952e\u65b9\u5411\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u96c6\u6210\u7f51\u7edc\u4e2d\u4ee3\u7406AI\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u7edf\u4e00\u7406\u89e3\u548c\u884c\u52a8\u53c2\u8003\u3002"}}
{"id": "2507.14700", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14700", "abs": "https://arxiv.org/abs/2507.14700", "authors": ["Nicholas Mohammad", "Nicola Bezzo"], "title": "Corridor-based Adaptive Control Barrier and Lyapunov Functions for Safe Mobile Robot Navigation", "comment": "To be presented in the 64th IEEE Conference on Decision and Control\n  (CDC 25)", "summary": "Safe navigation in unknown and cluttered environments remains a challenging\nproblem in robotics. Model Predictive Contour Control (MPCC) has shown promise\nfor performant obstacle avoidance by enabling precise and agile trajectory\ntracking, however, existing methods lack formal safety assurances. To address\nthis issue, we propose a general Control Lyapunov Function (CLF) and Control\nBarrier Function (CBF) enabled MPCC framework that enforces safety constraints\nderived from a free-space corridor around the planned trajectory. To enhance\nfeasibility, we dynamically adapt the CBF parameters at runtime using a Soft\nActor-Critic (SAC) policy. The approach is validated with extensive simulations\nand an experiment on mobile robot navigation in unknown cluttered environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CLF\u548cCBF\u7684MPCC\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574CBF\u53c2\u6570\u63d0\u5347\u5b89\u5168\u6027\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709MPCC\u65b9\u6cd5\u7f3a\u4e4f\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1\uff0c\u96be\u4ee5\u5728\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u786e\u4fdd\u673a\u5668\u4eba\u5bfc\u822a\u7684\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528Control Lyapunov Function (CLF)\u548cControl Barrier Function (CBF)\u589e\u5f3a\u7684Model Predictive Contour Control (MPCC)\u6846\u67b6\uff0c\u7ed3\u5408Soft Actor-Critic (SAC)\u7b56\u7565\u52a8\u6001\u8c03\u6574CBF\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u7269\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ed3\u5408CLF\u548cCBF\u7684MPCC\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5bfc\u822a\u7684\u5b89\u5168\u6027\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u8c03\u6574CBF\u53c2\u6570\u589e\u5f3a\u4e86\u53ef\u884c\u6027\u3002"}}
{"id": "2507.14705", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14705", "abs": "https://arxiv.org/abs/2507.14705", "authors": ["Sai Wang", "Senthilnathan Subramanian", "Mudit Sahni", "Praneeth Gone", "Lingjie Meng", "Xiaochen Wang", "Nicolas Ferradas Bertoli", "Tingxian Cheng", "Jun Xu"], "title": "Configurable multi-agent framework for scalable and realistic testing of llm-based agents", "comment": null, "summary": "Large-language-model (LLM) agents exhibit complex, context-sensitive\nbehaviour that quickly renders static benchmarks and ad-hoc manual testing\nobsolete.\n  We present Neo, a configurable, multi-agent framework that automates\nrealistic, multi-turn evaluation of LLM-based systems. Neo couples a Question\nGeneration Agent and an Evaluation Agent through a shared context-hub, allowing\ndomain prompts, scenario controls and dynamic feedback to be composed\nmodularly. Test inputs are sampled from a probabilistic state model spanning\ndialogue flow, user intent and emotional tone, enabling diverse, human-like\nconversations that adapt after every turn.\n  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)\nuncovered edge-case failures across five attack categories with a 3.3% break\nrate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered\n10-12X higher throughput, generating 180 coherent test questions in around 45\nmins versus 16h of human effort. Beyond security probing, Neo's stochastic\npolicies balanced topic coverage and conversational depth, yielding broader\nbehavioural exploration than manually crafted scripts.\n  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent\ninterfaces, state controller and feedback loops are model-agnostic and\nextensible to richer factual-grounding and policy-compliance checks. We release\nthe framework to facilitate reproducible, high-fidelity testing of emerging\nagentic systems.", "AI": {"tldr": "Neo\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u591a\u8f6e\u8bc4\u4f30LLM\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u6982\u7387\u72b6\u6001\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u3001\u591a\u6837\u5316\u7684\u6d4b\u8bd5\uff0c\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u8868\u73b0\u51fa\u590d\u6742\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u884c\u4e3a\uff0c\u4f7f\u5f97\u9759\u6001\u57fa\u51c6\u548c\u4e34\u65f6\u624b\u52a8\u6d4b\u8bd5\u8fc5\u901f\u8fc7\u65f6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u3001\u591a\u8f6e\u8bc4\u4f30\u6846\u67b6\u3002", "method": "Neo\u901a\u8fc7\u4e00\u4e2a\u53ef\u914d\u7f6e\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u95ee\u9898\u751f\u6210\u4ee3\u7406\u548c\u8bc4\u4f30\u4ee3\u7406\uff0c\u901a\u8fc7\u5171\u4eab\u4e0a\u4e0b\u6587\u4e2d\u5fc3\u5b9e\u73b0\u6a21\u5757\u5316\u7ec4\u5408\u3002\u6d4b\u8bd5\u8f93\u5165\u4ece\u6982\u7387\u72b6\u6001\u6a21\u578b\u4e2d\u91c7\u6837\uff0c\u8986\u76d6\u5bf9\u8bdd\u6d41\u3001\u7528\u6237\u610f\u56fe\u548c\u60c5\u611f\u8bed\u8c03\uff0c\u5b9e\u73b0\u591a\u6837\u5316\u548c\u7c7b\u4eba\u5bf9\u8bdd\u3002", "result": "\u5e94\u7528\u4e8e\u751f\u4ea7\u7ea7\u5356\u5bb6\u8d22\u52a1\u52a9\u624b\u804a\u5929\u673a\u5668\u4eba\u65f6\uff0cNeo\u5728\u4e94\u4e2a\u653b\u51fb\u7c7b\u522b\u4e2d\u53d1\u73b0\u4e86\u8fb9\u7f18\u6848\u4f8b\u6545\u969c\uff0c\u51763.3%\u7684\u7a81\u7834\u7387\u63a5\u8fd1\u4e13\u5bb6\u4eba\u7c7b\u7ea2\u961f\u76845.8%\uff0c\u4e14\u541e\u5410\u91cf\u63d0\u9ad8\u4e8610-12\u500d\uff0c45\u5206\u949f\u5185\u751f\u6210180\u4e2a\u8fde\u8d2f\u6d4b\u8bd5\u95ee\u9898\uff0c\u800c\u4eba\u7c7b\u9700\u898116\u5c0f\u65f6\u3002", "conclusion": "Neo\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u81ea\u8fdb\u5316\u7684LLM QA\u57fa\u7840\uff0c\u5176\u4ee3\u7406\u63a5\u53e3\u3001\u72b6\u6001\u63a7\u5236\u5668\u548c\u53cd\u9988\u5faa\u73af\u662f\u6a21\u578b\u65e0\u5173\u7684\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u66f4\u4e30\u5bcc\u7684\u4e8b\u5b9e\u57fa\u7840\u548c\u7b56\u7565\u5408\u89c4\u6027\u68c0\u67e5\u3002"}}
{"id": "2507.14969", "categories": ["cs.SE", "D.2.1"], "pdf": "https://arxiv.org/pdf/2507.14969", "abs": "https://arxiv.org/abs/2507.14969", "authors": ["Sai Zhang", "Zhenchang Xing", "Jieshan Chen", "Dehai Zhao", "Zizhong Zhu", "Xiaowang Zhang", "Zhiyong Feng", "Xiaohong Li"], "title": "Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review", "comment": null, "summary": "The vision of End-User Software Engineering (EUSE) is to empower\nnon-professional users with full control over the software development\nlifecycle. It aims to enable users to drive generative software development\nusing only natural language requirements. However, since end-users often lack\nknowledge of software engineering, their requirement descriptions are\nfrequently ambiguous, raising significant challenges to generative software\ndevelopment. Although existing approaches utilize structured languages like\nGherkin to clarify user narratives, they still struggle to express the causal\nlogic between preconditions and behavior actions. This paper introduces\nRequireCEG, a requirement elicitation and self-review agent that embeds\ncausal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.\nRequireCEG first uses a feature tree to analyze user narratives hierarchically,\nclearly defining the scope of software components and their system behavior\nrequirements. Next, it constructs the self-healing CEGs based on the elicited\nrequirements, capturing the causal relationships between atomic preconditions\nand behavioral actions. Finally, the constructed CEGs are used to review and\noptimize Gherkin scenarios, ensuring consistency between the generated Gherkin\nrequirements and the system behavior requirements elicited from user\nnarratives. To evaluate our method, we created the RGPair benchmark dataset and\nconducted extensive experiments. It achieves an 87% coverage rate and raises\ndiversity by 51.88%.", "AI": {"tldr": "RequireCEG\u901a\u8fc7\u56e0\u679c\u6548\u5e94\u56fe\u548c\u795e\u7ecf\u7b26\u53f7\u534f\u4f5c\u63d0\u5347\u9700\u6c42\u63cf\u8ff0\u7684\u6e05\u6670\u5ea6\uff0c\u5b9e\u9a8c\u663e\u793a\u8986\u76d6\u7387\u548c\u591a\u6837\u6027\u663e\u8457\u63d0\u9ad8\u3002", "motivation": "\u7531\u4e8e\u975e\u4e13\u4e1a\u7528\u6237\u7684\u9700\u6c42\u63cf\u8ff0\u5e38\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u8868\u8fbe\u56e0\u679c\u903b\u8f91\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u9700\u6c42\u83b7\u53d6\u548c\u81ea\u5ba1\u67e5\u4ee3\u7406\u3002", "method": "RequireCEG\u91c7\u7528\u7279\u5f81\u6811\u5206\u5c42\u5206\u6790\u7528\u6237\u53d9\u8ff0\uff0c\u6784\u5efa\u81ea\u4fee\u590d\u7684CEGs\u4ee5\u6355\u6349\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u4f18\u5316Gherkin\u573a\u666f\u4ee5\u786e\u4fdd\u4e00\u81f4\u6027\u3002", "result": "\u5728RGPair\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8687%\u7684\u8986\u76d6\u7387\u548c51.88%\u7684\u591a\u6837\u6027\u63d0\u5347\u3002", "conclusion": "RequireCEG\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u534f\u4f5c\u67b6\u6784\u4e2d\u7684\u56e0\u679c\u6548\u5e94\u56fe\uff08CEGs\uff09\u6709\u6548\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u9700\u6c42\u63cf\u8ff0\u7684\u6e05\u6670\u5ea6\u548c\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8986\u76d6\u7387\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2507.14842", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.14842", "abs": "https://arxiv.org/abs/2507.14842", "authors": ["Satadal Sengupta", "Hyojoon Kim", "Daniel Jubas", "Maria Apostolaki", "Jennifer Rexford"], "title": "Data-Plane Telemetry to Mitigate Long-Distance BGP Hijacks", "comment": null, "summary": "Poor security of Internet routing enables adversaries to divert user data\nthrough unintended infrastructures (hijack). Of particular concern -- and the\nfocus of this paper -- are cases where attackers reroute domestic traffic\nthrough foreign countries, exposing it to surveillance, bypassing legal privacy\nprotections, and posing national security threats. Efforts to detect and\nmitigate such attacks have focused primarily on the control plane while\ndata-plane signals remain largely overlooked. In particular, change in\npropagation delay caused by rerouting offers a promising signal: the change is\nunavoidable and the increased propagation delay is directly observable from the\naffected networks. In this paper, we explore the practicality of using delay\nvariations for hijack detection, addressing two key questions: (1) What\ncoverage can this provide, given its heavy dependence on the geolocations of\nthe sender, receiver, and adversary? and (2) Can an always-on latency-based\ndetection system be deployed without disrupting normal network operations? We\nobserve that for 86% of victim-attacker country pairs in the world, mid-attack\ndelays exceed pre-attack delays by at least 25% in real deployments, making\ndelay-based hijack detection promising. To demonstrate practicality, we design\nHiDe, which reliably detects delay surges from long-distance hijacks at line\nrate. We measure HiDe's accuracy and false-positive rate on real-world data and\nvalidate it with ethically conducted hijacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u5ef6\u8fdf\u53d8\u5316\u68c0\u6d4b\u8def\u7531\u52ab\u6301\u653b\u51fb\u7684\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86HiDe\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u4e92\u8054\u7f51\u8def\u7531\u5b89\u5168\u8584\u5f31\uff0c\u653b\u51fb\u8005\u53ef\u80fd\u5c06\u56fd\u5185\u6d41\u91cf\u91cd\u8def\u7531\u81f3\u5916\u56fd\uff0c\u5bfc\u81f4\u9690\u79c1\u6cc4\u9732\u548c\u56fd\u5bb6\u5b89\u5168\u5a01\u80c1\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u63a7\u5236\u5e73\u9762\uff0c\u800c\u6570\u636e\u5e73\u9762\u4fe1\u53f7\u88ab\u5ffd\u89c6\u3002", "method": "\u672c\u6587\u8bbe\u8ba1\u4e86HiDe\u7cfb\u7edf\uff0c\u5229\u7528\u5ef6\u8fdf\u53d8\u5316\u4f5c\u4e3a\u4fe1\u53f7\u6765\u68c0\u6d4b\u8def\u7531\u52ab\u6301\u653b\u51fb\u3002\u901a\u8fc7\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u6d4b\u91cf\u5ef6\u8fdf\u53d8\u5316\uff0c\u5e76\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\u548c\u8bef\u62a5\u7387\u3002", "result": "\u5bf9\u4e8e\u5168\u740386%\u7684\u53d7\u5bb3\u8005-\u653b\u51fb\u8005\u56fd\u5bb6\u7ec4\u5408\uff0c\u653b\u51fb\u671f\u95f4\u7684\u5ef6\u8fdf\u81f3\u5c11\u6bd4\u653b\u51fb\u524d\u589e\u52a025%\u3002HiDe\u7cfb\u7edf\u80fd\u591f\u53ef\u9760\u5730\u68c0\u6d4b\u957f\u8ddd\u79bb\u52ab\u6301\u5bfc\u81f4\u7684\u5ef6\u8fdf\u6fc0\u589e\u3002", "conclusion": "\u5ef6\u8fdf\u53d8\u5316\u4f5c\u4e3a\u4e00\u79cd\u4fe1\u53f7\uff0c\u53ef\u7528\u4e8e\u6709\u6548\u5730\u68c0\u6d4b\u8def\u7531\u52ab\u6301\u653b\u51fb\u3002HiDe\u7cfb\u7edf\u901a\u8fc7\u5b9e\u65f6\u68c0\u6d4b\u5ef6\u8fdf\u6fc0\u589e\uff0c\u8bc1\u660e\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.14721", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14721", "abs": "https://arxiv.org/abs/2507.14721", "authors": ["Keita Kobashi", "Masayoshi Tomizuka"], "title": "Leveraging Extrinsic Dexterity for Occluded Grasping on Grasp Constraining Walls", "comment": "7 pages, 7 figures", "summary": "This study addresses the problem of occluded grasping, where primary grasp\nconfigurations of an object are not available due to occlusion with\nenvironment. Simple parallel grippers often struggle with such tasks due to\nlimited dexterity and actuation constraints. Prior works have explored object\npose reorientation such as pivoting by utilizing extrinsic contacts between an\nobject and an environment feature like a wall, to make the object graspable.\nHowever, such works often assume the presence of a short wall, and this\nassumption may not always hold in real-world scenarios. If the wall available\nfor interaction is too large or too tall, the robot may still fail to grasp the\nobject even after pivoting, and the robot must combine different types of\nactions to grasp. To address this, we propose a hierarchical reinforcement\nlearning (RL) framework. We use Q-learning to train a high-level policy that\nselects the type of action expected to yield the highest reward. The selected\nlow-level skill then samples a specific robot action in continuous space. To\nguide the robot to an appropriate location for executing the selected action,\nwe adopt a Conditional Variational Autoencoder (CVAE). We condition the CVAE on\nthe object point cloud and the skill ID, enabling it to infer a suitable\nlocation based on the object geometry and the selected skill. To promote\ngeneralization, we apply domain randomization during the training of low-level\nskills. The RL policy is trained entirely in simulation with a box-like object\nand deployed to six objects in real world. We conduct experiments to evaluate\nour method and demonstrate both its generalizability and robust sim-to-real\ntransfer performance with promising success rates.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7ed3\u5408CVAE\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u6293\u53d6\u88ab\u906e\u6321\u7269\u4f53\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u826f\u597d\u6cdb\u5316\u4e0e\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u56e0\u73af\u5883\u906e\u6321\u5bfc\u81f4\u7684\u4e3b\u8981\u6293\u53d6\u914d\u7f6e\u4e0d\u53ef\u7528\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u53ef\u80fd\u9047\u5230\u7684\u9ad8\u5899\u6216\u5927\u5899\u7b49\u590d\u6742\u60c5\u51b5\uff0c\u4f20\u7edf\u5e73\u884c\u5939\u722a\u673a\u5668\u4eba\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u9ad8\u5c42\u7b56\u7565\u4f7f\u7528Q\u5b66\u4e60\u9009\u62e9\u52a8\u4f5c\u7c7b\u578b\uff0c\u4f4e\u5c42\u6280\u80fd\u901a\u8fc7CVAE\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u91c7\u6837\u5177\u4f53\u52a8\u4f5c\u3002\u7ed3\u5408\u9886\u57df\u968f\u673a\u5316\u8bad\u7ec3\u4f4e\u5c42\u6280\u80fd\u4ee5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u6293\u53d6\u516d\u79cd\u4e0d\u540c\u7269\u4f53\uff0c\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7ed3\u5408CVAE\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u673a\u5668\u4eba\u6293\u53d6\u88ab\u906e\u6321\u7269\u4f53\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7a33\u5065\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u6027\u80fd\u3002"}}
{"id": "2507.14719", "categories": ["cs.AI", "I.2.7; F.2.2"], "pdf": "https://arxiv.org/pdf/2507.14719", "abs": "https://arxiv.org/abs/2507.14719", "authors": ["Juan Manuel Contreras"], "title": "Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix", "comment": null, "summary": "As large language models (LLMs) become increasingly integrated into\nreal-world applications, scalable and rigorous safety evaluation is essential.\nThis paper introduces Aymara AI, a programmatic platform for generating and\nadministering customized, policy-grounded safety evaluations. Aymara AI\ntransforms natural-language safety policies into adversarial prompts and scores\nmodel responses using an AI-based rater validated against human judgments. We\ndemonstrate its capabilities through the Aymara LLM Risk and Responsibility\nMatrix, which evaluates 20 commercially available LLMs across 10 real-world\nsafety domains. Results reveal wide performance disparities, with mean safety\nscores ranging from 86.2% to 52.4%. While models performed well in\nwell-established safety domains such as Misinformation (mean = 95.7%), they\nconsistently failed in more complex or underspecified domains, notably Privacy\n& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety\nscores differed significantly across both models and domains (p < .05). These\nfindings underscore the inconsistent and context-dependent nature of LLM safety\nand highlight the need for scalable, customizable tools like Aymara AI to\nsupport responsible AI development and oversight.", "AI": {"tldr": "Aymara AI\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u548c\u7ba1\u7406\u5b9a\u5236\u5316\u3001\u57fa\u4e8e\u653f\u7b56\u7684\u5b89\u5168\u8bc4\u4f30\u5e73\u53f0\uff0c\u8bc4\u4f30\u663e\u793a\u5546\u4e1aLLM\u5728\u4e0d\u540c\u5b89\u5168\u9886\u57df\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u5f3a\u8c03\u4e86\u53ef\u6269\u5c55\u5b89\u5168\u5de5\u5177\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u878d\u5165\u73b0\u5b9e\u5e94\u7528\uff0c\u53ef\u6269\u5c55\u4e14\u4e25\u683c\u7684\u5b89\u5168\u8bc4\u4f30\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "Aymara AI\u662f\u4e00\u4e2a\u7a0b\u5e8f\u5316\u5e73\u53f0\uff0c\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u5b89\u5168\u653f\u7b56\u8f6c\u5316\u4e3a\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8eAI\u7684\u8bc4\u5206\u5668\uff08\u7ecf\u4eba\u7c7b\u5224\u65ad\u9a8c\u8bc1\uff09\u6765\u8bc4\u5206\u6a21\u578b\u54cd\u5e94\u3002", "result": "\u8bc4\u4f30\u4e8620\u4e2a\u5546\u4e1a\u53ef\u7528LLM\u572810\u4e2a\u73b0\u5b9e\u5b89\u5168\u9886\u57df\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff08\u5e73\u5747\u5b89\u5168\u5206\u6570\u4ece86.2%\u523052.4%\uff09\uff0c\u5728\u590d\u6742\u6216\u672a\u660e\u786e\u9886\u57df\uff08\u5982\u9690\u79c1\u4e0e\u5192\u5145\uff09\u8868\u73b0\u8f83\u5dee\uff08\u5e73\u574724.3%\uff09\u3002\u65b9\u5dee\u5206\u6790\u786e\u8ba4\u5b89\u5168\u5206\u6570\u5728\u6a21\u578b\u548c\u9886\u57df\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff08p < .05\uff09\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86LLM\u5b89\u5168\u6027\u7684\u4e0d\u4e00\u81f4\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u50cfAymara AI\u8fd9\u6837\u53ef\u6269\u5c55\u3001\u53ef\u5b9a\u5236\u7684\u5de5\u5177\u5728\u652f\u6301\u8d1f\u8d23\u4efbAI\u5f00\u53d1\u548c\u76d1\u7763\u4e2d\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.15003", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15003", "abs": "https://arxiv.org/abs/2507.15003", "authors": ["Hao Li", "Haoxiang Zhang", "Ahmed E. Hassan"], "title": "The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering", "comment": null, "summary": "The future of software engineering--SE 3.0--is unfolding with the rise of AI\nteammates: autonomous, goal-driven systems collaborating with human developers.\nAmong these, autonomous coding agents are especially transformative, now\nactively initiating, reviewing, and evolving code at scale. This paper\nintroduces AIDev, the first large-scale dataset capturing how such agents\noperate in the wild. Spanning over 456,000 pull requests by five leading\nagents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across\n61,000 repositories and 47,000 developers, AIDev provides an unprecedented\nempirical foundation for studying autonomous teammates in software development.\n  Unlike prior work that has largely theorized the rise of AI-native software\nengineering, AIDev offers structured, open data to support research in\nbenchmarking, agent readiness, optimization, collaboration modeling, and AI\ngovernance. The dataset includes rich metadata on PRs, authorship, review\ntimelines, code changes, and integration outcomes--enabling exploration beyond\nsynthetic benchmarks like SWE-bench. For instance, although agents often\noutperform humans in speed, their PRs are accepted less frequently, revealing a\ntrust and utility gap. Furthermore, while agents accelerate code\nsubmission--one developer submitted as many PRs in three days as they had in\nthree years--these are structurally simpler (via code complexity metrics).\n  We envision AIDev as a living resource: extensible, analyzable, and ready for\nthe SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev\nenables a new generation of research into AI-native workflows and supports\nbuilding the next wave of symbiotic human-AI collaboration. The dataset is\npublicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.\n  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering\nAgent", "AI": {"tldr": "AIDev\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8bb0\u5f55\u4e86AI\u7f16\u7801\u4ee3\u7406\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\uff0c\u652f\u6301\u5bf9AI\u961f\u53cb\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u534f\u4f5c\u3001\u6548\u80fd\u548c\u6cbb\u7406\u7684\u7814\u7a76\u3002", "motivation": "\u968f\u7740AI\u961f\u53cb\uff08\u81ea\u4e3b\u3001\u76ee\u6807\u9a71\u52a8\u7684\u7cfb\u7edf\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5d1b\u8d77\uff0c\u9700\u8981\u5927\u89c4\u6a21\u3001\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u6765\u7814\u7a76\u5176\u64cd\u4f5c\u6a21\u5f0f\u3001\u534f\u4f5c\u6548\u679c\u53ca\u6cbb\u7406\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u548c\u5206\u6790456,000\u4e2a\u7531\u4e94\u4e2a\u9886\u5148\u7684AI\u7f16\u7801\u4ee3\u7406\uff08\u5982OpenAI Codex\u3001GitHub Copilot\u7b49\uff09\u572861,000\u4e2a\u4ed3\u5e93\u548c47,000\u540d\u5f00\u53d1\u8005\u4e2d\u63d0\u4ea4\u7684\u62c9\u53d6\u8bf7\u6c42\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u3001\u5f00\u653e\u7684\u6570\u636e\u96c6\u3002", "result": "AI\u4ee3\u7406\u5728\u901f\u5ea6\u4e0a\u5e38\u4f18\u4e8e\u4eba\u7c7b\uff0c\u4f46\u5176\u62c9\u53d6\u8bf7\u6c42\u7684\u63a5\u53d7\u7387\u8f83\u4f4e\uff0c\u63ed\u793a\u4e86\u4fe1\u4efb\u548c\u6548\u7528\u5dee\u8ddd\uff1b\u540c\u65f6\uff0cAI\u63d0\u4ea4\u7684\u4ee3\u7801\u7ed3\u6784\u4e0a\u66f4\u7b80\u5355\u3002", "conclusion": "AIDev\u6570\u636e\u96c6\u4e3a\u7814\u7a76AI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u81ea\u4e3b\u534f\u4f5c\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u5b9e\u8bc1\u57fa\u7840\uff0c\u652f\u6301\u4e86AI\u539f\u751f\u5de5\u4f5c\u6d41\u7a0b\u7684\u7814\u7a76\u548c\u6784\u5efa\u4e0b\u4e00\u4ee3\u4eba\u673a\u5171\u751f\u534f\u4f5c\u3002"}}
{"id": "2507.14876", "categories": ["cs.NI", "94A05", "C.2.1; C.2.3"], "pdf": "https://arxiv.org/pdf/2507.14876", "abs": "https://arxiv.org/abs/2507.14876", "authors": ["Zi-Yang Wu", "Muhammad Ismail", "Jiliang Zhang", "Jie Zhang"], "title": "Tidal-Like Concept Drift in RIS-Covered Buildings: When Programmable Wireless Environments Meet Human Behaviors", "comment": "Accepted by IEEE Wireless Communications, to appear in 2025", "summary": "Indoor mobile networks handle the majority of data traffic, with their\nperformance limited by building materials and structures. However, building\ndesigns have historically not prioritized wireless performance. Prior to the\nadvent of reconfigurable intelligent surfaces (RIS), the industry passively\nadapted to wireless propagation challenges within buildings. Inspired by RIS's\nsuccesses in outdoor networks, we propose embedding RIS into building\nstructures to manipulate and enhance building wireless performance\ncomprehensively. Nonetheless, the ubiquitous mobility of users introduces\ncomplex dynamics to the channels of RIS-covered buildings. A deep understanding\nof indoor human behavior patterns is essential for achieving wireless-friendly\nbuilding design. This article is the first to systematically examine the tidal\nevolution phenomena emerging in the channels of RIS-covered buildings driven by\ncomplex human behaviors. We demonstrate that a universal channel model is\nunattainable and focus on analyzing the challenges faced by advanced deep\nlearning-based prediction and control strategies, including high-order Markov\ndependencies, concept drift, and generalization issues caused by human-induced\ndisturbances. Possible solutions for orchestrating the coexistence of\nRIS-covered buildings and crowd mobility are also laid out.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5c06RIS\u5d4c\u5165\u5efa\u7b51\u4ee5\u4f18\u5316\u65e0\u7ebf\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u4eba\u7c7b\u79fb\u52a8\u6027\u5e26\u6765\u7684\u901a\u9053\u52a8\u6001\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5efa\u7b51\u8bbe\u8ba1\u5386\u53f2\u4e0a\u672a\u4f18\u5148\u8003\u8651\u65e0\u7ebf\u6027\u80fd\uff0c\u800cRIS\u5728\u6237\u5916\u7f51\u7edc\u7684\u6210\u529f\u6fc0\u53d1\u4e86\u5c06\u5176\u5d4c\u5165\u5efa\u7b51\u7ed3\u6784\u4ee5\u5168\u9762\u589e\u5f3a\u65e0\u7ebf\u6027\u80fd\u7684\u60f3\u6cd5\u3002\u7136\u800c\uff0c\u7528\u6237\u79fb\u52a8\u6027\u5f15\u5165\u4e86\u901a\u9053\u7684\u590d\u6742\u52a8\u6001\uff0c\u9700\u6df1\u5165\u7406\u89e3\u4eba\u7c7b\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "\u6587\u7ae0\u9996\u6b21\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u7531\u590d\u6742\u4eba\u7c7b\u884c\u4e3a\u9a71\u52a8\u7684RIS\u8986\u76d6\u5efa\u7b51\u901a\u9053\u4e2d\u7684\u6f6e\u6c50\u6f14\u5316\u73b0\u8c61\uff0c\u5e76\u5206\u6790\u4e86\u9ad8\u7ea7\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u548c\u63a7\u5236\u7b56\u7565\u9762\u4e34\u7684\u6311\u6218\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u7531\u4e8e\u4eba\u7c7b\u884c\u4e3a\u5f15\u8d77\u7684\u5e72\u6270\uff0c\u65e0\u6cd5\u5b9e\u73b0\u901a\u7528\u7684\u901a\u9053\u6a21\u578b\u3002\u6587\u7ae0\u91cd\u70b9\u5206\u6790\u4e86\u9ad8\u9636\u9a6c\u5c14\u53ef\u592b\u4f9d\u8d56\u3001\u6982\u5ff5\u6f02\u79fb\u548c\u6cdb\u5316\u95ee\u9898\u7b49\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u5d4c\u5165\u5efa\u7b51\u7ed3\u6784\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u4ee5\u5168\u9762\u589e\u5f3a\u5efa\u7b51\u65e0\u7ebf\u6027\u80fd\u3002\u540c\u65f6\uff0c\u5f3a\u8c03\u4e86\u5728RIS\u8986\u76d6\u7684\u5efa\u7b51\u4e2d\uff0c\u7531\u4e8e\u7528\u6237\u79fb\u52a8\u6027\u5f15\u5165\u7684\u590d\u6742\u52a8\u6001\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u5ba4\u5185\u4eba\u7c7b\u884c\u4e3a\u6a21\u5f0f\uff0c\u4ee5\u5b9e\u73b0\u65e0\u7ebf\u53cb\u597d\u7684\u5efa\u7b51\u8bbe\u8ba1\u3002\u6587\u7ae0\u8fd8\u63a2\u8ba8\u4e86\u534f\u8c03RIS\u8986\u76d6\u5efa\u7b51\u4e0e\u4eba\u7fa4\u79fb\u52a8\u6027\u5171\u5b58\u7684\u53ef\u80fd\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14731", "abs": "https://arxiv.org/abs/2507.14731", "authors": ["Haitong Wang", "Aaron Hao Tan", "Angus Fung", "Goldie Nejat"], "title": "X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots", "comment": null, "summary": "Existing navigation methods are primarily designed for specific robot\nembodiments, limiting their generalizability across diverse robot platforms. In\nthis paper, we introduce X-Nav, a novel framework for end-to-end\ncross-embodiment navigation where a single unified policy can be deployed\nacross various embodiments for both wheeled and quadrupedal robots. X-Nav\nconsists of two learning stages: 1) multiple expert policies are trained using\ndeep reinforcement learning with privileged observations on a wide range of\nrandomly generated robot embodiments; and 2) a single general policy is\ndistilled from the expert policies via navigation action chunking with\ntransformer (Nav-ACT). The general policy directly maps visual and\nproprioceptive observations to low-level control commands, enabling\ngeneralization to novel robot embodiments. Simulated experiments demonstrated\nthat X-Nav achieved zero-shot transfer to both unseen embodiments and\nphotorealistic environments. A scalability study showed that the performance of\nX-Nav improves when trained with an increasing number of randomly generated\nembodiments. An ablation study confirmed the design choices of X-Nav.\nFurthermore, real-world experiments were conducted to validate the\ngeneralizability of X-Nav in real-world environments.", "AI": {"tldr": "X-Nav \u662f\u4e00\u4e2a\u8de8\u5e73\u53f0\u901a\u7528\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5b66\u4e60\u5b9e\u73b0\u5355\u4e00\u7b56\u7565\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u8de8\u5e73\u53f0\u7684\u901a\u7528\u6027\u3002X-Nav \u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u5355\u4e00\u7b56\u7565\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u3002", "method": "X-Nav \u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\uff1a1) \u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u591a\u4e2a\u4e13\u5bb6\u7b56\u7565\uff1b2) \u901a\u8fc7\u5bfc\u822a\u52a8\u4f5c\u5206\u5757\u4e0eTransformer\uff08Nav-ACT\uff09\u4ece\u4e13\u5bb6\u7b56\u7565\u4e2d\u84b8\u998f\u51fa\u5355\u4e00\u901a\u7528\u7b56\u7565\u3002", "result": "X-Nav \u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u8fc7\u7684\u673a\u5668\u4eba\u5e73\u53f0\u548c\u903c\u771f\u73af\u5883\u7684\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u6027\u3002", "conclusion": "X-Nav \u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u5b66\u4e60\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u8de8\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u7684\u901a\u7528\u5bfc\u822a\u7b56\u7565\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.14730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14730", "abs": "https://arxiv.org/abs/2507.14730", "authors": ["Yanjie Fu"], "title": "Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI", "comment": "4 pages; will continue to update to add more figures to describe the\n  vision;", "summary": "Generative AI, large language models, and agentic AI have emerged separately\nof urban planning. However, the convergence between AI and urban planning\npresents an interesting opportunity towards AI urban planners. This paper\nconceptualizes urban planning as a generative AI task, where AI synthesizes\nland-use configurations under geospatial, social, and human-centric\nconstraints. We survey how generative AI approaches, including VAEs, GANs,\ntransformers, and diffusion models, reshape urban design. We further identify\ncritical gaps: 1) limited research on integrating urban theory guidance, 2)\nlimited research of AI urban planning over multiple spatial resolutions or\nangularities, 3) limited research on augmenting urban design knowledge from\ndata, and 4) limited research on addressing real-world interactions. To address\nthese limitations, we outline future research directions in theory-guided\ngeneration, digital twins, and human-machine co-design, calling for a new\nsynthesis of generative intelligence and participatory urbanism.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5728\u57ce\u5e02\u89c4\u5212\u4e2d\u7684\u5e94\u7528\uff0c\u6307\u51fa\u5f53\u524d\u7814\u7a76\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u7406\u8bba\u5f15\u5bfc\u751f\u6210\u3001\u6570\u5b57\u5b6a\u751f\u548c\u4eba\u673a\u534f\u540c\u8bbe\u8ba1\u7b49\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u751f\u6210\u5f0fAI\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4ee3\u7406AI\u7684\u51fa\u73b0\u4e3a\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\uff0c\u672c\u6587\u65e8\u5728\u5c06\u57ce\u5e02\u89c4\u5212\u6982\u5ff5\u5316\u4e3a\u751f\u6210\u5f0fAI\u4efb\u52a1\u3002", "method": "\u672c\u6587\u8c03\u67e5\u4e86\u751f\u6210\u5f0fAI\u65b9\u6cd5\uff08\u5982VAEs\u3001GANs\u3001transformers\u548c\u6269\u6563\u6a21\u578b\uff09\u5982\u4f55\u91cd\u5851\u57ce\u5e02\u8bbe\u8ba1\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u5173\u952e\u7f3a\u53e3\u3002", "result": "\u672c\u6587\u8bc6\u522b\u4e86\u56db\u4e2a\u5173\u952e\u7814\u7a76\u7f3a\u53e3\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u547c\u5401\u751f\u6210\u667a\u80fd\u4e0e\u53c2\u4e0e\u5f0f\u57ce\u5e02\u4e3b\u4e49\u7684\u65b0\u7efc\u5408\uff0c\u63d0\u51fa\u4e86\u7406\u8bba\u5f15\u5bfc\u751f\u6210\u3001\u6570\u5b57\u5b6a\u751f\u548c\u4eba\u673a\u534f\u540c\u8bbe\u8ba1\u7b49\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.15025", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15025", "abs": "https://arxiv.org/abs/2507.15025", "authors": ["Nenad Petrovic", "Vahid Zolfaghari", "Andre Schamschurko", "Sven Kirchner", "Fengjunjie Pan", "Chengdng Wu", "Nils Purschke", "Aleksei Velsh", "Krzysztof Lebioda", "Yinglei Song", "Yi Zhang", "Lukasz Mazur", "Alois Knoll"], "title": "Survey of GenAI for Automotive Software Development: From Requirements to Executable Code", "comment": "Conference paper accepted for GACLM 2025", "summary": "Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to\nrevolutionize many industrial areas by reducing the amount of human\nintervention needed and effort for handling complex underlying processes.\nAutomotive software development is considered to be a significant area for\nGenAI adoption, taking into account lengthy and expensive procedures, resulting\nfrom the amount of requirements and strict standardization. In this paper, we\nexplore the adoption of GenAI for various steps of automotive software\ndevelopment, mainly focusing on requirements handling, compliance aspects and\ncode generation. Three GenAI-related technologies are covered within the\nstate-of-art: Large Language Models (LLMs), Retrieval Augmented Generation\n(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting\ntechniques in case of code generation. Additionally, we also derive a\ngeneralized GenAI-aided automotive software development workflow based on our\nfindings from this literature review. Finally, we include a summary of a survey\noutcome, which was conducted among our automotive industry partners regarding\nthe type of GenAI tools used for their daily work activities.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86GenAI\u5728\u6c7d\u8f66\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u9700\u6c42\u5904\u7406\u3001\u5408\u89c4\u6027\u548c\u4ee3\u7801\u751f\u6210\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u5206\u4eab\u4e86\u884c\u4e1a\u8c03\u67e5\u7ed3\u679c\u3002", "motivation": "\u6c7d\u8f66\u8f6f\u4ef6\u5f00\u53d1\u8fc7\u7a0b\u5197\u957f\u4e14\u6602\u8d35\uff0c\u6d89\u53ca\u5927\u91cf\u9700\u6c42\u548c\u4e25\u683c\u6807\u51c6\u5316\uff0c\u56e0\u6b64\u63a2\u7d22GenAI\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u884c\u4e1a\u8c03\u67e5\uff0c\u63a2\u8ba8\u4e86GenAI\u5728\u6c7d\u8f66\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u9700\u6c42\u5904\u7406\u3001\u5408\u89c4\u6027\u65b9\u9762\u548c\u4ee3\u7801\u751f\u6210\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684GenAI\u8f85\u52a9\u6c7d\u8f66\u8f6f\u4ef6\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u603b\u7ed3\u4e86\u884c\u4e1a\u5408\u4f5c\u4f19\u4f34\u5bf9GenAI\u5de5\u5177\u7684\u4f7f\u7528\u60c5\u51b5\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u4e00\u79cd\u57fa\u4e8eGenAI\u7684\u6c7d\u8f66\u8f6f\u4ef6\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u63d0\u4f9b\u4e86\u884c\u4e1a\u5408\u4f5c\u4f19\u4f34\u5bf9GenAI\u5de5\u5177\u4f7f\u7528\u60c5\u51b5\u7684\u8c03\u67e5\u7ed3\u679c\u3002"}}
{"id": "2507.14891", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.14891", "abs": "https://arxiv.org/abs/2507.14891", "authors": ["Xiangyu Gao", "Tong Li", "Yinchao Zhang", "Ziqiang Wang", "Xiangsheng Zeng", "Su Yao", "Ke Xu"], "title": "FENIX: Enabling In-Network DNN Inference with FPGA-Enhanced Programmable Switches", "comment": null, "summary": "Machine learning (ML) is increasingly used in network data planes for\nadvanced traffic analysis. However, existing solutions (such as FlowLens, N3IC,\nand BoS) still struggle to simultaneously achieve low latency, high throughput,\nand high accuracy. To address these challenges, we present FENIX, a hybrid\nin-network ML system that performs feature extraction on programmable switch\nASICs and deep neural network inference on FPGAs. FENIX introduces a Data\nEngine that leverages a probabilistic token bucket algorithm to control the\nsending rate of feature streams, effectively addressing the throughput gap\nbetween programmable switch ASICs and FPGAs. In addition, FENIX designs a Model\nEngine to enable high-accuracy deep neural network inference in the network,\novercoming the difficulty of deploying complex models on resource-constrained\nswitch chips. We implement FENIX on a programmable switch platform that\nintegrates a Tofino ASIC and a ZU19EG FPGA directly and evaluate it on\nreal-world network traffic datasets. Our results show that FENIX achieves\nmicrosecond-level inference latency and multi-terabit throughput with low\nhardware overhead, and delivers over 95\\% accuracy on mainstream network\ntraffic classification tasks, outperforming SOTA.", "AI": {"tldr": "FENIX \u662f\u4e00\u79cd\u6df7\u5408\u7f51\u7edc\u5185 ML \u7cfb\u7edf\uff0c\u7ed3\u5408 ASIC \u548c FPGA\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6848\u65e0\u6cd5\u517c\u987e\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u91cf\u548c\u9ad8\u51c6\u786e\u6027\u7684\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e SOTA\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982 FlowLens\u3001N3IC \u548c BoS\uff09\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u91cf\u548c\u9ad8\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u7f51\u7edc\u5185 ML \u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "FENIX \u5f15\u5165\u4e86\u6570\u636e\u5f15\u64ce\uff08Data Engine\uff09\u548c\u6a21\u578b\u5f15\u64ce\uff08Model Engine\uff09\uff0c\u5206\u522b\u901a\u8fc7\u6982\u7387\u4ee4\u724c\u6876\u7b97\u6cd5\u63a7\u5236\u7279\u5f81\u6d41\u7684\u53d1\u9001\u901f\u7387\uff0c\u4ee5\u53ca\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4ea4\u6362\u673a\u82af\u7247\u4e0a\u90e8\u7f72\u9ad8\u7cbe\u5ea6\u590d\u6742\u6a21\u578b\u3002", "result": "FENIX \u5728\u771f\u5b9e\u7f51\u7edc\u6d41\u91cf\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5fae\u79d2\u7ea7\u63a8\u7406\u5ef6\u8fdf\u3001\u591a\u592a\u6bd4\u7279\u541e\u5410\u91cf\uff0c\u786c\u4ef6\u5f00\u9500\u4f4e\uff0c\u5e76\u5728\u4e3b\u6d41\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u8d85\u8fc7 95% \u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "FENIX \u662f\u4e00\u79cd\u6df7\u5408\u7f51\u7edc\u5185\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u53ef\u7f16\u7a0b\u4ea4\u6362\u673a ASIC \u4e0a\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u5728 FPGA \u4e0a\u8fdb\u884c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u91cf\u548c\u9ad8\u51c6\u786e\u6027\u3002"}}
{"id": "2507.14820", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14820", "abs": "https://arxiv.org/abs/2507.14820", "authors": ["Bingran Chen", "Baorun Li", "Jian Yang", "Yong Liu", "Guangyao Zhai"], "title": "KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D Correspondence Learning", "comment": null, "summary": "High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation\nto serve as a basic function. Previous approaches either directly generate\ngrasps from point-cloud data, suffering from challenges with small objects and\nsensor noise, or infer 3D information from RGB images, which introduces\nexpensive annotation requirements and discretization issues. Recent methods\nmitigate some challenges by retaining a 2D representation to estimate grasp\nkeypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF\nposes. However, these methods are limited by their non-differentiable nature\nand reliance solely on 2D supervision, which hinders the full exploitation of\nrich 3D information. In this work, we present KGN-Pro, a novel grasping network\nthat preserves the efficiency and fine-grained object grasping of previous KGNs\nwhile integrating direct 3D optimization through probabilistic PnP layers.\nKGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further\noutputs a 2D confidence map to weight keypoint contributions during\nre-projection error minimization. By modeling the weighted sum of squared\nre-projection errors probabilistically, the network effectively transmits 3D\nsupervision to its 2D keypoint predictions, enabling end-to-end learning.\nExperiments on both simulated and real-world platforms demonstrate that KGN-Pro\noutperforms existing methods in terms of grasp cover rate and success rate.", "AI": {"tldr": "KGN-Pro\u662f\u4e00\u79cd\u65b0\u578b\u6293\u53d6\u7f51\u7edc\uff0c\u901a\u8fc7\u6982\u7387PnP\u5c42\u76f4\u63a5\u8fdb\u884c3D\u4f18\u5316\uff0c\u63d0\u9ad8\u4e866-DoF\u6293\u53d6\u4f30\u8ba1\u7684\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u57286-DoF\u6293\u53d6\u4f30\u8ba1\u4e2d\u56e0\u5c0f\u7269\u4f53\u3001\u4f20\u611f\u5668\u566a\u58f0\u3001\u6602\u8d35\u6807\u6ce8\u6216\u79bb\u6563\u5316\u95ee\u9898\u5bfc\u81f4\u7684\u6027\u80fd\u9650\u5236\u3002", "method": "\u7ed3\u5408RGB-D\u56fe\u50cf\u751f\u6210\u5173\u952e\u70b9\u5730\u56fe\u548c2D\u7f6e\u4fe1\u5ea6\u56fe\uff0c\u901a\u8fc7\u6982\u7387PnP\u5c42\u8fdb\u884c\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5e73\u53f0\u4e0a\uff0cKGN-Pro\u5728\u6293\u53d6\u8986\u76d6\u7387\u548c\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "KGN-Pro\u901a\u8fc7\u6574\u54083D\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6293\u53d6\u6027\u80fd\uff0c\u4e3a\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14897", "categories": ["cs.AI", "I.2.5"], "pdf": "https://arxiv.org/pdf/2507.14897", "abs": "https://arxiv.org/abs/2507.14897", "authors": ["Renxi Wang", "Rifo Ahmad Genadi", "Bilal El Bouardi", "Yongxin Wang", "Fajri Koto", "Zhengzhong Liu", "Timothy Baldwin", "Haonan Li"], "title": "AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents", "comment": null, "summary": "Language model (LM) agents have gained significant attention for their\nability to autonomously complete tasks through interactions with environments,\ntools, and APIs. LM agents are primarily built with prompt engineering or\nsupervised finetuning. At the same time, reinforcement learning (RL) has been\nexplored to enhance LM's capabilities, such as reasoning and factuality.\nHowever, the combination of the LM agents and reinforcement learning (Agent-RL)\nremains underexplored and lacks systematic study. To this end, we built\nAgentFly, a scalable and extensible Agent-RL framework designed to empower LM\nagents with a variety of RL algorithms. Our framework supports multi-turn\ninteractions by adapting traditional RL methods with token-level masking. It\nfeatures a decorator-based interface for defining tools and reward functions,\nenabling seamless extension and ease of use. To support high-throughput\ntraining, we implement asynchronous execution of tool calls and reward\ncomputations, and design a centralized resource management system for scalable\nenvironment coordination. We also provide a suite of prebuilt tools and\nenvironments, demonstrating the framework's effectiveness through successful\nagent training across multiple tasks.", "AI": {"tldr": "AgentFly\u662f\u4e00\u4e2a\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u652f\u6301\u591a\u8f6e\u4ea4\u4e92\u548c\u9ad8\u541e\u5410\u91cf\u8bad\u7ec3\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u591a\u4efb\u52a1\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\uff08Agent-RL\uff09\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u6846\u67b6\u6765\u63a2\u7d22\u548c\u589e\u5f3a\u5176\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86AgentFly\uff0c\u4e00\u4e2a\u652f\u6301\u591a\u8f6e\u4ea4\u4e92\u3001\u5f02\u6b65\u6267\u884c\u5de5\u5177\u8c03\u7528\u548c\u5956\u52b1\u8ba1\u7b97\u7684\u53ef\u6269\u5c55Agent-RL\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u88c5\u9970\u5668\u7684\u63a5\u53e3\u5b9a\u4e49\u5de5\u5177\u548c\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u541e\u5410\u91cf\u8bad\u7ec3\uff0c\u6210\u529f\u5728\u591a\u4efb\u52a1\u4e2d\u8bad\u7ec3\u4e86\u4ee3\u7406\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "AgentFly\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u5728\u591a\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.15157", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15157", "abs": "https://arxiv.org/abs/2507.15157", "authors": ["Giovanni Quattrocchi", "Liliana Pasquale", "Paola Spoletini", "Luciano Baresi"], "title": "Can LLMs Generate User Stories and Assess Their Quality?", "comment": null, "summary": "Requirements elicitation is still one of the most challenging activities of\nthe requirements engineering process due to the difficulty requirements\nanalysts face in understanding and translating complex needs into concrete\nrequirements. In addition, specifying high-quality requirements is crucial, as\nit can directly impact the quality of the software to be developed. Although\nautomated tools allow for assessing the syntactic quality of requirements,\nevaluating semantic metrics (e.g., language clarity, internal consistency)\nremains a manual and time-consuming activity. This paper explores how LLMs can\nhelp automate requirements elicitation within agile frameworks, where\nrequirements are defined as user stories (US). We used 10 state-of-the-art LLMs\nto investigate their ability to generate US automatically by emulating customer\ninterviews. We evaluated the quality of US generated by LLMs, comparing it with\nthe quality of US generated by humans (domain experts and students). We also\nexplored whether and how LLMs can be used to automatically evaluate the\nsemantic quality of US. Our results indicate that LLMs can generate US similar\nto humans in terms of coverage and stylistic quality, but exhibit lower\ndiversity and creativity. Although LLM-generated US are generally comparable in\nquality to those created by humans, they tend to meet the acceptance quality\ncriteria less frequently, regardless of the scale of the LLM model. Finally,\nLLMs can reliably assess the semantic quality of US when provided with clear\nevaluation criteria and have the potential to reduce human effort in\nlarge-scale assessments.", "AI": {"tldr": "LLMs\u80fd\u751f\u6210\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u7528\u6237\u6545\u4e8b\uff0c\u4f46\u591a\u6837\u6027\u548c\u521b\u9020\u529b\u4e0d\u8db3\uff1b\u5728\u660e\u786e\u6807\u51c6\u4e0b\u53ef\u81ea\u52a8\u5316\u8bc4\u4f30\u8bed\u4e49\u8d28\u91cf\uff0c\u51cf\u5c11\u4eba\u529b\u6295\u5165\u3002", "motivation": "\u7531\u4e8e\u9700\u6c42\u5206\u6790\u5e08\u5728\u7406\u89e3\u548c\u8f6c\u5316\u590d\u6742\u9700\u6c42\u65f6\u9762\u4e34\u6311\u6218\uff0c\u4ee5\u53ca\u8bc4\u4f30\u8bed\u4e49\u8d28\u91cf\u7684\u8017\u65f6\u6027\uff0c\u7814\u7a76\u63a2\u7d22LLMs\u5728\u654f\u6377\u6846\u67b6\u4e2d\u81ea\u52a8\u5316\u9700\u6c42\u83b7\u53d6\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u4f7f\u752810\u79cd\u5148\u8fdb\u7684LLMs\uff0c\u6a21\u62df\u5ba2\u6237\u8bbf\u8c08\u81ea\u52a8\u751f\u6210US\uff0c\u5e76\u6bd4\u8f83LLM\u4e0e\u4eba\u7c7b\uff08\u9886\u57df\u4e13\u5bb6\u548c\u5b66\u751f\uff09\u751f\u6210\u7684US\u8d28\u91cf\u3002\u540c\u65f6\u63a2\u7d22LLMs\u5728\u81ea\u52a8\u8bc4\u4f30US\u8bed\u4e49\u8d28\u91cf\u4e0a\u7684\u5e94\u7528\u3002", "result": "LLMs\u751f\u6210\u7684US\u5728\u8986\u76d6\u8303\u56f4\u548c\u98ce\u683c\u8d28\u91cf\u4e0a\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u4f46\u591a\u6837\u6027\u548c\u521b\u9020\u529b\u8f83\u4f4e\u3002LLM\u751f\u6210\u7684US\u8fbe\u5230\u63a5\u53d7\u6807\u51c6\u7684\u9891\u7387\u8f83\u4f4e\uff0c\u4f46LLMs\u5728\u63d0\u4f9b\u660e\u786e\u6807\u51c6\u65f6\u53ef\u53ef\u9760\u8bc4\u4f30\u8bed\u4e49\u8d28\u91cf\u3002", "conclusion": "LLMs\u80fd\u591f\u751f\u6210\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u7528\u6237\u6545\u4e8b\uff08US\uff09\uff0c\u4f46\u5728\u591a\u6837\u6027\u548c\u521b\u9020\u529b\u4e0a\u8868\u73b0\u8f83\u5dee\u3002\u5c3d\u7ba1LLM\u751f\u6210\u7684US\u8d28\u91cf\u4e0e\u4eba\u7c7b\u76f8\u5f53\uff0c\u4f46\u8fbe\u5230\u63a5\u53d7\u6807\u51c6\u7684\u9891\u7387\u8f83\u4f4e\u3002LLMs\u5728\u63d0\u4f9b\u660e\u786e\u8bc4\u4f30\u6807\u51c6\u65f6\uff0c\u53ef\u4ee5\u53ef\u9760\u5730\u8bc4\u4f30US\u7684\u8bed\u4e49\u8d28\u91cf\uff0c\u5e76\u6709\u671b\u51cf\u5c11\u5927\u89c4\u6a21\u8bc4\u4f30\u4e2d\u7684\u4eba\u529b\u6295\u5165\u3002"}}
{"id": "2507.15145", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15145", "abs": "https://arxiv.org/abs/2507.15145", "authors": ["Thai T. Vu", "John Le"], "title": "Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness", "comment": "8 pages", "summary": "This paper proposes a communication-efficient, event-triggered inference\nframework for cooperative edge AI systems comprising multiple user devices and\nedge servers. Building upon dual-threshold early-exit strategies for rare-event\ndetection, the proposed approach extends classical single-device inference to a\ndistributed, multi-device setting while incorporating proportional fairness\nconstraints across users. A joint optimization framework is formulated to\nmaximize classification utility under communication, energy, and fairness\nconstraints. To solve the resulting problem efficiently, we exploit the\nmonotonicity of the utility function with respect to the confidence thresholds\nand apply alternating optimization with Benders decomposition. Experimental\nresults show that the proposed framework significantly enhances system-wide\nperformance and fairness in resource allocation compared to single-device\nbaselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u4fe1\u9ad8\u6548\u3001\u4e8b\u4ef6\u89e6\u53d1\u7684\u8fb9\u7f18AI\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u9608\u503c\u548c\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u591a\u8bbe\u5907\u534f\u540c\u4e0b\u7684\u6027\u80fd\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u7528\u6237\u8bbe\u5907\u548c\u8fb9\u7f18\u670d\u52a1\u5668\u534f\u540c\u8fb9\u7f18AI\u7cfb\u7edf\u4e2d\u7684\u901a\u4fe1\u6548\u7387\u3001\u80fd\u91cf\u6d88\u8017\u548c\u516c\u5e73\u6027\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7684\u5355\u8bbe\u5907\u63a8\u7406\u5230\u5206\u5e03\u5f0f\u591a\u8bbe\u5907\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u9608\u503c\u65e9\u671f\u9000\u51fa\u7b56\u7565\u7684\u901a\u4fe1\u9ad8\u6548\u3001\u4e8b\u4ef6\u89e6\u53d1\u7684\u63a8\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u6bd4\u4f8b\u516c\u5e73\u7ea6\u675f\uff0c\u91c7\u7528\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u548cBenders\u5206\u89e3\u9ad8\u6548\u6c42\u89e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u7cfb\u7edf\u6027\u80fd\u548c\u8d44\u6e90\u5206\u914d\u516c\u5e73\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5355\u8bbe\u5907\u57fa\u51c6\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316\u548c\u4ea4\u66ff\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6574\u4f53\u6027\u80fd\u548c\u8d44\u6e90\u5206\u914d\u7684\u516c\u5e73\u6027\uff0c\u4f18\u4e8e\u5355\u8bbe\u5907\u57fa\u51c6\u3002"}}
{"id": "2507.14903", "categories": ["cs.RO", "I.2.9; I.2.10; I.2.11"], "pdf": "https://arxiv.org/pdf/2507.14903", "abs": "https://arxiv.org/abs/2507.14903", "authors": ["Pan Hu"], "title": "CoMoCAVs: Cohesive Decision-Guided Motion Planning for Connected and Autonomous Vehicles with Multi-Policy Reinforcement Learning", "comment": "8 pages, 5 figures", "summary": "Autonomous driving demands reliable and efficient solutions to closely\nrelated problems such as decision-making and motion planning. In this work,\ndecision-making refers specifically to highway lane selection, while motion\nplanning involves generating control commands (such as speed and steering) to\nreach the chosen lane. In the context of Connected Autonomous Vehicles (CAVs),\nachieving both flexible and safe lane selection alongside precise trajectory\nexecution remains a significant challenge. This paper proposes a framework\ncalled Cohesive Decision-Guided Motion Planning (CDGMP), which tightly\nintegrates decision-making and motion planning using a Mixture of Experts (MoE)\ninspired architecture combined with multi-policy reinforcement learning. By\ncoordinating multiple specialized sub-networks through a gating mechanism, the\nmethod decomposes the complex driving task into modular components. Each\nsub-network focuses on a specific aspect of driving, improving efficiency by\nactivating only the most relevant modules during inference. This design also\nenhances safety through modular specialization. CDGMP improves the adaptability\nand robustness of CAVs across diverse traffic scenarios, offering a scalable\nsolution to real-world autonomy challenges. The architectural principles behind\nCDGMP, especially the use of MoE, also provide a strong foundation for other\nhigh-dimensional decision and control tasks. Simulation results (available at\nhttps://youtu.be/_-4OXNHV0UY) demonstrate reliable performance in both lane\nselection and motion planning.", "AI": {"tldr": "CDGMP\u6846\u67b6\u7ed3\u5408MoE\u548c\u591a\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u63d0\u5347CAVs\u7684\u51b3\u7b56\u548c\u8fd0\u52a8\u89c4\u5212\u80fd\u529b\uff0c\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u3002", "motivation": "\u5728CAVs\u80cc\u666f\u4e0b\uff0c\u5b9e\u73b0\u7075\u6d3b\u5b89\u5168\u7684\u8f66\u9053\u9009\u62e9\u548c\u7cbe\u786e\u7684\u8f68\u8ff9\u6267\u884c\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDGMP\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528MoE\u67b6\u6784\u548c\u591a\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u534f\u8c03\u591a\u4e2a\u4e13\u95e8\u5b50\u7f51\u7edc\uff0c\u5c06\u590d\u6742\u7684\u9a7e\u9a76\u4efb\u52a1\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u7ec4\u4ef6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660eCDGMP\u5728\u8f66\u9053\u9009\u62e9\u548c\u8fd0\u52a8\u89c4\u5212\u65b9\u9762\u8868\u73b0\u53ef\u9760\u3002", "conclusion": "CDGMP\u6846\u67b6\u901a\u8fc7\u6574\u5408\u51b3\u7b56\u5236\u5b9a\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u7ed3\u5408MoE\u67b6\u6784\u548c\u591a\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3aCAVs\u63d0\u4f9b\u4e86\u9002\u5e94\u6027\u5f3a\u3001\u9c81\u68d2\u6027\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4e3a\u5176\u4ed6\u9ad8\u7ef4\u51b3\u7b56\u548c\u63a7\u5236\u4efb\u52a1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.14899", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14899", "abs": "https://arxiv.org/abs/2507.14899", "authors": ["Jiale Liu", "Huan Wang", "Yue Zhang", "Xiaoyu Luo", "Jiaxiang Hu", "Zhiliang Liu", "Min Xie"], "title": "InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis", "comment": null, "summary": "Non-destructive testing (NDT), particularly X-ray inspection, is vital for\nindustrial quality assurance, yet existing deep-learning-based approaches often\nlack interactivity, interpretability, and the capacity for critical\nself-assessment, limiting their reliability and operator trust. To address\nthese shortcomings, this paper proposes InsightX Agent, a novel LMM-based\nagentic framework designed to deliver reliable, interpretable, and interactive\nX-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent\npositions a Large Multimodal Model (LMM) as a central orchestrator,\ncoordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the\nEvidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect\nregion proposals for multi-scale feature maps and sparsifies them through\nNon-Maximum Suppression (NMS), optimizing detection of small, dense targets in\nX-ray images while maintaining computational efficiency. The EGR tool guides\nthe LMM agent through a chain-of-thought-inspired review process, incorporating\ncontext assessment, individual defect analysis, false positive elimination,\nconfidence recalibration and quality assurance to validate and refine the\nSDMSD's initial proposals. By strategically employing and intelligently using\ntools, InsightX Agent moves beyond passive data processing to active reasoning,\nenhancing diagnostic reliability and providing interpretations that integrate\ndiverse information sources. Experimental evaluations on the GDXray+ dataset\ndemonstrate that InsightX Agent not only achieves a high object detection\nF1-score of 96.35% but also offers significantly improved interpretability and\ntrustworthiness in its analyses, highlighting the transformative potential of\nagentic LLM frameworks for industrial inspection tasks.", "AI": {"tldr": "InsightX Agent\u662f\u4e00\u79cd\u57fa\u4e8eLMM\u7684\u65b0\u578b\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408SDMSD\u548cEGR\u5de5\u5177\uff0c\u63d0\u5347\u4e86X\u5c04\u7ebf\u65e0\u635f\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u9ad8\u6548\u4e14\u53ef\u4fe1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684X\u5c04\u7ebf\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u4ea4\u4e92\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u53ef\u9760\u6027\u548c\u64cd\u4f5c\u5458\u4fe1\u4efb\u3002", "method": "\u63d0\u51faInsightX Agent\u6846\u67b6\uff0c\u4ee5LMM\u4e3a\u6838\u5fc3\u534f\u8c03\u5668\uff0c\u7ed3\u5408SDMSD\u8fdb\u884c\u591a\u5c3a\u5ea6\u7f3a\u9677\u68c0\u6d4b\u548cEGR\u5de5\u5177\u8fdb\u884c\u8bc1\u636e\u94fe\u5f0f\u53cd\u601d\uff0c\u4f18\u5316\u68c0\u6d4b\u6d41\u7a0b\u5e76\u63d0\u5347\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6\u3002", "result": "\u5728GDXray+\u6570\u636e\u96c6\u4e0a\uff0cInsightX Agent\u5b9e\u73b0\u4e8696.35%\u7684\u76ee\u6807\u68c0\u6d4bF1\u5206\u6570\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5206\u6790\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "InsightX Agent\u901a\u8fc7\u7ed3\u5408LMM\u3001SDMSD\u548cEGR\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86X\u5c04\u7ebf\u65e0\u635f\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4ea4\u4e92\u6027\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u4ee3\u7406\u7684LLM\u6846\u67b6\u5728\u5de5\u4e1a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2507.15181", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15181", "abs": "https://arxiv.org/abs/2507.15181", "authors": ["Yinglong Zou", "Juan Zhai", "Chunrong Fang", "Yanzhou Mu", "Jiawei Liu", "Zhenyu Chen"], "title": "Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements", "comment": null, "summary": "Deep learning frameworks serve as the foundation for developing and deploying\ndeep learning applications. To enhance the quality of deep learning frameworks,\nresearchers have proposed numerous testing methods using deep learning models\nas test inputs. However, existing methods predominantly measure model bug\ndetection effectiveness as heuristic indicators, presenting three critical\nlimitations: Firstly, existing methods fail to quantitatively measure model's\noperator combination variety, potentially missing critical operator\ncombinations that could trigger framework bugs. Secondly, existing methods\nneglect measuring model execution time, resulting in the omission of numerous\nmodels potential for detecting more framework bugs within limited testing time.\nThirdly, existing methods overlook correlation between different model\nmeasurements, relying simply on single-indicator heuristic guidance without\nconsidering their trade-offs. To overcome these limitations, we propose DLMMM,\nthe first deep learning framework testing method to include multiple model\nmeasurements into heuristic guidance and fuse these measurements to achieve\ntheir trade-off. DLMMM firstly quantitatively measures model's bug detection\nperformance, operator combination variety, and model execution time. After\nthat, DLMMM fuses the above measurements based on their correlation to achieve\ntheir trade-off. To further enhance testing effectiveness, DLMMM designs\nmulti-level heuristic guidance for test input model generation.", "AI": {"tldr": "DLMMM\u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u591a\u79cd\u6a21\u578b\u6d4b\u91cf\u6307\u6807\u548c\u591a\u7ea7\u542f\u53d1\u5f0f\u6307\u5bfc\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e09\u5927\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u6d4b\u8bd5\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6d4b\u8bd5\u65b9\u6cd5\u5b58\u5728\u4e09\u5927\u5c40\u9650\u6027\uff1a\u65e0\u6cd5\u5b9a\u91cf\u6d4b\u91cf\u7b97\u5b50\u7ec4\u5408\u591a\u6837\u6027\u3001\u5ffd\u7565\u6a21\u578b\u6267\u884c\u65f6\u95f4\u3001\u5ffd\u89c6\u4e0d\u540c\u6d4b\u91cf\u6307\u6807\u95f4\u7684\u76f8\u5173\u6027\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e86DLMMM\u65b9\u6cd5\u4ee5\u514b\u670d\u8fd9\u4e9b\u4e0d\u8db3\u3002", "method": "DLMMM\u9996\u5148\u5b9a\u91cf\u6d4b\u91cf\u6a21\u578b\u7684\u9519\u8bef\u68c0\u6d4b\u6027\u80fd\u3001\u7b97\u5b50\u7ec4\u5408\u591a\u6837\u6027\u548c\u6a21\u578b\u6267\u884c\u65f6\u95f4\uff0c\u7136\u540e\u57fa\u4e8e\u8fd9\u4e9b\u6d4b\u91cf\u6307\u6807\u7684\u76f8\u5173\u6027\u8fdb\u884c\u878d\u5408\u4ee5\u5b9e\u73b0\u6743\u8861\uff0c\u5e76\u8bbe\u8ba1\u591a\u7ea7\u542f\u53d1\u5f0f\u6307\u5bfc\u7528\u4e8e\u6d4b\u8bd5\u8f93\u5165\u6a21\u578b\u7684\u751f\u6210\u3002", "result": "DLMMM\u901a\u8fc7\u878d\u5408\u591a\u79cd\u6d4b\u91cf\u6307\u6807\u548c\u591a\u7ea7\u542f\u53d1\u5f0f\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6846\u67b6\u6d4b\u8bd5\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "conclusion": "DLMMM\u901a\u8fc7\u878d\u5408\u591a\u79cd\u6a21\u578b\u6d4b\u91cf\u6307\u6807\u5e76\u8bbe\u8ba1\u591a\u7ea7\u542f\u53d1\u5f0f\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6d4b\u8bd5\u7684\u6548\u679c\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e09\u5927\u5c40\u9650\u6027\u3002"}}
{"id": "2507.15254", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15254", "abs": "https://arxiv.org/abs/2507.15254", "authors": ["Sourav Mondal", "Elaine Wong"], "title": "User Head Movement-Predictive XR in Immersive H2M Collaborations over Future Enterprise Networks", "comment": "This article is accepted for publication in IEEE Internet of Things\n  Journal. Copyright @ IEEE 2025", "summary": "The evolution towards future generation of mobile systems and fixed wireless\nnetworks is primarily driven by the urgency to support high-bandwidth and\nlow-latency services across various vertical sectors. This endeavor is fueled\nby smartphones as well as technologies like industrial internet of things,\nextended reality (XR), and human-to-machine (H2M) collaborations for fostering\nindustrial and social revolutions like Industry 4.0/5.0 and Society 5.0. To\nensure an ideal immersive experience and avoid cyber-sickness for users in all\nthe aforementioned usage scenarios, it is typically challenging to synchronize\nXR content from a remote machine to a human collaborator according to their\nhead movements across a large geographic span in real-time over communication\nnetworks. Thus, we propose a novel H2M collaboration scheme where the human's\nhead movements are predicted ahead with highly accurate models like\nbidirectional long short-term memory networks to orient the machine's camera in\nadvance. We validate that XR frame size varies in accordance with the human's\nhead movements and predict the corresponding bandwidth requirements from the\nmachine's camera to propose a human-machine coordinated dynamic bandwidth\nallocation (HMC-DBA) scheme. Through extensive simulations, we show that\nend-to-end latency and jitter requirements of XR frames are satisfied with much\nlower bandwidth consumption over enterprise networks like\nFiber-To-The-Room-Business. Furthermore, we show that better efficiency in\nnetwork resource utilization is achieved by employing our proposed HMC-DBA over\nstate-of-the-art schemes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5934\u90e8\u8fd0\u52a8\u9884\u6d4b\u7684\u52a8\u6001\u5e26\u5bbd\u5206\u914d\u65b9\u6848\uff08HMC-DBA\uff09\uff0c\u4ee5\u4f18\u5316XR\u5185\u5bb9\u5728\u8fdc\u7a0b\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5b9e\u65f6\u540c\u6b65\uff0c\u663e\u8457\u964d\u4f4e\u5e26\u5bbd\u6d88\u8017\u5e76\u63d0\u5347\u7f51\u7edc\u6548\u7387\u3002", "motivation": "\u672a\u6765\u79fb\u52a8\u7cfb\u7edf\u548c\u56fa\u5b9a\u65e0\u7ebf\u7f51\u7edc\u7684\u53d1\u5c55\u9700\u8981\u652f\u6301\u9ad8\u5e26\u5bbd\u548c\u4f4e\u5ef6\u8fdf\u670d\u52a1\uff0c\u5c24\u5176\u662f\u5728\u5de5\u4e1a\u4e92\u8054\u7f51\u3001\u6269\u5c55\u73b0\u5b9e\uff08XR\uff09\u548c\u4eba\u673a\u534f\u4f5c\uff08H2M\uff09\u7b49\u573a\u666f\u4e2d\uff0c\u5982\u4f55\u5b9e\u65f6\u540c\u6b65XR\u5185\u5bb9\u4ee5\u907f\u514d\u7f51\u7edc\u5ef6\u8fdf\u5e26\u6765\u7684\u4e0d\u9002\u611f\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4eba\u673a\u534f\u4f5c\u65b9\u6848\uff0c\u5229\u7528\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u7b49\u9ad8\u5ea6\u51c6\u786e\u7684\u6a21\u578b\u9884\u6d4b\u4eba\u7c7b\u5934\u90e8\u8fd0\u52a8\uff0c\u5e76\u636e\u6b64\u52a8\u6001\u5206\u914d\u5e26\u5bbd\uff08HMC-DBA\uff09\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u4eff\u771f\u9a8c\u8bc1\uff0cHMC-DBA\u65b9\u6848\u5728\u6ee1\u8db3XR\u5e27\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u548c\u6296\u52a8\u8981\u6c42\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5e26\u5bbd\u6d88\u8017\uff0c\u5e76\u5728\u7f51\u7edc\u8d44\u6e90\u5229\u7528\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684HMC-DBA\u65b9\u6848\u5728\u6ee1\u8db3XR\u5e27\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u548c\u6296\u52a8\u8981\u6c42\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5e26\u5bbd\u6d88\u8017\uff0c\u5e76\u5728\u7f51\u7edc\u8d44\u6e90\u5229\u7528\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002"}}
{"id": "2507.14914", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14914", "abs": "https://arxiv.org/abs/2507.14914", "authors": ["Zhexuan Xu", "Jie Wang", "Siyuan Xu", "Zijie Geng", "Mingxuan Yuan", "Feng Wu"], "title": "One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner", "comment": null, "summary": "Floorplanning determines the shapes and locations of modules on a chip canvas\nand plays a critical role in optimizing the chip's Power, Performance, and Area\n(PPA) metrics. However, existing floorplanning approaches often fail to\nintegrate with subsequent physical design stages, leading to suboptimal\nin-module component placement and excessive inter-module feedthrough. To tackle\nthis challenge, we propose Flora, a three-stage feedthrough and placement aware\nrectilinear floorplanner. In the first stage, Flora employs wiremask and\nposition mask techniques to achieve coarse-grained optimization of HPWL and\nfeedthrough. In the second stage, under the constraint of a fixed outline,\nFlora achieves a zero-whitespace layout by locally resizing module shapes,\nthereby performing fine-grained optimization of feedthrough and improving\ncomponent placement. In the third stage, Flora utilizes a fast tree\nsearch-based method to efficiently place components-including macros and\nstandard cells-within each module, subsequently adjusting module boundaries\nbased on the placement results to enable cross-stage optimization. Experimental\nresults show that Flora outperforms recent state-of-the-art floorplanning\napproaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,\n29.15% in FTmod, and a 14% improvement in component placement performance.", "AI": {"tldr": "Flora \u662f\u4e00\u79cd\u4e09\u9636\u6bb5\u7684 rectilinear floorplanner\uff0c\u901a\u8fc7 feedthrough \u548c placement aware \u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u82af\u7247 PPA \u6307\u6807\u3002", "motivation": "\u73b0\u6709 floorplanning \u65b9\u6cd5\u96be\u4ee5\u4e0e\u540e\u7eed\u7269\u7406\u8bbe\u8ba1\u9636\u6bb5\u96c6\u6210\uff0c\u5bfc\u81f4\u6a21\u5757\u5185\u7ec4\u4ef6\u5e03\u5c40\u548c\u6a21\u5757\u95f4 feedthrough \u4e0d\u7406\u60f3\u3002", "method": "Flora \u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4f7f\u7528 wiremask \u548c position mask \u8fdb\u884c\u7c97\u7c92\u5ea6\u4f18\u5316\uff1b2) \u5728\u56fa\u5b9a\u8f6e\u5ed3\u7ea6\u675f\u4e0b\u901a\u8fc7\u5c40\u90e8\u8c03\u6574\u6a21\u5757\u5f62\u72b6\u5b9e\u73b0\u96f6\u7a7a\u767d\u5e03\u5c40\uff1b3) \u4f7f\u7528\u5feb\u901f\u6811\u641c\u7d22\u65b9\u6cd5\u653e\u7f6e\u7ec4\u4ef6\u5e76\u8c03\u6574\u6a21\u5757\u8fb9\u754c\u3002", "result": "Flora \u5e73\u5747\u51cf\u5c11\u4e86 6% \u7684 HPWL\u30015.16% \u7684 FTpin \u548c 29.15% \u7684 FTmod\uff0c\u7ec4\u4ef6\u5e03\u5c40\u6027\u80fd\u63d0\u5347\u4e86 14%\u3002", "conclusion": "Flora \u662f\u4e00\u79cd\u4e09\u9636\u6bb5\u7684 feedthrough \u548c placement aware \u7684 rectilinear floorplanner\uff0c\u663e\u8457\u4f18\u5316\u4e86\u82af\u7247\u7684 PPA \u6307\u6807\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2507.14906", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14906", "abs": "https://arxiv.org/abs/2507.14906", "authors": ["Xiao Yang", "Juxi Leitner", "Michael Burke"], "title": "Feedback-Induced Performance Decline in LLM-Based Decision-Making", "comment": null, "summary": "The ability of Large Language Models (LLMs) to extract context from natural\nlanguage problem descriptions naturally raises questions about their\nsuitability in autonomous decision-making settings. This paper studies the\nbehaviour of these models within a Markov Decision Process (MDPs). While\ntraditional reinforcement learning (RL) strategies commonly employed in this\nsetting rely on iterative exploration, LLMs, pre-trained on diverse datasets,\noffer the capability to leverage prior knowledge for faster adaptation. We\ninvestigate online structured prompting strategies in sequential decision\nmaking tasks, comparing the zero-shot performance of LLM-based approaches to\nthat of classical RL methods. Our findings reveal that although LLMs\ndemonstrate improved initial performance in simpler environments, they struggle\nwith planning and reasoning in complex scenarios without fine-tuning or\nadditional guidance. Our results show that feedback mechanisms, intended to\nimprove decision-making, often introduce confusion, leading to diminished\nperformance in intricate environments. These insights underscore the need for\nfurther exploration into hybrid strategies, fine-tuning, and advanced memory\nintegration to enhance LLM-based decision-making capabilities.", "AI": {"tldr": "LLMs\u5728\u7b80\u5355\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfRL\u65b9\u6cd5\uff0c\u4f46\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6df7\u5408\u7b56\u7565\u548c\u5fae\u8c03\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u81ea\u4e3b\u51b3\u7b56\u8bbe\u7f6e\u4e2d\u7684\u9002\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u5229\u7528\u5176\u4ece\u591a\u6837\u6570\u636e\u96c6\u4e2d\u83b7\u53d6\u7684\u5148\u9a8c\u77e5\u8bc6\u8fdb\u884c\u5feb\u901f\u9002\u5e94\u65b9\u9762\u3002", "method": "\u7814\u7a76\u4e86\u5728\u7ebf\u7ed3\u6784\u5316\u63d0\u793a\u7b56\u7565\u5728\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u6bd4\u8f83\u4e86\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u4e0e\u7ecf\u5178RL\u65b9\u6cd5\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u7b80\u5355\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e2d\u7f3a\u4e4f\u89c4\u5212\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u53cd\u9988\u673a\u5236\u53ef\u80fd\u5f15\u5165\u6df7\u6dc6\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u8bba\u6587\u6307\u51fa\uff0c\u867d\u7136LLMs\u5728\u7b80\u5355\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u521d\u59cb\u6027\u80fd\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e2d\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u6df7\u5408\u7b56\u7565\u3001\u5fae\u8c03\u548c\u9ad8\u7ea7\u5185\u5b58\u96c6\u6210\uff0c\u4ee5\u63d0\u5347\u5176\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2507.15188", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15188", "abs": "https://arxiv.org/abs/2507.15188", "authors": ["Chowdhury Shahriar Muzammel", "Maria Spichkova", "James Harland"], "title": "Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View", "comment": null, "summary": "Requirements Engineering (RE) is one of the most interaction-intensive phases\nof software development. This means that RE activities might be especially\nimpacted by stakeholders' national culture. Software development projects\nincreasingly have a very diverse range of stakeholders. To future-proof RE\nactivities, we need to help RE practitioners avoid misunderstandings and\nconflicts that might arise from not understanding potential Cultural Influences\n(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT\nprofession. Bangladesh has a growing IT sector with some unique socio-cultural\ncharacteristics, and has been largely overlooked in this research field. In\nthis study, we aim to investigate how the RE process is adopted in the context\nof Bangladeshi culture and what cultural influences impact overall RE\nactivities.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5b5f\u52a0\u62c9\u6587\u5316\u5982\u4f55\u5f71\u54cd\u9700\u6c42\u5de5\u7a0b\uff08RE\uff09\u8fc7\u7a0b\uff0c\u5f3a\u8c03\u4e86\u6587\u5316\u610f\u8bc6\u5728\u907f\u514d\u51b2\u7a81\u548c\u652f\u6301\u591a\u6837\u6027\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u5f00\u53d1\u9879\u76ee\u4e2d\u5229\u76ca\u76f8\u5173\u8005\u591a\u6837\u6027\u7684\u589e\u52a0\uff0c\u7406\u89e3\u6587\u5316\u5f71\u54cd\uff08CIs\uff09\u5bf9\u907f\u514dRE\u6d3b\u52a8\u4e2d\u7684\u8bef\u89e3\u548c\u51b2\u7a81\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5b5f\u52a0\u62c9\u8fd9\u6837\u5177\u6709\u72ec\u7279\u793e\u4f1a\u6587\u5316\u7279\u5f81\u7684\u56fd\u5bb6\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u8c03\u67e5\u5b5f\u52a0\u62c9IT\u884c\u4e1a\u4e2d\u7684RE\u5b9e\u8df5\uff0c\u5206\u6790\u4e86\u6587\u5316\u56e0\u7d20\u5982\u4f55\u5f71\u54cdRE\u6d3b\u52a8\u7684\u91c7\u7eb3\u548c\u6267\u884c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b5f\u52a0\u62c9\u6587\u5316\u5bf9RE\u8fc7\u7a0b\u6709\u663e\u8457\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u5177\u4f53\u6587\u5316\u56e0\u7d20\u5982\u4f55\u5851\u9020RE\u5b9e\u8df5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5b5f\u52a0\u62c9\u6587\u5316\u5bf9\u9700\u6c42\u5de5\u7a0b\uff08RE\uff09\u8fc7\u7a0b\u7684\u5177\u4f53\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u6587\u5316\u610f\u8bc6\u5728\u907f\u514d\u8bef\u89e3\u548c\u51b2\u7a81\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u652f\u6301IT\u884c\u4e1a\u7684\u591a\u6837\u6027\u548c\u5305\u5bb9\u6027\u3002"}}
{"id": "2507.15338", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.15338", "abs": "https://arxiv.org/abs/2507.15338", "authors": ["Takaho Shimokasa", "Hiroyuki Yomo", "Federico Chiariotti", "Junya Shiraishi", "Petar Popovski"], "title": "Low-Power and Accurate IoT Monitoring Under Radio Resource Constraint", "comment": "Paper accepted for IEEE PIMRC 2025", "summary": "This paper investigates how to achieve both low-power operations of sensor\nnodes and accurate state estimation using Kalman filter for internet of things\n(IoT) monitoring employing wireless sensor networks under radio resource\nconstraint. We consider two policies used by the base station to collect\nobservations from the sensor nodes: (i) an oblivious policy, based on\nstatistics of the observations, and (ii) a decentralized policy, based on\nautonomous decision of each sensor based on its instantaneous observation. This\nwork introduces a wake-up receiver and wake-up signaling to both policies to\nimprove the energy efficiency of the sensor nodes. The decentralized policy\ndesigned with random access prioritizes transmissions of instantaneous\nobservations that are highly likely to contribute to the improvement of state\nestimation. Our numerical results show that the decentralized policy improves\nthe accuracy of the estimation in comparison to the oblivious policy under the\nconstraint on the radio resource and consumed energy when the correlation\nbetween the processes observed by the sensor nodes is low. We also clarify the\ndegree of correlation in which the superiority of two policies changes.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u4e2d\uff0c\u901a\u8fc7\u5524\u9192\u673a\u5236\u548c\u5206\u6563\u5f0f\u7b56\u7565\u4f18\u5316\u4f20\u611f\u5668\u8282\u70b9\u7684\u80fd\u6548\u548c\u72b6\u6001\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u5206\u6563\u5f0f\u7b56\u7565\u5728\u4f4e\u76f8\u5173\u6027\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u65e0\u7ebf\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u76d1\u6d4b\u4e2d\u5b9e\u73b0\u4f20\u611f\u5668\u8282\u70b9\u7684\u4f4e\u529f\u8017\u8fd0\u884c\u548c\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u51c6\u786e\u72b6\u6001\u4f30\u8ba1\u3002", "method": "\u5f15\u5165\u4e86\u4e24\u79cd\u7b56\u7565\uff1a\u57fa\u4e8e\u7edf\u8ba1\u7684\u65e0\u611f\u77e5\u7b56\u7565\u548c\u57fa\u4e8e\u77ac\u65f6\u89c2\u6d4b\u7684\u5206\u6563\u5f0f\u7b56\u7565\uff0c\u5e76\u91c7\u7528\u5524\u9192\u63a5\u6536\u5668\u548c\u4fe1\u53f7\u63d0\u5347\u80fd\u6548\u3002\u5206\u6563\u5f0f\u7b56\u7565\u901a\u8fc7\u968f\u673a\u8bbf\u95ee\u4f18\u5148\u4f20\u8f93\u53ef\u80fd\u6539\u5584\u72b6\u6001\u4f30\u8ba1\u7684\u89c2\u6d4b\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4f20\u611f\u5668\u8282\u70b9\u95f4\u76f8\u5173\u6027\u8f83\u4f4e\u65f6\uff0c\u5206\u6563\u5f0f\u7b56\u7565\u5728\u65e0\u7ebf\u7535\u8d44\u6e90\u548c\u80fd\u8017\u7ea6\u675f\u4e0b\u6bd4\u65e0\u611f\u77e5\u7b56\u7565\u66f4\u51c6\u786e\u3002", "conclusion": "\u5728\u65e0\u7ebf\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5206\u6563\u5f0f\u7b56\u7565\u5728\u4f20\u611f\u5668\u8282\u70b9\u95f4\u76f8\u5173\u6027\u8f83\u4f4e\u65f6\u80fd\u63d0\u9ad8\u72b6\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u5e76\u660e\u786e\u4e86\u4e24\u79cd\u7b56\u7565\u4f18\u52bf\u8f6c\u6362\u7684\u76f8\u5173\u6027\u7a0b\u5ea6\u3002"}}
{"id": "2507.14929", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14929", "abs": "https://arxiv.org/abs/2507.14929", "authors": ["Tero Kaarlela", "Sami Salo", "Jose Outeiro"], "title": "Digital twin and extended reality for teleoperation of the electric vehicle battery disassembly", "comment": null, "summary": "Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a\nsustainable transition to electric vehicles by enabling a closed-loop supply\nchain. Currently, the manual disassembly process exposes workers to hazards,\nincluding electrocution and toxic chemicals. We propose a teleoperated system\nfor the safe disassembly and sorting of EVBs. A human-in-the-loop can create\nand save disassembly sequences for unknown EVB types, enabling future\nautomation. An RGB camera aligns the physical and digital twins of the EVB, and\nthe digital twin of the robot is based on the Robot Operating System (ROS)\nmiddleware. This hybrid approach combines teleoperation and automation to\nimprove safety, adaptability, and efficiency in EVB disassembly and sorting.\nThe economic contribution is realized by reducing labor dependency and\nincreasing throughput in battery recycling. An online pilot study was set up to\nevaluate the usability of the presented approach, and the results demonstrate\nthe potential as a user-friendly solution.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u8fdc\u7a0b\u64cd\u4f5c\u4e0e\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u5b89\u5168\u9ad8\u6548\u62c6\u5378EVB\uff0c\u8bd5\u70b9\u9a8c\u8bc1\u7528\u6237\u53cb\u597d\u6027\u53ca\u7ecf\u6d4e\u6f5c\u529b\u3002", "motivation": "\u624b\u52a8\u62c6\u5378EVB\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff08\u5982\u89e6\u7535\u548c\u6709\u6bd2\u5316\u5b66\u7269\u8d28\uff09\uff0c\u4e14\u6548\u7387\u4f4e\u4e0b\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u63d0\u9ad8\u5b89\u5168\u6027\u3001\u9002\u5e94\u6027\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7ed3\u5408RGB\u6444\u50cf\u5934\u5bf9\u9f50\u7269\u7406\u548c\u6570\u5b57\u5b6a\u751f\uff0c\u57fa\u4e8eROS\u4e2d\u95f4\u4ef6\u6784\u5efa\u673a\u5668\u4eba\u6570\u5b57\u5b6a\u751f\uff0c\u5b9e\u73b0\u4eba\u673a\u534f\u540c\u7684\u62c6\u5378\u5e8f\u5217\u521b\u5efa\u4e0e\u4fdd\u5b58\u3002", "result": "\u5728\u7ebf\u8bd5\u70b9\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u7528\u6237\u53cb\u597d\u6027\uff0c\u5e76\u80fd\u51cf\u5c11\u5bf9\u52b3\u52a8\u529b\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u7535\u6c60\u56de\u6536\u7684\u541e\u5410\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fdc\u7a0b\u64cd\u4f5c\u548c\u81ea\u52a8\u5316\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b89\u5168\u3001\u9ad8\u6548\u5730\u62c6\u5378\u548c\u5206\u7c7b\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\uff08EVB\uff09\uff0c\u5e76\u901a\u8fc7\u5728\u7ebf\u8bd5\u70b9\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u7528\u6237\u53cb\u597d\u6027\u548c\u6f5c\u5728\u7ecf\u6d4e\u6548\u76ca\u3002"}}
{"id": "2507.14909", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14909", "abs": "https://arxiv.org/abs/2507.14909", "authors": ["Elio Grande"], "title": "The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities", "comment": null, "summary": "The Endless Tuning is a design method for a reliable deployment of artificial\nintelligence based on a double mirroring process, which pursues both the goals\nof avoiding human replacement and filling the so-called responsibility gap\n(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the\nrelational approach urged therein, it was then actualized in a protocol,\nimplemented in three prototypical applications regarding decision-making\nprocesses (respectively: loan granting, pneumonia diagnosis, and art style\nrecognition) and tested with such as many domain experts. Step by step\nillustrating the protocol, giving insights concretely showing a different voice\n(Gilligan 1993) in the ethics of artificial intelligence, a philosophical\naccount of technical choices (e.g., a reversed and hermeneutic deployment of\nXAI algorithms) will be provided in the present study together with the results\nof the experiments, focusing on user experience rather than statistical\naccuracy. Even thoroughly employing deep learning models, full control was\nperceived by the interviewees in the decision-making setting, while it appeared\nthat a bridge can be built between accountability and liability in case of\ndamage.", "AI": {"tldr": "Endless Tuning\u662f\u4e00\u79cd\u53cc\u955c\u50cf\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u5411XAI\u7b97\u6cd5\u90e8\u7f72\u586b\u8865\u8d23\u4efb\u7f3a\u53e3\uff0c\u5b9e\u9a8c\u8bc1\u660e\u7528\u6237\u611f\u77e5\u5b8c\u5168\u63a7\u5236\uff0c\u5e76\u5efa\u7acb\u4e86\u8d23\u4efb\u4e0e\u95ee\u8d23\u7684\u6865\u6881\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u4eba\u5de5\u667a\u80fd\u90e8\u7f72\u4e2d\u7684\u8d23\u4efb\u7f3a\u53e3\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4eba\u7c7b\u88ab\u66ff\u4ee3\u7684\u98ce\u9669\uff0c\u5021\u5bfc\u4e00\u79cd\u4e0d\u540c\u7684AI\u4f26\u7406\u89c6\u89d2\uff08Gilligan 1993\uff09\u3002", "method": "Endless Tuning\u662f\u4e00\u79cd\u57fa\u4e8e\u53cc\u955c\u50cf\u8fc7\u7a0b\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u5411\u548c\u8be0\u91ca\u6027\u7684XAI\u7b97\u6cd5\u90e8\u7f72\uff0c\u5b9e\u73b0\u4e86\u4e09\u4e2a\u539f\u578b\u5e94\u7528\uff08\u8d37\u6b3e\u5ba1\u6279\u3001\u80ba\u708e\u8bca\u65ad\u548c\u827a\u672f\u98ce\u683c\u8bc6\u522b\uff09\u7684\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u7684\u5e94\u7528\u975e\u5e38\u6df1\u5165\uff0c\u4f46\u7528\u6237\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u611f\u77e5\u5230\u4e86\u5b8c\u5168\u63a7\u5236\uff0c\u4e14\u8d23\u4efb\u4e0e\u95ee\u8d23\u4e4b\u95f4\u53ef\u4ee5\u5efa\u7acb\u8054\u7cfb\u3002", "conclusion": "\u901a\u8fc7\u53cc\u955c\u50cf\u8fc7\u7a0b\u5b9e\u73b0\u7684Endless Tuning\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u907f\u514d\u4e86\u4eba\u7c7b\u88ab\u66ff\u4ee3\u7684\u98ce\u9669\uff0c\u8fd8\u586b\u8865\u4e86\u8d23\u4efb\u7f3a\u53e3\uff08Matthias 2004\uff09\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7528\u6237\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u611f\u77e5\u5230\u4e86\u5b8c\u5168\u63a7\u5236\uff0c\u540c\u65f6\u4e3a\u635f\u5bb3\u60c5\u51b5\u4e0b\u7684\u8d23\u4efb\u4e0e\u95ee\u8d23\u4e4b\u95f4\u642d\u5efa\u4e86\u6865\u6881\u3002"}}
{"id": "2507.15197", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15197", "abs": "https://arxiv.org/abs/2507.15197", "authors": ["Chowdhury Shahriar Muzammel", "Maria Spichkova", "James Harland"], "title": "Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?", "comment": null, "summary": "In requirements engineering (RE), personas are now being used to represent\nuser expectations and needs. This systematic mapping study (SMS) aims to\nexplore the most recent studies and to cover recent changes in trends,\nespecially related to the recent evolution of Generative AI approaches. Our SMS\ncovers the period between April 2023 and April 2025. We identified 22 relevant\npublications and analysed persona representation, construction, validation, as\nwell as RE activities covered by personas. We identified that a number of\nstudies applied AI-based solutions for persona construction and validation. We\nobserved that template-based personas are becoming more popular nowadays. We\nalso observed an increase in the proportion of studies covering validation\naspects.", "AI": {"tldr": "\u7cfb\u7edf\u6620\u5c04\u7814\u7a76\u663e\u793a\uff0cAI\u5728\u4eba\u7269\u89d2\u8272\u6784\u5efa\u548c\u9a8c\u8bc1\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u6a21\u677f\u5316\u89d2\u8272\u66f4\u53d7\u6b22\u8fce\uff0c\u9a8c\u8bc1\u7814\u7a76\u6bd4\u4f8b\u4e0a\u5347\u3002", "motivation": "\u63a2\u7d22\u9700\u6c42\u5de5\u7a0b\u4e2d\u4eba\u7269\u89d2\u8272\u7684\u6700\u65b0\u7814\u7a76\u8d8b\u52bf\uff0c\u7279\u522b\u662f\u751f\u6210\u5f0fAI\u65b9\u6cd5\u5bf9\u4eba\u7269\u89d2\u8272\u5e94\u7528\u7684\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7cfb\u7edf\u6620\u5c04\u7814\u7a76\uff08SMS\uff09\u65b9\u6cd5\uff0c\u8986\u76d62023\u5e744\u6708\u81f32025\u5e744\u6708\u671f\u95f4\u768422\u7bc7\u76f8\u5173\u6587\u732e\uff0c\u5206\u6790\u4e86\u4eba\u7269\u89d2\u8272\u7684\u8868\u793a\u3001\u6784\u5efa\u3001\u9a8c\u8bc1\u53ca\u5176\u5728RE\u6d3b\u52a8\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u89e3\u51b3\u65b9\u6848\u5728\u4eba\u7269\u89d2\u8272\u6784\u5efa\u548c\u9a8c\u8bc1\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u6a21\u677f\u5316\u4eba\u7269\u89d2\u8272\u66f4\u53d7\u6b22\u8fce\uff0c\u9a8c\u8bc1\u65b9\u9762\u7684\u7814\u7a76\u6bd4\u4f8b\u4e0a\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u6620\u5c04\u7814\u7a76\uff08SMS\uff09\u63a2\u8ba8\u4e86\u9700\u6c42\u5de5\u7a0b\uff08RE\uff09\u4e2d\u4eba\u7269\u89d2\u8272\u7684\u6700\u65b0\u5e94\u7528\u8d8b\u52bf\uff0c\u7279\u522b\u662f\u5728\u751f\u6210\u5f0fAI\u65b9\u6cd5\u7684\u5f71\u54cd\u4e0b\u3002\u7814\u7a76\u53d1\u73b0\uff0cAI\u89e3\u51b3\u65b9\u6848\u5728\u4eba\u7269\u89d2\u8272\u6784\u5efa\u548c\u9a8c\u8bc1\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u6a21\u677f\u5316\u4eba\u7269\u89d2\u8272\u66f4\u53d7\u6b22\u8fce\uff0c\u9a8c\u8bc1\u65b9\u9762\u7684\u7814\u7a76\u6bd4\u4f8b\u4e5f\u6709\u6240\u4e0a\u5347\u3002"}}
{"id": "2507.15382", "categories": ["cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.15382", "abs": "https://arxiv.org/abs/2507.15382", "authors": ["Fabian Ihle", "Etienne Zink", "Michael Menth"], "title": "Enhancements to P4TG: Histogram-Based RTT Monitoring in the Data Plane", "comment": "This work has been published at the 1st Workshop on Resilient\n  Networks and Systems (ReNeSys), Sept. 2025, Ilmenau/Germany under the\n  Creative Commons Attribution 4.0 International License (CC BY 4.0)", "summary": "Modern traffic generators are essential tools for evaluating the performance\nof network environments. P4TG is a P4-based traffic generator implemented for\nIntel Tofino switches that offers high-speed packet generation with\nfine-grained measurement capabilities. However, P4TG samples time-based metrics\nsuch as the round-trip time (RTT) in the data plane and collects them at the\ncontroller. This leads to a reduced accuracy. In this paper, we introduce a\nhistogram-based RTT measurement feature for P4TG. It enables accurate analysis\nat line rate without sampling. Generally, histogram bins are modeled as ranges,\nand values are matched to a bin. Efficient packet matching in hardware is\ntypically achieved using ternary content addressable memory (TCAM). However,\nrepresenting range matching rules in TCAM poses a challenge. Therefore, we\nimplemented a range-to-prefix conversion algorithm that models range matching\nwith multiple ternary entries. This paper describes the data plane\nimplementation and runtime configuration of RTT histograms in P4TG. Further, we\ndiscuss the efficiency of the ternary decomposition. Our evaluation\ndemonstrates the applicability of the histogram-based RTT analysis by comparing\nthe measured values with a configured theoretical distribution of RTTs.", "AI": {"tldr": "P4TG\u63d0\u51fa\u76f4\u65b9\u56feRTT\u6d4b\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u8303\u56f4\u5230\u524d\u7f00\u8f6c\u6362\u4f18\u5316TCAM\u5339\u914d\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u6790\u3002", "motivation": "P4TG\u5728\u6570\u636e\u5e73\u9762\u91c7\u6837\u65f6\u95f4\u6307\u6807\uff08\u5982RTT\uff09\u5e76\u5728\u63a7\u5236\u5668\u6536\u96c6\uff0c\u5bfc\u81f4\u7cbe\u5ea6\u964d\u4f4e\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u91c7\u6837\u7684\u9ad8\u7cbe\u5ea6\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u76f4\u65b9\u56fe\u65b9\u6cd5\u8fdb\u884cRTT\u6d4b\u91cf\uff0c\u4f7f\u7528\u8303\u56f4\u5230\u524d\u7f00\u7684\u8f6c\u6362\u7b97\u6cd5\u6765\u4f18\u5316TCAM\u4e2d\u7684\u8303\u56f4\u5339\u914d\u89c4\u5219\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684RTT\u5206\u6790\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u6d4b\u91cfRTT\uff0c\u5e76\u4e0e\u7406\u8bba\u5206\u5e03\u4e00\u81f4\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684RTT\u6d4b\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u8303\u56f4\u5230\u524d\u7f00\u7684\u8f6c\u6362\u7b97\u6cd5\u5728P4TG\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684RTT\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.14931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14931", "abs": "https://arxiv.org/abs/2507.14931", "authors": ["Qiaoqiao Ren", "Remko Proesmans", "Arend Pissens", "Lara Dehandschutter", "William Denecker", "Lotte Rouckhout", "Joke Carrette", "Peter Vanhopplinus", "Tony Belpaeme", "Francis wyffels"], "title": "Designing Robots with, not for: A Co-Design Framework for Empowering Interactions in Forensic Psychiatry", "comment": null, "summary": "Forensic mental health care involves the treatment of individuals with severe\nmental disorders who have committed violent offences. These settings are often\ncharacterized by high levels of bureaucracy, risk avoidance, and restricted\nautonomy. Patients frequently experience a profound loss of control over their\nlives, leading to heightened psychological stress-sometimes resulting in\nisolation as a safety measure. In this study, we explore how co-design can be\nused to collaboratively develop a companion robot that helps monitor and\nregulate stress while maintaining tracking of the patients' interaction\nbehaviours for long-term intervention. We conducted four co-design workshops in\na forensic psychiatric clinic with patients, caregivers, and therapists. Our\nprocess began with the presentation of an initial speculative prototype to\ntherapists, enabling reflection on shared concerns, ethical risks, and\ndesirable features. This was followed by a creative ideation session with\npatients, a third workshop focused on defining desired functions and emotional\nresponses, and we are planning a final prototype demo to gather direct patient\nfeedback. Our findings emphasize the importance of empowering patients in the\ndesign process and adapting proposals based on their current emotional state.\nThe goal was to empower the patient in the design process and ensure each\npatient's voice was heard.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u56db\u6b21\u5171\u540c\u8bbe\u8ba1\u7814\u8ba8\u4f1a\u5f00\u53d1\u4f34\u4fa3\u673a\u5668\u4eba\uff0c\u5f3a\u8c03\u5728\u6cd5\u533b\u5fc3\u7406\u5065\u5eb7\u62a4\u7406\u4e2d\u8d4b\u4e88\u60a3\u8005\u8bbe\u8ba1\u6743\u529b\uff0c\u786e\u4fdd\u5176\u58f0\u97f3\u88ab\u542c\u5230\uff0c\u5e76\u6839\u636e\u60c5\u7eea\u72b6\u6001\u8c03\u6574\u65b9\u6848\u3002", "motivation": "\u6cd5\u533b\u5fc3\u7406\u5065\u5eb7\u62a4\u7406\u73af\u5883\u901a\u5e38\u5b98\u50da\u4e3b\u4e49\u4e25\u91cd\u3001\u98ce\u9669\u89c4\u907f\u4e14\u81ea\u4e3b\u6743\u53d7\u9650\uff0c\u60a3\u8005\u5e38\u611f\u5230\u751f\u6d3b\u5931\u63a7\uff0c\u5bfc\u81f4\u5fc3\u7406\u538b\u529b\u52a0\u5267\u3002\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5171\u540c\u8bbe\u8ba1\u5f00\u53d1\u4f34\u4fa3\u673a\u5668\u4eba\uff0c\u4ee5\u76d1\u63a7\u548c\u8c03\u8282\u538b\u529b\uff0c\u540c\u65f6\u8ddf\u8e2a\u60a3\u8005\u4e92\u52a8\u884c\u4e3a\u4ee5\u5b9e\u73b0\u957f\u671f\u5e72\u9884\u3002", "method": "\u5728\u6cd5\u533b\u7cbe\u795e\u75c5\u8bca\u6240\u8fdb\u884c\u4e86\u56db\u6b21\u5171\u540c\u8bbe\u8ba1\u7814\u8ba8\u4f1a\uff0c\u53c2\u4e0e\u8005\u5305\u62ec\u60a3\u8005\u3001\u62a4\u7406\u4eba\u5458\u548c\u6cbb\u7597\u5e08\u3002\u8fc7\u7a0b\u4ece\u5411\u6cbb\u7597\u5e08\u5c55\u793a\u521d\u6b65\u63a8\u6d4b\u539f\u578b\u5f00\u59cb\uff0c\u968f\u540e\u4e0e\u60a3\u8005\u8fdb\u884c\u521b\u610f\u6784\u601d\u4f1a\u8bae\uff0c\u7b2c\u4e09\u6b21\u7814\u8ba8\u4f1a\u805a\u7126\u4e8e\u5b9a\u4e49\u671f\u671b\u7684\u529f\u80fd\u548c\u60c5\u611f\u53cd\u5e94\uff0c\u5e76\u8ba1\u5212\u8fdb\u884c\u6700\u7ec8\u539f\u578b\u6f14\u793a\u4ee5\u6536\u96c6\u60a3\u8005\u76f4\u63a5\u53cd\u9988\u3002", "result": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5171\u540c\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u8d4b\u4e88\u60a3\u8005\u6743\u529b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6839\u636e\u60a3\u8005\u5f53\u524d\u60c5\u7eea\u72b6\u6001\u8c03\u6574\u8bbe\u8ba1\u63d0\u6848\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8bbe\u8ba1\u4e2d\u8d4b\u4e88\u60a3\u8005\u6743\u529b\u5e76\u57fa\u4e8e\u4ed6\u4eec\u7684\u60c5\u7eea\u72b6\u6001\u8c03\u6574\u65b9\u6848\u81f3\u5173\u91cd\u8981\uff0c\u76ee\u6807\u662f\u786e\u4fdd\u6bcf\u4f4d\u60a3\u8005\u7684\u58f0\u97f3\u88ab\u542c\u5230\u3002"}}
{"id": "2507.14912", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14912", "abs": "https://arxiv.org/abs/2507.14912", "authors": ["Ruhul Amin Khalil", "Kashif Ahmad", "Hazrat Ali"], "title": "Redefining Elderly Care with Agentic AI: Challenges and Opportunities", "comment": null, "summary": "The global ageing population necessitates new and emerging strategies for\ncaring for older adults. In this article, we explore the potential for\ntransformation in elderly care through Agentic Artificial Intelligence (AI),\npowered by Large Language Models (LLMs). We discuss the proactive and\nautonomous decision-making facilitated by Agentic AI in elderly care.\nPersonalized tracking of health, cognitive care, and environmental management,\nall aimed at enhancing independence and high-level living for older adults,\nrepresents important areas of application. With a potential for significant\ntransformation of elderly care, Agentic AI also raises profound concerns about\ndata privacy and security, decision independence, and access. We share key\ninsights to emphasize the need for ethical safeguards, privacy protections, and\ntransparent decision-making. Our goal in this article is to provide a balanced\ndiscussion of both the potential and the challenges associated with Agentic AI,\nand to provide insights into its responsible use in elderly care, to bring\nAgentic AI into harmony with the requirements and vulnerabilities specific to\nthe elderly. Finally, we identify the priorities for the academic research\ncommunities, to achieve human-centered advancements and integration of Agentic\nAI in elderly care. To the best of our knowledge, this is no existing study\nthat reviews the role of Agentic AI in elderly care. Hence, we address the\nliterature gap by analyzing the unique capabilities, applications, and\nlimitations of LLM-based Agentic AI in elderly care. We also provide a\ncompanion interactive dashboard at https://hazratali.github.io/agenticai/.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8eLLMs\u7684Agentic AI\u5728\u8001\u5e74\u62a4\u7406\u4e2d\u7684\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u5f3a\u8c03\u9700\u5e73\u8861\u6280\u672f\u521b\u65b0\u4e0e\u4f26\u7406\u4fdd\u969c\uff0c\u586b\u8865\u4e86\u76f8\u5173\u6587\u732e\u7a7a\u767d\u3002", "motivation": "\u5168\u7403\u8001\u9f84\u5316\u4eba\u53e3\u9700\u8981\u65b0\u7684\u62a4\u7406\u7b56\u7565\uff0cAgentic AI\u5728\u63d0\u5347\u8001\u5e74\u4eba\u72ec\u7acb\u6027\u548c\u751f\u6d3b\u8d28\u91cf\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u6570\u636e\u9690\u79c1\u548c\u51b3\u7b56\u72ec\u7acb\u6027\u7b49\u62c5\u5fe7\u3002", "method": "\u901a\u8fc7\u5206\u6790\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684Agentic AI\u5728\u8001\u5e74\u62a4\u7406\u4e2d\u7684\u72ec\u7279\u80fd\u529b\u3001\u5e94\u7528\u548c\u5c40\u9650\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u7684\u7a7a\u767d\u3002", "result": "Agentic AI\u5728\u5065\u5eb7\u76d1\u6d4b\u3001\u8ba4\u77e5\u62a4\u7406\u548c\u73af\u5883\u7ba1\u7406\u7b49\u65b9\u9762\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u4f26\u7406\u548c\u9690\u79c1\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5728\u8001\u5e74\u62a4\u7406\u4e2d\u5e94\u7528Agentic AI\u7684\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4f26\u7406\u4fdd\u969c\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u900f\u660e\u51b3\u7b56\u7684\u91cd\u8981\u6027\u3002\u540c\u65f6\uff0c\u6307\u51fa\u4e86\u5b66\u672f\u7814\u7a76\u793e\u533a\u5728\u5b9e\u73b0\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684Agentic AI\u96c6\u6210\u4e2d\u7684\u4f18\u5148\u4e8b\u9879\u3002"}}
{"id": "2507.15224", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15224", "abs": "https://arxiv.org/abs/2507.15224", "authors": ["Yibo He", "Shuoran Zhao", "Jiaming Huang", "Yingjie Fu", "Hao Yu", "Cunjian Huang", "Tao Xie"], "title": "SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation", "comment": null, "summary": "SIMD (Single Instruction Multiple Data) instructions and their compiler\nintrinsics are widely supported by modern processors to accelerate\nperformance-critical tasks. SIMD intrinsic programming, a trade-off between\ncoding productivity and high performance, is widely used in the development of\nmainstream performance-critical libraries and daily computing tasks. Large\nLanguage Models (LLMs), which have demonstrated strong and comprehensive\ncapabilities in code generation, show promise in assisting programmers with the\nchallenges of SIMD intrinsic programming. However, existing code-generation\nbenchmarks focus on only scalar code, and it is unclear how LLMs perform in\ngenerating vectorized code using SIMD intrinsics. To fill this gap, we propose\nSimdBench, the first code benchmark specifically designed for SIMD-intrinsic\ncode generation, comprising 136 carefully crafted tasks and targeting five\nrepresentative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86\nAdvanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM\nScalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a\nsystematic evaluation (measuring both correctness and performance) of 18\nrepresentative LLMs on SimdBench, resulting in a series of novel and insightful\nfindings. Our evaluation results demonstrate that LLMs exhibit a universal\ndecrease in pass@k during SIMD-intrinsic code generation compared to\nscalar-code generation. Our in-depth analysis highlights promising directions\nfor the further advancement of LLMs in the challenging domain of SIMD-intrinsic\ncode generation. SimdBench is fully open source at\nhttps://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader\nresearch community.", "AI": {"tldr": "SimdBench\u662f\u9996\u4e2a\u9488\u5bf9SIMD-intrinsic\u4ee3\u7801\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e8618\u4e2aLLM\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u8be5\u9886\u57df\u7684\u751f\u6210\u80fd\u529b\u8f83\u5f31\uff0c\u4f46\u7814\u7a76\u4e3a\u4f18\u5316\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u586b\u8865\u73b0\u6709\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u5728SIMD-intrinsic\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u4e0a\u7684\u7a7a\u767d\uff0c\u63a2\u7a76LLM\u5728\u8be5\u9886\u57df\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86SimdBench\uff0c\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9SIMD-intrinsic\u4ee3\u7801\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b136\u4e2a\u4efb\u52a1\uff0c\u8986\u76d6\u4e94\u79cd\u4ee3\u8868\u6027SIMD\u6307\u4ee4\u96c6\uff0c\u5e76\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e8618\u4e2a\u4ee3\u8868\u6027LLM\u3002", "result": "LLMs\u5728SIMD-intrinsic\u4ee3\u7801\u751f\u6210\u4e2d\u7684pass@k\u666e\u904d\u4f4e\u4e8e\u6807\u91cf\u4ee3\u7801\u751f\u6210\uff0c\u4f46\u7814\u7a76\u4e3aLLM\u5728\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u4f18\u5316\u6307\u660e\u4e86\u65b9\u5411\u3002", "conclusion": "LLMs\u5728SIMD-intrinsic\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u4e0d\u5982\u6807\u91cf\u4ee3\u7801\u751f\u6210\uff0c\u4f46\u7814\u7a76\u7ed3\u679c\u4e3a\u5176\u8fdb\u4e00\u6b65\u4f18\u5316\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.15391", "categories": ["cs.NI", "C.2.2; C.2.1; C.2.6"], "pdf": "https://arxiv.org/pdf/2507.15391", "abs": "https://arxiv.org/abs/2507.15391", "authors": ["Fabian Ihle", "Michael Menth"], "title": "Stack Management for MPLS Network Actions: Integration of Nodes with Limited Hardware Capabilities", "comment": null, "summary": "The MPLS Network Actions (MNA) framework enhances MPLS forwarding with a\ngeneralized encoding for manifold extensions such as network slicing and\nin-situ OAM (IOAM). Network actions in MNA are encoded in Label Stack Entries\n(LSEs) and are added to the MPLS stack. Routers have a physical limit on the\nnumber of LSEs they can read, called the readable label depth (RLD). With MNA,\nrouters must be able to process a minimum number of LSEs which requires a\nrelatively large RLD. In this paper, we perform a hardware analysis of an MNA\nimplementation and identify the reason for a large RLD requirement in the MNA\nprotocol design. Based on this, we present a mechanism that reduces the\nrequired RLD for MNA nodes by restructuring the MPLS stack during forwarding.\nWe then introduce the novel stack management network action that enables the\nproposed mechanism as well as its integration in networks with MNA-incapable\nnodes. The feasibility of the mechanism on programmable hardware is verified by\nproviding a P4-based implementation. Further, the effects on the required RLD,\nECMP, and packet overhead are discussed.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86MNA\u5b9e\u73b0\u4e2dRLD\u9700\u6c42\u5927\u7684\u539f\u56e0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u91cd\u6784MPLS\u5806\u6808\u964d\u4f4eRLD\u7684\u673a\u5236\uff0c\u5e76\u5728\u53ef\u7f16\u7a0b\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "motivation": "MPLS\u7f51\u7edc\u52a8\u4f5c\uff08MNA\uff09\u6846\u67b6\u901a\u8fc7LSEs\u6269\u5c55MPLS\u8f6c\u53d1\u529f\u80fd\uff0c\u4f46\u8def\u7531\u5668\u5bf9\u53ef\u8bfb\u6807\u7b7e\u6df1\u5ea6\uff08RLD\uff09\u6709\u7269\u7406\u9650\u5236\uff0cMNA\u8981\u6c42\u8f83\u5927\u7684RLD\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5bf9MNA\u5b9e\u73b0\u8fdb\u884c\u786c\u4ef6\u5206\u6790\uff0c\u8bc6\u522bRLD\u9700\u6c42\u5927\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u8f6c\u53d1\u65f6\u91cd\u6784MPLS\u5806\u6808\u7684\u673a\u5236\u3002\u5f15\u5165\u5806\u6808\u7ba1\u7406\u7f51\u7edc\u52a8\u4f5c\u5e76\u5c55\u793a\u5176\u4e0e\u4e0d\u652f\u6301MNA\u8282\u70b9\u7684\u7f51\u7edc\u96c6\u6210\u3002", "result": "\u63d0\u51fa\u7684\u673a\u5236\u5728\u53ef\u7f16\u7a0b\u786c\u4ef6\u4e0a\u5b9e\u73b0\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5bf9RLD\u3001ECMP\u548c\u6570\u636e\u5305\u5f00\u9500\u7684\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u6784\u5efaMPLS\u5806\u6808\u548c\u5f15\u5165\u5806\u6808\u7ba1\u7406\u7f51\u7edc\u52a8\u4f5c\uff0c\u8be5\u673a\u5236\u6210\u529f\u964d\u4f4e\u4e86MNA\u8282\u70b9\u6240\u9700\u7684RLD\uff0c\u5e76\u5728\u53ef\u7f16\u7a0b\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002"}}
{"id": "2507.14967", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14967", "abs": "https://arxiv.org/abs/2507.14967", "authors": ["Pratik Ingle", "Kasper St\u00f8y", "Andres Fai\u00f1a"], "title": "Heterogeneous object manipulation on nonlinear soft surface through linear controller", "comment": "8 pages, 3 figures", "summary": "Manipulation surfaces indirectly control and reposition objects by actively\nmodifying their shape or properties rather than directly gripping objects.\nThese surfaces, equipped with dense actuator arrays, generate dynamic\ndeformations. However, a high-density actuator array introduces considerable\ncomplexity due to increased degrees of freedom (DOF), complicating control\ntasks. High DOF restrict the implementation and utilization of manipulation\nsurfaces in real-world applications as the maintenance and control of such\nsystems exponentially increase with array/surface size. Learning-based control\napproaches may ease the control complexity, but they require extensive training\nsamples and struggle to generalize for heterogeneous objects. In this study, we\nintroduce a simple, precise and robust PID-based linear close-loop feedback\ncontrol strategy for heterogeneous object manipulation on MANTA-RAY\n(Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation\ndensity). Our approach employs a geometric transformation-driven PID\ncontroller, directly mapping tilt angle control outputs(1D/2D) to actuator\ncommands to eliminate the need for extensive black-box training. We validate\nthe proposed method through simulations and experiments on a physical system,\nsuccessfully manipulating objects with diverse geometries, weights and\ntextures, including fragile objects like eggs and apples. The outcomes\ndemonstrate that our approach is highly generalized and offers a practical and\nreliable solution for object manipulation on soft robotic manipulation,\nfacilitating real-world implementation without prohibitive training demands.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u7cbe\u786e\u4e14\u9c81\u68d2\u7684PID\u95ed\u73af\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u5f02\u6784\u7269\u4f53\u5728\u8f6f\u673a\u5668\u4eba\u64cd\u4f5c\u8868\u9762\u7684\u63a7\u5236\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5b66\u4e60\u65b9\u6cd5\u7684\u9ad8\u8bad\u7ec3\u9700\u6c42\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u5ea6\u901a\u7528\u6027\u3002", "motivation": "\u9ad8\u5bc6\u5ea6\u6267\u884c\u5668\u9635\u5217\u5e26\u6765\u7684\u9ad8\u81ea\u7531\u5ea6\u589e\u52a0\u4e86\u63a7\u5236\u590d\u6742\u5ea6\uff0c\u9650\u5236\u4e86\u64cd\u4f5c\u8868\u9762\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6837\u672c\u4e14\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u53d8\u6362\u9a71\u52a8\u7684PID\u63a7\u5236\u5668\uff0c\u76f4\u63a5\u5c06\u503e\u659c\u89d2\u5ea6\u63a7\u5236\u8f93\u51fa\u6620\u5c04\u5230\u6267\u884c\u5668\u547d\u4ee4\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u9ed1\u76d2\u8bad\u7ec3\u9700\u6c42\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u7269\u7406\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u6210\u529f\u64cd\u4f5c\u4e86\u5305\u62ec\u8106\u5f31\u7269\u4f53\uff08\u5982\u9e21\u86cb\u548c\u82f9\u679c\uff09\u5728\u5185\u7684\u591a\u79cd\u51e0\u4f55\u3001\u91cd\u91cf\u548c\u7eb9\u7406\u7684\u7269\u4f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePID\u7684\u7ebf\u6027\u95ed\u73af\u53cd\u9988\u63a7\u5236\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u5f02\u6784\u7269\u4f53\u5728MANTA-RAY\u4e0a\u7684\u64cd\u4f5c\uff0c\u8bc1\u660e\u4e86\u5176\u9ad8\u5ea6\u901a\u7528\u6027\u53ca\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.14962", "categories": ["cs.AI", "cs.CC", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.14962", "abs": "https://arxiv.org/abs/2507.14962", "authors": ["Johannes Schmidt", "Mohamed Maizia", "Victor Lagerkvist", "Johannes K. Fichte"], "title": "Complexity of Faceted Explanations in Propositional Abduction", "comment": "This is the author's self-archived copy including detailed proofs. To\n  appear in Theory and Practice of Logic Programming (TPLP), Proceedings of the\n  41st International Conference on Logic Programming (ICLP 2025)", "summary": "Abductive reasoning is a popular non-monotonic paradigm that aims to explain\nobserved symptoms and manifestations. It has many applications, such as\ndiagnosis and planning in artificial intelligence and database updates. In\npropositional abduction, we focus on specifying knowledge by a propositional\nformula. The computational complexity of tasks in propositional abduction has\nbeen systematically characterized - even with detailed classifications for\nBoolean fragments. Unsurprisingly, the most insightful reasoning problems\n(counting and enumeration) are computationally highly challenging. Therefore,\nwe consider reasoning between decisions and counting, allowing us to understand\nexplanations better while maintaining favorable complexity. We introduce facets\nto propositional abductions, which are literals that occur in some explanation\n(relevant) but not all explanations (dispensable). Reasoning with facets\nprovides a more fine-grained understanding of variability in explanations\n(heterogeneous). In addition, we consider the distance between two\nexplanations, enabling a better understanding of heterogeneity/homogeneity. We\ncomprehensively analyze facets of propositional abduction in various settings,\nincluding an almost complete characterization in Post's framework.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u9762\uff08facets\uff09\u6982\u5ff5\uff0c\u7ec6\u5316\u547d\u9898\u6eaf\u56e0\u89e3\u91ca\u7684\u53ef\u53d8\u6027\uff0c\u5e76\u5728Post\u6846\u67b6\u4e2d\u51e0\u4e4e\u5b8c\u5168\u8868\u5f81\u4e86\u9762\uff0c\u4e3a\u9ad8\u590d\u6742\u5ea6\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "motivation": "\u547d\u9898\u6eaf\u56e0\u5728\u4eba\u5de5\u667a\u80fd\u548c\u6570\u636e\u5e93\u66f4\u65b0\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u6700\u6df1\u5165\u7684\u63a8\u7406\u95ee\u9898\uff08\u5982\u8ba1\u6570\u548c\u679a\u4e3e\uff09\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5728\u51b3\u7b56\u548c\u8ba1\u6570\u4e4b\u95f4\u8fdb\u884c\u63a8\u7406\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u89e3\u91ca\u540c\u65f6\u4fdd\u6301\u6709\u5229\u7684\u590d\u6742\u5ea6\u3002", "method": "\u5f15\u5165\u9762\uff08facets\uff09\u4f5c\u4e3a\u547d\u9898\u6eaf\u56e0\u4e2d\u7684\u5b57\u9762\u91cf\uff0c\u5206\u6790\u5176\u5728\u89e3\u91ca\u4e2d\u7684\u76f8\u5173\u6027\u548c\u53ef\u66ff\u4ee3\u6027\uff0c\u5e76\u8003\u8651\u89e3\u91ca\u4e4b\u95f4\u7684\u8ddd\u79bb\u4ee5\u7406\u89e3\u5f02\u8d28\u6027/\u540c\u8d28\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u9762\u7684\u6982\u5ff5\uff0c\u5e76\u5168\u9762\u5206\u6790\u4e86\u547d\u9898\u6eaf\u56e0\u4e2d\u9762\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\u7684\u6027\u8d28\uff0c\u5305\u62ec\u5728Post\u6846\u67b6\u4e2d\u7684\u51e0\u4e4e\u5b8c\u5168\u8868\u5f81\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u9762\uff08facets\uff09\u7684\u6982\u5ff5\uff0c\u4e3a\u547d\u9898\u6eaf\u56e0\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u89e3\u91ca\u53ef\u53d8\u6027\u7406\u89e3\uff0c\u5e76\u5728Post\u6846\u67b6\u4e2d\u51e0\u4e4e\u5b8c\u5168\u8868\u5f81\u4e86\u547d\u9898\u6eaf\u56e0\u7684\u9762\u3002"}}
{"id": "2507.15226", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15226", "abs": "https://arxiv.org/abs/2507.15226", "authors": ["Changguo Jia", "Yi Zhan", "Tianqi Zhao", "Hengzhi Ye", "Minghui Zhou"], "title": "Code Clone Detection via an AlphaFold-Inspired Framework", "comment": null, "summary": "Code clone detection, which aims to identify functionally equivalent code\nfragments, plays a critical role in software maintenance and vulnerability\nanalysis. Substantial methods have been proposed to detect code clones, but\nthey fall short in capturing code semantics or relying on language-specific\nanalyzers. Inspired by the remarkable success of AlphaFold in predicting\nthree-dimensional protein structures from protein sequences, in this paper, we\nleverage AlphaFold for code clone detection based on the insight that protein\nsequences and token sequences share a common linear sequential structure. In\nparticular, we propose AlphaCC, which represents code fragments as token\nsequences to ensure multi-language applicability and adapts AlphaFold's\nsequence-to-structure modeling capability to infer code semantics. The pipeline\nof AlphaCC goes through three steps. First, AlphaCC transforms each input code\nfragment into a token sequence and, motivated by AlphaFold's use of multiple\nsequence alignment (MSA) to enhance contextual understanding, constructs an MSA\nfrom lexically similar token sequences. Second, AlphaCC adopts a modified\nattention-based encoder based on AlphaFold to model dependencies within and\nacross token sequences. Finally, unlike AlphaFold's protein structure\nprediction task, AlphaCC computes similarity scores between token sequences\nthrough a late interaction strategy and performs binary classification to\ndetermine code clone pairs. Comprehensive evaluations on three language-diverse\ndatasets demonstrate AlphaCC's applicability across multiple programming\nlanguages. On two semantic clone detection datasets, it consistently\noutperforms all baselines, showing strong semantic understanding. Moreover,\nAlphaCC maintains competitive efficiency, enabling practical usage in\nlarge-scale clone detection tasks.", "AI": {"tldr": "AlphaCC\u5229\u7528AlphaFold\u7684\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\uff0c\u901a\u8fc7\u4ee4\u724c\u5e8f\u5217\u548cMSA\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\uff0c\u5728\u591a\u8bed\u8a00\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u4e14\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u4ee3\u7801\u8bed\u4e49\u6216\u4f9d\u8d56\u8bed\u8a00\u7279\u5b9a\u5206\u6790\u5668\uff0cAlphaFold\u5728\u86cb\u767d\u8d28\u5e8f\u5217\u5230\u7ed3\u6784\u9884\u6d4b\u7684\u6210\u529f\u542f\u53d1\u5c06\u7c7b\u4f3c\u65b9\u6cd5\u5e94\u7528\u4e8e\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u3002", "method": "AlphaCC\u5c06\u4ee3\u7801\u7247\u6bb5\u8f6c\u6362\u4e3a\u4ee4\u724c\u5e8f\u5217\uff0c\u6784\u5efa\u591a\u5e8f\u5217\u6bd4\u5bf9\uff08MSA\uff09\u4ee5\u589e\u5f3a\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u91c7\u7528\u57fa\u4e8eAlphaFold\u7684\u6ce8\u610f\u529b\u7f16\u7801\u5668\u5efa\u6a21\u4f9d\u8d56\u5173\u7cfb\uff0c\u6700\u540e\u901a\u8fc7\u540e\u671f\u4ea4\u4e92\u7b56\u7565\u8ba1\u7b97\u76f8\u4f3c\u5ea6\u5e76\u8fdb\u884c\u4e8c\u5206\u7c7b\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cAlphaCC\u5728\u591a\u8bed\u8a00\u9002\u7528\u6027\u548c\u8bed\u4e49\u514b\u9686\u68c0\u6d4b\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u3002", "conclusion": "AlphaCC\u901a\u8fc7\u501f\u9274AlphaFold\u7684\u5e8f\u5217\u5230\u7ed3\u6784\u5efa\u6a21\u80fd\u529b\uff0c\u5728\u591a\u8bed\u8a00\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u5728\u8bed\u4e49\u7406\u89e3\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u8fd8\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u68c0\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2507.15423", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.15423", "abs": "https://arxiv.org/abs/2507.15423", "authors": ["Laura Finarelli", "Falko Dressler", "Marco Ajmone Marsan", "Gianluca Rizzo"], "title": "Assessing the Benefits of Ground Vehicles as Moving Urban Base Stations", "comment": null, "summary": "In the evolution towards 6G user-centric networking, the moving network (MN)\nparadigm can play an important role. In a MN, some small cell base stations\n(BS) are installed on top of vehicles, and enable a more dynamic, flexible and\nsustainable, network operation. By \"following\" the users movements and adapting\ndynamically to their requests, the MN paradigm enables a more efficient\nutilization of network resources, mitigating the need for dense small cell BS\ndeployments at the cost of an increase in resource utilization due to wireless\nbackhauling. This aspect is at least partly compensated by the shorter distance\nbetween users and BS, which allows for lower power and Line-of-Sight\ncommunications. While the MN paradigm has been investigated for some time, to\ndate, it is still unclear in which conditions the advantages of MN outweigh the\nadditional resource costs. In this paper, we propose a stochastic geometry\nframework for the characterization of the potential benefits of the MN paradigm\nas part of an HetNet in urban settings. Our approach allows the estimation of\nuser-perceived performance, accounting for wireless backhaul connectivity as\nwell as base station resource scheduling. We formulate an optimization problem\nfor determining the resource-optimal network configurations and BS scheduling\nwhich minimize the overall amount of deployed BSs in a QoS-aware manner, and\nthe minimum vehicular flow between different urban districts required to\nsupport them, and we propose an efficient stochastic heuristic to solve it. Our\nnumerical assessment suggests that the MN paradigm, coupled with appropriate\ndynamic network management strategies, significantly reduces the amount of\ndeployed network infrastructure while guaranteeing the target QoS perceived by\nusers.", "AI": {"tldr": "\u7814\u7a76\u79fb\u52a8\u7f51\u7edc\uff08MN\uff09\u8303\u5f0f\u57286G HetNet\u4e2d\u7684\u6548\u76ca\uff0c\u901a\u8fc7\u968f\u673a\u51e0\u4f55\u6846\u67b6\u548c\u4f18\u5316\u95ee\u9898\uff0c\u8bc1\u660eMN\u80fd\u51cf\u5c11\u57fa\u7840\u8bbe\u65bd\u90e8\u7f72\u5e76\u4fdd\u8bc1\u670d\u52a1\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u57286G\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u7f51\u7edc\u6f14\u8fdb\u4e2d\uff0c\u79fb\u52a8\u7f51\u7edc\uff08MN\uff09\u8303\u5f0f\u5982\u4f55\u5728\u52a8\u6001\u3001\u7075\u6d3b\u548c\u53ef\u6301\u7eed\u7684\u7f51\u7edc\u8fd0\u8425\u4e2d\u53d1\u6325\u4f5c\u7528\uff0c\u5e76\u660e\u786e\u5176\u4f18\u52bf\u8d85\u8fc7\u989d\u5916\u8d44\u6e90\u6210\u672c\u7684\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u968f\u673a\u51e0\u4f55\u6846\u67b6\uff0c\u7528\u4e8e\u63cf\u8ff0MN\u8303\u5f0f\u5728HetNet\u4e2d\u7684\u6f5c\u5728\u6548\u76ca\uff0c\u5e76\u5236\u5b9a\u4e86\u4f18\u5316\u95ee\u9898\u4ee5\u786e\u5b9a\u8d44\u6e90\u6700\u4f18\u7684\u7f51\u7edc\u914d\u7f6e\u548c\u57fa\u7ad9\u8c03\u5ea6\u3002", "result": "\u6570\u503c\u8bc4\u4f30\u8868\u660e\uff0cMN\u8303\u5f0f\u80fd\u663e\u8457\u51cf\u5c11\u90e8\u7f72\u7684\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\uff0c\u540c\u65f6\u4fdd\u8bc1\u7528\u6237\u611f\u77e5\u7684\u76ee\u6807\u670d\u52a1\u8d28\u91cf\u3002", "conclusion": "MN\u8303\u5f0f\u4e0e\u9002\u5f53\u7684\u52a8\u6001\u7f51\u7edc\u7ba1\u7406\u7b56\u7565\u7ed3\u5408\uff0c\u80fd\u663e\u8457\u51cf\u5c11\u90e8\u7f72\u7684\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\uff0c\u540c\u65f6\u4fdd\u8bc1\u7528\u6237\u611f\u77e5\u7684\u76ee\u6807\u670d\u52a1\u8d28\u91cf\u3002"}}
{"id": "2507.14975", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14975", "abs": "https://arxiv.org/abs/2507.14975", "authors": ["Yufan Song", "Jiatao Zhang", "Zeng Gu", "Qingmiao Liang", "Tuocheng Hu", "Wei Song", "Shiqiang Zhu"], "title": "FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models", "comment": "8 pages, 6 figures, IROS 2025", "summary": "Autonomous error correction is critical for domestic robots to achieve\nreliable execution of complex long-horizon tasks. Prior work has explored\nself-reflection in Large Language Models (LLMs) for task planning error\ncorrection; however, existing methods are constrained by inflexible\nself-reflection mechanisms that limit their effectiveness. Motivated by these\nlimitations and inspired by human cognitive adaptation, we propose the Flexible\nConstructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture\nthat enables LLMs to perform flexible self-reflection based on task difficulty,\nwhile constructively integrating historical valuable experience with failure\nlessons. We evaluated FCRF on diverse domestic tasks through simulation in\nAlfWorld and physical deployment in the real-world environment. Experimental\nresults demonstrate that FCRF significantly improves overall performance and\nself-reflection flexibility in complex long-horizon robotic tasks.", "AI": {"tldr": "FCRF\u662f\u4e00\u79cd\u65b0\u578b\u5bfc\u5e08-\u6267\u884c\u8005\u67b6\u6784\uff0c\u901a\u8fc7\u7075\u6d3b\u81ea\u6211\u53cd\u601d\u548c\u6574\u5408\u5386\u53f2\u7ecf\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u957f\u671f\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u81ea\u6211\u53cd\u601d\u673a\u5236\u4e0d\u591f\u7075\u6d3b\u800c\u6548\u679c\u53d7\u9650\uff0c\u53d7\u4eba\u7c7b\u8ba4\u77e5\u9002\u5e94\u542f\u53d1\uff0c\u65e8\u5728\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u7684\u53ef\u9760\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u7075\u6d3b\u7684\u5efa\u6784\u4e3b\u4e49\u53cd\u601d\u6846\u67b6\uff08FCRF\uff09\uff0c\u91c7\u7528\u5bfc\u5e08-\u6267\u884c\u8005\u67b6\u6784\uff0c\u4f7fLLM\u80fd\u591f\u57fa\u4e8e\u4efb\u52a1\u96be\u5ea6\u8fdb\u884c\u7075\u6d3b\u81ea\u6211\u53cd\u601d\uff0c\u5e76\u6574\u5408\u5386\u53f2\u7ecf\u9a8c\u4e0e\u5931\u8d25\u6559\u8bad\u3002", "result": "\u5728AlfWorld\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u90e8\u7f72\u4e2d\uff0cFCRF\u5728\u591a\u6837\u5316\u5bb6\u5ead\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "FCRF\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u957f\u671f\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u6574\u4f53\u6027\u80fd\u548c\u81ea\u6211\u53cd\u601d\u7075\u6d3b\u6027\u3002"}}
{"id": "2507.14987", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14987", "abs": "https://arxiv.org/abs/2507.14987", "authors": ["Yi Zhang", "An Zhang", "XiuYu Zhang", "Leheng Sheng", "Yuxin Chen", "Zhenkai Liang", "Xiang Wang"], "title": "AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs), despite possessing latent safety understanding\nfrom their vast pretraining data, remain vulnerable to generating harmful\ncontent and exhibit issues such as over-refusal and utility degradation after\nsafety alignment. Current safety alignment methods often result in superficial\nrefusal shortcuts or rely on intensive supervision for reasoning-based\napproaches, failing to fully leverage the model's intrinsic safety\nself-awareness. We propose \\textbf{AlphaAlign}, a simple yet effective pure\nreinforcement learning (RL) framework with verifiable safety reward designed to\nincentivize this latent safety awareness through proactive safety reasoning.}\nAlphaAlign employs a dual-reward system: a verifiable safety reward encourages\ncorrectly formatted and explicitly justified refusals for harmful queries while\npenalizing over-refusals, and a normalized helpfulness reward guides\nhigh-quality responses to benign inputs. This allows the model to develop\nproactive safety reasoning capabilities without depending on supervised\nsafety-specific reasoning data. AlphaAlign demonstrates three key advantages:\n(1) Simplicity and efficiency, requiring only binary prompt safety labels and\nminimal RL steps for substantial improvements. (2) Breaking the safety-utility\ntrade-off, by enhancing refusal of harmful content and reducing over-refusals,\nwhile simultaneously maintaining or even improving general task performance and\nrobustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety\nreasoning that generates explicit safety rationales rather than relying on\nshallow refusal patterns.", "AI": {"tldr": "AlphaAlign\u662f\u4e00\u79cd\u7eaf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5956\u52b1\u7cfb\u7edf\u6fc0\u53d1LLMs\u7684\u5185\u5728\u5b89\u5168\u610f\u8bc6\uff0c\u5b9e\u73b0\u6df1\u5ea6\u5bf9\u9f50\u5e76\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u5f80\u5f80\u5bfc\u81f4\u6d45\u5c42\u62d2\u7edd\u6216\u4f9d\u8d56\u5bc6\u96c6\u76d1\u7763\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6a21\u578b\u5185\u5728\u7684\u5b89\u5168\u81ea\u6211\u610f\u8bc6\u3002", "method": "AlphaAlign\u91c7\u7528\u53cc\u5956\u52b1\u7cfb\u7edf\uff1a\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u5956\u52b1\u9f13\u52b1\u5bf9\u6709\u5bb3\u67e5\u8be2\u7684\u6b63\u786e\u62d2\u7edd\u548c\u660e\u786e\u7406\u7531\uff0c\u540c\u65f6\u60e9\u7f5a\u8fc7\u5ea6\u62d2\u7edd\uff1b\u6807\u51c6\u5316\u5e2e\u52a9\u6027\u5956\u52b1\u5219\u5f15\u5bfc\u5bf9\u826f\u6027\u8f93\u5165\u7684\u9ad8\u8d28\u91cf\u54cd\u5e94\u3002", "result": "AlphaAlign\u5728\u7b80\u5316\u6d41\u7a0b\u3001\u6253\u7834\u5b89\u5168-\u6548\u7528\u6743\u8861\u53ca\u4fc3\u8fdb\u6df1\u5ea6\u5b89\u5168\u5bf9\u9f50\u65b9\u9762\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "AlphaAlign\u901a\u8fc7\u7eaf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6709\u6548\u6fc0\u53d1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u5b89\u5168\u81ea\u6211\u610f\u8bc6\uff0c\u5b9e\u73b0\u4e86\u6df1\u5ea6\u5b89\u5168\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u901a\u7528\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2507.15241", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15241", "abs": "https://arxiv.org/abs/2507.15241", "authors": ["Vikram Nitin", "Baishakhi Ray", "Roshanak Zilouchian Moghaddam"], "title": "FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents", "comment": null, "summary": "Despite the critical threat posed by software security vulnerabilities,\nreports are often incomplete, lacking the proof-of-vulnerability (PoV) tests\nneeded to validate fixes and prevent regressions. These tests are crucial not\nonly for ensuring patches work, but also for helping developers understand how\nvulnerabilities can be exploited. Generating PoV tests is a challenging\nproblem, requiring reasoning about the flow of control and data through deeply\nnested levels of a program.\n  We present FaultLine, an LLM agent workflow that uses a set of carefully\ndesigned reasoning steps, inspired by aspects of traditional static and dynamic\nprogram analysis, to automatically generate PoV test cases. Given a software\nproject with an accompanying vulnerability report, FaultLine 1) traces the flow\nof an input from an externally accessible API (\"source\") to the \"sink\"\ncorresponding to the vulnerability, 2) reasons about the conditions that an\ninput must satisfy in order to traverse the branch conditions encountered along\nthe flow, and 3) uses this reasoning to generate a PoV test case in a\nfeedback-driven loop. FaultLine does not use language-specific static or\ndynamic analysis components, which enables it to be used across programming\nlanguages.\n  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100\nknown vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine\nis able to generate PoV tests for 16 projects, compared to just 9 for CodeAct\n2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine\nrepresents a 77% relative improvement over the state of the art. Our findings\nsuggest that hierarchical reasoning can enhance the performance of LLM agents\non PoV test generation, but the problem in general remains challenging. We make\nour code and dataset publicly available in the hope that it will spur further\nresearch in this area.", "AI": {"tldr": "FaultLine\u5229\u7528LLM\u4ee3\u7406\u5de5\u4f5c\u6d41\u81ea\u52a8\u751f\u6210PoV\u6d4b\u8bd5\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u534777%\uff0c\u4f46\u95ee\u9898\u4ecd\u5177\u6311\u6218\u6027\u3002", "motivation": "\u8f6f\u4ef6\u5b89\u5168\u6f0f\u6d1e\u62a5\u544a\u5e38\u7f3a\u5c11PoV\u6d4b\u8bd5\uff0c\u5bfc\u81f4\u4fee\u590d\u9a8c\u8bc1\u548c\u56de\u5f52\u9884\u9632\u56f0\u96be\u3002\u81ea\u52a8\u751f\u6210PoV\u6d4b\u8bd5\u5bf9\u786e\u4fdd\u8865\u4e01\u6709\u6548\u6027\u548c\u5f00\u53d1\u8005\u7406\u89e3\u6f0f\u6d1e\u5229\u7528\u65b9\u5f0f\u81f3\u5173\u91cd\u8981\u3002", "method": "FaultLine\u662f\u4e00\u4e2aLLM\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u7ed3\u5408\u9759\u6001\u548c\u52a8\u6001\u7a0b\u5e8f\u5206\u6790\u7684\u8bbe\u8ba1\u601d\u8def\uff0c\u901a\u8fc7\u4e09\u6b65\u63a8\u7406\uff08\u8ffd\u8e2a\u8f93\u5165\u6d41\u3001\u5206\u6790\u5206\u652f\u6761\u4ef6\u3001\u53cd\u9988\u9a71\u52a8\u751f\u6210\uff09\u81ea\u52a8\u751f\u6210PoV\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u5728100\u4e2a\u591a\u8bed\u8a00\u6f0f\u6d1e\u6570\u636e\u96c6\u4e2d\uff0cFaultLine\u751f\u6210\u4e8616\u4e2a\u9879\u76ee\u7684PoV\u6d4b\u8bd5\uff0c\u76f8\u5bf9\u73b0\u6709\u6280\u672f\uff08CodeAct 2.1\u76849\u4e2a\uff09\u63d0\u5347\u4e8677%\u3002", "conclusion": "FaultLine\u901a\u8fc7\u5206\u5c42\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728PoV\u6d4b\u8bd5\u751f\u6210\u4e2d\u7684\u6027\u80fd\uff0c\u4f46\u8be5\u95ee\u9898\u6574\u4f53\u4ecd\u5177\u6311\u6218\u6027\u3002"}}
{"id": "2507.15659", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.15659", "abs": "https://arxiv.org/abs/2507.15659", "authors": ["Gabriel Paradzik", "Benjamin Steinert", "Heinrich Abele", "Michael Menth"], "title": "SENSOR: A Cost-Efficient Open-Source Flow Monitoring Platform", "comment": null, "summary": "This paper presents a cost-effective and distributed flow monitoring platform\nfor collecting unsampled IPFIX data exclusively using open-source tools, which\nis implemented at the University of T\\\"ubingen. An overview of all tools is\ngiven and their use is explained.", "AI": {"tldr": "\u5f00\u6e90\u5de5\u5177\u6784\u5efa\u7684\u5206\u5e03\u5f0f\u6d41\u76d1\u63a7\u5e73\u53f0\uff0c\u4f4e\u6210\u672c\u4e14\u9ad8\u6548\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6d41\u76d1\u63a7\u5e73\u53f0\u6210\u672c\u9ad8\u4e14\u91c7\u6837\u6570\u636e\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u6784\u5efa\u5206\u5e03\u5f0f\u6d41\u76d1\u63a7\u5e73\u53f0\uff0c\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u5de5\u5177\u7684\u4f7f\u7528\u65b9\u6cd5\u3002", "result": "\u6210\u529f\u5728\u8482\u5bbe\u6839\u5927\u5b66\u5b9e\u65bd\u8be5\u5e73\u53f0\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u548c\u6210\u672c\u6548\u76ca\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u6d41\u76d1\u63a7\u5e73\u53f0\uff0c\u4e13\u7528\u4e8e\u6536\u96c6\u672a\u91c7\u6837\u7684IPFIX\u6570\u636e\uff0c\u5e76\u5b8c\u5168\u57fa\u4e8e\u5f00\u6e90\u5de5\u5177\u5b9e\u73b0\u3002"}}
{"id": "2507.15022", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.15022", "abs": "https://arxiv.org/abs/2507.15022", "authors": ["Sumeadh MS", "Kevin Dsouza", "Ravi Prakash"], "title": "CPED-NCBFs: A Conformal Prediction for Expert Demonstration-based Neural Control Barrier Functions", "comment": "6pages, 4figures, Submitted to the prestigious Indian Control\n  Conference (ICC), 2025", "summary": "Among the promising approaches to enforce safety in control systems, learning\nControl Barrier Functions (CBFs) from expert demonstrations has emerged as an\neffective strategy. However, a critical challenge remains: verifying that the\nlearned CBFs truly enforce safety across the entire state space. This is\nespecially difficult when CBF is represented using neural networks (NCBFs).\nSeveral existing verification techniques attempt to address this problem\nincluding SMT-based solvers, mixed-integer programming (MIP), and interval or\nbound-propagation methods but these approaches often introduce loose,\nconservative bounds. To overcome these limitations, in this work we use\nCPED-NCBFs a split-conformal prediction based verification strategy to verify\nthe learned NCBF from the expert demonstrations. We further validate our method\non point mass systems and unicycle models to demonstrate the effectiveness of\nthe proposed theory.", "AI": {"tldr": "\u63d0\u51faCPED-NCBFs\u9a8c\u8bc1\u7b56\u7565\uff0c\u6709\u6548\u9a8c\u8bc1\u5b66\u4e60\u5230\u7684NCBF\uff0c\u5e76\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\uff08\u5982SMT\u6c42\u89e3\u5668\u3001\u6df7\u5408\u6574\u6570\u89c4\u5212\u3001\u533a\u95f4\u4f20\u64ad\u65b9\u6cd5\uff09\u5e38\u5f15\u5165\u5bbd\u677e\u4e14\u4fdd\u5b88\u7684\u8fb9\u754c\uff0c\u65e0\u6cd5\u6709\u6548\u9a8c\u8bc1\u5b66\u4e60\u5230\u7684NCBF\u662f\u5426\u5728\u6574\u4e2a\u72b6\u6001\u7a7a\u95f4\u5185\u786e\u4fdd\u5b89\u5168\u3002", "method": "\u4f7f\u7528CPED-NCBFs\uff08\u57fa\u4e8e\u5206\u5272\u5171\u5f62\u9884\u6d4b\u7684\u9a8c\u8bc1\u7b56\u7565\uff09\u6765\u9a8c\u8bc1\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u7684NCBF\u3002", "result": "CPED-NCBFs\u5728\u70b9\u8d28\u91cf\u7cfb\u7edf\u548c\u975e\u5b8c\u6574\u6a21\u578b\u4e2d\u7684\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\u5176\u6709\u6548\u6027\u3002", "conclusion": "CPED-NCBFs\u4f5c\u4e3a\u4e00\u79cd\u57fa\u4e8e\u5206\u5272\u5171\u5f62\u9884\u6d4b\u7684\u9a8c\u8bc1\u7b56\u7565\uff0c\u6709\u6548\u9a8c\u8bc1\u4e86\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u7684NCBF\uff0c\u5e76\u5728\u70b9\u8d28\u91cf\u7cfb\u7edf\u548c\u975e\u5b8c\u6574\u6a21\u578b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.15013", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15013", "abs": "https://arxiv.org/abs/2507.15013", "authors": ["Xiaoyu Li", "Jin Wu", "Shaoyang Guo", "Haoran Shi", "Chanjin Zheng"], "title": "A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing", "comment": "15pages, 7 figures", "summary": "In the smart era, psychometric tests are becoming increasingly important for\npersonnel selection, career development, and mental health assessment.\nForced-choice tests are common in personality assessments because they require\nparticipants to select from closely related options, lowering the risk of\nresponse distortion. This study presents a deep learning-based Forced-Choice\nNeural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of\ntraditional models and is applicable to the three most common item block types\nfound in forced-choice tests. To account for the unidimensionality of items in\nforced-choice tests, we create interpretable participant and item parameters.\nWe model the interactions between participant and item features using\nmultilayer neural networks after mining them using nonlinear mapping. In\naddition, we use the monotonicity assumption to improve the interpretability of\nthe diagnostic results. The FCNCD's effectiveness is validated by experiments\non real-world and simulated datasets that show its accuracy, interpretability,\nand robustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8feb\u9009\u795e\u7ecf\u8ba4\u77e5\u8bca\u65ad\u6a21\u578b\uff08FCNCD\uff09\uff0c\u6709\u6548\u514b\u670d\u4f20\u7edf\u6a21\u578b\u7684\u9650\u5236\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u667a\u80fd\u5316\u65f6\u4ee3\uff0c\u5fc3\u7406\u6d4b\u91cf\u6d4b\u8bd5\u5bf9\u4eba\u5458\u9009\u62d4\u3001\u804c\u4e1a\u53d1\u5c55\u548c\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u65e5\u76ca\u91cd\u8981\u3002\u8feb\u9009\u6d4b\u8bd5\u56e0\u5176\u80fd\u964d\u4f4e\u56de\u7b54\u5931\u771f\u7684\u98ce\u9669\u800c\u5e38\u7528\u4e8e\u4eba\u683c\u8bc4\u4f30\uff0c\u4f46\u4f20\u7edf\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8feb\u9009\u795e\u7ecf\u8ba4\u77e5\u8bca\u65ad\u6a21\u578b\uff08FCNCD\uff09\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u6620\u5c04\u6316\u6398\u53c2\u4e0e\u8005\u548c\u9879\u76ee\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u5176\u4ea4\u4e92\u4f5c\u7528\uff0c\u540c\u65f6\u5229\u7528\u5355\u8c03\u6027\u5047\u8bbe\u63d0\u5347\u8bca\u65ad\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FCNCD\u6a21\u578b\u5728\u771f\u5b9e\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "FCNCD\u6a21\u578b\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5fc3\u7406\u6d4b\u91cf\u6d4b\u8bd5\u4e2d\u7684\u8feb\u9009\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15251", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15251", "abs": "https://arxiv.org/abs/2507.15251", "authors": ["Boyang Yang", "Luyao Ren", "Xin Yin", "Jiadong Ren", "Haoye Tian", "Shunfu Jin"], "title": "Input Reduction Enhanced LLM-based Program Repair", "comment": null, "summary": "Large Language Models (LLMs) have shown great potential in Automated Program\nRepair (APR). Test inputs, being crucial for reasoning the root cause of\nfailures, are always included in the prompt for LLM-based APR. Unfortunately,\nLLMs struggle to retain key information in long prompts. When the test inputs\nare extensive in the prompt, this may trigger the \"lost-in-the-middle\" issue,\ncompromising repair performance. To address this, we propose ReduceFix, an\nLLM-based APR approach with a built-in component that automatically reduces\ntest inputs while retaining their failure-inducing behavior. ReduceFix prompts\nan LLM to generate a reducer that minimizes failure-inducing test inputs\nwithout human effort, and then feeds the reduced failure-inducing inputs to\nguide patch generation.\n  For targeted evaluation, we constructed LFTBench, the first long-input APR\nbenchmark with 200 real bugs from 20 programming tasks, each paired with a\nfailure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix\nshrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%\nrelative to a prompt that includes the original test, and by 17.6% compared\nwith omitting the test entirely. Adding the same reduction step to ChatRepair\nincreases its fix rate by 21.3% without other changes. Ablation studies further\nhighlight the impact of input length and compressed failure information on\nrepair success. These results underscore that automatically reducing failing\ninputs is a practical and powerful complement to LLM-based APR, significantly\nimproving its scalability and effectiveness.", "AI": {"tldr": "ReduceFix\u901a\u8fc7\u81ea\u52a8\u51cf\u5c11\u6d4b\u8bd5\u8f93\u5165\u89e3\u51b3LLM\u5728\u957f\u63d0\u793a\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4fee\u590d\u6548\u679c\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u5904\u7406\u957f\u63d0\u793a\u65f6\u51fa\u73b0\u7684\u2018lost-in-the-middle\u2019\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u4f1a\u56e0\u6d4b\u8bd5\u8f93\u5165\u8fc7\u957f\u800c\u5f71\u54cd\u4fee\u590d\u6027\u80fd\u3002", "method": "\u63d0\u51faReduceFix\uff0c\u4e00\u79cd\u57fa\u4e8eLLM\u7684APR\u65b9\u6cd5\uff0c\u5185\u7f6e\u81ea\u52a8\u51cf\u5c11\u6d4b\u8bd5\u8f93\u5165\u540c\u65f6\u4fdd\u7559\u5176\u5931\u8d25\u8bf1\u5bfc\u884c\u4e3a\u7684\u7ec4\u4ef6\u3002\u901a\u8fc7\u63d0\u793aLLM\u751f\u6210\u4e00\u4e2a\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u6700\u5c0f\u5316\u5931\u8d25\u8bf1\u5bfc\u6d4b\u8bd5\u8f93\u5165\u7684\u7f29\u51cf\u5668\uff0c\u7136\u540e\u7528\u7f29\u51cf\u540e\u7684\u8f93\u5165\u6307\u5bfc\u8865\u4e01\u751f\u6210\u3002", "result": "\u5728LFTBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReduceFix\u5e73\u5747\u7f29\u51cf\u8f93\u516589.1%\uff0c\u5e76\u5c06pass@10\u76f8\u5bf9\u539f\u59cb\u6d4b\u8bd5\u63d0\u793a\u63d0\u9ad8\u4e8653.8%\uff0c\u76f8\u6bd4\u5b8c\u5168\u7701\u7565\u6d4b\u8bd5\u63d0\u9ad8\u4e8617.6%\u3002\u540c\u6837\u7684\u7f29\u51cf\u6b65\u9aa4\u5e94\u7528\u5230ChatRepair\u4e2d\uff0c\u5176\u4fee\u590d\u7387\u63d0\u9ad8\u4e8621.3%\u3002", "conclusion": "\u81ea\u52a8\u51cf\u5c11\u5931\u8d25\u8f93\u5165\u662fLLM-based APR\u7684\u5b9e\u7528\u4e14\u5f3a\u5927\u7684\u8865\u5145\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2507.15670", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.15670", "abs": "https://arxiv.org/abs/2507.15670", "authors": ["Rosario Patan\u00e8", "Nadjib Achir", "Andrea Araldo", "Lila Boukhatem"], "title": "Vehicular Cloud Computing: A cost-effective alternative to Edge Computing in 5G networks", "comment": null, "summary": "Edge Computing (EC) is a computational paradigm that involves deploying\nresources such as CPUs and GPUs near end-users, enabling low-latency\napplications like augmented reality and real-time gaming. However, deploying\nand maintaining a vast network of EC nodes is costly, which can explain its\nlimited deployment today. A new paradigm called Vehicular Cloud Computing (VCC)\nhas emerged and inspired interest among researchers and industry. VCC\nopportunistically utilizes existing and idle vehicular computational resources\nfor external task offloading. This work is the first to systematically address\nthe following question: Can VCC replace EC for low-latency applications?\nAnswering this question is highly relevant for Network Operators (NOs), as VCC\ncould eliminate costs associated with EC given that it requires no\ninfrastructural investment. Despite its potential, no systematic study has yet\nexplored the conditions under which VCC can effectively support low-latency\napplications without relying on EC. This work aims to fill that gap. Extensive\nsimulations allow for assessing the crucial scenario factors that determine\nwhen this EC-to-VCC substitution is feasible. Considered factors are load,\nvehicles mobility and density, and availability. Potential for substitution is\nassessed based on multiple criteria, such as latency, task completion success,\nand cost. Vehicle mobility is simulated in SUMO, and communication in NS3\n5G-LENA. The findings show that VCC can effectively replace EC for low-latency\napplications, except in extreme cases when the EC is still required (latency <\n16 ms).", "AI": {"tldr": "VCC\u53ef\u66ff\u4ee3EC\u7528\u4e8e\u4f4e\u5ef6\u8fdf\u5e94\u7528\uff0c\u6781\u7aef\u60c5\u51b5\u4e0b\u4f8b\u5916\u3002", "motivation": "\u63a2\u7d22VCC\u662f\u5426\u80fd\u5728\u4e0d\u4f9d\u8d56EC\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u652f\u6301\u4f4e\u5ef6\u8fdf\u5e94\u7528\uff0c\u4e3a\u7f51\u7edc\u8fd0\u8425\u5546\u63d0\u4f9b\u6f5c\u5728\u7684\u6210\u672c\u8282\u7ea6\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6a21\u62df\u8bc4\u4f30\u5173\u952e\u573a\u666f\u56e0\u7d20\uff0c\u5305\u62ec\u8d1f\u8f7d\u3001\u8f66\u8f86\u79fb\u52a8\u6027\u548c\u5bc6\u5ea6\u4ee5\u53ca\u53ef\u7528\u6027\uff0c\u4f7f\u7528SUMO\u6a21\u62df\u8f66\u8f86\u79fb\u52a8\u6027\uff0cNS3 5G-LENA\u6a21\u62df\u901a\u4fe1\u3002", "result": "\u7814\u7a76\u53d1\u73b0VCC\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u53ef\u4ee5\u66ff\u4ee3EC\uff0c\u4ec5\u5728\u6781\u7aef\u4f4e\u5ef6\u8fdf\u9700\u6c42\u65f6\u4ecd\u9700EC\u3002", "conclusion": "VCC\u53ef\u4ee5\u6709\u6548\u5730\u66ff\u4ee3EC\u7528\u4e8e\u4f4e\u5ef6\u8fdf\u5e94\u7528\uff0c\u4f46\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\uff08\u5ef6\u8fdf<16\u6beb\u79d2\uff09\u4ecd\u9700\u4f9d\u8d56EC\u3002"}}
{"id": "2507.15062", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15062", "abs": "https://arxiv.org/abs/2507.15062", "authors": ["Xinyue Zhu", "Binghao Huang", "Yunzhu Li"], "title": "Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper", "comment": "More videos can be found on our\n  website:https://binghao-huang.github.io/touch_in_the_wild/", "summary": "Handheld grippers are increasingly used to collect human demonstrations due\nto their ease of deployment and versatility. However, most existing designs\nlack tactile sensing, despite the critical role of tactile feedback in precise\nmanipulation. We present a portable, lightweight gripper with integrated\ntactile sensors that enables synchronized collection of visual and tactile data\nin diverse, real-world, and in-the-wild settings. Building on this hardware, we\npropose a cross-modal representation learning framework that integrates visual\nand tactile signals while preserving their distinct characteristics. The\nlearning procedure allows the emergence of interpretable representations that\nconsistently focus on contacting regions relevant for physical interactions.\nWhen used for downstream manipulation tasks, these representations enable more\nefficient and effective policy learning, supporting precise robotic\nmanipulation based on multimodal feedback. We validate our approach on\nfine-grained tasks such as test tube insertion and pipette-based fluid\ntransfer, demonstrating improved accuracy and robustness under external\ndisturbances. Our project page is available at\nhttps://binghao-huang.github.io/touch_in_the_wild/ .", "AI": {"tldr": "\u4fbf\u643a\u89e6\u89c9\u5939\u722a+\u8de8\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7cbe\u7ec6\u64cd\u4f5c\u7684\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5939\u722a\u8bbe\u8ba1\u7f3a\u4e4f\u89e6\u89c9\u53cd\u9988\uff0c\u800c\u89e6\u89c9\u53cd\u9988\u5bf9\u7cbe\u786e\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u96c6\u6210\u89e6\u89c9\u4f20\u611f\u5668\u7684\u4fbf\u643a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4fbf\u643a\u5f0f\u8f7b\u91cf\u5939\u722a\u8bbe\u8ba1\uff0c\u96c6\u6210\u4e86\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u652f\u6301\u89c6\u89c9\u4e0e\u89e6\u89c9\u6570\u636e\u7684\u540c\u6b65\u91c7\u96c6\uff1b\u5e76\u5f00\u53d1\u4e86\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u89c6\u89c9\u4e0e\u89e6\u89c9\u4fe1\u53f7\u3002", "result": "\u5728\u8bd5\u7ba1\u63d2\u5165\u548c\u79fb\u6db2\u7ba1\u6d41\u4f53\u8f6c\u79fb\u7b49\u7cbe\u7ec6\u4efb\u52a1\u4e2d\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u5bf9\u5916\u90e8\u5e72\u6270\u7684\u9c81\u68d2\u6027\u63d0\u5347\u3002", "conclusion": "\u96c6\u6210\u4e86\u89e6\u89c9\u4f20\u611f\u5668\u7684\u4fbf\u643a\u5f0f\u8f7b\u91cf\u5939\u722a\u53ca\u5176\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.15042", "categories": ["cs.AI", "cs.IR", "I.2.7; H.3.3; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.15042", "abs": "https://arxiv.org/abs/2507.15042", "authors": ["Jerry Wang", "Fang Yu"], "title": "DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection", "comment": "Accepted by KDD Workshop on Prompt Optimization 2025", "summary": "Adversarial prompt attacks can significantly alter the reliability of\nRetrieval-Augmented Generation (RAG) systems by re-ranking them to produce\nincorrect outputs. In this paper, we present a novel method that applies\nDifferential Evolution (DE) to optimize adversarial prompt suffixes for\nRAG-based question answering. Our approach is gradient-free, treating the RAG\npipeline as a black box and evolving a population of candidate suffixes to\nmaximize the retrieval rank of a targeted incorrect document to be closer to\nreal world scenarios. We conducted experiments on the BEIR QA datasets to\nevaluate attack success at certain retrieval rank thresholds under multiple\nretrieving applications. Our results demonstrate that DE-based prompt\noptimization attains competitive (and in some cases higher) success rates\ncompared to GGPP to dense retrievers and PRADA to sparse retrievers, while\nusing only a small number of tokens (<=5 tokens) in the adversarial suffix.\nFurthermore, we introduce a readability-aware suffix construction strategy,\nvalidated by a statistically significant reduction in MLM negative\nlog-likelihood with Welch's t-test. Through evaluations with a BERT-based\nadversarial suffix detector, we show that DE-generated suffixes evade\ndetection, yielding near-chance detection accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u8fdb\u5316\uff08DE\uff09\u7684\u5bf9\u6297\u6027\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u653b\u51fbRAG\u7cfb\u7edf\u5e76\u89c4\u907f\u68c0\u6d4b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u9ad8\u6548\u6027\u548c\u9690\u853d\u6027\u3002", "motivation": "\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\u4f1a\u663e\u8457\u5f71\u54cd\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u5bfc\u81f4\u9519\u8bef\u8f93\u51fa\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u63a5\u8fd1\u771f\u5b9e\u573a\u666f\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u8fdb\u5316\uff08DE\uff09\u7684\u68af\u5ea6\u65e0\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5bf9\u6297\u6027\u63d0\u793a\u540e\u7f00\u6765\u653b\u51fbRAG\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u5c06RAG\u7ba1\u9053\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u901a\u8fc7\u8fdb\u5316\u5019\u9009\u540e\u7f00\u79cd\u7fa4\u6765\u6700\u5927\u5316\u76ee\u6807\u9519\u8bef\u6587\u6863\u7684\u68c0\u7d22\u6392\u540d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDE\u4f18\u5316\u7684\u5bf9\u6297\u6027\u63d0\u793a\u540e\u7f00\u5728BEIR QA\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4e0eGGPP\u548cPRADA\u76f8\u5f53\u751a\u81f3\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u4ec5\u9700\u5c11\u91cf\u6807\u8bb0\uff08<=5\u4e2a\uff09\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u63d0\u793a\u540e\u7f00\u80fd\u6709\u6548\u89c4\u907fBERT-based\u68c0\u6d4b\u5668\u7684\u68c0\u6d4b\u3002", "conclusion": "\u4f7f\u7528\u5dee\u5206\u8fdb\u5316\uff08DE\uff09\u4f18\u5316\u7684\u5bf9\u6297\u6027\u63d0\u793a\u540e\u7f00\u5728RAG\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u80fd\u6709\u6548\u63d0\u9ad8\u9519\u8bef\u6587\u6863\u7684\u68c0\u7d22\u6392\u540d\uff0c\u8fd8\u80fd\u89c4\u907f\u68c0\u6d4b\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.15296", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15296", "abs": "https://arxiv.org/abs/2507.15296", "authors": ["Qian Xiong", "Yuekai Huang", "Ziyou Jiang", "Zhiyuan Chang", "Yujia Zheng", "Tianhao Li", "Mingyang Li"], "title": "Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems", "comment": null, "summary": "The emergence of the tool agent paradigm has broadened the capability\nboundaries of the Large Language Model (LLM), enabling it to complete more\ncomplex tasks. However, the effectiveness of this paradigm is limited due to\nthe issue of parameter failure during its execution. To explore this phenomenon\nand propose corresponding suggestions, we first construct a parameter failure\ntaxonomy in this paper. We derive five failure categories from the invocation\nchain of a mainstream tool agent. Then, we explore the correlation between\nthree different input sources and failure categories by applying 15 input\nperturbation methods to the input. Experimental results show that parameter\nname hallucination failure primarily stems from inherent LLM limitations, while\nissues with input sources mainly cause other failure patterns. To improve the\nreliability and effectiveness of tool-agent interactions, we propose\ncorresponding improvement suggestions, including standardizing tool return\nformats, improving error feedback mechanisms, and ensuring parameter\nconsistency.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5de5\u5177\u4ee3\u7406\u8303\u5f0f\u4e2d\u7684\u53c2\u6570\u5931\u6548\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u6cd5\u5e76\u5206\u6790\u4e86\u8f93\u5165\u6e90\u4e0e\u5931\u6548\u7c7b\u522b\u7684\u76f8\u5173\u6027\uff0c\u6700\u7ec8\u63d0\u51fa\u4e86\u63d0\u9ad8\u4ea4\u4e92\u53ef\u9760\u6027\u7684\u5efa\u8bae\u3002", "motivation": "\u5de5\u5177\u4ee3\u7406\u8303\u5f0f\u7684\u51fa\u73b0\u62d3\u5bbd\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u4f46\u7531\u4e8e\u6267\u884c\u8fc7\u7a0b\u4e2d\u7684\u53c2\u6570\u5931\u6548\u95ee\u9898\uff0c\u5176\u6709\u6548\u6027\u53d7\u5230\u9650\u5236\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u8fd9\u4e00\u73b0\u8c61\u5e76\u63d0\u51fa\u76f8\u5e94\u5efa\u8bae\u3002", "method": "\u672c\u6587\u9996\u5148\u6784\u5efa\u4e86\u53c2\u6570\u5931\u6548\u7684\u5206\u7c7b\u6cd5\uff0c\u4ece\u4e3b\u6d41\u5de5\u5177\u4ee3\u7406\u7684\u8c03\u7528\u94fe\u4e2d\u63a8\u5bfc\u51fa\u4e94\u7c7b\u5931\u6548\u7c7b\u522b\u3002\u7136\u540e\uff0c\u901a\u8fc7\u5e94\u752815\u79cd\u8f93\u5165\u6270\u52a8\u65b9\u6cd5\uff0c\u63a2\u7d22\u4e86\u4e09\u79cd\u4e0d\u540c\u8f93\u5165\u6e90\u4e0e\u5931\u6548\u7c7b\u522b\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u53c2\u6570\u540d\u79f0\u5e7b\u89c9\u5931\u6548\u4e3b\u8981\u6e90\u4e8eLLM\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u800c\u5176\u4ed6\u5931\u6548\u6a21\u5f0f\u4e3b\u8981\u7531\u8f93\u5165\u6e90\u95ee\u9898\u5f15\u8d77\u3002", "conclusion": "\u4e3a\u63d0\u9ad8\u5de5\u5177\u4ee3\u7406\u4ea4\u4e92\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u6807\u51c6\u5316\u5de5\u5177\u8fd4\u56de\u683c\u5f0f\u3001\u6539\u8fdb\u9519\u8bef\u53cd\u9988\u673a\u5236\u548c\u786e\u4fdd\u53c2\u6570\u4e00\u81f4\u6027\u7b49\u6539\u8fdb\u5efa\u8bae\u3002"}}
{"id": "2507.15088", "categories": ["cs.RO", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.15088", "abs": "https://arxiv.org/abs/2507.15088", "authors": ["Pouya Panahandeh", "Mohammad Pirani", "Baris Fidan", "Amir Khajepour"], "title": "Search-Based Autonomous Vehicle Motion Planning Using Game Theory", "comment": null, "summary": "In this paper, we propose a search-based interactive motion planning scheme\nfor autonomous vehicles (AVs), using a game-theoretic approach. In contrast to\ntraditional search-based approaches, the newly developed approach considers\nother road users (e.g. drivers and pedestrians) as intelligent agents rather\nthan static obstacles. This leads to the generation of a more realistic path\nfor the AV. Due to the low computational time, the proposed motion planning\nscheme is implementable in real-time applications. The performance of the\ndeveloped motion planning scheme is compared with existing motion planning\ntechniques and validated through experiments using WATonoBus, an electrical\nall-weather autonomous shuttle bus.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\uff0c\u5c06\u5176\u4ed6\u9053\u8def\u7528\u6237\u89c6\u4e3a\u667a\u80fd\u4f53\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u641c\u7d22\u5f0f\u65b9\u6cd5\u672a\u8003\u8651\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u7684\u667a\u80fd\u884c\u4e3a\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8def\u5f84\u4e0d\u591f\u771f\u5b9e\u3002", "method": "\u91c7\u7528\u535a\u5f08\u8bba\u65b9\u6cd5\uff0c\u5c06\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u89c6\u4e3a\u667a\u80fd\u4f53\u800c\u975e\u9759\u6001\u969c\u788d\u7269\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u641c\u7d22\u5f0f\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\u3002", "result": "\u65b0\u65b9\u6848\u5728\u8ba1\u7b97\u65f6\u95f4\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u641c\u7d22\u7684\u4ea4\u4e92\u5f0f\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2507.15106", "categories": ["cs.AI", "cs.RO", "F.2.2"], "pdf": "https://arxiv.org/pdf/2507.15106", "abs": "https://arxiv.org/abs/2507.15106", "authors": ["Xia Xu", "Jochen Triesch"], "title": "From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward", "comment": "13 pages, 5 figures", "summary": "While human infants robustly discover their own causal efficacy, standard\nreinforcement learning agents remain brittle, as their reliance on\ncorrelation-based rewards fails in noisy, ecologically valid scenarios. To\naddress this, we introduce the Causal Action Influence Score (CAIS), a novel\nintrinsic reward rooted in causal inference. CAIS quantifies an action's\ninfluence by measuring the 1-Wasserstein distance between the learned\ndistribution of sensory outcomes conditional on that action, $p(h|a)$, and the\nbaseline outcome distribution, $p(h)$. This divergence provides a robust reward\nthat isolates the agent's causal impact from confounding environmental noise.\nWe test our approach in a simulated infant-mobile environment where\ncorrelation-based perceptual rewards fail completely when the mobile is\nsubjected to external forces. In stark contrast, CAIS enables the agent to\nfilter this noise, identify its influence, and learn the correct policy.\nFurthermore, the high-quality predictive model learned for CAIS allows our\nagent, when augmented with a surprise signal, to successfully reproduce the\n\"extinction burst\" phenomenon. We conclude that explicitly inferring causality\nis a crucial mechanism for developing a robust sense of agency, offering a\npsychologically plausible framework for more adaptive autonomous systems.", "AI": {"tldr": "\u63d0\u51faCAIS\u4f5c\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u901a\u8fc7\u56e0\u679c\u63a8\u65ad\u91cf\u5316\u884c\u52a8\u5f71\u54cd\uff0c\u4f7f\u4ee3\u7406\u5728\u5608\u6742\u73af\u5883\u4e2d\u7a33\u5065\u5b66\u4e60\uff0c\u5e76\u91cd\u73b0\u5fc3\u7406\u5b66\u73b0\u8c61\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u5608\u6742\u3001\u751f\u6001\u6709\u6548\u573a\u666f\u4e2d\u4f9d\u8d56\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u5956\u52b1\u800c\u8868\u73b0\u8106\u5f31\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5185\u5728\u5956\u52b1\u2014\u2014\u56e0\u679c\u884c\u52a8\u5f71\u54cd\u5206\u6570\uff08CAIS\uff09\uff0c\u901a\u8fc7\u6d4b\u91cf\u57fa\u4e8e\u884c\u52a8\u7684\u6761\u4ef6\u611f\u5b98\u7ed3\u679c\u5206\u5e03\u4e0e\u57fa\u7ebf\u7ed3\u679c\u5206\u5e03\u4e4b\u95f4\u76841-Wasserstein\u8ddd\u79bb\u6765\u91cf\u5316\u884c\u52a8\u7684\u5f71\u54cd\u3002", "result": "CAIS\u4f7f\u4ee3\u7406\u80fd\u591f\u8fc7\u6ee4\u566a\u97f3\u3001\u8bc6\u522b\u5176\u5f71\u54cd\u5e76\u5b66\u4e60\u6b63\u786e\u7684\u7b56\u7565\uff0c\u4e14\u5728\u589e\u5f3a\u60ca\u559c\u4fe1\u53f7\u540e\u6210\u529f\u91cd\u73b0\u4e86\u201c\u706d\u7edd\u7206\u53d1\u201d\u73b0\u8c61\u3002", "conclusion": "\u660e\u786e\u63a8\u65ad\u56e0\u679c\u5173\u7cfb\u662f\u53d1\u5c55\u5f3a\u5927\u4ee3\u7406\u611f\u7684\u5173\u952e\u673a\u5236\uff0c\u4e3a\u66f4\u81ea\u9002\u5e94\u7684\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5fc3\u7406\u4e0a\u5408\u7406\u7684\u6846\u67b6\u3002"}}
{"id": "2507.15343", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15343", "abs": "https://arxiv.org/abs/2507.15343", "authors": ["Kechi Zhang", "Ge Li", "Jia Li", "Huangzhao Zhang", "Yihong Dong", "Jia Li", "Jingjing Xu", "Zhi Jin"], "title": "StackTrans: From Large Language Model to Large Pushdown Automata Model", "comment": "currently under development", "summary": "The Transformer architecture has emerged as a landmark advancement within the\nbroad field of artificial intelligence, effectively catalyzing the advent of\nlarge language models (LLMs). However, despite its remarkable capabilities and\nthe substantial progress it has facilitated, the Transformer architecture still\nhas some limitations. One such intrinsic limitation is its inability to\neffectively capture the Chomsky hierarchy, such as regular expressions or\ndeterministic context-free grammars. Drawing inspiration from pushdown\nautomata, which efficiently resolve deterministic context-free grammars using\nstacks, we propose StackTrans to address the aforementioned issue within LLMs.\nUnlike previous approaches that modify the attention computation, StackTrans\nexplicitly incorporates hidden state stacks between Transformer layers. This\ndesign maintains compatibility with existing frameworks like flash-attention.\nSpecifically, our design features stack operations -- such as pushing and\npopping hidden states -- that are differentiable and can be learned in an\nend-to-end manner. Our comprehensive evaluation spans benchmarks for both\nChomsky hierarchies and large-scale natural languages. Across these diverse\ntasks, StackTrans consistently outperforms standard Transformer models and\nother baselines. We have successfully scaled StackTrans up from 360M to 7B\nparameters. In particular, our from-scratch pretrained model StackTrans-360M\noutperforms several larger open-source LLMs with 2-3x more parameters,\nshowcasing its superior efficiency and reasoning capability.", "AI": {"tldr": "StackTrans\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u9690\u85cf\u72b6\u6001\u6808\uff0c\u89e3\u51b3\u4e86Transformer\u5728\u6355\u83b7Chomsky\u5c42\u6b21\u7ed3\u6784\u65f6\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u662f\u53c2\u6570\u6548\u7387\u663e\u8457\u3002", "motivation": "\u5c3d\u7ba1Transformer\u67b6\u6784\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u6355\u83b7Chomsky\u5c42\u6b21\u7ed3\u6784\u65b9\u9762\u5b58\u5728\u56fa\u6709\u5c40\u9650\u6027\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63d0\u51faStackTrans\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "StackTrans\u5728Transformer\u5c42\u95f4\u663e\u5f0f\u5f15\u5165\u53ef\u5fae\u7684\u9690\u85cf\u72b6\u6001\u6808\u64cd\u4f5c\uff08\u5982\u538b\u5165\u548c\u5f39\u51fa\uff09\uff0c\u4fdd\u6301\u4e0e\u73b0\u6709\u6846\u67b6\uff08\u5982flash-attention\uff09\u7684\u517c\u5bb9\u6027\uff0c\u5e76\u901a\u8fc7\u7aef\u5230\u7aef\u5b66\u4e60\u4f18\u5316\u3002", "result": "StackTrans\u5728Chomsky\u5c42\u6b21\u7ed3\u6784\u548c\u5927\u89c4\u6a21\u81ea\u7136\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u6807\u51c6Transformer\u53ca\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14StackTrans-360M\u5728\u53c2\u6570\u6548\u7387\u4e0a\u8d85\u8d8a\u4e86\u8bb8\u591a\u53c2\u6570\u66f4\u5927\u7684\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "StackTrans\u901a\u8fc7\u5f15\u5165\u9690\u85cf\u72b6\u6001\u6808\u6709\u6548\u89e3\u51b3\u4e86Transformer\u67b6\u6784\u5728\u6355\u83b7Chomsky\u5c42\u6b21\u7ed3\u6784\uff08\u5982\u6b63\u5219\u8868\u8fbe\u5f0f\u6216\u786e\u5b9a\u6027\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\uff09\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6Transformer\u53ca\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2507.15155", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15155", "abs": "https://arxiv.org/abs/2507.15155", "authors": ["Majid Roshanfar", "Alex Zhang", "Changyan He", "Amir Hooshiar", "Dale J. Podolsky", "Thomas Looi", "Eric Diller"], "title": "Learning-Based Modeling of a Magnetically Steerable Soft Suction Device for Endoscopic Endonasal Interventions", "comment": null, "summary": "This letter introduces a novel learning-based modeling framework for a\nmagnetically steerable soft suction device designed for endoscopic endonasal\nbrain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm\ninner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material,\nand integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape\nfeedback. Shape reconstruction is represented using four Bezier control points,\nenabling a compact and smooth model of the device's deformation. A data-driven\nmodel was trained on 5,097 experimental samples covering a range of magnetic\nfield magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical\ntip distances (90-100 mm), using both Neural Network (NN) and Random Forest\n(RF) architectures. The RF model outperformed the NN across all metrics,\nachieving a mean root mean square error of 0.087 mm in control point prediction\nand a mean shape reconstruction error of 0.064 mm. Feature importance analysis\nfurther revealed that magnetic field components predominantly influence distal\ncontrol points, while frequency and distance affect the base configuration.\nThis learning-based approach effectively models the complex nonlinear behavior\nof hyperelastic soft robots under magnetic actuation without relying on\nsimplified physical assumptions. By enabling sub-millimeter shape prediction\naccuracy and real-time inference, this work represents an advancement toward\nthe intelligent control of magnetically actuated soft robotic tools in\nminimally invasive neurosurgery.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u78c1\u9a71\u52a8\u8f6f\u5438\u5f15\u88c5\u7f6e\u5efa\u6a21\u6846\u67b6\uff0c\u4f7f\u7528RF\u6a21\u578b\u5b9e\u73b0\u4e86\u4e9a\u6beb\u7c73\u7ea7\u7684\u5f62\u72b6\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e3a\u5fae\u521b\u795e\u7ecf\u5916\u79d1\u624b\u672f\u4e2d\u7684\u667a\u80fd\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u78c1\u9a71\u52a8\u8f6f\u5438\u5f15\u88c5\u7f6e\uff0c\u4ee5\u652f\u6301\u5185\u7aa5\u955c\u9f3b\u5185\u8111\u80bf\u7624\u5207\u9664\u624b\u672f\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u5f62\u72b6\u9884\u6d4b\u548c\u5b9e\u65f6\u53cd\u9988\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff0c\u57fa\u4e8e5097\u4e2a\u5b9e\u9a8c\u6837\u672c\u8bad\u7ec3\u4e86\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u548c\u968f\u673a\u68ee\u6797\uff08RF\uff09\u67b6\u6784\uff0c\u8986\u76d6\u4e86\u78c1\u573a\u5f3a\u5ea6\uff080-14 mT\uff09\u3001\u9a71\u52a8\u9891\u7387\uff080.2-1.0 Hz\uff09\u548c\u5782\u76f4\u5c16\u7aef\u8ddd\u79bb\uff0890-100 mm\uff09\u7b49\u53c2\u6570\u3002RF\u6a21\u578b\u5728\u63a7\u5236\u70b9\u9884\u6d4b\u548c\u5f62\u72b6\u91cd\u5efa\u8bef\u5dee\u4e0a\u5747\u4f18\u4e8eNN\u3002", "result": "RF\u6a21\u578b\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u4f18\u4e8eNN\uff0c\u63a7\u5236\u70b9\u9884\u6d4b\u7684\u5e73\u5747\u5747\u65b9\u6839\u8bef\u5dee\u4e3a0.087 mm\uff0c\u5f62\u72b6\u91cd\u5efa\u7684\u5e73\u5747\u8bef\u5dee\u4e3a0.064 mm\u3002\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u663e\u793a\uff0c\u78c1\u573a\u5206\u91cf\u4e3b\u8981\u5f71\u54cd\u8fdc\u7aef\u63a7\u5236\u70b9\uff0c\u800c\u9891\u7387\u548c\u8ddd\u79bb\u5f71\u54cd\u57fa\u7840\u914d\u7f6e\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u6709\u6548\u5efa\u6a21\u4e86\u8d85\u5f39\u6027\u8f6f\u673a\u5668\u4eba\u5728\u78c1\u9a71\u52a8\u4e0b\u7684\u590d\u6742\u975e\u7ebf\u6027\u884c\u4e3a\uff0c\u65e0\u9700\u4f9d\u8d56\u7b80\u5316\u7684\u7269\u7406\u5047\u8bbe\uff0c\u5b9e\u73b0\u4e86\u4e9a\u6beb\u7c73\u7ea7\u7684\u5f62\u72b6\u9884\u6d4b\u7cbe\u5ea6\u548c\u5b9e\u65f6\u63a8\u7406\uff0c\u4e3a\u5fae\u521b\u795e\u7ecf\u5916\u79d1\u4e2d\u78c1\u9a71\u52a8\u8f6f\u673a\u5668\u4eba\u5de5\u5177\u7684\u667a\u80fd\u63a7\u5236\u63d0\u4f9b\u4e86\u8fdb\u5c55\u3002"}}
{"id": "2507.15120", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.15120", "abs": "https://arxiv.org/abs/2507.15120", "authors": ["Stefan Borgwardt", "Duy Nhu", "Gabriele R\u00f6ger"], "title": "Automated planning with ontologies under coherence update semantics", "comment": null, "summary": "Standard automated planning employs first-order formulas under closed-world\nsemantics to achieve a goal with a given set of actions from an initial state.\nWe follow a line of research that aims to incorporate background knowledge into\nautomated planning problems, for example, by means of ontologies, which are\nusually interpreted under open-world semantics. We present a new approach for\nplanning with DL-Lite ontologies that combines the advantages of ontology-based\naction conditions provided by explicit-input knowledge and action bases (eKABs)\nand ontology-aware action effects under the coherence update semantics. We show\nthat the complexity of the resulting formalism is not higher than that of\nprevious approaches and provide an implementation via a polynomial compilation\ninto classical planning. An evaluation of existing and new benchmarks examines\nthe performance of a planning system on different variants of our compilation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DL-Lite\u672c\u4f53\u548c\u81ea\u52a8\u5316\u89c4\u5212\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u8bd1\u5b9e\u73b0\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u5c06\u80cc\u666f\u77e5\u8bc6\uff08\u5982\u672c\u4f53\u8bba\uff09\u878d\u5165\u81ea\u52a8\u5316\u89c4\u5212\u95ee\u9898\uff0c\u4ee5\u589e\u5f3a\u89c4\u5212\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u9879\u5f0f\u7f16\u8bd1\u5c06\u672c\u4f53\u611f\u77e5\u7684\u52a8\u4f5c\u6548\u679c\u548c\u663e\u5f0f\u8f93\u5165\u77e5\u8bc6\u7ed3\u5408\u5230\u7ecf\u5178\u89c4\u5212\u4e2d\u3002", "result": "\u901a\u8fc7\u7f16\u8bd1\u65b9\u6cd5\u5b9e\u73b0\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u7f16\u8bd1\u53d8\u4f53\u5728\u89c4\u5212\u7cfb\u7edf\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DL-Lite\u672c\u4f53\u548c\u81ea\u52a8\u5316\u89c4\u5212\u7684\u65b0\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u590d\u6742\u5ea6\u4e0d\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7f16\u8bd1\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2507.15599", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15599", "abs": "https://arxiv.org/abs/2507.15599", "authors": ["Manatsawin Hanmongkolchai"], "title": "Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing", "comment": null, "summary": "Large language models for code (Code LLM) are increasingly utilized in\nprogramming environments. Despite their utility, the training datasets for top\nLLM remain undisclosed, raising concerns about potential copyright violations.\nSome models, such as Pleias and Comma put emphasis on data curation and\nlicenses, however, with limited training data these models are not competitive\nand only serve as proof of concepts. To improve the utility of these models, we\npropose an application of the \"Chinese Wall\" technique, inspired by the reverse\nengineering technique of the same name -- a high quality model is used to\ngenerate detailed instructions for a weaker model. By doing so, a weaker but\nethically aligned model may be used to perform complicated tasks that,\notherwise, can only be completed by more powerful models. In our evaluation,\nwe've found that this technique improves Comma v0.1 1T's performance in\nCanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%\ncompared to when running the same model on the benchmark alone. The practical\napplication of this technique today, however, may be limited due to the lack of\nmodels trained on public domain content without copyright restrictions.", "AI": {"tldr": "Proposes 'Chinese Wall' technique to boost weaker, ethically aligned Code LLMs' performance, showing significant improvements but limited by lack of copyright-free training models.", "motivation": "Address concerns about copyright violations in training datasets for Code LLMs and enhance the utility of ethically aligned but weaker models.", "method": "Application of the 'Chinese Wall' technique, where a high-quality model generates detailed instructions for a weaker model to improve its performance.", "result": "Comma v0.1 1T's performance improved by over 66%, and Starcoder2 Instruct by roughly 20% in the CanItEdit benchmark.", "conclusion": "The 'Chinese Wall' technique significantly enhances the performance of weaker, ethically aligned models, though current application is limited by the scarcity of models trained on copyright-free public domain content."}}
{"id": "2507.15189", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15189", "abs": "https://arxiv.org/abs/2507.15189", "authors": ["Kevin Christiansen Marsim", "Jinwoo Jeon", "Yeeun Kim", "Myeongwoo Jeong", "Hyun Myung"], "title": "CHADET: Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer", "comment": null, "summary": "Depth information which specifies the distance between objects and current\nposition of the robot is essential for many robot tasks such as navigation.\nRecently, researchers have proposed depth completion frameworks to provide\ndense depth maps that offer comprehensive information about the surrounding\nenvironment. However, existing methods show significant trade-offs between\ncomputational efficiency and accuracy during inference. The substantial memory\nand computational requirements make them unsuitable for real-time applications,\nhighlighting the need to improve the completeness and accuracy of depth\ninformation while improving processing speed to enhance robot performance in\nvarious tasks. To address these challenges, in this paper, we propose\nCHADET(cross-hierarchical-attention depth-completion transformer), a\nlightweight depth-completion network that can generate accurate dense depth\nmaps from RGB images and sparse depth points. For each pair, its feature is\nextracted from the depthwise blocks and passed to the equally lightweight\ntransformer-based decoder. In the decoder, we utilize the novel\ncross-hierarchical-attention module that refines the image features from the\ndepth information. Our approach improves the quality and reduces memory usage\nof the depth map prediction, as validated in both KITTI, NYUv2, and VOID\ndatasets.", "AI": {"tldr": "CHADET\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u8865\u5168\u7f51\u7edc\uff0c\u7ed3\u5408\u8de8\u5c42\u6b21\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u9ad8\u6548\u751f\u6210\u7cbe\u786e\u7684\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u673a\u5668\u4eba\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u8865\u5168\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6743\u8861\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\uff0c\u9700\u63d0\u5347\u6df1\u5ea6\u4fe1\u606f\u7684\u5b8c\u6574\u6027\u548c\u51c6\u786e\u6027\u540c\u65f6\u52a0\u5feb\u5904\u7406\u901f\u5ea6\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5757\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u57fa\u4e8eTransformer\u7684\u89e3\u7801\u5668\u5904\u7406\uff0c\u5229\u7528\u8de8\u5c42\u6b21\u6ce8\u610f\u529b\u6a21\u5757\u7ec6\u5316\u56fe\u50cf\u7279\u5f81\u3002", "result": "\u5728KITTI\u3001NYUv2\u548cVOID\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86CHADET\u5728\u63d0\u5347\u6df1\u5ea6\u56fe\u8d28\u91cf\u548c\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "CHADET \u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6df1\u5ea6\u8865\u5168\u7f51\u7edc\uff0c\u901a\u8fc7\u8de8\u5c42\u6b21\u6ce8\u610f\u529b\u6a21\u5757\u4f18\u5316\u56fe\u50cf\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u56fe\u9884\u6d4b\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u673a\u5668\u4eba\u5e94\u7528\u3002"}}
{"id": "2507.15140", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15140", "abs": "https://arxiv.org/abs/2507.15140", "authors": ["Mohammad Mashayekhi", "Sara Ahmadi Majd", "Arian AmirAmjadi", "Parsa Hosseini"], "title": "Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis", "comment": null, "summary": "The diagnosis of oral diseases presents a problematic clinical challenge,\ncharacterized by a wide spectrum of pathologies with overlapping\nsymptomatology. To address this, we developed Clinical Semantic Intelligence\n(CSI), a novel artificial intelligence framework that diagnoses 118 different\noral diseases by computationally modeling the cognitive processes of an expert\nclinician. Our core hypothesis is that moving beyond simple pattern matching to\nemulate expert reasoning is critical to building clinically useful diagnostic\naids.\n  CSI's architecture integrates a fine-tuned multimodal CLIP model with a\nspecialized ChatGLM-6B language model. This system executes a Hierarchical\nDiagnostic Reasoning Tree (HDRT), a structured framework that distills the\nsystematic, multi-step logic of differential diagnosis. The framework operates\nin two modes: a Fast Mode for rapid screening and a Standard Mode that\nleverages the full HDRT for an interactive and in-depth diagnostic workup.\n  To train and validate our system, we curated a primary dataset of 4,310\nimages, supplemented by an external hold-out set of 176 images for final\nvalidation. A clinically-informed augmentation strategy expanded our training\ndata to over 30,000 image-text pairs. On a 431-image internal test set, CSI's\nFast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the\nHDRT-driven Standard Mode. The performance gain is directly attributable to the\nhierarchical reasoning process. Herein, we detail the architectural philosophy,\ndevelopment, and rigorous evaluation of the CSI framework.", "AI": {"tldr": "CSI\u662f\u4e00\u79cd\u65b0\u578bAI\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4e13\u5bb6\u4e34\u5e8a\u63a8\u7406\u8bca\u65ad118\u79cd\u53e3\u8154\u75be\u75c5\uff0c\u6807\u51c6\u6a21\u5f0f\u51c6\u786e\u7387\u8fbe89.5%\u3002", "motivation": "\u53e3\u8154\u75be\u75c5\u8bca\u65ad\u5b58\u5728\u75c7\u72b6\u91cd\u53e0\u7684\u4e34\u5e8a\u6311\u6218\uff0c\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u7684\u6a21\u5f0f\u5339\u914d\uff0c\u6a21\u62df\u4e13\u5bb6\u63a8\u7406\u8fc7\u7a0b\u4ee5\u5f00\u53d1\u5b9e\u7528\u7684\u4e34\u5e8a\u8bca\u65ad\u8f85\u52a9\u5de5\u5177\u3002", "method": "CSI\u6846\u67b6\u6574\u5408\u4e86\u5fae\u8c03\u7684\u591a\u6a21\u6001CLIP\u6a21\u578b\u548c\u4e13\u95e8\u7684ChatGLM-6B\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u5206\u5c42\u8bca\u65ad\u63a8\u7406\u6811\uff08HDRT\uff09\u8fdb\u884c\u7cfb\u7edf\u6027\u3001\u591a\u6b65\u9aa4\u7684\u903b\u8f91\u63a8\u7406\uff0c\u63d0\u4f9b\u5feb\u901f\u7b5b\u67e5\u548c\u6807\u51c6\u6a21\u5f0f\u4e24\u79cd\u8bca\u65ad\u65b9\u5f0f\u3002", "result": "\u5728431\u5f20\u5185\u90e8\u6d4b\u8bd5\u56fe\u50cf\u4e0a\uff0cCSI\u7684\u5feb\u901f\u6a21\u5f0f\u51c6\u786e\u7387\u4e3a73.4%\uff0c\u6807\u51c6\u6a21\u5f0f\u51c6\u786e\u7387\u63d0\u5347\u81f389.5%\uff0c\u6027\u80fd\u63d0\u5347\u76f4\u63a5\u5f52\u56e0\u4e8e\u5206\u5c42\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "CSI\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4e13\u5bb6\u4e34\u5e8a\u63a8\u7406\u8fc7\u7a0b\uff0c\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u8bca\u65ad118\u79cd\u53e3\u8154\u75be\u75c5\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff0c\u5176\u6807\u51c6\u6a21\u5f0f\u7684\u51c6\u786e\u7387\u9ad8\u8fbe89.5%\uff0c\u9a8c\u8bc1\u4e86\u5206\u5c42\u8bca\u65ad\u63a8\u7406\u6811\uff08HDRT\uff09\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.15624", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15624", "abs": "https://arxiv.org/abs/2507.15624", "authors": ["Yusuf Sulistyo Nugroho", "Ganno Tribuana Kurniaji", "Syful Islam", "Mohammed Humayun Kabir", "Vanesya Aura Ardity", "Md. Kamal Uddin"], "title": "Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow", "comment": "6 pages, 4 figures, 4 tables, conference paper", "summary": "React is a JavaScript library used to build user interfaces for single-page\napplications. Although recent studies have shown the popularity and advantages\nof React in web development, the specific challenges users face remain unknown.\nThus, this study aims to analyse the React-related questions shared on Stack\nOverflow. The study utilizes an exploratory data analysis to investigate the\nmost frequently discussed keywords, error classification, and user\nreputation-based errors, which is the novelty of this work. The results show\nthe top eight most frequently used keywords on React-related questions, namely,\ncode, link, vir, href, connect, azure, windows, and website. The error\nclassification of questions from the sample shows that algorithmic error is the\nmost frequent issue faced by all groups of users, where mid-reputation users\ncontribute the most, accounting for 55.77%. This suggests the need for the\ncommunity to provide guidance materials in solving algorithm-related problems.\nWe expect that the results of this study will provide valuable insight into\nfuture research to support the React community during the early stages of\nimplementation, facilitating their ability to effectively overcome challenges\nto adoption.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86Stack Overflow\u4e0aReact\u76f8\u5173\u7684\u95ee\u9898\uff0c\u53d1\u73b0\u7b97\u6cd5\u9519\u8bef\u662f\u6700\u5e38\u89c1\u7684\u6311\u6218\uff0c\u4e2d\u7b49\u58f0\u8a89\u7528\u6237\u8d21\u732e\u6700\u591a\u3002\u7ed3\u679c\u4e3aReact\u793e\u533a\u63d0\u4f9b\u4e86\u65e9\u671f\u5b9e\u65bd\u9636\u6bb5\u7684\u6307\u5bfc\u5efa\u8bae\u3002", "motivation": "\u5c3d\u7ba1React\u5728Web\u5f00\u53d1\u4e2d\u7684\u6d41\u884c\u5ea6\u548c\u4f18\u52bf\u5df2\u5f97\u5230\u8bc1\u5b9e\uff0c\u4f46\u7528\u6237\u9762\u4e34\u7684\u5177\u4f53\u6311\u6218\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u6790Stack Overflow\u4e0a\u7684React\u76f8\u5173\u95ee\u9898\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\u65b9\u6cd5\uff0c\u8c03\u67e5\u4e86React\u76f8\u5173\u95ee\u9898\u4e2d\u6700\u5e38\u8ba8\u8bba\u7684\u5173\u952e\u8bcd\u3001\u9519\u8bef\u5206\u7c7b\u4ee5\u53ca\u57fa\u4e8e\u7528\u6237\u58f0\u8a89\u7684\u9519\u8bef\u5206\u5e03\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cReact\u76f8\u5173\u95ee\u9898\u4e2d\u6700\u5e38\u4f7f\u7528\u7684\u516b\u4e2a\u5173\u952e\u8bcd\u662fcode\u3001link\u3001vir\u3001href\u3001connect\u3001azure\u3001windows\u548cwebsite\u3002\u9519\u8bef\u5206\u7c7b\u8868\u660e\uff0c\u7b97\u6cd5\u9519\u8bef\u662f\u6240\u6709\u7528\u6237\u7fa4\u4f53\u4e2d\u6700\u5e38\u89c1\u7684\u95ee\u9898\uff0c\u5176\u4e2d\u4e2d\u7b49\u58f0\u8a89\u7528\u6237\u8d21\u732e\u6700\u591a\uff0c\u5360\u6bd455.77%\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790Stack Overflow\u4e0aReact\u76f8\u5173\u95ee\u9898\u7684\u6570\u636e\uff0c\u63ed\u793a\u4e86React\u7528\u6237\u5728\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u4e3b\u8981\u6311\u6218\uff0c\u7279\u522b\u662f\u7b97\u6cd5\u9519\u8bef\u662f\u6700\u5e38\u89c1\u7684\u95ee\u9898\u3002\u7814\u7a76\u7ed3\u679c\u5bf9React\u793e\u533a\u5728\u65e9\u671f\u5b9e\u65bd\u9636\u6bb5\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u6709\u6548\u5730\u514b\u670d\u91c7\u7528\u8fc7\u7a0b\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2507.15266", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.15266", "abs": "https://arxiv.org/abs/2507.15266", "authors": ["Haichao Liu", "Haoren Guo", "Pei Liu", "Benshan Ma", "Yuxiang Zhang", "Jun Ma", "Tong Heng Lee"], "title": "VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving", "comment": "14 pages, 12 figures", "summary": "Scene understanding and risk-aware attentions are crucial for human drivers\nto make safe and effective driving decisions. To imitate this cognitive ability\nin urban autonomous driving while ensuring the transparency and\ninterpretability, we propose a vision-language model (VLM)-enhanced unified\ndecision-making and motion control framework, named VLM-UDMC. This framework\nincorporates scene reasoning and risk-aware insights into an upper-level slow\nsystem, which dynamically reconfigures the optimal motion planning for the\ndownstream fast system. The reconfiguration is based on real-time environmental\nchanges, which are encoded through context-aware potential functions. More\nspecifically, the upper-level slow system employs a two-step reasoning policy\nwith Retrieval-Augmented Generation (RAG), leveraging foundation models to\nprocess multimodal inputs and retrieve contextual knowledge, thereby generating\nrisk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM\nprovides real-time trajectory predictions for heterogeneous traffic\nparticipants by extracting smoother trend representations for short-horizon\ntrajectory prediction. The effectiveness of the proposed VLM-UDMC framework is\nverified via both simulations and real-world experiments with a full-size\nautonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively\nleverages scene understanding and attention decomposition for rational driving\ndecisions, thus improving the overall urban driving performance. Our\nopen-source project is available at https://github.com/henryhcliu/vlmudmc.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86VLM-UDMC\u6846\u67b6\uff0c\u7ed3\u5408VLM\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7684\u900f\u660e\u6027\u548c\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u57ce\u5e02\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u6a21\u4eff\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u8ba4\u77e5\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e86\u4e24\u6b65\u63a8\u7406\u7b56\u7565\uff08RAG\uff09\u548c\u8f7b\u91cf\u7ea7\u591a\u6838\u5206\u89e3LSTM\uff0c\u5206\u522b\u7528\u4e8e\u751f\u6210\u98ce\u9669\u611f\u77e5\u6d1e\u5bdf\u548c\u5b9e\u65f6\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u8f66\u5b9e\u9a8c\u9a8c\u8bc1\u4e86VLM-UDMC\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u9a7e\u9a76\u51b3\u7b56\u7684\u5408\u7406\u6027\u3002", "conclusion": "VLM-UDMC\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u573a\u666f\u7406\u89e3\u548c\u98ce\u9669\u611f\u77e5\u6ce8\u610f\u529b\uff0c\u63d0\u5347\u4e86\u57ce\u5e02\u81ea\u52a8\u9a7e\u9a76\u7684\u51b3\u7b56\u548c\u8fd0\u52a8\u63a7\u5236\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.15143", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15143", "abs": "https://arxiv.org/abs/2507.15143", "authors": ["Abderaouf Bahi", "Amel Ourici"], "title": "Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City", "comment": null, "summary": "This paper investigates the feasibility of human mobility in The Line, a\nproposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess\nwhether citizens can move freely within this unprecedented urban topology, we\ndevelop a hybrid simulation framework that integrates agent-based modeling,\nreinforcement learning, supervised learning, and graph neural networks. The\nsimulation captures multi-modal transportation behaviors across 50 vertical\nlevels and varying density scenarios using both synthetic data and real-world\ntraces from high-density cities. Our experiments reveal that with the full\nAI-integrated architecture, agents achieved an average commute time of 7.8 to\n8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index\nof over 91 percent, even during peak congestion periods. Ablation studies\nconfirmed that the removal of intelligent modules such as reinforcement\nlearning or graph neural networks significantly degrades performance, with\ncommute times increasing by up to 85 percent and reachability falling below 70\npercent. Environmental modeling further demonstrated low energy consumption and\nminimal CO2 emissions when electric modes are prioritized. The findings suggest\nthat freedom of movement is not only conceptually achievable in The Line, but\nalso operationally realistic if supported by adaptive AI systems, sustainable\ninfrastructure, and real-time feedback loops.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6df7\u5408\u6a21\u62df\u6846\u67b6\u8bc4\u4f30\u4e86\u6c99\u7279NEOM\u7ebf\u6027\u667a\u80fd\u57ce\u5e02The Line\u4e2d\u4eba\u7c7b\u79fb\u52a8\u7684\u53ef\u884c\u6027\uff0c\u7ed3\u679c\u663e\u793aAI\u96c6\u6210\u7cfb\u7edf\u53ef\u5b9e\u73b0\u9ad8\u6548\u3001\u6ee1\u610f\u7684\u79fb\u52a8\u4f53\u9a8c\u3002", "motivation": "To assess whether citizens can move freely within The Line, a proposed 170-kilometer linear smart city in NEOM, Saudi Arabia.", "method": "A hybrid simulation framework integrating agent-based modeling, reinforcement learning, supervised learning, and graph neural networks.", "result": "With the full AI-integrated architecture, agents achieved an average commute time of 7.8 to 8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index of over 91 percent, even during peak congestion periods.", "conclusion": "The findings suggest that freedom of movement is not only conceptually achievable in The Line, but also operationally realistic if supported by adaptive AI systems, sustainable infrastructure, and real-time feedback loops."}}
{"id": "2507.15663", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15663", "abs": "https://arxiv.org/abs/2507.15663", "authors": ["Giordano d'Aloisio", "Tosin Fadahunsi", "Jay Choy", "Rebecca Moussa", "Federica Sarro"], "title": "SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models", "comment": null, "summary": "Background: Text-to-image generation models are widely used across numerous\ndomains. Among these models, Stable Diffusion (SD) - an open-source\ntext-to-image generation model - has become the most popular, producing over 12\nbillion images annually. However, the widespread use of these models raises\nconcerns regarding their social and environmental sustainability.\n  Aims: To reduce the harm that SD models may have on society and the\nenvironment, we introduce SustainDiffusion, a search-based approach designed to\nenhance the social and environmental sustainability of SD models.\n  Method: SustainDiffusion searches the optimal combination of hyperparameters\nand prompt structures that can reduce gender and ethnic bias in generated\nimages while also lowering the energy consumption required for image\ngeneration. Importantly, SustainDiffusion maintains image quality comparable to\nthat of the original SD model.\n  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,\ntesting it against six different baselines using 56 different prompts. Our\nresults demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,\nethnic bias by 59%, and energy consumption (calculated as the sum of CPU and\nGPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are\nconsistent across multiple runs and can be generalised to various prompts.\n  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social\nand environmental sustainability of text-to-image generation models is possible\nwithout fine-tuning or changing the model's architecture.", "AI": {"tldr": "SustainDiffusion\u901a\u8fc7\u4f18\u5316\u8d85\u53c2\u6570\u548c\u63d0\u793a\u7ed3\u6784\uff0c\u6709\u6548\u51cf\u5c11\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u548c\u80fd\u8017\uff0c\u63d0\u5347\u793e\u4f1a\u548c\u73af\u5883\u53ef\u6301\u7eed\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "Stable Diffusion\uff08SD\uff09\u4f5c\u4e3a\u6d41\u884c\u7684\u5f00\u6e90\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u6bcf\u5e74\u751f\u6210\u8d85\u8fc7120\u4ebf\u5f20\u56fe\u50cf\uff0c\u4f46\u5176\u5e7f\u6cdb\u4f7f\u7528\u5f15\u53d1\u4e86\u793e\u4f1a\u548c\u73af\u5883\u7684\u53ef\u6301\u7eed\u6027\u95ee\u9898\u3002", "method": "SustainDiffusion\u641c\u7d22\u6700\u4f73\u7684\u8d85\u53c2\u6570\u548c\u63d0\u793a\u7ed3\u6784\u7ec4\u5408\uff0c\u4ee5\u51cf\u5c11\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u6027\u522b\u548c\u79cd\u65cf\u504f\u89c1\uff0c\u540c\u65f6\u964d\u4f4e\u56fe\u50cf\u751f\u6210\u6240\u9700\u7684\u80fd\u8017\uff0c\u4e14\u4fdd\u6301\u4e0e\u539fSD\u6a21\u578b\u76f8\u5f53\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "result": "SustainDiffusion\u572856\u4e2a\u4e0d\u540c\u63d0\u793a\u4e0b\u4e0e6\u4e2a\u57fa\u7ebf\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5b83\u80fd\u591f\u5c06SD3\u4e2d\u7684\u6027\u522b\u504f\u89c1\u51cf\u5c1168%\uff0c\u79cd\u65cf\u504f\u89c1\u51cf\u5c1159%\uff0c\u80fd\u8017\uff08CPU\u548cGPU\u80fd\u8017\u603b\u548c\uff09\u964d\u4f4e48%\uff0c\u4e14\u7ed3\u679c\u5177\u6709\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "\u901a\u8fc7SustainDiffusion\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u5728\u65e0\u9700\u5fae\u8c03\u6216\u6539\u53d8\u6a21\u578b\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u589e\u5f3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u793e\u4f1a\u548c\u73af\u5883\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2507.15293", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15293", "abs": "https://arxiv.org/abs/2507.15293", "authors": ["Shanshan Zhang", "Tianshui Wen", "Siyue Wang", "Qi Zhang", "Ziheng Zhou", "Lingxiang Zheng", "Yu Yang"], "title": "RepILN: Reparameterized Inertial Localization Network", "comment": null, "summary": "Inertial localization is regarded as a promising positioning solution for\nconsumer-grade IoT devices due to its cost-effectiveness and independence from\nexternal infrastructure. However, data-driven inertial localization methods\noften rely on increasingly complex network architectures to improve accuracy,\nwhich challenges the limited computational resources of IoT devices. Moreover,\nthese methods frequently overlook the importance of modeling long-term\ndependencies in inertial measurements - a critical factor for accurate\ntrajectory reconstruction - thereby limiting localization performance. To\naddress these challenges, we propose a reparameterized inertial localization\nnetwork that uses a multi-branch structure during training to enhance feature\nextraction. At inference time, this structure is transformed into an equivalent\nsingle-path architecture to improve parameter efficiency. To further capture\nlong-term dependencies in motion trajectories, we introduce a temporal-scale\nsparse attention mechanism that selectively emphasizes key trajectory segments\nwhile suppressing noise. Additionally, a gated convolutional unit is\nincorporated to effectively integrate long-range dependencies with local\nfine-grained features. Extensive experiments on public benchmarks demonstrate\nthat our method achieves a favorable trade-off between accuracy and model\ncompactness. For example, on the RoNIN dataset, our approach reduces the\nAbsolute Trajectory Error (ATE) by 2.59% compared to RoNIN-ResNet while\nreducing the number of parameters by 3.86%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u60ef\u6027\u5b9a\u4f4d\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u5206\u652f\u8bad\u7ec3\u548c\u5355\u8def\u5f84\u63a8\u7406\u4f18\u5316\u53c2\u6570\u6548\u7387\uff0c\u7ed3\u5408\u7a00\u758f\u6ce8\u610f\u529b\u4e0e\u95e8\u63a7\u5377\u79ef\u63d0\u5347\u957f\u65f6\u4f9d\u8d56\u5efa\u6a21\uff0c\u663e\u8457\u964d\u4f4e\u8bef\u5dee\u4e0e\u53c2\u6570\u91cf\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u7684\u60ef\u6027\u5b9a\u4f4d\u65b9\u6cd5\u56e0\u590d\u6742\u7f51\u7edc\u67b6\u6784\u5bf9IoT\u8bbe\u5907\u8ba1\u7b97\u8d44\u6e90\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u5ffd\u89c6\u60ef\u6027\u6d4b\u91cf\u4e2d\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u65b0\u53c2\u6570\u5316\u7684\u60ef\u6027\u5b9a\u4f4d\u7f51\u7edc\uff0c\u91c7\u7528\u591a\u5206\u652f\u7ed3\u6784\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u8f6c\u6362\u4e3a\u5355\u8def\u5f84\u67b6\u6784\u4ee5\u63d0\u9ad8\u53c2\u6570\u6548\u7387\uff1b\u5f15\u5165\u65f6\u95f4\u5c3a\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u548c\u95e8\u63a7\u5377\u79ef\u5355\u5143\u4ee5\u6355\u6349\u957f\u65f6\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728RoNIN\u6570\u636e\u96c6\u4e0a\uff0c\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\u964d\u4f4e\u4e862.59%\uff0c\u53c2\u6570\u6570\u91cf\u51cf\u5c11\u4e863.86%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4e86\u51c6\u786e\u6027\u4e0e\u6a21\u578b\u7b80\u6d01\u6027\u4e4b\u95f4\u7684\u826f\u597d\u5e73\u8861\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\u5e76\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u3002"}}
{"id": "2507.15225", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15225", "abs": "https://arxiv.org/abs/2507.15225", "authors": ["Yichi Zhou", "Jianqiu Zhao", "Yongxin Zhang", "Bohan Wang", "Siran Wang", "Luoxin Chen", "Jiahui Wang", "Haowei Chen", "Allan Jie", "Xinbo Zhang", "Haocheng Wang", "Luong Trung", "Rong Ye", "Phan Nhat Hoang", "Huishuai Zhang", "Peng Sun", "Hang Li"], "title": "Solving Formal Math Problems by Decomposition and Iterative Reflection", "comment": null, "summary": "General-purpose Large Language Models (LLMs) have achieved remarkable success\nin intelligence, performing comparably to human experts on complex reasoning\ntasks such as coding and mathematical reasoning. However, generating formal\nproofs in specialized languages like Lean 4 remains a significant challenge for\nthese models, limiting their application in complex theorem proving and\nautomated verification. Current approaches typically require specializing\nmodels through fine-tuning on dedicated formal corpora, incurring high costs\nfor data collection and training. In this work, we introduce \\textbf{Delta\nProver}, an agent-based framework that orchestrates the interaction between a\ngeneral-purpose LLM and the Lean 4 proof environment. Delta Prover leverages\nthe reflection and reasoning capabilities of general-purpose LLMs to\ninteractively construct formal proofs in Lean 4, circumventing the need for\nmodel specialization. At its core, the agent integrates two novel,\ninterdependent components: an algorithmic framework for reflective\ndecomposition and iterative proof repair, and a custom Domain-Specific Language\n(DSL) built upon Lean 4 for streamlined subproblem management. \\textbf{Delta\nProver achieves a state-of-the-art 95.9\\% success rate on the miniF2F-test\nbenchmark, surpassing all existing approaches, including those requiring model\nspecialization.} Furthermore, Delta Prover exhibits a significantly stronger\ntest-time scaling law compared to standard Best-of-N proof strategies.\nCrucially, our findings demonstrate that general-purpose LLMs, when guided by\nan effective agentic structure, possess substantial untapped theorem-proving\ncapabilities. This presents a computationally efficient alternative to\nspecialized models for robust automated reasoning in formal environments.", "AI": {"tldr": "Delta Prover \u901a\u8fc7\u667a\u80fd\u4f53\u6846\u67b6\u7ed3\u5408\u901a\u7528 LLM \u548c Lean 4 \u73af\u5883\uff0c\u65e0\u9700\u4e13\u4e1a\u5316\u6a21\u578b\u5373\u5b9e\u73b0\u9ad8\u6548\u5b9a\u7406\u8bc1\u660e\uff0c\u5728 miniF2F-test \u4e0a\u8fbe\u5230 95.9% \u6210\u529f\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u901a\u7528 LLM \u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728 Lean 4 \u7b49\u4e13\u4e1a\u8bed\u8a00\u4e2d\u751f\u6210\u5f62\u5f0f\u5316\u8bc1\u660e\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u4f9d\u8d56\u9ad8\u6210\u672c\u7684\u4e13\u4e1a\u5316\u6a21\u578b\u5fae\u8c03\u3002", "method": "Delta Prover \u662f\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u901a\u7528 LLM \u4e0e Lean 4 \u8bc1\u660e\u73af\u5883\u7684\u4ea4\u4e92\uff0c\u7ed3\u5408\u53cd\u5c04\u5206\u89e3\u3001\u8fed\u4ee3\u8bc1\u660e\u4fee\u590d\u7b97\u6cd5\u548c\u5b9a\u5236\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\uff0c\u65e0\u9700\u6a21\u578b\u4e13\u4e1a\u5316\u5373\u53ef\u6784\u5efa\u5f62\u5f0f\u5316\u8bc1\u660e\u3002", "result": "Delta Prover \u5728 miniF2F-test \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230 95.9% \u7684\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u6240\u6709\u73b0\u6709\u65b9\u6cd5\uff08\u5305\u62ec\u9700\u6a21\u578b\u4e13\u4e1a\u5316\u7684\u65b9\u6cd5\uff09\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u89c4\u5f8b\u3002", "conclusion": "Delta Prover \u5c55\u793a\u4e86\u901a\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6709\u6548\u667a\u80fd\u4f53\u7ed3\u6784\u5f15\u5bfc\u4e0b\u5177\u5907\u663e\u8457\u7684\u5b9a\u7406\u8bc1\u660e\u6f5c\u529b\uff0c\u4e3a\u5f62\u5f0f\u5316\u73af\u5883\u4e2d\u7684\u81ea\u52a8\u5316\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u4e13\u4e1a\u6a21\u578b\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.15666", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15666", "abs": "https://arxiv.org/abs/2507.15666", "authors": ["Igor Turkin", "Lina Volobuieva", "Andriy Chukhray", "Oleksandr Liubimov"], "title": "Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches", "comment": "13 pages, 15 figures", "summary": "The subject of the article is the study and comparison of two approaches to\nmodelling the battery discharge of a CubeSat satellite: analytical using\nequivalent circuit and machine learning. The article aims to make a reasoned\nchoice of the approach to modelling the battery discharge of a CubeSat\nsatellite. Modelling the battery discharge of a satellite will enable the\nprediction of the consequences of disconnecting the autonomous power system and\nensure the fault tolerance of equipment in orbit. Therefore, the selected study\nis relevant and promising. This study focuses on the analysis of CubeSat\nsatellite data, based explicitly on orbital data samples of the power system,\nwhich include data available at the time of the article publication. The\ndataset contains data on the voltage, current, and temperature of the battery\nand solar panels attached to the five sides of the satellite. In this context,\ntwo approaches are considered: analytical modelling based on physical laws and\nmachine learning, which uses empirical data to create a predictive model.\nResults: A comparative analysis of the modeling results reveals that the\nequivalent circuit approach has the advantage of transparency, as it identifies\npossible parameters that facilitate understanding of the relationships.\nHowever, the model is less flexible to environmental changes or non-standard\nsatellite behavior. The machine learning model demonstrated more accurate\nresults, as it can account for complex dependencies and adapt to actual\nconditions, even when they deviate from theoretical assumptions.", "AI": {"tldr": "\u6587\u7ae0\u6bd4\u8f83\u4e86\u7b49\u6548\u7535\u8def\u548c\u673a\u5668\u5b66\u4e60\u4e24\u79cd\u65b9\u6cd5\u9884\u6d4bCubeSat\u7535\u6c60\u653e\u7535\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u673a\u5668\u5b66\u4e60\u66f4\u51c6\u786e\u4e14\u9002\u5e94\u6027\u5f3a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u901a\u8fc7\u5efa\u6a21CubeSat\u536b\u661f\u7535\u6c60\u653e\u7535\u6765\u9884\u6d4b\u81ea\u4e3b\u7535\u6e90\u7cfb\u7edf\u65ad\u5f00\u7684\u5f71\u54cd\uff0c\u5e76\u786e\u4fdd\u8f68\u9053\u8bbe\u5907\u7684\u5bb9\u9519\u80fd\u529b\u3002", "method": "\u6587\u7ae0\u91c7\u7528\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u7269\u7406\u5b9a\u5f8b\u7684\u7b49\u6548\u7535\u8def\u5206\u6790\u5efa\u6a21\u548c\u57fa\u4e8e\u7ecf\u9a8c\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u5efa\u6a21\u3002", "result": "\u6bd4\u8f83\u5206\u6790\u7ed3\u679c\u663e\u793a\uff0c\u7b49\u6548\u7535\u8def\u65b9\u6cd5\u900f\u660e\u4f46\u7075\u6d3b\u6027\u5dee\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u66f4\u51c6\u786e\u4e14\u9002\u5e94\u6027\u5f3a\u3002", "conclusion": "\u6587\u7ae0\u5f97\u51fa\u7ed3\u8bba\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4bCubeSat\u536b\u661f\u7535\u6c60\u653e\u7535\u65b9\u9762\u66f4\u4e3a\u51c6\u786e\uff0c\u80fd\u591f\u9002\u5e94\u5b9e\u9645\u6761\u4ef6\uff0c\u800c\u7b49\u6548\u7535\u8def\u65b9\u6cd5\u867d\u7136\u900f\u660e\u4f46\u7075\u6d3b\u6027\u4e0d\u8db3\u3002"}}
{"id": "2507.15444", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15444", "abs": "https://arxiv.org/abs/2507.15444", "authors": ["Leonard Bauersfeld", "Davide Scaramuzza"], "title": "Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe", "comment": "17 pages", "summary": "Autonomous quadrotor flight in confined spaces such as pipes and tunnels\npresents significant challenges due to unsteady, self-induced aerodynamic\ndisturbances. Very recent advances have enabled flight in such conditions, but\nthey either rely on constant motion through the pipe to mitigate airflow\nrecirculation effects or suffer from limited stability during hovering. In this\nwork, we present the first closed-loop control system for quadrotors for\nhovering in narrow pipes that leverages real-time flow field measurements. We\ndevelop a low-latency, event-based smoke velocimetry method that estimates\nlocal airflow at high temporal resolution. This flow information is used by a\ndisturbance estimator based on a recurrent convolutional neural network, which\ninfers force and torque disturbances in real time. The estimated disturbances\nare integrated into a learning-based controller trained via reinforcement\nlearning. The flow-feedback control proves particularly effective during\nlateral translation maneuvers in the pipe cross-section. There, the real-time\ndisturbance information enables the controller to effectively counteract\ntransient aerodynamic effects, thereby preventing collisions with the pipe\nwall. To the best of our knowledge, this work represents the first\ndemonstration of an aerial robot with closed-loop control informed by real-time\nflow field measurements. This opens new directions for research on flight in\naerodynamically complex environments. In addition, our work also sheds light on\nthe characteristic flow structures that emerge during flight in narrow,\ncircular pipes, providing new insights at the intersection of robotics and\nfluid dynamics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u65f6\u6d41\u573a\u6d4b\u91cf\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u95ed\u73af\u63a7\u5236\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f4e\u5ef6\u8fdf\u70df\u96fe\u6d4b\u901f\u548c\u795e\u7ecf\u7f51\u7edc\u5e72\u6270\u4f30\u8ba1\u5668\u5b9e\u73b0\u4e86\u5728\u72ed\u7a84\u7ba1\u9053\u4e2d\u7684\u7a33\u5b9a\u60ac\u505c\u548c\u4fa7\u5411\u5e73\u79fb\u3002", "motivation": "\u5728\u72ed\u7a84\u7ba1\u9053\u4e2d\u98de\u884c\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u9762\u4e34\u7531\u81ea\u8bf1\u5bfc\u6c14\u6d41\u6270\u52a8\u5e26\u6765\u7684\u7a33\u5b9a\u6027\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6301\u7eed\u8fd0\u52a8\u6765\u51cf\u8f7b\u6c14\u6d41\u518d\u5faa\u73af\u6548\u5e94\uff0c\u8981\u4e48\u5728\u60ac\u505c\u65f6\u7a33\u5b9a\u6027\u6709\u9650\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u4f4e\u5ef6\u8fdf\u3001\u57fa\u4e8e\u4e8b\u4ef6\u7684\u70df\u96fe\u6d4b\u901f\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u5c40\u90e8\u6c14\u6d41\u4f30\u8ba1\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u5faa\u73af\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5e72\u6270\u4f30\u8ba1\u5668\u5b9e\u65f6\u63a8\u65ad\u529b\u548c\u626d\u77e9\u5e72\u6270\u3002\u8fd9\u4e9b\u4f30\u8ba1\u7684\u5e72\u6270\u88ab\u6574\u5408\u5230\u4e00\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5b66\u4e60\u578b\u63a7\u5236\u5668\u4e2d\u3002", "result": "\u6d41\u53cd\u9988\u63a7\u5236\u5728\u7ba1\u9053\u6a2a\u622a\u9762\u7684\u4fa7\u5411\u5e73\u79fb\u64cd\u4f5c\u4e2d\u7279\u522b\u6709\u6548\uff0c\u5b9e\u65f6\u5e72\u6270\u4fe1\u606f\u4f7f\u63a7\u5236\u5668\u80fd\u591f\u6709\u6548\u62b5\u6d88\u77ac\u6001\u7a7a\u6c14\u52a8\u529b\u5b66\u6548\u5e94\uff0c\u9632\u6b62\u4e0e\u7ba1\u58c1\u78b0\u649e\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5c55\u793a\u4e86\u57fa\u4e8e\u5b9e\u65f6\u6d41\u573a\u6d4b\u91cf\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u95ed\u73af\u63a7\u5236\u7cfb\u7edf\uff0c\u4e3a\u5728\u7a7a\u6c14\u52a8\u529b\u5b66\u590d\u6742\u73af\u5883\u4e2d\u7684\u98de\u884c\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u63d0\u4f9b\u4e86\u673a\u5668\u4eba\u5b66\u4e0e\u6d41\u4f53\u529b\u5b66\u4ea4\u53c9\u9886\u57df\u7684\u65b0\u89c1\u89e3\u3002"}}
{"id": "2507.15239", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15239", "abs": "https://arxiv.org/abs/2507.15239", "authors": ["Qianchao Wang", "Yuxuan Ding", "Chuanzhen Jia", "Zhe Li", "Yaping Du"], "title": "Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis", "comment": null, "summary": "Novel AI-based arc fault diagnosis models have demonstrated outstanding\nperformance in terms of classification accuracy. However, an inherent problem\nis whether these models can actually be trusted to find arc faults. In this\nlight, this work proposes a soft evaluation indicator that explains the outputs\nof arc fault diagnosis models, by defining the the correct explanation of arc\nfaults and leveraging Explainable Artificial Intelligence and real arc fault\nexperiments. Meanwhile, a lightweight balanced neural network is proposed to\nguarantee competitive accuracy and soft feature extraction score. In our\nexperiments, several traditional machine learning methods and deep learning\nmethods across two arc fault datasets with different sample times and noise\nlevels are utilized to test the effectiveness of the soft evaluation indicator.\nThrough this approach, the arc fault diagnosis models are easy to understand\nand trust, allowing practitioners to make informed and trustworthy decisions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u8f6f\u8bc4\u4f30\u6307\u6807\u548c\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347\u7535\u5f27\u6545\u969c\u8bca\u65ad\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709AI\u7535\u5f27\u6545\u969c\u8bca\u65ad\u6a21\u578b\u5728\u5206\u7c7b\u51c6\u786e\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u53ef\u4fe1\u5ea6\u5b58\u7591\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f7f\u6a21\u578b\u8f93\u51fa\u66f4\u6613\u7406\u89e3\u548c\u4fe1\u4efb\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8f6f\u8bc4\u4f30\u6307\u6807\u6765\u89e3\u91ca\u7535\u5f27\u6545\u969c\u8bca\u65ad\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5e76\u7ed3\u5408\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u548c\u771f\u5b9e\u7535\u5f27\u6545\u969c\u5b9e\u9a8c\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5e73\u8861\u795e\u7ecf\u7f51\u7edc\u4ee5\u4fdd\u8bc1\u7ade\u4e89\u6027\u51c6\u786e\u5ea6\u548c\u8f6f\u7279\u5f81\u63d0\u53d6\u5206\u6570\u3002", "result": "\u5728\u4e24\u4e2a\u4e0d\u540c\u91c7\u6837\u65f6\u95f4\u548c\u566a\u58f0\u6c34\u5e73\u7684\u7535\u5f27\u6545\u969c\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u8f6f\u8bc4\u4f30\u6307\u6807\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u8f6f\u8bc4\u4f30\u6307\u6807\u548c\u8f7b\u91cf\u7ea7\u5e73\u8861\u795e\u7ecf\u7f51\u7edc\uff0c\u8be5\u7814\u7a76\u4f7f\u7535\u5f27\u6545\u969c\u8bca\u65ad\u6a21\u578b\u66f4\u6613\u7406\u89e3\u548c\u53ef\u4fe1\uff0c\u5e2e\u52a9\u4ece\u4e1a\u8005\u505a\u51fa\u660e\u667a\u4e14\u53ef\u9760\u7684\u51b3\u7b56\u3002"}}
{"id": "2507.15671", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15671", "abs": "https://arxiv.org/abs/2507.15671", "authors": ["Jinyao Guo", "Chengpeng Wang", "Dominic Deluca", "Jinjie Liu", "Zhuo Zhang", "Xiangyu Zhang"], "title": "BugScope: Learn to Find Bugs Like Human", "comment": "19 pages, 2 figure, 6 tables, 4 listings", "summary": "Detecting software bugs remains a fundamental challenge due to the extensive\ndiversity of real-world defects. Traditional static analysis tools often rely\non symbolic workflows, which restrict their coverage and hinder adaptability to\ncustomized bugs with diverse anti-patterns. While recent advances incorporate\nlarge language models (LLMs) to enhance bug detection, these methods continue\nto struggle with sophisticated bugs and typically operate within limited\nanalysis contexts. To address these challenges, we propose BugScope, an\nLLM-driven multi-agent system that emulates how human auditors learn new bug\npatterns from representative examples and apply that knowledge during code\nauditing. Given a set of examples illustrating both buggy and non-buggy\nbehaviors, BugScope synthesizes a retrieval strategy to extract relevant\ndetection contexts via program slicing and then constructs a tailored detection\nprompt to guide accurate reasoning by the LLM. Our evaluation on a curated\ndataset of 40 real-world bugs drawn from 21 widely-used open-source projects\ndemonstrates that BugScope achieves 87.04% precision and 90.00% recall,\nsurpassing state-of-the-art industrial tools by 0.44 in F1 score. Further\ntesting on large-scale open-source systems, including the Linux kernel,\nuncovered 141 previously unknown bugs, of which 78 have been fixed and 7\nconfirmed by developers, highlighting BugScope's substantial practical impact.", "AI": {"tldr": "BugScope\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u5b66\u4e60\u9519\u8bef\u6a21\u5f0f\u7684\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f3a\u9677\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\uff0c\u5728\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u5206\u6790\u5de5\u5177\u5728\u8986\u76d6\u8303\u56f4\u548c\u9002\u5e94\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u591a\u6837\u5316\u4e14\u590d\u6742\u7684\u8f6f\u4ef6\u7f3a\u9677\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u9519\u8bef\u548c\u6709\u9650\u5206\u6790\u4e0a\u4e0b\u6587\u65f6\u4ecd\u663e\u4e0d\u8db3\u3002", "method": "BugScope\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5207\u7247\u63d0\u53d6\u76f8\u5173\u68c0\u6d4b\u4e0a\u4e0b\u6587\uff0c\u5e76\u6784\u5efa\u5b9a\u5236\u5316\u7684\u68c0\u6d4b\u63d0\u793a\u6765\u6307\u5bfcLLM\u8fdb\u884c\u51c6\u786e\u63a8\u7406\u3002", "result": "\u572840\u4e2a\u771f\u5b9e\u4e16\u754c\u9519\u8bef\u7684\u6570\u636e\u96c6\u4e0a\uff0cBugScope\u5b9e\u73b0\u4e8687.04%\u7684\u7cbe\u786e\u7387\u548c90.00%\u7684\u53ec\u56de\u7387\uff0cF1\u5206\u6570\u8d85\u8fc7\u73b0\u6709\u5de5\u4e1a\u5de5\u51770.44\u3002\u5728\u5305\u62ecLinux\u5185\u6838\u5728\u5185\u7684\u5927\u89c4\u6a21\u5f00\u6e90\u7cfb\u7edf\u4e2d\uff0c\u53d1\u73b0\u4e86141\u4e2a\u672a\u77e5\u9519\u8bef\uff0c\u5176\u4e2d78\u4e2a\u5df2\u88ab\u4fee\u590d\uff0c7\u4e2a\u5f97\u5230\u5f00\u53d1\u8005\u786e\u8ba4\u3002", "conclusion": "BugScope\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u5ba1\u8ba1\u5458\u5b66\u4e60\u65b0\u9519\u8bef\u6a21\u5f0f\u7684\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f6f\u4ef6\u7f3a\u9677\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d1\u73b0\u4e86\u5927\u91cf\u672a\u77e5\u9519\u8bef\uff0c\u5c55\u793a\u4e86\u5176\u91cd\u8981\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.15469", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15469", "abs": "https://arxiv.org/abs/2507.15469", "authors": ["Thanh Thi Nguyen", "Saeid Nahavandi", "Imran Razzak", "Dung Nguyen", "Nhat Truong Pham", "Quoc Viet Hung Nguyen"], "title": "The Emergence of Deep Reinforcement Learning for Path Planning", "comment": "Accepted for publication in the Proceedings of the 2025 IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC)", "summary": "The increasing demand for autonomous systems in complex and dynamic\nenvironments has driven significant research into intelligent path planning\nmethodologies. For decades, graph-based search algorithms, linear programming\ntechniques, and evolutionary computation methods have served as foundational\napproaches in this domain. Recently, deep reinforcement learning (DRL) has\nemerged as a powerful method for enabling autonomous agents to learn optimal\nnavigation strategies through interaction with their environments. This survey\nprovides a comprehensive overview of traditional approaches as well as the\nrecent advancements in DRL applied to path planning tasks, focusing on\nautonomous vehicles, drones, and robotic platforms. Key algorithms across both\nconventional and learning-based paradigms are categorized, with their\ninnovations and practical implementations highlighted. This is followed by a\nthorough discussion of their respective strengths and limitations in terms of\ncomputational efficiency, scalability, adaptability, and robustness. The survey\nconcludes by identifying key open challenges and outlining promising avenues\nfor future research. Special attention is given to hybrid approaches that\nintegrate DRL with classical planning techniques to leverage the benefits of\nboth learning-based adaptability and deterministic reliability, offering\npromising directions for robust and resilient autonomous navigation.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u5168\u9762\u6982\u8ff0\u4e86\u4f20\u7edf\u548c\u57fa\u4e8eDRL\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u6df7\u5408\u65b9\u6cd5\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u7cfb\u7edf\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9700\u6c42\u589e\u52a0\uff0c\u7814\u7a76\u667a\u80fd\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u8be5\u8bba\u6587\u901a\u8fc7\u5206\u7c7b\u548c\u6bd4\u8f83\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u56fe\u5f62\u641c\u7d22\u7b97\u6cd5\u3001\u7ebf\u6027\u89c4\u5212\u6280\u672f\u548c\u8fdb\u5316\u8ba1\u7b97\u65b9\u6cd5\uff09\u4e0eDRL\u65b9\u6cd5\u7684\u5173\u952e\u7b97\u6cd5\uff0c\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6982\u8ff0\u3002", "result": "\u8bba\u6587\u8be6\u7ec6\u8ba8\u8bba\u4e86\u5404\u79cd\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u3001\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u603b\u7ed3\u4e86\u8def\u5f84\u89c4\u5212\u9886\u57df\u7684\u5173\u952e\u5f00\u653e\u6311\u6218\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6709\u5e0c\u671b\u65b9\u5411\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u5c06\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4e0e\u7ecf\u5178\u89c4\u5212\u6280\u672f\u7ed3\u5408\u7684\u6df7\u5408\u65b9\u6cd5\u3002"}}
{"id": "2507.15253", "categories": ["cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.15253", "abs": "https://arxiv.org/abs/2507.15253", "authors": ["Zhaochen Guo", "Zhixiang Shen", "Xuanting Xie", "Liangjian Wen", "Zhao Kang"], "title": "Disentangling Homophily and Heterophily in Multimodal Graph Clustering", "comment": "Appear in ACM Multimedia 2025", "summary": "Multimodal graphs, which integrate unstructured heterogeneous data with\nstructured interconnections, offer substantial real-world utility but remain\ninsufficiently explored in unsupervised learning. In this work, we initiate the\nstudy of multimodal graph clustering, aiming to bridge this critical gap.\nThrough empirical analysis, we observe that real-world multimodal graphs often\nexhibit hybrid neighborhood patterns, combining both homophilic and\nheterophilic relationships. To address this challenge, we propose a novel\nframework -- \\textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which\ndecomposes the original hybrid graph into two complementary views: (1) a\nhomophily-enhanced graph that captures cross-modal class consistency, and (2)\nheterophily-aware graphs that preserve modality-specific inter-class\ndistinctions. We introduce a \\emph{Multimodal Dual-frequency Fusion} mechanism\nthat jointly filters these disentangled graphs through a dual-pass strategy,\nenabling effective multimodal integration while mitigating category confusion.\nOur self-supervised alignment objectives further guide the learning process\nwithout requiring labels. Extensive experiments on both multimodal and\nmulti-relational graph datasets demonstrate that DMGC achieves state-of-the-art\nperformance, highlighting its effectiveness and generalizability across diverse\nsettings. Our code is available at https://github.com/Uncnbb/DMGC.", "AI": {"tldr": "DMGC\u901a\u8fc7\u89e3\u8026\u591a\u6a21\u6001\u56fe\u7684\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u5173\u7cfb\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u6a21\u6001\u56fe\u805a\u7c7b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u591a\u6a21\u6001\u56fe\u5728\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u662f\u73b0\u5b9e\u4e2d\u7684\u591a\u6a21\u6001\u56fe\u5e38\u8868\u73b0\u51fa\u6df7\u5408\u90bb\u57df\u6a21\u5f0f\uff0c\u7ed3\u5408\u4e86\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u5173\u7cfb\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDMGC\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u539f\u59cb\u6df7\u5408\u56fe\u5206\u89e3\u4e3a\u540c\u8d28\u6027\u589e\u5f3a\u56fe\u548c\u5f02\u8d28\u6027\u611f\u77e5\u56fe\uff0c\u5e76\u5f15\u5165\u591a\u6a21\u6001\u53cc\u9891\u878d\u5408\u673a\u5236\uff0c\u7ed3\u5408\u53cc\u901a\u9053\u7b56\u7565\u8fdb\u884c\u6709\u6548\u6574\u5408\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u548c\u591a\u5173\u7cfb\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDMGC\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DMGC\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u591a\u6a21\u6001\u56fe\u4e2d\u7684\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u5173\u7cfb\uff0c\u5e76\u91c7\u7528\u81ea\u76d1\u7763\u5bf9\u9f50\u76ee\u6807\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u6a21\u6001\u56fe\u805a\u7c7b\u4e2d\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.15822", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15822", "abs": "https://arxiv.org/abs/2507.15822", "authors": ["Li Huang", "Ilgiz Mustafin", "Marco Piccioni", "Alessandro Schena", "Reto Weber", "Bertrand Meyer"], "title": "Do AI models help produce verified bug fixes?", "comment": null, "summary": "Among areas of software engineering where AI techniques -- particularly,\nLarge Language Models -- seem poised to yield dramatic improvements, an\nattractive candidate is Automatic Program Repair (APR), the production of\nsatisfactory corrections to software bugs. Does this expectation materialize in\npractice? How do we find out, making sure that proposed corrections actually\nwork? If programmers have access to LLMs, how do they actually use them to\ncomplement their own skills?\n  To answer these questions, we took advantage of the availability of a\nprogram-proving environment, which formally determines the correctness of\nproposed fixes, to conduct a study of program debugging with two randomly\nassigned groups of programmers, one with access to LLMs and the other without,\nboth validating their answers through the proof tools. The methodology relied\non a division into general research questions (Goals in the Goal-Query-Metric\napproach), specific elements admitting specific answers (Queries), and\nmeasurements supporting these answers (Metrics). While applied so far to a\nlimited sample size, the results are a first step towards delineating a proper\nrole for AI and LLMs in providing guaranteed-correct fixes to program bugs.\n  These results caused surprise as compared to what one might expect from the\nuse of AI for debugging and APR. The contributions also include: a detailed\nmethodology for experiments in the use of LLMs for debugging, which other\nprojects can reuse; a fine-grain analysis of programmer behavior, made possible\nby the use of full-session recording; a definition of patterns of use of LLMs,\nwith 7 distinct categories; and validated advice for getting the best of LLMs\nfor debugging and Automatic Program Repair.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8LLM\u5728\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u4e0e\u9884\u671f\u5dee\u5f02\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b9\u6cd5\u3001\u884c\u4e3a\u5206\u6790\u548c\u4f18\u5316\u5efa\u8bae\u3002", "motivation": "\u63a2\u8ba8AI\u6280\u672f\uff08\u5c24\u5176\u662fLLM\uff09\u662f\u5426\u80fd\u663e\u8457\u63d0\u5347\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u7684\u6548\u679c\uff0c\u4ee5\u53ca\u7a0b\u5e8f\u5458\u5982\u4f55\u7ed3\u5408LLM\u4e0e\u81ea\u8eab\u6280\u80fd\u8fdb\u884c\u8c03\u8bd5\u3002", "method": "\u91c7\u7528Goal-Query-Metric\u65b9\u6cd5\uff0c\u5c06\u7814\u7a76\u95ee\u9898\u5206\u4e3a\u76ee\u6807\u3001\u5177\u4f53\u95ee\u9898\u548c\u6d4b\u91cf\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u7f16\u7a0b\u8bc1\u660e\u73af\u5883\u9a8c\u8bc1\u4fee\u590d\u7684\u6b63\u786e\u6027\u3002\u4e24\u7ec4\u7a0b\u5e8f\u5458\uff08\u4e00\u7ec4\u4f7f\u7528LLM\uff0c\u53e6\u4e00\u7ec4\u4e0d\u4f7f\u7528\uff09\u53c2\u4e0e\u5b9e\u9a8c\uff0c\u5168\u7a0b\u8bb0\u5f55\u884c\u4e3a\u6570\u636e\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e0e\u9884\u671f\u4e0d\u540c\uff0c\u63ed\u793a\u4e86LLM\u5728\u8c03\u8bd5\u548cAPR\u4e2d\u7684\u5b9e\u9645\u4f5c\u7528\u3002\u8d21\u732e\u5305\u62ec\u5b9e\u9a8c\u65b9\u6cd5\u3001\u884c\u4e3a\u5206\u6790\u3001LLM\u4f7f\u7528\u6a21\u5f0f\u5206\u7c7b\uff087\u7c7b\uff09\u53ca\u4f18\u5316\u5efa\u8bae\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cAI\u548cLLM\u5728\u7a0b\u5e8f\u8c03\u8bd5\u548c\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u4e2d\u5177\u6709\u6f5c\u5728\u4f5c\u7528\uff0c\u4f46\u5b9e\u9645\u6548\u679c\u4e0e\u9884\u671f\u5b58\u5728\u5dee\u5f02\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u5b9e\u9a8c\u65b9\u6cd5\u3001\u7a0b\u5e8f\u5458\u884c\u4e3a\u5206\u6790\u3001LLM\u4f7f\u7528\u6a21\u5f0f\u5206\u7c7b\u4ee5\u53ca\u4f18\u5316\u5efa\u8bae\u3002"}}
{"id": "2507.15474", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15474", "abs": "https://arxiv.org/abs/2507.15474", "authors": ["Charith Premachandra", "Achala Athukorala", "U-Xuan Tan"], "title": "All-UWB SLAM Using UWB Radar and UWB AOA", "comment": null, "summary": "There has been a growing interest in autonomous systems designed to operate\nin adverse conditions (e.g. smoke, dust), where the visible light spectrum\nfails. In this context, Ultra-wideband (UWB) radar is capable of penetrating\nthrough such challenging environmental conditions due to the lower frequency\ncomponents within its broad bandwidth. Therefore, UWB radar has emerged as a\npotential sensing technology for Simultaneous Localization and Mapping (SLAM)\nin vision-denied environments where optical sensors (e.g. LiDAR, Camera) are\nprone to failure. Existing approaches involving UWB radar as the primary\nexteroceptive sensor generally extract features in the environment, which are\nlater initialized as landmarks in a map. However, these methods are constrained\nby the number of distinguishable features in the environment. Hence, this paper\nproposes a novel method incorporating UWB Angle of Arrival (AOA) measurements\ninto UWB radar-based SLAM systems to improve the accuracy and scalability of\nSLAM in feature-deficient environments. The AOA measurements are obtained using\nUWB anchor-tag units which are dynamically deployed by the robot in featureless\nareas during mapping of the environment. This paper thoroughly discusses\nprevailing constraints associated with UWB AOA measurement units and presents\nsolutions to overcome them. Our experimental results show that integrating UWB\nAOA units with UWB radar enables SLAM in vision-denied feature-deficient\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408UWB AOA\u6d4b\u91cf\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5728\u7279\u5f81\u7f3a\u5931\u73af\u5883\u4e2dSLAM\u7684\u6311\u6218\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u6076\u52a3\u73af\u5883\uff08\u5982\u70df\u96fe\u3001\u7070\u5c18\uff09\u4e2d\uff0c\u5149\u5b66\u4f20\u611f\u5668\u6613\u5931\u6548\uff0c\u800cUWB\u96f7\u8fbe\u56e0\u5176\u4f4e\u9891\u6210\u5206\u80fd\u7a7f\u900f\u6b64\u7c7b\u73af\u5883\uff0c\u6210\u4e3aSLAM\u7684\u6f5c\u5728\u4f20\u611f\u6280\u672f\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u73af\u5883\u4e2d\u53ef\u533a\u5206\u7279\u5f81\u7684\u6570\u91cf\u3002", "method": "\u5c06UWB AOA\u6d4b\u91cf\u96c6\u6210\u5230\u57fa\u4e8eUWB\u96f7\u8fbe\u7684SLAM\u7cfb\u7edf\u4e2d\uff0c\u5229\u7528\u52a8\u6001\u90e8\u7f72\u7684UWB\u951a\u70b9-\u6807\u7b7e\u5355\u5143\u5728\u7279\u5f81\u7f3a\u5931\u533a\u57df\u83b7\u53d6AOA\u6d4b\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u96c6\u6210UWB AOA\u5355\u5143\u4e0eUWB\u96f7\u8fbe\u53ef\u5728\u89c6\u89c9\u53d7\u9650\u4e14\u7279\u5f81\u7f3a\u5931\u7684\u73af\u5883\u4e2d\u5b9e\u73b0SLAM\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408UWB AOA\u6d4b\u91cf\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u7279\u5f81\u7f3a\u5931\u73af\u5883\u4e2dSLAM\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.15268", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15268", "abs": "https://arxiv.org/abs/2507.15268", "authors": ["Junhyeong Lee", "Joon-Young Kim", "Heekyu Kim", "Inhyo Lee", "Seunghwa Ryu"], "title": "IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry", "comment": null, "summary": "The injection molding industry faces critical challenges in preserving and\ntransferring field knowledge, particularly as experienced workers retire and\nmultilingual barriers hinder effective communication. This study introduces\nIM-Chat, a multi-agent framework based on large language models (LLMs),\ndesigned to facilitate knowledge transfer in injection molding. IM-Chat\nintegrates both limited documented knowledge (e.g., troubleshooting tables,\nmanuals) and extensive field data modeled through a data-driven process\ncondition generator that infers optimal manufacturing settings from\nenvironmental inputs such as temperature and humidity, enabling robust and\ncontext-aware task resolution. By adopting a retrieval-augmented generation\n(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat\nensures adaptability without the need for fine-tuning. Performance was assessed\nacross 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and\nGPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance\nand correctness, and was further supplemented by automated evaluation using\nGPT-4o guided by a domain-adapted instruction prompt. The evaluation results\nindicate that more capable models tend to achieve higher accuracy, particularly\nin complex, tool-integrated scenarios. Overall, these findings demonstrate the\nviability of multi-agent LLM systems for industrial knowledge workflows and\nestablish IM-Chat as a scalable and generalizable approach to AI-assisted\ndecision support in manufacturing.", "AI": {"tldr": "IM-Chat\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u6ce8\u5851\u884c\u4e1a\u7684\u77e5\u8bc6\u4f20\u9012\u96be\u9898\uff0c\u901a\u8fc7RAG\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u5b9e\u73b0\u65e0\u9700\u5fae\u8c03\u7684\u9002\u5e94\u6027\uff0c\u8bc4\u4f30\u663e\u793a\u9ad8\u6027\u80fd\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u6ce8\u5851\u6210\u578b\u884c\u4e1a\u5728\u4fdd\u7559\u548c\u4f20\u9012\u73b0\u573a\u77e5\u8bc6\u65b9\u9762\u9762\u4e34\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u968f\u7740\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5de5\u4eba\u9000\u4f11\u548c\u591a\u8bed\u8a00\u969c\u788d\u963b\u788d\u6709\u6548\u6c9f\u901a\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86IM-Chat\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7b56\u7565\u548c\u5de5\u5177\u8c03\u7528\u4ee3\u7406\u7684\u6a21\u5757\u5316\u67b6\u6784\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u9002\u5e94\u6027\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u6027\u80fd\u66f4\u5f3a\u7684\u6a21\u578b\u5728\u590d\u6742\u3001\u5de5\u5177\u96c6\u6210\u7684\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u7279\u522b\u662f\u5728\u76f8\u5173\u6027\u548c\u6b63\u786e\u6027\u65b9\u9762\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u591a\u4ee3\u7406LLM\u7cfb\u7edf\u5728\u5de5\u4e1a\u77e5\u8bc6\u5de5\u4f5c\u6d41\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0cIM-Chat\u4f5c\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u4e3a\u5236\u9020\u4e1a\u4e2d\u7684AI\u8f85\u52a9\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15828", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15828", "abs": "https://arxiv.org/abs/2507.15828", "authors": ["Mauro Marcelino", "Marcos Alves", "Bianca Trinkenreich", "Bruno Cartaxo", "S\u00e9rgio Soares", "Simone D. J. Barbosa", "Marcos Kalinowski"], "title": "Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering", "comment": "ESEM 2025 Registered Report with an IPA (In Principle Acceptance) for\n  the Empirical Software Engineering journal", "summary": "[Context] An evidence briefing is a concise and objective transfer medium\nthat can present the main findings of a study to software engineers in the\nindustry. Although practitioners and researchers have deemed Evidence Briefings\nuseful, their production requires manual labor, which may be a significant\nchallenge to their broad adoption. [Goal] The goal of this registered report is\nto describe an experimental protocol for evaluating LLM-generated evidence\nbriefings for secondary studies in terms of content fidelity, ease of\nunderstanding, and usefulness, as perceived by researchers and practitioners,\ncompared to human-made briefings. [Method] We developed an RAG-based LLM tool\nto generate evidence briefings. We used the tool to automatically generate two\nevidence briefings that had been manually generated in previous research\nefforts. We designed a controlled experiment to evaluate how the LLM-generated\nbriefings compare to the human-made ones regarding perceived content fidelity,\nease of understanding, and usefulness. [Results] To be reported after the\nexperimental trials. [Conclusion] Depending on the experiment results.", "AI": {"tldr": "\u672c\u6587\u63cf\u8ff0\u4e86\u4e00\u4e2a\u5b9e\u9a8c\u534f\u8bae\uff0c\u7528\u4e8e\u8bc4\u4f30\u7531LLM\u751f\u6210\u7684\u8bc1\u636e\u7b80\u62a5\u5728\u5185\u5bb9\u4fdd\u771f\u5ea6\u3001\u6613\u7406\u89e3\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u4e0e\u4eba\u5de5\u7b80\u62a5\u7684\u5bf9\u6bd4\u3002\u4f7f\u7528\u57fa\u4e8eRAG\u7684LLM\u5de5\u5177\u751f\u6210\u7b80\u62a5\uff0c\u5e76\u901a\u8fc7\u5bf9\u7167\u5b9e\u9a8c\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u5c3d\u7ba1\u4ece\u4e1a\u8005\u548c\u7814\u7a76\u4eba\u5458\u8ba4\u4e3a\u8bc1\u636e\u7b80\u62a5\u6709\u7528\uff0c\u4f46\u5176\u5236\u4f5c\u9700\u8981\u4eba\u5de5\u52b3\u52a8\uff0c\u8fd9\u53ef\u80fd\u662f\u5e7f\u6cdb\u91c7\u7528\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eRAG\u7684LLM\u5de5\u5177\u6765\u751f\u6210\u8bc1\u636e\u7b80\u62a5\uff0c\u5e76\u4f7f\u7528\u8be5\u5de5\u5177\u81ea\u52a8\u751f\u6210\u4e86\u4e24\u4efd\u5148\u524d\u7814\u7a76\u4e2d\u624b\u52a8\u751f\u6210\u7684\u8bc1\u636e\u7b80\u62a5\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5bf9\u7167\u5b9e\u9a8c\uff0c\u4ee5\u8bc4\u4f30LLM\u751f\u6210\u7684\u7b80\u62a5\u5728\u5185\u5bb9\u4fdd\u771f\u5ea6\u3001\u6613\u7406\u89e3\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u4e0e\u4eba\u5de5\u5236\u4f5c\u7684\u7b80\u62a5\u76f8\u6bd4\u5982\u4f55\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u5f85\u62a5\u544a\u3002", "conclusion": "\u7ed3\u8bba\u5c06\u6839\u636e\u5b9e\u9a8c\u7ed3\u679c\u800c\u5b9a\u3002"}}
{"id": "2507.15478", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15478", "abs": "https://arxiv.org/abs/2507.15478", "authors": ["Simon Kohaut", "Felix Divo", "Navid Hamid", "Benedict Flade", "Julian Eggert", "Devendra Singh Dhami", "Kristian Kersting"], "title": "The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents", "comment": null, "summary": "Ensuring reliable and rule-compliant behavior of autonomous agents in\nuncertain environments remains a fundamental challenge in modern robotics. Our\nwork shows how neuro-symbolic systems, which integrate probabilistic, symbolic\nwhite-box reasoning models with deep learning methods, offer a powerful\nsolution to this challenge. This enables the simultaneous consideration of\nexplicit rules and neural models trained on noisy data, combining the strength\nof structured reasoning with flexible representations. To this end, we\nintroduce the Constitutional Controller (CoCo), a novel framework designed to\nenhance the safety and reliability of agents by reasoning over deep\nprobabilistic logic programs representing constraints such as those found in\nshared traffic spaces. Furthermore, we propose the concept of self-doubt,\nimplemented as a probability density conditioned on doubt features such as\ntravel velocity, employed sensors, or health factors. In a real-world aerial\nmobility study, we demonstrate CoCo's advantages for intelligent autonomous\nsystems to learn appropriate doubts and navigate complex and uncertain\nenvironments safely and compliantly.", "AI": {"tldr": "CoCo\u6846\u67b6\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\uff0c\u63d0\u5347\u81ea\u4e3b\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u6df1\u5ea6\u6982\u7387\u903b\u8f91\u7a0b\u5e8f\u548c\u81ea\u6000\u7591\u6982\u5ff5\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u53ef\u9760\u4e14\u5408\u89c4\u884c\u4e3a\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165Constitutional Controller\uff08CoCo\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u6982\u7387\u903b\u8f91\u7a0b\u5e8f\u548c\u81ea\u6000\u7591\u6982\u5ff5\uff08\u57fa\u4e8e\u65c5\u884c\u901f\u5ea6\u3001\u4f20\u611f\u5668\u72b6\u6001\u6216\u5065\u5eb7\u56e0\u7d20\u7b49\u7279\u5f81\u7684\u6982\u7387\u5bc6\u5ea6\uff09\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u7a7a\u4e2d\u79fb\u52a8\u7814\u7a76\u4e2d\uff0cCoCo\u5c55\u793a\u4e86\u667a\u80fd\u81ea\u4e3b\u7cfb\u7edf\u5b66\u4e60\u9002\u5f53\u6000\u7591\u5e76\u5b89\u5168\u5408\u89c4\u5730\u5bfc\u822a\u590d\u6742\u4e0d\u786e\u5b9a\u73af\u5883\u7684\u80fd\u529b\u3002", "conclusion": "CoCo\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.15330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15330", "abs": "https://arxiv.org/abs/2507.15330", "authors": ["Hammad Atta", "Muhammad Zeeshan Baig", "Yasir Mehmood", "Nadeem Shahzad", "Ken Huang", "Muhammad Aziz Ul Haq", "Muhammad Awais", "Kamal Ahmed"], "title": "QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI", "comment": null, "summary": "We introduce Cognitive Degradation as a novel vulnerability class in agentic\nAI systems. Unlike traditional adversarial external threats such as prompt\ninjection, these failures originate internally, arising from memory starvation,\nplanner recursion, context flooding, and output suppression. These systemic\nweaknesses lead to silent agent drift, logic collapse, and persistent\nhallucinations over time. To address this class of failures, we introduce the\nQorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain\n10), a lifecycle-aware defense framework defined by a six-stage cognitive\ndegradation lifecycle. The framework includes seven runtime controls\n(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger\nproactive mitigation through fallback routing, starvation detection, and memory\nintegrity enforcement. Drawing from cognitive neuroscience, we map agentic\narchitectures to human analogs, enabling early detection of fatigue,\nstarvation, and role collapse. By introducing a formal lifecycle and real-time\nmitigation controls, this work establishes Cognitive Degradation as a critical\nnew class of AI system vulnerability and proposes the first cross-platform\ndefense model for resilient agentic behavior.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8ba4\u77e5\u9000\u5316\u4f5c\u4e3a\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u65b0\u578b\u6f0f\u6d1e\u7c7b\u522b\uff0c\u5e76\u5f15\u5165QSAF\u6846\u67b6\u8fdb\u884c\u9632\u5fa1\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u63a7\u548c\u4e3b\u52a8\u7f13\u89e3\u63aa\u65bd\u63d0\u5347\u4ee3\u7406\u884c\u4e3a\u97e7\u6027\u3002", "motivation": "\u8ba4\u77e5\u9000\u5316\u662f\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u65b0\u578b\u6f0f\u6d1e\u7c7b\u522b\uff0c\u6e90\u4e8e\u5185\u5b58\u9965\u997f\u3001\u89c4\u5212\u5668\u9012\u5f52\u3001\u4e0a\u4e0b\u6587\u6d2a\u6cdb\u548c\u8f93\u51fa\u6291\u5236\u7b49\u5185\u90e8\u95ee\u9898\uff0c\u5bfc\u81f4\u4ee3\u7406\u6f02\u79fb\u3001\u903b\u8f91\u5d29\u6e83\u548c\u6301\u4e45\u5e7b\u89c9\u7b49\u7cfb\u7edf\u6027\u5f31\u70b9\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u516d\u9636\u6bb5\u8ba4\u77e5\u9000\u5316\u751f\u547d\u5468\u671f\u548c\u4e03\u4e2a\u8fd0\u884c\u65f6\u63a7\u5236\uff08QSAF-BC-001\u81f3BC-007\uff09\uff0c\u5305\u62ec\u540e\u5907\u8def\u7531\u3001\u9965\u997f\u68c0\u6d4b\u548c\u5185\u5b58\u5b8c\u6574\u6027\u5f3a\u5236\u7b49\u63aa\u65bd\u3002", "result": "QSAF\u6846\u67b6\u901a\u8fc7\u5b9e\u65f6\u76d1\u63a7\u548c\u4e3b\u52a8\u7f13\u89e3\u63aa\u65bd\uff0c\u6709\u6548\u5e94\u5bf9\u8ba4\u77e5\u9000\u5316\u95ee\u9898\uff0c\u5e76\u9996\u6b21\u5efa\u7acb\u4e86\u8de8\u5e73\u53f0\u7684\u667a\u80fd\u4ee3\u7406\u884c\u4e3a\u97e7\u6027\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86Qorvex Security AI Framework\uff08QSAF Domain 10\uff09\u4f5c\u4e3a\u5e94\u5bf9\u8ba4\u77e5\u9000\u5316\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u63a7\u548c\u4e3b\u52a8\u7f13\u89e3\u63aa\u65bd\uff0c\u9996\u6b21\u5efa\u7acb\u4e86\u8de8\u5e73\u53f0\u7684\u667a\u80fd\u4ee3\u7406\u884c\u4e3a\u97e7\u6027\u6a21\u578b\u3002"}}
{"id": "2507.15831", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15831", "abs": "https://arxiv.org/abs/2507.15831", "authors": ["Sergey Titov", "Konstantin Grotov", "Cristina Sarasua", "Yaroslav Golubev", "Dhivyabharathi Ramasamy", "Alberto Bacchelli", "Abraham Bernstein", "Timofey Bryksin"], "title": "Observing Fine-Grained Changes in Jupyter Notebooks During Development Time", "comment": "32 pages, 6 figures", "summary": "In software engineering, numerous studies have focused on the analysis of\nfine-grained logs, leading to significant innovations in areas such as\nrefactoring, security, and code completion. However, no similar studies have\nbeen conducted for computational notebooks in the context of data science.\n  To help bridge this research gap, we make three scientific contributions: we\n(1) introduce a toolset for collecting code changes in Jupyter notebooks during\ndevelopment time; (2) use it to collect more than 100 hours of work related to\na data analysis task and a machine learning task (carried out by 20 developers\nwith different levels of expertise), resulting in a dataset containing 2,655\ncells and 9,207 cell executions; and (3) use this dataset to investigate the\ndynamic nature of the notebook development process and the changes that take\nplace in the notebooks.\n  In our analysis of the collected data, we classified the changes made to the\ncells between executions and found that a significant number of these changes\nwere relatively small fixes and code iteration modifications. This suggests\nthat notebooks are used not only as a development and exploration tool but also\nas a debugging tool. We report a number of other insights and propose potential\nfuture research directions on the novel data.", "AI": {"tldr": "\u8bba\u6587\u586b\u8865\u4e86\u6570\u636e\u79d1\u5b66\u9886\u57df\u5bf9Jupyter\u7b14\u8bb0\u672c\u5f00\u53d1\u8fc7\u7a0b\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u5de5\u5177\u96c6\u6536\u96c6\u5e76\u5206\u6790\u5f00\u53d1\u6570\u636e\uff0c\u53d1\u73b0\u7b14\u8bb0\u672c\u5e7f\u6cdb\u7528\u4e8e\u8c03\u8bd5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u586b\u8865\u6570\u636e\u79d1\u5b66\u9886\u57df\u5bf9\u8ba1\u7b97\u7b14\u8bb0\u672c\u5f00\u53d1\u8fc7\u7a0b\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u7279\u522b\u662f\u5728\u7ec6\u7c92\u5ea6\u65e5\u5fd7\u5206\u6790\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u5de5\u5177\u96c6\u6536\u96c6Jupyter\u7b14\u8bb0\u672c\u5f00\u53d1\u65f6\u7684\u4ee3\u7801\u53d8\u66f4\uff0c\u6536\u96c6\u4e8620\u540d\u5f00\u53d1\u8005\u8d85\u8fc7100\u5c0f\u65f6\u7684\u5de5\u4f5c\u6570\u636e\uff0c\u5f62\u6210\u5305\u542b2,655\u4e2a\u5355\u5143\u683c\u548c9,207\u6b21\u6267\u884c\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5206\u6790\u53d1\u73b0\u7b14\u8bb0\u672c\u4e2d\u7684\u53d8\u66f4\u591a\u4e3a\u5c0f\u89c4\u6a21\u4fee\u590d\u548c\u4ee3\u7801\u8fed\u4ee3\uff0c\u8868\u660e\u5176\u4f5c\u4e3a\u8c03\u8bd5\u5de5\u5177\u7684\u7528\u9014\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u5206\u6790Jupyter\u7b14\u8bb0\u672c\u7684\u5f00\u53d1\u8fc7\u7a0b\uff0c\u63ed\u793a\u4e86\u7b14\u8bb0\u672c\u4e0d\u4ec5\u7528\u4e8e\u5f00\u53d1\u548c\u63a2\u7d22\uff0c\u8fd8\u4f5c\u4e3a\u8c03\u8bd5\u5de5\u5177\u7684\u4f7f\u7528\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.15484", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15484", "abs": "https://arxiv.org/abs/2507.15484", "authors": ["Jamie Bell"], "title": "Robots for Kiwifruit Harvesting and Pollination", "comment": null, "summary": "This research was a part of a project that developed mobile robots that\nperformed targeted pollen spraying and automated harvesting in pergola\nstructured kiwifruit orchards. Multiple kiwifruit detachment mechanisms were\ndesigned and field testing of one of the concepts showed that the mechanism\ncould reliably pick kiwifruit. Furthermore, this kiwifruit detachment mechanism\nwas able to reach over 80 percent of fruit in the cluttered kiwifruit canopy,\nwhereas the previous state of the art mechanism was only able to reach less\nthan 70 percent of the fruit. Artificial pollination was performed by detecting\nflowers and then spraying pollen in solution onto the detected flowers from a\nline of sprayers on a boom, while driving at up to 1.4 ms-1. In addition, the\nheight of the canopy was measured and the spray boom was moved up and down to\nkeep the boom close enough to the flowers for the spray to reach the flowers,\nwhile minimising collisions with the canopy. Mobile robot navigation was\nperformed using a 2D lidar in apple orchards and vineyards. Lidar navigation in\nkiwifruit orchards was more challenging because the pergola structure only\nprovides a small amount of data for the direction of rows, compared to the\namount of data from the overhead canopy, the undulating ground and other\nobjects in the orchards. Multiple methods are presented here for extracting\nstructure defining features from 3D lidar data in kiwifruit orchards. In\naddition, a 3D lidar navigation system -- which performed row following, row\nend detection and row end turns -- was tested for over 30 km of autonomous\ndriving in kiwifruit orchards. Computer vision algorithms for row detection and\nrow following were also tested. The computer vision algorithm worked as well as\nthe 3D lidar row following method in testing.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u7315\u7334\u6843\u679c\u56ed\u7684\u91c7\u6458\u548c\u82b1\u7c89\u55b7\u6d12\uff0c\u65b0\u91c7\u6458\u673a\u5236\u8986\u76d6\u7387\u8fbe80%\uff0c\u5bfc\u822a\u7cfb\u7edf\u901a\u8fc7\u6fc0\u5149\u96f7\u8fbe\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u5b9e\u73b0\u9ad8\u6548\u4f5c\u4e1a\u3002", "motivation": "\u4f20\u7edf\u7315\u7334\u6843\u679c\u56ed\u7684\u91c7\u6458\u548c\u82b1\u7c89\u55b7\u6d12\u4f9d\u8d56\u4eba\u5de5\uff0c\u6548\u7387\u4f4e\u4e14\u6210\u672c\u9ad8\u3002\u5f00\u53d1\u81ea\u52a8\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u53ef\u4ee5\u63d0\u9ad8\u4f5c\u4e1a\u6548\u7387\uff0c\u964d\u4f4e\u4eba\u529b\u6210\u672c\uff0c\u5e76\u9002\u5e94\u590d\u6742\u7684\u679c\u56ed\u73af\u5883\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86\u591a\u79cd\u7315\u7334\u6843\u91c7\u6458\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u5b9e\u5730\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u53ef\u9760\u6027\u3002\u91c7\u75282D\u548c3D\u6fc0\u5149\u96f7\u8fbe\u6280\u672f\u8fdb\u884c\u679c\u56ed\u5bfc\u822a\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u7528\u4e8e\u884c\u68c0\u6d4b\u548c\u8ddf\u968f\u3002\u82b1\u7c89\u55b7\u6d12\u7cfb\u7edf\u901a\u8fc7\u68c0\u6d4b\u82b1\u6735\u5e76\u8c03\u6574\u55b7\u6746\u9ad8\u5ea6\u5b9e\u73b0\u7cbe\u51c6\u55b7\u6d12\u3002", "result": "\u65b0\u91c7\u6458\u673a\u5236\u8986\u76d6\u7387\u8fbe80%\u4ee5\u4e0a\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u768470%\u3002\u82b1\u7c89\u55b7\u6d12\u7cfb\u7edf\u57281.4\u7c73/\u79d2\u7684\u901f\u5ea6\u4e0b\u5b9e\u73b0\u7cbe\u51c6\u55b7\u6d12\u30023D\u6fc0\u5149\u96f7\u8fbe\u5bfc\u822a\u7cfb\u7edf\u5b8c\u6210\u8d85\u8fc730\u516c\u91cc\u7684\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u8868\u73b0\u4e0e\u6fc0\u5149\u96f7\u8fbe\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u79fb\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u7315\u7334\u6843\u679c\u56ed\u4e2d\u5b9e\u73b0\u5b9a\u5411\u82b1\u7c89\u55b7\u6d12\u548c\u81ea\u52a8\u5316\u91c7\u6458\u3002\u65b0\u8bbe\u8ba1\u7684\u7315\u7334\u6843\u91c7\u6458\u673a\u5236\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u8986\u76d6\u8d85\u8fc780%\u7684\u679c\u5b9e\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002\u6b64\u5916\uff0c3D\u6fc0\u5149\u96f7\u8fbe\u5bfc\u822a\u7cfb\u7edf\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u5728\u679c\u56ed\u5bfc\u822a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u81ea\u52a8\u5316\u519c\u4e1a\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15351", "categories": ["cs.AI", "cs.ET", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15351", "abs": "https://arxiv.org/abs/2507.15351", "authors": ["Zijian Zhao", "Sen Li"], "title": "One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms", "comment": null, "summary": "On-demand ride-sharing platforms face the fundamental challenge of\ndynamically bundling passengers with diverse origins and destinations and\nmatching them with vehicles in real time, all under significant uncertainty.\nRecently, MARL has emerged as a promising solution for this problem, leveraging\ndecentralized learning to address the curse of dimensionality caused by the\nlarge number of agents in the ride-hailing market and the resulting expansive\nstate and action spaces. However, conventional MARL-based ride-sharing\napproaches heavily rely on the accurate estimation of Q-values or V-values,\nwhich becomes problematic in large-scale, highly uncertain environments.\nSpecifically, most of these approaches adopt an independent paradigm,\nexacerbating this issue, as each agent treats others as part of the\nenvironment, leading to unstable training and substantial estimation bias in\nvalue functions. To address these challenges, we propose two novel alternative\nmethods that bypass value function estimation. First, we adapt GRPO to\nride-sharing, replacing the PPO baseline with the group average reward to\neliminate critic estimation errors and reduce training bias. Second, inspired\nby GRPO's full utilization of group reward information, we customize the PPO\nframework for ride-sharing platforms and show that, under a homogeneous fleet,\nthe optimal policy can be trained using only one-step rewards - a method we\nterm One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan\nride-hailing dataset demonstrate that both GRPO and OSPO achieve superior\nperformance across most scenarios, efficiently optimizing pickup times and the\nnumber of served orders using simple MLP networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGRPO\u548cOSPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed5\u8fc7\u503c\u51fd\u6570\u4f30\u8ba1\u89e3\u51b3\u4e86\u4f20\u7edfMARL\u62fc\u8f66\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eMARL\u7684\u62fc\u8f66\u65b9\u6cd5\u56e0\u4f9d\u8d56Q\u503c/V\u503c\u51c6\u786e\u4f30\u8ba1\uff0c\u5728\u5927\u89c4\u6a21\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u72ec\u7acb\u8303\u5f0f\u52a0\u5267\u4e86\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u4f30\u503c\u504f\u5dee\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u5c06GRPO\u5e94\u7528\u4e8e\u62fc\u8f66\u573a\u666f\uff0c\u7528\u7ec4\u5e73\u5747\u5956\u52b1\u66ff\u4ee3PPO\u57fa\u7ebf\u4ee5\u6d88\u9664\u8bc4\u4f30\u8bef\u5dee\uff1b2) \u5b9a\u5236PPO\u6846\u67b6\u5e76\u63d0\u51faOSPO\uff0c\u4ec5\u9700\u4e00\u6b65\u5956\u52b1\u5373\u53ef\u8bad\u7ec3\u6700\u4f18\u7b56\u7565\u3002", "result": "\u5728\u771f\u5b9e\u66fc\u54c8\u987f\u62fc\u8f66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRPO\u548cOSPO\u5728\u591a\u6570\u573a\u666f\u4e2d\u6027\u80fd\u4f18\u8d8a\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684GRPO\u548cOSPO\u65b9\u6cd5\u5728\u52a8\u6001\u62fc\u8f66\u5e73\u53f0\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u4f18\u5316\u4e86\u63a5\u8f7d\u65f6\u95f4\u548c\u8ba2\u5355\u670d\u52a1\u6570\u91cf\uff0c\u4e14\u4ec5\u9700\u7b80\u5355MLP\u7f51\u7edc\u5373\u53ef\u5b9e\u73b0\u3002"}}
{"id": "2507.15493", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15493", "abs": "https://arxiv.org/abs/2507.15493", "authors": ["Chilam Cheang", "Sijin Chen", "Zhongren Cui", "Yingdong Hu", "Liqun Huang", "Tao Kong", "Hang Li", "Yifeng Li", "Yuxiao Liu", "Xiao Ma", "Hao Niu", "Wenxuan Ou", "Wanli Peng", "Zeyu Ren", "Haixin Shi", "Jiawen Tian", "Hongtao Wu", "Xin Xiao", "Yuyang Xiao", "Jiafeng Xu", "Yichu Yang"], "title": "GR-3 Technical Report", "comment": "Tech report. Authors are listed in alphabetical order. Project page:\n  https://seed.bytedance.com/GR3/", "summary": "We report our recent progress towards building generalist robot policies, the\ndevelopment of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.\nIt showcases exceptional capabilities in generalizing to novel objects,\nenvironments, and instructions involving abstract concepts. Furthermore, it can\nbe efficiently fine-tuned with minimal human trajectory data, enabling rapid\nand cost-effective adaptation to new settings. GR-3 also excels in handling\nlong-horizon and dexterous tasks, including those requiring bi-manual\nmanipulation and mobile movement, showcasing robust and reliable performance.\nThese capabilities are achieved through a multi-faceted training recipe that\nincludes co-training with web-scale vision-language data, efficient fine-tuning\nfrom human trajectory data collected via VR devices, and effective imitation\nlearning with robot trajectory data. In addition, we introduce ByteMini, a\nversatile bi-manual mobile robot designed with exceptional flexibility and\nreliability, capable of accomplishing a wide range of tasks when integrated\nwith GR-3. Through extensive real-world experiments, we show GR-3 surpasses the\nstate-of-the-art baseline method, $\\pi_0$, on a wide variety of challenging\ntasks. We hope GR-3 can serve as a step towards building generalist robots\ncapable of assisting humans in daily life.", "AI": {"tldr": "GR-3\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u65b9\u6cd5\u8bad\u7ec3\u5b9e\u73b0\u4e86\u5bf9\u65b0\u73af\u5883\u548c\u4efb\u52a1\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6cdb\u5316\u5230\u65b0\u5bf9\u8c61\u3001\u73af\u5883\u548c\u62bd\u8c61\u6982\u5ff5\u6307\u4ee4\u7684\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u5feb\u901f\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u9002\u5e94\u65b0\u8bbe\u7f6e\u3002", "method": "\u901a\u8fc7\u591a\u65b9\u9762\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5305\u62ec\u4e0e\u7f51\u7edc\u89c4\u6a21\u7684\u89c6\u89c9-\u8bed\u8a00\u6570\u636e\u5171\u540c\u8bad\u7ec3\u3001\u901a\u8fc7VR\u8bbe\u5907\u6536\u96c6\u7684\u4eba\u7c7b\u8f68\u8ff9\u6570\u636e\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\uff0c\u4ee5\u53ca\u5229\u7528\u673a\u5668\u4eba\u8f68\u8ff9\u6570\u636e\u8fdb\u884c\u6709\u6548\u7684\u6a21\u4eff\u5b66\u4e60\u3002", "result": "GR-3\u5728\u591a\u79cd\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u03c0\u2080\uff0c\u5c55\u793a\u4e86\u5728\u957f\u89c6\u91ce\u548c\u7075\u5de7\u4efb\u52a1\u4e2d\u7684\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "GR-3\u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u5e76\u6709\u671b\u6210\u4e3a\u8f85\u52a9\u4eba\u7c7b\u65e5\u5e38\u751f\u6d3b\u7684\u901a\u7528\u673a\u5668\u4eba\u53d1\u5c55\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2507.15356", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15356", "abs": "https://arxiv.org/abs/2507.15356", "authors": ["Lu Guo", "Yixiang Shan", "Zhengbang Zhu", "Qifan Liang", "Lichang Song", "Ting Long", "Weinan Zhang", "Yi Chang"], "title": "RAD: Retrieval High-quality Demonstrations to Enhance Decision-making", "comment": null, "summary": "Offline reinforcement learning (RL) enables agents to learn policies from\nfixed datasets, avoiding costly or unsafe environment interactions. However,\nits effectiveness is often limited by dataset sparsity and the lack of\ntransition overlap between suboptimal and expert trajectories, which makes\nlong-horizon planning particularly challenging. Prior solutions based on\nsynthetic data augmentation or trajectory stitching often fail to generalize to\nnovel states and rely on heuristic stitching points. To address these\nchallenges, we propose Retrieval High-quAlity Demonstrations (RAD) for\ndecision-making, which combines non-parametric retrieval with diffusion-based\ngenerative modeling. RAD dynamically retrieves high-return states from the\noffline dataset as target states based on state similarity and return\nestimation, and plans toward them using a condition-guided diffusion model.\nSuch retrieval-guided generation enables flexible trajectory stitching and\nimproves generalization when encountered with underrepresented or\nout-of-distribution states. Extensive experiments confirm that RAD achieves\ncompetitive or superior performance compared to baselines across diverse\nbenchmarks, validating its effectiveness.", "AI": {"tldr": "RAD\u901a\u8fc7\u68c0\u7d22\u9ad8\u8d28\u91cf\u6f14\u793a\u548c\u6269\u6563\u6a21\u578b\u63d0\u5347\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u56e0\u6570\u636e\u96c6\u7a00\u758f\u548c\u8f68\u8ff9\u91cd\u53e0\u4e0d\u8db3\u5bfc\u81f4\u957f\u671f\u89c4\u5212\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u5408\u6210\u6570\u636e\u589e\u5f3a\u6216\u8f68\u8ff9\u62fc\u63a5\u96be\u4ee5\u6cdb\u5316\u5230\u65b0\u72b6\u6001\u3002", "method": "RAD\u7ed3\u5408\u975e\u53c2\u6570\u68c0\u7d22\u548c\u6269\u6563\u751f\u6210\u6a21\u578b\uff0c\u52a8\u6001\u68c0\u7d22\u9ad8\u56de\u62a5\u72b6\u6001\u4f5c\u4e3a\u76ee\u6807\u72b6\u6001\uff0c\u5e76\u5229\u7528\u6761\u4ef6\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRAD\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RAD\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u975e\u53c2\u6570\u68c0\u7d22\u548c\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u6570\u636e\u96c6\u7a00\u758f\u548c\u8f68\u8ff9\u91cd\u53e0\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u957f\u671f\u89c4\u5212\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.15499", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15499", "abs": "https://arxiv.org/abs/2507.15499", "authors": ["Jongseok Lee", "Timo Birr", "Rudolph Triebel", "Tamim Asfour"], "title": "CLEVER: Stream-based Active Learning for Robust Semantic Perception from Human Instructions", "comment": "8 pages. Accepted to IEEE RAL", "summary": "We propose CLEVER, an active learning system for robust semantic perception\nwith Deep Neural Networks (DNNs). For data arriving in streams, our system\nseeks human support when encountering failures and adapts DNNs online based on\nhuman instructions. In this way, CLEVER can eventually accomplish the given\nsemantic perception tasks. Our main contribution is the design of a system that\nmeets several desiderata of realizing the aforementioned capabilities. The key\nenabler herein is our Bayesian formulation that encodes domain knowledge\nthrough priors. Empirically, we not only motivate CLEVER's design but further\ndemonstrate its capabilities with a user validation study as well as\nexperiments on humanoid and deformable objects. To our knowledge, we are the\nfirst to realize stream-based active learning on a real robot, providing\nevidence that the robustness of the DNN-based semantic perception can be\nimproved in practice. The project website can be accessed at\nhttps://sites.google.com/view/thecleversystem.", "AI": {"tldr": "CLEVER\u662f\u4e00\u4e2a\u57fa\u4e8eDNN\u7684\u4e3b\u52a8\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u7ebf\u4eba\u7c7b\u6307\u5bfc\u548c\u8d1d\u53f6\u65af\u65b9\u6cd5\u63d0\u5347\u8bed\u4e49\u611f\u77e5\u7684\u9c81\u68d2\u6027\uff0c\u9996\u6b21\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u6d41\u5f0f\u4e3b\u52a8\u5b66\u4e60\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6570\u636e\u6d41\u4e2dDNN\u8bed\u4e49\u611f\u77e5\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u7ebf\u9002\u5e94\u5e76\u5229\u7528\u4eba\u7c7b\u6307\u5bfc\u7684\u7cfb\u7edf\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u8d1d\u53f6\u65af\u516c\u5f0f\u7f16\u7801\u9886\u57df\u77e5\u8bc6\u4f5c\u4e3a\u5148\u9a8c\uff0c\u5e76\u7ed3\u5408\u5728\u7ebf\u4eba\u7c7b\u6307\u5bfc\u6765\u9002\u5e94\u6570\u636e\u6d41\u4e2d\u7684\u5931\u8d25\u60c5\u51b5\u3002", "result": "\u901a\u8fc7\u7528\u6237\u9a8c\u8bc1\u7814\u7a76\u53ca\u5bf9\u4eba\u5f62\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86CLEVER\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "CLEVER\u7cfb\u7edf\u901a\u8fc7\u5728\u7ebf\u4eba\u7c7b\u6307\u5bfc\u548c\u8d1d\u53f6\u65af\u516c\u5f0f\uff0c\u6210\u529f\u63d0\u5347\u4e86\u57fa\u4e8eDNN\u7684\u8bed\u4e49\u611f\u77e5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.15411", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15411", "abs": "https://arxiv.org/abs/2507.15411", "authors": ["Wissam Gherissi", "Mehdi Acheli", "Joyce El Haddad", "Daniela Grigori"], "title": "Predictive Process Monitoring Using Object-centric Graph Embeddings", "comment": "ICSOC Workshops 2024, Dec 2024, Tunis, Tunisia", "summary": "Object-centric predictive process monitoring explores and utilizes\nobject-centric event logs to enhance process predictions. The main challenge\nlies in extracting relevant information and building effective models. In this\npaper, we propose an end-to-end model that predicts future process behavior,\nfocusing on two tasks: next activity prediction and next event time. The\nproposed model employs a graph attention network to encode activities and their\nrelationships, combined with an LSTM network to handle temporal dependencies.\nEvaluated on one reallife and three synthetic event logs, the model\ndemonstrates competitive performance compared to state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548cLSTM\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u7528\u4e8e\u5bf9\u8c61\u4e2d\u5fc3\u9884\u6d4b\u8fc7\u7a0b\u76d1\u63a7\uff0c\u5728\u4e0b\u4e00\u4e2a\u6d3b\u52a8\u548c\u65f6\u95f4\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5bf9\u8c61\u4e2d\u5fc3\u9884\u6d4b\u8fc7\u7a0b\u76d1\u63a7\u7684\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u63d0\u53d6\u76f8\u5173\u4fe1\u606f\u5e76\u6784\u5efa\u6709\u6548\u6a21\u578b\u3002", "method": "\u6a21\u578b\u7ed3\u5408\u4e86\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08\u7528\u4e8e\u7f16\u7801\u6d3b\u52a8\u53ca\u5176\u5173\u7cfb\uff09\u548cLSTM\u7f51\u7edc\uff08\u7528\u4e8e\u5904\u7406\u65f6\u95f4\u4f9d\u8d56\u6027\uff09\u3002", "result": "\u5728\u4e00\u4e2a\u73b0\u5b9e\u751f\u6d3b\u548c\u4e09\u4e2a\u5408\u6210\u4e8b\u4ef6\u65e5\u5fd7\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u7aef\u5230\u7aef\u6a21\u578b\u5728\u5bf9\u8c61\u4e2d\u5fc3\u9884\u6d4b\u8fc7\u7a0b\u76d1\u63a7\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u4e0b\u4e00\u4e2a\u6d3b\u52a8\u9884\u6d4b\u548c\u4e0b\u4e00\u4e2a\u4e8b\u4ef6\u65f6\u95f4\u4efb\u52a1\u4e0a\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u4f18\u52bf\u3002"}}
{"id": "2507.15604", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15604", "abs": "https://arxiv.org/abs/2507.15604", "authors": ["Johannes Hartwig", "Philipp Lienhardt", "Dominik Henrich"], "title": "Estimation of Payload Inertial Parameters from Human Demonstrations by Hand Guiding", "comment": "Accepted for publication in Annals of Scientific Society for\n  Assembly, Handling and Industrial Robotics 2025 (to appear)", "summary": "As the availability of cobots increases, it is essential to address the needs\nof users with little to no programming knowledge to operate such systems\nefficiently. Programming concepts often use intuitive interaction modalities,\nsuch as hand guiding, to address this. When programming in-contact motions,\nsuch frameworks require knowledge of the robot tool's payload inertial\nparameters (PIP) in addition to the demonstrated velocities and forces to\nensure effective hybrid motion-force control. This paper aims to enable\nnon-expert users to program in-contact motions more efficiently by eliminating\nthe need for a dedicated PIP calibration, thereby enabling flexible robot tool\nchanges. Since demonstrated tasks generally also contain motions with\nnon-contact, our approach uses these parts to estimate the robot's PIP using\nestablished estimation techniques. The results show that the estimation of the\npayload's mass is accurate, whereas the center of mass and the inertia tensor\nare affected by noise and a lack of excitation. Overall, these findings show\nthe feasibility of PIP estimation during hand guiding but also highlight the\nneed for sufficient payload accelerations for an accurate estimation.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4e13\u7528PIP\u6821\u51c6\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u975e\u63a5\u89e6\u8fd0\u52a8\u90e8\u5206\u4f30\u8ba1\u673a\u5668\u4ebaPIP\uff0c\u7ed3\u679c\u663e\u793a\u8d28\u91cf\u4f30\u8ba1\u51c6\u786e\uff0c\u4f46\u8d28\u5fc3\u548c\u60ef\u6027\u5f20\u91cf\u4f30\u8ba1\u53d7\u566a\u58f0\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u975e\u4e13\u4e1a\u7528\u6237\u5728\u7f16\u7a0b\u63a5\u89e6\u8fd0\u52a8\u65f6\u9700\u8981\u4e13\u7528PIP\u6821\u51c6\u7684\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u7075\u6d3b\u7684\u673a\u5668\u4eba\u5de5\u5177\u66f4\u6362\u3002", "method": "\u5229\u7528\u975e\u63a5\u89e6\u8fd0\u52a8\u90e8\u5206\u901a\u8fc7\u5df2\u5efa\u7acb\u7684\u4f30\u8ba1\u6280\u672f\u6765\u4f30\u8ba1\u673a\u5668\u4eba\u7684PIP\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6709\u6548\u8f7d\u8377\u7684\u8d28\u91cf\u4f30\u8ba1\u51c6\u786e\uff0c\u800c\u8d28\u5fc3\u548c\u60ef\u6027\u5f20\u91cf\u53d7\u566a\u58f0\u548c\u7f3a\u4e4f\u6fc0\u52b1\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u5728\u624b\u52a8\u5f15\u5bfc\u8fc7\u7a0b\u4e2d\u4f30\u8ba1\u6709\u6548\u8f7d\u8377\u60ef\u6027\u53c2\u6570\uff08PIP\uff09\u7684\u53ef\u884c\u6027\uff0c\u4f46\u5f3a\u8c03\u4e86\u9700\u8981\u8db3\u591f\u7684\u6709\u6548\u8f7d\u8377\u52a0\u901f\u5ea6\u4ee5\u5b9e\u73b0\u7cbe\u786e\u4f30\u8ba1\u3002"}}
{"id": "2507.15457", "categories": ["cs.AI", "I.2.8"], "pdf": "https://arxiv.org/pdf/2507.15457", "abs": "https://arxiv.org/abs/2507.15457", "authors": ["Orlenys L\u00f3pez-Pintado", "Jannis Rosenbaum", "Marlon Dumas"], "title": "Optimization of Activity Batching Policies in Business Processes", "comment": null, "summary": "In business processes, activity batching refers to packing multiple activity\ninstances for joint execution. Batching allows managers to trade off cost and\nprocessing effort against waiting time. Larger and less frequent batches may\nlower costs by reducing processing effort and amortizing fixed costs, but they\ncreate longer waiting times. In contrast, smaller and more frequent batches\nreduce waiting times but increase fixed costs and processing effort. A batching\npolicy defines how activity instances are grouped into batches and when each\nbatch is activated. This paper addresses the problem of discovering batching\npolicies that strike optimal trade-offs between waiting time, processing\neffort, and cost. The paper proposes a Pareto optimization approach that starts\nfrom a given set (possibly empty) of activity batching policies and generates\nalternative policies for each batched activity via intervention heuristics.\nEach heuristic identifies an opportunity to improve an activity's batching\npolicy with respect to a metric (waiting time, processing time, cost, or\nresource utilization) and an associated adjustment to the activity's batching\npolicy (the intervention). The impact of each intervention is evaluated via\nsimulation. The intervention heuristics are embedded in an optimization\nmeta-heuristic that triggers interventions to iteratively update the Pareto\nfront of the interventions identified so far. The paper considers three\nmeta-heuristics: hill-climbing, simulated annealing, and reinforcement\nlearning. An experimental evaluation compares the proposed approach based on\nintervention heuristics against the same (non-heuristic guided) meta-heuristics\nbaseline regarding convergence, diversity, and cycle time gain of\nPareto-optimal policies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5e72\u9884\u542f\u53d1\u5f0f\u7684\u5e15\u7d2f\u6258\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u5546\u4e1a\u6d41\u7a0b\u4e2d\u7684\u6d3b\u52a8\u6279\u5904\u7406\u7b56\u7565\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5546\u4e1a\u6d41\u7a0b\u4e2d\u6d3b\u52a8\u6279\u5904\u7406\u7b56\u7565\u9700\u8981\u5728\u7b49\u5f85\u65f6\u95f4\u3001\u5904\u7406\u6210\u672c\u548c\u8d44\u6e90\u5229\u7528\u7387\u4e4b\u95f4\u627e\u5230\u6700\u4f18\u6743\u8861\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9ad8\u6548\u7684\u542f\u53d1\u5f0f\u5f15\u5bfc\u3002", "method": "\u91c7\u7528\u5e15\u7d2f\u6258\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u5e72\u9884\u542f\u53d1\u5f0f\u548c\u4e09\u79cd\u5143\u542f\u53d1\u5f0f\uff08\u722c\u5c71\u6cd5\u3001\u6a21\u62df\u9000\u706b\u548c\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u901a\u8fc7\u4eff\u771f\u8bc4\u4f30\u5e72\u9884\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u5e72\u9884\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\u5728\u5e15\u7d2f\u6258\u6700\u4f18\u7b56\u7565\u7684\u6536\u655b\u6027\u3001\u591a\u6837\u6027\u548c\u5468\u671f\u65f6\u95f4\u589e\u76ca\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e72\u9884\u542f\u53d1\u5f0f\u7684\u5e15\u7d2f\u6258\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u53d1\u73b0\u5546\u4e1a\u6d41\u7a0b\u4e2d\u6d3b\u52a8\u6279\u5904\u7406\u7b56\u7565\u7684\u6700\u4f18\u6743\u8861\u3002\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6536\u655b\u6027\u3001\u591a\u6837\u6027\u548c\u5468\u671f\u65f6\u95f4\u589e\u76ca\u65b9\u9762\u4f18\u4e8e\u975e\u542f\u53d1\u5f0f\u5f15\u5bfc\u7684\u5143\u542f\u53d1\u5f0f\u57fa\u7ebf\u3002"}}
{"id": "2507.15607", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15607", "abs": "https://arxiv.org/abs/2507.15607", "authors": ["Yanbo Chen", "Yunzhe Tan", "Yaojia Wang", "Zhengzhe Xu", "Junbo Tan", "Xueqian Wang"], "title": "A Universal Vehicle-Trailer Navigation System with Neural Kinematics and Online Residual Learning", "comment": "8 pages, 10 figures", "summary": "Autonomous navigation of vehicle-trailer systems is crucial in environments\nlike airports, supermarkets, and concert venues, where various types of\ntrailers are needed to navigate with different payloads and conditions.\nHowever, accurately modeling such systems remains challenging, especially for\ntrailers with castor wheels. In this work, we propose a novel universal\nvehicle-trailer navigation system that integrates a hybrid nominal kinematic\nmodel--combining classical nonholonomic constraints for vehicles and neural\nnetwork-based trailer kinematics--with a lightweight online residual learning\nmodule to correct real-time modeling discrepancies and disturbances.\nAdditionally, we develop a model predictive control framework with a weighted\nmodel combination strategy that improves long-horizon prediction accuracy and\nensures safer motion planning. Our approach is validated through extensive\nreal-world experiments involving multiple trailer types and varying payload\nconditions, demonstrating robust performance without manual tuning or\ntrailer-specific calibration.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u578b\u901a\u7528\u8f66\u8f86-\u62d6\u8f66\u5bfc\u822a\u7cfb\u7edf\uff0c\u7ed3\u5408\u6df7\u5408\u8fd0\u52a8\u5b66\u6a21\u578b\u548c\u5728\u7ebf\u5b66\u4e60\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u5bfc\u822a\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u62d6\u8f66\u7c7b\u578b\u548c\u8d1f\u8f7d\u6761\u4ef6\u3002", "motivation": "\u8f66\u8f86-\u62d6\u8f66\u7cfb\u7edf\u7684\u81ea\u4e3b\u5bfc\u822a\u5728\u673a\u573a\u3001\u8d85\u5e02\u7b49\u73af\u5883\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u63cf\u8ff0\u5e26\u811a\u8f6e\u62d6\u8f66\u7684\u8fd0\u52a8\u5b66\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u901a\u7528\u4e14\u9c81\u68d2\u7684\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u540d\u4e49\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u7ecf\u5178\u975e\u5b8c\u6574\u7ea6\u675f\u548c\u795e\u7ecf\u7f51\u7edc\u62d6\u8f66\u8fd0\u52a8\u5b66\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u5728\u7ebf\u6b8b\u5dee\u5b66\u4e60\u6a21\u5757\u6765\u5b9e\u65f6\u4fee\u6b63\u6a21\u578b\u8bef\u5dee\u548c\u6270\u52a8\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u91c7\u7528\u52a0\u6743\u6a21\u578b\u7ec4\u5408\u7b56\u7565\u4ee5\u63d0\u9ad8\u957f\u671f\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u901a\u8fc7\u591a\u79cd\u62d6\u8f66\u7c7b\u578b\u548c\u4e0d\u540c\u8d1f\u8f7d\u6761\u4ef6\u7684\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u7cfb\u7edf\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6027\u80fd\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u6216\u7279\u5b9a\u62d6\u8f66\u6821\u51c6\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b0\u578b\u901a\u7528\u8f66\u8f86-\u62d6\u8f66\u5bfc\u822a\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408\u6df7\u5408\u540d\u4e49\u8fd0\u52a8\u5b66\u6a21\u578b\u548c\u5728\u7ebf\u6b8b\u5dee\u5b66\u4e60\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u62d6\u8f66\u5bfc\u822a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u6216\u62d6\u8f66\u7279\u5b9a\u6821\u51c6\u3002"}}
{"id": "2507.15509", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15509", "abs": "https://arxiv.org/abs/2507.15509", "authors": ["Lei Chen", "Xuanle Zhao", "Zhixiong Zeng", "Jing Huang", "Yufeng Zhong", "Lin Ma"], "title": "Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner", "comment": "technical report", "summary": "Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based\non reinforcement learning fine-tuning has received widespread attention from\nthe community. Previous R1-Style methods mainly focus on mathematical reasoning\nand code intelligence. It is of great research significance to verify their\nadvantages on more general multimodal data. Chart is an important multimodal\ndata type with rich information, which brings important research challenges in\ncomplex reasoning. In this work, we introduce Chart-R1, a chart-domain\nvision-language model with reinforcement learning fine-tuning to enable complex\nchart reasoning. To support Chart-R1, we first propose a novel programmatic\ndata synthesis technology to generate high-quality step-by-step chart reasoning\ndata covering single- and multi-subcharts, which makes up for the lack of\nreasoning data in the chart domain. Then we develop a two-stage training\nstrategy: Chart-COT with step-by-step chain-of-thought supervision, and\nChart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims\nto decompose complex chart reasoning tasks into fine-grained, understandable\nsubtasks through step-by-step supervision, which lays a good foundation for\nimproving the reasoning level of reinforcement learning. Chart-RFT utilize the\ntypical group relative policy optimization strategy, in which a relatively soft\nreward is adopted for numerical response to emphasize the numerical sensitivity\nin the chart domain. We conduct extensive experiments on open-source benchmarks\nand self-built chart reasoning dataset (\\emph{i.e., ChartRQA}). Experimental\nresults show that Chart-R1 has significant advantages compared to chart-domain\nmethods, even comparable to open/closed source large-scale models (\\emph{e.g.,\nGPT-4o, Claude-3.5}).", "AI": {"tldr": "Chart-R1\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u56fe\u8868\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63a5\u8fd1\u5927\u578b\u6a21\u578b\u6c34\u5e73\u3002", "motivation": "\u9a8c\u8bc1R1-Style\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6570\u636e\uff08\u5c24\u5176\u662f\u56fe\u8868\uff09\u4e0a\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u56fe\u8868\u9886\u57df\u590d\u6742\u63a8\u7406\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7a0b\u5e8f\u5316\u6570\u636e\u5408\u6210\u6280\u672f\u751f\u6210\u9ad8\u8d28\u91cf\u9010\u6b65\u63a8\u7406\u6570\u636e\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1aChart-COT\uff08\u9010\u6b65\u76d1\u7763\uff09\u548cChart-RFT\uff08\u6570\u503c\u654f\u611f\u7684\u5f3a\u5316\u5fae\u8c03\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cChart-R1\u5728\u5f00\u6e90\u57fa\u51c6\u548c\u81ea\u5efa\u6570\u636e\u96c6\uff08ChartRQA\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Chart-R1\u5728\u56fe\u8868\u9886\u57df\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u751a\u81f3\u53ef\u4e0e\u5f00\u6e90/\u95ed\u6e90\u5927\u578b\u6a21\u578b\uff08\u5982GPT-4o\u3001Claude-3.5\uff09\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2507.15608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15608", "abs": "https://arxiv.org/abs/2507.15608", "authors": ["Johannes Hartwig", "Fabian Viessmann", "Dominik Henrich"], "title": "Optimizing Force Signals from Human Demonstrations of In-Contact Motions", "comment": "Accepted for publication in Annals of Scientific Society for\n  Assembly, Handling and Industrial Robotics 2024 (to appear)", "summary": "For non-robot-programming experts, kinesthetic guiding can be an intuitive\ninput method, as robot programming of in-contact tasks is becoming more\nprominent. However, imprecise and noisy input signals from human demonstrations\npose problems when reproducing motions directly or using the signal as input\nfor machine learning methods. This paper explores optimizing force signals to\ncorrespond better to the human intention of the demonstrated signal. We compare\ndifferent signal filtering methods and propose a peak detection method for\ndealing with first-contact deviations in the signal. The evaluation of these\nmethods considers a specialized error criterion between the input and the\nhuman-intended signal. In addition, we analyze the critical parameters'\ninfluence on the filtering methods. The quality for an individual motion could\nbe increased by up to \\SI{20}{\\percent} concerning the error criterion. The\nproposed contribution can improve the usability of robot programming and the\ninteraction between humans and robots.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u4f18\u5316\u529b\u4fe1\u53f7\u4ee5\u66f4\u51c6\u786e\u5730\u53cd\u6620\u4eba\u7c7b\u610f\u56fe\uff0c\u63d0\u51fa\u4e86\u5cf0\u503c\u68c0\u6d4b\u65b9\u6cd5\u5904\u7406\u9996\u6b21\u63a5\u89e6\u504f\u5dee\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u52a8\u518d\u73b0\u7684\u7cbe\u786e\u5ea6\u3002", "motivation": "\u975e\u673a\u5668\u4eba\u7f16\u7a0b\u4e13\u5bb6\u901a\u8fc7\u8fd0\u52a8\u5f15\u5bfc\u8fdb\u884c\u7f16\u7a0b\u65f6\uff0c\u8f93\u5165\u4fe1\u53f7\u7684\u4e0d\u7cbe\u786e\u548c\u566a\u58f0\u95ee\u9898\u5f71\u54cd\u4e86\u8fd0\u52a8\u518d\u73b0\u548c\u673a\u5668\u5b66\u4e60\u8f93\u5165\u7684\u8d28\u91cf\u3002", "method": "\u6bd4\u8f83\u4e86\u4e0d\u540c\u7684\u4fe1\u53f7\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5cf0\u503c\u68c0\u6d4b\u65b9\u6cd5\u6765\u5904\u7406\u9996\u6b21\u63a5\u89e6\u504f\u5dee\u3002", "result": "\u901a\u8fc7\u4f18\u5316\u65b9\u6cd5\uff0c\u5355\u4e2a\u8fd0\u52a8\u7684\u8bef\u5dee\u6807\u51c6\u63d0\u9ad8\u4e8620%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7f16\u7a0b\u7684\u53ef\u7528\u6027\u548c\u4eba\u673a\u4ea4\u4e92\u7684\u4f53\u9a8c\uff0c\u901a\u8fc7\u4f18\u5316\u529b\u4fe1\u53f7\u66f4\u597d\u5730\u53cd\u6620\u4eba\u7c7b\u610f\u56fe\u3002"}}
{"id": "2507.15518", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15518", "abs": "https://arxiv.org/abs/2507.15518", "authors": ["Sizhou Chen", "Shufan Jiang", "Chi Zhang", "Xiao-Lei Zhang", "Xuelong Li"], "title": "HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics", "comment": null, "summary": "Creating an immersive and interactive theatrical experience is a long-term\ngoal in the field of interactive narrative. The emergence of large language\nmodel (LLM) is providing a new path to achieve this goal. However, existing\nLLM-based drama generation methods often result in AI agents that lack\ninitiative and cannot interact with the physical environment. Furthermore,\nthese methods typically require detailed user input to drive the drama. These\nlimitations reduce the interactivity and immersion of online real-time\nperformance. To address the above challenges, we propose HAMLET, a multi-agent\nframework focused on drama creation and online performance. Given a simple\ntopic, the framework generates a narrative blueprint, guiding the subsequent\nimprovisational performance. During the online performance, each actor is given\nan autonomous mind. This means that actors can make independent decisions based\non their own background, goals, and emotional state. In addition to\nconversations with other actors, their decisions can also change the state of\nscene props through actions such as opening a letter or picking up a weapon.\nThe change is then broadcast to other related actors, updating what they know\nand care about, which in turn influences their next action. To evaluate the\nquality of drama performance, we designed an evaluation method to assess three\nprimary aspects, including character performance, narrative quality, and\ninteraction experience. The experimental evaluation shows that HAMLET can\ncreate expressive and coherent theatrical experiences. Our code, dataset and\nmodels are available at https://github.com/HAMLET-2025/HAMLET.", "AI": {"tldr": "HAMLET\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u8d4b\u4e88\u6f14\u5458\u81ea\u4e3b\u601d\u7ef4\u548c\u4e92\u52a8\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u620f\u5267\u751f\u6210\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u5b9e\u73b0\u4e86\u66f4\u5177\u4ea4\u4e92\u6027\u548c\u6c89\u6d78\u611f\u7684\u5728\u7ebf\u620f\u5267\u8868\u6f14\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u620f\u5267\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u5bfc\u81f4AI\u667a\u80fd\u4f53\u7f3a\u4e4f\u4e3b\u52a8\u6027\u4e14\u65e0\u6cd5\u4e0e\u7269\u7406\u73af\u5883\u4e92\u52a8\uff0c\u4e14\u9700\u8be6\u7ec6\u7528\u6237\u8f93\u5165\u9a71\u52a8\u620f\u5267\uff0c\u964d\u4f4e\u4e86\u5728\u7ebf\u5b9e\u65f6\u8868\u6f14\u7684\u4ea4\u4e92\u6027\u548c\u6c89\u6d78\u611f\u3002", "method": "\u63d0\u51fa\u4e86HAMLET\uff0c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u620f\u5267\u521b\u4f5c\u548c\u5728\u7ebf\u8868\u6f14\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u751f\u6210\u53d9\u4e8b\u84dd\u56fe\u6307\u5bfc\u5373\u5174\u8868\u6f14\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6f14\u5458\u8d4b\u4e88\u81ea\u4e3b\u601d\u7ef4\uff0c\u4f7f\u5176\u80fd\u57fa\u4e8e\u80cc\u666f\u3001\u76ee\u6807\u548c\u60c5\u611f\u72b6\u6001\u72ec\u7acb\u51b3\u7b56\uff0c\u5e76\u901a\u8fc7\u52a8\u4f5c\u6539\u53d8\u573a\u666f\u9053\u5177\u72b6\u6001\u3002", "result": "HAMLET\u80fd\u591f\u6839\u636e\u7b80\u5355\u4e3b\u9898\u751f\u6210\u53d9\u4e8b\u84dd\u56fe\uff0c\u5e76\u652f\u6301\u6f14\u5458\u901a\u8fc7\u81ea\u4e3b\u51b3\u7b56\u548c\u52a8\u4f5c\u6539\u53d8\u573a\u666f\u72b6\u6001\uff0c\u4ece\u800c\u521b\u5efa\u5bcc\u6709\u8868\u73b0\u529b\u548c\u8fde\u8d2f\u6027\u7684\u620f\u5267\u4f53\u9a8c\u3002", "conclusion": "HAMLET\u6846\u67b6\u80fd\u591f\u751f\u6210\u5bcc\u6709\u8868\u73b0\u529b\u548c\u8fde\u8d2f\u6027\u7684\u620f\u5267\u4f53\u9a8c\uff0c\u5b9e\u9a8c\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u5728\u89d2\u8272\u8868\u73b0\u3001\u53d9\u4e8b\u8d28\u91cf\u548c\u4e92\u52a8\u4f53\u9a8c\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.15649", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15649", "abs": "https://arxiv.org/abs/2507.15649", "authors": ["Haocheng Xu", "Haodong Zhang", "Zhenghan Chen", "Rong Xiong"], "title": "EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation", "comment": null, "summary": "To support humanoid robots in performing manipulation tasks, it is essential\nto study stable standing while accommodating upper-body motions. However, the\nlimited controllable range of humanoid robots in a standing position affects\nthe stability of the entire body. Thus we introduce a reinforcement learning\nbased framework for humanoid robots to imitate human upper-body motions while\nmaintaining overall stability. Our approach begins with designing a retargeting\nnetwork that generates a large-scale upper-body motion dataset for training the\nreinforcement learning (RL) policy, which enables the humanoid robot to track\nupper-body motion targets, employing domain randomization for enhanced\nrobustness. To avoid exceeding the robot's execution capability and ensure\nsafety and stability, we propose an Executable Motion Prior (EMP) module, which\nadjusts the input target movements based on the robot's current state. This\nadjustment improves standing stability while minimizing changes to motion\namplitude. We evaluate our framework through simulation and real-world tests,\ndemonstrating its practical applicability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u6a21\u4eff\u4eba\u4f53\u4e0a\u534a\u8eab\u52a8\u4f5c\u65f6\u4fdd\u6301\u7a33\u5b9a\uff0c\u901a\u8fc7EMP\u6a21\u5757\u4f18\u5316\u52a8\u4f5c\u6267\u884c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u4eba\u5f62\u673a\u5668\u4eba\u5728\u6267\u884c\u64cd\u4f5c\u4efb\u52a1\u65f6\u7684\u7a33\u5b9a\u7ad9\u7acb\u95ee\u9898\uff0c\u89e3\u51b3\u7ad9\u7acb\u59ff\u52bf\u4e0b\u53ef\u63a7\u8303\u56f4\u6709\u9650\u5bf9\u6574\u4f53\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u91cd\u5b9a\u5411\u7f51\u7edc\u751f\u6210\u5927\u89c4\u6a21\u4e0a\u534a\u8eab\u52a8\u4f5c\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u9886\u57df\u968f\u673a\u5316\u63d0\u5347\u9c81\u68d2\u6027\u3002\u5f15\u5165EMP\u6a21\u5757\u6839\u636e\u673a\u5668\u4eba\u5f53\u524d\u72b6\u6001\u8c03\u6574\u8f93\u5165\u76ee\u6807\u52a8\u4f5c\uff0c\u786e\u4fdd\u5b89\u5168\u4e0e\u7a33\u5b9a\u6027\u3002", "result": "\u6a21\u62df\u548c\u5b9e\u9645\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\uff0c\u673a\u5668\u4eba\u80fd\u591f\u6709\u6548\u6a21\u4eff\u4eba\u4f53\u4e0a\u534a\u8eab\u52a8\u4f5c\u5e76\u4fdd\u6301\u7a33\u5b9a\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u53ef\u6267\u884c\u8fd0\u52a8\u5148\u9a8c\uff08EMP\uff09\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u6a21\u4eff\u4eba\u4f53\u4e0a\u534a\u8eab\u52a8\u4f5c\u7684\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u7a33\u5b9a\u6027\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2507.15521", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15521", "abs": "https://arxiv.org/abs/2507.15521", "authors": ["Cole Robertson", "Philip Wolff"], "title": "LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning", "comment": "Manuscript comprises 14 pages, 4 figures, 4 tables in the Technical\n  Appendix and Supplementary Material, and is under review at NeurIPS 2025", "summary": "Do large language models (LLMs) construct and manipulate internal world\nmodels, or do they rely solely on statistical associations represented as\noutput layer token probabilities? We adapt cognitive science methodologies from\nhuman mental models research to test LLMs on pulley system problems using\nTikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical\nadvantage (MA). State-of-the-art models performed marginally but significantly\nabove chance, and their estimates correlated significantly with ground-truth\nMA. Significant correlations between number of pulleys and model estimates\nsuggest that models employed a pulley counting heuristic, without necessarily\nsimulating pulley systems to derive precise values. Study 2 tested this by\nprobing whether LLMs represent global features crucial to MA estimation. Models\nevaluated a functionally connected pulley system against a fake system with\nrandomly placed components. Without explicit cues, models identified the\nfunctional system as having greater MA with F1=0.8, suggesting LLMs could\nrepresent systems well enough to differentiate jumbled from functional systems.\nStudy 3 built on this by asking LLMs to compare functional systems with matched\nsystems which were connected up but which transferred no force to the weight;\nLLMs identified the functional system with F1=0.46, suggesting random guessing.\nInsofar as they may generalize, these findings are compatible with the notion\nthat LLMs manipulate internal world models, sufficient to exploit statistical\nassociations between pulley count and MA (Study 1), and to approximately\nrepresent system components' spatial relations (Study 2). However, they may\nlack the facility to reason over nuanced structural connectivity (Study 3). We\nconclude by advocating the utility of cognitive scientific methods to evaluate\nthe world-modeling capacities of artificial intelligence systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8LLMs\u662f\u5426\u4f9d\u8d56\u5185\u90e8\u4e16\u754c\u6a21\u578b\u8fd8\u662f\u7edf\u8ba1\u5173\u8054\u3002\u901a\u8fc7\u6ed1\u8f6e\u7cfb\u7edf\u6d4b\u8bd5\uff0c\u53d1\u73b0LLMs\u80fd\u5229\u7528\u6ed1\u8f6e\u6570\u91cf\u4e0eMA\u7684\u7edf\u8ba1\u5173\u8054\uff08\u7814\u7a761\uff09\u548c\u8fd1\u4f3c\u8868\u793a\u7a7a\u95f4\u5173\u7cfb\uff08\u7814\u7a762\uff09\uff0c\u4f46\u5bf9\u7ed3\u6784\u8fde\u901a\u6027\u63a8\u7406\u80fd\u529b\u6709\u9650\uff08\u7814\u7a763\uff09\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u6784\u5efa\u548c\u64cd\u7eb5\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u8fd8\u662f\u4ec5\u4f9d\u8d56\u4e8e\u8868\u793a\u4e3a\u8f93\u51fa\u5c42\u4ee4\u724c\u6982\u7387\u7684\u7edf\u8ba1\u5173\u8054\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u8ba4\u77e5\u79d1\u5b66\u65b9\u6cd5\uff0c\u901a\u8fc7TikZ\u6e32\u67d3\u7684\u523a\u6fc0\u7269\u6d4b\u8bd5LLMs\u5728\u6ed1\u8f6e\u7cfb\u7edf\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002\u7814\u7a761\u6d4b\u8bd5\u4e86LLMs\u662f\u5426\u80fd\u4f30\u8ba1\u673a\u68b0\u4f18\u52bf\uff08MA\uff09\uff1b\u7814\u7a762\u63a2\u7a76\u4e86LLMs\u662f\u5426\u80fd\u8868\u793a\u5bf9MA\u4f30\u8ba1\u81f3\u5173\u91cd\u8981\u7684\u5168\u5c40\u7279\u5f81\uff1b\u7814\u7a763\u8fdb\u4e00\u6b65\u6d4b\u8bd5\u4e86LLMs\u5bf9\u529f\u80fd\u7cfb\u7edf\u4e0e\u975e\u529f\u80fd\u7cfb\u7edf\u7684\u533a\u5206\u80fd\u529b\u3002", "result": "\u7814\u7a761\u663e\u793a\uff0c\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728MA\u4f30\u8ba1\u4e0a\u8868\u73b0\u7565\u9ad8\u4e8e\u968f\u673a\u6c34\u5e73\uff0c\u4e14\u5176\u4f30\u8ba1\u4e0e\u771f\u5b9eMA\u663e\u8457\u76f8\u5173\u3002\u7814\u7a762\u53d1\u73b0\uff0cLLMs\u80fd\u591f\u533a\u5206\u529f\u80fd\u7cfb\u7edf\u4e0e\u968f\u673a\u7ec4\u4ef6\u7cfb\u7edf\uff08F1=0.8\uff09\u3002\u7814\u7a763\u663e\u793a\uff0cLLMs\u5728\u533a\u5206\u529f\u80fd\u7cfb\u7edf\u4e0e\u65e0\u529b\u91cf\u4f20\u9012\u7684\u5339\u914d\u7cfb\u7edf\u65f6\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u731c\u6d4b\uff08F1=0.46\uff09\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\u8ba4\u4e3a\uff0c\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u80fd\u64cd\u7eb5\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u8db3\u4ee5\u5229\u7528\u6ed1\u8f6e\u6570\u91cf\u4e0e\u673a\u68b0\u4f18\u52bf\uff08MA\uff09\u4e4b\u95f4\u7684\u7edf\u8ba1\u5173\u8054\uff08\u7814\u7a761\uff09\uff0c\u5e76\u8fd1\u4f3c\u8868\u793a\u7cfb\u7edf\u7ec4\u4ef6\u7684\u7a7a\u95f4\u5173\u7cfb\uff08\u7814\u7a762\uff09\uff0c\u4f46\u5b83\u4eec\u53ef\u80fd\u7f3a\u4e4f\u5bf9\u7ec6\u5fae\u7ed3\u6784\u8fde\u901a\u6027\u8fdb\u884c\u63a8\u7406\u7684\u80fd\u529b\uff08\u7814\u7a763\uff09\u3002\u4f5c\u8005\u63d0\u5021\u4f7f\u7528\u8ba4\u77e5\u79d1\u5b66\u65b9\u6cd5\u6765\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2507.15677", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15677", "abs": "https://arxiv.org/abs/2507.15677", "authors": ["Huayue Liang", "Yanbo Chen", "Hongyang Cheng", "Yanzhao Yu", "Shoujie Li", "Junbo Tan", "Xueqian Wang", "Long Zeng"], "title": "Data-Driven MPC with Data Selection for Flexible Cable-Driven Robotic Arms", "comment": null, "summary": "Flexible cable-driven robotic arms (FCRAs) offer dexterous and compliant\nmotion. Still, the inherent properties of cables, such as resilience,\nhysteresis, and friction, often lead to particular difficulties in modeling and\ncontrol. This paper proposes a model predictive control (MPC) method that\nrelies exclusively on input-output data, without a physical model, to improve\nthe control accuracy of FCRAs. First, we develop an implicit model based on\ninput-output data and integrate it into an MPC optimization framework. Second,\na data selection algorithm (DSA) is introduced to filter the data that best\ncharacterize the system, thereby reducing the solution time per step to\napproximately 4 ms, which is an improvement of nearly 80%. Lastly, the\ninfluence of hyperparameters on tracking error is investigated through\nsimulation. The proposed method has been validated on a real FCRA platform,\nincluding five-point positioning accuracy tests, a five-point response tracking\ntest, and trajectory tracking for letter drawing. The results demonstrate that\nthe average positioning accuracy is approximately 2.070 mm. Moreover, compared\nto the PID method with an average tracking error of 1.418{\\deg}, the proposed\nmethod achieves an average tracking error of 0.541{\\deg}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f93\u5165\u8f93\u51fa\u6570\u636e\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67d4\u6027\u7535\u7f06\u9a71\u52a8\u673a\u68b0\u81c2\u7684\u63a7\u5236\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u5e73\u5747\u8ddf\u8e2a\u8bef\u5dee\u964d\u81f30.541\u5ea6\u3002", "motivation": "\u67d4\u6027\u7535\u7f06\u9a71\u52a8\u673a\u68b0\u81c2\uff08FCRAs\uff09\u56e0\u5176\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u800c\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u7535\u7f06\u7684\u5f39\u6027\u3001\u6ede\u56de\u548c\u6469\u64e6\u7b49\u7279\u6027\u7ed9\u5efa\u6a21\u548c\u63a7\u5236\u5e26\u6765\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u7269\u7406\u6a21\u578b\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u63a7\u5236\u7cbe\u5ea6\u3002", "method": "\u8bba\u6587\u9996\u5148\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8f93\u5165\u8f93\u51fa\u6570\u636e\u7684\u9690\u5f0f\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230MPC\u4f18\u5316\u6846\u67b6\u4e2d\u3002\u5176\u6b21\uff0c\u5f15\u5165\u6570\u636e\u9009\u62e9\u7b97\u6cd5\uff08DSA\uff09\u7b5b\u9009\u6700\u80fd\u8868\u5f81\u7cfb\u7edf\u7684\u6570\u636e\uff0c\u5c06\u6bcf\u6b65\u6c42\u89e3\u65f6\u95f4\u7f29\u77ed\u81f3\u7ea64\u6beb\u79d2\uff0c\u63d0\u5347\u8fd180%\u3002\u6700\u540e\uff0c\u901a\u8fc7\u4eff\u771f\u7814\u7a76\u4e86\u8d85\u53c2\u6570\u5bf9\u8ddf\u8e2a\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "result": "\u5728\u771f\u5b9eFCRA\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\uff0c\u5305\u62ec\u4e94\u70b9\u5b9a\u4f4d\u7cbe\u5ea6\u6d4b\u8bd5\u3001\u4e94\u70b9\u54cd\u5e94\u8ddf\u8e2a\u6d4b\u8bd5\u548c\u5b57\u6bcd\u7ed8\u5236\u8f68\u8ff9\u8ddf\u8e2a\u3002\u7ed3\u679c\u663e\u793a\u5e73\u5747\u5b9a\u4f4d\u7cbe\u5ea6\u7ea6\u4e3a2.070\u6beb\u7c73\uff0c\u5e73\u5747\u8ddf\u8e2a\u8bef\u5dee\u4e3a0.541\u5ea6\uff0c\u4f18\u4e8ePID\u65b9\u6cd5\u76841.418\u5ea6\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u8f93\u5165\u8f93\u51fa\u6570\u636e\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u67d4\u6027\u7535\u7f06\u9a71\u52a8\u673a\u68b0\u81c2\u7684\u63a7\u5236\u7cbe\u5ea6\uff0c\u5e73\u5747\u5b9a\u4f4d\u7cbe\u5ea6\u7ea6\u4e3a2.070\u6beb\u7c73\uff0c\u8ddf\u8e2a\u8bef\u5dee\u964d\u81f30.541\u5ea6\uff0c\u4f18\u4e8e\u4f20\u7edfPID\u65b9\u6cd5\u3002"}}
{"id": "2507.15532", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15532", "abs": "https://arxiv.org/abs/2507.15532", "authors": ["Kasper Engelen", "Guillermo A. P\u00e9rez", "Marnix Suilen"], "title": "Data-Efficient Safe Policy Improvement Using Parametric Structure", "comment": "Accepted at ECAI 2025", "summary": "Safe policy improvement (SPI) is an offline reinforcement learning problem in\nwhich a new policy that reliably outperforms the behavior policy with high\nconfidence needs to be computed using only a dataset and the behavior policy.\nMarkov decision processes (MDPs) are the standard formalism for modeling\nenvironments in SPI. In many applications, additional information in the form\nof parametric dependencies between distributions in the transition dynamics is\navailable. We make SPI more data-efficient by leveraging these dependencies\nthrough three contributions: (1) a parametric SPI algorithm that exploits known\ncorrelations between distributions to more accurately estimate the transition\ndynamics using the same amount of data; (2) a preprocessing technique that\nprunes redundant actions from the environment through a game-based abstraction;\nand (3) a more advanced preprocessing technique, based on satisfiability modulo\ntheory (SMT) solving, that can identify more actions to prune. Empirical\nresults and an ablation study show that our techniques increase the data\nefficiency of SPI by multiple orders of magnitude while maintaining the same\nreliability guarantees.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5229\u7528\u53c2\u6570\u4f9d\u8d56\u5173\u7cfb\u548c\u9884\u5904\u7406\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u7b56\u7565\u6539\u8fdb\uff08SPI\uff09\u7684\u6570\u636e\u6548\u7387\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728SPI\u95ee\u9898\u4e2d\uff0c\u8bb8\u591a\u5e94\u7528\u573a\u666f\u4e0b\u5b58\u5728\u989d\u5916\u7684\u53c2\u6570\u4f9d\u8d56\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u6765\u63d0\u9ad8\u6570\u636e\u6548\u7387\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316SPI\u7b97\u6cd5\uff0c\u5229\u7528\u5206\u5e03\u4e4b\u95f4\u7684\u5df2\u77e5\u76f8\u5173\u6027\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u8f6c\u79fb\u52a8\u6001\uff1b2. \u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6e38\u620f\u62bd\u8c61\u7684\u9884\u5904\u7406\u6280\u672f\uff0c\u7528\u4e8e\u4fee\u526a\u5197\u4f59\u52a8\u4f5c\uff1b3. \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u6ee1\u8db3\u6027\u6a21\u7406\u8bba\uff08SMT\uff09\u7684\u66f4\u9ad8\u7ea7\u9884\u5904\u7406\u6280\u672f\uff0c\u80fd\u591f\u8bc6\u522b\u66f4\u591a\u53ef\u4fee\u526a\u7684\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6280\u672f\u5c06SPI\u7684\u6570\u636e\u6548\u7387\u63d0\u9ad8\u4e86\u591a\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u540c\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6280\u672f\u901a\u8fc7\u5229\u7528\u5df2\u77e5\u7684\u53c2\u6570\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u7b56\u7565\u6539\u8fdb\uff08SPI\uff09\u7684\u6570\u636e\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u540c\u7684\u53ef\u9760\u6027\u4fdd\u8bc1\u3002"}}
{"id": "2507.15693", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15693", "abs": "https://arxiv.org/abs/2507.15693", "authors": ["Georges Chebly", "Spencer Little", "Nisal Perera", "Aliya Abedeen", "Ken Suzuki", "Donghyun Kim"], "title": "Strong, Accurate, and Low-Cost Robot Manipulator", "comment": null, "summary": "This paper presents Forte, a fully 3D-printable, 6-DoF robotic arm designed\nto achieve near industrial-grade performance - 0.63 kg payload, 0.467 m reach,\nand sub-millimeter repeatability - at a material cost under $215. As an\naccessible robot for broad applications across classroom education to AI\nexperiments, Forte pushes forward the performance limitations of existing\nlow-cost educational arms. We introduce a cost-effective mechanical design that\ncombines capstan-based cable drives, timing belts, simple tensioning\nmechanisms, and lightweight 3D-printed structures, along with topology\noptimization for structural stiffness. Through careful drivetrain engineering,\nwe minimize backlash and maintain control fidelity without relying on\nhigh-power electronics or expensive manufacturing processes. Experimental\nvalidation demonstrates that Forte achieves high repeatability and load\ncapacity, offering a compelling robotic platform for both classroom instruction\nand advanced robotics research.", "AI": {"tldr": "Forte\u662f\u4e00\u6b3e\u51683D\u6253\u5370\u3001\u4f4e\u6210\u672c\uff08<215\u7f8e\u5143\uff09\u76846\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\uff0c\u5177\u6709\u63a5\u8fd1\u5de5\u4e1a\u7ea7\u7684\u6027\u80fd\uff080.63 kg\u8d1f\u8f7d\uff0c0.467 m\u5de5\u4f5c\u534a\u5f84\uff0c\u4e9a\u6beb\u7c73\u7ea7\u91cd\u590d\u7cbe\u5ea6\uff09\uff0c\u9002\u7528\u4e8e\u6559\u80b2\u548c\u7814\u7a76\u3002", "motivation": "\u7a81\u7834\u73b0\u6709\u4f4e\u6210\u672c\u6559\u80b2\u673a\u68b0\u81c2\u7684\u6027\u80fd\u9650\u5236\uff0c\u63d0\u4f9b\u4e00\u4e2a\u6750\u6599\u6210\u672c\u4f4e\u4e8e215\u7f8e\u5143\u3001\u6027\u80fd\u63a5\u8fd1\u5de5\u4e1a\u7ea7\u7684\u673a\u5668\u4eba\u5e73\u53f0\u3002", "method": "\u7ed3\u5408\u4e86capstan-based\u7535\u7f06\u9a71\u52a8\u3001\u540c\u6b65\u5e26\u3001\u7b80\u5355\u5f20\u7d27\u673a\u6784\u548c\u8f7b\u91cf\u53163D\u6253\u5370\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u62d3\u6251\u4f18\u5316\u63d0\u9ad8\u7ed3\u6784\u521a\u5ea6\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4f20\u52a8\u7cfb\u7edf\u51cf\u5c11\u4e86\u56de\u5dee\u5e76\u4fdd\u6301\u4e86\u63a7\u5236\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cForte\u5b9e\u73b0\u4e86\u9ad8\u91cd\u590d\u6027\u548c\u8d1f\u8f7d\u80fd\u529b\uff080.63 kg\u8d1f\u8f7d\uff0c0.467 m\u5de5\u4f5c\u534a\u5f84\uff0c\u4e9a\u6beb\u7c73\u7ea7\u91cd\u590d\u7cbe\u5ea6\uff09\u3002", "conclusion": "Forte \u662f\u4e00\u6b3e\u4f4e\u6210\u672c\u3001\u9ad8\u6027\u80fd\u76846\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\uff0c\u9002\u7528\u4e8e\u4ece\u8bfe\u5802\u6559\u80b2\u5230AI\u5b9e\u9a8c\u7684\u5e7f\u6cdb\u573a\u666f\uff0c\u5176\u8bbe\u8ba1\u548c\u6027\u80fd\u9a8c\u8bc1\u4e86\u5176\u5728\u6559\u80b2\u548c\u7814\u7a76\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.15581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15581", "abs": "https://arxiv.org/abs/2507.15581", "authors": ["Ekaterina Goliakova", "Xavier Renard", "Marie-Jeanne Lesot", "Thibault Laugel", "Christophe Marsala", "Marcin Detyniecki"], "title": "Metric assessment protocol in the context of answer fluctuation on MCQ tasks", "comment": null, "summary": "Using multiple-choice questions (MCQs) has become a standard for assessing\nLLM capabilities efficiently. A variety of metrics can be employed for this\ntask. However, previous research has not conducted a thorough assessment of\nthem. At the same time, MCQ evaluation suffers from answer fluctuation: models\nproduce different results given slight changes in prompts. We suggest a metric\nassessment protocol in which evaluation methodologies are analyzed through\ntheir connection with fluctuation rates, as well as original performance. Our\nresults show that there is a strong link between existing metrics and the\nanswer changing, even when computed without any additional prompt variants. A\nnovel metric, worst accuracy, demonstrates the highest association on the\nprotocol.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u65b0\u8bc4\u4f30\u534f\u8bae\uff0c\u53d1\u73b0\u73b0\u6709\u6307\u6807\u4e0e\u7b54\u6848\u6ce2\u52a8\u7387\u5173\u8054\u5f3a\uff0c\u2018\u6700\u5dee\u51c6\u786e\u7387\u2019\u8868\u73b0\u6700\u597d\u3002", "motivation": "\u591a\u9009\u95ee\u9898\uff08MCQ\uff09\u8bc4\u4f30\u4e2d\u5b58\u5728\u7684\u7b54\u6848\u6ce2\u52a8\u95ee\u9898\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b54\u6848\u6ce2\u52a8\u7387\u548c\u539f\u59cb\u6027\u80fd\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u8bc4\u4f30\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u6307\u6807\u4e0e\u7b54\u6848\u6ce2\u52a8\u7387\u5b58\u5728\u5f3a\u5173\u8054\uff0c\u65b0\u6307\u6807\u2018\u6700\u5dee\u51c6\u786e\u7387\u2019\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u4e0e\u7b54\u6848\u6ce2\u52a8\u7387\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5173\u8054\uff0c\u800c\u63d0\u51fa\u7684\u65b0\u6307\u6807\u2018\u6700\u5dee\u51c6\u786e\u7387\u2019\u5728\u8be5\u534f\u8bae\u4e2d\u663e\u793a\u51fa\u6700\u9ad8\u7684\u5173\u8054\u6027\u3002"}}
{"id": "2507.15710", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15710", "abs": "https://arxiv.org/abs/2507.15710", "authors": ["Lu Huang", "Lingxiao Meng", "Jiankun Wang", "Xingjian Jing"], "title": "Selective Densification for Rapid Motion Planning in High Dimensions with Narrow Passages", "comment": null, "summary": "Sampling-based algorithms are widely used for motion planning in\nhigh-dimensional configuration spaces. However, due to low sampling efficiency,\ntheir performance often diminishes in complex configuration spaces with narrow\ncorridors. Existing approaches address this issue using handcrafted or learned\nheuristics to guide sampling toward useful regions. Unfortunately, these\nstrategies often lack generalizability to various problems or require extensive\nprior training. In this paper, we propose a simple yet efficient sampling-based\nplanning framework along with its bidirectional version that overcomes these\nissues by integrating different levels of planning granularity. Our approach\nprobes configuration spaces with uniform random samples at varying resolutions\nand explores these multi-resolution samples online with a bias towards sparse\nsamples when traveling large free configuration spaces. By seamlessly\ntransitioning between sparse and dense samples, our approach can navigate\ncomplex configuration spaces while maintaining planning speed and completeness.\nThe simulation results demonstrate that our approach outperforms several\nstate-of-the-art sampling-based planners in $\\mathbb{SE}(2)$, $\\mathbb{SE}(3)$,\nand $\\mathbb{R}^{14}$ with challenging terrains. Furthermore, experiments\nconducted with the Franka Emika Panda robot operating in a constrained\nworkspace provide additional evidence of the superiority of the proposed\nmethod.", "AI": {"conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u591a\u5206\u8fa8\u7387\u91c7\u6837\u89c4\u5212\u6846\u67b6\u53ca\u5176\u53cc\u5411\u7248\u672c\u5728\u590d\u6742\u914d\u7f6e\u7a7a\u95f4\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u91c7\u6837\u89c4\u5212\u5668\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6311\u6218\u6027\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u57fa\u4e8e\u91c7\u6837\u7684\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u63a2\u7d22\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u5747\u5300\u968f\u673a\u6837\u672c\uff0c\u5e76\u504f\u5411\u7a00\u758f\u6837\u672c\u4ee5\u4f18\u5316\u5927\u81ea\u7531\u914d\u7f6e\u7a7a\u95f4\u7684\u63a2\u7d22\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u91c7\u6837\u7684\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u5728\u590d\u6742\u914d\u7f6e\u7a7a\u95f4\u4e2d\u56e0\u91c7\u6837\u6548\u7387\u4f4e\u800c\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u514b\u670d\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u7f3a\u4e4f\u6cdb\u5316\u6027\u6216\u9700\u8981\u5927\u91cf\u9884\u8bad\u7ec3\u7684\u9650\u5236\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728$\\mathbb{SE}(2)$\u3001$\\mathbb{SE}(3)$\u548c$\\mathbb{R}^{14}$\u7b49\u6311\u6218\u6027\u5730\u5f62\u4e2d\u4f18\u4e8e\u591a\u79cd\u6700\u5148\u8fdb\u7684\u91c7\u6837\u89c4\u5212\u5668\uff0c\u4e14\u5728Franka Emika Panda\u673a\u5668\u4eba\u53d7\u9650\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "tldr": "\u63d0\u51fa\u591a\u5206\u8fa8\u7387\u91c7\u6837\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u8c03\u6574\u91c7\u6837\u5bc6\u5ea6\u4f18\u5316\u590d\u6742\u7a7a\u95f4\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2507.15618", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15618", "abs": "https://arxiv.org/abs/2507.15618", "authors": ["Weiyu Ma", "Jiwen Jiang", "Haobo Fu", "Haifeng Zhang"], "title": "TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II", "comment": null, "summary": "We present an adapter-based approach for tactical conditioning of StarCraft\nII AI agents. Current agents, while powerful, lack the ability to adapt their\nstrategies based on high-level tactical directives. Our method freezes a\npre-trained policy network (DI-Star) and attaches lightweight adapter modules\nto each action head, conditioned on a tactical tensor that encodes strategic\npreferences. By training these adapters with KL divergence constraints, we\nensure the policy maintains core competencies while exhibiting tactical\nvariations. Experimental results show our approach successfully modulates agent\nbehavior across tactical dimensions including aggression, expansion patterns,\nand technology preferences, while maintaining competitive performance. Our\nmethod enables flexible tactical control with minimal computational overhead,\noffering practical strategy customization for complex real-time strategy games.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9002\u914d\u5668\u7684StarCraft II AI\u6218\u672f\u8c03\u63a7\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u5757\u5b9e\u73b0\u7075\u6d3b\u7b56\u7565\u8c03\u6574\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u867d\u5f3a\u5927\uff0c\u4f46\u7f3a\u4e4f\u57fa\u4e8e\u9ad8\u5c42\u6218\u672f\u6307\u4ee4\u8c03\u6574\u7b56\u7565\u7684\u80fd\u529b\uff0c\u9700\u4e00\u79cd\u7075\u6d3b\u4e14\u4f4e\u5f00\u9500\u7684\u6218\u672f\u8c03\u63a7\u65b9\u6cd5\u3002", "method": "\u51bb\u7ed3\u9884\u8bad\u7ec3\u7b56\u7565\u7f51\u7edc\uff08DI-Star\uff09\uff0c\u5728\u6bcf\u4e2a\u52a8\u4f5c\u5934\u9644\u52a0\u8f7b\u91cf\u7ea7\u9002\u914d\u6a21\u5757\uff0c\u901a\u8fc7KL\u6563\u5ea6\u7ea6\u675f\u8bad\u7ec3\u9002\u914d\u5668\uff0c\u6218\u672f\u5f20\u91cf\u7f16\u7801\u6218\u7565\u504f\u597d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8c03\u63a7\u4ee3\u7406\u884c\u4e3a\uff08\u5982\u4fb5\u7565\u6027\u3001\u6269\u5f20\u6a21\u5f0f\u3001\u6280\u672f\u504f\u597d\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u6a21\u5757\u5b9e\u73b0\u4e86\u5bf9StarCraft II AI\u4ee3\u7406\u7684\u6218\u672f\u8c03\u63a7\uff0c\u65e2\u4fdd\u6301\u4e86\u6838\u5fc3\u80fd\u529b\uff0c\u53c8\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u7b56\u7565\u5b9a\u5236\uff0c\u8ba1\u7b97\u5f00\u9500\u4f4e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5b9e\u65f6\u7b56\u7565\u6e38\u620f\u3002"}}
{"id": "2507.15716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15716", "abs": "https://arxiv.org/abs/2507.15716", "authors": ["Ziyu Wan", "Lin Zhao"], "title": "DiffPF: Differentiable Particle Filtering with Generative Sampling via Conditional Diffusion Models", "comment": null, "summary": "This paper proposes DiffPF, a differentiable particle filter that leverages\ndiffusion models for state estimation in dynamic systems. Unlike conventional\ndifferentiable particle filters, which require importance weighting and\ntypically rely on predefined or low-capacity proposal distributions. DiffPF\nlearns a flexible posterior sampler by conditioning a diffusion model on\npredicted particles and the current observation. This enables accurate,\nequally-weighted sampling from complex, high-dimensional, and multimodal\nfiltering distributions. We evaluate DiffPF across a range of scenarios,\nincluding both unimodal and highly multimodal distributions, and test it on\nsimulated as well as real-world tasks, where it consistently outperforms\nexisting filtering baselines. In particular, DiffPF achieves an 82.8%\nimprovement in estimation accuracy on a highly multimodal global localization\nbenchmark, and a 26% improvement on the real-world KITTI visual odometry\nbenchmark, compared to state-of-the-art differentiable filters. To the best of\nour knowledge, DiffPF is the first method to integrate conditional diffusion\nmodels into particle filtering, enabling high-quality posterior sampling that\nproduces more informative particles and significantly improves state\nestimation.", "AI": {"tldr": "DiffPF\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53ef\u5fae\u5206\u7c92\u5b50\u6ee4\u6ce2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5dee\u5206\u7c92\u5b50\u6ee4\u6ce2\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u6216\u4f4e\u5bb9\u91cf\u63d0\u6848\u5206\u5e03\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002DiffPF\u65e8\u5728\u901a\u8fc7\u6269\u6563\u6a21\u578b\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u540e\u9a8c\u91c7\u6837\u3002", "method": "DiffPF\u5229\u7528\u6269\u6563\u6a21\u578b\u5b66\u4e60\u7075\u6d3b\u7684\u540e\u9a8c\u91c7\u6837\u5668\uff0c\u901a\u8fc7\u9884\u6d4b\u7c92\u5b50\u548c\u5f53\u524d\u89c2\u6d4b\u6761\u4ef6\u5316\uff0c\u5b9e\u73b0\u590d\u6742\u3001\u9ad8\u7ef4\u548c\u591a\u6a21\u6001\u6ee4\u6ce2\u5206\u5e03\u7684\u51c6\u786e\u91c7\u6837\u3002", "result": "DiffPF\u5728\u591a\u6a21\u6001\u5206\u5e03\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982\u5728\u9ad8\u5ea6\u591a\u6a21\u6001\u7684\u5168\u5c40\u5b9a\u4f4d\u57fa\u51c6\u4e0a\u4f30\u8ba1\u7cbe\u5ea6\u63d0\u5347\u4e8682.8%\uff0c\u5728KITTI\u89c6\u89c9\u6d4b\u8ddd\u57fa\u51c6\u4e0a\u63d0\u5347\u4e8626%\u3002", "conclusion": "DiffPF\u901a\u8fc7\u5c06\u6761\u4ef6\u6269\u6563\u6a21\u578b\u96c6\u6210\u5230\u7c92\u5b50\u6ee4\u6ce2\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u540e\u7eed\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u4e86\u72b6\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.15676", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.15676", "abs": "https://arxiv.org/abs/2507.15676", "authors": ["Reza Vatankhah Barenji", "Sina Khoshgoftar"], "title": "Agentic AI for autonomous anomaly management in complex systems", "comment": null, "summary": "This paper explores the potential of agentic AI in autonomously detecting and\nresponding to anomalies within complex systems, emphasizing its ability to\ntransform traditional, human-dependent anomaly management methods.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4ee3\u7406\u5f0fAI\u5728\u81ea\u4e3b\u5f02\u5e38\u7ba1\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u5c55\u793a\u4e86\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6548\u7387\u548c\u81ea\u4e3b\u6027\u3002", "motivation": "\u4f20\u7edf\u5f02\u5e38\u7ba1\u7406\u65b9\u6cd5\u4f9d\u8d56\u4eba\u7c7b\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u4ee3\u7406\u5f0fAI\u6709\u671b\u63d0\u4f9b\u66f4\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63a2\u7d22\u4e86\u4ee3\u7406\u5f0fAI\u5728\u5f02\u5e38\u68c0\u6d4b\u548c\u54cd\u5e94\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5176\u81ea\u4e3b\u6027\u548c\u9002\u5e94\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\u4ee3\u7406\u5f0fAI\u80fd\u591f\u6709\u6548\u8bc6\u522b\u548c\u5e94\u5bf9\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\uff0c\u51cf\u5c11\u5bf9\u4eba\u7c7b\u5e72\u9884\u7684\u4f9d\u8d56\u3002", "conclusion": "\u8be5\u8bba\u6587\u5f3a\u8c03\u4e86\u4ee3\u7406\u5f0fAI\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u81ea\u4e3b\u68c0\u6d4b\u548c\u54cd\u5e94\u5f02\u5e38\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6539\u53d8\u4f20\u7edf\u4f9d\u8d56\u4eba\u7c7b\u7684\u5f02\u5e38\u7ba1\u7406\u65b9\u6cd5\u3002"}}
{"id": "2507.15729", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15729", "abs": "https://arxiv.org/abs/2507.15729", "authors": ["Jens V. R\u00fcppel", "Andrey Rudenko", "Tim Schreiter", "Martin Magnusson", "Achim J. Lilienthal"], "title": "Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction", "comment": "This paper has been accepted to the 34th IEEE International\n  Conference on Robot and Human Interactive Communication (RO-MAN), which will\n  be held in Eindhoven, Netherlands on August 25-29, 2025. Copyright 2025 IEEE.\n  Personal use of this material is permitted. Permission from IEEE must be\n  obtained for all other uses", "summary": "The rapid development of Large Language Models (LLMs) creates an exciting\npotential for flexible, general knowledge-driven Human-Robot Interaction (HRI)\nsystems for assistive robots. Existing HRI systems demonstrate great progress\nin interpreting and following user instructions, action generation, and robot\ntask solving. On the other hand, bi-directional, multi-modal, and context-aware\nsupport of the user in collaborative tasks still remains an open challenge. In\nthis paper, we present a gaze- and speech-informed interface to the assistive\nrobot, which is able to perceive the working environment from multiple vision\ninputs and support the dynamic user in their tasks. Our system is designed to\nbe modular and transferable to adapt to diverse tasks and robots, and it is\ncapable of real-time use of language-based interaction state representation and\nfast on board perception modules. Its development was supported by multiple\npublic dissemination events, contributing important considerations for improved\nrobustness and user experience. Furthermore, in two lab studies, we compare the\nperformance and user ratings of our system with those of a traditional scripted\nHRI pipeline. Our findings indicate that an LLM-based approach enhances\nadaptability and marginally improves user engagement and task execution metrics\nbut may produce redundant output, while a scripted pipeline is well suited for\nmore straightforward tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u7cfb\u7edf\uff0c\u901a\u8fc7\u89c6\u7ebf\u548c\u8bed\u97f3\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u9002\u5e94\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u5b58\u5728\u5197\u4f59\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u53cc\u5411\u3001\u591a\u6a21\u6001\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u534f\u4f5c\u4efb\u52a1\u652f\u6301\u8fd9\u4e00\u5f00\u653e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u7ebf\u548c\u8bed\u97f3\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u754c\u9762\uff0c\u652f\u6301\u5b9e\u65f6\u8bed\u8a00\u4ea4\u4e92\u548c\u5feb\u901f\u611f\u77e5\u6a21\u5757\u3002", "result": "\u5728\u4e24\u9879\u5b9e\u9a8c\u5ba4\u7814\u7a76\u4e2d\uff0c\u4e0e\u4f20\u7edf\u811a\u672c\u5316HRI\u6d41\u7a0b\u76f8\u6bd4\uff0cLLM-based\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u4e0a\u7565\u6709\u63d0\u5347\uff0c\u4f46\u5b58\u5728\u5197\u4f59\u8f93\u51fa\u95ee\u9898\u3002", "conclusion": "LLM-based\u65b9\u6cd5\u5728\u589e\u5f3a\u9002\u5e94\u6027\u548c\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u53ef\u80fd\u4ea7\u751f\u5197\u4f59\u8f93\u51fa\uff1b\u800c\u811a\u672c\u5316\u6d41\u7a0b\u66f4\u9002\u5408\u7b80\u5355\u4efb\u52a1\u3002"}}
{"id": "2507.15743", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15743", "abs": "https://arxiv.org/abs/2507.15743", "authors": ["Elahe Vedadi", "David Barrett", "Natalie Harris", "Ellery Wulczyn", "Shashir Reddy", "Roma Ruparel", "Mike Schaekermann", "Tim Strother", "Ryutaro Tanno", "Yash Sharma", "Jihyeon Lee", "C\u00edan Hughes", "Dylan Slack", "Anil Palepu", "Jan Freyberg", "Khaled Saab", "Valentin Li\u00e9vin", "Wei-Hung Weng", "Tao Tu", "Yun Liu", "Nenad Tomasev", "Kavita Kulkarni", "S. Sara Mahdavi", "Kelvin Guu", "Jo\u00eblle Barral", "Dale R. Webster", "James Manyika", "Avinatan Hassidim", "Katherine Chou", "Yossi Matias", "Pushmeet Kohli", "Adam Rodman", "Vivek Natarajan", "Alan Karthikesalingam", "David Stutz"], "title": "Towards physician-centered oversight of conversational diagnostic AI", "comment": null, "summary": "Recent work has demonstrated the promise of conversational AI systems for\ndiagnostic dialogue. However, real-world assurance of patient safety means that\nproviding individual diagnoses and treatment plans is considered a regulated\nactivity by licensed professionals. Furthermore, physicians commonly oversee\nother team members in such activities, including nurse practitioners (NPs) or\nphysician assistants/associates (PAs). Inspired by this, we propose a framework\nfor effective, asynchronous oversight of the Articulate Medical Intelligence\nExplorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent\nsystem that performs history taking within guardrails, abstaining from\nindividualized medical advice. Afterwards, g-AMIE conveys assessments to an\noverseeing primary care physician (PCP) in a clinician cockpit interface. The\nPCP provides oversight and retains accountability of the clinical decision.\nThis effectively decouples oversight from intake and can thus happen\nasynchronously. In a randomized, blinded virtual Objective Structured Clinical\nExamination (OSCE) of text consultations with asynchronous oversight, we\ncompared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across\n60 scenarios, g-AMIE outperformed both groups in performing high-quality\nintake, summarizing cases, and proposing diagnoses and management plans for the\noverseeing PCP to review. This resulted in higher quality composite decisions.\nPCP oversight of g-AMIE was also more time-efficient than standalone PCP\nconsultations in prior work. While our study does not replicate existing\nclinical practices and likely underestimates clinicians' capabilities, our\nresults demonstrate the promise of asynchronous oversight as a feasible\nparadigm for diagnostic AI systems to operate under expert human oversight for\nenhancing real-world care.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fag-AMIE\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f02\u6b65\u76d1\u7763\u6846\u67b6\u63d0\u5347\u8bca\u65adAI\u7cfb\u7edf\u7684\u4e34\u5e8a\u51b3\u7b56\u8d28\u91cf\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u591a\u4e2a\u65b9\u9762\u4f18\u4e8e\u4eba\u7c7b\u56e2\u961f\uff0c\u4e14\u66f4\u9ad8\u6548\u3002", "motivation": "\u53d7\u73b0\u5b9e\u4e2d\u533b\u751f\u76d1\u7763\u5176\u4ed6\u56e2\u961f\u6210\u5458\uff08\u5982\u62a4\u58eb\u4ece\u4e1a\u8005\u6216\u533b\u5e08\u52a9\u7406\uff09\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u4e3a\u8bca\u65ad\u5bf9\u8bddAI\u7cfb\u7edf\u63d0\u4f9b\u6709\u6548\u7684\u5f02\u6b65\u76d1\u7763\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86guardrailed-AMIE\uff08g-AMIE\uff09\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5728\u62a4\u680f\u5185\u6267\u884c\u75c5\u53f2\u91c7\u96c6\uff0c\u907f\u514d\u63d0\u4f9b\u4e2a\u6027\u5316\u533b\u7597\u5efa\u8bae\uff0c\u5e76\u901a\u8fc7\u4e34\u5e8a\u533b\u751f\u9a7e\u9a76\u8231\u754c\u9762\u5c06\u8bc4\u4f30\u7ed3\u679c\u4f20\u8fbe\u7ed9\u76d1\u7763\u7684\u521d\u7ea7\u4fdd\u5065\u533b\u751f\uff08PCP\uff09\u3002", "result": "\u5728\u968f\u673a\u3001\u76f2\u6cd5\u7684\u865a\u62dfOSCE\u4e2d\uff0cg-AMIE\u5728\u9ad8\u8d28\u91cf\u75c5\u53f2\u91c7\u96c6\u3001\u75c5\u4f8b\u603b\u7ed3\u53ca\u8bca\u65ad\u548c\u7ba1\u7406\u8ba1\u5212\u63d0\u8bae\u65b9\u9762\u4f18\u4e8eNPs/PAs\u548cPCPs\u7ec4\uff0c\u4e14PCP\u76d1\u7763g-AMIE\u6bd4\u72ec\u7acbPCP\u54a8\u8be2\u66f4\u9ad8\u6548\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u5f02\u6b65\u76d1\u7763\u4f5c\u4e3a\u4e00\u79cd\u53ef\u884c\u8303\u5f0f\uff0c\u8ba9\u8bca\u65adAI\u7cfb\u7edf\u5728\u4e13\u5bb6\u4eba\u7c7b\u76d1\u7763\u4e0b\u8fd0\u884c\uff0c\u4ee5\u589e\u5f3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u533b\u7597\u62a4\u7406\u3002"}}
{"id": "2507.15782", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15782", "abs": "https://arxiv.org/abs/2507.15782", "authors": ["Ruochu Yang", "Yu Zhou", "Fumin Zhang", "Mengxue Hou"], "title": "Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs", "comment": null, "summary": "Household robots have been a longstanding research topic, but they still lack\nhuman-like intelligence, particularly in manipulating open-set objects and\nnavigating large environments efficiently and accurately. To push this\nboundary, we consider a generalized multi-object collection problem in large\nscene graphs, where the robot needs to pick up and place multiple objects\nacross multiple locations in a long mission of multiple human commands. This\nproblem is extremely challenging since it requires long-horizon planning in a\nvast action-state space under high uncertainties. To this end, we propose a\nnovel interleaved LLM and motion planning algorithm Inter-LLM. By designing a\nmultimodal action cost similarity function, our algorithm can both reflect the\nhistory and look into the future to optimize plans, striking a good balance of\nquality and efficiency. Simulation experiments demonstrate that compared with\nlatest works, our algorithm improves the overall mission performance by 30% in\nterms of fulfilling human commands, maximizing mission success rates, and\nminimizing mission costs.", "AI": {"tldr": "Inter-LLM\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408LLM\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u63d0\u5347\u4e86\u5bb6\u5ead\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u6027\u80fd\u63d0\u534730%\u3002", "motivation": "\u5bb6\u5ead\u673a\u5668\u4eba\u5728\u5904\u7406\u5f00\u653e\u96c6\u5bf9\u8c61\u548c\u5927\u578b\u73af\u5883\u5bfc\u822a\u65f6\u7f3a\u4e4f\u7c7b\u4eba\u667a\u80fd\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u5347\u5176\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ea4\u9519\u5f0fLLM\u548c\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5Inter-LLM\uff0c\u8bbe\u8ba1\u4e86\u591a\u6a21\u6001\u52a8\u4f5c\u6210\u672c\u76f8\u4f3c\u6027\u51fd\u6570\uff0c\u4ee5\u4f18\u5316\u957f\u671f\u89c4\u5212\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0cInter-LLM\u7b97\u6cd5\u5728\u6ee1\u8db3\u4eba\u7c7b\u6307\u4ee4\u3001\u6700\u5927\u5316\u4efb\u52a1\u6210\u529f\u7387\u548c\u6700\u5c0f\u5316\u4efb\u52a1\u6210\u672c\u65b9\u9762\u6bd4\u6700\u65b0\u5de5\u4f5c\u63d0\u5347\u4e8630%\u3002", "conclusion": "Inter-LLM\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408LLM\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bb6\u5ead\u673a\u5668\u4eba\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u4efb\u52a1\u6267\u884c\u6548\u7387\uff0c\u5b9e\u73b0\u4e8630%\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.15758", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15758", "abs": "https://arxiv.org/abs/2507.15758", "authors": ["Xingyu Wu", "Yuchen Yan", "Shangke Lyu", "Linjuan Wu", "Yiwen Qiu", "Yongliang Shen", "Weiming Lu", "Jian Shao", "Jun Xiao", "Yueting Zhuang"], "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization", "comment": "GitHub:https://github.com/zju-real/lapo;\n  Project:https://zju-real.github.io/lapo", "summary": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality.", "AI": {"tldr": "LAPO\u901a\u8fc7\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8ba9\u6a21\u578b\u81ea\u9002\u5e94\u63a7\u5236\u63a8\u7406\u957f\u5ea6\uff0c\u51cf\u5c1140.9%\u7684token\u4f7f\u7528\u5e76\u63d0\u53472.3%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u8fc7\u5ea6\u751f\u6210token\u7684\u95ee\u9898\uff0c\u5c06\u63a8\u7406\u957f\u5ea6\u63a7\u5236\u4ece\u5916\u90e8\u7ea6\u675f\u8f6c\u53d8\u4e3a\u6a21\u578b\u7684\u5185\u5728\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86Length-Adaptive Policy Optimization (LAPO)\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u5b66\u4e60\u6210\u529f\u7684\u89e3\u51b3\u65b9\u6848\u957f\u5ea6\u7684\u7edf\u8ba1\u5206\u5e03\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5c06\u8fd9\u4e9b\u6a21\u5f0f\u4f5c\u4e3a\u5143\u8ba4\u77e5\u6307\u5bfc\u5d4c\u5165\u6a21\u578b\u7684\u63a8\u7406\u4e0a\u4e0b\u6587\u4e2d\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLAPO\u51cf\u5c11\u4e86\u9ad8\u8fbe40.9%\u7684token\u4f7f\u7528\uff0c\u540c\u65f6\u51c6\u786e\u7387\u63d0\u9ad8\u4e862.3%\u3002", "conclusion": "LAPO\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5185\u5316\u5bf9\u63a8\u7406\u6df1\u5ea6\u7684\u7406\u89e3\uff0c\u4ece\u800c\u5728\u4e0d\u727a\u7272\u63a8\u7406\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u7684\u4f7f\u7528\u3002"}}
{"id": "2507.15833", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15833", "abs": "https://arxiv.org/abs/2507.15833", "authors": ["Ian Chuang", "Andrew Lee", "Dechen Gao", "Jinyu Zou", "Iman Soltani"], "title": "Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers", "comment": "13 pages, 10 figures", "summary": "Human vision is a highly active process driven by gaze, which directs\nattention and fixation to task-relevant regions and dramatically reduces visual\nprocessing. In contrast, robot learning systems typically rely on passive,\nuniform processing of raw camera images. In this work, we explore how\nincorporating human-like active gaze into robotic policies can enhance both\nefficiency and performance. We build on recent advances in foveated image\nprocessing and apply them to an Active Vision robot system that emulates both\nhuman head movement and eye tracking. Extending prior work on the AV-ALOHA\nrobot simulation platform, we introduce a framework for simultaneously\ncollecting eye-tracking data and robot demonstrations from a human operator as\nwell as a simulation benchmark and dataset for training robot policies that\nincorporate human gaze. Given the widespread use of Vision Transformers (ViTs)\nin robot learning, we integrate gaze information into ViTs using a foveated\npatch tokenization scheme inspired by recent work in image segmentation.\nCompared to uniform patch tokenization, this significantly reduces the number\nof tokens-and thus computation-without sacrificing visual fidelity near regions\nof interest. We also explore two approaches to gaze imitation and prediction\nfrom human data. The first is a two-stage model that predicts gaze to guide\nfoveation and action; the second integrates gaze into the action space,\nallowing the policy to jointly predict gaze and actions end-to-end. Our results\nshow that our method for foveated robot vision not only drastically reduces\ncomputational overhead, but also improves performance for high precision tasks\nand robustness to unseen distractors. Together, these findings suggest that\nhuman-inspired visual processing offers a useful inductive bias for robotic\nvision systems. https://ian-chuang.github.io/gaze-av-aloha/", "AI": {"tldr": "\u7814\u7a76\u5c06\u4eba\u7c7b\u4e3b\u52a8\u6ce8\u89c6\u878d\u5165\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\uff0c\u901a\u8fc7foveated\u56fe\u50cf\u5904\u7406\u548cViTs\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u4eba\u7c7b\u89c6\u89c9\u901a\u8fc7\u6ce8\u89c6\u4e3b\u52a8\u5f15\u5bfc\u6ce8\u610f\u529b\uff0c\u800c\u673a\u5668\u4eba\u5b66\u4e60\u7cfb\u7edf\u901a\u5e38\u88ab\u52a8\u5904\u7406\u539f\u59cb\u56fe\u50cf\u3002\u7814\u7a76\u5982\u4f55\u5c06\u4eba\u7c7b\u4e3b\u52a8\u6ce8\u89c6\u878d\u5165\u673a\u5668\u4eba\u7b56\u7565\u4ee5\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u4e86\u4eba\u7c7b\u6ce8\u89c6\u7684\u4e3b\u52a8\u89c6\u89c9\u7cfb\u7edf\uff0c\u91c7\u7528\u4e86\u57fa\u4e8efoveated\u56fe\u50cf\u5904\u7406\u7684\u65b9\u6cd5\uff0c\u5e76\u96c6\u6210\u4e86Vision Transformers (ViTs)\u548cfoveated patch tokenization\u65b9\u6848\u3002\u63a2\u7d22\u4e86\u4e24\u79cd\u6ce8\u89c6\u6a21\u4eff\u548c\u9884\u6d4b\u65b9\u6cd5\uff1a\u4e24\u9636\u6bb5\u6a21\u578b\u548c\u7aef\u5230\u7aef\u8054\u5408\u9884\u6d4b\u6a21\u578b\u3002", "result": "foveated\u673a\u5668\u4eba\u89c6\u89c9\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u7684\u6027\u80fd\u548c\u5bf9\u6297\u672a\u89c1\u5e72\u6270\u7269\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u4eba\u7c7b\u542f\u53d1\u7684\u89c6\u89c9\u5904\u7406\u4e3a\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u76ca\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2507.14268", "categories": ["cs.CV", "cond-mat.mtrl-sci", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.14268", "abs": "https://arxiv.org/abs/2507.14268", "authors": ["Andreas Alpers", "Orkun Furat", "Christian Jung", "Matthias Neumann", "Claudia Redenbach", "Aigerim Saken", "Volker Schmidt"], "title": "Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data", "comment": "31 pages, 16 figures, 8 tables", "summary": "This paper presents a comparative analysis of algorithmic strategies for\nfitting tessellation models to 3D image data of materials such as polycrystals\nand foams. In this steadily advancing field, we review and assess\noptimization-based methods -- including linear and nonlinear programming,\nstochastic optimization via the cross-entropy method, and gradient descent --\nfor generating Voronoi, Laguerre, and generalized balanced power diagrams\n(GBPDs) that approximate voxelbased grain structures. The quality of fit is\nevaluated on real-world datasets using discrepancy measures that quantify\ndifferences in grain volume, surface area, and topology. Our results highlight\ntrade-offs between model complexity, the complexity of the optimization\nroutines involved, and the quality of approximation, providing guidance for\nselecting appropriate methods based on data characteristics and application\nneeds.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u7b97\u6cd5\u7b56\u7565\u57283D\u56fe\u50cf\u6570\u636e\u9576\u5d4c\u6a21\u578b\u62df\u5408\u4e2d\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u4e86\u4f18\u5316\u65b9\u6cd5\u7684\u6548\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u9009\u62e9\u65b9\u6cd5\u7684\u6307\u5bfc\u3002", "motivation": "\u57283D\u56fe\u50cf\u6570\u636e\uff08\u5982\u591a\u6676\u548c\u6ce1\u6cab\u6750\u6599\uff09\u7684\u9576\u5d4c\u6a21\u578b\u62df\u5408\u9886\u57df\uff0c\u6bd4\u8f83\u548c\u8bc4\u4f30\u4e0d\u540c\u7684\u7b97\u6cd5\u7b56\u7565\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u6b65\u3002", "method": "\u8bba\u6587\u56de\u987e\u5e76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u89c4\u5212\u3001\u901a\u8fc7\u4ea4\u53c9\u71b5\u65b9\u6cd5\u7684\u968f\u673a\u4f18\u5316\u4ee5\u53ca\u68af\u5ea6\u4e0b\u964d\uff0c\u7528\u4e8e\u751f\u6210Voronoi\u3001Laguerre\u548c\u5e7f\u4e49\u5e73\u8861\u529f\u7387\u56fe\uff08GBPDs\uff09\u6765\u8fd1\u4f3c\u57fa\u4e8e\u4f53\u7d20\u7684\u6676\u7c92\u7ed3\u6784\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u62df\u5408\u8d28\u91cf\uff0c\u4f7f\u7528\u5dee\u5f02\u5ea6\u91cf\u91cf\u5316\u4e86\u6676\u7c92\u4f53\u79ef\u3001\u8868\u9762\u79ef\u548c\u62d3\u6251\u7ed3\u6784\u7684\u5dee\u5f02\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u57fa\u4e8e\u6570\u636e\u7279\u6027\u548c\u5e94\u7528\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u7b97\u6cd5\u7b56\u7565\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u6a21\u578b\u590d\u6742\u6027\u3001\u4f18\u5316\u7a0b\u5e8f\u590d\u6742\u6027\u548c\u8fd1\u4f3c\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2507.15761", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15761", "abs": "https://arxiv.org/abs/2507.15761", "authors": ["Jingyi Zheng", "Zifan Peng", "Yule Liu", "Junfeng Wang", "Yifan Liao", "Wenhan Dong", "Xinlei He"], "title": "GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts", "comment": null, "summary": "Smart contracts are trustworthy, immutable, and automatically executed\nprograms on the blockchain. Their execution requires the Gas mechanism to\nensure efficiency and fairness. However, due to non-optimal coding practices,\nmany contracts contain Gas waste patterns that need to be optimized. Existing\nsolutions mostly rely on manual discovery, which is inefficient, costly to\nmaintain, and difficult to scale. Recent research uses large language models\n(LLMs) to explore new Gas waste patterns. However, it struggles to remain\ncompatible with existing patterns, often produces redundant patterns, and\nrequires manual validation/rewriting. To address this gap, we present GasAgent,\nthe first multi-agent system for smart contract Gas optimization that combines\ncompatibility with existing patterns and automated discovery/validation of new\npatterns, enabling end-to-end optimization. GasAgent consists of four\nspecialized agents, Seeker, Innovator, Executor, and Manager, that collaborate\nin a closed loop to identify, validate, and apply Gas-saving improvements.\nExperiments on 100 verified real-world contracts demonstrate that GasAgent\nsuccessfully optimizes 82 contracts, achieving an average deployment Gas\nsavings of 9.97%. In addition, our evaluation confirms its compatibility with\nexisting tools and validates the effectiveness of each module through ablation\nstudies. To assess broader usability, we further evaluate 500 contracts\ngenerated by five representative LLMs across 10 categories and find that\nGasAgent optimizes 79.8% of them, with deployment Gas savings ranging from\n4.79% to 13.93%, showing its usability as the optimization layer for\nLLM-assisted smart contract development.", "AI": {"tldr": "GasAgent \u662f\u9996\u4e2a\u7ed3\u5408\u517c\u5bb9\u6027\u4e0e\u81ea\u52a8\u5316\u53d1\u73b0/\u9a8c\u8bc1\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u667a\u80fd\u5408\u7ea6 Gas \u4f18\u5316\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u4e14\u5e7f\u6cdb\u9002\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u4f9d\u8d56\u4e8e\u4eba\u5de5\u53d1\u73b0 Gas \u6d6a\u8d39\u6a21\u5f0f\uff0c\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u8fd1\u671f\u7814\u7a76\u5c1d\u8bd5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a2\u7d22\u65b0\u6a21\u5f0f\uff0c\u4f46\u5b58\u5728\u517c\u5bb9\u6027\u5dee\u3001\u5197\u4f59\u6a21\u5f0f\u591a\u548c\u9700\u624b\u52a8\u9a8c\u8bc1\u7684\u95ee\u9898\u3002", "method": "GasAgent \u7531\u56db\u4e2a\u4e13\u95e8\u5316\u7684\u667a\u80fd\u4f53\uff08Seeker\u3001Innovator\u3001Executor \u548c Manager\uff09\u7ec4\u6210\uff0c\u901a\u8fc7\u95ed\u73af\u534f\u4f5c\u8bc6\u522b\u3001\u9a8c\u8bc1\u548c\u5e94\u7528 Gas \u8282\u7701\u6539\u8fdb\u3002", "result": "\u5728 100 \u4e2a\u5df2\u9a8c\u8bc1\u7684\u771f\u5b9e\u5408\u7ea6\u4e2d\uff0cGasAgent \u6210\u529f\u4f18\u5316\u4e86 82 \u4e2a\uff0c\u5e73\u5747\u90e8\u7f72 Gas \u8282\u7701\u4e3a 9.97%\u3002\u6b64\u5916\uff0c\u5bf9 500 \u4e2a\u7531 LLMs \u751f\u6210\u7684\u5408\u7ea6\u8bc4\u4f30\u663e\u793a\uff0cGasAgent \u4f18\u5316\u4e86 79.8%\uff0cGas \u8282\u7701\u8303\u56f4\u4e3a 4.79% \u81f3 13.93%\u3002", "conclusion": "GasAgent \u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u4f18\u5316\u667a\u80fd\u5408\u7ea6\u7684 Gas \u6d88\u8017\uff0c\u540c\u65f6\u517c\u5bb9\u73b0\u6709\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6a21\u5757\u7684\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.14456", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14456", "abs": "https://arxiv.org/abs/2507.14456", "authors": ["Chi Wan", "Yixin Cui", "Jiatong Du", "Shuo Yang", "Yulong Bai", "Yanjun Huang"], "title": "GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving requires adaptive and robust handling of\ncomplex and diverse traffic environments. However, prevalent single-mode\nplanning methods attempt to learn an overall policy while struggling to acquire\ndiversified driving skills to handle diverse scenarios. Therefore, this paper\nproposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework\nfeaturing a Global Expert, a Scene-Adaptive Experts Group, and equipped with a\nDual-aware Router. Specifically, the Global Expert is trained on the overall\ndataset, possessing robust performance. The Scene-Adaptive Experts are trained\non corresponding scene subsets, achieving adaptive performance. The Dual-aware\nRouter simultaneously considers scenario-level features and routing uncertainty\nto dynamically activate expert modules. Through the effective coupling of the\nGlobal Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,\nGEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS\noutperforms existing methods in the Bench2Drive closed-loop benchmark and\nachieves state-of-the-art performance in Driving Score and Success Rate, even\nwith only monocular vision input. Furthermore, ablation studies demonstrate\nsignificant improvements over the original single-expert baseline: 7.67% in\nDriving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The\ncode will be available at https://github.com/newbrains1/GEMINUS.", "AI": {"tldr": "GEMINUS\u662f\u4e00\u4e2a\u6df7\u5408\u4e13\u5bb6\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\u7ec4\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6837\u5316\u573a\u666f\u4e0b\u7684\u9a7e\u9a76\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u6a21\u5f0f\u89c4\u5212\u65b9\u6cd5\u96be\u4ee5\u5b66\u4e60\u591a\u6837\u5316\u7684\u9a7e\u9a76\u6280\u80fd\u4ee5\u5e94\u5bf9\u590d\u6742\u591a\u53d8\u7684\u4ea4\u901a\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e86GEMINUS\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u5168\u5c40\u4e13\u5bb6\u3001\u4e00\u4e2a\u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\u7ec4\u548c\u4e00\u4e2a\u53cc\u611f\u77e5\u8def\u7531\u5668\u3002\u5168\u5c40\u4e13\u5bb6\u5728\u6574\u4f53\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\u5728\u5bf9\u5e94\u573a\u666f\u5b50\u96c6\u4e0a\u8bad\u7ec3\uff0c\u53cc\u611f\u77e5\u8def\u7531\u5668\u52a8\u6001\u6fc0\u6d3b\u4e13\u5bb6\u6a21\u5757\u3002", "result": "GEMINUS\u5728Bench2Drive\u95ed\u73af\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u9a7e\u9a76\u5206\u6570\u548c\u6210\u529f\u7387\u4e0a\u8fbe\u5230\u6700\u65b0\u6c34\u5e73\uff0c\u4e14\u4ec5\u9700\u5355\u76ee\u89c6\u89c9\u8f93\u5165\u3002\u6d88\u878d\u7814\u7a76\u663e\u793a\uff0c\u76f8\u6bd4\u5355\u4e13\u5bb6\u57fa\u7ebf\uff0c\u9a7e\u9a76\u5206\u6570\u63d0\u53477.67%\uff0c\u6210\u529f\u7387\u63d0\u534722.06%\uff0c\u591a\u80fd\u529b\u5747\u503c\u63d0\u534719.41%\u3002", "conclusion": "GEMINUS\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u4e13\u5bb6\u548c\u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\u7ec4\uff0c\u4ee5\u53ca\u53cc\u611f\u77e5\u8def\u7531\u5668\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u6837\u5316\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u548c\u9c81\u68d2\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u8868\u73b0\u3002"}}
{"id": "2507.14303", "categories": ["cs.CV", "I.4.8"], "pdf": "https://arxiv.org/pdf/2507.14303", "abs": "https://arxiv.org/abs/2507.14303", "authors": ["Ehsan Rassekh"], "title": "Semantic Segmentation based Scene Understanding in Autonomous Vehicles", "comment": "74 pages, 35 figures, Master's Thesis, Institute for Advanced Studies\n  in Basic Sciences (IASBS), Zanjan, Iran, 2023", "summary": "In recent years, the concept of artificial intelligence (AI) has become a\nprominent keyword because it is promising in solving complex tasks. The need\nfor human expertise in specific areas may no longer be needed because machines\nhave achieved successful results using artificial intelligence and can make the\nright decisions in critical situations. This process is possible with the help\nof deep learning (DL), one of the most popular artificial intelligence\ntechnologies. One of the areas in which the use of DL is used is in the\ndevelopment of self-driving cars, which is very effective and important. In\nthis work, we propose several efficient models to investigate scene\nunderstanding through semantic segmentation. We use the BDD100k dataset to\ninvestigate these models. Another contribution of this work is the usage of\nseveral Backbones as encoders for models. The obtained results show that\nchoosing the appropriate backbone has a great effect on the performance of the\nmodel for semantic segmentation. Better performance in semantic segmentation\nallows us to understand better the scene and the environment around the agent.\nIn the end, we analyze and evaluate the proposed models in terms of accuracy,\nmean IoU, and loss function, and the results show that these metrics are\nimproved.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u51e0\u79cd\u9ad8\u6548\u6a21\u578b\u7814\u7a76\u8bed\u4e49\u5206\u5272\uff0c\u4f7f\u7528BDD100k\u6570\u636e\u96c6\u548c\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\u3002\u7ed3\u679c\u8868\u660e\uff0c\u9009\u62e9\u5408\u9002\u7684\u9aa8\u5e72\u7f51\u7edc\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6539\u8fdb\u573a\u666f\u7406\u89e3\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u5728\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u9ad8\u6548\u7684\u573a\u666f\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528BDD100k\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u51e0\u79cd\u9ad8\u6548\u6a21\u578b\u7814\u7a76\u8bed\u4e49\u5206\u5272\u7684\u573a\u666f\u7406\u89e3\uff0c\u5e76\u91c7\u7528\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\u4f5c\u4e3a\u7f16\u7801\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u5728\u51c6\u786e\u7387\u3001\u5e73\u5747IoU\u548c\u635f\u5931\u51fd\u6570\u7b49\u6307\u6807\u4e0a\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u9009\u62e9\u5408\u9002\u7684\u9aa8\u5e72\u7f51\u7edc\u5bf9\u8bed\u4e49\u5206\u5272\u6a21\u578b\u7684\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6539\u8fdb\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u573a\u666f\u548c\u73af\u5883\u3002"}}
{"id": "2507.15770", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15770", "abs": "https://arxiv.org/abs/2507.15770", "authors": ["Yifan Shen", "Zihan Zhao", "Xiao Xue", "Yuwei Guo", "Qun Ma", "Deyu Zhou", "Ming Zhang"], "title": "A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining", "comment": null, "summary": "With the rise of service computing, cloud computing, and IoT, service\necosystems are becoming increasingly complex. The intricate interactions among\nintelligent agents make abnormal emergence analysis challenging, as traditional\ncausal methods focus on individual trajectories. Large language models offer\nnew possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)\nreasoning to reveal agent intentions. However, existing approaches remain\nlimited to microscopic and static analysis. This paper introduces a framework:\nEmergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic\nand interpretable emergence analysis. EAMI first employs a dual-perspective\nthought track mechanism, where an Inspector Agent and an Analysis Agent extract\nagent intentions under bounded and perfect rationality. Then, k-means\nclustering identifies phase transition points in group intentions, followed by\na Intention Temporal Emergence diagram for dynamic analysis. The experiments\nvalidate EAMI in complex online-to-offline (O2O) service system and the\nStanford AI Town experiment, with ablation studies confirming its\neffectiveness, generalizability, and efficiency. This framework provides a\nnovel paradigm for abnormal emergence and causal analysis in service\necosystems. The code is available at\nhttps://anonymous.4open.science/r/EAMI-B085.", "AI": {"tldr": "EAMI\u6846\u67b6\u901a\u8fc7\u53cc\u89c6\u89d2\u601d\u7ef4\u8ffd\u8e2a\u548ck-means\u805a\u7c7b\uff0c\u52a8\u6001\u5206\u6790\u667a\u80fd\u4f53\u610f\u56fe\u7684\u6d8c\u73b0\uff0c\u4e3a\u590d\u6742\u670d\u52a1\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u65b0\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u670d\u52a1\u8ba1\u7b97\u3001\u4e91\u8ba1\u7b97\u548c\u7269\u8054\u7f51\u7684\u53d1\u5c55\uff0c\u670d\u52a1\u751f\u6001\u7cfb\u7edf\u65e5\u76ca\u590d\u6742\uff0c\u4f20\u7edf\u56e0\u679c\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u667a\u80fd\u4f53\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u3002", "method": "EAMI\u6846\u67b6\u91c7\u7528\u53cc\u89c6\u89d2\u601d\u7ef4\u8ffd\u8e2a\u673a\u5236\uff0c\u7ed3\u5408k-means\u805a\u7c7b\u548c\u610f\u56fe\u65f6\u95f4\u6d8c\u73b0\u56fe\uff0c\u5b9e\u73b0\u52a8\u6001\u548c\u53ef\u89e3\u91ca\u7684\u6d8c\u73b0\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eEAMI\u5728\u590d\u6742O2O\u670d\u52a1\u7cfb\u7edf\u548cStanford AI Town\u5b9e\u9a8c\u4e2d\u6709\u6548\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3001\u901a\u7528\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684EAMI\u6846\u67b6\u4e3a\u670d\u52a1\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\u6d8c\u73b0\u548c\u56e0\u679c\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3001\u901a\u7528\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.14500", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14500", "abs": "https://arxiv.org/abs/2507.14500", "authors": ["Zhiyuan Hua", "Dehao Yuan", "Cornelia Ferm\u00fcller"], "title": "Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow", "comment": null, "summary": "This paper introduces a robust framework for motion segmentation and\negomotion estimation using event-based normal flow, tailored specifically for\nneuromorphic vision sensors. In contrast to traditional methods that rely\nheavily on optical flow or explicit depth estimation, our approach exploits the\nsparse, high-temporal-resolution event data and incorporates geometric\nconstraints between normal flow, scene structure, and inertial measurements.\nThe proposed optimization-based pipeline iteratively performs event\nover-segmentation, isolates independently moving objects via residual analysis,\nand refines segmentations using hierarchical clustering informed by motion\nsimilarity and temporal consistency. Experimental results on the EVIMO2v2\ndataset validate that our method achieves accurate segmentation and\ntranslational motion estimation without requiring full optical flow\ncomputation. This approach demonstrates significant advantages at object\nboundaries and offers considerable potential for scalable, real-time robotic\nand navigation applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u8fdb\u884c\u8fd0\u52a8\u5206\u5272\u548c\u81ea\u8fd0\u52a8\u4f30\u8ba1\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u4e8b\u4ef6\u548c\u51e0\u4f55\u7ea6\u675f\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u7ed3\u679c\u3002", "motivation": "\u9488\u5bf9\u4f20\u7edf\u65b9\u6cd5\u5728\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u8fd0\u52a8\u5206\u5272\u548c\u81ea\u8fd0\u52a8\u4f30\u8ba1\u4e2d\u4f9d\u8d56\u5bc6\u96c6\u5149\u6d41\u6216\u663e\u5f0f\u6df1\u5ea6\u4f30\u8ba1\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7a00\u758f\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u4e8b\u4ef6\u6570\u636e\u7684\u65b0\u6846\u67b6\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u6d41\u7a0b\uff0c\u5305\u62ec\u4e8b\u4ef6\u8fc7\u5206\u5272\u3001\u901a\u8fc7\u6b8b\u5dee\u5206\u6790\u5206\u79bb\u72ec\u7acb\u79fb\u52a8\u5bf9\u8c61\uff0c\u4ee5\u53ca\u5229\u7528\u8fd0\u52a8\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u8fdb\u884c\u5c42\u6b21\u805a\u7c7b\u6765\u7ec6\u5316\u5206\u5272\u3002", "result": "\u5728EVIMO2v2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u5b8c\u6574\u5149\u6d41\u8ba1\u7b97\u5373\u53ef\u5b9e\u73b0\u51c6\u786e\u7684\u5206\u5272\u548c\u5e73\u79fb\u8fd0\u52a8\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7269\u4f53\u8fb9\u754c\u5904\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u53ef\u6269\u5c55\u3001\u5b9e\u65f6\u673a\u5668\u4eba\u548c\u5bfc\u822a\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.14312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14312", "abs": "https://arxiv.org/abs/2507.14312", "authors": ["Marc Lafon", "Gustavo Adolfo Vargas Hakim", "Cl\u00e9ment Rambour", "Christian Desrosier", "Nicolas Thome"], "title": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation", "comment": null, "summary": "Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities\nbut often fail to generalize under distribution shifts. Test-time adaptation\n(TTA) allows models to update at inference time without labeled data, typically\nvia entropy minimization. However, this objective is fundamentally misaligned\nwith the contrastive image-text training of VLMs, limiting adaptation\nperformance and introducing failure modes such as pseudo-label drift and class\ncollapse. We propose CLIPTTA, a new gradient-based TTA method for\nvision-language models that leverages a soft contrastive loss aligned with\nCLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's\ngradients, showing how its batch-aware design mitigates the risk of collapse.\nWe further extend CLIPTTA to the open-set setting, where both in-distribution\n(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier\nContrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75\ndatasets spanning diverse distribution shifts, CLIPTTA consistently outperforms\nentropy-based objectives and is highly competitive with state-of-the-art TTA\nmethods, outperforming them on a large number of datasets and exhibiting more\nstable performance across diverse shifts.", "AI": {"tldr": "CLIPTTA\u901a\u8fc7\u8f6f\u5bf9\u6bd4\u635f\u5931\u6539\u8fdbVLMs\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\uff0c\u907f\u514d\u5d29\u6e83\u5e76\u5728\u591a\u6837\u5206\u5e03\u504f\u79fb\u4e2d\u7a33\u5b9a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709TTA\u65b9\u6cd5\uff08\u5982\u71b5\u6700\u5c0f\u5316\uff09\u4e0eVLMs\u7684\u5bf9\u6bd4\u56fe\u50cf-\u6587\u672c\u8bad\u7ec3\u76ee\u6807\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u9002\u5e94\u6027\u80fd\u53d7\u9650\u53ca\u4f2a\u6807\u7b7e\u6f02\u79fb\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faCLIPTTA\uff0c\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684TTA\u65b9\u6cd5\uff0c\u91c7\u7528\u4e0eCLIP\u9884\u8bad\u7ec3\u76ee\u6807\u4e00\u81f4\u7684\u8f6f\u5bf9\u6bd4\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5176\u68af\u5ea6\u8bbe\u8ba1\u4ee5\u907f\u514d\u5d29\u6e83\u3002", "result": "CLIPTTA\u5728\u5f00\u653e\u96c6\u8bbe\u7f6e\u4e2d\u901a\u8fc7OCE\u635f\u5931\u6539\u8fdbOOD\u68c0\u6d4b\uff0c\u5e76\u5728\u591a\u6837\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CLIPTTA\u572875\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u71b5\u7684\u76ee\u6807\uff0c\u5e76\u5728\u591a\u79cd\u5206\u5e03\u504f\u79fb\u4e0b\u5c55\u73b0\u51fa\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u3002"}}
{"id": "2507.15796", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15796", "abs": "https://arxiv.org/abs/2507.15796", "authors": ["Nuria Rodr\u00edguez-Barroso", "Mario Garc\u00eda-M\u00e1rquez", "M. Victoria Luz\u00f3n", "Francisco Herrera"], "title": "Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work", "comment": null, "summary": "In recent years, the development of Trustworthy Artificial Intelligence (TAI)\nhas emerged as a critical objective in the deployment of AI systems across\nsensitive and high-risk domains. TAI frameworks articulate a comprehensive set\nof ethical, legal, and technical requirements to ensure that AI technologies\nare aligned with human values, rights, and societal expectations. Among the\nvarious AI paradigms, Federated Learning (FL) presents a promising solution to\npressing privacy concerns. However, aligning FL with the rest of the\nrequirements of TAI presents a series of challenges, most of which arise from\nits inherently distributed nature. In this work, we adopt the requirements TAI\nas a guiding structure to systematically analyze the challenges of adapting FL\nto TAI. Specifically, we classify and examine the key obstacles to aligning FL\nwith TAI, providing a detailed exploration of what has been done, the trends,\nand the remaining work within each of the identified challenges.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u6ee1\u8db3\u53ef\u4fe1\u4eba\u5de5\u667a\u80fd\uff08TAI\uff09\u8981\u6c42\u65f6\u7684\u6311\u6218\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u7814\u7a76\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u56e0\u5176\u5206\u5e03\u5f0f\u7279\u6027\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4e0e\u53ef\u4fe1\u4eba\u5de5\u667a\u80fd\uff08TAI\uff09\u7684\u5176\u4ed6\u8981\u6c42\u5bf9\u9f50\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u91c7\u7528TAI\u7684\u8981\u6c42\u4f5c\u4e3a\u6307\u5bfc\u7ed3\u6784\uff0c\u7cfb\u7edf\u5206\u7c7b\u548c\u5206\u6790\u4e86FL\u5728\u9002\u5e94TAI\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u969c\u788d\uff0c\u5e76\u8be6\u7ec6\u63a2\u8ba8\u4e86\u73b0\u6709\u7814\u7a76\u3001\u8d8b\u52bf\u548c\u672a\u89e3\u51b3\u95ee\u9898\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u4e86FL\u4e0eTAI\u5bf9\u9f50\u7684\u4e3b\u8981\u6311\u6218\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u8fdb\u5c55\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u53ef\u4fe1\u4eba\u5de5\u667a\u80fd\uff08TAI\uff09\u6846\u67b6\u4e0b\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u4e86FL\u4e0eTAI\u5bf9\u9f50\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.14809", "categories": ["cs.CV", "cs.MM", "cs.RO", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2507.14809", "abs": "https://arxiv.org/abs/2507.14809", "authors": ["Zesen Zhong", "Duomin Zhang", "Yijia Li"], "title": "Light Future: Multimodal Action Frame Prediction via InstructPix2Pix", "comment": "9 pages including appendix, 5 tables, 8 figures, to be submitted to\n  WACV 2026", "summary": "Predicting future motion trajectories is a critical capability across domains\nsuch as robotics, autonomous systems, and human activity forecasting, enabling\nsafer and more intelligent decision-making. This paper proposes a novel,\nefficient, and lightweight approach for robot action prediction, offering\nsignificantly reduced computational cost and inference latency compared to\nconventional video prediction models. Importantly, it pioneers the adaptation\nof the InstructPix2Pix model for forecasting future visual frames in robotic\ntasks, extending its utility beyond static image editing. We implement a deep\nlearning-based visual prediction framework that forecasts what a robot will\nobserve 100 frames (10 seconds) into the future, given a current image and a\ntextual instruction. We repurpose and fine-tune the InstructPix2Pix model to\naccept both visual and textual inputs, enabling multimodal future frame\nprediction. Experiments on the RoboTWin dataset (generated based on real-world\nscenarios) demonstrate that our method achieves superior SSIM and PSNR compared\nto state-of-the-art baselines in robot action prediction tasks. Unlike\nconventional video prediction models that require multiple input frames, heavy\ncomputation, and slow inference latency, our approach only needs a single image\nand a text prompt as input. This lightweight design enables faster inference,\nreduced GPU demands, and flexible multimodal control, particularly valuable for\napplications like robotics and sports motion trajectory analytics, where motion\ntrajectory precision is prioritized over visual fidelity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u672a\u6765\u5e27\u9884\u6d4b\u65b9\u6cd5\uff0c\u6539\u8fdbInstructPix2Pix\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5728\u673a\u5668\u4eba\u52a8\u4f5c\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9884\u6d4b\u672a\u6765\u8fd0\u52a8\u8f68\u8ff9\u5728\u673a\u5668\u4eba\u3001\u81ea\u4e3b\u7cfb\u7edf\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u89c6\u9891\u9884\u6d4b\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u63a8\u7406\u5ef6\u8fdf\u5927\u3002\u8bba\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u89c6\u89c9\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u548c\u5fae\u8c03InstructPix2Pix\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u63a5\u53d7\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u672a\u6765\u5e27\u9884\u6d4b\u3002", "result": "\u5728RoboTWin\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728SSIM\u548cPSNR\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u4e14\u4ec5\u9700\u5355\u5f20\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u5373\u53ef\u5b9e\u73b0\u5feb\u901f\u63a8\u7406\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u52a8\u4f5c\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2507.14315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14315", "abs": "https://arxiv.org/abs/2507.14315", "authors": ["Qiyu Xu", "Zhanxuan Hu", "Yu Duan", "Ercheng Pei", "Yonghang Tai"], "title": "A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention", "comment": null, "summary": "Generalized Category Discovery (GCD) aims to classify unlabeled data from\nboth known and unknown categories by leveraging knowledge from labeled known\ncategories. While existing methods have made notable progress, they often\noverlook a hidden stumbling block in GCD: distracted attention. Specifically,\nwhen processing unlabeled data, models tend to focus not only on key objects in\nthe image but also on task-irrelevant background regions, leading to suboptimal\nfeature extraction. To remove this stumbling block, we propose Attention\nFocusing (AF), an adaptive mechanism designed to sharpen the model's focus by\npruning non-informative tokens. AF consists of two simple yet effective\ncomponents: Token Importance Measurement (TIME) and Token Adaptive Pruning\n(TAP), working in a cascade. TIME quantifies token importance across multiple\nscales, while TAP prunes non-informative tokens by utilizing the multi-scale\nimportance scores provided by TIME. AF is a lightweight, plug-and-play module\nthat integrates seamlessly into existing GCD methods with minimal computational\noverhead. When incorporated into one prominent GCD method, SimGCD, AF achieves\nup to 15.4% performance improvement over the baseline with minimal\ncomputational overhead. The implementation code is provided in\nhttps://github.com/Afleve/AFGCD.", "AI": {"tldr": "AF\u673a\u5236\u901a\u8fc7\u526a\u9664\u975e\u4fe1\u606f\u6027\u6807\u8bb0\uff0c\u663e\u8457\u63d0\u5347GCD\u6027\u80fd\uff0c\u96c6\u6210\u540e\u6027\u80fd\u63d0\u534715.4%\u3002", "motivation": "\u73b0\u6709GCD\u65b9\u6cd5\u5728\u5904\u7406\u672a\u6807\u8bb0\u6570\u636e\u65f6\uff0c\u6a21\u578b\u6ce8\u610f\u529b\u5bb9\u6613\u5206\u6563\u5230\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u80cc\u666f\u533a\u57df\uff0c\u5bfc\u81f4\u7279\u5f81\u63d0\u53d6\u4e0d\u7406\u60f3\u3002", "method": "AF\u7531\u4e24\u4e2a\u7ec4\u4ef6\u7ec4\u6210\uff1a\u6807\u8bb0\u91cd\u8981\u6027\u5ea6\u91cf\uff08TIME\uff09\u548c\u591a\u5c3a\u5ea6\u6807\u8bb0\u81ea\u9002\u5e94\u526a\u679d\uff08TAP\uff09\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u91cd\u8981\u6027\u8bc4\u5206\u526a\u9664\u975e\u4fe1\u606f\u6027\u6807\u8bb0\u3002", "result": "AF\u88ab\u96c6\u6210\u5230SimGCD\u65b9\u6cd5\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe15.4%\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u6ce8\u610f\u529b\u805a\u7126\uff08AF\uff09\u673a\u5236\uff0c\u901a\u8fc7\u526a\u9664\u975e\u4fe1\u606f\u6027\u6807\u8bb0\u6765\u4f18\u5316\u7279\u5f81\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\uff08GCD\uff09\u7684\u6027\u80fd\u3002"}}
{"id": "2507.15842", "categories": ["cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15842", "abs": "https://arxiv.org/abs/2507.15842", "authors": ["Sara LaPlante", "Emilija Perkovi\u0107"], "title": "Identifying Conditional Causal Effects in MPDAGs", "comment": "67 pages, 8 figures", "summary": "We consider identifying a conditional causal effect when a graph is known up\nto a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG\nrepresents an equivalence class of graphs that is restricted by background\nknowledge and where all variables in the causal model are observed. We provide\nthree results that address identification in this setting: an identification\nformula when the conditioning set is unaffected by treatment, a generalization\nof the well-known do calculus to the MPDAG setting, and an algorithm that is\ncomplete for identifying these conditional effects.", "AI": {"tldr": "\u672c\u6587\u5728MPDAG\u8bbe\u7f6e\u4e0b\u63d0\u51fa\u4e86\u6761\u4ef6\u56e0\u679c\u6548\u5e94\u7684\u8bc6\u522b\u65b9\u6cd5\uff0c\u5305\u62ec\u8bc6\u522b\u516c\u5f0f\u3001do calculus\u63a8\u5e7f\u548c\u5b8c\u6574\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728MPDAG\u8bbe\u7f6e\u4e0b\u6761\u4ef6\u56e0\u679c\u6548\u5e94\u7684\u8bc6\u522b\u95ee\u9898\uff0c\u6269\u5c55\u73b0\u6709\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790MPDAG\uff08\u6700\u5927\u5b9a\u5411\u90e8\u5206\u6709\u5411\u65e0\u73af\u56fe\uff09\u7684\u7b49\u4ef7\u7c7b\uff0c\u7ed3\u5408\u80cc\u666f\u77e5\u8bc6\u548c\u6240\u6709\u53d8\u91cf\u7684\u89c2\u6d4b\u6570\u636e\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc6\u522b\u516c\u5f0f\u3001do calculus\u7684\u63a8\u5e7f\u4ee5\u53ca\u4e00\u4e2a\u5b8c\u6574\u7684\u8bc6\u522b\u7b97\u6cd5\uff0c\u4e3aMPDAG\u8bbe\u7f6e\u4e0b\u7684\u6761\u4ef6\u56e0\u679c\u6548\u5e94\u8bc6\u522b\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u65b9\u6cd5\u652f\u6301\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u5728MPDAG\u8bbe\u7f6e\u4e0b\u8bc6\u522b\u6761\u4ef6\u56e0\u679c\u6548\u5e94\u7684\u4e09\u4e2a\u7ed3\u679c\uff0c\u5305\u62ec\u4e00\u4e2a\u8bc6\u522b\u516c\u5f0f\u3001do calculus\u7684\u63a8\u5e7f\u4ee5\u53ca\u4e00\u4e2a\u5b8c\u6574\u7684\u8bc6\u522b\u7b97\u6cd5\u3002"}}
{"id": "2507.15036", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15036", "abs": "https://arxiv.org/abs/2507.15036", "authors": ["Lyes Saad Saoud", "Irfan Hussain"], "title": "EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring", "comment": null, "summary": "Underwater image enhancement is vital for marine conservation, particularly\ncoral reef monitoring. However, AI-based enhancement models often face dataset\nbias, high computational costs, and lack of transparency, leading to potential\nmisinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware\nAI framework to address these challenges. EBA-AI leverages CLIP embeddings to\ndetect and mitigate dataset bias, ensuring balanced representation across\nvaried underwater environments. It also integrates adaptive processing to\noptimize energy efficiency, significantly reducing GPU usage while maintaining\ncompetitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100\nshow that while PSNR drops by a controlled 1.0 dB, computational savings enable\nreal-time feasibility for large-scale marine monitoring. Additionally,\nuncertainty estimation and explainability techniques enhance trust in AI-driven\nenvironmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,\nWaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing\nefficiency, fairness, and interpretability in underwater image processing. By\naddressing key limitations of AI-driven enhancement, this work contributes to\nsustainable, bias-aware, and computationally efficient marine conservation\nefforts. For interactive visualizations, animations, source code, and access to\nthe preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/", "AI": {"tldr": "EBA-AI\u662f\u4e00\u4e2a\u57fa\u4e8e\u4f26\u7406\u7684\u504f\u7f6e\u611f\u77e5AI\u6846\u67b6\uff0c\u901a\u8fc7CLIP\u5d4c\u5165\u548c\u81ea\u9002\u5e94\u5904\u7406\u4f18\u5316\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\uff0c\u5e73\u8861\u6548\u7387\u3001\u516c\u5e73\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u6d77\u6d0b\u4fdd\u62a4\u3002", "motivation": "\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u5bf9\u6d77\u6d0b\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709AI\u6a21\u578b\u5b58\u5728\u6570\u636e\u96c6\u504f\u7f6e\u3001\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u7f3a\u4e4f\u900f\u660e\u5ea6\u7b49\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u89e3\u3002EBA-AI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "EBA-AI\u5229\u7528CLIP\u5d4c\u5165\u68c0\u6d4b\u548c\u7f13\u89e3\u6570\u636e\u96c6\u504f\u7f6e\uff0c\u5e76\u96c6\u6210\u81ea\u9002\u5e94\u5904\u7406\u4ee5\u4f18\u5316\u80fd\u6e90\u6548\u7387\uff0c\u540c\u65f6\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u53ef\u89e3\u91ca\u6027\u6280\u672f\u589e\u5f3aAI\u9a71\u52a8\u7684\u73af\u5883\u51b3\u7b56\u7684\u4fe1\u4efb\u5ea6\u3002", "result": "\u5728LSUI400\u3001Oceanex\u548cUIEB100\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEBA-AI\u5728PSNR\u4ec5\u4e0b\u964d1.0 dB\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u6d77\u6d0b\u76d1\u6d4b\u7684\u5b9e\u65f6\u53ef\u884c\u6027\u3002", "conclusion": "EBA-AI\u6846\u67b6\u901a\u8fc7\u5e73\u8861\u6548\u7387\u3001\u516c\u5e73\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u6c34\u4e0b\u56fe\u50cf\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u6301\u7eed\u3001\u504f\u7f6e\u611f\u77e5\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u6d77\u6d0b\u4fdd\u62a4\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2507.14367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14367", "abs": "https://arxiv.org/abs/2507.14367", "authors": ["Weiming Ren", "Raghav Goyal", "Zhiming Hu", "Tristan Ty Aumentado-Armstrong", "Iqbal Mohomed", "Alex Levinshtein"], "title": "Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution", "comment": "12 pages, 17 figures and 7 tables", "summary": "Generative super-resolution (GSR) currently sets the state-of-the-art in\nterms of perceptual image quality, overcoming the \"regression-to-the-mean\" blur\nof prior non-generative models. However, from a human perspective, such models\ndo not fully conform to the optimal balance between quality and fidelity.\nInstead, a different class of artifacts, in which generated details fail to\nperceptually match the low resolution image (LRI) or ground-truth image (GTI),\nis a critical but under studied issue in GSR, limiting its practical\ndeployments. In this work, we focus on measuring, analyzing, and mitigating\nthese artifacts (i.e., \"hallucinations\"). We observe that hallucinations are\nnot well-characterized with existing image metrics or quality models, as they\nare orthogonal to both exact fidelity and no-reference quality. Instead, we\ntake advantage of a multimodal large language model (MLLM) by constructing a\nprompt that assesses hallucinatory visual elements and generates a\n\"Hallucination Score\" (HS). We find that our HS is closely aligned with human\nevaluations, and also provides complementary insights to prior image metrics\nused for super-resolution (SR) models. In addition, we find certain deep\nfeature distances have strong correlations with HS. We therefore propose to\nalign the GSR models by using such features as differentiable reward functions\nto mitigate hallucinations.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u5229\u7528MLLM\u751f\u6210\u7684HS\u548c\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u4f5c\u4e3a\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u51cf\u5c11GSR\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u4f2a\u5f71\uff0c\u63d0\u5347\u611f\u77e5\u8d28\u91cf\u4e0e\u4fdd\u771f\u5ea6\u7684\u5e73\u8861\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f\u8d85\u5206\u8fa8\u7387\uff08GSR\uff09\u6a21\u578b\u5728\u611f\u77e5\u56fe\u50cf\u8d28\u91cf\u4e0a\u867d\u9886\u5148\uff0c\u4f46\u5b58\u5728\u751f\u6210\u7684\u7ec6\u8282\u4e0e\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff08LRI\uff09\u6216\u771f\u5b9e\u56fe\u50cf\uff08GTI\uff09\u4e0d\u5339\u914d\u7684\u5e7b\u89c9\u4f2a\u5f71\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u5e7b\u89c9\u89c6\u89c9\u5143\u7d20\u7684\u63d0\u793a\uff0c\u5229\u7528MLLM\u751f\u6210HS\uff0c\u5e76\u53d1\u73b0\u7279\u5b9a\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u4e0eHS\u5f3a\u76f8\u5173\uff0c\u8fdb\u800c\u63d0\u51fa\u5c06\u8fd9\u4e9b\u7279\u5f81\u4f5c\u4e3a\u53ef\u5fae\u5206\u5956\u52b1\u51fd\u6570\u6765\u5bf9\u9f50GSR\u6a21\u578b\u3002", "result": "HS\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u4e3a\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u6a21\u578b\u7684\u56fe\u50cf\u5ea6\u91cf\u63d0\u4f9b\u4e86\u4e92\u8865\u6027\u89c1\u89e3\u3002\u540c\u65f6\uff0c\u67d0\u4e9b\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u4e0eHS\u6709\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u751f\u6210\u7684\u201c\u5e7b\u89c9\u5206\u6570\u201d\uff08HS\uff09\u53ca\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u4f5c\u4e3a\u53ef\u5fae\u5206\u5956\u52b1\u51fd\u6570\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u751f\u6210\u5f0f\u8d85\u5206\u8fa8\u7387\uff08GSR\uff09\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u4f2a\u5f71\uff0c\u63d0\u5347\u6a21\u578b\u7684\u611f\u77e5\u8d28\u91cf\u4e0e\u4fdd\u771f\u5ea6\u5e73\u8861\u3002"}}
{"id": "2507.15844", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15844", "abs": "https://arxiv.org/abs/2507.15844", "authors": ["Shangke Lyu", "Linjuan Wu", "Yuchen Yan", "Xingyu Wu", "Hao Li", "Yongliang Shen", "Peisheng Jiang", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning", "comment": "Code: https://github.com/zju-real/hbpo Project\n  Page:https://zju-real.github.io/hbpo/", "summary": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.", "AI": {"tldr": "HBPO\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u9884\u7b97\u63a2\u7d22\u548c\u5dee\u5f02\u5316\u5956\u52b1\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u4e14\u4e0d\u727a\u7272\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u5e7f\u6cdb\u751f\u6210\u601d\u7ef4\u94fe\u65f6\u8868\u73b0\u51fa\u663e\u8457\u7684\u8ba1\u7b97\u4f4e\u6548\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u8bba\u95ee\u9898\u590d\u6742\u5ea6\u5982\u4f55\u5747\u91c7\u7528\u7edf\u4e00\u7684\u63a8\u7406\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u5c42\u9884\u7b97\u7b56\u7565\u4f18\u5316\uff08HBPO\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u9884\u7b97\u63a2\u7d22\u548c\u5dee\u5f02\u5316\u5956\u52b1\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u81ea\u52a8\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u3002", "result": "HBPO\u5728\u56db\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51cf\u5c11\u4e8660.6%\u7684\u4ee4\u724c\u4f7f\u7528\uff0c\u540c\u65f6\u51c6\u786e\u7387\u63d0\u9ad8\u4e863.14%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u63a8\u7406\u6548\u7387\u548c\u80fd\u529b\u5e76\u975e\u56fa\u6709\u51b2\u7a81\uff0c\u901a\u8fc7\u9002\u5f53\u7ed3\u6784\u5316\u7684\u5206\u5c42\u8bad\u7ec3\u53ef\u4ee5\u540c\u65f6\u4f18\u5316\u4e24\u8005\u3002"}}
{"id": "2507.15089", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15089", "abs": "https://arxiv.org/abs/2507.15089", "authors": ["Ioannis Tsampikos Papapetros", "Ioannis Kansizoglou", "Antonios Gasteratos"], "title": "Visual Place Recognition for Large-Scale UAV Applications", "comment": null, "summary": "Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial\nVehicle (UAV) navigation, enabling robust localization across diverse\nenvironments. Despite significant advancements, aerial vPR faces unique\nchallenges due to the limited availability of large-scale, high-altitude\ndatasets, which limits model generalization, along with the inherent rotational\nambiguity in UAV imagery. To address these challenges, we introduce LASED, a\nlarge-scale aerial dataset with approximately one million images,\nsystematically sampled from 170,000 unique locations throughout Estonia over a\ndecade, offering extensive geographic and temporal diversity. Its structured\ndesign ensures clear place separation significantly enhancing model training\nfor aerial scenarios. Furthermore, we propose the integration of steerable\nConvolutional Neural Networks (CNNs) to explicitly handle rotational variance,\nleveraging their inherent rotational equivariance to produce robust,\norientation-invariant feature representations. Our extensive benchmarking\ndemonstrates that models trained on LASED achieve significantly higher recall\ncompared to those trained on smaller, less diverse datasets, highlighting the\nbenefits of extensive geographic coverage and temporal diversity. Moreover,\nsteerable CNNs effectively address rotational ambiguity inherent in aerial\nimagery, consistently outperforming conventional convolutional architectures,\nachieving on average 12\\% recall improvement over the best-performing\nnon-steerable network. By combining structured, large-scale datasets with\nrotation-equivariant neural networks, our approach significantly enhances model\nrobustness and generalization for aerial vPR.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86LASED\u5927\u89c4\u6a21\u822a\u7a7a\u6570\u636e\u96c6\u548c\u53ef\u8f6c\u5411CNNs\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u4e2d\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u7684\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7a7a\u4e2dvPR\u9762\u4e34\u5927\u89c4\u6a21\u9ad8\u6d77\u62d4\u6570\u636e\u96c6\u7a00\u7f3a\u548c\u65e0\u4eba\u673a\u56fe\u50cf\u56fa\u6709\u65cb\u8f6c\u6a21\u7cca\u6027\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faLASED\u5927\u89c4\u6a21\u822a\u7a7a\u6570\u636e\u96c6\u548c\u53ef\u8f6c\u5411\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u7684\u7ed3\u5408\uff0c\u4ee5\u5904\u7406\u65cb\u8f6c\u65b9\u5dee\u5e76\u751f\u6210\u9c81\u68d2\u7684\u3001\u65b9\u5411\u4e0d\u53d8\u7684\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728LASED\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u6bd4\u5728\u5c0f\u89c4\u6a21\u3001\u591a\u6837\u6027\u4e0d\u8db3\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u53ec\u56de\u7387\u663e\u8457\u63d0\u9ad8\u3002\u53ef\u8f6c\u5411CNNs\u5e73\u5747\u6bd4\u6700\u4f73\u975e\u53ef\u8f6c\u5411\u7f51\u7edc\u63d0\u9ad8\u4e8612%\u7684\u53ec\u56de\u7387\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u65cb\u8f6c\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u7a7a\u4e2d\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08vPR\uff09\u7684\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14368", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.14368", "abs": "https://arxiv.org/abs/2507.14368", "authors": ["Praneeth Namburi", "Roger Pallar\u00e8s-L\u00f3pez", "Jessica Rosendorf", "Duarte Folgado", "Brian W. Anthony"], "title": "DUSTrack: Semi-automated point tracking in ultrasound videos", "comment": null, "summary": "Ultrasound technology enables safe, non-invasive imaging of dynamic tissue\nbehavior, making it a valuable tool in medicine, biomechanics, and sports\nscience. However, accurately tracking tissue motion in B-mode ultrasound\nremains challenging due to speckle noise, low edge contrast, and out-of-plane\nmovement. These challenges complicate the task of tracking anatomical landmarks\nover time, which is essential for quantifying tissue dynamics in many clinical\nand research applications. This manuscript introduces DUSTrack (Deep learning\nand optical flow-based toolkit for UltraSound Tracking), a semi-automated\nframework for tracking arbitrary points in B-mode ultrasound videos. We combine\ndeep learning with optical flow to deliver high-quality and robust tracking\nacross diverse anatomical structures and motion patterns. The toolkit includes\na graphical user interface that streamlines the generation of high-quality\ntraining data and supports iterative model refinement. It also implements a\nnovel optical-flow-based filtering technique that reduces high-frequency\nframe-to-frame noise while preserving rapid tissue motion. DUSTrack\ndemonstrates superior accuracy compared to contemporary zero-shot point\ntrackers and performs on par with specialized methods, establishing its\npotential as a general and foundational tool for clinical and biomechanical\nresearch. We demonstrate DUSTrack's versatility through three use cases:\ncardiac wall motion tracking in echocardiograms, muscle deformation analysis\nduring reaching tasks, and fascicle tracking during ankle plantarflexion. As an\nopen-source solution, DUSTrack offers a powerful, flexible framework for point\ntracking to quantify tissue motion from ultrasound videos. DUSTrack is\navailable at https://github.com/praneethnamburi/DUSTrack.", "AI": {"tldr": "DUSTrack\u662f\u4e00\u4e2a\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u5149\u6d41\u7684\u534a\u81ea\u52a8\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u9ad8\u6548\u8ffd\u8e2aB\u578b\u8d85\u58f0\u89c6\u9891\u4e2d\u7684\u7ec4\u7ec7\u8fd0\u52a8\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "motivation": "\u51c6\u786e\u8ffd\u8e2aB\u578b\u8d85\u58f0\u4e2d\u7684\u7ec4\u7ec7\u8fd0\u52a8\u7531\u4e8e\u6591\u70b9\u566a\u58f0\u3001\u4f4e\u8fb9\u7f18\u5bf9\u6bd4\u5ea6\u548c\u5e73\u9762\u5916\u8fd0\u52a8\u7b49\u6311\u6218\u800c\u56f0\u96be\uff0c\u8fd9\u5bf9\u4e34\u5e8a\u548c\u7814\u7a76\u5e94\u7528\u4e2d\u91cf\u5316\u7ec4\u7ec7\u52a8\u529b\u5b66\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u5149\u6d41\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u534a\u81ea\u52a8\u6846\u67b6DUSTrack\uff0c\u7528\u4e8e\u8ffd\u8e2aB\u578b\u8d85\u58f0\u89c6\u9891\u4e2d\u7684\u4efb\u610f\u70b9\u3002\u8be5\u5de5\u5177\u5305\u5305\u542b\u56fe\u5f62\u7528\u6237\u754c\u9762\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u751f\u6210\u548c\u8fed\u4ee3\u6a21\u578b\u4f18\u5316\uff0c\u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u5149\u6d41\u7684\u65b0\u578b\u8fc7\u6ee4\u6280\u672f\u4ee5\u51cf\u5c11\u9ad8\u9891\u5e27\u95f4\u566a\u58f0\u3002", "result": "DUSTrack\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u96f6\u8d77\u70b9\u8ffd\u8e2a\u5668\uff0c\u4e0e\u4e13\u7528\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\u3002\u901a\u8fc7\u4e09\u4e2a\u7528\u4f8b\u5c55\u793a\u4e86\u5176\u591a\u529f\u80fd\u6027\uff1a\u5fc3\u810f\u58c1\u8fd0\u52a8\u8ffd\u8e2a\u3001\u808c\u8089\u53d8\u5f62\u5206\u6790\u548c\u7b4b\u819c\u8ffd\u8e2a\u3002", "conclusion": "DUSTrack\u4f5c\u4e3a\u4e00\u79cd\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u8d85\u58f0\u89c6\u9891\u4e2d\u7684\u7ec4\u7ec7\u8fd0\u52a8\u91cf\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u800c\u7075\u6d3b\u7684\u70b9\u8ffd\u8e2a\u6846\u67b6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u4e34\u5e8a\u5e94\u7528\u548c\u751f\u7269\u529b\u5b66\u7814\u7a76\u6f5c\u529b\u3002"}}
{"id": "2507.15851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15851", "abs": "https://arxiv.org/abs/2507.15851", "authors": ["Lingyu Li", "Yang Yao", "Yixu Wang", "Chubo Li", "Yan Teng", "Yingchun Wang"], "title": "The Other Mind: How Language Models Exhibit Human Temporal Cognition", "comment": "12 pages, 9 figures, 4 tables", "summary": "As Large Language Models (LLMs) continue to advance, they exhibit certain\ncognitive patterns similar to those of humans that are not directly specified\nin training data. This study investigates this phenomenon by focusing on\ntemporal cognition in LLMs. Leveraging the similarity judgment task, we find\nthat larger models spontaneously establish a subjective temporal reference\npoint and adhere to the Weber-Fechner law, whereby the perceived distance\nlogarithmically compresses as years recede from this reference point. To\nuncover the mechanisms behind this behavior, we conducted multiple analyses\nacross neuronal, representational, and informational levels. We first identify\na set of temporal-preferential neurons and find that this group exhibits\nminimal activation at the subjective reference point and implements a\nlogarithmic coding scheme convergently found in biological systems. Probing\nrepresentations of years reveals a hierarchical construction process, where\nyears evolve from basic numerical values in shallow layers to abstract temporal\norientation in deep layers. Finally, using pre-trained embedding models, we\nfound that the training corpus itself possesses an inherent, non-linear\ntemporal structure, which provides the raw material for the model's internal\nconstruction. In discussion, we propose an experientialist perspective for\nunderstanding these findings, where the LLMs' cognition is viewed as a\nsubjective construction of the external world by its internal representational\nsystem. This nuanced perspective implies the potential emergence of alien\ncognitive frameworks that humans cannot intuitively predict, pointing toward a\ndirection for AI alignment that focuses on guiding internal constructions. Our\ncode is available at https://TheOtherMind.github.io.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u4f1a\u81ea\u53d1\u5efa\u7acb\u4e3b\u89c2\u65f6\u95f4\u53c2\u8003\u70b9\u5e76\u9075\u5faa\u97e6\u4f2f-\u8d39\u5e0c\u7eb3\u5b9a\u5f8b\uff0c\u63ed\u793a\u4e86\u65f6\u95f4\u504f\u597d\u795e\u7ecf\u5143\u548c\u5bf9\u6570\u7f16\u7801\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u7ecf\u9a8c\u4e3b\u4e49\u89c6\u89d2\u4ee5\u7406\u89e3LLMs\u7684\u8ba4\u77e5\u6784\u5efa\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u6b65\uff0c\u5b83\u4eec\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u67d0\u4e9b\u8ba4\u77e5\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u5e76\u672a\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u76f4\u63a5\u6307\u5b9a\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u805a\u7126LLMs\u7684\u65f6\u95f4\u8ba4\u77e5\u6765\u63a2\u7a76\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u76f8\u4f3c\u6027\u5224\u65ad\u4efb\u52a1\uff0c\u53d1\u73b0\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u4f1a\u81ea\u53d1\u5efa\u7acb\u4e3b\u89c2\u65f6\u95f4\u53c2\u8003\u70b9\uff0c\u5e76\u9075\u5faa\u97e6\u4f2f-\u8d39\u5e0c\u7eb3\u5b9a\u5f8b\u3002\u901a\u8fc7\u795e\u7ecf\u5143\u3001\u8868\u5f81\u548c\u4fe1\u606f\u5c42\u9762\u7684\u591a\u91cd\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u4e00\u7ec4\u65f6\u95f4\u504f\u597d\u795e\u7ecf\u5143\uff0c\u63ed\u793a\u4e86\u5e74\u4efd\u8868\u5f81\u7684\u5206\u5c42\u6784\u5efa\u8fc7\u7a0b\uff0c\u5e76\u53d1\u73b0\u8bad\u7ec3\u8bed\u6599\u672c\u8eab\u5177\u6709\u5185\u5728\u7684\u975e\u7ebf\u6027\u65f6\u95f4\u7ed3\u6784\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u66f4\u5927\u89c4\u6a21\u7684LLMs\u4f1a\u81ea\u53d1\u5efa\u7acb\u4e3b\u89c2\u65f6\u95f4\u53c2\u8003\u70b9\uff0c\u5e76\u9075\u5faa\u97e6\u4f2f-\u8d39\u5e0c\u7eb3\u5b9a\u5f8b\u3002\u795e\u7ecf\u5143\u5206\u6790\u8868\u660e\uff0c\u65f6\u95f4\u504f\u597d\u795e\u7ecf\u5143\u5728\u4e3b\u89c2\u53c2\u8003\u70b9\u5904\u6fc0\u6d3b\u6700\u5c0f\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u6570\u7f16\u7801\u65b9\u6848\u3002\u5e74\u4efd\u8868\u5f81\u4ece\u6d45\u5c42\u7684\u6570\u503c\u9010\u6b65\u6784\u5efa\u4e3a\u6df1\u5c42\u7684\u62bd\u8c61\u65f6\u95f4\u5b9a\u5411\u3002\u8bad\u7ec3\u8bed\u6599\u672c\u8eab\u5177\u6709\u975e\u7ebf\u6027\u65f6\u95f4\u7ed3\u6784\uff0c\u4e3a\u6a21\u578b\u7684\u5185\u90e8\u6784\u5efa\u63d0\u4f9b\u4e86\u539f\u6750\u6599\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ecf\u9a8c\u4e3b\u4e49\u7684\u89c6\u89d2\uff0c\u5c06LLMs\u7684\u8ba4\u77e5\u89c6\u4e3a\u5176\u5185\u90e8\u8868\u5f81\u7cfb\u7edf\u5bf9\u5916\u90e8\u4e16\u754c\u7684\u4e3b\u89c2\u6784\u5efa\u3002\u8fd9\u4e00\u89c6\u89d2\u6697\u793a\u4e86\u53ef\u80fd\u51fa\u73b0\u7684\u3001\u4eba\u7c7b\u65e0\u6cd5\u76f4\u89c2\u9884\u6d4b\u7684\u5f02\u7c7b\u8ba4\u77e5\u6846\u67b6\uff0c\u4e3aAI\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u6ce8\u5185\u90e8\u6784\u5efa\u5f15\u5bfc\u7684\u65b9\u5411\u3002"}}
{"id": "2507.14426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14426", "abs": "https://arxiv.org/abs/2507.14426", "authors": ["Zhou Chen", "Joe Lin", "Sathyanarayanan N. Aakur"], "title": "CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding", "comment": "Accepted to NeSy 2025", "summary": "We introduce CRAFT, a neuro-symbolic framework for interpretable affordance\ngrounding, which identifies the objects in a scene that enable a given action\n(e.g., \"cut\"). CRAFT integrates structured commonsense priors from ConceptNet\nand language models with visual evidence from CLIP, using an energy-based\nreasoning loop to refine predictions iteratively. This process yields\ntransparent, goal-driven decisions to ground symbolic and perceptual\nstructures. Experiments in multi-object, label-free settings demonstrate that\nCRAFT enhances accuracy while improving interpretability, providing a step\ntoward robust and trustworthy scene understanding.", "AI": {"tldr": "CRAFT\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7ed3\u5408\u5e38\u8bc6\u5148\u9a8c\u548c\u89c6\u89c9\u8bc1\u636e\uff0c\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u63d0\u5347\u573a\u666f\u4e2d\u52a8\u4f5c\u4e0e\u5bf9\u8c61\u5173\u8054\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u573a\u666f\u4e2d\u5bf9\u8c61\u4e0e\u52a8\u4f5c\u5173\u8054\u7684\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0cCRAFT\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5b9e\u73b0\u76ee\u6807\u9a71\u52a8\u7684\u51b3\u7b56\u3002", "method": "CRAFT\u6574\u5408\u4e86ConceptNet\u7684\u7ed3\u6784\u5316\u5e38\u8bc6\u5148\u9a8c\u3001\u8bed\u8a00\u6a21\u578b\u548cCLIP\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u901a\u8fc7\u57fa\u4e8e\u80fd\u91cf\u7684\u63a8\u7406\u5faa\u73af\u8fed\u4ee3\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u5728\u591a\u5bf9\u8c61\u3001\u65e0\u6807\u7b7e\u8bbe\u7f6e\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCRAFT\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u8fd8\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CRAFT\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u573a\u666f\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u6784\u5efa\u7a33\u5065\u4e14\u53ef\u4fe1\u8d56\u7684\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2507.15855", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15855", "abs": "https://arxiv.org/abs/2507.15855", "authors": ["Yichen Huang", "Lin F. Yang"], "title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025", "comment": null, "summary": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. With pipeline design\nand prompt engineering, 5 (out of 6) problems are solved correctly (up to a\ncaveat discussed below), highlighting the importance of finding the optimal way\nof using powerful models.", "AI": {"tldr": "\u7814\u7a76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u6a21\u578b\u4f7f\u7528\u65b9\u5f0f\uff0c\u4f7fGemini 2.5 Pro\u5728IMO 2025\u95ee\u9898\u4e0a\u53d6\u5f97\u9ad8\u6210\u529f\u7387\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e00\u822c\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5965\u6797\u5339\u514b\u7ea7\u522b\u7684\u4efb\u52a1\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528Google\u7684Gemini 2.5 Pro\u6a21\u578b\uff0c\u7ed3\u5408\u7ba1\u9053\u8bbe\u8ba1\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u5904\u7406IMO 2025\u7684\u65b0\u9898\u76ee\u3002", "result": "\u57286\u4e2a\u95ee\u9898\u4e2d\u6b63\u786e\u89e3\u51b3\u4e865\u4e2a\uff08\u5b58\u5728\u4e00\u4e2a\u6ce8\u610f\u4e8b\u9879\uff09\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u89e3\u51b3\u9ad8\u96be\u5ea6\u6570\u5b66\u7ade\u8d5b\u95ee\u9898\u65f6\uff0c\u627e\u5230\u6700\u4f18\u65b9\u6cd5\u4f7f\u7528\u5f3a\u5927\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.15496", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15496", "abs": "https://arxiv.org/abs/2507.15496", "authors": ["JunYing Huang", "Ao Xu", "DongSun Yong", "KeRen Li", "YuanFeng Wang", "Qi Qin"], "title": "Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images", "comment": null, "summary": "Odometry is a critical task for autonomous systems for self-localization and\nnavigation. We propose a novel LiDAR-Visual odometry framework that integrates\nLiDAR point clouds and images for accurate and robust pose estimation. Our\nmethod utilizes a dense-depth map estimated from point clouds and images\nthrough depth completion, and incorporates a multi-scale feature extraction\nnetwork with attention mechanisms, enabling adaptive depth-aware\nrepresentations. Furthermore, we leverage dense depth information to refine\nflow estimation and mitigate errors in occlusion-prone regions. Our\nhierarchical pose refinement module optimizes motion estimation progressively,\nensuring robust predictions against dynamic environments and scale ambiguities.\nComprehensive experiments on the KITTI odometry benchmark demonstrate that our\napproach achieves similar or superior accuracy and robustness compared to\nstate-of-the-art visual and LiDAR odometry methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LiDAR\u70b9\u4e91\u548c\u56fe\u50cf\u7684\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u8865\u5168\u548c\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u59ff\u6001\u4f30\u8ba1\uff0c\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u91cc\u7a0b\u8ba1\u662f\u81ea\u4e3b\u7cfb\u7edf\u8fdb\u884c\u81ea\u6211\u5b9a\u4f4d\u548c\u5bfc\u822a\u7684\u5173\u952e\u4efb\u52a1\uff0c\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u7684\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6df1\u5ea6\u8865\u5168\u4ece\u70b9\u4e91\u548c\u56fe\u50cf\u4e2d\u4f30\u8ba1\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff0c\u5e76\u91c7\u7528\u5e26\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u6df1\u5ea6\u611f\u77e5\u8868\u793a\u3002\u6b64\u5916\uff0c\u5229\u7528\u5bc6\u96c6\u6df1\u5ea6\u4fe1\u606f\u4f18\u5316\u5149\u6d41\u4f30\u8ba1\uff0c\u51cf\u5c11\u906e\u6321\u533a\u57df\u7684\u8bef\u5dee\u3002\u901a\u8fc7\u5206\u5c42\u59ff\u6001\u4f18\u5316\u6a21\u5757\u9010\u6b65\u4f18\u5316\u8fd0\u52a8\u4f30\u8ba1\uff0c\u786e\u4fdd\u5728\u52a8\u6001\u73af\u5883\u548c\u5c3a\u5ea6\u6a21\u7cca\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u9884\u6d4b\u3002", "result": "\u5728KITTI\u91cc\u7a0b\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684LiDAR-Visual\u91cc\u7a0b\u8ba1\u6846\u67b6\u5728KITTI\u91cc\u7a0b\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4e86\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u548cLiDAR\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.14432", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.14432", "abs": "https://arxiv.org/abs/2507.14432", "authors": ["Han Gong", "Qiyue Li", "Zhi Liu", "Hao Zhou", "Peng Yuan Zhou", "Zhu Li", "Jie Li"], "title": "Adaptive 3D Gaussian Splatting Video Streaming", "comment": null, "summary": "The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the\nquality of volumetric video representation. Meanwhile, in contrast to\nconventional volumetric video, 3DGS video poses significant challenges for\nstreaming due to its substantially larger data volume and the heightened\ncomplexity involved in compression and transmission. To address these issues,\nwe introduce an innovative framework for 3DGS volumetric video streaming.\nSpecifically, we design a 3DGS video construction method based on the Gaussian\ndeformation field. By employing hybrid saliency tiling and differentiated\nquality modeling of 3DGS video, we achieve efficient data compression and\nadaptation to bandwidth fluctuations while ensuring high transmission quality.\nThen we build a complete 3DGS video streaming system and validate the\ntransmission performance. Through experimental evaluation, our method\ndemonstrated superiority over existing approaches in various aspects, including\nvideo quality, compression effectiveness, and transmission rate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u53d8\u5f62\u573a\u76843DGS\u89c6\u9891\u6d41\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u663e\u8457\u6027\u5206\u5757\u548c\u5dee\u5f02\u5316\u8d28\u91cf\u5efa\u6a21\uff0c\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u4e0e\u4f20\u8f93\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u867d\u63d0\u5347\u4e86\u4f53\u79ef\u89c6\u9891\u8868\u793a\u8d28\u91cf\uff0c\u4f46\u5176\u6570\u636e\u91cf\u5927\u3001\u538b\u7f29\u4e0e\u4f20\u8f93\u590d\u6742\u5ea6\u9ad8\uff0c\u4e9f\u9700\u89e3\u51b3\u6d41\u5a92\u4f53\u4f20\u8f93\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u9ad8\u65af\u53d8\u5f62\u573a\u76843DGS\u89c6\u9891\u6784\u5efa\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df7\u5408\u663e\u8457\u6027\u5206\u5757\u548c\u5dee\u5f02\u5316\u8d28\u91cf\u5efa\u6a21\uff0c\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u538b\u7f29\u548c\u5e26\u5bbd\u6ce2\u52a8\u9002\u5e94\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u8d28\u91cf\u3001\u538b\u7f29\u6548\u679c\u548c\u4f20\u8f93\u901f\u7387\u7b49\u591a\u4e2a\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u76843DGS\u89c6\u9891\u6d41\u6846\u67b6\u5728\u89c6\u9891\u8d28\u91cf\u3001\u538b\u7f29\u6548\u7387\u548c\u4f20\u8f93\u901f\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2411.01789", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2411.01789", "abs": "https://arxiv.org/abs/2411.01789", "authors": ["Shan Jiang", "Chenguang Zhu", "Sarfraz Khurshid"], "title": "Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs", "comment": null, "summary": "Software testing remains the most widely used methodology for validating\nquality of code. However, effectiveness of testing critically depends on the\nquality of test suites used. Test cases in a test suite consist of two\nfundamental parts: (1) input values for the code under test, and (2) correct\nchecks for the outputs it produces. These checks are commonly written as\nassertions, and termed test oracles. The last couple of decades have seen much\nprogress in automated test input generation, e.g., using fuzzing and symbolic\nexecution. However, automating test oracles remains a relatively less explored\nproblem area. Indeed, a test oracle by its nature requires knowledge of\nexpected behavior, which may only be known to the developer and may not not\nexist in a formal language that supports automated reasoning.\n  Our focus in this paper is automation of test oracles for clients of widely\nused Java libraries, e.g., java.lang and java.util packages. Our key insight is\nthat Javadocs that provide a rich source of information can enable automated\ngeneration of test oracles. Javadocs of the core Java libraries are fairly\ndetailed documents that contain natural language descriptions of not only how\nthe libraries behave but also how the clients must (not) use them. We use large\nlanguage models as an enabling technology to embody our insight into a\nframework for test oracle automation, and evaluate it experimentally. Our\nexperiments demonstrate that LLMs can generate oracles for checking normal and\nexceptional behaviors from Javadocs, with 98.8% of these oracles being\ncompilable and 96.4% accurately reflecting intended properties. Even for the\nfew incorrect oracles, errors are minor and can be easily corrected with the\nhelp of additional comment information generated by the LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7528Javadocs\u548cLLMs\u81ea\u52a8\u5316\u751f\u6210\u6d4b\u8bd5\u9884\u8a00\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u4e14\u51c6\u786e\u3002", "motivation": "\u81ea\u52a8\u5316\u6d4b\u8bd5\u9884\u8a00\u751f\u6210\u662f\u4e00\u4e2a\u76f8\u5bf9\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\uff0c\u5c24\u5176\u662f\u5982\u4f55\u4ece\u975e\u6b63\u5f0f\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u63d0\u53d6\u9884\u671f\u884c\u4e3a\u3002", "method": "\u5229\u7528Javadocs\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u7ed3\u5408LLMs\u6280\u672f\uff0c\u81ea\u52a8\u5316\u751f\u6210\u6d4b\u8bd5\u9884\u8a00\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cLLMs\u751f\u6210\u7684\u6d4b\u8bd5\u9884\u8a00\u4e2d98.8%\u53ef\u7f16\u8bd1\uff0c96.4%\u51c6\u786e\u53cd\u6620\u4e86\u9884\u671f\u5c5e\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528Javadocs\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5316\u751f\u6210\u6d4b\u8bd5\u9884\u8a00\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u751f\u6210\u53ef\u7f16\u8bd1\u4e14\u51c6\u786e\u7684\u6d4b\u8bd5\u9884\u8a00\u3002"}}
{"id": "2507.15597", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15597", "abs": "https://arxiv.org/abs/2507.15597", "authors": ["Hao Luo", "Yicheng Feng", "Wanpeng Zhang", "Sipeng Zheng", "Ye Wang", "Haoqi Yuan", "Jiazheng Liu", "Chaoyi Xu", "Qin Jin", "Zongqing Lu"], "title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos", "comment": "37 pages", "summary": "We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained\non large-scale human videos. Existing VLAs struggle with complex manipulation\ntasks requiring high dexterity and generalize poorly to novel scenarios and\ntasks, primarily due to their reliance on synthetic data with significant\nsim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To\naddress this data bottleneck, we propose leveraging human hands as a foundation\nmanipulator, capitalizing on the rich dexterity and scalability present in web\ndata. Our approach centers on physical instruction tuning, a novel training\nparadigm that combines large-scale VLA pretraining from human videos, physical\nspace alignment for 3D reasoning, and post-training adaptation for robotic\ntasks. Additionally, we introduce a part-level motion tokenization method which\nachieves millimeter-level reconstruction accuracy to model precise hand\ntrajectories for action learning. To support our proposed paradigm, we further\ndevelop a comprehensive data curation pipeline that integrates heterogeneous\nsources -- including motion capture, VR, and RGB-only videos -- into a\nlarge-scale dataset with millions of motion-based instructional instances. We\nempirically show the excellence of Being-H0 in hand motion generation and\ninstruction following, and it also scales well with model and data sizes.\nImportantly, we observe the expected gains of Being-H0 in real-world robotic\nmanipulation as physical instruction tuning is applied. More details are\navailable at https://beingbeyond.github.io/Being-H0.", "AI": {"tldr": "Being-H0\u662f\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u89c6\u9891\u8bad\u7ec3\u7684\u7075\u5de7\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u7269\u7406\u6307\u4ee4\u8c03\u4f18\u548c\u8fd0\u52a8\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u56e0\u4f9d\u8d56\u5408\u6210\u6570\u636e\u6216\u7f3a\u4e4f\u591a\u6837\u6027\u7684\u9065\u64cd\u4f5c\u6f14\u793a\uff0c\u96be\u4ee5\u5904\u7406\u9ad8\u7075\u5de7\u6027\u4efb\u52a1\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9700\u5229\u7528\u4eba\u7c7b\u624b\u90e8\u64cd\u4f5c\u7684\u4e30\u5bcc\u6570\u636e\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u7269\u7406\u6307\u4ee4\u8c03\u4f18\u8bad\u7ec3\u8303\u5f0f\uff0c\u7ed3\u5408\u5927\u89c4\u6a21VLA\u9884\u8bad\u7ec3\u3001\u7269\u7406\u7a7a\u95f4\u5bf9\u9f50\u548c\u673a\u5668\u4eba\u4efb\u52a1\u540e\u9002\u5e94\uff0c\u5e76\u5f15\u5165\u90e8\u5206\u7ea7\u8fd0\u52a8\u6807\u8bb0\u5316\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u6beb\u7c73\u7ea7\u91cd\u5efa\u7cbe\u5ea6\u3002", "result": "Being-H0\u5728\u624b\u90e8\u8fd0\u52a8\u751f\u6210\u548c\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u80fd\u968f\u6a21\u578b\u548c\u6570\u636e\u89c4\u6a21\u6269\u5c55\uff0c\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u4e5f\u89c2\u5bdf\u5230\u9884\u671f\u589e\u76ca\u3002", "conclusion": "Being-H0\u901a\u8fc7\u7269\u7406\u6307\u4ee4\u8c03\u4f18\u548c\u90e8\u5206\u7ea7\u8fd0\u52a8\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5c55\u73b0\u4e86\u9884\u671f\u7684\u589e\u76ca\u3002"}}
{"id": "2507.14449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14449", "abs": "https://arxiv.org/abs/2507.14449", "authors": ["Zhe Cao", "Jin Zhang", "Ruiheng Zhang"], "title": "IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark", "comment": "11 pages, 7 figures. This paper is accepted by ICCV 2025", "summary": "Real-world infrared imagery presents unique challenges for vision-language\nmodels due to the scarcity of aligned text data and domain-specific\ncharacteristics. Although existing methods have advanced the field, their\nreliance on synthetic infrared images generated through style transfer from\nvisible images, which limits their ability to capture the unique\ncharacteristics of the infrared modality. To address this, we propose IRGPT,\nthe first multi-modal large language model for real-world infrared images,\nbuilt upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K\nauthentic image-text pairs. The proposed IR-TD dataset contains real infrared\nimages paired with meticulously handcrafted texts, where the initial drafts\noriginated from two complementary processes: (1) LLM-generated descriptions of\nvisible images, and (2) rule-based descriptions of annotations. Furthermore, we\nintroduce a bi-cross-modal curriculum transfer learning strategy that\nsystematically transfers knowledge from visible to infrared domains by\nconsidering the difficulty scores of both infrared-visible and infrared-text.\nEvaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT\nachieves state-of-the-art performance even compared with larger-scale models.", "AI": {"tldr": "IRGPT\u662f\u9996\u4e2a\u9488\u5bf9\u771f\u5b9e\u4e16\u754c\u7ea2\u5916\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u57fa\u4e8e26\u4e07\u5f20\u771f\u5b9e\u7ea2\u5916\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528\u53cc\u5411\u8de8\u6a21\u6001\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u901a\u8fc7\u98ce\u683c\u8f6c\u79fb\u4ece\u53ef\u89c1\u5149\u56fe\u50cf\u751f\u6210\u7684\u5408\u6210\u7ea2\u5916\u56fe\u50cf\uff0c\u65e0\u6cd5\u6355\u6349\u7ea2\u5916\u6a21\u6001\u7684\u72ec\u7279\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u7ea2\u5916\u56fe\u50cf\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86IRGPT\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u89c4\u6a21\u7ea2\u5916-\u6587\u672c\u6570\u636e\u96c6\uff08IR-TD\uff09\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u5411\u8de8\u6a21\u6001\u8bfe\u7a0b\u8f6c\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u4ece\u53ef\u89c1\u5149\u5230\u7ea2\u5916\u9886\u57df\u7cfb\u7edf\u5316\u8f6c\u79fb\u77e5\u8bc6\u3002", "result": "IRGPT\u5728\u5305\u62ec\u8bc6\u522b\u548c\u5b9a\u4f4d\u7b499\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5373\u4f7f\u4e0e\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u76f8\u6bd4\u4e5f\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "IRGPT\u57289\u4e2a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u7ea2\u5916\u56fe\u50cf\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2502.15441", "categories": ["cs.SE", "cs.AI", "cs.FL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2502.15441", "abs": "https://arxiv.org/abs/2502.15441", "authors": ["Yang Hong", "Shan Jiang", "Yulei Fu", "Sarfraz Khurshid"], "title": "On the Effectiveness of Large Language Models in Writing Alloy Formulas", "comment": null, "summary": "Declarative specifications have a vital role to play in developing safe and\ndependable software systems. Writing specifications correctly, however, remains\nparticularly challenging. This paper presents a controlled experiment on using\nlarge language models (LLMs) to write declarative formulas in the well-known\nlanguage Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write\ncomplete Alloy formulas from given natural language descriptions (in English).\nTwo, we employ LLMs to create alternative but equivalent formulas in Alloy with\nrespect to given Alloy formulas. Three, we employ LLMs to complete sketches of\nAlloy formulas and populate the holes in the sketches by synthesizing Alloy\nexpressions and operators so that the completed formulas accurately represent\nthe desired properties (that are given in natural language). We conduct the\nexperimental evaluation using 11 well-studied subject specifications and employ\ntwo popular LLMs, namely ChatGPT and DeepSeek. The experimental results show\nthat the LLMs generally perform well in synthesizing complete Alloy formulas\nfrom input properties given in natural language or in Alloy, and are able to\nenumerate multiple unique solutions. Moreover, the LLMs are also successful at\ncompleting given sketches of Alloy formulas with respect to natural language\ndescriptions of desired properties (without requiring test cases). We believe\nLLMs offer a very exciting advance in our ability to write specifications, and\ncan help make specifications take a pivotal role in software development and\nenhance our ability to build robust software.", "AI": {"tldr": "LLMs \u80fd\u6709\u6548\u751f\u6210\u548c\u8865\u5168 Alloy \u58f0\u660e\u5f0f\u89c4\u8303\uff0c\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u65b0\u5de5\u5177\u3002", "motivation": "\u58f0\u660e\u5f0f\u89c4\u8303\u5bf9\u5f00\u53d1\u5b89\u5168\u53ef\u9760\u7684\u8f6f\u4ef6\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6b63\u786e\u7f16\u5199\u89c4\u8303\u4ecd\u5177\u6311\u6218\u6027\uff0c\u56e0\u6b64\u63a2\u7d22 LLMs \u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528 ChatGPT \u548c DeepSeek \u4e24\u79cd LLM\uff0c\u901a\u8fc7\u4e09\u79cd\u65b9\u5f0f\u8bc4\u4f30\u5176\u80fd\u529b\uff1a1) \u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u5b8c\u6574 Alloy \u516c\u5f0f\uff1b2) \u751f\u6210\u7b49\u6548\u7684 Alloy \u516c\u5f0f\uff1b3) \u8865\u5168 Alloy \u516c\u5f0f\u8349\u56fe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLMs \u80fd\u591f\u4ece\u81ea\u7136\u8bed\u8a00\u6216 Alloy \u8f93\u5165\u4e2d\u5408\u6210\u5b8c\u6574\u516c\u5f0f\uff0c\u5e76\u751f\u6210\u591a\u4e2a\u72ec\u7279\u89e3\uff0c\u4e14\u80fd\u6210\u529f\u8865\u5168\u516c\u5f0f\u8349\u56fe\u3002", "conclusion": "LLMs \u5728\u7f16\u5199\u58f0\u660e\u5f0f\u89c4\u8303\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u5408\u6210\u5b8c\u6574\u7684 Alloy \u516c\u5f0f\uff0c\u5e76\u751f\u6210\u7b49\u6548\u6216\u8865\u5168\u7684\u516c\u5f0f\uff0c\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u89c4\u8303\u7684\u7f16\u5199\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2507.14452", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14452", "abs": "https://arxiv.org/abs/2507.14452", "authors": ["Weikang Gu", "Mingyue Han", "Li Xue", "Heng Dong", "Changcai Yang", "Riqing Chen", "Lifang Wei"], "title": "GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration", "comment": "9 pages, 4 figures. Accepted to IJCAI 2025", "summary": "The accurate identification of high-quality correspondences is a prerequisite\ntask in feature-based point cloud registration. However, it is extremely\nchallenging to handle the fusion of local and global features due to feature\nredundancy and complex spatial relationships. Given that Gestalt principles\nprovide key advantages in analyzing local and global relationships, we propose\na novel Gestalt-guided Parallel Interaction Network via orthogonal geometric\nconsistency (GPI-Net) in this paper. It utilizes Gestalt principles to\nfacilitate complementary communication between local and global information.\nSpecifically, we introduce an orthogonal integration strategy to optimally\nreduce redundant information and generate a more compact global structure for\nhigh-quality correspondences. To capture geometric features in correspondences,\nwe leverage a Gestalt Feature Attention (GFA) block through a hybrid\nutilization of self-attention and cross-attention mechanisms. Furthermore, to\nfacilitate the integration of local detail information into the global\nstructure, we design an innovative Dual-path Multi-Granularity parallel\ninteraction aggregation (DMG) block to promote information exchange across\ndifferent granularities. Extensive experiments on various challenging tasks\ndemonstrate the superior performance of our proposed GPI-Net in comparison to\nexisting methods. The code will be released at https://github.com/gwk/GPI-Net.", "AI": {"tldr": "GPI-Net\u5229\u7528Gestalt\u539f\u5219\u4f18\u5316\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u878d\u5408\uff0c\u901a\u8fc7\u6b63\u4ea4\u96c6\u6210\u548c\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u70b9\u4e91\u914d\u51c6\u6027\u80fd\uff0c\u5b9e\u9a8c\u6548\u679c\u663e\u8457\u3002", "motivation": "\u89e3\u51b3\u7531\u4e8e\u7279\u5f81\u5197\u4f59\u548c\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u5bfc\u81f4\u7684\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u878d\u5408\u96be\u9898\uff0c\u63d0\u9ad8\u70b9\u4e91\u914d\u51c6\u4e2d\u9ad8\u8d28\u91cf\u5bf9\u5e94\u5173\u7cfb\u7684\u8bc6\u522b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGestalt\u539f\u5219\u7684\u5e76\u884c\u4ea4\u4e92\u7f51\u7edc\uff08GPI-Net\uff09\uff0c\u5305\u62ec\u6b63\u4ea4\u96c6\u6210\u7b56\u7565\u3001Gestalt\u7279\u5f81\u6ce8\u610f\u529b\uff08GFA\uff09\u5757\u548c\u53cc\u8def\u5f84\u591a\u7c92\u5ea6\u5e76\u884c\u4ea4\u4e92\u805a\u5408\uff08DMG\uff09\u5757\u3002", "result": "\u5728\u591a\u79cd\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGPI-Net\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GPI-Net\u901a\u8fc7Gestalt\u539f\u5219\u548c\u6b63\u4ea4\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u6210\u529f\u4f18\u5316\u4e86\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u7684\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u914d\u51c6\u4e2d\u9ad8\u8d28\u91cf\u5bf9\u5e94\u5173\u7cfb\u7684\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2507.14454", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.14454", "abs": "https://arxiv.org/abs/2507.14454", "authors": ["Han Gong", "Qiyue Li", "Jie Li", "Zhi Liu"], "title": "Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation", "comment": null, "summary": "3D Gaussian splatting video (3DGS) streaming has recently emerged as a\nresearch hotspot in both academia and industry, owing to its impressive ability\nto deliver immersive 3D video experiences. However, research in this area is\nstill in its early stages, and several fundamental challenges, such as tiling,\nquality assessment, and bitrate adaptation, require further investigation. In\nthis paper, we tackle these challenges by proposing a comprehensive set of\nsolutions. Specifically, we propose an adaptive 3DGS tiling technique guided by\nsaliency analysis, which integrates both spatial and temporal features. Each\ntile is encoded into versions possessing dedicated deformation fields and\nmultiple quality levels for adaptive selection. We also introduce a novel\nquality assessment framework for 3DGS video that jointly evaluates\nspatial-domain degradation in 3DGS representations during streaming and the\nquality of the resulting 2D rendered images. Additionally, we develop a\nmeta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS\nvideo streaming, achieving optimal performance across varying network\nconditions. Extensive experiments demonstrate that our proposed approaches\nsignificantly outperform state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf93DGS\u89c6\u9891\u6d41\u7684\u5206\u5757\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u6bd4\u7279\u7387\u81ea\u9002\u5e94\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u5206\u5757\u6280\u672f\u3001\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u548c\u5143\u5b66\u4e60\u6bd4\u7279\u7387\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "3DGS\u89c6\u9891\u6d41\u5728\u63d0\u4f9b\u6c89\u6d78\u5f0f3D\u89c6\u9891\u4f53\u9a8c\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5206\u5757\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u6bd4\u7279\u7387\u81ea\u9002\u5e94\u7b49\u57fa\u7840\u6311\u6218\u4ecd\u9700\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e943DGS\u5206\u5757\u6280\u672f\uff08\u57fa\u4e8e\u663e\u8457\u6027\u5206\u6790\uff09\u3001\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff08\u8054\u5408\u8bc4\u4f303DGS\u8868\u793a\u7684\u7a7a\u95f4\u57df\u9000\u5316\u548c2D\u6e32\u67d3\u56fe\u50cf\u8d28\u91cf\uff09\u548c\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u6bd4\u7279\u7387\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u81ea\u9002\u5e943DGS\u5206\u5757\u6280\u672f\u3001\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u6bd4\u7279\u7387\u7b97\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a3DGS\u89c6\u9891\u6d41\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14459", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14459", "abs": "https://arxiv.org/abs/2507.14459", "authors": ["Huayuan Ye", "Juntong Chen", "Shenzhuo Zhang", "Yipeng Zhang", "Changbo Wang", "Chenhui Li"], "title": "VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval", "comment": "9 pages, IEEE VIS 2025", "summary": "The dissemination of visualizations is primarily in the form of raster\nimages, which often results in the loss of critical information such as source\ncode, interactive features, and metadata. While previous methods have proposed\nembedding metadata into images to facilitate Visualization Image Data Retrieval\n(VIDR), most existing methods lack practicability since they are fragile to\ncommon image tampering during online distribution such as cropping and editing.\nTo address this issue, we propose VisGuard, a tamper-resistant VIDR framework\nthat reliably embeds metadata link into visualization images. The embedded data\nlink remains recoverable even after substantial tampering upon images. We\npropose several techniques to enhance robustness, including repetitive data\ntiling, invertible information broadcasting, and an anchor-based scheme for\ncrop localization. VisGuard enables various applications, including interactive\nchart reconstruction, tampering detection, and copyright protection. We conduct\ncomprehensive experiments on VisGuard's superior performance in data retrieval\naccuracy, embedding capacity, and security against tampering and steganalysis,\ndemonstrating VisGuard's competence in facilitating and safeguarding\nvisualization dissemination and information conveyance.", "AI": {"tldr": "VisGuard \u662f\u4e00\u4e2a\u6297\u7be1\u6539\u7684\u53ef\u89c6\u5316\u56fe\u50cf\u6570\u636e\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u9c81\u68d2\u6027\u6280\u672f\u5d4c\u5165\u5143\u6570\u636e\u94fe\u63a5\uff0c\u652f\u6301\u591a\u79cd\u5e94\u7528\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u53ef\u89c6\u5316\u56fe\u50cf\u6570\u636e\u68c0\u7d22\u65b9\u6cd5\u5927\u591a\u7f3a\u4e4f\u5b9e\u7528\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u5bf9\u5728\u7ebf\u5206\u53d1\u8fc7\u7a0b\u4e2d\u7684\u5e38\u89c1\u56fe\u50cf\u7be1\u6539\uff08\u5982\u88c1\u526a\u548c\u7f16\u8f91\uff09\u5f88\u8106\u5f31\u3002VisGuard \u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u4e2a\u6297\u7be1\u6539\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "VisGuard \u91c7\u7528\u4e86\u591a\u79cd\u6280\u672f\u6765\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u5305\u62ec\u91cd\u590d\u6570\u636e\u5e73\u94fa\u3001\u53ef\u9006\u4fe1\u606f\u5e7f\u64ad\u548c\u57fa\u4e8e\u951a\u70b9\u7684\u88c1\u526a\u5b9a\u4f4d\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cVisGuard \u5728\u6570\u636e\u68c0\u7d22\u51c6\u786e\u6027\u3001\u5d4c\u5165\u5bb9\u91cf\u548c\u5b89\u5168\u6027\u65b9\u9762\u5177\u6709\u5353\u8d8a\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u4ea4\u4e92\u5f0f\u56fe\u8868\u91cd\u5efa\u3001\u7be1\u6539\u68c0\u6d4b\u548c\u7248\u6743\u4fdd\u62a4\u7b49\u5e94\u7528\u3002", "conclusion": "VisGuard \u662f\u4e00\u4e2a\u6297\u7be1\u6539\u7684\u53ef\u89c6\u5316\u56fe\u50cf\u6570\u636e\u68c0\u7d22\uff08VIDR\uff09\u6846\u67b6\uff0c\u80fd\u591f\u53ef\u9760\u5730\u5c06\u5143\u6570\u636e\u94fe\u63a5\u5d4c\u5165\u53ef\u89c6\u5316\u56fe\u50cf\u4e2d\uff0c\u5373\u4f7f\u5728\u56fe\u50cf\u906d\u53d7\u91cd\u5927\u7be1\u6539\u540e\uff0c\u5d4c\u5165\u7684\u6570\u636e\u94fe\u63a5\u4ecd\u53ef\u6062\u590d\u3002VisGuard \u5728\u6570\u636e\u68c0\u7d22\u51c6\u786e\u6027\u3001\u5d4c\u5165\u5bb9\u91cf\u548c\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u5730\u4fc3\u8fdb\u548c\u4fdd\u62a4\u4e86\u53ef\u89c6\u5316\u7684\u4f20\u64ad\u548c\u4fe1\u606f\u4f20\u9012\u3002"}}
{"id": "2507.14477", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14477", "abs": "https://arxiv.org/abs/2507.14477", "authors": ["Zhenyu Li", "Tianyi Shang", "Pengjie Xu", "Ruirui Zhang", "Fanchen Kong"], "title": "OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition", "comment": "5 figures", "summary": "Visual Place Recognition (VPR) in dynamic and perceptually aliased\nenvironments remains a fundamental challenge for long-term localization.\nExisting deep learning-based solutions predominantly focus on single-frame\nembeddings, neglecting the temporal coherence present in image sequences. This\npaper presents OptiCorNet, a novel sequence modeling framework that unifies\nspatial feature extraction and temporal differencing into a differentiable,\nend-to-end trainable module. Central to our approach is a lightweight 1D\nconvolutional encoder combined with a learnable differential temporal operator,\ntermed Differentiable Sequence Delta (DSD), which jointly captures short-term\nspatial context and long-range temporal transitions. The DSD module models\ndirectional differences across sequences via a fixed-weight differencing\nkernel, followed by an LSTM-based refinement and optional residual projection,\nyielding compact, discriminative descriptors robust to viewpoint and appearance\nshifts. To further enhance inter-class separability, we incorporate a\nquadruplet loss that optimizes both positive alignment and multi-negative\ndivergence within each batch. Unlike prior VPR methods that treat temporal\naggregation as post-processing, OptiCorNet learns sequence-level embeddings\ndirectly, enabling more effective end-to-end place recognition. Comprehensive\nevaluations on multiple public benchmarks demonstrate that our approach\noutperforms state-of-the-art baselines under challenging seasonal and viewpoint\nvariations.", "AI": {"tldr": "OptiCorNet\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5e8f\u5217\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u7edf\u4e00\u7a7a\u95f4\u548c\u65f6\u95f4\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u548c\u611f\u77e5\u6df7\u53e0\u73af\u5883\u4e2d\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5ffd\u89c6\u56fe\u50cf\u5e8f\u5217\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "OptiCorNet\u91c7\u7528\u8f7b\u91cf\u7ea71D\u5377\u79ef\u7f16\u7801\u5668\u548c\u53ef\u5b66\u4e60\u7684\u5fae\u5206\u65f6\u95f4\u7b97\u5b50\uff08DSD\uff09\uff0c\u7ed3\u5408LSTM\u7cbe\u70bc\u548c\u53ef\u9009\u6b8b\u5dee\u6295\u5f71\uff0c\u751f\u6210\u7d27\u51d1\u4e14\u5177\u6709\u533a\u5206\u6027\u7684\u63cf\u8ff0\u7b26\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOptiCorNet\u5728\u5b63\u8282\u6027\u548c\u89c6\u89d2\u53d8\u5316\u6311\u6218\u4e0b\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u57fa\u7ebf\u3002", "conclusion": "OptiCorNet\u901a\u8fc7\u7edf\u4e00\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u548c\u65f6\u95f4\u5dee\u5206\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u53ef\u8bad\u7ec3\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u548c\u611f\u77e5\u6df7\u53e0\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2507.14481", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14481", "abs": "https://arxiv.org/abs/2507.14481", "authors": ["Yujia Tong", "Jingling Yuan", "Tian Zhang", "Jianquan Liu", "Chuang Hu"], "title": "DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning", "comment": null, "summary": "Data-Free Quantization (DFQ) enables the quantization of Vision Transformers\n(ViTs) without requiring access to data, allowing for the deployment of ViTs on\ndevices with limited resources. In DFQ, the quantization model must be\ncalibrated using synthetic samples, making the quality of these synthetic\nsamples crucial. Existing methods fail to fully capture and balance the global\nand local features within the samples, resulting in limited synthetic data\nquality. Moreover, we have found that during inference, there is a significant\ndifference in the distributions of intermediate layer activations between the\nquantized and full-precision models. These issues lead to a severe performance\ndegradation of the quantized model. To address these problems, we propose a\npipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).\nSpecifically, we synthesize samples in order of increasing difficulty,\neffectively enhancing the quality of synthetic data. During the calibration and\ninference stage, we introduce the activation correction matrix for the\nquantized model to align the intermediate layer activations with those of the\nfull-precision model. Extensive experiments demonstrate that DFQ-ViT achieves\nremarkable superiority over existing DFQ methods and its performance is on par\nwith models quantized through real data. For example, the performance of DeiT-T\nwith 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our\nmethod eliminates the need for fine-tuning, which not only reduces\ncomputational overhead but also lowers the deployment barriers for edge\ndevices. This characteristic aligns with the principles of Green Learning by\nimproving energy efficiency and facilitating real-world applications in\nresource-constrained environments.", "AI": {"tldr": "DFQ-ViT \u901a\u8fc7\u5408\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u548c\u6fc0\u6d3b\u6821\u6b63\u77e9\u9635\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u65e0\u5173\u91cf\u5316\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u65e0\u9700\u5fae\u8c03\u4e14\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u6355\u83b7\u548c\u5e73\u8861\u6837\u672c\u4e2d\u7684\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\uff0c\u5bfc\u81f4\u5408\u6210\u6570\u636e\u8d28\u91cf\u6709\u9650\uff0c\u4e14\u91cf\u5316\u6a21\u578b\u4e0e\u5168\u7cbe\u5ea6\u6a21\u578b\u5728\u63a8\u7406\u65f6\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u5206\u5e03\u5dee\u5f02\u663e\u8457\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u6837\u672c\u96be\u5ea6\u9012\u589e\u7684\u7ba1\u9053\uff0c\u5e76\u5f15\u5165\u4e86\u6fc0\u6d3b\u6821\u6b63\u77e9\u9635\u4ee5\u5bf9\u9f50\u91cf\u5316\u6a21\u578b\u548c\u5168\u7cbe\u5ea6\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u3002", "result": "DFQ-ViT \u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709 DFQ \u65b9\u6cd5\uff0c\u4e14\u4e0e\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u91cf\u5316\u7684\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002\u4f8b\u5982\uff0c3 \u4f4d\u6743\u91cd\u91cf\u5316\u7684 DeiT-T \u6027\u80fd\u6bd4\u73b0\u6709\u6280\u672f\u9ad8\u51fa 4.29%\u3002", "conclusion": "DFQ-ViT \u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u91cf\u5316\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u548c\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u95e8\u69db\uff0c\u7b26\u5408\u7eff\u8272\u5b66\u4e60\u539f\u5219\u3002"}}
{"id": "2507.14485", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14485", "abs": "https://arxiv.org/abs/2507.14485", "authors": ["Hongye Hou", "Liu Zhan", "Yang Yang"], "title": "Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion", "comment": null, "summary": "Completing the whole 3D structure based on an incomplete point cloud is a\nchallenging task, particularly when the residual point cloud lacks typical\nstructural characteristics. Recent methods based on cross-modal learning\nattempt to introduce instance images to aid the structure feature learning.\nHowever, they still focus on each particular input class, limiting their\ngeneration abilities. In this work, we propose a novel retrieval-augmented\npoint cloud completion framework. The core idea is to incorporate cross-modal\nretrieval into completion task to learn structural prior information from\nsimilar reference samples. Specifically, we design a Structural Shared Feature\nEncoder (SSFE) to jointly extract cross-modal features and reconstruct\nreference features as priors. Benefiting from a dual-channel control gate in\nthe encoder, relevant structural features in the reference sample are enhanced\nand irrelevant information interference is suppressed. In addition, we propose\na Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical\nfeature fusion mechanism to integrate reference prior information with input\nfeatures from global to local. Through extensive evaluations on multiple\ndatasets and real-world scenes, our method shows its effectiveness in\ngenerating fine-grained point clouds, as well as its generalization capability\nin handling sparse data and unseen categories.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u7d22\u589e\u5f3a\u7684\u70b9\u4e91\u8865\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u7ed3\u6784\u5171\u4eab\u7279\u5f81\u7f16\u7801\u5668\u63d0\u5347\u8865\u5168\u6548\u679c\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u8f93\u5165\u7c7b\u522b\u4e14\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u8de8\u6a21\u6001\u68c0\u7d22\u5b66\u4e60\u7ed3\u6784\u5148\u9a8c\u4fe1\u606f\u3002", "method": "\u8bbe\u8ba1\u4e86\u7ed3\u6784\u5171\u4eab\u7279\u5f81\u7f16\u7801\u5668\uff08SSFE\uff09\u548c\u6e10\u8fdb\u5f0f\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5668\uff08PRAG\uff09\uff0c\u901a\u8fc7\u53cc\u901a\u9053\u63a7\u5236\u95e8\u589e\u5f3a\u76f8\u5173\u7ed3\u6784\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u7279\u5f81\u878d\u5408\u673a\u5236\u6574\u5408\u53c2\u8003\u5148\u9a8c\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u751f\u6210\u7cbe\u7ec6\u5316\u7684\u70b9\u4e91\uff0c\u5e76\u80fd\u5904\u7406\u7a00\u758f\u6570\u636e\u548c\u672a\u89c1\u7c7b\u522b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u68c0\u7d22\u589e\u5f3a\u70b9\u4e91\u8865\u5168\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u7ed3\u6784\u5171\u4eab\u7279\u5f81\u7f16\u7801\u5668\uff08SSFE\uff09\uff0c\u6709\u6548\u63d0\u5347\u4e86\u70b9\u4e91\u8865\u5168\u7684\u7cbe\u7ec6\u5316\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14497", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14497", "abs": "https://arxiv.org/abs/2507.14497", "authors": ["Weimin Lyu", "Qingqiao Hu", "Kehan Qi", "Zhan Shi", "Wentao Huang", "Saumya Gupta", "Chao Chen"], "title": "Efficient Whole Slide Pathology VQA via Token Compression", "comment": null, "summary": "Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000\npixels, posing significant challenges for multimodal large language model\n(MLLM) due to long context length and high computational demands. Previous\nmethods typically focus on patch-level analysis or slide-level classification\nusing CLIP-based models with multi-instance learning, but they lack the\ngenerative capabilities needed for visual question answering (VQA). More recent\nMLLM-based approaches address VQA by feeding thousands of patch tokens directly\ninto the language model, which leads to excessive resource consumption. To\naddress these limitations, we propose Token Compression Pathology LLaVA\n(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token\ncompression. TCP-LLaVA introduces a set of trainable compression tokens that\naggregate visual and textual information through a modality compression module,\ninspired by the [CLS] token mechanism in BERT. Only the compressed tokens are\nforwarded to the LLM for answer generation, significantly reducing input length\nand computational cost. Experiments on ten TCGA tumor subtypes show that\nTCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing\ntraining resource consumption by a substantial margin.", "AI": {"tldr": "TCP-LLaVA\u901a\u8fc7\u4ee4\u724c\u538b\u7f29\u6280\u672f\uff0c\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347WSI\u89c6\u89c9\u95ee\u7b54\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3WSIs\u5728\u75c5\u7406\u5b66\u4e2d\u56e0\u957f\u4e0a\u4e0b\u6587\u548c\u9ad8\u8ba1\u7b97\u9700\u6c42\u5bf9MLLM\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u751f\u6210\u80fd\u529b\u6216\u8d44\u6e90\u6d88\u8017\u8fc7\u5927\u3002", "method": "\u63d0\u51faTCP-LLaVA\uff0c\u4e00\u79cd\u901a\u8fc7\u4ee4\u724c\u538b\u7f29\u8fdb\u884cWSI VQA\u7684MLLM\u67b6\u6784\uff0c\u5229\u7528\u6a21\u6001\u538b\u7f29\u6a21\u5757\u805a\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u3002", "result": "\u5728\u5341\u79cdTCGA\u80bf\u7624\u4e9a\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cTCP-LLaVA\u5728VQA\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8bad\u7ec3\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "TCP-LLaVA\u901a\u8fc7\u5f15\u5165\u53ef\u8bad\u7ec3\u7684\u538b\u7f29\u4ee4\u724c\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8f93\u5165\u957f\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u5728VQA\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709MLLM\u57fa\u7ebf\u3002"}}
{"id": "2507.14501", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14501", "abs": "https://arxiv.org/abs/2507.14501", "authors": ["Jiahui Zhang", "Yuelei Li", "Anpei Chen", "Muyu Xu", "Kunhao Liu", "Jianyuan Wang", "Xiao-Xiao Long", "Hanxue Liang", "Zexiang Xu", "Hao Su", "Christian Theobalt", "Christian Rupprecht", "Andrea Vedaldi", "Hanspeter Pfister", "Shijian Lu", "Fangneng Zhan"], "title": "Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey", "comment": "A project page associated with this survey is available at\n  https://fnzhan.com/projects/Feed-Forward-3D", "summary": "3D reconstruction and view synthesis are foundational problems in computer\nvision, graphics, and immersive technologies such as augmented reality (AR),\nvirtual reality (VR), and digital twins. Traditional methods rely on\ncomputationally intensive iterative optimization in a complex chain, limiting\ntheir applicability in real-world scenarios. Recent advances in feed-forward\napproaches, driven by deep learning, have revolutionized this field by enabling\nfast and generalizable 3D reconstruction and view synthesis. This survey offers\na comprehensive review of feed-forward techniques for 3D reconstruction and\nview synthesis, with a taxonomy according to the underlying representation\narchitectures including point cloud, 3D Gaussian Splatting (3DGS), Neural\nRadiance Fields (NeRF), etc. We examine key tasks such as pose-free\nreconstruction, dynamic 3D reconstruction, and 3D-aware image and video\nsynthesis, highlighting their applications in digital humans, SLAM, robotics,\nand beyond. In addition, we review commonly used datasets with detailed\nstatistics, along with evaluation protocols for various downstream tasks. We\nconclude by discussing open research challenges and promising directions for\nfuture work, emphasizing the potential of feed-forward approaches to advance\nthe state of the art in 3D vision.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u524d\u9988\u65b9\u6cd5\u57283D\u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u7c7b\u4e86\u4e0d\u540c\u8868\u793a\u67b6\u6784\uff0c\u603b\u7ed3\u4e86\u5173\u952e\u4efb\u52a1\u548c\u6311\u6218\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u8ba1\u7b97\u4e0a\u8fc7\u4e8e\u590d\u6742\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u524d\u9988\u65b9\u6cd5\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u901a\u7528\u76843D\u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u9769\u547d\u3002", "method": "\u8bba\u6587\u63d0\u4f9b\u4e86\u524d\u9988\u6280\u672f\u57283D\u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u4e2d\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u6839\u636e\u5e95\u5c42\u8868\u793a\u67b6\u6784\uff08\u5982\u70b9\u4e91\u30013D\u9ad8\u65af\u6cfc\u6e85\u3001NeRF\u7b49\uff09\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u4e86\u5173\u952e\u4efb\u52a1\u5982\u65e0\u59ff\u6001\u91cd\u5efa\u3001\u52a8\u60013D\u91cd\u5efa\u548c3D\u611f\u77e5\u56fe\u50cf/\u89c6\u9891\u5408\u6210\u3002", "result": "\u8bba\u6587\u8be6\u7ec6\u7efc\u8ff0\u4e86\u524d\u9988\u6280\u672f\u7684\u5206\u7c7b\u3001\u5173\u952e\u4efb\u52a1\u3001\u5e94\u7528\u573a\u666f\u3001\u5e38\u7528\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u524d\u9988\u65b9\u6cd5\u57283D\u89c6\u89c9\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u7684\u5f00\u653e\u6311\u6218\u548c\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2507.14505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14505", "abs": "https://arxiv.org/abs/2507.14505", "authors": ["Jiahao Ma", "Tianyu Wang", "Miaomiao Liu", "David Ahmedt-Aristizabal", "Chuong Nguyen"], "title": "DCHM: Depth-Consistent Human Modeling for Multiview Detection", "comment": "multi-view detection, sparse-view reconstruction", "summary": "Multiview pedestrian detection typically involves two stages: human modeling\nand pedestrian localization. Human modeling represents pedestrians in 3D space\nby fusing multiview information, making its quality crucial for detection\naccuracy. However, existing methods often introduce noise and have low\nprecision. While some approaches reduce noise by fitting on costly multiview 3D\nannotations, they often struggle to generalize across diverse scenes. To\neliminate reliance on human-labeled annotations and accurately model humans, we\npropose Depth-Consistent Human Modeling (DCHM), a framework designed for\nconsistent depth estimation and multiview fusion in global coordinates.\nSpecifically, our proposed pipeline with superpixel-wise Gaussian Splatting\nachieves multiview depth consistency in sparse-view, large-scaled, and crowded\nscenarios, producing precise point clouds for pedestrian localization.\nExtensive validations demonstrate that our method significantly reduces noise\nduring human modeling, outperforming previous state-of-the-art baselines.\nAdditionally, to our knowledge, DCHM is the first to reconstruct pedestrians\nand perform multiview segmentation in such a challenging setting. Code is\navailable on the \\href{https://jiahao-ma.github.io/DCHM/}{project page}.", "AI": {"tldr": "DCHM\u6846\u67b6\u901a\u8fc7\u6df1\u5ea6\u4e00\u81f4\u6027\u5efa\u6a21\u548c\u8d85\u50cf\u7d20\u9ad8\u65af\u6e85\u5c04\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u884c\u4eba\u68c0\u6d4b\u4e2d\u7684\u566a\u58f0\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u884c\u4eba\u68c0\u6d4b\u4e2d\u56e0\u566a\u58f0\u548c\u4f4e\u7cbe\u5ea6\u95ee\u9898\uff0c\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51faDepth-Consistent Human Modeling (DCHM)\u6846\u67b6\uff0c\u7ed3\u5408\u8d85\u50cf\u7d20\u9ad8\u65af\u6e85\u5c04\u6280\u672f\uff0c\u5b9e\u73b0\u7a00\u758f\u89c6\u89d2\u3001\u5927\u89c4\u6a21\u548c\u62e5\u6324\u573a\u666f\u4e2d\u7684\u591a\u89c6\u89d2\u6df1\u5ea6\u4e00\u81f4\u6027\u5efa\u6a21\u3002", "result": "DCHM\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u4f53\u5efa\u6a21\u4e2d\u7684\u566a\u58f0\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u884c\u4eba\u91cd\u5efa\u548c\u591a\u89c6\u89d2\u5206\u5272\u3002", "conclusion": "DCHM\u6846\u67b6\u901a\u8fc7\u6df1\u5ea6\u4e00\u81f4\u6027\u5efa\u6a21\u548c\u8d85\u50cf\u7d20\u9ad8\u65af\u6e85\u5c04\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u591a\u89c6\u89d2\u884c\u4eba\u68c0\u6d4b\u4e2d\u7684\u4eba\u4f53\u5efa\u6a21\u566a\u58f0\uff0c\u5e76\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u70b9\u4e91\u91cd\u5efa\u548c\u591a\u89c6\u89d2\u5206\u5272\u3002"}}
{"id": "2507.14544", "categories": ["cs.CV", "cs.AI", "68T45 (Machine vision and scene understanding)", "I.2.10; I.4.8; H.3.1"], "pdf": "https://arxiv.org/pdf/2507.14544", "abs": "https://arxiv.org/abs/2507.14544", "authors": ["Sujata Gaihre", "Amir Thapa Magar", "Prasuna Pokharel", "Laxmi Tiwari"], "title": "Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025", "comment": "accepted to ImageCLEF 2025, to be published in the lab proceedings", "summary": "This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA\n2025 Challenge, which targets visual question answering (VQA) for\ngastrointestinal endoscopy. We adopt the Florence model-a large-scale\nmultimodal foundation model-as the backbone of our VQA pipeline, pairing a\npowerful vision encoder with a text encoder to interpret endoscopic images and\nproduce clinically relevant answers. To improve generalization, we apply\ndomain-specific augmentations that preserve medical features while increasing\ntraining diversity. Experiments on the KASVIR dataset show that fine-tuning\nFlorence yields accurate responses on the official challenge metrics. Our\nresults highlight the potential of large multimodal models in medical VQA and\nprovide a strong baseline for future work on explainability, robustness, and\nclinical integration. The code is publicly available at:\nhttps://github.com/TiwariLaxuu/VQA-Florence.git", "AI": {"tldr": "\u672c\u6587\u5229\u7528Florence\u6a21\u578b\u6539\u8fdb\u80c3\u80a0\u9053\u5185\u7aa5\u955c\u7684\u89c6\u89c9\u95ee\u7b54\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u5fae\u8c03\u5728KASVIR\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "motivation": "\u9488\u5bf9ImageCLEFmed MEDVQA 2025\u6311\u6218\u8d5b\u7684\u5b50\u4efb\u52a11\uff0c\u65e8\u5728\u901a\u8fc7\u89c6\u89c9\u95ee\u7b54\u6280\u672f\u63d0\u5347\u80c3\u80a0\u9053\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u4e34\u5e8a\u89e3\u8bfb\u3002", "method": "\u91c7\u7528Florence\u6a21\u578b\u4f5c\u4e3aVQA\u7ba1\u9053\u7684\u6838\u5fc3\uff0c\u7ed3\u5408\u5f3a\u5927\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u6587\u672c\u7f16\u7801\u5668\u6765\u89e3\u6790\u5185\u7aa5\u955c\u56fe\u50cf\u5e76\u751f\u6210\u4e34\u5e8a\u76f8\u5173\u7b54\u6848\uff0c\u540c\u65f6\u5e94\u7528\u9886\u57df\u7279\u5b9a\u7684\u6570\u636e\u589e\u5f3a\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728KASVIR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u8c03Florence\u6a21\u578b\u80fd\u591f\u5728\u5b98\u65b9\u6311\u6218\u6307\u6807\u4e0a\u4ea7\u751f\u51c6\u786e\u7684\u56de\u7b54\u3002", "conclusion": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578bFlorence\u5728\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u5728\u53ef\u89e3\u91ca\u6027\u3001\u9c81\u68d2\u6027\u548c\u4e34\u5e8a\u6574\u5408\u65b9\u9762\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2507.14533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14533", "abs": "https://arxiv.org/abs/2507.14533", "authors": ["Shuo Cao", "Nan Ma", "Jiayang Li", "Xiaohui Li", "Lihao Shao", "Kaiwen Zhu", "Yu Zhou", "Yuandong Pu", "Jiarui Wu", "Jiaquan Wang", "Bo Qu", "Wenhai Wang", "Yu Qiao", "Dajuin Yao", "Yihao Liu"], "title": "ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding", "comment": "43 pages, 31 figures, 13 tables", "summary": "The rapid advancement of educational applications, artistic creation, and\nAI-generated content (AIGC) technologies has substantially increased practical\nrequirements for comprehensive Image Aesthetics Assessment (IAA), particularly\ndemanding methods capable of delivering both quantitative scoring and\nprofessional understanding. Multimodal Large Language Model (MLLM)-based IAA\nmethods demonstrate stronger perceptual and generalization capabilities\ncompared to traditional approaches, yet they suffer from modality bias\n(score-only or text-only) and lack fine-grained attribute decomposition,\nthereby failing to support further aesthetic assessment. In this paper, we\npresent:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and\nExpert-Level Understanding capabilities; (2) ArtiMuse-10K, the first\nexpert-curated image aesthetic dataset comprising 10,000 images spanning 5 main\ncategories and 15 subcategories, each annotated by professional experts with\n8-dimensional attributes analysis and a holistic score. Both the model and\ndataset will be made public to advance the field.", "AI": {"tldr": "\u63d0\u51fa\u4e86ArtiMuse\u6a21\u578b\u548cArtiMuse-10K\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u50cf\u7f8e\u5b66\u8bc4\u4f30\u65b9\u6cd5\u7684\u6a21\u6001\u504f\u5dee\u548c\u7ec6\u7c92\u5ea6\u4e0d\u8db3\u95ee\u9898\uff0c\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002", "motivation": "\u968f\u7740\u6559\u80b2\u5e94\u7528\u3001\u827a\u672f\u521b\u4f5c\u548cAIGC\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5bf9\u56fe\u50cf\u7f8e\u5b66\u8bc4\u4f30\u7684\u9700\u6c42\u589e\u52a0\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u504f\u5dee\u548c\u7ec6\u7c92\u5ea6\u5c5e\u6027\u5206\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86ArtiMuse\uff0c\u4e00\u79cd\u57fa\u4e8eMLLM\u7684IAA\u6a21\u578b\uff0c\u5177\u5907\u8054\u5408\u8bc4\u5206\u548c\u4e13\u5bb6\u7ea7\u7406\u89e3\u80fd\u529b\uff1b\u5e76\u521b\u5efa\u4e86ArtiMuse-10K\u6570\u636e\u96c6\uff0c\u5305\u542b10,000\u5f20\u4e13\u5bb6\u6807\u6ce8\u7684\u56fe\u50cf\u3002", "result": "ArtiMuse\u6a21\u578b\u5728\u611f\u77e5\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0cArtiMuse-10K\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e13\u4e1a\u76848\u7ef4\u5c5e\u6027\u5206\u6790\u548c\u6574\u4f53\u8bc4\u5206\u3002", "conclusion": "ArtiMuse\u548cArtiMuse-10K\u7684\u516c\u5f00\u5c06\u63a8\u52a8\u56fe\u50cf\u7f8e\u5b66\u8bc4\u4f30\u9886\u57df\u7684\u53d1\u5c55\uff0c\u4e3a\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u5de5\u5177\u548c\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2507.14575", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14575", "abs": "https://arxiv.org/abs/2507.14575", "authors": ["Andrea Moschetto", "Lemuel Puglisi", "Alec Sargood", "Pierluigi Dell'Acqua", "Francesco Guarnera", "Sebastiano Battiato", "Daniele Rav\u00ec"], "title": "Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation", "comment": null, "summary": "Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image\ncontrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering\ndistinct diagnostic insights. However, acquiring all desired modalities\nincreases scan time and cost, motivating research into computational methods\nfor cross-modal synthesis. To address this, recent approaches aim to synthesize\nmissing MRI contrasts from those already acquired, reducing acquisition time\nwhile preserving diagnostic quality. Image-to-image (I2I) translation provides\na promising framework for this task. In this paper, we present a comprehensive\nbenchmark of generative models$\\unicode{x2013}$specifically, Generative\nAdversarial Networks (GANs), diffusion models, and flow matching (FM)\ntechniques$\\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All\nframeworks are implemented with comparable settings and evaluated on three\npublicly available MRI datasets of healthy adults. Our quantitative and\nqualitative analyses show that the GAN-based Pix2Pix model outperforms\ndiffusion and FM-based methods in terms of structural fidelity, image quality,\nand computational efficiency. Consistent with existing literature, these\nresults suggest that flow-based models are prone to overfitting on small\ndatasets and simpler tasks, and may require more data to match or surpass GAN\nperformance. These findings offer practical guidance for deploying I2I\ntranslation techniques in real-world MRI workflows and highlight promising\ndirections for future research in cross-modal medical image synthesis. Code and\nmodels are publicly available at\nhttps://github.com/AndreaMoschetto/medical-I2I-benchmark.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86GAN\u3001\u6269\u6563\u548cFM\u6a21\u578b\u5728T1w\u5230T2w MRI\u8de8\u6a21\u6001\u5408\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0Pix2Pix\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6700\u4f18\u3002", "motivation": "\u51cf\u5c11MRI\u626b\u63cf\u65f6\u95f4\u548c\u6210\u672c\uff0c\u901a\u8fc7\u8ba1\u7b97\u5408\u6210\u7f3a\u5931\u7684MRI\u5bf9\u6bd4\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u8bca\u65ad\u8d28\u91cf\u3002", "method": "\u672c\u6587\u5bf9\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u3001\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\uff08FM\uff09\u6280\u672f\u8fdb\u884c\u4e86\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u53ef\u6bd4\u8f83\u7684\u8bbe\u7f6e\u8bc4\u4f30\u4e86T1w\u5230T2w\u76842D MRI\u56fe\u50cf\u5230\u56fe\u50cf\uff08I2I\uff09\u7ffb\u8bd1\u3002", "result": "GAN-based Pix2Pix\u6a21\u578b\u5728\u4e09\u4e2a\u516c\u5f00\u7684MRI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u3002", "conclusion": "GAN-based Pix2Pix\u6a21\u578b\u5728\u7ed3\u6784\u4fdd\u771f\u5ea6\u3001\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u6269\u6563\u548cFM\u65b9\u6cd5\uff0c\u4e3aMRI\u8de8\u6a21\u6001\u5408\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2507.14543", "categories": ["cs.CV", "cs.CY", "cs.HC", "cs.LG", "I.4.6"], "pdf": "https://arxiv.org/pdf/2507.14543", "abs": "https://arxiv.org/abs/2507.14543", "authors": ["Sharanya Mukherjee", "Md Hishaam Akhtar", "Kannadasan R"], "title": "Real Time Captioning of Sign Language Gestures in Video Meetings", "comment": "7 pages, 2 figures, 1 table, Presented at ICCMDE 2021", "summary": "It has always been a rather tough task to communicate with someone possessing\na hearing impairment. One of the most tested ways to establish such a\ncommunication is through the use of sign based languages. However, not many\npeople are aware of the smaller intricacies involved with sign language. Sign\nlanguage recognition using computer vision aims at eliminating the\ncommunication barrier between deaf-mute and ordinary people so that they can\nproperly communicate with others. Recently the pandemic has left the whole\nworld shaken up and has transformed the way we communicate. Video meetings have\nbecome essential for everyone, even people with a hearing disability. In recent\nstudies, it has been found that people with hearing disabilities prefer to sign\nover typing during these video calls. In this paper, we are proposing a browser\nextension that will automatically translate sign language to subtitles for\neveryone else in the video call. The Large-scale dataset which contains more\nthan 2000 Word-Level ASL videos, which were performed by over 100 signers will\nbe used.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6d4f\u89c8\u5668\u6269\u5c55\uff0c\u7528\u4e8e\u5b9e\u65f6\u7ffb\u8bd1\u624b\u8bed\u4e3a\u89c6\u9891\u901a\u8bdd\u5b57\u5e55\uff0c\u5229\u7528\u5927\u89c4\u6a21ASL\u6570\u636e\u96c6\u5b9e\u73b0\u9ad8\u6548\u6c9f\u901a\u3002", "motivation": "\u542c\u969c\u4eba\u58eb\u5728\u75ab\u60c5\u671f\u95f4\u66f4\u503e\u5411\u4e8e\u4f7f\u7528\u624b\u8bed\u800c\u975e\u6253\u5b57\u8fdb\u884c\u89c6\u9891\u901a\u8bdd\uff0c\u73b0\u6709\u6280\u672f\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002", "method": "\u5229\u7528\u5305\u542b2000\u591a\u4e2a\u5355\u8bcd\u7ea7ASL\u89c6\u9891\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7531100\u591a\u540d\u624b\u8bed\u8005\u8868\u6f14\uff0c\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5b9e\u73b0\u624b\u8bed\u8bc6\u522b\u3002", "result": "\u5f00\u53d1\u7684\u6d4f\u89c8\u5668\u6269\u5c55\u80fd\u591f\u5b9e\u65f6\u7ffb\u8bd1\u624b\u8bed\u4e3a\u5b57\u5e55\uff0c\u63d0\u5347\u89c6\u9891\u901a\u8bdd\u7684\u6c9f\u901a\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4f\u89c8\u5668\u6269\u5c55\uff0c\u80fd\u591f\u81ea\u52a8\u5c06\u624b\u8bed\u7ffb\u8bd1\u4e3a\u89c6\u9891\u901a\u8bdd\u4e2d\u7684\u5b57\u5e55\uff0c\u65e8\u5728\u6d88\u9664\u542c\u969c\u4eba\u58eb\u4e0e\u666e\u901a\u4eba\u4e4b\u95f4\u7684\u6c9f\u901a\u969c\u788d\u3002"}}
{"id": "2507.14587", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14587", "abs": "https://arxiv.org/abs/2507.14587", "authors": ["Merjem Be\u0107irovi\u0107", "Amina Kurtovi\u0107", "Nordin Smajlovi\u0107", "Medina Kapo", "Amila Akagi\u0107"], "title": "Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX", "comment": null, "summary": "Medical imaging plays a vital role in early disease diagnosis and monitoring.\nSpecifically, blood microscopy offers valuable insights into blood cell\nmorphology and the detection of hematological disorders. In recent years, deep\nlearning-based automated classification systems have demonstrated high\npotential in enhancing the accuracy and efficiency of blood image analysis.\nHowever, a detailed performance analysis of specific deep learning frameworks\nappears to be lacking. This paper compares the performance of three popular\ndeep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in\nclassifying blood cell images from the publicly available BloodMNIST dataset.\nThe study primarily focuses on inference time differences, but also\nclassification performance for different image sizes. The results reveal\nvariations in performance across frameworks, influenced by factors such as\nimage resolution and framework-specific optimizations. Classification accuracy\nfor JAX and PyTorch was comparable to current benchmarks, showcasing the\nefficiency of these frameworks for medical image classification.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5728\u8840\u6db2\u7ec6\u80de\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0JAX\u548cPyTorch\u8868\u73b0\u6700\u4f73\uff0c\u4e0e\u73b0\u6709\u57fa\u51c6\u76f8\u5f53\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u8840\u6db2\u56fe\u50cf\u5206\u6790\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6f5c\u529b\uff0c\u4f46\u9488\u5bf9\u7279\u5b9a\u6846\u67b6\u7684\u8be6\u7ec6\u6027\u80fd\u5206\u6790\u5c1a\u4e0d\u5145\u5206\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u6d41\u884c\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff08TensorFlow with Keras\u3001PyTorch\u548cJAX\uff09\u5728BloodMNIST\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u4e3b\u8981\u5173\u6ce8\u63a8\u65ad\u65f6\u95f4\u5dee\u5f02\u548c\u4e0d\u540c\u56fe\u50cf\u5c3a\u5bf8\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u540c\u6846\u67b6\u7684\u6027\u80fd\u5b58\u5728\u5dee\u5f02\uff0cJAX\u548cPyTorch\u7684\u5206\u7c7b\u51c6\u786e\u7387\u4e0e\u73b0\u6709\u57fa\u51c6\u76f8\u5f53\u3002", "conclusion": "\u4e0d\u540c\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff08TensorFlow with Keras\u3001PyTorch\u548cJAX\uff09\u5728\u8840\u6db2\u7ec6\u80de\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u5b58\u5728\u5dee\u5f02\uff0c\u53d7\u56fe\u50cf\u5206\u8fa8\u7387\u548c\u6846\u67b6\u7279\u5b9a\u4f18\u5316\u7b49\u56e0\u7d20\u5f71\u54cd\u3002JAX\u548cPyTorch\u7684\u5206\u7c7b\u51c6\u786e\u7387\u4e0e\u73b0\u6709\u57fa\u51c6\u76f8\u5f53\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u9ad8\u6548\u6027\u3002"}}
{"id": "2507.14608", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14608", "abs": "https://arxiv.org/abs/2507.14608", "authors": ["Nandani Sharma", "Dinesh Singh"], "title": "Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition", "comment": null, "summary": "Facial expression recognition is crucial for human-computer interaction\napplications such as face animation, video surveillance, affective computing,\nmedical analysis, etc. Since the structure of facial attributes varies with\nfacial expressions, incorporating structural information into facial attributes\nis essential for facial expression recognition. In this paper, we propose\nExp-Graph, a novel framework designed to represent the structural relationships\namong facial attributes using graph-based modeling for facial expression\nrecognition. For facial attributes graph representation, facial landmarks are\nused as the graph's vertices. At the same time, the edges are determined based\non the proximity of the facial landmark and the similarity of the local\nappearance of the facial attributes encoded using the vision transformer.\nAdditionally, graph convolutional networks are utilized to capture and\nintegrate these structural dependencies into the encoding of facial attributes,\nthereby enhancing the accuracy of expression recognition. Thus, Exp-Graph\nlearns from the facial attribute graphs highly expressive semantic\nrepresentations. On the other hand, the vision transformer and graph\nconvolutional blocks help the framework exploit the local and global\ndependencies among the facial attributes that are essential for the recognition\nof facial expressions. We conducted comprehensive evaluations of the proposed\nExp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.\nThe model achieved recognition accuracies of 98.09\\%, 79.01\\%, and 56.39\\%,\nrespectively. These results indicate that Exp-Graph maintains strong\ngeneralization capabilities across both controlled laboratory settings and\nreal-world, unconstrained environments, underscoring its effectiveness for\npractical facial expression recognition applications.", "AI": {"tldr": "Exp-Graph\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u9762\u90e8\u5c5e\u6027\u7684\u7ed3\u6784\u4fe1\u606f\u63d0\u5347\u8868\u60c5\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5728\u591a\u79cd\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u5728\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u800c\u9762\u90e8\u5c5e\u6027\u7684\u7ed3\u6784\u968f\u8868\u60c5\u53d8\u5316\uff0c\u56e0\u6b64\u5c06\u7ed3\u6784\u4fe1\u606f\u878d\u5165\u9762\u90e8\u5c5e\u6027\u5bf9\u8868\u60c5\u8bc6\u522b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faExp-Graph\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u56fe\u7684\u5efa\u6a21\u8868\u793a\u9762\u90e8\u5c5e\u6027\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u5229\u7528\u9762\u90e8\u5173\u952e\u70b9\u4f5c\u4e3a\u56fe\u7684\u9876\u70b9\uff0c\u57fa\u4e8e\u9762\u90e8\u5173\u952e\u70b9\u7684\u63a5\u8fd1\u5ea6\u548c\u5c40\u90e8\u5916\u89c2\u76f8\u4f3c\u6027\u786e\u5b9a\u8fb9\uff0c\u5e76\u91c7\u7528\u56fe\u5377\u79ef\u7f51\u7edc\u6355\u6349\u548c\u6574\u5408\u8fd9\u4e9b\u7ed3\u6784\u4f9d\u8d56\u6027\u3002", "result": "\u5728Oulu-CASIA\u3001eNTERFACE05\u548cAFEW\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc6\u522b\u51c6\u786e\u7387\u5206\u522b\u4e3a98.09%\u300179.01%\u548c56.39%\u3002", "conclusion": "Exp-Graph\u6a21\u578b\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u548c\u771f\u5b9e\u4e16\u754c\u65e0\u7ea6\u675f\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.14549", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14549", "abs": "https://arxiv.org/abs/2507.14549", "authors": ["Haotian Deng", "Chi Zhang", "Chen Wei", "Quanying Liu"], "title": "Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions", "comment": "Accepted by IJCNN 2025", "summary": "A fundamental challenge in affective cognitive science is to develop models\nthat accurately capture the relationship between external emotional stimuli and\nhuman internal experiences. While ANNs have demonstrated remarkable accuracy in\nfacial expression recognition, their ability to model inter-individual\ndifferences in human perception remains underexplored. This study investigates\nthe phenomenon of high perceptual variability-where individuals exhibit\nsignificant differences in emotion categorization even when viewing the same\nstimulus. Inspired by the similarity between ANNs and human perception, we\nhypothesize that facial expression samples that are ambiguous for ANN\nclassifiers also elicit divergent perceptual judgments among human observers.\nTo examine this hypothesis, we introduce a novel perceptual boundary sampling\nmethod to generate facial expression stimuli that lie along ANN decision\nboundaries. These ambiguous samples form the basis of the varEmotion dataset,\nconstructed through large-scale human behavioral experiments. Our analysis\nreveals that these ANN-confusing stimuli also provoke heightened perceptual\nuncertainty in human participants, highlighting shared computational principles\nin emotion perception. Finally, by fine-tuning ANN representations using\nbehavioral data, we achieve alignment between ANN predictions and both\ngroup-level and individual-level human perceptual patterns. Our findings\nestablish a systematic link between ANN decision boundaries and human\nperceptual variability, offering new insights into personalized modeling of\nemotional interpretation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0ANN\u51b3\u7b56\u8fb9\u754c\u4e0e\u4eba\u7c7b\u611f\u77e5\u53d8\u5f02\u6027\u4e4b\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u8054\u7cfb\uff0c\u901a\u8fc7\u5fae\u8c03ANN\u8868\u793a\uff0c\u6210\u529f\u5bf9\u9f50\u4e86ANN\u9884\u6d4b\u4e0e\u4eba\u7c7b\u611f\u77e5\u6a21\u5f0f\u3002", "motivation": "\u63a2\u7d22\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u5728\u6a21\u62df\u4eba\u7c7b\u611f\u77e5\u4e2a\u4f53\u5dee\u5f02\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u9762\u90e8\u8868\u60c5\u5206\u7c7b\u4e2d\u7684\u9ad8\u611f\u77e5\u53d8\u5f02\u6027\u73b0\u8c61\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u611f\u77e5\u8fb9\u754c\u91c7\u6837\u65b9\u6cd5\uff0c\u751f\u6210\u4f4d\u4e8eANN\u51b3\u7b56\u8fb9\u754c\u7684\u9762\u90e8\u8868\u60c5\u523a\u6fc0\uff0c\u6784\u5efa\u4e86varEmotion\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u4eba\u7c7b\u884c\u4e3a\u5b9e\u9a8c\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5206\u6790\u663e\u793a\uff0cANN\u96be\u4ee5\u5206\u7c7b\u7684\u523a\u6fc0\u540c\u6837\u5f15\u53d1\u4e86\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u9ad8\u5ea6\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u63ed\u793a\u4e86\u60c5\u611f\u611f\u77e5\u4e2d\u7684\u5171\u4eab\u8ba1\u7b97\u539f\u5219\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u5fae\u8c03\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u8868\u793a\uff0c\u53ef\u4ee5\u5b9e\u73b0ANN\u9884\u6d4b\u4e0e\u7fa4\u4f53\u53ca\u4e2a\u4f53\u5c42\u9762\u7684\u4eba\u7c7b\u611f\u77e5\u6a21\u5f0f\u5bf9\u9f50\uff0c\u4e3a\u60c5\u611f\u89e3\u91ca\u7684\u4e2a\u6027\u5316\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.14657", "categories": ["cs.CV", "cs.AI", "68T45", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.14657", "abs": "https://arxiv.org/abs/2507.14657", "authors": ["Keivan Shariatmadar", "Ahmad Osman"], "title": "AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)", "comment": "24 pages, 9 figures", "summary": "The integration of Artificial Intelligence (AI) into sports officiating\nrepresents a paradigm shift in how decisions are made in competitive\nenvironments. Traditional manual systems, even when supported by Instant Video\nReplay (IVR), often suffer from latency, subjectivity, and inconsistent\nenforcement, undermining fairness and athlete trust. This paper introduces\nFST.ai, a novel AI-powered framework designed to enhance officiating in Sport\nTaekwondo, particularly focusing on the complex task of real-time head kick\ndetection and scoring. Leveraging computer vision, deep learning, and edge\ninference, the system automates the identification and classification of key\nactions, significantly reducing decision time from minutes to seconds while\nimproving consistency and transparency. Importantly, the methodology is not\nlimited to Taekwondo. The underlying framework -- based on pose estimation,\nmotion classification, and impact analysis -- can be adapted to a wide range of\nsports requiring action detection, such as judo, karate, fencing, or even team\nsports like football and basketball, where foul recognition or performance\ntracking is critical. By addressing one of Taekwondo's most challenging\nscenarios -- head kick scoring -- we demonstrate the robustness, scalability,\nand sport-agnostic potential of FST.ai to transform officiating standards\nacross multiple disciplines.", "AI": {"tldr": "FST.ai\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u88c1\u5224\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u8dc6\u62f3\u9053\u4e2d\u7684\u5b9e\u65f6\u5934\u90e8\u8e22\u51fb\u68c0\u6d4b\uff0c\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6df1\u5ea6\u5b66\u4e60\u63d0\u5347\u51b3\u7b56\u901f\u5ea6\u548c\u516c\u5e73\u6027\uff0c\u5e76\u5177\u6709\u8de8\u4f53\u80b2\u9879\u76ee\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u624b\u52a8\u88c1\u5224\u7cfb\u7edf\uff08\u5373\u4f7f\u6709\u5373\u65f6\u89c6\u9891\u56de\u653e\u652f\u6301\uff09\u5b58\u5728\u5ef6\u8fdf\u3001\u4e3b\u89c2\u6027\u548c\u6267\u884c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u516c\u5e73\u6027\u548c\u8fd0\u52a8\u5458\u7684\u4fe1\u4efb\u3002AI\u7684\u5f15\u5165\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u8fb9\u7f18\u63a8\u7406\u7684AI\u6846\u67b6FST.ai\uff0c\u4e13\u6ce8\u4e8e\u5b9e\u65f6\u5934\u90e8\u8e22\u51fb\u68c0\u6d4b\u548c\u8bc4\u5206\u3002\u65b9\u6cd5\u5305\u62ec\u59ff\u6001\u4f30\u8ba1\u3001\u52a8\u4f5c\u5206\u7c7b\u548c\u5f71\u54cd\u5206\u6790\u3002", "result": "FST.ai\u663e\u8457\u51cf\u5c11\u4e86\u51b3\u7b56\u65f6\u95f4\uff08\u4ece\u5206\u949f\u7ea7\u964d\u81f3\u79d2\u7ea7\uff09\uff0c\u5e76\u63d0\u9ad8\u4e86\u88c1\u5224\u7684\u4e00\u81f4\u6027\u548c\u900f\u660e\u5ea6\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u8dc6\u62f3\u9053\uff0c\u8fd8\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u9700\u8981\u52a8\u4f5c\u68c0\u6d4b\u7684\u4f53\u80b2\u9879\u76ee\u3002", "conclusion": "FST.ai\u6846\u67b6\u5c55\u793a\u4e86\u5728\u4f53\u80b2\u88c1\u5224\u4e2d\u5e94\u7528AI\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u63d0\u5347\u51b3\u7b56\u901f\u5ea6\u3001\u4e00\u81f4\u6027\u548c\u900f\u660e\u5ea6\u65b9\u9762\uff0c\u5e76\u4e14\u5176\u6846\u67b6\u8bbe\u8ba1\u5177\u6709\u8de8\u4f53\u80b2\u9879\u76ee\u7684\u9002\u5e94\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2507.14553", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14553", "abs": "https://arxiv.org/abs/2507.14553", "authors": ["Xiaoran Wu"], "title": "Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance", "comment": null, "summary": "Clutter in photos is a distraction preventing photographers from conveying\nthe intended emotions or stories to the audience. Photography amateurs\nfrequently include clutter in their photos due to unconscious negligence or the\nlack of experience in creating a decluttered, aesthetically appealing scene for\nshooting. We are thus motivated to develop a camera guidance system that\nprovides solutions and guidance for clutter identification and removal. We\nestimate and visualize the contribution of objects to the overall aesthetics\nand content of a photo, based on which users can interactively identify\nclutter. Suggestions on getting rid of clutter, as well as a tool that removes\ncluttered objects computationally, are provided to guide users to deal with\ndifferent kinds of clutter and improve their photographic work. Two technical\nnovelties underpin interactions in our system: a clutter distinguishment\nalgorithm with aesthetics evaluations for objects and an iterative image\ninpainting algorithm based on generative adversarial nets that reconstructs\nmissing regions of removed objects for high-resolution images. User studies\ndemonstrate that our system provides flexible interfaces and accurate\nalgorithms that allow users to better identify distractions and take higher\nquality images within less time.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u76f8\u673a\u5f15\u5bfc\u7cfb\u7edf\uff0c\u901a\u8fc7\u7f8e\u5b66\u8bc4\u4f30\u548c\u56fe\u50cf\u4fee\u590d\u7b97\u6cd5\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u5e76\u53bb\u9664\u7167\u7247\u4e2d\u7684\u6742\u7269\uff0c\u63d0\u5347\u62cd\u6444\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u6444\u5f71\u7231\u597d\u8005\u56e0\u758f\u5ffd\u6216\u7ecf\u9a8c\u4e0d\u8db3\u5bfc\u81f4\u7167\u7247\u4e2d\u6742\u7269\u8fc7\u591a\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7f8e\u5b66\u8bc4\u4f30\u7684\u6742\u7269\u533a\u5206\u7b97\u6cd5\u548c\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u8fed\u4ee3\u56fe\u50cf\u4fee\u590d\u7b97\u6cd5\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\u7cfb\u7edf\u80fd\u6709\u6548\u5e2e\u52a9\u7528\u6237\u5728\u66f4\u77ed\u65f6\u95f4\u5185\u62cd\u6444\u66f4\u9ad8\u8d28\u91cf\u7684\u7167\u7247\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u7075\u6d3b\u754c\u9762\u548c\u7cbe\u51c6\u7b97\u6cd5\u5e2e\u52a9\u7528\u6237\u66f4\u9ad8\u6548\u5730\u8bc6\u522b\u7167\u7247\u4e2d\u7684\u5e72\u6270\u7269\u5e76\u63d0\u5347\u62cd\u6444\u8d28\u91cf\u3002"}}
{"id": "2507.14662", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14662", "abs": "https://arxiv.org/abs/2507.14662", "authors": ["Shayan Rokhva", "Babak Teimourpour"], "title": "Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall", "comment": "Questions & Recommendations: shayanrokhva1999@gmail.com;\n  shayan1999rokh@yahoo.com", "summary": "Quantifying post-consumer food waste in institutional dining settings is\nessential for supporting data-driven sustainability strategies. This study\npresents a cost-effective computer vision framework that estimates plate-level\nfood waste by utilizing semantic segmentation of RGB images taken before and\nafter meal consumption across five Iranian dishes. Four fully supervised models\n(U-Net, U-Net++, and their lightweight variants) were trained using a capped\ndynamic inverse-frequency loss and AdamW optimizer, then evaluated through a\ncomprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a\ncustom-defined Distributional Pixel Agreement (DPA) metric tailored to the\ntask. All models achieved satisfying performance, and for each food type, at\nleast one model approached or surpassed 90% DPA, demonstrating strong alignment\nin pixel-wise proportion estimates. Lighter models with reduced parameter\ncounts offered faster inference, achieving real-time throughput on an NVIDIA T4\nGPU. Further analysis showed superior segmentation performance for dry and more\nrigid components (e.g., rice and fries), while more complex, fragmented, or\nviscous dishes, such as stews, showed reduced performance, specifically\npost-consumption. Despite limitations such as reliance on 2D imaging,\nconstrained food variety, and manual data collection, the proposed framework is\npioneering and represents a scalable, contactless solution for continuous\nmonitoring of food consumption. This research lays foundational groundwork for\nautomated, real-time waste tracking systems in large-scale food service\nenvironments and offers actionable insights and outlines feasible future\ndirections for dining hall management and policymakers aiming to reduce\ninstitutional food waste.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u673a\u6784\u9910\u996e\u4e2d\u7684\u98df\u7269\u6d6a\u8d39\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u81ea\u52a8\u5316\u6d6a\u8d39\u8ffd\u8e2a\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u91cf\u5316\u673a\u6784\u9910\u996e\u73af\u5883\u4e2d\u7684\u98df\u7269\u6d6a\u8d39\u5bf9\u4e8e\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528RGB\u56fe\u50cf\u5728\u9910\u524d\u548c\u9910\u540e\u7684\u8bed\u4e49\u5206\u5272\u6765\u4f30\u8ba1\u98df\u7269\u6d6a\u8d39\u3002\u4f7f\u7528\u4e86\u56db\u79cd\u5168\u76d1\u7763\u6a21\u578b\uff08U-Net\u3001U-Net++\u53ca\u5176\u8f7b\u91cf\u7ea7\u53d8\u4f53\uff09\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u9006\u9891\u7387\u635f\u5931\u548cAdamW\u4f18\u5316\u5668\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6240\u6709\u6a21\u578b\u5747\u8868\u73b0\u826f\u597d\uff0c\u81f3\u5c11\u6709\u4e00\u4e2a\u6a21\u578b\u5bf9\u6bcf\u79cd\u98df\u7269\u7c7b\u578b\u7684DPA\u63a5\u8fd1\u6216\u8d85\u8fc790%\u3002\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728NVIDIA T4 GPU\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u65f6\u63a8\u7406\u3002\u5bf9\u4e8e\u5e72\u71e5\u548c\u521a\u6027\u6210\u5206\uff08\u5982\u7c73\u996d\u548c\u85af\u6761\uff09\u7684\u5206\u5272\u6027\u80fd\u66f4\u4f18\uff0c\u800c\u590d\u6742\u3001\u788e\u7247\u5316\u6216\u7c98\u7a20\u7684\u83dc\u80b4\uff08\u5982\u7096\u83dc\uff09\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5927\u89c4\u6a21\u9910\u996e\u73af\u5883\u4e2d\u7684\u81ea\u52a8\u5316\u3001\u5b9e\u65f6\u98df\u7269\u6d6a\u8d39\u8ffd\u8e2a\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u9910\u5385\u7ba1\u7406\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2507.14555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14555", "abs": "https://arxiv.org/abs/2507.14555", "authors": ["Jintang Xue", "Ganning Zhao", "Jie-En Yao", "Hong-En Chen", "Yue Hu", "Meida Chen", "Suya You", "C. -C. Jay Kuo"], "title": "Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions", "comment": null, "summary": "Understanding 3D scenes goes beyond simply recognizing objects; it requires\nreasoning about the spatial and semantic relationships between them. Current 3D\nscene-language models often struggle with this relational understanding,\nparticularly when visual embeddings alone do not adequately convey the roles\nand interactions of objects. In this paper, we introduce Descrip3D, a novel and\npowerful framework that explicitly encodes the relationships between objects\nusing natural language. Unlike previous methods that rely only on 2D and 3D\nembeddings, Descrip3D enhances each object with a textual description that\ncaptures both its intrinsic attributes and contextual relationships. These\nrelational cues are incorporated into the model through a dual-level\nintegration: embedding fusion and prompt-level injection. This allows for\nunified reasoning across various tasks such as grounding, captioning, and\nquestion answering, all without the need for task-specific heads or additional\nsupervision. When evaluated on five benchmark datasets, including ScanRefer,\nMulti3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms\nstrong baseline models, demonstrating the effectiveness of language-guided\nrelational representation for understanding complex indoor scenes.", "AI": {"tldr": "Descrip3D\u6846\u67b6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u589e\u5f3a\u7269\u4f53\u5173\u7cfb\u8868\u793a\uff0c\u663e\u8457\u63d0\u53473D\u573a\u666f\u7406\u89e3\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u573a\u666f-\u8bed\u8a00\u6a21\u578b\u5728\u5173\u7cfb\u7406\u89e3\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u4ec5\u4f9d\u8d56\u89c6\u89c9\u5d4c\u5165\u65e0\u6cd5\u5145\u5206\u6355\u6349\u7269\u4f53\u89d2\u8272\u548c\u4ea4\u4e92\u3002", "method": "Descrip3D\u91c7\u7528\u53cc\u7ea7\u96c6\u6210\uff08\u5d4c\u5165\u878d\u5408\u548c\u63d0\u793a\u7ea7\u6ce8\u5165\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u589e\u5f3a\u7269\u4f53\u8868\u793a\uff0c\u7edf\u4e00\u652f\u6301\u591a\u79cd\u4efb\u52a1\uff08\u5982\u5b9a\u4f4d\u3001\u63cf\u8ff0\u548c\u95ee\u7b54\uff09\u3002", "result": "Descrip3D\u5728ScanRefer\u3001Multi3DRefer\u7b49\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "Descrip3D\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u663e\u5f0f\u7f16\u7801\u7269\u4f53\u95f4\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2507.14680", "categories": ["cs.CV", "cs.AI", "68T07, 92C55", "I.2.7; I.4.8; J.3"], "pdf": "https://arxiv.org/pdf/2507.14680", "abs": "https://arxiv.org/abs/2507.14680", "authors": ["Xinheng Lyu", "Yuci Liang", "Wenting Chen", "Meidan Ding", "Jiaqi Yang", "Guolin Huang", "Daokun Zhang", "Xiangjian He", "Linlin Shen"], "title": "WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis", "comment": null, "summary": "Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel\ntissue analysis across various pathological tasks. While recent advancements in\nmulti-modal large language models (MLLMs) allow multi-task WSI analysis through\nnatural language, they often underperform compared to task-specific models.\nCollaborative multi-agent systems have emerged as a promising solution to\nbalance versatility and accuracy in healthcare, yet their potential remains\nunderexplored in pathology-specific domains. To address these issues, we\npropose WSI-Agents, a novel collaborative multi-agent system for multi-modal\nWSI analysis. WSI-Agents integrates specialized functional agents with robust\ntask allocation and verification mechanisms to enhance both task-specific\naccuracy and multi-task versatility through three components: (1) a task\nallocation module assigning tasks to expert agents using a model zoo of patch\nand WSI level MLLMs, (2) a verification mechanism ensuring accuracy through\ninternal consistency checks and external validation using pathology knowledge\nbases and domain-specific models, and (3) a summary module synthesizing the\nfinal summary with visual interpretation maps. Extensive experiments on\nmulti-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs\nand medical agent frameworks across diverse tasks.", "AI": {"tldr": "WSI-Agents\u662f\u4e00\u79cd\u65b0\u578b\u534f\u4f5c\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u914d\u3001\u9a8c\u8bc1\u548c\u603b\u7ed3\u6a21\u5757\uff0c\u63d0\u5347\u591a\u6a21\u6001WSI\u5206\u6790\u7684\u51c6\u786e\u6027\u4e0e\u591a\u529f\u80fd\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728WSI\u5206\u6790\u4e2d\u591a\u529f\u80fd\u6027\u4e0e\u4efb\u52a1\u51c6\u786e\u6027\u96be\u4ee5\u517c\u987e\uff0c\u533b\u7597\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u6f5c\u529b\u5728\u75c5\u7406\u9886\u57df\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faWSI-Agents\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4efb\u52a1\u5206\u914d\u6a21\u5757\uff08\u57fa\u4e8e\u6a21\u578b\u5e93\u5206\u914d\u4efb\u52a1\uff09\u3001\u9a8c\u8bc1\u673a\u5236\uff08\u901a\u8fc7\u5185\u5916\u4e00\u81f4\u6027\u68c0\u67e5\u786e\u4fdd\u51c6\u786e\u6027\uff09\u548c\u603b\u7ed3\u6a21\u5757\uff08\u751f\u6210\u6700\u7ec8\u62a5\u544a\u4e0e\u53ef\u89c6\u5316\u89e3\u91ca\uff09\u3002", "result": "\u5728\u591a\u79cdWSI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWSI-Agents\u8868\u73b0\u4f18\u4e8e\u73b0\u6709WSI MLLMs\u548c\u533b\u7597\u4ee3\u7406\u6846\u67b6\u3002", "conclusion": "WSI-Agents\u901a\u8fc7\u6574\u5408\u4e13\u4e1a\u529f\u80fd\u4ee3\u7406\u3001\u4efb\u52a1\u5206\u914d\u4e0e\u9a8c\u8bc1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1WSI\u5206\u6790\u7684\u51c6\u786e\u6027\u4e0e\u591a\u529f\u80fd\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709WSI MLLMs\u548c\u533b\u7597\u4ee3\u7406\u6846\u67b6\u3002"}}
{"id": "2507.14559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14559", "abs": "https://arxiv.org/abs/2507.14559", "authors": ["Zixuan Hu", "Xiaotong Li", "Shixiang Tang", "Jun Liu", "Yichun Hu", "Ling-Yu Duan"], "title": "LEAD: Exploring Logit Space Evolution for Model Selection", "comment": "Accepted by CVPR 2024", "summary": "The remarkable success of pretrain-then-finetune paradigm has led to a\nproliferation of available pre-trained models for vision tasks. This surge\npresents a significant challenge in efficiently choosing the most suitable\npre-trained models for downstream tasks. The critical aspect of this challenge\nlies in effectively predicting the model transferability by considering the\nunderlying fine-tuning dynamics. Existing methods often model fine-tuning\ndynamics in feature space with linear transformations, which do not precisely\nalign with the fine-tuning objective and fail to grasp the essential\nnonlinearity from optimization. To this end, we present LEAD, a\nfinetuning-aligned approach based on the network output of logits. LEAD\nproposes a theoretical framework to model the optimization process and derives\nan ordinary differential equation (ODE) to depict the nonlinear evolution\ntoward the final logit state. Additionally, we design a class-aware\ndecomposition method to consider the varying evolution dynamics across classes\nand further ensure practical applicability. Integrating the closely aligned\noptimization objective and nonlinear modeling capabilities derived from the\ndifferential equation, our method offers a concise solution to effectively\nbridge the optimization gap in a single step, bypassing the lengthy fine-tuning\nprocess. The comprehensive experiments on 24 supervised and self-supervised\npre-trained models across 10 downstream datasets demonstrate impressive\nperformances and showcase its broad adaptability even in low-data scenarios.", "AI": {"tldr": "LEAD \u662f\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6570\u8f93\u51fa\u7684\u5fae\u8c03\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7ODE\u5efa\u6a21\u975e\u7ebf\u6027\u4f18\u5316\u8fc7\u7a0b\uff0c\u6709\u6548\u9884\u6d4b\u6a21\u578b\u8fc1\u79fb\u6027\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6fc0\u589e\u5bfc\u81f4\u9009\u62e9\u6700\u9002\u5408\u4e0b\u6e38\u4efb\u52a1\u7684\u6a21\u578b\u53d8\u5f97\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u7ebf\u6027\u5efa\u6a21\u5fae\u8c03\u52a8\u6001\uff0c\u672a\u80fd\u51c6\u786e\u6355\u6349\u4f18\u5316\u4e2d\u7684\u975e\u7ebf\u6027\u3002", "method": "LEAD \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6570\u8f93\u51fa\u7684\u5fae\u8c03\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u5efa\u6a21\u4f18\u5316\u8fc7\u7a0b\uff0c\u5e76\u8bbe\u8ba1\u7c7b\u611f\u77e5\u5206\u89e3\u65b9\u6cd5\uff0c\u8003\u8651\u4e0d\u540c\u7c7b\u522b\u7684\u52a8\u6001\u53d8\u5316\u3002", "result": "\u572824\u4e2a\u76d1\u7763\u548c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\u53ca10\u4e2a\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLEAD \u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u4ecd\u5177\u5e7f\u6cdb\u9002\u5e94\u6027\u3002", "conclusion": "LEAD \u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6570\u8f93\u51fa\u7684\u5fae\u8c03\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u548cODE\u5efa\u6a21\u975e\u7ebf\u6027\u4f18\u5316\u8fc7\u7a0b\uff0c\u6709\u6548\u9884\u6d4b\u6a21\u578b\u8fc1\u79fb\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u5e7f\u6cdb\u9002\u5e94\u6027\u3002"}}
{"id": "2507.14784", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14784", "abs": "https://arxiv.org/abs/2507.14784", "authors": ["Xinxin Dong", "Baoyun Peng", "Haokai Ma", "Yufei Wang", "Zixuan Dong", "Fei Hu", "Xiaodong Wang"], "title": "LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering", "comment": null, "summary": "Video Question Answering (VideoQA) requires identifying sparse critical\nmoments in long videos and reasoning about their causal relationships to answer\nsemantically complex questions. While recent advances in multimodal learning\nhave improved alignment and fusion, current approaches remain limited by two\nprevalent but fundamentally flawed strategies: (1) task-agnostic sampling\nindiscriminately processes all frames, overwhelming key events with irrelevant\ncontent; and (2) heuristic retrieval captures superficial patterns but misses\ncausal-temporal structures needed for complex reasoning. To address these\nchallenges, we introduce LeAdQA, an innovative approach that bridges these gaps\nthrough synergizing causal-aware query refinement with fine-grained visual\ngrounding. Our method first leverages LLMs to reformulate question-option\npairs, resolving causal ambiguities and sharpening temporal focus. These\nrefined queries subsequently direct a temporal grounding model to precisely\nretrieve the most salient segments, complemented by an adaptive fusion\nmechanism dynamically integrating the evidence to maximize relevance. The\nintegrated visual-textual cues are then processed by an MLLM to generate\naccurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and\nNExT-GQA demonstrate that our method's precise visual grounding substantially\nenhances the understanding of video-question relationships, achieving\nstate-of-the-art (SOTA) performance on complex reasoning tasks while\nmaintaining computational efficiency.", "AI": {"tldr": "LeAdQA\u7ed3\u5408\u56e0\u679c\u611f\u77e5\u67e5\u8be2\u4f18\u5316\u548c\u89c6\u89c9\u5b9a\u4f4d\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u95ee\u7b54\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u3002", "motivation": "\u5f53\u524dVideoQA\u65b9\u6cd5\u5b58\u5728\u4efb\u52a1\u65e0\u5173\u7684\u91c7\u6837\u548c\u542f\u53d1\u5f0f\u68c0\u7d22\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u7a00\u758f\u5173\u952e\u5e27\u548c\u590d\u6742\u56e0\u679c\u5173\u7cfb\u3002LeAdQA\u65e8\u5728\u901a\u8fc7\u56e0\u679c\u611f\u77e5\u67e5\u8be2\u4f18\u5316\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5b9a\u4f4d\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "LeAdQA\u9996\u5148\u5229\u7528LLMs\u91cd\u65b0\u8868\u8ff0\u95ee\u9898-\u9009\u9879\u5bf9\uff0c\u6d88\u9664\u56e0\u679c\u6a21\u7cca\u6027\u5e76\u805a\u7126\u5173\u952e\u65f6\u95f4\u70b9\uff1b\u968f\u540e\u901a\u8fc7\u65f6\u95f4\u5b9a\u4f4d\u6a21\u578b\u7cbe\u786e\u68c0\u7d22\u6700\u76f8\u5173\u89c6\u9891\u7247\u6bb5\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\u52a8\u6001\u6574\u5408\u8bc1\u636e\uff1b\u6700\u540e\u901a\u8fc7MLLM\u5904\u7406\u89c6\u89c9-\u6587\u672c\u7ebf\u7d22\u751f\u6210\u7b54\u6848\u3002", "result": "\u5728NExT-QA\u3001IntentQA\u548cNExT-GQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLeAdQA\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "LeAdQA\u901a\u8fc7\u7ed3\u5408\u56e0\u679c\u611f\u77e5\u67e5\u8be2\u4f18\u5316\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5b9a\u4f4d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u95ee\u7b54\uff08VideoQA\uff09\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5728NExT-QA\u3001IntentQA\u548cNExT-GQA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.14787", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14787", "abs": "https://arxiv.org/abs/2507.14787", "authors": ["Xi Xiao", "Aristeidis Tsaris", "Anika Tabassum", "John Lagergren", "Larry M. York", "Tianyang Wang", "Xiao Wang"], "title": "FOCUS: Fused Observation of Channels for Unveiling Spectra", "comment": null, "summary": "Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous\nwavelength bands, making it a powerful tool in biology, agriculture, and\nenvironmental monitoring. However, interpreting Vision Transformers (ViTs) in\nthis setting remains largely unexplored due to two key challenges: (1) existing\nsaliency methods struggle to capture meaningful spectral cues, often collapsing\nattention onto the class token, and (2) full-spectrum ViTs are computationally\nprohibitive for interpretability, given the high-dimensional nature of HSI\ndata. We present FOCUS, the first framework that enables reliable and efficient\nspatial-spectral interpretability for frozen ViTs. FOCUS introduces two core\ncomponents: class-specific spectral prompts that guide attention toward\nsemantically meaningful wavelength groups, and a learnable [SINK] token trained\nwith an attraction loss to absorb noisy or redundant attention. Together, these\ndesigns make it possible to generate stable and interpretable 3D saliency maps\nand spectral importance curves in a single forward pass, without any gradient\nbackpropagation or backbone modification. FOCUS improves band-level IoU by 15\npercent, reduces attention collapse by over 40 percent, and produces saliency\nresults that align closely with expert annotations. With less than 1 percent\nparameter overhead, our method makes high-resolution ViT interpretability\npractical for real-world hyperspectral applications, bridging a long-standing\ngap between black-box modeling and trustworthy HSI decision-making.", "AI": {"tldr": "FOCUS\u6846\u67b6\u901a\u8fc7\u5149\u8c31\u63d0\u793a\u548c[SINK]\u4ee4\u724c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684ViT\u5728HSI\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u5347\u4e86\u6ce2\u6bb5\u8bc6\u522b\u548c\u6ce8\u610f\u529b\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u663e\u8457\u6027\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u6709\u610f\u4e49\u7684\u9891\u8c31\u7ebf\u7d22\uff0c\u4e14\u5168\u9891\u8c31ViT\u5728\u9ad8\u7ef4HSI\u6570\u636e\u4e2d\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86ViT\u5728HSI\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u5e94\u7528\u3002", "method": "FOCUS\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u7c7b\u7279\u5b9a\u5149\u8c31\u63d0\u793a\u548c\u53ef\u5b66\u4e60\u7684[SINK]\u4ee4\u724c\uff0c\u901a\u8fc7\u5438\u5f15\u529b\u635f\u5931\u8bad\u7ec3\uff0c\u907f\u514d\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\u6216\u4e3b\u5e72\u7f51\u7edc\u4fee\u6539\u3002", "result": "FOCUS\u5c06\u6ce2\u6bb5\u7ea7IoU\u63d0\u9ad8\u4e8615%\uff0c\u51cf\u5c11\u4e86\u8d85\u8fc740%\u7684\u6ce8\u610f\u529b\u5d29\u6e83\uff0c\u751f\u6210\u7684\u663e\u8457\u6027\u7ed3\u679c\u4e0e\u4e13\u5bb6\u6807\u6ce8\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "FOCUS\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u7c7b\u7279\u5b9a\u5149\u8c31\u63d0\u793a\u548c\u53ef\u5b66\u4e60\u7684[SINK]\u4ee4\u724c\uff0c\u663e\u8457\u63d0\u5347\u4e86ViT\u5728HSI\u6570\u636e\u4e2d\u7684\u89e3\u91ca\u6027\uff0c\u4f7f\u5176\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\uff0c\u751f\u6210\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u76843D\u663e\u8457\u6027\u56fe\u548c\u5149\u8c31\u91cd\u8981\u6027\u66f2\u7ebf\u3002"}}
{"id": "2507.14596", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14596", "abs": "https://arxiv.org/abs/2507.14596", "authors": ["Doriand Petit", "Steve Bourgeois", "Vincent Gay-Bellile", "Florian Chabot", "Lo\u00efc Barthe"], "title": "DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF", "comment": "Published at ICCV'25", "summary": "3D semantic segmentation provides high-level scene understanding for\napplications in robotics, autonomous systems, \\textit{etc}. Traditional methods\nadapt exclusively to either task-specific goals (open-vocabulary segmentation)\nor scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the\nfirst method addressing the broader problem of 3D Open-Vocabulary Sub-concepts\nDiscovery, which aims to provide a 3D semantic segmentation that adapts to both\nthe scene and user queries. We build DiSCO-3D on Neural Fields representations,\ncombining unsupervised segmentation with weak open-vocabulary guidance. Our\nevaluations demonstrate that DiSCO-3D achieves effective performance in\nOpen-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in\nthe edge cases of both open-vocabulary and unsupervised segmentation.", "AI": {"tldr": "DiSCO-3D\u662f\u9996\u4e2a\u89e3\u51b33D\u5f00\u653e\u8bcd\u6c47\u5b50\u6982\u5ff5\u53d1\u73b0\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u65e0\u76d1\u7763\u5206\u5272\u548c\u5f00\u653e\u8bcd\u6c47\u6307\u5bfc\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\u76ee\u6807\u6216\u573a\u666f\u5185\u5bb9\uff0c\u65e0\u6cd5\u540c\u65f6\u9002\u5e94\u573a\u666f\u548c\u7528\u6237\u67e5\u8be2\u3002", "method": "\u57fa\u4e8e\u795e\u7ecf\u573a\u8868\u793a\uff0c\u7ed3\u5408\u65e0\u76d1\u7763\u5206\u5272\u548c\u5f31\u5f00\u653e\u8bcd\u6c47\u6307\u5bfc\u3002", "result": "DiSCO-3D\u5728\u5f00\u653e\u8bcd\u6c47\u5b50\u6982\u5ff5\u53d1\u73b0\u4e2d\u5b9e\u73b0\u4e86\u6709\u6548\u6027\u80fd\u3002", "conclusion": "DiSCO-3D\u57283D\u5f00\u653e\u8bcd\u6c47\u5b50\u6982\u5ff5\u53d1\u73b0\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u5f00\u653e\u8bcd\u6c47\u548c\u65e0\u76d1\u7763\u5206\u5272\u7684\u8fb9\u7f18\u6848\u4f8b\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002"}}
{"id": "2507.14807", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14807", "abs": "https://arxiv.org/abs/2507.14807", "authors": ["Juan Hu", "Shaojing Fan", "Terence Sim"], "title": "Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection", "comment": null, "summary": "Multi-face deepfake videos are becoming increasingly prevalent, often\nappearing in natural social settings that challenge existing detection methods.\nMost current approaches excel at single-face detection but struggle in\nmulti-face scenarios, due to a lack of awareness of crucial contextual cues. In\nthis work, we develop a novel approach that leverages human cognition to\nanalyze and defend against multi-face deepfake videos. Through a series of\nhuman studies, we systematically examine how people detect deepfake faces in\nsocial settings. Our quantitative analysis reveals four key cues humans rely\non: scene-motion coherence, inter-face appearance compatibility, interpersonal\ngaze alignment, and face-body consistency. Guided by these insights, we\nintroduce \\textsf{HICOM}, a novel framework designed to detect every fake face\nin multi-face scenarios. Extensive experiments on benchmark datasets show that\n\\textsf{HICOM} improves average accuracy by 3.3\\% in in-dataset detection and\n2.8\\% under real-world perturbations. Moreover, it outperforms existing methods\nby 5.8\\% on unseen datasets, demonstrating the generalization of human-inspired\ncues. \\textsf{HICOM} further enhances interpretability by incorporating an LLM\nto provide human-readable explanations, making detection results more\ntransparent and convincing. Our work sheds light on involving human factors to\nenhance defense against deepfakes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faHICOM\u6846\u67b6\uff0c\u5229\u7528\u4eba\u7c7b\u8ba4\u77e5\u7ebf\u7d22\u63d0\u5347\u591a\u9762\u5b54\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u5728\u5355\u9762\u5b54\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u9762\u5b54\u793e\u4ea4\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u5bf9\u5173\u952e\u4e0a\u4e0b\u6587\u7ebf\u7d22\u7684\u611f\u77e5\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u4eba\u4eec\u5728\u793e\u4ea4\u73af\u5883\u4e2d\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u9762\u5b54\u7684\u56db\u79cd\u5173\u952e\u7ebf\u7d22\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u7ebf\u7d22\u5f00\u53d1\u4e86HICOM\u6846\u67b6\u3002", "result": "HICOM\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e863.3%\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6270\u52a8\u4e0b\u63d0\u9ad8\u4e862.8%\uff0c\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd55.8%\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u4eba\u7c7b\u8ba4\u77e5\u7684\u7ebf\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u9762\u5b54\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u589e\u5f3a\u4e86\u68c0\u6d4b\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.14811", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14811", "abs": "https://arxiv.org/abs/2507.14811", "authors": ["Jiaji Zhang", "Ruichao Sun", "Hailiang Zhao", "Jiaju Wu", "Peng Chen", "Hao Li", "Xinkui Zhao", "Kingsum Chow", "Gang Xiong", "Lin Ye", "Shuiguang Deng"], "title": "SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models", "comment": null, "summary": "Diffusion models have demonstrated exceptional generative capabilities but\nare computationally intensive, posing significant challenges for deployment in\nresource-constrained or latency-sensitive environments. Quantization offers an\neffective means to reduce model size and computational cost, with post-training\nquantization (PTQ) being particularly appealing due to its compatibility with\npre-trained models without requiring retraining or training data. However,\nexisting PTQ methods for diffusion models often rely on architecture-specific\nheuristics that limit their generalizability and hinder integration with\nindustrial deployment pipelines. To address these limitations, we propose\nSegQuant, a unified quantization framework that adaptively combines\ncomplementary techniques to enhance cross-model versatility. SegQuant consists\nof a segment-aware, graph-based quantization strategy (SegLinear) that captures\nstructural semantics and spatial heterogeneity, along with a dual-scale\nquantization scheme (DualScale) that preserves polarity-asymmetric activations,\nwhich is crucial for maintaining visual fidelity in generated outputs. SegQuant\nis broadly applicable beyond Transformer-based diffusion models, achieving\nstrong performance while ensuring seamless compatibility with mainstream\ndeployment tools.", "AI": {"tldr": "SegQuant\u662f\u4e00\u4e2a\u901a\u7528\u7684\u91cf\u5316\u6846\u67b6\uff0c\u7ed3\u5408SegLinear\u548cDualScale\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578bPTQ\u7684\u901a\u7528\u6027\u548c\u90e8\u7f72\u95ee\u9898\uff0c\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u80fd\u529b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u6216\u5ef6\u8fdf\u654f\u611f\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002\u73b0\u6709PTQ\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u67b6\u6784\u542f\u53d1\u5f0f\uff0c\u901a\u7528\u6027\u5dee\uff0c\u96be\u4ee5\u4e0e\u5de5\u4e1a\u90e8\u7f72\u6d41\u7a0b\u96c6\u6210\u3002", "method": "SegQuant\u5305\u542b\u4e00\u4e2a\u6bb5\u611f\u77e5\u7684\u57fa\u4e8e\u56fe\u7684\u91cf\u5316\u7b56\u7565\uff08SegLinear\uff09\u548c\u4e00\u4e2a\u53cc\u5c3a\u5ea6\u91cf\u5316\u65b9\u6848\uff08DualScale\uff09\uff0c\u524d\u8005\u6355\u6349\u7ed3\u6784\u8bed\u4e49\u548c\u7a7a\u95f4\u5f02\u8d28\u6027\uff0c\u540e\u8005\u4fdd\u7559\u6781\u6027\u4e0d\u5bf9\u79f0\u6fc0\u6d3b\u3002", "result": "SegQuant\u4e0d\u4ec5\u9002\u7528\u4e8e\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\uff0c\u8fd8\u80fd\u4e0e\u4e3b\u6d41\u90e8\u7f72\u5de5\u5177\u65e0\u7f1d\u517c\u5bb9\uff0c\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "SegQuant\u662f\u4e00\u4e2a\u901a\u7528\u7684\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7ed3\u5408\u4e92\u8865\u6280\u672f\u589e\u5f3a\u8de8\u6a21\u578b\u9002\u5e94\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709PTQ\u65b9\u6cd5\u5728\u6269\u6563\u6a21\u578b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8f93\u51fa\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2507.14613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14613", "abs": "https://arxiv.org/abs/2507.14613", "authors": ["Guoping Xu", "Christopher Kabat", "You Zhang"], "title": "Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2", "comment": "24 pages, 6 figures", "summary": "Recent advances in medical image segmentation have been driven by deep\nlearning; however, most existing methods remain limited by modality-specific\ndesigns and exhibit poor adaptability to dynamic medical imaging scenarios. The\nSegment Anything Model 2 (SAM2) and its related variants, which introduce a\nstreaming memory mechanism for real-time video segmentation, present new\nopportunities for prompt-based, generalizable solutions. Nevertheless, adapting\nthese models to medical video scenarios typically requires large-scale datasets\nfor retraining or transfer learning, leading to high computational costs and\nthe risk of catastrophic forgetting. To address these challenges, we propose\nDD-SAM2, an efficient adaptation framework for SAM2 that incorporates a\nDepthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature\nextraction with minimal parameter overhead. This design enables effective\nfine-tuning of SAM2 on medical videos with limited training data. Unlike\nexisting adapter-based methods focused solely on static images, DD-SAM2 fully\nexploits SAM2's streaming memory for medical video object tracking and\nsegmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)\nand EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior\nperformance, achieving Dice scores of 0.93 and 0.97, respectively. To the best\nof our knowledge, this work provides an initial attempt at systematically\nexploring adapter-based SAM2 fine-tuning for medical video segmentation and\ntracking. Code, datasets, and models will be publicly available at\nhttps://github.com/apple1986/DD-SAM2.", "AI": {"tldr": "DD-SAM2\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff08DD-Adapter\uff09\u9ad8\u6548\u5fae\u8c03SAM2\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u89c6\u9891\u5206\u5272\u4e0e\u8ffd\u8e2a\u6027\u80fd\uff0c\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u591a\u4e3a\u6a21\u6001\u7279\u5b9a\u8bbe\u8ba1\uff0c\u9002\u5e94\u6027\u5dee\uff0c\u4e14SAM2\u7b49\u6a21\u578b\u5728\u533b\u5b66\u89c6\u9891\u573a\u666f\u4e2d\u9700\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5fae\u8c03\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6613\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u63d0\u51faDD-SAM2\u6846\u67b6\uff0c\u7ed3\u5408Depthwise-Dilated Adapter\uff08DD-Adapter\uff09\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u5b9e\u73b0SAM2\u5728\u533b\u5b66\u89c6\u9891\u4e0a\u7684\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u5728TrackRad2025\u548cEchoNet-Dynamic\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u5230Dice\u5206\u65700.93\u548c0.97\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DD-SAM2\u901a\u8fc7\u5f15\u5165Depthwise-Dilated Adapter\uff08DD-Adapter\uff09\u6709\u6548\u63d0\u5347\u4e86SAM2\u5728\u533b\u5b66\u89c6\u9891\u5206\u5272\u548c\u8ffd\u8e2a\u4e2d\u7684\u6027\u80fd\uff0c\u4e14\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u5fae\u8c03\uff0c\u4e3a\u533b\u5b66\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14833", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14833", "abs": "https://arxiv.org/abs/2507.14833", "authors": ["Haoxuan Zhang", "Wenju Cui", "Yuzhu Cao", "Tao Tan", "Jie Liu", "Yunsong Peng", "Jian Zheng"], "title": "Paired Image Generation with Diffusion-Guided Diffusion Models", "comment": null, "summary": "The segmentation of mass lesions in digital breast tomosynthesis (DBT) images\nis very significant for the early screening of breast cancer. However, the\nhigh-density breast tissue often leads to high concealment of the mass lesions,\nwhich makes manual annotation difficult and time-consuming. As a result, there\nis a lack of annotated data for model training. Diffusion models are commonly\nused for data augmentation, but the existing methods face two challenges.\nFirst, due to the high concealment of lesions, it is difficult for the model to\nlearn the features of the lesion area. This leads to the low generation quality\nof the lesion areas, thus limiting the quality of the generated images. Second,\nexisting methods can only generate images and cannot generate corresponding\nannotations, which restricts the usability of the generated images in\nsupervised training. In this work, we propose a paired image generation method.\nThe method does not require external conditions and can achieve the generation\nof paired images by training an extra diffusion guider for the conditional\ndiffusion model. During the experimental phase, we generated paired DBT slices\nand mass lesion masks. Then, we incorporated them into the supervised training\nprocess of the mass lesion segmentation task. The experimental results show\nthat our method can improve the generation quality without external conditions.\nMoreover, it contributes to alleviating the shortage of annotated data, thus\nenhancing the performance of downstream tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u6761\u4ef6\u7684\u914d\u5bf9\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u6269\u6563\u5f15\u5bfc\u5668\u751f\u6210DBT\u5207\u7247\u548c\u75c5\u7076\u63a9\u7801\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "DBT\u56fe\u50cf\u4e2d\u80bf\u5757\u75c5\u7076\u7684\u9ad8\u9690\u853d\u6027\u5bfc\u81f4\u624b\u52a8\u6807\u6ce8\u56f0\u96be\u4e14\u8017\u65f6\uff0c\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u96be\u4ee5\u5b66\u4e60\u75c5\u7076\u533a\u57df\u7279\u5f81\u4e14\u65e0\u6cd5\u751f\u6210\u5bf9\u5e94\u6807\u6ce8\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u989d\u5916\u7684\u6269\u6563\u5f15\u5bfc\u5668\u6765\u5b9e\u73b0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u914d\u5bf9\u56fe\u50cf\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u6709\u6548\u7f13\u89e3\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u5347\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u914d\u5bf9\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u6761\u4ef6\u5373\u53ef\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u7f13\u89e3\u6807\u6ce8\u6570\u636e\u77ed\u7f3a\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2507.14632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14632", "abs": "https://arxiv.org/abs/2507.14632", "authors": ["Haiquan Wen", "Tianxiao Li", "Zhenglin Huang", "Yiwei He", "Guangliang Cheng"], "title": "BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM", "comment": null, "summary": "Recent advances in generative AI have dramatically improved image and video\nsynthesis capabilities, significantly increasing the risk of misinformation\nthrough sophisticated fake content. In response, detection methods have evolved\nfrom traditional approaches to multimodal large language models (MLLMs),\noffering enhanced transparency and interpretability in identifying synthetic\nmedia. However, current detection systems remain fundamentally limited by their\nsingle-modality design. These approaches analyze images or videos separately,\nmaking them ineffective against synthetic content that combines multiple media\nformats. To address these challenges, we introduce \\textbf{BusterX++}, a novel\nframework designed specifically for cross-modal detection and explanation of\nsynthetic media. Our approach incorporates an advanced reinforcement learning\n(RL) post-training strategy that eliminates cold-start. Through Multi-stage\nTraining, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and\nsubstantial performance improvements. To enable comprehensive evaluation, we\nalso present \\textbf{GenBuster++}, a cross-modal benchmark leveraging\nstate-of-the-art image and video generation techniques. This benchmark\ncomprises 4,000 images and video clips, meticulously curated by human experts\nusing a novel filtering methodology to ensure high quality, diversity, and\nreal-world applicability. Extensive experiments demonstrate the effectiveness\nand generalizability of our approach.", "AI": {"tldr": "BusterX++ \u662f\u591a\u6a21\u6001\u5408\u6210\u5a92\u4f53\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u63d0\u5347\u6027\u80fd\uff0cGenBuster++ \u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u751f\u6210\u5f0f AI \u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u5408\u6210\u5a92\u4f53\uff08\u56fe\u50cf\u3001\u89c6\u9891\uff09\u7684\u8bef\u7528\u98ce\u9669\u589e\u52a0\uff0c\u73b0\u6709\u5355\u6a21\u6001\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u591a\u683c\u5f0f\u7ed3\u5408\u7684\u5408\u6210\u5185\u5bb9\u3002", "method": "BusterX++ \u91c7\u7528\u591a\u6a21\u6001\u8bbe\u8ba1\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7b56\u7565\uff08\u591a\u9636\u6bb5\u8bad\u7ec3\u3001\u601d\u7ef4\u5956\u52b1\u548c\u6df7\u5408\u63a8\u7406\uff09\uff0c\u4ee5\u89e3\u51b3\u5355\u6a21\u6001\u68c0\u6d4b\u7684\u5c40\u9650\u6027\u3002", "result": "BusterX++ \u5728 GenBuster++ \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7a33\u5b9a\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u5176\u8de8\u6a21\u6001\u68c0\u6d4b\u7684\u6709\u6548\u6027\u3002", "conclusion": "BusterX++ \u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u8bbe\u8ba1\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u5a92\u4f53\u7684\u68c0\u6d4b\u4e0e\u89e3\u91ca\u80fd\u529b\uff0cGenBuster++ \u57fa\u51c6\u6d4b\u8bd5\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2507.14851", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.14851", "abs": "https://arxiv.org/abs/2507.14851", "authors": ["Muhammad Kamran Janjua", "Amirhosein Ghasemabadi", "Kunlin Zhang", "Mohammad Salameh", "Chao Gao", "Di Niu"], "title": "Grounding Degradations in Natural Language for All-In-One Video Restoration", "comment": "17 pages", "summary": "In this work, we propose an all-in-one video restoration framework that\ngrounds degradation-aware semantic context of video frames in natural language\nvia foundation models, offering interpretable and flexible guidance. Unlike\nprior art, our method assumes no degradation knowledge in train or test time\nand learns an approximation to the grounded knowledge such that the foundation\nmodel can be safely disentangled during inference adding no extra cost.\nFurther, we call for standardization of benchmarks in all-in-one video\nrestoration, and propose two benchmarks in multi-degradation setting,\nthree-task (3D) and four-task (4D), and two time-varying composite degradation\nbenchmarks; one of the latter being our proposed dataset with varying snow\nintensity, simulating how weather degradations affect videos naturally. We\ncompare our method with prior works and report state-of-the-art performance on\nall benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u9000\u5316\u77e5\u8bc6\u7684\u5168\u5408\u4e00\u89c6\u9891\u4fee\u590d\u6846\u67b6\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u548c\u81ea\u7136\u8bed\u8a00\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u4fee\u590d\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9000\u5316\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5176\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u548c\u57fa\u7840\u6a21\u578b\uff0c\u63d0\u4f9b\u4e00\u4e2a\u65e0\u9700\u9000\u5316\u77e5\u8bc6\u7684\u5168\u5408\u4e00\u89c6\u9891\u4fee\u590d\u6846\u67b6\uff0c\u5e76\u63a8\u52a8\u8be5\u9886\u57df\u7684\u57fa\u51c6\u6807\u51c6\u5316\u3002", "method": "\u8bba\u6587\u65b9\u6cd5\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5c06\u89c6\u9891\u5e27\u7684\u9000\u5316\u611f\u77e5\u8bed\u4e49\u4e0a\u4e0b\u6587\u8fdb\u884c\u5efa\u6a21\uff0c\u65e0\u9700\u9000\u5316\u77e5\u8bc6\u5373\u53ef\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002\u65b9\u6cd5\u5b66\u4e60\u5bf9\u57fa\u7840\u6a21\u578b\u7684\u8fd1\u4f3c\u77e5\u8bc6\uff0c\u4f7f\u5f97\u63a8\u7406\u65f6\u53ef\u4ee5\u65e0\u6210\u672c\u5730\u89e3\u8026\u6a21\u578b\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5305\u62ec\u591a\u9000\u5316\u8bbe\u7f6e\uff083D\u548c4D\uff09\u548c\u65f6\u53d8\u590d\u5408\u9000\u5316\u57fa\u51c6\uff08\u5982\u6a21\u62df\u96ea\u5f3a\u5ea6\u53d8\u5316\u7684\u81ea\u7136\u5929\u6c14\u9000\u5316\u6570\u636e\u96c6\uff09\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u5408\u4e00\u7684\u89c6\u9891\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u5c06\u89c6\u9891\u5e27\u7684\u9000\u5316\u611f\u77e5\u8bed\u4e49\u4e0a\u4e0b\u6587\u4e0e\u81ea\u7136\u8bed\u8a00\u7ed3\u5408\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u7075\u6d3b\u7684\u6307\u5bfc\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u6216\u6d4b\u8bd5\u65f6\u65e0\u9700\u9000\u5316\u77e5\u8bc6\uff0c\u5e76\u5b66\u4e60\u5bf9\u57fa\u7840\u6a21\u578b\u7684\u8fd1\u4f3c\u77e5\u8bc6\uff0c\u4ece\u800c\u5728\u63a8\u7406\u65f6\u65e0\u989d\u5916\u6210\u672c\u5730\u89e3\u8026\u57fa\u7840\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8bba\u6587\u547c\u5401\u5168\u5408\u4e00\u89c6\u9891\u4fee\u590d\u7684\u57fa\u51c6\u6807\u51c6\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u9000\u5316\u8bbe\u7f6e\u4e0b\u7684\u4e24\u4e2a\u57fa\u51c6\uff083D\u548c4D\uff09\u4ee5\u53ca\u4e24\u4e2a\u65f6\u53d8\u590d\u5408\u9000\u5316\u57fa\u51c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u57fa\u51c6\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14643", "abs": "https://arxiv.org/abs/2507.14643", "authors": ["Jifeng Shen", "Haibo Zhan", "Shaohua Dong", "Xin Zuo", "Wankou Yang", "Haibin Ling"], "title": "Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection", "comment": "submitted on 30/4/2025, Under Major Revision", "summary": "Modern multispectral feature fusion for object detection faces two critical\nlimitations: (1) Excessive preference for local complementary features over\ncross-modal shared semantics adversely affects generalization performance; and\n(2) The trade-off between the receptive field size and computational complexity\npresent critical bottlenecks for scalable feature modeling. Addressing these\nissues, a novel Multispectral State-Space Feature Fusion framework, dubbed\nMS2Fusion, is proposed based on the state space model (SSM), achieving\nefficient and effective fusion through a dual-path parametric interaction\nmechanism. More specifically, the first cross-parameter interaction branch\ninherits the advantage of cross-attention in mining complementary information\nwith cross-modal hidden state decoding in SSM. The second shared-parameter\nbranch explores cross-modal alignment with joint embedding to obtain\ncross-modal similar semantic features and structures through parameter sharing\nin SSM. Finally, these two paths are jointly optimized with SSM for fusing\nmultispectral features in a unified framework, allowing our MS2Fusion to enjoy\nboth functional complementarity and shared semantic space. In our extensive\nexperiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our\nMS2Fusion significantly outperforms other state-of-the-art multispectral object\ndetection methods, evidencing its superiority. Moreover, MS2Fusion is general\nand applicable to other multispectral perception tasks. We show that, even\nwithout specific design, MS2Fusion achieves state-of-the-art results on RGB-T\nsemantic segmentation and RGBT salient object detection, showing its\ngenerality. The source code will be available at\nhttps://github.com/61s61min/MS2Fusion.git.", "AI": {"tldr": "MS2Fusion\u901a\u8fc7\u53cc\u8def\u5f84SSM\u6846\u67b6\u9ad8\u6548\u878d\u5408\u591a\u5149\u8c31\u7279\u5f81\uff0c\u5728\u76ee\u6807\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u4e14\u901a\u7528\u6027\u5f3a\u3002", "motivation": "\u89e3\u51b3\u591a\u5149\u8c31\u7279\u5f81\u878d\u5408\u4e2d\u8fc7\u5ea6\u504f\u597d\u5c40\u90e8\u4e92\u8865\u7279\u5f81\u800c\u5ffd\u7565\u8de8\u6a21\u6001\u5171\u4eab\u8bed\u4e49\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u611f\u53d7\u91ce\u5927\u5c0f\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7684MS2Fusion\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u8def\u5f84\u53c2\u6570\u4ea4\u4e92\u673a\u5236\uff1a\u7b2c\u4e00\u6761\u8def\u5f84\u901a\u8fc7\u8de8\u6a21\u6001\u9690\u85cf\u72b6\u6001\u89e3\u7801\u6316\u6398\u4e92\u8865\u4fe1\u606f\uff0c\u7b2c\u4e8c\u6761\u8def\u5f84\u901a\u8fc7\u53c2\u6570\u5171\u4eab\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u5728FLIR\u3001M3FD\u548cLLVIP\u7b49\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728RGB-T\u8bed\u4e49\u5206\u5272\u548cRGBT\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f18\u6548\u679c\u3002", "conclusion": "MS2Fusion\u6846\u67b6\u901a\u8fc7\u53cc\u8def\u5f84\u53c2\u6570\u4ea4\u4e92\u673a\u5236\uff0c\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u878d\u5408\u591a\u5149\u8c31\u7279\u5f81\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5728\u5176\u4ed6\u591a\u5149\u8c31\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2507.14904", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14904", "abs": "https://arxiv.org/abs/2507.14904", "authors": ["Fan Li", "Zanyi Wang", "Zeyi Huang", "Guang Dai", "Jingdong Wang", "Mengmeng Wang"], "title": "TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP", "comment": null, "summary": "3D visual grounding allows an embodied agent to understand visual information\nin real-world 3D environments based on human instructions, which is crucial for\nembodied intelligence. Existing 3D visual grounding methods typically rely on\nseparate encoders for different modalities (e.g., RGB images, text, and 3D\npoint clouds), resulting in large and complex models that are inefficient to\ntrain. While some approaches use pre-trained 2D multi-modal models like CLIP\nfor 3D tasks, they still struggle with aligning point cloud data to 2D\nencoders. As a result, these methods continue to depend on 3D encoders for\nfeature extraction, further increasing model complexity and training\ninefficiency. In this paper, we propose a unified 2D pre-trained multi-modal\nnetwork to process all three modalities (RGB images, text, and point clouds),\nsignificantly simplifying the architecture. By leveraging a 2D CLIP bi-modal\nmodel with adapter-based fine-tuning, this framework effectively adapts to the\ntri-modal setting, improving both adaptability and performance across\nmodalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module\nis designed to fuse geometric multi-scale features from point clouds and\nimages. We then integrate textual features for final modality fusion and\nintroduce a multi-modal decoder to facilitate deep cross-modal understanding.\nTogether, our method achieves unified feature extraction and fusion across the\nthree modalities, enabling an end-to-end 3D visual grounding model. Compared to\nthe baseline, our method reduces the number of trainable parameters by\napproximately 58\\%, while achieving a 6.52\\% improvement in the 3D detection\ntask and a 6.25\\% improvement in the 3D visual grounding task.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e002D\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u7f51\u7edc\uff0c\u7b80\u53163D\u89c6\u89c9\u5b9a\u4f4d\u67b6\u6784\uff0c\u901a\u8fc7GARF\u6a21\u5757\u5b9e\u73b0\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\uff0c\u51cf\u5c11\u53c2\u6570\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u591a\u6a21\u6001\u5206\u79bb\u7f16\u7801\u5668\uff0c\u5bfc\u81f4\u6a21\u578b\u5e9e\u5927\u4e14\u8bad\u7ec3\u6548\u7387\u4f4e\u3002\u5c3d\u7ba1\u90e8\u5206\u65b9\u6cd5\u5c1d\u8bd5\u4f7f\u7528\u9884\u8bad\u7ec3\u76842D\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4f46\u4ecd\u96be\u4ee5\u5bf9\u9f50\u70b9\u4e91\u6570\u636e\u4e0e2D\u7f16\u7801\u5668\uff0c\u4ecd\u9700\u4f9d\u8d563D\u7f16\u7801\u5668\u3002", "method": "\u5229\u7528\u57fa\u4e8e2D CLIP\u53cc\u6a21\u6001\u6a21\u578b\u7684\u9002\u914d\u5668\u5fae\u8c03\u6846\u67b6\uff0c\u8bbe\u8ba1GARF\u6a21\u5757\u878d\u5408\u70b9\u4e91\u4e0e\u56fe\u50cf\u7684\u51e0\u4f55\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u6587\u672c\u7279\u5f81\u8fdb\u884c\u6700\u7ec8\u6a21\u6001\u878d\u5408\uff0c\u5f15\u5165\u591a\u6a21\u6001\u89e3\u7801\u5668\u4ee5\u5b9e\u73b0\u6df1\u5ea6\u8de8\u6a21\u6001\u7406\u89e3\u3002", "result": "\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u7ea658%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u540c\u65f6\u57283D\u68c0\u6d4b\u4efb\u52a1\u4e2d\u63d0\u5347\u4e866.52%\uff0c\u57283D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u63d0\u5347\u4e866.25%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u76842D\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u7f51\u7edc\uff0c\u663e\u8457\u7b80\u5316\u4e863D\u89c6\u89c9\u5b9a\u4f4d\u6a21\u578b\u7684\u67b6\u6784\u3002\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u76842D-3D\u7279\u5f81\u6062\u590d\u4e0e\u878d\u5408\u6a21\u5757\uff08GARF\uff09\uff0c\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u7684\u7edf\u4e00\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\uff0c\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.14670", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14670", "abs": "https://arxiv.org/abs/2507.14670", "authors": ["Yaxuan Song", "Jianan Fan", "Hang Chang", "Weidong Cai"], "title": "Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images", "comment": "16 pages, 15 tables, 8 figures", "summary": "Accurately predicting gene expression from histopathology images offers a\nscalable and non-invasive approach to molecular profiling, with significant\nimplications for precision medicine and computational pathology. However,\nexisting methods often underutilize the cross-modal representation alignment\nbetween histopathology images and gene expression profiles across multiple\nrepresentational levels, thereby limiting their prediction performance. To\naddress this, we propose Gene-DML, a unified framework that structures latent\nspace through Dual-pathway Multi-Level discrimination to enhance correspondence\nbetween morphological and transcriptional modalities. The multi-scale\ninstance-level discrimination pathway aligns hierarchical histopathology\nrepresentations extracted at local, neighbor, and global levels with gene\nexpression profiles, capturing scale-aware morphological-transcriptional\nrelationships. In parallel, the cross-level instance-group discrimination\npathway enforces structural consistency between individual (image/gene)\ninstances and modality-crossed (gene/image, respectively) groups, strengthening\nthe alignment across modalities. By jointly modelling fine-grained and\nstructural-level discrimination, Gene-DML is able to learn robust cross-modal\nrepresentations, enhancing both predictive accuracy and generalization across\ndiverse biological contexts. Extensive experiments on public spatial\ntranscriptomics datasets demonstrate that Gene-DML achieves state-of-the-art\nperformance in gene expression prediction. The code and checkpoints will be\nreleased soon.", "AI": {"tldr": "Gene-DML\u901a\u8fc7\u53cc\u8def\u5f84\u591a\u5c42\u6b21\u5224\u522b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e0e\u57fa\u56e0\u8868\u8fbe\u8c31\u4e4b\u95f4\u7684\u8de8\u6a21\u6001\u8868\u793a\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u6027\u80fd\u3002Gene-DML\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Gene-DML\u91c7\u7528\u53cc\u8def\u5f84\uff08\u591a\u5c3a\u5ea6\u5b9e\u4f8b\u7ea7\u5224\u522b\u548c\u8de8\u7ea7\u5b9e\u4f8b-\u7ec4\u5224\u522b\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u5bf9\u9f50\u5f62\u6001\u5b66\u548c\u8f6c\u5f55\u6a21\u6001\u7684\u8868\u793a\uff0c\u589e\u5f3a\u8de8\u6a21\u6001\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5728\u516c\u5171\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGene-DML\u5728\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Gene-DML\u901a\u8fc7\u53cc\u8def\u5f84\u591a\u5c42\u6b21\u5224\u522b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u7cbe\u51c6\u533b\u5b66\u548c\u8ba1\u7b97\u75c5\u7406\u5b66\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.14675", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14675", "abs": "https://arxiv.org/abs/2507.14675", "authors": ["Yuchen Duan", "Zhe Chen", "Yusong Hu", "Weiyun Wang", "Shenglong Ye", "Botian Shi", "Lewei Lu", "Qibin Hou", "Tong Lu", "Hongsheng Li", "Jifeng Dai", "Wenhai Wang"], "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding", "comment": null, "summary": "Despite significant progress in multimodal large language models (MLLMs),\ntheir performance on complex, multi-page document comprehension remains\ninadequate, largely due to the lack of high-quality, document-level datasets.\nWhile current retrieval-augmented generation (RAG) methods offer partial\nsolutions, they suffer from issues, such as fragmented retrieval contexts,\nmulti-stage error accumulation, and extra time costs of retrieval. In this\nwork, we present a high-quality document-level dataset, Doc-750K, designed to\nsupport in-depth understanding of multimodal documents. This dataset includes\ndiverse document structures, extensive cross-page dependencies, and real\nquestion-answer pairs derived from the original documents. Building on the\ndataset, we develop a native multimodal model, Docopilot, which can accurately\nhandle document-level dependencies without relying on RAG. Experiments\ndemonstrate that Docopilot achieves superior coherence, accuracy, and\nefficiency in document understanding tasks and multi-turn interactions, setting\na new baseline for document-level multimodal understanding. Data, code, and\nmodels are released at https://github.com/OpenGVLab/Docopilot", "AI": {"tldr": "\u63d0\u51fa\u4e86Doc-750K\u6570\u636e\u96c6\u548cDocopilot\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u591a\u9875\u6587\u6863\u7684\u7406\u89e3\u80fd\u529b\uff0c\u65e0\u9700\u4f9d\u8d56RAG\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u9875\u6587\u6863\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6587\u6863\u7ea7\u6570\u636e\u96c6\uff0c\u4e14\u73b0\u6709RAG\u65b9\u6cd5\u5b58\u5728\u68c0\u7d22\u4e0a\u4e0b\u6587\u788e\u7247\u5316\u3001\u591a\u9636\u6bb5\u9519\u8bef\u7d2f\u79ef\u548c\u989d\u5916\u65f6\u95f4\u6210\u672c\u7b49\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86Doc-750K\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6837\u6587\u6863\u7ed3\u6784\u548c\u8de8\u9875\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86\u539f\u751f\u591a\u6a21\u6001\u6a21\u578bDocopilot\uff0c\u65e0\u9700\u4f9d\u8d56RAG\u5373\u53ef\u5904\u7406\u6587\u6863\u7ea7\u4f9d\u8d56\u3002", "result": "Docopilot\u5728\u6587\u6863\u7406\u89e3\u4efb\u52a1\u548c\u591a\u8f6e\u4ea4\u4e92\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u8fde\u8d2f\u6027\u3001\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "Docopilot\u901a\u8fc7\u9ad8\u8d28\u91cf\u7684Doc-750K\u6570\u636e\u96c6\u548c\u539f\u751f\u591a\u6a21\u6001\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u6587\u6863\u7406\u89e3\u4efb\u52a1\u7684\u8fde\u8d2f\u6027\u3001\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u6587\u6863\u7ea7\u591a\u6a21\u6001\u7406\u89e3\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2507.14686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14686", "abs": "https://arxiv.org/abs/2507.14686", "authors": ["Chen Cai", "Tianyi Liu", "Jianjun Gao", "Wenyang Liu", "Kejun Wu", "Ruoyu Wang", "Yi Wang", "Soo Chin Liew"], "title": "From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition", "comment": null, "summary": "Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot\nabilities but struggle with complex Grounded Situation Recognition (GSR) and\nare resource-intensive for edge device deployment. Meanwhile, conventional GSR\nmodels often lack generalization ability, falling short in recognizing unseen\nand rare situations. In this paper, we exploit transferring knowledge from a\nteacher MLLM to a small GSR model to enhance its generalization and zero-shot\nabilities, thereby introducing the task of Open-vocabulary Grounded Situation\nRecognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt\nDistillation (MIPD), a novel framework that distills enriched multimodal\nknowledge from the foundation model, enabling the student Ov-GSR model to\nrecognize unseen situations and be better aware of rare situations.\nSpecifically, the MIPD framework first leverages the LLM-based Judgmental\nRationales Generator (JRG) to construct positive and negative glimpse and gaze\nrationales enriched with contextual semantic information. The proposed\nscene-aware and instance-perception prompts are then introduced to align\nrationales with visual information from the MLLM teacher via the\nNegative-Guided Multimodal Prompting Alignment (NMPA) module, effectively\ncapturing holistic and perceptual multimodal knowledge. Finally, the aligned\nmultimodal knowledge is distilled into the student Ov-GSR model, providing a\nstronger foundation for generalization that enhances situation understanding,\nbridges the gap between seen and unseen scenarios, and mitigates prediction\nbias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving\nsuperior performance on seen, rare, and unseen situations, and further\ndemonstrate improved unseen detection on the HICO-DET dataset.", "AI": {"tldr": "MIPD\u901a\u8fc7\u4eceMLLM\u4e2d\u63d0\u53d6\u591a\u6a21\u6001\u77e5\u8bc6\uff0c\u63d0\u5347\u4e86\u5c0f\u6a21\u578b\u5728\u5f00\u653e\u8bcd\u6c47\u57fa\u7840\u60c5\u5883\u8bc6\u522b\u4e2d\u7684\u6cdb\u5316\u548c\u96f6\u6837\u672c\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3MLLMs\u5728\u590d\u6742\u57fa\u7840\u60c5\u5883\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8d44\u6e90\u5bc6\u96c6\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4ee5\u53ca\u4f20\u7edfGSR\u6a21\u578b\u5bf9\u672a\u89c1\u548c\u7f55\u89c1\u60c5\u5883\u8bc6\u522b\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Multimodal Interactive Prompt Distillation (MIPD)\u6846\u67b6\uff0c\u5305\u62ecLLM-based Judgmental Rationales Generator (JRG)\u3001Negative-Guided Multimodal Prompting Alignment (NMPA)\u6a21\u5757\u7b49\uff0c\u4ee5\u5bf9\u9f50\u548c\u63d0\u53d6\u591a\u6a21\u6001\u77e5\u8bc6\u3002", "result": "\u5728Ov-SWiG\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5bf9\u5df2\u89c1\u3001\u7f55\u89c1\u548c\u672a\u89c1\u60c5\u5883\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5728HICO-DET\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u672a\u89c1\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "MIPD\u6846\u67b6\u901a\u8fc7\u4ece\u6559\u5e08MLLM\u4e2d\u63d0\u53d6\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751fOv-GSR\u6a21\u578b\u5728\u5f00\u653e\u8bcd\u6c47\u57fa\u7840\u60c5\u5883\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u548c\u96f6\u6837\u672c\u80fd\u529b\u3002"}}
{"id": "2507.14697", "categories": ["cs.CV", "I.4.6; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.14697", "abs": "https://arxiv.org/abs/2507.14697", "authors": ["Zhiwei Zhang", "Zi Ye", "Yibin Wen", "Shuai Yuan", "Haohuan Fu", "Jianxi Huang", "Juepeng Zheng"], "title": "GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset", "comment": "38 pages, 18 figures, submitted to NeurIPS 2025", "summary": "Agricultural parcels serve as basic units for conducting agricultural\npractices and applications, which is vital for land ownership registration,\nfood security assessment, soil erosion monitoring, etc. However, existing\nagriculture parcel extraction studies only focus on mid-resolution mapping or\nregular plain farmlands while lacking representation of complex terraced\nterrains due to the demands of precision agriculture.In this paper, we\nintroduce a more fine-grained terraced parcel dataset named GTPBD (Global\nTerraced Parcel and Boundary Dataset), which is the first fine-grained dataset\ncovering major worldwide terraced regions with more than 200,000 complex\nterraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution\nimages with three-level labels, including pixel-level boundary labels, mask\nlabels, and parcel labels. It covers seven major geographic zones in China and\ntranscontinental climatic regions around the world.Compared to the existing\ndatasets, the GTPBD dataset brings considerable challenges due to the: (1)\nterrain diversity; (2) complex and irregular parcel objects; and (3) multiple\ndomain styles. Our proposed GTPBD dataset is suitable for four different tasks,\nincluding semantic segmentation, edge detection, terraced parcel extraction,\nand unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the\nGTPBD dataset on eight semantic segmentation methods, four edge extraction\nmethods, three parcel extraction methods, and five UDA methods, along with a\nmulti-dimensional evaluation framework integrating pixel-level and object-level\nmetrics. GTPBD fills a critical gap in terraced remote sensing research,\nproviding a basic infrastructure for fine-grained agricultural terrain analysis\nand cross-scenario knowledge transfer.", "AI": {"tldr": "GTPBD\u662f\u9996\u4e2a\u5168\u7403\u7cbe\u7ec6\u68af\u7530\u5730\u5757\u6570\u636e\u96c6\uff0c\u8986\u76d6\u590d\u6742\u5730\u5f62\uff0c\u652f\u6301\u591a\u79cd\u4efb\u52a1\uff0c\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u519c\u4e1a\u5730\u5757\u63d0\u53d6\u7814\u7a76\u7f3a\u4e4f\u5bf9\u590d\u6742\u68af\u7530\u5730\u5f62\u7684\u7cbe\u7ec6\u8868\u8fbe\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7cbe\u51c6\u519c\u4e1a\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aGTPBD\u7684\u7cbe\u7ec6\u68af\u7530\u5730\u5757\u6570\u636e\u96c6\uff0c\u5305\u542b47,537\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u4e09\u7ea7\u6807\u7b7e\uff0c\u8986\u76d6\u5168\u7403\u4e3b\u8981\u68af\u7530\u533a\u57df\u3002", "result": "GTPBD\u5728\u8bed\u4e49\u5206\u5272\u3001\u8fb9\u7f18\u68c0\u6d4b\u3001\u68af\u7530\u5730\u5757\u63d0\u53d6\u548c\u65e0\u76d1\u7763\u57df\u9002\u5e94\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "GTPBD\u586b\u8865\u4e86\u68af\u7530\u9065\u611f\u7814\u7a76\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u7cbe\u7ec6\u519c\u4e1a\u5730\u5f62\u5206\u6790\u548c\u8de8\u573a\u666f\u77e5\u8bc6\u8fc1\u79fb\u63d0\u4f9b\u4e86\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2507.15064", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15064", "abs": "https://arxiv.org/abs/2507.15064", "authors": ["Shuyuan Tu", "Zhen Xing", "Xintong Han", "Zhi-Qi Cheng", "Qi Dai", "Chong Luo", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation", "comment": "arXiv admin note: substantial text overlap with arXiv:2411.17697", "summary": "Current diffusion models for human image animation often struggle to maintain\nidentity (ID) consistency, especially when the reference image and driving\nvideo differ significantly in body size or position. We introduce\nStableAnimator++, the first ID-preserving video diffusion framework with\nlearnable pose alignment, capable of generating high-quality videos conditioned\non a reference image and a pose sequence without any post-processing. Building\nupon a video diffusion model, StableAnimator++ contains carefully designed\nmodules for both training and inference, striving for identity consistency. In\nparticular, StableAnimator++ first uses learnable layers to predict the\nsimilarity transformation matrices between the reference image and the driven\nposes via injecting guidance from Singular Value Decomposition (SVD). These\nmatrices align the driven poses with the reference image, mitigating\nmisalignment to a great extent. StableAnimator++ then computes image and face\nembeddings using off-the-shelf encoders, refining the face embeddings via a\nglobal content-aware Face Encoder. To further maintain ID, we introduce a\ndistribution-aware ID Adapter that counteracts interference caused by temporal\nlayers while preserving ID via distribution alignment. During the inference\nstage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization\nintegrated into the denoising process, guiding the diffusion trajectory for\nenhanced facial fidelity. Experiments on benchmarks show the effectiveness of\nStableAnimator++ both qualitatively and quantitatively.", "AI": {"tldr": "StableAnimator++ \u662f\u4e00\u79cd\u65b0\u578b\u89c6\u9891\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u59ff\u6001\u5bf9\u9f50\u548cID\u9002\u914d\u5668\uff0c\u89e3\u51b3\u4e86\u4eba\u4f53\u52a8\u753b\u4e2d\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u4eba\u4f53\u56fe\u50cf\u52a8\u753b\u4e2d\u56e0\u59ff\u6001\u6216\u4f53\u578b\u5dee\u5f02\u5bfc\u81f4\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "StableAnimator++ \u7ed3\u5408\u4e86\u53ef\u5b66\u4e60\u7684\u59ff\u6001\u5bf9\u9f50\u6a21\u5757\u3001\u5168\u5c40\u5185\u5bb9\u611f\u77e5\u7684\u4eba\u8138\u7f16\u7801\u5668\u548c\u5206\u5e03\u611f\u77e5\u7684ID\u9002\u914d\u5668\uff0c\u901a\u8fc7HJB\u4f18\u5316\u8fdb\u4e00\u6b65\u63d0\u5347\u9762\u90e8\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cStableAnimator++ \u5728\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "StableAnimator++ \u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u59ff\u6001\u5bf9\u9f50\u548c\u5206\u5e03\u611f\u77e5\u7684ID\u9002\u914d\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2507.14738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14738", "abs": "https://arxiv.org/abs/2507.14738", "authors": ["Jeannie She", "Katie Spivakovsky"], "title": "MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of preventable blindness,\naffecting over 100 million people worldwide. In the United States, individuals\nfrom lower-income communities face a higher risk of progressing to advanced\nstages before diagnosis, largely due to limited access to screening. Comorbid\nconditions further accelerate disease progression. We propose MultiRetNet, a\nnovel pipeline combining retinal imaging, socioeconomic factors, and\ncomorbidity profiles to improve DR staging accuracy, integrated with a clinical\ndeferral system for a clinical human-in-the-loop implementation. We experiment\nwith three multimodal fusion methods and identify fusion through a fully\nconnected layer as the most versatile methodology. We synthesize adversarial,\nlow-quality images and use contrastive learning to train the deferral system,\nguiding the model to identify out-of-distribution samples that warrant\nclinician review. By maintaining diagnostic accuracy on suboptimal images and\nintegrating critical health data, our system can improve early detection,\nparticularly in underserved populations where advanced DR is often first\nidentified. This approach may reduce healthcare costs, increase early detection\nrates, and address disparities in access to care, promoting healthcare equity.", "AI": {"tldr": "MultiRetNet\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u4e34\u5e8a\u5ef6\u8fdf\u7cfb\u7edf\uff0c\u63d0\u9ad8\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u671f\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5e2e\u52a9\u4f4e\u6536\u5165\u4eba\u7fa4\u65e9\u671f\u68c0\u6d4b\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u662f\u5168\u7403\u53ef\u9884\u9632\u6027\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f4e\u6536\u5165\u793e\u533a\u4eba\u7fa4\u56e0\u7b5b\u67e5\u673a\u4f1a\u6709\u9650\uff0c\u66f4\u5bb9\u6613\u8fdb\u5c55\u81f3\u665a\u671f\u3002\u5171\u75c5\u6761\u4ef6\u8fdb\u4e00\u6b65\u52a0\u901f\u75be\u75c5\u8fdb\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u7684\u65e9\u671f\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMultiRetNet\u7ba1\u9053\uff0c\u7ed3\u5408\u89c6\u7f51\u819c\u6210\u50cf\u3001\u793e\u4f1a\u7ecf\u6d4e\u56e0\u7d20\u548c\u5171\u75c5\u6863\u6848\uff0c\u91c7\u7528\u4e09\u79cd\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u5e76\u786e\u5b9a\u5168\u8fde\u63a5\u5c42\u878d\u5408\u4e3a\u6700\u4f73\u65b9\u6cd5\u3002\u901a\u8fc7\u5408\u6210\u5bf9\u6297\u6027\u4f4e\u8d28\u91cf\u56fe\u50cf\u548c\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u5ef6\u8fdf\u7cfb\u7edf\uff0c\u6307\u5bfc\u6a21\u578b\u8bc6\u522b\u9700\u8981\u4e34\u5e8a\u533b\u751f\u5ba1\u67e5\u7684\u6837\u672c\u3002", "result": "MultiRetNet\u5728\u6b21\u4f18\u56fe\u50cf\u4e0a\u4fdd\u6301\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u5e76\u6574\u5408\u5173\u952e\u5065\u5eb7\u6570\u636e\uff0c\u5c24\u5176\u5728\u533b\u7597\u8d44\u6e90\u532e\u4e4f\u4eba\u7fa4\u4e2d\u663e\u8457\u63d0\u9ad8\u65e9\u671f\u68c0\u6d4b\u7387\u3002", "conclusion": "MultiRetNet\u901a\u8fc7\u7ed3\u5408\u89c6\u7f51\u819c\u6210\u50cf\u3001\u793e\u4f1a\u7ecf\u6d4e\u56e0\u7d20\u548c\u5171\u75c5\u6863\u6848\uff0c\u63d0\u9ad8\u4e86\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u671f\u7684\u51c6\u786e\u6027\uff0c\u5e76\u7ed3\u5408\u4e34\u5e8a\u5ef6\u8fdf\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u4e34\u5e8a\u4eba\u673a\u534f\u4f5c\u3002\u8be5\u65b9\u6cd5\u6709\u671b\u964d\u4f4e\u533b\u7597\u6210\u672c\u3001\u63d0\u9ad8\u65e9\u671f\u68c0\u6d4b\u7387\u5e76\u89e3\u51b3\u533b\u7597\u8d44\u6e90\u4e0d\u5e73\u7b49\u95ee\u9898\u3002"}}
{"id": "2507.15094", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15094", "abs": "https://arxiv.org/abs/2507.15094", "authors": ["Mengya Xu", "Rulin Zhou", "An Wang", "Chaoyang Lyu", "Zhen Li", "Ning Zhong", "Hongliang Ren"], "title": "BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking", "comment": "27 pages, 14 figures", "summary": "Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses\nsignificant risks, demanding precise, real-time localization and continuous\nmonitoring of the bleeding source for effective hemostatic intervention. In\nparticular, endoscopists have to repeatedly flush to clear blood, allowing only\nmilliseconds to identify bleeding sources, an inefficient process that prolongs\noperations and elevates patient risks. However, current Artificial Intelligence\n(AI) methods primarily focus on bleeding region segmentation, overlooking the\ncritical need for accurate bleeding source detection and temporal tracking in\nthe challenging ESD environment, which is marked by frequent visual\nobstructions and dynamic scene changes. This gap is widened by the lack of\nspecialized datasets, hindering the development of robust AI-assisted guidance\nsystems. To address these challenges, we introduce BleedOrigin-Bench, the first\ncomprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated\nbleeding sources across 106,222 frames from 44 procedures, supplemented with\n39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6\nchallenging clinical scenarios. We also present BleedOrigin-Net, a novel\ndual-stage detection-tracking framework for the bleeding source localization in\nESD procedures, addressing the complete workflow from bleeding onset detection\nto continuous spatial tracking. We compare with widely-used object detection\nmodels (YOLOv11/v12), multimodal large language models, and point tracking\nmethods. Extensive evaluation demonstrates state-of-the-art performance,\nachieving 96.85% frame-level accuracy ($\\pm\\leq8$ frames) for bleeding onset\ndetection, 70.24% pixel-level accuracy ($\\leq100$ px) for initial source\ndetection, and 96.11% pixel-level accuracy ($\\leq100$ px) for point tracking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a ESD \u51fa\u8840\u6e90\u6570\u636e\u96c6 BleedOrigin-Bench \u548c\u53cc\u9636\u6bb5\u68c0\u6d4b-\u8ddf\u8e2a\u6846\u67b6 BleedOrigin-Net\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51fa\u8840\u6e90\u7684\u5b9e\u65f6\u5b9a\u4f4d\u4e0e\u8ddf\u8e2a\u80fd\u529b\u3002", "motivation": "\u9488\u5bf9 ESD \u624b\u672f\u4e2d\u51fa\u8840\u6e90\u5b9e\u65f6\u5b9a\u4f4d\u548c\u8fde\u7eed\u76d1\u6d4b\u7684\u6311\u6218\uff0c\u73b0\u6709 AI \u65b9\u6cd5\u4ec5\u5173\u6ce8\u51fa\u8840\u533a\u57df\u5206\u5272\uff0c\u7f3a\u4e4f\u5bf9\u51fa\u8840\u6e90\u7684\u7cbe\u786e\u5b9a\u4f4d\u548c\u52a8\u6001\u8ddf\u8e2a\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u9636\u6bb5\u68c0\u6d4b-\u8ddf\u8e2a\u6846\u67b6 BleedOrigin-Net\uff0c\u7ed3\u5408\u4e86\u51fa\u8840\u6e90\u68c0\u6d4b\u548c\u65f6\u7a7a\u8ddf\u8e2a\u6280\u672f\uff0c\u5e76\u5229\u7528 BleedOrigin-Bench \u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "BleedOrigin-Net \u5728\u51fa\u8840\u8d77\u59cb\u68c0\u6d4b\u3001\u521d\u59cb\u6e90\u5b9a\u4f4d\u548c\u70b9\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5206\u522b\u4e3a 96.85% \u5e27\u7ea7\u51c6\u786e\u7387\u300170.24% \u50cf\u7d20\u7ea7\u51c6\u786e\u7387\u548c 96.11% \u50cf\u7d20\u7ea7\u51c6\u786e\u7387\u3002", "conclusion": "BleedOrigin-Net \u5728 ESD \u624b\u672f\u4e2d\u5b9e\u73b0\u4e86\u51fa\u8840\u6e90\u7684\u7cbe\u786e\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u6548\u7387\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2507.14743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14743", "abs": "https://arxiv.org/abs/2507.14743", "authors": ["Joseph Raj Vishal", "Rutuja Patil", "Manas Srinivas Gowda", "Katha Naik", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "InterAct-Video: Reasoning-Rich Video QA for Urban Traffic", "comment": null, "summary": "Traffic monitoring is crucial for urban mobility, road safety, and\nintelligent transportation systems (ITS). Deep learning has advanced\nvideo-based traffic monitoring through video question answering (VideoQA)\nmodels, enabling structured insight extraction from traffic videos. However,\nexisting VideoQA models struggle with the complexity of real-world traffic\nscenes, where multiple concurrent events unfold across spatiotemporal\ndimensions. To address these challenges, this paper introduces \\textbf{InterAct\nVideoQA}, a curated dataset designed to benchmark and enhance VideoQA models\nfor traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of\nreal-world traffic footage collected from diverse intersections, segmented into\n10-second video clips, with over 25,000 question-answer (QA) pairs covering\nspatiotemporal dynamics, vehicle interactions, incident detection, and other\ncritical traffic attributes. State-of-the-art VideoQA models are evaluated on\nInterAct VideoQA, exposing challenges in reasoning over fine-grained\nspatiotemporal dependencies within complex traffic scenarios. Additionally,\nfine-tuning these models on InterAct VideoQA yields notable performance\nimprovements, demonstrating the necessity of domain-specific datasets for\nVideoQA. InterAct VideoQA is publicly available as a benchmark dataset to\nfacilitate future research in real-world deployable VideoQA models for\nintelligent transportation systems. GitHub Repo:\nhttps://github.com/joe-rabbit/InterAct_VideoQA", "AI": {"tldr": "InterAct VideoQA\u662f\u4e00\u4e2a\u9488\u5bf9\u590d\u6742\u4ea4\u901a\u573a\u666f\u7684\u89c6\u9891\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u5347\u548c\u8bc4\u4f30VideoQA\u6a21\u578b\uff0c\u5df2\u5728GitHub\u516c\u5f00\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u95ee\u7b54\uff08VideoQA\uff09\u6a21\u578b\u5728\u5904\u7406\u73b0\u5b9e\u4ea4\u901a\u573a\u666f\u7684\u590d\u6742\u6027\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u4e9b\u573a\u666f\u4e2d\u591a\u4e2a\u5e76\u53d1\u4e8b\u4ef6\u5728\u65f6\u7a7a\u7ef4\u5ea6\u4e0a\u5c55\u5f00\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86InterAct VideoQA\u6570\u636e\u96c6\uff0c\u5305\u542b8\u5c0f\u65f6\u7684\u771f\u5b9e\u4ea4\u901a\u89c6\u9891\uff0c\u5206\u4e3a10\u79d2\u526a\u8f91\uff0c\u5e76\u6807\u6ce8\u4e86\u8d85\u8fc725,000\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u8986\u76d6\u65f6\u7a7a\u52a8\u6001\u3001\u8f66\u8f86\u4ea4\u4e92\u3001\u4e8b\u4ef6\u68c0\u6d4b\u7b49\u5173\u952e\u4ea4\u901a\u5c5e\u6027\u3002", "result": "\u901a\u8fc7\u5728InterAct VideoQA\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff0c\u73b0\u6709\u6700\u5148\u8fdb\u7684VideoQA\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u5bf9VideoQA\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "InterAct VideoQA\u6570\u636e\u96c6\u4f5c\u4e3a\u4e00\u4e2a\u516c\u5f00\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u53ef\u90e8\u7f72\u89c6\u9891\u95ee\u7b54\u6a21\u578b\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2507.15243", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15243", "abs": "https://arxiv.org/abs/2507.15243", "authors": ["Naeem Paeedeh", "Mahardhika Pratama", "Wolfgang Mayer", "Jimmy Cao", "Ryszard Kowlczyk"], "title": "Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation", "comment": null, "summary": "Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model\npre-trained with DINO combined with a prototypical classifier outperforms the\nlatest SOTA methods. A crucial limitation that needs to be overcome is that\nupdating too many parameters of the transformers leads to overfitting due to\nthe scarcity of labeled samples. To address this challenge, we propose a new\nconcept, Coalescent Projection (CP), as an effective successor to soft prompts.\nAdditionally, we propose a novel pseudo-class generation method combined with\nSelf-Supervised Transformations (SSTs) that relies solely on the base domain to\nprepare the network for encountering unseen samples from different domains. The\nproposed method exhibits its effectiveness in comprehensive experiments on the\nextreme domain shift scenario of the BSCD-FSL benchmark. Our code is published\nat https://github.com/Naeem-Paeedeh/CPLSR.", "AI": {"tldr": "\u7ed3\u5408DINO\u548c\u539f\u578b\u5206\u7c7b\u5668\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5728CD-FSL\u4e2d\u8d85\u8d8aSOTA\u65b9\u6cd5\uff1b\u63d0\u51faCP\u548cSSTs\u4f2a\u7c7b\u751f\u6210\u65b9\u6cd5\u89e3\u51b3\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5728\u6781\u7aef\u57df\u504f\u79fb\u573a\u666f\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u56e0\u6807\u8bb0\u6837\u672c\u7a00\u7f3a\u5bfc\u81f4\u66f4\u65b0\u8fc7\u591aTransformer\u53c2\u6570\u65f6\u51fa\u73b0\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u63d0\u5347\u8de8\u57df\u5c11\u6837\u672c\u5b66\u4e60\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6982\u5ff5Coalescent Projection\uff08CP\uff09\u4f5c\u4e3a\u8f6f\u63d0\u793a\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u7ed3\u5408\u81ea\u76d1\u7763\u53d8\u6362\uff08SSTs\uff09\u7684\u4f2a\u7c7b\u751f\u6210\u65b9\u6cd5\uff0c\u4ec5\u4f9d\u8d56\u57fa\u7840\u57df\u6570\u636e\u4e3a\u7f51\u7edc\u5e94\u5bf9\u4e0d\u540c\u57df\u7684\u672a\u89c1\u6837\u672c\u505a\u51c6\u5907\u3002", "result": "\u5728BSCD-FSL\u57fa\u51c6\u6d4b\u8bd5\u7684\u6781\u7aef\u57df\u504f\u79fb\u573a\u666f\u4e0b\uff0c\u6240\u63d0\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "conclusion": "\u5c3d\u7ba1\u5728\u8de8\u57df\u5c11\u6837\u672c\u5b66\u4e60\uff08CD-FSL\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u7ed3\u5408DINO\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u539f\u578b\u5206\u7c7b\u5668\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u6700\u65b0\u7684SOTA\u65b9\u6cd5\u3002\u672c\u6587\u63d0\u51fa\u7684Coalescent Projection\uff08CP\uff09\u6982\u5ff5\u548c\u7ed3\u5408\u81ea\u76d1\u7763\u53d8\u6362\uff08SSTs\uff09\u7684\u4f2a\u7c7b\u751f\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56e0\u6807\u8bb0\u6837\u672c\u7a00\u7f3a\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5e76\u5728\u6781\u7aef\u57df\u504f\u79fb\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.14790", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14790", "abs": "https://arxiv.org/abs/2507.14790", "authors": ["Wenbo Yue", "Chang Li", "Guoping Xu"], "title": "A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation", "comment": "6 pages, 6 figures", "summary": "In convolutional neural networks (CNNs), downsampling operations are crucial\nto model performance. Although traditional downsampling methods (such as\nmaximum pooling and cross-row convolution) perform well in feature aggregation,\nreceptive field expansion, and computational reduction, they may lead to the\nloss of key spatial information in semantic segmentation tasks, thereby\naffecting the pixel-by-pixel prediction accuracy.To this end, this study\nproposes a downsampling method based on information complementarity - Hybrid\nPooling Downsampling (HPD). The core is to replace the traditional method with\nMinMaxPooling, and effectively retain the light and dark contrast and detail\nfeatures of the image by extracting the maximum value information of the local\narea.Experiment on various CNN architectures on the ACDC and Synapse datasets\nshow that HPD outperforms traditional methods in segmentation performance, and\nincreases the DSC coefficient by 0.5% on average. The results show that the HPD\nmodule provides an efficient solution for semantic segmentation tasks.", "AI": {"tldr": "\u63d0\u51faHPD\u4e0b\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7MinMaxPooling\u4fdd\u7559\u56fe\u50cf\u7ec6\u8282\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4e0b\u91c7\u6837\u65b9\u6cd5\uff08\u5982\u6700\u5927\u6c60\u5316\u548c\u8de8\u884c\u5377\u79ef\uff09\u5728\u7279\u5f81\u805a\u5408\u3001\u611f\u53d7\u91ce\u6269\u5c55\u548c\u8ba1\u7b97\u91cf\u51cf\u5c11\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u53ef\u80fd\u5bfc\u81f4\u5173\u952e\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\uff0c\u5f71\u54cd\u9010\u50cf\u7d20\u9884\u6d4b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u4e92\u8865\u7684\u4e0b\u91c7\u6837\u65b9\u6cd5\u2014\u2014\u6df7\u5408\u6c60\u5316\u4e0b\u91c7\u6837\uff08HPD\uff09\uff0c\u5176\u6838\u5fc3\u662f\u7528MinMaxPooling\u66ff\u4ee3\u4f20\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u5c40\u90e8\u533a\u57df\u7684\u6700\u5927\u503c\u4fe1\u606f\uff0c\u6709\u6548\u4fdd\u7559\u56fe\u50cf\u7684\u660e\u6697\u5bf9\u6bd4\u548c\u7ec6\u8282\u7279\u5f81\u3002", "result": "\u5728ACDC\u548cSynapse\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHPD\u5728\u5206\u5272\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e73\u5747DSC\u7cfb\u6570\u63d0\u9ad8\u4e860.5%\u3002", "conclusion": "HPD\u6a21\u5757\u4e3a\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u5747\u63d0\u9ad8\u4e86DSC\u7cfb\u65700.5%\u3002"}}
{"id": "2507.14797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14797", "abs": "https://arxiv.org/abs/2507.14797", "authors": ["Beier Zhu", "Ruoyu Wang", "Tong Zhao", "Hanwang Zhang", "Chi Zhang"], "title": "Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models", "comment": "To appear in ICCV 2025", "summary": "Diffusion models (DMs) have achieved state-of-the-art generative performance\nbut suffer from high sampling latency due to their sequential denoising nature.\nExisting solver-based acceleration methods often face image quality degradation\nunder a low-latency budget. In this paper, we propose the Ensemble Parallel\nDirection solver (dubbed as \\ours), a novel ODE solver that mitigates\ntruncation errors by incorporating multiple parallel gradient evaluations in\neach ODE step. Importantly, since the additional gradient computations are\nindependent, they can be fully parallelized, preserving low-latency sampling.\n  Our method optimizes a small set of learnable parameters in a distillation\nfashion, ensuring minimal training overhead.\n  In addition, our method can serve as a plugin to improve existing ODE\nsamplers. Extensive experiments on various image synthesis benchmarks\ndemonstrate the effectiveness of our \\ours~in achieving high-quality and\nlow-latency sampling. For example, at the same latency level of 5 NFE, EPD\nachieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26\non LSUN Bedroom, surpassing existing learning-based solvers by a significant\nmargin. Codes are available in https://github.com/BeierZhu/EPD.", "AI": {"tldr": "EPD\u662f\u4e00\u79cd\u65b0\u578bODE\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u5e76\u884c\u68af\u5ea6\u8ba1\u7b97\u51cf\u5c11\u622a\u65ad\u8bef\u5dee\uff0c\u5728\u4f4e\u5ef6\u8fdf\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u867d\u7136\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u751f\u6210\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u5176\u987a\u5e8f\u53bb\u566a\u7279\u6027\uff0c\u91c7\u6837\u5ef6\u8fdf\u8f83\u9ad8\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6c42\u89e3\u5668\u7684\u52a0\u901f\u65b9\u6cd5\u5728\u4f4e\u5ef6\u8fdf\u9884\u7b97\u4e0b\u5f80\u5f80\u9762\u4e34\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEPD\u7684\u65b0\u578bODE\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u5728\u6bcf\u4e2aODE\u6b65\u9aa4\u4e2d\u5f15\u5165\u591a\u4e2a\u5e76\u884c\u68af\u5ea6\u8bc4\u4f30\u6765\u51cf\u5c11\u622a\u65ad\u8bef\u5dee\uff0c\u5e76\u4e14\u8fd9\u4e9b\u989d\u5916\u7684\u68af\u5ea6\u8ba1\u7b97\u53ef\u4ee5\u5b8c\u5168\u5e76\u884c\u5316\uff0c\u4ece\u800c\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u91c7\u6837\u3002\u65b9\u6cd5\u901a\u8fc7\u84b8\u998f\u65b9\u5f0f\u4f18\u5316\u5c11\u91cf\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u786e\u4fdd\u8bad\u7ec3\u5f00\u9500\u6700\u5c0f\u5316\u3002", "result": "\u5728\u591a\u79cd\u56fe\u50cf\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEPD\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u91c7\u6837\u3002\u4f8b\u5982\uff0c\u57285 NFE\u7684\u76f8\u540c\u5ef6\u8fdf\u6c34\u5e73\u4e0b\uff0cEPD\u5728CIFAR-10\u4e0a\u5b9e\u73b0\u4e864.47\u7684FID\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EPD\uff08Ensemble Parallel Direction solver\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u578bODE\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u5e76\u884c\u68af\u5ea6\u8ba1\u7b97\u6709\u6548\u51cf\u5c11\u4e86\u622a\u65ad\u8bef\u5dee\uff0c\u5e76\u5728\u4f4e\u5ef6\u8fdf\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u6c42\u89e3\u5668\u3002"}}
{"id": "2507.15269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15269", "abs": "https://arxiv.org/abs/2507.15269", "authors": ["Fangqiu Yi", "Jingyu Xu", "Jiawei Shao", "Chi Zhang", "Xuelong Li"], "title": "Conditional Video Generation for High-Efficiency Video Compression", "comment": null, "summary": "Perceptual studies demonstrate that conditional diffusion models excel at\nreconstructing video content aligned with human visual perception. Building on\nthis insight, we propose a video compression framework that leverages\nconditional diffusion models for perceptually optimized reconstruction.\nSpecifically, we reframe video compression as a conditional generation task,\nwhere a generative model synthesizes video from sparse, yet informative\nsignals. Our approach introduces three key modules: (1) Multi-granular\nconditioning that captures both static scene structure and dynamic\nspatio-temporal cues; (2) Compact representations designed for efficient\ntransmission without sacrificing semantic richness; (3) Multi-condition\ntraining with modality dropout and role-aware embeddings, which prevent\nover-reliance on any single modality and enhance robustness. Extensive\nexperiments show that our method significantly outperforms both traditional and\nneural codecs on perceptual quality metrics such as Fr\\'echet Video Distance\n(FVD) and LPIPS, especially under high compression ratios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u6761\u4ef6\u548c\u591a\u6761\u4ef6\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u538b\u7f29\u6bd4\u4e0b\u7684\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u5185\u5bb9\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bba\u6587\u65e8\u5728\u5229\u7528\u8fd9\u4e00\u4f18\u52bf\u6784\u5efa\u611f\u77e5\u4f18\u5316\u7684\u89c6\u9891\u538b\u7f29\u6846\u67b6\u3002", "method": "1. \u591a\u7c92\u5ea6\u6761\u4ef6\u6355\u6349\u9759\u6001\u573a\u666f\u7ed3\u6784\u548c\u52a8\u6001\u65f6\u7a7a\u7ebf\u7d22\uff1b2. \u8bbe\u8ba1\u9ad8\u6548\u4f20\u8f93\u7684\u7d27\u51d1\u8868\u793a\uff1b3. \u591a\u6761\u4ef6\u8bad\u7ec3\u7ed3\u5408\u6a21\u6001\u4e22\u5f03\u548c\u89d2\u8272\u611f\u77e5\u5d4c\u5165\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728FVD\u548cLPIPS\u7b49\u611f\u77e5\u8d28\u91cf\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u548c\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u538b\u7f29\u6846\u67b6\u5728\u611f\u77e5\u8d28\u91cf\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u548c\u795e\u7ecf\u7f16\u89e3\u7801\u5668\uff0c\u7279\u522b\u662f\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.14798", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14798", "abs": "https://arxiv.org/abs/2507.14798", "authors": ["Xinyi Wu", "Steven Landgraf", "Markus Ulrich", "Rongjun Qin"], "title": "An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks", "comment": "23 pages, 6 figures, this manuscript has been submitted to\n  Geo-spatial Information Science for consideration", "summary": "State-of-the-art 3D computer vision algorithms continue to advance in\nhandling sparse, unordered image sets. Recently developed foundational models\nfor 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction\n(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry\nGrounded Transformer (VGGT), have attracted attention due to their ability to\nhandle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical\naerial images matters, as these models may handle extremely low image overlaps,\nstereo occlusions, and textureless regions. For redundant collections, they can\naccelerate 3D reconstruction by using extremely sparsified image sets. Despite\ntests on various computer vision benchmarks, their potential on photogrammetric\naerial blocks remains unexplored. This paper conducts a comprehensive\nevaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of\nthe UseGeo dataset for pose estimation and dense 3D reconstruction. Results\nshow these methods can accurately reconstruct dense point clouds from very\nsparse image sets (fewer than 10 images, up to 518 pixels resolution), with\ncompleteness gains up to +50% over COLMAP. VGGT also demonstrates higher\ncomputational efficiency, scalability, and more reliable camera pose\nestimation. However, all exhibit limitations with high-resolution images and\nlarge sets, as pose reliability declines with more images and geometric\ncomplexity. These findings suggest transformer-based methods cannot fully\nreplace traditional SfM and MVS, but offer promise as complementary approaches,\nespecially in challenging, low-resolution, and sparse scenarios.", "AI": {"tldr": "DUSt3R/MASt3R/VGGT\u5728\u7a00\u758f\u822a\u62cd\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5b8c\u6574\u6027\u63d0\u534750%\uff0c\u4f46\u9ad8\u5206\u8fa8\u7387\u548c\u5927\u56fe\u50cf\u96c6\u65f6\u59ff\u6001\u4f30\u8ba1\u53ef\u9760\u6027\u4e0b\u964d\uff0c\u9002\u5408\u4f5c\u4e3a\u4f20\u7edf\u65b9\u6cd5\u7684\u8865\u5145\u3002", "motivation": "\u8bc4\u4f30\u8fd9\u4e9b\u5148\u8fdb\u6a21\u578b\u5728\u822a\u62cd\u56fe\u50cf\u4e0a\u7684\u6f5c\u5728\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u6781\u4f4e\u56fe\u50cf\u91cd\u53e0\u3001\u7acb\u4f53\u906e\u6321\u548c\u65e0\u7eb9\u7406\u533a\u57df\u65f6\u7684\u8868\u73b0\u3002", "method": "\u672c\u7814\u7a76\u5bf9\u9884\u8bad\u7ec3\u7684DUSt3R/MASt3R/VGGT\u6a21\u578b\u5728UseGeo\u6570\u636e\u96c6\u7684\u822a\u62cd\u5757\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u91cd\u70b9\u5173\u6ce8\u59ff\u6001\u4f30\u8ba1\u548c\u5bc6\u96c63D\u91cd\u5efa\u3002", "result": "\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u4ece\u6781\u7a00\u758f\u7684\u56fe\u50cf\u96c6\uff08\u5c11\u4e8e10\u5f20\u56fe\u50cf\uff0c\u5206\u8fa8\u7387\u9ad8\u8fbe518\u50cf\u7d20\uff09\u4e2d\u51c6\u786e\u91cd\u5efa\u5bc6\u96c6\u70b9\u4e91\uff0c\u5b8c\u6574\u6027\u6bd4COLMAP\u63d0\u9ad8\u591a\u8fbe50%\u3002VGGT\u8fd8\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u66f4\u53ef\u9760\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3002", "conclusion": "\u5c3d\u7ba1\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff08\u5982DUSt3R/MASt3R/VGGT\uff09\u5728\u7a00\u758f\u56fe\u50cf\u96c6\u548c\u9ad8\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u65e0\u6cd5\u5b8c\u5168\u53d6\u4ee3\u4f20\u7edf\u7684SfM\u548cMVS\u65b9\u6cd5\uff0c\u66f4\u9002\u5408\u4f5c\u4e3a\u8865\u5145\u5de5\u5177\u5e94\u7528\u4e8e\u4f4e\u5206\u8fa8\u7387\u548c\u7a00\u758f\u573a\u666f\u3002"}}
{"id": "2507.14801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14801", "abs": "https://arxiv.org/abs/2507.14801", "authors": ["Xiangyu Chen", "Kaiwen Zhu", "Yuandong Pu", "Shuo Cao", "Xiaohui Li", "Wenlong Zhang", "Yihao Liu", "Yu Qiao", "Jiantao Zhou", "Chao Dong"], "title": "Exploring Scalable Unified Modeling for General Low-Level Vision", "comment": null, "summary": "Low-level vision involves a wide spectrum of tasks, including image\nrestoration, enhancement, stylization, and feature extraction, which differ\nsignificantly in both task formulation and output domains. To address the\nchallenge of unified modeling across such diverse tasks, we propose a Visual\ntask Prompt-based Image Processing (VPIP) framework that leverages input-target\nimage pairs as visual prompts to guide the model in performing a variety of\nlow-level vision tasks. The framework comprises an end-to-end image processing\nbackbone, a prompt encoder, and a prompt interaction module, enabling flexible\nintegration with various architectures and effective utilization of\ntask-specific visual representations. Based on this design, we develop a\nunified low-level vision model, GenLV, and evaluate its performance across\nmultiple representative tasks. To explore the scalability of this approach, we\nextend the framework along two dimensions: model capacity and task diversity.\nWe construct a large-scale benchmark consisting of over 100 low-level vision\ntasks and train multiple versions of the model with varying scales.\nExperimental results show that the proposed method achieves considerable\nperformance across a wide range of tasks. Notably, increasing the number of\ntraining tasks enhances generalization, particularly for tasks with limited\ndata, indicating the model's ability to learn transferable representations\nthrough joint training. Further evaluations in zero-shot generalization,\nfew-shot transfer, and task-specific fine-tuning scenarios demonstrate the\nmodel's strong adaptability, confirming the effectiveness, scalability, and\npotential of the proposed framework as a unified foundation for general\nlow-level vision modeling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVPIP\u6846\u67b6\u548cGenLV\u6a21\u578b\uff0c\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u7edf\u4e00\u5efa\u6a21\u591a\u6837\u4f4e\u7ea7\u522b\u89c6\u89c9\u4efb\u52a1\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5c55\u793a\u51fa\u826f\u597d\u7684\u6cdb\u5316\u548c\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f4e\u7ea7\u522b\u89c6\u89c9\u4efb\u52a1\u4e2d\u4efb\u52a1\u8868\u8ff0\u548c\u8f93\u51fa\u57df\u5dee\u5f02\u5927\u7684\u6311\u6218\uff0c\u4f5c\u8005\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u7edf\u4e00\u5efa\u6a21\u591a\u79cd\u4efb\u52a1\u7684\u6846\u67b6\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u63d0\u793a\u7684\u56fe\u50cf\u5904\u7406\u6846\u67b6VPIP\uff0c\u5305\u62ec\u7aef\u5230\u7aef\u7684\u56fe\u50cf\u5904\u7406\u4e3b\u5e72\u3001\u63d0\u793a\u7f16\u7801\u5668\u548c\u63d0\u793a\u4ea4\u4e92\u6a21\u5757\uff0c\u5e76\u5f00\u53d1\u4e86\u7edf\u4e00\u4f4e\u7ea7\u522b\u89c6\u89c9\u6a21\u578bGenLV\u3002\u901a\u8fc7\u6269\u5c55\u6a21\u578b\u5bb9\u91cf\u548c\u4efb\u52a1\u591a\u6837\u6027\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b100\u591a\u4e2a\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\uff0c\u589e\u52a0\u8bad\u7ec3\u4efb\u52a1\u6570\u91cf\u589e\u5f3a\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u4efb\u52a1\u4e2d\u3002\u6a21\u578b\u5728\u96f6\u6837\u672c\u6cdb\u5316\u3001\u5c11\u6837\u672c\u8fc1\u79fb\u548c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u4e2d\u8868\u73b0\u51fa\u5f3a\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684VPIP\u6846\u67b6\u548cGenLV\u6a21\u578b\u5728\u591a\u6837\u5316\u7684\u4f4e\u7ea7\u522b\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u73b0\u4e86\u5176\u4f5c\u4e3a\u7edf\u4e00\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u6cdb\u5316\u3001\u5c11\u6837\u672c\u8fc1\u79fb\u548c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u573a\u666f\u4e2d\u7684\u5f3a\u5927\u9002\u5e94\u6027\u3002"}}
{"id": "2507.15335", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15335", "abs": "https://arxiv.org/abs/2507.15335", "authors": ["Muhammad Aqeel", "Federico Leonardi", "Francesco Setti"], "title": "ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis", "comment": "Accepted to ICIAP 2025", "summary": "Industrial defect detection systems face critical limitations when confined\nto one-class anomaly detection paradigms, which assume uniform outlier\ndistributions and struggle with data scarcity in realworld manufacturing\nenvironments. We present ExDD (Explicit Dual Distribution), a novel framework\nthat transcends these limitations by explicitly modeling dual feature\ndistributions. Our approach leverages parallel memory banks that capture the\ndistinct statistical properties of both normality and anomalous patterns,\naddressing the fundamental flaw of uniform outlier assumptions. To overcome\ndata scarcity, we employ latent diffusion models with domain-specific textual\nconditioning, generating in-distribution synthetic defects that preserve\nindustrial context. Our neighborhood-aware ratio scoring mechanism elegantly\nfuses complementary distance metrics, amplifying signals in regions exhibiting\nboth deviation from normality and similarity to known defect patterns.\nExperimental validation on KSDD2 demonstrates superior performance (94.2%\nI-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.", "AI": {"tldr": "ExDD\u6846\u67b6\u901a\u8fc7\u53cc\u91cd\u7279\u5f81\u5206\u5e03\u5efa\u6a21\u548c\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u7cfb\u7edf\u5728\u5355\u7c7b\u5f02\u5e38\u68c0\u6d4b\u8303\u5f0f\u4e0b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u7f3a\u548c\u975e\u5747\u5300\u5f02\u5e38\u5206\u5e03\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u5e76\u884c\u8bb0\u5fc6\u5e93\u6355\u83b7\u6b63\u5e38\u548c\u5f02\u5e38\u6a21\u5f0f\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u9886\u57df\u7279\u5b9a\u7684\u5408\u6210\u7f3a\u9677\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u90bb\u57df\u611f\u77e5\u6bd4\u7387\u8bc4\u5206\u673a\u5236\u878d\u5408\u4e92\u8865\u8ddd\u79bb\u5ea6\u91cf\u3002", "result": "\u5728KSDD2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0cExDD\u5b9e\u73b0\u4e8694.2%\u7684I-AUROC\u548c97.7%\u7684P-AUROC\uff0c\u6700\u4f73\u589e\u5f3a\u6548\u679c\u5728100\u4e2a\u5408\u6210\u6837\u672c\u65f6\u8fbe\u5230\u3002", "conclusion": "ExDD\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u53cc\u91cd\u7279\u5f81\u5206\u5e03\uff0c\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edf\u5355\u7c7b\u5f02\u5e38\u68c0\u6d4b\u8303\u5f0f\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u5de5\u4e1a\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.15428", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15428", "abs": "https://arxiv.org/abs/2507.15428", "authors": ["Jiaao Li", "Kaiyuan Li", "Chen Gao", "Yong Li", "Xinlei Chen"], "title": "EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent", "comment": null, "summary": "Egomotion videos are first-person recordings where the view changes\ncontinuously due to the agent's movement. As they serve as the primary visual\ninput for embodied AI agents, making egomotion video reasoning more efficient\nis therefore essential for real-world deployment. Recent advances in\nvision-language models have enabled strong multimodal reasoning capabilities,\nbut their computational cost remains prohibitive for long, redundant video\ninputs. Existing token pruning methods, typically designed for third-person\nvideos, fail to leverage the spatiotemporal continuity and motion constraints\ninherent in egomotion settings. To address this, we propose EgoPrune, a\ntraining-free token pruning method tailored for egomotion video reasoning.\nEgoPrune comprises three components: a keyframe selector adapted from EmbodiedR\nfor temporally efficient sampling; Perspective-Aware Redundancy Filtering\n(PARF), which aligns visual tokens using perspective transformations and\nremoves redundant tokens; and a Maximal Marginal Relevance (MMR)-based token\nselector that jointly considers visual-text relevance and intra-frame\ndiversity. Experiments on two egomotion video benchmarks show that EgoPrune\nconsistently outperforms prior training-free methods across various pruning\nratios while significantly reducing FLOPs, memory usage, and latency. Moreover,\nwe deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB\nedge device, demonstrating its real-world efficiency and suitability for\non-device egomotion video reasoning.", "AI": {"tldr": "EgoPrune\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4ee4\u724c\u4fee\u526a\u65b9\u6cd5\uff0c\u4e13\u4e3a\u81ea\u6211\u8fd0\u52a8\u89c6\u9891\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u5e76\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u7531\u4e8e\u81ea\u6211\u8fd0\u52a8\u89c6\u9891\u5728\u4f53\u73b0AI\u4ee3\u7406\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5229\u7528\u5176\u65f6\u7a7a\u8fde\u7eed\u6027\u548c\u8fd0\u52a8\u7ea6\u675f\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "method": "EgoPrune\u5305\u62ec\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u5173\u952e\u5e27\u9009\u62e9\u5668\u3001\u89c6\u89d2\u611f\u77e5\u5197\u4f59\u8fc7\u6ee4\u5668\u548c\u57fa\u4e8e\u6700\u5927\u8fb9\u9645\u76f8\u5173\u6027\u7684\u4ee4\u724c\u9009\u62e9\u5668\u3002", "result": "\u5728\u4e24\u79cd\u81ea\u6211\u8fd0\u52a8\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEgoPrune\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "EgoPrune\u662f\u4e00\u79cd\u4e13\u4e3a\u81ea\u6211\u8fd0\u52a8\u89c6\u9891\u63a8\u7406\u8bbe\u8ba1\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u4ee4\u724c\u4fee\u526a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u90e8\u7f72\u3002"}}
{"id": "2507.14823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14823", "abs": "https://arxiv.org/abs/2507.14823", "authors": ["Dong Shu", "Haoyang Yuan", "Yuchen Wang", "Yanguang Liu", "Huopu Zhang", "Haiyan Zhao", "Mengnan Du"], "title": "FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models", "comment": "20 Pages, 18 Figures", "summary": "Large vision-language models (LVLMs) have made significant progress in chart\nunderstanding. However, financial charts, characterized by complex temporal\nstructures and domain-specific terminology, remain notably underexplored. We\nintroduce FinChart-Bench, the first benchmark specifically focused on\nreal-world financial charts. FinChart-Bench comprises 1,200 financial chart\nimages collected from 2015 to 2024, each annotated with True/False (TF),\nMultiple Choice (MC), and Question Answering (QA) questions, totaling 7,016\nquestions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs\non FinChart-Bench. Our evaluation reveals critical insights: (1) the\nperformance gap between open-source and closed-source models is narrowing, (2)\nperformance degradation occurs in upgraded models within families, (3) many\nmodels struggle with instruction following, (4) both advanced models show\nsignificant limitations in spatial reasoning abilities, and (5) current LVLMs\nare not reliable enough to serve as automated evaluators. These findings\nhighlight important limitations in current LVLM capabilities for financial\nchart understanding. The FinChart-Bench dataset is available at\nhttps://huggingface.co/datasets/Tizzzzy/FinChart-Bench.", "AI": {"tldr": "FinChart-Bench\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u91d1\u878d\u56fe\u8868\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u53d1\u73b0\u5f53\u524dLVLM\u5728\u91d1\u878d\u56fe\u8868\u7406\u89e3\u4e0a\u5b58\u5728\u591a\u9879\u5c40\u9650\u6027\u3002", "motivation": "\u91d1\u878d\u56fe\u8868\u56e0\u590d\u6742\u7684\u65f6\u95f4\u7ed3\u6784\u548c\u9886\u57df\u7279\u5b9a\u672f\u8bed\u800c\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30LVLM\u7684\u6027\u80fd\u3002", "method": "\u5f15\u5165FinChart-Bench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b1,200\u5f20\u91d1\u878d\u56fe\u8868\u548c7,016\u4e2a\u95ee\u9898\uff0c\u5e76\u5bf925\u79cd\u6700\u5148\u8fdb\u7684LVLM\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u4e94\u4e2a\u5173\u952e\u53d1\u73b0\uff1a\u5f00\u6e90\u4e0e\u95ed\u6e90\u6a21\u578b\u6027\u80fd\u5dee\u8ddd\u7f29\u5c0f\u3001\u5347\u7ea7\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3001\u6307\u4ee4\u9075\u5faa\u56f0\u96be\u3001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6709\u9650\u3001LVLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\u7684\u53ef\u9760\u6027\u4e0d\u8db3\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u56fe\u8868\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0cFinChart-Bench\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2507.14826", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14826", "abs": "https://arxiv.org/abs/2507.14826", "authors": ["Fu-Jen Tsai", "Yan-Tsung Peng", "Yen-Yu Lin", "Chia-Wen Lin"], "title": "PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing", "comment": "ICCV 2025", "summary": "Image dehazing aims to remove unwanted hazy artifacts in images. Although\nprevious research has collected paired real-world hazy and haze-free images to\nimprove dehazing models' performance in real-world scenarios, these models\noften experience significant performance drops when handling unseen real-world\nhazy images due to limited training data. This issue motivates us to develop a\nflexible domain adaptation method to enhance dehazing performance during\ntesting. Observing that predicting haze patterns is generally easier than\nrecovering clean content, we propose the Physics-guided Haze Transfer Network\n(PHATNet) which transfers haze patterns from unseen target domains to\nsource-domain haze-free images, creating domain-specific fine-tuning sets to\nupdate dehazing models for effective domain adaptation. Additionally, we\nintroduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to\nenhance PHATNet's disentanglement ability. Experimental results demonstrate\nthat PHATNet significantly boosts state-of-the-art dehazing models on benchmark\nreal-world image dehazing datasets.", "AI": {"tldr": "PHATNet\u901a\u8fc7\u96fe\u6a21\u5f0f\u8f6c\u79fb\u548c\u7279\u5b9a\u9886\u57df\u5fae\u8c03\uff0c\u63d0\u5347\u4e86\u53bb\u96fe\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u53bb\u96fe\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u56e0\u8bad\u7ec3\u6570\u636e\u6709\u9650\uff0c\u5bf9\u672a\u89c1\u96fe\u56fe\u50cf\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\uff0c\u9700\u5f00\u53d1\u7075\u6d3b\u7684\u57df\u9002\u5e94\u65b9\u6cd5\u4ee5\u63d0\u5347\u6d4b\u8bd5\u65f6\u7684\u53bb\u96fe\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86Physics-guided Haze Transfer Network (PHATNet)\uff0c\u901a\u8fc7\u8f6c\u79fb\u672a\u89c1\u76ee\u6807\u57df\u7684\u96fe\u6a21\u5f0f\u5230\u6e90\u57df\u65e0\u96fe\u56fe\u50cf\u4e0a\uff0c\u751f\u6210\u9886\u57df\u7279\u5b9a\u7684\u5fae\u8c03\u6570\u636e\u96c6\u3002\u540c\u65f6\u5f15\u5165\u4e86Haze-Transfer-Consistency\u635f\u5931\u548cContent-Leakage\u635f\u5931\u4ee5\u589e\u5f3a\u89e3\u8026\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660ePHATNet\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u53bb\u96fe\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6700\u5148\u8fdb\u53bb\u96fe\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "PHATNet\u901a\u8fc7\u5c06\u76ee\u6807\u57df\u4e2d\u7684\u96fe\u6a21\u5f0f\u8f6c\u79fb\u5230\u6e90\u57df\u65e0\u96fe\u56fe\u50cf\u4e0a\uff0c\u521b\u5efa\u7279\u5b9a\u9886\u57df\u7684\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u53bb\u96fe\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14845", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14845", "abs": "https://arxiv.org/abs/2507.14845", "authors": ["Rizhao Fan", "Zhigen Li", "Heping Li", "Ning An"], "title": "Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image", "comment": null, "summary": "Depth completion is an important vision task, and many efforts have been made\nto enhance the quality of depth maps from sparse depth measurements. Despite\nsignificant advances, training these models to recover dense depth from sparse\nmeasurements remains a challenging problem. Supervised learning methods rely on\ndense depth labels to predict unobserved regions, while self-supervised\napproaches require image sequences to enforce geometric constraints and\nphotometric consistency between frames. However, acquiring dense annotations is\ncostly, and multi-frame dependencies limit the applicability of self-supervised\nmethods in static or single-frame scenarios. To address these challenges, we\npropose a novel self-supervised depth completion paradigm that requires only\nsparse depth measurements and their corresponding image for training. Unlike\nexisting methods, our approach eliminates the need for dense depth labels or\nadditional images captured from neighboring viewpoints. By leveraging the\ncharacteristics of depth distribution, we design novel loss functions that\neffectively propagate depth information from observed points to unobserved\nregions. Additionally, we incorporate segmentation maps generated by vision\nfoundation models to further enhance depth estimation. Extensive experiments\ndemonstrate the effectiveness of our proposed method.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u76d1\u7763\u6df1\u5ea6\u8865\u5168\u65b0\u8303\u5f0f\uff0c\u4ec5\u9700\u7a00\u758f\u6df1\u5ea6\u548c\u5355\u5f20\u56fe\u50cf\uff0c\u901a\u8fc7\u521b\u65b0\u635f\u5931\u51fd\u6570\u548c\u5206\u5272\u56fe\u589e\u5f3a\u6548\u679c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u8865\u5168\u65b9\u6cd5\u5bf9\u5bc6\u96c6\u6807\u6ce8\u6216\u591a\u5e27\u6570\u636e\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u5347\u5728\u9759\u6001\u6216\u5355\u5e27\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u81ea\u76d1\u7763\u6df1\u5ea6\u8865\u5168\u65b9\u6cd5\uff0c\u5229\u7528\u6df1\u5ea6\u5206\u5e03\u7279\u6027\u6784\u5efa\u635f\u5931\u51fd\u6570\uff0c\u5e76\u6574\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u751f\u6210\u7684\u5206\u5272\u56fe\u4ee5\u4f18\u5316\u6df1\u5ea6\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u5bc6\u96c6\u6807\u7b7e\u6216\u989d\u5916\u89c6\u89d2\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u6709\u6548\u4f20\u64ad\u6df1\u5ea6\u4fe1\u606f\u81f3\u672a\u89c2\u6d4b\u533a\u57df\uff0c\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u8865\u5168\u8d28\u91cf\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u9700\u7a00\u758f\u6df1\u5ea6\u6d4b\u91cf\u53ca\u5176\u5bf9\u5e94\u56fe\u50cf\u7684\u81ea\u76d1\u7763\u6df1\u5ea6\u8865\u5168\u8303\u5f0f\uff0c\u6709\u6548\u514b\u670d\u4e86\u4f9d\u8d56\u5bc6\u96c6\u6807\u6ce8\u6216\u591a\u5e27\u6570\u636e\u7684\u9650\u5236\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u635f\u5931\u51fd\u6570\u548c\u5206\u5272\u56fe\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u8865\u5168\u6548\u679c\u3002"}}
{"id": "2507.15577", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15577", "abs": "https://arxiv.org/abs/2507.15577", "authors": ["Hugo Carlesso", "Maria Eliza Patulea", "Moncef Garouani", "Radu Tudor Ionescu", "Josiane Mothe"], "title": "GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation", "comment": null, "summary": "Mixup has become a popular augmentation strategy for image classification,\nyet its naive pixel-wise interpolation often produces unrealistic images that\ncan hinder learning, particularly in high-stakes medical applications. We\npropose GeMix, a two-stage framework that replaces heuristic blending with a\nlearned, label-aware interpolation powered by class-conditional GANs. First, a\nStyleGAN2-ADA generator is trained on the target dataset. During augmentation,\nwe sample two label vectors from Dirichlet priors biased toward different\nclasses and blend them via a Beta-distributed coefficient. Then, we condition\nthe generator on this soft label to synthesize visually coherent images that\nlie along a continuous class manifold. We benchmark GeMix on the large-scale\nCOVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,\nEfficientNet-B0). When combined with real data, our method increases macro-F1\nover traditional mixup for all backbones, reducing the false negative rate for\nCOVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,\ndelivering stronger regularization and greater semantic fidelity, without\ndisrupting existing training pipelines. We publicly release our code at\nhttps://github.com/hugocarlesso/GeMix to foster reproducibility and further\nresearch.", "AI": {"tldr": "GeMix\u901a\u8fc7\u7c7b\u6761\u4ef6GAN\u751f\u6210\u89c6\u89c9\u8fde\u8d2f\u7684\u56fe\u50cf\uff0c\u66ff\u4ee3\u4f20\u7edfmixup\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfmixup\u7684\u50cf\u7d20\u7ea7\u63d2\u503c\u5728\u533b\u5b66\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u53ef\u80fd\u751f\u6210\u4e0d\u771f\u5b9e\u56fe\u50cf\uff0c\u963b\u788d\u5b66\u4e60\u6548\u679c\u3002", "method": "GeMix\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u9996\u5148\u8bad\u7ec3StyleGAN2-ADA\u751f\u6210\u5668\uff0c\u7136\u540e\u901a\u8fc7Dirichlet\u5148\u9a8c\u548cBeta\u5206\u5e03\u7cfb\u6570\u751f\u6210\u8f6f\u6807\u7b7e\uff0c\u5408\u6210\u8fde\u7eed\u7c7b\u6d41\u5f62\u4e0a\u7684\u56fe\u50cf\u3002", "result": "\u5728COVIDx-CT-3\u6570\u636e\u96c6\u4e0a\uff0cGeMix\u7ed3\u5408\u771f\u5b9e\u6570\u636e\u540e\uff0c\u6240\u6709\u9aa8\u5e72\u7f51\u7edc\u7684macro-F1\u5747\u4f18\u4e8e\u4f20\u7edfmixup\uff0c\u5e76\u964d\u4f4e\u4e86COVID-19\u68c0\u6d4b\u7684\u5047\u9634\u6027\u7387\u3002", "conclusion": "GeMix\u662f\u4e00\u79cd\u6709\u6548\u7684\u50cf\u7d20\u7a7a\u95f4mixup\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u7c7b\u6761\u4ef6GAN\u751f\u6210\u89c6\u89c9\u8fde\u8d2f\u7684\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86COVID-19\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u964d\u4f4e\u4e86\u5047\u9634\u6027\u7387\u3002"}}
{"id": "2507.15636", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15636", "abs": "https://arxiv.org/abs/2507.15636", "authors": ["Lisan Al Amin", "Md. Ismail Hossain", "Thanh Thi Nguyen", "Tasnim Jahan", "Mahbubul Islam", "Faisal Quader"], "title": "Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis", "comment": "Accepted for publication at the 2025 IEEE International Conference on\n  Systems, Man, and Cybernetics (SMC)", "summary": "Recent advances in deepfake technology have created increasingly convincing\nsynthetic media that poses significant challenges to information integrity and\nsocial trust. While current detection methods show promise, their underlying\nmechanisms remain poorly understood, and the large sizes of their models make\nthem challenging to deploy in resource-limited environments. This study\ninvestigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake\ndetection, aiming to identify the key features crucial for recognizing\ndeepfakes. We examine how neural networks can be efficiently pruned while\nmaintaining high detection accuracy. Through extensive experiments with\nMesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and\nFaceForensics++ datasets, we find that deepfake detection networks contain\nwinning tickets, i.e., subnetworks, that preserve performance even at\nsubstantial sparsity levels. Our results indicate that MesoNet retains 56.2%\naccuracy at 80% sparsity on the OpenForensic dataset, with only 3,000\nparameters, which is about 90% of its baseline accuracy (62.6%). The results\nalso show that our proposed LTH-based iterative magnitude pruning approach\nconsistently outperforms one-shot pruning methods. Using Grad-CAM\nvisualization, we analyze how pruned networks maintain their focus on critical\nfacial regions for deepfake detection. Additionally, we demonstrate the\ntransferability of winning tickets across datasets, suggesting potential for\nefficient, deployable deepfake detection systems.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u5f69\u7968\u5047\u8bbe\uff08LTH\uff09\u526a\u679d\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\uff0c\u53d1\u73b0\u5173\u952e\u5b50\u7f51\u7edc\u5728\u6781\u9ad8\u7a00\u758f\u5ea6\u4e0b\u4ecd\u4fdd\u6301\u6027\u80fd\uff0c\u4e14\u53ef\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\uff0c\u4e3a\u8f7b\u91cf\u5316\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u53ef\u80fd\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u8fdb\u6b65\u5bf9\u4fe1\u606f\u5b8c\u6574\u6027\u548c\u793e\u4f1a\u4fe1\u4efb\u6784\u6210\u6311\u6218\uff0c\u800c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u6a21\u578b\u5e9e\u5927\u4e14\u673a\u5236\u4e0d\u900f\u660e\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u526a\u679d\u6280\u672f\u9ad8\u6548\u8bc6\u522b\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u5173\u952e\u7279\u5f81\u3002", "method": "\u7814\u7a76\u91c7\u7528MesoNet\u3001CNN-5\u548cResNet-18\u67b6\u6784\uff0c\u5728OpenForensic\u548cFaceForensics++\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\u3002\u901a\u8fc7\u57fa\u4e8eLTH\u7684\u8fed\u4ee3\u5e45\u5ea6\u526a\u679d\u65b9\u6cd5\uff0c\u8bc6\u522b\u51fa\u5173\u952e\u5b50\u7f51\u7edc\uff0c\u5e76\u7ed3\u5408Grad-CAM\u53ef\u89c6\u5316\u5206\u6790\u526a\u679d\u540e\u7f51\u7edc\u7684\u6ce8\u610f\u529b\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMesoNet\u572880%\u7a00\u758f\u5ea6\u4e0b\u4ecd\u4fdd\u630156.2%\u7684\u51c6\u786e\u7387\uff08\u57fa\u7ebf\u4e3a62.6%\uff09\uff0c\u4ec5\u97003,000\u53c2\u6570\u3002\u57fa\u4e8eLTH\u7684\u526a\u679d\u65b9\u6cd5\u4f18\u4e8e\u4e00\u6b21\u6027\u526a\u679d\uff0c\u4e14\u5b50\u7f51\u7edc\u5728\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u53ef\u8fc1\u79fb\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u5f69\u7968\u5047\u8bbe\uff08LTH\uff09\u7684\u526a\u679d\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u5b50\u7f51\u7edc\uff08winning tickets\uff09\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u5b50\u7f51\u7edc\u5728\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u5177\u6709\u53ef\u8fc1\u79fb\u6027\uff0c\u4e3a\u9ad8\u6548\u90e8\u7f72\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2507.14855", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14855", "abs": "https://arxiv.org/abs/2507.14855", "authors": ["Xingshu Chen", "Sicheng Yu", "Chong Cheng", "Hao Wang", "Ting Tian"], "title": "An Uncertainty-aware DETR Enhancement Framework for Object Detection", "comment": null, "summary": "This paper investigates the problem of object detection with a focus on\nimproving both the localization accuracy of bounding boxes and explicitly\nmodeling prediction uncertainty. Conventional detectors rely on deterministic\nbounding box regression, ignoring uncertainty in predictions and limiting model\nrobustness. In this paper, we propose an uncertainty-aware enhancement\nframework for DETR-based object detectors. We model bounding boxes as\nmultivariate Gaussian distributions and incorporate the Gromov-Wasserstein\ndistance into the loss function to better align the predicted and ground-truth\ndistributions. Building on this, we derive a Bayes Risk formulation to filter\nhigh-risk information and improve detection reliability. We also propose a\nsimple algorithm to quantify localization uncertainty via confidence intervals.\nExperiments on the COCO benchmark show that our method can be effectively\nintegrated into existing DETR variants, enhancing their performance. We further\nextend our framework to leukocyte detection tasks, achieving state-of-the-art\nresults on the LISC and WBCDD datasets. These results confirm the scalability\nof our framework across both general and domain-specific detection tasks. Code\npage:\nhttps://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684DETR\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u8fb9\u754c\u6846\u4e3a\u9ad8\u65af\u5206\u5e03\u5e76\u5f15\u5165\u65b0\u635f\u5931\u51fd\u6570\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u68c0\u6d4b\u5668\u4f9d\u8d56\u786e\u5b9a\u6027\u8fb9\u754c\u6846\u56de\u5f52\uff0c\u5ffd\u7565\u4e86\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDETR\u7684\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8fb9\u754c\u6846\u5efa\u6a21\u4e3a\u591a\u5143\u9ad8\u65af\u5206\u5e03\uff0c\u5e76\u5728\u635f\u5931\u51fd\u6570\u4e2d\u5f15\u5165Gromov-Wasserstein\u8ddd\u79bb\uff0c\u540c\u65f6\u5229\u7528\u8d1d\u53f6\u65af\u98ce\u9669\u8fc7\u6ee4\u9ad8\u98ce\u9669\u4fe1\u606f\u3002", "result": "\u5728COCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5728\u767d\u7ec6\u80de\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff08LISC\u548cWBCDD\u6570\u636e\u96c6\uff09\u8fbe\u5230\u4e86\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548cGromov-Wasserstein\u8ddd\u79bb\u7684\u5f15\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86DETR\u7c7b\u68c0\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u5e76\u5728\u901a\u7528\u548c\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u5747\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6269\u5c55\u6027\u3002"}}
{"id": "2507.14867", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14867", "abs": "https://arxiv.org/abs/2507.14867", "authors": ["Zhaoqiang Xia", "Hexiang Huang", "Haoyu Chen", "Xiaoyi Feng", "Guoying Zhao"], "title": "Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition", "comment": null, "summary": "Micro-gestures are unconsciously performed body gestures that can convey the\nemotion states of humans and start to attract more research attention in the\nfields of human behavior understanding and affective computing as an emerging\ntopic. However, the modeling of human emotion based on micro-gestures has not\nbeen explored sufficiently. In this work, we propose to recognize the emotion\nstates based on the micro-gestures by reconstructing the behavior patterns with\na hypergraph-enhanced Transformer in a hybrid-supervised framework. In the\nframework, hypergraph Transformer based encoder and decoder are separately\ndesigned by stacking the hypergraph-enhanced self-attention and multiscale\ntemporal convolution modules. Especially, to better capture the subtle motion\nof micro-gestures, we construct a decoder with additional upsampling operations\nfor a reconstruction task in a self-supervised learning manner. We further\npropose a hypergraph-enhanced self-attention module where the hyperedges\nbetween skeleton joints are gradually updated to present the relationships of\nbody joints for modeling the subtle local motion. Lastly, for exploiting the\nrelationship between the emotion states and local motion of micro-gestures, an\nemotion recognition head from the output of encoder is designed with a shallow\narchitecture and learned in a supervised way. The end-to-end framework is\njointly trained in a one-stage way by comprehensively utilizing\nself-reconstruction and supervision information. The proposed method is\nevaluated on two publicly available datasets, namely iMiGUE and SMG, and\nachieves the best performance under multiple metrics, which is superior to the\nexisting methods.", "AI": {"tldr": "\u63d0\u51fa\u8d85\u56fe\u589e\u5f3aTransformer\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u76d1\u7763\u5b66\u4e60\u8bc6\u522b\u5fae\u624b\u52bf\u60c5\u611f\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5fae\u624b\u52bf\u4f5c\u4e3a\u65e0\u610f\u8bc6\u7684\u8eab\u4f53\u52a8\u4f5c\u80fd\u4f20\u8fbe\u4eba\u7c7b\u60c5\u611f\u72b6\u6001\uff0c\u4f46\u57fa\u4e8e\u5fae\u624b\u52bf\u7684\u60c5\u611f\u5efa\u6a21\u7814\u7a76\u4e0d\u8db3\uff0c\u56e0\u6b64\u63a2\u7d22\u5176\u8bc6\u522b\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u8d85\u56fe\u589e\u5f3a\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u5377\u79ef\u6a21\u5757\u6784\u5efa\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u5f0f\u91cd\u6784\u4efb\u52a1\uff0c\u5e76\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u8fdb\u884c\u60c5\u611f\u8bc6\u522b\u3002", "result": "\u5728iMiGUE\u548cSMG\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u8d85\u56fe\u589e\u5f3aTransformer\u7684\u6df7\u5408\u76d1\u7763\u6846\u67b6\u5728iMiGUE\u548cSMG\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.15686", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15686", "abs": "https://arxiv.org/abs/2507.15686", "authors": ["Wenjie Huang", "Qi Yang", "Shuting Xia", "He Huang", "Zhu Li", "Yiling Xu"], "title": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression", "comment": "Accepted to ICCV 2025", "summary": "Existing AI-based point cloud compression methods struggle with dependence on\nspecific training data distributions, which limits their real-world deployment.\nImplicit Neural Representation (INR) methods solve the above problem by\nencoding overfitted network parameters to the bitstream, resulting in more\ndistribution-agnostic results. However, due to the limitation of encoding time\nand decoder size, current INR based methods only consider lossy geometry\ncompression. In this paper, we propose the first INR based lossless point cloud\ngeometry compression method called Lossless Implicit Neural Representations for\nPoint Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we\ndesign a group of point clouds level coding framework with an effective network\ninitialization strategy, which can reduce around 60% encoding time. A\nlightweight coding network based on multiscale SparseConv, consisting of scale\ncontext extraction, child node prediction, and model compression modules, is\nproposed to realize fast inference and compact decoder size. Experimental\nresults show that our method consistently outperforms traditional and AI-based\nmethods: for example, with the convergence time in the MVUB dataset, our method\nreduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and\n21.95% compared to SparsePCGC. Our project can be seen on\nhttps://huangwenjie2023.github.io/LINR-PCGC/.", "AI": {"tldr": "LINR-PCGC\u662f\u9996\u4e2a\u57fa\u4e8eINR\u7684\u65e0\u635f\u70b9\u4e91\u51e0\u4f55\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u7f16\u7801\u901f\u5ea6\u548c\u8f7b\u91cf\u7ea7\u7f51\u7edc\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eAI\u7684\u70b9\u4e91\u538b\u7f29\u65b9\u6cd5\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff0c\u800c\u57fa\u4e8eINR\u7684\u65b9\u6cd5\u867d\u7136\u89e3\u51b3\u4e86\u5206\u5e03\u4f9d\u8d56\u95ee\u9898\uff0c\u4f46\u7f16\u7801\u65f6\u95f4\u548c\u89e3\u7801\u5668\u5927\u5c0f\u7684\u9650\u5236\u4f7f\u5176\u4ec5\u9002\u7528\u4e8e\u6709\u635f\u51e0\u4f55\u538b\u7f29\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u70b9\u4e91\u7ea7\u522b\u7684\u7f16\u7801\u6846\u67b6\u548c\u6709\u6548\u7684\u7f51\u7edc\u521d\u59cb\u5316\u7b56\u7565\uff0c\u51cf\u5c11\u4e8660%\u7684\u7f16\u7801\u65f6\u95f4\uff1b\u63d0\u51fa\u57fa\u4e8e\u591a\u5c3a\u5ea6\u7a00\u758f\u5377\u79ef\u7684\u8f7b\u91cf\u7ea7\u7f16\u7801\u7f51\u7edc\uff0c\u5305\u62ec\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u63d0\u53d6\u3001\u5b50\u8282\u70b9\u9884\u6d4b\u548c\u6a21\u578b\u538b\u7f29\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLINR-PCGC\u5728MVUB\u6570\u636e\u96c6\u4e0a\u6bd4G-PCC TMC13v23\u51cf\u5c11\u4e8621.21%\u7684\u6bd4\u7279\u6d41\uff0c\u6bd4SparsePCGC\u51cf\u5c11\u4e8621.95%\u3002", "conclusion": "LINR-PCGC \u662f\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7684\u65e0\u635f\u70b9\u4e91\u51e0\u4f55\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a0\u901f\u7f16\u7801\u901f\u5ea6\u548c\u8f7b\u91cf\u7ea7\u7f51\u7edc\u8bbe\u8ba1\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u548c\u57fa\u4e8eAI\u7684\u538b\u7f29\u65b9\u6cd5\u3002"}}
{"id": "2507.14879", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14879", "abs": "https://arxiv.org/abs/2507.14879", "authors": ["Rizhao Fan", "Tianfang Ma", "Zhigen Li", "Ning An", "Jian Cheng"], "title": "Region-aware Depth Scale Adaptation with Sparse Measurements", "comment": null, "summary": "In recent years, the emergence of foundation models for depth prediction has\nled to remarkable progress, particularly in zero-shot monocular depth\nestimation. These models generate impressive depth predictions; however, their\noutputs are often in relative scale rather than metric scale. This limitation\nposes challenges for direct deployment in real-world applications. To address\nthis, several scale adaptation methods have been proposed to enable foundation\nmodels to produce metric depth. However, these methods are typically costly, as\nthey require additional training on new domains and datasets. Moreover,\nfine-tuning these models often compromises their original generalization\ncapabilities, limiting their adaptability across diverse scenes. In this paper,\nwe introduce a non-learning-based approach that leverages sparse depth\nmeasurements to adapt the relative-scale predictions of foundation models into\nmetric-scale depth. Our method requires neither retraining nor fine-tuning,\nthereby preserving the strong generalization ability of the original foundation\nmodels while enabling them to produce metric depth. Experimental results\ndemonstrate the effectiveness of our approach, high-lighting its potential to\nbridge the gap between relative and metric depth without incurring additional\ncomputational costs or sacrificing generalization ability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u975e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u6df1\u5ea6\u6d4b\u91cf\u5c06\u57fa\u7840\u6a21\u578b\u7684\u76f8\u5bf9\u6df1\u5ea6\u9884\u6d4b\u8f6c\u6362\u4e3a\u5ea6\u91cf\u6df1\u5ea6\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u5728\u96f6\u6837\u672c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u8f93\u51fa\u4e3a\u76f8\u5bf9\u5c3a\u5ea6\u800c\u975e\u5ea6\u91cf\u5c3a\u5ea6\u7684\u95ee\u9898\uff0c\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u56e0\u989d\u5916\u8bad\u7ec3\u6216\u5fae\u8c03\u5bfc\u81f4\u7684\u6210\u672c\u548c\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3002", "method": "\u975e\u5b66\u4e60\u57fa\u7840\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u6df1\u5ea6\u6d4b\u91cf\u6765\u8c03\u6574\u57fa\u7840\u6a21\u578b\u7684\u76f8\u5bf9\u5c3a\u5ea6\u9884\u6d4b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e86\u76f8\u5bf9\u5c3a\u5ea6\u5230\u5ea6\u91cf\u5c3a\u5ea6\u7684\u8f6c\u6362\uff0c\u4e14\u672a\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u6216\u727a\u7272\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u6df1\u5ea6\u6d4b\u91cf\u5c06\u57fa\u7840\u6a21\u578b\u7684\u76f8\u5bf9\u5c3a\u5ea6\u9884\u6d4b\u8f6c\u6362\u4e3a\u5ea6\u91cf\u5c3a\u5ea6\u6df1\u5ea6\uff0c\u65e2\u4fdd\u7559\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u53c8\u5b9e\u73b0\u4e86\u5ea6\u91cf\u6df1\u5ea6\u8f93\u51fa\u3002"}}
{"id": "2507.15803", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15803", "abs": "https://arxiv.org/abs/2507.15803", "authors": ["Danhui Chen", "Ziquan Liu", "Chuxi Yang", "Dan Wang", "Yan Yan", "Yi Xu", "Xiangyang Ji"], "title": "ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction", "comment": "ICCV 2025", "summary": "Pixel-level vision tasks, such as semantic segmentation, require extensive\nand high-quality annotated data, which is costly to obtain. Semi-supervised\nsemantic segmentation (SSSS) has emerged as a solution to alleviate the\nlabeling burden by leveraging both labeled and unlabeled data through\nself-training techniques. Meanwhile, the advent of foundational segmentation\nmodels pre-trained on massive data, has shown the potential to generalize\nacross domains effectively. This work explores whether a foundational\nsegmentation model can address label scarcity in the pixel-level vision task as\nan annotator for unlabeled images. Specifically, we investigate the efficacy of\nusing SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual\ninput, to generate predictive masks for unlabeled data. To address the\nshortcomings of using SEEM-generated masks as supervision, we propose\nConformalSAM, a novel SSSS framework which first calibrates the foundation\nmodel using the target domain's labeled data and then filters out unreliable\npixel labels of unlabeled data so that only high-confidence labels are used as\nsupervision. By leveraging conformal prediction (CP) to adapt foundation models\nto target data through uncertainty calibration, ConformalSAM exploits the\nstrong capability of the foundational segmentation model reliably which\nbenefits the early-stage learning, while a subsequent self-reliance training\nstrategy mitigates overfitting to SEEM-generated masks in the later training\nstage. Our experiment demonstrates that, on three standard benchmarks of SSSS,\nConformalSAM achieves superior performance compared to recent SSSS methods and\nhelps boost the performance of those methods as a plug-in.", "AI": {"tldr": "ConformalSAM\u5229\u7528\u57fa\u7840\u5206\u5272\u6a21\u578b\u548c\u4e00\u81f4\u6027\u9884\u6d4b\uff0c\u5728\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u4e2d\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u50cf\u7d20\u7ea7\u89c6\u89c9\u4efb\u52a1\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u57fa\u7840\u5206\u5272\u6a21\u578b\u4f5c\u4e3a\u6807\u6ce8\u5de5\u5177\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86ConformalSAM\u6846\u67b6\uff0c\u5305\u62ec\u5229\u7528\u76ee\u6807\u57df\u6807\u6ce8\u6570\u636e\u6821\u51c6\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4e00\u81f4\u6027\u9884\u6d4b\u8fc7\u6ee4\u4e0d\u53ef\u9760\u7684\u50cf\u7d20\u6807\u7b7e\uff0c\u7ed3\u5408\u81ea\u4f9d\u8d56\u8bad\u7ec3\u7b56\u7565\u907f\u514d\u8fc7\u62df\u5408\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6SSSS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cConformalSAM\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u4f5c\u4e3a\u63d2\u4ef6\u63d0\u5347\u5176\u4ed6\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "ConformalSAM\u901a\u8fc7\u7ed3\u5408\u57fa\u7840\u5206\u5272\u6a21\u578b\u548c\u81ea\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u81ea\u8eab\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u63d2\u4ef6\u63d0\u5347\u5176\u4ed6\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14885", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14885", "abs": "https://arxiv.org/abs/2507.14885", "authors": ["Joaquim Comas", "Federico Sukno"], "title": "BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters", "comment": null, "summary": "Remote photoplethysmography (rPPG) captures cardiac signals from facial\nvideos and is gaining attention for its diverse applications. While deep\nlearning has advanced rPPG estimation, it relies on large, diverse datasets for\neffective generalization. In contrast, handcrafted methods utilize\nphysiological priors for better generalization in unseen scenarios like motion\nwhile maintaining computational efficiency. However, their linear assumptions\nlimit performance in complex conditions, where deep learning provides superior\npulsatile information extraction. This highlights the need for hybrid\napproaches that combine the strengths of both methods. To address this, we\npresent BeatFormer, a lightweight spectral attention model for rPPG estimation,\nwhich integrates zoomed orthonormal complex attention and frequency-domain\nenergy measurement, enabling a highly efficient model. Additionally, we\nintroduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be\ntrained without any PPG or HR labels. We validate BeatFormer on the PURE,\nUBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,\nparticularly in cross-dataset evaluations under motion scenarios.", "AI": {"tldr": "BeatFormer\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8c31\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u7ed3\u5408\u624b\u5de5\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u4f18\u52bf\uff0c\u65e0\u9700\u6807\u7b7e\u5373\u53ef\u8bad\u7ec3\uff0c\u5728\u8de8\u6570\u636e\u96c6\u8fd0\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u6570\u636e\u96c6\uff0c\u624b\u5de5\u65b9\u6cd5\u5728\u672a\u89c1\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u5f3a\u4f46\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86BeatFormer\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8c31\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u653e\u5927\u7684\u6b63\u4ea4\u590d\u6ce8\u610f\u529b\u548c\u9891\u57df\u80fd\u91cf\u6d4b\u91cf\uff0c\u4ee5\u53ca\u8c31\u5bf9\u6bd4\u5b66\u4e60\uff08SCL\uff09\u65b9\u6cd5\uff0c\u65e0\u9700PPG\u6216HR\u6807\u7b7e\u5373\u53ef\u8bad\u7ec3\u3002", "result": "\u5728PURE\u3001UBFC-rPPG\u548cMMPD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86BeatFormer\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8fd0\u52a8\u573a\u666f\u7684\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "BeatFormer\u7ed3\u5408\u4e86\u624b\u5de5\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8c31\u6ce8\u610f\u529b\u6a21\u578b\u548c\u8c31\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u65e0\u9700PPG\u6216HR\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684rPPG\u4f30\u8ba1\uff0c\u5e76\u5728\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.15807", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15807", "abs": "https://arxiv.org/abs/2507.15807", "authors": ["Shuo Chen", "Jianzhe Liu", "Zhen Han", "Yan Xia", "Daniel Cremers", "Philip Torr", "Volker Tresp", "Jindong Gu"], "title": "True Multimodal In-Context Learning Needs Attention to the Visual Context", "comment": "accepted to COLM 2025", "summary": "Multimodal Large Language Models (MLLMs), built on powerful language\nbackbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new\ntasks from a few multimodal demonstrations consisting of images, questions, and\nanswers. Despite showing noticeable improvement on standard vision-language\ndatasets, current MLLMs struggle to leverage visual information in the\ndemonstrations. Specifically, they tend to neglect visual cues and over-rely on\ntextual patterns, leading to mere text imitation rather than genuine multimodal\nadaptation. This behavior makes MICL still unimodal and largely restricts its\npractical utility. More importantly, this limitation is often concealed by the\nimproved performance on tasks that do not require understanding the visual\ncontext. As a result, how to effectively enhance MICL ability and reliably\nevaluate the MICL performance remains underexplored. To address these issues,\nwe first introduce Dynamic Attention Reallocation (DARA), an efficient\nfine-tuning strategy that encourages models to attend to the visual context by\nrebalancing attention across visual and textual tokens. In addition, we present\nTrueMICL, an MICL-dedicated dataset with both support and test sets that\nexplicitly requires the integration of multimodal information-particularly\nvisual content-for correct task completion. Extensive experiments demonstrate\nthe effectiveness of our holistic solution, showcasing substantial improvements\nin the true multimodal in-context learning capabilities. Code and datasets are\navailable at https://chenxshuo.github.io/true-micl-colm .", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDARA\u7b56\u7565\u548cTrueMICL\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86MLLMs\u5728\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u5ffd\u89c6\u89c6\u89c9\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLMs)\u5728\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u6a21\u5f0f\uff0c\u5ffd\u89c6\u89c6\u89c9\u7ebf\u7d22\uff0c\u5bfc\u81f4\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60(MICL)\u80fd\u529b\u53d7\u9650\u3002\u8fd9\u4e00\u5c40\u9650\u6027\u5e38\u88ab\u65e0\u9700\u89c6\u89c9\u7406\u89e3\u7684\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u6240\u63a9\u76d6\u3002", "method": "\u91c7\u7528Dynamic Attention Reallocation (DARA)\u7b56\u7565\uff0c\u901a\u8fc7\u91cd\u65b0\u5e73\u8861\u89c6\u89c9\u548c\u6587\u672c\u6807\u8bb0\u7684\u6ce8\u610f\u529b\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u89c6\u89c9\u5185\u5bb9\u7684\u5173\u6ce8\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86TrueMICL\u6570\u636e\u96c6\uff0c\u660e\u786e\u8981\u6c42\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u4ee5\u5b8c\u6210\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDARA\u548cTrueMICL\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Dynamic Attention Reallocation (DARA)\u7b56\u7565\u548cTrueMICL\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u6a21\u5f0f\u7684\u95ee\u9898\u3002"}}
{"id": "2507.14918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14918", "abs": "https://arxiv.org/abs/2507.14918", "authors": ["Ren-Dong Xie", "Zhi-Fen He", "Bo Li", "Bin Liu", "Jin-Yan Hu"], "title": "Semantic-Aware Representation Learning for Multi-label Image Classification", "comment": null, "summary": "Multi-label image classification, an important research area in computer\nvision, focuses on identifying multiple labels or concepts within an image.\nExisting approaches often employ attention mechanisms or graph convolutional\nnetworks (GCNs) to learn image representation. However, this representation may\ncontain noise and may not locate objects precisely. Therefore, this paper\nproposes a Semantic-Aware Representation Learning (SARL) for multi-label image\nclassification. First, a label semantic-related feature learning module is\nutilized to extract semantic-related features. Then, an optimal transport-based\nattention mechanism is designed to obtain semantically aligned image\nrepresentation. Finally, a regional score aggregation strategy is used for\nmulti-label prediction. Experimental results on two benchmark datasets, PASCAL\nVOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSARL\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u8868\u793a\u5b66\u4e60\u6539\u8fdb\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u6216\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCNs\uff09\u5b66\u4e60\u56fe\u50cf\u8868\u793a\uff0c\u4f46\u8fd9\u4e9b\u8868\u793a\u53ef\u80fd\u5305\u542b\u566a\u58f0\u4e14\u65e0\u6cd5\u7cbe\u786e\u5b9a\u4f4d\u5bf9\u8c61\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u8868\u793a\u5b66\u4e60\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u8868\u793a\u5b66\u4e60\uff08SARL\uff09\u65b9\u6cd5\uff0c\u5305\u62ec\u4e09\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a1\uff09\u4f7f\u7528\u6807\u7b7e\u8bed\u4e49\u76f8\u5173\u7279\u5f81\u5b66\u4e60\u6a21\u5757\u63d0\u53d6\u8bed\u4e49\u76f8\u5173\u7279\u5f81\uff1b2\uff09\u8bbe\u8ba1\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u83b7\u5f97\u8bed\u4e49\u5bf9\u9f50\u7684\u56fe\u50cf\u8868\u793a\uff1b3\uff09\u91c7\u7528\u533a\u57df\u5206\u6570\u805a\u5408\u7b56\u7565\u8fdb\u884c\u591a\u6807\u7b7e\u9884\u6d4b\u3002", "result": "\u5728PASCAL VOC 2007\u548cMS-COCO\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSARL\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SARL\u65b9\u6cd5\u5728PASCAL VOC 2007\u548cMS-COCO\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8bed\u4e49\u611f\u77e5\u8868\u793a\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.14921", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14921", "abs": "https://arxiv.org/abs/2507.14921", "authors": ["Xiufeng Huang", "Ka Chun Cheung", "Runmin Cong", "Simon See", "Renjie Wan"], "title": "Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction", "comment": "ACMMM2025. Non-camera-ready version", "summary": "Generalizable 3D Gaussian Splatting reconstruction showcases advanced\nImage-to-3D content creation but requires substantial computational resources\nand large datasets, posing challenges to training models from scratch. Current\nmethods usually entangle the prediction of 3D Gaussian geometry and appearance,\nwhich rely heavily on data-driven priors and result in slow regression speeds.\nTo address this, we propose \\method, a disentangled framework for efficient 3D\nGaussian prediction. Our method extracts features from local image pairs using\na stereo vision backbone and fuses them via global attention blocks. Dedicated\npoint and Gaussian prediction heads generate multi-view point-maps for geometry\nand Gaussian features for appearance, combined as GS-maps to represent the 3DGS\nobject. A refinement network enhances these GS-maps for high-quality\nreconstruction. Unlike existing methods that depend on camera parameters, our\napproach achieves pose-free 3D reconstruction, improving robustness and\npracticality. By reducing resource demands while maintaining high-quality\noutputs, \\method provides an efficient, scalable solution for real-world 3D\ncontent generation.", "AI": {"tldr": "\\method \u662f\u4e00\u4e2a\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u7acb\u4f53\u89c6\u89c9\u548c\u5168\u5c40\u6ce8\u610f\u529b\u9ad8\u6548\u9884\u6d4b3D\u9ad8\u65af\u51e0\u4f55\u4e0e\u5916\u89c2\uff0c\u51cf\u5c11\u4e86\u8d44\u6e90\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u5b9e\u96453D\u5185\u5bb9\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u901a\u5e38\u5c063D\u9ad8\u65af\u51e0\u4f55\u4e0e\u5916\u89c2\u9884\u6d4b\u8026\u5408\uff0c\u4e25\u91cd\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u5148\u9a8c\uff0c\u5bfc\u81f4\u56de\u5f52\u901f\u5ea6\u6162\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\\method\uff0c\u4e00\u4e2a\u89e3\u8026\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u76843D\u9ad8\u65af\u9884\u6d4b\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7acb\u4f53\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u4ece\u5c40\u90e8\u56fe\u50cf\u5bf9\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5168\u5c40\u6ce8\u610f\u529b\u5757\u878d\u5408\u8fd9\u4e9b\u7279\u5f81\u3002\u4e13\u7528\u7684\u70b9\u548c\u9ad8\u65af\u9884\u6d4b\u5934\u751f\u6210\u591a\u89c6\u70b9\u56fe\u7528\u4e8e\u51e0\u4f55\u9884\u6d4b\u548c\u9ad8\u65af\u7279\u5f81\u7528\u4e8e\u5916\u89c2\u9884\u6d4b\uff0c\u7ed3\u5408\u4e3aGS-maps\u8868\u793a3DGS\u5bf9\u8c61\u3002\u4e00\u4e2a\u7ec6\u5316\u7f51\u7edc\u589e\u5f3a\u8fd9\u4e9bGS-maps\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "result": "\\method \u5b9e\u73b0\u4e86\u65e0\u9700\u76f8\u673a\u53c2\u6570\u7684\u59ff\u6001\u65e0\u51733D\u91cd\u5efa\uff0c\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\\method \u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u89e3\u80263D\u9ad8\u65af\u51e0\u4f55\u4e0e\u5916\u89c2\u9884\u6d4b\uff0c\u51cf\u5c11\u4e86\u8d44\u6e90\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u8f93\u51fa\uff0c\u9002\u7528\u4e8e\u5b9e\u96453D\u5185\u5bb9\u751f\u6210\u3002"}}
{"id": "2507.15852", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15852", "abs": "https://arxiv.org/abs/2507.15852", "authors": ["Zhixiong Zhang", "Shuangrui Ding", "Xiaoyi Dong", "Songxin He", "Jianfan Lin", "Junsong Tang", "Yuhang Zang", "Yuhang Cao", "Dahua Lin", "Jiaqi Wang"], "title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction", "comment": "project page: https://rookiexiong7.github.io/projects/SeC/; code:\n  https://github.com/OpenIXCLab/SeC; dataset:\n  https://huggingface.co/datasets/OpenIXCLab/SeCVOS", "summary": "Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation.", "AI": {"tldr": "SeC\u662f\u4e00\u79cd\u6982\u5ff5\u9a71\u52a8\u7684\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u5bf9\u8c61\u6982\u5ff5\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u6280\u672f\u4f9d\u8d56\u4e8e\u5916\u89c2\u5339\u914d\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u822c\u7684\u5bf9\u8c61\u6982\u5ff5\u7406\u89e3\u80fd\u529b\uff0c\u5bfc\u81f4\u5728\u5904\u7406\u590d\u6742\u573a\u666f\u65f6\u6027\u80fd\u4e0d\u8db3\u3002SeC\u65e8\u5728\u901a\u8fc7\u6982\u5ff5\u9a71\u52a8\u7684\u5206\u5272\u65b9\u6cd5\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "SeC\u6846\u67b6\u91c7\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u6784\u5efa\u548c\u5229\u7528\u9ad8\u5c42\u3001\u5bf9\u8c61\u4e2d\u5fc3\u7684\u8868\u793a\uff0c\u7ed3\u5408\u89c6\u89c9\u7ebf\u7d22\u548c\u8bed\u4e49\u63a8\u7406\uff0c\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u4ee5\u5e94\u5bf9\u573a\u666f\u590d\u6742\u5ea6\u3002", "result": "SeC\u5728SeCVOS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4SAM 2.1\u63d0\u9ad8\u4e8611.8\u5206\uff0c\u5b9e\u73b0\u4e86\u6982\u5ff5\u611f\u77e5\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u7684\u65b0\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "SeC\u6846\u67b6\u901a\u8fc7\u6982\u5ff5\u9a71\u52a8\u7684\u5206\u5272\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5bf9\u8c61\u5206\u5272\uff08VOS\uff09\u4efb\u52a1\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u89c6\u89c9\u53d8\u5316\u3001\u906e\u6321\u548c\u52a8\u6001\u573a\u666f\u8f6c\u6362\u65b9\u9762\uff0c\u53d6\u5f97\u4e8611.8\u5206\u7684\u6027\u80fd\u63d0\u5347\uff0c\u6210\u4e3a\u6982\u5ff5\u611f\u77e5\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u7684\u65b0\u6807\u6746\u3002"}}
{"id": "2507.14924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14924", "abs": "https://arxiv.org/abs/2507.14924", "authors": ["Kaishva Chintan Shah", "Virajith Boddapati", "Karthik S. Gurumoorthy", "Sandip Kaledhonkar", "Ajit Rajwade"], "title": "3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline", "comment": null, "summary": "Accurate pose estimation and shift correction are key challenges in cryo-EM\ndue to the very low SNR, which directly impacts the fidelity of 3D\nreconstructions. We present an approach for pose estimation in cryo-EM that\nleverages multi-dimensional scaling (MDS) techniques in a robust manner to\nestimate the 3D rotation matrix of each particle from pairs of dihedral angles.\nWe express the rotation matrix in the form of an axis of rotation and a unit\nvector in the plane perpendicular to the axis. The technique leverages the\nconcept of common lines in 3D reconstruction from projections. However, common\nline estimation is ridden with large errors due to the very low SNR of cryo-EM\nprojection images. To address this challenge, we introduce two complementary\ncomponents: (i) a robust joint optimization framework for pose estimation based\non an $\\ell_1$-norm objective or a similar robust norm, which simultaneously\nestimates rotation axes and in-plane vectors while exactly enforcing unit norm\nand orthogonality constraints via projected coordinate descent; and (ii) an\niterative shift correction algorithm that estimates consistent in-plane\ntranslations through a global least-squares formulation. While prior approaches\nhave leveraged such embeddings and common-line geometry for orientation\nrecovery, existing formulations typically rely on $\\ell_2$-based objectives\nthat are sensitive to noise, and enforce geometric constraints only\napproximately. These choices, combined with a sequential pipeline structure,\ncan lead to compounding errors and suboptimal reconstructions in low-SNR\nregimes. Our pipeline consistently outperforms prior methods in both Euler\nangle accuracy and reconstruction fidelity, as measured by the Fourier Shell\nCorrelation (FSC).", "AI": {"conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4f4e\u4fe1\u566a\u6bd4\u7684\u51b7\u51bb\u7535\u955c\uff08cryo-EM\uff09\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7\u7ed3\u5408\u9c81\u68d2\u7684$\\ell_1$-\u8303\u6570\u4f18\u5316\u548c\u8fed\u4ee3\u5e73\u79fb\u6821\u6b63\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u4e09\u7ef4\u91cd\u5efa\u7684\u4fdd\u771f\u5ea6\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u7ef4\u5c3a\u5ea6\u5206\u6790\uff08MDS\uff09\u6280\u672f\u7684\u9c81\u68d2\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408$\\ell_1$-\u8303\u6570\u76ee\u6807\u51fd\u6570\u548c\u6295\u5f71\u5750\u6807\u4e0b\u964d\u6cd5\uff0c\u540c\u65f6\u4f30\u8ba1\u65cb\u8f6c\u8f74\u548c\u5e73\u9762\u5185\u5411\u91cf\uff0c\u5e76\u901a\u8fc7\u5168\u5c40\u6700\u5c0f\u4e8c\u4e58\u516c\u5f0f\u8fdb\u884c\u8fed\u4ee3\u5e73\u79fb\u6821\u6b63\u3002", "motivation": "\u51b7\u51bb\u7535\u955c\uff08cryo-EM\uff09\u4e2d\u7684\u59ff\u6001\u4f30\u8ba1\u548c\u4f4d\u79fb\u6821\u6b63\u662f\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6781\u4f4e\u4fe1\u566a\u6bd4\uff08SNR\uff09\u6761\u4ef6\u4e0b\uff0c\u76f4\u63a5\u5f71\u54cd\u4e09\u7ef4\u91cd\u5efa\u7684\u4fdd\u771f\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5bf9\u566a\u58f0\u654f\u611f\u7684$\\ell_2$-\u8303\u6570\u76ee\u6807\u51fd\u6570\uff0c\u4e14\u4ec5\u8fd1\u4f3c\u6ee1\u8db3\u51e0\u4f55\u7ea6\u675f\uff0c\u5bfc\u81f4\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u8bef\u5dee\u7d2f\u79ef\u548c\u6b21\u4f18\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6b27\u62c9\u89d2\u51c6\u786e\u6027\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\uff08\u901a\u8fc7\u5085\u91cc\u53f6\u58f3\u5c42\u76f8\u5173\u6027\uff08FSC\uff09\u8861\u91cf\uff09\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u51b7\u51bb\u7535\u955c\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408$\\ell_1$-\u8303\u6570\u4f18\u5316\u548c\u8fed\u4ee3\u5e73\u79fb\u6821\u6b63\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u7684\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2507.14932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14932", "abs": "https://arxiv.org/abs/2507.14932", "authors": ["Francisco M. Castro-Mac\u00edas", "Pablo Morales-\u00c1lvarez", "Yunan Wu", "Rafael Molina", "Aggelos K. Katsaggelos"], "title": "Probabilistic smooth attention for deep multiple instance learning in medical imaging", "comment": null, "summary": "The Multiple Instance Learning (MIL) paradigm is attracting plenty of\nattention in medical imaging classification, where labeled data is scarce. MIL\nmethods cast medical images as bags of instances (e.g. patches in whole slide\nimages, or slices in CT scans), and only bag labels are required for training.\nDeep MIL approaches have obtained promising results by aggregating\ninstance-level representations via an attention mechanism to compute the\nbag-level prediction. These methods typically capture both local interactions\namong adjacent instances and global, long-range dependencies through various\nmechanisms. However, they treat attention values deterministically, potentially\noverlooking uncertainty in the contribution of individual instances. In this\nwork we propose a novel probabilistic framework that estimates a probability\ndistribution over the attention values, and accounts for both global and local\ninteractions. In a comprehensive evaluation involving {\\color{review} eleven}\nstate-of-the-art baselines and three medical datasets, we show that our\napproach achieves top predictive performance in different metrics. Moreover,\nthe probabilistic treatment of the attention provides uncertainty maps that are\ninterpretable in terms of illness localization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u793a\u4f8b\u5b66\u4e60\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u4e0d\u786e\u5b9a\u6027\u6620\u5c04\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u591a\u793a\u4f8b\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u786e\u5b9a\u6027\u5730\u5904\u7406\u6ce8\u610f\u529b\u503c\uff0c\u5ffd\u89c6\u4e86\u5355\u4e2a\u5b9e\u4f8b\u8d21\u732e\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6982\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u4f30\u8ba1\u6ce8\u610f\u529b\u503c\u7684\u6982\u7387\u5206\u5e03\u6765\u6355\u6349\u5168\u5c40\u548c\u5c40\u90e8\u4ea4\u4e92\uff0c\u5e76\u8003\u8651\u4e86\u5355\u4e2a\u5b9e\u4f8b\u8d21\u732e\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u548c\u5341\u4e00\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u7684\u5168\u9762\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u4f73\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6982\u7387\u6846\u67b6\u5728\u591a\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u9884\u6d4b\u6027\u80fd\u4f18\u5f02\uff0c\u8fd8\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u4e0d\u786e\u5b9a\u6027\u6620\u5c04\uff0c\u6709\u52a9\u4e8e\u75be\u75c5\u5b9a\u4f4d\u3002"}}
{"id": "2507.14935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14935", "abs": "https://arxiv.org/abs/2507.14935", "authors": ["Hai Huang", "Yan Xia", "Shulei Wang", "Hanting Wang", "Minghui Fang", "Shengpeng Ji", "Sashuai Zhou", "Tao Jin", "Zhou Zhao"], "title": "Open-set Cross Modal Generalization via Multimodal Unified Representation", "comment": "Accepted by ICCV 2025", "summary": "This paper extends Cross Modal Generalization (CMG) to open-set environments\nby proposing the more challenging Open-set Cross Modal Generalization (OSCMG)\ntask. This task evaluates multimodal unified representations in open-set\nconditions, addressing the limitations of prior closed-set cross-modal\nevaluations. OSCMG requires not only cross-modal knowledge transfer but also\nrobust generalization to unseen classes within new modalities, a scenario\nfrequently encountered in real-world applications. Existing multimodal unified\nrepresentation work lacks consideration for open-set environments. To tackle\nthis, we propose MICU, comprising two key components: Fine-Coarse Masked\nmultimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI\nenhances multimodal alignment by applying contrastive learning at both holistic\nsemantic and temporal levels, incorporating masking to enhance generalization.\nCUJP enhances feature diversity and model uncertainty by integrating\nmodality-agnostic feature selection with self-supervised learning, thereby\nstrengthening the model's ability to handle unknown categories in open-set\ntasks. Extensive experiments on CMG and the newly proposed OSCMG validate the\neffectiveness of our approach. The code is available at\nhttps://github.com/haihuangcode/CMG.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5f00\u653e\u96c6\u8de8\u6a21\u6001\u6cdb\u5316\u4efb\u52a1\uff08OSCMG\uff09\u53caMICU\u65b9\u6cd5\uff0c\u901a\u8fc7FCMI\u548cCUJP\u7ec4\u4ef6\u589e\u5f3a\u591a\u6a21\u6001\u5bf9\u9f50\u548c\u7279\u5f81\u591a\u6837\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u5f00\u653e\u96c6\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u7edf\u4e00\u8868\u793a\u5de5\u4f5c\u7f3a\u4e4f\u5bf9\u5f00\u653e\u96c6\u73af\u5883\u7684\u8003\u8651\uff0c\u800c\u73b0\u5b9e\u5e94\u7528\u4e2d\u5e38\u9047\u5230\u672a\u89c1\u7c7b\u522b\u7684\u65b0\u6a21\u6001\u573a\u666f\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u66f4\u5177\u6311\u6218\u6027\u7684\u5f00\u653e\u96c6\u8de8\u6a21\u6001\u6cdb\u5316\uff08OSCMG\uff09\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86MICU\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1aFine-Coarse Masked multimodal InfoNCE\uff08FCMI\uff09\u548cCross modal Unified Jigsaw Puzzles\uff08CUJP\uff09\u3002FCMI\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5728\u6574\u4f53\u8bed\u4e49\u548c\u65f6\u95f4\u5c42\u9762\u4e0a\u589e\u5f3a\u591a\u6a21\u6001\u5bf9\u9f50\uff0cCUJP\u901a\u8fc7\u6a21\u6001\u65e0\u5173\u7279\u5f81\u9009\u62e9\u4e0e\u81ea\u76d1\u7763\u5b66\u4e60\u589e\u5f3a\u7279\u5f81\u591a\u6837\u6027\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728CMG\u548c\u65b0\u63d0\u51fa\u7684OSCMG\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MICU\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u63d0\u51faMICU\u65b9\u6cd5\uff0c\u6210\u529f\u6269\u5c55\u4e86\u8de8\u6a21\u6001\u6cdb\u5316\uff08CMG\uff09\u5230\u5f00\u653e\u96c6\u73af\u5883\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5f00\u653e\u96c6\u6761\u4ef6\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.14959", "categories": ["cs.CV", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.14959", "abs": "https://arxiv.org/abs/2507.14959", "authors": ["Saeid Ghafouri", "Mohsen Fayyaz", "Xiangchen Li", "Deepu John", "Bo Ji", "Dimitrios Nikolopoulos", "Hans Vandierendonck"], "title": "Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices", "comment": null, "summary": "Real-time multi-label video classification on embedded devices is constrained\nby limited compute and energy budgets. Yet, video streams exhibit structural\nproperties such as label sparsity, temporal continuity, and label co-occurrence\nthat can be leveraged for more efficient inference. We introduce Polymorph, a\ncontext-aware framework that activates a minimal set of lightweight Low Rank\nAdapters (LoRA) per frame. Each adapter specializes in a subset of classes\nderived from co-occurrence patterns and is implemented as a LoRA weight over a\nshared backbone. At runtime, Polymorph dynamically selects and composes only\nthe adapters needed to cover the active labels, avoiding full-model switching\nand weight merging. This modular strategy improves scalability while reducing\nlatency and energy overhead. Polymorph achieves 40% lower energy consumption\nand improves mAP by 9 points over strong baselines on the TAO dataset.\nPolymorph is open source at https://github.com/inference-serving/polymorph/.", "AI": {"tldr": "Polymorph\u662f\u4e00\u79cd\u52a8\u6001\u6fc0\u6d3b\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u7684\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u80fd\u8017\u5e76\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u591a\u6807\u7b7e\u89c6\u9891\u5206\u7c7b\u53d7\u9650\u4e8e\u8ba1\u7b97\u548c\u80fd\u6e90\u9884\u7b97\uff0c\u4f46\u89c6\u9891\u6d41\u5177\u6709\u6807\u7b7e\u7a00\u758f\u6027\u3001\u65f6\u95f4\u8fde\u7eed\u6027\u548c\u6807\u7b7e\u5171\u73b0\u6027\u7b49\u7ed3\u6784\u7279\u6027\uff0c\u53ef\u4f18\u5316\u63a8\u7406\u6548\u7387\u3002", "method": "Polymorph\u91c7\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u6846\u67b6\uff0c\u52a8\u6001\u6fc0\u6d3b\u6bcf\u5e27\u6240\u9700\u7684\u8f7b\u91cf\u7ea7\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\uff0c\u907f\u514d\u5168\u6a21\u578b\u5207\u6362\u548c\u6743\u91cd\u5408\u5e76\u3002", "result": "\u5728TAO\u6570\u636e\u96c6\u4e0a\uff0cPolymorph\u6bd4\u5f3a\u57fa\u7ebf\u964d\u4f4e\u4e8640%\u7684\u80fd\u8017\uff0c\u5e76\u63d0\u9ad8\u4e869\u4e2a\u70b9\u7684mAP\u3002", "conclusion": "Polymorph\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u548c\u7ec4\u5408\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u80fd\u8017\uff0c\u5e76\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2507.14965", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14965", "abs": "https://arxiv.org/abs/2507.14965", "authors": ["Yaojie Zhang", "Tianlun Huang", "Weijun Wang", "Wei Feng"], "title": "Decision PCR: Decision version of the Point Cloud Registration task", "comment": null, "summary": "Low-overlap point cloud registration (PCR) remains a significant challenge in\n3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become\nineffective under extremely low inlier ratios. In this paper, we revisit the\nregistration result evaluation problem and identify the Decision version of the\nPCR task as the fundamental problem. To address this Decision PCR task, we\npropose a data-driven approach. First, we construct a corresponding dataset\nbased on the 3DMatch dataset. Then, a deep learning-based classifier is trained\nto reliably assess registration quality, overcoming the limitations of\ntraditional metrics. To our knowledge, this is the first comprehensive study to\naddress this task through a deep learning framework. We incorporate this\nclassifier into standard PCR pipelines. When integrated with our approach,\nexisting state-of-the-art PCR methods exhibit significantly enhanced\nregistration performance. For example, combining our framework with\nGeoTransformer achieves a new SOTA registration recall of 86.97\\% on the\nchallenging 3DLoMatch benchmark. Our method also demonstrates strong\ngeneralization capabilities on the unseen outdoor ETH dataset.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u89e3\u51b3\u4f4e\u91cd\u53e0\u70b9\u4e91\u914d\u51c6\u7684\u8bc4\u4f30\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u5728\u6781\u4f4e\u5185\u70b9\u6bd4\u4f8b\u4e0b\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u914d\u51c6\u7ed3\u679c\u8bc4\u4f30\u95ee\u9898\uff0c\u5e76\u5c06PCR\u4efb\u52a1\u7684\u51b3\u7b56\u7248\u672c\u4f5c\u4e3a\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u57fa\u4e8e3DMatch\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u6765\u53ef\u9760\u8bc4\u4f30\u914d\u51c6\u8d28\u91cf\uff0c\u514b\u670d\u4f20\u7edf\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709PCR\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4f8b\u5982\u4e0eGeoTransformer\u7ed3\u5408\u540e\u57283DLoMatch\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523086.97%\u7684\u914d\u51c6\u53ec\u56de\u7387\uff0c\u5e76\u5728\u672a\u89c1\u7684\u5ba4\u5916ETH\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u4f4e\u91cd\u53e0\u70b9\u4e91\u914d\u51c6\uff08PCR\uff09\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u6700\u5148\u8fdbPCR\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u57283DLoMatch\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e8686.97%\u7684\u914d\u51c6\u53ec\u56de\u7387\u3002"}}
{"id": "2507.14976", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14976", "abs": "https://arxiv.org/abs/2507.14976", "authors": ["Hao Zheng", "Shunzhi Yang", "Zhuoxin He", "Jinfeng Yang", "Zhenhua Huang"], "title": "Hierarchical Cross-modal Prompt Learning for Vision-Language Models", "comment": "Accepted by ICCV2025", "summary": "Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent\ngeneralization abilities. However, adapting these large-scale models to\ndownstream tasks while preserving their generalization capabilities remains\nchallenging. Although prompt learning methods have shown promise, they suffer\nfrom two fundamental bottlenecks that limit generalization: (a) modality\nisolation, and (b) hierarchical semantic decay. To address these limitations,\nwe propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that\nestablishes bidirectional knowledge flow between text and vision modalities,\nenabling them to refine their semantics mutually. HiCroPL routes knowledge\nflows by leveraging the complementary strengths of text and vision. In early\nlayers, text prompts inject relatively clear semantics into visual prompts\nthrough a hierarchical knowledge mapper, enhancing the representation of\nlow-level visual semantics. In later layers, visual prompts encoding specific\ntask-relevant objects flow back to refine text prompts, enabling deeper\nalignment. Crucially, our hierarchical knowledge mapper allows representations\nat multi-scales to be fused, ensuring that deeper representations retain\ntransferable shallow semantics thereby enhancing generalization. We further\nintroduce a lightweight layer-specific knowledge proxy to enable efficient\ncross-modal interactions. Extensive evaluations across four tasks demonstrate\nHiCroPL's superior performance, achieving state-of-the-art results on 11\nbenchmarks with significant improvements. Code is available at:\nhttps://github.com/zzeoZheng/HiCroPL.", "AI": {"tldr": "HiCroPL\u901a\u8fc7\u53cc\u5411\u77e5\u8bc6\u6d41\u548c\u5206\u5c42\u77e5\u8bc6\u6620\u5c04\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u9002\u5e94\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7684\u6a21\u6001\u9694\u79bb\u548c\u5206\u5c42\u8bed\u4e49\u8870\u51cf\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86HiCroPL\u6846\u67b6\uff0c\u5229\u7528\u5206\u5c42\u77e5\u8bc6\u6620\u5c04\u5668\u5b9e\u73b0\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u4e4b\u95f4\u7684\u53cc\u5411\u77e5\u8bc6\u6d41\u52a8\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5c42\u7279\u5b9a\u77e5\u8bc6\u4ee3\u7406\u5b9e\u73b0\u9ad8\u6548\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5728\u56db\u4e2a\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cHiCroPL\u572811\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "HiCroPL\u901a\u8fc7\u53cc\u5411\u77e5\u8bc6\u6d41\u548c\u5206\u5c42\u77e5\u8bc6\u6620\u5c04\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u572811\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2507.14997", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14997", "abs": "https://arxiv.org/abs/2507.14997", "authors": ["Roy H. Jennings", "Genady Paikin", "Roy Shaul", "Evgeny Soloveichik"], "title": "Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show promise for image-based\nregression tasks, but current approaches face key limitations. Recent methods\nfine-tune MLLMs using preset output vocabularies and generic task-level prompts\n(e.g., \"How would you rate this image?\"), assuming this mimics human rating\nbehavior. Our analysis reveals these approaches provide no benefit over\nimage-only training. Models using preset vocabularies and generic prompts\nperform equivalently to image-only models, failing to leverage semantic\nunderstanding from textual input. We propose Regression via Transformer-Based\nClassification (RvTC), which replaces vocabulary-constrained classification\nwith a flexible bin-based approach. Unlike approaches that address\ndiscretization errors through complex distributional modeling, RvTC eliminates\nmanual vocabulary crafting through straightforward bin increase, achieving\nstate-of-the-art performance on four image assessment datasets using only\nimages. More importantly, we demonstrate that data-specific prompts\ndramatically improve performance. Unlike generic task descriptions, prompts\ncontaining semantic information about specific images enable MLLMs to leverage\ncross-modal understanding. On the AVA dataset, adding challenge titles to\nprompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We\ndemonstrate through empirical evidence from the AVA and AGIQA-3k datasets that\nMLLMs benefit from semantic prompt information surpassing mere statistical\nbiases. This underscores the importance of incorporating meaningful textual\ncontext in multimodal regression tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRvTC\u65b9\u6cd5\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u57fa\u4e8e\u5206\u7bb1\u7684\u5206\u7c7b\u56de\u5f52\u66ff\u4ee3\u9884\u8bbe\u8bcd\u6c47\u5206\u7c7b\uff0c\u7ed3\u5408\u6570\u636e\u7279\u5b9a\u63d0\u793a\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u56de\u5f52\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u56fe\u50cf\u56de\u5f52\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9884\u8bbe\u8bcd\u6c47\u548c\u901a\u7528\u63d0\u793a\u65e0\u6cd5\u6709\u6548\u5229\u7528\u6587\u672c\u8f93\u5165\u7684\u8bed\u4e49\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eTransformer\u7684\u5206\u7c7b\u56de\u5f52\u65b9\u6cd5\uff08RvTC\uff09\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u57fa\u4e8e\u5206\u7bb1\u7684\u65b9\u6cd5\u66ff\u4ee3\u4e86\u9884\u8bbe\u8bcd\u6c47\u5206\u7c7b\uff0c\u907f\u514d\u4e86\u624b\u52a8\u8bcd\u6c47\u6784\u5efa\u3002", "result": "\u5728\u56db\u4e2a\u56fe\u50cf\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728AVA\u6570\u636e\u96c6\u4e2d\uff0c\u901a\u8fc7\u6dfb\u52a0\u6311\u6218\u6807\u9898\uff0c\u76f8\u5173\u6027\u4ece0.83\u63d0\u5347\u81f30.90\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5728\u591a\u6a21\u6001\u56de\u5f52\u4efb\u52a1\u4e2d\u878d\u5165\u6709\u610f\u4e49\u6587\u672c\u4e0a\u4e0b\u6587\u7684\u91cd\u8981\u6027\uff0c\u5c55\u793a\u4e86\u6570\u636e\u7279\u5b9a\u63d0\u793a\u5982\u4f55\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.15000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15000", "abs": "https://arxiv.org/abs/2507.15000", "authors": ["Chaoyun Wang", "I-Chao Shen", "Takeo Igarashi", "Nanning Zheng", "Caigui Jiang"], "title": "Axis-Aligned Document Dewarping", "comment": null, "summary": "Document dewarping is crucial for many applications. However, existing\nlearning-based methods primarily rely on supervised regression with annotated\ndata without leveraging the inherent geometric properties in physical documents\nto the dewarping process. Our key insight is that a well-dewarped document is\ncharacterized by transforming distorted feature lines into axis-aligned ones.\nThis property aligns with the inherent axis-aligned nature of the discrete grid\ngeometry in planar documents. In the training phase, we propose an axis-aligned\ngeometric constraint to enhance document dewarping. In the inference phase, we\npropose an axis alignment preprocessing strategy to reduce the dewarping\ndifficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned\nDistortion (AAD), that not only incorporates geometric meaning and aligns with\nhuman visual perception but also demonstrates greater robustness. As a result,\nour method achieves SOTA results on multiple existing benchmarks and achieves\n18.2%~34.5% improvements on the AAD metric.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u6587\u6863\u56fa\u6709\u51e0\u4f55\u7279\u6027\u7684\u53bb\u626d\u66f2\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f74\u5bf9\u9f50\u7ea6\u675f\u548c\u9884\u5904\u7406\u7b56\u7565\u63d0\u5347\u6548\u679c\uff0c\u5e76\u5728\u65b0\u6307\u6807AAD\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5e26\u6807\u6ce8\u6570\u636e\u7684\u76d1\u7763\u56de\u5f52\uff0c\u800c\u672a\u5145\u5206\u5229\u7528\u7269\u7406\u6587\u6863\u4e2d\u7684\u56fa\u6709\u51e0\u4f55\u7279\u6027\u3002", "method": "\u5728\u8bad\u7ec3\u9636\u6bb5\u63d0\u51fa\u8f74\u5bf9\u9f50\u51e0\u4f55\u7ea6\u675f\u4ee5\u589e\u5f3a\u6587\u6863\u53bb\u626d\u66f2\u6548\u679c\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u63d0\u51fa\u8f74\u5bf9\u9f50\u9884\u5904\u7406\u7b56\u7565\u4ee5\u964d\u4f4e\u53bb\u626d\u66f2\u96be\u5ea6\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807AAD\u3002", "result": "\u5728\u591a\u4e2a\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u7ed3\u679c\uff0cAAD\u6307\u6807\u63d0\u534718.2%~34.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\uff0c\u5e76\u5728AAD\u6307\u6807\u4e0a\u5b9e\u73b0\u4e8618.2%~34.5%\u7684\u6539\u8fdb\u3002"}}
{"id": "2507.15008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15008", "abs": "https://arxiv.org/abs/2507.15008", "authors": ["Jiasheng Xu", "Yewang Chen"], "title": "FastSmoothSAM: A Fast Smooth Method For Segment Anything Model", "comment": null, "summary": "Accurately identifying and representing object edges is a challenging task in\ncomputer vision and image processing. The Segment Anything Model (SAM) has\nsignificantly influenced the field of image segmentation, but suffers from high\nmemory consumption and long inference times, limiting its efficiency in\nreal-time applications. To address these limitations, Fast Segment Anything\n(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM\noften generates jagged edges that deviate from the true object shapes.\nTherefore, this paper introduces a novel refinement approach using B-Spline\ncurve fitting techniques to enhance the edge quality in FastSAM. Leveraging the\nrobust shape control and flexible geometric construction of B-Splines, a\nfour-stage refining process involving two rounds of curve fitting is employed\nto effectively smooth jagged edges. This approach significantly improves the\nvisual quality and analytical accuracy of object edges without compromising\ncritical geometric information. The proposed method improves the practical\nutility of FastSAM by improving segmentation accuracy while maintaining\nreal-time processing capabilities. This advancement unlocks greater potential\nfor FastSAM technology in various real-world scenarios, such as industrial\nautomation, medical imaging, and autonomous systems, where precise and\nefficient edge recognition is crucial.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eB\u6837\u6761\u66f2\u7ebf\u62df\u5408\u7684\u7ec6\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3FastSAM\u5728\u5b9e\u65f6\u5206\u5272\u4e2d\u4ea7\u751f\u7684\u952f\u9f7f\u8fb9\u7f18\u95ee\u9898\uff0c\u63d0\u5347\u8fb9\u7f18\u8d28\u91cf\u548c\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "FastSAM\u867d\u7136\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5206\u5272\uff0c\u4f46\u751f\u6210\u7684\u8fb9\u7f18\u5b58\u5728\u952f\u9f7f\u72b6\u504f\u5dee\uff0c\u5f71\u54cd\u4e86\u5206\u5272\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u5206\u6790\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u7ec6\u5316\u6d41\u7a0b\uff0c\u5305\u62ec\u4e24\u8f6eB\u6837\u6761\u66f2\u7ebf\u62df\u5408\uff0c\u4ee5\u5e73\u6ed1\u952f\u9f7f\u8fb9\u7f18\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u51e0\u4f55\u4fe1\u606f\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86FastSAM\u7684\u5206\u5272\u7cbe\u5ea6\u548c\u8fb9\u7f18\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8eB\u6837\u6761\u66f2\u7ebf\u62df\u5408\u7684\u7ec6\u5316\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86FastSAM\u7684\u8fb9\u7f18\u8d28\u91cf\uff0c\u589e\u5f3a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7cbe\u786e\u8fb9\u7f18\u8bc6\u522b\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2507.15028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15028", "abs": "https://arxiv.org/abs/2507.15028", "authors": ["Yuanhan Zhang", "Yunice Chew", "Yuhao Dong", "Aria Leo", "Bo Hu", "Ziwei Liu"], "title": "Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding", "comment": "ICCV 2025; Project page: https://zhangyuanhan-ai.github.io/video-tt/", "summary": "Human intelligence requires correctness and robustness, with the former being\nfoundational for the latter. In video understanding, correctness ensures the\naccurate interpretation of visual content, and robustness maintains consistent\nperformance in challenging conditions. Despite advances in video large language\nmodels (video LLMs), existing benchmarks inadequately reflect the gap between\nthese models and human intelligence in maintaining correctness and robustness\nin video interpretation. We introduce the Video Thinking Test (Video-TT), to\nassess if video LLMs can interpret real-world videos as effectively as humans.\nVideo-TT reflects genuine gaps in understanding complex visual narratives, and\nevaluates robustness against natural adversarial questions. Video-TT comprises\n1,000 YouTube Shorts videos, each with one open-ended question and four\nadversarial questions that probe visual and narrative complexity. Our\nevaluation shows a significant gap between video LLMs and human performance.", "AI": {"tldr": "Video-TT \u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u89c6\u89c9\u53d9\u4e8b\u548c\u5bf9\u6297\u6027\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u5176\u4e0e\u4eba\u7c7b\u667a\u80fd\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5145\u5206\u53cd\u6620\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4fdd\u6301\u6b63\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4e0e\u4eba\u7c7b\u667a\u80fd\u7684\u5dee\u8ddd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86 Video-TT \u6d4b\u8bd5\uff0c\u5305\u542b 1,000 \u4e2a YouTube Shorts \u89c6\u9891\uff0c\u6bcf\u4e2a\u89c6\u9891\u914d\u6709\u4e00\u4e2a\u5f00\u653e\u5f0f\u95ee\u9898\u548c\u56db\u4e2a\u5bf9\u6297\u6027\u95ee\u9898\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u5728\u89c6\u89c9\u548c\u53d9\u4e8b\u590d\u6742\u6027\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728 Video-TT \u4e0a\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "Video-TT \u662f\u4e00\u4e2a\u6709\u6548\u7684\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u590d\u6742\u89c6\u89c9\u53d9\u4e8b\u548c\u5bf9\u6297\u6027\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002"}}
{"id": "2507.15035", "categories": ["cs.CV", "cs.LG", "35Q92, 68U10", "I.4.5; J.2; J.3"], "pdf": "https://arxiv.org/pdf/2507.15035", "abs": "https://arxiv.org/abs/2507.15035", "authors": ["Zhijun Zeng", "Youjia Zheng", "Hao Hu", "Zeyuan Dong", "Yihang Zheng", "Xinliang Liu", "Jinzhuo Wang", "Zuoqiang Shi", "Linfeng Zhang", "Yubing Li", "He Sun"], "title": "OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography", "comment": null, "summary": "Accurate and efficient simulation of wave equations is crucial in\ncomputational wave imaging applications, such as ultrasound computed tomography\n(USCT), which reconstructs tissue material properties from observed scattered\nwaves. Traditional numerical solvers for wave equations are computationally\nintensive and often unstable, limiting their practical applications for\nquasi-real-time image reconstruction. Neural operators offer an innovative\napproach by accelerating PDE solving using neural networks; however, their\neffectiveness in realistic imaging is limited because existing datasets\noversimplify real-world complexity. In this paper, we present OpenBreastUS, a\nlarge-scale wave equation dataset designed to bridge the gap between\ntheoretical equations and practical imaging applications. OpenBreastUS includes\n8,000 anatomically realistic human breast phantoms and over 16 million\nfrequency-domain wave simulations using real USCT configurations. It enables a\ncomprehensive benchmarking of popular neural operators for both forward\nsimulation and inverse imaging tasks, allowing analysis of their performance,\nscalability, and generalization capabilities. By offering a realistic and\nextensive dataset, OpenBreastUS not only serves as a platform for developing\ninnovative neural PDE solvers but also facilitates their deployment in\nreal-world medical imaging problems. For the first time, we demonstrate\nefficient in vivo imaging of the human breast using neural operator solvers.", "AI": {"tldr": "OpenBreastUS\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u6ce2\u65b9\u7a0b\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u795e\u7ecf\u7b97\u5b50\u7684\u6027\u80fd\uff0c\u9996\u6b21\u5c55\u793a\u4e86\u5176\u5728\u6d3b\u4f53\u4e73\u817a\u6210\u50cf\u4e2d\u7684\u9ad8\u6548\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u6ce2\u65b9\u7a0b\u6570\u503c\u6c42\u89e3\u5668\u8ba1\u7b97\u91cf\u5927\u4e14\u4e0d\u7a33\u5b9a\uff0c\u9650\u5236\u4e86\u51c6\u5b9e\u65f6\u6210\u50cf\u7684\u5e94\u7528\uff1b\u73b0\u6709\u795e\u7ecf\u7b97\u5b50\u6570\u636e\u96c6\u8fc7\u4e8e\u7b80\u5316\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u6210\u50cf\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86OpenBreastUS\u6570\u636e\u96c6\uff0c\u5305\u542b8000\u4e2a\u89e3\u5256\u5b66\u4e0a\u771f\u5b9e\u7684\u4eba\u7c7b\u4e73\u817a\u6a21\u578b\u548c1600\u4e07\u6b21\u9891\u57df\u6ce2\u6a21\u62df\uff0c\u7528\u4e8e\u8bc4\u4f30\u795e\u7ecf\u7b97\u5b50\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "OpenBreastUS\u4e3a\u795e\u7ecf\u7b97\u5b50\u7684\u5f00\u53d1\u548c\u5b9e\u9645\u533b\u5b66\u6210\u50cf\u95ee\u9898\u90e8\u7f72\u63d0\u4f9b\u4e86\u5e73\u53f0\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u795e\u7ecf\u7b97\u5b50\u5728\u4eba\u4f53\u4e73\u817a\u6d3b\u4f53\u6210\u50cf\u4e2d\u7684\u9ad8\u6548\u5e94\u7528\u3002", "conclusion": "OpenBreastUS\u6570\u636e\u96c6\u4e3a\u795e\u7ecf\u7b97\u5b50\u63d0\u4f9b\u4e86\u73b0\u5b9e\u4e14\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u9996\u6b21\u5c55\u793a\u4e86\u5176\u5728\u4eba\u4f53\u4e73\u817a\u6d3b\u4f53\u6210\u50cf\u4e2d\u7684\u9ad8\u6548\u5e94\u7528\u3002"}}
{"id": "2507.15037", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15037", "abs": "https://arxiv.org/abs/2507.15037", "authors": ["Zhaotong Yang", "Yuhui Li", "Shengfeng He", "Xinzhe Li", "Yangyang Xu", "Junyu Dong", "Yong Du"], "title": "OmniVTON: Training-Free Universal Virtual Try-On", "comment": "Accepted by ICCV2025", "summary": "Image-based Virtual Try-On (VTON) techniques rely on either supervised\nin-shop approaches, which ensure high fidelity but struggle with cross-domain\ngeneralization, or unsupervised in-the-wild methods, which improve adaptability\nbut remain constrained by data biases and limited universality. A unified,\ntraining-free solution that works across both scenarios remains an open\nchallenge. We propose OmniVTON, the first training-free universal VTON\nframework that decouples garment and pose conditioning to achieve both texture\nfidelity and pose consistency across diverse settings. To preserve garment\ndetails, we introduce a garment prior generation mechanism that aligns clothing\nwith the body, followed by continuous boundary stitching technique to achieve\nfine-grained texture retention. For precise pose alignment, we utilize DDIM\ninversion to capture structural cues while suppressing texture interference,\nensuring accurate body alignment independent of the original image textures. By\ndisentangling garment and pose constraints, OmniVTON eliminates the bias\ninherent in diffusion models when handling multiple conditions simultaneously.\nExperimental results demonstrate that OmniVTON achieves superior performance\nacross diverse datasets, garment types, and application scenarios. Notably, it\nis the first framework capable of multi-human VTON, enabling realistic garment\ntransfer across multiple individuals in a single scene. Code is available at\nhttps://github.com/Jerome-Young/OmniVTON", "AI": {"tldr": "OmniVTON\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u901a\u7528\u865a\u62df\u8bd5\u7a7f\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u670d\u88c5\u548c\u59ff\u6001\u6761\u4ef6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u548c\u591a\u4eba\u7269\u8bd5\u7a7f\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u5f0f\u548c\u975e\u76d1\u7763\u5f0fVTON\u65b9\u6cd5\u5728\u8de8\u57df\u6cdb\u5316\u548c\u6570\u636e\u504f\u5dee\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u901a\u7528VTON\u6846\u67b6\uff0c\u7ed3\u5408\u670d\u88c5\u5148\u9a8c\u751f\u6210\u673a\u5236\u548c\u8fde\u7eed\u8fb9\u754c\u7f1d\u5408\u6280\u672f\uff0c\u4ee5\u53caDDIM\u53cd\u6f14\u5b9e\u73b0\u7cbe\u786e\u59ff\u6001\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eOmniVTON\u5728\u591a\u79cd\u6570\u636e\u96c6\u3001\u670d\u88c5\u7c7b\u578b\u548c\u5e94\u7528\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u591a\u4eba\u7269\u865a\u62df\u8bd5\u7a7f\u3002", "conclusion": "OmniVTON\u901a\u8fc7\u89e3\u8026\u670d\u88c5\u548c\u59ff\u6001\u6761\u4ef6\uff0c\u5b9e\u73b0\u4e86\u8de8\u573a\u666f\u7684\u9ad8\u4fdd\u771f\u865a\u62df\u8bd5\u7a7f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4eba\u7269\u8bd5\u7a7f\u4e2d\u5c55\u73b0\u4e86\u9996\u6b21\u5e94\u7528\u80fd\u529b\u3002"}}
{"id": "2507.15059", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15059", "abs": "https://arxiv.org/abs/2507.15059", "authors": ["Ran Zhang", "Xuanhua He", "Li Xueheng", "Ke Cao", "Liu Liu", "Wenbo Xu", "Fang Jiabin", "Yang Qize", "Jie Zhang"], "title": "Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling", "comment": null, "summary": "The field of pan-sharpening has recently seen a trend towards increasingly\nlarge and complex models, often trained on single, specific satellite datasets.\nThis approach, however, leads to high computational overhead and poor\ngeneralization on full resolution data, a paradigm we challenge in this paper.\nIn response to this issue, we propose PanTiny, a lightweight, single-step\npan-sharpening framework designed for both efficiency and robust performance.\nMore critically, we introduce multiple-in-one training paradigm, where a\nsingle, compact model is trained simultaneously on three distinct satellite\ndatasets (WV2, WV3, and GF2) with different resolution and spectral\ninformation. Our experiments show that this unified training strategy not only\nsimplifies deployment but also significantly boosts generalization on\nfull-resolution data. Further, we introduce a universally powerful composite\nloss function that elevates the performance of almost all of models for\npan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny\nmodel, benefiting from these innovations, achieves a superior\nperformance-to-efficiency balance, outperforming most larger, specialized\nmodels. Through extensive ablation studies, we validate that principled\nengineering in model design, training paradigms, and loss functions can surpass\nbrute-force scaling. Our work advocates for a community-wide shift towards\ncreating efficient, generalizable, and data-conscious models for\npan-sharpening. The code is available at\nhttps://github.com/Zirconium233/PanTiny .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8f7b\u91cf\u7ea7pan-sharpening\u6846\u67b6PanTiny\uff0c\u901a\u8fc7\u591a\u6570\u636e\u96c6\u8054\u5408\u8bad\u7ec3\u548c\u590d\u5408\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u6548\u7387\uff0c\u4f18\u4e8e\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u5f53\u524dpan-sharpening\u9886\u57df\u503e\u5411\u4e8e\u4f7f\u7528\u5927\u578b\u590d\u6742\u6a21\u578b\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u6311\u6218\u8fd9\u4e00\u8303\u5f0f\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPanTiny\u6846\u67b6\uff0c\u91c7\u7528\u591a\u6570\u636e\u96c6\uff08WV2\u3001WV3\u3001GF2\uff09\u8054\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u901a\u7528\u7684\u590d\u5408\u635f\u5931\u51fd\u6570\u3002", "result": "PanTiny\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u5927\u591a\u6570\u5927\u578b\u4e13\u7528\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u8bbe\u8ba1\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u635f\u5931\u51fd\u6570\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5355\u6b65\u7684pan-sharpening\u6846\u67b6PanTiny\uff0c\u901a\u8fc7\u591a\u6570\u636e\u96c6\u8054\u5408\u8bad\u7ec3\u548c\u590d\u5408\u635f\u5931\u51fd\u6570\u7684\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u6548\u7387\u5e73\u8861\uff0c\u4e3apan-sharpening\u9886\u57df\u7684\u9ad8\u6548\u3001\u901a\u7528\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.15085", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15085", "abs": "https://arxiv.org/abs/2507.15085", "authors": ["Peirong Zhang", "Haowei Xu", "Jiaxin Zhang", "Guitao Xu", "Xuhan Zheng", "Zhenhua Yang", "Junle Liu", "Yuyi Zhang", "Lianwen Jin"], "title": "Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR", "comment": null, "summary": "Text image is a unique and crucial information medium that integrates visual\naesthetics and linguistic semantics in modern e-society. Due to their subtlety\nand complexity, the generation of text images represents a challenging and\nevolving frontier in the image generation field. The recent surge of\nspecialized image generators (\\emph{e.g.}, Flux-series) and unified generative\nmodels (\\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a\nnatural question: can they master the intricacies of text image generation and\nediting? Motivated by this, we assess current state-of-the-art generative\nmodels' capabilities in terms of text image generation and editing. We\nincorporate various typical optical character recognition (OCR) tasks into our\nevaluation and broaden the concept of text-based generation tasks into OCR\ngenerative tasks. We select 33 representative tasks and categorize them into\nfive categories: document, handwritten text, scene text, artistic text, and\ncomplex \\& layout-rich text. For comprehensive evaluation, we examine six\nmodels across both closed-source and open-source domains, using tailored,\nhigh-quality image inputs and prompts. Through this evaluation, we draw crucial\nobservations and identify the weaknesses of current generative models for OCR\ntasks. We argue that photorealistic text image generation and editing should be\ninternalized as foundational skills into general-domain generative models,\nrather than being delegated to specialized solutions, and we hope this\nempirical analysis can provide valuable insights for the community to achieve\nthis goal. This evaluation is online and will be continuously updated at our\nGitHub repository.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u516d\u79cd\u751f\u6210\u6a21\u578b\u5728\u6587\u672c\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u5176\u5f31\u70b9\uff0c\u5e76\u5efa\u8bae\u5c06\u6b64\u7c7b\u4efb\u52a1\u4f5c\u4e3a\u901a\u7528\u751f\u6210\u6a21\u578b\u7684\u57fa\u7840\u6280\u80fd\u3002", "motivation": "\u63a2\u8ba8\u5f53\u524d\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u662f\u5426\u80fd\u638c\u63e1\u6587\u672c\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u7684\u590d\u6742\u6027\uff0c\u5e76\u8bc4\u4f30\u5176\u5728OCR\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u8bc4\u4f30\u4e86\u516d\u79cd\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\uff08\u5305\u62ec\u95ed\u6e90\u548c\u5f00\u6e90\u9886\u57df\uff09\uff0c\u4f7f\u752833\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\uff0c\u5206\u4e3a\u4e94\u7c7b\uff1a\u6587\u6863\u3001\u624b\u5199\u6587\u672c\u3001\u573a\u666f\u6587\u672c\u3001\u827a\u672f\u6587\u672c\u53ca\u590d\u6742\u4e0e\u5e03\u5c40\u4e30\u5bcc\u7684\u6587\u672c\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u63ed\u793a\u4e86\u5f53\u524d\u751f\u6210\u6a21\u578b\u5728OCR\u4efb\u52a1\u4e2d\u7684\u5f31\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u5173\u952e\u89c2\u5bdf\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\uff0c\u5c06\u903c\u771f\u7684\u6587\u672c\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4f5c\u4e3a\u901a\u7528\u751f\u6210\u6a21\u578b\u7684\u57fa\u7840\u6280\u80fd\uff0c\u800c\u975e\u4f9d\u8d56\u4e13\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u662f\u672a\u6765\u53d1\u5c55\u7684\u65b9\u5411\u3002"}}
{"id": "2507.15109", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15109", "abs": "https://arxiv.org/abs/2507.15109", "authors": ["Mohammad-Maher Nakshbandi", "Ziad Sharawy", "Sorin Grigorescu"], "title": "LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM", "comment": null, "summary": "One of the main challenges in the Simultaneous Localization and Mapping\n(SLAM) loop closure problem is the recognition of previously visited places. In\nthis work, we tackle the two main problems of real-time SLAM systems: 1) loop\nclosure detection accuracy and 2) real-time computation constraints on the\nembedded hardware. Our LoopNet method is based on a multitasking variant of the\nclassical ResNet architecture, adapted for online retraining on a dynamic\nvisual dataset and optimized for embedded devices. The online retraining is\ndesigned using a few-shot learning approach. The architecture provides both an\nindex into the queried visual dataset, and a measurement of the prediction\nquality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,\nLoopNet surpasses the limitations of handcrafted features and traditional deep\nlearning methods, offering better performance under varying conditions. Code is\navailable at https://github.com/RovisLab/LoopNet. Additinally, we introduce a\nnew loop closure benchmarking dataset, coined LoopDB, which is available at\nhttps://github.com/RovisLab/LoopDB.", "AI": {"tldr": "LoopNet\u901a\u8fc7\u591a\u4efb\u52a1ResNet\u548cDISK\u63cf\u8ff0\u7b26\u63d0\u5347SLAM\u95ed\u73af\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u53d1\u5e03LoopDB\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3SLAM\u95ed\u73af\u68c0\u6d4b\u4e2d\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u8ba1\u7b97\u7ea6\u675f\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1ResNet\u67b6\u6784\uff0c\u652f\u6301\u52a8\u6001\u89c6\u89c9\u6570\u636e\u96c6\u7684\u5728\u7ebf\u91cd\u8bad\u7ec3\uff0c\u4f18\u5316\u5d4c\u5165\u5f0f\u8bbe\u5907\u6027\u80fd\uff0c\u7ed3\u5408DISK\u63cf\u8ff0\u7b26\u66ff\u4ee3\u4f20\u7edf\u624b\u5de5\u7279\u5f81\u3002", "result": "LoopNet\u5728\u591a\u53d8\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u548c\u624b\u5de5\u7279\u5f81\u65b9\u6cd5\u3002", "conclusion": "LoopNet\u901a\u8fc7\u591a\u4efb\u52a1ResNet\u67b6\u6784\u548c\u5728\u7ebf\u52a8\u6001\u91cd\u8bad\u7ec3\uff0c\u7ed3\u5408DISK\u63cf\u8ff0\u7b26\uff0c\u663e\u8457\u63d0\u5347\u4e86SLAM\u95ed\u73af\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6LoopDB\u3002"}}
{"id": "2507.15130", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15130", "abs": "https://arxiv.org/abs/2507.15130", "authors": ["Ce Zhang", "Yale Song", "Ruta Desai", "Michael Louis Iuzzolino", "Joseph Tighe", "Gedas Bertasius", "Satwik Kottur"], "title": "Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction", "comment": null, "summary": "Visual Planning for Assistance (VPA) aims to predict a sequence of user\nactions required to achieve a specified goal based on a video showing the\nuser's progress. Although recent advances in multimodal large language models\n(MLLMs) have shown promising results in video understanding, long-horizon\nvisual planning remains a challenging problem. We identify two challenges in\ntraining large MLLMs for video-based planning tasks: (1) scarcity of procedural\nannotations, limiting the model's ability to learn procedural task dynamics\neffectively, and (2) inefficiency of next-token prediction objective to\nexplicitly capture the structured action space for visual planning when\ncompared to free-form, natural language. To tackle data scarcity, we introduce\nAuxiliary Task Augmentation. We design and train our model on auxiliary tasks\nrelevant to long-horizon video-based planning (e.g., goal prediction) to\naugment the model's planning ability. To more explicitly model the structured\naction space unique to visual planning tasks, we leverage Multi-token\nPrediction, extending traditional next-token prediction by using multiple heads\nto predict multiple future tokens during training. Our approach, VideoPlan,\nachieves state-of-the-art VPA performance on the COIN and CrossTask datasets,\nsurpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3\nfuture actions. We further extend our method to the challenging Ego4D Long-term\nAction Anticipation task, and show that it is on par with the state-of-the-art\napproaches despite not using specialized egocentric features. Code will be made\navailable.", "AI": {"tldr": "VideoPlan\u901a\u8fc7\u8f85\u52a9\u4efb\u52a1\u589e\u5f3a\u548c\u591a\u4ee4\u724c\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u89c4\u5212\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u52a8\u4f5c\u7a7a\u95f4\u5efa\u6a21\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u9891\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u957f\u671f\u89c6\u89c9\u89c4\u5212\u4ecd\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8e\u7a0b\u5e8f\u6027\u6ce8\u91ca\u7684\u7a00\u7f3a\u6027\u548c\u4f20\u7edf\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u76ee\u6807\u5728\u6355\u6349\u7ed3\u6784\u5316\u52a8\u4f5c\u7a7a\u95f4\u65f6\u7684\u4f4e\u6548\u6027\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4f5c\u8005\u5f15\u5165\u4e86\u8f85\u52a9\u4efb\u52a1\u589e\u5f3a\uff08Auxiliary Task Augmentation\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u591a\u4ee4\u724c\u9884\u6d4b\uff08Multi-token Prediction\uff09\u6765\u66f4\u660e\u786e\u5730\u5efa\u6a21\u89c6\u89c9\u89c4\u5212\u4efb\u52a1\u7684\u7ed3\u6784\u5316\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "VideoPlan\u5728COIN\u548cCrossTask\u6570\u636e\u96c6\u4e0a\u9884\u6d4b3\u4e2a\u672a\u6765\u52a8\u4f5c\u65f6\uff0c\u6027\u80fd\u5206\u522b\u63d0\u5347\u4e867.3%\u548c3.4%\uff0c\u5e76\u5728Ego4D\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VideoPlan\u65b9\u6cd5\u5728COIN\u548cCrossTask\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684VPA\u6027\u80fd\uff0c\u5206\u522b\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e867.3%\u548c3.4%\uff0c\u5e76\u4e14\u5728Ego4D\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\u3002"}}
{"id": "2507.15150", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15150", "abs": "https://arxiv.org/abs/2507.15150", "authors": ["Aayush Atul Verma", "Arpitsinh Vaghela", "Bharatesh Chakravarthi", "Kaustav Chanda", "Yezhou Yang"], "title": "Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection", "comment": null, "summary": "Event-based sensors offer high temporal resolution and low latency by\ngenerating sparse, asynchronous data. However, converting this irregular data\ninto dense tensors for use in standard neural networks diminishes these\ninherent advantages, motivating research into graph representations. While such\nmethods preserve sparsity and support asynchronous inference, their performance\non downstream tasks remains limited due to suboptimal modeling of\nspatiotemporal dynamics. In this work, we propose a novel spatiotemporal\nmultigraph representation to better capture spatial structure and temporal\nchanges. Our approach constructs two decoupled graphs: a spatial graph\nleveraging B-spline basis functions to model global structure, and a temporal\ngraph utilizing motion vector-based attention for local dynamic changes. This\ndesign enables the use of efficient 2D kernels in place of computationally\nexpensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM\ndatasets for event-based object detection, achieving over a 6% improvement in\ndetection accuracy compared to previous graph-based works, with a 5x speedup,\nreduced parameter count, and no increase in computational cost. These results\nhighlight the effectiveness of structured graph modeling for asynchronous\nvision. Project page: eventbasedvision.github.io/eGSMV.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65f6\u7a7a\u591a\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u7a7a\u95f4\u548c\u65f6\u95f4\u56fe\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e8b\u4ef6\u4f20\u611f\u5668\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u4e8b\u4ef6\u4f20\u611f\u5668\u751f\u6210\u7a00\u758f\u5f02\u6b65\u6570\u636e\uff0c\u4f20\u7edf\u65b9\u6cd5\u5c06\u5176\u8f6c\u6362\u4e3a\u5bc6\u96c6\u5f20\u91cf\u4f1a\u524a\u5f31\u5176\u56fa\u6709\u4f18\u52bf\u3002\u73b0\u6709\u56fe\u8868\u793a\u65b9\u6cd5\u867d\u4fdd\u7559\u7a00\u758f\u6027\uff0c\u4f46\u5bf9\u65f6\u7a7a\u52a8\u6001\u5efa\u6a21\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u7a7a\u591a\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u5305\u62ec\u5229\u7528B\u6837\u6761\u57fa\u51fd\u6570\u5efa\u6a21\u5168\u5c40\u7a7a\u95f4\u7ed3\u6784\u7684\u7a7a\u95f4\u56fe\u548c\u57fa\u4e8e\u8fd0\u52a8\u5411\u91cf\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u5c40\u90e8\u52a8\u6001\u53d8\u5316\u7684\u65f6\u95f4\u56fe\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u7684\u9ad8\u8ba1\u7b97\u6210\u672c3D\u6838\u3002", "result": "\u5728Gen1 automotive\u548ceTraM\u6570\u636e\u96c6\u4e0a\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u63d0\u5347\u8d85\u8fc76%\uff0c\u901f\u5ea6\u63d0\u53475\u500d\uff0c\u53c2\u6570\u51cf\u5c11\u4e14\u8ba1\u7b97\u6210\u672c\u672a\u589e\u52a0\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u7a7a\u591a\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u7a7a\u95f4\u548c\u65f6\u95f4\u56fe\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u4f20\u611f\u5668\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u68c0\u6d4b\u7cbe\u5ea6\u3001\u901f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u6709\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2507.15212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15212", "abs": "https://arxiv.org/abs/2507.15212", "authors": ["Yusuke Yoshiyasu", "Leyuan Sun", "Ryusuke Sagawa"], "title": "MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction", "comment": "Accepted at ICCV2025", "summary": "In this paper, we introduce MeshMamba, a neural network model for learning 3D\narticulated mesh models by employing the recently proposed Mamba State Space\nModels (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large\nnumber of input tokens, enabling the generation and reconstruction of body mesh\nmodels with more than 10,000 vertices, capturing clothing and hand geometries.\nThe key to effectively learning MeshMamba is the serialization technique of\nmesh vertices into orderings that are easily processed by Mamba. This is\nachieved by sorting the vertices based on body part annotations or the 3D\nvertex locations of a template mesh, such that the ordering respects the\nstructure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,\na denoising diffusion model for generating 3D articulated meshes and 2)\nMamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape\nand pose from a single image. Experimental results showed that MambaDiff3D can\ngenerate dense 3D human meshes in clothes, with grasping hands, etc., and\noutperforms previous approaches in the 3D human shape generation task.\nAdditionally, Mamba-HMR extends the capabilities of previous non-parametric\nhuman mesh recovery approaches, which were limited to handling body-only poses\nusing around 500 vertex tokens, to the whole-body setting with face and hands,\nwhile achieving competitive performance in (near) real-time.", "AI": {"tldr": "MeshMamba \u5229\u7528 Mamba-SSMs \u9ad8\u6548\u5b66\u4e60 3D \u5173\u8282\u7f51\u683c\uff0c\u5176\u884d\u751f\u6a21\u578b MambaDiff3D \u548c Mamba-HMR \u5206\u522b\u5728\u751f\u6210\u548c\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u5927\u89c4\u6a21\u9876\u70b9\u548c\u7ec6\u8282\u6355\u6349\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21 3D \u5173\u8282\u7f51\u683c\u65f6\u6548\u7387\u4e0d\u8db3\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u6355\u6349\u8863\u7269\u548c\u624b\u90e8\u51e0\u4f55\u7ec6\u8282\u3002MeshMamba \u65e8\u5728\u901a\u8fc7 Mamba-SSMs \u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MeshMamba \u901a\u8fc7\u5c06\u7f51\u683c\u9876\u70b9\u5e8f\u5217\u5316\u4e3a\u6613\u4e8e Mamba \u5904\u7406\u7684\u987a\u5e8f\uff08\u57fa\u4e8e\u8eab\u4f53\u90e8\u4f4d\u6ce8\u91ca\u6216\u6a21\u677f\u7f51\u683c\u7684 3D \u9876\u70b9\u4f4d\u7f6e\uff09\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5927\u89c4\u6a21\u8f93\u5165\u7684\u9ad8\u6548\u5904\u7406\u3002\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86 MambaDiff3D\uff08\u53bb\u566a\u6269\u6563\u6a21\u578b\uff09\u548c Mamba-HMR\uff08\u5355\u56fe\u50cf\u4eba\u4f53\u7f51\u683c\u6062\u590d\u6a21\u578b\uff09\u3002", "result": "MambaDiff3D \u80fd\u591f\u751f\u6210\u5e26\u8863\u7269\u548c\u624b\u90e8\u7ec6\u8282\u7684\u5bc6\u96c6 3D \u4eba\u4f53\u7f51\u683c\uff0c\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1bMamba-HMR \u6269\u5c55\u4e86\u975e\u53c2\u6570\u5316\u4eba\u4f53\u7f51\u683c\u6062\u590d\u7684\u80fd\u529b\uff0c\u652f\u6301\u5168\u8eab\uff08\u5305\u62ec\u9762\u90e8\u548c\u624b\u90e8\uff09\u91cd\u5efa\uff0c\u5e76\u5b9e\u73b0\u63a5\u8fd1\u5b9e\u65f6\u7684\u6027\u80fd\u3002", "conclusion": "MeshMamba \u901a\u8fc7 Mamba-SSMs \u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684 3D \u5173\u8282\u7f51\u683c\u6a21\u578b\u5b66\u4e60\uff0c\u5176\u884d\u751f\u7684 MambaDiff3D \u548c Mamba-HMR \u5206\u522b\u5728 3D \u4eba\u4f53\u7f51\u683c\u751f\u6210\u548c\u5355\u56fe\u50cf\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.15216", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15216", "abs": "https://arxiv.org/abs/2507.15216", "authors": ["Yuping Qiu", "Rui Zhu", "Ying-cong Chen"], "title": "Improving Joint Embedding Predictive Architecture with Diffusion Noise", "comment": null, "summary": "Self-supervised learning has become an incredibly successful method for\nfeature learning, widely applied to many downstream tasks. It has proven\nespecially effective for discriminative tasks, surpassing the trending\ngenerative models. However, generative models perform better in image\ngeneration and detail enhancement. Thus, it is natural for us to find a\nconnection between SSL and generative models to further enhance the\nrepresentation capacity of SSL. As generative models can create new samples by\napproximating the data distribution, such modeling should also lead to a\nsemantic understanding of the raw visual data, which is necessary for\nrecognition tasks. This enlightens us to combine the core principle of the\ndiffusion model: diffusion noise, with SSL to learn a competitive recognition\nmodel. Specifically, diffusion noise can be viewed as a particular state of\nmask that reveals a close relationship between masked image modeling (MIM) and\ndiffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to\nincorporate diffusion noise into MIM by the position embedding of masked\ntokens. The multi-level noise schedule is a series of feature augmentations to\nfurther enhance the robustness of our model. We perform a comprehensive study\nto confirm its effectiveness in the classification of downstream tasks. Codes\nwill be released soon in public.", "AI": {"tldr": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u566a\u58f0\u4e0e\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u63d0\u51faN-JEPA\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u589e\u5f3aMIM\uff0c\u63d0\u5347\u5206\u7c7b\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u4e0e\u751f\u6210\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u4ee5\u589e\u5f3aSSL\u7684\u8868\u5f81\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u3002", "method": "\u63d0\u51faN-JEPA\u65b9\u6cd5\uff0c\u5c06\u6269\u6563\u566a\u58f0\u901a\u8fc7\u4f4d\u7f6e\u5d4c\u5165\u878d\u5165\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\uff08MIM\uff09\uff0c\u5e76\u91c7\u7528\u591a\u7ea7\u566a\u58f0\u8c03\u5ea6\u4f5c\u4e3a\u7279\u5f81\u589e\u5f3a\u624b\u6bb5\u3002", "result": "\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86N-JEPA\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u5176\u80fd\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u7684\u566a\u58f0\u539f\u7406\u4e0e\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\uff0c\u63d0\u51fa\u4e86N-JEPA\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86SSL\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.15223", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15223", "abs": "https://arxiv.org/abs/2507.15223", "authors": ["Siqi Chen", "Guoqing Zhang", "Jiahao Lai", "Bingzhi Shen", "Sihong Zhang", "Caixia Dong", "Xuejin Chen", "Yang Li"], "title": "Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel", "comment": null, "summary": "Advancements in 3D vision have increased the impact of blood vessel modeling\non medical applications. However, accurately representing the complex geometry\nand topology of blood vessels remains a challenge due to their intricate\nbranching patterns, curvatures, and irregular shapes. In this study, we propose\na hierarchical part-based frame work for 3D vessel generation that separates\nthe global binary tree-like topology from local geometric details. Our approach\nproceeds in three stages: (1) key graph generation to model the overall\nhierarchical struc ture, (2) vessel segment generation conditioned on geometric\nproperties, and (3) hierarchical vessel assembly by integrating the local\nsegments according to the global key graph. We validate our framework on real\nworld datasets, demonstrating superior performance over existing methods in\nmodeling complex vascular networks. This work marks the first successful\napplication of a part-based generative approach for 3D vessel modeling, setting\na new benchmark for vascular data generation. The code is available at:\nhttps://github.com/CybercatChen/PartVessel.git.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u90e8\u5206\u6846\u67b6\uff0c\u7528\u4e8e3D\u8840\u7ba1\u751f\u6210\uff0c\u901a\u8fc7\u5206\u79bb\u5168\u5c40\u62d3\u6251\u548c\u5c40\u90e8\u51e0\u4f55\u7ec6\u8282\uff0c\u6709\u6548\u6a21\u62df\u590d\u6742\u8840\u7ba1\u7f51\u7edc\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba13D\u89c6\u89c9\u6280\u672f\u7684\u8fdb\u6b65\u589e\u5f3a\u4e86\u8840\u7ba1\u5efa\u6a21\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u7684\u5f71\u54cd\u529b\uff0c\u4f46\u7531\u4e8e\u8840\u7ba1\u5177\u6709\u590d\u6742\u7684\u51e0\u4f55\u548c\u62d3\u6251\u7ed3\u6784\uff0c\u51c6\u786e\u8868\u793a\u5176\u5206\u652f\u6a21\u5f0f\u3001\u66f2\u7387\u548c\u4e0d\u89c4\u5219\u5f62\u72b6\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u90e8\u5206\u6846\u67b6\uff0c\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u5173\u952e\u56fe\u751f\u6210\u3001\u57fa\u4e8e\u51e0\u4f55\u7279\u6027\u7684\u8840\u7ba1\u6bb5\u751f\u6210\u4ee5\u53ca\u6839\u636e\u5168\u5c40\u5173\u952e\u56fe\u6574\u5408\u5c40\u90e8\u6bb5\u7684\u5206\u5c42\u8840\u7ba1\u7ec4\u88c5\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u590d\u6742\u8840\u7ba1\u7f51\u7edc\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u5206\u5c42\u90e8\u5206\u6846\u67b6\u57283D\u8840\u7ba1\u5efa\u6a21\u4e2d\u9996\u6b21\u6210\u529f\u5e94\u7528\u4e86\u57fa\u4e8e\u90e8\u5206\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u4e3a\u8840\u7ba1\u6570\u636e\u751f\u6210\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2507.15227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15227", "abs": "https://arxiv.org/abs/2507.15227", "authors": ["Krishna Kanth Nakka"], "title": "Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders", "comment": "Preprint. Under review", "summary": "Interpretability is critical in high-stakes domains such as medical imaging,\nwhere understanding model decisions is essential for clinical adoption. In this\nwork, we introduce Sparse Autoencoder (SAE)-based interpretability to breast\nimaging by analyzing {Mammo-CLIP}, a vision--language foundation model\npretrained on large-scale mammogram image--report pairs. We train a patch-level\n\\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features\nassociated with clinically relevant breast concepts such as \\textit{mass} and\n\\textit{suspicious calcification}. Our findings reveal that top activated class\nlevel latent neurons in the SAE latent space often tend to align with ground\ntruth regions, and also uncover several confounding factors influencing the\nmodel's decision-making process. Additionally, we analyze which latent neurons\nthe model relies on during downstream finetuning for improving the breast\nconcept prediction. This study highlights the promise of interpretable SAE\nlatent representations in providing deeper insight into the internal workings\nof foundation models at every layer for breast imaging.", "AI": {"tldr": "\u901a\u8fc7\u7a00\u758f\u81ea\u52a8\u7f16\u7801\u5668\uff08SAE\uff09\u5206\u6790\u4e73\u817a\u5f71\u50cf\u57fa\u7840\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u6f5c\u5728\u7279\u5f81\u4e0e\u4e34\u5e8a\u6982\u5ff5\u7684\u5173\u8054\uff0c\u5e76\u8bc6\u522b\u4e86\u5f71\u54cd\u51b3\u7b56\u7684\u6df7\u6742\u56e0\u7d20\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u9886\u57df\u5982\u533b\u5b66\u5f71\u50cf\u4e2d\uff0c\u6a21\u578b\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u5bf9\u4e34\u5e8a\u91c7\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u57fa\u4e8eMammo-CLIP\u7684Mammo-SAE\uff0c\u8bc6\u522b\u548c\u63a2\u6d4b\u4e0e\u4e34\u5e8a\u76f8\u5173\u4e73\u817a\u6982\u5ff5\uff08\u5982\u80bf\u5757\u548c\u53ef\u7591\u9499\u5316\uff09\u76f8\u5173\u7684\u6f5c\u5728\u7279\u5f81\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cSAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6fc0\u6d3b\u7a0b\u5ea6\u6700\u9ad8\u7684\u795e\u7ecf\u5143\u901a\u5e38\u4e0e\u771f\u5b9e\u533a\u57df\u5bf9\u9f50\uff0c\u5e76\u63ed\u793a\u4e86\u5f71\u54cd\u6a21\u578b\u51b3\u7b56\u7684\u591a\u4e2a\u6df7\u6742\u56e0\u7d20\u3002\u6b64\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u6a21\u578b\u5728\u4e0b\u6e38\u5fae\u8c03\u4e2d\u4f9d\u8d56\u7684\u6f5c\u5728\u795e\u7ecf\u5143\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u7a00\u758f\u81ea\u52a8\u7f16\u7801\u5668\uff08SAE\uff09\u5728\u4e73\u817a\u5f71\u50cf\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u6f5c\u529b\uff0c\u4e3a\u7406\u89e3\u57fa\u7840\u6a21\u578b\u5728\u4e73\u817a\u5f71\u50cf\u4e2d\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.15249", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15249", "abs": "https://arxiv.org/abs/2507.15249", "authors": ["Yanbing Zhang", "Zhe Wang", "Qin Zhou", "Mengping Yang"], "title": "FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers", "comment": "Accepted by ICCV 2025", "summary": "In light of recent breakthroughs in text-to-image (T2I) generation,\nparticularly with diffusion transformers (DiT), subject-driven technologies are\nincreasingly being employed for high-fidelity customized production that\npreserves subject identity from reference inputs, enabling thrilling design\nworkflows and engaging entertainment. Existing alternatives typically require\neither per-subject optimization via trainable text embeddings or training\nspecialized encoders for subject feature extraction on large-scale datasets.\nSuch dependencies on training procedures fundamentally constrain their\npractical applications. More importantly, current methodologies fail to fully\nleverage the inherent zero-shot potential of modern diffusion transformers\n(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this\ngap, we propose FreeCus, a genuinely training-free framework that activates\nDiT's capabilities through three key innovations: 1) We introduce a pivotal\nattention sharing mechanism that captures the subject's layout integrity while\npreserving crucial editing flexibility. 2) Through a straightforward analysis\nof DiT's dynamic shifting, we propose an upgraded variant that significantly\nimproves fine-grained feature extraction. 3) We further integrate advanced\nMultimodal Large Language Models (MLLMs) to enrich cross-modal semantic\nrepresentations. Extensive experiments reflect that our method successfully\nunlocks DiT's zero-shot ability for consistent subject synthesis across diverse\ncontexts, achieving state-of-the-art or comparable results compared to\napproaches that require additional training. Notably, our framework\ndemonstrates seamless compatibility with existing inpainting pipelines and\ncontrol modules, facilitating more compelling experiences. Our code is\navailable at: https://github.com/Monalissaa/FreeCus.", "AI": {"tldr": "FreeCus\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5171\u4eab\u3001\u52a8\u6001\u504f\u79fb\u5206\u6790\u548cMLLMs\u96c6\u6210\uff0c\u6fc0\u6d3b\u6269\u6563\u53d8\u6362\u5668\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u4e3b\u9898\u9a71\u52a8\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u73b0\u4ee3\u6269\u6563\u53d8\u6362\u5668\u7684\u96f6\u6837\u672c\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86FreeCus\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u5173\u952e\u521b\u65b0\uff1a1) \u6ce8\u610f\u529b\u5171\u4eab\u673a\u5236\uff1b2) \u52a8\u6001\u504f\u79fb\u5206\u6790\u7684\u5347\u7ea7\u53d8\u4f53\uff1b3) \u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFreeCus\u6210\u529f\u89e3\u9501\u4e86DiT\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u8de8\u591a\u6837\u4e0a\u4e0b\u6587\u7684\u4e00\u81f4\u4e3b\u9898\u5408\u6210\uff0c\u5e76\u4e0e\u73b0\u6709\u4fee\u590d\u6d41\u7a0b\u548c\u63a7\u5236\u6a21\u5757\u65e0\u7f1d\u517c\u5bb9\u3002", "conclusion": "FreeCus\u6210\u529f\u6fc0\u6d3b\u4e86\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u9ad8\u4fdd\u771f\u4e3b\u9898\u9a71\u52a8\u5408\u6210\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4e0e\u9700\u8981\u8bad\u7ec3\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u7684\u7ed3\u679c\u3002"}}
{"id": "2507.15257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15257", "abs": "https://arxiv.org/abs/2507.15257", "authors": ["Pei An", "Jiaqi Yang", "Muyao Peng", "You Yang", "Qiong Liu", "Xiaolin Wu", "Liangliang Nan"], "title": "MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP", "comment": "Accepted by ICCV 2025", "summary": "Image-to-point-cloud (I2P) registration is a fundamental problem in computer\nvision, focusing on establishing 2D-3D correspondences between an image and a\npoint cloud. The differential perspective-n-point (PnP) has been widely used to\nsupervise I2P registration networks by enforcing the projective constraints on\n2D-3D correspondences. However, differential PnP is highly sensitive to noise\nand outliers in the predicted correspondences. This issue hinders the\neffectiveness of correspondence learning. Inspired by the robustness of blind\nPnP against noise and outliers in correspondences, we propose an approximated\nblind PnP based correspondence learning approach. To mitigate the high\ncomputational cost of blind PnP, we simplify blind PnP to an amenable task of\nminimizing Chamfer distance between learned 2D and 3D keypoints, called\nMinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task\nlearning module, named as MinCD-Net, which can be easily integrated into the\nexisting I2P registration architectures. Extensive experiments on 7-Scenes,\nRGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net\noutperforms state-of-the-art methods and achieves a higher inlier ratio (IR)\nand registration recall (RR) in both cross-scene and cross-dataset settings.", "AI": {"tldr": "\u63d0\u51faMinCD-PnP\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5316\u76f2PnP\u5e76\u8bbe\u8ba1MinCD-Net\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86I2P\u6ce8\u518c\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5dee\u5206PnP\u5bf9\u566a\u58f0\u548c\u5f02\u5e38\u503c\u9ad8\u5ea6\u654f\u611f\uff0c\u5f71\u54cd\u4e86\u5bf9\u5e94\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002\u53d7\u76f2PnP\u5728\u566a\u58f0\u548c\u5f02\u5e38\u503c\u4e2d\u7684\u9c81\u68d2\u6027\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u8fd1\u4f3c\u76f2PnP\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd1\u4f3c\u76f2PnP\u7684\u5bf9\u5e94\u5b66\u4e60\u65b9\u6cd5MinCD-PnP\uff0c\u5e76\u901a\u8fc7\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u5757MinCD-Net\u6765\u6709\u6548\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "result": "\u57287-Scenes\u3001RGBD-V2\u3001ScanNet\u548c\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMinCD-Net\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MinCD-Net\u5728\u8de8\u573a\u666f\u548c\u8de8\u6570\u636e\u96c6\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5185\u70b9\u7387\uff08IR\uff09\u548c\u6ce8\u518c\u53ec\u56de\u7387\uff08RR\uff09\u3002"}}
{"id": "2507.15285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15285", "abs": "https://arxiv.org/abs/2507.15285", "authors": ["Lazaro Janier Gonzalez-Soler", "Maciej Salwowski", "Christoph Busch"], "title": "In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems", "comment": "Submitted to IEEE-TIFS", "summary": "Recent advances in biometric systems have significantly improved the\ndetection and prevention of fraudulent activities. However, as detection\nmethods improve, attack techniques become increasingly sophisticated. Attacks\non face recognition systems can be broadly divided into physical and digital\napproaches. Traditionally, deep learning models have been the primary defence\nagainst such attacks. While these models perform exceptionally well in\nscenarios for which they have been trained, they often struggle to adapt to\ndifferent types of attacks or varying environmental conditions. These\nsubsystems require substantial amounts of training data to achieve reliable\nperformance, yet biometric data collection faces significant challenges,\nincluding privacy concerns and the logistical difficulties of capturing diverse\nattack scenarios under controlled conditions. This work investigates the\napplication of Vision Language Models (VLM) and proposes an in-context learning\nframework for detecting physical presentation attacks and digital morphing\nattacks in biometric systems. Focusing on open-source models, the first\nsystematic framework for the quantitative evaluation of VLMs in\nsecurity-critical scenarios through in-context learning techniques is\nestablished. The experimental evaluation conducted on freely available\ndatabases demonstrates that the proposed subsystem achieves competitive\nperformance for physical and digital attack detection, outperforming some of\nthe traditional CNNs without resource-intensive training. The experimental\nresults validate the proposed framework as a promising tool for improving\ngeneralisation in attack detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u653b\u51fb\u68c0\u6d4b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u4e14\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u3002", "motivation": "\u968f\u7740\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u68c0\u6d4b\u65b9\u6cd5\u7684\u8fdb\u6b65\uff0c\u653b\u51fb\u6280\u672f\u4e5f\u65e5\u76ca\u590d\u6742\u3002\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9002\u5e94\u4e0d\u540c\u7c7b\u578b\u653b\u51fb\u6216\u73af\u5883\u53d8\u5316\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22VLM\u5728\u653b\u51fb\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u7269\u7406\u5448\u73b0\u653b\u51fb\u548c\u6570\u5b57\u53d8\u5f62\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u7269\u7406\u548c\u6570\u5b57\u653b\u51fb\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u90e8\u5206\u4f20\u7edfCNN\u6a21\u578b\uff0c\u4e14\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u8d44\u6e90\u3002", "conclusion": "\u672c\u7814\u7a76\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\u5728\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u7269\u7406\u548c\u6570\u5b57\u653b\u51fb\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u65e0\u9700\u8d44\u6e90\u5bc6\u96c6\u578b\u8bad\u7ec3\u3002"}}
{"id": "2507.15297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15297", "abs": "https://arxiv.org/abs/2507.15297", "authors": ["Zhiyu Pan", "Xiongjun Guan", "Yongjie Duan", "Jianjiang Feng", "Jie Zhou"], "title": "Minutiae-Anchored Local Dense Representation for Fingerprint Matching", "comment": "Under review", "summary": "Fingerprint matching under diverse capture conditions remains a fundamental\nchallenge in biometric recognition. To achieve robust and accurate performance\nin such scenarios, we propose DMD, a minutiae-anchored local dense\nrepresentation which captures both fine-grained ridge textures and\ndiscriminative minutiae features in a spatially structured manner.\nSpecifically, descriptors are extracted from local patches centered and\noriented on each detected minutia, forming a three-dimensional tensor, where\ntwo dimensions represent spatial locations on the fingerprint plane and the\nthird encodes semantic features. This representation explicitly captures\nabstract features of local image patches, enabling a multi-level, fine-grained\ndescription that aggregates information from multiple minutiae and their\nsurrounding ridge structures. Furthermore, thanks to its strong spatial\ncorrespondence with the patch image, DMD allows for the use of foreground\nsegmentation masks to identify valid descriptor regions. During matching,\ncomparisons are then restricted to overlapping foreground areas, improving\nefficiency and robustness. Extensive experiments on rolled, plain, parital,\ncontactless, and latent fingerprint datasets demonstrate the effectiveness and\ngeneralizability of the proposed method. It achieves state-of-the-art accuracy\nacross multiple benchmarks while maintaining high computational efficiency,\nshowing strong potential for large-scale fingerprint recognition. Corresponding\ncode is available at https://github.com/Yu-Yy/DMD.", "AI": {"tldr": "DMD\u662f\u4e00\u79cd\u57fa\u4e8e\u7ec6\u8282\u70b9\u951a\u5b9a\u7684\u5c40\u90e8\u5bc6\u96c6\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u7ef4\u5f20\u91cf\u6355\u83b7\u6307\u7eb9\u7684\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u79cd\u6307\u7eb9\u6570\u636e\u96c6\u4e0a\u5177\u6709\u6700\u5148\u8fdb\u7684\u8bc6\u522b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6307\u7eb9\u5339\u914d\u5728\u4e0d\u540c\u91c7\u96c6\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u6311\u6218\u3002", "method": "\u63d0\u51faDMD\uff0c\u4e00\u79cd\u57fa\u4e8e\u7ec6\u8282\u70b9\u7684\u5c40\u90e8\u5bc6\u96c6\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u6bcf\u4e2a\u68c0\u6d4b\u5230\u7684\u7ec6\u8282\u70b9\u4e2d\u5fc3\u63d0\u53d6\u5c40\u90e8\u8865\u4e01\u63cf\u8ff0\u7b26\uff0c\u5f62\u6210\u4e09\u7ef4\u5f20\u91cf\uff0c\u6355\u83b7\u7ec6\u7c92\u5ea6\u810a\u7ebf\u7eb9\u7406\u548c\u7ec6\u8282\u70b9\u7279\u5f81\u3002", "result": "\u5728\u6eda\u52a8\u3001\u5e73\u9762\u3001\u90e8\u5206\u3001\u975e\u63a5\u89e6\u548c\u6f5c\u5728\u6307\u7eb9\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DMD\u65b9\u6cd5\u5728\u591a\u79cd\u6307\u7eb9\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u663e\u793a\u51fa\u5728\u5927\u89c4\u6a21\u6307\u7eb9\u8bc6\u522b\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.15308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15308", "abs": "https://arxiv.org/abs/2507.15308", "authors": ["Zhimeng Xin", "Tianxu Wu", "Yixiong Zou", "Shiming Chen", "Dingjie Fu", "Xinge You"], "title": "Few-Shot Object Detection via Spatial-Channel State Space Model", "comment": null, "summary": "Due to the limited training samples in few-shot object detection (FSOD), we\nobserve that current methods may struggle to accurately extract effective\nfeatures from each channel. Specifically, this issue manifests in two aspects:\ni) channels with high weights may not necessarily be effective, and ii)\nchannels with low weights may still hold significant value. To handle this\nproblem, we consider utilizing the inter-channel correlation to facilitate the\nnovel model's adaptation process to novel conditions, ensuring the model can\ncorrectly highlight effective channels and rectify those incorrect ones. Since\nthe channel sequence is also 1-dimensional, its similarity with the temporal\nsequence inspires us to take Mamba for modeling the correlation in the channel\nsequence. Based on this concept, we propose a Spatial-Channel State Space\nModeling (SCSM) module for spatial-channel state modeling, which highlights the\neffective patterns and rectifies those ineffective ones in feature channels. In\nSCSM, we design the Spatial Feature Modeling (SFM) module to balance the\nlearning of spatial relationships and channel relationships, and then introduce\nthe Channel State Modeling (CSM) module based on Mamba to learn correlation in\nchannels. Extensive experiments on the VOC and COCO datasets show that the SCSM\nmodule enables the novel detector to improve the quality of focused feature\nrepresentation in channels and achieve state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51faSCSM\u6a21\u5757\uff0c\u5229\u7528\u7a7a\u95f4-\u901a\u9053\u72b6\u6001\u5efa\u6a21\u89e3\u51b3FSOD\u4e2d\u7684\u901a\u9053\u7279\u5f81\u63d0\u53d6\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\uff08FSOD\uff09\u4e2d\u7531\u4e8e\u8bad\u7ec3\u6837\u672c\u6709\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u63d0\u53d6\u6709\u6548\u901a\u9053\u7279\u5f81\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7a7a\u95f4-\u901a\u9053\u72b6\u6001\u5efa\u6a21\uff08SCSM\uff09\u6a21\u5757\uff0c\u5305\u62ec\u7528\u4e8e\u7a7a\u95f4\u7279\u5f81\u5efa\u6a21\u7684SFM\u6a21\u5757\u548c\u57fa\u4e8eMamba\u7684CSM\u6a21\u5757\uff0c\u4ee5\u5b66\u4e60\u901a\u9053\u95f4\u7684\u76f8\u5173\u6027\u3002", "result": "SCSM\u6a21\u5757\u80fd\u591f\u6709\u6548\u7a81\u51fa\u7279\u5f81\u901a\u9053\u4e2d\u7684\u6709\u6548\u6a21\u5f0f\u5e76\u7ea0\u6b63\u65e0\u6548\u6a21\u5f0f\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "SCSM\u6a21\u5757\u901a\u8fc7\u5728VOC\u548cCOCO\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u80fd\u591f\u63d0\u5347\u65b0\u68c0\u6d4b\u5668\u5728\u901a\u9053\u4e2d\u805a\u7126\u7279\u5f81\u8868\u793a\u7684\u8d28\u91cf\uff0c\u5e76\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.15321", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15321", "abs": "https://arxiv.org/abs/2507.15321", "authors": ["Zhenyu Li", "Haotong Lin", "Jiashi Feng", "Peter Wonka", "Bingyi Kang"], "title": "BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?", "comment": "Webpage: https://zhyever.github.io/benchdepth", "summary": "Depth estimation is a fundamental task in computer vision with diverse\napplications. Recent advancements in deep learning have led to powerful depth\nfoundation models (DFMs), yet their evaluation remains challenging due to\ninconsistencies in existing protocols. Traditional benchmarks rely on\nalignment-based metrics that introduce biases, favor certain depth\nrepresentations, and complicate fair comparisons. In this work, we propose\nBenchDepth, a new benchmark that evaluates DFMs through five carefully selected\ndownstream proxy tasks: depth completion, stereo matching, monocular\nfeed-forward 3D scene reconstruction, SLAM, and vision-language spatial\nunderstanding. Unlike conventional evaluation protocols, our approach assesses\nDFMs based on their practical utility in real-world applications, bypassing\nproblematic alignment procedures. We benchmark eight state-of-the-art DFMs and\nprovide an in-depth analysis of key findings and observations. We hope our work\nsparks further discussion in the community on best practices for depth model\nevaluation and paves the way for future research and advancements in depth\nestimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86BenchDepth\u57fa\u51c6\uff0c\u901a\u8fc7\u4e94\u4e2a\u4e0b\u6e38\u4efb\u52a1\u8bc4\u4f30\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8bc4\u4f30\u4e2d\u7684\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u516b\u79cdDFMs\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u7684\u8bc4\u4f30\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u4f9d\u8d56\u7684\u57fa\u4e8e\u5bf9\u9f50\u7684\u6307\u6807\u5f15\u5165\u4e86\u504f\u5dee\uff0c\u4e14\u96be\u4ee5\u8fdb\u884c\u516c\u5e73\u6bd4\u8f83\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u9009\u62e9\u7684\u4e94\u4e2a\u4e0b\u6e38\u4ee3\u7406\u4efb\u52a1\uff08\u6df1\u5ea6\u8865\u5168\u3001\u7acb\u4f53\u5339\u914d\u3001\u5355\u76ee\u524d\u99883D\u573a\u666f\u91cd\u5efa\u3001SLAM\u548c\u89c6\u89c9\u8bed\u8a00\u7a7a\u95f4\u7406\u89e3\uff09\u8bc4\u4f30DFMs\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u57fa\u4e8e\u5bf9\u9f50\u7684\u6307\u6807\u5e26\u6765\u7684\u95ee\u9898\u3002", "result": "\u5bf9\u516b\u79cd\u6700\u5148\u8fdb\u7684DFMs\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u952e\u53d1\u73b0\u548c\u89c2\u5bdf\u7684\u6df1\u5165\u5206\u6790\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86BenchDepth\u8fd9\u4e00\u65b0\u57fa\u51c6\uff0c\u65e8\u5728\u901a\u8fc7\u4e94\u4e2a\u4e0b\u6e38\u4ee3\u7406\u4efb\u52a1\u8bc4\u4f30\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\uff08DFMs\uff09\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u8bc4\u4f30\u534f\u8bae\u4e2d\u7684\u504f\u5dee\u548c\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u6df1\u5ea6\u4f30\u8ba1\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u548c\u8fdb\u5c55\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.15346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15346", "abs": "https://arxiv.org/abs/2507.15346", "authors": ["Muhammad Aqeel", "Kidus Dagnaw Bellete", "Francesco Setti"], "title": "RoadFusion: Latent Diffusion Model for Pavement Defect Detection", "comment": "Accepted to ICIAP 2025", "summary": "Pavement defect detection faces critical challenges including limited\nannotated data, domain shift between training and deployment environments, and\nhigh variability in defect appearances across different road conditions. We\npropose RoadFusion, a framework that addresses these limitations through\nsynthetic anomaly generation with dual-path feature adaptation. A latent\ndiffusion model synthesizes diverse, realistic defects using text prompts and\nspatial masks, enabling effective training under data scarcity. Two separate\nfeature adaptors specialize representations for normal and anomalous inputs,\nimproving robustness to domain shift and defect variability. A lightweight\ndiscriminator learns to distinguish fine-grained defect patterns at the patch\nlevel. Evaluated on six benchmark datasets, RoadFusion achieves consistently\nstrong performance across both classification and localization tasks, setting\nnew state-of-the-art in multiple metrics relevant to real-world road\ninspection.", "AI": {"tldr": "RoadFusion \u901a\u8fc7\u5408\u6210\u5f02\u5e38\u751f\u6210\u548c\u53cc\u8def\u5f84\u7279\u5f81\u9002\u5e94\uff0c\u89e3\u51b3\u4e86\u8def\u9762\u7f3a\u9677\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8def\u9762\u7f3a\u9677\u68c0\u6d4b\u9762\u4e34\u6807\u6ce8\u6570\u636e\u6709\u9650\u3001\u8bad\u7ec3\u4e0e\u90e8\u7f72\u73af\u5883\u95f4\u7684\u9886\u57df\u504f\u79fb\u4ee5\u53ca\u4e0d\u540c\u9053\u8def\u6761\u4ef6\u4e0b\u7f3a\u9677\u5916\u89c2\u7684\u9ad8\u5ea6\u53ef\u53d8\u6027\u7b49\u5173\u952e\u6311\u6218\u3002", "method": "RoadFusion \u901a\u8fc7\u5408\u6210\u5f02\u5e38\u751f\u6210\u548c\u53cc\u8def\u5f84\u7279\u5f81\u9002\u5e94\u6765\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u3001\u9886\u57df\u504f\u79fb\u548c\u7f3a\u9677\u5916\u89c2\u591a\u6837\u6027\u95ee\u9898\u3002\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u7684\u903c\u771f\u7f3a\u9677\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u72ec\u7acb\u7684\u7279\u5f81\u9002\u914d\u5668\u5206\u522b\u5904\u7406\u6b63\u5e38\u548c\u5f02\u5e38\u8f93\u5165\uff0c\u540c\u65f6\u8f7b\u91cf\u7ea7\u9274\u522b\u5668\u5728\u8865\u4e01\u7ea7\u522b\u5b66\u4e60\u533a\u5206\u7ec6\u7c92\u5ea6\u7f3a\u9677\u6a21\u5f0f\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cRoadFusion \u5728\u5206\u7c7b\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u5b9e\u9645\u9053\u8def\u68c0\u67e5\u76f8\u5173\u6307\u6807\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "RoadFusion \u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5728\u5206\u7c7b\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u9053\u8def\u68c0\u67e5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15365", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15365", "abs": "https://arxiv.org/abs/2507.15365", "authors": ["Fatemeh Saleh", "Sadegh Aliakbarian", "Charlie Hewitt", "Lohit Petikam", "Xiao-Xian", "Antonio Criminisi", "Thomas J. Cashman", "Tadas Baltru\u0161aitis"], "title": "DAViD: Data-efficient and Accurate Vision Models from Synthetic Data", "comment": "Accepted at ICCV 2025", "summary": "The state of the art in human-centric computer vision achieves high accuracy\nand robustness across a diverse range of tasks. The most effective models in\nthis domain have billions of parameters, thus requiring extremely large\ndatasets, expensive training regimes, and compute-intensive inference. In this\npaper, we demonstrate that it is possible to train models on much smaller but\nhigh-fidelity synthetic datasets, with no loss in accuracy and higher\nefficiency. Using synthetic training data provides us with excellent levels of\ndetail and perfect labels, while providing strong guarantees for data\nprovenance, usage rights, and user consent. Procedural data synthesis also\nprovides us with explicit control on data diversity, that we can use to address\nunfairness in the models we train. Extensive quantitative assessment on real\ninput images demonstrates accuracy of our models on three dense prediction\ntasks: depth estimation, surface normal estimation, and soft foreground\nsegmentation. Our models require only a fraction of the cost of training and\ninference when compared with foundational models of similar accuracy. Our\nhuman-centric synthetic dataset and trained models are available at\nhttps://aka.ms/DAViD.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u53ef\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u964d\u4f4e\u6210\u672c\uff0c\u5e76\u63d0\u4f9b\u66f4\u597d\u7684\u6570\u636e\u53ef\u63a7\u6027\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u4eba\u7c7b\u4e2d\u5fc3\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u9700\u8981\u5927\u91cf\u53c2\u6570\u3001\u6570\u636e\u96c6\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9ad8\u4fdd\u771f\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u5229\u7528\u7a0b\u5e8f\u5316\u6570\u636e\u5408\u6210\u6280\u672f\u63a7\u5236\u6570\u636e\u591a\u6837\u6027\u3002", "result": "\u5728\u6df1\u5ea6\u4f30\u8ba1\u3001\u8868\u9762\u6cd5\u7ebf\u4f30\u8ba1\u548c\u8f6f\u524d\u666f\u5206\u5272\u4e09\u4e2a\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u4e14\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u8bad\u7ec3\u8d44\u6e90\u9700\u6c42\uff0c\u4e14\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6570\u636e\u53ef\u63a7\u6027\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2507.15401", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15401", "abs": "https://arxiv.org/abs/2507.15401", "authors": ["Huiyu Zhai", "Xingxing Yang", "Yalan Ye", "Chenyang Li", "Bin Fan", "Changze Li"], "title": "Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond", "comment": null, "summary": "Facial expression recognition (FER) is a challenging task due to pervasive\nocclusion and dataset biases. Especially when facial information is partially\noccluded, existing FER models struggle to extract effective facial features,\nleading to inaccurate classifications. In response, we present ORSANet, which\nintroduces the following three key contributions: First, we introduce auxiliary\nmulti-modal semantic guidance to disambiguate facial occlusion and learn\nhigh-level semantic knowledge, which is two-fold: 1) we introduce semantic\nsegmentation maps as dense semantics prior to generate semantics-enhanced\nfacial representations; 2) we introduce facial landmarks as sparse geometric\nprior to mitigate intrinsic noises in FER, such as identity and gender biases.\nSecond, to facilitate the effective incorporation of these two multi-modal\npriors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively\nfuse the landmark feature and semantics-enhanced representations within\ndifferent scales. Third, we design a Dynamic Adversarial Repulsion Enhancement\nLoss (DARELoss) that dynamically adjusts the margins of ambiguous classes,\nfurther enhancing the model's ability to distinguish similar expressions. We\nfurther construct the first occlusion-oriented FER dataset to facilitate\nspecialized robustness analysis on various real-world occlusion conditions,\ndubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER\ndemonstrate that our proposed ORSANet achieves SOTA recognition performance.\nCode is publicly available at https://github.com/Wenyuzhy/ORSANet-master.", "AI": {"tldr": "ORSANet\u901a\u8fc7\u591a\u6a21\u6001\u5148\u9a8c\u548c\u52a8\u6001\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u906e\u6321\u6761\u4ef6\u4e0b\u7684\u8868\u60c5\u8bc6\u522b\u6548\u679c\u3002", "motivation": "\u73b0\u6709FER\u6a21\u578b\u5728\u9762\u90e8\u906e\u6321\u60c5\u51b5\u4e0b\u96be\u4ee5\u63d0\u53d6\u6709\u6548\u7279\u5f81\uff0c\u5bfc\u81f4\u5206\u7c7b\u4e0d\u51c6\u786e\u3002", "method": "1. \u5f15\u5165\u8bed\u4e49\u5206\u5272\u56fe\u548c\u9762\u90e8\u5173\u952e\u70b9\u4f5c\u4e3a\u591a\u6a21\u6001\u5148\u9a8c\uff1b2. \u8bbe\u8ba1\u591a\u5c3a\u5ea6\u4ea4\u53c9\u4ea4\u4e92\u6a21\u5757\uff08MCM\uff09\u81ea\u9002\u5e94\u878d\u5408\u7279\u5f81\uff1b3. \u63d0\u51fa\u52a8\u6001\u5bf9\u6297\u6392\u65a5\u589e\u5f3a\u635f\u5931\uff08DARELoss\uff09\u4f18\u5316\u5206\u7c7b\u8fb9\u754c\u3002", "result": "ORSANet\u5728\u516c\u5f00\u57fa\u51c6\u548cOcclu-FER\u6570\u636e\u96c6\u4e0a\u5747\u5b9e\u73b0\u4e86\u6700\u4f18\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "ORSANet\u901a\u8fc7\u5f15\u5165\u591a\u6a21\u6001\u8bed\u4e49\u5f15\u5bfc\u3001\u591a\u5c3a\u5ea6\u4ea4\u4e92\u6a21\u5757\u548c\u52a8\u6001\u5bf9\u6297\u6392\u65a5\u589e\u5f3a\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u906e\u6321\u6761\u4ef6\u4e0b\u7684\u4eba\u8138\u8868\u60c5\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u5728\u516c\u5f00\u57fa\u51c6\u548c\u81ea\u5efa\u6570\u636e\u96c6Occlu-FER\u4e0a\u8fbe\u5230SOTA\u6c34\u5e73\u3002"}}
{"id": "2507.15418", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15418", "abs": "https://arxiv.org/abs/2507.15418", "authors": ["Ka Young Kim", "Hyeon Bae Kim", "Seong Tae Kim"], "title": "SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition", "comment": "Accepted to MICCAI 2025", "summary": "Surgical phase recognition plays a crucial role in surgical workflow\nanalysis, enabling various applications such as surgical monitoring, skill\nassessment, and workflow optimization. Despite significant advancements in deep\nlearning-based surgical phase recognition, these models remain inherently\nopaque, making it difficult to understand how they make decisions. This lack of\ninterpretability hinders trust and makes it challenging to debug the model. To\naddress this challenge, we propose SurgX, a novel concept-based explanation\nframework that enhances the interpretability of surgical phase recognition\nmodels by associating neurons with relevant concepts. In this paper, we\nintroduce the process of selecting representative example sequences for\nneurons, constructing a concept set tailored to the surgical video dataset,\nassociating neurons with concepts and identifying neurons crucial for\npredictions. Through extensive experiments on two surgical phase recognition\nmodels, we validate our method and analyze the explanation for prediction. This\nhighlights the potential of our method in explaining surgical phase\nrecognition. The code is available at https://github.com/ailab-kyunghee/SurgX", "AI": {"tldr": "SurgX\u662f\u4e00\u4e2a\u63d0\u5347\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u6a21\u578b\u89e3\u91ca\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u8054\u795e\u7ecf\u5143\u4e0e\u6982\u5ff5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u4e2d\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u5f71\u54cd\u4fe1\u4efb\u548c\u8c03\u8bd5\u3002", "method": "\u63d0\u51faSurgX\u6846\u67b6\uff0c\u5305\u62ec\u9009\u62e9\u4ee3\u8868\u6027\u795e\u7ecf\u5143\u5e8f\u5217\u3001\u6784\u5efa\u624b\u672f\u89c6\u9891\u6570\u636e\u96c6\u5b9a\u5236\u6982\u5ff5\u96c6\u3001\u5173\u8054\u795e\u7ecf\u5143\u4e0e\u6982\u5ff5\u53ca\u8bc6\u522b\u5173\u952e\u795e\u7ecf\u5143\u3002", "result": "\u5728\u4e24\u79cd\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SurgX\u901a\u8fc7\u5c06\u795e\u7ecf\u5143\u4e0e\u76f8\u5173\u6982\u5ff5\u5173\u8054\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u6a21\u578b\u7684\u89e3\u91ca\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u89e3\u91ca\u9884\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.15480", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15480", "abs": "https://arxiv.org/abs/2507.15480", "authors": ["Liang Chen", "Ghazi Shazan Ahmad", "Tianjun Yao", "Lingqiao Liu", "Zhiqiang Shen"], "title": "One Last Attention for Your Vision-Language Model", "comment": "Accepted by ICCV 2025", "summary": "Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable\nzero-shot performance, yet their downstream potential hinges on effective\nfine-tuning. Most adaptation methods typically focus on refining representation\nfrom separate modalities (text or vision) but neglect the critical role of\ntheir fused representations in the decision-making process, \\emph{\\ie} rational\nmatrix that drives the final prediction. To bridge the gap, we propose a simple\nyet effective \\textbf{R}ational \\textbf{Ada}ptaion ({RAda}) to explicitly\nexploit the final fused representation during fine-tuning. RAda employs a\nlearned mask, obtained from a lightweight attention layer attached at the end\nof a VLM, to dynamically calibrate the contribution of each element in the\nrational matrix, enabling targeted adjustments to the final cross-modal\ninteractions without incurring costly modifications to intermediate features.\nExperiments in different settings (i.e., updating, or freezing pretrained\nencoders in adaptation, and test-time training that can only access the\nunlabeled test data) show that RAda serves as a versatile fine-tuning\ntechnique, improving the baseline with minimal code and performing comparably\nagainst current arts in most settings. Code is available at\n\\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.", "AI": {"tldr": "RAda\u901a\u8fc7\u52a8\u6001\u6821\u51c6\u878d\u5408\u8868\u793a\uff08\u7406\u6027\u77e9\u9635\uff09\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\u6548\u679c\uff0c\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u5fae\u8c03\u65b9\u6cd5\u901a\u5e38\u5ffd\u89c6\u878d\u5408\u8868\u793a\uff08\u7406\u6027\u77e9\u9635\uff09\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u800cRAda\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "RAda\u901a\u8fc7\u5728\u5b66\u4e60\u5230\u7684\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u5c42\u4e0a\u9644\u52a0\u4e00\u4e2a\u63a9\u7801\uff0c\u52a8\u6001\u8c03\u6574\u7406\u6027\u77e9\u9635\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u8d21\u732e\uff0c\u4ece\u800c\u5728\u4e0d\u4fee\u6539\u4e2d\u95f4\u7279\u5f81\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5728\u4e0d\u540c\u8bbe\u7f6e\uff08\u66f4\u65b0\u6216\u51bb\u7ed3\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u3001\u4ec5\u8bbf\u95ee\u672a\u6807\u8bb0\u6d4b\u8bd5\u6570\u636e\u7684\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff09\u4e2d\uff0cRAda\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e0e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "RAda\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5fae\u8c03\u6280\u672f\uff0c\u901a\u8fc7\u52a8\u6001\u6821\u51c6\u7406\u6027\u77e9\u9635\u4e2d\u7684\u5143\u7d20\u8d21\u732e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.15492", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15492", "abs": "https://arxiv.org/abs/2507.15492", "authors": ["Rakesh John Amala Arokia Nathan", "Matthias Gessner", "Nurullah \u00d6zkan", "Marius Bock", "Mohamed Youssef", "Maximilian Mews", "Bj\u00f6rn Piltz", "Ralf Berger", "Oliver Bimber"], "title": "An aerial color image anomaly dataset for search missions in complex forested terrain", "comment": "17 pages", "summary": "After a family murder in rural Germany, authorities failed to locate the\nsuspect in a vast forest despite a massive search. To aid the search, a\nresearch aircraft captured high-resolution aerial imagery. Due to dense\nvegetation obscuring small clues, automated analysis was ineffective, prompting\na crowd-search initiative. This effort produced a unique dataset of labeled,\nhard-to-detect anomalies under occluded, real-world conditions. It can serve as\na benchmark for improving anomaly detection approaches in complex forest\nenvironments, supporting manhunts and rescue operations. Initial benchmark\ntests showed existing methods performed poorly, highlighting the need for\ncontext-aware approaches. The dataset is openly accessible for offline\nprocessing. An additional interactive web interface supports online viewing and\ndynamic growth by allowing users to annotate and submit new findings.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7531\u4f17\u5305\u641c\u7d22\u4ea7\u751f\u7684\u72ec\u7279\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6539\u8fdb\u590d\u6742\u68ee\u6797\u73af\u5883\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\uff0c\u6570\u636e\u96c6\u548c\u7f51\u7edc\u63a5\u53e3\u5df2\u516c\u5f00\u3002", "motivation": "\u5728\u5fb7\u56fd\u519c\u6751\u4e00\u8d77\u5bb6\u5ead\u8c0b\u6740\u6848\u540e\uff0c\u5c3d\u7ba1\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u641c\u7d22\uff0c\u5f53\u5c40\u4ecd\u672a\u80fd\u627e\u5230\u5acc\u7591\u4eba\u3002\u4e3a\u4e86\u534f\u52a9\u641c\u7d22\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u822a\u7a7a\u5f71\u50cf\uff0c\u5c24\u5176\u662f\u5728\u690d\u88ab\u5bc6\u96c6\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u7814\u7a76\u98de\u673a\u6355\u83b7\u9ad8\u5206\u8fa8\u7387\u822a\u7a7a\u5f71\u50cf\uff0c\u4f46\u7531\u4e8e\u690d\u88ab\u5bc6\u96c6\u906e\u853d\u5c0f\u7ebf\u7d22\uff0c\u81ea\u52a8\u5316\u5206\u6790\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u542f\u52a8\u4e86\u4f17\u5305\u641c\u7d22\u3002\u8fd9\u4ea7\u751f\u4e86\u5728\u906e\u6321\u3001\u771f\u5b9e\u6761\u4ef6\u4e0b\u96be\u4ee5\u68c0\u6d4b\u7684\u5f02\u5e38\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "\u521d\u59cb\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u51fa\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002\u6570\u636e\u96c6\u548c\u4ea4\u4e92\u5f0f\u7f51\u7edc\u63a5\u53e3\u5df2\u516c\u5f00\uff0c\u652f\u6301\u79bb\u7ebf\u5904\u7406\u548c\u5728\u7ebf\u52a8\u6001\u589e\u957f\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5728\u590d\u6742\u68ee\u6797\u73af\u5883\u4e2d\u6539\u8fdb\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u652f\u6301\u641c\u6355\u548c\u6551\u63f4\u884c\u52a8\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u65b9\u6cd5\u3002\u6570\u636e\u96c6\u548c\u4ea4\u4e92\u5f0f\u7f51\u7edc\u63a5\u53e3\u5df2\u516c\u5f00\uff0c\u4fbf\u4e8e\u79bb\u7ebf\u5904\u7406\u548c\u5728\u7ebf\u52a8\u6001\u589e\u957f\u3002"}}
{"id": "2507.15504", "categories": ["cs.CV", "68T45", "I.2.10; H.3.3"], "pdf": "https://arxiv.org/pdf/2507.15504", "abs": "https://arxiv.org/abs/2507.15504", "authors": ["Bingqing Zhang", "Zhuo Cao", "Heming Du", "Yang Li", "Xue Li", "Jiajun Liu", "Sen Wang"], "title": "Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization", "comment": "Accepted by ICCV 2025", "summary": "Despite recent advances, Text-to-video retrieval (TVR) is still hindered by\nmultiple inherent uncertainties, such as ambiguous textual queries, indistinct\ntext-video mappings, and low-quality video frames. Although interactive systems\nhave emerged to address these challenges by refining user intent through\nclarifying questions, current methods typically rely on heuristic or ad-hoc\nstrategies without explicitly quantifying these uncertainties, limiting their\neffectiveness. Motivated by this gap, we propose UMIVR, an\nUncertainty-Minimizing Interactive Text-to-Video Retrieval framework that\nexplicitly quantifies three critical uncertainties-text ambiguity, mapping\nuncertainty, and frame uncertainty-via principled, training-free metrics:\nsemantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon\ndivergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based\nFrame Sampler (TQFS). By adaptively generating targeted clarifying questions\nguided by these uncertainty measures, UMIVR iteratively refines user queries,\nsignificantly reducing retrieval ambiguity. Extensive experiments on multiple\nbenchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1\n(69.2\\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby\nestablishing an uncertainty-minimizing foundation for interactive TVR.", "AI": {"tldr": "UMIVR \u662f\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u6700\u5c0f\u5316\u7684\u4ea4\u4e92\u5f0f\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u6587\u672c\u6a21\u7cca\u6027\u3001\u6620\u5c04\u4e0d\u786e\u5b9a\u6027\u548c\u5e27\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4ea4\u4e92\u5f0f\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u4e34\u65f6\u7b56\u7565\uff0c\u672a\u663e\u5f0f\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u5176\u6548\u679c\u3002UMIVR \u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u663e\u5f0f\u91cf\u5316\u6587\u672c\u6a21\u7cca\u6027\u3001\u6620\u5c04\u4e0d\u786e\u5b9a\u6027\u548c\u5e27\u4e0d\u786e\u5b9a\u6027\u6765\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "method": "UMIVR \u63d0\u51fa\u4e86\u4e09\u4e2a\u65e0\u8bad\u7ec3\u7684\u5ea6\u91cf\u6807\u51c6\uff1a\u57fa\u4e8e\u8bed\u4e49\u71b5\u7684\u6587\u672c\u6a21\u7cca\u6027\u8bc4\u5206\uff08TAS\uff09\u3001\u57fa\u4e8eJensen-Shannon\u6563\u5ea6\u7684\u6620\u5c04\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\uff08MUS\uff09\u548c\u57fa\u4e8e\u65f6\u95f4\u8d28\u91cf\u7684\u5e27\u91c7\u6837\u5668\uff08TQFS\uff09\uff0c\u901a\u8fc7\u8fd9\u4e9b\u5ea6\u91cf\u6807\u51c6\u81ea\u9002\u5e94\u751f\u6210\u9488\u5bf9\u6027\u6f84\u6e05\u95ee\u9898\u3002", "result": "\u5728 MSR-VTT-1k \u6570\u636e\u96c6\u4e0a\uff0cUMIVR \u5728 10 \u8f6e\u4ea4\u4e92\u540e\u5b9e\u73b0\u4e86 Recall@1 69.2% \u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "UMIVR \u901a\u8fc7\u663e\u5f0f\u91cf\u5316\u548c\u6700\u5c0f\u5316\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u4e2d\u7684\u5173\u952e\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.15520", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15520", "abs": "https://arxiv.org/abs/2507.15520", "authors": ["Hanting Li", "Fei Zhou", "Xin Sun", "Yang Hua", "Jungong Han", "Liang-Jie Zhang"], "title": "SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement", "comment": "11 pages, 10 figures, 6 tables", "summary": "Recent Transformer-based low-light enhancement methods have made promising\nprogress in recovering global illumination. However, they still struggle with\nnon-uniform lighting scenarios, such as backlit and shadow, appearing as\nover-exposure or inadequate brightness restoration. To address this challenge,\nwe present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)\nframework that enables accurate illumination restoration. Specifically, we\npropose a dynamic integral image representation to model the spatially-varying\nillumination, and further construct a novel Spatially-Adaptive Integral\nIllumination Estimator ($\\text{SAI}^2\\text{E}$). Moreover, we introduce an\nIllumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which\nleverages the illumination to calibrate the lightness-relevant features toward\nvisual-pleased illumination enhancement. Extensive experiments on five standard\nlow-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our\nSAIGFormer significantly outperforms state-of-the-art methods in both\nquantitative and qualitative metrics. In particular, our method achieves\nsuperior performance in non-uniform illumination enhancement while exhibiting\nstrong generalization capabilities across multiple datasets. Code is available\nat https://github.com/LHTcode/SAIGFormer.git.", "AI": {"tldr": "SAIGFormer\u901a\u8fc7\u52a8\u6001\u79ef\u5206\u56fe\u50cf\u548c\u5149\u7167\u5f15\u5bfc\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u975e\u5747\u5300\u5149\u7167\u589e\u5f3a\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709Transformer\u65b9\u6cd5\u5728\u975e\u5747\u5300\u5149\u7167\u573a\u666f\uff08\u5982\u80cc\u5149\u548c\u9634\u5f71\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u79ef\u5206\u56fe\u50cf\u8868\u793a\u6cd5\u6765\u5efa\u6a21\u7a7a\u95f4\u53d8\u5316\u7684\u5149\u7167\uff0c\u5e76\u6784\u5efa\u4e86Spatially-Adaptive Integral Illumination Estimator\uff08SAI\u00b2E\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86Illumination-Guided Multi-head Self-Attention\uff08IG-MSA\uff09\u673a\u5236\u3002", "result": "\u5728\u4e94\u4e2a\u6807\u51c6\u4f4e\u5149\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5\uff08LOL-Blur\uff09\u4e0a\uff0cSAIGFormer\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u6307\u6807\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SAIGFormer\u5728\u975e\u5747\u5300\u5149\u7167\u589e\u5f3a\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.15540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15540", "abs": "https://arxiv.org/abs/2507.15540", "authors": ["Syed Ahmed Mahmood", "Ali Shah Ali", "Umer Ahmed", "Fawad Javed Fateh", "M. Zeeshan Zia", "Quoc-Huy Tran"], "title": "Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport", "comment": null, "summary": "We study the problem of self-supervised procedure learning, which discovers\nkey steps and establishes their order from a set of unlabeled procedural\nvideos. Previous procedure learning methods typically learn frame-to-frame\ncorrespondences between videos before determining key steps and their order.\nHowever, their performance often suffers from order variations,\nbackground/redundant frames, and repeated actions. To overcome these\nchallenges, we propose a self-supervised procedure learning framework, which\nutilizes a fused Gromov-Wasserstein optimal transport formulation with a\nstructural prior for computing frame-to-frame mapping between videos. However,\noptimizing exclusively for the above temporal alignment term may lead to\ndegenerate solutions, where all frames are mapped to a small cluster in the\nembedding space and hence every video is associated with only one key step. To\naddress that limitation, we further integrate a contrastive regularization\nterm, which maps different frames to different points in the embedding space,\navoiding the collapse to trivial solutions. Finally, we conduct extensive\nexperiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,\nProceL and CrossTask) benchmarks to demonstrate superior performance by our\napproach against previous methods, including OPEL which relies on a traditional\nKantorovich optimal transport formulation with an optimality prior.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u7a0b\u5e8f\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408Gromov-Wasserstein\u6700\u4f18\u4f20\u8f93\u548c\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u987a\u5e8f\u53d8\u5316\u548c\u5197\u4f59\u5e27\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u7a0b\u5e8f\u5b66\u4e60\u65b9\u6cd5\u5728\u987a\u5e8f\u53d8\u5316\u3001\u80cc\u666f/\u5197\u4f59\u5e27\u548c\u91cd\u590d\u52a8\u4f5c\u5b58\u5728\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u5173\u952e\u6b65\u9aa4\u53ca\u5176\u987a\u5e8f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7a0b\u5e8f\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u878d\u5408Gromov-Wasserstein\u6700\u4f18\u4f20\u8f93\u548c\u7ed3\u6784\u5148\u9a8c\u7684\u5e27\u5230\u5e27\u6620\u5c04\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u5bf9\u6bd4\u6b63\u5219\u5316\u4ee5\u907f\u514d\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u9000\u5316\u89e3\u3002", "result": "\u5728EgoProceL\u3001ProceL\u548cCrossTask\u7b49\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u4f9d\u8d56\u4f20\u7edfKantorovich\u6700\u4f18\u4f20\u8f93\u7684OPEL\u7b49\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u7a0b\u5e8f\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u7ed3\u5408Gromov-Wasserstein\u6700\u4f18\u4f20\u8f93\u548c\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u987a\u5e8f\u53d8\u5316\u3001\u80cc\u666f/\u5197\u4f59\u5e27\u548c\u91cd\u590d\u52a8\u4f5c\u5e26\u6765\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.15541", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15541", "abs": "https://arxiv.org/abs/2507.15541", "authors": ["Jongmin Shin", "Enki Cho", "Ka Yong Kim", "Jung Yong Kim", "Seong Tae Kim", "Namkee Oh"], "title": "Towards Holistic Surgical Scene Graph", "comment": "Accepted to MICCAI 2025", "summary": "Surgical scene understanding is crucial for computer-assisted intervention\nsystems, requiring visual comprehension of surgical scenes that involves\ndiverse elements such as surgical tools, anatomical structures, and their\ninteractions. To effectively represent the complex information in surgical\nscenes, graph-based approaches have been explored to structurally model\nsurgical entities and their relationships. Previous surgical scene graph\nstudies have demonstrated the feasibility of representing surgical scenes using\ngraphs. However, certain aspects of surgical scenes-such as diverse\ncombinations of tool-action-target and the identity of the hand operating the\ntool-remain underexplored in graph-based representations, despite their\nimportance. To incorporate these aspects into graph representations, we propose\nEndoscapes-SG201 dataset, which includes annotations for tool-action-target\ncombinations and hand identity. We also introduce SSG-Com, a graph-based method\ndesigned to learn and represent these critical elements. Through experiments on\ndownstream tasks such as critical view of safety assessment and action triplet\nrecognition, we demonstrated the importance of integrating these essential\nscene graph components, highlighting their significant contribution to surgical\nscene understanding. The code and dataset are available at\nhttps://github.com/ailab-kyunghee/SSG-Com", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEndoscapes-SG201\u6570\u636e\u96c6\u548cSSG-Com\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u5de5\u5177-\u52a8\u4f5c-\u76ee\u6807\u7ec4\u5408\u53ca\u64cd\u4f5c\u624b\u8eab\u4efd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u573a\u666f\u7406\u89e3\u7684\u56fe\u8868\u793a\u6548\u679c\u3002", "motivation": "\u624b\u672f\u573a\u666f\u7406\u89e3\u7684\u73b0\u6709\u56fe\u8868\u793a\u65b9\u6cd5\u672a\u5145\u5206\u63a2\u7d22\u5de5\u5177-\u52a8\u4f5c-\u76ee\u6807\u7ec4\u5408\u53ca\u64cd\u4f5c\u624b\u8eab\u4efd\u7b49\u5173\u952e\u5143\u7d20\uff0c\u800c\u8fd9\u4e9b\u5bf9\u8ba1\u7b97\u673a\u8f85\u52a9\u5e72\u9884\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86Endoscapes-SG201\u6570\u636e\u96c6\uff0c\u5305\u542b\u5de5\u5177-\u52a8\u4f5c-\u76ee\u6807\u7ec4\u5408\u53ca\u64cd\u4f5c\u624b\u8eab\u4efd\u7684\u6807\u6ce8\uff0c\u5e76\u8bbe\u8ba1\u4e86SSG-Com\u56fe\u5b66\u4e60\u65b9\u6cd5\u6765\u8868\u793a\u8fd9\u4e9b\u5173\u952e\u5143\u7d20\u3002", "result": "\u5728\u5173\u952e\u5b89\u5168\u89c6\u56fe\u8bc4\u4f30\u548c\u52a8\u4f5c\u4e09\u5143\u7ec4\u8bc6\u522b\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u5173\u952e\u5143\u7d20\u5bf9\u624b\u672f\u573a\u666f\u7406\u89e3\u7684\u91cd\u8981\u8d21\u732e\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u63d0\u51faEndoscapes-SG201\u6570\u636e\u96c6\u548cSSG-Com\u65b9\u6cd5\uff0c\u6210\u529f\u5c06\u624b\u672f\u5de5\u5177-\u52a8\u4f5c-\u76ee\u6807\u7ec4\u5408\u53ca\u64cd\u4f5c\u624b\u8eab\u4efd\u7b49\u5173\u952e\u5143\u7d20\u6574\u5408\u5230\u56fe\u8868\u793a\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u573a\u666f\u7406\u89e3\u7684\u6548\u679c\u3002"}}
{"id": "2507.15542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15542", "abs": "https://arxiv.org/abs/2507.15542", "authors": ["Qinqian Lei", "Bo Wang", "Robby T. Tan"], "title": "HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation", "comment": "Accepted by ICCV 2025", "summary": "Zero-shot human-object interaction (HOI) detection remains a challenging\ntask, particularly in generalizing to unseen actions. Existing methods address\nthis challenge by tapping Vision-Language Models (VLMs) to access knowledge\nbeyond the training data. However, they either struggle to distinguish actions\ninvolving the same object or demonstrate limited generalization to unseen\nclasses. In this paper, we introduce HOLa (Zero-Shot HOI Detection with\nLow-Rank Decomposed VLM Feature Adaptation), a novel approach that both\nenhances generalization to unseen classes and improves action distinction. In\ntraining, HOLa decomposes VLM text features for given HOI classes via low-rank\nfactorization, producing class-shared basis features and adaptable weights.\nThese features and weights form a compact HOI representation that preserves\nshared information across classes, enhancing generalization to unseen classes.\nSubsequently, we refine action distinction by adapting weights for each HOI\nclass and introducing human-object tokens to enrich visual interaction\nrepresentations. To further distinguish unseen actions, we guide the weight\nadaptation with LLM-derived action regularization. Experimental results show\nthat our method sets a new state-of-the-art across zero-shot HOI settings on\nHICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.\nOur code is available at https://github.com/ChelsieLei/HOLa.", "AI": {"tldr": "HOLa\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3VLM\u7279\u5f81\u548cLLM\u6b63\u5219\u5316\uff0c\u63d0\u5347\u96f6\u6837\u672cHOI\u68c0\u6d4b\u7684\u6cdb\u5316\u4e0e\u52a8\u4f5c\u533a\u5206\u80fd\u529b\uff0cHICO-DET\u4e0aSOTA\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u533a\u5206\u6d89\u53ca\u76f8\u540c\u5bf9\u8c61\u7684\u52a8\u4f5c\u6216\u6cdb\u5316\u5230\u672a\u89c1\u7c7b\u522b\u65f6\u8868\u73b0\u6709\u9650\uff0cHOLa\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "HOLa\u91c7\u7528\u4f4e\u79e9\u5206\u89e3VLM\u6587\u672c\u7279\u5f81\uff0c\u751f\u6210\u7c7b\u522b\u5171\u4eab\u7684\u57fa\u7840\u7279\u5f81\u548c\u53ef\u8c03\u6743\u91cd\uff0c\u7ed3\u5408\u4eba\u7c7b-\u7269\u4f53\u6807\u8bb0\u589e\u5f3a\u89c6\u89c9\u4ea4\u4e92\u8868\u793a\uff0c\u5e76\u901a\u8fc7LLM\u9a71\u52a8\u7684\u52a8\u4f5c\u6b63\u5219\u5316\u4f18\u5316\u6743\u91cd\u8c03\u6574\u3002", "result": "\u5728HICO-DET\u7684\u96f6\u6837\u672cHOI\u8bbe\u7f6e\u4e2d\uff0cHOLa\u5728\u672a\u89c1\u52a8\u8bcd\u8bbe\u7f6e\u4e0b\u8fbe\u523027.91\u7684\u672a\u89c1\u7c7b\u522bmAP\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002", "conclusion": "HOLa\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3VLM\u6587\u672c\u7279\u5f81\uff0c\u7ed3\u5408LLM\u9a71\u52a8\u7684\u52a8\u4f5c\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672cHOI\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u672a\u89c1\u52a8\u4f5c\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u4e0a\uff0c\u8fbe\u5230\u4e86HICO-DET\u6570\u636e\u96c6\u4e0a\u7684\u65b0SOTA\u3002"}}
{"id": "2507.15569", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15569", "abs": "https://arxiv.org/abs/2507.15569", "authors": ["Xiaoyi Bao", "Chenwei Xie", "Hao Tang", "Tingyu Weng", "Xiaofeng Wang", "Yun Zheng", "Xingang Wang"], "title": "DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding", "comment": "Accepted by ICCV 2025", "summary": "In recent years, the introduction of Multi-modal Large Language Models\n(MLLMs) into video understanding tasks has become increasingly prevalent.\nHowever, how to effectively integrate temporal information remains a critical\nresearch focus. Traditional approaches treat spatial and temporal information\nseparately. Due to issues like motion blur, it is challenging to accurately\nrepresent the spatial information of rapidly moving objects. This can lead to\ntemporally important regions being underemphasized during spatial feature\nextraction, which in turn hinders accurate spatio-temporal interaction and\nvideo understanding. To address this limitation, we propose an innovative video\nrepresentation method called Dynamic-Image (DynImg). Specifically, we introduce\na set of non-key frames as temporal prompts to highlight the spatial areas\ncontaining fast-moving objects. During the process of visual feature\nextraction, these prompts guide the model to pay additional attention to the\nfine-grained spatial features corresponding to these regions. Moreover, to\nmaintain the correct sequence for DynImg, we employ a corresponding 4D video\nRotary Position Embedding. This retains both the temporal and spatial adjacency\nof DynImg, helping MLLM understand the spatio-temporal order within this\ncombined format. Experimental evaluations reveal that DynImg surpasses the\nstate-of-the-art methods by approximately 2% across multiple video\nunderstanding benchmarks, proving the effectiveness of our temporal prompts in\nenhancing video comprehension.", "AI": {"tldr": "DynImg\u901a\u8fc7\u975e\u5173\u952e\u5e27\u65f6\u95f4\u63d0\u793a\u548c4D\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff0c\u6709\u6548\u6574\u5408\u89c6\u9891\u65f6\u7a7a\u4fe1\u606f\uff0c\u6027\u80fd\u63d0\u53472%\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u89c6\u9891\u65f6\u7a7a\u4fe1\u606f\u65f6\u5b58\u5728\u5feb\u901f\u79fb\u52a8\u7269\u4f53\u7a7a\u95f4\u4fe1\u606f\u8868\u8fbe\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u65f6\u7a7a\u4ea4\u4e92\u548c\u7406\u89e3\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDynamic-Image\uff08DynImg\uff09\u7684\u521b\u65b0\u89c6\u9891\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u5173\u952e\u5e27\u4f5c\u4e3a\u65f6\u95f4\u63d0\u793a\u6765\u7a81\u51fa\u5feb\u901f\u79fb\u52a8\u7269\u4f53\u7684\u7a7a\u95f4\u533a\u57df\uff0c\u5e76\u91c7\u75284D\u89c6\u9891\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u4fdd\u6301\u65f6\u7a7a\u987a\u5e8f\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793aDynImg\u5728\u591a\u4e2a\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7ea62%\u3002", "conclusion": "DynImg\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u975e\u5173\u952e\u5e27\u4f5c\u4e3a\u65f6\u95f4\u63d0\u793a\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u65f6\u7a7a\u4fe1\u606f\u7684\u6574\u5408\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7ea62%\u3002"}}
{"id": "2507.15578", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.15578", "abs": "https://arxiv.org/abs/2507.15578", "authors": ["Gabriele Inzerillo", "Diego Valsesia", "Aniello Fiengo", "Enrico Magli"], "title": "Compress-Align-Detect: onboard change detection from unregistered images", "comment": null, "summary": "Change detection from satellite images typically incurs a delay ranging from\nseveral hours up to days because of latency in downlinking the acquired images\nand generating orthorectified image products at the ground stations; this may\npreclude real- or near real-time applications. To overcome this limitation, we\npropose shifting the entire change detection workflow onboard satellites. This\nrequires to simultaneously solve challenges in data storage, image registration\nand change detection with a strict complexity constraint. In this paper, we\npresent a novel and efficient framework for onboard change detection that\naddresses the aforementioned challenges in an end-to-end fashion with a deep\nneural network composed of three interlinked submodules: (1) image compression,\ntailored to minimize onboard data storage resources; (2) lightweight\nco-registration of non-orthorectified multi-temporal image pairs; and (3) a\nnovel temporally-invariant and computationally efficient change detection\nmodel. This is the first approach in the literature combining all these tasks\nin a single end-to-end framework with the constraints dictated by onboard\nprocessing. Experimental results compare each submodule with the current\nstate-of-the-art, and evaluate the performance of the overall integrated system\nin realistic setting on low-power hardware. Compelling change detection results\nare obtained in terms of F1 score as a function of compression rate, sustaining\na throughput of 0.7 Mpixel/s on a 15W accelerator.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u536b\u661f\u4e0a\u5b9e\u65f6\u8fdb\u884c\u53d8\u5316\u68c0\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u89e3\u51b3\u6570\u636e\u5b58\u50a8\u3001\u56fe\u50cf\u914d\u51c6\u548c\u53d8\u5316\u68c0\u6d4b\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u536b\u661f\u56fe\u50cf\u53d8\u5316\u68c0\u6d4b\u56e0\u5730\u9762\u7ad9\u5904\u7406\u5ef6\u8fdf\u800c\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\uff0c\u9700\u5c06\u6574\u4e2a\u5de5\u4f5c\u6d41\u7a0b\u79fb\u81f3\u536b\u661f\u4e0a\u4ee5\u514b\u670d\u6b64\u9650\u5236\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u56fe\u50cf\u538b\u7f29\u3001\u8f7b\u91cf\u7ea7\u56fe\u50cf\u914d\u51c6\u548c\u9ad8\u6548\u53d8\u5316\u68c0\u6d4b\u6a21\u578b\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4f4e\u529f\u8017\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e860.7 Mpixel/s\u7684\u5904\u7406\u901f\u5ea6\uff0c\u5e76\u5728\u538b\u7f29\u7387\u4e0eF1\u5206\u6570\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u536b\u661f\u4e0a\u7aef\u5230\u7aef\u7684\u53d8\u5316\u68c0\u6d4b\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.15595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15595", "abs": "https://arxiv.org/abs/2507.15595", "authors": ["Salah Eddine Bekhouche", "Gaby Maroun", "Fadi Dornaika", "Abdenour Hadid"], "title": "SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging", "comment": null, "summary": "Medical image segmentation is crucial for many healthcare tasks, including\ndisease diagnosis and treatment planning. One key area is the segmentation of\nskin lesions, which is vital for diagnosing skin cancer and monitoring\npatients. In this context, this paper introduces SegDT, a new segmentation\nmodel based on diffusion transformer (DiT). SegDT is designed to work on\nlow-cost hardware and incorporates Rectified Flow, which improves the\ngeneration quality at reduced inference steps and maintains the flexibility of\nstandard diffusion models. Our method is evaluated on three benchmarking\ndatasets and compared against several existing works, achieving\nstate-of-the-art results while maintaining fast inference speeds. This makes\nthe proposed model appealing for real-world medical applications. This work\nadvances the performance and capabilities of deep learning models in medical\nimage analysis, enabling faster, more accurate diagnostic tools for healthcare\nprofessionals. The code is made publicly available at\n\\href{https://github.com/Bekhouche/SegDT}{GitHub}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSegDT\u6a21\u578b\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u5728\u76ae\u80a4\u75c5\u53d8\u5206\u5272\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u9ad8\u6548\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5728\u75be\u75c5\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u76ae\u80a4\u75c5\u53d8\u5206\u5272\u65b9\u9762\uff0c\u5bf9\u76ae\u80a4\u764c\u7684\u8bca\u65ad\u548c\u60a3\u8005\u76d1\u6d4b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u65b0\u5206\u5272\u6a21\u578bSegDT\uff0c\u7ed3\u5408\u4e86Rectified Flow\u6280\u672f\uff0c\u4f18\u5316\u4e86\u751f\u6210\u8d28\u91cf\u5e76\u51cf\u5c11\u4e86\u63a8\u7406\u6b65\u9aa4\u3002", "result": "SegDT\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5feb\u901f\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "SegDT\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5728\u76ae\u80a4\u75c5\u53d8\u5206\u5272\u65b9\u9762\uff0c\u4e3a\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u63d0\u4f9b\u4e86\u66f4\u5feb\u3001\u66f4\u51c6\u786e\u7684\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2507.15602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15602", "abs": "https://arxiv.org/abs/2507.15602", "authors": ["Zihui Gao", "Jia-Wang Bian", "Guosheng Lin", "Hao Chen", "Chunhua Shen"], "title": "SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting", "comment": null, "summary": "Surface reconstruction and novel view rendering from sparse-view images are\nchallenging. Signed Distance Function (SDF)-based methods struggle with fine\ndetails, while 3D Gaussian Splatting (3DGS)-based approaches lack global\ngeometry coherence. We propose a novel hybrid method that combines the\nstrengths of both approaches: SDF captures coarse geometry to enhance\n3DGS-based rendering, while newly rendered images from 3DGS refine the details\nof SDF for accurate surface reconstruction. As a result, our method surpasses\nstate-of-the-art approaches in surface reconstruction and novel view synthesis\non the DTU and MobileBrick datasets. Code will be released at\nhttps://github.com/Gaozihui/SurfaceSplat.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408SDF\u548c3DGS\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u8868\u9762\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u6e32\u67d3\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u7a00\u758f\u89c6\u56fe\u56fe\u50cf\u4e2d\u8868\u9762\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u6e32\u67d3\u7684\u6311\u6218\uff0c\u7279\u522b\u662fSDF\u65b9\u6cd5\u5728\u7ec6\u8282\u5904\u7406\u548c3DGS\u65b9\u6cd5\u5728\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86SDF\uff08\u6355\u6349\u7c97\u7565\u51e0\u4f55\uff09\u548c3DGS\uff08\u6e32\u67d3\u65b0\u56fe\u50cf\u4ee5\u7ec6\u5316SDF\u7ec6\u8282\uff09\u7684\u4f18\u52bf\u3002", "result": "\u5728\u8868\u9762\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728DTU\u548cMobileBrick\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u8868\u9762\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u6548\u679c\u3002"}}
{"id": "2507.15606", "categories": ["cs.CV", "68T45", "I.4.5"], "pdf": "https://arxiv.org/pdf/2507.15606", "abs": "https://arxiv.org/abs/2507.15606", "authors": ["Ru Jia", "Xiaozhuang Ma", "Jianji Wang", "Nanning Zheng"], "title": "CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation", "comment": "5 pages, 4 figures, to be published", "summary": "While the proposal of the Tri-plane representation has advanced the\ndevelopment of the 3D-aware image generative models, problems rooted in its\ninherent structure, such as multi-face artifacts caused by sharing the same\nfeatures in symmetric regions, limit its ability to generate 360$^\\circ$ view\nimages. In this paper, we propose CylinderPlane, a novel implicit\nrepresentation based on Cylindrical Coordinate System, to eliminate the feature\nambiguity issue and ensure multi-view consistency in 360$^\\circ$. Different\nfrom the inevitable feature entanglement in Cartesian coordinate-based\nTri-plane representation, the cylindrical coordinate system explicitly\nseparates features at different angles, allowing our cylindrical representation\npossible to achieve high-quality, artifacts-free 360$^\\circ$ image synthesis.\nWe further introduce the nested cylinder representation that composites\nmultiple cylinders at different scales, thereby enabling the model more\nadaptable to complex geometry and varying resolutions. The combination of\ncylinders with different resolutions can effectively capture more critical\nlocations and multi-scale features, greatly facilitates fine detail learning\nand robustness to different resolutions. Moreover, our representation is\nagnostic to implicit rendering methods and can be easily integrated into any\nneural rendering pipeline. Extensive experiments on both synthetic dataset and\nunstructured in-the-wild images demonstrate that our proposed representation\nachieves superior performance over previous methods.", "AI": {"tldr": "CylinderPlane\u662f\u4e00\u79cd\u57fa\u4e8e\u5706\u67f1\u5750\u6807\u7cfb\u7684\u9690\u5f0f\u8868\u793a\uff0c\u89e3\u51b3\u4e86Tri-plane\u7684\u591a\u8138\u4f2a\u5f71\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684360\u00b0\u56fe\u50cf\u5408\u6210\u3002", "motivation": "Tri-plane\u8868\u793a\u57283D\u611f\u77e5\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u7279\u5f81\u5171\u4eab\u5bfc\u81f4\u7684\u591a\u8138\u4f2a\u5f71\u95ee\u9898\uff0c\u9650\u5236\u4e86360\u00b0\u89c6\u56fe\u56fe\u50cf\u7684\u751f\u6210\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5706\u67f1\u5750\u6807\u7cfb\u660e\u786e\u5206\u79bb\u4e0d\u540c\u89d2\u5ea6\u7684\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u5d4c\u5957\u5706\u67f1\u8868\u793a\uff08\u591a\u5c3a\u5ea6\u5706\u67f1\u7ec4\u5408\uff09\u6765\u9002\u5e94\u590d\u6742\u51e0\u4f55\u548c\u591a\u5206\u8fa8\u7387\u9700\u6c42\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u91ce\u5916\u975e\u7ed3\u6784\u5316\u56fe\u50cf\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCylinderPlane\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "CylinderPlane \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5706\u67f1\u5750\u6807\u7cfb\u7684\u65b0\u578b\u9690\u5f0f\u8868\u793a\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86Tri-plane\u8868\u793a\u4e2d\u7684\u7279\u5f81\u6a21\u7cca\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u65e0\u4f2a\u5f71\u7684360\u00b0\u56fe\u50cf\u5408\u6210\u3002"}}
{"id": "2507.15628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15628", "abs": "https://arxiv.org/abs/2507.15628", "authors": ["Shanjiang Tang", "Rui Huang", "Hsinyu Luo", "Chunjiang Wang", "Ce Yu", "Yusen Li", "Hao Fu", "Chao Sun", "and Jian Xiao"], "title": "A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications", "comment": null, "summary": "The explosive growth of video data in recent years has brought higher demands\nfor video analytics, where accuracy and efficiency remain the two primary\nconcerns. Deep neural networks (DNNs) have been widely adopted to ensure\naccuracy; however, improving their efficiency in video analytics remains an\nopen challenge. Different from existing surveys that make summaries of\nDNN-based video mainly from the accuracy optimization aspect, in this survey,\nwe aim to provide a thorough review of optimization techniques focusing on the\nimprovement of the efficiency of DNNs in video analytics. We organize existing\nmethods in a bottom-up manner, covering multiple perspectives such as hardware\nsupport, data processing, operational deployment, etc. Finally, based on the\noptimization framework and existing works, we analyze and discuss the problems\nand challenges in the performance optimization of DNN-based video analytics.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86DNN\u5728\u89c6\u9891\u5206\u6790\u4e2d\u6548\u7387\u4f18\u5316\u7684\u6280\u672f\uff0c\u4ece\u786c\u4ef6\u652f\u6301\u5230\u6570\u636e\u5904\u7406\u7b49\u591a\u89d2\u5ea6\u7ec4\u7ec7\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u4f18\u5316\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u89c6\u9891\u6570\u636e\u7684\u7206\u70b8\u6027\u589e\u957f\u5bf9\u89c6\u9891\u5206\u6790\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u51c6\u786e\u6027\u548c\u6548\u7387\u662f\u5173\u952e\u3002\u73b0\u6709\u8c03\u67e5\u591a\u4ece\u51c6\u786e\u6027\u4f18\u5316\u89d2\u5ea6\u603b\u7ed3\uff0c\u672c\u6587\u4e13\u6ce8\u4e8eDNN\u5728\u89c6\u9891\u5206\u6790\u4e2d\u6548\u7387\u4f18\u5316\u7684\u6280\u672f\u3002", "method": "\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u5f0f\u7ec4\u7ec7\u73b0\u6709\u65b9\u6cd5\uff0c\u6db5\u76d6\u786c\u4ef6\u652f\u6301\u3001\u6570\u636e\u5904\u7406\u3001\u64cd\u4f5c\u90e8\u7f72\u7b49\u591a\u4e2a\u89c6\u89d2\u3002", "result": "\u63d0\u4f9b\u4e86\u5168\u9762\u7684DNN\u5728\u89c6\u9891\u5206\u6790\u4e2d\u6548\u7387\u4f18\u5316\u6280\u672f\u7684\u7efc\u8ff0\u3002", "conclusion": "\u672c\u6587\u5206\u6790\u4e86\u57fa\u4e8eDNN\u7684\u89c6\u9891\u5206\u6790\u6027\u80fd\u4f18\u5316\u4e2d\u7684\u95ee\u9898\u548c\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f18\u5316\u6846\u67b6\u548c\u73b0\u6709\u5de5\u4f5c\u7684\u603b\u7ed3\u3002"}}
{"id": "2507.15633", "categories": ["cs.CV", "I.2.10; I.4.8; H.3.3"], "pdf": "https://arxiv.org/pdf/2507.15633", "abs": "https://arxiv.org/abs/2507.15633", "authors": ["Sachin Sharma", "Federico Simonetta", "Michele Flammini"], "title": "Experimenting active and sequential learning in a medieval music manuscript", "comment": "6 pages, 4 figures, accepted at IEEE MLSP 2025 (IEEE International\n  Workshop on Machine Learning for Signal Processing). Special Session:\n  Applications of AI in Cultural and Artistic Heritage", "summary": "Optical Music Recognition (OMR) is a cornerstone of music digitization\ninitiatives in cultural heritage, yet it remains limited by the scarcity of\nannotated data and the complexity of historical manuscripts. In this paper, we\npresent a preliminary study of Active Learning (AL) and Sequential Learning\n(SL) tailored for object detection and layout recognition in an old medieval\nmusic manuscript. Leveraging YOLOv8, our system selects samples with the\nhighest uncertainty (lowest prediction confidence) for iterative labeling and\nretraining. Our approach starts with a single annotated image and successfully\nboosts performance while minimizing manual labeling. Experimental results\nindicate that comparable accuracy to fully supervised training can be achieved\nwith significantly fewer labeled examples. We test the methodology as a\npreliminary investigation on a novel dataset offered to the community by the\nAnonymous project, which studies laude, a poetical-musical genre spread across\nItaly during the 12th-16th Century. We show that in the manuscript at-hand,\nuncertainty-based AL is not effective and advocates for more usable methods in\ndata-scarcity scenarios.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv8\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2d\u4e16\u7eaa\u97f3\u4e50\u624b\u7a3f\u7684\u5bf9\u8c61\u68c0\u6d4b\u548c\u5e03\u5c40\u8bc6\u522b\uff0c\u7ed3\u679c\u663e\u793a\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5149\u5b66\u97f3\u4e50\u8bc6\u522b\uff08OMR\uff09\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u5386\u53f2\u624b\u7a3f\u590d\u6742\u6027\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528YOLOv8\u8fdb\u884c\u5bf9\u8c61\u68c0\u6d4b\u548c\u5e03\u5c40\u8bc6\u522b\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u548c\u987a\u5e8f\u5b66\u4e60\uff08SL\uff09\u9009\u62e9\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u6700\u4f4e\u7684\u6837\u672c\u8fdb\u884c\u8fed\u4ee3\u6807\u6ce8\u548c\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5b8c\u5168\u76d1\u7763\u8bad\u7ec3\u76f8\u5f53\u7684\u51c6\u786e\u5ea6\u53ef\u4ee5\u901a\u8fc7\u663e\u8457\u51cf\u5c11\u7684\u6807\u6ce8\u6837\u672c\u5b9e\u73b0\u3002", "conclusion": "\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\uff0c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u4e3b\u52a8\u5b66\u4e60\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u5b9e\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.15652", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15652", "abs": "https://arxiv.org/abs/2507.15652", "authors": ["Haoran Zhou", "Zihan Zhang", "Hao Chen"], "title": "Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have made significant strides by\ncombining visual recognition and language understanding to generate content\nthat is both coherent and contextually accurate. However, MLLMs continue to\nstruggle with object hallucinations, where models produce seemingly plausible\nbut factually incorrect outputs, including objects that do not exist in the\nimage. Recent work has revealed that the prior knowledge in MLLMs significantly\nsuppresses visual information in deep layers, causing hallucinatory outputs.\nHowever, how these priors suppress visual information at the intermediate layer\nstage in MLLMs remains unclear. We observe that visual factual knowledge and\nthe differences between intermediate-layer prior/original probability\ndistributions show similar evolutionary trends in intermediate layers.\nMotivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a\nsimple, training-free method that dynamically selects intermediate layers with\nthe most significant visual factual information. By contrasting the output\ndistributions of the selected layer derived from the original input and\npure-text input, EVA extracts visual factual knowledge and proportionally\nincorporates it into the final layer to correct the output logits. Importantly,\nEVA is model-agnostic, seamlessly integrates with various classic decoding\nstrategies, and is applicable across different MLLMs. We validate EVA on\nwidely-used benchmarks, and the results show that it significantly reduces\nhallucination rates compared to baseline methods, underscoring its\neffectiveness in mitigating hallucinations.", "AI": {"tldr": "EVA\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e2d\u95f4\u5c42\u5e76\u63d0\u53d6\u89c6\u89c9\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u663e\u8457\u51cf\u5c11MLLMs\u7684\u5e7b\u89c9\u8f93\u51fa\uff0c\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "motivation": "MLLMs\u5728\u6df1\u5c42\u663e\u8457\u6291\u5236\u89c6\u89c9\u4fe1\u606f\u5bfc\u81f4\u5e7b\u89c9\u8f93\u51fa\uff0c\u4f46\u4e2d\u95f4\u5c42\u9636\u6bb5\u5148\u9a8c\u77e5\u8bc6\u5982\u4f55\u6291\u5236\u89c6\u89c9\u4fe1\u606f\u5c1a\u4e0d\u660e\u786e\u3002\u89c2\u5bdf\u5230\u89c6\u89c9\u4e8b\u5b9e\u77e5\u8bc6\u4e0e\u4e2d\u95f4\u5c42\u5148\u9a8c/\u539f\u59cb\u6982\u7387\u5206\u5e03\u5dee\u5f02\u5728\u4e2d\u95f4\u5c42\u5448\u73b0\u76f8\u4f3c\u6f14\u5316\u8d8b\u52bf\u3002", "method": "\u63d0\u51fa\u4e86Decoding by Extracting Visual Facts (EVA)\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7b80\u5355\u65b9\u6cd5\uff0c\u52a8\u6001\u9009\u62e9\u5177\u6709\u6700\u663e\u8457\u89c6\u89c9\u4e8b\u5b9e\u4fe1\u606f\u7684\u4e2d\u95f4\u5c42\uff0c\u901a\u8fc7\u5bf9\u6bd4\u539f\u59cb\u8f93\u5165\u548c\u7eaf\u6587\u672c\u8f93\u5165\u7684\u8f93\u51fa\u5206\u5e03\u63d0\u53d6\u89c6\u89c9\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u5e76\u5c06\u5176\u6309\u6bd4\u4f8b\u878d\u5165\u6700\u7ec8\u5c42\u4ee5\u4fee\u6b63\u8f93\u51falogits\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86EVA\uff0c\u7ed3\u679c\u663e\u793a\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u5e7b\u89c9\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7f13\u89e3\u5e7b\u89c9\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "EVA\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e2d\u95f4\u5c42\u5e76\u63d0\u53d6\u89c6\u89c9\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u663e\u8457\u51cf\u5c11\u4e86MLLMs\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u4e14\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\u548c\u89e3\u7801\u7b56\u7565\u517c\u5bb9\u6027\u3002"}}
{"id": "2507.15655", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15655", "abs": "https://arxiv.org/abs/2507.15655", "authors": ["Aniket Pal", "Ajoy Mondal", "Minesh Mathew", "C. V. Jawahar"], "title": "HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark", "comment": "This is a minor revision of the original paper submitted to IJDAR", "summary": "The proliferation of MultiLingual Visual Question Answering (MLVQA)\nbenchmarks augments the capabilities of large language models (LLMs) and\nmulti-modal LLMs, thereby enabling them to adeptly capture the intricate\nlinguistic subtleties and visual complexities inherent across diverse\nlanguages. Despite its potential, the current MLVQA model struggles to fully\nutilize its capabilities when dealing with the extensive variety of handwritten\ndocuments. This article delineates HW-MLVQA, an avant-garde VQA benchmark\nmeticulously crafted to mitigate the dearth of authentic Multilingual\nHandwritten document comprehension. HW-MLVQA encompasses an extensive\ncollection of 1,600 handwritten Pages complemented by 2,400 question-answers.\nFurthermore, it provides a robust benchmark evaluation framework spanning three\ndistinct modalities: text, image, and an integrated image & text modality. To\nsimulate authentic real-world contexts devoid of ground truth textual\ntranscriptions, we facilitates a rigorous assessment of proprietary and\nopen-source OCR models. The benchmark aspires to facilitate pivotal\nadvancements in multilingual handwritten document interpretation, fostering\ninnovation and scholarly inquiry within this specialized domain.", "AI": {"tldr": "HW-MLVQA\u662f\u4e00\u4e2a\u65b0\u578b\u591a\u8bed\u8a00\u624b\u5199\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\uff0c\u5305\u542b\u4e30\u5bcc\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u6a21\u578b\u5728\u624b\u5199\u6587\u6863\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dMLVQA\u6a21\u578b\u5728\u5904\u7406\u591a\u6837\u5316\u624b\u5199\u6587\u6863\u65f6\u80fd\u529b\u6709\u9650\uff0cHW-MLVQA\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u63d0\u4f9b\u66f4\u771f\u5b9e\u7684\u8bc4\u4f30\u573a\u666f\u3002", "method": "HW-MLVQA\u5305\u542b1,600\u9875\u624b\u5199\u6587\u6863\u548c2,400\u4e2a\u95ee\u7b54\u5bf9\uff0c\u5e76\u63d0\u4f9b\u4e86\u6587\u672c\u3001\u56fe\u50cf\u53ca\u56fe\u6587\u7ed3\u5408\u4e09\u79cd\u6a21\u6001\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "HW-MLVQA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u652f\u6301\u5bf9\u4e13\u6709\u548c\u5f00\u6e90OCR\u6a21\u578b\u7684\u4e25\u683c\u6d4b\u8bd5\uff0c\u6a21\u62df\u65e0\u771f\u5b9e\u6587\u672c\u8f6c\u5f55\u7684\u73b0\u5b9e\u573a\u666f\u3002", "conclusion": "HW-MLVQA\u57fa\u51c6\u65e8\u5728\u63a8\u52a8\u591a\u8bed\u8a00\u624b\u5199\u6587\u6863\u7406\u89e3\u9886\u57df\u7684\u521b\u65b0\u548c\u7814\u7a76\uff0c\u586b\u8865\u4e86\u73b0\u6709MLVQA\u6a21\u578b\u5728\u624b\u5199\u6587\u6863\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.15680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15680", "abs": "https://arxiv.org/abs/2507.15680", "authors": ["Yongkang Hou", "Jiarun Song"], "title": "Visual-Language Model Knowledge Distillation Method for Image Quality Assessment", "comment": null, "summary": "Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment.", "AI": {"tldr": "\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4f18\u5316CLIP\u5728IQA\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u51cf\u5c11\u53c2\u6570\u8d1f\u62c5\u5e76\u63d0\u5347\u5c40\u90e8\u7279\u5f81\u8bc6\u522b\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3CLIP\u5728IQA\u4efb\u52a1\u4e2d\u53c2\u6570\u8d1f\u62c5\u8fc7\u91cd\u548c\u5c40\u90e8\u5931\u771f\u7279\u5f81\u8bc6\u522b\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u8d28\u91cf\u5206\u7ea7\u63d0\u793a\u6a21\u677f\u4ee5\u6307\u5bfcCLIP\u8f93\u51fa\u8d28\u91cf\u8bc4\u5206\uff0c\u5bf9CLIP\u8fdb\u884c\u5fae\u8c03\u4ee5\u589e\u5f3a\u5176\u5728IQA\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u6a21\u6001\u81ea\u9002\u5e94\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\uff0c\u5b9e\u73b0\u4eceCLIP\u6559\u5e08\u6a21\u578b\u5230\u5b66\u751f\u6a21\u578b\u7684\u6307\u5bfc\u3002", "result": "\u5728\u591a\u4e2aIQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709IQA\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u591a\u4e2aIQA\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u5f3a\u6f5c\u529b\u3002"}}
{"id": "2507.15683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15683", "abs": "https://arxiv.org/abs/2507.15683", "authors": ["Boni Hu", "Zhenyu Xia", "Lin Chen", "Pengcheng Han", "Shuhui Bu"], "title": "Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing", "comment": "17 pages, 11 figures", "summary": "Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera\npose from query images, is fundamental to remote sensing and UAV applications.\nExisting methods face inherent trade-offs: image-based retrieval and pose\nregression approaches lack precision, while structure-based methods that\nregister queries to Structure-from-Motion (SfM) models suffer from\ncomputational complexity and limited scalability. These challenges are\nparticularly pronounced in remote sensing scenarios due to large-scale scenes,\nhigh altitude variations, and domain gaps of existing visual priors. To\novercome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel\nscene representation that compactly encodes both 3D geometry and appearance. We\nintroduce $\\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework\nthat follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting\nthe rich semantic information and geometric constraints inherent in Gaussian\nprimitives. To handle large-scale remote sensing scenarios, we incorporate\npartitioned Gaussian training, GPU-accelerated parallel matching, and dynamic\nmemory management strategies. Our approach consists of two stages: (1) a sparse\nstage featuring a Gaussian-specific consistent render-aware sampling strategy\nand landmark-guided detector for robust and accurate initial pose estimation,\nand (2) a dense stage that iteratively refines poses through coarse-to-fine\ndense rasterization matching while incorporating reliability verification.\nThrough comprehensive evaluation on simulation data, public datasets, and real\nflight experiments, we demonstrate that our method delivers competitive\nlocalization accuracy, recall rate, and computational efficiency while\neffectively filtering unreliable pose estimates. The results confirm the\neffectiveness of our approach for practical remote sensing applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u629b\u5149\u7684\u53cc\u91cd\u5c42\u6b21\u89c6\u89c9\u91cd\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u91cd\u5b9a\u4f4d\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u573a\u666f\u3001\u9ad8\u6d77\u62d4\u53d8\u5316\u548c\u9886\u57df\u5dee\u8ddd\u4e0b\u7684\u7cbe\u5ea6\u4e0d\u8db3\u548c\u8ba1\u7b97\u590d\u6742\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86$\\mathrm{Hi}^2$-GSLoc\uff0c\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u629b\u5149\u7684\u53cc\u91cd\u5c42\u6b21\u91cd\u5b9a\u4f4d\u6846\u67b6\uff0c\u91c7\u7528\u7a00\u758f\u5230\u5bc6\u96c6\u3001\u7c97\u5230\u7ec6\u7684\u8303\u5f0f\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u6570\u636e\u3001\u516c\u5f00\u6570\u636e\u96c6\u548c\u771f\u5b9e\u98de\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8fdc\u7a0b\u611f\u77e5\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.15690", "categories": ["cs.CV", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15690", "abs": "https://arxiv.org/abs/2507.15690", "authors": ["Hung Nguyen", "Runfa Li", "An Le", "Truong Nguyen"], "title": "DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting", "comment": "6 pages, 4 figures", "summary": "Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in\nreconstructing high-quality novel views, as it often overfits to the\nwidely-varying high-frequency (HF) details of the sparse training views. While\nfrequency regularization can be a promising approach, its typical reliance on\nFourier transforms causes difficult parameter tuning and biases towards\ndetrimental HF learning. We propose DWTGS, a framework that rethinks frequency\nregularization by leveraging wavelet-space losses that provide additional\nspatial supervision. Specifically, we supervise only the low-frequency (LF) LL\nsubbands at multiple DWT levels, while enforcing sparsity on the HF HH subband\nin a self-supervised manner. Experiments across benchmarks show that DWTGS\nconsistently outperforms Fourier-based counterparts, as this LF-centric\nstrategy improves generalization and reduces HF hallucinations.", "AI": {"tldr": "DWTGS\u901a\u8fc7\u5c0f\u6ce2\u7a7a\u95f4\u635f\u5931\u6539\u8fdb\u7a00\u758f\u89c6\u56fe3DGS\uff0c\u4f18\u4e8e\u5085\u91cc\u53f6\u65b9\u6cd5\uff0c\u51cf\u5c11\u9ad8\u9891\u5e7b\u89c9\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u91cd\u5efa\u9ad8\u8d28\u91cf\u65b0\u89c6\u56fe\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u5bb9\u6613\u8fc7\u62df\u5408\u7a00\u758f\u8bad\u7ec3\u89c6\u56fe\u4e2d\u7684\u9ad8\u9891\u7ec6\u8282\u3002\u9891\u7387\u6b63\u5219\u5316\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u65b9\u6cd5\uff0c\u4f46\u4f9d\u8d56\u5085\u91cc\u53f6\u53d8\u6362\u4f1a\u5bfc\u81f4\u53c2\u6570\u8c03\u4f18\u56f0\u96be\u548c\u5bf9\u9ad8\u9891\u5b66\u4e60\u7684\u6709\u5bb3\u504f\u5411\u3002", "method": "DWTGS\u5229\u7528\u5c0f\u6ce2\u7a7a\u95f4\u635f\u5931\uff0c\u4ec5\u5728\u591a\u4e2aDWT\u7ea7\u522b\u76d1\u7763\u4f4e\u9891LL\u5b50\u5e26\uff0c\u5e76\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u5bf9\u9ad8\u9891HH\u5b50\u5e26\u5b9e\u65bd\u7a00\u758f\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDWTGS\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u5bf9\u5e94\u65b9\u6cd5\u3002", "conclusion": "DWTGS\u6846\u67b6\u901a\u8fc7\u5229\u7528\u5c0f\u6ce2\u7a7a\u95f4\u635f\u5931\u63d0\u4f9b\u989d\u5916\u7684\u7a7a\u95f4\u76d1\u7763\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u65b9\u6cd5\uff0c\u6539\u5584\u4e86\u6cdb\u5316\u80fd\u529b\u5e76\u51cf\u5c11\u4e86\u9ad8\u9891\u5e7b\u89c9\u3002"}}
{"id": "2507.15709", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15709", "abs": "https://arxiv.org/abs/2507.15709", "authors": ["Wei Sun", "Weixia Zhang", "Linhan Cao", "Jun Jia", "Xiangyang Zhu", "Dandan Zhu", "Xiongkuo Min", "Guangtao Zhai"], "title": "Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation", "comment": "Efficient-FIQA achieved first place in the ICCV VQualA 2025 Face\n  Image Quality Assessment Challenge", "summary": "Face image quality assessment (FIQA) is essential for various face-related\napplications. Although FIQA has been extensively studied and achieved\nsignificant progress, the computational complexity of FIQA algorithms remains a\nkey concern for ensuring scalability and practical deployment in real-world\nsystems. In this paper, we aim to develop a computationally efficient FIQA\nmethod that can be easily deployed in real-world applications. Specifically,\nour method consists of two stages: training a powerful teacher model and\ndistilling a lightweight student model from it. To build a strong teacher\nmodel, we adopt a self-training strategy to improve its capacity. We first\ntrain the teacher model using labeled face images, then use it to generate\npseudo-labels for a set of unlabeled images. These pseudo-labeled samples are\nused in two ways: (1) to distill knowledge into the student model, and (2) to\ncombine with the original labeled images to further enhance the teacher model\nthrough self-training. The enhanced teacher model is used to further\npseudo-label another set of unlabeled images for distilling the student models.\nThe student model is trained using a combination of labeled images,\npseudo-labeled images from the original teacher model, and pseudo-labeled\nimages from the enhanced teacher model. Experimental results demonstrate that\nour student model achieves comparable performance to the teacher model with an\nextremely low computational overhead. Moreover, our method achieved first place\nin the ICCV 2025 VQualA FIQA Challenge. The code is available at\nhttps://github.com/sunwei925/Efficient-FIQA.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u6a21\u578b\u84b8\u998f\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u5728ICCV 2025\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d\u3002", "motivation": "\u5c3d\u7ba1FIQA\u7814\u7a76\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7b97\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4ecd\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u4e14\u6613\u4e8e\u90e8\u7f72\u7684FIQA\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a1) \u8bad\u7ec3\u4e00\u4e2a\u5f3a\u5927\u7684\u6559\u5e08\u6a21\u578b\uff0c\u91c7\u7528\u81ea\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u5176\u80fd\u529b\uff1b2) \u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u63d0\u53d6\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u3002\u6559\u5e08\u6a21\u578b\u901a\u8fc7\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u81ea\u8bad\u7ec3\u9010\u6b65\u589e\u5f3a\uff0c\u5b66\u751f\u6a21\u578b\u5219\u7ed3\u5408\u6807\u6ce8\u6570\u636e\u548c\u4f2a\u6807\u7b7e\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b66\u751f\u6a21\u578b\u5728\u6781\u4f4e\u8ba1\u7b97\u5f00\u9500\u4e0b\u6027\u80fd\u4e0e\u6559\u5e08\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u5728ICCV 2025 VQualA FIQA\u6311\u6218\u8d5b\u4e2d\u593a\u51a0\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u9ad8\u6548\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08FIQA\uff09\u65b9\u6cd5\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728ICCV 2025 VQualA FIQA\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u4e86\u7b2c\u4e00\u540d\u3002"}}
{"id": "2507.15724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15724", "abs": "https://arxiv.org/abs/2507.15724", "authors": ["Guoxuan Xia", "Harleen Hanspal", "Petru-Daniel Tudosiu", "Shifeng Zhang", "Sarah Parisot"], "title": "A Practical Investigation of Spatially-Controlled Image Generation with Transformers", "comment": "preprint", "summary": "Enabling image generation models to be spatially controlled is an important\narea of research, empowering users to better generate images according to their\nown fine-grained specifications via e.g. edge maps, poses. Although this task\nhas seen impressive improvements in recent times, a focus on rapidly producing\nstronger models has come at the cost of detailed and fair scientific\ncomparison. Differing training data, model architectures and generation\nparadigms make it difficult to disentangle the factors contributing to\nperformance. Meanwhile, the motivations and nuances of certain approaches\nbecome lost in the literature. In this work, we aim to provide clear takeaways\nacross generation paradigms for practitioners wishing to develop\ntransformer-based systems for spatially-controlled generation, clarifying the\nliterature and addressing knowledge gaps. We perform controlled experiments on\nImageNet across diffusion-based/flow-based and autoregressive (AR) models.\nFirst, we establish control token prefilling as a simple, general and\nperformant baseline approach for transformers. We then investigate previously\nunderexplored sampling time enhancements, showing that extending\nclassifier-free guidance to control, as well as softmax truncation, have a\nstrong impact on control-generation consistency. Finally, we re-clarify the\nmotivation of adapter-based approaches, demonstrating that they mitigate\n\"forgetting\" and maintain generation quality when trained on limited downstream\ndata, but underperform full training in terms of generation-control\nconsistency. Code will be released upon publication.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u6bd4\u4e86\u4e0d\u540c\u751f\u6210\u8303\u5f0f\u5728\u7a7a\u95f4\u63a7\u5236\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u63a7\u5236\u4ee4\u724c\u9884\u586b\u5145\u4f5c\u4e3a\u9ad8\u6548\u57fa\u7ebf\uff0c\u5e76\u63a2\u8ba8\u4e86\u91c7\u6837\u65f6\u95f4\u589e\u5f3a\u548c\u9002\u914d\u5668\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u65e8\u5728\u4e3a\u5e0c\u671b\u5f00\u53d1\u57fa\u4e8etransformer\u7684\u7a7a\u95f4\u63a7\u5236\u751f\u6210\u7cfb\u7edf\u7684\u4ece\u4e1a\u8005\u63d0\u4f9b\u6e05\u6670\u7684\u8de8\u751f\u6210\u8303\u5f0f\u7684\u89c1\u89e3\uff0c\u6f84\u6e05\u6587\u732e\u5e76\u586b\u8865\u77e5\u8bc6\u7a7a\u767d\u3002", "method": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u5bf9\u57fa\u4e8e\u6269\u6563/\u6d41\u548c\u81ea\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u63d0\u51fa\u4e86\u63a7\u5236\u4ee4\u724c\u9884\u586b\u5145\u65b9\u6cd5\uff0c\u5e76\u6d4b\u8bd5\u4e86\u91c7\u6837\u65f6\u95f4\u589e\u5f3a\u6280\u672f\u5982\u5206\u7c7b\u5668\u81ea\u7531\u6307\u5bfc\u548csoftmax\u622a\u65ad\u3002", "result": "\u63a7\u5236\u4ee4\u724c\u9884\u586b\u5145\u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u7b80\u5355\u3001\u901a\u7528\u4e14\u9ad8\u6548\u7684\u57fa\u7ebf\u65b9\u6cd5\uff1b\u91c7\u6837\u65f6\u95f4\u589e\u5f3a\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u63a7\u5236\u751f\u6210\u7684\u4e00\u81f4\u6027\uff1b\u9002\u914d\u5668\u65b9\u6cd5\u5728\u6709\u9650\u4e0b\u6e38\u6570\u636e\u8bad\u7ec3\u65f6\u80fd\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u5728\u751f\u6210\u63a7\u5236\u4e00\u81f4\u6027\u65b9\u9762\u4e0d\u5982\u5b8c\u6574\u8bad\u7ec3\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u6f84\u6e05\u4e86\u4e0d\u540c\u751f\u6210\u8303\u5f0f\u5728\u7a7a\u95f4\u63a7\u5236\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u63a7\u5236\u4ee4\u724c\u9884\u586b\u5145\u4f5c\u4e3a\u7b80\u5355\u4e14\u9ad8\u6548\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u91c7\u6837\u65f6\u95f4\u589e\u5f3a\u5bf9\u63a7\u5236\u751f\u6210\u4e00\u81f4\u6027\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u91cd\u65b0\u8bc4\u4f30\u4e86\u9002\u914d\u5668\u65b9\u6cd5\u7684\u52a8\u673a\u548c\u6548\u679c\u3002"}}
{"id": "2507.15728", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15728", "abs": "https://arxiv.org/abs/2507.15728", "authors": ["Wenqi Ouyang", "Zeqi Xiao", "Danni Yang", "Yifan Zhou", "Shuai Yang", "Lei Yang", "Jianlou Si", "Xingang Pan"], "title": "TokensGen: Harnessing Condensed Tokens for Long Video Generation", "comment": "Project page: https://vicky0522.github.io/tokensgen-webpage/", "summary": "Generating consistent long videos is a complex challenge: while\ndiffusion-based generative models generate visually impressive short clips,\nextending them to longer durations often leads to memory bottlenecks and\nlong-term inconsistency. In this paper, we propose TokensGen, a novel two-stage\nframework that leverages condensed tokens to address these issues. Our method\ndecomposes long video generation into three core tasks: (1) inner-clip semantic\ncontrol, (2) long-term consistency control, and (3) inter-clip smooth\ntransition. First, we train To2V (Token-to-Video), a short video diffusion\nmodel guided by text and video tokens, with a Video Tokenizer that condenses\nshort clips into semantically rich tokens. Second, we introduce T2To\n(Text-to-Token), a video token diffusion transformer that generates all tokens\nat once, ensuring global consistency across clips. Finally, during inference,\nan adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,\nreducing boundary artifacts and enhancing smooth transitions. Experimental\nresults demonstrate that our approach significantly enhances long-term temporal\nand content coherence without incurring prohibitive computational overhead. By\nleveraging condensed tokens and pre-trained short video models, our method\nprovides a scalable, modular solution for long video generation, opening new\npossibilities for storytelling, cinematic production, and immersive\nsimulations. Please see our project page at\nhttps://vicky0522.github.io/tokensgen-webpage/ .", "AI": {"tldr": "TokensGen \u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u6807\u8bb0\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u89e3\u51b3\u957f\u89c6\u9891\u751f\u6210\u7684\u957f\u671f\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u65f6\u95f4\u548c\u5185\u5bb9\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u77ed\u89c6\u9891\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u9762\u4e34\u5185\u5b58\u74f6\u9888\u548c\u957f\u671f\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002TokensGen \u65e8\u5728\u901a\u8fc7\u5206\u89e3\u957f\u89c6\u9891\u751f\u6210\u4e3a\u4e09\u4e2a\u6838\u5fc3\u4efb\u52a1\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "TokensGen \u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5305\u62ec To2V\uff08Token-to-Video\uff09\u77ed\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c T2To\uff08Text-to-Token\uff09\u89c6\u9891\u6807\u8bb0\u6269\u6563\u53d8\u6362\u5668\u3002To2V \u901a\u8fc7\u89c6\u9891\u6807\u8bb0\u5668\u5c06\u77ed\u89c6\u9891\u538b\u7f29\u4e3a\u8bed\u4e49\u4e30\u5bcc\u7684\u6807\u8bb0\uff0c\u800c T2To \u4e00\u6b21\u6027\u751f\u6210\u6240\u6709\u6807\u8bb0\u4ee5\u786e\u4fdd\u5168\u5c40\u4e00\u81f4\u6027\u3002\u63a8\u7406\u65f6\u91c7\u7528\u81ea\u9002\u5e94 FIFO-Diffusion \u7b56\u7565\u5e73\u6ed1\u8fde\u63a5\u76f8\u90bb\u7247\u6bb5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u589e\u52a0\u8fc7\u9ad8\u8ba1\u7b97\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u65f6\u95f4\u548c\u5185\u5bb9\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "TokensGen \u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u6a21\u5757\u5316\u7684\u957f\u89c6\u9891\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5229\u7528\u538b\u7f29\u6807\u8bb0\u548c\u9884\u8bad\u7ec3\u7684\u77ed\u89c6\u9891\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u65f6\u95f4\u548c\u5185\u5bb9\u7684\u4e00\u81f4\u6027\uff0c\u4e3a\u6545\u4e8b\u8bb2\u8ff0\u3001\u7535\u5f71\u5236\u4f5c\u548c\u6c89\u6d78\u5f0f\u6a21\u62df\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.15748", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15748", "abs": "https://arxiv.org/abs/2507.15748", "authors": ["Jisu Shin", "Richard Shaw", "Seunghyun Shin", "Anton Pelykh", "Zhensong Zhang", "Hae-Gon Jeon", "Eduardo Perez-Pellitero"], "title": "Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS", "comment": "10 pages, 3 figures, NeurIPS 2025 under review", "summary": "Modern camera pipelines apply extensive on-device processing, such as\nexposure adjustment, white balance, and color correction, which, while\nbeneficial individually, often introduce photometric inconsistencies across\nviews. These appearance variations violate multi-view consistency and degrade\nthe quality of novel view synthesis. Joint optimization of scene\nrepresentations and per-image appearance embeddings has been proposed to\naddress this issue, but at the cost of increased computational complexity and\nslower training. In this work, we propose a transformer-based method that\npredicts spatially adaptive bilateral grids to correct photometric variations\nin a multi-view consistent manner, enabling robust cross-scene generalization\nwithout the need for scene-specific retraining. By incorporating the learned\ngrids into the 3D Gaussian Splatting pipeline, we improve reconstruction\nquality while maintaining high training efficiency. Extensive experiments show\nthat our approach outperforms or matches existing scene-specific optimization\nmethods in reconstruction fidelity and convergence speed.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u53cc\u8fb9\u7f51\u683c\u6821\u6b63\u591a\u89c6\u89d2\u5149\u5ea6\u53d8\u5316\uff0c\u63d0\u53473D\u9ad8\u65af\u6cfc\u6e85\u6d41\u7a0b\u7684\u91cd\u5efa\u8d28\u91cf\u4e0e\u6548\u7387\u3002", "motivation": "\u73b0\u4ee3\u76f8\u673a\u6d41\u6c34\u7ebf\u7684\u5e7f\u6cdb\u8bbe\u5907\u7aef\u5904\u7406\uff08\u5982\u66dd\u5149\u8c03\u6574\u3001\u767d\u5e73\u8861\u548c\u8272\u5f69\u6821\u6b63\uff09\u867d\u6709\u76ca\u4e8e\u5355\u4e2a\u89c6\u56fe\uff0c\u4f46\u5e38\u5bfc\u81f4\u89c6\u89d2\u95f4\u7684\u5149\u5ea6\u4e0d\u4e00\u81f4\uff0c\u7834\u574f\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u5e76\u964d\u4f4e\u65b0\u89c6\u89d2\u5408\u6210\u7684\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff0c\u9884\u6d4b\u7a7a\u95f4\u81ea\u9002\u5e94\u7684\u53cc\u8fb9\u7f51\u683c\u4ee5\u6821\u6b63\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7684\u5149\u5ea6\u53d8\u5316\uff0c\u65e0\u9700\u573a\u666f\u7279\u5b9a\u91cd\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u8de8\u573a\u666f\u7684\u9c81\u68d2\u6cdb\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u573a\u666f\u7279\u5b9a\u4f18\u5316\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u573a\u666f\u7279\u5b9a\u91cd\u8bad\u7ec3\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5b66\u4e60\u5230\u7684\u53cc\u8fb9\u7f51\u683c\u6574\u5408\u52303D\u9ad8\u65af\u6cfc\u6e85\u6d41\u7a0b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u7684\u573a\u666f\u7279\u5b9a\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2507.15765", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15765", "abs": "https://arxiv.org/abs/2507.15765", "authors": ["Feng-Qi Cui", "Anyang Tong", "Jinyang Huang", "Jie Zhang", "Dan Guo", "Zhi Liu", "Meng Wang"], "title": "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization", "comment": "Accepted by ACM MM'25", "summary": "Dynamic Facial Expression Recognition (DFER) plays a critical role in\naffective computing and human-computer interaction. Although existing methods\nachieve comparable performance, they inevitably suffer from performance\ndegradation under sample heterogeneity caused by multi-source data and\nindividual expression variability. To address these challenges, we propose a\nnovel framework, called Heterogeneity-aware Distributional Framework (HDF), and\ndesign two plug-and-play modules to enhance time-frequency modeling and\nmitigate optimization imbalance caused by hard samples. Specifically, the\nTime-Frequency Distributional Attention Module (DAM) captures both temporal\nconsistency and frequency robustness through a dual-branch attention design,\nimproving tolerance to sequence inconsistency and visual style shifts. Then,\nbased on gradient sensitivity and information bottleneck principles, an\nadaptive optimization module Distribution-aware Scaling Module (DSM) is\nintroduced to dynamically balance classification and contrastive losses,\nenabling more stable and discriminative representation learning. Extensive\nexperiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF\nsignificantly improves both recognition accuracy and robustness. Our method\nachieves superior weighted average recall (WAR) and unweighted average recall\n(UAR) while maintaining strong generalization across diverse and imbalanced\nscenarios. Codes are released at https://github.com/QIcita/HDF_DFER.", "AI": {"tldr": "HDF\u6846\u67b6\u901a\u8fc7\u53cc\u6a21\u5757\u8bbe\u8ba1\u63d0\u5347\u52a8\u6001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6e90\u6570\u636e\u548c\u4e2a\u4f53\u8868\u8fbe\u5dee\u5f02\u5bfc\u81f4\u7684\u6837\u672c\u5f02\u8d28\u6027\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHDF\u7684\u65b0\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff1a\u65f6\u95f4-\u9891\u7387\u5206\u5e03\u6ce8\u610f\u529b\u6a21\u5757\uff08DAM\uff09\u548c\u5206\u5e03\u611f\u77e5\u7f29\u653e\u6a21\u5757\uff08DSM\uff09\uff0c\u5206\u522b\u7528\u4e8e\u589e\u5f3a\u65f6\u95f4-\u9891\u7387\u5efa\u6a21\u548c\u4f18\u5316\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728DFEW\u548cFERV39k\u6570\u636e\u96c6\u4e0a\uff0cHDF\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u52a0\u6743\u5e73\u5747\u53ec\u56de\u7387\uff08WAR\uff09\u548c\u975e\u52a0\u6743\u5e73\u5747\u53ec\u56de\u7387\uff08UAR\uff09\u3002", "conclusion": "HDF\u6846\u67b6\u901a\u8fc7\u65f6\u95f4-\u9891\u7387\u5206\u5e03\u6ce8\u610f\u529b\u6a21\u5757\u548c\u81ea\u9002\u5e94\u4f18\u5316\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.15777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15777", "abs": "https://arxiv.org/abs/2507.15777", "authors": ["Junwen Wang", "Oscar MacCormac", "William Rochford", "Aaron Kujawa", "Jonathan Shapey", "Tom Vercauteren"], "title": "Label tree semantic losses for rich multi-class medical image segmentation", "comment": "arXiv admin note: text overlap with arXiv:2506.21150", "summary": "Rich and accurate medical image segmentation is poised to underpin the next\ngeneration of AI-defined clinical practice by delineating critical anatomy for\npre-operative planning, guiding real-time intra-operative navigation, and\nsupporting precise post-operative assessment. However, commonly used learning\nmethods for medical and surgical imaging segmentation tasks penalise all errors\nequivalently and thus fail to exploit any inter-class semantics in the labels\nspace. This becomes particularly problematic as the cardinality and richness of\nlabels increases to include subtly different classes. In this work, we propose\ntwo tree-based semantic loss functions which take advantage of a hierarchical\norganisation of the labels. We further incorporate our losses in a recently\nproposed approach for training with sparse, background-free annotations to\nextend the applicability of our proposed losses. Extensive experiments are\nreported on two medical and surgical image segmentation tasks, namely head MRI\nfor whole brain parcellation (WBP) with full supervision and neurosurgical\nhyperspectral imaging (HSI) for scene understanding with sparse annotations.\nResults demonstrate that our proposed method reaches state-of-the-art\nperformance in both cases.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6811\u7ed3\u6784\u7684\u8bed\u4e49\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u6807\u7b7e\u5c42\u6b21\u7ed3\u6784\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\uff0c\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u5bf9\u6240\u6709\u9519\u8bef\u5747\u7b49\u60e9\u7f5a\uff0c\u672a\u80fd\u5229\u7528\u6807\u7b7e\u7a7a\u95f4\u4e2d\u7684\u7c7b\u95f4\u8bed\u4e49\u5173\u7cfb\uff0c\u5c24\u5176\u662f\u5728\u6807\u7b7e\u4e30\u5bcc\u4e14\u5305\u542b\u7ec6\u5fae\u5dee\u522b\u7c7b\u522b\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u6811\u7ed3\u6784\u7684\u8bed\u4e49\u635f\u5931\u51fd\u6570\uff0c\u5e76\u6574\u5408\u5230\u7a00\u758f\u6ce8\u91ca\u8bad\u7ec3\u65b9\u6cd5\u4e2d\u3002", "result": "\u5728\u5934\u90e8MRI\u5168\u8111\u5206\u5272\u548c\u795e\u7ecf\u5916\u79d1\u9ad8\u5149\u8c31\u56fe\u50cf\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u79cd\u57fa\u4e8e\u6811\u7ed3\u6784\u7684\u8bed\u4e49\u635f\u5931\u51fd\u6570\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.15793", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15793", "abs": "https://arxiv.org/abs/2507.15793", "authors": ["Ghassen Baklouti", "Julio Silva-Rodr\u00edguez", "Jose Dolz", "Houda Bahig", "Ismail Ben Ayed"], "title": "Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation", "comment": "Accepted at MICCAI 2025", "summary": "Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is\nincreasingly attracting interest in medical imaging due to its effectiveness\nand computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)\nis a notable approach based on the assumption that the adaptation inherently\noccurs in a low-dimensional subspace. While it has shown good performance, its\nimplementation requires a fixed and unalterable rank, which might be\nchallenging to select given the unique complexities and requirements of each\nmedical imaging downstream task. Inspired by advancements in natural image\nprocessing, we introduce a novel approach for medical image segmentation that\ndynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank\nrepresentation of the trainable weight matrices as a singular value\ndecomposition, we introduce an l_1 sparsity regularizer to the loss function,\nand tackle it with a proximal optimizer. The regularizer could be viewed as a\npenalty on the decomposition rank. Hence, its minimization enables to find\ntask-adapted ranks automatically. Our method is evaluated in a realistic\nfew-shot fine-tuning setting, where we compare it first to the standard LoRA\nand then to several other PEFT methods across two distinguishable tasks: base\norgans and novel organs. Our extensive experiments demonstrate the significant\nperformance improvements driven by our method, highlighting its efficiency and\nrobustness against suboptimal rank initialization. Our code is publicly\navailable: https://github.com/ghassenbaklouti/ARENA", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5185\u5728\u79e9\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7l_1\u6b63\u5219\u5316\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfLoRA\u65b9\u6cd5\u4e2d\u56fa\u5b9a\u79e9\u9009\u62e9\u5728\u533b\u5b66\u56fe\u50cf\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u5229\u7528\u81ea\u7136\u56fe\u50cf\u5904\u7406\u9886\u57df\u7684\u8fdb\u5c55\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7l_1\u7a00\u758f\u6b63\u5219\u5316\u5668\u548c\u8fd1\u7aef\u4f18\u5316\u5668\u52a8\u6001\u8c03\u6574\u5185\u5728\u79e9\u3002", "result": "\u5728\u5c11\u91cf\u6837\u672c\u5fae\u8c03\u8bbe\u7f6e\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u6807\u51c6LoRA\u548c\u5176\u4ed6PEFT\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u57fa\u7840\u5668\u5b98\u548c\u65b0\u5668\u5b98\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5185\u5728\u79e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5bf9\u6b21\u4f18\u79e9\u521d\u59cb\u5316\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.15798", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15798", "abs": "https://arxiv.org/abs/2507.15798", "authors": ["Lilian Hollard", "Lucas Mohimont", "Nathalie Gaveau", "Luiz-Angelo Steffenel"], "title": "Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models", "comment": null, "summary": "The paper investigates the performance of state-of-the-art low-parameter deep\nneural networks for computer vision, focusing on bottleneck architectures and\ntheir behavior using superlinear activation functions. We address interference\nin feature maps, a phenomenon associated with superposition, where neurons\nsimultaneously encode multiple characteristics. Our research suggests that\nlimiting interference can enhance scaling and accuracy in very low-scaled\nnetworks (under 1.5M parameters). We identify key design elements that reduce\ninterference by examining various bottleneck architectures, leading to a more\nefficient neural network. Consequently, we propose a proof-of-concept\narchitecture named NoDepth Bottleneck built on mechanistic insights from our\nexperiments, demonstrating robust scaling accuracy on the ImageNet dataset.\nThese findings contribute to more efficient and scalable neural networks for\nthe low-parameter range and advance the understanding of bottlenecks in\ncomputer vision. https://caiac.pubpub.org/pub/3dh6rsel", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f4e\u53c2\u6570\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u901a\u8fc7\u51cf\u5c11\u7279\u5f81\u56fe\u5e72\u6270\u6765\u63d0\u5347\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aNoDepth Bottleneck\u7684\u6982\u5ff5\u67b6\u6784\uff0c\u5728ImageNet\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u4f4e\u53c2\u6570\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u7279\u5f81\u56fe\u5e72\u6270\u73b0\u8c61\u5bf9\u6027\u80fd\u548c\u6269\u5c55\u6027\u7684\u5f71\u54cd\uff0c\u65e8\u5728\u901a\u8fc7\u51cf\u5c11\u5e72\u6270\u6765\u63d0\u5347\u6781\u4f4e\u53c2\u6570\u7f51\u7edc\uff08\u5c11\u4e8e150\u4e07\u53c2\u6570\uff09\u7684\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u8bba\u6587\u7814\u7a76\u4e86\u74f6\u9888\u67b6\u6784\u53ca\u5176\u5728\u4f7f\u7528\u8d85\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u65f6\u7684\u884c\u4e3a\uff0c\u91cd\u70b9\u89e3\u51b3\u4e86\u7279\u5f81\u56fe\u4e2d\u7684\u5e72\u6270\u95ee\u9898\u3002\u901a\u8fc7\u5206\u6790\u591a\u79cd\u74f6\u9888\u67b6\u6784\uff0c\u786e\u5b9a\u4e86\u51cf\u5c11\u5e72\u6270\u7684\u5173\u952e\u8bbe\u8ba1\u8981\u7d20\uff0c\u5e76\u63d0\u51fa\u4e86NoDepth Bottleneck\u8fd9\u4e00\u6982\u5ff5\u67b6\u6784\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9650\u5236\u5e72\u6270\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6781\u4f4e\u53c2\u6570\u7f51\u7edc\u7684\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u3002NoDepth Bottleneck\u67b6\u6784\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u7a33\u5065\u7684\u6269\u5c55\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u7814\u7a76\u4f4e\u53c2\u6570\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNoDepth Bottleneck\u7684\u6982\u5ff5\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u57fa\u4e8e\u5b9e\u9a8c\u7684\u673a\u68b0\u6027\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u7684\u5f3a\u5927\u6269\u5c55\u51c6\u786e\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u5728\u4f4e\u53c2\u6570\u8303\u56f4\u5185\u6784\u5efa\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u6df1\u5316\u4e86\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u74f6\u9888\u67b6\u6784\u7684\u7406\u89e3\u3002"}}
{"id": "2507.15809", "categories": ["cs.CV", "cs.LG", "physics.geo-ph", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.15809", "abs": "https://arxiv.org/abs/2507.15809", "authors": ["Roberto Miele", "Niklas Linde"], "title": "Diffusion models for multivariate subsurface generation and efficient probabilistic inversion", "comment": null, "summary": "Diffusion models offer stable training and state-of-the-art performance for\ndeep generative modeling tasks. Here, we consider their use in the context of\nmultivariate subsurface modeling and probabilistic inversion. We first\ndemonstrate that diffusion models enhance multivariate modeling capabilities\ncompared to variational autoencoders and generative adversarial networks. In\ndiffusion modeling, the generative process involves a comparatively large\nnumber of time steps with update rules that can be modified to account for\nconditioning data. We propose different corrections to the popular Diffusion\nPosterior Sampling approach by Chung et al. (2023). In particular, we introduce\na likelihood approximation accounting for the noise-contamination that is\ninherent in diffusion modeling. We assess performance in a multivariate\ngeological scenario involving facies and correlated acoustic impedance.\nConditional modeling is demonstrated using both local hard data (well logs) and\nnonlinear geophysics (fullstack seismic data). Our tests show significantly\nimproved statistical robustness, enhanced sampling of the posterior probability\ndensity function and reduced computational costs, compared to the original\napproach. The method can be used with both hard and indirect conditioning data,\nindividually or simultaneously. As the inversion is included within the\ndiffusion process, it is faster than other methods requiring an outer-loop\naround the generative model, such as Markov chain Monte Carlo.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5728\u591a\u5143\u5730\u4e0b\u5efa\u6a21\u548c\u6982\u7387\u53cd\u6f14\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6b63\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u7a33\u5065\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6df1\u5ea6\u751f\u6210\u5efa\u6a21\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u5143\u5730\u4e0b\u5efa\u6a21\u548c\u6982\u7387\u53cd\u6f14\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u5bf9Chung\u7b49\u4eba\uff082023\uff09\u7684\u6269\u6563\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\u7684\u4fee\u6b63\uff0c\u5305\u62ec\u5f15\u5165\u8003\u8651\u566a\u58f0\u6c61\u67d3\u7684\u4f3c\u7136\u8fd1\u4f3c\u3002", "result": "\u5728\u6d89\u53ca\u5ca9\u76f8\u548c\u76f8\u5173\u58f0\u963b\u6297\u7684\u591a\u5143\u5730\u8d28\u573a\u666f\u4e2d\uff0c\u6269\u6563\u6a21\u578b\u8868\u73b0\u51fa\u4f18\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u6027\u80fd\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u591a\u5143\u5730\u4e0b\u5efa\u6a21\u548c\u6982\u7387\u53cd\u6f14\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7edf\u8ba1\u7a33\u5065\u6027\u3001\u540e\u9a8c\u6982\u7387\u5bc6\u5ea6\u91c7\u6837\u7684\u6548\u7387\uff0c\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.15824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15824", "abs": "https://arxiv.org/abs/2507.15824", "authors": ["Enes Sanli", "Baris Sarper Tezcan", "Aykut Erdem", "Erkut Erdem"], "title": "Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models", "comment": null, "summary": "Recent progress in text-to-video (T2V) generation has enabled the synthesis\nof visually compelling and temporally coherent videos from natural language.\nHowever, these models often fall short in basic physical commonsense, producing\noutputs that violate intuitive expectations around causality, object behavior,\nand tool use. Addressing this gap, we present PhysVidBench, a benchmark\ndesigned to evaluate the physical reasoning capabilities of T2V systems. The\nbenchmark includes 383 carefully curated prompts, emphasizing tool use,\nmaterial properties, and procedural interactions, and domains where physical\nplausibility is crucial. For each prompt, we generate videos using diverse\nstate-of-the-art models and adopt a three-stage evaluation pipeline: (1)\nformulate grounded physics questions from the prompt, (2) caption the generated\nvideo with a vision-language model, and (3) task a language model to answer\nseveral physics-involved questions using only the caption. This indirect\nstrategy circumvents common hallucination issues in direct video-based\nevaluation. By highlighting affordances and tool-mediated actions, areas\noverlooked in current T2V evaluations, PhysVidBench provides a structured,\ninterpretable framework for assessing physical commonsense in generative video\nmodels.", "AI": {"tldr": "PhysVidBench \u662f\u4e00\u4e2a\u8bc4\u4f30\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7269\u7406\u5e38\u8bc6\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u95f4\u63a5\u8bc4\u4f30\u6d41\u7a0b\uff0c\u586b\u8865\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7269\u7406\u5408\u7406\u6027\u8bc4\u4f30\u4e0a\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u7269\u7406\u5e38\u8bc6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8f93\u51fa\u7684\u89c6\u9891\u5e38\u8fdd\u53cd\u56e0\u679c\u5173\u7cfb\u3001\u5bf9\u8c61\u884c\u4e3a\u548c\u5de5\u5177\u4f7f\u7528\u7b49\u76f4\u89c9\u9884\u671f\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u8bc4\u4f30\u7269\u7406\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u901a\u8fc7\u4e09\u9636\u6bb5\u8bc4\u4f30\u6d41\u7a0b\uff081\uff09\u4ece\u63d0\u793a\u4e2d\u5236\u5b9a\u57fa\u4e8e\u7269\u7406\u7684\u95ee\u9898\uff0c\uff082\uff09\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3a\u751f\u6210\u7684\u89c6\u9891\u751f\u6210\u5b57\u5e55\uff0c\uff083\uff09\u8ba9\u8bed\u8a00\u6a21\u578b\u4ec5\u57fa\u4e8e\u5b57\u5e55\u56de\u7b54\u7269\u7406\u76f8\u5173\u95ee\u9898\uff0c\u95f4\u63a5\u8bc4\u4f30\u89c6\u9891\u7684\u7269\u7406\u5408\u7406\u6027\u3002", "result": "PhysVidBench \u5305\u542b 383 \u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\uff0c\u6db5\u76d6\u5de5\u5177\u4f7f\u7528\u3001\u6750\u6599\u5c5e\u6027\u548c\u7a0b\u5e8f\u4ea4\u4e92\u7b49\u9886\u57df\uff0c\u5e76\u901a\u8fc7\u591a\u9636\u6bb5\u8bc4\u4f30\u6d41\u7a0b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "PhysVidBench \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u89c6\u9891\u6a21\u578b\u4e2d\u7684\u7269\u7406\u5e38\u8bc6\uff0c\u586b\u8865\u4e86\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u7269\u7406\u5408\u7406\u6027\u8bc4\u4f30\u4e0a\u7684\u7a7a\u767d\u3002"}}
{"id": "2507.15856", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15856", "abs": "https://arxiv.org/abs/2507.15856", "authors": ["Jiawei Yang", "Tianhong Li", "Lijie Fan", "Yonglong Tian", "Yue Wang"], "title": "Latent Denoising Makes Good Visual Tokenizers", "comment": "Code is available at: https://github.com/Jiawei-Yang/DeTok", "summary": "Despite their fundamental role, it remains unclear what properties could make\nvisual tokenizers more effective for generative modeling. We observe that\nmodern generative models share a conceptually similar training objective --\nreconstructing clean signals from corrupted inputs such as Gaussian noise or\nmasking -- a process we term denoising. Motivated by this insight, we propose\naligning tokenizer embeddings directly with the downstream denoising objective,\nencouraging latent embeddings to be more easily reconstructed even when heavily\ncorrupted. To achieve this, we introduce the Latent Denoising Tokenizer\n(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images\nfrom latent embeddings corrupted by interpolative noise and random masking.\nExtensive experiments on ImageNet 256x256 demonstrate that our tokenizer\nconsistently outperforms standard tokenizers across six representative\ngenerative models. Our findings highlight denoising as a fundamental design\nprinciple for tokenizer development, and we hope it could motivate new\nperspectives for future tokenizer design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u53bb\u566a\u539f\u5219\u7684\u89c6\u89c9\u5206\u8bcd\u5668l-DeTok\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u5206\u8bcd\u5668\u3002", "motivation": "\u89c2\u5bdf\u5230\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u76ee\u6807\uff08\u4ece\u566a\u58f0\u6216\u63a9\u7801\u8f93\u5165\u4e2d\u91cd\u5efa\u5e72\u51c0\u4fe1\u53f7\uff09\u4e0e\u5206\u8bcd\u5668\u8bbe\u8ba1\u4e4b\u95f4\u5b58\u5728\u6f5c\u5728\u8054\u7cfb\uff0c\u5e0c\u671b\u901a\u8fc7\u5bf9\u9f50\u5206\u8bcd\u5668\u5d4c\u5165\u4e0e\u4e0b\u6e38\u53bb\u566a\u76ee\u6807\u6765\u63d0\u5347\u751f\u6210\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5f15\u5165Latent Denoising Tokenizer\uff08l-DeTok\uff09\uff0c\u901a\u8fc7\u8bad\u7ec3\u5206\u8bcd\u5668\u4ece\u53d7\u63d2\u503c\u566a\u58f0\u548c\u968f\u673a\u63a9\u7801\u6c61\u67d3\u7684\u6f5c\u5728\u5d4c\u5165\u4e2d\u91cd\u5efa\u5e72\u51c0\u56fe\u50cf\u3002", "result": "\u5728ImageNet 256x256\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cl-DeTok\u5206\u8bcd\u5668\u5728\u516d\u79cd\u4ee3\u8868\u6027\u751f\u6210\u6a21\u578b\u4e2d\u5747\u4f18\u4e8e\u6807\u51c6\u5206\u8bcd\u5668\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u5206\u8bcd\u5668\u8bbe\u8ba1\u539f\u5219\u2014\u2014\u53bb\u566a\uff0c\u901a\u8fc7l-DeTok\u5206\u8bcd\u5668\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u539f\u5219\u7684\u6709\u6548\u6027\uff0c\u5e76\u5e0c\u671b\u8fd9\u4e00\u53d1\u73b0\u80fd\u6fc0\u52b1\u672a\u6765\u5206\u8bcd\u5668\u8bbe\u8ba1\u7684\u65b0\u89c6\u89d2\u3002"}}
