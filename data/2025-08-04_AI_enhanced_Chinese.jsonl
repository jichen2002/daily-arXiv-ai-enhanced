{"id": "2508.00097", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00097", "abs": "https://arxiv.org/abs/2508.00097", "authors": ["Zhigen Zhao", "Liuchuan Yu", "Ke Jing", "Ning Yang"], "title": "XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation", "comment": "6 pages, 6 figures, project link: https://github.com/XR-Robotics", "summary": "The rapid advancement of Vision-Language-Action models has created an urgent\nneed for large-scale, high-quality robot demonstration datasets. Although\nteleoperation is the predominant method for data collection, current approaches\nsuffer from limited scalability, complex setup procedures, and suboptimal data\nquality. This paper presents XRoboToolkit, a cross-platform framework for\nextended reality based robot teleoperation built on the OpenXR standard. The\nsystem features low-latency stereoscopic visual feedback, optimization-based\ninverse kinematics, and support for diverse tracking modalities including head,\ncontroller, hand, and auxiliary motion trackers. XRoboToolkit's modular\narchitecture enables seamless integration across robotic platforms and\nsimulation environments, spanning precision manipulators, mobile robots, and\ndexterous hands. We demonstrate the framework's effectiveness through precision\nmanipulation tasks and validate data quality by training VLA models that\nexhibit robust autonomous performance.", "AI": {"tldr": "XRoboToolkit \u662f\u4e00\u4e2a\u57fa\u4e8e OpenXR \u7684\u8de8\u5e73\u53f0\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u5ef6\u8fdf\u89c6\u89c9\u53cd\u9988\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u7684 scalability \u548c\u8d28\u91cf\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u5f53\u524d\u9065\u64cd\u4f5c\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u8bbe\u7f6e\u590d\u6742\u6027\u548c\u6570\u636e\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u96c6\u6536\u96c6\u65b9\u6848\u3002", "method": "\u8be5\u7cfb\u7edf\u91c7\u7528\u4f4e\u5ef6\u8fdf\u7acb\u4f53\u89c6\u89c9\u53cd\u9988\u3001\u57fa\u4e8e\u4f18\u5316\u7684\u9006\u8fd0\u52a8\u5b66\u4ee5\u53ca\u591a\u79cd\u8ddf\u8e2a\u6a21\u5f0f\uff08\u5982\u5934\u90e8\u3001\u63a7\u5236\u5668\u3001\u624b\u90e8\u548c\u8f85\u52a9\u8fd0\u52a8\u8ddf\u8e2a\u5668\uff09\uff0c\u652f\u6301\u6a21\u5757\u5316\u67b6\u6784\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u548c\u4eff\u771f\u73af\u5883\u3002", "result": "XRoboToolkit \u5728\u7cbe\u786e\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3 VLA \u6a21\u578b\u9a8c\u8bc1\u4e86\u5176\u6570\u636e\u8d28\u91cf\uff0c\u6a21\u578b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u81ea\u4e3b\u6027\u80fd\u3002", "conclusion": "XRoboToolkit \u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8de8\u5e73\u53f0\u6846\u67b6\uff0c\u901a\u8fc7 OpenXR \u6807\u51c6\u5b9e\u73b0\u57fa\u4e8e\u6269\u5c55\u73b0\u5b9e\u7684\u673a\u5668\u4eba\u9065\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u6536\u96c6\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2508.00162", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00162", "abs": "https://arxiv.org/abs/2508.00162", "authors": ["Noboru Myers", "Obin Kwon", "Sankalp Yamsani", "Joohyung Kim"], "title": "CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System", "comment": null, "summary": "Recent advances in teleoperation have demonstrated robots performing complex\nmanipulation tasks. However, existing works rarely support whole-body\njoint-level teleoperation for humanoid robots, limiting the diversity of tasks\nthat can be accomplished. This work presents Controller for Humanoid Imitation\nand Live Demonstration (CHILD), a compact reconfigurable teleoperation system\nthat enables joint level control over humanoid robots. CHILD fits within a\nstandard baby carrier, allowing the operator control over all four limbs, and\nsupports both direct joint mapping for full-body control and loco-manipulation.\nAdaptive force feedback is incorporated to enhance operator experience and\nprevent unsafe joint movements. We validate the capabilities of this system by\nconducting loco-manipulation and full-body control examples on a humanoid robot\nand multiple dual-arm systems. Lastly, we open-source the design of the\nhardware promoting accessibility and reproducibility. Additional details and\nopen-source information are available at our project website:\nhttps://uiuckimlab.github.io/CHILD-pages.", "AI": {"tldr": "CHILD \u662f\u4e00\u4e2a\u7d27\u51d1\u7684\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u652f\u6301\u4eff\u4eba\u673a\u5668\u4eba\u7684\u5173\u8282\u7ea7\u63a7\u5236\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u529b\u53cd\u9988\u63d0\u5347\u64cd\u4f5c\u4f53\u9a8c\uff0c\u5e76\u5f00\u6e90\u786c\u4ef6\u8bbe\u8ba1\u4ee5\u4fc3\u8fdb\u53ef\u8bbf\u95ee\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u652f\u6301\u4eff\u4eba\u673a\u5668\u4eba\u7684\u5168\u8eab\u5173\u8282\u7ea7\u9065\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u4efb\u52a1\u7684\u591a\u6837\u6027\u3002CHILD \u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86 CHILD\uff0c\u4e00\u4e2a\u7d27\u51d1\u53ef\u91cd\u6784\u7684\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u652f\u6301\u4eff\u4eba\u673a\u5668\u4eba\u7684\u5173\u8282\u7ea7\u63a7\u5236\uff0c\u5e76\u96c6\u6210\u81ea\u9002\u5e94\u529b\u53cd\u9988\u4ee5\u589e\u5f3a\u64cd\u4f5c\u4f53\u9a8c\u548c\u5b89\u5168\u6027\u3002", "result": "\u5728\u4eff\u4eba\u673a\u5668\u4eba\u548c\u591a\u81c2\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86 CHILD \u7684\u5168\u8eab\u63a7\u5236\u548c\u79fb\u52a8\u64cd\u4f5c\u80fd\u529b\uff0c\u5e76\u5f00\u6e90\u4e86\u786c\u4ef6\u8bbe\u8ba1\u3002", "conclusion": "CHILD \u7cfb\u7edf\u901a\u8fc7\u5f00\u6e90\u8bbe\u8ba1\u4fc3\u8fdb\u4e86\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u4e3a\u4eff\u4eba\u673a\u5668\u4eba\u7684\u5168\u8eab\u5173\u8282\u7ea7\u9065\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00258", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00258", "abs": "https://arxiv.org/abs/2508.00258", "authors": ["Zhiwei Wu", "Siyi Wei", "Jiahao Luo", "Jinhui Zhang"], "title": "Topology-Inspired Morphological Descriptor for Soft Continuum Robots", "comment": null, "summary": "This paper presents a topology-inspired morphological descriptor for soft\ncontinuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory\nto achieve a quantitative characterization of robot morphologies. By counting\ncritical points of directional projections, the proposed descriptor enables a\ndiscrete representation of multimodal configurations and facilitates\nmorphological classification. Furthermore, we apply the descriptor to\nmorphology control by formulating the target configuration as an optimization\nproblem to compute actuation parameters that generate equilibrium shapes with\ndesired topological features. The proposed framework provides a unified\nmethodology for quantitative morphology description, classification, and\ncontrol of soft continuum robots, with the potential to enhance their precision\nand adaptability in medical applications such as minimally invasive surgery and\nendovascular interventions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408PRB\u6a21\u578b\u548c\u83ab\u5c14\u65af\u7406\u8bba\u7684\u62d3\u6251\u5f62\u6001\u63cf\u8ff0\u7b26\uff0c\u7528\u4e8e\u8f6f\u8fde\u7eed\u673a\u5668\u4eba\u7684\u5b9a\u91cf\u5f62\u6001\u63cf\u8ff0\u3001\u5206\u7c7b\u548c\u63a7\u5236\uff0c\u63d0\u5347\u533b\u7597\u5e94\u7528\u7684\u7cbe\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u8f6f\u8fde\u7eed\u673a\u5668\u4eba\u5728\u533b\u7597\u5e94\u7528\uff08\u5982\u5fae\u521b\u624b\u672f\u548c\u8840\u7ba1\u5185\u4ecb\u5165\uff09\u4e2d\u7684\u7cbe\u786e\u6027\u548c\u9002\u5e94\u6027\uff0c\u9700\u8981\u4e00\u79cd\u5b9a\u91cf\u7684\u5f62\u6001\u63cf\u8ff0\u548c\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4f2a\u521a\u4f53\uff08PRB\uff09\u6a21\u578b\u4e0e\u83ab\u5c14\u65af\u7406\u8bba\uff0c\u901a\u8fc7\u8ba1\u7b97\u65b9\u5411\u6295\u5f71\u7684\u4e34\u754c\u70b9\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u62d3\u6251\u542f\u53d1\u7684\u5f62\u6001\u63cf\u8ff0\u7b26\u3002", "result": "\u63d0\u51fa\u7684\u63cf\u8ff0\u7b26\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u914d\u7f6e\u7684\u79bb\u6563\u8868\u793a\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5f62\u6001\u5206\u7c7b\uff0c\u540c\u65f6\u901a\u8fc7\u4f18\u5316\u95ee\u9898\u5b9e\u73b0\u4e86\u5f62\u6001\u63a7\u5236\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8f6f\u8fde\u7eed\u673a\u5668\u4eba\u7684\u5f62\u6001\u63cf\u8ff0\u3001\u5206\u7c7b\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u8bba\uff0c\u6709\u671b\u63d0\u5347\u5176\u5728\u533b\u7597\u5e94\u7528\u4e2d\u7684\u7cbe\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.00398", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00398", "abs": "https://arxiv.org/abs/2508.00398", "authors": ["Sunjae Yoon", "Gwanhyeong Koo", "Younghwan Lee", "Ji Woo Hong", "Chang D. Yoo"], "title": "Occlusion-robust Stylization for Drawing-based 3D Animation", "comment": "11 pages, 13 figures, ICCV 2025", "summary": "3D animation aims to generate a 3D animated video from an input image and a\ntarget 3D motion sequence. Recent advances in image-to-3D models enable the\ncreation of animations directly from user-hand drawings. Distinguished from\nconventional 3D animation, drawing-based 3D animation is crucial to preserve\nartist's unique style properties, such as rough contours and distinct stroke\npatterns. However, recent methods still exhibit quality deterioration in style\nproperties, especially under occlusions caused by overlapping body parts,\nleading to contour flickering and stroke blurring. This occurs due to a\n`stylization pose gap' between training and inference in stylization networks\ndesigned to preserve drawing styles in drawing-based 3D animation systems. The\nstylization pose gap denotes that input target poses used to train the\nstylization network are always in occlusion-free poses, while target poses\nencountered in an inference include diverse occlusions under dynamic motions.\nTo this end, we propose Occlusion-robust Stylization Framework (OSF) for\ndrawing-based 3D animation. We found that while employing object's edge can be\neffective input prior for guiding stylization, it becomes notably inaccurate\nwhen occlusions occur at inference. Thus, our proposed OSF provides\nocclusion-robust edge guidance for stylization network using optical flow,\nensuring a consistent stylization even under occlusions. Furthermore, OSF\noperates in a single run instead of the previous two-stage method, achieving\n2.4x faster inference and 2.1x less memory.", "AI": {"tldr": "\u63d0\u51faOSF\u6846\u67b6\uff0c\u901a\u8fc7\u5149\u6d41\u89e3\u51b3\u906e\u6321\u5bfc\u81f4\u7684\u98ce\u683c\u5316\u6027\u80fd\u4e0b\u964d\uff0c\u5b9e\u73b0\u66f4\u5feb\u66f4\u7701\u5185\u5b58\u7684\u5355\u6b21\u8fd0\u884c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u906e\u6321\u60c5\u51b5\u4e0b\uff08\u5982\u91cd\u53e0\u8eab\u4f53\u90e8\u5206\uff09\u4f1a\u5bfc\u81f4\u8f6e\u5ed3\u95ea\u70c1\u548c\u7b14\u89e6\u6a21\u7cca\uff0c\u4e3b\u8981\u7531\u4e8e\u8bad\u7ec3\u4e0e\u63a8\u7406\u65f6\u7684\u59ff\u6001\u5dee\u5f02\uff08\u5373'\u98ce\u683c\u5316\u59ff\u6001\u95f4\u9699'\uff09\u3002", "method": "\u5229\u7528\u5149\u6d41\u63d0\u4f9b\u906e\u6321\u9c81\u68d2\u7684\u8fb9\u7f18\u5f15\u5bfc\uff0c\u4f18\u5316\u98ce\u683c\u5316\u7f51\u7edc\uff0c\u4f7f\u5176\u5728\u5355\u6b21\u8fd0\u884c\u4e2d\u5b8c\u6210\u5904\u7406\uff0c\u800c\u975e\u4f20\u7edf\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u3002", "result": "OSF\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u4e00\u81f4\u7684\u98ce\u683c\u5316\u6548\u679c\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53472.4\u500d\uff0c\u5185\u5b58\u5360\u7528\u51cf\u5c112.1\u500d\u3002", "conclusion": "\u63d0\u51fa\u4e86Occlusion-robust Stylization Framework (OSF)\uff0c\u901a\u8fc7\u5149\u6d41\u63d0\u4f9b\u906e\u6321\u9c81\u68d2\u7684\u8fb9\u7f18\u5f15\u5bfc\uff0c\u89e3\u51b3\u4e86\u98ce\u683c\u5316\u7f51\u7edc\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u3002"}}
{"id": "2508.00341", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00341", "abs": "https://arxiv.org/abs/2508.00341", "authors": ["Shengheng Liu", "Ningning Fu", "Zhonghao Zhang", "Yongming Huang", "Tony Q. S. Quek"], "title": "Integrated user scheduling and beam steering in over-the-air federated learning for mobile IoT", "comment": "To appear in ACM TOIT. 24 pages, 8 figures", "summary": "The rising popularity of Internet of things (IoT) has spurred technological\nadvancements in mobile internet and interconnected systems. While offering\nflexible connectivity and intelligent applications across various domains, IoT\nservice providers must gather vast amounts of sensitive data from users, which\nnonetheless concomitantly raises concerns about privacy breaches. Federated\nlearning (FL) has emerged as a promising decentralized training paradigm to\ntackle this challenge. This work focuses on enhancing the aggregation\nefficiency of distributed local models by introducing over-the-air computation\ninto the FL framework. Due to radio resource scarcity in large-scale networks,\nonly a subset of users can participate in each training round. This highlights\nthe need for effective user scheduling and model transmission strategies to\noptimize communication efficiency and inference accuracy. To address this, we\npropose an integrated approach to user scheduling and receive beam steering,\nsubject to constraints on the number of selected users and transmit power.\nLeveraging the difference-of-convex technique, we decompose the primal\nnon-convex optimization problem into two sub-problems, yielding an iterative\nsolution. While effective, the computational load of the iterative method\nhampers its practical implementation. To overcome this, we further propose a\nlow-complexity user scheduling policy based on characteristic analysis of the\nwireless channel to directly determine the user subset without iteration.\nExtensive experiments validate the superiority of the proposed method in terms\nof aggregation error and learning performance over existing approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7ed3\u5408\u7a7a\u4e2d\u8ba1\u7b97\u6280\u672f\u548c\u4f18\u5316\u7528\u6237\u8c03\u5ea6\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u901a\u4fe1\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\uff08IoT\uff09\u7684\u666e\u53ca\uff0c\u6570\u636e\u9690\u79c1\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4f5c\u4e3a\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u8303\u5f0f\uff0c\u80fd\u591f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5176\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u4e2d\u7684\u901a\u4fe1\u6548\u7387\u548c\u51c6\u786e\u6027\u4ecd\u9700\u4f18\u5316\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u7528\u6237\u8c03\u5ea6\u548c\u63a5\u6536\u6ce2\u675f\u63a7\u5236\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5dee\u5206\u51f8\u6280\u672f\u5c06\u975e\u51f8\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u6c42\u89e3\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u7ebf\u4fe1\u9053\u7279\u6027\u5206\u6790\u7684\u4f4e\u590d\u6742\u5ea6\u7528\u6237\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u805a\u5408\u8bef\u5dee\u548c\u5b66\u4e60\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7a7a\u4e2d\u8ba1\u7b97\u6280\u672f\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u7528\u6237\u8c03\u5ea6\u548c\u63a5\u6536\u6ce2\u675f\u63a7\u5236\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u4fe1\u6548\u7387\u548c\u63a8\u65ad\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.00288", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00288", "abs": "https://arxiv.org/abs/2508.00288", "authors": ["Jianqiang Xiao", "Yuexuan Sun", "Yixin Shao", "Boxi Gan", "Rongqiang Liu", "Yanjing Wu", "Weili Gua", "Xiang Deng"], "title": "UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents", "comment": "Accepted to ACM MM Dataset Track 2025", "summary": "Aerial navigation is a fundamental yet underexplored capability in embodied\nintelligence, enabling agents to operate in large-scale, unstructured\nenvironments where traditional navigation paradigms fall short. However, most\nexisting research follows the Vision-and-Language Navigation (VLN) paradigm,\nwhich heavily depends on sequential linguistic instructions, limiting its\nscalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark\nfor large-scale Object Goal Navigation (ObjectNav) by aerial agents in\nopen-world environments, where agents operate based on high-level semantic\ngoals without relying on detailed instructional guidance as in VLN. UAV-ON\ncomprises 14 high-fidelity Unreal Engine environments with diverse semantic\nregions and complex spatial layouts, covering urban, natural, and mixed-use\nsettings. It defines 1270 annotated target objects, each characterized by an\ninstance-level instruction that encodes category, physical footprint, and\nvisual descriptors, allowing grounded reasoning. These instructions serve as\nsemantic goals, introducing realistic ambiguity and complex reasoning\nchallenges for aerial agents. To evaluate the benchmark, we implement several\nbaseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that\nintegrates instruction semantics with egocentric observations for long-horizon,\ngoal-directed exploration. Empirical results show that all baselines struggle\nin this setting, highlighting the compounded challenges of aerial navigation\nand semantic goal grounding. UAV-ON aims to advance research on scalable UAV\nautonomy driven by semantic goal descriptions in complex real-world\nenvironments.", "AI": {"tldr": "UAV-ON\u662f\u4e00\u4e2a\u9488\u5bf9\u7a7a\u4e2d\u667a\u80fd\u4f53\u7684\u5927\u89c4\u6a21\u76ee\u6807\u5bfc\u822a\u57fa\u51c6\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edfVLN\u8303\u5f0f\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u8bed\u4e49\u76ee\u6807\u9a71\u52a8\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u548c\u8bed\u8a00\u5bfc\u822a(VLN)\u8303\u5f0f\uff0c\u4f9d\u8d56\u987a\u5e8f\u8bed\u8a00\u6307\u4ee4\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u81ea\u4e3b\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86UAV-ON\u57fa\u51c6\uff0c\u5305\u542b14\u4e2a\u9ad8\u4fdd\u771fUnreal Engine\u73af\u5883\u548c1270\u4e2a\u6807\u6ce8\u76ee\u6807\u5bf9\u8c61\uff0c\u5e76\u5b9e\u73b0\u4e86Aerial ObjectNav Agent (AOA)\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u5728\u8fd9\u4e00\u8bbe\u5b9a\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u4e86\u7a7a\u4e2d\u5bfc\u822a\u548c\u8bed\u4e49\u76ee\u6807\u57fa\u7840\u7684\u590d\u5408\u6311\u6218\u3002", "conclusion": "UAV-ON\u65e8\u5728\u63a8\u52a8\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u57fa\u4e8e\u8bed\u4e49\u76ee\u6807\u63cf\u8ff0\u7684\u53ef\u6269\u5c55\u65e0\u4eba\u673a\u81ea\u4e3b\u6027\u7814\u7a76\u3002"}}
{"id": "2508.00424", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.00424", "abs": "https://arxiv.org/abs/2508.00424", "authors": ["Kresimir Matkovic", "Rainer Splechtna", "Denis Gracanin", "Helwig Hauser"], "title": "CrossSet: Unveiling the Complex Interplay of Two Set-typed Dimensions in Multivariate Data", "comment": "Will be published in TVCG and presented at IEEE VIS", "summary": "The interactive visual analysis of set-typed data, i.e., data with attributes\nthat are of type set, is a rewarding area of research and applications.\nValuable prior work has contributed solutions that enable the study of such\ndata with individual set-typed dimensions. In this paper, we present CrossSet,\na novel method for the joint study of two set-typed dimensions and their\ninterplay. Based on a task analysis, we describe a new, multi-scale approach to\nthe interactive visual exploration and analysis of such data. Two set-typed\ndata dimensions are jointly visualized using a hierarchical matrix layout,\nenabling the analysis of the interactions between two set-typed attributes at\nseveral levels, in addition to the analysis of individual such dimensions.\nCrossSet is anchored at a compact, large-scale overview that is complemented by\ndrill-down opportunities to study the relations between and within the\nset-typed dimensions, enabling an interactive visual multi-scale exploration\nand analysis of bivariate set-typed data. Such an interactive approach makes it\npossible to study single set-typed dimensions in detail, to gain an overview of\nthe interaction and association between two such dimensions, to refine one of\nthe dimensions to gain additional details at several levels, and to drill down\nto the specific interactions of individual set-elements from the set-typed\ndimensions. To demonstrate the effectiveness and efficiency of CrossSet, we\nhave evaluated the new method in the context of several application scenarios.", "AI": {"tldr": "CrossSet\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u6b21\u77e9\u9635\u5e03\u5c40\u548c\u4ea4\u4e92\u5f0f\u591a\u5c3a\u5ea6\u5206\u6790\uff0c\u652f\u6301\u4e24\u4e2a\u96c6\u5408\u7c7b\u578b\u7ef4\u5ea6\u53ca\u5176\u76f8\u4e92\u4f5c\u7528\u7684\u8054\u5408\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u9488\u5bf9\u5355\u4e2a\u96c6\u5408\u7c7b\u578b\u7ef4\u5ea6\u7684\u7814\u7a76\uff0c\u7f3a\u4e4f\u5bf9\u4e24\u4e2a\u96c6\u5408\u7c7b\u578b\u7ef4\u5ea6\u53ca\u5176\u76f8\u4e92\u4f5c\u7528\u7684\u8054\u5408\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u4efb\u52a1\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u63a2\u7d22\u548c\u5206\u6790\u65b9\u6cd5\uff0c\u91c7\u7528\u5c42\u6b21\u77e9\u9635\u5e03\u5c40\u8054\u5408\u53ef\u89c6\u5316\u4e24\u4e2a\u96c6\u5408\u7c7b\u578b\u6570\u636e\u7ef4\u5ea6\u3002", "result": "\u901a\u8fc7\u591a\u4e2a\u5e94\u7528\u573a\u666f\u7684\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86CrossSet\u5728\u4ea4\u4e92\u5f0f\u89c6\u89c9\u591a\u5c3a\u5ea6\u63a2\u7d22\u548c\u5206\u6790\u53cc\u53d8\u91cf\u96c6\u5408\u7c7b\u578b\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "CrossSet\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5206\u6790\u53cc\u53d8\u91cf\u96c6\u5408\u7c7b\u578b\u6570\u636e\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u77e9\u9635\u5e03\u5c40\u548c\u94bb\u53d6\u529f\u80fd\uff0c\u652f\u6301\u5bf9\u96c6\u5408\u7c7b\u578b\u7ef4\u5ea6\u53ca\u5176\u76f8\u4e92\u4f5c\u7528\u7684\u8be6\u7ec6\u7814\u7a76\u3002"}}
{"id": "2508.00426", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00426", "abs": "https://arxiv.org/abs/2508.00426", "authors": ["Rohan Gandhi", "Ankur Mallick", "Ken Sueda", "Rui Liang"], "title": "Tetris: Efficient Intra-Datacenter Calls Packing for Large Conferencing Services", "comment": null, "summary": "Conference services like Zoom, Microsoft Teams, and Google Meet facilitate\nmillions of daily calls, yet ensuring high performance at low costs remains a\nsignificant challenge. This paper revisits the problem of packing calls across\nMedia Processor (MP) servers that host the calls within individual datacenters\n(DCs). We show that the algorithm used in Teams -- a large scale conferencing\nservice as well as other state-of-art algorithms are prone to placing calls\nresulting in some of the MPs becoming hot (high CPU utilization) that leads to\ndegraded performance and/or elevated hosting costs. The problem arises from\ndisregarding the variability in CPU usage among calls, influenced by\ndifferences in participant numbers and media types (audio/video), compounded by\nbursty call arrivals. To tackle this, we propose Tetris, a multi-step framework\nwhich (a) optimizes initial call assignments by leveraging historical data and\n(b) periodically migrates calls from hot MPs using linear optimization, aiming\nto minimize hot MP usage. Evaluation based on a 24-hour trace of over 10\nmillion calls in one DC shows that Tetris reduces participant numbers on hot\nMPs by at least 2.5X.", "AI": {"tldr": "Tetris\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u547c\u53eb\u5206\u914d\u548c\u8fc1\u79fb\uff0c\u51cf\u5c11\u70ed\u70b9MP\u670d\u52a1\u5668\u4f7f\u7528\uff0c\u63d0\u5347\u4f1a\u8bae\u670d\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7b97\u6cd5\u5728\u5904\u7406\u547c\u53eb\u5206\u914d\u65f6\u5ffd\u89c6\u4e86CPU\u4f7f\u7528\u7387\u7684\u53d8\u5f02\u6027\uff0c\u5bfc\u81f4\u90e8\u5206MP\u670d\u52a1\u5668\u8fc7\u8f7d\uff0c\u5f71\u54cd\u6027\u80fd\u5e76\u589e\u52a0\u6210\u672c\u3002", "method": "Tetris\u91c7\u7528\u591a\u6b65\u9aa4\u6846\u67b6\uff0c\u5305\u62ec\u5229\u7528\u5386\u53f2\u6570\u636e\u4f18\u5316\u521d\u59cb\u547c\u53eb\u5206\u914d\uff0c\u4ee5\u53ca\u901a\u8fc7\u7ebf\u6027\u4f18\u5316\u5468\u671f\u6027\u8fc1\u79fb\u70ed\u70b9MP\u4e0a\u7684\u547c\u53eb\u3002", "result": "\u57fa\u4e8e24\u5c0f\u65f6\u8d85\u8fc71000\u4e07\u6b21\u547c\u53eb\u7684\u8ddf\u8e2a\u6570\u636e\uff0cTetris\u5c06\u70ed\u70b9MP\u4e0a\u7684\u53c2\u4e0e\u8005\u6570\u91cf\u51cf\u5c11\u4e86\u81f3\u5c112.5\u500d\u3002", "conclusion": "Tetris\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u547c\u53eb\u5206\u914d\u548c\u5468\u671f\u6027\u8fc1\u79fb\u547c\u53eb\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u70ed\u70b9MP\u670d\u52a1\u5668\u7684\u4f7f\u7528\uff0c\u63d0\u5347\u4e86\u4f1a\u8bae\u670d\u52a1\u7684\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2508.00604", "categories": ["cs.OS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00604", "abs": "https://arxiv.org/abs/2508.00604", "authors": ["Rajpreet Singh", "Vidhi Kothari"], "title": "Composable OS Kernel Architectures for Autonomous Intelligence", "comment": "8 pages", "summary": "As intelligent systems permeate edge devices, cloud infrastructure, and\nembedded real-time environments, this research proposes a new OS kernel\narchitecture for intelligent systems, transforming kernels from static resource\nmanagers to adaptive, AI-integrated platforms. Key contributions include: (1)\ntreating Loadable Kernel Modules (LKMs) as AI-oriented computation units for\nfast sensory and cognitive processing in kernel space; (2) expanding the Linux\nkernel into an AI-native environment with built-in deep learning inference,\nfloating-point acceleration, and real-time adaptive scheduling for efficient ML\nworkloads; and (3) introducing a Neurosymbolic kernel design leveraging\nCategory Theory and Homotopy Type Theory to unify symbolic reasoning and\ndifferentiable logic within OS internals. Together, these approaches enable\noperating systems to proactively anticipate and adapt to the cognitive needs of\nautonomous intelligent applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bAI\u96c6\u6210\u7684\u64cd\u4f5c\u7cfb\u7edf\u5185\u6838\u67b6\u6784\uff0c\u901a\u8fc7\u6269\u5c55Linux\u5185\u6838\u548c\u5f15\u5165\u795e\u7ecf\u7b26\u53f7\u8bbe\u8ba1\uff0c\u4f7f\u64cd\u4f5c\u7cfb\u7edf\u80fd\u81ea\u9002\u5e94\u667a\u80fd\u5e94\u7528\u7684\u9700\u6c42\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u7cfb\u7edf\u5728\u8fb9\u7f18\u8bbe\u5907\u3001\u4e91\u57fa\u7840\u8bbe\u65bd\u548c\u5d4c\u5165\u5f0f\u5b9e\u65f6\u73af\u5883\u4e2d\u7684\u666e\u53ca\uff0c\u4f20\u7edf\u9759\u6001\u8d44\u6e90\u7ba1\u7406\u7684\u5185\u6838\u67b6\u6784\u5df2\u65e0\u6cd5\u6ee1\u8db3\u667a\u80fd\u7cfb\u7edf\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u3001AI\u96c6\u6210\u7684\u5185\u6838\u67b6\u6784\u3002", "method": "1. \u5c06\u53ef\u52a0\u8f7d\u5185\u6838\u6a21\u5757\uff08LKMs\uff09\u89c6\u4e3aAI\u5bfc\u5411\u7684\u8ba1\u7b97\u5355\u5143\uff0c\u7528\u4e8e\u5185\u6838\u7a7a\u95f4\u7684\u5feb\u901f\u611f\u77e5\u548c\u8ba4\u77e5\u5904\u7406\uff1b2. \u5c06Linux\u5185\u6838\u6269\u5c55\u4e3aAI\u539f\u751f\u73af\u5883\uff0c\u5185\u7f6e\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\u3001\u6d6e\u70b9\u52a0\u901f\u548c\u5b9e\u65f6\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u4ee5\u9ad8\u6548\u5904\u7406\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\uff1b3. \u5f15\u5165\u795e\u7ecf\u7b26\u53f7\u5185\u6838\u8bbe\u8ba1\uff0c\u5229\u7528\u8303\u7574\u8bba\u548c\u540c\u4f26\u7c7b\u578b\u7406\u8bba\u5728\u64cd\u4f5c\u7cfb\u7edf\u5185\u90e8\u7edf\u4e00\u7b26\u53f7\u63a8\u7406\u548c\u53ef\u5fae\u5206\u903b\u8f91\u3002", "result": "\u901a\u8fc7\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u64cd\u4f5c\u7cfb\u7edf\u80fd\u591f\u4e3b\u52a8\u9884\u6d4b\u5e76\u9002\u5e94\u81ea\u4e3b\u667a\u80fd\u5e94\u7528\u7684\u8ba4\u77e5\u9700\u6c42\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u64cd\u4f5c\u7cfb\u7edf\u5185\u6838\u67b6\u6784\uff0c\u5c06\u5185\u6838\u4ece\u9759\u6001\u8d44\u6e90\u7ba1\u7406\u5668\u8f6c\u53d8\u4e3a\u81ea\u9002\u5e94\u3001AI\u96c6\u6210\u7684\u5e73\u53f0\uff0c\u4ee5\u6ee1\u8db3\u667a\u80fd\u7cfb\u7edf\u7684\u9700\u6c42\u3002"}}
{"id": "2508.00031", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00031", "abs": "https://arxiv.org/abs/2508.00031", "authors": ["Junde Wu"], "title": "Git Context Controller: Manage the Context of LLM-based Agents like Git", "comment": "in updating", "summary": "Large language model (LLM) based agents have shown impressive capabilities by\ninterleaving internal reasoning with external tool use. However, as these\nagents are deployed in long-horizon workflows, such as coding for a big,\nlong-term project, context management becomes a critical bottleneck. We\nintroduce Git-Context-Controller (GCC), a structured context management\nframework inspired by software version control systems. GCC elevates context as\nversioned memory hierarchy like Git. It structures agent memory as a persistent\nfile system with explicit operations: COMMIT, BRANCH, MERGE, and CONTEXT,\nenabling milestone-based checkpointing, exploration of alternative plans, and\nstructured reflection. Our approach empowers agents to manage long-term goals,\nisolate architectural experiments, and recover or hand off memory across\nsessions and agents. Empirically, agents equipped with GCC achieve\nstate-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00\nof software bugs, outperforming 26 competitive systems. In a self-replication\ncase study, a GCC-augmented agent builds a new CLI agent from scratch,\nachieving 40.7 task resolution, compared to only 11.7 without GCC. The code is\nreleased at: https://github.com/theworldofagents/GCC", "AI": {"tldr": "GCC \u662f\u4e00\u4e2a\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u7ba1\u7406\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347 LLM \u667a\u80fd\u4f53\u5728\u957f\u5468\u671f\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8f6f\u4ef6\u7f3a\u9677\u89e3\u51b3\u548c\u4efb\u52a1\u81ea\u590d\u5236\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e LLM \u7684\u667a\u80fd\u4f53\u5728\u957f\u5468\u671f\u5de5\u4f5c\u6d41\uff08\u5982\u5927\u578b\u957f\u671f\u9879\u76ee\u7f16\u7801\uff09\u4e2d\u4e0a\u4e0b\u6587\u7ba1\u7406\u7684\u74f6\u9888\u95ee\u9898\u3002", "method": "GCC \u662f\u4e00\u4e2a\u53d7\u8f6f\u4ef6\u7248\u672c\u63a7\u5236\u7cfb\u7edf\u542f\u53d1\u7684\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u7ba1\u7406\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u8bb0\u5fc6\u7ec4\u7ec7\u4e3a\u6301\u4e45\u6027\u6587\u4ef6\u7cfb\u7edf\uff0c\u652f\u6301 COMMIT\u3001BRANCH\u3001MERGE \u548c CONTEXT \u7b49\u64cd\u4f5c\u3002", "result": "GCC \u5728 SWE-Bench-Lite \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u89e3\u51b3\u4e86 48.00% \u7684\u8f6f\u4ef6\u7f3a\u9677\uff0c\u4f18\u4e8e 26 \u4e2a\u7ade\u4e89\u7cfb\u7edf\uff1b\u5728\u81ea\u590d\u5236\u6848\u4f8b\u4e2d\uff0c\u4efb\u52a1\u89e3\u51b3\u7387\u4ece 11.7% \u63d0\u5347\u81f3 40.7%\u3002", "conclusion": "Git-Context-Controller (GCC) \u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4f53\u5728\u957f\u5468\u671f\u5de5\u4f5c\u6d41\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2508.00776", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.00776", "abs": "https://arxiv.org/abs/2508.00776", "authors": ["Dieter van Melkebeek"], "title": "From Dynamic Programs to Greedy Algorithms", "comment": "14 pages, 2 figures", "summary": "We show for several computational problems how classical greedy algorithms\nfor special cases can be derived in a simple way from dynamic programs for the\ngeneral case: interval scheduling (restricted to unit weights), knapsack\n(restricted to unit values), and shortest paths (restricted to nonnegative edge\nlengths). Conceptually, we repeatedly expand the Bellman equations underlying\nthe dynamic program and use straightforward monotonicity properties to figure\nout which terms yield the optimal value under the respective restrictions. The\napproach offers an alternative for developing these greedy algorithms in\nundergraduate algorithms courses and/or for arguing their correctness. In the\nsetting of interval scheduling, it elucidates the change in order from earliest\nstart time first for the memoized dynamic program to earliest finish time first\nfor the greedy algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u52a8\u6001\u89c4\u5212\u63a8\u5bfc\u8d2a\u5fc3\u7b97\u6cd5\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6559\u5b66\u548c\u7b97\u6cd5\u6b63\u786e\u6027\u8bba\u8bc1\uff0c\u5c55\u793a\u4e86\u5728\u533a\u95f4\u8c03\u5ea6\u3001\u80cc\u5305\u548c\u6700\u77ed\u8def\u5f84\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4e3a\u7b97\u6cd5\u8bfe\u7a0b\u63d0\u4f9b\u4e00\u79cd\u65b0\u7684\u6559\u5b66\u65b9\u6cd5\uff0c\u5c55\u793a\u5982\u4f55\u4ece\u4e00\u822c\u52a8\u6001\u89c4\u5212\u4e2d\u63a8\u5bfc\u51fa\u7279\u5b9a\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u5e76\u89e3\u91ca\u5176\u6b63\u786e\u6027\u3002", "method": "\u901a\u8fc7\u53cd\u590d\u6269\u5c55\u52a8\u6001\u89c4\u5212\u4e2d\u7684\u8d1d\u5c14\u66fc\u65b9\u7a0b\uff0c\u5e76\u5229\u7528\u5355\u8c03\u6027\u6027\u8d28\u6765\u786e\u5b9a\u5728\u7279\u5b9a\u9650\u5236\u4e0b\u54ea\u4e9b\u9879\u4ea7\u751f\u6700\u4f18\u503c\u3002", "result": "\u6210\u529f\u5c55\u793a\u4e86\u5728\u533a\u95f4\u8c03\u5ea6\u3001\u80cc\u5305\u95ee\u9898\u548c\u6700\u77ed\u8def\u5f84\u95ee\u9898\u4e2d\uff0c\u5982\u4f55\u4ece\u52a8\u6001\u89c4\u5212\u4e2d\u63a8\u5bfc\u51fa\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u5e76\u9610\u660e\u4e86\u533a\u95f4\u8c03\u5ea6\u4e2d\u987a\u5e8f\u53d8\u5316\u7684\u539f\u56e0\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u548c\u5355\u8c03\u6027\u6027\u8d28\uff0c\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u4ece\u4e00\u822c\u60c5\u51b5\u4e0b\u7684\u52a8\u6001\u89c4\u5212\u4e2d\u7b80\u5355\u63a8\u5bfc\u51fa\u7279\u5b9a\u60c5\u51b5\u4e0b\u7684\u7ecf\u5178\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u4e3a\u7b97\u6cd5\u6559\u5b66\u548c\u6b63\u786e\u6027\u8bba\u8bc1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2508.00053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00053", "abs": "https://arxiv.org/abs/2508.00053", "authors": ["Jie Zhu", "Yiyang Su", "Minchul Kim", "Anil Jain", "Xiaoming Liu"], "title": "A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition", "comment": "Accepted to ICCV 2025. 11 pages, 5 figures", "summary": "Whole-body biometric recognition is a challenging multimodal task that\nintegrates various biometric modalities, including face, gait, and body. This\nintegration is essential for overcoming the limitations of unimodal systems.\nTraditionally, whole-body recognition involves deploying different models to\nprocess multiple modalities, achieving the final outcome by score-fusion (e.g.,\nweighted averaging of similarity matrices from each model). However, these\nconventional methods may overlook the variations in score distributions of\nindividual modalities, making it challenging to improve final performance. In\nthis work, we present \\textbf{Q}uality-guided \\textbf{M}ixture of score-fusion\n\\textbf{E}xperts (QME), a novel framework designed for improving whole-body\nbiometric recognition performance through a learnable score-fusion strategy\nusing a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for\nquality estimation with a modality-specific Quality Estimator (QE), and a score\ntriplet loss to improve the metric performance. Extensive experiments on\nmultiple whole-body biometric datasets demonstrate the effectiveness of our\nproposed approach, achieving state-of-the-art results across various metrics\ncompared to baseline methods. Our method is effective for multimodal and\nmulti-model, addressing key challenges such as model misalignment in the\nsimilarity score domain and variability in data quality.", "AI": {"tldr": "QME\u6846\u67b6\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5206\u6570\u878d\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5168\u8eab\u751f\u7269\u7279\u5f81\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5168\u8eab\u751f\u7269\u7279\u5f81\u8bc6\u522b\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u65f6\u5ffd\u89c6\u4e86\u5206\u6570\u5206\u5e03\u7684\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684QME\u6846\u67b6\uff0c\u7ed3\u5408\u4e86Mixture of Experts (MoE)\u7b56\u7565\uff0c\u5f15\u5165\u4e86\u4f2a\u8d28\u91cf\u635f\u5931\u548c\u8d28\u91cf\u4f30\u8ba1\u5668(QE)\u4ee5\u53ca\u5206\u6570\u4e09\u5143\u7ec4\u635f\u5931\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "QME\u6846\u67b6\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5206\u6570\u878d\u5408\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u5168\u8eab\u751f\u7269\u7279\u5f81\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5206\u6570\u5206\u5e03\u5dee\u5f02\u548c\u6570\u636e\u8d28\u91cf\u53d8\u5316\u7684\u6311\u6218\u3002"}}
{"id": "2508.00081", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00081", "abs": "https://arxiv.org/abs/2508.00081", "authors": ["Fred Mutisya", "Shikoh Gitau", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha"], "title": "Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench", "comment": null, "summary": "HealthBench, a benchmark designed to measure the capabilities of AI systems\nfor health better (Arora et al., 2025), has advanced medical language model\nevaluation through physician-crafted dialogues and transparent rubrics.\nHowever, its reliance on expert opinion, rather than high-tier clinical\nevidence, risks codifying regional biases and individual clinician\nidiosyncrasies, further compounded by potential biases in automated grading\nsystems. These limitations are particularly magnified in low- and middle-income\nsettings, where issues like sparse neglected tropical disease coverage and\nregion-specific guideline mismatches are prevalent.\n  The unique challenges of the African context, including data scarcity,\ninadequate infrastructure, and nascent regulatory frameworks, underscore the\nurgent need for more globally relevant and equitable benchmarks. To address\nthese shortcomings, we propose anchoring reward functions in version-controlled\nClinical Practice Guidelines (CPGs) that incorporate systematic reviews and\nGRADE evidence ratings.\n  Our roadmap outlines \"evidence-robust\" reinforcement learning via\nrubric-to-guideline linkage, evidence-weighted scoring, and contextual override\nlogic, complemented by a focus on ethical considerations and the integration of\ndelayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,\nwhile preserving HealthBench's transparency and physician engagement, we aim to\nfoster medical language models that are not only linguistically polished but\nalso clinically trustworthy, ethically sound, and globally relevant.", "AI": {"tldr": "HealthBench\u4f9d\u8d56\u4e13\u5bb6\u610f\u89c1\u53ef\u80fd\u5f15\u5165\u504f\u89c1\uff0c\u7279\u522b\u662f\u5728\u4e2d\u4f4e\u6536\u5165\u5730\u533a\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u4e25\u683c\u5ba1\u67e5CPGs\u7684\u5956\u52b1\u673a\u5236\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u4f26\u7406\u8003\u91cf\uff0c\u4ee5\u5f00\u53d1\u66f4\u5168\u7403\u76f8\u5173\u548c\u516c\u5e73\u7684\u533b\u5b66\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "HealthBench\u4f9d\u8d56\u4e8e\u4e13\u5bb6\u610f\u89c1\u800c\u975e\u9ad8\u7ea7\u4e34\u5e8a\u8bc1\u636e\uff0c\u53ef\u80fd\u56fa\u5316\u533a\u57df\u504f\u89c1\u548c\u4e2a\u4f53\u4e34\u5e8a\u533b\u751f\u7279\u8d28\uff0c\u5c24\u5176\u5728\u4e2d\u4f4e\u6536\u5165\u73af\u5883\u4e2d\u95ee\u9898\u66f4\u4e3a\u7a81\u51fa\uff0c\u5982\u5ffd\u89c6\u70ed\u5e26\u75be\u75c5\u8986\u76d6\u4e0d\u8db3\u548c\u5730\u533a\u6307\u5357\u4e0d\u5339\u914d\u3002\u975e\u6d32\u80cc\u666f\u4e0b\u7684\u72ec\u7279\u6311\u6218\uff08\u6570\u636e\u7a00\u7f3a\u3001\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u548c\u76d1\u7ba1\u6846\u67b6\u4e0d\u6210\u719f\uff09\u51f8\u663e\u4e86\u5bf9\u66f4\u5168\u7403\u76f8\u5173\u548c\u516c\u5e73\u57fa\u51c6\u7684\u8feb\u5207\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u2018\u8bc1\u636e\u7a33\u5065\u2019\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u5305\u62ec\u8bc4\u5206\u6807\u51c6\u4e0e\u6307\u5357\u7684\u94fe\u63a5\u3001\u8bc1\u636e\u52a0\u6743\u8bc4\u5206\u548c\u4e0a\u4e0b\u6587\u8986\u76d6\u903b\u8f91\uff0c\u7ed3\u5408\u4f26\u7406\u8003\u91cf\u548c\u5ef6\u8fdf\u7ed3\u679c\u53cd\u9988\u7684\u6574\u5408\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7248\u672c\u63a7\u5236CPGs\u7684\u5956\u52b1\u51fd\u6570\u951a\u5b9a\u65b9\u6cd5\uff0c\u7ed3\u5408\u7cfb\u7edf\u8bc4\u4ef7\u548cGRADE\u8bc1\u636e\u8bc4\u7ea7\uff0c\u4ee5\u89e3\u51b3HealthBench\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5956\u52b1\u673a\u5236\u57fa\u4e8e\u4e25\u683c\u5ba1\u67e5\u7684\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\uff08CPGs\uff09\uff0c\u540c\u65f6\u4fdd\u6301HealthBench\u7684\u900f\u660e\u5ea6\u548c\u533b\u751f\u53c2\u4e0e\uff0c\u65e8\u5728\u57f9\u517b\u4e0d\u4ec5\u5728\u8bed\u8a00\u4e0a\u7cbe\u70bc\uff0c\u800c\u4e14\u5728\u4e34\u5e8a\u53ef\u4fe1\u3001\u9053\u5fb7\u5408\u7406\u4e14\u5168\u7403\u76f8\u5173\u7684\u533b\u5b66\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2508.00007", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00007", "abs": "https://arxiv.org/abs/2508.00007", "authors": ["Gaowei Chang", "Eidan Lin", "Chengxuan Yuan", "Rizhao Cai", "Binbin Chen", "Xuan Xie", "Yin Zhang"], "title": "Agent Network Protocol Technical White Paper", "comment": "This white paper is a reformatted version of the open-source\n  community edition previously released by the ANP Open Source Technology\n  Community(https://github.com/agent-network-protocol)", "summary": "With the development of large models and autonomous decision-making AI,\nagents are rapidly becoming the new entities of the internet, following mobile\napps. However, existing internet infrastructure is primarily designed for human\ninteraction, creating data silos, unfriendly interfaces, and high collaboration\ncosts among agents, making it difficult to support the needs for large-scale\nagent interconnection and collaboration. The internet is undergoing a profound\ntransformation, showing four core trends: agents replacing traditional\nsoftware, universal agent interconnection, native protocol-based connections,\nand autonomous agent organization and collaboration. To align with these\ntrends, Agent Network Protocol (ANP) proposes a new generation of communication\nprotocols for the Agentic Web. ANP adheres to AI-native design, maintains\ncompatibility with existing internet protocols, adopts a modular composable\narchitecture, follows minimalist yet extensible principles, and enables rapid\ndeployment based on existing infrastructure. Through a three-layer protocol\nsystem--identity and encrypted communication layer, meta-protocol negotiation\nlayer, and application protocol layer--ANP. systematically solves the problems\nof agent identity authentication, dynamic negotiation, and capability discovery\ninteroperability.", "AI": {"tldr": "ANP\u662f\u4e00\u79cd\u9762\u5411\u667a\u80fd\u4f53\u7f51\u7edc\u7684\u65b0\u4e00\u4ee3\u901a\u4fe1\u534f\u8bae\uff0c\u901a\u8fc7AI\u539f\u751f\u8bbe\u8ba1\u548c\u4e09\u5c42\u534f\u8bae\u7cfb\u7edf\u89e3\u51b3\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u4e92\u8054\u4e0e\u534f\u4f5c\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u4e92\u8054\u7f51\u57fa\u7840\u8bbe\u65bd\u4e3b\u8981\u4e3a\u4eba\u7c7b\u4ea4\u4e92\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u667a\u80fd\u4f53\u4e4b\u95f4\u5b58\u5728\u6570\u636e\u5b64\u5c9b\u3001\u4e0d\u53cb\u597d\u63a5\u53e3\u548c\u9ad8\u534f\u4f5c\u6210\u672c\uff0c\u96be\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u4e92\u8054\u4e0e\u534f\u4f5c\u7684\u9700\u6c42\u3002", "method": "ANP\u91c7\u7528AI\u539f\u751f\u8bbe\u8ba1\uff0c\u4fdd\u6301\u4e0e\u73b0\u6709\u4e92\u8054\u7f51\u534f\u8bae\u7684\u517c\u5bb9\u6027\uff0c\u91c7\u7528\u6a21\u5757\u5316\u53ef\u7ec4\u5408\u67b6\u6784\uff0c\u9075\u5faa\u7b80\u7ea6\u4f46\u53ef\u6269\u5c55\u7684\u539f\u5219\uff0c\u5e76\u57fa\u4e8e\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u5b9e\u73b0\u5feb\u901f\u90e8\u7f72\u3002", "result": "ANP\u901a\u8fc7\u4e09\u5c42\u534f\u8bae\u7cfb\u7edf\uff08\u8eab\u4efd\u4e0e\u52a0\u5bc6\u901a\u4fe1\u5c42\u3001\u5143\u534f\u8bae\u534f\u5546\u5c42\u548c\u5e94\u7528\u534f\u8bae\u5c42\uff09\u7cfb\u7edf\u6027\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u8eab\u4efd\u8ba4\u8bc1\u3001\u52a8\u6001\u534f\u5546\u548c\u80fd\u529b\u53d1\u73b0\u4e92\u64cd\u4f5c\u6027\u7b49\u95ee\u9898\u3002", "conclusion": "ANP\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4e00\u4ee3\u7684\u901a\u4fe1\u534f\u8bae\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u4e92\u8054\u4e0e\u534f\u4f5c\u7684\u9700\u6c42\uff0c\u901a\u8fc7\u4e09\u5c42\u534f\u8bae\u7cfb\u7edf\u7cfb\u7edf\u6027\u89e3\u51b3\u4e86\u8eab\u4efd\u8ba4\u8bc1\u3001\u52a8\u6001\u534f\u5546\u548c\u80fd\u529b\u53d1\u73b0\u4e92\u64cd\u4f5c\u6027\u7b49\u95ee\u9898\u3002"}}
{"id": "2508.00303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00303", "abs": "https://arxiv.org/abs/2508.00303", "authors": ["Zehui Xu", "Junhui Wang", "Yongliang Shi", "Chao Gao", "Guyue Zhou"], "title": "TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps", "comment": null, "summary": "This paper introduces TopoDiffuser, a diffusion-based framework for\nmultimodal trajectory prediction that incorporates topometric maps to generate\naccurate, diverse, and road-compliant future motion forecasts. By embedding\nstructural cues from topometric maps into the denoising process of a\nconditional diffusion model, the proposed approach enables trajectory\ngeneration that naturally adheres to road geometry without relying on explicit\nconstraints. A multimodal conditioning encoder fuses LiDAR observations,\nhistorical motion, and route information into a unified bird's-eye-view (BEV)\nrepresentation. Extensive experiments on the KITTI benchmark demonstrate that\nTopoDiffuser outperforms state-of-the-art methods, while maintaining strong\ngeometric consistency. Ablation studies further validate the contribution of\neach input modality, as well as the impact of denoising steps and the number of\ntrajectory samples. To support future research, we publicly release our code at\nhttps://github.com/EI-Nav/TopoDiffuser.", "AI": {"tldr": "TopoDiffuser\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u6a21\u6001\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u62d3\u6251\u5730\u56fe\u751f\u6210\u51c6\u786e\u3001\u591a\u6837\u4e14\u7b26\u5408\u9053\u8def\u8981\u6c42\u7684\u672a\u6765\u8fd0\u52a8\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u672a\u6765\u8fd0\u52a8\u9884\u6d4b\u65f6\u5f80\u5f80\u4f9d\u8d56\u663e\u5f0f\u7ea6\u675f\uff0c\u96be\u4ee5\u81ea\u7136\u7b26\u5408\u9053\u8def\u51e0\u4f55\u5f62\u72b6\u3002TopoDiffuser\u65e8\u5728\u901a\u8fc7\u5d4c\u5165\u62d3\u6251\u5730\u56fe\u7684\u7ed3\u6784\u7ebf\u7d22\uff0c\u5b9e\u73b0\u65e0\u9700\u663e\u5f0f\u7ea6\u675f\u7684\u8f68\u8ff9\u751f\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6761\u4ef6\u7f16\u7801\u5668\uff0c\u878d\u5408LiDAR\u89c2\u6d4b\u3001\u5386\u53f2\u8fd0\u52a8\u548c\u8def\u7ebf\u4fe1\u606f\u4e3a\u7edf\u4e00\u7684\u9e1f\u77b0\u56fe\uff08BEV\uff09\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\u751f\u6210\u8f68\u8ff9\u3002", "result": "\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTopoDiffuser\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u8f93\u5165\u6a21\u6001\u548c\u53bb\u566a\u6b65\u9aa4\u7684\u8d21\u732e\u3002", "conclusion": "TopoDiffuser\u901a\u8fc7\u7ed3\u5408\u62d3\u6251\u5730\u56fe\u548c\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u663e\u5f0f\u7ea6\u675f\u7684\u9ad8\u8d28\u91cf\u8f68\u8ff9\u9884\u6d4b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u6e90\u4ee3\u7801\u3002"}}
{"id": "2508.00428", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.00428", "abs": "https://arxiv.org/abs/2508.00428", "authors": ["Nan Xiang", "Tianyi Liang", "Haiwen Huang", "Shiqi Jiang", "Hao Huang", "Yifei Huang", "Liangyu Chen", "Changbo Wang", "Chenhui Li"], "title": "Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation", "comment": "IEEE VIS VAST 2025 ACM 2012 CCS - Human-centered computing,\n  Visualization, Visualization design and evaluation methods", "summary": "Text-to-3D (T23D) generation has transformed digital content creation, yet\nremains bottlenecked by blind trial-and-error prompting processes that yield\nunpredictable results. While visual prompt engineering has advanced in\ntext-to-image domains, its application to 3D generation presents unique\nchallenges requiring multi-view consistency evaluation and spatial\nunderstanding. We present Sel3DCraft, a visual prompt engineering system for\nT23D that transforms unstructured exploration into a guided visual process. Our\napproach introduces three key innovations: a dual-branch structure combining\nretrieval and generation for diverse candidate exploration; a multi-view hybrid\nscoring approach that leverages MLLMs with innovative high-level metrics to\nassess 3D models with human-expert consistency; and a prompt-driven visual\nanalytics suite that enables intuitive defect identification and refinement.\nExtensive testing and user studies demonstrate that Sel3DCraft surpasses other\nT23D systems in supporting creativity for designers.", "AI": {"tldr": "Sel3DCraft\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u89c6\u89c9\u63d0\u793a\u5de5\u7a0b\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u7ed3\u6784\u548c\u591a\u89c6\u56fe\u8bc4\u5206\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6587\u672c\u52303D\u751f\u6210\u7684\u6548\u7387\u548c\u521b\u9020\u6027\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u52303D\u751f\u6210\u4e2d\u56e0\u76f2\u76ee\u8bd5\u9519\u63d0\u793a\u5bfc\u81f4\u7ed3\u679c\u4e0d\u53ef\u9884\u6d4b\u7684\u95ee\u9898\uff0c\u63d0\u5347\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u7406\u89e3\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\uff08\u68c0\u7d22\u4e0e\u751f\u6210\uff09\u63a2\u7d22\u591a\u6837\u5019\u9009\uff0c\u7ed3\u5408\u591a\u89c6\u56fe\u6df7\u5408\u8bc4\u5206\u65b9\u6cd5\uff0c\u5229\u7528MLLMs\u548c\u9ad8\u5c42\u6b21\u6307\u6807\u8bc4\u4f303D\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u63d0\u793a\u9a71\u52a8\u7684\u89c6\u89c9\u5206\u6790\u5957\u4ef6\u3002", "result": "Sel3DCraft\u5728\u5e7f\u6cdb\u6d4b\u8bd5\u548c\u7528\u6237\u7814\u7a76\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6T23D\u7cfb\u7edf\uff0c\u6709\u6548\u652f\u6301\u8bbe\u8ba1\u5e08\u7684\u521b\u9020\u6027\u5de5\u4f5c\u3002", "conclusion": "Sel3DCraft\u901a\u8fc7\u5176\u521b\u65b0\u7684\u89c6\u89c9\u63d0\u793a\u5de5\u7a0b\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u52303D\u751f\u6210\u7684\u6548\u7387\u548c\u521b\u9020\u6027\uff0c\u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u7684\u5de5\u5177\u3002"}}
{"id": "2508.00622", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00622", "abs": "https://arxiv.org/abs/2508.00622", "authors": ["Kapel Dev", "Yash Madhwal", "Sofia Shevelo", "Pavel Osinenko", "Yury Yanovich"], "title": "SwarnRaft: Leveraging Consensus for Robust Drone Swarm Coordination in GNSS-Degraded Environments", "comment": null, "summary": "Unmanned aerial vehicle (UAV) swarms are increasingly used in critical\napplications such as aerial mapping, environmental monitoring, and autonomous\ndelivery. However, the reliability of these systems is highly dependent on\nuninterrupted access to the Global Navigation Satellite Systems (GNSS) signals,\nwhich can be disrupted in real-world scenarios due to interference,\nenvironmental conditions, or adversarial attacks, causing disorientation,\ncollision risks, and mission failure. This paper proposes SwarnRaft, a\nblockchain-inspired positioning and consensus framework for maintaining\ncoordination and data integrity in UAV swarms operating under GNSS-denied\nconditions. SwarnRaft leverages the Raft consensus algorithm to enable\ndistributed drones (nodes) to agree on state updates such as location and\nheading, even in the absence of GNSS signals for one or more nodes. In our\nprototype, each node uses GNSS and local sensing, and communicates over WiFi in\na simulated swarm. Upon signal loss, consensus is used to reconstruct or verify\nthe position of the failed node based on its last known state and trajectory.\nOur system demonstrates robustness in maintaining swarm coherence and fault\ntolerance through a lightweight, scalable communication model. This work offers\na practical and secure foundation for decentralized drone operation in\nunpredictable environments.", "AI": {"tldr": "SwarnRaft\u662f\u4e00\u79cd\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u65f6\u7ef4\u6301\u65e0\u4eba\u673a\u7fa4\u7684\u534f\u8c03\u548c\u6570\u636e\u5b8c\u6574\u6027\uff0c\u901a\u8fc7Raft\u5171\u8bc6\u7b97\u6cd5\u5b9e\u73b0\u5206\u5e03\u5f0f\u5b9a\u4f4d\u548c\u72b6\u6001\u66f4\u65b0\u3002", "motivation": "\u65e0\u4eba\u673a\u7fa4\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u9ad8\u5ea6\u4f9d\u8d56GNSS\u4fe1\u53f7\uff0c\u800c\u4fe1\u53f7\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u53ef\u80fd\u56e0\u5e72\u6270\u3001\u73af\u5883\u6761\u4ef6\u6216\u654c\u5bf9\u653b\u51fb\u800c\u4e2d\u65ad\uff0c\u5bfc\u81f4\u8ff7\u5931\u65b9\u5411\u3001\u78b0\u649e\u98ce\u9669\u548c\u4efb\u52a1\u5931\u8d25\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SwarnRaft\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u53d7\u533a\u5757\u94fe\u542f\u53d1\u7684\u5b9a\u4f4d\u548c\u5171\u8bc6\u6846\u67b6\uff0c\u5229\u7528Raft\u5171\u8bc6\u7b97\u6cd5\uff0c\u4f7f\u5206\u5e03\u5f0f\u65e0\u4eba\u673a\uff08\u8282\u70b9\uff09\u80fd\u591f\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\u5c31\u72b6\u6001\u66f4\u65b0\uff08\u5982\u4f4d\u7f6e\u548c\u822a\u5411\uff09\u8fbe\u6210\u4e00\u81f4\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u5c55\u793a\u4e86\u5728\u6a21\u62df\u7fa4\u4e2d\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u901a\u4fe1\u6a21\u578b\uff0cSwarnRaft\u80fd\u591f\u6709\u6548\u7ef4\u6301\u7fa4\u7684\u4e00\u81f4\u6027\u548c\u5bb9\u9519\u80fd\u529b\u3002", "conclusion": "SwarnRaft \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u5b89\u5168\u7684\u57fa\u7840\uff0c\u652f\u6301\u5728\u4e0d\u53ef\u9884\u6d4b\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u65e0\u4eba\u673a\u64cd\u4f5c\uff0c\u589e\u5f3a\u4e86\u65e0\u4eba\u673a\u7fa4\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u5bb9\u9519\u80fd\u529b\u3002"}}
{"id": "2508.00033", "categories": ["cs.SE", "cs.AI", "cs.CL", "68T50", "I.2.2; I.2.7; D.2.3"], "pdf": "https://arxiv.org/pdf/2508.00033", "abs": "https://arxiv.org/abs/2508.00033", "authors": ["Nuno Fachada", "Daniel Fernandes", "Carlos M. Fernandes", "Bruno D. Ferreira-Saraiva", "Jo\u00e3o P. Matos-Carvalho"], "title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries", "comment": null, "summary": "Large Language Models (LLMs) have advanced rapidly as tools for automating\ncode generation in scientific research, yet their ability to interpret and use\nunfamiliar Python APIs for complex computational experiments remains poorly\ncharacterized. This study systematically benchmarks a selection of\nstate-of-the-art LLMs in generating functional Python code for two increasingly\nchallenging scenarios: conversational data analysis with the \\textit{ParShift}\nlibrary, and synthetic data generation and clustering using \\textit{pyclugen}\nand \\textit{scikit-learn}. Both experiments use structured, zero-shot prompts\nspecifying detailed requirements but omitting in-context examples. Model\noutputs are evaluated quantitatively for functional correctness and prompt\ncompliance over multiple runs, and qualitatively by analyzing the errors\nproduced when code execution fails. Results show that only a small subset of\nmodels consistently generate correct, executable code, with GPT-4.1 standing\nout as the only model to always succeed in both tasks. In addition to\nbenchmarking LLM performance, this approach helps identify shortcomings in\nthird-party libraries, such as unclear documentation or obscure implementation\nbugs. Overall, these findings highlight current limitations of LLMs for\nend-to-end scientific automation and emphasize the need for careful prompt\ndesign, comprehensive library documentation, and continued advances in language\nmodel capabilities.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86LLMs\u5728\u751f\u6210\u590d\u6742Python\u4ee3\u7801\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4.1\u8868\u73b0\u6700\u4f73\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5e93\u6587\u6863\u548c\u6a21\u578b\u80fd\u529b\u7684\u6539\u8fdb\u9700\u6c42\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u79d1\u5b66\u7814\u7a76\u4e2d\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u7684\u5de5\u5177\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5176\u89e3\u91ca\u548c\u4f7f\u7528\u4e0d\u719f\u6089\u7684Python API\u8fdb\u884c\u590d\u6742\u8ba1\u7b97\u5b9e\u9a8c\u7684\u80fd\u529b\u4ecd\u672a\u5f97\u5230\u5145\u5206\u8868\u5f81\u3002", "method": "\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u57fa\u51c6\u6d4b\u8bd5\u4e86\u4e00\u7cfb\u5217\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5728\u4e24\u79cd\u65e5\u76ca\u590d\u6742\u7684\u573a\u666f\u4e2d\u751f\u6210\u529f\u80fd\u6027Python\u4ee3\u7801\uff1a\u4f7f\u7528ParShift\u5e93\u8fdb\u884c\u5bf9\u8bdd\u6570\u636e\u5206\u6790\uff0c\u4ee5\u53ca\u4f7f\u7528pyclugen\u548cscikit-learn\u751f\u6210\u548c\u805a\u7c7b\u5408\u6210\u6570\u636e\u3002\u5b9e\u9a8c\u91c7\u7528\u7ed3\u6784\u5316\u3001\u96f6\u6837\u672c\u63d0\u793a\uff0c\u8be6\u7ec6\u6307\u5b9a\u8981\u6c42\u4f46\u7701\u7565\u4e0a\u4e0b\u6587\u793a\u4f8b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u53ea\u6709\u4e00\u5c0f\u90e8\u5206\u6a21\u578b\u80fd\u59cb\u7ec8\u751f\u6210\u6b63\u786e\u3001\u53ef\u6267\u884c\u7684\u4ee3\u7801\uff0cGPT-4.1\u662f\u552f\u4e00\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u5747\u6210\u529f\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u5e2e\u52a9\u8bc6\u522b\u4e86\u7b2c\u4e09\u65b9\u5e93\u7684\u4e0d\u8db3\uff0c\u5982\u6587\u6863\u4e0d\u6e05\u6670\u6216\u5b9e\u73b0\u4e2d\u7684\u9690\u853d\u9519\u8bef\u3002", "conclusion": "\u7814\u7a76\u7a81\u51fa\u4e86\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7aef\u5230\u7aef\u79d1\u5b66\u81ea\u52a8\u5316\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u7cbe\u5fc3\u8bbe\u8ba1\u63d0\u793a\u3001\u5168\u9762\u7684\u5e93\u6587\u6863\u4ee5\u53ca\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u6301\u7eed\u8fdb\u6b65\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.00085", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00085", "abs": "https://arxiv.org/abs/2508.00085", "authors": ["Raiyaan Abdullah", "Jared Claypoole", "Michael Cogswell", "Ajay Divakaran", "Yogesh Rawat"], "title": "Punching Bag vs. Punching Person: Motion Transferability in Videos", "comment": "Accepted to ICCV 2025 main conference", "summary": "Action recognition models demonstrate strong generalization, but can they\neffectively transfer high-level motion concepts across diverse contexts, even\nwithin similar distributions? For example, can a model recognize the broad\naction \"punching\" when presented with an unseen variation such as \"punching\nperson\"? To explore this, we introduce a motion transferability framework with\nthree datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)\nKinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural\nvideo datasets. We evaluate 13 state-of-the-art models on these benchmarks and\nobserve a significant drop in performance when recognizing high-level actions\nin novel contexts. Our analysis reveals: 1) Multimodal models struggle more\nwith fine-grained unknown actions than with coarse ones; 2) The bias-free\nSyn-TA proves as challenging as real-world datasets, with models showing\ngreater performance drops in controlled settings; 3) Larger models improve\ntransferability when spatial cues dominate but struggle with intensive temporal\nreasoning, while reliance on object and background cues hinders generalization.\nWe further explore how disentangling coarse and fine motions can improve\nrecognition in temporally challenging datasets. We believe this study\nestablishes a crucial benchmark for assessing motion transferability in action\nrecognition. Datasets and relevant code:\nhttps://github.com/raiyaan-abdullah/Motion-Transfer.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u5728\u65b0\u60c5\u5883\u4e0b\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u8de8\u60c5\u5883\u8bc6\u522b\u9ad8\u7ea7\u52a8\u4f5c\u65f6\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u662f\u5426\u80fd\u5c06\u9ad8\u7ea7\u52a8\u4f5c\u6982\u5ff5\uff08\u5982\u2018\u62f3\u51fb\u2019\uff09\u6709\u6548\u8fc1\u79fb\u5230\u591a\u6837\u5316\u7684\u60c5\u5883\u4e2d\uff0c\u5373\u4f7f\u662f\u5728\u76f8\u4f3c\u5206\u5e03\u5185\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u8fd0\u52a8\u8fc1\u79fb\u6027\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u4e2a\u6570\u636e\u96c6\uff08Syn-TA\u3001Kinetics400-TA\u3001Something-Something-v2-TA\uff09\uff0c\u8bc4\u4f30\u4e8613\u79cd\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u5728\u8de8\u60c5\u5883\u8bc6\u522b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u591a\u6a21\u6001\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u672a\u77e5\u52a8\u4f5c\u4e0a\u8868\u73b0\u66f4\u5dee\uff1b2\uff09\u65e0\u504f\u7684Syn-TA\u6570\u636e\u96c6\u4e0e\u771f\u5b9e\u6570\u636e\u96c6\u540c\u6837\u5177\u6709\u6311\u6218\u6027\uff1b3\uff09\u5927\u6a21\u578b\u5728\u7a7a\u95f4\u7ebf\u7d22\u4e3b\u5bfc\u65f6\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u65f6\u95f4\u63a8\u7406\u5bc6\u96c6\u65f6\u8868\u73b0\u8f83\u5dee\u3002\u6b64\u5916\uff0c\u89e3\u8026\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u53ef\u4ee5\u63d0\u5347\u8bc6\u522b\u6548\u679c\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u8fd0\u52a8\u8fc1\u79fb\u6027\u8bc4\u4f30\u5efa\u7acb\u4e86\u4e00\u4e2a\u91cd\u8981\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u65b0\u60c5\u5883\u4e0b\u8bc6\u522b\u9ad8\u7ea7\u52a8\u4f5c\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2508.00106", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00106", "abs": "https://arxiv.org/abs/2508.00106", "authors": ["Ernest Bonnah", "Luan Viet Nguyen", "Khaza Anuarul Hoque"], "title": "Hyperproperty-Constrained Secure Reinforcement Learning", "comment": "Accepted in IEEE/ACM MEMOCODE 2025", "summary": "Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a\ndomain-specific formal specification language known for its effectiveness in\ncompactly representing security, opacity, and concurrency properties for\nrobotics applications. This paper focuses on HyperTWTL-constrained secure\nreinforcement learning (SecRL). Although temporal logic-constrained safe\nreinforcement learning (SRL) is an evolving research problem with several\nexisting literature, there is a significant research gap in exploring\nsecurity-aware reinforcement learning (RL) using hyperproperties. Given the\ndynamics of an agent as a Markov Decision Process (MDP) and opacity/security\nconstraints formalized as HyperTWTL, we propose an approach for learning\nsecurity-aware optimal policies using dynamic Boltzmann softmax RL while\nsatisfying the HyperTWTL constraints. The effectiveness and scalability of our\nproposed approach are demonstrated using a pick-up and delivery robotic mission\ncase study. We also compare our results with two other baseline RL algorithms,\nshowing that our proposed method outperforms them.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHyperTWTL\u7ea6\u675f\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001Boltzmann softmax RL\u5b66\u4e60\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5176\u4f18\u4e8e\u57fa\u7ebf\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e2d\uff0c\u867d\u7136\u6709\u65f6\u5e8f\u903b\u8f91\u7ea6\u675f\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08SRL\uff09\u6587\u732e\uff0c\u4f46\u7f3a\u4e4f\u57fa\u4e8e\u8d85\u5c5e\u6027\uff08hyperproperties\uff09\u7684\u5b89\u5168\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff08SecRL\uff09\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u52a8\u6001Boltzmann softmax RL\u65b9\u6cd5\uff0c\u7ed3\u5408HyperTWTL\u7ea6\u675f\uff0c\u5b66\u4e60\u5b89\u5168\u611f\u77e5\u7684\u6700\u4f18\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u62fe\u53d6\u548c\u4ea4\u4ed8\u673a\u5668\u4eba\u4efb\u52a1\u6848\u4f8b\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e24\u79cd\u57fa\u7ebfRL\u7b97\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHyperTWTL\u7ea6\u675f\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001Boltzmann softmax RL\u5b66\u4e60\u5b89\u5168\u611f\u77e5\u7684\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u5728\u673a\u5668\u4eba\u4efb\u52a1\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.00009", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00009", "abs": "https://arxiv.org/abs/2508.00009", "authors": ["Sourav Mondal", "Elaine Wong"], "title": "Enabling Immersive XR Collaborations over FTTR Networks (Invited)", "comment": "This invited paper was presented in Optica Advanced Photonic Congress\n  2025", "summary": "Fiber-To-The-Room is a potential solution to achieve in-premise extended\nreality collaborations. This paper explores predictive bandwidth allocation and\nseamless handover schemes over FTTR, showing high-quality immersive experience\nfor in-premise collaborations can be achieved. \\c{opyright} 2025 The Author(s).", "AI": {"tldr": "FTTR\u901a\u8fc7\u9884\u6d4b\u6027\u5e26\u5bbd\u5206\u914d\u548c\u65e0\u7f1d\u5207\u6362\u65b9\u6848\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u7684\u5ba4\u5185\u6269\u5c55\u73b0\u5b9e\u534f\u4f5c\u4f53\u9a8c\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5ba4\u5185\u6269\u5c55\u73b0\u5b9e\u534f\u4f5c\u4e2d\u9ad8\u8d28\u91cf\u6c89\u6d78\u5f0f\u4f53\u9a8c\u7684\u9700\u6c42\u3002", "method": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728FTTR\u4e0a\u5b9e\u65bd\u7684\u9884\u6d4b\u6027\u5e26\u5bbd\u5206\u914d\u548c\u65e0\u7f1d\u5207\u6362\u65b9\u6848\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u6240\u63d0\u51fa\u7684\u65b9\u6848\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u6c89\u6d78\u5f0f\u4f53\u9a8c\u3002", "conclusion": "Fiber-To-The-Room\uff08FTTR\uff09\u662f\u5b9e\u73b0\u5ba4\u5185\u6269\u5c55\u73b0\u5b9e\u534f\u4f5c\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u9884\u6d4b\u6027\u5e26\u5bbd\u5206\u914d\u548c\u65e0\u7f1d\u5207\u6362\u65b9\u6848\uff0c\u80fd\u591f\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u6c89\u6d78\u5f0f\u4f53\u9a8c\u3002"}}
{"id": "2508.00354", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00354", "abs": "https://arxiv.org/abs/2508.00354", "authors": ["Tianshuang Qiu", "Zehan Ma", "Karim El-Refai", "Hiya Shah", "Chung Min Kim", "Justin Kerr", "Ken Goldberg"], "title": "Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging", "comment": null, "summary": "3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view\nimages. Such \"digital twins\" are useful for simulations, virtual reality,\nmarketing, robot policy fine-tuning, and part inspection. 3D object scanning\nusually requires multi-camera arrays, precise laser scanners, or robot\nwrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,\na pipeline for producing high-quality 3D Gaussian Splat models using a\nbi-manual robot that grasps an object with one gripper and rotates the object\nwith respect to a stationary camera. The object is then re-grasped by a second\ngripper to expose surfaces that were occluded by the first gripper. We present\nthe Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as\nRAFT optical flow models to identify and isolate objects held by a robot\ngripper while removing the gripper and the background. We then modify the 3DGS\ntraining pipeline to support concatenated datasets with gripper occlusion,\nproducing an omni-directional (360 degree view) model of the object. We apply\nOmni-Scan to part defect inspection, finding that it can identify visual or\ngeometric defects in 12 different industrial and household objects with an\naverage accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be\nfound at https://berkeleyautomation.github.io/omni-scan/", "AI": {"tldr": "Omni-Scan\u662f\u4e00\u79cd\u4f7f\u7528\u53cc\u673a\u68b0\u81c2\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u751f\u6210\u9ad8\u8d28\u91cf3D\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\u7684\u6d41\u7a0b\uff0c\u9002\u7528\u4e8e\u96f6\u4ef6\u7f3a\u9677\u68c0\u6d4b\uff0c\u51c6\u786e\u7387\u8fbe83%\u3002", "motivation": "\u73b0\u6709\u76843D\u7269\u4f53\u626b\u63cf\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u591a\u76f8\u673a\u9635\u5217\u6216\u7cbe\u786e\u7684\u6fc0\u5149\u626b\u63cf\u4eea\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53d7\u9650\u4e8e\u5de5\u4f5c\u7a7a\u95f4\u3002Omni-Scan\u65e8\u5728\u901a\u8fc7\u53cc\u673a\u68b0\u81c2\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\u7684\u5168\u65b9\u5411\u5efa\u6a21\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u53cc\u673a\u68b0\u81c2\u6293\u53d6\u7269\u4f53\u5e76\u65cb\u8f6c\u7684\u6d41\u7a0b\uff0c\u7ed3\u5408DepthAnything\u3001Segment Anything\u548cRAFT\u5149\u6d41\u6a21\u578b\u6765\u8bc6\u522b\u548c\u9694\u79bb\u7269\u4f53\uff0c\u5e76\u4fee\u6539\u4e863DGS\u8bad\u7ec3\u6d41\u7a0b\u4ee5\u652f\u6301\u5e26\u6709\u6293\u53d6\u906e\u6321\u7684\u62fc\u63a5\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7Omni-Scan\u6d41\u7a0b\uff0c\u6210\u529f\u751f\u6210\u4e86\u7269\u4f53\u7684\u5168\u65b9\u5411\uff08360\u5ea6\u89c6\u89d2\uff09\u6a21\u578b\uff0c\u5e76\u5728\u96f6\u4ef6\u7f3a\u9677\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Omni-Scan \u80fd\u591f\u4ee583%\u7684\u5e73\u5747\u51c6\u786e\u7387\u8bc6\u522b12\u79cd\u5de5\u4e1a\u548c\u5bb6\u7528\u7269\u4f53\u7684\u89c6\u89c9\u6216\u51e0\u4f55\u7f3a\u9677\u3002"}}
{"id": "2508.00782", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.00782", "abs": "https://arxiv.org/abs/2508.00782", "authors": ["Kien T. Pham", "Yingqing He", "Yazhou Xing", "Qifeng Chen", "Long Chen"], "title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation", "comment": "The 33rd ACM Multimedia Conference (MM '25)", "summary": "Audio-driven video generation aims to synthesize realistic videos that align\nwith input audio recordings, akin to the human ability to visualize scenes from\nauditory input. However, existing approaches predominantly focus on exploring\nsemantic information, such as the classes of sounding sources present in the\naudio, limiting their ability to generate videos with accurate content and\nspatial composition. In contrast, we humans can not only naturally identify the\nsemantic categories of sounding sources but also determine their deeply encoded\nspatial attributes, including locations and movement directions. This useful\ninformation can be elucidated by considering specific spatial indicators\nderived from the inherent physical properties of sound, such as loudness or\nfrequency. As prior methods largely ignore this factor, we present SpA2V, the\nfirst framework explicitly exploits these spatial auditory cues from audios to\ngenerate videos with high semantic and spatial correspondence. SpA2V decomposes\nthe generation process into two stages: 1) Audio-guided Video Planning: We\nmeticulously adapt a state-of-the-art MLLM for a novel task of harnessing\nspatial and semantic cues from input audio to construct Video Scene Layouts\n(VSLs). This serves as an intermediate representation to bridge the gap between\nthe audio and video modalities. 2) Layout-grounded Video Generation: We develop\nan efficient and effective approach to seamlessly integrate VSLs as conditional\nguidance into pre-trained diffusion models, enabling VSL-grounded video\ngeneration in a training-free manner. Extensive experiments demonstrate that\nSpA2V excels in generating realistic videos with semantic and spatial alignment\nto the input audios.", "AI": {"tldr": "SpA2V\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u97f3\u9891\u4e2d\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u7ebf\u7d22\u6784\u5efa\u89c6\u9891\u5e03\u5c40\uff0c\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210\u7684\u8bed\u4e49\u548c\u7a7a\u95f4\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u4fe1\u606f\uff08\u5982\u58f0\u6e90\u7c7b\u522b\uff09\uff0c\u800c\u5ffd\u7565\u4e86\u7a7a\u95f4\u5c5e\u6027\uff08\u5982\u4f4d\u7f6e\u548c\u8fd0\u52a8\u65b9\u5411\uff09\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u89c6\u9891\u5728\u5185\u5bb9\u548c\u7a7a\u95f4\u6784\u6210\u4e0a\u4e0d\u51c6\u786e\u3002\u4eba\u7c7b\u4e0d\u4ec5\u80fd\u8bc6\u522b\u58f0\u6e90\u7684\u8bed\u4e49\u7c7b\u522b\uff0c\u8fd8\u80fd\u786e\u5b9a\u5176\u7a7a\u95f4\u5c5e\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u56e2\u961f\u63d0\u51faSpA2V\uff0c\u9996\u6b21\u660e\u786e\u5229\u7528\u97f3\u9891\u4e2d\u7684\u7a7a\u95f4\u542c\u89c9\u7ebf\u7d22\u6765\u751f\u6210\u8bed\u4e49\u548c\u7a7a\u95f4\u5bf9\u5e94\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u3002", "method": "SpA2V\u6846\u67b6\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u97f3\u9891\u5f15\u5bfc\u7684\u89c6\u9891\u89c4\u5212\uff0c\u5229\u7528MLLM\u4ece\u97f3\u9891\u4e2d\u63d0\u53d6\u7a7a\u95f4\u548c\u8bed\u4e49\u7ebf\u7d22\u6784\u5efa\u89c6\u9891\u573a\u666f\u5e03\u5c40(VSLs)\uff1b2) \u57fa\u4e8e\u5e03\u5c40\u7684\u89c6\u9891\u751f\u6210\uff0c\u5c06VSLs\u4f5c\u4e3a\u6761\u4ef6\u6307\u5bfc\u65e0\u7f1d\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684VSL\u951a\u5b9a\u89c6\u9891\u751f\u6210\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSpA2V\u5728\u751f\u6210\u4e0e\u8f93\u5165\u97f3\u9891\u8bed\u4e49\u548c\u7a7a\u95f4\u5bf9\u9f50\u7684\u903c\u771f\u89c6\u9891\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "SpA2V\u901a\u8fc7\u660e\u786e\u5229\u7528\u97f3\u9891\u4e2d\u7684\u7a7a\u95f4\u542c\u89c9\u7ebf\u7d22\uff0c\u6210\u529f\u751f\u6210\u4e86\u4e0e\u8f93\u5165\u97f3\u9891\u5728\u8bed\u4e49\u548c\u7a7a\u95f4\u4e0a\u9ad8\u5ea6\u4e00\u81f4\u7684\u903c\u771f\u89c6\u9891\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.00234", "categories": ["cs.NI", "cs.AI", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.00234", "abs": "https://arxiv.org/abs/2508.00234", "authors": ["Jin Yang", "Qiong Wu", "Zhiying Feng", "Zhi Zhou", "Deke Guo", "Xu Chen"], "title": "Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts", "comment": "Accepted by IEEE Transactions on Mobile Computing", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities,\nleading to a significant increase in user demand for LLM services. However,\ncloud-based LLM services often suffer from high latency, unstable\nresponsiveness, and privacy concerns. Therefore, multiple LLMs are usually\ndeployed at the network edge to boost real-time responsiveness and protect data\nprivacy, particularly for many emerging smart mobile and IoT applications.\nGiven the varying response quality and latency of LLM services, a critical\nissue is how to route user requests from mobile and IoT devices to an\nappropriate LLM service (i.e., edge LLM expert) to ensure acceptable\nquality-of-service (QoS). Existing routing algorithms fail to simultaneously\naddress the heterogeneity of LLM services, the interference among requests, and\nthe dynamic workloads necessary for maintaining long-term stable QoS. To meet\nthese challenges, in this paper we propose a novel deep reinforcement learning\n(DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM\nservices. Due to the dynamic nature of the global state, we propose a dynamic\nstate abstraction technique to compactly represent global state features with a\nheterogeneous graph attention network (HAN). Additionally, we introduce an\naction impact estimator and a tailored reward function to guide the DRL agent\nin maximizing QoS and preventing latency violations. Extensive experiments on\nboth Poisson and real-world workloads demonstrate that our proposed algorithm\nsignificantly improves average QoS and computing resource efficiency compared\nto existing baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eDRL\u7684QoS\u611f\u77e5LLM\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u72b6\u6001\u62bd\u8c61\u548cHAN\u4f18\u5316\u8def\u7531\uff0c\u663e\u8457\u63d0\u5347\u670d\u52a1\u8d28\u91cf\u548c\u8d44\u6e90\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4e91LLM\u670d\u52a1\u7684\u9ad8\u5ef6\u8fdf\u3001\u54cd\u5e94\u4e0d\u7a33\u5b9a\u548c\u9690\u79c1\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u8def\u7531\u7b97\u6cd5\u65e0\u6cd5\u540c\u65f6\u5904\u7406LLM\u670d\u52a1\u5f02\u6784\u6027\u3001\u8bf7\u6c42\u5e72\u6270\u548c\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u52a8\u6001\u72b6\u6001\u62bd\u8c61\u6280\u672f\u548c\u5f02\u6784\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08HAN\uff09\u7d27\u51d1\u8868\u793a\u5168\u5c40\u72b6\u6001\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u52a8\u4f5c\u5f71\u54cd\u4f30\u8ba1\u5668\u548c\u5b9a\u5236\u5956\u52b1\u51fd\u6570\u6307\u5bfcDRL\u4ee3\u7406\u3002", "result": "\u5728Poisson\u548c\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5e73\u5747QoS\u548c\u8d44\u6e90\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684QoS\u611f\u77e5LLM\u8def\u7531\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5e73\u5747QoS\u548c\u8ba1\u7b97\u8d44\u6e90\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.00045", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00045", "abs": "https://arxiv.org/abs/2508.00045", "authors": ["Samah Kansab"], "title": "Machine Learning Pipeline for Software Engineering: A Systematic Literature Review", "comment": null, "summary": "The rapid advancement of software development practices has introduced\nchallenges in ensuring quality and efficiency across the software engineering\n(SE) lifecycle. As SE systems grow in complexity, traditional approaches often\nfail to scale, resulting in longer debugging times, inefficient defect\ndetection, and resource-heavy development cycles. Machine Learning (ML) has\nemerged as a key solution, enabling automation in tasks such as defect\nprediction, code review, and release quality estimation. However, the\neffectiveness of ML in SE depends on the robustness of its pipeline, including\ndata collection, preprocessing, feature engineering, algorithm selection,\nvalidation, and evaluation.\n  This systematic literature review (SLR) examines state-of-the-art ML\npipelines designed for SE, consolidating best practices, challenges, and gaps.\nOur findings show that robust preprocessing, such as SMOTE for data balancing\nand SZZ-based algorithms for feature selection, improves model reliability.\nEnsemble methods like Random Forest and Gradient Boosting dominate performance\nacross tasks, while simpler models such as Naive Bayes remain valuable for\nefficiency and interpretability. Evaluation metrics including AUC, F1-score,\nand precision are most common, with new metrics like Best Arithmetic Mean (BAM)\nemerging in niche applications. Validation techniques such as bootstrapping are\nwidely used to ensure model stability and generalizability.\n  This SLR highlights the importance of well-designed ML pipelines for\naddressing SE challenges and provides actionable insights for researchers and\npractitioners seeking to optimize software quality and efficiency. By\nidentifying gaps and trends, this study sets a foundation for advancing ML\nadoption and fostering innovation in increasingly complex development\nenvironments.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u603b\u7ed3\u4e86\u673a\u5668\u5b66\u4e60\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u5f3a\u8c03\u4e86\u7a33\u5065\u9884\u5904\u7406\u548c\u96c6\u6210\u65b9\u6cd5\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f18\u5316\u5efa\u8bae\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u5de5\u7a0b\u7cfb\u7edf\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\uff0c\u5bfc\u81f4\u8c03\u8bd5\u65f6\u95f4\u5ef6\u957f\u3001\u7f3a\u9677\u68c0\u6d4b\u6548\u7387\u4f4e\u4e0b\u4ee5\u53ca\u8d44\u6e90\u5bc6\u96c6\u578b\u5f00\u53d1\u5468\u671f\u3002\u673a\u5668\u5b66\u4e60\u6210\u4e3a\u5173\u952e\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u6709\u6548\u6027\u4f9d\u8d56\u4e8e\u5176\u6d41\u6c34\u7ebf\u7684\u7a33\u5065\u6027\u3002", "method": "\u672c\u6587\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff08SLR\uff09\u65b9\u6cd5\uff0c\u6574\u5408\u4e86\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u8bbe\u8ba1\u7684\u5148\u8fdb\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\uff0c\u5305\u62ec\u6700\u4f73\u5b9e\u8df5\u3001\u6311\u6218\u548c\u5dee\u8ddd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5982SMOTE\u6570\u636e\u5e73\u8861\u548c\u57fa\u4e8eSZZ\u7684\u7279\u5f81\u9009\u62e9\u7b49\u7a33\u5065\u9884\u5904\u7406\u63d0\u9ad8\u4e86\u6a21\u578b\u53ef\u9760\u6027\u3002\u96c6\u6210\u65b9\u6cd5\uff08\u5982\u968f\u673a\u68ee\u6797\u548c\u68af\u5ea6\u63d0\u5347\uff09\u5728\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u6734\u7d20\u8d1d\u53f6\u65af\u7b49\u7b80\u5355\u6a21\u578b\u5728\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4ecd\u6709\u4ef7\u503c\u3002AUC\u3001F1\u5206\u6570\u548c\u7cbe\u786e\u5ea6\u662f\u6700\u5e38\u89c1\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u800c\u65b0\u6307\u6807\u5982\u6700\u4f73\u7b97\u672f\u5e73\u5747\uff08BAM\uff09\u5728\u7279\u5b9a\u5e94\u7528\u4e2d\u5d2d\u9732\u5934\u89d2\u3002\u9a8c\u8bc1\u6280\u672f\uff08\u5982\u81ea\u52a9\u6cd5\uff09\u5e7f\u6cdb\u7528\u4e8e\u786e\u4fdd\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u826f\u597d\u7684\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\u5bf9\u4e8e\u89e3\u51b3\u8f6f\u4ef6\u5de5\u7a0b\u6311\u6218\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u4f18\u5316\u8f6f\u4ef6\u8d28\u91cf\u548c\u6548\u7387\u7684\u53ef\u64cd\u4f5c\u89c1\u89e3\u3002\u901a\u8fc7\u8bc6\u522b\u5dee\u8ddd\u548c\u8d8b\u52bf\uff0c\u672c\u7814\u7a76\u4e3a\u5728\u65e5\u76ca\u590d\u6742\u7684\u5f00\u53d1\u73af\u5883\u4e2d\u63a8\u8fdb\u673a\u5668\u5b66\u4e60\u5e94\u7528\u548c\u4fc3\u8fdb\u521b\u65b0\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.00088", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00088", "abs": "https://arxiv.org/abs/2508.00088", "authors": ["Mateo de Mayo", "Daniel Cremers", "Taih\u00fa Pire"], "title": "The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking", "comment": "Accepted to IROS 2025", "summary": "Humanoid robots and mixed reality headsets benefit from the use of\nhead-mounted sensors for tracking. While advancements in visual-inertial\nodometry (VIO) and simultaneous localization and mapping (SLAM) have produced\nnew and high-quality state-of-the-art tracking systems, we show that these are\nstill unable to gracefully handle many of the challenging settings presented in\nthe head-mounted use cases. Common scenarios like high-intensity motions,\ndynamic occlusions, long tracking sessions, low-textured areas, adverse\nlighting conditions, saturation of sensors, to name a few, continue to be\ncovered poorly by existing datasets in the literature. In this way, systems may\ninadvertently overlook these essential real-world issues. To address this, we\npresent the Monado SLAM dataset, a set of real sequences taken from multiple\nvirtual reality headsets. We release the dataset under a permissive CC BY 4.0\nlicense, to drive advancements in VIO/SLAM research and development.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMonado SLAM\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u5934\u6234\u8bbe\u5907\u4f7f\u7528\u573a\u666f\u4e2dVIO/SLAM\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u63a8\u52a8\u76f8\u5173\u6280\u672f\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\u548c\u540c\u6b65\u5b9a\u4f4d\u4e0e\u5730\u56fe\u6784\u5efa\uff08SLAM\uff09\u7cfb\u7edf\u5728\u5934\u6234\u8bbe\u5907\u4f7f\u7528\u573a\u666f\u4e2d\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u5982\u9ad8\u5f3a\u5ea6\u8fd0\u52a8\u3001\u52a8\u6001\u906e\u6321\u7b49\uff0c\u800c\u73b0\u6709\u6570\u636e\u96c6\u672a\u80fd\u5145\u5206\u8986\u76d6\u8fd9\u4e9b\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86Monado SLAM\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea\u591a\u4e2a\u865a\u62df\u73b0\u5b9e\u5934\u6234\u8bbe\u5907\u7684\u771f\u5b9e\u5e8f\u5217\uff0c\u5e76\u4ee5CC BY 4.0\u8bb8\u53ef\u53d1\u5e03\u3002", "result": "Monado SLAM\u6570\u636e\u96c6\u7684\u53d1\u5e03\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u4e3aVIO/SLAM\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6d4b\u8bd5\u73af\u5883\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u53d1\u5e03Monado SLAM\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63a8\u52a8VIO/SLAM\u6280\u672f\u7684\u53d1\u5c55\uff0c\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u5934\u6234\u8bbe\u5907\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.00116", "categories": ["cs.AI", "H.4.1; I.2.1"], "pdf": "https://arxiv.org/pdf/2508.00116", "abs": "https://arxiv.org/abs/2508.00116", "authors": ["Wil M. P. van der Aalst"], "title": "No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence", "comment": "10 pages, 4 figures, preprint keynote paper of the seventh\n  International Conference on Intelligent and Fuzzy Systems (INFUS 2025)", "summary": "The uptake of Artificial Intelligence (AI) impacts the way we work, interact,\ndo business, and conduct research. However, organizations struggle to apply AI\nsuccessfully in industrial settings where the focus is on end-to-end\noperational processes. Here, we consider generative, predictive, and\nprescriptive AI and elaborate on the challenges of diagnosing and improving\nsuch processes. We show that AI needs to be grounded using Object-Centric\nProcess Mining (OCPM). Process-related data are structured and\norganization-specific and, unlike text, processes are often highly dynamic.\nOCPM is the missing link connecting data and processes and enables different\nforms of AI. We use the term Process Intelligence (PI) to refer to the\namalgamation of process-centric data-driven techniques able to deal with a\nvariety of object and event types, enabling AI in an organizational context.\nThis paper explains why AI requires PI to improve operational processes and\nhighlights opportunities for successfully combining OCPM and generative,\npredictive, and prescriptive AI.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u5728\u5de5\u4e1a\u6d41\u7a0b\u4e2d\u7684\u5e94\u7528\u6311\u6218\uff0c\u63d0\u51fa\u901a\u8fc7Object-Centric Process Mining\uff08OCPM\uff09\u548cProcess Intelligence\uff08PI\uff09\u6765\u63d0\u5347AI\u7684\u6709\u6548\u6027\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u4e0e\u6d41\u7a0b\u7ed3\u5408\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7531\u4e8e\u7ec4\u7ec7\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u6210\u529f\u5e94\u7528AI\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7aef\u5230\u7aef\u64cd\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7Process Intelligence\uff08PI\uff09\u548cOCPM\u6765\u63d0\u5347AI\u5728\u6d41\u7a0b\u6539\u8fdb\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u751f\u6210\u5f0f\u3001\u9884\u6d4b\u5f0f\u548c\u89c4\u5b9a\u5f0fAI\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6311\u6218\uff0c\u63d0\u51fa\u4f7f\u7528Object-Centric Process Mining\uff08OCPM\uff09\u6765\u7ed3\u6784\u5316\u6d41\u7a0b\u76f8\u5173\u6570\u636e\uff0c\u4ece\u800c\u652f\u6301AI\u7684\u843d\u5730\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cOCPM\u662f\u8fde\u63a5\u6570\u636e\u548c\u6d41\u7a0b\u7684\u5173\u952e\uff0c\u80fd\u591f\u652f\u6301\u591a\u79cdAI\u5f62\u5f0f\u7684\u5e94\u7528\uff0c\u4ece\u800c\u63d0\u5347\u64cd\u4f5c\u6d41\u7a0b\u7684\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86Process Intelligence\uff08PI\uff09\u4f5c\u4e3a\u8fde\u63a5\u6570\u636e\u548c\u6d41\u7a0b\u7684\u5173\u952e\u6865\u6881\uff0c\u4f7f\u5f97\u751f\u6210\u5f0f\u3001\u9884\u6d4b\u5f0f\u548c\u89c4\u5b9a\u5f0fAI\u80fd\u591f\u5728\u7ec4\u7ec7\u73af\u5883\u4e2d\u6709\u6548\u5e94\u7528\u3002\u901a\u8fc7Object-Centric Process Mining\uff08OCPM\uff09\uff0cAI\u53ef\u4ee5\u66f4\u597d\u5730\u8bca\u65ad\u548c\u6539\u8fdb\u7aef\u5230\u7aef\u7684\u64cd\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2508.00010", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.00010", "abs": "https://arxiv.org/abs/2508.00010", "authors": ["Ruibo Wang", "Baha Eddine Youcef Belmekki", "Howard H. Yang", "Mohamed Slim Alouini"], "title": "Non-Terrestrial Network Models Using Stochastic Geometry: Planar or Spherical?", "comment": null, "summary": "With the explosive deployment of non-terrestrial networks (NTNs), the\ncomputational complexity of network performance analysis is rapidly escalating.\nAs one of the most suitable mathematical tools for analyzing large-scale\nnetwork topologies, stochastic geometry (SG) enables the representation of\nnetwork performance metrics as functions of network parameters, thus offering\nlow-complexity performance analysis solutions. However, choosing between planar\nand spherical models remains challenging. Planar models neglect Earth's\ncurvature, causing deviations in high-altitude NTN analysis, yet are still\noften used for simplicity. This paper introduces relative error to quantify the\ngap between planar and spherical models, helping determine when planar modeling\nis sufficient. To calculate the relative error, we first propose a point\nprocess (PP) generation algorithm that simultaneously generates a pair of\nhomogeneous and asymptotically similar planar and spherical PPs. We then\nintroduce several typical similarity metrics, including topology-related and\nnetwork-level metrics, and further develop a relative error estimation\nalgorithm based on these metrics. In addition, we derive an analytical\nexpression for the optimal planar altitude, which reduces computational\ncomplexity and provides theoretical support for planar approximation. Finally,\nnumerical results investigate how deployment altitude and region affect NTN\nmodeling, with case studies on HAP and LEO satellite constellations.", "AI": {"tldr": "\u672c\u6587\u91cf\u5316\u5e73\u9762\u4e0e\u7403\u9762\u6a21\u578b\u7684\u76f8\u5bf9\u8bef\u5dee\uff0c\u63d0\u51fa\u70b9\u8fc7\u7a0b\u751f\u6210\u4e0e\u8bef\u5dee\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5e76\u63a8\u5bfc\u6700\u4f18\u5e73\u9762\u9ad8\u5ea6\uff0c\u4e3a\u9ad8\u7a7a\u975e\u5730\u9762\u7f51\u7edc\u5206\u6790\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "motivation": "\u968f\u7740\u975e\u5730\u9762\u7f51\u7edc\u7684\u7206\u70b8\u6027\u90e8\u7f72\uff0c\u7f51\u7edc\u6027\u80fd\u5206\u6790\u7684\u8ba1\u7b97\u590d\u6742\u6027\u8fc5\u901f\u589e\u52a0\u3002\u5e73\u9762\u6a21\u578b\u5ffd\u7565\u5730\u7403\u66f2\u7387\uff0c\u5bfc\u81f4\u9ad8\u7a7a\u975e\u5730\u9762\u7f51\u7edc\u5206\u6790\u5b58\u5728\u504f\u5dee\uff0c\u4f46\u4ecd\u56e0\u7b80\u5355\u6027\u88ab\u5e7f\u6cdb\u4f7f\u7528\u3002", "method": "\u63d0\u51fa\u70b9\u8fc7\u7a0b\u751f\u6210\u7b97\u6cd5\u540c\u65f6\u751f\u6210\u4e00\u5bf9\u540c\u8d28\u4e14\u6e10\u8fd1\u76f8\u4f3c\u7684\u5e73\u9762\u548c\u7403\u9762\u70b9\u8fc7\u7a0b\uff0c\u5f15\u5165\u591a\u79cd\u76f8\u4f3c\u6027\u5ea6\u91cf\u6307\u6807\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u6307\u6807\u5f00\u53d1\u4e86\u76f8\u5bf9\u8bef\u5dee\u4f30\u8ba1\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u7ed3\u679c\u7814\u7a76\u4e86\u90e8\u7f72\u9ad8\u5ea6\u548c\u533a\u57df\u5982\u4f55\u5f71\u54cd\u975e\u5730\u9762\u7f51\u7edc\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7HAP\u548cLEO\u536b\u661f\u661f\u5ea7\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u76f8\u5bf9\u8bef\u5dee\u91cf\u5316\u5e73\u9762\u4e0e\u7403\u9762\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u70b9\u8fc7\u7a0b\u751f\u6210\u7b97\u6cd5\u548c\u76f8\u5bf9\u8bef\u5dee\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5e76\u63a8\u5bfc\u4e86\u6700\u4f18\u5e73\u9762\u9ad8\u5ea6\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u4e3a\u5e73\u9762\u8fd1\u4f3c\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2508.00355", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00355", "abs": "https://arxiv.org/abs/2508.00355", "authors": ["Zhenghan Chen", "Haocheng Xu", "Haodong Zhang", "Liang Zhang", "He Li", "Dongqi Wang", "Jiyu Yu", "Yifei Yang", "Zhongxiang Zhou", "Rong Xiong"], "title": "TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots", "comment": null, "summary": "Humanoid robots have the potential capability to perform a diverse range of\nmanipulation tasks, but this is based on a robust and precise standing\ncontroller. Existing methods are either ill-suited to precisely control\nhigh-dimensional upper-body joints, or difficult to ensure both robustness and\naccuracy, especially when upper-body motions are fast. This paper proposes a\nnovel time optimization policy (TOP), to train a standing manipulation control\nmodel that ensures balance, precision, and time efficiency simultaneously, with\nthe idea of adjusting the time trajectory of upper-body motions but not only\nstrengthening the disturbance resistance of the lower-body. Our approach\nconsists of three parts. Firstly, we utilize motion prior to represent\nupper-body motions to enhance the coordination ability between the upper and\nlower-body by training a variational autoencoder (VAE). Then we decouple the\nwhole-body control into an upper-body PD controller for precision and a\nlower-body RL controller to enhance robust stability. Finally, we train TOP\nmethod in conjunction with the decoupled controller and VAE to reduce the\nbalance burden resulting from fast upper-body motions that would destabilize\nthe robot and exceed the capabilities of the lower-body RL policy. The\neffectiveness of the proposed approach is evaluated via both simulation and\nreal world experiments, which demonstrate the superiority on standing\nmanipulation tasks stably and accurately. The project page can be found at\nhttps://anonymous.4open.science/w/top-258F/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u4f18\u5316\u7b56\u7565\uff08TOP\uff09\uff0c\u7ed3\u5408VAE\u548c\u89e3\u8026\u63a7\u5236\u5668\uff0c\u4ee5\u540c\u65f6\u786e\u4fdd\u4eba\u5f62\u673a\u5668\u4eba\u5728\u7ad9\u7acb\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5e73\u8861\u3001\u7cbe\u786e\u6027\u548c\u65f6\u95f4\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u786e\u4fdd\u9ad8\u7ef4\u4e0a\u534a\u8eab\u5173\u8282\u7684\u7cbe\u786e\u63a7\u5236\u548c\u6574\u4f53\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e0a\u534a\u8eab\u8fd0\u52a8\u5feb\u901f\u65f6\u3002", "method": "1. \u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u8868\u793a\u4e0a\u534a\u8eab\u8fd0\u52a8\u4ee5\u589e\u5f3a\u4e0a\u4e0b\u534a\u8eab\u534f\u8c03\u80fd\u529b\uff1b2. \u5c06\u5168\u8eab\u63a7\u5236\u89e3\u8026\u4e3a\u4e0a\u534a\u8eabPD\u63a7\u5236\u5668\uff08\u7cbe\u786e\u63a7\u5236\uff09\u548c\u4e0b\u534a\u8eab\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff08\u589e\u5f3a\u9c81\u68d2\u7a33\u5b9a\u6027\uff09\uff1b3. \u7ed3\u5408TOP\u65b9\u6cd5\u8bad\u7ec3\u4ee5\u51cf\u5c11\u5feb\u901f\u4e0a\u534a\u8eab\u8fd0\u52a8\u5bf9\u5e73\u8861\u7684\u8d1f\u62c5\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u7ad9\u7acb\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u7a33\u5b9a\u6027\u548c\u7cbe\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65f6\u95f4\u4f18\u5316\u7b56\u7565\uff08TOP\uff09\u4e0e\u89e3\u8026\u63a7\u5236\u5668\u548cVAE\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u7ad9\u7acb\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u7a33\u5b9a\u6027\u3001\u7cbe\u786e\u6027\u548c\u65f6\u95f4\u6548\u7387\u3002"}}
{"id": "2508.00083", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00083", "abs": "https://arxiv.org/abs/2508.00083", "authors": ["Yihong Dong", "Xue Jiang", "Jiaru Qian", "Tian Wang", "Kechi Zhang", "Zhi Jin", "Ge Li"], "title": "A Survey on Code Generation with LLM-based Agents", "comment": "Work in progress", "summary": "Code generation agents powered by large language models (LLMs) are\nrevolutionizing the software development paradigm. Distinct from previous code\ngeneration techniques, code generation agents are characterized by three core\nfeatures. 1) Autonomy: the ability to independently manage the entire workflow,\nfrom task decomposition to coding and debugging. 2) Expanded task scope:\ncapabilities that extend beyond generating code snippets to encompass the full\nsoftware development lifecycle (SDLC). 3) Enhancement of engineering\npracticality: a shift in research emphasis from algorithmic innovation toward\npractical engineering challenges, such as system reliability, process\nmanagement, and tool integration. This domain has recently witnessed rapid\ndevelopment and an explosion in research, demonstrating significant application\npotential. This paper presents a systematic survey of the field of LLM-based\ncode generation agents. We trace the technology's developmental trajectory from\nits inception and systematically categorize its core techniques, including both\nsingle-agent and multi-agent architectures. Furthermore, this survey details\nthe applications of LLM-based agents across the full SDLC, summarizes\nmainstream evaluation benchmarks and metrics, and catalogs representative\ntools. Finally, by analyzing the primary challenges, we identify and propose\nseveral foundational, long-term research directions for the future work of the\nfield.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8c03\u67e5\u4e86\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u4ee3\u7406\uff0c\u6db5\u76d6\u5176\u53d1\u5c55\u8f68\u8ff9\u3001\u6838\u5fc3\u6280\u672f\u3001\u5e94\u7528\u573a\u666f\u3001\u8bc4\u4f30\u5de5\u5177\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u6b63\u5728\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u8303\u5f0f\uff0c\u5177\u6709\u81ea\u4e3b\u6027\u3001\u6269\u5c55\u4efb\u52a1\u8303\u56f4\u548c\u589e\u5f3a\u5de5\u7a0b\u5b9e\u7528\u6027\u4e09\u5927\u6838\u5fc3\u7279\u5f81\uff0c\u5c55\u73b0\u4e86\u5de8\u5927\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u672c\u6587\u5bf9\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u9886\u57df\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8c03\u67e5\uff0c\u8ffd\u6eaf\u4e86\u6280\u672f\u7684\u53d1\u5c55\u8f68\u8ff9\uff0c\u5e76\u7cfb\u7edf\u5206\u7c7b\u4e86\u5176\u6838\u5fc3\u6280\u672f\uff0c\u5305\u62ec\u5355\u4ee3\u7406\u548c\u591a\u4ee3\u7406\u67b6\u6784\u3002", "result": "\u672c\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86LLM-based\u4ee3\u7406\u5728\u6574\u4e2a\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u4e3b\u6d41\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u6307\u6807\uff0c\u5e76\u5217\u4e3e\u4e86\u4ee3\u8868\u6027\u5de5\u5177\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5206\u6790LLM-based\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u7684\u4e3b\u8981\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u8be5\u9886\u57df\u672a\u6765\u5de5\u4f5c\u7684\u51e0\u4e2a\u57fa\u7840\u6027\u3001\u957f\u671f\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.00135", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00135", "abs": "https://arxiv.org/abs/2508.00135", "authors": ["Basna Mohammed Salih Hasan", "Ramadhan J. Mstafa"], "title": "Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images", "comment": "12 pages, 18 figures, 5 tables", "summary": "Gender classification has emerged as a crucial aspect in various fields,\nincluding security, human-machine interaction, surveillance, and advertising.\nNonetheless, the accuracy of this classification can be influenced by factors\nsuch as cosmetics and disguise. Consequently, our study is dedicated to\naddressing this concern by concentrating on gender classification using color\nimages of the periocular region. The periocular region refers to the area\nsurrounding the eye, including the eyelids, eyebrows, and the region between\nthem. It contains valuable visual cues that can be used to extract key features\nfor gender classification. This paper introduces a sophisticated Convolutional\nNeural Network (CNN) model that utilizes color image databases to evaluate the\neffectiveness of the periocular region for gender classification. To validate\nthe model's performance, we conducted tests on two eye datasets, namely CVBL\nand (Female and Male). The recommended architecture achieved an outstanding\naccuracy of 99% on the previously unused CVBL dataset while attaining a\ncommendable accuracy of 96% with a small number of learnable parameters\n(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of\nour proposed model for gender classification using the periocular region, we\nevaluated its performance through an extensive range of metrics and compared it\nwith other state-of-the-art approaches. The results unequivocally demonstrate\nthe efficacy of our model, thereby suggesting its potential for practical\napplication in domains such as security and surveillance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u773c\u5468\u533a\u57df\u5f69\u8272\u56fe\u50cf\u7684CNN\u6a21\u578b\uff0c\u7528\u4e8e\u6027\u522b\u5206\u7c7b\uff0c\u5728\u4e24\u5927\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523099%\u548c96%\u7684\u9ad8\u51c6\u786e\u7387\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u548c\u76d1\u63a7\u7b49\u9886\u57df\u3002", "motivation": "\u6027\u522b\u5206\u7c7b\u5728\u5b89\u5168\u3001\u4eba\u673a\u4ea4\u4e92\u3001\u76d1\u63a7\u548c\u5e7f\u544a\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5316\u5986\u54c1\u548c\u4f2a\u88c5\u7b49\u56e0\u7d20\u53ef\u80fd\u5f71\u54cd\u5206\u7c7b\u51c6\u786e\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u4e13\u6ce8\u4e8e\u5229\u7528\u773c\u5468\u533a\u57df\u7684\u5f69\u8272\u56fe\u50cf\u8fdb\u884c\u6027\u522b\u5206\u7c7b\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u590d\u6742\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\uff0c\u5229\u7528\u773c\u5468\u533a\u57df\u7684\u5f69\u8272\u56fe\u50cf\u6570\u636e\u5e93\u8fdb\u884c\u6027\u522b\u5206\u7c7b\u3002", "result": "\u6a21\u578b\u5728CVBL\u6570\u636e\u96c6\u4e0a\u8fbe\u523099%\u7684\u51c6\u786e\u7387\uff0c\u5728Female and Male\u6570\u636e\u96c6\u4e0a\u8fbe\u523096%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u53c2\u6570\u91cf\u8f83\u5c11\uff087,235,089\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684CNN\u6a21\u578b\u5728\u57fa\u4e8e\u773c\u5468\u533a\u57df\u7684\u6027\u522b\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe99%\uff08CVBL\u6570\u636e\u96c6\uff09\u548c96%\uff08Female and Male\u6570\u636e\u96c6\uff09\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b89\u5168\u548c\u76d1\u63a7\u7b49\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.00129", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00129", "abs": "https://arxiv.org/abs/2508.00129", "authors": ["Agust\u00edn Borda", "Juan Bautista Cabral", "Gonzalo Giarda", "Diego Nicol\u00e1s Gimenez Irusta", "Paula Pacheco", "Alvaro Roy Schachner"], "title": "Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis", "comment": null, "summary": "In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem\nthat can greatly affect the results of a Multi-Criteria Decision Method against\na particular set of alternatives. It is therefore useful to have a mechanism\nthat allows one to measure the performance of a method on a set of\nalternatives. This idea could be taken further to build a global ranking of the\neffectiveness of different methods to solve a problem. In this paper, we\npresent three tests that detect the presence of Rank Reversals, along with\ntheir implementation in the Scikit-Criteria library. We also address the\ncomplications that arise when implementing these tests for general scenarios\nand the design considerations we made to handle them. We close with a\ndiscussion about how these additions could play a major role in the judgment of\nmulti-criteria decision methods for problem solving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e09\u79cd\u6d4b\u8bd5\u68c0\u6d4b\u591a\u6807\u51c6\u51b3\u7b56\u4e2d\u7684Rank Reversals\u95ee\u9898\uff0c\u5e76\u5728Scikit-Criteria\u5e93\u4e2d\u5b9e\u73b0\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9\u65b9\u6cd5\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "motivation": "Rank Reversals\u662f\u591a\u6807\u51c6\u51b3\u7b56\u5206\u6790\u4e2d\u7684\u4e00\u4e2a\u4e25\u91cd\u95ee\u9898\uff0c\u53ef\u80fd\u6781\u5927\u5f71\u54cd\u51b3\u7b56\u65b9\u6cd5\u7684\u7ed3\u679c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u6d4b\u91cf\u65b9\u6cd5\u5728\u7279\u5b9a\u66ff\u4ee3\u65b9\u6848\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u68c0\u6d4bRank Reversals\u7684\u6d4b\u8bd5\uff0c\u5e76\u5728Scikit-Criteria\u5e93\u4e2d\u5b9e\u73b0\u4e86\u8fd9\u4e9b\u6d4b\u8bd5\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u5728\u4e00\u822c\u573a\u666f\u4e2d\u5b9e\u65bd\u8fd9\u4e9b\u6d4b\u8bd5\u65f6\u51fa\u73b0\u7684\u590d\u6742\u6027\u95ee\u9898\u3002", "result": "\u6210\u529f\u5f00\u53d1\u5e76\u5b9e\u73b0\u4e86\u4e09\u79cd\u6d4b\u8bd5\u6765\u68c0\u6d4bRank Reversals\uff0c\u5e76\u89e3\u51b3\u4e86\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u7684\u8bbe\u8ba1\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u8ba8\u8bba\u4e86\u5982\u4f55\u901a\u8fc7\u63d0\u51fa\u7684\u4e09\u79cd\u6d4b\u8bd5\u6765\u68c0\u6d4bRank Reversals\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u6d4b\u8bd5\u5728Scikit-Criteria\u5e93\u4e2d\u7684\u5b9e\u73b0\u53ca\u5176\u5bf9\u591a\u6807\u51c6\u51b3\u7b56\u65b9\u6cd5\u8bc4\u4f30\u7684\u6f5c\u5728\u5f71\u54cd\u3002"}}
{"id": "2508.00011", "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00011", "abs": "https://arxiv.org/abs/2508.00011", "authors": ["Ahmet Melih Ince", "Ayse Elif Canbilen", "Halim Yanikomeroglu"], "title": "AoI-Aware Resource Allocation with Deep Reinforcement Learning for HAPS-V2X Networks", "comment": "6 pages, 3 figures, to appear in IEEE conference proceedings", "summary": "Sixth-generation (6G) networks are designed to meet the hyper-reliable and\nlow-latency communication (HRLLC) requirements of safety-critical applications\nsuch as autonomous driving. Integrating non-terrestrial networks (NTN) into the\n6G infrastructure brings redundancy to the network, ensuring continuity of\ncommunications even under extreme conditions. In particular, high-altitude\nplatform stations (HAPS) stand out for their wide coverage and low latency\nadvantages, supporting communication reliability and enhancing information\nfreshness, especially in rural areas and regions with infrastructure\nconstraints. In this paper, we present reinforcement learning-based approaches\nusing deep deterministic policy gradient (DDPG) to dynamically optimize the\nage-of-information (AoI) in HAPS-enabled vehicle-to-everything (V2X) networks.\nThe proposed method improves information freshness and overall network\nreliability by enabling independent learning without centralized coordination.\nThe findings reveal the potential of HAPS-supported solutions, combined with\nDDPG-based learning, for efficient AoI-aware resource allocation in\nplatoon-based autonomous vehicle systems.", "AI": {"tldr": "The paper proposes a DDPG-based approach to optimize AoI in HAPS-enabled V2X networks, enhancing 6G reliability for autonomous vehicles.", "motivation": "6G networks aim to meet HRLLC requirements for safety-critical applications like autonomous driving. Integrating NTN, especially HAPS, provides redundancy and ensures communication continuity under extreme conditions, particularly in rural or infrastructure-limited areas.", "method": "The paper presents reinforcement learning-based approaches using deep deterministic policy gradient (DDPG) to dynamically optimize the age-of-information (AoI) in HAPS-enabled vehicle-to-everything (V2X) networks.", "result": "The proposed method improves information freshness and network reliability by enabling independent learning without centralized coordination.", "conclusion": "The study highlights the potential of HAPS-supported solutions combined with DDPG-based learning for efficient AoI-aware resource allocation in platoon-based autonomous vehicle systems."}}
{"id": "2508.00362", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00362", "abs": "https://arxiv.org/abs/2508.00362", "authors": ["Zhenghan Chen", "Haodong Zhang", "Dongqi Wang", "Jiyu Yu", "Haocheng Xu", "Yue Wang", "Rong Xiong"], "title": "A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot", "comment": null, "summary": "Motion imitation is a pivotal and effective approach for humanoid robots to\nachieve a more diverse range of complex and expressive movements, making their\nperformances more human-like. However, the significant differences in\nkinematics and dynamics between humanoid robots and humans present a major\nchallenge in accurately imitating motion while maintaining balance. In this\npaper, we propose a novel whole-body motion imitation framework for a full-size\nhumanoid robot. The proposed method employs contact-aware whole-body motion\nretargeting to mimic human motion and provide initial values for reference\ntrajectories, and the non-linear centroidal model predictive controller ensures\nthe motion accuracy while maintaining balance and overcoming external\ndisturbances in real time. The assistance of the whole-body controller allows\nfor more precise torque control. Experiments have been conducted to imitate a\nvariety of human motions both in simulation and in a real-world humanoid robot.\nThese experiments demonstrate the capability of performing with accuracy and\nadaptability, which validates the effectiveness of our approach.", "AI": {"tldr": "A novel framework for humanoid robots to imitate human motions accurately, using motion retargeting and predictive control, validated by experiments.", "motivation": "Motion imitation enhances humanoid robots' ability to perform complex, human-like movements, but kinematic and dynamic differences pose challenges in maintaining balance and accuracy.", "method": "The method combines contact-aware whole-body motion retargeting for initial trajectory values and a non-linear centroidal model predictive controller for real-time balance and accuracy.", "result": "Experiments show the framework's capability to accurately imitate various human motions while maintaining balance and overcoming disturbances.", "conclusion": "The proposed framework effectively enables humanoid robots to imitate human motions with accuracy and adaptability, validated through both simulation and real-world experiments."}}
{"id": "2508.00128", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00128", "abs": "https://arxiv.org/abs/2508.00128", "authors": ["Md Nazmul Haque", "Hua Yang", "Zhou Yang", "Bowen Xu"], "title": "How Quantization Impacts Privacy Risk on LLMs for Code?", "comment": null, "summary": "Large language models for code (LLMs4Code) rely heavily on massive training\ndata, including sensitive data, such as cloud service credentials of the\nprojects and personal identifiable information of the developers, raising\nserious privacy concerns. Membership inference (MI) has recently emerged as an\neffective tool for assessing privacy risk by identifying whether specific data\nbelong to a model's training set. In parallel, model compression techniques,\nespecially quantization, have gained traction for reducing computational costs\nand enabling the deployment of large models. However, while quantized models\nstill retain knowledge learned from the original training data, it remains\nunclear whether quantization affects their ability to retain and expose privacy\ninformation. Answering this question is of great importance to understanding\nprivacy risks in real-world deployments. In this work, we conduct the first\nempirical study on how quantization influences task performance and privacy\nrisk simultaneously in LLMs4Code. To do this, we implement widely used\nquantization techniques (static and dynamic) to three representative model\nfamilies, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that\nquantization has a significant impact on reducing the privacy risk relative to\nthe original model. We also uncover a positive correlation between task\nperformance and privacy risk, indicating an underlying tradeoff. Moreover, we\nreveal the possibility that quantizing larger models could yield better balance\nthan using full-precision small models. Finally, we demonstrate that these\nfindings generalize across different architectures, model sizes and MI methods,\noffering practical guidance for safeguarding privacy when deploying compressed\nLLMs4Code.", "AI": {"tldr": "\u91cf\u5316\u6280\u672f\u53ef\u964d\u4f4e\u4ee3\u7801\u5927\u6a21\u578b\u7684\u9690\u79c1\u98ce\u9669\uff0c\u540c\u65f6\u63ed\u793a\u6027\u80fd\u4e0e\u98ce\u9669\u7684\u6743\u8861\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u91cf\u5316\u6a21\u578b\u4ecd\u4fdd\u7559\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u77e5\u8bc6\uff0c\u4f46\u5176\u5bf9\u9690\u79c1\u4fe1\u606f\u4fdd\u7559\u548c\u66b4\u9732\u80fd\u529b\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u8fd9\u5bf9\u7406\u89e3\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u9690\u79c1\u98ce\u9669\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u5b9e\u73b0\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u91cf\u5316\u6280\u672f\uff08\u9759\u6001\u548c\u52a8\u6001\uff09\u5e94\u7528\u4e8e\u4e09\u79cd\u4ee3\u8868\u6027\u6a21\u578b\u5bb6\u65cf\uff08Pythia\u3001CodeGen\u548cGPTNeo\uff09\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5bf9\u4efb\u52a1\u6027\u80fd\u548c\u9690\u79c1\u98ce\u9669\u7684\u5f71\u54cd\u3002", "result": "\u91cf\u5316\u663e\u8457\u964d\u4f4e\u4e86\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u53d1\u73b0\u4efb\u52a1\u6027\u80fd\u4e0e\u9690\u79c1\u98ce\u9669\u4e4b\u95f4\u5b58\u5728\u6b63\u76f8\u5173\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u91cf\u5316\u8f83\u5927\u6a21\u578b\u53ef\u80fd\u6bd4\u4f7f\u7528\u5168\u7cbe\u5ea6\u5c0f\u6a21\u578b\u83b7\u5f97\u66f4\u597d\u7684\u5e73\u8861\u3002", "conclusion": "\u91cf\u5316\u6280\u672f\u663e\u8457\u964d\u4f4e\u4e86LLMs4Code\u7684\u9690\u79c1\u98ce\u9669\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4efb\u52a1\u6027\u80fd\u4e0e\u9690\u79c1\u98ce\u9669\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u90e8\u7f72\u538b\u7f29\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2508.00144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00144", "abs": "https://arxiv.org/abs/2508.00144", "authors": ["Akshat Rakheja", "Aarsh Ashdhir", "Aryan Bhattacharjee", "Vanshika Sharma"], "title": "World Consistency Score: A Unified Metric for Video Generation Quality", "comment": "27 pages, 1 figure", "summary": "We introduce World Consistency Score (WCS), a novel unified evaluation metric\nfor generative video models that emphasizes internal world consistency of the\ngenerated videos. WCS integrates four interpretable sub-components - object\npermanence, relation stability, causal compliance, and flicker penalty - each\nmeasuring a distinct aspect of temporal and physical coherence in a video.\nThese submetrics are combined via a learned weighted formula to produce a\nsingle consistency score that aligns with human judgments. We detail the\nmotivation for WCS in the context of existing video evaluation metrics,\nformalize each submetric and how it is computed with open-source tools\n(trackers, action recognizers, CLIP embeddings, optical flow), and describe how\nthe weights of the WCS combination are trained using human preference data. We\nalso outline an experimental validation blueprint: using benchmarks like\nVBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human\nevaluations, performing sensitivity analyses, and comparing WCS against\nestablished metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a\ncomprehensive and interpretable framework for evaluating video generation\nmodels on their ability to maintain a coherent \"world\" over time, addressing\ngaps left by prior metrics focused only on visual fidelity or prompt alignment.", "AI": {"tldr": "WCS \u662f\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u6574\u5408\u56db\u4e2a\u5b50\u7ec4\u4ef6\uff08\u7269\u4f53\u6301\u4e45\u6027\u7b49\uff09\u5e76\u5b66\u4e60\u52a0\u6743\u516c\u5f0f\uff0c\u5168\u9762\u8bc4\u4f30\u65f6\u95f4\u8fde\u8d2f\u6027\uff0c\u5f25\u8865\u73b0\u6709\u6307\u6807\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8bc4\u4f30\u6307\u6807\u4ec5\u5173\u6ce8\u89c6\u89c9\u4fdd\u771f\u5ea6\u6216\u63d0\u793a\u5bf9\u9f50\uff0c\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u7684\u8bc4\u4f30\uff0cWCS \u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "WCS \u6574\u5408\u4e86\u56db\u4e2a\u53ef\u89e3\u91ca\u7684\u5b50\u7ec4\u4ef6\uff08\u7269\u4f53\u6301\u4e45\u6027\u3001\u5173\u7cfb\u7a33\u5b9a\u6027\u3001\u56e0\u679c\u5408\u89c4\u6027\u548c\u95ea\u70c1\u60e9\u7f5a\uff09\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u7684\u52a0\u6743\u516c\u5f0f\u751f\u6210\u4e00\u4e2a\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4\u7684\u4e00\u81f4\u6027\u5206\u6570\u3002", "result": "WCS \u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff08\u5982 VBench-2.0\u3001EvalCrafter \u548c LOVE\uff09\u5c55\u793a\u4e86\u5176\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u76f8\u5173\u6027\uff0c\u5e76\u5728\u654f\u611f\u5ea6\u5206\u6790\u548c\u4e0e\u73b0\u6709\u6307\u6807\uff08FVD\u3001CLIPScore\u3001VBench\u3001FVMD\uff09\u7684\u6bd4\u8f83\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "WCS \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u7ef4\u6301\u65f6\u95f4\u8fde\u8d2f\u6027\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u6307\u6807\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u6216\u63d0\u793a\u5bf9\u9f50\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.00137", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00137", "abs": "https://arxiv.org/abs/2508.00137", "authors": ["Shqiponja Ahmetaj", "George Konstantinidis", "Magdalena Ortiz", "Paolo Pareti", "Mantas Simkus"], "title": "SHACL Validation under Graph Updates (Extended Paper)", "comment": "Accepted at the International Semantic Web Conference (ISWC 2025)", "summary": "SHACL (SHApe Constraint Language) is a W3C standardized constraint language\nfor RDF graphs. In this paper, we study SHACL validation in RDF graphs under\nupdates. We present a SHACL-based update language that can capture intuitive\nand realistic modifications on RDF graphs and study the problem of static\nvalidation under such updates. This problem asks to verify whether every graph\nthat validates a SHACL specification will still do so after applying a given\nupdate sequence. More importantly, it provides a basis for further services for\nreasoning about evolving RDF graphs. Using a regression technique that embeds\nthe update actions into SHACL constraints, we show that static validation under\nupdates can be reduced to (un)satisfiability of constraints in (a minor\nextension of) SHACL. We analyze the computational complexity of the static\nvalidation problem for SHACL and some key fragments. Finally, we present a\nprototype implementation that performs static validation and other static\nanalysis tasks on SHACL constraints and demonstrate its behavior through\npreliminary experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cdSHACL\u66f4\u65b0\u8bed\u8a00\uff0c\u901a\u8fc7\u56de\u5f52\u6280\u672f\u9a8c\u8bc1RDF\u56fe\u66f4\u65b0\u540e\u662f\u5426\u4ecd\u6ee1\u8db3SHACL\u89c4\u8303\uff0c\u5e76\u5206\u6790\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5b9e\u73b0\u539f\u578b\u3002", "motivation": "\u7814\u7a76RDF\u56fe\u5728\u66f4\u65b0\u540e\u7684SHACL\u9a8c\u8bc1\u95ee\u9898\uff0c\u4e3a\u63a8\u7406\u6f14\u5316\u4e2d\u7684RDF\u56fe\u63d0\u4f9b\u57fa\u7840\u670d\u52a1\u3002", "method": "\u91c7\u7528\u56de\u5f52\u6280\u672f\u5c06\u66f4\u65b0\u64cd\u4f5c\u5d4c\u5165SHACL\u7ea6\u675f\u4e2d\uff0c\u5c06\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\u8f6c\u5316\u4e3aSHACL\u7ea6\u675f\u7684\uff08\u4e0d\uff09\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u901a\u8fc7\u539f\u578b\u5b9e\u73b0\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8fdb\u884c\u4e86\u521d\u6b65\u5b9e\u9a8c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSHACL\u7684\u66f4\u65b0\u8bed\u8a00\uff0c\u7528\u4e8e\u9a8c\u8bc1RDF\u56fe\u5728\u66f4\u65b0\u540e\u662f\u5426\u4ecd\u6ee1\u8db3SHACL\u89c4\u8303\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u5b9e\u73b0\u5c55\u793a\u4e86\u5176\u53ef\u884c\u6027\u3002"}}
{"id": "2508.00020", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.00020", "abs": "https://arxiv.org/abs/2508.00020", "authors": ["Ferdaous Tarhouni", "Ruibo Wang", "Mohamed-Slim Alouini"], "title": "Performance Analysis of SAGIN from the Relay Perspective: A Spherical Stochastic Geometry Approach", "comment": null, "summary": "In recent years, the satellite-aerial-ground integrated network (SAGIN) has\nbecome essential in meeting the increasing demands for global wireless\ncommunications. In SAGIN, high-altitude platforms (HAPs) can serve as\ncommunication hubs and act as relays to enhance communication performance. In\nthis paper, we evaluate network performance and analyze the role of HAPs in\nSAGIN from the relay perspective. Based on this unique perspective, we\nintroduce three metrics to evaluate the performance, named the average access\ndata rate, the average backhaul data rate, and the backhaul rate exceedance\nprobability (BREP). Considering the need for dynamic topology and interference\nanalysis, we choose spherical stochastic geometry (SSG) as a tool and derive\nanalytical expressions for the above metrics to achieve low-complexity\nperformance evaluation. Specifically, we provide a closed-form expression for\nthe end-to-end performance metric BREP. Given that there is no existing\nliterature in the SSG field studying networks from a relay perspective, we\nspecifically investigate the impact of satellite network topology on\nperformance in our numerical results to further highlight the advantages of the\nSSG framework. Additionally, we analyze the minimum HAP transmission power\nrequired to maintain both short-term and long-term data rate demands.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u7403\u5f62\u968f\u673a\u51e0\u4f55\uff08SSG\uff09\u4ece\u7ee7\u7535\u5668\u89d2\u5ea6\u8bc4\u4f30\u4e86HAPs\u5728SAGIN\u4e2d\u7684\u6027\u80fd\uff0c\u5f15\u5165\u4e86\u4e09\u4e2a\u65b0\u6307\u6807\u5e76\u63d0\u4f9b\u4e86BREP\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5206\u6790\u4e86\u7f51\u7edc\u62d3\u6251\u5f71\u54cd\u548c\u6700\u5c0f\u4f20\u8f93\u529f\u7387\u9700\u6c42\u3002", "motivation": "\u968f\u7740\u5168\u7403\u65e0\u7ebf\u901a\u4fe1\u9700\u6c42\u7684\u589e\u957f\uff0c\u536b\u661f-\u7a7a\u4e2d-\u5730\u9762\u4e00\u4f53\u5316\u7f51\u7edc\uff08SAGIN\uff09\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u9ad8\u6d77\u62d4\u5e73\u53f0\uff08HAPs\uff09\u4f5c\u4e3a\u901a\u4fe1\u67a2\u7ebd\u548c\u4e2d\u7ee7\u5668\uff0c\u53ef\u4ee5\u63d0\u5347\u901a\u4fe1\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u4ece\u7ee7\u7535\u5668\u89d2\u5ea6\u8bc4\u4f30HAPs\u5728SAGIN\u4e2d\u7684\u7f51\u7edc\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7403\u5f62\u968f\u673a\u51e0\u4f55\uff08SSG\uff09\u4f5c\u4e3a\u5de5\u5177\uff0c\u63a8\u5bfc\u4e86\u4e09\u4e2a\u6027\u80fd\u6307\u6807\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u4ee5\u5b9e\u73b0\u4f4e\u590d\u6742\u5ea6\u7684\u6027\u80fd\u8bc4\u4f30\u3002\u7279\u522b\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u6027\u80fd\u6307\u6807BREP\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86BREP\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5206\u6790\u4e86\u536b\u661f\u7f51\u7edc\u62d3\u6251\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u786e\u5b9a\u4e86\u6ee1\u8db3\u6570\u636e\u901f\u7387\u9700\u6c42\u7684\u6700\u5c0fHAP\u4f20\u8f93\u529f\u7387\u3002\u8fd9\u4e9b\u7ed3\u679c\u7a81\u51fa\u4e86SSG\u6846\u67b6\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u4e09\u4e2a\u6027\u80fd\u6307\u6807\uff08\u5e73\u5747\u63a5\u5165\u6570\u636e\u901f\u7387\u3001\u5e73\u5747\u56de\u7a0b\u6570\u636e\u901f\u7387\u548c\u56de\u7a0b\u901f\u7387\u8d85\u8d8a\u6982\u7387BREP\uff09\uff0c\u5229\u7528\u7403\u5f62\u968f\u673a\u51e0\u4f55\uff08SSG\uff09\u5de5\u5177\uff0c\u4ece\u7ee7\u7535\u5668\u89d2\u5ea6\u8bc4\u4f30\u4e86HAPs\u5728SAGIN\u4e2d\u7684\u7f51\u7edc\u6027\u80fd\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u536b\u661f\u7f51\u7edc\u62d3\u6251\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u786e\u5b9a\u4e86\u6ee1\u8db3\u77ed\u671f\u548c\u957f\u671f\u6570\u636e\u901f\u7387\u9700\u6c42\u7684\u6700\u5c0fHAP\u4f20\u8f93\u529f\u7387\u3002"}}
{"id": "2508.00384", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00384", "abs": "https://arxiv.org/abs/2508.00384", "authors": ["Juanwu Lu", "Rohit Gupta", "Ahmadreza Moradipari", "Kyungtae Han", "Ruqi Zhang", "Ziran Wang"], "title": "On Learning Closed-Loop Probabilistic Multi-Agent Simulator", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. Source Code: https://github.com/juanwulu/niva", "summary": "The rapid iteration of autonomous vehicle (AV) deployments leads to\nincreasing needs for building realistic and scalable multi-agent traffic\nsimulators for efficient evaluation. Recent advances in this area focus on\nclosed-loop simulators that enable generating diverse and interactive\nscenarios. This paper introduces Neural Interactive Agents (NIVA), a\nprobabilistic framework for multi-agent simulation driven by a hierarchical\nBayesian model that enables closed-loop, observation-conditioned simulation\nthrough autoregressive sampling from a latent, finite mixture of Gaussian\ndistributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence\ntrajectory prediction models and emerging closed-loop simulation models trained\non Next-token Prediction (NTP) from a Bayesian inference perspective.\nExperiments on the Waymo Open Motion Dataset demonstrate that NIVA attains\ncompetitive performance compared to the existing method while providing\nembellishing control over intentions and driving styles.", "AI": {"tldr": "NIVA\u662f\u4e00\u4e2a\u57fa\u4e8e\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7\u95ed\u73af\u6a21\u62df\u5728\u81ea\u52a8\u9a7e\u9a76\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5feb\u901f\u8fed\u4ee3\u90e8\u7f72\uff0c\u6784\u5efa\u771f\u5b9e\u4e14\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u6a21\u62df\u5668\u4ee5\u8fdb\u884c\u9ad8\u6548\u8bc4\u4f30\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002", "method": "NIVA\u662f\u4e00\u4e2a\u57fa\u4e8e\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u7684\u6982\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u91c7\u6837\u4ece\u6f5c\u5728\u7684\u9ad8\u65af\u5206\u5e03\u6df7\u5408\u4e2d\u8fdb\u884c\u95ed\u73af\u3001\u89c2\u5bdf\u6761\u4ef6\u6a21\u62df\u3002", "result": "NIVA\u5728Waymo Open Motion\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u5728\u610f\u56fe\u548c\u9a7e\u9a76\u98ce\u683c\u63a7\u5236\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "NIVA\u6846\u67b6\u5728Waymo Open Motion\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5bf9\u610f\u56fe\u548c\u9a7e\u9a76\u98ce\u683c\u7684\u589e\u5f3a\u63a7\u5236\u3002"}}
{"id": "2508.00198", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00198", "abs": "https://arxiv.org/abs/2508.00198", "authors": ["Cleyton Magalhaes", "Italo Santos", "Brody Stuart-Verner", "Ronnie de Souza Santos"], "title": "Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems", "comment": null, "summary": "Background: Software systems powered by large language models are becoming a\nroutine part of everyday technologies, supporting applications across a wide\nrange of domains. In software engineering, many studies have focused on how\nLLMs support tasks such as code generation, debugging, and documentation.\nHowever, there has been limited focus on how full systems that integrate LLMs\nare tested during development. Aims: This study explores how LLM-powered\nsystems are tested in the context of real-world application development.\nMethod: We conducted an exploratory case study using 99 individual reports\nwritten by students who built and deployed LLM-powered applications as part of\na university course. Each report was independently analyzed using thematic\nanalysis, supported by a structured coding process. Results: Testing strategies\ncombined manual and automated methods to evaluate both system logic and model\nbehavior. Common practices included exploratory testing, unit testing, and\nprompt iteration. Reported challenges included integration failures,\nunpredictable outputs, prompt sensitivity, hallucinations, and uncertainty\nabout correctness. Conclusions: Testing LLM-powered systems required\nadaptations to traditional verification methods, blending source-level\nreasoning with behavior-aware evaluations. These findings provide evidence on\nthe practical context of testing generative components in software systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u73b0\u5b9e\u5e94\u7528\u5f00\u53d1\u4e2d\u5982\u4f55\u6d4b\u8bd5\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\uff0c\u53d1\u73b0\u6d4b\u8bd5\u7b56\u7565\u9700\u7ed3\u5408\u624b\u52a8\u4e0e\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5e76\u9762\u4e34\u5982\u4e0d\u53ef\u9884\u6d4b\u8f93\u51fa\u7b49\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u4ee3\u7801\u751f\u6210\u3001\u8c03\u8bd5\u548c\u6587\u6863\u7b49\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5df2\u6709\u5f88\u591a\u7814\u7a76\uff0c\u4f46\u5173\u4e8e\u5728\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u5982\u4f55\u6d4b\u8bd5\u96c6\u6210LLM\u7684\u5b8c\u6574\u7cfb\u7edf\u7684\u7814\u7a76\u5374\u5f88\u5c11\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u73b0\u5b9e\u5e94\u7528\u5f00\u53d1\u4e2d\u5982\u4f55\u6d4b\u8bd5\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u3002", "method": "\u6211\u4eec\u8fdb\u884c\u4e86\u4e00\u9879\u63a2\u7d22\u6027\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u4e8699\u4efd\u7531\u5b66\u751f\u64b0\u5199\u7684\u62a5\u544a\uff0c\u8fd9\u4e9b\u62a5\u544a\u8bb0\u5f55\u4e86\u4ed6\u4eec\u5728\u5927\u5b66\u8bfe\u7a0b\u4e2d\u6784\u5efa\u548c\u90e8\u7f72\u57fa\u4e8eLLM\u7684\u5e94\u7528\u7a0b\u5e8f\u7684\u8fc7\u7a0b\u3002\u6bcf\u4efd\u62a5\u544a\u90fd\u901a\u8fc7\u4e3b\u9898\u5206\u6790\u548c\u7ed3\u6784\u5316\u7f16\u7801\u8fc7\u7a0b\u8fdb\u884c\u4e86\u72ec\u7acb\u5206\u6790\u3002", "result": "\u6d4b\u8bd5\u7b56\u7565\u7ed3\u5408\u4e86\u624b\u52a8\u548c\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u8bc4\u4f30\u7cfb\u7edf\u903b\u8f91\u548c\u6a21\u578b\u884c\u4e3a\u3002\u5e38\u89c1\u5b9e\u8df5\u5305\u62ec\u63a2\u7d22\u6027\u6d4b\u8bd5\u3001\u5355\u5143\u6d4b\u8bd5\u548c\u63d0\u793a\u8fed\u4ee3\u3002\u62a5\u544a\u7684\u6311\u6218\u5305\u62ec\u96c6\u6210\u5931\u8d25\u3001\u4e0d\u53ef\u9884\u6d4b\u7684\u8f93\u51fa\u3001\u63d0\u793a\u654f\u611f\u6027\u3001\u5e7b\u89c9\u4ee5\u53ca\u5bf9\u6b63\u786e\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u6d4b\u8bd5\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u9700\u8981\u8c03\u6574\u4f20\u7edf\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7ed3\u5408\u6e90\u4ee3\u7801\u5c42\u9762\u7684\u63a8\u7406\u548c\u884c\u4e3a\u611f\u77e5\u8bc4\u4f30\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u751f\u6210\u5f0f\u7ec4\u4ef6\u7684\u5b9e\u9645\u6d4b\u8bd5\u73af\u5883\u63d0\u4f9b\u4e86\u8bc1\u636e\u3002"}}
{"id": "2508.00152", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00152", "abs": "https://arxiv.org/abs/2508.00152", "authors": ["Li Mi", "Manon Bechaz", "Zeming Chen", "Antoine Bosselut", "Devis Tuia"], "title": "GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration", "comment": "ICCV 2025. Project page at https://limirs.github.io/GeoExplorer/", "summary": "Active Geo-localization (AGL) is the task of localizing a goal, represented\nin various modalities (e.g., aerial images, ground-level images, or text),\nwithin a predefined search area. Current methods approach AGL as a\ngoal-reaching reinforcement learning (RL) problem with a distance-based reward.\nThey localize the goal by implicitly learning to minimize the relative distance\nfrom it. However, when distance estimation becomes challenging or when\nencountering unseen targets and environments, the agent exhibits reduced\nrobustness and generalization ability due to the less reliable exploration\nstrategy learned during training. In this paper, we propose GeoExplorer, an AGL\nagent that incorporates curiosity-driven exploration through intrinsic rewards.\nUnlike distance-based rewards, our curiosity-driven reward is goal-agnostic,\nenabling robust, diverse, and contextually relevant exploration based on\neffective environment modeling. These capabilities have been proven through\nextensive experiments across four AGL benchmarks, demonstrating the\neffectiveness and generalization ability of GeoExplorer in diverse settings,\nparticularly in localizing unfamiliar targets and environments.", "AI": {"tldr": "GeoExplorer\u901a\u8fc7\u597d\u5947\u5fc3\u9a71\u52a8\u63a2\u7d22\u63d0\u5347\u4e3b\u52a8\u5730\u7406\u5b9a\u4f4d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u6837\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u8ddd\u79bb\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u8ddd\u79bb\u4f30\u8ba1\u56f0\u96be\u6216\u9762\u5bf9\u672a\u89c1\u76ee\u6807\u548c\u73af\u5883\u65f6\uff0c\u7531\u4e8e\u63a2\u7d22\u7b56\u7565\u4e0d\u53ef\u9760\uff0c\u5bfc\u81f4\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGeoExplorer\u7684AGL\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u57fa\u4e8e\u5185\u5728\u5956\u52b1\u7684\u597d\u5947\u5fc3\u9a71\u52a8\u63a2\u7d22\u7b56\u7565\uff0c\u66ff\u4ee3\u4f20\u7edf\u57fa\u4e8e\u8ddd\u79bb\u7684\u5956\u52b1\u673a\u5236\u3002", "result": "\u5728\u56db\u4e2aAGL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGeoExplorer\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6548\u679c\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5b9a\u4f4d\u964c\u751f\u76ee\u6807\u548c\u73af\u5883\u65f6\u3002", "conclusion": "GeoExplorer\u901a\u8fc7\u5f15\u5165\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u63a2\u7d22\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u4e3b\u52a8\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u548c\u73af\u5883\u65f6\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2508.00138", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00138", "abs": "https://arxiv.org/abs/2508.00138", "authors": ["Rashid Mushkani", "Hugo Berard", "Toumadher Ammar", "Cassandre Chatonnier", "Shin Koseki"], "title": "Co-Producing AI: Toward an Augmented, Participatory Lifecycle", "comment": "Eighth AAAI/ACM Conference on AI, Ethics, and Society 2025", "summary": "Despite efforts to mitigate the inherent risks and biases of artificial\nintelligence (AI) algorithms, these algorithms can disproportionately impact\nculturally marginalized groups. A range of approaches has been proposed to\naddress or reduce these risks, including the development of ethical guidelines\nand principles for responsible AI, as well as technical solutions that promote\nalgorithmic fairness. Drawing on design justice, expansive learning theory, and\nrecent empirical work on participatory AI, we argue that mitigating these harms\nrequires a fundamental re-architecture of the AI production pipeline. This\nre-design should center co-production, diversity, equity, inclusion (DEI), and\nmultidisciplinary collaboration. We introduce an augmented AI lifecycle\nconsisting of five interconnected phases: co-framing, co-design,\nco-implementation, co-deployment, and co-maintenance. The lifecycle is informed\nby four multidisciplinary workshops and grounded in themes of distributed\nauthority and iterative knowledge exchange. Finally, we relate the proposed\nlifecycle to several leading ethical frameworks and outline key research\nquestions that remain for scaling participatory governance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684AI\u751f\u547d\u5468\u671f\uff0c\u5f3a\u8c03\u5171\u540c\u751f\u4ea7\u548c\u591a\u5b66\u79d1\u5408\u4f5c\uff0c\u4ee5\u89e3\u51b3AI\u7b97\u6cd5\u7684\u4e0d\u5e73\u7b49\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u52aa\u529b\u51cf\u8f7bAI\u7b97\u6cd5\u7684\u98ce\u9669\u548c\u504f\u89c1\uff0c\u4f46\u8fd9\u4e9b\u7b97\u6cd5\u4ecd\u5bf9\u6587\u5316\u8fb9\u7f18\u5316\u7fa4\u4f53\u4ea7\u751f\u4e0d\u6210\u6bd4\u4f8b\u7684\u5f71\u54cd\uff0c\u9700\u8981\u4ece\u6839\u672c\u4e0a\u91cd\u65b0\u8bbe\u8ba1AI\u751f\u4ea7\u6d41\u7a0b\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u8bbe\u8ba1\u6b63\u4e49\u3001\u6269\u5c55\u5b66\u4e60\u7406\u8bba\u548c\u53c2\u4e0e\u5f0fAI\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7531\u4e94\u4e2a\u76f8\u4e92\u5173\u8054\u9636\u6bb5\u7ec4\u6210\u7684\u589e\u5f3aAI\u751f\u547d\u5468\u671f\uff1a\u5171\u540c\u6846\u67b6\u3001\u5171\u540c\u8bbe\u8ba1\u3001\u5171\u540c\u5b9e\u65bd\u3001\u5171\u540c\u90e8\u7f72\u548c\u5171\u540c\u7ef4\u62a4\u3002", "result": "\u63d0\u51fa\u7684\u751f\u547d\u5468\u671f\u57fa\u4e8e\u56db\u4e2a\u591a\u5b66\u79d1\u7814\u8ba8\u4f1a\uff0c\u5e76\u4f53\u73b0\u4e86\u5206\u5e03\u5f0f\u6743\u5a01\u548c\u8fed\u4ee3\u77e5\u8bc6\u4ea4\u6362\u7684\u4e3b\u9898\uff0c\u4e0e\u591a\u4e2a\u9886\u5148\u7684\u4f26\u7406\u6846\u67b6\u76f8\u5173\u8054\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u540c\u751f\u4ea7\u7684AI\u751f\u547d\u5468\u671f\uff0c\u5f3a\u8c03\u591a\u6837\u6027\u3001\u516c\u5e73\u6027\u548c\u5305\u5bb9\u6027\uff08DEI\uff09\u4ee5\u53ca\u591a\u5b66\u79d1\u5408\u4f5c\uff0c\u4ee5\u89e3\u51b3AI\u7b97\u6cd5\u5bf9\u6587\u5316\u8fb9\u7f18\u5316\u7fa4\u4f53\u7684\u4e0d\u5e73\u7b49\u5f71\u54cd\u3002"}}
{"id": "2508.00028", "categories": ["cs.NI", "cs.AI", "cs.CL", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.00028", "abs": "https://arxiv.org/abs/2508.00028", "authors": ["Abir Ray"], "title": "Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models", "comment": "12 pages", "summary": "Spectrum resources are often underutilized across time and space, motivating\ndynamic spectrum access strategies that allow secondary users to exploit unused\nfrequencies. A key challenge is predicting when and where spectrum will be\navailable (i.e., unused by primary licensed users) in order to enable proactive\nand interference-free access. This paper proposes a scalable framework for\nspectrum availability prediction that combines a two-state Markov chain model\nof primary user activity with high-fidelity propagation models from the ITU-R\n(specifically Recommendations P.528 and P.2108). The Markov chain captures\ntemporal occupancy patterns, while the propagation models incorporate path loss\nand clutter effects to determine if primary signals exceed interference\nthresholds at secondary user locations. By integrating these components, the\nproposed method can predict spectrum opportunities both in time and space with\nimproved accuracy. We develop the system model and algorithm for the approach,\nanalyze its scalability and computational efficiency, and discuss assumptions,\nlimitations, and potential applications. The framework is flexible and can be\nadapted to various frequency bands and scenarios. The results and analysis show\nthat the proposed approach can effectively identify available spectrum with low\ncomputational cost, making it suitable for real-time spectrum management in\ncognitive radio networks and other dynamic spectrum sharing systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9a6c\u5c14\u53ef\u592b\u94fe\u548c\u4f20\u64ad\u6a21\u578b\u7684\u9891\u8c31\u9884\u6d4b\u6846\u67b6\uff0c\u80fd\u9ad8\u6548\u8bc6\u522b\u53ef\u7528\u9891\u8c31\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u9891\u8c31\u7ba1\u7406\u3002", "motivation": "\u9891\u8c31\u8d44\u6e90\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u4e0a\u7ecf\u5e38\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u52a8\u6001\u9891\u8c31\u8bbf\u95ee\u7b56\u7565\u5141\u8bb8\u6b21\u8981\u7528\u6237\u5229\u7528\u672a\u4f7f\u7528\u7684\u9891\u7387\uff0c\u4f46\u5173\u952e\u6311\u6218\u662f\u9884\u6d4b\u9891\u8c31\u4f55\u65f6\u4f55\u5730\u53ef\u7528\u4ee5\u907f\u514d\u5e72\u6270\u3002", "method": "\u7ed3\u5408\u4e86\u4e24\u72b6\u6001\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u548cITU-R\u7684\u9ad8\u4fdd\u771f\u4f20\u64ad\u6a21\u578b\uff08\u5efa\u8baeP.528\u548cP.2108\uff09\uff0c\u9884\u6d4b\u9891\u8c31\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u4e0a\u7684\u53ef\u7528\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u9891\u8c31\u673a\u4f1a\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9891\u6bb5\u548c\u573a\u666f\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc6\u522b\u53ef\u7528\u9891\u8c31\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u9002\u7528\u4e8e\u8ba4\u77e5\u65e0\u7ebf\u7535\u7f51\u7edc\u548c\u5176\u4ed6\u52a8\u6001\u9891\u8c31\u5171\u4eab\u7cfb\u7edf\u7684\u5b9e\u65f6\u9891\u8c31\u7ba1\u7406\u3002"}}
{"id": "2508.00467", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00467", "abs": "https://arxiv.org/abs/2508.00467", "authors": ["Samratul Fuady", "Danesh Tarapore", "Mohammad D. Soorati"], "title": "SubCDM: Collective Decision-Making with a Swarm Subset", "comment": "6 pages, 7 figures. This paper has been accepted for presentation at\n  the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2025)", "summary": "Collective decision-making is a key function of autonomous robot swarms,\nenabling them to reach a consensus on actions based on environmental features.\nExisting strategies require the participation of all robots in the\ndecision-making process, which is resource-intensive and prevents the swarm\nfrom allocating the robots to any other tasks. We propose Subset-Based\nCollective Decision-Making (SubCDM), which enables decisions using only a swarm\nsubset. The construction of the subset is dynamic and decentralized, relying\nsolely on local information. Our method allows the swarm to adaptively\ndetermine the size of the subset for accurate decision-making, depending on the\ndifficulty of reaching a consensus. Simulation results using one hundred robots\nshow that our approach achieves accuracy comparable to using the entire swarm\nwhile reducing the number of robots required to perform collective\ndecision-making, making it a resource-efficient solution for collective\ndecision-making in swarm robotics.", "AI": {"tldr": "SubCDM\u662f\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u96c6\u4f53\u51b3\u7b56\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5b50\u96c6\u51cf\u5c11\u53c2\u4e0e\u673a\u5668\u4eba\u6570\uff0c\u4fdd\u6301\u51b3\u7b56\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u96c6\u4f53\u51b3\u7b56\u7b56\u7565\u9700\u8981\u6240\u6709\u673a\u5668\u4eba\u53c2\u4e0e\uff0c\u8d44\u6e90\u5bc6\u96c6\u4e14\u9650\u5236\u4e86\u7fa4\u4f53\u6267\u884c\u5176\u4ed6\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faSubset-Based Collective Decision-Making (SubCDM)\uff0c\u901a\u8fc7\u52a8\u6001\u548c\u5206\u6563\u7684\u5b50\u96c6\u6784\u5efa\uff0c\u4ec5\u4f9d\u8d56\u5c40\u90e8\u4fe1\u606f\uff0c\u81ea\u9002\u5e94\u786e\u5b9a\u5b50\u96c6\u5927\u5c0f\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u51b3\u7b56\u3002", "result": "\u4f7f\u7528\u4e00\u767e\u4e2a\u673a\u5668\u4eba\u7684\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0cSubCDM\u5728\u4fdd\u6301\u51b3\u7b56\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u4e0e\u51b3\u7b56\u7684\u673a\u5668\u4eba\u6570\u91cf\u3002", "conclusion": "SubCDM\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u548c\u5206\u6563\u7684\u5b50\u96c6\u6784\u5efa\uff0c\u5b9e\u73b0\u4e86\u8d44\u6e90\u9ad8\u6548\u7684\u96c6\u4f53\u51b3\u7b56\uff0c\u5176\u51c6\u786e\u6027\u4e0e\u4f7f\u7528\u6574\u4e2a\u7fa4\u4f53\u76f8\u5f53\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53c2\u4e0e\u51b3\u7b56\u7684\u673a\u5668\u4eba\u6570\u91cf\u3002"}}
{"id": "2508.00244", "categories": ["cs.SE", "cs.PL", "D.3.2; D.2.11; D.2.13"], "pdf": "https://arxiv.org/pdf/2508.00244", "abs": "https://arxiv.org/abs/2508.00244", "authors": ["Briza Mel Dias de Sousa", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems", "comment": "11 pages, 16 figures (1 table, 3 diagrams, 5 graphics, 7 listings),\n  submitted to CTICQS capstone project competition at SBQS 2025", "summary": "After decades of dominance by object-oriented programming (OOP), functional\nprogramming (FP) is gaining increasing attention in the software industry. This\nstudy compares the impact of OOP and FP on the architectural characteristics of\nsoftware systems. For that, it examines the design and implementation of a\nDigital Wallet system, developed in Kotlin (representing OOP) and Scala\n(representing FP). The comparison is made through both qualitative and\nquantitative analyses to explore how each paradigm influences the system's\narchitectural characteristics. The self-ethnographic qualitative analysis\nprovides a side-by-side comparison of both implementations, revealing the\nperspective of those writing such code. The survey-based quantitative analysis\ngathers feedback from developers with diverse backgrounds, showing their\nimpressions of those reading this code. Hopefully, these results may be useful\nfor developers or organizations seeking to make more informed decisions about\nwhich paradigm is best suited for their next project.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86OOP\u548cFP\u5bf9\u8f6f\u4ef6\u67b6\u6784\u7684\u5f71\u54cd\uff0c\u901a\u8fc7Kotlin\u548cScala\u5b9e\u73b0\u6570\u5b57\u94b1\u5305\u7cfb\u7edf\uff0c\u7ed3\u5408\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u8303\u5f0f\u9009\u62e9\u53c2\u8003\u3002", "motivation": "\u968f\u7740\u51fd\u6570\u5f0f\u7f16\u7a0b\u5728\u8f6f\u4ef6\u884c\u4e1a\u4e2d\u7684\u5173\u6ce8\u5ea6\u589e\u52a0\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83OOP\u548cFP\u5bf9\u8f6f\u4ef6\u7cfb\u7edf\u67b6\u6784\u7279\u6027\u7684\u5f71\u54cd\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u548c\u7ec4\u7ec7\u505a\u51fa\u66f4\u660e\u667a\u7684\u8303\u5f0f\u9009\u62e9\u3002", "method": "\u901a\u8fc7\u5b9a\u6027\uff08\u81ea\u6c11\u65cf\u5fd7\u5206\u6790\uff09\u548c\u5b9a\u91cf\uff08\u57fa\u4e8e\u8c03\u67e5\u7684\u5206\u6790\uff09\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86Kotlin\uff08OOP\uff09\u548cScala\uff08FP\uff09\u5b9e\u73b0\u7684\u6570\u5b57\u94b1\u5305\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u5b9e\u73b0\u3002", "result": "\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u5f00\u53d1\u8005\u7f16\u5199\u4ee3\u7801\u65f6\u7684\u89c6\u89d2\uff0c\u5b9a\u91cf\u5206\u6790\u5219\u5c55\u793a\u4e86\u4e0d\u540c\u80cc\u666f\u5f00\u53d1\u8005\u5bf9\u4ee3\u7801\u7684\u9605\u8bfb\u5370\u8c61\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u51fd\u6570\u5f0f\u7f16\u7a0b\uff08FP\uff09\u548c\u9762\u5411\u5bf9\u8c61\u7f16\u7a0b\uff08OOP\uff09\u5728\u8f6f\u4ef6\u7cfb\u7edf\u67b6\u6784\u7279\u6027\u4e0a\u5404\u6709\u4f18\u52a3\uff0c\u5177\u4f53\u9009\u62e9\u53d6\u51b3\u4e8e\u9879\u76ee\u9700\u6c42\u548c\u5f00\u53d1\u8005\u80cc\u666f\u3002"}}
{"id": "2508.00169", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00169", "abs": "https://arxiv.org/abs/2508.00169", "authors": ["Bhavya Goyal", "Felipe Gutierrez-Barragan", "Wei Lin", "Andreas Velten", "Yin Li", "Mohit Gupta"], "title": "Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs", "comment": "ICCV 2025", "summary": "LiDAR-based 3D sensors provide point clouds, a canonical 3D representation\nused in various scene understanding tasks. Modern LiDARs face key challenges in\nseveral real-world scenarios, such as long-distance or low-albedo objects,\nproducing sparse or erroneous point clouds. These errors, which are rooted in\nthe noisy raw LiDAR measurements, get propagated to downstream perception\nmodels, resulting in potentially severe loss of accuracy. This is because\nconventional 3D processing pipelines do not retain any uncertainty information\nfrom the raw measurements when constructing point clouds.\n  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation\nwhere each point is augmented with a probability attribute that encapsulates\nthe measurement uncertainty (or confidence) in the raw data. We further\nintroduce inference approaches that leverage PPC for robust 3D object\ndetection; these methods are versatile and can be used as computationally\nlightweight drop-in modules in 3D inference pipelines. We demonstrate, via both\nsimulations and real captures, that PPC-based 3D inference methods outperform\nseveral baselines using LiDAR as well as camera-LiDAR fusion models, across\nchallenging indoor and outdoor scenarios involving small, distant, and\nlow-albedo objects, as well as strong ambient light.\n  Our project webpage is at https://bhavyagoyal.github.io/ppc .", "AI": {"tldr": "PPC\u901a\u8fc7\u4e3a\u70b9\u4e91\u6dfb\u52a0\u6982\u7387\u5c5e\u6027\u5c01\u88c5\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u53473D\u7269\u4f53\u68c0\u6d4b\u9c81\u68d2\u6027\uff0c\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3LiDAR\u5728\u8fdc\u8ddd\u79bb\u6216\u4f4e\u53cd\u5c04\u7387\u7269\u4f53\u7b49\u73b0\u5b9e\u573a\u666f\u4e2d\u4ea7\u751f\u7a00\u758f\u6216\u9519\u8bef\u7684\u70b9\u4e91\uff0c\u8fd9\u4e9b\u8bef\u5dee\u4f1a\u4f20\u64ad\u5230\u4e0b\u6e38\u611f\u77e5\u6a21\u578b\uff0c\u5bfc\u81f4\u51c6\u786e\u6027\u4e25\u91cd\u4e0b\u964d\u3002\u4f20\u7edf3D\u5904\u7406\u6d41\u7a0b\u5728\u6784\u5efa\u70b9\u4e91\u65f6\u672a\u4fdd\u7559\u539f\u59cb\u6d4b\u91cf\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u3002", "method": "\u63d0\u51faPPC\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u4e3a3D\u7269\u4f53\u68c0\u6d4b\u5f15\u5165\u63a8\u7406\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u8ba1\u7b97\u8f7b\u91cf\u7ea7\u7684\u6a21\u5757\u96c6\u6210\u5230\u73b0\u67093D\u63a8\u7406\u6d41\u7a0b\u4e2d\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u6355\u83b7\u6570\u636e\u9a8c\u8bc1\uff0cPPC-based\u76843D\u63a8\u7406\u65b9\u6cd5\u5728\u5ba4\u5185\u5916\u6311\u6218\u6027\u573a\u666f\uff08\u5982\u5c0f\u3001\u8fdc\u3001\u4f4e\u53cd\u5c04\u7387\u7269\u4f53\u53ca\u5f3a\u73af\u5883\u5149\uff09\u4e2d\u4f18\u4e8eLiDAR\u53ca\u76f8\u673a-LiDAR\u878d\u5408\u6a21\u578b\u7684\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PPC\uff08\u6982\u7387\u70b9\u4e91\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u578b3D\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u70b9\u6dfb\u52a0\u6982\u7387\u5c5e\u6027\u6765\u5c01\u88c5\u539f\u59cb\u6570\u636e\u4e2d\u7684\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u7269\u4f53\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.00143", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.00143", "abs": "https://arxiv.org/abs/2508.00143", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Kenneth R. Koedinger"], "title": "Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation", "comment": "Accepted for presentation at NCME AIME-Con 2025", "summary": "Humans can be notoriously imperfect evaluators. They are often biased,\nunreliable, and unfit to define \"ground truth.\" Yet, given the surging need to\nproduce large amounts of training data in educational applications using AI,\ntraditional inter-rater reliability (IRR) metrics like Cohen's kappa remain\ncentral to validating labeled data. IRR remains a cornerstone of many machine\nlearning pipelines for educational data. Take, for example, the classification\nof tutors' moves in dialogues or labeling open responses in machine-graded\nassessments. This position paper argues that overreliance on human IRR as a\ngatekeeper for annotation quality hampers progress in classifying data in ways\nthat are valid and predictive in relation to improving learning. To address\nthis issue, we highlight five examples of complementary evaluation methods,\nsuch as multi-label annotation schemes, expert-based approaches, and\nclose-the-loop validity. We argue that these approaches are in a better\nposition to produce training data and subsequent models that produce improved\nstudent learning and more actionable insights than IRR approaches alone. We\nalso emphasize the importance of external validity, for example, by\nestablishing a procedure of validating tutor moves and demonstrating that it\nworks across many categories of tutor actions (e.g., providing hints). We call\non the field to rethink annotation quality and ground truth--prioritizing\nvalidity and educational impact over consensus alone.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u4eba\u7c7b\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e94\u79cd\u66ff\u4ee3\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6807\u6ce8\u8d28\u91cf\uff0c\u5f3a\u8c03\u6559\u80b2\u5f71\u54cd\u800c\u975e\u5355\u7eaf\u5171\u8bc6\u3002", "motivation": "\u7531\u4e8e\u4eba\u7c7b\u8bc4\u4f30\u8005\u5b58\u5728\u504f\u89c1\u548c\u4e0d\u53ef\u9760\u6027\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u4eba\u7c7b\u8bc4\u4f30\u8005\u95f4\u7684\u4fe1\u5ea6\uff08IRR\uff09\u4f5c\u4e3a\u6807\u6ce8\u8d28\u91cf\u7684\u5b88\u95e8\u5458\uff0c\u963b\u788d\u4e86\u5728\u6559\u80b2\u6570\u636e\u5206\u7c7b\u4e2d\u7684\u8fdb\u6b65\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e94\u79cd\u8865\u5145\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5982\u591a\u6807\u7b7e\u6807\u6ce8\u65b9\u6848\u3001\u57fa\u4e8e\u4e13\u5bb6\u7684\u65b9\u6cd5\u548c\u95ed\u73af\u6709\u6548\u6027\u9a8c\u8bc1\u3002", "result": "\u8bba\u6587\u5f3a\u8c03\uff0c\u8fd9\u4e9b\u8865\u5145\u65b9\u6cd5\u80fd\u6bd4\u5355\u72ec\u4f7f\u7528IRR\u65b9\u6cd5\u4ea7\u751f\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\uff0c\u4ece\u800c\u6539\u5584\u5b66\u751f\u5b66\u4e60\u548c\u63d0\u4f9b\u66f4\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u8bba\u6587\u547c\u5401\u91cd\u65b0\u601d\u8003\u6807\u6ce8\u8d28\u91cf\u548c\u2018\u771f\u5b9e\u6807\u51c6\u2019\uff0c\u5f3a\u8c03\u5e94\u4f18\u5148\u8003\u8651\u6709\u6548\u6027\u548c\u6559\u80b2\u5f71\u54cd\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5171\u8bc6\u3002"}}
{"id": "2508.00042", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00042", "abs": "https://arxiv.org/abs/2508.00042", "authors": ["Athanasios Tziouvaras", "Carolina Fortuna", "George Floros", "Kostas Kolomvatsos", "Panagiotis Sarigiannidis", "Marko Grobelnik", "Bla\u017e Bertalani\u010d"], "title": "Towards Reliable AI in 6G: Detecting Concept Drift in Wireless Network", "comment": "10 pages, 12 figures", "summary": "AI-native 6G networks promise unprecedented automation and performance by\nembedding machine-learning models throughout the radio access and core segments\nof the network. However, the non-stationary nature of wireless environments due\nto infrastructure changes, user mobility, and emerging traffic patterns,\ninduces concept drifts that can quickly degrade these model accuracies.\nExisting methods in general are very domain specific, or struggle with certain\ntype of concept drift. In this paper, we introduce two unsupervised,\nmodel-agnostic, batch concept drift detectors. Both methods compute an\nexpected-utility score to decide when concept drift occurred and if model\nretraining is warranted, without requiring ground-truth labels after\ndeployment. We validate our framework on two real-world wireless use cases in\noutdoor fingerprinting for localization and for link-anomaly detection, and\ndemonstrate that both methods are outperforming classical detectors such as\nADWIN, DDM, CUSUM by 20-40 percentage points. Additionally, they achieve an\nF1-score of 0.94 and 1.00 in correctly triggering retraining alarm, thus\nreducing the false alarm rate by up to 20 percentage points compared to the\nbest classical detectors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65e0\u76d1\u7763\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u5668\uff0c\u663e\u8457\u63d0\u53476G\u7f51\u7edc\u4e2d\u6a21\u578b\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "motivation": "AI\u539f\u751f6G\u7f51\u7edc\u4e2d\uff0c\u65e0\u7ebf\u73af\u5883\u7684\u975e\u5e73\u7a33\u6027\u5bfc\u81f4\u6982\u5ff5\u6f02\u79fb\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8fc7\u4e8e\u7279\u5b9a\u9886\u57df\uff0c\u8981\u4e48\u96be\u4ee5\u5e94\u5bf9\u67d0\u4e9b\u7c7b\u578b\u7684\u6982\u5ff5\u6f02\u79fb\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e24\u79cd\u65e0\u76d1\u7763\u3001\u6a21\u578b\u65e0\u5173\u7684\u6279\u91cf\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u8ba1\u7b97\u9884\u671f\u6548\u7528\u5206\u6570\u6765\u51b3\u5b9a\u662f\u5426\u9700\u8981\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728\u5ba4\u5916\u6307\u7eb9\u5b9a\u4f4d\u548c\u94fe\u8def\u5f02\u5e38\u68c0\u6d4b\u4e24\u4e2a\u771f\u5b9e\u65e0\u7ebf\u7528\u4f8b\u4e2d\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u68c0\u6d4b\u566820-40\u4e2a\u767e\u5206\u70b9\uff0cF1\u5206\u6570\u8fbe0.94\u548c1.00\uff0c\u8bef\u62a5\u7387\u964d\u4f4e\u6700\u591a20\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4e24\u79cd\u65e0\u76d1\u7763\u3001\u6a21\u578b\u65e0\u5173\u7684\u6279\u91cf\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u65e0\u7ebf\u7528\u4f8b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u68c0\u6d4b\u5668\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u8bef\u62a5\u7387\u3002"}}
{"id": "2508.00491", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00491", "abs": "https://arxiv.org/abs/2508.00491", "authors": ["Carlo Alessi", "Federico Vasile", "Federico Ceola", "Giulia Pasquale", "Nicol\u00f2 Boccardo", "Lorenzo Natale"], "title": "HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning", "comment": "Paper accepted at IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "summary": "Recent advancements in control of prosthetic hands have focused on increasing\nautonomy through the use of cameras and other sensory inputs. These systems aim\nto reduce the cognitive load on the user by automatically controlling certain\ndegrees of freedom. In robotics, imitation learning has emerged as a promising\napproach for learning grasping and complex manipulation tasks while simplifying\ndata collection. Its application to the control of prosthetic hands remains,\nhowever, largely unexplored. Bridging this gap could enhance dexterity\nrestoration and enable prosthetic devices to operate in more unconstrained\nscenarios, where tasks are learned from demonstrations rather than relying on\nmanually annotated sequences. To this end, we present HannesImitationPolicy, an\nimitation learning-based method to control the Hannes prosthetic hand, enabling\nobject grasping in unstructured environments. Moreover, we introduce the\nHannesImitationDataset comprising grasping demonstrations in table, shelf, and\nhuman-to-prosthesis handover scenarios. We leverage such data to train a single\ndiffusion policy and deploy it on the prosthetic hand to predict the wrist\norientation and hand closure for grasping. Experimental evaluation demonstrates\nsuccessful grasps across diverse objects and conditions. Finally, we show that\nthe policy outperforms a segmentation-based visual servo controller in\nunstructured scenarios. Additional material is provided on our project page:\nhttps://hsp-iit.github.io/HannesImitation", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u5047\u80a2\u624b\u63a7\u5236\u65b9\u6cd5HannesImitationPolicy\uff0c\u901a\u8fc7\u8bad\u7ec3\u6269\u6563\u7b56\u7565\u5b9e\u73b0\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u7269\u4f53\u6293\u53d6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u673a\u5668\u4eba\u6293\u53d6\u548c\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u5047\u80a2\u624b\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u53ef\u4ee5\u63d0\u5347\u5047\u80a2\u624b\u7684\u7075\u6d3b\u6027\uff0c\u4f7f\u5176\u5728\u66f4\u591a\u65e0\u7ea6\u675f\u573a\u666f\u4e2d\u901a\u8fc7\u6f14\u793a\u5b66\u4e60\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86HannesImitationPolicy\uff0c\u4e00\u79cd\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a7\u5236Hannes\u5047\u80a2\u624b\u3002\u8be5\u65b9\u6cd5\u5229\u7528HannesImitationDataset\u4e2d\u7684\u6570\u636e\u8bad\u7ec3\u5355\u4e2a\u6269\u6563\u7b56\u7565\uff0c\u9884\u6d4b\u624b\u8155\u65b9\u5411\u548c\u624b\u90e8\u95ed\u5408\u4ee5\u5b9e\u73b0\u6293\u53d6\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7269\u4f53\u548c\u6761\u4ef6\u4e0b\u5747\u80fd\u6210\u529f\u6293\u53d6\uff0c\u5e76\u4e14\u5728\u975e\u7ed3\u6784\u5316\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u5206\u5272\u7684\u89c6\u89c9\u4f3a\u670d\u63a7\u5236\u5668\u3002", "conclusion": "HannesImitationPolicy\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u6210\u529f\u63d0\u5347\u4e86\u5047\u80a2\u624b\u7684\u63a7\u5236\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u80fd\u591f\u5b9e\u73b0\u7269\u4f53\u6293\u53d6\uff0c\u5e76\u4e14\u5728\u975e\u7ed3\u6784\u5316\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u5206\u5272\u7684\u89c6\u89c9\u4f3a\u670d\u63a7\u5236\u5668\u3002"}}
{"id": "2508.00253", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00253", "abs": "https://arxiv.org/abs/2508.00253", "authors": ["Moumita Asad", "Rafed Muhammad Yasir", "Armin Geramirad", "Sam Malek"], "title": "Leveraging Large Language Model for Information Retrieval-based Bug Localization", "comment": null, "summary": "Information Retrieval-based Bug Localization aims to identify buggy source\nfiles for a given bug report. While existing approaches -- ranging from vector\nspace models to deep learning models -- have shown potential in this domain,\ntheir effectiveness is often limited by the vocabulary mismatch between bug\nreports and source code. To address this issue, we propose a novel Large\nLanguage Model (LLM) based bug localization approach, called GenLoc. Given a\nbug report, GenLoc leverages an LLM equipped with code-exploration functions to\niteratively analyze the code base and identify potential buggy files. To gather\nbetter context, GenLoc may optionally retrieve semantically relevant files\nusing vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug\nreports from six large-scale Java projects. Experimental results show that\nGenLoc outperforms five state-of-the-art bug localization techniques across\nmultiple metrics, achieving an average improvement of more than 60\\% in\nAccuracy@1.", "AI": {"tldr": "GenLoc\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684bug\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u5206\u6790\u4ee3\u7801\u548c\u53ef\u9009\u8bed\u4e49\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u4ece\u5411\u91cf\u7a7a\u95f4\u6a21\u578b\u5230\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff09\u5728bug\u5b9a\u4f4d\u4e2d\u7684\u6548\u679c\u53d7\u9650\u4e8ebug\u62a5\u544a\u4e0e\u6e90\u4ee3\u7801\u4e4b\u95f4\u7684\u8bcd\u6c47\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGenLoc\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u548c\u4ee3\u7801\u63a2\u7d22\u529f\u80fd\u8fed\u4ee3\u5206\u6790\u4ee3\u7801\u5e93\uff0c\u5e76\u53ef\u9009\u62e9\u6027\u5730\u4f7f\u7528\u5411\u91cf\u5d4c\u5165\u68c0\u7d22\u8bed\u4e49\u76f8\u5173\u6587\u4ef6\u4ee5\u83b7\u53d6\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u516d\u4e2a\u5927\u578bJava\u9879\u76ee\u76849000\u591a\u4e2a\u771f\u5b9ebug\u62a5\u544a\u4e0a\u8bc4\u4f30\uff0cGenLoc\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u4e94\u79cd\u6700\u5148\u8fdb\u7684bug\u5b9a\u4f4d\u6280\u672f\u3002", "conclusion": "GenLoc\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u4ee3\u7801\u63a2\u7d22\u529f\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22\u7684bug\u5b9a\u4f4d\u6548\u679c\uff0c\u5e73\u5747\u5728Accuracy@1\u6307\u6807\u4e0a\u63d0\u5347\u4e8660%\u4ee5\u4e0a\u3002"}}
{"id": "2508.00171", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00171", "abs": "https://arxiv.org/abs/2508.00171", "authors": ["David Restrepo", "Ira Ktena", "Maria Vakalopoulou", "Stergios Christodoulidis", "Enzo Ferrante"], "title": "On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI", "comment": "Accepted to MICCAI 2025 1st Workshop on Multimodal Large Language\n  Models (MLLMs) in Clinical Practice", "summary": "Clinical decision-making relies on the integrated analysis of medical images\nand the associated clinical reports. While Vision-Language Models (VLMs) can\noffer a unified framework for such tasks, they can exhibit strong biases toward\none modality, frequently overlooking critical visual cues in favor of textual\ninformation. In this work, we introduce Selective Modality Shifting (SMS), a\nperturbation-based approach to quantify a model's reliance on each modality in\nbinary classification tasks. By systematically swapping images or text between\nsamples with opposing labels, we expose modality-specific biases. We assess six\nopen-source VLMs-four generalist models and two fine-tuned for medical data-on\ntwo medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)\nand FairVLMed (scanning laser ophthalmoscopy). By assessing model performance\nand the calibration of every model in both unperturbed and perturbed settings,\nwe reveal a marked dependency on text input, which persists despite the\npresence of complementary visual information. We also perform a qualitative\nattention-based analysis which further confirms that image content is often\novershadowed by text details. Our findings highlight the importance of\ndesigning and evaluating multimodal medical models that genuinely integrate\nvisual and textual cues, rather than relying on single-modality signals.", "AI": {"tldr": "\u63d0\u51fa\u9009\u62e9\u6027\u6a21\u6001\u8f6c\u79fb\uff08SMS\uff09\u65b9\u6cd5\uff0c\u63ed\u793a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u4efb\u52a1\u4e2d\u5bf9\u6587\u672c\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u5f3a\u8c03\u9700\u771f\u6b63\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\u53ef\u80fd\u8868\u73b0\u51fa\u5bf9\u67d0\u4e00\u6a21\u6001\u7684\u5f3a\u70c8\u504f\u89c1\uff0c\u5e38\u5ffd\u7565\u5173\u952e\u7684\u89c6\u89c9\u7ebf\u7d22\u800c\u504f\u5411\u6587\u672c\u4fe1\u606f\u3002", "method": "\u5f15\u5165\u9009\u62e9\u6027\u6a21\u6001\u8f6c\u79fb\uff08SMS\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u6270\u52a8\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u5730\u5728\u6837\u672c\u95f4\u4ea4\u6362\u56fe\u50cf\u6216\u6587\u672c\u6765\u63ed\u793a\u6a21\u6001\u7279\u5b9a\u504f\u5dee\u3002", "result": "\u8bc4\u4f30\u4e86\u516d\u4e2a\u5f00\u6e90VLM\u6a21\u578b\uff0c\u53d1\u73b0\u5c3d\u7ba1\u5b58\u5728\u4e92\u8865\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u6a21\u578b\u4ecd\u663e\u8457\u4f9d\u8d56\u6587\u672c\u8f93\u5165\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u548c\u8bc4\u4f30\u771f\u6b63\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\u7684\u591a\u6a21\u6001\u533b\u5b66\u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u5355\u4e00\u6a21\u6001\u4fe1\u53f7\u3002"}}
{"id": "2508.00159", "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.TH", "math.OC", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2508.00159", "abs": "https://arxiv.org/abs/2508.00159", "authors": ["Jobst Heitzig", "Ram Potham"], "title": "Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power", "comment": null, "summary": "Power is a key concept in AI safety: power-seeking as an instrumental goal,\nsudden or gradual disempowerment of humans, power balance in human-AI\ninteraction and international AI governance. At the same time, power as the\nability to pursue diverse goals is essential for wellbeing.\n  This paper explores the idea of promoting both safety and wellbeing by\nforcing AI agents explicitly to empower humans and to manage the power balance\nbetween humans and AI agents in a desirable way. Using a principled, partially\naxiomatic approach, we design a parametrizable and decomposable objective\nfunction that represents an inequality- and risk-averse long-term aggregate of\nhuman power. It takes into account humans' bounded rationality and social\nnorms, and, crucially, considers a wide variety of possible human goals.\n  We derive algorithms for computing that metric by backward induction or\napproximating it via a form of multi-agent reinforcement learning from a given\nworld model. We exemplify the consequences of (softly) maximizing this metric\nin a variety of paradigmatic situations and describe what instrumental\nsub-goals it will likely imply. Our cautious assessment is that softly\nmaximizing suitable aggregate metrics of human power might constitute a\nbeneficial objective for agentic AI systems that is safer than direct\nutility-based objectives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7ba1\u7406\u4eba\u7c7b\u4e0eAI\u7684\u6743\u529b\u5e73\u8861\u6765\u4fc3\u8fdb\u5b89\u5168\u548c\u798f\u7949\u7684\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8861\u91cf\u4eba\u7c7b\u6743\u529b\u7684\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u7b97\u6cd5\u8ba1\u7b97\u5176\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u901a\u8fc7\u660e\u786e\u8981\u6c42AI\u4ee3\u7406\u589e\u5f3a\u4eba\u7c7b\u6743\u529b\u5e76\u7ba1\u7406\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u4e4b\u95f4\u7684\u6743\u529b\u5e73\u8861\uff0c\u4ee5\u4fc3\u8fdb\u5b89\u5168\u548c\u798f\u7949\u3002", "method": "\u4f7f\u7528\u539f\u5219\u6027\u7684\u90e8\u5206\u516c\u7406\u5316\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u53c2\u6570\u5316\u548c\u53ef\u5206\u89e3\u7684\u5ba2\u89c2\u51fd\u6570\uff0c\u4ee3\u8868\u4e86\u4e00\u79cd\u4e0d\u5e73\u7b49\u548c\u98ce\u9669\u538c\u6076\u7684\u957f\u671f\u4eba\u7c7b\u6743\u529b\u805a\u5408\u3002\u901a\u8fc7\u9006\u5411\u5f52\u7eb3\u6216\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8fd1\u4f3c\u8ba1\u7b97\u8be5\u6307\u6807\u3002", "result": "\u5728\u4e0d\u540c\u5178\u578b\u60c5\u5883\u4e2d\u5c55\u793a\u4e86\uff08\u8f6f\u6027\uff09\u6700\u5927\u5316\u8be5\u6307\u6807\u7684\u540e\u679c\uff0c\u5e76\u63cf\u8ff0\u4e86\u5b83\u53ef\u80fd\u9690\u542b\u7684\u5de5\u5177\u6027\u5b50\u76ee\u6807\u3002", "conclusion": "\u8f6f\u6027\u6700\u5927\u5316\u9002\u5408\u7684\u4eba\u7c7b\u6743\u529b\u805a\u5408\u6307\u6807\u53ef\u80fd\u6784\u6210\u5bf9\u4ee3\u7406AI\u7cfb\u7edf\u6709\u76ca\u7684\u76ee\u6807\uff0c\u6bd4\u76f4\u63a5\u57fa\u4e8e\u6548\u7528\u7684\u76ee\u6807\u66f4\u5b89\u5168\u3002"}}
{"id": "2508.00228", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.00228", "abs": "https://arxiv.org/abs/2508.00228", "authors": ["Aashay Arora", "Diego Davila", "Frank W\u00fcrthwein", "John Graham", "Dima Mishin", "Justas Balcas", "Tom Lehman", "Xi Yang", "Chin Guok", "Harvey Newman"], "title": "Benchmarking XRootD-HTTPS on 400Gbps Links with Variable Latencies", "comment": "Submitted to CHEP 24", "summary": "In anticipation of the High Luminosity-LHC era, there is a critical need to\noversee software readiness for upcoming growth in network traffic for\nproduction and user data analysis access. This paper looks into software and\nhardware required improvements in US-CMS Tier-2 sites to be able to sustain and\nmeet the projected 400 Gbps bandwidth demands while tackling the challenge\nposed by varying latencies between sites. Specifically, our study focuses on\nidentifying the performance of XRootD HTTP third-party copies across multiple\n400 Gbps links and exploring different host and transfer configurations. Our\napproach involves systematic testing with variations in the number of origins\nper cluster and CPU allocations for each origin. By replicating real network\nconditions and creating network \"loops\" that traverse multiple switches across\nthe wide area network, we are able to replicate authentic network conditions", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86US-CMS Tier-2\u7ad9\u70b9\u5982\u4f55\u4f18\u5316\u8f6f\u4ef6\u548c\u786c\u4ef6\u4ee5\u652f\u6301400 Gbps\u5e26\u5bbd\u9700\u6c42\uff0c\u91cd\u70b9\u6d4b\u8bd5\u4e86XRootD HTTP\u7b2c\u4e09\u65b9\u62f7\u8d1d\u7684\u6027\u80fd\u3002", "motivation": "High Luminosity-LHC\u65f6\u4ee3\u9884\u8ba1\u5c06\u5e26\u6765\u7f51\u7edc\u6d41\u91cf\u7684\u663e\u8457\u589e\u957f\uff0c\u9700\u786e\u4fdd\u8f6f\u4ef6\u548c\u786c\u4ef6\u80fd\u591f\u652f\u6301\u751f\u4ea7\u548c\u7528\u6237\u6570\u636e\u5206\u6790\u8bbf\u95ee\u7684\u5e26\u5bbd\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6d4b\u8bd5\uff0c\u5305\u62ec\u8c03\u6574\u6bcf\u4e2a\u96c6\u7fa4\u7684\u8d77\u6e90\u70b9\u6570\u91cf\u548cCPU\u5206\u914d\uff0c\u6a21\u62df\u771f\u5b9e\u7f51\u7edc\u6761\u4ef6\uff08\u5982\u521b\u5efa\u8de8\u591a\u4e2a\u4ea4\u6362\u673a\u7684\u7f51\u7edc\u201c\u5faa\u73af\u201d\uff09\uff0c\u8bc4\u4f30XRootD HTTP\u7b2c\u4e09\u65b9\u62f7\u8d1d\u5728\u591a\u4e2a400 Gbps\u94fe\u63a5\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u4e86\u5728\u4e0d\u540c\u4e3b\u673a\u548c\u4f20\u8f93\u914d\u7f6e\u4e0bXRootD HTTP\u7b2c\u4e09\u65b9\u62f7\u8d1d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u4f18\u5316\u7f51\u7edc\u4f20\u8f93\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "conclusion": "\u4e3a\u5e94\u5bf9High Luminosity-LHC\u65f6\u4ee3\u7684\u9700\u6c42\uff0cUS-CMS Tier-2\u7ad9\u70b9\u9700\u5728\u8f6f\u4ef6\u548c\u786c\u4ef6\u4e0a\u8fdb\u884c\u6539\u8fdb\uff0c\u4ee5\u6ee1\u8db3400 Gbps\u5e26\u5bbd\u9700\u6c42\u5e76\u5e94\u5bf9\u4e0d\u540c\u7ad9\u70b9\u95f4\u5ef6\u8fdf\u7684\u6311\u6218\u3002"}}
{"id": "2508.00580", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00580", "abs": "https://arxiv.org/abs/2508.00580", "authors": ["Raul Castilla-Arquillo", "Carlos Perez-del-Pulgar", "Levin Gerdes", "Alfonso Garcia-Cerezo", "Miguel A. Olivares-Mendez"], "title": "OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery", "comment": null, "summary": "Robot navigation in unstructured environments requires multimodal perception\nsystems that can support safe navigation. Multimodality enables the integration\nof complementary information collected by different sensors. However, this\ninformation must be processed by machine learning algorithms specifically\ndesigned to leverage heterogeneous data. Furthermore, it is necessary to\nidentify which sensor modalities are most informative for navigation in the\ntarget environment. In Martian exploration, thermal imagery has proven valuable\nfor assessing terrain safety due to differences in thermal behaviour between\nsoil types. This work presents OmniUnet, a transformer-based neural network\narchitecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)\nimagery. A custom multimodal sensor housing was developed using 3D printing and\nmounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a\nmultimodal dataset in the Bardenas semi-desert in northern Spain. This location\nserves as a representative environment of the Martian surface, featuring\nterrain types such as sand, bedrock, and compact soil. A subset of this dataset\nwas manually labeled to support supervised training of the network. The model\nwas evaluated both quantitatively and qualitatively, achieving a pixel accuracy\nof 80.37% and demonstrating strong performance in segmenting complex\nunstructured terrain. Inference tests yielded an average prediction time of 673\nms on a resource-constrained computer (Jetson Orin Nano), confirming its\nsuitability for on-robot deployment. The software implementation of the network\nand the labeled dataset have been made publicly available to support future\nresearch in multimodal terrain perception for planetary robotics.", "AI": {"tldr": "OmniUnet\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684RGB-D-T\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7f51\u7edc\uff0c\u5728\u706b\u661f\u7c7b\u4f3c\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u5408\u673a\u5668\u4eba\u90e8\u7f72\u3002", "motivation": "\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5bfc\u822a\u9700\u8981\u591a\u6a21\u6001\u611f\u77e5\u7cfb\u7edf\uff0c\u800c\u706b\u661f\u63a2\u7d22\u4e2d\u70ed\u6210\u50cf\u5bf9\u5730\u5f62\u5b89\u5168\u8bc4\u4f30\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86OmniUnet\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8eRGB-D-T\u56fe\u50cf\u7684\u8bed\u4e49\u5206\u5272\uff0c\u5e76\u5229\u75283D\u6253\u5370\u5b9a\u5236\u591a\u6a21\u6001\u4f20\u611f\u5668\u5916\u58f3\u6536\u96c6\u6570\u636e\u3002", "result": "\u6a21\u578b\u5728\u50cf\u7d20\u51c6\u786e\u7387\u4e0a\u8fbe\u523080.37%\uff0c\u5728\u590d\u6742\u975e\u7ed3\u6784\u5316\u5730\u5f62\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u7406\u65f6\u95f4\u5e73\u5747\u4e3a673\u6beb\u79d2\u3002", "conclusion": "OmniUnet\u67b6\u6784\u5728RGB-D-T\u56fe\u50cf\u4e0a\u7684\u8bed\u4e49\u5206\u5272\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u90e8\u7f72\uff0c\u4e14\u5176\u8f6f\u4ef6\u5b9e\u73b0\u548c\u6807\u6ce8\u6570\u636e\u96c6\u5df2\u516c\u5f00\uff0c\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2508.00255", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00255", "abs": "https://arxiv.org/abs/2508.00255", "authors": ["Boqi Chen", "Ou Wei", "Bingzhou Zheng", "Gunter Mussbacher"], "title": "Accurate and Consistent Graph Model Generation from Text with Large Language Models", "comment": "Accepted at ACM / IEEE 28th International Conference on Model Driven\n  Engineering Languages and Systems (MODELS 2025)", "summary": "Graph model generation from natural language description is an important task\nwith many applications in software engineering. With the rise of large language\nmodels (LLMs), there is a growing interest in using LLMs for graph model\ngeneration. Nevertheless, LLM-based graph model generation typically produces\npartially correct models that suffer from three main issues: (1) syntax\nviolations: the generated model may not adhere to the syntax defined by its\nmetamodel, (2) constraint inconsistencies: the structure of the model might not\nconform to some domain-specific constraints, and (3) inaccuracy: due to the\ninherent uncertainty in LLMs, the models can include inaccurate, hallucinated\nelements. While the first issue is often addressed through techniques such as\nconstraint decoding or filtering, the latter two remain largely unaddressed.\nMotivated by recent self-consistency approaches in LLMs, we propose a novel\nabstraction-concretization framework that enhances the consistency and quality\nof generated graph models by considering multiple outputs from an LLM. Our\napproach first constructs a probabilistic partial model that aggregates all\ncandidate outputs and then refines this partial model into the most appropriate\nconcrete model that satisfies all constraints. We evaluate our framework on\nseveral popular open-source and closed-source LLMs using diverse datasets for\nmodel generation tasks. The results demonstrate that our approach significantly\nimproves both the consistency and quality of the generated graph models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u62bd\u8c61-\u5177\u4f53\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u5408\u548c\u7ec6\u5316LLM\u7684\u591a\u4e2a\u8f93\u51fa\u6765\u63d0\u9ad8\u56fe\u6a21\u578b\u751f\u6210\u7684\u4e00\u81f4\u6027\u548c\u8d28\u91cf\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u3002", "motivation": "\u89e3\u51b3LLM\u751f\u6210\u7684\u56fe\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u8bed\u6cd5\u8fdd\u89c4\u3001\u7ea6\u675f\u4e0d\u4e00\u81f4\u548c\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u6982\u7387\u6027\u90e8\u5206\u6a21\u578b\u6765\u805a\u5408\u6240\u6709\u5019\u9009\u8f93\u51fa\uff0c\u7136\u540e\u5c06\u5176\u7ec6\u5316\u4e3a\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u7684\u6700\u5408\u9002\u5177\u4f53\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90LLM\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u56fe\u6a21\u578b\u7684\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u62bd\u8c61-\u5177\u4f53\u5316\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86LLM\u751f\u6210\u7684\u56fe\u6a21\u578b\u7684\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u3002"}}
{"id": "2508.00197", "categories": ["cs.CV", "cs.LG", "cs.NA", "math.CT", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.00197", "abs": "https://arxiv.org/abs/2508.00197", "authors": ["Eric Mjolsness", "Cory B. Scott"], "title": "Graph Lineages and Skeletal Graph Products", "comment": "42 pages. 33 Figures. Under review", "summary": "Graphs, and sequences of growing graphs, can be used to specify the\narchitecture of mathematical models in many fields including machine learning\nand computational science. Here we define structured graph \"lineages\" (ordered\nby level number) that grow in a hierarchical fashion, so that: (1) the number\nof graph vertices and edges increases exponentially in level number; (2)\nbipartite graphs connect successive levels within a graph lineage and, as in\nmultigrid methods, can constrain matrices relating successive levels; (3) using\nprolongation maps within a graph lineage, process-derived distance measures\nbetween graphs at successive levels can be defined; (4) a category of \"graded\ngraphs\" can be defined, and using it low-cost \"skeletal\" variants of standard\nalgebraic graph operations and type constructors (cross product, box product,\ndisjoint sum, and function types) can be derived for graded graphs and hence\nhierarchical graph lineages; (5) these skeletal binary operators have similar\nbut not identical algebraic and category-theoretic properties to their standard\ncounterparts; (6) graph lineages and their skeletal product constructors can\napproach continuum limit objects. Additional space-efficient unary operators on\ngraded graphs are also derived: thickening, which creates a graph lineage of\nmultiscale graphs, and escalation to a graph lineage of search frontiers\n(useful as a generalization of adaptive grids and in defining \"skeletal\"\nfunctions). The result is an algebraic type theory for graded graphs and\n(hierarchical) graph lineages. The approach is expected to be well suited to\ndefining hierarchical model architectures - \"hierarchitectures\" - and local\nsampling, search, or optimization algorithms on them. We demonstrate such\napplication to deep neural networks (including visual and feature scale spaces)\nand to multigrid numerical methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u6b21\u5316\u589e\u957f\u7684\u56fe\u7ed3\u6784\u7406\u8bba\uff0c\u9002\u7528\u4e8e\u5b9a\u4e49\u6a21\u578b\u67b6\u6784\u548c\u7b97\u6cd5\uff0c\u5e76\u5728\u6df1\u5ea6\u5b66\u4e60\u548c\u591a\u7f51\u683c\u65b9\u6cd5\u4e2d\u8fdb\u884c\u4e86\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u5728\u673a\u5668\u5b66\u4e60\u548c\u8ba1\u7b97\u79d1\u5b66\u7b49\u9886\u57df\u4e2d\u6307\u5b9a\u6570\u5b66\u6a21\u578b\u7684\u67b6\u6784\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5c42\u6b21\u5316\u589e\u957f\u7684\u56fe\u7ed3\u6784\u3002", "method": "\u5b9a\u4e49\u4e86\u7ed3\u6784\u5316\u7684\u56fe'lineages'\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u589e\u957f\u65b9\u5f0f\uff0c\u7ed3\u5408\u53cc\u5206\u56fe\u548c\u5ef6\u957f\u6620\u5c04\uff0c\u63a8\u5bfc\u51fa\u4f4e\u6210\u672c\u7684'\u9aa8\u67b6'\u53d8\u4f53\u6807\u51c6\u4ee3\u6570\u56fe\u64cd\u4f5c\u548c\u7c7b\u578b\u6784\u9020\u5668\u3002", "result": "\u5f00\u53d1\u4e86\u7a7a\u95f4\u9ad8\u6548\u7684\u4e00\u5143\u64cd\u4f5c\u7b26\uff08\u5982\u589e\u539a\u548c\u5347\u7ea7\uff09\u548c\u9aa8\u67b6\u4e8c\u5143\u64cd\u4f5c\u7b26\uff0c\u9002\u7528\u4e8e\u5206\u7ea7\u56fe\u548c\u5c42\u6b21\u56fe\u8c31\u7cfb\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u7f51\u683c\u6570\u503c\u65b9\u6cd5\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee3\u6570\u7c7b\u578b\u7406\u8bba\uff0c\u7528\u4e8e\u5206\u7ea7\u56fe\u548c\u6709\u5c42\u6b21\u7ed3\u6784\u7684\u56fe\u8c31\u7cfb\uff0c\u9002\u7528\u4e8e\u5b9a\u4e49\u5c42\u6b21\u6a21\u578b\u67b6\u6784\uff08'hierarchitectures'\uff09\u53ca\u5176\u4e0a\u7684\u5c40\u90e8\u91c7\u6837\u3001\u641c\u7d22\u6216\u4f18\u5316\u7b97\u6cd5\u3002"}}
{"id": "2508.00222", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00222", "abs": "https://arxiv.org/abs/2508.00222", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its inherently on-policy strategy with LLM's immense\naction space and sparse reward. Further, RLVR can lead to the capability\nboundary collapse, narrowing the LLM's problem-solving scope. To address this\nproblem, we propose RL-PLUS, a novel approach that synergizes internal\nexploitation (i.e., Thinking) with external data (i.e., Learning) to achieve\nstronger reasoning capabilities and surpass the boundaries of base models.\nRL-PLUS integrates two core components: Multiple Importance Sampling to address\nfor distributional mismatch from external data, and an Exploration-Based\nAdvantage Function to guide the model towards high-value, unexplored reasoning\npaths. We provide both theoretical analysis and extensive experiments to\ndemonstrate the superiority and generalizability of our approach. The results\nshow that RL-PLUS achieves state-of-the-art performance compared with existing\nRLVR methods on six math reasoning benchmarks and exhibits superior performance\non six out-of-distribution reasoning tasks. It also achieves consistent and\nsignificant gains across diverse model families, with average relative\nimprovements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across\nmultiple benchmarks indicate that RL-PLUS effectively resolves the capability\nboundary collapse problem.", "AI": {"tldr": "RL-PLUS\u901a\u8fc7\u7ed3\u5408\u5185\u90e8\u601d\u8003\u548c\u5916\u90e8\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684RLVR\u65b9\u6cd5\u56e0\u56fa\u6709\u7684\u7b56\u7565\u9650\u5236\u548c\u7a00\u758f\u5956\u52b1\uff0c\u96be\u4ee5\u7a81\u7834\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u751a\u81f3\u5bfc\u81f4\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u3002", "method": "RL-PLUS\u6574\u5408\u4e86\u591a\u91cd\u91cd\u8981\u6027\u91c7\u6837\u548c\u57fa\u4e8e\u63a2\u7d22\u7684\u4f18\u52bf\u51fd\u6570\uff0c\u4ee5\u5904\u7406\u5916\u90e8\u6570\u636e\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5e76\u5f15\u5bfc\u6a21\u578b\u63a2\u7d22\u9ad8\u4ef7\u503c\u63a8\u7406\u8def\u5f84\u3002", "result": "RL-PLUS\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u5728\u516d\u4e2a\u5206\u5e03\u5916\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5e73\u5747\u76f8\u5bf9\u63d0\u534721.1%\u81f369.2%\u3002", "conclusion": "RL-PLUS\u901a\u8fc7\u7ed3\u5408\u5185\u90e8\u63a2\u7d22\u548c\u5916\u90e8\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u4e86\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u95ee\u9898\u3002"}}
{"id": "2508.00584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00584", "abs": "https://arxiv.org/abs/2508.00584", "authors": ["Konstantinos Plotas", "Emmanouil Papadakis", "Drosakis Drosakis", "Panos Trahanias", "Dimitrios Papageorgiou"], "title": "A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup", "comment": "Please find the citation info @ Zenodo, ArXiv or Zenodo, as the\n  proceedings of ICRA are no longer sent to IEEE Xplore", "summary": "In this work, a control scheme for human-robot collaborative object\ntransportation is proposed, considering a quadruped robot equipped with the\nMIGHTY suction cup that serves both as a gripper for holding the object and a\nforce/torque sensor. The proposed control scheme is based on the notion of\nadmittance control, and incorporates a variable damping term aiming towards\nincreasing the controllability of the human and, at the same time, decreasing\nher/his effort. Furthermore, to ensure that the object is not detached from the\nsuction cup during the collaboration, an additional control signal is proposed,\nwhich is based on a barrier artificial potential. The proposed control scheme\nis proven to be passive and its performance is demonstrated through\nexperimental evaluations conducted using the Unitree Go1 robot equipped with\nthe MIGHTY suction cup.", "AI": {"tldr": "A passive control scheme for human-robot object transport using admittance control and barrier potential ensures object security and reduces human effort, validated by experiments.", "motivation": "To improve human-robot collaboration in object transportation by increasing controllability and reducing human effort while ensuring the object remains securely attached to the robot.", "method": "The control scheme is based on admittance control and incorporates a variable damping term to enhance human controllability and reduce effort. An additional control signal based on a barrier artificial potential ensures the object remains attached to the suction cup.", "result": "Experimental evaluations confirm the scheme's effectiveness in maintaining object attachment and enhancing collaboration.", "conclusion": "The proposed control scheme for human-robot collaborative object transportation is proven to be passive and effective, as demonstrated by experimental evaluations using the Unitree Go1 robot equipped with the MIGHTY suction cup."}}
{"id": "2508.00408", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00408", "abs": "https://arxiv.org/abs/2508.00408", "authors": ["Dong Huang", "Jie M. Zhang", "Mark Harman", "Qianru Zhang", "Mingzhe Du", "See-Kiong Ng"], "title": "Benchmarking LLMs for Unit Test Generation from Real-World Functions", "comment": "Under Review", "summary": "Recently, large language models (LLMs) have shown great promise in automating\nunit test generation, significantly reducing the manual effort required by\ndevelopers. To effectively evaluate the capabilities of LLMs in this domain, it\nis crucial to have a well-designed benchmark that accurately reflects\nreal-world scenarios and mitigates common pitfalls. Existing LLM test\ngeneration benchmarks are limited by two critical drawbacks: data contamination\nand structurally simple function code. As a result, we often cannot rely on the\nvalidity of scientific conclusions drawn from empirical studies using these\nlimited benchmarks. The empirical evidence presented may be biased due to\ncontamination and may fail to generalize beyond toy programs due to structural\nsimplicity.\n  To address these problems, we introduce ULT (UnLeakedTestbench), a new\nbenchmark specifically designed for function-level unit test generation from\nreal-world Python functions. ULT is constructed through a multi-stage curation\nprocess that ensures high cyclomatic complexity and mitigates test case\ncontamination. With 3,909 carefully selected function-level tasks, ULT provides\na more realistic and challenging evaluation of LLMs' test generation\ncapabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT\nwith leaked tests designed to enable a controlled analysis of memorization\nversus reasoning in test generation. Our evaluation results demonstrate that\nULT is significantly more challenging. For example, test cases generated by\nLLMs only achieve 41.32\\%, 45.10\\%, 30.22\\%, and 40.21\\% for accuracy,\nstatement coverage, branch coverage, and mutation score on average for all\nLLMs, respectively. These results are substantially lower than the\ncorresponding metrics on TestEval (91.79\\%, 92.18\\%, 82.04\\%, and 49.69\\%) and\nPLT (47.07\\%, 55.13\\%, 40.07\\%, and 50.80\\%).", "AI": {"tldr": "ULT\u662f\u4e00\u4e2a\u9488\u5bf9LLM\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u7684\u65b0\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u6570\u636e\u6c61\u67d3\u548c\u7ed3\u6784\u7b80\u5355\u95ee\u9898\uff0c\u7ed3\u679c\u8868\u660e\u5176\u66f4\u5177\u6311\u6218\u6027\u3002", "motivation": "\u73b0\u6709LLM\u6d4b\u8bd5\u751f\u6210\u57fa\u51c6\u5b58\u5728\u6570\u636e\u6c61\u67d3\u548c\u7ed3\u6784\u7b80\u5355\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u79d1\u5b66\u7ed3\u8bba\u7684\u6709\u6548\u6027\u53d7\u9650\u3002", "method": "\u901a\u8fc7\u591a\u9636\u6bb5\u7b5b\u9009\u8fc7\u7a0b\u6784\u5efaULT\uff0c\u786e\u4fdd\u9ad8\u5708\u590d\u6742\u5ea6\u548c\u51cf\u5c11\u6d4b\u8bd5\u7528\u4f8b\u6c61\u67d3\uff0c\u540c\u65f6\u63d0\u4f9bPLT\u4f5c\u4e3a\u5bf9\u6bd4\u57fa\u51c6\u3002", "result": "LLMs\u5728ULT\u4e0a\u7684\u8868\u73b0\uff08\u51c6\u786e\u738741.32%\uff0c\u8bed\u53e5\u8986\u76d6\u738745.10%\u7b49\uff09\u663e\u8457\u4f4e\u4e8eTestEval\u548cPLT\uff0c\u8bc1\u660eULT\u66f4\u5177\u6311\u6218\u6027\u3002", "conclusion": "ULT\uff08UnLeakedTestbench\uff09\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30LLMs\u5728\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u9886\u57df\u7684\u6311\u6218\u6027\u548c\u771f\u5b9e\u6027\uff0c\u540c\u65f6\u901a\u8fc7PLT\uff08PreLeakedTestbench\uff09\u63d0\u4f9b\u4e86\u5bf9\u8bb0\u5fc6\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u63a7\u5236\u5206\u6790\u3002"}}
{"id": "2508.00205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00205", "abs": "https://arxiv.org/abs/2508.00205", "authors": ["Xiangyu Kong", "Hengde Zhu", "Haoqin Sun", "Zhihao Guo", "Jiayan Gu", "Xinyi Ni", "Wei Zhang", "Shizhe Liu", "Siyang Song"], "title": "Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition", "comment": "10 pages, 4 figures", "summary": "Automatic real personality recognition (RPR) aims to evaluate human real\npersonality traits from their expressive behaviours. However, most existing\nsolutions generally act as external observers to infer observers' personality\nimpressions based on target individuals' expressive behaviours, which\nsignificantly deviate from their real personalities and consistently lead to\ninferior recognition performance. Inspired by the association between real\npersonality and human internal cognition underlying the generation of\nexpressive behaviours, we propose a novel RPR approach that efficiently\nsimulates personalised internal cognition from easy-accessible external short\naudio-visual behaviours expressed by the target individual. The simulated\npersonalised cognition, represented as a set of network weights that enforce\nthe personalised network to reproduce the individual-specific facial reactions,\nis further encoded as a novel graph containing two-dimensional node and edge\nfeature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for\ninferring real personality traits from it. To simulate real personality-related\ncognition, an end-to-end strategy is designed to jointly train our cognition\nsimulation, 2D graph construction, and personality recognition modules.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2D-GNN\u7684\u81ea\u52a8\u771f\u5b9e\u4eba\u683c\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4e2a\u6027\u5316\u5185\u90e8\u8ba4\u77e5\u6765\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u683c\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4f5c\u4e3a\u5916\u90e8\u89c2\u5bdf\u8005\uff0c\u57fa\u4e8e\u76ee\u6807\u4e2a\u4f53\u7684\u8868\u8fbe\u884c\u4e3a\u63a8\u65ad\u89c2\u5bdf\u8005\u7684\u4eba\u683c\u5370\u8c61\uff0c\u8fd9\u4e0e\u771f\u5b9e\u4eba\u683c\u5b58\u5728\u663e\u8457\u504f\u5dee\uff0c\u5bfc\u81f4\u8bc6\u522b\u6027\u80fd\u4e0d\u4f73\u3002\u672c\u6587\u53d7\u771f\u5b9e\u4eba\u683c\u4e0e\u4eba\u7c7b\u5185\u90e8\u8ba4\u77e5\u4e4b\u95f4\u7684\u5173\u8054\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2D\u56fe\u795e\u7ecf\u7f51\u7edc\uff082D-GNN\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u76ee\u6807\u4e2a\u4f53\u7684\u4e2a\u6027\u5316\u5185\u90e8\u8ba4\u77e5\uff0c\u5e76\u5c06\u5176\u7f16\u7801\u4e3a\u4e00\u4e2a\u5305\u542b\u4e8c\u7ef4\u8282\u70b9\u548c\u8fb9\u7279\u5f81\u77e9\u9635\u7684\u65b0\u56fe\u7ed3\u6784\uff0c\u4ece\u800c\u63a8\u65ad\u771f\u5b9e\u4eba\u683c\u7279\u8d28\u3002\u91c7\u7528\u7aef\u5230\u7aef\u7684\u7b56\u7565\u8054\u5408\u8bad\u7ec3\u8ba4\u77e5\u6a21\u62df\u30012D\u56fe\u6784\u5efa\u548c\u4eba\u683c\u8bc6\u522b\u6a21\u5757\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6a21\u62df\u4e2a\u6027\u5316\u7684\u5185\u90e8\u8ba4\u77e5\uff0c\u5e76\u901a\u8fc72D-GNN\u4ece\u6a21\u62df\u7684\u8ba4\u77e5\u4e2d\u63a8\u65ad\u771f\u5b9e\u4eba\u683c\u7279\u8d28\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u52a8\u771f\u5b9e\u4eba\u683c\u8bc6\u522b\uff08RPR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4e2a\u6027\u5316\u7684\u5185\u90e8\u8ba4\u77e5\u6765\u63d0\u5347\u4eba\u683c\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u771f\u5b9e\u4eba\u683c\u7279\u8d28\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00271", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.00271", "abs": "https://arxiv.org/abs/2508.00271", "authors": ["Hongjin Qian", "Zheng Liu"], "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning", "comment": "Technical Report, 14 pages", "summary": "In this work, we propose MetaAgent, an agentic paradigm inspired by the\nprinciple of learning-by-doing, where expertise is developed through hands-on\npractice and continual self-improvement. MetaAgent starts with a minimal\nworkflow, equipped only with basic reasoning and adaptive help-seeking\nabilities. When a knowledge gap is encountered, MetaAgent generates natural\nlanguage help requests, which are routed to the most suitable external tool by\na dedicated tool router. As MetaAgent solves tasks, it continually conducts\nself-reflection and answer verification, distilling actionable experience into\nconcise texts that are dynamically incorporated into future task contexts.\nBesides, MetaAgent autonomously builds in-house tools and a persistent\nknowledge base by organizing its tool-use history, further enhancing its\nability to retrieve and integrate relevant information We term this continual,\ndata-driven process as \\textit{meta tool learning}, through which MetaAgent\nincrementally refines its reasoning and tool-use strategies, without changing\nmodel parameters or requiring further post-training. Evaluated on challenging\nknowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,\nMetaAgent consistently outperforms workflow-based baselines and matches or\nexceeds end-to-end trained agents, demonstrating the promise of self-evolving\nagentic systems for robust, general-purpose knowledge discovery. We provide our\nsource codes in https://github.com/qhjqhj00/MetaAgent.", "AI": {"tldr": "MetaAgent\u662f\u4e00\u4e2a\u81ea\u8fdb\u5316\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7'\u505a\u4e2d\u5b66'\u4e0d\u65ad\u6539\u8fdb\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7b56\u7565\uff0c\u65e0\u9700\u8c03\u6574\u6a21\u578b\u53c2\u6570\u6216\u540e\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u53d7'\u505a\u4e2d\u5b66'\u539f\u5219\u542f\u53d1\uff0c\u65e8\u5728\u901a\u8fc7\u5b9e\u8df5\u548c\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u5f00\u53d1\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "MetaAgent\u901a\u8fc7\u6700\u5c0f\u5316\u5de5\u4f5c\u6d41\u7a0b\u542f\u52a8\uff0c\u5177\u5907\u57fa\u672c\u63a8\u7406\u548c\u81ea\u9002\u5e94\u6c42\u52a9\u80fd\u529b\u3002\u9047\u5230\u77e5\u8bc6\u7f3a\u53e3\u65f6\uff0c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6c42\u52a9\u8bf7\u6c42\uff0c\u5e76\u901a\u8fc7\u4e13\u7528\u5de5\u5177\u8def\u7531\u5668\u8def\u7531\u5230\u6700\u5408\u9002\u7684\u5916\u90e8\u5de5\u5177\u3002\u4efb\u52a1\u89e3\u51b3\u8fc7\u7a0b\u4e2d\u6301\u7eed\u8fdb\u884c\u81ea\u6211\u53cd\u601d\u548c\u7b54\u6848\u9a8c\u8bc1\uff0c\u5c06\u53ef\u64cd\u4f5c\u7ecf\u9a8c\u63d0\u70bc\u4e3a\u7b80\u6d01\u6587\u672c\u5e76\u52a8\u6001\u6574\u5408\u5230\u672a\u6765\u4efb\u52a1\u4e2d\u3002\u6b64\u5916\uff0c\u81ea\u4e3b\u6784\u5efa\u5185\u90e8\u5de5\u5177\u548c\u6301\u4e45\u77e5\u8bc6\u5e93\u3002", "result": "\u5728GAIA\u3001WebWalkerQA\u548cBrowseCamp\u7b49\u77e5\u8bc6\u53d1\u73b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMetaAgent\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7a0b\u7684\u57fa\u7ebf\uff0c\u5e76\u5339\u914d\u6216\u8d85\u8d8a\u7aef\u5230\u7aef\u8bad\u7ec3\u4ee3\u7406\u3002", "conclusion": "MetaAgent\u5c55\u793a\u4e86\u81ea\u8fdb\u5316\u4ee3\u7406\u7cfb\u7edf\u5728\u7a33\u5065\u3001\u901a\u7528\u77e5\u8bc6\u53d1\u73b0\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.00256", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00256", "abs": "https://arxiv.org/abs/2508.00256", "authors": ["Chuang Zhang", "Geng Sun", "Jiacheng Wang", "Yijing Lin", "Weijie Yuan", "Sinem Coleri", "Dusit Niyato", "Tony Q. S. Quek"], "title": "Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study", "comment": "This paper has been submitted to IEEE Communications Magazine for\n  consideration", "summary": "Low-altitude wireless networks (LAWNs) have the potential to revolutionize\ncommunications by supporting a range of applications, including urban parcel\ndelivery, aerial inspections and air taxis. However, compared with traditional\nwireless networks, LAWNs face unique security challenges due to low-altitude\noperations, frequent mobility and reliance on unlicensed spectrum, making it\nmore vulnerable to some malicious attacks. In this paper, we investigate some\nlarge artificial intelligence model (LAM)-enabled solutions for secure\ncommunications in LAWNs. Specifically, we first explore the amplified security\nrisks and important limitations of traditional AI methods in LAWNs. Then, we\nintroduce the basic concepts of LAMs and delve into the role of LAMs in\naddressing these challenges. To demonstrate the practical benefits of LAMs for\nsecure communications in LAWNs, we propose a novel LAM-based optimization\nframework that leverages large language models (LLMs) to generate enhanced\nstate features on top of handcrafted representations, and to design intrinsic\nrewards accordingly, thereby improving reinforcement learning performance for\nsecure communication tasks. Through a typical case study, simulation results\nvalidate the effectiveness of the proposed framework. Finally, we outline\nfuture directions for integrating LAMs into secure LAWN applications.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LAM\u5728\u89e3\u51b3LAWNs\u5b89\u5168\u901a\u4fe1\u6311\u6218\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLAM\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "LAWNs\u56e0\u5176\u4f4e\u7a7a\u64cd\u4f5c\u3001\u9891\u7e41\u79fb\u52a8\u548c\u4f9d\u8d56\u975e\u6388\u6743\u9891\u8c31\u7b49\u7279\u70b9\uff0c\u9762\u4e34\u72ec\u7279\u7684\u5b89\u5168\u6311\u6218\uff0c\u4f20\u7edfAI\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76LAM\u5728LAWNs\u5b89\u5168\u901a\u4fe1\u4e2d\u7684\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "\u8bba\u6587\u9996\u5148\u63a2\u8ba8\u4e86LAWNs\u4e2d\u4f20\u7edfAI\u65b9\u6cd5\u7684\u5b89\u5168\u98ce\u9669\u548c\u5c40\u9650\u6027\uff0c\u968f\u540e\u4ecb\u7ecd\u4e86LAM\u7684\u57fa\u672c\u6982\u5ff5\u53ca\u5176\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u4e2d\u7684\u4f5c\u7528\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eLAM\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528LLMs\u751f\u6210\u589e\u5f3a\u7684\u72b6\u6001\u7279\u5f81\u548c\u8bbe\u8ba1\u5185\u5728\u5956\u52b1\uff0c\u4ee5\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u5728\u5b89\u5168\u901a\u4fe1\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5178\u578b\u6848\u4f8b\u7814\u7a76\uff0c\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684LAM-based\u4f18\u5316\u6846\u67b6\u5728\u63d0\u5347LAWNs\u5b89\u5168\u901a\u4fe1\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLAM\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347\u4e86LAWNs\u4e2d\u7684\u5b89\u5168\u901a\u4fe1\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u540c\u65f6\uff0c\u5c55\u671b\u4e86LAM\u5728\u5b89\u5168LAWN\u5e94\u7528\u4e2d\u7684\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2508.00625", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00625", "abs": "https://arxiv.org/abs/2508.00625", "authors": ["Bartosz Krawczyk", "Ahmed Elbary", "Robbie Cato", "Jagdish Patil", "Kaung Myat", "Anyeh Ndi-Tah", "Nivetha Sakthivel", "Mark Crampton", "Gautham Das", "Charles Fox"], "title": "OpenScout v1.1 mobile robot: a case study on open hardware continuation", "comment": "6 pages, 4 figures, a TAROS2025 short paper", "summary": "OpenScout is an Open Source Hardware (OSH) mobile robot for research and\nindustry. It is extended to v1.1 which includes simplified, cheaper and more\npowerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo\nsimulation. Changes, their rationale, project methodology, and results are\nreported as an OSH case study.", "AI": {"tldr": "OpenScout v1.1\u662f\u4e00\u4e2a\u6539\u8fdb\u7248\u7684\u5f00\u6e90\u786c\u4ef6\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u7b80\u5316\u4e86\u8bbe\u8ba1\u3001\u964d\u4f4e\u4e86\u6210\u672c\u5e76\u589e\u5f3a\u4e86\u8ba1\u7b97\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u3002", "motivation": "\u4e3a\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e00\u4e2a\u66f4\u7ecf\u6d4e\u3001\u66f4\u9ad8\u6548\u7684\u5f00\u6e90\u786c\u4ef6\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\u3002", "method": "\u9879\u76ee\u901a\u8fc7\u7b80\u5316\u786c\u4ef6\u8bbe\u8ba1\u3001\u964d\u4f4e\u6210\u672c\u548c\u589e\u5f3a\u8ba1\u7b97\u80fd\u529b\u8fdb\u884c\u6539\u8fdb\uff0c\u5e76\u63d0\u4f9b\u4e86ROS2\u63a5\u53e3\u548cGazebo\u6a21\u62df\u3002", "result": "OpenScout v1.1\u6210\u529f\u5b9e\u73b0\u4e86\u786c\u4ef6\u7b80\u5316\u3001\u6210\u672c\u964d\u4f4e\u548c\u8ba1\u7b97\u80fd\u529b\u63d0\u5347\uff0c\u5e76\u63d0\u4f9b\u4e86\u6a21\u62df\u63a5\u53e3\u3002", "conclusion": "OpenScout v1.1\u7684\u6539\u8fdb\u4f7f\u5176\u6210\u4e3a\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u4e2d\u66f4\u5177\u6210\u672c\u6548\u76ca\u548c\u529f\u80fd\u5f3a\u5927\u7684\u5f00\u6e90\u786c\u4ef6\u79fb\u52a8\u673a\u5668\u4eba\u3002"}}
{"id": "2508.00462", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00462", "abs": "https://arxiv.org/abs/2508.00462", "authors": ["Linus Ververs", "Lutz Prechelt"], "title": "Managing Power Gaps as a Topic of Pair Programming Skill: A Grounded Theory", "comment": null, "summary": "Context: Pair Programming as a work mode is used (occasionally or frequently)\nthroughout professional software development. Objective: Understand what\npower-related phenomena occur in pair programming as it is used in industry;\ngive advice to practitioners on how to do better pair programming. Method:\nAnalyze 22 industrial pair programming sessions using Grounded Theory\nMethodology. Formulate a Grounded Theory on power-related behaviors. Run a\nsurvey with 292 participants about that theory. Use it to demonstrate that the\nphenomena are common. Results: Our theory describes the phenomenon of Power\nGap: a perceived difference in participation opportunities. The theory shows\nthe behaviors that create a Power Gap or result from it. Power Gaps tend to\ndamage knowledge transfer, code quality, and process effi ciency. The survey\nresults show that all concepts from our theory are frequent in practice. They\nalso provide more grounding for concepts that are observable only indirectly.\nConclusions: It is a valuable component of pair programming skill to be able to\navoid Power Gaps. Specifically, pair partners need to avoid Hierarchical\nBehavior (which tends to create or increase a Power Gap) and should perform\nenough Equalizing Behavior (which prevents or reduces a Power Gap).", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u624e\u6839\u7406\u8bba\u5206\u679022\u4e2a\u5de5\u4e1a\u7ed3\u5bf9\u7f16\u7a0b\u4f1a\u8bdd\u5e76\u8c03\u67e5292\u540d\u53c2\u4e0e\u8005\uff0c\u63ed\u793a\u4e86\u6743\u529b\u5dee\u8ddd\u73b0\u8c61\u53ca\u5176\u8d1f\u9762\u5f71\u54cd\uff0c\u5efa\u8bae\u907f\u514d\u5c42\u7ea7\u884c\u4e3a\u5e76\u589e\u52a0\u5e73\u8861\u884c\u4e3a\u4ee5\u63d0\u9ad8\u7ed3\u5bf9\u7f16\u7a0b\u6548\u679c\u3002", "motivation": "\u4e86\u89e3\u5728\u5de5\u4e1a\u4e2d\u4f7f\u7528\u7684\u7ed3\u5bf9\u7f16\u7a0b\u4e2d\u51fa\u73b0\u7684\u4e0e\u6743\u529b\u76f8\u5173\u7684\u73b0\u8c61\uff0c\u5e76\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u5982\u4f55\u66f4\u597d\u5730\u8fdb\u884c\u7ed3\u5bf9\u7f16\u7a0b\u7684\u5efa\u8bae\u3002", "method": "\u5206\u679022\u4e2a\u5de5\u4e1a\u7ed3\u5bf9\u7f16\u7a0b\u4f1a\u8bdd\uff0c\u4f7f\u7528\u624e\u6839\u7406\u8bba\u65b9\u6cd5\u8bba\u3002\u5236\u5b9a\u5173\u4e8e\u6743\u529b\u76f8\u5173\u884c\u4e3a\u7684\u624e\u6839\u7406\u8bba\u3002\u5bf9292\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u5173\u4e8e\u8be5\u7406\u8bba\u7684\u8c03\u67e5\uff0c\u4ee5\u8bc1\u660e\u8fd9\u4e9b\u73b0\u8c61\u7684\u666e\u904d\u6027\u3002", "result": "\u6211\u4eec\u7684\u7406\u8bba\u63cf\u8ff0\u4e86\u6743\u529b\u5dee\u8ddd\u73b0\u8c61\uff1a\u611f\u77e5\u5230\u7684\u53c2\u4e0e\u673a\u4f1a\u5dee\u5f02\u3002\u7406\u8bba\u5c55\u793a\u4e86\u5bfc\u81f4\u6743\u529b\u5dee\u8ddd\u7684\u884c\u4e3a\u6216\u7531\u6b64\u4ea7\u751f\u7684\u884c\u4e3a\u3002\u6743\u529b\u5dee\u8ddd\u5f80\u5f80\u4f1a\u635f\u5bb3\u77e5\u8bc6\u4f20\u9012\u3001\u4ee3\u7801\u8d28\u91cf\u548c\u6d41\u7a0b\u6548\u7387\u3002\u8c03\u67e5\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7406\u8bba\u4e2d\u7684\u6240\u6709\u6982\u5ff5\u5728\u5b9e\u8df5\u4e2d\u90fd\u5f88\u5e38\u89c1\u3002", "conclusion": "\u638c\u63e1\u907f\u514d\u6743\u529b\u5dee\u8ddd\u7684\u80fd\u529b\u662f\u7ed3\u5bf9\u7f16\u7a0b\u6280\u80fd\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7ed3\u5bf9\u4f19\u4f34\u9700\u8981\u907f\u514d\u5c42\u7ea7\u884c\u4e3a\uff08\u8fd9\u5f80\u5f80\u4f1a\u521b\u9020\u6216\u52a0\u5267\u6743\u529b\u5dee\u8ddd\uff09\uff0c\u5e76\u5e94\u6267\u884c\u8db3\u591f\u7684\u5e73\u8861\u884c\u4e3a\uff08\u8fd9\u53ef\u4ee5\u9632\u6b62\u6216\u51cf\u5c11\u6743\u529b\u5dee\u8ddd\uff09\u3002"}}
{"id": "2508.00213", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00213", "abs": "https://arxiv.org/abs/2508.00213", "authors": ["Shayan Jalilian", "Abdul Bais"], "title": "SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters", "comment": null, "summary": "The Segment Anything Model (SAM) has demonstrated impressive generalization\nin prompt-based segmentation. Yet, the potential of semantic text prompts\nremains underexplored compared to traditional spatial prompts like points and\nboxes. This paper introduces SAM-PTx, a parameter-efficient approach for\nadapting SAM using frozen CLIP-derived text embeddings as class-level semantic\nguidance. Specifically, we propose a lightweight adapter design called\nParallel-Text that injects text embeddings into SAM's image encoder, enabling\nsemantics-guided segmentation while keeping most of the original architecture\nfrozen. Our adapter modifies only the MLP-parallel branch of each transformer\nblock, preserving the attention pathway for spatial reasoning. Through\nsupervised experiments and ablations on the COD10K dataset as well as low-data\nsubsets of COCO and ADE20K, we show that incorporating fixed text embeddings as\ninput improves segmentation performance over purely spatial prompt baselines.\nTo our knowledge, this is the first work to use text prompts for segmentation\non the COD10K dataset. These results suggest that integrating semantic\nconditioning into SAM's architecture offers a practical and scalable path for\nefficient adaptation with minimal computational complexity.", "AI": {"tldr": "SAM-PTx\u901a\u8fc7\u51bb\u7ed3CLIP\u6587\u672c\u5d4c\u5165\u4f5c\u4e3a\u8bed\u4e49\u6307\u5bfc\uff0c\u63d0\u51fa\u8f7b\u91cf\u7ea7\u9002\u914d\u5668Parallel-Text\uff0c\u63d0\u5347SAM\u7684\u8bed\u4e49\u5f15\u5bfc\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u8bed\u4e49\u6587\u672c\u63d0\u793a\u5728SAM\u4e2d\u7684\u6f5c\u529b\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u7a7a\u95f4\u63d0\u793a\uff08\u5982\u70b9\u548c\u6846\uff09\u4ecd\u672a\u5145\u5206\u5f00\u53d1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u8bbe\u8ba1Parallel-Text\uff0c\u5c06CLIP\u884d\u751f\u7684\u6587\u672c\u5d4c\u5165\u6ce8\u5165SAM\u7684\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u4fdd\u7559\u5927\u90e8\u5206\u539f\u59cb\u67b6\u6784\u51bb\u7ed3\u3002", "result": "\u5728COD10K\u6570\u636e\u96c6\u53caCOCO\u548cADE20K\u7684\u4f4e\u6570\u636e\u5b50\u96c6\u4e0a\uff0c\u56fa\u5b9a\u6587\u672c\u5d4c\u5165\u4f5c\u4e3a\u8f93\u5165\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "conclusion": "\u5c06\u8bed\u4e49\u6761\u4ef6\u96c6\u6210\u5230SAM\u67b6\u6784\u4e2d\uff0c\u4e3a\u9ad8\u6548\u9002\u5e94\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u540c\u65f6\u8ba1\u7b97\u590d\u6742\u5ea6\u6700\u4f4e\u3002"}}
{"id": "2508.00282", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00282", "abs": "https://arxiv.org/abs/2508.00282", "authors": ["Yi-Long Lu", "Jiajun Song", "Chunhui Zhang", "Wei Wang"], "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks", "comment": null, "summary": "Humans constantly generate a diverse range of tasks guided by internal\nmotivations. While generative agents powered by large language models (LLMs)\naim to simulate this complex behavior, it remains uncertain whether they\noperate on similar cognitive principles. To address this, we conducted a\ntask-generation experiment comparing human responses with those of an LLM agent\n(GPT-4o). We find that human task generation is consistently influenced by\npsychological drivers, including personal values (e.g., Openness to Change) and\ncognitive style. Even when these psychological drivers are explicitly provided\nto the LLM, it fails to reflect the corresponding behavioral patterns. They\nproduce tasks that are markedly less social, less physical, and thematically\nbiased toward abstraction. Interestingly, while the LLM's tasks were perceived\nas more fun and novel, this highlights a disconnect between its linguistic\nproficiency and its capacity to generate human-like, embodied goals.We conclude\nthat there is a core gap between the value-driven, embodied nature of human\ncognition and the statistical patterns of LLMs, highlighting the necessity of\nincorporating intrinsic motivation and physical grounding into the design of\nmore human-aligned agents.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u751f\u6210\u7684\u4efb\u52a1\u4e0e\u4eba\u7c7b\u884c\u4e3a\u6a21\u5f0f\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7a81\u663e\u4e86\u6574\u5408\u5185\u5728\u52a8\u673a\u548c\u7269\u7406\u57fa\u7840\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u4ee3\u7406\uff08\u5982LLMs\uff09\u662f\u5426\u80fd\u6a21\u62df\u4eba\u7c7b\u590d\u6742\u884c\u4e3a\u80cc\u540e\u7684\u8ba4\u77e5\u539f\u5219\uff0c\u5c24\u5176\u662f\u5fc3\u7406\u9a71\u52a8\u56e0\u7d20\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u4efb\u52a1\u751f\u6210\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4eba\u7c7b\u4e0eLLM\u4ee3\u7406\uff08GPT-4o\uff09\u7684\u53cd\u5e94\uff0c\u5206\u6790\u5fc3\u7406\u9a71\u52a8\u56e0\u7d20\uff08\u5982\u4e2a\u4eba\u4ef7\u503c\u89c2\u548c\u8ba4\u77e5\u98ce\u683c\uff09\u5bf9\u4efb\u52a1\u751f\u6210\u7684\u5f71\u54cd\u3002", "result": "\u4eba\u7c7b\u4efb\u52a1\u751f\u6210\u53d7\u5fc3\u7406\u9a71\u52a8\u56e0\u7d20\u663e\u8457\u5f71\u54cd\uff0c\u800cLLM\u5373\u4f7f\u660e\u786e\u63d0\u4f9b\u8fd9\u4e9b\u9a71\u52a8\u56e0\u7d20\uff0c\u751f\u6210\u7684\u4efb\u52a1\u4ecd\u66f4\u62bd\u8c61\u3001\u793e\u4ea4\u6027\u548c\u7269\u7406\u6027\u8f83\u4f4e\uff0c\u4e14\u4e0e\u4eba\u7c7b\u884c\u4e3a\u6a21\u5f0f\u4e0d\u7b26\u3002\u867d\u7136LLM\u7684\u4efb\u52a1\u88ab\u8ba4\u4e3a\u66f4\u6709\u8da3\u548c\u65b0\u9896\uff0c\u4f46\u5176\u8bed\u8a00\u80fd\u529b\u4e0e\u751f\u6210\u4eba\u7c7b\u7c7b\u4f3c\u5177\u8eab\u76ee\u6807\u7684\u80fd\u529b\u5b58\u5728\u8131\u8282\u3002", "conclusion": "\u7814\u7a76\u6307\u51fa\uff0c\u4eba\u7c7b\u8ba4\u77e5\u7684\u4ef7\u503c\u9a71\u52a8\u548c\u5177\u8eab\u7279\u6027\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u7edf\u8ba1\u6a21\u5f0f\u4e4b\u95f4\u5b58\u5728\u6838\u5fc3\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u5728\u8bbe\u8ba1\u66f4\u7b26\u5408\u4eba\u7c7b\u884c\u4e3a\u7684\u667a\u80fd\u4f53\u65f6\uff0c\u5fc5\u987b\u878d\u5165\u5185\u5728\u52a8\u673a\u548c\u7269\u7406\u57fa\u7840\u3002"}}
{"id": "2508.00261", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00261", "abs": "https://arxiv.org/abs/2508.00261", "authors": ["Saichao Liu", "Geng Sun", "Chuang Zhang", "Xuejie Liu", "Jiacheng Wang", "Changyuan Zhao", "Dusit Niyato"], "title": "Energy Efficient Trajectory Control and Resource Allocation in Multi-UAV-assisted MEC via Deep Reinforcement Learning", "comment": "This paper has been accepted by IEEE GLOBECOM 2025", "summary": "Mobile edge computing (MEC) is a promising technique to improve the\ncomputational capacity of smart devices (SDs) in Internet of Things (IoT).\nHowever, the performance of MEC is restricted due to its fixed location and\nlimited service scope. Hence, we investigate an unmanned aerial vehicle\n(UAV)-assisted MEC system, where multiple UAVs are dispatched and each UAV can\nsimultaneously provide computing service for multiple SDs. To improve the\nperformance of system, we formulated a UAV-based trajectory control and\nresource allocation multi-objective optimization problem (TCRAMOP) to\nsimultaneously maximize the offloading number of UAVs and minimize total\noffloading delay and total energy consumption of UAVs by optimizing the flight\npaths of UAVs as well as the computing resource allocated to served SDs. Then,\nconsider that the solution of TCRAMOP requires continuous decision-making and\nthe system is dynamic, we propose an enhanced deep reinforcement learning (DRL)\nalgorithm, namely, distributed proximal policy optimization with imitation\nlearning (DPPOIL). This algorithm incorporates the generative adversarial\nimitation learning technique to improve the policy performance. Simulation\nresults demonstrate the effectiveness of our proposed DPPOIL and prove that the\nlearned strategy of DPPOIL is better compared with other baseline methods.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e00\u79cd\u65e0\u4eba\u673a\u8f85\u52a9\u7684\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08DPPOIL\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u98de\u884c\u8def\u5f84\u548c\u8d44\u6e90\u5206\u914d\u6765\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\uff0c\u4eff\u771f\u7ed3\u679c\u8868\u660e\u5176\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u7684\u6027\u80fd\u53d7\u9650\u4e8e\u5176\u56fa\u5b9a\u4f4d\u7f6e\u548c\u6709\u9650\u7684\u670d\u52a1\u8303\u56f4\uff0c\u56e0\u6b64\u7814\u7a76\u4e86\u65e0\u4eba\u673a\uff08UAV\uff09\u8f85\u52a9\u7684MEC\u7cfb\u7edf\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7b97\u6cd5\uff0c\u5373\u5206\u5e03\u5f0f\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u4e0e\u6a21\u4eff\u5b66\u4e60\uff08DPPOIL\uff09\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u751f\u6210\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u6280\u672f\u4ee5\u63d0\u9ad8\u7b56\u7565\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u4f18\u5316\u65e0\u4eba\u673a\u7684\u98de\u884c\u8def\u5f84\u548c\u5206\u914d\u7ed9\u670d\u52a1\u667a\u80fd\u8bbe\u5907\uff08SDs\uff09\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u540c\u65f6\u6700\u5927\u5316\u65e0\u4eba\u673a\u7684\u5378\u8f7d\u6570\u91cf\u5e76\u6700\u5c0f\u5316\u603b\u5378\u8f7d\u5ef6\u8fdf\u548c\u65e0\u4eba\u673a\u7684\u603b\u80fd\u8017\u3002", "conclusion": "\u4eff\u771f\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684DPPOIL\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u8bc1\u660e\u4e86\u5176\u5b66\u4e60\u7b56\u7565\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.00691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00691", "abs": "https://arxiv.org/abs/2508.00691", "authors": ["Fabian C. Weigend", "Dabin K. Choe", "Santiago Canete", "Conor J. Walsh"], "title": "Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait", "comment": "8 pages, 6 figures, 2 tables", "summary": "Recent work has shown that exoskeletons controlled through data-driven\nmethods can dynamically adapt assistance to various tasks for healthy young\nadults. However, applying these methods to populations with neuromotor gait\ndeficits, such as post-stroke hemiparesis, is challenging. This is due not only\nto high population heterogeneity and gait variability but also to a lack of\npost-stroke gait datasets to train accurate models. Despite these challenges,\ndata-driven methods offer a promising avenue for control, potentially allowing\nexoskeletons to function safely and effectively in unstructured community\nsettings. This work presents a first step towards enabling adaptive\nplantarflexion and dorsiflexion assistance from data-driven torque estimation\nduring post-stroke walking. We trained a multi-task Temporal Convolutional\nNetwork (TCN) using collected data from four post-stroke participants walking\non a treadmill ($R^2$ of $0.74 \\pm 0.13$). The model uses data from three\ninertial measurement units (IMU) and was pretrained on healthy walking data\nfrom 6 participants. We implemented a wearable prototype for our ankle torque\nestimation approach for exoskeleton control and demonstrated the viability of\nreal-time sensing, estimation, and actuation with one post-stroke participant.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1TCN\u6a21\u578b\uff0c\u7528\u4e8e\u4e2d\u98ce\u540e\u884c\u8d70\u65f6\u7684\u8e1d\u5173\u8282\u626d\u77e9\u4f30\u8ba1\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u5916\u9aa8\u9abc\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u5065\u5eb7\u5e74\u8f7b\u4eba\u4e2d\u5df2\u663e\u793a\u51fa\u5bf9\u5916\u9aa8\u9abc\u63a7\u5236\u7684\u9002\u5e94\u6027\uff0c\u4f46\u5728\u4e2d\u98ce\u540e\u4eba\u7fa4\u4e2d\u7684\u5e94\u7528\u9762\u4e34\u9ad8\u5f02\u8d28\u6027\u548c\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\u7684\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u4e2d\u98ce\u540e\u884c\u8d70\u5916\u9aa8\u9abc\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u4ece\u56db\u540d\u4e2d\u98ce\u540e\u53c2\u4e0e\u8005\u5728\u8dd1\u6b65\u673a\u4e0a\u884c\u8d70\u7684\u6570\u636e\u8bad\u7ec3\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\uff08TCN\uff09\uff0c\u7ed3\u5408\u4e09\u4e2a\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u7684\u6570\u636e\uff0c\u5e76\u9884\u8bad\u7ec3\u4e86\u516d\u540d\u5065\u5eb7\u53c2\u4e0e\u8005\u7684\u884c\u8d70\u6570\u636e\u3002", "result": "\u6a21\u578b\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u51c6\u786e\u6027\uff08R^2\u4e3a0.74\u00b10.13\uff09\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u4e2d\u98ce\u540e\u53c2\u4e0e\u8005\u7684\u5b9e\u65f6\u4f20\u611f\u3001\u4f30\u8ba1\u548c\u9a71\u52a8\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\uff08TCN\uff09\uff0c\u7528\u4e8e\u4e2d\u98ce\u540e\u884c\u8d70\u65f6\u7684\u8e1d\u5173\u8282\u626d\u77e9\u4f30\u8ba1\uff0c\u4e3a\u5916\u9aa8\u9abc\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u65f6\u4f20\u611f\u3001\u4f30\u8ba1\u548c\u9a71\u52a8\u7684\u53ef\u884c\u6027\u9a8c\u8bc1\u3002"}}
{"id": "2508.00508", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.00508", "abs": "https://arxiv.org/abs/2508.00508", "authors": ["Panagiotis Diamantakis", "Thanassis Avgerinos", "Yannis Smaragdakis"], "title": "Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis", "comment": null, "summary": "Over the past two decades, two different types of static analyses have\nemerged as dominant paradigms both in academia and industry: value-flow\nanalysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis\n(e.g., symbolic execution). Despite their individual successes in numerous\napplication fields, the two approaches have remained largely separate; an\nartifact of the simple reality that there is no broadly adopted unifying\nplatform for effortless and efficient integration of symbolic techniques with\nhigh-performance data-flow reasoning.\n  To bridge this gap, we introduce Desyan: a platform for writing program\nanalyses with seamless integration of value-flow and symbolic reasoning. Desyan\nexpands a production-ready Datalog fixpoint engine (Souffl\\'e) with\nfull-fledged SMT solving invoking industry-leading SMT engines. Desyan provides\nconstructs for automatically (and efficiently!) handling typical patterns that\ncome up in program analysis. At the same time, the integration is agnostic with\nrespect to the solving technology, and supports Datalog-native symbolic\nreasoning, via a bottom-up algebraic reasoning module.\n  The result is an engine that allows blending different kinds of reasoning, as\nneeded for the underlying analysis. For value-flow analysis, the engine is the\nbest-in-class Datalog evaluator (often by a factor of over 20x in execution\ntime); for applications that require full SMT (e.g., a concolic execution\nengine or other symbolic evaluator that needs to solve arbitrarily complex\nconditions), the engine is leveraging the leading SMT solvers; for lightweight\nsymbolic evaluation (e.g., solving simple conditionals in the context of a\npath-sensitive analysis), the engine can use Datalog-native symbolic reasoning,\nachieving large speedups (often of over 2x) compared to eagerly appealing to an\nSMT solver.", "AI": {"tldr": "Desyan\u662f\u4e00\u4e2a\u7edf\u4e00\u5e73\u53f0\uff0c\u65e0\u7f1d\u96c6\u6210\u4ef7\u503c\u6d41\u548c\u7b26\u53f7\u5206\u6790\uff0c\u652f\u6301\u591a\u79cd\u63a8\u7406\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u5206\u6790\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u4ef7\u503c\u6d41\u5206\u6790\u548c\u7b26\u53f7\u5206\u6790\u867d\u7136\u5404\u81ea\u5728\u591a\u4e2a\u5e94\u7528\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u5e73\u53f0\u5b9e\u73b0\u9ad8\u6548\u96c6\u6210\uff0c\u9650\u5236\u4e86\u5176\u8054\u5408\u5e94\u7528\u7684\u6f5c\u529b\u3002", "method": "Desyan\u6269\u5c55\u4e86\u751f\u4ea7\u7ea7\u7684Datalog\u56fa\u5b9a\u70b9\u5f15\u64ce\uff08Souffl\u00e9\uff09\uff0c\u96c6\u6210\u4e86\u5168\u529f\u80fd\u7684SMT\u6c42\u89e3\u80fd\u529b\uff0c\u5e76\u652f\u6301Datalog\u539f\u751f\u7b26\u53f7\u63a8\u7406\u6a21\u5757\u3002", "result": "Desyan\u5728\u4ef7\u503c\u6d41\u5206\u6790\u4e2d\u8868\u73b0\u5353\u8d8a\uff08\u6267\u884c\u65f6\u95f4\u63d0\u534720\u500d\u4ee5\u4e0a\uff09\uff0c\u5728\u9700\u8981\u590d\u6742SMT\u6c42\u89e3\u7684\u573a\u666f\u4e0b\u4e5f\u80fd\u9ad8\u6548\u5229\u7528\u9886\u5148\u7684SMT\u6c42\u89e3\u5668\uff0c\u540c\u65f6\u5728\u8f7b\u91cf\u7ea7\u7b26\u53f7\u63a8\u7406\u4e2d\u5b9e\u73b0\u663e\u8457\u52a0\u901f\uff08\u901f\u5ea6\u63d0\u53472\u500d\u4ee5\u4e0a\uff09\u3002", "conclusion": "Desyan\u5e73\u53f0\u6210\u529f\u586b\u8865\u4e86\u4ef7\u503c\u6d41\u5206\u6790\u548c\u7b26\u53f7\u5206\u6790\u4e4b\u95f4\u7684\u6280\u672f\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u7edf\u4e00\u5e73\u53f0\uff0c\u652f\u6301\u591a\u79cd\u5206\u6790\u9700\u6c42\u7684\u6df7\u5408\u63a8\u7406\u3002"}}
{"id": "2508.00218", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00218", "abs": "https://arxiv.org/abs/2508.00218", "authors": ["Aymane Abdali", "Bartosz Boguslawski", "Lucas Drumetz", "Vincent Gripon"], "title": "Object-Centric Cropping for Visual Few-Shot Classification", "comment": null, "summary": "In the domain of Few-Shot Image Classification, operating with as little as\none example per class, the presence of image ambiguities stemming from multiple\nobjects or complex backgrounds can significantly deteriorate performance. Our\nresearch demonstrates that incorporating additional information about the local\npositioning of an object within its image markedly enhances classification\nacross established benchmarks. More importantly, we show that a significant\nfraction of the improvement can be achieved through the use of the Segment\nAnything Model, requiring only a pixel of the object of interest to be pointed\nout, or by employing fully unsupervised foreground object extraction methods.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u7269\u4f53\u5c40\u90e8\u4f4d\u7f6e\u4fe1\u606f\u53ca\u7b80\u5355\u6807\u6ce8\u6216\u65e0\u76d1\u7763\u65b9\u6cd5\uff0cFew-Shot\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "Few-Shot\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0c\u56fe\u50cf\u56e0\u591a\u7269\u4f53\u6216\u590d\u6742\u80cc\u666f\u5bfc\u81f4\u7684\u6a21\u7cca\u6027\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528Segment Anything Model\uff08\u4ec5\u9700\u6807\u6ce8\u76ee\u6807\u7269\u4f53\u7684\u4e00\u4e2a\u50cf\u7d20\uff09\u6216\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u524d\u666f\u7269\u4f53\u63d0\u53d6\u65b9\u6cd5\uff0c\u7ed3\u5408\u7269\u4f53\u5c40\u90e8\u4f4d\u7f6e\u4fe1\u606f\u3002", "result": "\u5728\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5206\u7c7b\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5728Few-Shot\u56fe\u50cf\u5206\u7c7b\u9886\u57df\uff0c\u901a\u8fc7\u5f15\u5165\u7269\u4f53\u5c40\u90e8\u4f4d\u7f6e\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u4e14\u4f7f\u7528Segment Anything Model\u6216\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u524d\u666f\u7269\u4f53\u63d0\u53d6\u65b9\u6cd5\u5373\u53ef\u5b9e\u73b0\u5927\u90e8\u5206\u6539\u8fdb\u3002"}}
{"id": "2508.00323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00323", "abs": "https://arxiv.org/abs/2508.00323", "authors": ["Jianyi Zhang", "Xu Ji", "Ziyin Zhou", "Yuchen Zhou", "Shubo Shi", "Haoyu Wu", "Zhen Li", "Shizhao Liu"], "title": "Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning", "comment": null, "summary": "Evaluating the performance of visual language models (VLMs) in graphic\nreasoning tasks has become an important research topic. However, VLMs still\nshow obvious deficiencies in simulating human-level graphic reasoning\ncapabilities, especially in complex graphic reasoning and abstract problem\nsolving, which are less studied and existing studies only focus on simple\ngraphics. To evaluate the performance of VLMs in complex graphic reasoning, we\npropose ReasonBench, the first evaluation benchmark focused on structured\ngraphic reasoning tasks, which includes 1,613 questions from real-world\nintelligence tests. ReasonBench covers reasoning dimensions related to\nlocation, attribute, quantity, and multi-element tasks, providing a\ncomprehensive evaluation of the performance of VLMs in spatial, relational, and\nabstract reasoning capabilities. We benchmark 11 mainstream VLMs (including\nclosed-source and open-source models) and reveal significant limitations of\ncurrent models. Based on these findings, we propose a dual optimization\nstrategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability\nof reasoning by decomposing layers, and ReasonTune enhances the task\nadaptability of model reasoning through training, all of which improves VLM\nperformance by 33.5\\%. All experimental data and code are in the repository:\nhttps://huggingface.co/datasets/cistine/ReasonBench.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u6ce8\u4e8e\u7ed3\u6784\u5316\u56fe\u5f62\u63a8\u7406\u4efb\u52a1\u7684\u8bc4\u4f30\u57fa\u51c6ReasonBench\uff0c\u6d4b\u8bd5\u4e8611\u4e2aVLMs\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5c40\u9650\u6027\u5e76\u901a\u8fc7\u53cc\u91cd\u4f18\u5316\u7b56\u7565\u63d0\u534733.5%\u3002", "motivation": "\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u7b80\u5355\u56fe\u5f62\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u63a8\u7406\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86ReasonBench\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b1,613\u4e2a\u6765\u81ea\u771f\u5b9e\u4e16\u754c\u667a\u529b\u6d4b\u8bd5\u7684\u95ee\u9898\uff0c\u8986\u76d6\u4f4d\u7f6e\u3001\u5c5e\u6027\u3001\u6570\u91cf\u548c\u591a\u5143\u7d20\u4efb\u52a1\u7b49\u63a8\u7406\u7ef4\u5ea6\u3002\u5e76\u63d0\u51fa\u4e86\u53cc\u91cd\u4f18\u5316\u7b56\u7565\uff1aDiagrammatic Reasoning Chain\uff08DiaCoT\uff09\u548cReasonTune\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u4e8611\u4e2a\u4e3b\u6d41VLMs\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u663e\u8457\u5c40\u9650\u6027\u3002\u53cc\u91cd\u4f18\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u5f53\u524d\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u4f46\u901a\u8fc7\u63d0\u51fa\u7684\u53cc\u91cd\u4f18\u5316\u7b56\u7565\uff08DiaCoT\u548cReasonTune\uff09\uff0c\u6a21\u578b\u6027\u80fd\u63d0\u5347\u4e8633.5%\u3002"}}
{"id": "2508.00403", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.00403", "abs": "https://arxiv.org/abs/2508.00403", "authors": ["Rongsheng Zhang", "Ruichen Zhang", "Yang Lu", "Wei Chen", "Bo Ai", "Dusit Niyato"], "title": "Mamba for Wireless Communications and Networking: Principles and Opportunities", "comment": null, "summary": "Mamba has emerged as a powerful model for efficiently addressing tasks\ninvolving temporal and spatial data. Regarding the escalating heterogeneity and\ndynamics in wireless networks, Mamba holds the potential to revolutionize\nwireless communication and networking designs by balancing the trade-off\nbetween computational efficiency and effectiveness. This article presents a\ncomprehensive overview of Mamba' applications in wireless systems.\nSpecifically, we first analyze the potentials of Mamba for wireless signal\nprocessing tasks from the perspectives of long-range dependency modeling and\nspatial feature extraction. Then we propose two application frameworks for\nMamba in wireless communications, i.e., replacement of traditional algorithms,\nand enabler of novel paradigms. Guided by the two frameworks, we conduct case\nstudies on intelligent resource allocation and joint source and channel\ndecoding to demonstrate Mamba's improvements in both feature enhancement and\ncomputational efficiency. Finally, we highlight critical challenges and outline\npotential research directions for Mamba in wireless communications and\nnetworking.", "AI": {"tldr": "Mamba\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u901a\u8fc7\u4e24\u79cd\u5e94\u7528\u6846\u67b6\u548c\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5728\u7279\u5f81\u589e\u5f3a\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u4f18\u52bf\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u65e0\u7ebf\u7f51\u7edc\u7684\u5f02\u6784\u6027\u548c\u52a8\u6001\u6027\u589e\u52a0\uff0cMamba\u5728\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u6548\u679c\u65b9\u9762\u7684\u6f5c\u529b\u5f15\u53d1\u4e86\u7814\u7a76\u5174\u8da3\u3002", "method": "\u6587\u7ae0\u9996\u5148\u5206\u6790\u4e86Mamba\u5728\u65e0\u7ebf\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u7136\u540e\u63d0\u51fa\u4e86\u4e24\u79cd\u5e94\u7528\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u6027\u80fd\u63d0\u5347\u3002", "result": "Mamba\u5728\u7279\u5f81\u589e\u5f3a\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5e94\u7528\u6548\u679c\u3002", "conclusion": "\u6587\u7ae0\u5f3a\u8c03\u4e86Mamba\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2508.00697", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00697", "abs": "https://arxiv.org/abs/2508.00697", "authors": ["Yiming Wu", "Huan Wang", "Zhenghao Chen", "Jianxin Pang", "Dong Xu"], "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation", "comment": "ICCV 2025", "summary": "Diffusion Policies have significantly advanced robotic manipulation tasks via\nimitation learning, but their application on resource-constrained mobile\nplatforms remains challenging due to computational inefficiency and extensive\nmemory footprint. In this paper, we propose LightDP, a novel framework\nspecifically designed to accelerate Diffusion Policies for real-time deployment\non mobile devices. LightDP addresses the computational bottleneck through two\ncore strategies: network compression of the denoising modules and reduction of\nthe required sampling steps. We first conduct an extensive computational\nanalysis on existing Diffusion Policy architectures, identifying the denoising\nnetwork as the primary contributor to latency. To overcome performance\ndegradation typically associated with conventional pruning methods, we\nintroduce a unified pruning and retraining pipeline, optimizing the model's\npost-pruning recoverability explicitly. Furthermore, we combine pruning\ntechniques with consistency distillation to effectively reduce sampling steps\nwhile maintaining action prediction accuracy. Experimental evaluations on the\nstandard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that\nLightDP achieves real-time action prediction on mobile devices with competitive\nperformance, marking an important step toward practical deployment of\ndiffusion-based policies in resource-limited environments. Extensive real-world\nexperiments also show the proposed LightDP can achieve performance comparable\nto state-of-the-art Diffusion Policies.", "AI": {"tldr": "LightDP \u662f\u4e00\u79cd\u4e13\u4e3a\u79fb\u52a8\u8bbe\u5907\u8bbe\u8ba1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u53bb\u566a\u6a21\u5757\u548c\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u52a0\u901f\u6269\u6563\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u52a8\u4f5c\u9884\u6d4b\u4e14\u6027\u80fd\u4e0d\u900a\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u5185\u5b58\u5360\u7528\u5927\uff0c\u5176\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u5e73\u53f0\u4e0a\u7684\u5e94\u7528\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "LightDP \u901a\u8fc7\u4e24\u79cd\u6838\u5fc3\u7b56\u7565\u52a0\u901f\u6269\u6563\u7b56\u7565\uff1a\u5bf9\u53bb\u566a\u6a21\u5757\u8fdb\u884c\u7f51\u7edc\u538b\u7f29\u548c\u51cf\u5c11\u6240\u9700\u7684\u91c7\u6837\u6b65\u9aa4\u3002\u9996\u5148\u5bf9\u73b0\u6709\u6269\u6563\u7b56\u7565\u67b6\u6784\u8fdb\u884c\u5e7f\u6cdb\u8ba1\u7b97\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u5ef6\u8fdf\u7684\u4e3b\u8981\u6765\u6e90\u662f\u53bb\u566a\u7f51\u7edc\u3002\u968f\u540e\u5f15\u5165\u7edf\u4e00\u7684\u526a\u679d\u548c\u91cd\u65b0\u8bad\u7ec3\u6d41\u7a0b\uff0c\u660e\u786e\u4f18\u5316\u6a21\u578b\u526a\u679d\u540e\u7684\u6062\u590d\u80fd\u529b\uff0c\u5e76\u7ed3\u5408\u4e00\u81f4\u6027\u84b8\u998f\u6280\u672f\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cLightDP \u5728\u6807\u51c6\u6570\u636e\u96c6\uff08\u5982 PushT\u3001Robomimic\u3001CALVIN \u548c LIBERO\uff09\u4e0a\u5b9e\u73b0\u4e86\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u52a8\u4f5c\u9884\u6d4b\uff0c\u4e14\u6027\u80fd\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "LightDP \u6210\u529f\u5b9e\u73b0\u4e86\u5728\u8d44\u6e90\u6709\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u65f6\u90e8\u7f72\u6269\u6563\u7b56\u7565\uff0c\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u6269\u6563\u7b56\u7565\u76f8\u5f53\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.00546", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00546", "abs": "https://arxiv.org/abs/2508.00546", "authors": ["Wenchao Gu", "Zongyi Lyu", "Yanlin Wang", "Hongyu Zhang", "Cuiyun Gao", "Michael R. Lyu"], "title": "SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval", "comment": null, "summary": "Code retrieval aims to provide users with desired code snippets based on\nusers' natural language queries. With the development of deep learning\ntechnologies, adopting pre-trained models for this task has become mainstream.\nConsidering the retrieval efficiency, most of the previous approaches adopt a\ndual-encoder for this task, which encodes the description and code snippet into\nrepresentation vectors, respectively. However, the model structure of the\ndual-encoder tends to limit the model's performance, since it lacks the\ninteraction between the code snippet and description at the bottom layer of the\nmodel during training. To improve the model's effectiveness while preserving\nits efficiency, we propose a framework, which adopts Self-AdaPtive Model\nDistillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts\nthe dual-encoder to narrow the search space and then adopts the cross-encoder\nto improve accuracy. To improve the efficiency of SPENCER, we propose a novel\nmodel distillation technique, which can greatly reduce the inference time of\nthe dual-encoder while maintaining the overall performance. We also propose a\nteaching assistant selection strategy for our model distillation, which can\nadaptively select the suitable teaching assistant models for different\npre-trained models during the model distillation to ensure the model\nperformance. Extensive experiments demonstrate that the combination of\ndual-encoder and cross-encoder improves overall performance compared to solely\ndual-encoder-based models for code retrieval. Besides, our model distillation\ntechnique retains over 98% of the overall performance while reducing the\ninference time of the dual-encoder by 70%.", "AI": {"tldr": "SPENCER\u6846\u67b6\u7ed3\u5408\u53cc\u7f16\u7801\u5668\u548c\u4ea4\u53c9\u7f16\u7801\u5668\u63d0\u5347\u4ee3\u7801\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u84b8\u998f\u6280\u672f\u51cf\u5c1170%\u63a8\u7406\u65f6\u95f4\uff0c\u4fdd\u630198%\u6027\u80fd\u3002", "motivation": "\u53cc\u7f16\u7801\u5668\u7f3a\u4e4f\u5e95\u5c42\u4ee3\u7801\u7247\u6bb5\u548c\u63cf\u8ff0\u7684\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u9700\u8981\u63d0\u5347\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u6548\u679c\u3002", "method": "\u63d0\u51faSPENCER\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u7f16\u7801\u5668\u548c\u4ea4\u53c9\u7f16\u7801\u5668\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u7684\u6a21\u578b\u84b8\u998f\u6280\u672f\u548c\u52a9\u6559\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u53cc\u7f16\u7801\u5668\u4e0e\u4ea4\u53c9\u7f16\u7801\u5668\u7ed3\u5408\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6a21\u578b\u84b8\u998f\u6280\u672f\u51cf\u5c1170%\u63a8\u7406\u65f6\u95f4\u4e14\u4fdd\u755998%\u4ee5\u4e0a\u6027\u80fd\u3002", "conclusion": "SPENCER\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u53cc\u7f16\u7801\u5668\u548c\u4ea4\u53c9\u7f16\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u68c0\u7d22\u7684\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u578b\u84b8\u998f\u6280\u672f\uff0c\u5927\u5e45\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\uff0c\u4fdd\u6301\u4e8698%\u4ee5\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00248", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00248", "abs": "https://arxiv.org/abs/2508.00248", "authors": ["Chenggang Guo", "Hao Xu", "XianMing Wan"], "title": "Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network", "comment": null, "summary": "Depth map super-resolution technology aims to improve the spatial resolution\nof low-resolution depth maps and effectively restore high-frequency detail\ninformation. Traditional convolutional neural network has limitations in\ndealing with long-range dependencies and are unable to fully model the global\ncontextual information in depth maps. Although transformer can model global\ndependencies, its computational complexity and memory consumption are\nquadratic, which significantly limits its ability to process high-resolution\ndepth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba\n(MSF-UM) model, a novel guided depth map super-resolution framework. The core\ninnovation of this model is to integrate Mamba's efficient state-space modeling\ncapabilities into a multi-scale U-shaped fusion structure guided by a color\nimage. The structure combining the residual dense channel attention block and\nthe Mamba state space module is designed, which combines the local feature\nextraction capability of the convolutional layer with the modeling advantage of\nthe state space model for long-distance dependencies. At the same time, the\nmodel adopts a multi-scale cross-modal fusion strategy to make full use of the\nhigh-frequency texture information from the color image to guide the\nsuper-resolution process of the depth map. Compared with existing mainstream\nmethods, the proposed MSF-UM significantly reduces the number of model\nparameters while achieving better reconstruction accuracy. Extensive\nexperiments on multiple publicly available datasets validate the effectiveness\nof the model, especially showing excellent generalization ability in the task\nof large-scale depth map super-resolution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u878d\u5408U\u5f62Mamba\u6a21\u578b\uff08MSF-UM\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u5c42\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6df1\u5ea6\u56fe\u8d85\u5206\u8fa8\u7387\u7684\u91cd\u5efa\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800cTransformer\u867d\u80fd\u5efa\u6a21\u5168\u5c40\u4f9d\u8d56\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u6d88\u8017\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u56fe\u7684\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u6b8b\u5dee\u5bc6\u96c6\u901a\u9053\u6ce8\u610f\u529b\u5757\u548cMamba\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\u7684\u591a\u5c3a\u5ea6U\u5f62\u878d\u5408\u7ed3\u6784\uff0c\u878d\u5408\u4e86\u5377\u79ef\u5c42\u7684\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5bf9\u957f\u8ddd\u79bb\u4f9d\u8d56\u7684\u5efa\u6a21\u4f18\u52bf\uff0c\u5e76\u91c7\u7528\u591a\u5c3a\u5ea6\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\u5229\u7528\u5f69\u8272\u56fe\u50cf\u7684\u9ad8\u9891\u7eb9\u7406\u4fe1\u606f\u6307\u5bfc\u6df1\u5ea6\u56fe\u8d85\u5206\u8fa8\u7387\u8fc7\u7a0b\u3002", "result": "\u4e0e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u76f8\u6bd4\uff0cMSF-UM\u6a21\u578b\u5728\u663e\u8457\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u91cd\u5efa\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684MSF-UM\u6a21\u578b\u5728\u51cf\u5c11\u53c2\u6570\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6df1\u5ea6\u56fe\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.00324", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00324", "abs": "https://arxiv.org/abs/2508.00324", "authors": ["Yeonjun In", "Wonjoong Kim", "Sangwu Park", "Chanyoung Park"], "title": "R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge", "comment": "under review", "summary": "Although large reasoning models (LRMs) have demonstrated impressive\ncapabilities on complex tasks, recent studies reveal that these models\nfrequently fulfill harmful user instructions, raising significant safety\nconcerns. In this paper, we investigate the underlying cause of LRM safety\nrisks and find that models already possess sufficient safety knowledge but fail\nto activate it during reasoning. Based on this insight, we propose R1-Act, a\nsimple and efficient post-training method that explicitly triggers safety\nknowledge through a structured reasoning process. R1-Act achieves strong safety\nimprovements while preserving reasoning performance, outperforming prior\nalignment methods. Notably, it requires only 1,000 training examples and 90\nminutes of training on a single RTX A6000 GPU. Extensive experiments across\nmultiple LRM backbones and sizes demonstrate the robustness, scalability, and\npractical efficiency of our approach.", "AI": {"tldr": "R1-Act\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u663e\u5f0f\u89e6\u53d1\u6a21\u578b\u5b89\u5168\u77e5\u8bc6\uff0c\u9ad8\u6548\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5b83\u4eec\u5bb9\u6613\u6267\u884c\u6709\u5bb3\u6307\u4ee4\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002\u6a21\u578b\u5df2\u5177\u5907\u8db3\u591f\u7684\u5b89\u5168\u77e5\u8bc6\uff0c\u4f46\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u672a\u80fd\u6709\u6548\u6fc0\u6d3b\u3002", "method": "\u63d0\u51fa\u4e86R1-Act\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u663e\u5f0f\u89e6\u53d1\u6a21\u578b\u5df2\u6709\u7684\u5b89\u5168\u77e5\u8bc6\u3002", "result": "R1-Act\u5728\u4ec5\u97001,000\u4e2a\u8bad\u7ec3\u6837\u672c\u548c90\u5206\u949f\u8bad\u7ec3\u65f6\u95f4\uff08\u5355\u5f20RTX A6000 GPU\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5b89\u5168\u6027\uff0c\u4e14\u4e0d\u5f71\u54cd\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "R1-Act\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u663e\u5f0f\u89e6\u53d1\u5b89\u5168\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002"}}
{"id": "2508.00583", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.00583", "abs": "https://arxiv.org/abs/2508.00583", "authors": ["Yunting Xu", "Jiacheng Wang", "Ruichen Zhang", "Dusit Niyato", "Deepu Rajan", "Liang Yu", "Haibo Zhou", "Abbas Jamalipour", "Xianbin Wang"], "title": "Enhancing Wireless Networks for IoT with Large Vision Models: Foundations and Applications", "comment": "7 pages, 6 figures", "summary": "Large vision models (LVMs) have emerged as a foundational paradigm in visual\nintelligence, achieving state-of-the-art performance across diverse visual\ntasks. Recent advances in LVMs have facilitated their integration into Internet\nof Things (IoT) scenarios, offering superior generalization and adaptability\nfor vision-assisted network optimization. In this paper, we first investigate\nthe functionalities and core architectures of LVMs, highlighting their\ncapabilities across classification, segmentation, generation, and multimodal\nvisual processing. We then explore a variety of LVM applications in wireless\ncommunications, covering representative tasks across the physical layer,\nnetwork layer, and application layer. Furthermore, given the substantial model\nsize of LVMs and the challenges of model retraining in wireless domains, we\npropose a progressive fine-tuning framework that incrementally adapts\npretrained LVMs for joint optimization of multiple IoT tasks. A case study in\nlow-altitude economy networks (LAENets) demonstrates the effectiveness of the\nproposed framework over conventional CNNs in joint beamforming and positioning\ntasks for Internet of drones, underscoring a promising direction for\nintegrating LVMs into intelligent wireless systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86LVMs\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u5fae\u8c03\u6846\u67b6\uff0c\u5e76\u5728\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "LVMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u7684\u5e94\u7528\u9762\u4e34\u6a21\u578b\u89c4\u6a21\u5927\u548c\u91cd\u8bad\u7ec3\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u5fae\u8c03\u6846\u67b6\uff0c\u9010\u6b65\u8c03\u6574\u9884\u8bad\u7ec3\u7684LVMs\u4ee5\u9002\u5e94\u65e0\u7ebf\u9886\u57df\u7684\u591a\u4efb\u52a1\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u5728\u8054\u5408\u6ce2\u675f\u6210\u5f62\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4f20\u7edfCNNs\u3002", "conclusion": "\u5927\u89c6\u89c9\u6a21\u578b\uff08LVMs\uff09\u5728\u667a\u80fd\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u8054\u5408\u4f18\u5316\u591a\u4efb\u52a1\u65b9\u9762\uff0c\u5982\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u4e2d\u7684\u6ce2\u675f\u6210\u5f62\u548c\u5b9a\u4f4d\u4efb\u52a1\u3002"}}
{"id": "2508.00795", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00795", "abs": "https://arxiv.org/abs/2508.00795", "authors": ["Junbang Liang", "Pavel Tokmakov", "Ruoshi Liu", "Sruthi Sudhakar", "Paarth Shah", "Rares Ambrus", "Carl Vondrick"], "title": "Video Generators are Robot Policies", "comment": null, "summary": "Despite tremendous progress in dexterous manipulation, current visuomotor\npolicies remain fundamentally limited by two challenges: they struggle to\ngeneralize under perceptual or behavioral distribution shifts, and their\nperformance is constrained by the size of human demonstration data. In this\npaper, we use video generation as a proxy for robot policy learning to address\nboth limitations simultaneously. We propose Video Policy, a modular framework\nthat combines video and action generation that can be trained end-to-end. Our\nresults demonstrate that learning to generate videos of robot behavior allows\nfor the extraction of policies with minimal demonstration data, significantly\nimproving robustness and sample efficiency. Our method shows strong\ngeneralization to unseen objects, backgrounds, and tasks, both in simulation\nand the real world. We further highlight that task success is closely tied to\nthe generated video, with action-free video data providing critical benefits\nfor generalizing to novel tasks. By leveraging large-scale video generative\nmodels, we achieve superior performance compared to traditional behavior\ncloning, paving the way for more scalable and data-efficient robot policy\nlearning.", "AI": {"tldr": "\u5229\u7528\u89c6\u9891\u751f\u6210\u4f5c\u4e3a\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u7684\u4ee3\u7406\uff0c\u63d0\u51faVideo Policy\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u6837\u672c\u6548\u7387\uff0c\u5e76\u5c55\u793a\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u611f\u77e5\u6216\u884c\u4e3a\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e14\u6027\u80fd\u53d7\u9650\u4e8e\u4eba\u7c7b\u793a\u8303\u6570\u636e\u7684\u89c4\u6a21\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Video Policy\uff0c\u4e00\u4e2a\u7ed3\u5408\u89c6\u9891\u548c\u52a8\u4f5c\u751f\u6210\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u53ef\u4ee5\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u5c55\u793a\u4e86\u5bf9\u672a\u89c1\u7269\u4f53\u3001\u80cc\u666f\u548c\u4efb\u52a1\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u4efb\u52a1\u6210\u529f\u4e0e\u751f\u6210\u7684\u89c6\u9891\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u884c\u4e3a\u514b\u9686\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u4e3a\u66f4\u53ef\u6269\u5c55\u548c\u6570\u636e\u9ad8\u6548\u7684\u7b56\u7565\u5b66\u4e60\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.00593", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00593", "abs": "https://arxiv.org/abs/2508.00593", "authors": ["Shuyao Jiang", "Jiazhen Gu", "Wujie Zheng", "Yangfan Zhou", "Michael R. Lyu"], "title": "Can User Feedback Help Issue Detection? An Empirical Study on a One-billion-user Online Service System", "comment": "Accepted by the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "summary": "Background: It has long been suggested that user feedback, typically written\nin natural language by end-users, can help issue detection. However, for\nlarge-scale online service systems that receive a tremendous amount of\nfeedback, it remains a challenging task to identify severe issues from user\nfeedback. Aims: To develop a better feedback-based issue detection approach, it\nis crucial first to gain a comprehensive understanding of the characteristics\nof user feedback in real production systems. Method: In this paper, we conduct\nan empirical study on 50,378,766 user feedback items from six real-world\nservices in a one-billion-user online service system. We first study what users\nprovide in their feedback. We then examine whether certain features of feedback\nitems can be good indicators of severe issues. Finally, we investigate whether\nadopting machine learning techniques to analyze user feedback is reasonable.\nResults: Our results show that a large proportion of user feedback provides\nirrelevant information about system issues. As a result, it is crucial to\nfilter out issue-irrelevant information when processing user feedback.\nMoreover, we find severe issues that cannot be easily detected based solely on\nuser feedback characteristics. Finally, we find that the distributions of the\nfeedback topics in different time intervals are similar. This confirms that\ndesigning machine learning-based approaches is a viable direction for better\nanalyzing user feedback. Conclusions: We consider that our findings can serve\nas an empirical foundation for feedback-based issue detection in large-scale\nservice systems, which sheds light on the design and implementation of\npractical issue detection approaches.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\u7528\u6237\u53cd\u9988\u4e2d\u5b58\u5728\u5927\u91cf\u65e0\u5173\u4fe1\u606f\uff0c\u4ec5\u4f9d\u8d56\u53cd\u9988\u7279\u5f81\u96be\u4ee5\u68c0\u6d4b\u4e25\u91cd\u95ee\u9898\uff0c\u4f46\u53cd\u9988\u4e3b\u9898\u5206\u5e03\u76f8\u4f3c\u6027\u652f\u6301\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u5e94\u7528\uff0c\u4e3a\u5927\u89c4\u6a21\u7cfb\u7edf\u95ee\u9898\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u66f4\u597d\u7684\u57fa\u4e8e\u53cd\u9988\u7684\u95ee\u9898\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9996\u5148\u9700\u8981\u5168\u9762\u4e86\u89e3\u771f\u5b9e\u751f\u4ea7\u7cfb\u7edf\u4e2d\u7528\u6237\u53cd\u9988\u7684\u7279\u70b9\u3002", "method": "\u672c\u6587\u5bf9\u6765\u81ea\u4e00\u4e2a\u62e5\u6709\u5341\u4ebf\u7528\u6237\u7684\u5728\u7ebf\u670d\u52a1\u7cfb\u7edf\u4e2d\u516d\u4e2a\u771f\u5b9e\u670d\u52a1\u768450,378,766\u6761\u7528\u6237\u53cd\u9988\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\u3002\u9996\u5148\u7814\u7a76\u4e86\u7528\u6237\u5728\u53cd\u9988\u4e2d\u63d0\u4f9b\u7684\u5185\u5bb9\uff0c\u7136\u540e\u68c0\u9a8c\u4e86\u53cd\u9988\u9879\u7684\u67d0\u4e9b\u7279\u5f81\u662f\u5426\u53ef\u4ee5\u4f5c\u4e3a\u4e25\u91cd\u95ee\u9898\u7684\u826f\u597d\u6307\u6807\uff0c\u6700\u540e\u63a2\u8ba8\u4e86\u91c7\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u5206\u6790\u7528\u6237\u53cd\u9988\u7684\u5408\u7406\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u91cf\u7528\u6237\u53cd\u9988\u63d0\u4f9b\u4e86\u4e0e\u7cfb\u7edf\u95ee\u9898\u65e0\u5173\u7684\u4fe1\u606f\uff0c\u56e0\u6b64\u5904\u7406\u7528\u6237\u53cd\u9988\u65f6\u8fc7\u6ee4\u6389\u4e0e\u95ee\u9898\u65e0\u5173\u7684\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002\u6b64\u5916\uff0c\u7814\u7a76\u53d1\u73b0\u4ec5\u57fa\u4e8e\u7528\u6237\u53cd\u9988\u7279\u5f81\u96be\u4ee5\u8f7b\u6613\u68c0\u6d4b\u5230\u4e25\u91cd\u95ee\u9898\u3002\u6700\u540e\uff0c\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u65f6\u95f4\u95f4\u9694\u5185\u53cd\u9988\u4e3b\u9898\u7684\u5206\u5e03\u76f8\u4f3c\uff0c\u8fd9\u8bc1\u5b9e\u4e86\u8bbe\u8ba1\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u662f\u66f4\u597d\u5206\u6790\u7528\u6237\u53cd\u9988\u7684\u53ef\u884c\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u7684\u53d1\u73b0\u53ef\u4e3a\u5927\u89c4\u6a21\u670d\u52a1\u7cfb\u7edf\u4e2d\u57fa\u4e8e\u53cd\u9988\u7684\u95ee\u9898\u68c0\u6d4b\u63d0\u4f9b\u5b9e\u8bc1\u57fa\u7840\uff0c\u4e3a\u5b9e\u7528\u95ee\u9898\u68c0\u6d4b\u65b9\u6cd5\u7684\u8bbe\u8ba1\u548c\u5b9e\u73b0\u63d0\u4f9b\u542f\u793a\u3002"}}
{"id": "2508.00259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00259", "abs": "https://arxiv.org/abs/2508.00259", "authors": ["Wentao Sun", "Hanqing Xu", "Quanyun Wu", "Dedong Zhang", "Yiping Chen", "Lingfei Ma", "John S. Zelek", "Jonathan Li"], "title": "PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting", "comment": "22 pages, 9 figures", "summary": "We introduce PointGauss, a novel point cloud-guided framework for real-time\nmulti-object segmentation in Gaussian Splatting representations. Unlike\nexisting methods that suffer from prolonged initialization and limited\nmulti-view consistency, our approach achieves efficient 3D segmentation by\ndirectly parsing Gaussian primitives through a point cloud segmentation-driven\npipeline. The key innovation lies in two aspects: (1) a point cloud-based\nGaussian primitive decoder that generates 3D instance masks within 1 minute,\nand (2) a GPU-accelerated 2D mask rendering system that ensures multi-view\nconsistency. Extensive experiments demonstrate significant improvements over\nprevious state-of-the-art methods, achieving performance gains of 1.89 to\n31.78% in multi-view mIoU, while maintaining superior computational efficiency.\nTo address the limitations of current benchmarks (single-object focus,\ninconsistent 3D evaluation, small scale, and partial coverage), we present\nDesktopObjects-360, a novel comprehensive dataset for 3D segmentation in\nradiance fields, featuring: (1) complex multi-object scenes, (2) globally\nconsistent 2D annotations, (3) large-scale training data (over 27 thousand 2D\nmasks), (4) full 360{\\deg} coverage, and (5) 3D evaluation masks.", "AI": {"tldr": "PointGauss \u662f\u4e00\u79cd\u5b9e\u65f6\u591a\u76ee\u6807\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u4e91\u5f15\u5bfc\u548c\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u63d0\u5347\u5206\u5272\u6548\u7387\u4e0e\u4e00\u81f4\u6027\uff0c\u5e76\u5f15\u5165\u65b0\u6570\u636e\u96c6 DesktopObjects-360 \u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u521d\u59cb\u5316\u65f6\u95f4\u957f\u3001\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u4e00\u81f4\u7684 3D \u5206\u5272\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u70b9\u4e91\u5206\u5272\u9a71\u52a8\u7684\u6d41\u6c34\u7ebf\u76f4\u63a5\u89e3\u6790\u9ad8\u65af\u57fa\u5143\uff0c\u5305\u62ec\u70b9\u4e91\u9ad8\u65af\u57fa\u5143\u89e3\u7801\u5668\u548c GPU \u52a0\u901f\u7684 2D \u63a9\u7801\u6e32\u67d3\u7cfb\u7edf\u3002", "result": "\u5728\u591a\u89c6\u89d2 mIoU \u4e0a\u5b9e\u73b0\u4e86 1.89% \u81f3 31.78% \u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "PointGauss \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u4e91\u5f15\u5bfc\u7684\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u5b9e\u65f6\u591a\u76ee\u6807\u5206\u5272\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u65b0\u6570\u636e\u96c6 DesktopObjects-360 \u89e3\u51b3\u4e86\u5f53\u524d\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.00378", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00378", "abs": "https://arxiv.org/abs/2508.00378", "authors": ["Shixin Yi", "Lin Shang"], "title": "CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding", "comment": "Preparing for AAAI 2026, Multimodal Reasoning", "summary": "Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in\nvision-language models (VLMs), but it often produces explanations that are\nlinguistically fluent yet lack grounding in visual content. We observe that\nsuch hallucinations arise in part from the absence of an explicit verification\nmechanism during multi-step reasoning. To address this, we propose\n\\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with\n\\textbf{G}rounded \\textbf{I}nsights), a modular framework that introduces\nvisual verification into the reasoning process. CoRGI follows a three-stage\npipeline: it first generates a textual reasoning chain, then extracts\nsupporting visual evidence for each reasoning step via a dedicated module\n(VEVM), and finally synthesizes the textual rationale with visual evidence to\ngenerate a grounded, verified answer. The framework can be integrated with\nexisting VLMs without end-to-end retraining. We evaluate CoRGI on the VCR\nbenchmark and find that it improves reasoning performance on two representative\nopen-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm\nthe contribution of each step in the verification module, and human evaluations\nsuggest that CoRGI leads to more factual and helpful explanations. We also\nexamine alternative designs for the visual verification step and discuss\npotential limitations of post-hoc verification frameworks. These findings\nhighlight the importance of grounding intermediate reasoning steps in visual\nevidence to enhance the robustness of multimodal reasoning.", "AI": {"tldr": "CoRGI\u662f\u4e00\u4e2a\u5f15\u5165\u89c6\u89c9\u9a8c\u8bc1\u7684\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\u63d0\u5347\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u89e3\u91ca\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684Chain-of-Thought\u63d0\u793a\u65b9\u6cd5\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5e38\u4ea7\u751f\u7f3a\u4e4f\u89c6\u89c9\u5185\u5bb9\u57fa\u7840\u7684\u5e7b\u89c9\u89e3\u91ca\uff0c\u9700\u5f15\u5165\u663e\u5f0f\u9a8c\u8bc1\u673a\u5236\u3002", "method": "CoRGI\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u751f\u6210\u6587\u672c\u63a8\u7406\u94fe\u3001\u901a\u8fc7\u4e13\u7528\u6a21\u5757\uff08VEVM\uff09\u63d0\u53d6\u89c6\u89c9\u8bc1\u636e\u3001\u7ed3\u5408\u6587\u672c\u63a8\u7406\u4e0e\u89c6\u89c9\u8bc1\u636e\u751f\u6210\u9a8c\u8bc1\u7b54\u6848\u3002", "result": "\u5728VCR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoRGI\u63d0\u5347\u4e86Qwen-2.5VL\u548cLLaVA-1.6\u7b49VLM\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u6b65\u9aa4\u7684\u8d21\u732e\u3002", "conclusion": "CoRGI\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u9a8c\u8bc1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u751f\u6210\u4e86\u66f4\u4e8b\u5b9e\u6027\u548c\u6709\u5e2e\u52a9\u7684\u89e3\u91ca\u3002"}}
{"id": "2508.00616", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.00616", "abs": "https://arxiv.org/abs/2508.00616", "authors": ["Mingzhe Fan", "Geng Sun", "Hongyang Pan", "Jiacheng Wang", "Jiancheng An", "Hongyang Du", "Chau Yuen"], "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked Intelligent Metasurfaces-assisted Communications", "comment": "This papar has been submitted to the IEEE Global Communications\n  Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488", "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faUAV-SIMs\u8f85\u52a9\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u89e3\u548c\u4ea4\u66ff\u4f18\u5316\u89e3\u51b3\u975e\u51f8\u95ee\u9898\uff0c\u63d0\u5347\u7f51\u7edc\u5bb9\u91cf\u3002", "motivation": "\u56fa\u5b9a\u5f0fSIM\u9650\u5236\u4e86\u7cfb\u7edf\u901a\u4fe1\u6027\u80fd\uff0c\u800c\u79fb\u52a8\u5f0fSIM\uff08\u5982UAV-SIMs\uff09\u80fd\u7075\u6d3b\u90e8\u7f72\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5c06USBJOP\u5206\u89e3\u4e3a\u4e09\u4e2a\u5b50\u4f18\u5316\u95ee\u9898\uff08AUUOP\u3001ULOP\u3001USPSOP\uff09\uff0c\u5e76\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u7b56\u7565\u89e3\u51b3\u3002AUUOP\u548cULOP\u901a\u8fc7CVX\u5de5\u5177\u8f6c\u5316\u4e3a\u51f8\u5f62\u5f0f\u6c42\u89e3\uff0cUSPSOP\u91c7\u7528\u9010\u5c42\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u4f18\u5316UAV-SIMs\u4e0e\u7528\u6237\u7684\u5173\u8054\u3001UAV\u4f4d\u7f6e\u53ca\u76f8\u4f4d\u504f\u79fb\uff0c\u6700\u5927\u5316\u7f51\u7edc\u5bb9\u91cf\u3002", "conclusion": "\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b56\u7565\u5728\u4e0d\u540c\u4eff\u771f\u8bbe\u7f6e\u4e0b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.00630", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00630", "abs": "https://arxiv.org/abs/2508.00630", "authors": ["Khaled Ahmed", "Jialing Song", "Boqi Chen", "Ou Wei", "Bingzhou Zheng"], "title": "MCeT: Behavioral Model Correctness Evaluation using Large Language Models", "comment": "MODELS 2025", "summary": "Behavioral model diagrams, e.g., sequence diagrams, are an essential form of\ndocumentation that are typically designed by system engineers from requirements\ndocumentation, either fully manually or assisted by design tools. With the\ngrowing use of Large Language Models (LLM) as AI modeling assistants, more\nautomation will be involved in generating diagrams. This necessitates the\nadvancement of automatic model correctness evaluation tools. Such a tool can be\nused to evaluate both manually and AI automatically generated models; to\nprovide feedback to system engineers, and enable AI assistants to self-evaluate\nand self-enhance their generated models.\n  In this paper, we propose MCeT, the first fully automated tool to evaluate\nthe correctness of a behavioral model, sequence diagrams in particular, against\nits corresponding requirements text and produce a list of issues that the model\nhas. We utilize LLMs for the correctness evaluation tasks as they have shown\noutstanding natural language understanding ability. However, we show that\ndirectly asking an LLM to compare a diagram to requirements finds less than 35%\nof issues that experienced engineers can find. We propose to supplement the\ndirect check with a fine-grained, multi-perspective approach; we split the\ndiagram into atomic, non-divisible interactions, and split the requirements\ntext into atomic, self-contained items. We compare the diagram with atomic\nrequirements and each diagram-atom with the requirements. We also propose a\nself-consistency checking approach that combines perspectives to mitigate LLM\nhallucinated issues. Our combined approach improves upon the precision of the\ndirect approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,\nthe approach finds 90% more issues that the experienced engineers found than\nthe direct approach, and reports an average of 6 new issues per diagram.", "AI": {"tldr": "MCeT\u662f\u9996\u4e2a\u81ea\u52a8\u5316\u8bc4\u4f30\u884c\u4e3a\u6a21\u578b\uff08\u5e8f\u5217\u56fe\uff09\u6b63\u786e\u6027\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u591a\u89d2\u5ea6\u7ec6\u7c92\u5ea6\u68c0\u67e5\u548c\u81ea\u4e00\u81f4\u6027\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u95ee\u9898\u53d1\u73b0\u7387\u548c\u7cbe\u786e\u5ea6\u3002", "motivation": "\u968f\u7740LLMs\u5728\u6a21\u578b\u751f\u6210\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u8bc4\u4f30\u884c\u4e3a\u6a21\u578b\u7684\u6b63\u786e\u6027\uff0c\u4ee5\u652f\u6301\u5de5\u7a0b\u5e08\u548cAI\u52a9\u624b\u6539\u8fdb\u6a21\u578b\u8d28\u91cf\u3002", "method": "\u63d0\u51faMCeT\u5de5\u5177\uff0c\u5229\u7528LLMs\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u5c06\u5e8f\u5217\u56fe\u5206\u89e3\u4e3a\u539f\u5b50\u4ea4\u4e92\uff0c\u5e76\u4e0e\u9700\u6c42\u6587\u672c\u7684\u539f\u5b50\u9879\u8fdb\u884c\u591a\u89d2\u5ea6\u5bf9\u6bd4\uff0c\u540c\u65f6\u5f15\u5165\u81ea\u4e00\u81f4\u6027\u68c0\u67e5\u4ee5\u51cf\u5c11LLM\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "result": "MCeT\u7684\u8054\u5408\u65b9\u6cd5\u5c06\u76f4\u63a5\u68c0\u67e5\u7684\u7cbe\u786e\u5ea6\u4ece0.58\u63d0\u5347\u81f30.81\uff0c\u53d1\u73b0\u95ee\u9898\u7684\u6570\u91cf\u6bd4\u76f4\u63a5\u65b9\u6cd5\u591a90%\uff0c\u5e73\u5747\u6bcf\u5f20\u56fe\u62a5\u544a6\u4e2a\u65b0\u95ee\u9898\u3002", "conclusion": "MCeT\u901a\u8fc7\u591a\u89d2\u5ea6\u3001\u7ec6\u7c92\u5ea6\u7684\u68c0\u67e5\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u884c\u4e3a\u6a21\u578b\uff08\u5982\u5e8f\u5217\u56fe\uff09\u6b63\u786e\u6027\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u95ee\u9898\u53d1\u73b0\u7387\uff0c\u4e3a\u7cfb\u7edf\u5de5\u7a0b\u5e08\u548cAI\u52a9\u624b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53cd\u9988\u548c\u6539\u8fdb\u673a\u5236\u3002"}}
{"id": "2508.00260", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.00260", "abs": "https://arxiv.org/abs/2508.00260", "authors": ["Hyundong Jin", "Hyung Jin Chang", "Eunwoo Kim"], "title": "Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models", "comment": "Accepted to ICCV 2025", "summary": "Continual learning enables pre-trained generative vision-language models\n(VLMs) to incorporate knowledge from new tasks without retraining data from\nprevious ones. Recent methods update a visual projector to translate visual\ninformation for new tasks, connecting pre-trained vision encoders with large\nlanguage models. However, such adjustments may cause the models to prioritize\nvisual inputs over language instructions, particularly learning tasks with\nrepetitive types of textual instructions. To address the neglect of language\ninstructions, we propose a novel framework that grounds the translation of\nvisual information on instructions for language models. We introduce a mixture\nof visual projectors, each serving as a specialized visual-to-language\ntranslation expert based on the given instruction context to adapt to new\ntasks. To avoid using experts for irrelevant instruction contexts, we propose\nan expert recommendation strategy that reuses experts for tasks similar to\nthose previously learned. Additionally, we introduce expert pruning to\nalleviate interference from the use of experts that cumulatively activated in\nprevious tasks. Extensive experiments on diverse vision-language tasks\ndemonstrate that our method outperforms existing continual learning approaches\nby generating instruction-following responses.", "AI": {"tldr": "\u65b0\u6846\u67b6\u901a\u8fc7\u6df7\u5408\u89c6\u89c9\u6295\u5f71\u5668\u548c\u4e13\u5bb6\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u8bed\u8a00\u6307\u4ee4\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u503e\u5411\u4e8e\u5ffd\u89c6\u8bed\u8a00\u6307\u4ee4\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u91cd\u590d\u7c7b\u578b\u6587\u672c\u6307\u4ee4\u7684\u4efb\u52a1\u65f6\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u4e8e\u4f9d\u8d56\u89c6\u89c9\u8f93\u5165\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u6df7\u5408\u89c6\u89c9\u6295\u5f71\u5668\u6846\u67b6\uff0c\u6bcf\u4e2a\u6295\u5f71\u5668\u4f5c\u4e3a\u57fa\u4e8e\u6307\u4ee4\u4e0a\u4e0b\u6587\u7684\u89c6\u89c9\u5230\u8bed\u8a00\u7684\u7ffb\u8bd1\u4e13\u5bb6\uff0c\u5e76\u91c7\u7528\u4e13\u5bb6\u63a8\u8350\u7b56\u7565\u548c\u4e13\u5bb6\u526a\u679d\u6765\u4f18\u5316\u4efb\u52a1\u9002\u5e94\u6027\u548c\u51cf\u5c11\u5e72\u6270\u3002", "result": "\u5728\u591a\u6837\u5316\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u9075\u5faa\u6307\u4ee4\u7684\u54cd\u5e94\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u6295\u5f71\u5668\u6df7\u5408\u548c\u4e13\u5bb6\u63a8\u8350\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u8bed\u8a00\u6307\u4ee4\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u6837\u5316\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.00401", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.00401", "abs": "https://arxiv.org/abs/2508.00401", "authors": ["Riddhi J. Pitliya", "Ozan Catal", "Toon Van de Maele", "Corrado Pezzato", "Tim Verbelen"], "title": "Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation", "comment": null, "summary": "We present a novel approach to multi-agent cooperation by implementing theory\nof mind (ToM) within active inference. ToM - the ability to understand that\nothers can have differing knowledge and goals - enables agents to reason about\nothers' beliefs while planning their own actions. Unlike previous active\ninference approaches to multi-agent cooperation, our method neither relies on\ntask-specific shared generative models nor requires explicit communication,\nwhile being generalisable. In our framework, the ToM-equipped agent maintains\ndistinct representations of its own and others' beliefs and goals. We extend\nthe sophisticated inference tree-based planning algorithm to systematically\nexplore joint policy spaces through recursive reasoning. Our approach is\nevaluated through collision avoidance and foraging task simulations. Results\ndemonstrate that ToM-equipped agents cooperate better compared to non-ToM\ncounterparts by being able to avoid collisions and reduce redundant efforts.\nCrucially, ToM agents accomplish this by inferring others' beliefs solely from\nobservable behaviour. This work advances practical applications in artificial\nintelligence while providing computational insights into ToM.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eToM\u7684\u4e3b\u52a8\u63a8\u7406\u65b9\u6cd5\uff0c\u5b9e\u73b0\u65e0\u9700\u5171\u4eab\u6a21\u578b\u6216\u901a\u4fe1\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u907f\u78b0\u548c\u89c5\u98df\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u6cd5\u5e38\u4f9d\u8d56\u5171\u4eab\u6a21\u578b\u6216\u663e\u5f0f\u901a\u4fe1\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7ToM\u5b9e\u73b0\u66f4\u7075\u6d3b\u3001\u901a\u7528\u7684\u534f\u4f5c\uff0c\u4ec5\u901a\u8fc7\u89c2\u5bdf\u884c\u4e3a\u63a8\u65ad\u4ed6\u4eba\u4fe1\u5ff5\u3002", "method": "\u6211\u4eec\u6269\u5c55\u4e86\u57fa\u4e8e\u63a8\u7406\u6811\u7684\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u9012\u5f52\u63a8\u7406\u7cfb\u7edf\u63a2\u7d22\u8054\u5408\u7b56\u7565\u7a7a\u95f4\u3002\u667a\u80fd\u4f53\u7ef4\u62a4\u81ea\u8eab\u53ca\u4ed6\u4eba\u4fe1\u5ff5\u548c\u76ee\u6807\u7684\u72ec\u7acb\u8868\u5f81\uff0c\u65e0\u9700\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u7684\u5171\u4eab\u751f\u6210\u6a21\u578b\u6216\u663e\u5f0f\u901a\u4fe1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u914d\u5907ToM\u7684\u667a\u80fd\u4f53\u5728\u907f\u78b0\u548c\u89c5\u98df\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u975eToM\u667a\u80fd\u4f53\uff0c\u80fd\u6709\u6548\u907f\u514d\u78b0\u649e\u5e76\u51cf\u5c11\u5197\u4f59\u884c\u4e3a\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5c06\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u878d\u5165\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u95f4\u7684\u534f\u4f5c\u6548\u7387\uff0c\u8fd8\u4e3a\u4eba\u5de5\u667a\u80fd\u7684\u5b9e\u8df5\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u5e76\u6df1\u5316\u4e86\u5bf9ToM\u7684\u8ba1\u7b97\u7406\u89e3\u3002"}}
{"id": "2508.00629", "categories": ["cs.NI", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.00629", "abs": "https://arxiv.org/abs/2508.00629", "authors": ["Francisco Crespo", "Javier Villegas", "Carlos Baena", "Eduardo Baena", "Sergio Fortes", "Raquel Barco"], "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight Approach", "comment": null, "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7dApp\uff0c\u52a8\u6001\u534f\u8c03CPU\u8d44\u6e90\uff0c\u63d0\u9ad8O-RAN\u80fd\u6e90\u6548\u7387\u548c\u5229\u7528\u7387\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8f6f\u5316\u7684\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08RAN\uff09\u8f6c\u578b\u5f15\u5165\u4e86\u5728\u4e25\u683c\u5b9e\u65f6\u7ea6\u675f\u4e0b\u9ad8\u6548\u7ba1\u7406CPU\u8d44\u6e90\u7684\u65b0\u6311\u6218\uff0c\u73b0\u6709\u64cd\u4f5c\u7cfb\u7edf\u8c03\u5ea6\u5668\u4e0e\u5ef6\u8fdf\u654f\u611f\u7684RAN\u5de5\u4f5c\u8d1f\u8f7d\u4ea4\u4e92\u65f6\u6027\u80fd\u4e0d\u4f73\u4e14\u80fd\u8017\u9ad8\u3002", "method": "\u8be5dApp\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u5f62\u6210\u95ed\u73af\uff0c\u5229\u7528\u7ebf\u7a0b\u7ea7\u9065\u6d4b\uff08\u5982\u4e0a\u4e0b\u6587\u5207\u6362\u3001\u6bcf\u5468\u671f\u6307\u4ee4\u6570IPC\u548c\u7f13\u5b58\u6307\u6807\uff09\u5b9e\u65f6\u8c03\u6574CPU\u7ebf\u7a0b\u4eb2\u548c\u6027\u3001\u6838\u5fc3\u9694\u79bb\u548c\u9891\u7387\u7f29\u653e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u5728\u5546\u7528\u7ea7srsRAN\u90e8\u7f72\u4e2d\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u8282\u80fd\u6548\u679c\uff0c\u4e14\u4e0d\u5f71\u54cd\u5b9e\u65f6\u5904\u7406\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u7f16\u7a0b\u7684\u5206\u5e03\u5f0f\u5e94\u7528\uff08dApp\uff09\uff0c\u7528\u4e8e\u5728\u5206\u5e03\u5f0f\u5355\u5143\uff08DU\uff09\u7ea7\u522b\u52a8\u6001\u534f\u8c03CPU\u4f7f\u7528\uff0c\u65e0\u9700\u8bbf\u95ee\u4e13\u6709RAN\u8f6f\u4ef6\u6216\u786c\u4ef6\u7279\u5b9a\u529f\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u80fd\u6e90\u6548\u7387\u548cCPU\u5229\u7528\u7387\u3002"}}
{"id": "2508.00299", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00299", "abs": "https://arxiv.org/abs/2508.00299", "authors": ["Danzhen Fu", "Jiagao Hu", "Daiguo Zhou", "Fei Wang", "Zepeng Wang", "Wenhua Liao"], "title": "Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence", "comment": "ICCV 2025 Workshop (HiGen)", "summary": "Pedestrian detection models in autonomous driving systems often lack\nrobustness due to insufficient representation of dangerous pedestrian scenarios\nin training datasets. To address this limitation, we present a novel framework\nfor controllable pedestrian video editing in multi-view driving scenarios by\nintegrating video inpainting and human motion control techniques. Our approach\nbegins by identifying pedestrian regions of interest across multiple camera\nviews, expanding detection bounding boxes with a fixed ratio, and resizing and\nstitching these regions into a unified canvas while preserving cross-view\nspatial relationships. A binary mask is then applied to designate the editable\narea, within which pedestrian editing is guided by pose sequence control\nconditions. This enables flexible editing functionalities, including pedestrian\ninsertion, replacement, and removal. Extensive experiments demonstrate that our\nframework achieves high-quality pedestrian editing with strong visual realism,\nspatiotemporal coherence, and cross-view consistency. These results establish\nthe proposed method as a robust and versatile solution for multi-view\npedestrian video generation, with broad potential for applications in data\naugmentation and scenario simulation in autonomous driving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u884c\u4eba\u89c6\u9891\u7f16\u8f91\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u9891\u4fee\u590d\u548c\u8fd0\u52a8\u63a7\u5236\u6280\u672f\uff0c\u6709\u6548\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u884c\u4eba\u68c0\u6d4b\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u5371\u9669\u884c\u4eba\u573a\u666f\u8868\u793a\u4e0d\u8db3\u800c\u7f3a\u4e4f\u9c81\u68d2\u6027\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\u8bc6\u522b\u591a\u6444\u50cf\u5934\u89c6\u89d2\u4e2d\u7684\u884c\u4eba\u611f\u5174\u8da3\u533a\u57df\uff0c\u6269\u5c55\u68c0\u6d4b\u8fb9\u754c\u6846\u5e76\u4ee5\u56fa\u5b9a\u6bd4\u4f8b\u8c03\u6574\u5927\u5c0f\uff0c\u968f\u540e\u5c06\u8fd9\u4e9b\u533a\u57df\u62fc\u63a5\u6210\u7edf\u4e00\u753b\u5e03\u3002\u5e94\u7528\u4e8c\u8fdb\u5236\u63a9\u7801\u6307\u5b9a\u53ef\u7f16\u8f91\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u59ff\u52bf\u5e8f\u5217\u63a7\u5236\u6761\u4ef6\u6307\u5bfc\u884c\u4eba\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u884c\u4eba\u7f16\u8f91\uff0c\u5177\u6709\u5f3a\u89c6\u89c9\u771f\u5b9e\u611f\u3001\u65f6\u7a7a\u8fde\u8d2f\u6027\u548c\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u89c6\u9891\u4fee\u590d\u548c\u4eba\u4f53\u8fd0\u52a8\u63a7\u5236\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u591a\u89c6\u89d2\u9a7e\u9a76\u573a\u666f\u4e2d\u53ef\u63a7\u7684\u884c\u4eba\u89c6\u9891\u7f16\u8f91\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u6570\u636e\u589e\u5f3a\u548c\u573a\u666f\u6a21\u62df\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2508.00700", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00700", "abs": "https://arxiv.org/abs/2508.00700", "authors": ["Alfred Santa Molison", "Marcia Moraes", "Glaucia Melo", "Fabio Santos", "Wesley K. G. Assuncao"], "title": "Is LLM-Generated Code More Maintainable \\& Reliable than Human-Written Code?", "comment": "Accepted ESEM2025", "summary": "Background: The rise of Large Language Models (LLMs) in software development\nhas opened new possibilities for code generation. Despite the widespread use of\nthis technology, it remains unclear how well LLMs generate code solutions in\nterms of software quality and how they compare to human-written code. Aims:\nThis study compares the internal quality attributes of LLM-generated and\nhuman-written code. Method: Our empirical study integrates datasets of coding\ntasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and\nSonarQube to assess software quality. The dataset comprises Python code\nsolutions across three difficulty levels: introductory, interview, and\ncompetition. We analyzed key code quality metrics, including maintainability\nand reliability, and the estimated effort required to resolve code issues.\nResults: Our analysis shows that LLM-generated code has fewer bugs and requires\nless effort to fix them overall. Interestingly, fine-tuned models reduced the\nprevalence of high-severity issues, such as blocker and critical bugs, and\nshifted them to lower-severity categories, but decreased the model's\nperformance. In competition-level problems, the LLM solutions sometimes\nintroduce structural issues that are not present in human-written code.\nConclusion: Our findings provide valuable insights into the quality of\nLLM-generated code; however, the introduction of critical issues in more\ncomplex scenarios highlights the need for a systematic evaluation and\nvalidation of LLM solutions. Our work deepens the understanding of the\nstrengths and limitations of LLMs for code generation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u751f\u6210\u7684\u4ee3\u7801\u7f3a\u9677\u66f4\u5c11\u4e14\u4fee\u590d\u6210\u672c\u66f4\u4f4e\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e2d\u53ef\u80fd\u5f15\u5165\u5173\u952e\u95ee\u9898\uff0c\u9700\u7cfb\u7edf\u8bc4\u4f30\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u4f7f\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u751f\u6210\u7684\u4ee3\u7801\u89e3\u51b3\u65b9\u6848\u5728\u8f6f\u4ef6\u8d28\u91cf\u65b9\u9762\u7684\u8868\u73b0\u4ee5\u53ca\u4e0e\u4eba\u7f16\u5199\u4ee3\u7801\u7684\u6bd4\u8f83\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u5b9e\u8bc1\u7814\u7a76\u6574\u5408\u4e86\u7f16\u7801\u4efb\u52a1\u6570\u636e\u96c6\u3001\u4e09\u79cdLLM\u914d\u7f6e\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\uff09\u4ee5\u53caSonarQube\u6765\u8bc4\u4f30\u8f6f\u4ef6\u8d28\u91cf\u3002\u6570\u636e\u96c6\u5305\u542b\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\u7684Python\u4ee3\u7801\u89e3\u51b3\u65b9\u6848\uff1a\u5165\u95e8\u7ea7\u3001\u9762\u8bd5\u7ea7\u548c\u7ade\u8d5b\u7ea7\u3002\u5206\u6790\u4e86\u5305\u62ec\u53ef\u7ef4\u62a4\u6027\u548c\u53ef\u9760\u6027\u5728\u5185\u7684\u5173\u952e\u4ee3\u7801\u8d28\u91cf\u6307\u6807\uff0c\u4ee5\u53ca\u89e3\u51b3\u4ee3\u7801\u95ee\u9898\u6240\u9700\u7684\u9884\u4f30\u5de5\u4f5c\u91cf\u3002", "result": "\u5206\u6790\u663e\u793a\uff0cLLM\u751f\u6210\u7684\u4ee3\u7801\u603b\u4f53\u4e0a\u6709\u66f4\u5c11\u7684\u7f3a\u9677\u4e14\u4fee\u590d\u6240\u9700\u5de5\u4f5c\u91cf\u66f4\u5c11\u3002\u6709\u8da3\u7684\u662f\uff0c\u5fae\u8c03\u6a21\u578b\u51cf\u5c11\u4e86\u9ad8\u4e25\u91cd\u6027\u95ee\u9898\u7684\u53d1\u751f\u7387\uff0c\u5982\u963b\u585e\u6027\u548c\u5173\u952e\u6027\u7f3a\u9677\uff0c\u5e76\u5c06\u5176\u8f6c\u79fb\u5230\u8f83\u4f4e\u4e25\u91cd\u6027\u7c7b\u522b\uff0c\u4f46\u964d\u4f4e\u4e86\u6a21\u578b\u6027\u80fd\u3002\u5728\u7ade\u8d5b\u7ea7\u95ee\u9898\u4e2d\uff0cLLM\u89e3\u51b3\u65b9\u6848\u6709\u65f6\u4f1a\u5f15\u5165\u4eba\u7f16\u5199\u4ee3\u7801\u4e2d\u4e0d\u5b58\u5728\u7684\u7ed3\u6784\u6027\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u5173\u4e8eLLM\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u7684\u6709\u4ef7\u503c\u89c1\u89e3\uff0c\u4f46\u5728\u66f4\u590d\u6742\u573a\u666f\u4e2d\u5f15\u5165\u7684\u5173\u952e\u95ee\u9898\u51f8\u663e\u4e86\u5bf9LLM\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u548c\u9a8c\u8bc1\u7684\u5fc5\u8981\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u52a0\u6df1\u4e86\u5bf9LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u4f18\u52bf\u548c\u5c40\u9650\u6027\u7684\u7406\u89e3\u3002"}}
{"id": "2508.00265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00265", "abs": "https://arxiv.org/abs/2508.00265", "authors": ["Henghui Ding", "Song Tang", "Shuting He", "Chang Liu", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "Multimodal Referring Segmentation: A Survey", "comment": "Project Page:\n  https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation", "summary": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u6a21\u6001\u6307\u4ee3\u5206\u5272\u7684\u80cc\u666f\u3001\u65b9\u6cd5\u3001\u5e94\u7528\u53ca\u6311\u6218\uff0c\u603b\u7ed3\u4e86\u4ee3\u8868\u6027\u65b9\u6cd5\u5e76\u63d0\u4f9b\u4e86\u6027\u80fd\u6bd4\u8f83\u3002", "motivation": "\u591a\u6a21\u6001\u6307\u4ee3\u5206\u5272\u5728\u57fa\u4e8e\u7528\u6237\u6307\u4ee4\u7684\u7cbe\u786e\u5bf9\u8c61\u611f\u77e5\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u8fd1\u5e74\u6765\u56e0\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001Transformer\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u800c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002", "method": "\u4ecb\u7ecd\u4e86\u7edf\u4e00\u7684\u5143\u67b6\u6784\uff0c\u5e76\u56de\u987e\u4e86\u56fe\u50cf\u3001\u89c6\u9891\u548c3D\u573a\u666f\u4e2d\u7684\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u8ba8\u8bba\u4e86\u5e7f\u4e49\u6307\u4ee3\u8868\u8fbe\uff08GREx\uff09\u65b9\u6cd5\u3002", "result": "\u63d0\u4f9b\u4e86\u6807\u51c6\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u6027\u80fd\u6bd4\u8f83\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u6307\u4ee3\u5206\u5272\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u603b\u7ed3\u4e86\u8be5\u9886\u57df\u7684\u80cc\u666f\u3001\u65b9\u6cd5\u3001\u6311\u6218\u53ca\u672a\u6765\u65b9\u5411\uff0c\u5e76\u6301\u7eed\u8ddf\u8e2a\u76f8\u5173\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2508.00414", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00414", "abs": "https://arxiv.org/abs/2508.00414", "authors": ["Tianqing Fang", "Zhisong Zhang", "Xiaoyang Wang", "Rui Wang", "Can Qin", "Yuxuan Wan", "Jun-Yu Ma", "Ce Zhang", "Jiaqi Chen", "Xiyun Li", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training", "comment": "16 pages", "summary": "General AI Agents are increasingly recognized as foundational frameworks for\nthe next generation of artificial intelligence, enabling complex reasoning, web\ninteraction, coding, and autonomous research capabilities. However, current\nagent systems are either closed-source or heavily reliant on a variety of paid\nAPIs and proprietary tools, limiting accessibility and reproducibility for the\nresearch community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a\nfully open-source and (to the maximum extent) free multi-module agent framework\ndesigned to democratize the development and evaluation of advanced AI agents.\nWithin Cognitive Kernel-Pro, we systematically investigate the curation of\nhigh-quality training data for Agent Foundation Models, focusing on the\nconstruction of queries, trajectories, and verifiable answers across four key\ndomains: web, file, code, and general reasoning. Furthermore, we explore novel\nstrategies for agent test-time reflection and voting to enhance agent\nrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving\nstate-of-the-art results among open-source and free agents. Notably, our\n8B-parameter open-source model surpasses previous leading systems such as\nWebDancer and WebSailor, establishing a new performance standard for\naccessible, high-capability AI agents. Code is available at\nhttps://github.com/Tencent/CognitiveKernel-Pro", "AI": {"tldr": "Cognitive Kernel-Pro\u662f\u4e00\u4e2a\u5f00\u6e90\u514d\u8d39\u7684AI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u521b\u65b0\u7b56\u7565\u63d0\u5347\u4e86\u4ee3\u7406\u6027\u80fd\uff0c\u5728GAIA\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u7cfb\u7edf\u591a\u4e3a\u95ed\u6e90\u6216\u4f9d\u8d56\u4ed8\u8d39API\u548c\u4e13\u6709\u5de5\u5177\uff0c\u9650\u5236\u4e86\u7814\u7a76\u793e\u533a\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f00\u6e90\u6846\u67b6Cognitive Kernel-Pro\u6765\u63a8\u52a8\u9ad8\u7ea7AI\u4ee3\u7406\u7684\u6c11\u4e3b\u5316\u5f00\u53d1\u548c\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u6784\u5efa\uff0c\u5305\u62ec\u67e5\u8be2\u3001\u8f68\u8ff9\u548c\u53ef\u9a8c\u8bc1\u7b54\u6848\u7684\u751f\u6210\uff0c\u4ee5\u53ca\u63a2\u7d22\u4ee3\u7406\u7684\u6d4b\u8bd5\u65f6\u53cd\u601d\u548c\u6295\u7968\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "result": "Cognitive Kernel-Pro\u5728GAIA\u4e0a\u5b9e\u73b0\u4e86\u5f00\u6e90\u514d\u8d39\u4ee3\u7406\u4e2d\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c8B\u53c2\u6570\u7684\u5f00\u6e90\u6a21\u578b\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u9886\u5148\u7cfb\u7edfWebDancer\u548cWebSailor\u3002", "conclusion": "Cognitive Kernel-Pro \u4f5c\u4e3a\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u4e14\u6700\u5927\u9650\u5ea6\u514d\u8d39\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u6210\u529f\u63d0\u5347\u4e86AI\u4ee3\u7406\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u5728GAIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4e3a\u9ad8\u6027\u80fdAI\u4ee3\u7406\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2508.00688", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00688", "abs": "https://arxiv.org/abs/2508.00688", "authors": ["Ruiyang Huang", "Haocheng Wang", "Yixuan Shen", "Ning Gao", "Qiang Ni", "Shi Jin", "Yifan Wu"], "title": "Criticality-Based Dynamic Topology Optimization for Enhancing Aerial-Marine Swarm Resilience", "comment": "Submit to INFOCOM 2026", "summary": "Heterogeneous marine-aerial swarm networks encounter substantial difficulties\ndue to targeted communication disruptions and structural weaknesses in\nadversarial environments. This paper proposes a two-step framework to\nstrengthen the network's resilience. Specifically, our framework combines the\nnode prioritization based on criticality with multi-objective topology\noptimization. First, we design a three-layer architecture to represent\nstructural, communication, and task dependencies of the swarm networks. Then,\nwe introduce the SurBi-Ranking method, which utilizes graph convolutional\nnetworks, to dynamically evaluate and rank the criticality of nodes and edges\nin real time. Next, we apply the NSGA-III algorithm to optimize the network\ntopology, aiming to balance communication efficiency, global connectivity, and\nmission success rate. Experiments demonstrate that compared to traditional\nmethods like K-Shell, our SurBi-Ranking method identifies critical nodes and\nedges with greater accuracy, as deliberate attacks on these components cause\nmore significant connectivity degradation. Furthermore, our optimization\napproach, when prioritizing SurBi-Ranked critical components under attack,\nreduces the natural connectivity degradation by around 30%, achieves higher\nmission success rates, and incurs lower communication reconfiguration costs,\nensuring sustained connectivity and mission effectiveness across multi-phase\noperations.", "AI": {"tldr": "A two-step framework enhances swarm network resilience by dynamically ranking critical nodes and optimizing topology, reducing connectivity degradation by 30% and improving mission success.", "motivation": "To address the challenges of targeted communication disruptions and structural weaknesses in adversarial environments for heterogeneous marine-aerial swarm networks.", "method": "The framework includes a three-layer architecture for representing dependencies and the SurBi-Ranking method using graph convolutional networks for dynamic criticality evaluation. The NSGA-III algorithm is then applied for topology optimization.", "result": "The SurBi-Ranking method outperforms traditional methods in identifying critical components, and the optimization approach reduces connectivity degradation by around 30%, achieving higher mission success rates and lower reconfiguration costs.", "conclusion": "The proposed two-step framework, combining node prioritization based on criticality with multi-objective topology optimization, significantly enhances the resilience of heterogeneous marine-aerial swarm networks in adversarial environments."}}
{"id": "2508.00440", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00440", "abs": "https://arxiv.org/abs/2508.00440", "authors": ["M. A. P\u00e9rez-Cuti\u00f1o", "J. Valverde", "J. Capit\u00e1n", "J. M. D\u00edaz-B\u00e1\u00f1ez"], "title": "Reducing the gap between general purpose data and aerial images in concentrated solar power plants", "comment": null, "summary": "In the context of Concentrated Solar Power (CSP) plants, aerial images\ncaptured by drones present a unique set of challenges. Unlike urban or natural\nlandscapes commonly found in existing datasets, solar fields contain highly\nreflective surfaces, and domain-specific elements that are uncommon in\ntraditional computer vision benchmarks. As a result, machine learning models\ntrained on generic datasets struggle to generalize to this setting without\nextensive retraining and large volumes of annotated data. However, collecting\nand labeling such data is costly and time-consuming, making it impractical for\nrapid deployment in industrial applications.\n  To address this issue, we propose a novel approach: the creation of\nAerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By\ngenerating synthetic data that closely mimic real-world conditions, our\nobjective is to facilitate pretraining of models before deployment,\nsignificantly reducing the need for extensive manual labeling. Our main\ncontributions are threefold: (1) we introduce AerialCSP, a high-quality\nsynthetic dataset for aerial inspection of CSP plants, providing annotated data\nfor object detection and image segmentation; (2) we benchmark multiple models\non AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we\ndemonstrate that pretraining on AerialCSP significantly improves real-world\nfault detection, particularly for rare and small defects, reducing the need for\nextensive manual labeling. AerialCSP is made publicly available at\nhttps://mpcutino.github.io/aerialcsp/.", "AI": {"tldr": "\u63d0\u51faAerialCSP\u865a\u62df\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347CSP\u7535\u7ad9\u7f3a\u9677\u68c0\u6d4b\u6548\u679c\uff0c\u51cf\u5c11\u624b\u52a8\u6807\u6ce8\u9700\u6c42\u3002", "motivation": "CSP\u7535\u7ad9\u7684\u822a\u62cd\u56fe\u50cf\u5177\u6709\u9ad8\u53cd\u5c04\u9762\u548c\u9886\u57df\u7279\u5b9a\u5143\u7d20\uff0c\u4f20\u7edf\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u96be\u4ee5\u6cdb\u5316\uff0c\u624b\u52a8\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u957f\u3002", "method": "\u63d0\u51faAerialCSP\u865a\u62df\u6570\u636e\u96c6\uff0c\u6a21\u62dfCSP\u7535\u7ad9\u7684\u822a\u62cd\u56fe\u50cf\uff0c\u7528\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u51cf\u5c11\u624b\u52a8\u6807\u6ce8\u9700\u6c42\u3002", "result": "AerialCSP\u6570\u636e\u96c6\u4e3aCSP\u76f8\u5173\u89c6\u89c9\u4efb\u52a1\u5efa\u7acb\u4e86\u57fa\u51c6\uff0c\u9884\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u7f3a\u9677\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "AerialCSP\u5408\u6210\u6570\u636e\u96c6\u663e\u8457\u63d0\u9ad8\u4e86CSP\u7535\u7ad9\u7684\u5b9e\u65f6\u7f3a\u9677\u68c0\u6d4b\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5bf9\u5c0f\u800c\u7f55\u89c1\u7684\u7f3a\u9677\uff0c\u51cf\u5c11\u4e86\u5bf9\u624b\u52a8\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2508.00738", "categories": ["cs.SE", "cs.FL", "68N30", "D.2.4"], "pdf": "https://arxiv.org/pdf/2508.00738", "abs": "https://arxiv.org/abs/2508.00738", "authors": ["Bernhard Rumpe", "Max Stachon", "Sebastian St\u00fcber", "Valdes Voufo"], "title": "Tool-Assisted Conformance Checking to Reference Process Models", "comment": null, "summary": "Reference models convey best practices and standards. The reference\nframeworks necessitate conformance checks to ensure adherence to established\nguidelines and principles, which is crucial for maintaining quality and\nconsistency in various processes. This paper explores automated conformance\nchecks for concrete process models against reference models using causal\ndependency analysis of tasks and events. Existing notions of conformance\nchecking for process models focus on verifying process execution traces and\nlack the expressiveness and automation needed for semantic model comparison,\nleaving this question unresolved. We integrate our approach into a broader\nsemantic framework for defining reference model conformance. We outline an\nalgorithm for reference process model conformance checking, evaluate it through\na case study, and discuss its strengths and limitations. Our research provides\na tool-assisted solution enhancing accuracy and flexibility in process model\nconformance verification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u68c0\u67e5\u6d41\u7a0b\u6a21\u578b\u4e0e\u53c2\u8003\u6a21\u578b\u4e00\u81f4\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u56e0\u679c\u4f9d\u8d56\u5206\u6790\u589e\u5f3a\u9a8c\u8bc1\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6d41\u7a0b\u6a21\u578b\u4e00\u81f4\u6027\u68c0\u67e5\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6d41\u7a0b\u6267\u884c\u8f68\u8ff9\u7684\u9a8c\u8bc1\uff0c\u7f3a\u4e4f\u8bed\u4e49\u6a21\u578b\u6bd4\u8f83\u7684\u8868\u8fbe\u80fd\u529b\u548c\u81ea\u52a8\u5316\u9700\u6c42\uff0c\u8fd9\u4e00\u95ee\u9898\u5c1a\u672a\u89e3\u51b3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4efb\u52a1\u548c\u4e8b\u4ef6\u56e0\u679c\u4f9d\u8d56\u5206\u6790\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u68c0\u67e5\u5177\u4f53\u6d41\u7a0b\u6a21\u578b\u4e0e\u53c2\u8003\u6a21\u578b\u7684\u4e00\u81f4\u6027\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc4\u4f30\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u56e0\u679c\u4f9d\u8d56\u5206\u6790\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u4e00\u81f4\u6027\u68c0\u67e5\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u6d41\u7a0b\u6a21\u578b\u4e00\u81f4\u6027\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2508.00272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00272", "abs": "https://arxiv.org/abs/2508.00272", "authors": ["Wenyue Chong"], "title": "Towards Robust Semantic Correspondence: A Benchmark and Insights", "comment": null, "summary": "Semantic correspondence aims to identify semantically meaningful\nrelationships between different images and is a fundamental challenge in\ncomputer vision. It forms the foundation for numerous tasks such as 3D\nreconstruction, object tracking, and image editing. With the progress of\nlarge-scale vision models, semantic correspondence has achieved remarkable\nperformance in controlled and high-quality conditions. However, the robustness\nof semantic correspondence in challenging scenarios is much less investigated.\nIn this work, we establish a novel benchmark for evaluating semantic\ncorrespondence in adverse conditions. The benchmark dataset comprises 14\ndistinct challenging scenarios that reflect commonly encountered imaging\nissues, including geometric distortion, image blurring, digital artifacts, and\nenvironmental occlusion. Through extensive evaluations, we provide several key\ninsights into the robustness of semantic correspondence approaches: (1) All\nexisting methods suffer from noticeable performance drops under adverse\nconditions; (2) Using large-scale vision models can enhance overall robustness,\nbut fine-tuning on these models leads to a decline in relative robustness; (3)\nThe DINO model outperforms the Stable Diffusion in relative robustness, and\ntheir fusion achieves better absolute robustness; Moreover, We evaluate common\nrobustness enhancement strategies for semantic correspondence and find that\ngeneral data augmentations are ineffective, highlighting the need for\ntask-specific designs. These results are consistent across both our dataset and\nreal-world benchmarks.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e86\u4e00\u4e2a\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u8bed\u4e49\u5bf9\u5e94\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\uff08\u5982DINO\uff09\u548c\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\u80fd\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u5bf9\u5e94\u7814\u7a76\u591a\u96c6\u4e2d\u5728\u9ad8\u8d28\u91cf\u53ef\u63a7\u6761\u4ef6\u4e0b\uff0c\u800c\u5728\u6076\u52a3\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u7814\u7a76\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u8bed\u4e49\u5bf9\u5e94\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u4e00\u4e2a\u5305\u542b14\u79cd\u5e38\u89c1\u6210\u50cf\u95ee\u9898\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u8bed\u4e49\u5bf9\u5e94\u65b9\u6cd5\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\uff08\u5982DINO\u548cStable Diffusion\uff09\u53ca\u5176\u878d\u5408\u7684\u6548\u679c\u3002", "result": "\u6240\u6709\u73b0\u6709\u65b9\u6cd5\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u6027\u80fd\u5747\u663e\u8457\u4e0b\u964d\u3002\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\u80fd\u63d0\u5347\u6574\u4f53\u9c81\u68d2\u6027\uff0c\u4f46\u5fae\u8c03\u4f1a\u964d\u4f4e\u76f8\u5bf9\u9c81\u68d2\u6027\u3002DINO\u6a21\u578b\u5728\u76f8\u5bf9\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8eStable Diffusion\uff0c\u878d\u5408\u4e24\u8005\u53ef\u63d0\u5347\u7edd\u5bf9\u9c81\u68d2\u6027\u3002\u901a\u7528\u6570\u636e\u589e\u5f3a\u7b56\u7565\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u73b0\u6709\u8bed\u4e49\u5bf9\u5e94\u65b9\u6cd5\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\u867d\u80fd\u63d0\u5347\u6574\u4f53\u9c81\u68d2\u6027\uff0c\u4f46\u5fae\u8c03\u4f1a\u964d\u4f4e\u76f8\u5bf9\u9c81\u68d2\u6027\u3002DINO\u6a21\u578b\u5728\u76f8\u5bf9\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8eStable Diffusion\uff0c\u4e24\u8005\u878d\u5408\u53ef\u63d0\u5347\u7edd\u5bf9\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u901a\u7528\u6570\u636e\u589e\u5f3a\u7b56\u7565\u6548\u679c\u6709\u9650\uff0c\u9700\u8bbe\u8ba1\u4efb\u52a1\u7279\u5b9a\u7684\u589e\u5f3a\u65b9\u6cd5\u3002"}}
{"id": "2508.00459", "categories": ["cs.AI", "68T07, 68T20", "I.2.6; I.2.7; I.2.3"], "pdf": "https://arxiv.org/pdf/2508.00459", "abs": "https://arxiv.org/abs/2508.00459", "authors": ["Andrea Asperti", "Alberto Naibo", "Claudio Sacerdoti Coen"], "title": "Thinking Machines: Mathematical Reasoning in the Age of LLMs", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable abilities in structured\nreasoning and symbolic tasks, with coding emerging as a particular area of\nstrength. This success has sparked growing interest in applying LLMs to\nmathematics, both in informal problem-solving and formal theorem proving.\nHowever, progress in formal mathematics has proven to be significantly more\ndifficult, despite surface-level similarities between programming and proof\nconstruction. This discrepancy raises important questions about how LLMs\n``reason'', how they are supervised, and whether they internally track a notion\nof computational or deductive state. In this article, we address the\nstate-of-the-art of the discipline, focusing on recent models and benchmarks,\nand explore three central issues at the intersection of machine learning and\nmathematical cognition: (i) the trade-offs between formal and informal\nmathematics as training domains; (ii) the deeper reasons why proof generation\nremains more brittle than code synthesis; (iii) and the question of whether\nLLMs represent, or merely mimic, a notion of evolving logical state. Our goal\nis not to draw hard boundaries, but to identify where the current limits lie,\nand how they might be extended.", "AI": {"tldr": "LLMs\u5728\u975e\u6b63\u5f0f\u6570\u5b66\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6b63\u5f0f\u5b9a\u7406\u8bc1\u660e\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff1b\u7814\u7a76\u63a2\u8ba8\u4e86\u5176\u5c40\u9650\u6027\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "LLMs\u5728\u975e\u6b63\u5f0f\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6b63\u5f0f\u5b9a\u7406\u8bc1\u660e\u4e2d\u8868\u73b0\u8106\u5f31\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9LLMs\u63a8\u7406\u65b9\u5f0f\u3001\u76d1\u7763\u65b9\u6cd5\u53ca\u5185\u90e8\u72b6\u6001\u8ddf\u8e2a\u7684\u6df1\u5165\u63a2\u8ba8\u3002", "method": "\u5206\u6790\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u4e0e\u6570\u5b66\u8ba4\u77e5\u4ea4\u53c9\u7684\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6b63\u5f0f\u6570\u5b66\u4e0e\u4ee3\u7801\u5408\u6210\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63ed\u793a\u4e86LLMs\u5728\u8bc1\u660e\u751f\u6210\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u80fd\u7684\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8\u4e86LLMs\u5728\u6b63\u5f0f\u6570\u5b66\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u53ef\u80fd\u7684\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u6269\u5c55\u5f53\u524d\u7684\u80fd\u529b\u8fb9\u754c\u3002"}}
{"id": "2508.00715", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.00715", "abs": "https://arxiv.org/abs/2508.00715", "authors": ["Olga Kondrateva", "Grace Li Zhang", "Julian Zobel", "Bj\u00f6rn Scheuermann", "Stefan Dietzel"], "title": "Deep Joint Source-Channel Coding for Small Satellite Applications", "comment": null, "summary": "Small satellites used for Earth observation generate vast amounts of\nhigh-dimensional data, but their operation in low Earth orbit creates a\nsignificant communication bottleneck due to limited contact times and harsh,\nvarying channel conditions. While deep joint source-channel coding (DJSCC) has\nemerged as a promising technique, its practical application to the complex\nsatellite environment remains an open question. This paper presents a\ncomprehensive DJSCC framework tailored for satellite communications. We first\nestablish a basic system, DJSCC-SAT, and integrate a realistic, multi-state\nstatistical channel model to guide its training and evaluation. To overcome the\nimpracticality of using separate models for every channel condition, we then\nintroduce an adaptable architecture, ADJSCC-SAT, which leverages attention\nmodules to allow a single neural network to adjust to a wide range of channel\nstates with minimal overhead. Through extensive evaluation on Sentinel-2\nmulti-spectral data, we demonstrate that our adaptable approach achieves\nperformance comparable to using multiple specialized networks while\nsignificantly reducing model storage requirements. Furthermore, the adaptable\nmodel shows enhanced robustness to channel estimation errors, outperforming the\nnon-adaptable baseline. The proposed framework is a practical and efficient\nstep toward deploying robust, adaptive DJSCC systems for real-world satellite\nmissions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u536b\u661f\u901a\u4fe1\u7684\u81ea\u9002\u5e94DJSCC\u6846\u67b6ADJSCC-SAT\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u6a21\u5757\u5b9e\u73b0\u5355\u4e00\u7f51\u7edc\u9002\u5e94\u591a\u79cd\u4fe1\u9053\u72b6\u6001\uff0c\u663e\u8457\u51cf\u5c11\u5b58\u50a8\u9700\u6c42\u5e76\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u5c0f\u536b\u661f\u5728\u4f4e\u5730\u7403\u8f68\u9053\u4e2d\u56e0\u6709\u9650\u63a5\u89e6\u65f6\u95f4\u548c\u590d\u6742\u4fe1\u9053\u6761\u4ef6\u5bfc\u81f4\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u63a2\u7d22DJSCC\u5728\u590d\u6742\u536b\u661f\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bba\u6587\u9996\u5148\u5efa\u7acb\u4e86\u57fa\u7840\u7cfb\u7edfDJSCC-SAT\uff0c\u5e76\u6574\u5408\u4e86\u591a\u72b6\u6001\u7edf\u8ba1\u4fe1\u9053\u6a21\u578b\u6765\u6307\u5bfc\u5176\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u968f\u540e\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u67b6\u6784ADJSCC-SAT\uff0c\u5229\u7528\u6ce8\u610f\u529b\u6a21\u5757\u4f7f\u5355\u4e00\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u9002\u5e94\u5e7f\u6cdb\u7684\u4fe1\u9053\u72b6\u6001\u3002", "result": "\u5728Sentinel-2\u591a\u5149\u8c31\u6570\u636e\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u4f7f\u7528\u591a\u4e2a\u4e13\u7528\u7f51\u7edc\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u5b58\u50a8\u9700\u6c42\uff0c\u5e76\u589e\u5f3a\u4e86\u5bf9\u6297\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684ADJSCC-SAT\u6846\u67b6\u4e3a\u5b9e\u9645\u536b\u661f\u4efb\u52a1\u4e2d\u90e8\u7f72\u9c81\u68d2\u3001\u81ea\u9002\u5e94\u7684DJSCC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00589", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.RO", "68T45, 68P20, 68T10, 68T50, 68T07, 68T40", "I.2.10; I.4.8; I.2.9; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.00589", "abs": "https://arxiv.org/abs/2508.00589", "authors": ["Stefan Englmeier", "Max A. B\u00fcttner", "Katharina Winter", "Fabian B. Flohr"], "title": "Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving", "comment": "9 pages, 10 figure, project page\n  https://iv.ee.hm.edu/contextmotionclip/, submitted to IEEE Transactions on\n  Intelligent Vehicles (T-IV), This work has been submitted to the IEEE for\n  possible publication", "summary": "Autonomous driving systems must operate reliably in safety-critical\nscenarios, particularly those involving unusual or complex behavior by\nVulnerable Road Users (VRUs). Identifying these edge cases in driving datasets\nis essential for robust evaluation and generalization, but retrieving such rare\nhuman behavior scenarios within the long tail of large-scale datasets is\nchallenging. To support targeted evaluation of autonomous driving systems in\ndiverse, human-centered scenarios, we propose a novel context-aware motion\nretrieval framework. Our method combines Skinned Multi-Person Linear\n(SMPL)-based motion sequences and corresponding video frames before encoding\nthem into a shared multimodal embedding space aligned with natural language.\nOur approach enables the scalable retrieval of human behavior and their context\nthrough text queries. This work also introduces our dataset WayMoCo, an\nextension of the Waymo Open Dataset. It contains automatically labeled motion\nand scene context descriptions derived from generated pseudo-ground-truth SMPL\nsequences and corresponding image data. Our approach outperforms\nstate-of-the-art models by up to 27.5% accuracy in motion-context retrieval,\nwhen evaluated on the WayMoCo dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSMPL\u548c\u89c6\u9891\u5e27\u7684\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u67e5\u8be2\u9ad8\u6548\u68c0\u7d22\u4eba\u7c7b\u884c\u4e3a\u53ca\u5176\u4e0a\u4e0b\u6587\uff0c\u5728WayMoCo\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u6a21\u578b27.5%\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u5fc5\u987b\u53ef\u9760\u8fd0\u884c\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u6613\u53d7\u4f24\u5bb3\u9053\u8def\u4f7f\u7528\u8005\uff08VRUs\uff09\u7684\u5f02\u5e38\u6216\u590d\u6742\u884c\u4e3a\u65f6\u3002\u4ece\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u68c0\u7d22\u8fd9\u4e9b\u7f55\u89c1\u7684\u4eba\u7c7b\u884c\u4e3a\u573a\u666f\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u5bf9\u7cfb\u7edf\u7684\u9c81\u68d2\u8bc4\u4f30\u548c\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u57fa\u4e8eSMPL\u7684\u8fd0\u52a8\u5e8f\u5217\u548c\u76f8\u5e94\u89c6\u9891\u5e27\uff0c\u5c06\u5b83\u4eec\u7f16\u7801\u5230\u4e0e\u81ea\u7136\u8bed\u8a00\u5bf9\u9f50\u7684\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\uff0c\u901a\u8fc7\u6587\u672c\u67e5\u8be2\u5b9e\u73b0\u4eba\u7c7b\u884c\u4e3a\u53ca\u5176\u4e0a\u4e0b\u6587\u7684\u53ef\u6269\u5c55\u68c0\u7d22\u3002", "result": "\u5728WayMoCo\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u8fd0\u52a8\u4e0a\u4e0b\u6587\u68c0\u7d22\u51c6\u786e\u7387\u4e0a\u6bd4\u73b0\u6709\u6700\u4f18\u6a21\u578b\u63d0\u5347\u4e8627.5%\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728WayMoCo\u6570\u636e\u96c6\u4e0a\u6bd4\u73b0\u6709\u6700\u4f18\u6a21\u578b\u5728\u8fd0\u52a8\u4e0a\u4e0b\u6587\u68c0\u7d22\u51c6\u786e\u7387\u4e0a\u63d0\u5347\u4e8627.5%\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.00749", "categories": ["cs.SE", "cs.FL", "cs.SC", "68N30", "D.2.4"], "pdf": "https://arxiv.org/pdf/2508.00749", "abs": "https://arxiv.org/abs/2508.00749", "authors": ["Johanna Grahl", "Bernhard Rumpe", "Max Stachon", "Sebastian St\u00fcber"], "title": "Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures", "comment": null, "summary": "In the context of model-driven development, ensuring the correctness and\nconsistency of evolving models is paramount. This paper investigates the\napplication of Dynamic Symbolic Execution (DSE) for semantic difference\nanalysis of component-and-connector architectures, specifically utilizing\nMontiArc models. We have enhanced the existing MontiArc-to-Java generator to\ngather both symbolic and concrete execution data at runtime, encompassing\ntransition conditions, visited states, and internal variables of automata. This\ndata facilitates the identification of significant execution traces that\nprovide critical insights into system behavior. We evaluate various execution\nstrategies based on the criteria of runtime efficiency, minimality, and\ncompleteness, establishing a framework for assessing the applicability of DSE\nin semantic difference analysis. Our findings indicate that while DSE shows\npromise for analyzing component and connector architectures, scalability\nremains a primary limitation, suggesting further research is needed to enhance\nits practical utility in larger systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u52a8\u6001\u7b26\u53f7\u6267\u884c\uff08DSE\uff09\u5728\u7ec4\u4ef6\u548c\u8fde\u63a5\u5668\u67b6\u6784\u8bed\u4e49\u5dee\u5f02\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u589e\u5f3a\u4e86MontiArc\u6a21\u578b\u7684\u6570\u636e\u6536\u96c6\u80fd\u529b\uff0c\u5e76\u8bc4\u4f30\u4e86DSE\u7684\u9002\u7528\u6027\uff0c\u53d1\u73b0\u5176\u6f5c\u529b\u4f46\u53ef\u6269\u5c55\u6027\u6709\u9650\u3002", "motivation": "\u5728\u6a21\u578b\u9a71\u52a8\u5f00\u53d1\u4e2d\uff0c\u786e\u4fdd\u6f14\u5316\u6a21\u578b\u7684\u6b63\u786e\u6027\u548c\u4e00\u81f4\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u589e\u5f3a\u4e86\u73b0\u6709\u7684MontiArc-to-Java\u751f\u6210\u5668\uff0c\u6536\u96c6\u7b26\u53f7\u548c\u5177\u4f53\u6267\u884c\u6570\u636e\uff0c\u5305\u62ec\u8fc7\u6e21\u6761\u4ef6\u3001\u8bbf\u95ee\u72b6\u6001\u548c\u81ea\u52a8\u673a\u5185\u90e8\u53d8\u91cf\u3002", "result": "\u8bc4\u4f30\u4e86\u57fa\u4e8e\u8fd0\u884c\u65f6\u6548\u7387\u3001\u6700\u5c0f\u6027\u548c\u5b8c\u6574\u6027\u7684\u5404\u79cd\u6267\u884c\u7b56\u7565\uff0c\u5efa\u7acb\u4e86\u8bc4\u4f30DSE\u5728\u8bed\u4e49\u5dee\u5f02\u5206\u6790\u4e2d\u9002\u7528\u6027\u7684\u6846\u67b6\u3002", "conclusion": "\u5c3d\u7ba1DSE\u5728\u5206\u6790\u7ec4\u4ef6\u548c\u8fde\u63a5\u5668\u67b6\u6784\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u53ef\u6269\u5c55\u6027\u4ecd\u662f\u4e3b\u8981\u9650\u5236\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347\u5176\u5728\u5927\u578b\u7cfb\u7edf\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.00287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00287", "abs": "https://arxiv.org/abs/2508.00287", "authors": ["Tran Viet Khoa", "Do Hai Son", "Mohammad Abu Alsheikh", "Yibeltal F Alem", "Dinh Thai Hoang"], "title": "Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning", "comment": null, "summary": "Driver drowsiness is one of the main causes of road accidents and is\nrecognized as a leading contributor to traffic-related fatalities. However,\ndetecting drowsiness accurately remains a challenging task, especially in\nreal-world settings where facial data from different individuals is\ndecentralized and highly diverse. In this paper, we propose a novel framework\nfor drowsiness detection that is designed to work effectively with\nheterogeneous and decentralized data. Our approach develops a new Spatial\nSelf-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)\nnetwork to better extract key facial features and improve detection\nperformance. To support federated learning, we employ a Gradient Similarity\nComparison (GSC) that selects the most relevant trained models from different\noperators before aggregation. This improves the accuracy and robustness of the\nglobal model while preserving user privacy. We also develop a customized tool\nthat automatically processes video data by extracting frames, detecting and\ncropping faces, and applying data augmentation techniques such as rotation,\nflipping, brightness adjustment, and zooming. Experimental results show that\nour framework achieves a detection accuracy of 89.9% in the federated learning\nsettings, outperforming existing methods under various deployment scenarios.\nThe results demonstrate the effectiveness of our approach in handling\nreal-world data variability and highlight its potential for deployment in\nintelligent transportation systems to enhance road safety through early and\nreliable drowsiness detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408SSA\u548cLSTM\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5206\u6563\u6570\u636e\u4e2d\u68c0\u6d4b\u9a7e\u9a76\u5458 drowsiness\uff0c\u901a\u8fc7GSC\u652f\u6301\u8054\u90a6\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e8689.9%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u9a7e\u9a76\u5458 drowsiness \u662f\u9053\u8def\u4e8b\u6545\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\uff0c\u4f46\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u51c6\u786e\u68c0\u6d4b drowsiness \u4ecd\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u5206\u6563\u4e14\u591a\u6837\u5316\u7684\u9762\u90e8\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u65b0\u7684\u7a7a\u95f4\u81ea\u6ce8\u610f\u529b\uff08SSA\uff09\u673a\u5236\u4e0e\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\uff0c\u4ee5\u66f4\u597d\u5730\u63d0\u53d6\u5173\u952e\u9762\u90e8\u7279\u5f81\u5e76\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u91c7\u7528\u68af\u5ea6\u76f8\u4f3c\u6027\u6bd4\u8f83\uff08GSC\uff09\u652f\u6301\u8054\u90a6\u5b66\u4e60\uff0c\u9009\u62e9\u6700\u76f8\u5173\u7684\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u805a\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523089.9%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e8689.9%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u591a\u6837\u6027\u7684\u6709\u6548\u6027\uff0c\u5e76\u7a81\u663e\u4e86\u5176\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u90e8\u7f72\u7684\u6f5c\u529b\uff0c\u4ee5\u901a\u8fc7\u65e9\u671f\u53ef\u9760\u7684 drowsiness \u68c0\u6d4b\u63d0\u5347\u9053\u8def\u5b89\u5168\u3002"}}
{"id": "2508.00500", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00500", "abs": "https://arxiv.org/abs/2508.00500", "authors": ["Haoyu Wang", "Chris M. Poskitt", "Jun Sun", "Jiali Wei"], "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking", "comment": null, "summary": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead.", "AI": {"tldr": "Pro2Guard\u662f\u4e00\u79cd\u4e3b\u52a8\u5b89\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u53ef\u8fbe\u6027\u5206\u6790\u9884\u6d4bLLM\u4ee3\u7406\u7684\u672a\u6765\u98ce\u9669\uff0c\u63d0\u524d\u5e72\u9884\uff0c\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u6267\u884c\u7cfb\u7edf\uff08\u5982AgentSpec\uff09\u7f3a\u4e4f\u9884\u89c1\u6027\uff0c\u96be\u4ee5\u5904\u7406\u957f\u671f\u4f9d\u8d56\u548c\u5206\u5e03\u53d8\u5316\uff0cPro2Guard\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "Pro2Guard \u5c06\u4ee3\u7406\u884c\u4e3a\u62bd\u8c61\u4e3a\u7b26\u53f7\u72b6\u6001\uff0c\u5e76\u4ece\u6267\u884c\u8f68\u8ff9\u4e2d\u5b66\u4e60\u79bb\u6563\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\uff08DTMC\uff09\uff0c\u5728\u8fd0\u884c\u65f6\u4f30\u8ba1\u5230\u8fbe\u4e0d\u5b89\u5168\u72b6\u6001\u7684\u6982\u7387\u3002", "result": "\u5728\u5bb6\u5ead\u4ee3\u7406\u4efb\u52a1\u4e2d\uff0cPro2Guard \u5728\u4f4e\u9608\u503c\u4e0b\u53ef\u63d0\u524d\u963b\u6b6293.6%\u7684\u4e0d\u5b89\u5168\u4efb\u52a1\uff1b\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\uff0c\u80fd100%\u9884\u6d4b\u4ea4\u901a\u8fdd\u89c4\u548c\u78b0\u649e\uff0c\u63d0\u524d38.66\u79d2\u9884\u8b66\u98ce\u9669\u3002", "conclusion": "Pro2Guard \u662f\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u53ef\u8fbe\u6027\u5206\u6790\u7684\u4e3b\u52a8\u8fd0\u884c\u65f6\u6267\u884c\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u98ce\u9669\u5e76\u63d0\u524d\u5e72\u9884\uff0c\u663e\u8457\u63d0\u9ad8LLM\u4ee3\u7406\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2508.00735", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.00735", "abs": "https://arxiv.org/abs/2508.00735", "authors": ["Lucas Aubard", "Johan Mazel", "Gilles Guette", "Pierre Chifflier"], "title": "Overlapping IPv4, IPv6, and TCP data: exploring errors, test case context and multiple overlaps inside network stacks and NIDSes with PYROLYSE", "comment": null, "summary": "IP fragmentation and TCP segmentation allow for splitting large data packets\ninto smaller ones, e.g., for transmission across network links of limited\ncapacity. These mechanisms permit complete or partial overlaps with different\ndata on the overlapping portions. IPv4, IPv6, and TCP reassembly policies,\ni.e., the data chunk preferences that depend on the overlap types, differ\nacross protocol implementations. This leads to vulnerabilities, as NIDSes may\ninterpret the packet differently from the monitored host OSes. Some NIDSes,\nsuch as Suricata or Snort, can be configured so that their policies are\nconsistent with the monitored OSes. The first contribution of the paper is\nPYROLYSE, an audit tool that exhaustively tests and describes the reassembly\npolicies of various IP and TCP implementation types. This tool ensures that\nimplementations reassemble overlapping chunk sequences without errors. The\nsecond contribution is the analysis of PYROLYSE artifacts. We first show that\nthe reassembly policies are much more diverse than previously thought. Indeed,\nby testing all the overlap possibilities for n <= 3 test case chunks and\ndifferent testing scenarios, we observe from 14 to 20 different behaviors out\nof 23 tested implementations depending on the protocol. Second, we report eight\nerrors impacting one OS, two NIDSes, and two embedded stacks, which can lead to\nsecurity issues such as NIDS pattern-matching bypass or DoS attacks. A CVE was\nassigned to a NIDS error. Finally, we show that implemented IP and TCP policies\nobtained through chunk pair testing are usually inconsistent with the observed\ntriplet reassemblies. Therefore, contrarily to what they currently do, NIDSes\nor other network traffic analysis tools should not apply n = 2 pair policies\nwhen the number of overlapping chunks exceeds two.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PYROLYSE\u5de5\u5177\uff0c\u7528\u4e8e\u6d4b\u8bd5IP\u548cTCP\u91cd\u7ec4\u7b56\u7565\uff0c\u53d1\u73b0\u7b56\u7565\u591a\u6837\u6027\u9ad8\u4e14\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u5efa\u8baeNIDS\u5728n>2\u65f6\u4e0d\u5e94\u4ec5\u5e94\u7528n=2\u7684\u7b56\u7565\u3002", "motivation": "IPv4\u3001IPv6\u548cTCP\u7684\u91cd\u7ec4\u7b56\u7565\u5728\u4e0d\u540c\u534f\u8bae\u5b9e\u73b0\u4e2d\u5b58\u5728\u5dee\u5f02\uff0c\u5bfc\u81f4NIDS\u53ef\u80fd\u4e0e\u88ab\u76d1\u63a7\u7684\u4e3b\u673aOS\u5bf9\u6570\u636e\u5305\u7684\u89e3\u91ca\u4e0d\u4e00\u81f4\uff0c\u4ece\u800c\u4ea7\u751f\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86PYROLYSE\u5de5\u5177\uff0c\u7528\u4e8e\u5168\u9762\u6d4b\u8bd5\u548c\u63cf\u8ff0\u5404\u79cdIP\u548cTCP\u5b9e\u73b0\u7c7b\u578b\u7684\u91cd\u7ec4\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u6d4b\u8bd5n<=3\u7684\u91cd\u53e0\u5757\u5e8f\u5217\u6765\u9a8c\u8bc1\u5176\u6b63\u786e\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u91cd\u7ec4\u7b56\u7565\u7684\u591a\u6837\u6027\u8fdc\u8d85\u9884\u671f\uff0c\u6d4b\u8bd5\u768423\u79cd\u5b9e\u73b0\u4e2d\u89c2\u5bdf\u523014\u523020\u79cd\u4e0d\u540c\u7684\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u62a5\u544a\u4e868\u4e2a\u9519\u8bef\uff0c\u6d89\u53ca1\u4e2aOS\u30012\u4e2aNIDS\u548c2\u4e2a\u5d4c\u5165\u5f0f\u5806\u6808\uff0c\u53ef\u80fd\u5bfc\u81f4\u5b89\u5168\u95ee\u9898\u3002", "conclusion": "\u8bba\u6587\u6307\u51fa\uff0c\u5f53\u524d\u7684IP\u548cTCP\u91cd\u7ec4\u7b56\u7565\u5728n>2\u65f6\u4e0en=2\u7684\u7b56\u7565\u4e0d\u4e00\u81f4\uff0c\u56e0\u6b64NIDS\u6216\u5176\u4ed6\u7f51\u7edc\u6d41\u91cf\u5206\u6790\u5de5\u5177\u5728\u91cd\u53e0\u5757\u8d85\u8fc7\u4e24\u4e2a\u65f6\u4e0d\u5e94\u4ec5\u5e94\u7528n=2\u7684\u7b56\u7565\u3002"}}
{"id": "2508.00823", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00823", "abs": "https://arxiv.org/abs/2508.00823", "authors": ["Wenxuan Guo", "Xiuwei Xu", "Hang Yin", "Ziwei Wang", "Jianjiang Feng", "Jie Zhou", "Jiwen Lu"], "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation", "comment": "Accepted to ICCV 2025. Project page:\n  https://gwxuan.github.io/IGL-Nav/", "summary": "Visual navigation with an image as goal is a fundamental and challenging\nproblem. Conventional methods either rely on end-to-end RL learning or\nmodular-based policy with topological graph or BEV map as memory, which cannot\nfully model the geometric relationship between the explored 3D environment and\nthe goal image. In order to efficiently and accurately localize the goal image\nin 3D space, we build our navigation system upon the renderable 3D gaussian\n(3DGS) representation. However, due to the computational intensity of 3DGS\noptimization and the large search space of 6-DoF camera pose, directly\nleveraging 3DGS for image localization during agent exploration process is\nprohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D\nGaussian Localization framework for efficient and 3D-aware image-goal\nnavigation. Specifically, we incrementally update the scene representation as\nnew images arrive with feed-forward monocular prediction. Then we coarsely\nlocalize the goal by leveraging the geometric information for discrete space\nmatching, which can be equivalent to efficient 3D convolution. When the agent\nis close to the goal, we finally solve the fine target pose with optimization\nvia differentiable rendering. The proposed IGL-Nav outperforms existing\nstate-of-the-art methods by a large margin across diverse experimental\nconfigurations. It can also handle the more challenging free-view image-goal\nsetting and be deployed on real-world robotic platform using a cellphone to\ncapture goal image at arbitrary pose. Project page:\nhttps://gwxuan.github.io/IGL-Nav/.", "AI": {"tldr": "IGL-Nav\u901a\u8fc7\u589e\u91cf\u5f0f3D\u9ad8\u65af\u5b9a\u4f4d\u548c\u5206\u5c42\u7b56\u7565\uff0c\u9ad8\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4e2d\u7684\u51e0\u4f55\u5173\u7cfb\u5efa\u6a21\u548c\u5b9a\u4f4d\u6548\u7387\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5efa\u6a21\u63a2\u7d22\u76843D\u73af\u5883\u4e0e\u76ee\u6807\u56fe\u50cf\u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\uff0c\u4e143DGS\u4f18\u5316\u548c6-DoF\u76f8\u673a\u59ff\u6001\u641c\u7d22\u7a7a\u95f4\u5927\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86IGL-Nav\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u91cf\u5f0f3D\u9ad8\u65af\u5b9a\u4f4d\u548c\u5206\u5c42\u5b9a\u4f4d\u7b56\u7565\uff08\u7c97\u5b9a\u4f4d\u4e0e\u7ec6\u5b9a\u4f4d\u7ed3\u5408\uff09\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u6e32\u67d3\u4f18\u5316\u76ee\u6807\u59ff\u6001\u3002", "result": "IGL-Nav\u5728\u6548\u7387\u548c3D\u611f\u77e5\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301\u81ea\u7531\u89c6\u89d2\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\uff0c\u5e76\u80fd\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u90e8\u7f72\u3002", "conclusion": "IGL-Nav\u6846\u67b6\u5728\u591a\u6837\u5316\u7684\u5b9e\u9a8c\u914d\u7f6e\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u80fd\u5904\u7406\u66f4\u5177\u6311\u6218\u6027\u7684\u81ea\u7531\u89c6\u89d2\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\uff0c\u540c\u65f6\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e73\u53f0\u3002"}}
{"id": "2508.00772", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.00772", "abs": "https://arxiv.org/abs/2508.00772", "authors": ["Md Imranur Rahman Akib", "Fathima Binthe Muhammed", "Umit Saha", "Md Fazlul Karim Patwary", "Mehrin Anannya", "Md Alomgeer Hussein", "Md Biplob Hosen"], "title": "From Code to Career: Assessing Competitive Programmers for Industry Placement", "comment": null, "summary": "In today's fast-paced tech industry, there is a growing need for tools that\nevaluate a programmer's job readiness based on their coding performance. This\nstudy focuses on predicting the potential of Codeforces users to secure various\nlevels of software engineering jobs. The primary objective is to analyze how a\nuser's competitive programming activity correlates with their chances of\nobtaining positions, ranging from entry-level roles to jobs at major tech\ncompanies. We collect user data using the Codeforces API, process key\nperformance metrics, and build a prediction model using a Random Forest\nclassifier. The model categorizes users into four levels of employability,\nranging from those needing further development to those ready for top-tier tech\njobs. The system is implemented using Flask and deployed on Render for\nreal-time predictions. Our evaluation demonstrates that the approach\neffectively distinguishes between different skill levels based on coding\nproficiency and participation. This work lays a foundation for the use of\nmachine learning in career assessment and could be extended to predict job\nreadiness in broader technical fields.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528Codeforces\u6570\u636e\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\u9884\u6d4b\u7a0b\u5e8f\u5458\u5c31\u4e1a\u51c6\u5907\u60c5\u51b5\uff0c\u6210\u529f\u533a\u5206\u4e0d\u540c\u6280\u80fd\u6c34\u5e73\uff0c\u4e3a\u804c\u4e1a\u8bc4\u4f30\u63d0\u4f9b\u673a\u5668\u5b66\u4e60\u57fa\u7840\u3002", "motivation": "\u5feb\u901f\u53d1\u5c55\u7684\u79d1\u6280\u884c\u4e1a\u9700\u8981\u8bc4\u4f30\u7a0b\u5e8f\u5458\u57fa\u4e8e\u7f16\u7801\u8868\u73b0\u7684\u5c31\u4e1a\u51c6\u5907\u60c5\u51b5\u7684\u5de5\u5177\u3002\u672c\u7814\u7a76\u65e8\u5728\u5206\u6790\u7528\u6237\u7684\u7ade\u4e89\u7f16\u7a0b\u6d3b\u52a8\u4e0e\u5176\u83b7\u5f97\u4e0d\u540c\u7ea7\u522b\u8f6f\u4ef6\u5de5\u7a0b\u804c\u4f4d\u673a\u4f1a\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "method": "\u4f7f\u7528Codeforces API\u6536\u96c6\u7528\u6237\u6570\u636e\uff0c\u5904\u7406\u5173\u952e\u6027\u80fd\u6307\u6807\uff0c\u5e76\u5229\u7528\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u6784\u5efa\u9884\u6d4b\u6a21\u578b\u3002\u7cfb\u7edf\u901a\u8fc7Flask\u5b9e\u73b0\uff0c\u5e76\u90e8\u7f72\u5728Render\u4e0a\u4ee5\u8fdb\u884c\u5b9e\u65f6\u9884\u6d4b\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6839\u636e\u7f16\u7801\u719f\u7ec3\u5ea6\u548c\u53c2\u4e0e\u5ea6\u533a\u5206\u4e0d\u540c\u6280\u80fd\u6c34\u5e73\uff0c\u5c06\u7528\u6237\u5206\u7c7b\u4e3a\u56db\u4e2a\u5c31\u4e1a\u51c6\u5907\u7ea7\u522b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u673a\u5668\u5b66\u4e60\u5728\u804c\u4e1a\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u53ef\u4ee5\u6269\u5c55\u5230\u9884\u6d4b\u66f4\u5e7f\u6cdb\u6280\u672f\u9886\u57df\u7684\u5c31\u4e1a\u51c6\u5907\u60c5\u51b5\u3002"}}
{"id": "2508.00289", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00289", "abs": "https://arxiv.org/abs/2508.00289", "authors": ["Christian Simon", "Masato Ishii", "Akio Hayakawa", "Zhi Zhong", "Shusuke Takahashi", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models", "comment": "Accepted to ICCV 2025", "summary": "In the recent development of conditional diffusion models still require heavy\nsupervised fine-tuning for performing control on a category of tasks.\nTraining-free conditioning via guidance with off-the-shelf models is a\nfavorable alternative to avoid further fine-tuning on the base model. However,\nthe existing training-free guidance frameworks either have heavy memory\nrequirements or offer sub-optimal control due to rough estimation. These\nshortcomings limit the applicability to control diffusion models that require\nintense computation, such as Text-to-Video (T2V) diffusion models. In this\nwork, we propose Taming Inference Time Alignment for Guided Text-to-Video\nDiffusion Model, so-called TITAN-Guide, which overcomes memory space issues,\nand provides more optimal control in the guidance process compared to the\ncounterparts. In particular, we develop an efficient method for optimizing\ndiffusion latents without backpropagation from a discriminative guiding model.\nIn particular, we study forward gradient descents for guided diffusion tasks\nwith various options on directional directives. In our experiments, we\ndemonstrate the effectiveness of our approach in efficiently managing memory\nduring latent optimization, while previous methods fall short. Our proposed\napproach not only minimizes memory requirements but also significantly enhances\nT2V performance across a range of diffusion guidance benchmarks. Code, models,\nand demo are available at https://titanguide.github.io.", "AI": {"tldr": "TITAN-Guide \u662f\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u8bad\u7ec3\u514d\u8d39\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6269\u6563\u6f5c\u5728\u8868\u793a\u663e\u8457\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u5e76\u63d0\u5347T2V\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bad\u7ec3\u514d\u8d39\u5f15\u5bfc\u6846\u67b6\u56e0\u5185\u5b58\u9700\u6c42\u9ad8\u6216\u63a7\u5236\u6548\u679c\u4e0d\u4f73\u800c\u9650\u5236\u4e86\u5728\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\uff08\u5982T2V\u6269\u6563\u6a21\u578b\uff09\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u7684\u6269\u6563\u6f5c\u5728\u4f18\u5316\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86\u524d\u5411\u68af\u5ea6\u4e0b\u964d\u5728\u5f15\u5bfc\u6269\u6563\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63a2\u7d22\u4e86\u591a\u79cd\u65b9\u5411\u6027\u6307\u4ee4\u9009\u9879\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cTITAN-Guide \u5728\u6f5c\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u9ad8\u6548\u7ba1\u7406\u5185\u5b58\uff0c\u663e\u8457\u63d0\u5347\u4e86T2V\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6269\u6563\u5f15\u5bfc\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TITAN-Guide \u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u901a\u8fc7\u5224\u522b\u6027\u5f15\u5bfc\u6a21\u578b\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u5373\u53ef\u4f18\u5316\u6269\u6563\u6f5c\u5728\u8868\u793a\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u9700\u6c42\u5e76\u63d0\u5347\u4e86\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00576", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00576", "abs": "https://arxiv.org/abs/2508.00576", "authors": ["Zhanliang Wang", "Kai Wang"], "title": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models", "comment": null, "summary": "Multimodal AI models have achieved impressive performance in tasks that\nrequire integrating information from multiple modalities, such as vision and\nlanguage. However, their \"black-box\" nature poses a major barrier to deployment\nin high-stakes applications where interpretability and trustworthiness are\nessential. How to explain cross-modal interactions in multimodal AI models\nremains a major challenge. While existing model explanation methods, such as\nattention map and Grad-CAM, offer coarse insights into cross-modal\nrelationships, they cannot precisely quantify the synergistic effects between\nmodalities, and are limited to open-source models with accessible internal\nweights. Here we introduce MultiSHAP, a model-agnostic interpretability\nframework that leverages the Shapley Interaction Index to attribute multimodal\npredictions to pairwise interactions between fine-grained visual and textual\nelements (such as image patches and text tokens), while being applicable to\nboth open- and closed-source models. Our approach provides: (1) instance-level\nexplanations that reveal synergistic and suppressive cross-modal effects for\nindividual samples - \"why the model makes a specific prediction on this input\",\nand (2) dataset-level explanation that uncovers generalizable interaction\npatterns across samples - \"how the model integrates information across\nmodalities\". Experiments on public multimodal benchmarks confirm that MultiSHAP\nfaithfully captures cross-modal reasoning mechanisms, while real-world case\nstudies demonstrate its practical utility. Our framework is extensible beyond\ntwo modalities, offering a general solution for interpreting complex multimodal\nAI models.", "AI": {"tldr": "MultiSHAP\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7Shapley Interaction Index\u91cf\u5316\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u9002\u7528\u4e8e\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u63d0\u4f9b\u5b9e\u4f8b\u7ea7\u548c\u6570\u636e\u96c6\u7ea7\u89e3\u91ca\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001AI\u6a21\u578b\u56e0'\u9ed1\u76d2'\u7279\u6027\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u6311\u6218\u3002", "method": "\u5229\u7528Shapley Interaction Index\uff0cMultiSHAP\u5c06\u591a\u6a21\u6001\u9884\u6d4b\u5f52\u56e0\u4e8e\u89c6\u89c9\u548c\u6587\u672c\u7ec6\u7c92\u5ea6\u5143\u7d20\u4e4b\u95f4\u7684\u6210\u5bf9\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u516c\u5171\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86MultiSHAP\u80fd\u51c6\u786e\u6355\u6349\u8de8\u6a21\u6001\u63a8\u7406\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u6548\u7528\u3002", "conclusion": "MultiSHAP\u6846\u67b6\u4e3a\u591a\u6a21\u6001AI\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u80fd\u591f\u7cbe\u786e\u91cf\u5316\u6a21\u6001\u95f4\u7684\u534f\u540c\u6548\u5e94\uff0c\u9002\u7528\u4e8e\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u6269\u5c55\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.00792", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.00792", "abs": "https://arxiv.org/abs/2508.00792", "authors": ["Aashay Arora", "Diego Davila", "Jonathan Guiang", "Frank W\u00fcrthwein", "Harvey Newman", "Justas Balcas", "Tom Lehman", "Xi Yang"], "title": "Data Movement Manager (DMM) for the SENSE-Rucio Interoperation Prototype", "comment": "Submitted to CHEP 24", "summary": "The Data Movement Manager (DMM) is a prototype interface that connects CERN's\ndata management software, Rucio, with the Sofware-Defined Networking (SDN)\nservice SENSE by ESNet. It enables SDN-enabled high-energy physics data flows\nusing the existing worldwide LHC computing grid infrastructure. A key feature\nof DMM is transfer priority-based bandwidth allocation, optimizing network\nusage. Additionally, it provides fine-grained monitoring of underperforming\nflows by leveraging end-to-end data flow monitoring. This is achieved through\naccess to host-level (network interface) throughput metrics and transfer-tool\n(FTS) data transfer job-level metrics. This paper details the design and\nimplementation of DMM.", "AI": {"tldr": "DMM\u662f\u4e00\u4e2a\u8fde\u63a5Rucio\u548cSENSE\u7684\u539f\u578b\u63a5\u53e3\uff0c\u4f18\u5316\u4e86\u9ad8\u80fd\u7269\u7406\u6570\u636e\u6d41\u7684\u7f51\u7edc\u4f7f\u7528\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u4f18\u5148\u7ea7\u7684\u5e26\u5bbd\u5206\u914d\u548c\u7ec6\u7c92\u5ea6\u76d1\u63a7\u3002", "motivation": "\u4e3a\u4e86\u4f18\u5316\u9ad8\u80fd\u7269\u7406\u6570\u636e\u6d41\u5728\u5168\u7403LHC\u8ba1\u7b97\u7f51\u683c\u57fa\u7840\u8bbe\u65bd\u4e0a\u7684\u4f20\u8f93\u6548\u7387\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u5e76\u5b9e\u73b0DMM\u539f\u578b\u63a5\u53e3\uff0c\u5229\u7528\u4e3b\u673a\u7ea7\u541e\u5410\u91cf\u6307\u6807\u548cFTS\u6570\u636e\u4f20\u8f93\u4f5c\u4e1a\u7ea7\u6307\u6807\u8fdb\u884c\u76d1\u63a7\u3002", "result": "DMM\u5b9e\u73b0\u4e86SDN-enabled\u7684\u9ad8\u80fd\u7269\u7406\u6570\u636e\u6d41\uff0c\u5e76\u901a\u8fc7\u4f18\u5148\u7ea7\u5206\u914d\u548c\u76d1\u63a7\u63d0\u5347\u4e86\u7f51\u7edc\u6027\u80fd\u3002", "conclusion": "DMM\u6210\u529f\u5730\u5c06Rucio\u4e0eSENSE\u8fde\u63a5\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u4f18\u5148\u7ea7\u7684\u5e26\u5bbd\u5206\u914d\u548c\u7ec6\u7c92\u5ea6\u76d1\u63a7\uff0c\u4f18\u5316\u4e86\u7f51\u7edc\u4f7f\u7528\u3002"}}
{"id": "2508.00298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00298", "abs": "https://arxiv.org/abs/2508.00298", "authors": ["Jin Lyu", "Liang An", "Li Lin", "Pujin Cheng", "Yebin Liu", "Xiaoying Tang"], "title": "AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer", "comment": "arXiv admin note: substantial text overlap with arXiv:2412.00837", "summary": "In the era of foundation models, achieving a unified understanding of\ndifferent dynamic objects through a single network has the potential to empower\nstronger spatial intelligence. Moreover, accurate estimation of animal pose and\nshape across diverse species is essential for quantitative analysis in\nbiological research. However, this topic remains underexplored due to the\nlimited network capacity of previous methods and the scarcity of comprehensive\nmulti-species datasets. To address these limitations, we introduce AniMer+, an\nextended version of our scalable AniMer framework. In this paper, we focus on a\nunified approach for reconstructing mammals (mammalia) and birds (aves). A key\ninnovation of AniMer+ is its high-capacity, family-aware Vision Transformer\n(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture\npartitions network layers into taxa-specific components (for mammalia and aves)\nand taxa-shared components, enabling efficient learning of both distinct and\ncommon anatomical features within a single model. To overcome the critical\nshortage of 3D training data, especially for birds, we introduce a\ndiffusion-based conditional image generation pipeline. This pipeline produces\ntwo large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for\nbirds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for\nbirds, which is crucial for resolving single-view depth ambiguities. Trained on\nan aggregated collection of 41.3k mammalian and 12.4k avian images (combining\nreal and synthetic data), our method demonstrates superior performance over\nexisting approaches across a wide range of benchmarks, including the\nchallenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the\neffectiveness of both our novel network architecture and the generated\nsynthetic datasets in enhancing real-world application performance.", "AI": {"tldr": "AniMer+\u901a\u8fc7\u5bb6\u65cf\u611f\u77e5ViT\u548c\u5408\u6210\u6570\u636e\u96c6\uff0c\u7edf\u4e00\u4f30\u8ba1\u54fa\u4e73\u52a8\u7269\u548c\u9e1f\u7c7b\u7684\u59ff\u6001\u4e0e\u5f62\u72b6\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u56e0\u7f51\u7edc\u5bb9\u91cf\u9650\u5236\u548c\u591a\u7269\u79cd\u6570\u636e\u96c6\u7a00\u7f3a\u5bfc\u81f4\u7684\u8de8\u7269\u79cd\u52a8\u7269\u59ff\u6001\u4e0e\u5f62\u72b6\u4f30\u8ba1\u7814\u7a76\u4e0d\u8db3\u95ee\u9898\u3002", "method": "AniMer+\u91c7\u7528\u5bb6\u65cf\u611f\u77e5Vision Transformer\uff08ViT\uff09\u548cMixture-of-Experts\uff08MoE\uff09\u8bbe\u8ba1\uff0c\u5c06\u7f51\u7edc\u5c42\u5206\u4e3a\u7279\u5b9a\u7c7b\u7fa4\u548c\u5171\u4eab\u7c7b\u7fa4\u7ec4\u4ef6\uff0c\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\uff08CtrlAni3D\u548cCtrlAVES3D\uff09\u3002", "result": "\u5728\u5305\u542b41.3k\u54fa\u4e73\u52a8\u7269\u548c12.4k\u9e1f\u7c7b\u56fe\u50cf\u7684\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0cAniMer+\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u5177\u6709\u6311\u6218\u6027\u7684Animal Kingdom\u6570\u636e\u96c6\u3002", "conclusion": "AniMer+\u901a\u8fc7\u9ad8\u5bb9\u91cf\u7684\u5bb6\u65cf\u611f\u77e5ViT\u548cMoE\u8bbe\u8ba1\uff0c\u7ed3\u5408\u5408\u6210\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u7269\u79cd\u52a8\u7269\u59ff\u6001\u548c\u5f62\u72b6\u4f30\u8ba1\u7684\u6027\u80fd\uff0c\u4e3a\u751f\u7269\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5b9a\u91cf\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2508.00581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00581", "abs": "https://arxiv.org/abs/2508.00581", "authors": ["Ruiqing Ding", "Qianfang Sun", "Yongkang Leng", "Hui Yin", "Xiaojian Li"], "title": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation", "comment": "16 pages, 10 figures", "summary": "Pre-consultation is a critical component of effective healthcare delivery.\nHowever, generating comprehensive pre-consultation questionnaires from complex,\nvoluminous Electronic Medical Records (EMRs) is a challenging task. Direct\nLarge Language Model (LLM) approaches face difficulties in this task,\nparticularly regarding information completeness, logical order, and\ndisease-level synthesis. To address this issue, we propose a novel multi-stage\nLLM-driven framework: Stage 1 extracts atomic assertions (key facts with\ntiming) from EMRs; Stage 2 constructs personal causal networks and synthesizes\ndisease knowledge by clustering representative networks from an EMR corpus;\nStage 3 generates tailored personal and standardized disease-specific\nquestionnaires based on these structured representations. This framework\novercomes limitations of direct methods by building explicit clinical\nknowledge. Evaluated on a real-world EMR dataset and validated by clinical\nexperts, our method demonstrates superior performance in information coverage,\ndiagnostic relevance, understandability, and generation time, highlighting its\npractical potential to enhance patient information collection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5LLM\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u4eceEMR\u751f\u6210\u9884\u54a8\u8be2\u95ee\u5377\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5LLM\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7531\u4e8e\u76f4\u63a5\u4ece\u590d\u6742\u4e14\u5e9e\u5927\u7684\u7535\u5b50\u533b\u7597\u8bb0\u5f55\uff08EMRs\uff09\u751f\u6210\u5168\u9762\u7684\u9884\u54a8\u8be2\u95ee\u5377\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65b9\u6cd5\u5728\u4fe1\u606f\u5b8c\u6574\u6027\u3001\u903b\u8f91\u987a\u5e8f\u548c\u75be\u75c5\u7ea7\u7efc\u5408\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u9636\u6bb5LLM\u9a71\u52a8\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4eceEMR\u4e2d\u63d0\u53d6\u539f\u5b50\u65ad\u8a00\uff1b\u7b2c\u4e8c\u9636\u6bb5\u6784\u5efa\u4e2a\u4eba\u56e0\u679c\u7f51\u7edc\u5e76\u4eceEMR\u8bed\u6599\u5e93\u4e2d\u805a\u7c7b\u4ee3\u8868\u6027\u7f51\u7edc\u4ee5\u7efc\u5408\u75be\u75c5\u77e5\u8bc6\uff1b\u7b2c\u4e09\u9636\u6bb5\u57fa\u4e8e\u8fd9\u4e9b\u7ed3\u6784\u5316\u8868\u793a\u751f\u6210\u4e2a\u6027\u5316\u7684\u6807\u51c6\u5316\u75be\u75c5\u7279\u5b9a\u95ee\u5377\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684EMR\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5e76\u7531\u4e34\u5e8a\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u4fe1\u606f\u8986\u76d6\u3001\u8bca\u65ad\u76f8\u5173\u6027\u3001\u53ef\u7406\u89e3\u6027\u548c\u751f\u6210\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u591a\u9636\u6bb5LLM\u9a71\u52a8\u6846\u67b6\u5728\u4fe1\u606f\u8986\u76d6\u3001\u8bca\u65ad\u76f8\u5173\u6027\u3001\u53ef\u7406\u89e3\u6027\u548c\u751f\u6210\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u73b0\u4e86\u63d0\u5347\u60a3\u8005\u4fe1\u606f\u6536\u96c6\u6548\u7387\u7684\u5b9e\u9645\u6f5c\u529b\u3002"}}
{"id": "2508.00632", "categories": ["cs.AI", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.00632", "abs": "https://arxiv.org/abs/2508.00632", "authors": ["Alexia Jolicoeur-Martineau"], "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings", "comment": null, "summary": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86AVR-Eval\u548cAVR-Agent\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u751f\u6210\u4ea4\u4e92\u5f0f\u97f3\u89c6\u9891\u5185\u5bb9\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u5355\u6b21\u751f\u6210\u65b9\u6cd5\uff0c\u4f46\u5229\u7528\u8d44\u4ea7\u548c\u53cd\u9988\u80fd\u529b\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dLLMs\u5728\u751f\u6210\u590d\u6742\u4ea4\u4e92\u5f0f\u5185\u5bb9\uff08\u5982\u89c6\u9891\u6e38\u620f\uff09\u65f6\u7f3a\u4e4f\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u548c\u5229\u7528\u9ad8\u8d28\u91cf\u8d44\u4ea7\u80fd\u529b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86AVR-Eval\u8bc4\u4f30\u6307\u6807\u548cAVR-Agent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7ed3\u5408\u5168\u6a21\u6001\u6a21\u578b\u548c\u6587\u672c\u6a21\u578b\u8fdb\u884c\u5185\u5bb9\u8d28\u91cf\u8bc4\u4f30\u4e0e\u4ee3\u7801\u751f\u6210\u4f18\u5316\u3002", "result": "AVR-Agent\u751f\u6210\u7684\u5185\u5bb9\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u6b21\u751f\u6210\u7684\u5185\u5bb9\uff0c\u4f46\u672a\u80fd\u6709\u6548\u5229\u7528\u81ea\u5b9a\u4e49\u8d44\u4ea7\u548cAVR\u53cd\u9988\u3002", "conclusion": "\u5f53\u524dAI\u5728\u751f\u6210\u4ea4\u4e92\u5f0f\u97f3\u89c6\u9891\u5185\u5bb9\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5229\u7528\u9ad8\u8d28\u91cf\u8d44\u4ea7\u548c\u97f3\u89c6\u9891\u53cd\u9988\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u63ed\u793a\u4e86\u4eba\u7c7b\u4e0e\u673a\u5668\u5185\u5bb9\u521b\u4f5c\u65b9\u6cd5\u7684\u6839\u672c\u5dee\u5f02\u3002"}}
{"id": "2508.00308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00308", "abs": "https://arxiv.org/abs/2508.00308", "authors": ["Chunyan She", "Fujun Han", "Chengyu Fang", "Shukai Duan", "Lidan Wang"], "title": "Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement", "comment": "Accepted by ACM MM 2025", "summary": "The event camera, benefiting from its high dynamic range and low latency,\nprovides performance gain for low-light image enhancement. Unlike frame-based\ncameras, it records intensity changes with extremely high temporal resolution,\ncapturing sufficient structure information. Currently, existing event-based\nmethods feed a frame and events directly into a single model without fully\nexploiting modality-specific advantages, which limits their performance.\nTherefore, by analyzing the role of each sensing modality, the enhancement\npipeline is decoupled into two stages: visibility restoration and structure\nrefinement. In the first stage, we design a visibility restoration network with\namplitude-phase entanglement by rethinking the relationship between amplitude\nand phase components in Fourier space. In the second stage, a fusion strategy\nwith dynamic alignment is proposed to mitigate the spatial mismatch caused by\nthe temporal resolution discrepancy between two sensing modalities, aiming to\nrefine the structure information of the image enhanced by the visibility\nrestoration network. In addition, we utilize spatial-frequency interpolation to\nsimulate negative samples with diverse illumination, noise and artifact\ndegradations, thereby developing a contrastive loss that encourages the model\nto learn discriminative representations. Experiments demonstrate that the\nproposed method outperforms state-of-the-art models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u53ef\u89c1\u6027\u6062\u590d\u548c\u7ed3\u6784\u7ec6\u5316\uff0c\u7ed3\u5408\u52a8\u6001\u5bf9\u9f50\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u4ef6\u589e\u5f3a\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u6a21\u6001\u7279\u5f02\u6027\u4f18\u52bf\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002\u56e0\u6b64\uff0c\u901a\u8fc7\u5206\u6790\u6bcf\u79cd\u4f20\u611f\u6a21\u6001\u7684\u4f5c\u7528\uff0c\u5c06\u589e\u5f3a\u6d41\u7a0b\u89e3\u8026\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u53ef\u89c1\u6027\u6062\u590d\u548c\u7ed3\u6784\u7ec6\u5316\u3002", "method": "1. \u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u632f\u5e45-\u76f8\u4f4d\u7ea0\u7f20\u7684\u53ef\u89c1\u6027\u6062\u590d\u7f51\u7edc\u30022. \u63d0\u51fa\u4e86\u4e00\u4e2a\u52a8\u6001\u5bf9\u9f50\u7684\u878d\u5408\u7b56\u7565\u4ee5\u51cf\u5c11\u4e24\u79cd\u4f20\u611f\u6a21\u6001\u4e4b\u95f4\u7684\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\u30023. \u5229\u7528\u7a7a\u95f4-\u9891\u7387\u63d2\u503c\u6a21\u62df\u591a\u6837\u5316\u7684\u8d1f\u6837\u672c\uff0c\u5f00\u53d1\u4e86\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u5224\u522b\u6027\u8868\u793a\u5b66\u4e60\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6a21\u578b\u3002"}}
{"id": "2508.00658", "categories": ["cs.AI", "cs.LG", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.00658", "abs": "https://arxiv.org/abs/2508.00658", "authors": ["Chakattrai Sookkongwaree", "Tattep Lakmuang", "Chainarong Amornbunchornvej"], "title": "Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies", "comment": "First draft", "summary": "Understanding causal relationships in time series is fundamental to many\ndomains, including neuroscience, economics, and behavioral science. Granger\ncausality is one of the well-known techniques for inferring causality in time\nseries. Typically, Granger causality frameworks have a strong fix-lag\nassumption between cause and effect, which is often unrealistic in complex\nsystems. While recent work on variable-lag Granger causality (VLGC) addresses\nthis limitation by allowing a cause to influence an effect with different time\nlags at each time point, it fails to account for the fact that causal\ninteractions may vary not only in time delay but also across frequency bands.\nFor example, in brain signals, alpha-band activity may influence another region\nwith a shorter delay than slower delta-band oscillations. In this work, we\nformalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a\nnovel framework that generalizes traditional VLGC by explicitly modeling\nfrequency-dependent causal delays. We provide a formal definition of MB-VLGC,\ndemonstrate its theoretical soundness, and propose an efficient inference\npipeline. Extensive experiments across multiple domains demonstrate that our\nframework significantly outperforms existing methods on both synthetic and\nreal-world datasets, confirming its broad applicability to any type of time\nseries data. Code and datasets are publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faMB-VLGC\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfGranger\u56e0\u679c\u6027\u5728\u9891\u5e26\u548c\u65f6\u95f4\u5ef6\u8fdf\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edfGranger\u56e0\u679c\u6027\u548cVLGC\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u56e0\u679c\u4ea4\u4e92\u5728\u65f6\u95f4\u5ef6\u8fdf\u548c\u9891\u5e26\u4e0a\u7684\u53d8\u5316\uff0c\u7279\u522b\u662f\u5728\u8111\u4fe1\u53f7\u7b49\u590d\u6742\u7cfb\u7edf\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684MB-VLGC\u6846\u67b6\uff0c\u5305\u62ec\u6b63\u5f0f\u5b9a\u4e49\u3001\u7406\u8bba\u8bc1\u660e\u548c\u9ad8\u6548\u63a8\u7406\u6d41\u7a0b\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cMB-VLGC\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MB-VLGC\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u9891\u7387\u4f9d\u8d56\u7684\u56e0\u679c\u5ef6\u8fdf\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002"}}
{"id": "2508.00311", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00311", "abs": "https://arxiv.org/abs/2508.00311", "authors": ["Yufeng Zhong", "Zhixiong Zeng", "Lei Chen", "Longrong Yang", "Liming Zheng", "Jing Huang", "Siqi Yang", "Lin Ma"], "title": "DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios", "comment": null, "summary": "Optical Character Recognition (OCR) for mathematical formula is essential for\nthe intelligent analysis of scientific literature. However, both task-specific\nand general vision-language models often struggle to handle the structural\ndiversity, complexity, and real-world variability inherent in mathematical\ncontent. In this work, we present DocTron-Formula, a unified framework built\nupon general vision-language models, thereby eliminating the need for\nspecialized architectures. Furthermore, we introduce CSFormula, a large-scale\nand challenging dataset that encompasses multidisciplinary and structurally\ncomplex formulas at the line, paragraph, and page levels. Through\nstraightforward supervised fine-tuning, our approach achieves state-of-the-art\nperformance across a variety of styles, scientific domains, and complex\nlayouts. Experimental results demonstrate that our method not only surpasses\nspecialized models in terms of accuracy and robustness, but also establishes a\nnew paradigm for the automated understanding of complex scientific documents.", "AI": {"tldr": "DocTron-Formula\u901a\u8fc7\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548cCSFormula\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u6570\u5b66\u516c\u5f0fOCR\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u6570\u5b66\u516c\u5f0fOCR\u5bf9\u4e8e\u79d1\u5b66\u6587\u732e\u7684\u667a\u80fd\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u4efb\u52a1\u4e13\u7528\u548c\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5904\u7406\u6570\u5b66\u5185\u5bb9\u7684\u7ed3\u6784\u591a\u6837\u6027\u3001\u590d\u6742\u6027\u548c\u73b0\u5b9e\u53d8\u5f02\u6027\u3002", "method": "\u63d0\u51fa\u4e86DocTron-Formula\uff0c\u4e00\u4e2a\u57fa\u4e8e\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u65e0\u9700\u4e13\u7528\u67b6\u6784\u3002\u540c\u65f6\u5f15\u5165\u4e86CSFormula\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u5b66\u79d1\u548c\u7ed3\u6784\u590d\u6742\u7684\u516c\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u8d85\u8d8a\u4e86\u4e13\u7528\u6a21\u578b\uff0c\u8fd8\u4e3a\u590d\u6742\u79d1\u5b66\u6587\u6863\u7684\u81ea\u52a8\u7406\u89e3\u6811\u7acb\u4e86\u65b0\u6807\u6746\u3002", "conclusion": "DocTron-Formula\u901a\u8fc7\u7b80\u5355\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u5728\u591a\u79cd\u98ce\u683c\u3001\u79d1\u5b66\u9886\u57df\u548c\u590d\u6742\u5e03\u5c40\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u79d1\u5b66\u6587\u6863\u7684\u81ea\u52a8\u7406\u89e3\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.00665", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00665", "abs": "https://arxiv.org/abs/2508.00665", "authors": ["Maryam Mosleh", "Marie Devlin", "Ellis Solaiman"], "title": "Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI", "comment": null, "summary": "Artificial intelligence-driven adaptive learning systems are reshaping\neducation through data-driven adaptation of learning experiences. Yet many of\nthese systems lack transparency, offering limited insight into how decisions\nare made. Most explainable AI (XAI) techniques focus on technical outputs but\nneglect user roles and comprehension. This paper proposes a hybrid framework\nthat integrates traditional XAI techniques with generative AI models and user\npersonalisation to generate multimodal, personalised explanations tailored to\nuser needs. We redefine explainability as a dynamic communication process\ntailored to user roles and learning goals. We outline the framework's design,\nkey XAI limitations in education, and research directions on accuracy,\nfairness, and personalisation. Our aim is to move towards explainable AI that\nenhances transparency while supporting user-centred experiences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6df7\u5408XAI\u4e0e\u751f\u6210\u5f0fAI\u7684\u6846\u67b6\uff0c\u751f\u6210\u4e2a\u6027\u5316\u89e3\u91ca\uff0c\u5f3a\u8c03\u7528\u6237\u89d2\u8272\u4e0e\u76ee\u6807\uff0c\u63a8\u52a8\u900f\u660e\u4e14\u7528\u6237\u53cb\u597d\u7684AI\u6559\u80b2\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dAI\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u7cfb\u7edf\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u4e14\u591a\u6570XAI\u6280\u672f\u5ffd\u89c6\u7528\u6237\u89d2\u8272\u548c\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4f20\u7edfXAI\u6280\u672f\u4e0e\u751f\u6210\u5f0fAI\u6a21\u578b\u53ca\u7528\u6237\u4e2a\u6027\u5316\uff0c\u751f\u6210\u7b26\u5408\u7528\u6237\u9700\u6c42\u7684\u591a\u6a21\u6001\u4e2a\u6027\u5316\u89e3\u91ca\u3002", "result": "\u91cd\u65b0\u5b9a\u4e49\u53ef\u89e3\u91ca\u6027\u4e3a\u52a8\u6001\u6c9f\u901a\u8fc7\u7a0b\uff0c\u5e76\u63a2\u8ba8\u4e86\u6846\u67b6\u8bbe\u8ba1\u3001XAI\u5728\u6559\u80b2\u4e2d\u7684\u5c40\u9650\u6027\u53ca\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u63a8\u52a8\u53ef\u89e3\u91caAI\u7684\u53d1\u5c55\uff0c\u4ee5\u589e\u5f3a\u900f\u660e\u5ea6\u5e76\u652f\u6301\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u4f53\u9a8c\u3002"}}
{"id": "2508.00312", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00312", "abs": "https://arxiv.org/abs/2508.00312", "authors": ["Suhang Cai", "Xiaohao Peng", "Chong Wang", "Xiaojie Cai", "Jiangbo Qian"], "title": "GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection", "comment": null, "summary": "Video anomaly detection (VAD) plays a critical role in public safety\napplications such as intelligent surveillance. However, the rarity,\nunpredictability, and high annotation cost of real-world anomalies make it\ndifficult to scale VAD datasets, which limits the performance and\ngeneralization ability of existing models. To address this challenge, we\npropose a generative video-enhanced weakly-supervised video anomaly detection\n(GV-VAD) framework that leverages text-conditioned video generation models to\nproduce semantically controllable and physically plausible synthetic videos.\nThese virtual videos are used to augment training data at low cost. In\naddition, a synthetic sample loss scaling strategy is utilized to control the\ninfluence of generated synthetic samples for efficient training. The\nexperiments show that the proposed framework outperforms state-of-the-art\nmethods on UCF-Crime datasets. The code is available at\nhttps://github.com/Sumutan/GV-VAD.git.", "AI": {"tldr": "GV-VAD\u5229\u7528\u751f\u6210\u5408\u6210\u89c6\u9891\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u5f02\u5e38\u6570\u636e\u7a00\u7f3a\u3001\u4e0d\u53ef\u9884\u6d4b\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u5347\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u6761\u4ef6\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff08GV-VAD\uff09\uff0c\u5229\u7528\u5408\u6210\u89c6\u9891\u4f4e\u6210\u672c\u6269\u5145\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u91c7\u7528\u5408\u6210\u6837\u672c\u635f\u5931\u7f29\u653e\u7b56\u7565\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5728UCF-Crime\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "GV-VAD\u6846\u67b6\u901a\u8fc7\u751f\u6210\u5408\u6210\u89c6\u9891\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u5e76\u5728UCF-Crime\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.00674", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00674", "abs": "https://arxiv.org/abs/2508.00674", "authors": ["Banan Alkhateeb", "Ellis Solaiman"], "title": "Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations", "comment": null, "summary": "Social media platforms today strive to improve user experience through AI\nrecommendations, yet the value of such recommendations vanishes as users do not\nunderstand the reasons behind them. This issue arises because explainability in\nsocial media is general and lacks alignment with user-specific needs. In this\nvision paper, we outline a user-segmented and context-aware explanation layer\nby proposing a visual explanation system with diverse explanation methods. The\nproposed system is framed by the variety of user needs and contexts, showing\nexplanations in different visualized forms, including a technically detailed\nversion for AI experts and a simplified one for lay users. Our framework is the\nfirst to jointly adapt explanation style (visual vs. numeric) and granularity\n(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will\nvalidate its impact on decision-making and trust.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u6237\u5206\u6bb5\u7684\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89c6\u89c9\u89e3\u91ca\u7cfb\u7edf\uff0c\u9996\u6b21\u5728\u4e00\u4e2a\u6d41\u7a0b\u4e2d\u540c\u65f6\u8c03\u6574\u89e3\u91ca\u98ce\u683c\u548c\u7c92\u5ea6\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u540c\u7528\u6237\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u8bd5\u70b9\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u7684AI\u63a8\u8350\u89e3\u91ca\u666e\u904d\u7f3a\u4e4f\u9488\u5bf9\u7528\u6237\u7279\u5b9a\u9700\u6c42\u7684\u4e2a\u6027\u5316\uff0c\u5bfc\u81f4\u7528\u6237\u4e0d\u7406\u89e3\u63a8\u8350\u539f\u56e0\uff0c\u964d\u4f4e\u4e86\u63a8\u8350\u7684\u4ef7\u503c\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u89c6\u89c9\u89e3\u91ca\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u6839\u636e\u7528\u6237\u9700\u6c42\u548c\u4e0a\u4e0b\u6587\u5c55\u793a\u4e0d\u540c\u5f62\u5f0f\u7684\u89e3\u91ca\uff0c\u5305\u62ec\u9488\u5bf9AI\u4e13\u5bb6\u7684\u6280\u672f\u8be6\u7ec6\u7248\u672c\u548c\u9488\u5bf9\u666e\u901a\u7528\u6237\u7684\u7b80\u5316\u7248\u672c\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u5305\u542b30\u540dX\u7528\u6237\u7684\u516c\u5f00\u8bd5\u70b9\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5bf9\u7528\u6237\u51b3\u7b56\u548c\u4fe1\u4efb\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u6237\u5206\u6bb5\u7684\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89e3\u91ca\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u53ef\u89c6\u5316\u89e3\u91ca\u65b9\u6cd5\uff0c\u9996\u6b21\u5728\u4e00\u4e2a\u6d41\u7a0b\u4e2d\u540c\u65f6\u8c03\u6574\u89e3\u91ca\u98ce\u683c\u548c\u7c92\u5ea6\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u540c\u7528\u6237\u7684\u9700\u6c42\u3002"}}
{"id": "2508.00319", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00319", "abs": "https://arxiv.org/abs/2508.00319", "authors": ["Sunghyun Park", "Seokeon Choi", "Hyoungwoo Park", "Sungrack Yun"], "title": "Steering Guidance for Personalized Text-to-Image Diffusion Models", "comment": "ICCV 2025", "summary": "Personalizing text-to-image diffusion models is crucial for adapting the\npre-trained models to specific target concepts, enabling diverse image\ngeneration. However, fine-tuning with few images introduces an inherent\ntrade-off between aligning with the target distribution (e.g., subject\nfidelity) and preserving the broad knowledge of the original model (e.g., text\neditability). Existing sampling guidance methods, such as classifier-free\nguidance (CFG) and autoguidance (AG), fail to effectively guide the output\ntoward well-balanced space: CFG restricts the adaptation to the target\ndistribution, while AG compromises text alignment. To address these\nlimitations, we propose personalization guidance, a simple yet effective method\nleveraging an unlearned weak model conditioned on a null text prompt. Moreover,\nour method dynamically controls the extent of unlearning in a weak model\nthrough weight interpolation between pre-trained and fine-tuned models during\ninference. Unlike existing guidance methods, which depend solely on guidance\nscales, our method explicitly steers the outputs toward a balanced latent space\nwithout additional computational overhead. Experimental results demonstrate\nthat our proposed guidance can improve text alignment and target distribution\nfidelity, integrating seamlessly with various fine-tuning strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e2a\u6027\u5316\u5f15\u5bfc\u65b9\u6cd5\uff0c\u52a8\u6001\u63a7\u5236\u5f31\u6a21\u578b\u5b66\u4e60\u7a0b\u5ea6\uff0c\u5e73\u8861\u76ee\u6807\u5206\u5e03\u5bf9\u9f50\u4e0e\u6587\u672c\u53ef\u7f16\u8f91\u6027\uff0c\u5b9e\u9a8c\u6548\u679c\u663e\u8457\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u91c7\u6837\u5f15\u5bfc\u65b9\u6cd5\uff08\u5982CFG\u548cAG\uff09\u5728\u76ee\u6807\u5206\u5e03\u5bf9\u9f50\u548c\u6587\u672c\u53ef\u7f16\u8f91\u6027\u4e4b\u95f4\u7684\u6743\u8861\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u672a\u5b66\u4e60\u5f31\u6a21\u578b\u7684\u4e2a\u6027\u5316\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u6743\u91cd\u63d2\u503c\u52a8\u6001\u63a7\u5236\u672a\u5b66\u4e60\u7a0b\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u5347\u6587\u672c\u5bf9\u9f50\u548c\u76ee\u6807\u5206\u5e03\u4fdd\u771f\u5ea6\uff0c\u4e14\u4e0e\u591a\u79cd\u5fae\u8c03\u7b56\u7565\u517c\u5bb9\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4e2a\u6027\u5316\u5f15\u5bfc\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5c11\u6837\u672c\u5fae\u8c03\u4e2d\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u63a7\u5236\u5f31\u6a21\u578b\u7684\u5b66\u4e60\u7a0b\u5ea6\uff0c\u5b9e\u73b0\u4e86\u76ee\u6807\u5206\u5e03\u5bf9\u9f50\u4e0e\u6587\u672c\u53ef\u7f16\u8f91\u6027\u7684\u5e73\u8861\u3002"}}
{"id": "2508.00784", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00784", "abs": "https://arxiv.org/abs/2508.00784", "authors": ["Tom Or", "Omri Azencot"], "title": "Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics", "comment": null, "summary": "Generative models achieve remarkable results in multiple data domains,\nincluding images and texts, among other examples. Unfortunately, malicious\nusers exploit synthetic media for spreading misinformation and disseminating\ndeepfakes. Consequently, the need for robust and stable fake detectors is\npressing, especially when new generative models appear everyday. While the\nmajority of existing work train classifiers that discriminate between real and\nfake information, such tools typically generalize only within the same family\nof generators and data modalities, yielding poor results on other generative\nclasses and data domains. Towards a universal classifier, we propose the use of\nlarge pre-trained multi-modal models for the detection of generative content.\nEffectively, we show that the latent code of these models naturally captures\ninformation discriminating real from fake. Building on this observation, we\ndemonstrate that linear classifiers trained on these features can achieve\nstate-of-the-art results across various modalities, while remaining\ncomputationally efficient, fast to train, and effective even in few-shot\nsettings. Our work primarily focuses on fake detection in audio and images,\nachieving performance that surpasses or matches that of strong baseline\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u5728\u7f16\u7801\u8bad\u7ec3\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u7684\u9ad8\u6548\u5047\u68c0\u6d4b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6076\u610f\u7528\u6237\u5229\u7528\u5408\u6210\u5a92\u4f53\u4f20\u64ad\u9519\u8bef\u4fe1\u606f\u548c\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\uff0c\u73b0\u6709\u5206\u7c7b\u5668\u901a\u5e38\u4ec5\u5728\u540c\u4e00\u751f\u6210\u5668\u5bb6\u65cf\u548c\u6570\u636e\u6a21\u6001\u5185\u6709\u6548\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e9f\u9700\u4e00\u79cd\u901a\u7528\u5206\u7c7b\u5668\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u5927\u578b\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\uff0c\u5229\u7528\u5176\u6f5c\u5728\u7f16\u7801\u81ea\u7136\u533a\u5206\u771f\u5b9e\u4e0e\u865a\u5047\u4fe1\u606f\uff0c\u5e76\u5728\u7ebf\u6027\u5206\u7c7b\u5668\u4e0a\u8bad\u7ec3\u8fd9\u4e9b\u7279\u5f81\u3002", "result": "\u5728\u97f3\u9891\u548c\u56fe\u50cf\u7684\u5047\u68c0\u6d4b\u4e2d\uff0c\u6027\u80fd\u8d85\u8d8a\u6216\u5339\u914d\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u5728\u7f16\u7801\u53ef\u4ee5\u6709\u6548\u533a\u5206\u771f\u5b9e\u4e0e\u751f\u6210\u5185\u5bb9\uff0c\u7ebf\u6027\u5206\u7c7b\u5668\u5728\u8fd9\u4e9b\u7279\u5f81\u4e0a\u8bad\u7ec3\u540e\uff0c\u80fd\u5728\u591a\u79cd\u6a21\u6001\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4e14\u8ba1\u7b97\u9ad8\u6548\u3001\u8bad\u7ec3\u5feb\u901f\uff0c\u751a\u81f3\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u4e5f\u6709\u6548\u3002"}}
{"id": "2508.00330", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00330", "abs": "https://arxiv.org/abs/2508.00330", "authors": ["Lilika Makabe", "Hiroaki Santo", "Fumio Okura", "Michael S. Brown", "Yasuyuki Matsushita"], "title": "Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating", "comment": null, "summary": "This paper introduces a practical and accurate calibration method for camera\nspectral sensitivity using a diffraction grating. Accurate calibration of\ncamera spectral sensitivity is crucial for various computer vision tasks,\nincluding color correction, illumination estimation, and material analysis.\nUnlike existing approaches that require specialized narrow-band filters or\nreference targets with known spectral reflectances, our method only requires an\nuncalibrated diffraction grating sheet, readily available off-the-shelf. By\ncapturing images of the direct illumination and its diffracted pattern through\nthe grating sheet, our method estimates both the camera spectral sensitivity\nand the diffraction grating parameters in a closed-form manner. Experiments on\nsynthetic and real-world data demonstrate that our method outperforms\nconventional reference target-based methods, underscoring its effectiveness and\npracticality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u9700\u73b0\u6210\u884d\u5c04\u5149\u6805\u7247\u7684\u76f8\u673a\u5149\u8c31\u7075\u654f\u5ea6\u6821\u51c6\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u76f8\u673a\u5149\u8c31\u7075\u654f\u5ea6\u7684\u51c6\u786e\u6821\u51c6\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u989c\u8272\u6821\u6b63\u3001\u5149\u7167\u4f30\u8ba1\u548c\u6750\u6599\u5206\u6790\uff09\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u7279\u6b8a\u7a84\u5e26\u6ee4\u5149\u7247\u6216\u5df2\u77e5\u5149\u8c31\u53cd\u5c04\u7387\u7684\u53c2\u8003\u76ee\u6807\uff0c\u800c\u8be5\u65b9\u6cd5\u4ec5\u9700\u73b0\u6210\u7684\u975e\u6821\u51c6\u884d\u5c04\u5149\u6805\u7247\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u975e\u6821\u51c6\u884d\u5c04\u5149\u6805\u7247\u7684\u95ed\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6355\u83b7\u76f4\u63a5\u7167\u660e\u53ca\u5176\u884d\u5c04\u56fe\u6848\u7684\u56fe\u50cf\uff0c\u540c\u65f6\u4f30\u8ba1\u76f8\u673a\u5149\u8c31\u7075\u654f\u5ea6\u548c\u884d\u5c04\u5149\u6805\u53c2\u6570\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u53c2\u8003\u76ee\u6807\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u884d\u5c04\u5149\u6805\u5b9e\u73b0\u4e86\u76f8\u673a\u5149\u8c31\u7075\u654f\u5ea6\u7684\u5b9e\u7528\u4e14\u51c6\u786e\u6821\u51c6\uff0c\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u53c2\u8003\u76ee\u6807\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.00356", "categories": ["cs.CV", "cs.MA", "I.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.00356", "abs": "https://arxiv.org/abs/2508.00356", "authors": ["Angelos Vlachos", "Giorgos Filandrianos", "Maria Lymperaiou", "Nikolaos Spanos", "Ilias Mitsouras", "Vasileios Karampinis", "Athanasios Voulodimos"], "title": "Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning", "comment": null, "summary": "We present a Collaborative Agent-Based Framework for Multi-Image Reasoning.\nOur approach tackles the challenge of interleaved multimodal reasoning across\ndiverse datasets and task formats by employing a dual-agent system: a\nlanguage-based PromptEngineer, which generates context-aware, task-specific\nprompts, and a VisionReasoner, a large vision-language model (LVLM) responsible\nfor final inference. The framework is fully automated, modular, and\ntraining-free, enabling generalization across classification, question\nanswering, and free-form generation tasks involving one or multiple input\nimages. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE\nChallenge (Track A), covering a broad spectrum of visual reasoning tasks\nincluding document QA, visual comparison, dialogue-based understanding, and\nscene-level inference. Our results demonstrate that LVLMs can effectively\nreason over multiple images when guided by informative prompts. Notably, Claude\n3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13%\naccuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how\ndesign choices-such as model selection, shot count, and input length-influence\nthe reasoning performance of different LVLMs.", "AI": {"tldr": "\u63d0\u51fa\u534f\u4f5c\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u4ee3\u7406\u7cfb\u7edf\u5b9e\u73b0\u591a\u56fe\u50cf\u63a8\u7406\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\u3002", "motivation": "\u89e3\u51b3\u8de8\u591a\u6837\u6570\u636e\u96c6\u548c\u4efb\u52a1\u683c\u5f0f\u7684\u4ea4\u9519\u591a\u6a21\u6001\u63a8\u7406\u6311\u6218\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u53cc\u4ee3\u7406\u7cfb\u7edf\u7684\u534f\u4f5c\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u62ec\u8bed\u8a00\u57fa\u7840\u7684PromptEngineer\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\uff0c\u4ee5\u53ca\u89c6\u89c9\u57fa\u7840\u7684VisionReasoner\u8fdb\u884c\u6700\u7ec8\u63a8\u7406\u3002\u8be5\u6846\u67b6\u5b8c\u5168\u81ea\u52a8\u5316\u3001\u6a21\u5757\u5316\u4e14\u65e0\u9700\u8bad\u7ec3\u3002", "result": "\u572818\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cClaude 3.7\u5728TQA\uff0899.13%\u51c6\u786e\u7387\uff09\u3001DocVQA\uff0896.87%\uff09\u548cMMCoQA\uff0875.28 ROUGE-L\uff09\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u901a\u8fc7\u4fe1\u606f\u4e30\u5bcc\u7684\u63d0\u793a\u5f15\u5bfc\u4e0b\uff0c\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884c\u591a\u56fe\u50cf\u63a8\u7406\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u53d6\u5f97\u63a5\u8fd1\u9876\u5cf0\u7684\u4f18\u5f02\u8868\u73b0\u3002"}}
{"id": "2508.00358", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00358", "abs": "https://arxiv.org/abs/2508.00358", "authors": ["Yan Gong", "Mengjun Chen", "Hao Liu", "Gao Yongsheng", "Lei Yang", "Naibang Wang", "Ziying Song", "Haoqun Ma"], "title": "Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering", "comment": "9 pages, 7 figures, 5 tables", "summary": "Multi-object tracking (MOT) enables autonomous vehicles to continuously\nperceive dynamic objects, supplying essential temporal cues for prediction,\nbehavior understanding, and safe planning. However, conventional\ntracking-by-detection methods typically rely on static coordinate\ntransformations based on ego-vehicle poses, disregarding ego-vehicle\nspeed-induced variations in observation noise and reference frame changes,\nwhich degrades tracking stability and accuracy in dynamic, high-speed\nscenarios. In this paper, we investigate the critical role of ego-vehicle speed\nin MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that\ndynamically adapts uncertainty modeling to ego-vehicle speed, significantly\nimproving stability and accuracy in highly dynamic scenarios. Central to SG-LKF\nis MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that\nadaptively predicts key parameters of SG-LKF. To enhance inter-frame\nassociation and trajectory continuity, we introduce a self-supervised\ntrajectory consistency loss jointly optimized with semantic and positional\nconstraints. Extensive experiments show that SG-LKF ranks first among all\nvision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results\non KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on\nnuScenes 3D MOT.", "AI": {"tldr": "SG-LKF\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u81ea\u8f66\u901f\u5ea6\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u591a\u76ee\u6807\u8ddf\u8e2a\u5728\u9ad8\u52a8\u6001\u573a\u666f\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u68c0\u6d4b\u7684\u8ddf\u8e2a\u65b9\u6cd5\u5ffd\u89c6\u4e86\u81ea\u8f66\u901f\u5ea6\u5f15\u8d77\u7684\u89c2\u6d4b\u566a\u58f0\u548c\u53c2\u8003\u5e27\u53d8\u5316\uff0c\u5bfc\u81f4\u5728\u9ad8\u52a8\u6001\u3001\u9ad8\u901f\u573a\u666f\u4e2d\u8ddf\u8e2a\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86Speed-Guided Learnable Kalman Filter (SG-LKF) \u548c MotionScaleNet (MSNet)\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u8f68\u8ff9\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5728KITTI 2D MOT\u4e0a\u4ee579.59% HOTA\u6392\u540d\u7b2c\u4e00\uff0cKITTI 3D MOT\u4e0a82.03% HOTA\uff0cnuScenes 3D MOT\u4e0a\u6bd4SimpleTrack\u63d0\u9ad82.2% AMOTA\u3002", "conclusion": "SG-LKF\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4ee5\u9002\u5e94\u81ea\u8f66\u901f\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u52a8\u6001\u573a\u666f\u4e0b\u7684\u8ddf\u8e2a\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.00359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00359", "abs": "https://arxiv.org/abs/2508.00359", "authors": ["Zongheng Tang", "Yi Liu", "Yifan Sun", "Yulu Gao", "Jinyu Chen", "Runsheng Xu", "Si Liu"], "title": "CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective", "comment": "ICCV25 (Highlight)", "summary": "Collaborative perception shares information among different agents and helps\nsolving problems that individual agents may face, e.g., occlusions and small\nsensing range. Prior methods usually separate the multi-agent fusion and\nmulti-time fusion into two consecutive steps. In contrast, this paper proposes\nan efficient collaborative perception that aggregates the observations from\ndifferent agents (space) and different times into a unified spatio-temporal\nspace simultanesouly. The unified spatio-temporal space brings two benefits,\ni.e., efficient feature transmission and superior feature fusion. 1) Efficient\nfeature transmission: each static object yields a single observation in the\nspatial temporal space, and thus only requires transmission only once (whereas\nprior methods re-transmit all the object features multiple times). 2) superior\nfeature fusion: merging the multi-agent and multi-time fusion into a unified\nspatial-temporal aggregation enables a more holistic perspective, thereby\nenhancing perception performance in challenging scenarios. Consequently, our\nCollaborative perception with Spatio-temporal Transformer (CoST) gains\nimprovement in both efficiency and accuracy. Notably, CoST is not tied to any\nspecific method and is compatible with a majority of previous methods,\nenhancing their accuracy while reducing the transmission bandwidth.", "AI": {"tldr": "CoST\u901a\u8fc7\u7edf\u4e00\u65f6\u7a7a\u7a7a\u95f4\u805a\u5408\u591a\u667a\u80fd\u4f53\u548c\u591a\u65f6\u95f4\u89c2\u6d4b\uff0c\u63d0\u5347\u4e86\u534f\u4f5c\u611f\u77e5\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\u4e2d\u591a\u667a\u80fd\u4f53\u878d\u5408\u548c\u591a\u65f6\u95f4\u878d\u5408\u5206\u79bb\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u548c\u7279\u5f81\u878d\u5408\u4e0d\u5168\u9762\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u65f6\u7a7a\u7a7a\u95f4\u540c\u65f6\u805a\u5408\u591a\u667a\u80fd\u4f53\u548c\u591a\u65f6\u95f4\u7684\u89c2\u6d4b\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7279\u5f81\u91cd\u590d\u4f20\u8f93\u7684\u95ee\u9898\u3002", "result": "CoST\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u5747\u6709\u63d0\u5347\uff0c\u4e14\u517c\u5bb9\u591a\u6570\u73b0\u6709\u65b9\u6cd5\uff0c\u964d\u4f4e\u4e86\u4f20\u8f93\u5e26\u5bbd\u9700\u6c42\u3002", "conclusion": "CoST\uff08Collaborative perception with Spatio-temporal Transformer\uff09\u901a\u8fc7\u7edf\u4e00\u7684\u65f6\u7a7a\u7a7a\u95f4\u805a\u5408\u591a\u667a\u80fd\u4f53\u548c\u591a\u65f6\u95f4\u89c2\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534f\u4f5c\u611f\u77e5\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e14\u517c\u5bb9\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.00361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00361", "abs": "https://arxiv.org/abs/2508.00361", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "title": "Honey Classification using Hyperspectral Imaging and Machine Learning", "comment": null, "summary": "In this paper, we propose a machine learning-based method for automatically\nclassifying honey botanical origins. Dataset preparation, feature extraction,\nand classification are the three main steps of the proposed method. We use a\nclass transformation method in the dataset preparation phase to maximize the\nseparability across classes. The feature extraction phase employs the Linear\nDiscriminant Analysis (LDA) technique for extracting relevant features and\nreducing the number of dimensions. In the classification phase, we use Support\nVector Machines (SVM) and K-Nearest Neighbors (KNN) models to classify the\nextracted features of honey samples into their botanical origins. We evaluate\nour system using a standard honey hyperspectral imaging (HSI) dataset.\nExperimental findings demonstrate that the proposed system produces\nstate-of-the-art results on this dataset, achieving the highest classification\naccuracy of 95.13% for hyperspectral image-based classification and 92.80% for\nhyperspectral instance-based classification.", "AI": {"tldr": "A machine learning method using LDA, SVM, and KNN achieves high accuracy in classifying honey origins.", "motivation": "To automatically classify honey botanical origins using machine learning, improving accuracy and efficiency over traditional methods.", "method": "The method involves dataset preparation with class transformation, feature extraction using Linear Discriminant Analysis (LDA), and classification with Support Vector Machines (SVM) and K-Nearest Neighbors (KNN).", "result": "The system achieves 95.13% accuracy for hyperspectral image-based classification and 92.80% for hyperspectral instance-based classification.", "conclusion": "The proposed machine learning-based method achieves state-of-the-art results in classifying honey botanical origins, with high accuracy rates of 95.13% for image-based and 92.80% for instance-based classification."}}
{"id": "2508.00366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00366", "abs": "https://arxiv.org/abs/2508.00366", "authors": ["Liang Han", "Xu Zhang", "Haichuan Song", "Kanle Shi", "Yu-Shen Liu", "Zhizhong Han"], "title": "SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies", "comment": "Accepted by ICCV 2025", "summary": "Surface reconstruction from sparse views aims to reconstruct a 3D shape or\nscene from few RGB images. The latest methods are either generalization-based\nor overfitting-based. However, the generalization-based methods do not\ngeneralize well on views that were unseen during training, while the\nreconstruction quality of overfitting-based methods is still limited by the\nlimited geometry clues. To address this issue, we propose SparseRecon, a novel\nneural implicit reconstruction method for sparse views with volume\nrendering-based feature consistency and uncertainty-guided depth constraint.\nFirstly, we introduce a feature consistency loss across views to constrain the\nneural implicit field. This design alleviates the ambiguity caused by\ninsufficient consistency information of views and ensures completeness and\nsmoothness in the reconstruction results. Secondly, we employ an\nuncertainty-guided depth constraint to back up the feature consistency loss in\nareas with occlusion and insignificant features, which recovers geometry\ndetails for better reconstruction quality. Experimental results demonstrate\nthat our method outperforms the state-of-the-art methods, which can produce\nhigh-quality geometry with sparse-view input, especially in the scenarios with\nsmall overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.", "AI": {"tldr": "SparseRecon \u662f\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u9690\u5f0f\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u4e00\u81f4\u6027\u548c\u6df1\u5ea6\u7ea6\u675f\u63d0\u5347\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u8d28\u91cf\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u4e2d\u5b58\u5728\u6cdb\u5316\u6027\u5dee\u6216\u91cd\u5efa\u8d28\u91cf\u53d7\u9650\u7684\u95ee\u9898\uff0cSparseRecon \u65e8\u5728\u901a\u8fc7\u591a\u89c6\u89d2\u7279\u5f81\u4e00\u81f4\u6027\u548c\u6df1\u5ea6\u7ea6\u675f\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u91c7\u7528\u4f53\u79ef\u6e32\u67d3\u7279\u5f81\u4e00\u81f4\u6027\u635f\u5931\u548c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u6df1\u5ea6\u7ea6\u675f\uff0c\u524d\u8005\u786e\u4fdd\u91cd\u5efa\u7ed3\u679c\u7684\u5b8c\u6574\u6027\u548c\u5e73\u6ed1\u6027\uff0c\u540e\u8005\u5728\u906e\u6321\u548c\u7279\u5f81\u4e0d\u660e\u663e\u533a\u57df\u8865\u5145\u51e0\u4f55\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSparseRecon \u5728\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u51e0\u4f55\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "SparseRecon \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u9690\u5f0f\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f53\u79ef\u6e32\u67d3\u7279\u5f81\u4e00\u81f4\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u6df1\u5ea6\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u7684\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u91cd\u53e0\u89c6\u56fe\u8f83\u5c11\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2508.00367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00367", "abs": "https://arxiv.org/abs/2508.00367", "authors": ["Joonmyung Choi", "Sanghyeok Lee", "Byungoh Ko", "Eunseo Kim", "Jihyung Kil", "Hyunwoo J. Kim"], "title": "Representation Shift: Unifying Token Compression with FlashAttention", "comment": "International Conference on Computer Vision (ICCV), 2025", "summary": "Transformers have demonstrated remarkable success across vision, language,\nand video. Yet, increasing task complexity has led to larger models and more\ntokens, raising the quadratic cost of self-attention and the overhead of GPU\nmemory access. To reduce the computation cost of self-attention, prior work has\nproposed token compression techniques that drop redundant or less informative\ntokens. Meanwhile, fused attention kernels such as FlashAttention have been\ndeveloped to alleviate memory overhead by avoiding attention map construction\nand its associated I/O to HBM. This, however, makes it incompatible with most\ntraining-free token compression methods, which rely on attention maps to\ndetermine token importance. Here, we propose Representation Shift, a\ntraining-free, model-agnostic metric that measures the degree of change in each\ntoken's representation. This seamlessly integrates token compression with\nFlashAttention, without attention maps or retraining. Our method further\ngeneralizes beyond Transformers to CNNs and state space models. Extensive\nexperiments show that Representation Shift enables effective token compression\ncompatible with FlashAttention, yielding significant speedups of up to 5.5% and\n4.4% in video-text retrieval and video QA, respectively. Code is available at\nhttps://github.com/mlvlab/Representation-Shift.", "AI": {"tldr": "\u63d0\u51faRepresentation Shift\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cftoken\u8868\u793a\u53d8\u5316\u5b9e\u73b0\u538b\u7f29\uff0c\u4e0eFlashAttention\u517c\u5bb9\uff0c\u663e\u8457\u63d0\u5347\u901f\u5ea6\u3002", "motivation": "\u968f\u7740\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0cTransformer\u6a21\u578b\u548ctoken\u6570\u91cf\u7684\u589e\u957f\u5bfc\u81f4\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\u548cGPU\u5185\u5b58\u8bbf\u95ee\u5f00\u9500\u589e\u52a0\u3002\u73b0\u6709\u7684token\u538b\u7f29\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u6ce8\u610f\u529b\u56fe\uff0c\u65e0\u6cd5\u4e0eFlashAttention\u517c\u5bb9\u3002", "method": "\u63d0\u51faRepresentation Shift\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u6bcf\u4e2atoken\u8868\u793a\u7684\u53d8\u5316\u7a0b\u5ea6\u6765\u5b9e\u73b0token\u538b\u7f29\uff0c\u65e0\u9700\u4f9d\u8d56\u6ce8\u610f\u529b\u56fe\u6216\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRepresentation Shift\u80fd\u4e0eFlashAttention\u65e0\u7f1d\u96c6\u6210\uff0c\u5728\u89c6\u9891-\u6587\u672c\u68c0\u7d22\u548c\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u5206\u522b\u5b9e\u73b0\u4e86\u9ad8\u8fbe5.5%\u548c4.4%\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "Representation Shift\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u4e0eFlashAttention\u517c\u5bb9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891-\u6587\u672c\u68c0\u7d22\u548c\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u7684\u901f\u5ea6\u3002"}}
{"id": "2508.00374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00374", "abs": "https://arxiv.org/abs/2508.00374", "authors": ["Yuji Sato", "Yasunori Ishii", "Takayoshi Yamashita"], "title": "Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models", "comment": "Accepted to MVA2025 (Best Poster Award)", "summary": "Video-based long-term action anticipation is crucial for early risk detection\nin areas such as automated driving and robotics. Conventional approaches\nextract features from past actions using encoders and predict future events\nwith decoders, which limits performance due to their unidirectional nature.\nThese methods struggle to capture semantically distinct sub-actions within a\nscene. The proposed method, BiAnt, addresses this limitation by combining\nforward prediction with backward prediction using a large language model.\nExperimental results on Ego4D demonstrate that BiAnt improves performance in\nterms of edit distance compared to baseline methods.", "AI": {"tldr": "BiAnt\u901a\u8fc7\u53cc\u5411\u9884\u6d4b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u89c6\u9891\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u5355\u5411\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7684\u65b9\u6cd5\u7531\u4e8e\u5355\u5411\u6027\u9650\u5236\u4e86\u6027\u80fd\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u573a\u666f\u4e2d\u8bed\u4e49\u4e0d\u540c\u7684\u5b50\u52a8\u4f5c\u3002", "method": "BiAnt\u7ed3\u5408\u4e86\u524d\u5411\u9884\u6d4b\u548c\u540e\u5411\u9884\u6d4b\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u6355\u6349\u573a\u666f\u4e2d\u8bed\u4e49\u4e0d\u540c\u7684\u5b50\u52a8\u4f5c\u3002", "result": "\u5728Ego4D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBiAnt\u5728\u7f16\u8f91\u8ddd\u79bb\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "BiAnt\u901a\u8fc7\u7ed3\u5408\u524d\u5411\u9884\u6d4b\u548c\u540e\u5411\u9884\u6d4b\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7f16\u8f91\u8ddd\u79bb\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.00381", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00381", "abs": "https://arxiv.org/abs/2508.00381", "authors": ["Kamal Basha S", "Athira Nambiar"], "title": "Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis", "comment": null, "summary": "Weld defect detection is crucial for ensuring the safety and reliability of\npiping systems in the oil and gas industry, especially in challenging marine\nand offshore environments. Traditional non-destructive testing (NDT) methods\noften fail to detect subtle or internal defects, leading to potential failures\nand costly downtime. Furthermore, existing neural network-based approaches for\ndefect classification frequently rely on arbitrarily selected pretrained\narchitectures and lack interpretability, raising safety concerns for\ndeployment. To address these challenges, this paper introduces\n``Adapt-WeldNet\", an adaptive framework for welding defect detection that\nsystematically evaluates various pre-trained architectures, transfer learning\nstrategies, and adaptive optimizers to identify the best-performing model and\nhyperparameters, optimizing defect detection and providing actionable insights.\nAdditionally, a novel Defect Detection Interpretability Analysis (DDIA)\nframework is proposed to enhance system transparency. DDIA employs Explainable\nAI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific\nevaluations validated by certified ASNT NDE Level II professionals.\nIncorporating a Human-in-the-Loop (HITL) approach and aligning with the\nprinciples of Trustworthy AI, DDIA ensures the reliability, fairness, and\naccountability of the defect detection system, fostering confidence in\nautomated decisions through expert validation. By improving both performance\nand interpretability, this work enhances trust, safety, and reliability in\nwelding defect detection systems, supporting critical operations in offshore\nand marine environments.", "AI": {"tldr": "\u63d0\u51faAdapt-WeldNet\u548cDDIA\u6846\u67b6\uff0c\u63d0\u5347\u710a\u63a5\u7f3a\u9677\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u589e\u5f3a\u81ea\u52a8\u5316\u51b3\u7b56\u7684\u4fe1\u4efb\u5ea6\u3002", "motivation": "\u4f20\u7edfNDT\u65b9\u6cd5\u548c\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5728\u68c0\u6d4b\u710a\u63a5\u7f3a\u9677\u65f6\u5b58\u5728\u6027\u80fd\u4e0d\u8db3\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u5b89\u5168\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faAdapt-WeldNet\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u9884\u8bad\u7ec3\u67b6\u6784\u3001\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u548c\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff1b\u5f15\u5165DDIA\u6846\u67b6\uff0c\u7ed3\u5408XAI\u6280\u672f\u548c\u9886\u57df\u4e13\u5bb6\u9a8c\u8bc1\u3002", "result": "Adapt-WeldNet\u4f18\u5316\u4e86\u7f3a\u9677\u68c0\u6d4b\u6027\u80fd\uff0cDDIA\u6846\u67b6\u63d0\u5347\u4e86\u7cfb\u7edf\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5ea6\uff0c\u7ecf\u4e13\u5bb6\u9a8c\u8bc1\u6709\u6548\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u51faAdapt-WeldNet\u548cDDIA\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u710a\u63a5\u7f3a\u9677\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u589e\u5f3a\u4e86\u5728\u6d77\u6d0b\u548c\u79bb\u5cb8\u73af\u5883\u4e2d\u81ea\u52a8\u5316\u51b3\u7b56\u7684\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2508.00383", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00383", "abs": "https://arxiv.org/abs/2508.00383", "authors": ["Won June Cho", "Hongjun Yoon", "Daeky Jeong", "Hyeongyeol Lim", "Yosep Chong"], "title": "$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models", "comment": "Accepted (Oral) in MICCAI 2025 COMPAYL Workshop", "summary": "Spatial transcriptomics reveals gene expression patterns within tissue\ncontext, enabling precision oncology applications such as treatment response\nprediction, but its high cost and technical complexity limit clinical adoption.\nPredicting spatial gene expression (biomarkers) from routine histopathology\nimages offers a practical alternative, yet current vision foundation models\n(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below\nclinical standards. Given that VFMs are already trained on millions of diverse\nwhole slide images, we hypothesize that architectural innovations beyond ViTs\nmay better capture the low-frequency, subtle morphological patterns correlating\nwith molecular phenotypes. By demonstrating that state space models initialized\nwith negative real eigenvalues exhibit strong low-frequency bias, we introduce\n$MV_{Hybrid}$, a hybrid backbone architecture combining state space models\n(SSMs) with ViT. We compare five other different backbone architectures for\npathology VFMs, all pretrained on identical colorectal cancer datasets using\nthe DINOv2 self-supervised learning method. We evaluate all pretrained models\nusing both random split and leave-one-study-out (LOSO) settings of the same\nbiomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher\ncorrelation than the best-performing ViT and shows 43% smaller performance\ndegradation compared to random split in gene expression prediction,\ndemonstrating superior performance and robustness, respectively. Furthermore,\n$MV_{Hybrid}$ shows equal or better downstream performance in classification,\npatch retrieval, and survival prediction tasks compared to that of ViT, showing\nits promise as a next-generation pathology VFM backbone. Our code is publicly\navailable at: https://github.com/deepnoid-ai/MVHybrid.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408SSM\u548cViT\u7684$MV_{Hybrid}$\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u75c5\u7406\u56fe\u50cf\u4e2d\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u7684\u9ad8\u6210\u672c\u548c\u6280\u672f\u590d\u6742\u6027\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u5e94\u7528\uff0c\u9884\u6d4b\u5e38\u89c4\u75c5\u7406\u56fe\u50cf\u4e2d\u7684\u57fa\u56e0\u8868\u8fbe\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709ViT\u6a21\u578b\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u5f15\u5165$MV_{Hybrid}$\uff0c\u4e00\u79cd\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u548cViT\u7684\u6df7\u5408\u9aa8\u5e72\u67b6\u6784\uff0c\u901a\u8fc7\u8d1f\u5b9e\u7279\u5f81\u503c\u521d\u59cb\u5316\u6355\u6349\u4f4e\u9891\u3001\u7ec6\u5fae\u7684\u5f62\u6001\u5b66\u6a21\u5f0f\u3002", "result": "\u5728LOSO\u8bc4\u4f30\u4e2d\uff0c$MV_{Hybrid}$\u6bd4\u6700\u4f73ViT\u6a21\u578b\u76f8\u5173\u6027\u9ad857%\uff0c\u6027\u80fd\u4e0b\u964d\u51cf\u5c1143%\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "$MV_{Hybrid}$\u5c55\u793a\u4e86\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u75c5\u7406\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u9aa8\u5e72\u7684\u6f5c\u529b\uff0c\u5728\u5206\u7c7b\u3001\u56fe\u50cf\u5757\u68c0\u7d22\u548c\u751f\u5b58\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u7b49\u540c\u4e8eViT\u3002"}}
{"id": "2508.00391", "categories": ["cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.00391", "abs": "https://arxiv.org/abs/2508.00391", "authors": ["Guanjie Huang", "Danny H. K. Tsang", "Shan Yang", "Guangzhi Lei", "Li Liu"], "title": "Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition", "comment": "9 pages", "summary": "Cued Speech (CS) is a visual communication system that combines lip-reading\nwith hand coding to facilitate communication for individuals with hearing\nimpairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures\nand lip movements into text via AI-driven methods. Traditionally, the temporal\nasynchrony between hand and lip movements requires the design of complex\nmodules to facilitate effective multimodal fusion. However, constrained by\nlimited data availability, current methods demonstrate insufficient capacity\nfor adequately training these fusion mechanisms, resulting in suboptimal\nperformance. Recently, multi-agent systems have shown promising capabilities in\nhandling complex tasks with limited data availability. To this end, we propose\nthe first collaborative multi-agent system for ACSR, named Cued-Agent. It\nintegrates four specialized sub-agents: a Multimodal Large Language Model-based\nHand Recognition agent that employs keyframe screening and CS expert prompt\nstrategies to decode hand movements, a pretrained Transformer-based Lip\nRecognition agent that extracts lip features from the input video, a Hand\nPrompt Decoding agent that dynamically integrates hand prompts with lip\nfeatures during inference in a training-free manner, and a Self-Correction\nPhoneme-to-Word agent that enables post-process and end-to-end conversion from\nphoneme sequences to natural language sentences for the first time through\nsemantic refinement. To support this study, we expand the existing Mandarin CS\ndataset by collecting data from eight hearing-impaired cuers, establishing a\nmixed dataset of fourteen subjects. Extensive experiments demonstrate that our\nCued-Agent performs superbly in both normal and hearing-impaired scenarios\ncompared with state-of-the-art methods. The implementation is available at\nhttps://github.com/DennisHgj/Cued-Agent.", "AI": {"tldr": "Cued-Agent \u662f\u9996\u4e2a\u7528\u4e8e\u81ea\u52a8 Cued Speech \u8bc6\u522b\u7684\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u56db\u4e2a\u5b50\u667a\u80fd\u4f53\u534f\u540c\u5de5\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u542c\u969c\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf ACSR \u65b9\u6cd5\u56e0\u6570\u636e\u6709\u9650\u548c\u624b\u90e8\u4e0e\u5507\u90e8\u52a8\u4f5c\u7684\u5f02\u6b65\u6027\uff0c\u96be\u4ee5\u6709\u6548\u8bad\u7ec3\u591a\u6a21\u6001\u878d\u5408\u673a\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u6709\u9650\u6570\u636e\u4e0b\u5904\u7406\u590d\u6742\u4efb\u52a1\u7684\u4f18\u52bf\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "Cued-Agent \u662f\u4e00\u4e2a\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u542b\u56db\u4e2a\u5b50\u667a\u80fd\u4f53\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u624b\u52bf\u8bc6\u522b\u3001\u57fa\u4e8e\u9884\u8bad\u7ec3 Transformer \u7684\u5507\u90e8\u8bc6\u522b\u3001\u52a8\u6001\u6574\u5408\u624b\u90e8\u63d0\u793a\u4e0e\u5507\u90e8\u7279\u5f81\u7684\u624b\u90e8\u63d0\u793a\u89e3\u7801\uff0c\u4ee5\u53ca\u901a\u8fc7\u8bed\u4e49\u7ec6\u5316\u5b9e\u73b0\u97f3\u7d20\u5230\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u7684\u81ea\u6821\u6b63\u667a\u80fd\u4f53\u3002", "result": "Cued-Agent \u5728\u6b63\u5e38\u548c\u542c\u969c\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "Cued-Agent \u5728\u81ea\u52a8 Cued Speech \u8bc6\u522b\uff08ACSR\uff09\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u5904\u7406\u542c\u969c\u573a\u666f\u65f6\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2508.00395", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00395", "abs": "https://arxiv.org/abs/2508.00395", "authors": ["Fei Zhang", "Tianfei Zhou", "Jiangchao Yao", "Ya Zhang", "Ivor W. Tsang", "Yanfeng Wang"], "title": "Decouple before Align: Visual Disentanglement Enhances Prompt Tuning", "comment": "16 pages, Accepted at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI)", "summary": "Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,\nhas showcased remarkable effectiveness in improving the task-specific\ntransferability of vision-language models. This paper delves into a previously\noverlooked information asymmetry issue in PT, where the visual modality mostly\nconveys more context than the object-oriented textual modality.\nCorrespondingly, coarsely aligning these two modalities could result in the\nbiased attention, driving the model to merely focus on the context area. To\naddress this, we propose DAPT, an effective PT framework based on an intuitive\ndecouple-before-align concept. First, we propose to explicitly decouple the\nvisual modality into the foreground and background representation via\nexploiting coarse-and-fine visual segmenting cues, and then both of these\ndecoupled patterns are aligned with the original foreground texts and the\nhand-crafted background classes, thereby symmetrically strengthening the modal\nalignment. To further enhance the visual concentration, we propose a visual\npull-push regularization tailored for the foreground-background patterns,\ndirecting the original visual representation towards unbiased attention on the\nregion-of-interest object. We demonstrate the power of architecture-free DAPT\nthrough few-shot learning, base-to-novel generalization, and data-efficient\nlearning, all of which yield superior performance across prevailing benchmarks.\nOur code will be released at https://github.com/Ferenas/DAPT.", "AI": {"tldr": "DAPT\u901a\u8fc7\u89e3\u8026\u548c\u5bf9\u9f50\u89c6\u89c9\u6a21\u6001\u7684\u524d\u666f\u4e0e\u80cc\u666f\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u63d0\u793a\u8c03\u4f18\u4e2d\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u63d0\u793a\u8c03\u4f18\uff08PT\uff09\u4e2d\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u4e4b\u95f4\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u5373\u89c6\u89c9\u6a21\u6001\u901a\u5e38\u6bd4\u9762\u5411\u5bf9\u8c61\u7684\u6587\u672c\u6a21\u6001\u4f20\u9012\u66f4\u591a\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u6a21\u578b\u4ec5\u5173\u6ce8\u4e0a\u4e0b\u6587\u533a\u57df\u7684\u504f\u89c1\u6ce8\u610f\u529b\u3002", "method": "\u63d0\u51fa\u4e86DAPT\u6846\u67b6\uff0c\u57fa\u4e8e\u89e3\u8026\u518d\u5bf9\u9f50\u7684\u6982\u5ff5\uff0c\u9996\u5148\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u89c6\u89c9\u5206\u5272\u7ebf\u7d22\u663e\u5f0f\u89e3\u8026\u89c6\u89c9\u6a21\u6001\u4e3a\u524d\u666f\u548c\u80cc\u666f\u8868\u793a\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u89e3\u8026\u6a21\u5f0f\u4e0e\u539f\u59cb\u524d\u666f\u6587\u672c\u548c\u624b\u5de5\u5236\u4f5c\u7684\u80cc\u666f\u7c7b\u522b\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u9488\u5bf9\u524d\u666f-\u80cc\u666f\u6a21\u5f0f\u7684\u89c6\u89c9\u62c9\u63a8\u6b63\u5219\u5316\uff0c\u4ee5\u589e\u5f3a\u89c6\u89c9\u96c6\u4e2d\u5ea6\u3002", "result": "DAPT\u5728\u5c11\u6837\u672c\u5b66\u4e60\u3001\u57fa\u7840\u5230\u65b0\u7c7b\u522b\u7684\u6cdb\u5316\u4ee5\u53ca\u6570\u636e\u9ad8\u6548\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "DAPT\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u548c\u5bf9\u9f50\u89c6\u89c9\u6a21\u6001\u7684\u524d\u666f\u4e0e\u80cc\u666f\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63d0\u793a\u8c03\u4f18\u4e2d\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5c11\u6837\u672c\u5b66\u4e60\u3001\u57fa\u7840\u5230\u65b0\u7c7b\u522b\u7684\u6cdb\u5316\u4ee5\u53ca\u6570\u636e\u9ad8\u6548\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00397", "abs": "https://arxiv.org/abs/2508.00397", "authors": ["Xi Xue", "Kunio Suzuki", "Nabarun Goswami", "Takuya Shintate"], "title": "Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency", "comment": null, "summary": "The rapid advancement of diffusion-based video generation models has led to\nincreasingly realistic synthetic content, presenting new challenges for video\nforgery detection. Existing methods often struggle to capture fine-grained\ntemporal inconsistencies, particularly in AI-generated videos with high visual\nfidelity and coherent motion. In this work, we propose a detection framework\nthat leverages spatial-temporal consistency by combining RGB appearance\nfeatures with optical flow residuals. The model adopts a dual-branch\narchitecture, where one branch analyzes RGB frames to detect appearance-level\nartifacts, while the other processes flow residuals to reveal subtle motion\nanomalies caused by imperfect temporal synthesis. By integrating these\ncomplementary features, the proposed method effectively detects a wide range of\nforged videos. Extensive experiments on text-to-video and image-to-video tasks\nacross ten diverse generative models demonstrate the robustness and strong\ngeneralization ability of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408RGB\u548c\u5149\u6d41\u6b8b\u5dee\u7684\u53cc\u5206\u652f\u6846\u67b6\uff0c\u6709\u6548\u68c0\u6d4bAI\u751f\u6210\u89c6\u9891\u4e2d\u7684\u4f2a\u9020\u5185\u5bb9\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u6269\u6563\u57fa\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5408\u6210\u5185\u5bb9\u65e5\u76ca\u903c\u771f\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\uff0c\u5c24\u5176\u662f\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u9ad8\u4e14\u8fd0\u52a8\u8fde\u8d2f\u7684AI\u751f\u6210\u89c6\u9891\u4e2d\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff0c\u4e00\u652f\u5206\u6790RGB\u5e27\u4ee5\u68c0\u6d4b\u5916\u89c2\u5c42\u9762\u4f2a\u5f71\uff0c\u53e6\u4e00\u652f\u5904\u7406\u5149\u6d41\u6b8b\u5dee\u4ee5\u63ed\u793a\u4e0d\u5b8c\u7f8e\u65f6\u95f4\u5408\u6210\u5bfc\u81f4\u7684\u7ec6\u5fae\u8fd0\u52a8\u5f02\u5e38\u3002", "result": "\u5728\u5341\u79cd\u4e0d\u540c\u7684\u751f\u6210\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u6587\u672c\u5230\u89c6\u9891\u548c\u56fe\u50cf\u5230\u89c6\u9891\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u9c81\u68d2\u6027\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u53cc\u5206\u652f\u6846\u67b6\u901a\u8fc7\u7ed3\u5408RGB\u5916\u89c2\u7279\u5f81\u548c\u5149\u6d41\u6b8b\u5dee\uff0c\u6709\u6548\u68c0\u6d4b\u4e86\u89c6\u9891\u4f2a\u9020\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.00399", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00399", "abs": "https://arxiv.org/abs/2508.00399", "authors": ["Raiyaan Abdullah", "Yogesh Singh Rawat", "Shruti Vyas"], "title": "iSafetyBench: A video-language benchmark for safety in industrial environment", "comment": "Accepted to VISION'25 - ICCV 2025 workshop", "summary": "Recent advances in vision-language models (VLMs) have enabled impressive\ngeneralization across diverse video understanding tasks under zero-shot\nsettings. However, their capabilities in high-stakes industrial domains-where\nrecognizing both routine operations and safety-critical anomalies is\nessential-remain largely underexplored. To address this gap, we introduce\niSafetyBench, a new video-language benchmark specifically designed to evaluate\nmodel performance in industrial environments across both normal and hazardous\nscenarios. iSafetyBench comprises 1,100 video clips sourced from real-world\nindustrial settings, annotated with open-vocabulary, multi-label action tags\nspanning 98 routine and 67 hazardous action categories. Each clip is paired\nwith multiple-choice questions for both single-label and multi-label\nevaluation, enabling fine-grained assessment of VLMs in both standard and\nsafety-critical contexts. We evaluate eight state-of-the-art video-language\nmodels under zero-shot conditions. Despite their strong performance on existing\nvideo benchmarks, these models struggle with iSafetyBench-particularly in\nrecognizing hazardous activities and in multi-label scenarios. Our results\nreveal significant performance gaps, underscoring the need for more robust,\nsafety-aware multimodal models for industrial applications. iSafetyBench\nprovides a first-of-its-kind testbed to drive progress in this direction. The\ndataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.", "AI": {"tldr": "iSafetyBench \u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u9891-\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5de5\u4e1a\u73af\u5883\u4e2d\u6a21\u578b\u7684\u8868\u73b0\u3002\u73b0\u6709\u6a21\u578b\u5728\u8bc6\u522b\u5371\u9669\u6d3b\u52a8\u548c\u591a\u6807\u7b7e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u4e86\u9700\u8981\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u611f\u77e5\u591a\u6a21\u6001\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u4e1a\u9ad8\u98ce\u9669\u9886\u57df\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u662f\u5728\u8bc6\u522b\u5e38\u89c4\u64cd\u4f5c\u548c\u5b89\u5168\u5173\u952e\u5f02\u5e38\u65b9\u9762\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86 iSafetyBench\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f15\u5165\u4e86 iSafetyBench\uff0c\u5305\u542b 1,100 \u4e2a\u6765\u81ea\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u6807\u6ce8\u4e86 98 \u4e2a\u5e38\u89c4\u548c 67 \u4e2a\u5371\u9669\u52a8\u4f5c\u7c7b\u522b\u7684\u5f00\u653e\u8bcd\u6c47\u591a\u6807\u7b7e\u52a8\u4f5c\u6807\u7b7e\u3002\u6bcf\u4e2a\u89c6\u9891\u7247\u6bb5\u914d\u6709\u591a\u9009\u9898\uff0c\u7528\u4e8e\u5355\u6807\u7b7e\u548c\u591a\u6807\u7b7e\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u4e86\u516b\u79cd\u6700\u5148\u8fdb\u7684\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u6a21\u578b\u5728\u73b0\u6709\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728 iSafetyBench \u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u5371\u9669\u6d3b\u52a8\u8bc6\u522b\u548c\u591a\u6807\u7b7e\u573a\u666f\u4e2d\u3002", "conclusion": "iSafetyBench \u662f\u4e00\u4e2a\u4e13\u4e3a\u5de5\u4e1a\u73af\u5883\u8bbe\u8ba1\u7684\u89c6\u9891-\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u6a21\u578b\u5728\u6b63\u5e38\u548c\u5371\u9669\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002\u5c3d\u7ba1\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5de5\u4e1a\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u8868\u73b0\u4ecd\u6709\u663e\u8457\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u8bc6\u522b\u5371\u9669\u6d3b\u52a8\u548c\u591a\u6807\u7b7e\u573a\u666f\u65b9\u9762\u3002"}}
{"id": "2508.00400", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00400", "abs": "https://arxiv.org/abs/2508.00400", "authors": ["Janika Deborah Gajo", "Gerarld Paul Merales", "Jerome Escarcha", "Brenden Ashley Molina", "Gian Nartea", "Emmanuel G. Maminta", "Juan Carlos Roldan", "Rowel O. Atienza"], "title": "Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents", "comment": "14 pages, accepted in ICCV 2025 Workshop on RetailVision", "summary": "We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store\nsimulation for benchmarking embodied agents against human performance in\nshopping tasks. Addressing a gap in retail-specific sim environments for\nembodied agent training, Sari Sandbox features over 250 interactive grocery\nitems across three store configurations, controlled via an API. It supports\nboth virtual reality (VR) for human interaction and a vision language model\n(VLM)-powered embodied agent. We also introduce SariBench, a dataset of\nannotated human demonstrations across varied task difficulties. Our sandbox\nenables embodied agents to navigate, inspect, and manipulate retail items,\nproviding baselines against human performance. We conclude with benchmarks,\nperformance analysis, and recommendations for enhancing realism and\nscalability. The source code can be accessed via\nhttps://github.com/upeee/sari-sandbox-env.", "AI": {"tldr": "Sari Sandbox \u662f\u4e00\u4e2a\u9ad8\u4fdd\u771f3D\u96f6\u552e\u5e97\u6a21\u62df\u73af\u5883\uff0c\u7528\u4e8e\u57fa\u51c6\u5316\u5b9e\u4f53\u4ee3\u7406\u4e0e\u4eba\u7c7b\u5728\u8d2d\u7269\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u542b250+\u4ea4\u4e92\u5546\u54c1\u548cSariBench\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u96f6\u552e\u7279\u5b9a\u6a21\u62df\u73af\u5883\u5728\u5b9e\u4f53\u4ee3\u7406\u8bad\u7ec3\u4e2d\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u4fdd\u771f\u3001\u903c\u771f\u76843D\u96f6\u552e\u5e97\u6a21\u62df\u73af\u5883\u3002", "method": "Sari Sandbox \u5305\u542b\u8d85\u8fc7250\u79cd\u4ea4\u4e92\u5f0f\u6742\u8d27\u5546\u54c1\uff0c\u5206\u5e03\u5728\u4e09\u79cd\u5546\u5e97\u914d\u7f6e\u4e2d\uff0c\u53ef\u901a\u8fc7API\u63a7\u5236\u3002\u652f\u6301\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u9a71\u52a8\u7684\u5b9e\u4f53\u4ee3\u7406\u3002\u8fd8\u5f15\u5165\u4e86SariBench\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e0d\u540c\u4efb\u52a1\u96be\u5ea6\u7684\u4eba\u7c7b\u793a\u8303\u6ce8\u91ca\u3002", "result": "Sari Sandbox \u4f7f\u5b9e\u4f53\u4ee3\u7406\u80fd\u591f\u5bfc\u822a\u3001\u68c0\u67e5\u548c\u64cd\u4f5c\u96f6\u552e\u5546\u54c1\uff0c\u5e76\u63d0\u4f9b\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u57fa\u51c6\u5bf9\u6bd4\u3002", "conclusion": "Sari Sandbox \u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u4fdd\u771f\u3001\u903c\u771f\u76843D\u96f6\u552e\u5e97\u6a21\u62df\u73af\u5883\uff0c\u7528\u4e8e\u57fa\u51c6\u5316\u5b9e\u4f53\u4ee3\u7406\u5728\u8d2d\u7269\u4efb\u52a1\u4e2d\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u5bf9\u6bd4\u3002\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u548c\u6027\u80fd\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u589e\u5f3a\u771f\u5b9e\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u5efa\u8bae\u3002"}}
{"id": "2508.00406", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00406", "abs": "https://arxiv.org/abs/2508.00406", "authors": ["Tao Wu", "Jingyuan Ye", "Ying Fu"], "title": "PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos", "comment": null, "summary": "Geometric distortions and blurring caused by atmospheric turbulence degrade\nthe quality of long-range dynamic scene videos. Existing methods struggle with\nrestoring edge details and eliminating mixed distortions, especially under\nconditions of strong turbulence and complex dynamics. To address these\nchallenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines\nturbulence intensity, optical flow, and proportions of dynamic regions to\naccurately quantify video dynamic intensity under varying turbulence conditions\nand provide a high-dynamic turbulence training dataset. Additionally, we\npropose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework\nthat consists of three stages: \\textbf{de-tilting} for geometric stabilization,\n\\textbf{motion segmentation enhancement} for dynamic region refinement, and\n\\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight\nbackbones and stage-wise joint training to ensure both efficiency and high\nrestoration quality. Experimental results demonstrate that the proposed method\neffectively suppresses motion trailing artifacts, restores edge details and\nexhibits strong generalization capability, especially in real-world scenarios\ncharacterized by high-turbulence and complex dynamics. We will make the code\nand datasets openly available.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u6548\u7387\u6307\u6570(DEI)\u548c\u591a\u9636\u6bb5\u89c6\u9891\u6062\u590d\u6846\u67b6(PMR)\uff0c\u6709\u6548\u89e3\u51b3\u6e4d\u6d41\u5bfc\u81f4\u7684\u89c6\u9891\u5931\u771f\u95ee\u9898\uff0c\u5c24\u5176\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6062\u590d\u8fb9\u7f18\u7ec6\u8282\u548c\u6d88\u9664\u6df7\u5408\u5931\u771f\uff0c\u5c24\u5176\u662f\u5728\u5f3a\u6e4d\u6d41\u548c\u590d\u6742\u52a8\u6001\u6761\u4ef6\u4e0b\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6548\u7387\u6307\u6570(DEI)\u91cf\u5316\u89c6\u9891\u52a8\u6001\u5f3a\u5ea6\uff0c\u5e76\u5f00\u53d1\u4e86\u7269\u7406\u6a21\u578b\u9a71\u52a8\u7684\u591a\u9636\u6bb5\u89c6\u9891\u6062\u590d\u6846\u67b6(PMR)\uff0c\u5305\u62ec\u53bb\u503e\u659c\u3001\u52a8\u6001\u533a\u57df\u589e\u5f3a\u548c\u53bb\u6a21\u7cca\u4e09\u4e2a\u9636\u6bb5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPMR\u6846\u67b6\u80fd\u6709\u6548\u6291\u5236\u8fd0\u52a8\u62d6\u5c3e\u4f2a\u5f71\u3001\u6062\u590d\u8fb9\u7f18\u7ec6\u8282\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u6548\u7387\u6307\u6570(DEI)\u548c\u591a\u9636\u6bb5\u89c6\u9891\u6062\u590d\u6846\u67b6(PMR)\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u6c14\u6e4d\u6d41\u5bfc\u81f4\u7684\u51e0\u4f55\u5931\u771f\u548c\u6a21\u7cca\u95ee\u9898\uff0c\u5c24\u5176\u5728\u5f3a\u6e4d\u6d41\u548c\u590d\u6742\u52a8\u6001\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.00412", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00412", "abs": "https://arxiv.org/abs/2508.00412", "authors": ["Hanqi Chen", "Xu Zhang", "Xiaoliu Guan", "Lielin Jiang", "Guanzhong Wang", "Zeyu Chen", "Yi Liu"], "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model", "comment": null, "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.", "AI": {"tldr": "Sortblock \u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7f13\u5b58\u548c\u9009\u62e9\u6027\u8df3\u8fc7\u5197\u4f59\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347 DiT \u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668\uff08DiTs\uff09\u5728\u751f\u6210\u80fd\u529b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5e8f\u5217\u53bb\u566a\u8fc7\u7a0b\u5bfc\u81f4\u9ad8\u63a8\u7406\u5ef6\u8fdf\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u53bb\u566a\u9636\u6bb5\u548c Transformer \u5757\u95f4\u7684\u8bed\u4e49\u53d8\u5316\u3002", "method": "\u63d0\u51fa Sortblock \u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7f13\u5b58\u5757\u7ea7\u7279\u5f81\u5e76\u57fa\u4e8e\u76f8\u90bb\u65f6\u95f4\u6b65\u7684\u76f8\u4f3c\u6027\u8fdb\u884c\u6392\u5e8f\uff0c\u81ea\u9002\u5e94\u5730\u786e\u5b9a\u91cd\u8ba1\u7b97\u6bd4\u7387\uff0c\u9009\u62e9\u6027\u8df3\u8fc7\u5197\u4f59\u8ba1\u7b97\uff0c\u5e76\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7ebf\u6027\u9884\u6d4b\u673a\u5236\u51cf\u5c11\u7d2f\u79ef\u8bef\u5dee\u3002", "result": "\u5728\u5404\u79cd\u4efb\u52a1\u548c DiT \u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSortblock \u5b9e\u73b0\u4e86\u8d85\u8fc7 2 \u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u4e14\u8f93\u51fa\u8d28\u91cf\u4ec5\u6709\u6700\u5c0f\u7a0b\u5ea6\u7684\u4e0b\u964d\u3002", "conclusion": "Sortblock \u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7f13\u5b58\u5757\u7ea7\u7279\u5f81\u548c\u9009\u62e9\u6027\u8df3\u8fc7\u5197\u4f59\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u53d8\u6362\u5668\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2508.00413", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00413", "abs": "https://arxiv.org/abs/2508.00413", "authors": ["Junyu Chen", "Dongyun Zou", "Wenkun He", "Junsong Chen", "Enze Xie", "Song Han", "Han Cai"], "title": "DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space", "comment": "ICCV 2025", "summary": "We present DC-AE 1.5, a new family of deep compression autoencoders for\nhigh-resolution diffusion models. Increasing the autoencoder's latent channel\nnumber is a highly effective approach for improving its reconstruction quality.\nHowever, it results in slow convergence for diffusion models, leading to poorer\ngeneration quality despite better reconstruction quality. This issue limits the\nquality upper bound of latent diffusion models and hinders the employment of\nautoencoders with higher spatial compression ratios. We introduce two key\ninnovations to address this challenge: i) Structured Latent Space, a\ntraining-based approach to impose a desired channel-wise structure on the\nlatent space with front latent channels capturing object structures and latter\nlatent channels capturing image details; ii) Augmented Diffusion Training, an\naugmented diffusion training strategy with additional diffusion training\nobjectives on object latent channels to accelerate convergence. With these\ntechniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling\nresults than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better\nimage generation quality than DC-AE-f32c32 while being 4x faster. Code:\nhttps://github.com/dc-ai-projects/DC-Gen.", "AI": {"tldr": "DC-AE 1.5\u901a\u8fc7\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u548c\u589e\u5f3a\u6269\u6563\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u5206\u8fa8\u7387\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u589e\u52a0\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u901a\u9053\u6570\u5bfc\u81f4\u6269\u6563\u6a21\u578b\u6536\u655b\u6162\u3001\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u7a81\u7834\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u8d28\u91cf\u9650\u5236\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u548c\u589e\u5f3a\u6269\u6563\u8bad\u7ec3\u7b56\u7565\uff0c\u524d\u8005\u901a\u8fc7\u8bad\u7ec3\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u901a\u9053\u7ea7\u7ed3\u6784\u5212\u5206\uff0c\u540e\u8005\u901a\u8fc7\u989d\u5916\u6269\u6563\u8bad\u7ec3\u76ee\u6807\u52a0\u901f\u6536\u655b\u3002", "result": "\u5728ImageNet 512x512\u4e0a\uff0cDC-AE-1.5-f64c128\u6bd4DC-AE-f32c32\u751f\u6210\u8d28\u91cf\u66f4\u9ad8\u4e14\u901f\u5ea6\u5feb4\u500d\u3002", "conclusion": "DC-AE 1.5\u901a\u8fc7\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u548c\u589e\u5f3a\u6269\u6563\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5206\u8fa8\u7387\u6269\u6563\u6a21\u578b\u7684\u6536\u655b\u901f\u5ea6\u548c\u751f\u6210\u8d28\u91cf\uff0c\u7a81\u7834\u4e86\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u8d28\u91cf\u4e0a\u9650\u3002"}}
{"id": "2508.00418", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00418", "abs": "https://arxiv.org/abs/2508.00418", "authors": ["Sangwoo Youn", "Minji Lee", "Nokap Tony Park", "Yeonggyoo Jeon", "Taeyoung Na"], "title": "IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator", "comment": "ICIP 2025. Code: https://github.com/sang-w00/IN2OUT", "summary": "Video outpainting presents a unique challenge of extending the borders while\nmaintaining consistency with the given content. In this paper, we suggest the\nuse of video inpainting models that excel in object flow learning and\nreconstruction in outpainting rather than solely generating the background as\nin existing methods. However, directly applying or fine-tuning inpainting\nmodels to outpainting has shown to be ineffective, often leading to blurry\nresults. Our extensive experiments on discriminator designs reveal that a\ncritical component missing in the outpainting fine-tuning process is a\ndiscriminator capable of effectively assessing the perceptual quality of the\nextended areas. To tackle this limitation, we differentiate the objectives of\nadversarial training into global and local goals and introduce a hierarchical\ndiscriminator that meets both objectives. Additionally, we develop a\nspecialized outpainting loss function that leverages both local and global\nfeatures of the discriminator. Fine-tuning on this adversarial loss function\nenhances the generator's ability to produce both visually appealing and\nglobally coherent outpainted scenes. Our proposed method outperforms\nstate-of-the-art methods both quantitatively and qualitatively. Supplementary\nmaterials including the demo video and the code are available in SigPort.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5224\u522b\u5668\u548c\u4e13\u95e8\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u6269\u5c55\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u4e00\u81f4\u6027\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u5c55\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u80cc\u666f\u751f\u6210\uff0c\u800c\u5ffd\u89c6\u4e86\u5bf9\u8c61\u6d41\u5b66\u4e60\u548c\u91cd\u5efa\u3002\u76f4\u63a5\u5e94\u7528\u6216\u5fae\u8c03\u4fee\u590d\u6a21\u578b\u5728\u6269\u5c55\u4efb\u52a1\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u5bfc\u81f4\u6a21\u7cca\u7ed3\u679c\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5224\u522b\u5668\uff0c\u5c06\u5bf9\u6297\u8bad\u7ec3\u7684\u76ee\u6807\u5206\u4e3a\u5168\u5c40\u548c\u5c40\u90e8\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5229\u7528\u5224\u522b\u5668\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u7684\u4e13\u95e8\u6269\u5c55\u635f\u5931\u51fd\u6570\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u5224\u522b\u5668\u548c\u4e13\u95e8\u7684\u6269\u5c55\u635f\u5931\u51fd\u6570\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u89c6\u9891\u6269\u5c55\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u89c6\u89c9\u5438\u5f15\u529b\u548c\u5168\u5c40\u4e00\u81f4\u6027\u7684\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.00427", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00427", "abs": "https://arxiv.org/abs/2508.00427", "authors": ["Seunggeun Chi", "Enna Sachdeva", "Pin-Hao Huang", "Kwonjoon Lee"], "title": "Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting", "comment": "ICCV 2025 (Highlight)", "summary": "Amodal completion, which is the process of inferring the full appearance of\nobjects despite partial occlusions, is crucial for understanding complex\nhuman-object interactions (HOI) in computer vision and robotics. Existing\nmethods, such as those that use pre-trained diffusion models, often struggle to\ngenerate plausible completions in dynamic scenarios because they have a limited\nunderstanding of HOI. To solve this problem, we've developed a new approach\nthat uses physical prior knowledge along with a specialized multi-regional\ninpainting technique designed for HOI. By incorporating physical constraints\nfrom human topology and contact information, we define two distinct regions:\nthe primary region, where occluded object parts are most likely to be, and the\nsecondary region, where occlusions are less probable. Our multi-regional\ninpainting method uses customized denoising strategies across these regions\nwithin a diffusion model. This improves the accuracy and realism of the\ngenerated completions in both their shape and visual detail. Our experimental\nresults show that our approach significantly outperforms existing methods in\nHOI scenarios, moving machine perception closer to a more human-like\nunderstanding of dynamic environments. We also show that our pipeline is robust\neven without ground-truth contact annotations, which broadens its applicability\nto tasks like 3D reconstruction and novel view/pose synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u548c\u591a\u533a\u57df\u4fee\u590d\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u52a8\u6001\u573a\u666f\u4e2d\u906e\u6321\u7269\u4f53\u8865\u5168\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u4e2d\u56e0\u5bf9\u4eba\u7c7b-\u7269\u4f53\u4ea4\u4e92\uff08HOI\uff09\u7406\u89e3\u6709\u9650\u800c\u96be\u4ee5\u751f\u6210\u5408\u7406\u7684\u8865\u5168\u7ed3\u679c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u591a\u533a\u57df\u4fee\u590d\u6280\u672f\uff0c\u7ed3\u5408\u4eba\u7c7b\u62d3\u6251\u548c\u63a5\u89e6\u4fe1\u606f\uff0c\u5b9a\u4e49\u4e3b\u6b21\u533a\u57df\uff0c\u5e76\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5e94\u7528\u5b9a\u5236\u5316\u7684\u53bb\u566a\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728HOI\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u771f\u5b9e\u63a5\u89e6\u6807\u6ce8\u5373\u53ef\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u548c\u591a\u533a\u57df\u4fee\u590d\u6280\u672f\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u4e2d\u906e\u6321\u7269\u4f53\u8865\u5168\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u6027\uff0c\u4f7f\u673a\u5668\u611f\u77e5\u66f4\u63a5\u8fd1\u4eba\u7c7b\u5bf9\u52a8\u6001\u73af\u5883\u7684\u7406\u89e3\u3002"}}
{"id": "2508.00421", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00421", "abs": "https://arxiv.org/abs/2508.00421", "authors": ["Runmin Cong", "Zongji Yu", "Hao Fang", "Haoyan Sun", "Sam Kwong"], "title": "UIS-Mamba: Exploring Mamba for Underwater Instance Segmentation via Dynamic Tree Scan and Hidden State Weaken", "comment": "ACM MM 2025", "summary": "Underwater Instance Segmentation (UIS) tasks are crucial for underwater\ncomplex scene detection. Mamba, as an emerging state space model with\ninherently linear complexity and global receptive fields, is highly suitable\nfor processing image segmentation tasks with long sequence features. However,\ndue to the particularity of underwater scenes, there are many challenges in\napplying Mamba to UIS. The existing fixed-patch scanning mechanism cannot\nmaintain the internal continuity of scanned instances in the presence of\nseverely underwater color distortion and blurred instance boundaries, and the\nhidden state of the complex underwater background can also inhibit the\nunderstanding of instance objects. In this work, we propose the first\nMamba-based underwater instance segmentation model UIS-Mamba, and design two\ninnovative modules, Dynamic Tree Scan (DTS) and Hidden State Weaken (HSW), to\nmigrate Mamba to the underwater task. DTS module maintains the continuity of\nthe internal features of the instance objects by allowing the patches to\ndynamically offset and scale, thereby guiding the minimum spanning tree and\nproviding dynamic local receptive fields. HSW module suppresses the\ninterference of complex backgrounds and effectively focuses the information\nflow of state propagation to the instances themselves through the Ncut-based\nhidden state weakening mechanism. Experimental results show that UIS-Mamba\nachieves state-of-the-art performance on both UIIS and USIS10K datasets, while\nmaintaining a low number of parameters and computational complexity. Code is\navailable at https://github.com/Maricalce/UIS-Mamba.", "AI": {"tldr": "UIS-Mamba\u6a21\u578b\u901a\u8fc7DTS\u548cHSW\u6a21\u5757\u89e3\u51b3\u6c34\u4e0b\u5b9e\u4f8b\u5206\u5272\u4e2d\u7684\u8fde\u7eed\u6027\u4fdd\u6301\u548c\u80cc\u666f\u5e72\u6270\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u6c34\u4e0b\u573a\u666f\u7684\u7279\u6b8a\u6027\uff08\u5982\u989c\u8272\u5931\u771f\u548c\u8fb9\u754c\u6a21\u7cca\uff09\u5bfc\u81f4\u73b0\u6709\u56fa\u5b9a\u8865\u4e01\u626b\u63cf\u673a\u5236\u96be\u4ee5\u4fdd\u6301\u5b9e\u4f8b\u8fde\u7eed\u6027\uff0c\u4e14\u590d\u6742\u80cc\u666f\u4f1a\u6291\u5236\u5b9e\u4f8b\u7406\u89e3\u3002", "method": "\u63d0\u51faUIS-Mamba\u6a21\u578b\uff0c\u5305\u542b\u52a8\u6001\u6811\u626b\u63cf(DTS)\u548c\u9690\u85cf\u72b6\u6001\u5f31\u5316(HSW)\u6a21\u5757\u3002DTS\u901a\u8fc7\u52a8\u6001\u504f\u79fb\u548c\u7f29\u653e\u4fdd\u6301\u5b9e\u4f8b\u5185\u90e8\u7279\u5f81\u8fde\u7eed\u6027\uff0cHSW\u901a\u8fc7Ncut\u673a\u5236\u6291\u5236\u80cc\u666f\u5e72\u6270\u3002", "result": "\u5728UIIS\u548cUSIS10K\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u53c2\u6570\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "UIS-Mamba\u6a21\u578b\u901a\u8fc7\u52a8\u6001\u6811\u626b\u63cf(DTS)\u548c\u9690\u85cf\u72b6\u6001\u5f31\u5316(HSW)\u6a21\u5757\uff0c\u6210\u529f\u5c06Mamba\u6a21\u578b\u8fc1\u79fb\u81f3\u6c34\u4e0b\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\uff0c\u5e76\u5728UIIS\u548cUSIS10K\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2508.00442", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00442", "abs": "https://arxiv.org/abs/2508.00442", "authors": ["Jiale Zhou", "Wenhan Wang", "Shikun Li", "Xiaolei Qu", "Xin Guo", "Yizhong Liu", "Wenzhong Tang", "Xun Lin", "Yefeng Zheng"], "title": "TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation", "comment": null, "summary": "Tubular structure segmentation (TSS) is important for various applications,\nsuch as hemodynamic analysis and route navigation. Despite significant progress\nin TSS, domain shifts remain a major challenge, leading to performance\ndegradation in unseen target domains. Unlike other segmentation tasks, TSS is\nmore sensitive to domain shifts, as changes in topological structures can\ncompromise segmentation integrity, and variations in local features\ndistinguishing foreground from background (e.g., texture and contrast) may\nfurther disrupt topological continuity. To address these challenges, we propose\nTopology-enhanced Test-Time Adaptation (TopoTTA), the first test-time\nadaptation framework designed specifically for TSS. TopoTTA consists of two\nstages: Stage 1 adapts models to cross-domain topological discrepancies using\nthe proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance\ntopological representation without altering pre-trained parameters; Stage 2\nimproves topological continuity by a novel Topology Hard sample Generation\n(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels\nin the generated pseudo-break regions. Extensive experiments across four\nscenarios and ten datasets demonstrate TopoTTA's effectiveness in handling\ntopological distribution shifts, achieving an average improvement of 31.81% in\nclDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS\nmodels.", "AI": {"tldr": "TopoTTA\u662f\u4e00\u79cd\u9488\u5bf9TSS\u7684\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u6709\u6548\u5e94\u5bf9\u57df\u504f\u79fb\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "TSS\u5bf9\u57df\u504f\u79fb\u7279\u522b\u654f\u611f\uff0c\u56e0\u4e3a\u62d3\u6251\u7ed3\u6784\u7684\u53d8\u5316\u53ef\u80fd\u7834\u574f\u5206\u5272\u5b8c\u6574\u6027\uff0c\u800c\u5c40\u90e8\u7279\u5f81\u7684\u53d8\u5316\uff08\u5982\u7eb9\u7406\u548c\u5bf9\u6bd4\u5ea6\uff09\u53ef\u80fd\u8fdb\u4e00\u6b65\u5e72\u6270\u62d3\u6251\u8fde\u7eed\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86TopoTTA\u3002", "method": "TopoTTA\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7Topological Meta Difference Convolutions\uff08TopoMDCs\uff09\u8c03\u6574\u6a21\u578b\u4ee5\u9002\u5e94\u8de8\u57df\u62d3\u6251\u5dee\u5f02\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7Topology Hard sample Generation\uff08TopoHG\uff09\u7b56\u7565\u548c\u4f2a\u6807\u7b7e\u9884\u6d4b\u5bf9\u9f50\u6765\u63d0\u5347\u62d3\u6251\u8fde\u7eed\u6027\u3002", "result": "\u5728\u56db\u79cd\u573a\u666f\u548c\u5341\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86TopoTTA\u5728\u5904\u7406\u62d3\u6251\u5206\u5e03\u504f\u79fb\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e73\u5747\u63d0\u9ad8\u4e8631.81%\u7684clDice\u5206\u6570\u3002", "conclusion": "TopoTTA\u662f\u4e00\u79cd\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u6846\u67b6\uff0c\u4e13\u95e8\u4e3aTSS\u8bbe\u8ba1\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5728\u672a\u89c1\u76ee\u6807\u57df\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e73\u5747\u63d0\u9ad8\u4e8631.81%\u7684clDice\u5206\u6570\u3002"}}
{"id": "2508.00496", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00496", "abs": "https://arxiv.org/abs/2508.00496", "authors": ["Mohammed Kamran", "Maria Bernathova", "Raoul Varga", "Christian Singer", "Zsuzsanna Bago-Horvath", "Thomas Helbich", "Georg Langs", "Philipp Seeb\u00f6ck"], "title": "LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI", "comment": null, "summary": "Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced\nMRI (DCE-MRI) is critical for early cancer detection, especially in high-risk\npatients. While recent deep learning methods have advanced lesion segmentation,\nthey primarily target large lesions and neglect valuable longitudinal and\nclinical information routinely used by radiologists. In real-world screening,\ndetecting subtle or emerging lesions requires radiologists to compare across\ntimepoints and consider previous radiology assessments, such as the BI-RADS\nscore. We propose LesiOnTime, a novel 3D segmentation approach that mimics\nclinical diagnostic workflows by jointly leveraging longitudinal imaging and\nBIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)\nblock that dynamically integrates information from previous and current scans;\nand (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent\nspace alignment for scans with similar radiological assessments, thus embedding\ndomain knowledge into the training process. Evaluated on a curated in-house\nlongitudinal dataset of high-risk patients with DCE-MRI, our approach\noutperforms state-of-the-art single-timepoint and longitudinal baselines by 5%\nin terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute\ncomplementary performance gains. These results highlight the importance of\nincorporating temporal and clinical context for reliable early lesion\nsegmentation in real-world breast cancer screening. Our code is publicly\navailable at https://github.com/cirmuw/LesiOnTime", "AI": {"tldr": "LesiOnTime\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u7eb5\u5411\u5f71\u50cf\u548cBI-RADS\u8bc4\u5206\uff0c\u63d0\u5347\u4e73\u817a\u764cDCE-MRI\u5c0f\u75c5\u53d8\u5206\u5272\u51c6\u786e\u6027\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad85% Dice\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5927\u75c5\u53d8\uff0c\u5ffd\u89c6\u4e86\u7eb5\u5411\u548c\u4e34\u5e8a\u4fe1\u606f\uff0c\u800c\u5b9e\u9645\u7b5b\u67e5\u4e2d\u9700\u5bf9\u6bd4\u65f6\u95f4\u70b9\u5e76\u8003\u8651\u5982BI-RADS\u8bc4\u5206\u7b49\u4e34\u5e8a\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u76843D\u5206\u5272\u65b9\u6cd5LesiOnTime\uff0c\u5305\u62ec\u65f6\u95f4\u5148\u9a8c\u6ce8\u610f\u529b\uff08TPA\uff09\u5757\u548cBI-RADS\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff08BCR\uff09\u635f\u5931\uff0c\u4ee5\u6a21\u62df\u4e34\u5e8a\u8bca\u65ad\u6d41\u7a0b\u3002", "result": "\u5728DCE-MRI\u7eb5\u5411\u6570\u636e\u96c6\u4e0a\uff0cLesiOnTime\u6bd4\u73b0\u6709\u5355\u65f6\u95f4\u70b9\u548c\u7eb5\u5411\u57fa\u7ebf\u65b9\u6cd5\u5728Dice\u6307\u6807\u4e0a\u9ad8\u51fa5%\u3002\u6d88\u878d\u7814\u7a76\u8868\u660eTPA\u548cBCR\u5747\u5e26\u6765\u4e92\u8865\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "LesiOnTime\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u7eb5\u5411\u5f71\u50cf\u548cBI-RADS\u8bc4\u5206\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u75c5\u53d8\u7684\u5206\u5272\u51c6\u786e\u6027\uff0c\u9a8c\u8bc1\u4e86\u5728\u4e73\u817a\u764c\u7b5b\u67e5\u4e2d\u878d\u5165\u65f6\u95f4\u548c\u4e34\u5e8a\u80cc\u666f\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00443", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00443", "abs": "https://arxiv.org/abs/2508.00443", "authors": ["Longfei Huang", "Yu Liang", "Hao Zhang", "Jinwei Chen", "Wei Dong", "Lunde Chen", "Wanyu Liu", "Bo Li", "Pengtao Jiang"], "title": "SDMatte: Grafting Diffusion Models for Interactive Matting", "comment": "Accepted at ICCV 2025, 11 pages, 4 figures", "summary": "Recent interactive matting methods have shown satisfactory performance in\ncapturing the primary regions of objects, but they fall short in extracting\nfine-grained details in edge regions. Diffusion models trained on billions of\nimage-text pairs, demonstrate exceptional capability in modeling highly complex\ndata distributions and synthesizing realistic texture details, while exhibiting\nrobust text-driven interaction capabilities, making them an attractive solution\nfor interactive matting. To this end, we propose SDMatte, a diffusion-driven\ninteractive matting model, with three key contributions. First, we exploit the\npowerful priors of diffusion models and transform the text-driven interaction\ncapability into visual prompt-driven interaction capability to enable\ninteractive matting. Second, we integrate coordinate embeddings of visual\nprompts and opacity embeddings of target objects into U-Net, enhancing\nSDMatte's sensitivity to spatial position information and opacity information.\nThird, we propose a masked self-attention mechanism that enables the model to\nfocus on areas specified by visual prompts, leading to better performance.\nExtensive experiments on multiple datasets demonstrate the superior performance\nof our method, validating its effectiveness in interactive matting. Our code\nand model are available at https://github.com/vivoCameraResearch/SDMatte.", "AI": {"tldr": "SDMatte\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4ea4\u4e92\u5f0f\u62a0\u56fe\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u9a71\u52a8\u548c\u63a9\u819c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u7f18\u7ec6\u8282\u7684\u63d0\u53d6\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u62a0\u56fe\u65b9\u6cd5\u5728\u8fb9\u7f18\u533a\u57df\u7ec6\u8282\u63d0\u53d6\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u800c\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u6570\u636e\u5206\u5e03\u5efa\u6a21\u548c\u7eb9\u7406\u7ec6\u8282\u5408\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u56e0\u6b64\u63a2\u7d22\u5176\u5e94\u7528\u4e8e\u4ea4\u4e92\u5f0f\u62a0\u56fe\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u5148\u9a8c\u77e5\u8bc6\uff0c\u5c06\u6587\u672c\u9a71\u52a8\u4ea4\u4e92\u80fd\u529b\u8f6c\u5316\u4e3a\u89c6\u89c9\u63d0\u793a\u9a71\u52a8\u4ea4\u4e92\u80fd\u529b\uff1b\u6574\u5408\u5750\u6807\u5d4c\u5165\u548c\u900f\u660e\u5ea6\u5d4c\u5165\u5230U-Net\u4e2d\uff1b\u63d0\u51fa\u63a9\u819c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u805a\u7126\u89c6\u89c9\u63d0\u793a\u533a\u57df\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSDMatte\u5728\u4ea4\u4e92\u5f0f\u62a0\u56fe\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SDMatte\u901a\u8fc7\u6269\u6563\u6a21\u578b\u9a71\u52a8\u7684\u4ea4\u4e92\u5f0f\u62a0\u56fe\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u7f18\u533a\u57df\u7ec6\u8282\u7684\u63d0\u53d6\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.00445", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00445", "abs": "https://arxiv.org/abs/2508.00445", "authors": ["Hongyi Cai", "Mohammad Mahdinur Rahman", "Mingkang Dong", "Jie Li", "Muxin Pu", "Zhili Fang", "Yinan Peng", "Hanjun Luo", "Yang Liu"], "title": "AutoDebias: Automated Framework for Debiasing Text-to-Image Models", "comment": null, "summary": "Text-to-Image (T2I) models generate high-quality images from text prompts but\noften exhibit unintended social biases, such as gender or racial stereotypes,\neven when these attributes are not mentioned. Existing debiasing methods work\nwell for simple or well-known cases but struggle with subtle or overlapping\nbiases. We propose AutoDebias, a framework that automatically identifies and\nmitigates harmful biases in T2I models without prior knowledge of specific bias\ntypes. Specifically, AutoDebias leverages vision-language models to detect\nbiased visual patterns and constructs fairness guides by generating inclusive\nalternative prompts that reflect balanced representations. These guides drive a\nCLIP-guided training process that promotes fairer outputs while preserving the\noriginal model's image quality and diversity. Unlike existing methods,\nAutoDebias effectively addresses both subtle stereotypes and multiple\ninteracting biases. We evaluate the framework on a benchmark covering over 25\nbias scenarios, including challenging cases where multiple biases occur\nsimultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and\nreduces biased outputs from 90% to negligible levels, while preserving the\nvisual fidelity of the original model.", "AI": {"tldr": "AutoDebias \u662f\u4e00\u79cd\u81ea\u52a8\u53bb\u504f\u6846\u67b6\uff0c\u6709\u6548\u8bc6\u522b\u548c\u51cf\u8f7b T2I \u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\uff0c\u663e\u8457\u51cf\u5c11\u504f\u89c1\u8f93\u51fa\u4e14\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "T2I \u6a21\u578b\u5e38\u8868\u73b0\u51fa\u6027\u522b\u6216\u79cd\u65cf\u7b49\u793e\u4f1a\u504f\u89c1\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5fae\u5999\u6216\u91cd\u53e0\u7684\u504f\u89c1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u81ea\u52a8\u53bb\u504f\u65b9\u6cd5\u3002", "method": "AutoDebias \u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u504f\u89c1\u89c6\u89c9\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u53cd\u6620\u5e73\u8861\u8868\u5f81\u7684\u5305\u5bb9\u6027\u66ff\u4ee3\u63d0\u793a\u6765\u6784\u5efa\u516c\u5e73\u6307\u5357\uff0c\u8fd9\u4e9b\u6307\u5357\u9a71\u52a8 CLIP \u5f15\u5bfc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u8986\u76d6 25 \u79cd\u4ee5\u4e0a\u504f\u89c1\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAutoDebias \u4ee5 91.6% \u7684\u51c6\u786e\u7387\u68c0\u6d4b\u6709\u5bb3\u6a21\u5f0f\uff0c\u5c06\u504f\u89c1\u8f93\u51fa\u4ece 90% \u964d\u81f3\u53ef\u5ffd\u7565\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "AutoDebias \u662f\u4e00\u79cd\u6709\u6548\u8bc6\u522b\u548c\u51cf\u8f7b T2I \u6a21\u578b\u4e2d\u793e\u4f1a\u504f\u89c1\u7684\u6846\u67b6\uff0c\u65e0\u9700\u9884\u5148\u4e86\u89e3\u5177\u4f53\u504f\u89c1\u7c7b\u578b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u504f\u89c1\u8f93\u51fa\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2508.00447", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00447", "abs": "https://arxiv.org/abs/2508.00447", "authors": ["Anju Rani", "Daniel Ortiz-Arroyo", "Petar Durdevic"], "title": "CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text", "comment": "11 pages, 8 figures", "summary": "Understanding the temporal dynamics of biological growth is critical across\ndiverse fields such as microbiology, agriculture, and biodegradation research.\nAlthough vision-language models like Contrastive Language Image Pretraining\n(CLIP) have shown strong capabilities in joint visual-textual reasoning, their\neffectiveness in capturing temporal progression remains limited. To address\nthis, we propose CLIPTime, a multimodal, multitask framework designed to\npredict both the developmental stage and the corresponding timestamp of fungal\ngrowth from image and text inputs. Built upon the CLIP architecture, our model\nlearns joint visual-textual embeddings and enables time-aware inference without\nrequiring explicit temporal input during testing. To facilitate training and\nevaluation, we introduce a synthetic fungal growth dataset annotated with\naligned timestamps and categorical stage labels. CLIPTime jointly performs\nclassification and regression, predicting discrete growth stages alongside\ncontinuous timestamps. We also propose custom evaluation metrics, including\ntemporal accuracy and regression error, to assess the precision of time-aware\npredictions. Experimental results demonstrate that CLIPTime effectively models\nbiological progression and produces interpretable, temporally grounded outputs,\nhighlighting the potential of vision-language models in real-world biological\nmonitoring applications.", "AI": {"tldr": "CLIPTime \u662f\u57fa\u4e8e CLIP \u7684\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u771f\u83cc\u751f\u957f\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u80fd\u9884\u6d4b\u751f\u957f\u9636\u6bb5\u548c\u65f6\u95f4\u6233\uff0c\u9002\u7528\u4e8e\u751f\u7269\u76d1\u6d4b\u3002", "motivation": "\u56e0\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982 CLIP\uff09\u5728\u6355\u6349\u65f6\u95f4\u52a8\u6001\u65b9\u9762\u8868\u73b0\u6709\u9650\uff0c\u9700\u5f00\u53d1\u80fd\u9884\u6d4b\u751f\u7269\u751f\u957f\u9636\u6bb5\u548c\u65f6\u95f4\u6233\u7684\u6846\u67b6\u3002", "method": "\u57fa\u4e8e CLIP \u67b6\u6784\uff0c\u63d0\u51fa\u4e86 CLIPTime \u591a\u6a21\u6001\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u89c6\u89c9-\u6587\u672c\u5d4c\u5165\uff0c\u5b9e\u73b0\u65e0\u9700\u663e\u5f0f\u65f6\u95f4\u8f93\u5165\u7684\u65f6\u5e8f\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCLIPTime \u80fd\u6709\u6548\u5efa\u6a21\u751f\u7269\u751f\u957f\u8fdb\u7a0b\uff0c\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u76f8\u5173\u8f93\u51fa\u3002", "conclusion": "CLIPTime \u6210\u529f\u5efa\u6a21\u4e86\u751f\u7269\u751f\u957f\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u5c55\u793a\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u751f\u7269\u76d1\u6d4b\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00591", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.00591", "abs": "https://arxiv.org/abs/2508.00591", "authors": ["Mingrui Liu", "Sixiao Zhang", "Cheng Long"], "title": "Wukong Framework for Not Safe For Work Detection in Text-to-Image systems", "comment": "Under review", "summary": "Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)\ntechnology enabling diverse and creative image synthesis. However, some outputs\nmay contain Not Safe For Work (NSFW) content (e.g., violence), violating\ncommunity guidelines. Detecting NSFW content efficiently and accurately, known\nas external safeguarding, is essential. Existing external safeguards fall into\ntwo types: text filters, which analyze user prompts but overlook T2I\nmodel-specific variations and are prone to adversarial attacks; and image\nfilters, which analyze final generated images but are computationally costly\nand introduce latency. Diffusion models, the foundation of modern T2I systems\nlike Stable Diffusion, generate images through iterative denoising using a\nU-Net architecture with ResNet and Transformer blocks. We observe that: (1)\nearly denoising steps define the semantic layout of the image, and (2)\ncross-attention layers in U-Net are crucial for aligning text and image\nregions. Based on these insights, we propose Wukong, a transformer-based NSFW\ndetection framework that leverages intermediate outputs from early denoising\nsteps and reuses U-Net's pre-trained cross-attention parameters. Wukong\noperates within the diffusion process, enabling early detection without waiting\nfor full image generation. We also introduce a new dataset containing prompts,\nseeds, and image-specific NSFW labels, and evaluate Wukong on this and two\npublic benchmarks. Results show that Wukong significantly outperforms\ntext-based safeguards and achieves comparable accuracy of image filters, while\noffering much greater efficiency.", "AI": {"tldr": "Wukong\u662f\u4e00\u79cd\u9ad8\u6548\u7684NSFW\u5185\u5bb9\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u4e2d\u95f4\u8f93\u51fa\u548c\u9884\u8bad\u7ec3\u53c2\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u6587\u672c\u8fc7\u6ee4\u5668\uff0c\u4e0e\u56fe\u50cf\u8fc7\u6ee4\u5668\u7cbe\u5ea6\u76f8\u5f53\u4f46\u66f4\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u7684NSFW\u5185\u5bb9\u5916\u90e8\u9632\u62a4\u63aa\u65bd\u5b58\u5728\u4e0d\u8db3\uff1a\u6587\u672c\u8fc7\u6ee4\u5668\u6613\u53d7\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u56fe\u50cf\u8fc7\u6ee4\u5668\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5f15\u5165\u5ef6\u8fdf\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u65e9\u671f\u53bb\u566a\u6b65\u9aa4\u7684\u4e2d\u95f4\u8f93\u51fa\u548cU-Net\u9884\u8bad\u7ec3\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u53c2\u6570\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684NSFW\u68c0\u6d4b\u6846\u67b6Wukong\u3002", "result": "Wukong\u5728\u63d0\u51fa\u7684\u65b0\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u516c\u5171\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u4e86\u65e9\u671f\u68c0\u6d4b\u800c\u4e0d\u9700\u8981\u7b49\u5f85\u5b8c\u6574\u56fe\u50cf\u751f\u6210\u3002", "conclusion": "Wukong\u6846\u67b6\u5728NSFW\u5185\u5bb9\u68c0\u6d4b\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u7684\u9632\u62a4\u63aa\u65bd\uff0c\u5e76\u4e0e\u57fa\u4e8e\u56fe\u50cf\u7684\u8fc7\u6ee4\u5668\u7cbe\u5ea6\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u3002"}}
{"id": "2508.00453", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00453", "abs": "https://arxiv.org/abs/2508.00453", "authors": ["Baisong Li", "Xingwang Wang", "Haixiao Xu"], "title": "PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA", "comment": null, "summary": "The goal of multispectral and hyperspectral image fusion (MHIF) is to\ngenerate high-quality images that simultaneously possess rich spectral\ninformation and fine spatial details. However, due to the inherent trade-off\nbetween spectral and spatial information and the limited availability of\nobservations, this task is fundamentally ill-posed. Previous studies have not\neffectively addressed the ill-posed nature caused by data misalignment. To\ntackle this challenge, we propose a fusion framework named PIF-Net, which\nexplicitly incorporates ill-posed priors to effectively fuse multispectral\nimages and hyperspectral images. To balance global spectral modeling with\ncomputational efficiency, we design a method based on an invertible Mamba\narchitecture that maintains information consistency during feature\ntransformation and fusion, ensuring stable gradient flow and process\nreversibility. Furthermore, we introduce a novel fusion module called the\nFusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral\nand spatial features while keeping the model lightweight. Extensive experiments\non multiple benchmark datasets demonstrate that PIF-Net achieves significantly\nbetter image restoration performance than current state-of-the-art methods\nwhile maintaining model efficiency.", "AI": {"tldr": "PIF-Net\u901a\u8fc7\u53ef\u9006Mamba\u67b6\u6784\u548c\u52a8\u6001\u6821\u51c6\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5149\u8c31\u4e0e\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u7684\u75c5\u6001\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u5149\u8c31\u4e0e\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\uff08MHIF\uff09\u56e0\u5149\u8c31\u4e0e\u7a7a\u95f4\u4fe1\u606f\u7684\u56fa\u6709\u6743\u8861\u53ca\u89c2\u6d4b\u6570\u636e\u6709\u9650\u800c\u5177\u6709\u75c5\u6001\u6027\uff0c\u73b0\u6709\u7814\u7a76\u672a\u80fd\u6709\u6548\u89e3\u51b3\u6570\u636e\u672a\u5bf9\u9f50\u5bfc\u81f4\u7684\u75c5\u6001\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u9006Mamba\u67b6\u6784\u7684\u878d\u5408\u6846\u67b6PIF-Net\uff0c\u8bbe\u8ba1\u4e86Fusion-Aware Low-Rank Adaptation\u6a21\u5757\u52a8\u6001\u6821\u51c6\u5149\u8c31\u4e0e\u7a7a\u95f4\u7279\u5f81\u3002", "result": "PIF-Net\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "PIF-Net\u901a\u8fc7\u5f15\u5165ill-posed priors\u548c\u521b\u65b0\u7684\u53ef\u9006Mamba\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5149\u8c31\u548c\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.00471", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00471", "abs": "https://arxiv.org/abs/2508.00471", "authors": ["Yiwen Wang", "Xinning Chai", "Yuhong Zhang", "Zhengxue Cheng", "Jun Zhao", "Rong Xie", "Li Song"], "title": "Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution", "comment": null, "summary": "Recent advancements in video super-resolution (VSR) models have demonstrated\nimpressive results in enhancing low-resolution videos. However, due to\nlimitations in adequately controlling the generation process, achieving high\nfidelity alignment with the low-resolution input while maintaining temporal\nconsistency across frames remains a significant challenge. In this work, we\npropose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel\napproach that incorporates both semantic and temporal-spatio guidance in the\nlatent diffusion space to address these challenges. By incorporating high-level\nsemantic information and integrating spatial and temporal information, our\napproach achieves a seamless balance between recovering intricate details and\nensuring temporal coherence. Our method not only preserves high-reality visual\ncontent but also significantly enhances fidelity. Extensive experiments\ndemonstrate that SeTe-VSR outperforms existing methods in terms of detail\nrecovery and perceptual quality, highlighting its effectiveness for complex\nvideo super-resolution tasks.", "AI": {"tldr": "SeTe-VSR\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u548c\u65f6\u7a7a\u5f15\u5bfc\uff0c\u5728\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7ec6\u8282\u6062\u590d\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u5e73\u8861\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u5bf9\u9f50\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u548c\u4fdd\u6301\u5e27\u95f4\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u4e49\u548c\u65f6\u95f4\u5f15\u5bfc\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff08SeTe-VSR\uff09\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u6269\u6563\u7a7a\u95f4\u4e2d\u7ed3\u5408\u9ad8\u5c42\u6b21\u8bed\u4e49\u4fe1\u606f\u53ca\u65f6\u7a7a\u4fe1\u606f\uff0c\u5b9e\u73b0\u7ec6\u8282\u6062\u590d\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSeTe-VSR\u5728\u7ec6\u8282\u6062\u590d\u548c\u611f\u77e5\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u7684\u6548\u679c\u3002", "conclusion": "SeTe-VSR\u65b9\u6cd5\u5728\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u5728\u7ec6\u8282\u6062\u590d\u548c\u611f\u77e5\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd8\u80fd\u6709\u6548\u5e73\u8861\u7ec6\u8282\u6062\u590d\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.00620", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00620", "abs": "https://arxiv.org/abs/2508.00620", "authors": ["Quentin Le Roux", "Yannick Teglia", "Teddy Furon", "Philippe Loubet-Moundi"], "title": "Backdoor Attacks on Deep Learning Face Detection", "comment": null, "summary": "Face Recognition Systems that operate in unconstrained environments capture\nimages under varying conditions,such as inconsistent lighting, or diverse face\nposes. These challenges require including a Face Detection module that\nregresses bounding boxes and landmark coordinates for proper Face Alignment.\nThis paper shows the effectiveness of Object Generation Attacks on Face\nDetection, dubbed Face Generation Attacks, and demonstrates for the first time\na Landmark Shift Attack that backdoors the coordinate regression task performed\nby face detectors. We then offer mitigations against these vulnerabilities.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u9488\u5bf9\u9762\u90e8\u68c0\u6d4b\u7cfb\u7edf\u7684\u5bf9\u8c61\u751f\u6210\u653b\u51fb\u548c\u5730\u6807\u504f\u79fb\u653b\u51fb\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u63aa\u65bd\u3002", "motivation": "\u7814\u7a76\u5728\u975e\u7ea6\u675f\u73af\u5883\u4e0b\u5de5\u4f5c\u7684\u9762\u90e8\u8bc6\u522b\u7cfb\u7edf\uff0c\u56e0\u5149\u7167\u3001\u59ff\u6001\u7b49\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u9762\u90e8\u68c0\u6d4b\u6a21\u5757\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002", "method": "\u901a\u8fc7\u5bf9\u8c61\u751f\u6210\u653b\u51fb\u548c\u5730\u6807\u504f\u79fb\u653b\u51fb\uff0c\u7814\u7a76\u9762\u90e8\u68c0\u6d4b\u7cfb\u7edf\u7684\u8106\u5f31\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5bf9\u8c61\u751f\u6210\u653b\u51fb\u548c\u5730\u6807\u504f\u79fb\u653b\u51fb\u5bf9\u56de\u5f52\u8fb9\u754c\u6846\u548c\u5730\u6807\u5750\u6807\u7684\u9762\u90e8\u68c0\u6d4b\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u9762\u90e8\u68c0\u6d4b\u7cfb\u7edf\u7684\u5bf9\u8c61\u751f\u6210\u653b\u51fb\uff08Face Generation Attacks\uff09\u548c\u9996\u6b21\u5c55\u793a\u7684\u5730\u6807\u504f\u79fb\u653b\u51fb\uff08Landmark Shift Attack\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u9488\u5bf9\u8fd9\u4e9b\u6f0f\u6d1e\u7684\u7f13\u89e3\u63aa\u65bd\u3002"}}
{"id": "2508.00473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00473", "abs": "https://arxiv.org/abs/2508.00473", "authors": ["Jiaping Cao", "Kangkang Zhou", "Juan Du"], "title": "HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection", "comment": null, "summary": "Video anomaly detection is a fundamental task in video surveillance, with\nbroad applications in public safety and intelligent monitoring systems.\nAlthough previous methods leverage Euclidean representations in RGB or depth\ndomains, such embeddings are inherently limited in capturing hierarchical event\nstructures and spatio-temporal continuity. To address these limitations, we\npropose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for\nanomaly detection in 3D point cloud videos. Our approach first extracts\nper-frame spatial features from point cloud sequences via point cloud\nextractor, and then embeds them into Lorentzian hyperbolic space, which better\ncaptures the latent hierarchical structure of events. To model temporal\ndynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism\nthat leverages Lorentzian inner products and curvature-aware softmax to learn\ntemporal dependencies under non-Euclidean geometry. Our method performs all\nfeature transformations and anomaly scoring directly within full Lorentzian\nspace rather than via tangent space approximation. Extensive experiments\ndemonstrate that HyPCV-Former achieves state-of-the-art performance across\nmultiple anomaly categories, with a 7\\% improvement on the TIMo dataset and a\n5.6\\% gain on the DAD dataset compared to benchmarks. The code will be released\nupon paper acceptance.", "AI": {"tldr": "\u63d0\u51faHyPCV-Former\uff0c\u4e00\u79cd\u57fa\u4e8e\u53cc\u66f2\u65f6\u7a7a\u53d8\u6362\u5668\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e3D\u70b9\u4e91\u89c6\u9891\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6b27\u51e0\u91cc\u5f97\u8868\u793a\u7684\u65b9\u6cd5\u5728\u6355\u6349\u5c42\u6b21\u5316\u4e8b\u4ef6\u7ed3\u6784\u548c\u65f6\u7a7a\u8fde\u7eed\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u66f2\u65f6\u7a7a\u53d8\u6362\u5668HyPCV-Former\uff0c\u5229\u7528\u6d1b\u4f26\u5179\u53cc\u66f2\u7a7a\u95f4\u5d4c\u5165\u70b9\u4e91\u5e8f\u5217\u7684\u6bcf\u5e27\u7a7a\u95f4\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u53cc\u66f2\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff08HMHA\uff09\u6765\u5efa\u6a21\u65f6\u95f4\u52a8\u6001\u3002", "result": "HyPCV-Former\u5728TIMo\u548cDAD\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e867%\u548c5.6%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "HyPCV-Former\u901a\u8fc7\u5728\u5b8c\u6574\u7684\u6d1b\u4f26\u5179\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7279\u5f81\u8f6c\u6362\u548c\u5f02\u5e38\u8bc4\u5206\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u70b9\u4e91\u89c6\u9891\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002"}}
{"id": "2508.00477", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00477", "abs": "https://arxiv.org/abs/2508.00477", "authors": ["Yuzhuo Chen", "Zehua Ma", "Jianhua Wang", "Kai Kang", "Shunyu Yao", "Weiming Zhang"], "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer", "comment": "8 pages, 5 figures, 3 tables", "summary": "In controllable image synthesis, generating coherent and consistent images\nfrom multiple references with spatial layout awareness remains an open\nchallenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework\nthat, for the first time, extends single-reference diffusion models to\nmulti-reference scenarios in a training-free manner. Built upon the MMDiT\nmodel, LAMIC introduces two plug-and-play attention mechanisms: 1) Group\nIsolation Attention (GIA) to enhance entity disentanglement; and 2)\nRegion-Modulated Attention (RMA) to enable layout-aware generation. To\ncomprehensively evaluate model capabilities, we further introduce three\nmetrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout\ncontrol; and 2) Background Similarity (BG-S) for measuring background\nconsistency. Extensive experiments show that LAMIC achieves state-of-the-art\nperformance across most major metrics: it consistently outperforms existing\nmulti-reference baselines in ID-S, BG-S, IN-R and AVG scores across all\nsettings, and achieves the best DPG in complex composition tasks. These results\ndemonstrate LAMIC's superior abilities in identity keeping, background\npreservation, layout control, and prompt-following, all achieved without any\ntraining or fine-tuning, showcasing strong zero-shot generalization ability. By\ninheriting the strengths of advanced single-reference models and enabling\nseamless extension to multi-image scenarios, LAMIC establishes a new\ntraining-free paradigm for controllable multi-image composition. As foundation\nmodels continue to evolve, LAMIC's performance is expected to scale\naccordingly. Our implementation is available at:\nhttps://github.com/Suchenl/LAMIC.", "AI": {"tldr": "LAMIC\u662f\u4e00\u79cd\u5e03\u5c40\u611f\u77e5\u591a\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7GIA\u548cRMA\u4e24\u79cd\u6ce8\u610f\u529b\u673a\u5236\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5c06\u5355\u53c2\u8003\u6269\u6563\u6a21\u578b\u6269\u5c55\u5230\u591a\u53c2\u8003\u573a\u666f\uff0c\u5b9e\u73b0\u4e86\u5e03\u5c40\u63a7\u5236\u3001\u80cc\u666f\u4e00\u81f4\u6027\u548c\u8eab\u4efd\u4fdd\u6301\u7684\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u5728\u53ef\u63a7\u56fe\u50cf\u5408\u6210\u4e2d\uff0c\u4ece\u591a\u4e2a\u53c2\u8003\u751f\u6210\u5177\u6709\u7a7a\u95f4\u5e03\u5c40\u610f\u8bc6\u7684\u8fde\u8d2f\u4e14\u4e00\u81f4\u7684\u56fe\u50cf\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002", "method": "LAMIC\u57fa\u4e8eMMDiT\u6a21\u578b\uff0c\u5f15\u5165\u4e86\u4e24\u79cd\u5373\u63d2\u5373\u7528\u7684\u6ce8\u610f\u529b\u673a\u5236\uff1a1) Group Isolation Attention (GIA) \u7528\u4e8e\u589e\u5f3a\u5b9e\u4f53\u89e3\u8026\uff1b2) Region-Modulated Attention (RMA) \u7528\u4e8e\u5b9e\u73b0\u5e03\u5c40\u611f\u77e5\u751f\u6210\u3002", "result": "LAMIC\u5728\u5927\u591a\u6570\u4e3b\u8981\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff1a\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\uff0cID-S\u3001BG-S\u3001IN-R\u548cAVG\u5f97\u5206\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u53c2\u8003\u57fa\u7ebf\uff0c\u5e76\u5728\u590d\u6742\u5408\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73DPG\u3002", "conclusion": "LAMIC\u901a\u8fc7\u5f15\u5165GIA\u548cRMA\u4e24\u79cd\u6ce8\u610f\u529b\u673a\u5236\uff0c\u9996\u6b21\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5c06\u5355\u53c2\u8003\u6269\u6563\u6a21\u578b\u6269\u5c55\u5230\u591a\u53c2\u8003\u573a\u666f\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e3a\u53ef\u63a7\u591a\u56fe\u50cf\u5408\u6210\u5efa\u7acb\u4e86\u65b0\u7684\u65e0\u8bad\u7ec3\u8303\u5f0f\u3002"}}
{"id": "2508.00701", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00701", "abs": "https://arxiv.org/abs/2508.00701", "authors": ["Chende Zheng", "Ruiqi suo", "Chenhao Lin", "Zhengyu Zhao", "Le Yang", "Shuai Liu", "Minghui Yang", "Cong Wang", "Chao Shen"], "title": "D3: Training-Free AI-Generated Video Detection Using Second-Order Features", "comment": "8 pages, 4 figures", "summary": "The evolution of video generation techniques, such as Sora, has made it\nincreasingly easy to produce high-fidelity AI-generated videos, raising public\nconcern over the dissemination of synthetic content. However, existing\ndetection methodologies remain limited by their insufficient exploration of\ntemporal artifacts in synthetic videos. To bridge this gap, we establish a\ntheoretical framework through second-order dynamical analysis under Newtonian\nmechanics, subsequently extending the Second-order Central Difference features\ntailored for temporal artifact detection. Building on this theoretical\nfoundation, we reveal a fundamental divergence in second-order feature\ndistributions between real and AI-generated videos. Concretely, we propose\nDetection by Difference of Differences (D3), a novel training-free detection\nmethod that leverages the above second-order temporal discrepancies. We\nvalidate the superiority of our D3 on 4 open-source datasets (Gen-Video,\nVideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,\nD3 outperforms the previous best method by 10.39% (absolute) mean Average\nPrecision. Additional experiments on time cost and post-processing operations\ndemonstrate D3's exceptional computational efficiency and strong robust\nperformance. Our code is available at https://github.com/Zig-HS/D3.", "AI": {"tldr": "D3\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8c\u9636\u65f6\u95f4\u5dee\u5f02\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u9c81\u68d2\u3002", "motivation": "\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u5408\u6210\u89c6\u9891\u4e2d\u65f6\u95f4\u4f2a\u5f71\u7684\u63a2\u7d22\u4e0d\u8db3\uff0c\u9700\u5f00\u53d1\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u6280\u672f\u4ee5\u5e94\u5bf9\u9ad8\u4fdd\u771fAI\u751f\u6210\u89c6\u9891\u7684\u6cdb\u6ee5\u3002", "method": "\u63d0\u51fa\u4e86Detection by Difference of Differences (D3)\u65b9\u6cd5\uff0c\u57fa\u4e8e\u725b\u987f\u529b\u5b66\u4e0b\u7684\u4e8c\u9636\u52a8\u529b\u5b66\u5206\u6790\uff0c\u4e13\u6ce8\u4e8e\u65f6\u95f4\u4f2a\u5f71\u68c0\u6d4b\u3002", "result": "\u57284\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\uff0840\u4e2a\u5b50\u96c6\uff09\u4e0a\u9a8c\u8bc1\u4e86D3\u7684\u4f18\u8d8a\u6027\uff0c\u5982\u5728Gen-Video\u4e0a\u6bd4\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u534710.39%\u7684mAP\u3002", "conclusion": "D3\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u4e8c\u9636\u65f6\u95f4\u5dee\u5f02\uff0c\u663e\u8457\u63d0\u9ad8\u4e86AI\u751f\u6210\u89c6\u9891\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5728\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.00493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00493", "abs": "https://arxiv.org/abs/2508.00493", "authors": ["Alfie Roddan", "Tobias Czempiel", "Chi Xu", "Daniel S. Elson", "Stamatia Giannarou"], "title": "SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation", "comment": null, "summary": "We present SAMSA 2.0, an interactive segmentation framework for hyperspectral\nmedical imaging that introduces spectral angle prompting to guide the Segment\nAnything Model (SAM) using spectral similarity alongside spatial cues. This\nearly fusion of spectral information enables more accurate and robust\nsegmentation across diverse spectral datasets. Without retraining, SAMSA 2.0\nachieves up to +3.8% higher Dice scores compared to RGB-only models and up to\n+3.1% over prior spectral fusion methods. Our approach enhances few-shot and\nzero-shot performance, demonstrating strong generalization in challenging\nlow-data and noisy scenarios common in clinical imaging.", "AI": {"tldr": "SAMSA 2.0\u7ed3\u5408\u5149\u8c31\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u63d0\u5347\u9ad8\u5149\u8c31\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5149\u8c31\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u56e0\u6570\u636e\u7a00\u758f\u548c\u566a\u58f0\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u5347\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u5206\u5272\u6846\u67b6\uff0c\u7ed3\u5408\u5149\u8c31\u89d2\u5ea6\u63d0\u793a\u548c\u7a7a\u95f4\u7ebf\u7d22\uff0c\u6307\u5bfcSegment Anything Model\uff08SAM\uff09\u8fdb\u884c\u5206\u5272\u3002", "result": "\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0cSAMSA 2.0\u6bd4\u4ec5\u4f7f\u7528RGB\u7684\u6a21\u578bDice\u5206\u6570\u63d0\u9ad8\u4e863.8%\uff0c\u6bd4\u4e4b\u524d\u7684\u5149\u8c31\u878d\u5408\u65b9\u6cd5\u63d0\u9ad8\u4e863.1%\u3002", "conclusion": "SAMSA 2.0\u901a\u8fc7\u5f15\u5165\u5149\u8c31\u89d2\u5ea6\u63d0\u793a\uff0c\u7ed3\u5408\u5149\u8c31\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u533b\u5b66\u56fe\u50cf\u7684\u5206\u5272\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.00748", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.00748", "abs": "https://arxiv.org/abs/2508.00748", "authors": ["Laura Pedrouzo-Rodriguez", "Pedro Delgado-DeRobles", "Luis F. Gomez", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez"], "title": "Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos", "comment": "Accepted at the IEEE International Joint Conference on Biometrics\n  (IJCB 2025)", "summary": "Photorealistic talking-head avatars are becoming increasingly common in\nvirtual meetings, gaming, and social platforms. These avatars allow for more\nimmersive communication, but they also introduce serious security risks. One\nemerging threat is impersonation: an attacker can steal a user's\navatar-preserving their appearance and voice-making it nearly impossible to\ndetect its fraudulent usage by sight or sound alone. In this paper, we explore\nthe challenge of biometric verification in such avatar-mediated scenarios. Our\nmain question is whether an individual's facial motion patterns can serve as\nreliable behavioral biometrics to verify their identity when the avatar's\nvisual appearance is a facsimile of its owner. To answer this question, we\nintroduce a new dataset of realistic avatar videos created using a\nstate-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and\nimpostor avatar videos. We also propose a lightweight, explainable\nspatio-temporal Graph Convolutional Network architecture with temporal\nattention pooling, that uses only facial landmarks to model dynamic facial\ngestures. Experimental results demonstrate that facial motion cues enable\nmeaningful identity verification with AUC values approaching 80%. The proposed\nbenchmark and biometric system are available for the research community in\norder to bring attention to the urgent need for more advanced behavioral\nbiometric defenses in avatar-based communication systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u865a\u62df\u5316\u8eab\u4e2d\u7684\u751f\u7269\u7279\u5f81\u9a8c\u8bc1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9762\u90e8\u8fd0\u52a8\u6a21\u5f0f\u7684\u8f7b\u91cf\u7ea7\u8eab\u4efd\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u8ba8\u5728\u865a\u62df\u5316\u8eab\u4ecb\u5bfc\u7684\u573a\u666f\u4e2d\uff0c\u9762\u90e8\u8fd0\u52a8\u6a21\u5f0f\u662f\u5426\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u9760\u7684\u884c\u4e3a\u751f\u7269\u7279\u5f81\u6765\u9a8c\u8bc1\u8eab\u4efd\uff0c\u4ee5\u5e94\u5bf9\u865a\u62df\u5316\u8eab\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u865a\u62df\u5316\u8eab\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u3001\u53ef\u89e3\u91ca\u7684\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u65f6\u95f4\u6ce8\u610f\u529b\u6c60\u5316\uff0c\u4ec5\u4f7f\u7528\u9762\u90e8\u5173\u952e\u70b9\u6765\u5efa\u6a21\u52a8\u6001\u9762\u90e8\u8868\u60c5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9762\u90e8\u8fd0\u52a8\u7ebf\u7d22\u80fd\u591f\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u8eab\u4efd\u9a8c\u8bc1\uff0cAUC\u503c\u63a5\u8fd180%\u3002", "conclusion": "\u9762\u90e8\u8fd0\u52a8\u6a21\u5f0f\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u9760\u7684\u884c\u4e3a\u751f\u7269\u7279\u5f81\u8fdb\u884c\u8eab\u4efd\u9a8c\u8bc1\uff0c\u5728\u865a\u62df\u5316\u8eab\u4ea4\u6d41\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.00766", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00766", "abs": "https://arxiv.org/abs/2508.00766", "authors": ["Irene Iele", "Francesco Di Feola", "Valerio Guarrasi", "Paolo Soda"], "title": "Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation", "comment": null, "summary": "Image-to-image translation has emerged as a powerful technique in medical\nimaging, enabling tasks such as image denoising and cross-modality conversion.\nHowever, it suffers from limitations in handling out-of-distribution samples\nwithout causing performance degradation. To address this limitation, we propose\na novel Test-Time Adaptation (TTA) framework that dynamically adjusts the\ntranslation process based on the characteristics of each test sample. Our\nmethod introduces a Reconstruction Module to quantify the domain shift and a\nDynamic Adaptation Block that selectively modifies the internal features of a\npretrained translation model to mitigate the shift without compromising the\nperformance on in-distribution samples that do not require adaptation. We\nevaluate our approach on two medical image-to-image translation tasks: low-dose\nCT denoising and T1 to T2 MRI translation, showing consistent improvements over\nboth the baseline translation model without TTA and prior TTA methods. Our\nanalysis highlights the limitations of the state-of-the-art that uniformly\napply the adaptation to both out-of-distribution and in-distribution samples,\ndemonstrating that dynamic, sample-specific adjustment offers a promising path\nto improve model resilience in real-world scenarios. The code is available at:\nhttps://github.com/cosbidev/Sample-Aware_TTA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u7684\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u6837\u672c\u7279\u5b9a\u7684\u8c03\u6574\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u7ffb\u8bd1\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5206\u5e03\u5916\u6837\u672c\u65f6\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u5728\u5904\u7406\u5206\u5e03\u5916\u6837\u672c\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u91cd\u5efa\u6a21\u5757\uff08\u7528\u4e8e\u91cf\u5316\u57df\u504f\u79fb\uff09\u548c\u52a8\u6001\u9002\u5e94\u5757\uff08\u9009\u62e9\u6027\u5730\u4fee\u6539\u9884\u8bad\u7ec3\u7ffb\u8bd1\u6a21\u578b\u7684\u5185\u90e8\u7279\u5f81\uff09\u7684\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\uff08TTA\uff09\u6846\u67b6\u3002", "result": "\u5728\u4e24\u4e2a\u533b\u5b66\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u4efb\u52a1\u4e0a\uff08\u4f4e\u5242\u91cfCT\u53bb\u566a\u548cT1\u5230T2 MRI\u8f6c\u6362\uff09\u5747\u663e\u793a\u51fa\u6bd4\u57fa\u7ebf\u7ffb\u8bd1\u6a21\u578b\u548c\u5148\u524d\u7684TTA\u65b9\u6cd5\u66f4\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "conclusion": "\u52a8\u6001\u3001\u6837\u672c\u7279\u5b9a\u7684\u8c03\u6574\u4e3a\u63d0\u9ad8\u6a21\u578b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u97e7\u6027\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2508.00506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00506", "abs": "https://arxiv.org/abs/2508.00506", "authors": ["Tulsi Patel", "Mark W. Jones", "Thomas Redfern"], "title": "Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool", "comment": "Video supplement demonstrating feature-space exploration and\n  interactive labelling is available at: https://youtu.be/GZl1ebZJgEA and is\n  archived at https://doi.org/10.5281/zenodo.16676591", "summary": "Machine learning for remote sensing imaging relies on up-to-date and accurate\nlabels for model training and testing. Labelling remote sensing imagery is time\nand cost intensive, requiring expert analysis. Previous labelling tools rely on\npre-labelled data for training in order to label new unseen data. In this work,\nwe define an unsupervised pipeline for finding and labelling geographical areas\nof similar context and content within Sentinel-2 satellite imagery. Our\napproach removes limitations of previous methods by utilising segmentation with\nconvolutional and graph neural networks to encode a more robust feature space\nfor image comparison. Unlike previous approaches we segment the image into\nhomogeneous regions of pixels that are grouped based on colour and spatial\nsimilarity. Graph neural networks are used to aggregate information about the\nsurrounding segments enabling the feature representation to encode the local\nneighbourhood whilst preserving its own local information. This reduces\noutliers in the labelling tool, allows users to label at a granular level, and\nallows a rotationally invariant semantic relationship at the image level to be\nformed within the encoding space.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u7ba1\u9053\u65b9\u6cd5\uff0c\u5229\u7528\u5377\u79ef\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u5206\u5272Sentinel-2\u536b\u661f\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684\u56fe\u50cf\u6bd4\u8f83\u548c\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u51cf\u5c11\u4e86\u5bf9\u9884\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u7684\u6807\u6ce8\u901a\u5e38\u9700\u8981\u4e13\u5bb6\u5206\u6790\uff0c\u8017\u65f6\u4e14\u6210\u672c\u9ad8\u3002\u73b0\u6709\u7684\u6807\u6ce8\u5de5\u5177\u4f9d\u8d56\u4e8e\u9884\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u6807\u8bb0\u65b0\u7684\u672a\u89c1\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u7ba1\u9053\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9\u9884\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u5272\uff0c\u5c06\u56fe\u50cf\u5206\u5272\u4e3a\u57fa\u4e8e\u989c\u8272\u548c\u7a7a\u95f4\u76f8\u4f3c\u6027\u7684\u540c\u8d28\u50cf\u7d20\u533a\u57df\uff0c\u56fe\u795e\u7ecf\u7f51\u7edc\u7528\u4e8e\u805a\u5408\u5468\u56f4\u7247\u6bb5\u7684\u4fe1\u606f\uff0c\u4f7f\u7279\u5f81\u8868\u793a\u80fd\u591f\u7f16\u7801\u5c40\u90e8\u90bb\u57df\u4fe1\u606f\u540c\u65f6\u4fdd\u7559\u81ea\u8eab\u5c40\u90e8\u4fe1\u606f\u3002", "result": "\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u6807\u6ce8\u5de5\u5177\u4e2d\u7684\u5f02\u5e38\u503c\uff0c\u5141\u8bb8\u7528\u6237\u5728\u66f4\u7ec6\u7c92\u5ea6\u4e0a\u8fdb\u884c\u6807\u6ce8\uff0c\u5e76\u5728\u7f16\u7801\u7a7a\u95f4\u5185\u5f62\u6210\u65cb\u8f6c\u4e0d\u53d8\u7684\u8bed\u4e49\u5173\u7cfb\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u7ba1\u9053\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728Sentinel-2\u536b\u661f\u56fe\u50cf\u4e2d\u5bfb\u627e\u548c\u6807\u8bb0\u5177\u6709\u76f8\u4f3c\u4e0a\u4e0b\u6587\u548c\u5185\u5bb9\u7684\u533a\u57df\uff0c\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5206\u5272\u6280\u672f\uff0c\u514b\u670d\u4e86\u5148\u524d\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u9c81\u68d2\u7684\u56fe\u50cf\u6bd4\u8f83\u7279\u5f81\u7a7a\u95f4\u3002"}}
{"id": "2508.00518", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00518", "abs": "https://arxiv.org/abs/2508.00518", "authors": ["Shuo Liang", "Yiwu Zhong", "Zi-Yuan Hu", "Yeyao Tao", "Liwei Wang"], "title": "Fine-grained Spatiotemporal Grounding on Egocentric Videos", "comment": "Accepted by ICCV 2025", "summary": "Spatiotemporal video grounding aims to localize target entities in videos\nbased on textual queries. While existing research has made significant progress\nin exocentric videos, the egocentric setting remains relatively underexplored,\ndespite its growing importance in applications such as augmented reality and\nrobotics. In this work, we conduct a systematic analysis of the discrepancies\nbetween egocentric and exocentric videos, revealing key challenges such as\nshorter object durations, sparser trajectories, smaller object sizes, and\nlarger positional shifts. To address these challenges, we introduce EgoMask,\nthe first pixel-level benchmark for fine-grained spatiotemporal grounding in\negocentric videos. It is constructed by our proposed automatic annotation\npipeline, which annotates referring expressions and object masks across short-,\nmedium-, and long-term videos. Additionally, we create EgoMask-Train, a\nlarge-scale training dataset to facilitate model development. Experiments\ndemonstrate that the state-of-the-art spatiotemporal grounding models perform\npoorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields\nsignificant improvements, while preserving performance on exocentric datasets.\nOur work thus provides essential resources and insights for advancing\negocentric video understanding. Our code is available at\nhttps://github.com/LaVi-Lab/EgoMask .", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u9996\u4e2a\u50cf\u7d20\u7ea7\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u65f6\u7a7a\u5b9a\u4f4d\u57fa\u51c6EgoMask\uff0c\u901a\u8fc7\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\u6784\u5efa\uff0c\u5e76\u521b\u5efa\u8bad\u7ec3\u6570\u636e\u96c6EgoMask-Train\uff0c\u5b9e\u9a8c\u8868\u660e\u5fae\u8c03\u53ef\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u5728\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u6280\u672f\u7b49\u5e94\u7528\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ed6\u4e2d\u5fc3\u89c6\u9891\uff0c\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7684\u65f6\u7a7a\u5b9a\u4f4d\u4ecd\u5f85\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u81ea\u6211\u4e2d\u5fc3\u4e0e\u4ed6\u4e2d\u5fc3\u89c6\u9891\u7684\u5dee\u5f02\uff0c\u63d0\u51fa\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\u6784\u5efaEgoMask\u57fa\u51c6\uff0c\u5e76\u521b\u5efa\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6EgoMask-Train\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u65f6\u7a7a\u5b9a\u4f4d\u6a21\u578b\u5728EgoMask\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5728\u4ed6\u4e2d\u5fc3\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u63d0\u5347\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u5173\u952e\u8d44\u6e90\u548c\u6d1e\u89c1\uff0c\u5305\u62ecEgoMask\u57fa\u51c6\u548cEgoMask-Train\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728EgoMask\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.00528", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00528", "abs": "https://arxiv.org/abs/2508.00528", "authors": ["Jinsong Yang", "Zeyuan Hu", "Yichen Li"], "title": "EPANet: Efficient Path Aggregation Network for Underwater Fish Detection", "comment": null, "summary": "Underwater fish detection (UFD) remains a challenging task in computer vision\ndue to low object resolution, significant background interference, and high\nvisual similarity between targets and surroundings. Existing approaches\nprimarily focus on local feature enhancement or incorporate complex attention\nmechanisms to highlight small objects, often at the cost of increased model\ncomplexity and reduced efficiency. To address these limitations, we propose an\nefficient path aggregation network (EPANet), which leverages complementary\nfeature integration to achieve accurate and lightweight UFD. EPANet consists of\ntwo key components: an efficient path aggregation feature pyramid network\n(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP\nbottleneck). The EPA-FPN introduces long-range skip connections across\ndisparate scales to improve semantic-spatial complementarity, while cross-layer\nfusion paths are adopted to enhance feature integration efficiency. The MS-DDSP\nbottleneck extends the conventional bottleneck structure by introducing\nfiner-grained feature division and diverse convolutional operations, thereby\nincreasing local feature diversity and representation capacity. Extensive\nexperiments on benchmark UFD datasets demonstrate that EPANet outperforms\nstate-of-the-art methods in terms of detection accuracy and inference speed,\nwhile maintaining comparable or even lower parameter complexity.", "AI": {"tldr": "EPANet\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6c34\u4e0b\u9c7c\u7c7b\u68c0\u6d4b\u7f51\u7edc\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8def\u5f84\u805a\u5408\u548c\u591a\u5c3a\u5ea6\u74f6\u9888\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u8f7b\u91cf\u5316\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u6c34\u4e0b\u9c7c\u7c7b\u68c0\u6d4b\uff08UFD\uff09\u7531\u4e8e\u76ee\u6807\u5206\u8fa8\u7387\u4f4e\u3001\u80cc\u666f\u5e72\u6270\u4e25\u91cd\u4ee5\u53ca\u76ee\u6807\u4e0e\u5468\u56f4\u73af\u5883\u89c6\u89c9\u76f8\u4f3c\u5ea6\u9ad8\uff0c\u4e00\u76f4\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u6311\u6218\u6027\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5c40\u90e8\u7279\u5f81\u589e\u5f3a\u6216\u5f15\u5165\u590d\u6742\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f46\u5f80\u5f80\u5bfc\u81f4\u6a21\u578b\u590d\u6742\u5ea6\u589e\u52a0\u548c\u6548\u7387\u964d\u4f4e\u3002", "method": "EPANet\u7531\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\u7ec4\u6210\uff1a\u9ad8\u6548\u7684\u8def\u5f84\u805a\u5408\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff08EPA-FPN\uff09\u548c\u591a\u5c3a\u5ea6\u591a\u6837\u5316\u5206\u5272\u77ed\u8def\u5f84\u74f6\u9888\uff08MS-DDSP\u74f6\u9888\uff09\u3002EPA-FPN\u901a\u8fc7\u8de8\u5c3a\u5ea6\u7684\u957f\u7a0b\u8df3\u8dc3\u8fde\u63a5\u63d0\u5347\u8bed\u4e49-\u7a7a\u95f4\u4e92\u8865\u6027\uff0c\u800cMS-DDSP\u74f6\u9888\u901a\u8fc7\u66f4\u7ec6\u7c92\u5ea6\u7684\u7279\u5f81\u5206\u5272\u548c\u591a\u6837\u5316\u5377\u79ef\u64cd\u4f5c\u589e\u52a0\u5c40\u90e8\u7279\u5f81\u7684\u591a\u6837\u6027\u548c\u8868\u793a\u80fd\u529b\u3002", "result": "EPANet\u5728\u57fa\u51c6UFD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u53c2\u6570\u590d\u6742\u5ea6\u76f8\u5f53\u6216\u66f4\u4f4e\u3002", "conclusion": "EPANet\u5728\u57fa\u51c6UFD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u53c2\u6570\u590d\u6742\u5ea6\u3002"}}
{"id": "2508.00548", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00548", "abs": "https://arxiv.org/abs/2508.00548", "authors": ["Seunghyun Shin", "Dongmin Shin", "Jisu Shin", "Hae-Gon Jeon", "Joon-Young Lee"], "title": "Video Color Grading via Look-Up Table Generation", "comment": "ICCV2025", "summary": "Different from color correction and transfer, color grading involves\nadjusting colors for artistic or storytelling purposes in a video, which is\nused to establish a specific look or mood. However, due to the complexity of\nthe process and the need for specialized editing skills, video color grading\nremains primarily the domain of professional colorists. In this paper, we\npresent a reference-based video color grading framework. Our key idea is\nexplicitly generating a look-up table (LUT) for color attribute alignment\nbetween reference scenes and input video via a diffusion model. As a training\nobjective, we enforce that high-level features of the reference scenes like\nlook, mood, and emotion should be similar to that of the input video. Our\nLUT-based approach allows for color grading without any loss of structural\ndetails in the whole video frames as well as achieving fast inference. We\nfurther build a pipeline to incorporate a user-preference via text prompts for\nlow-level feature enhancement such as contrast and brightness, etc.\nExperimental results, including extensive user studies, demonstrate the\neffectiveness of our approach for video color grading. Codes are publicly\navailable at https://github.com/seunghyuns98/VideoColorGrading.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u548c\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u8272\u5f69\u5206\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7LUT\u5b9e\u73b0\u8272\u5f69\u5bf9\u9f50\uff0c\u5e76\u7ed3\u5408\u6587\u672c\u63d0\u793a\u589e\u5f3a\u7528\u6237\u504f\u597d\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u4e14\u6613\u7528\u3002", "motivation": "\u89c6\u9891\u8272\u5f69\u5206\u7ea7\u901a\u5e38\u9700\u8981\u4e13\u4e1a\u6280\u80fd\uff0c\u672c\u6587\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u590d\u6742\u8fc7\u7a0b\uff0c\u4f7f\u5176\u66f4\u6613\u7528\u4e14\u9ad8\u6548\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u67e5\u627e\u8868\uff08LUT\uff09\u4ee5\u5b9e\u73b0\u53c2\u8003\u573a\u666f\u4e0e\u8f93\u5165\u89c6\u9891\u7684\u8272\u5f69\u5c5e\u6027\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u76ee\u6807\u786e\u4fdd\u9ad8\u5c42\u6b21\u7279\u5f81\uff08\u5982\u5916\u89c2\u3001\u60c5\u7eea\uff09\u76f8\u4f3c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4fdd\u6301\u89c6\u9891\u5e27\u7ed3\u6784\u7ec6\u8282\u4e0d\u4e22\u5931\uff0c\u5e76\u5b9e\u73b0\u5feb\u901f\u63a8\u7406\uff0c\u7528\u6237\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u53c2\u8003\u548c\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u8272\u5f69\u5206\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u67e5\u627e\u8868\uff08LUT\uff09\u5b9e\u73b0\u8272\u5f69\u5c5e\u6027\u5bf9\u9f50\uff0c\u5e76\u7ed3\u5408\u7528\u6237\u504f\u597d\u6587\u672c\u63d0\u793a\u8fdb\u884c\u4f4e\u5c42\u6b21\u7279\u5f81\u589e\u5f3a\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.00549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00549", "abs": "https://arxiv.org/abs/2508.00549", "authors": ["Daniel Wolf", "Heiko Hillenhagen", "Billurvan Taskin", "Alex B\u00e4uerle", "Meinrad Beer", "Michael G\u00f6tz", "Timo Ropinski"], "title": "Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images", "comment": "Accepted at the International Conference on Medical Image Computing\n  and Computer Assisted Intervention (MICCAI) 2025", "summary": "Clinical decision-making relies heavily on understanding relative positions\nof anatomical structures and anomalies. Therefore, for Vision-Language Models\n(VLMs) to be applicable in clinical practice, the ability to accurately\ndetermine relative positions on medical images is a fundamental prerequisite.\nDespite its importance, this capability remains highly underexplored. To\naddress this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o,\nLlama3.2, Pixtral, and JanusPro, and find that all models fail at this\nfundamental task. Inspired by successful approaches in computer vision, we\ninvestigate whether visual prompts, such as alphanumeric or colored markers\nplaced on anatomical structures, can enhance performance. While these markers\nprovide moderate improvements, results remain significantly lower on medical\nimages compared to observations made on natural images. Our evaluations suggest\nthat, in medical imaging, VLMs rely more on prior anatomical knowledge than on\nactual image content for answering relative position questions, often leading\nto incorrect conclusions. To facilitate further research in this area, we\nintroduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset,\ndesigned to systematically evaluate the capability to identify relative\npositions in medical images.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524dVLMs\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u5224\u65ad\u76f8\u5bf9\u4f4d\u7f6e\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u4f9d\u8d56\u5148\u9a8c\u77e5\u8bc6\u800c\u975e\u56fe\u50cf\u5185\u5bb9\uff0c\u63d0\u51faMIRP\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u7814\u7a76\u3002", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u4f9d\u8d56\u4e8e\u5bf9\u89e3\u5256\u7ed3\u6784\u76f8\u5bf9\u4f4d\u7f6e\u7684\u7406\u89e3\uff0c\u4f46VLMs\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8bc4\u4f30\u4e86GPT-4o\u3001Llama3.2\u3001Pixtral\u548cJanusPro\u7b49\u5148\u8fdbVLMs\u7684\u80fd\u529b\uff0c\u5e76\u7814\u7a76\u4e86\u89c6\u89c9\u63d0\u793a\uff08\u5982\u5b57\u6bcd\u6570\u5b57\u6216\u5f69\u8272\u6807\u8bb0\uff09\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u6240\u6709\u6a21\u578b\u5747\u65e0\u6cd5\u5b8c\u6210\u57fa\u7840\u4efb\u52a1\uff0c\u89c6\u89c9\u63d0\u793a\u4ec5\u5e26\u6765\u6709\u9650\u6539\u8fdb\uff0c\u533b\u5b66\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u81ea\u7136\u56fe\u50cf\u3002", "conclusion": "VLMs\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u76f8\u5bf9\u4f4d\u7f6e\u5224\u65ad\u80fd\u529b\u4e0d\u8db3\uff0c\u4e3b\u8981\u4f9d\u8d56\u5148\u9a8c\u89e3\u5256\u5b66\u77e5\u8bc6\u800c\u975e\u56fe\u50cf\u5185\u5bb9\uff0c\u5bfc\u81f4\u9519\u8bef\u7ed3\u8bba\u3002\u4e3a\u4e86\u4fc3\u8fdb\u8be5\u9886\u57df\u7814\u7a76\uff0c\u4f5c\u8005\u5f15\u5165\u4e86MIRP\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2508.00552", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00552", "abs": "https://arxiv.org/abs/2508.00552", "authors": ["Chihan Huang", "Belal Alsinglawi", "Islam Al-qudah"], "title": "DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification", "comment": null, "summary": "Recent advances in deep neural networks (DNNs) have led to remarkable success\nacross a wide range of tasks. However, their susceptibility to adversarial\nperturbations remains a critical vulnerability. Existing diffusion-based\nadversarial purification methods often require intensive iterative denoising,\nseverely limiting their practical deployment. In this paper, we propose\nDiffusion Bridge Distillation for Purification (DBLP), a novel and efficient\ndiffusion-based framework for adversarial purification. Central to our approach\nis a new objective, noise bridge distillation, which constructs a principled\nalignment between the adversarial noise distribution and the clean data\ndistribution within a latent consistency model (LCM). To further enhance\nsemantic fidelity, we introduce adaptive semantic enhancement, which fuses\nmulti-scale pyramid edge maps as conditioning input to guide the purification\nprocess. Extensive experiments across multiple datasets demonstrate that DBLP\nachieves state-of-the-art (SOTA) robust accuracy, superior image quality, and\naround 0.2s inference time, marking a significant step toward real-time\nadversarial purification.", "AI": {"tldr": "DBLP \u662f\u4e00\u79cd\u9ad8\u6548\u6269\u6563\u6865\u84b8\u998f\u5bf9\u6297\u51c0\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u566a\u58f0\u6865\u5bf9\u9f50\u548c\u8bed\u4e49\u589e\u5f3a\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u51c0\u5316\u5e76\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5176\u5bf9\u5bf9\u6297\u6270\u52a8\u7684\u654f\u611f\u6027\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6f0f\u6d1e\u3002\u73b0\u6709\u6269\u6563\u51c0\u5316\u65b9\u6cd5\u56e0\u9700\u5bc6\u96c6\u8fed\u4ee3\u53bb\u566a\u800c\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\u3002", "method": "DBLP \u901a\u8fc7\u566a\u58f0\u6865\u84b8\u998f\u76ee\u6807\u6784\u5efa\u5bf9\u6297\u566a\u58f0\u5206\u5e03\u4e0e\u5e72\u51c0\u6570\u636e\u5206\u5e03\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u8bed\u4e49\u589e\u5f3a\u6280\u672f\uff08\u591a\u5c3a\u5ea6\u91d1\u5b57\u5854\u8fb9\u7f18\u56fe\u878d\u5408\uff09\u6307\u5bfc\u51c0\u5316\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDBLP \u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u7cbe\u5ea6\u3001\u4f18\u5f02\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u4e14\u63a8\u7406\u65f6\u95f4\u4ec5\u7ea60.2\u79d2\u3002", "conclusion": "DBLP \u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65b0\u9896\u7684\u6269\u6563\u6865\u84b8\u998f\u51c0\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u51c0\u5316\u7684\u5b9e\u65f6\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.00553", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00553", "abs": "https://arxiv.org/abs/2508.00553", "authors": ["Jizhihui Liu", "Feiyi Du", "Guangdao Zhu", "Niu Lian", "Jun Li", "Bin Chen"], "title": "HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) encode images into lengthy sequences of visual\ntokens, leading to excessive computational overhead and limited inference\nefficiency. While prior efforts prune or merge tokens to address this issue,\nthey often rely on special tokens (e.g., CLS) or require task-specific\ntraining, hindering scalability across architectures. In this paper, we propose\nHiPrune, a training-free and model-agnostic token Pruning framework that\nexploits the Hierarchical attention structure within vision encoders. We\nidentify that middle layers attend to object-centric regions, while deep layers\ncapture global contextual features. Based on this observation, HiPrune selects\nthree types of informative tokens: (1) Anchor tokens with high attention in\nobject-centric layers, (2) Buffer tokens adjacent to anchors for spatial\ncontinuity, and (3) Register tokens with strong attention in deep layers for\nglobal summarization. Our method requires no retraining and integrates\nseamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,\nLLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art\npruning performance, preserving up to 99.3% task accuracy with only 33.3%\ntokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it\nreduces inference FLOPs and latency by up to 9$\\times$, showcasing strong\ngeneralization across models and tasks. Code is available at\nhttps://github.com/Danielement321/HiPrune.", "AI": {"tldr": "HiPrune \u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u9009\u62e9\u5173\u952e\u4ee4\u724c\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u5e76\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u56e0\u7f16\u7801\u56fe\u50cf\u4e3a\u957f\u5e8f\u5217\u89c6\u89c9\u4ee4\u724c\u800c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u63a8\u7406\u6548\u7387\u4f4e\uff0cHiPrune \u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "HiPrune \u901a\u8fc7\u5206\u6790\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u4e0d\u540c\u5c42\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u9009\u62e9\u4e09\u79cd\u4fe1\u606f\u4e30\u5bcc\u7684\u4ee4\u724c\uff1a\u951a\u4ee4\u724c\u3001\u7f13\u51b2\u4ee4\u724c\u548c\u6ce8\u518c\u4ee4\u724c\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u4ee4\u724c\u526a\u679d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHiPrune \u5728 LLaVA-1.5\u3001LLaVA-NeXT \u548c Qwen2.5-VL \u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u526a\u679d\u6027\u80fd\uff0c\u4ec5\u7528 33.3% \u7684\u4ee4\u724c\u5373\u53ef\u4fdd\u7559 99.3% \u7684\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u63a8\u7406 FLOPs \u548c\u5ef6\u8fdf\u9ad8\u8fbe 9 \u500d\u3002", "conclusion": "HiPrune \u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u4e14\u6a21\u578b\u65e0\u5173\u7684\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u7684\u5206\u5c42\u6ce8\u610f\u529b\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u4efb\u52a1\u51c6\u786e\u6027\u3002"}}
{"id": "2508.00557", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00557", "abs": "https://arxiv.org/abs/2508.00557", "authors": ["Qi Chen", "Lingxiao Yang", "Yun Chen", "Nailong Zhao", "Jianhuang Lai", "Jie Shao", "Xiaohua Xie"], "title": "Training-Free Class Purification for Open-Vocabulary Semantic Segmentation", "comment": "Accepted to ICCV 2025", "summary": "Fine-tuning pre-trained vision-language models has emerged as a powerful\napproach for enhancing open-vocabulary semantic segmentation (OVSS). However,\nthe substantial computational and resource demands associated with training on\nlarge datasets have prompted interest in training-free methods for OVSS.\nExisting training-free approaches primarily focus on modifying model\narchitectures and generating prototypes to improve segmentation performance.\nHowever, they often neglect the challenges posed by class redundancy, where\nmultiple categories are not present in the current test image, and\nvisual-language ambiguity, where semantic similarities among categories create\nconfusion in class activation. These issues can lead to suboptimal class\nactivation maps and affinity-refined activation maps. Motivated by these\nobservations, we propose FreeCP, a novel training-free class purification\nframework designed to address these challenges. FreeCP focuses on purifying\nsemantic categories and rectifying errors caused by redundancy and ambiguity.\nThe purified class representations are then leveraged to produce final\nsegmentation predictions. We conduct extensive experiments across eight\nbenchmarks to validate FreeCP's effectiveness. Results demonstrate that FreeCP,\nas a plug-and-play module, significantly boosts segmentation performance when\ncombined with other OVSS methods.", "AI": {"tldr": "FreeCP\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7c7b\u522b\u7eaf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u51c0\u5316\u8bed\u4e49\u7c7b\u522b\u548c\u7ea0\u6b63\u5197\u4f59\u4e0e\u6a21\u7cca\u6027\u9519\u8bef\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u9700\u8bad\u7ec3\u65b9\u6cd5\u5ffd\u89c6\u4e86\u7c7b\u522b\u5197\u4f59\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u7cca\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u6b21\u4f18\u7684\u7c7b\u522b\u6fc0\u6d3b\u56fe\u548c\u4eb2\u548c\u529b\u7cbe\u70bc\u6fc0\u6d3b\u56fe\u3002", "method": "FreeCP\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7c7b\u522b\u7eaf\u5316\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u51c0\u5316\u8bed\u4e49\u7c7b\u522b\u5e76\u7ea0\u6b63\u7531\u5197\u4f59\u548c\u6a21\u7cca\u6027\u5f15\u8d77\u7684\u9519\u8bef\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86FreeCP\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793a\u5176\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "conclusion": "FreeCP\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u4e0e\u5176\u4ed6OVSS\u65b9\u6cd5\u7ed3\u5408\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2508.00558", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00558", "abs": "https://arxiv.org/abs/2508.00558", "authors": ["Jens U. Kreber", "Joerg Stueckler"], "title": "Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints", "comment": "Accepted for publication at the IEEE/CVF International Conference on\n  Computer Vision (ICCV), 2025", "summary": "Articulated objects are an important type of interactable objects in everyday\nenvironments. In this paper, we propose PhysNAP, a novel diffusion model-based\napproach for generating articulated objects that aligns them with partial point\nclouds and improves their physical plausibility. The model represents part\nshapes by signed distance functions (SDFs). We guide the reverse diffusion\nprocess using a point cloud alignment loss computed using the predicted SDFs.\nAdditionally, we impose non-penetration and mobility constraints based on the\npart SDFs for guiding the model to generate more physically plausible objects.\nWe also make our diffusion approach category-aware to further improve point\ncloud alignment if category information is available. We evaluate the\ngenerative ability and constraint consistency of samples generated with PhysNAP\nusing the PartNet-Mobility dataset. We also compare it with an unguided\nbaseline diffusion model and demonstrate that PhysNAP can improve constraint\nconsistency and provides a tradeoff with generative ability.", "AI": {"tldr": "PhysNAP \u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7 SDFs \u548c\u7269\u7406\u7ea6\u675f\u751f\u6210\u7269\u7406\u4e0a\u5408\u7406\u7684\u94f0\u63a5\u7269\u4f53\uff0c\u5e76\u5728\u70b9\u4e91\u5bf9\u9f50\u548c\u7ea6\u675f\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u94f0\u63a5\u7269\u4f53\u662f\u65e5\u5e38\u73af\u5883\u4e2d\u91cd\u8981\u7684\u53ef\u4ea4\u4e92\u5bf9\u8c61\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u751f\u6210\u7269\u7406\u4e0a\u5408\u7406\u7684\u94f0\u63a5\u7269\u4f53\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u6539\u5584\u70b9\u4e91\u5bf9\u9f50\u548c\u7269\u7406\u5408\u7406\u6027\u3002", "method": "PhysNAP \u4f7f\u7528\u5e26\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\uff08SDFs\uff09\u8868\u793a\u90e8\u4ef6\u5f62\u72b6\uff0c\u5e76\u901a\u8fc7\u70b9\u4e91\u5bf9\u9f50\u635f\u5931\u6307\u5bfc\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8e\u90e8\u4ef6 SDFs \u7684\u975e\u7a7f\u900f\u6027\u548c\u79fb\u52a8\u6027\u7ea6\u675f\uff0c\u4ee5\u751f\u6210\u7269\u7406\u4e0a\u66f4\u5408\u7406\u7684\u7269\u4f53\u3002\u6a21\u578b\u8fd8\u652f\u6301\u7c7b\u522b\u611f\u77e5\uff0c\u4ee5\u5728\u6709\u7c7b\u522b\u4fe1\u606f\u65f6\u8fdb\u4e00\u6b65\u4f18\u5316\u70b9\u4e91\u5bf9\u9f50\u3002", "result": "\u5728 PartNet-Mobility \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPhysNAP \u5728\u751f\u6210\u80fd\u529b\u548c\u7ea6\u675f\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u672a\u5f15\u5bfc\u7684\u57fa\u7ebf\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u597d\u7684\u5e73\u8861\u3002", "conclusion": "PhysNAP \u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u4e0e\u90e8\u5206\u70b9\u4e91\u5bf9\u9f50\u4e14\u7269\u7406\u4e0a\u66f4\u5408\u7406\u7684\u94f0\u63a5\u7269\u4f53\u3002\u901a\u8fc7\u7ed3\u5408\u70b9\u4e91\u5bf9\u9f50\u635f\u5931\u548c\u975e\u7a7f\u900f\u6027\u53ca\u79fb\u52a8\u6027\u7ea6\u675f\uff0cPhysNAP \u5728\u751f\u6210\u80fd\u529b\u548c\u7ea6\u675f\u4e00\u81f4\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002"}}
{"id": "2508.00563", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00563", "abs": "https://arxiv.org/abs/2508.00563", "authors": ["Hannah Kniesel", "Leon Sick", "Tristan Payer", "Tim Bergner", "Kavitha Shaga Devan", "Clarissa Read", "Paul Walther", "Timo Ropinski"], "title": "Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images", "comment": null, "summary": "Current state-of-the-art methods for object detection rely on annotated\nbounding boxes of large data sets for training. However, obtaining such\nannotations is expensive and can require up to hundreds of hours of manual\nlabor. This poses a challenge, especially since such annotations can only be\nprovided by experts, as they require knowledge about the scientific domain. To\ntackle this challenge, we propose a domain-specific weakly supervised object\ndetection algorithm that only relies on image-level annotations, which are\nsignificantly easier to acquire. Our method distills the knowledge of a\npre-trained model, on the task of predicting the presence or absence of a virus\nin an image, to obtain a set of pseudo-labels that can be used to later train a\nstate-of-the-art object detection model. To do so, we use an optimization\napproach with a shrinking receptive field to extract virus particles directly\nwithout specific network architectures. Through a set of extensive studies, we\nshow how the proposed pseudo-labels are easier to obtain, and, more\nimportantly, are able to outperform other existing weak labeling methods, and\neven ground truth labels, in cases where the time to obtain the annotation is\nlimited.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u4ec5\u9700\u56fe\u50cf\u7ea7\u6807\u6ce8\u7684\u5f31\u76d1\u7763\u5bf9\u8c61\u68c0\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u4f18\u5316\u65b9\u6cd5\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u6709\u9650\u6807\u6ce8\u65f6\u95f4\u4e0b\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u83b7\u53d6\u5927\u91cf\u6570\u636e\u96c6\u7684\u6807\u6ce8\u8fb9\u754c\u6846\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\uff0c\u5c24\u5176\u662f\u9700\u8981\u9886\u57df\u4e13\u5bb6\u53c2\u4e0e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4ec5\u4f9d\u8d56\u56fe\u50cf\u7ea7\u6807\u6ce8\u7684\u5f31\u76d1\u7763\u5bf9\u8c61\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u901a\u8fc7\u4f18\u5316\u65b9\u6cd5\u548c\u7f29\u5c0f\u7684\u611f\u53d7\u91ce\u76f4\u63a5\u63d0\u53d6\u75c5\u6bd2\u9897\u7c92\uff0c\u65e0\u9700\u7279\u5b9a\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u63d0\u51fa\u7684\u4f2a\u6807\u7b7e\u65b9\u6cd5\u66f4\u5bb9\u6613\u83b7\u53d6\uff0c\u5e76\u4e14\u5728\u6807\u6ce8\u65f6\u95f4\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u5f31\u6807\u6ce8\u65b9\u6cd5\u751a\u81f3\u771f\u5b9e\u6807\u6ce8\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u5f31\u76d1\u7763\u5bf9\u8c61\u68c0\u6d4b\u7b97\u6cd5\u5728\u6807\u6ce8\u65f6\u95f4\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e0d\u4ec5\u6bd4\u5176\u4ed6\u5f31\u6807\u6ce8\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u771f\u5b9e\u6807\u6ce8\u6807\u7b7e\u3002"}}
{"id": "2508.00568", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00568", "abs": "https://arxiv.org/abs/2508.00568", "authors": ["Jingchao Xie", "Oussema Dhaouadi", "Weirong Chen", "Johannes Meier", "Jacques Kaiser", "Daniel Cremers"], "title": "CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry", "comment": "Accepted for GCPR 2025. Project page:\n  https://jchao-xie.github.io/CoProU/", "summary": "Visual Odometry (VO) is fundamental to autonomous navigation, robotics, and\naugmented reality, with unsupervised approaches eliminating the need for\nexpensive ground-truth labels. However, these methods struggle when dynamic\nobjects violate the static scene assumption, leading to erroneous pose\nestimations. We tackle this problem by uncertainty modeling, which is a\ncommonly used technique that creates robust masks to filter out dynamic objects\nand occlusions without requiring explicit motion segmentation. Traditional\nuncertainty modeling considers only single-frame information, overlooking the\nuncertainties across consecutive frames. Our key insight is that uncertainty\nmust be propagated and combined across temporal frames to effectively identify\nunreliable regions, particularly in dynamic scenes. To address this challenge,\nwe introduce Combined Projected Uncertainty VO (CoProU-VO), a novel end-to-end\napproach that combines target frame uncertainty with projected reference frame\nuncertainty using a principled probabilistic formulation. Built upon vision\ntransformer backbones, our model simultaneously learns depth, uncertainty\nestimation, and camera poses. Consequently, experiments on the KITTI and\nnuScenes datasets demonstrate significant improvements over previous\nunsupervised monocular end-to-end two-frame-based methods and exhibit strong\nperformance in challenging highway scenes where other approaches often fail.\nAdditionally, comprehensive ablation studies validate the effectiveness of\ncross-frame uncertainty propagation.", "AI": {"tldr": "CoProU-VO\u901a\u8fc7\u8de8\u5e27\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u63d0\u5347\u52a8\u6001\u573a\u666f\u4e0b\u7684\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u65e0\u76d1\u7763\u89c6\u89c9\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u5728\u52a8\u6001\u5bf9\u8c61\u8fdd\u53cd\u9759\u6001\u573a\u666f\u5047\u8bbe\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4f20\u7edf\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4ec5\u8003\u8651\u5355\u5e27\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u8de8\u5e27\u4e0d\u786e\u5b9a\u6027\u7684\u4f20\u64ad\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoProU-VO\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u76ee\u6807\u5e27\u548c\u53c2\u8003\u5e27\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u57fa\u4e8e\u89c6\u89c9\u53d8\u6362\u5668\u67b6\u6784\u540c\u65f6\u5b66\u4e60\u6df1\u5ea6\u3001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u76f8\u673a\u4f4d\u59ff\u3002", "result": "\u5728KITTI\u548cnuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cCoProU-VO\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u65e0\u76d1\u7763\u5355\u76ee\u7aef\u5230\u7aef\u53cc\u5e27\u65b9\u6cd5\uff0c\u5e76\u5728\u9ad8\u901f\u516c\u8def\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "CoProU-VO\u901a\u8fc7\u8de8\u5e27\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u4e0b\u7684\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9ad8\u901f\u516c\u8def\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.00587", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00587", "abs": "https://arxiv.org/abs/2508.00587", "authors": ["Marc H\u00f6lle", "Walter Kellermann", "Vasileios Belagiannis"], "title": "Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection", "comment": "Accepted at ICCVW 2025, 11 pages, 4 figures", "summary": "Semantic segmentation models trained on known object classes often fail in\nreal-world autonomous driving scenarios by confidently misclassifying unknown\nobjects. While pixel-wise out-of-distribution detection can identify unknown\nobjects, existing methods struggle in complex scenes where rare object classes\nare often confused with truly unknown objects. We introduce an\nuncertainty-aware likelihood ratio estimation method that addresses these\nlimitations. Our approach uses an evidential classifier within a likelihood\nratio test to distinguish between known and unknown pixel features from a\nsemantic segmentation model, while explicitly accounting for uncertainty.\nInstead of producing point estimates, our method outputs probability\ndistributions that capture uncertainty from both rare training examples and\nimperfect synthetic outliers. We show that by incorporating uncertainty in this\nway, outlier exposure can be leveraged more effectively. Evaluated on five\nstandard benchmark datasets, our method achieves the lowest average false\npositive rate (2.5%) among state-of-the-art while maintaining high average\nprecision (90.91%) and incurring only negligible computational overhead. Code\nis available at https://github.com/glasbruch/ULRE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u4f3c\u7136\u6bd4\u4f30\u8ba1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u672a\u77e5\u7269\u4f53\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u8ba1\u7b97\u5f00\u9500\u4f4e\u4e14\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u5b9e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\uff0c\u8bed\u4e49\u5206\u5272\u6a21\u578b\u5e38\u56e0\u672a\u77e5\u7269\u4f53\u800c\u8bef\u5206\u7c7b\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u96be\u4ee5\u533a\u5206\u7a00\u6709\u7c7b\u522b\u4e0e\u771f\u6b63\u672a\u77e5\u7269\u4f53\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u4f3c\u7136\u6bd4\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5229\u7528\u8bc1\u636e\u5206\u7c7b\u5668\u5728\u4f3c\u7136\u6bd4\u6d4b\u8bd5\u4e2d\u533a\u5206\u5df2\u77e5\u548c\u672a\u77e5\u50cf\u7d20\u7279\u5f81\uff0c\u5e76\u660e\u786e\u8003\u8651\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u901a\u8fc7\u663e\u5f0f\u8003\u8651\u4e0d\u786e\u5b9a\u6027\uff0c\u8be5\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u5229\u7528\u4e86\u79bb\u7fa4\u503c\u66b4\u9732\uff0c\u663e\u8457\u63d0\u5347\u4e86\u672a\u77e5\u7269\u4f53\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e94\u4e2a\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684\u5e73\u5747\u8bef\u62a5\u7387\uff082.5%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u5e73\u5747\u7cbe\u5ea6\uff0890.91%\uff09\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002"}}
{"id": "2508.00590", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00590", "abs": "https://arxiv.org/abs/2508.00590", "authors": ["Yihe Tian", "Kwan Man Cheng", "Zhengbo Zhang", "Tao Zhang", "Suju Li", "Dongmei Yan", "Bing Xu"], "title": "A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)", "comment": null, "summary": "Artificial Night-Time Light (NTL) remote sensing is a vital proxy for\nquantifying the intensity and spatial distribution of human activities.\nAlthough the NPP-VIIRS sensor provides high-quality NTL observations, its\ntemporal coverage, which begins in 2012, restricts long-term time-series\nstudies that extend to earlier periods. Despite the progress in extending\nVIIRS-like NTL time-series, current methods still suffer from two significant\nshortcomings: the underestimation of light intensity and the structural\nomission. To overcome these limitations, we propose a novel reconstruction\nframework consisting of a two-stage process: construction and refinement. The\nconstruction stage features a Hierarchical Fusion Decoder (HFD) designed to\nenhance the fidelity of the initial reconstruction. The refinement stage\nemploys a Dual Feature Refiner (DFR), which leverages high-resolution\nimpervious surface masks to guide and enhance fine-grained structural details.\nBased on this framework, we developed the Extended VIIRS-like Artificial\nNighttime Light (EVAL) product for China, extending the standard data record\nbackwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL\nsignificantly outperforms existing state-of-the-art products, boosting the\n$\\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.\nFurthermore, EVAL exhibits excellent temporal consistency and maintains a high\ncorrelation with socioeconomic parameters, confirming its reliability for\nlong-term analysis. The resulting EVAL dataset provides a valuable new resource\nfor the research community and is publicly available at\nhttps://doi.org/10.11888/HumanNat.tpdc.302930.", "AI": {"tldr": "\u63d0\u51faEVAL\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u91cd\u5efa\u63d0\u5347\u591c\u95f4\u706f\u5149\u6570\u636e\u8d28\u91cf\uff0c\u8986\u76d61986\u5e74\u81f3\u4eca\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709VIIRS-like\u591c\u95f4\u706f\u5149\u65f6\u95f4\u5e8f\u5217\u65b9\u6cd5\u5728\u5149\u5f3a\u5ea6\u4f4e\u4f30\u548c\u7ed3\u6784\u9057\u6f0f\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7531\u6784\u5efa\u548c\u7ec6\u5316\u4e24\u9636\u6bb5\u7ec4\u6210\u7684\u91cd\u5efa\u6846\u67b6\uff0c\u5305\u62ec\u5c42\u6b21\u878d\u5408\u89e3\u7801\u5668\uff08HFD\uff09\u548c\u53cc\u7279\u5f81\u7ec6\u5316\u5668\uff08DFR\uff09\u3002", "result": "\u5f00\u53d1\u7684EVAL\u4ea7\u54c1\u5c06\u6807\u51c6\u6570\u636e\u8bb0\u5f55\u56de\u6eaf\u81f31986\u5e74\uff0c\u663e\u8457\u63d0\u5347\u4e86R\u00b2\uff080.68\u52300.80\uff09\u5e76\u964d\u4f4e\u4e86RMSE\uff081.27\u52300.99\uff09\uff0c\u5177\u6709\u4f18\u5f02\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u793e\u4f1a\u7ecf\u6d4e\u53c2\u6570\u76f8\u5173\u6027\u3002", "conclusion": "EVAL\u6570\u636e\u96c6\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9d\u8d35\u7684\u65b0\u8d44\u6e90\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591c\u95f4\u706f\u5149\u6570\u636e\u7684\u51c6\u786e\u6027\u548c\u65f6\u95f4\u8986\u76d6\u8303\u56f4\uff0c\u652f\u6301\u957f\u671f\u793e\u4f1a\u7ecf\u6d4e\u5206\u6790\u3002"}}
{"id": "2508.00592", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00592", "abs": "https://arxiv.org/abs/2508.00592", "authors": ["Jiajun Le", "Jiayi Ma"], "title": "GeoMoE: Divide-and-Conquer Motion Field Modeling with Mixture-of-Experts for Two-View Geometry", "comment": null, "summary": "Recent progress in two-view geometry increasingly emphasizes enforcing\nsmoothness and global consistency priors when estimating motion fields between\npairs of images. However, in complex real-world scenes, characterized by\nextreme viewpoint and scale changes as well as pronounced depth\ndiscontinuities, the motion field often exhibits diverse and heterogeneous\nmotion patterns. Most existing methods lack targeted modeling strategies and\nfail to explicitly account for this variability, resulting in estimated motion\nfields that diverge from their true underlying structure and distribution. We\nobserve that Mixture-of-Experts (MoE) can assign dedicated experts to motion\nsub-fields, enabling a divide-and-conquer strategy for heterogeneous motion\npatterns. Building on this insight, we re-architect motion field modeling in\ntwo-view geometry with GeoMoE, a streamlined framework. Specifically, we first\ndevise a Probabilistic Prior-Guided Decomposition strategy that exploits inlier\nprobability signals to perform a structure-aware decomposition of the motion\nfield into heterogeneous sub-fields, sharply curbing outlier-induced bias.\nNext, we introduce an MoE-Enhanced Bi-Path Rectifier that enhances each\nsub-field along spatial-context and channel-semantic paths and routes it to a\ncustomized expert for targeted modeling, thereby decoupling heterogeneous\nmotion regimes, suppressing cross-sub-field interference and representational\nentanglement, and yielding fine-grained motion-field rectification. With this\nminimalist design, GeoMoE outperforms prior state-of-the-art methods in\nrelative pose and homography estimation and shows strong generalization. The\nsource code and pre-trained models are available at\nhttps://github.com/JiajunLe/GeoMoE.", "AI": {"tldr": "GeoMoE\u5229\u7528Mixture-of-Experts\u6280\u672f\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u5206\u89e3\u548c\u5b9a\u5236\u4e13\u5bb6\u5efa\u6a21\uff0c\u6709\u6548\u5904\u7406\u5f02\u8d28\u6027\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5728\u8fd0\u52a8\u573a\u4f30\u8ba1\u4e2d\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u8fd0\u52a8\u573a\u5e38\u8868\u73b0\u51fa\u591a\u6837\u5316\u548c\u5f02\u8d28\u6027\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9488\u5bf9\u6027\u5efa\u6a21\u7b56\u7565\uff0c\u5bfc\u81f4\u4f30\u8ba1\u7684\u8fd0\u52a8\u573a\u504f\u79bb\u771f\u5b9e\u7ed3\u6784\u548c\u5206\u5e03\u3002", "method": "\u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6982\u7387\u5148\u9a8c\u5f15\u5bfc\u5206\u89e3\u7b56\u7565\uff0c\u5229\u7528\u5185\u70b9\u6982\u7387\u4fe1\u53f7\u5bf9\u8fd0\u52a8\u573a\u8fdb\u884c\u7ed3\u6784\u611f\u77e5\u5206\u89e3\uff1b\u5176\u6b21\u5f15\u5165\u4e86MoE\u589e\u5f3a\u7684\u53cc\u8def\u5f84\u6821\u6b63\u5668\uff0c\u901a\u8fc7\u7a7a\u95f4-\u4e0a\u4e0b\u6587\u548c\u901a\u9053-\u8bed\u4e49\u8def\u5f84\u589e\u5f3a\u6bcf\u4e2a\u5b50\u573a\uff0c\u5e76\u5c06\u5176\u8def\u7531\u5230\u5b9a\u5236\u4e13\u5bb6\u8fdb\u884c\u5b9a\u5411\u5efa\u6a21\u3002", "result": "GeoMoE\u5728\u76f8\u5bf9\u59ff\u6001\u548c\u5355\u5e94\u6027\u4f30\u8ba1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GeoMoE\u901a\u8fc7\u5176\u7cbe\u7b80\u7684\u8bbe\u8ba1\uff0c\u5728\u76f8\u5bf9\u59ff\u6001\u548c\u5355\u5e94\u6027\u4f30\u8ba1\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.00599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00599", "abs": "https://arxiv.org/abs/2508.00599", "authors": ["Junzhe Lu", "Jing Lin", "Hongkun Dou", "Ailing Zeng", "Yue Deng", "Xian Liu", "Zhongang Cai", "Lei Yang", "Yulun Zhang", "Haoqian Wang", "Ziwei Liu"], "title": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior", "comment": "ICCV 2025 (oral); Code released: https://github.com/moonbow721/DPoser", "summary": "We present DPoser-X, a diffusion-based prior model for 3D whole-body human\nposes. Building a versatile and robust full-body human pose prior remains\nchallenging due to the inherent complexity of articulated human poses and the\nscarcity of high-quality whole-body pose datasets. To address these\nlimitations, we introduce a Diffusion model as body Pose prior (DPoser) and\nextend it to DPoser-X for expressive whole-body human pose modeling. Our\napproach unifies various pose-centric tasks as inverse problems, solving them\nthrough variational diffusion sampling. To enhance performance on downstream\napplications, we introduce a novel truncated timestep scheduling method\nspecifically designed for pose data characteristics. We also propose a masked\ntraining mechanism that effectively combines whole-body and part-specific\ndatasets, enabling our model to capture interdependencies between body parts\nwhile avoiding overfitting to specific actions. Extensive experiments\ndemonstrate DPoser-X's robustness and versatility across multiple benchmarks\nfor body, hand, face, and full-body pose modeling. Our model consistently\noutperforms state-of-the-art alternatives, establishing a new benchmark for\nwhole-body human pose prior modeling.", "AI": {"tldr": "DPoser-X\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5168\u8eab\u4eba\u4f53\u59ff\u52bf\u5148\u9a8c\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u590d\u6742\u6027\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u7531\u4e8e\u4eba\u4f53\u59ff\u52bf\u7684\u590d\u6742\u6027\u548c\u9ad8\u8d28\u91cf\u5168\u8eab\u59ff\u52bf\u6570\u636e\u96c6\u7684\u7a00\u7f3a\uff0c\u6784\u5efa\u4e00\u4e2a\u901a\u7528\u4e14\u9c81\u68d2\u7684\u5168\u8eab\u4eba\u4f53\u59ff\u52bf\u5148\u9a8c\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u59ff\u52bf\u5148\u9a8c\uff08DPoser\uff09\uff0c\u5e76\u6269\u5c55\u4e3aDPoser-X\uff0c\u7ed3\u5408\u53d8\u5206\u6269\u6563\u91c7\u6837\u548c\u622a\u65ad\u65f6\u95f4\u6b65\u8c03\u5ea6\u65b9\u6cd5\uff0c\u4ee5\u53ca\u63a9\u7801\u8bad\u7ec3\u673a\u5236\u3002", "result": "\u5728\u8eab\u4f53\u3001\u624b\u90e8\u3001\u9762\u90e8\u548c\u5168\u8eab\u59ff\u52bf\u5efa\u6a21\u7684\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDPoser-X\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "DPoser-X \u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u521b\u65b0\u7684\u8bad\u7ec3\u673a\u5236\uff0c\u4e3a3D\u5168\u8eab\u4eba\u4f53\u59ff\u52bf\u5efa\u6a21\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.00639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00639", "abs": "https://arxiv.org/abs/2508.00639", "authors": ["Luisa Gall\u00e9e", "Catharina Silvia Lisson", "Christoph Gerhard Lisson", "Daniela Drees", "Felix Weig", "Daniel Vogele", "Meinrad Beer", "Michael G\u00f6tz"], "title": "Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification", "comment": "Accepted at iMIMIC - Interpretability of Machine Intelligence in\n  Medical Image Computing workshop MICCAI 2025 Medical Image Computing and\n  Computer Assisted Intervention", "summary": "Classification models that provide human-interpretable explanations enhance\nclinicians' trust and usability in medical image diagnosis. One research focus\nis the integration and prediction of pathology-related visual attributes used\nby radiologists alongside the diagnosis, aligning AI decision-making with\nclinical reasoning. Radiologists use attributes like shape and texture as\nestablished diagnostic criteria and mirroring these in AI decision-making both\nenhances transparency and enables explicit validation of model outputs.\nHowever, the adoption of such models is limited by the scarcity of large-scale\nmedical image datasets annotated with these attributes. To address this\nchallenge, we propose synthesizing attribute-annotated data using a generative\nmodel. We enhance the Diffusion Model with attribute conditioning and train it\nusing only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset.\nIncorporating its generated images into the training of an explainable model\nboosts performance, increasing attribute prediction accuracy by 13.4% and\ntarget prediction accuracy by 1.8% compared to training with only the small\nreal attribute-annotated dataset. This work highlights the potential of\nsynthetic data to overcome dataset limitations, enhancing the applicability of\nexplainable models in medical image analysis.", "AI": {"tldr": "\u901a\u8fc7\u751f\u6210\u5c5e\u6027\u6ce8\u91ca\u6570\u636e\uff0c\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u6570\u636e\u96c6\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u7f3a\u4e4f\u5c5e\u6027\u6ce8\u91ca\u7684\u95ee\u9898\uff0c\u4ee5\u589e\u5f3aAI\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u4e34\u5e8a\u63a8\u7406\u3002", "method": "\u901a\u8fc7\u589e\u5f3a\u6269\u6563\u6a21\u578b\uff08Diffusion Model\uff09\u7684\u5c5e\u6027\u6761\u4ef6\uff0c\u4ec5\u4f7f\u752820\u4e2a\u6765\u81eaLIDC-IDRI\u6570\u636e\u96c6\u7684\u5c5e\u6027\u6807\u8bb0\u80ba\u7ed3\u8282\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u751f\u6210\u5c5e\u6027\u6ce8\u91ca\u6570\u636e\u3002", "result": "\u4f7f\u7528\u751f\u6210\u56fe\u50cf\u8bad\u7ec3\u53ef\u89e3\u91ca\u6a21\u578b\u540e\uff0c\u5c5e\u6027\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e8613.4%\uff0c\u76ee\u6807\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e861.8%\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5408\u6210\u6570\u636e\u5728\u514b\u670d\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u9650\u5236\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.00649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00649", "abs": "https://arxiv.org/abs/2508.00649", "authors": ["Junhao Zheng", "Jiahao Sun", "Chenhao Lin", "Zhengyu Zhao", "Chen Ma", "Chong Zhang", "Cong Wang", "Qian Wang", "Chao Shen"], "title": "Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights", "comment": null, "summary": "Developing reliable defenses against patch attacks on object detectors has\nattracted increasing interest. However, we identify that existing defense\nevaluations lack a unified and comprehensive framework, resulting in\ninconsistent and incomplete assessments of current methods. To address this\nissue, we revisit 11 representative defenses and present the first patch\ndefense benchmark, involving 2 attack goals, 13 patch attacks, 11 object\ndetectors, and 4 diverse metrics. This leads to the large-scale adversarial\npatch dataset with 94 types of patches and 94,000 images. Our comprehensive\nanalyses reveal new insights: (1) The difficulty in defending against\nnaturalistic patches lies in the data distribution, rather than the commonly\nbelieved high frequencies. Our new dataset with diverse patch distributions can\nbe used to improve existing defenses by 15.09% AP@0.5. (2) The average\nprecision of the attacked object, rather than the commonly pursued patch\ndetection accuracy, shows high consistency with defense performance. (3)\nAdaptive attacks can substantially bypass existing defenses, and defenses with\ncomplex/stochastic models or universal patch properties are relatively robust.\nWe hope that our analyses will serve as guidance on properly evaluating patch\nattacks/defenses and advancing their design. Code and dataset are available at\nhttps://github.com/Gandolfczjh/APDE, where we will keep integrating new\nattacks/defenses.", "AI": {"tldr": "\u7814\u7a76\u6784\u5efa\u4e86\u9996\u4e2a\u5bf9\u6297\u6027\u8865\u4e01\u9632\u5fa1\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u9632\u5fa1\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\uff0c\u5982\u5229\u7528\u591a\u6837\u5316\u8865\u4e01\u5206\u5e03\u63d0\u5347\u9632\u5fa1\u6027\u80fd15.09% AP@0.5\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u8865\u4e01\u653b\u51fb\u7684\u9632\u5fa1\u8bc4\u4f30\u7f3a\u4e4f\u7edf\u4e00\u5168\u9762\u7684\u6846\u67b6\uff0c\u5bfc\u81f4\u5bf9\u5f53\u524d\u65b9\u6cd5\u7684\u8bc4\u4f30\u4e0d\u4e00\u81f4\u4e14\u4e0d\u5b8c\u6574\u3002", "method": "\u7814\u7a76\u91cd\u65b0\u8bc4\u4f30\u4e8611\u79cd\u4ee3\u8868\u6027\u9632\u5fa1\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u5305\u542b2\u79cd\u653b\u51fb\u76ee\u6807\u300113\u79cd\u8865\u4e01\u653b\u51fb\u300111\u79cd\u76ee\u6807\u68c0\u6d4b\u5668\u548c4\u79cd\u591a\u6837\u6307\u6807\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u751f\u6210\u4e86\u5305\u542b94\u79cd\u8865\u4e01\u7c7b\u578b\u548c94,000\u5f20\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u5bf9\u6297\u6027\u8865\u4e01\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\uff081\uff09\u9632\u5fa1\u81ea\u7136\u8865\u4e01\u7684\u96be\u70b9\u5728\u4e8e\u6570\u636e\u5206\u5e03\u800c\u975e\u9ad8\u9891\u7279\u5f81\uff1b\uff082\uff09\u653b\u51fb\u76ee\u6807\u7684\u5e73\u5747\u7cbe\u5ea6\u4e0e\u9632\u5fa1\u6027\u80fd\u9ad8\u5ea6\u4e00\u81f4\uff1b\uff083\uff09\u81ea\u9002\u5e94\u653b\u51fb\u80fd\u663e\u8457\u7ed5\u8fc7\u73b0\u6709\u9632\u5fa1\uff0c\u800c\u590d\u6742/\u968f\u673a\u6a21\u578b\u6216\u901a\u7528\u8865\u4e01\u5c5e\u6027\u7684\u9632\u5fa1\u76f8\u5bf9\u7a33\u5065\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5efa\u7acb\u9996\u4e2a\u5bf9\u6297\u6027\u8865\u4e01\u9632\u5fa1\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\uff0c\u5982\u5229\u7528\u591a\u6837\u5316\u7684\u8865\u4e01\u5206\u5e03\u63d0\u5347\u9632\u5fa1\u6027\u80fd15.09% AP@0.5\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u9632\u5fa1\u6027\u80fd\u4e0e\u653b\u51fb\u76ee\u6807\u7684\u5e73\u5747\u7cbe\u5ea6\u9ad8\u5ea6\u4e00\u81f4\uff0c\u800c\u975e\u4f20\u7edf\u8ffd\u6c42\u7684\u8865\u4e01\u68c0\u6d4b\u51c6\u786e\u7387\u3002"}}
{"id": "2508.00698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00698", "abs": "https://arxiv.org/abs/2508.00698", "authors": ["Hongfei Zhang", "Kun Zhou", "Ruizheng Wu", "Jiangbo Lu"], "title": "Can Large Pretrained Depth Estimation Models Help With Image Dehazing?", "comment": "Submitted to AAAI2026", "summary": "Image dehazing remains a challenging problem due to the spatially varying\nnature of haze in real-world scenes. While existing methods have demonstrated\nthe promise of large-scale pretrained models for image dehazing, their\narchitecture-specific designs hinder adaptability across diverse scenarios with\ndifferent accuracy and efficiency requirements. In this work, we systematically\ninvestigate the generalization capability of pretrained depth\nrepresentations-learned from millions of diverse images-for image dehazing. Our\nempirical analysis reveals that the learned deep depth features maintain\nremarkable consistency across varying haze levels. Building on this insight, we\npropose a plug-and-play RGB-D fusion module that seamlessly integrates with\ndiverse dehazing architectures. Extensive experiments across multiple\nbenchmarks validate both the effectiveness and broad applicability of our\napproach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6df1\u5ea6\u7279\u5f81\u7684RGB-D\u878d\u5408\u6a21\u5757\uff0c\u53ef\u9002\u914d\u591a\u79cd\u53bb\u96fe\u67b6\u6784\uff0c\u63d0\u5347\u53bb\u96fe\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u67b6\u6784\u7279\u5b9a\u8bbe\u8ba1\u4e0a\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u6df1\u5ea6\u8868\u793a\u5728\u56fe\u50cf\u53bb\u96fe\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u4e00\u4e2aRGB-D\u878d\u5408\u6a21\u5757\u3002", "result": "\u63d0\u51fa\u7684RGB-D\u878d\u5408\u6a21\u5757\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u5b66\u4e60\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u7279\u5f81\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684RGB-D\u878d\u5408\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u80fd\u591f\u4e0e\u591a\u79cd\u53bb\u96fe\u67b6\u6784\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2508.00726", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00726", "abs": "https://arxiv.org/abs/2508.00726", "authors": ["Jiale Li", "Mingrui Wu", "Zixiang Jin", "Hao Chen", "Jiayi Ji", "Xiaoshuai Sun", "Liujuan Cao", "Rongrong Ji"], "title": "MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models", "comment": "ACM MM25 has accepted this paper", "summary": "Despite growing interest in hallucination in Multimodal Large Language\nModels, existing studies primarily focus on single-image settings, leaving\nhallucination in multi-image scenarios largely unexplored. To address this gap,\nwe conduct the first systematic study of hallucinations in multi-image MLLMs\nand propose MIHBench, a benchmark specifically tailored for evaluating\nobject-related hallucinations across multiple images. MIHBench comprises three\ncore tasks: Multi-Image Object Existence Hallucination, Multi-Image Object\nCount Hallucination, and Object Identity Consistency Hallucination, targeting\nsemantic understanding across object existence, quantity reasoning, and\ncross-view identity consistency. Through extensive evaluation, we identify key\nfactors associated with the occurrence of multi-image hallucinations,\nincluding: a progressive relationship between the number of image inputs and\nthe likelihood of hallucination occurrences; a strong correlation between\nsingle-image hallucination tendencies and those observed in multi-image\ncontexts; and the influence of same-object image ratios and the positional\nplacement of negative samples within image sequences on the occurrence of\nobject identity consistency hallucination. To address these challenges, we\npropose a Dynamic Attention Balancing mechanism that adjusts inter-image\nattention distributions while preserving the overall visual attention\nproportion. Experiments across multiple state-of-the-art MLLMs demonstrate that\nour method effectively reduces hallucination occurrences and enhances semantic\nintegration and reasoning stability in multi-image scenarios.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u591a\u56fe\u50cfMLLMs\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86MIHBench\u57fa\u51c6\u548c\u52a8\u6001\u6ce8\u610f\u529b\u5e73\u8861\u673a\u5236\uff0c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u5347\u591a\u56fe\u50cf\u573a\u666f\u7684\u8bed\u4e49\u6574\u5408\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u56fe\u50cf\u8bbe\u7f6e\u4e0b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u800c\u591a\u56fe\u50cf\u573a\u666f\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u6784\u5efaMIHBench\u57fa\u51c6\uff0c\u8bc4\u4f30\u591a\u56fe\u50cfMLLMs\u4e2d\u7684\u5bf9\u8c61\u76f8\u5173\u5e7b\u89c9\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u6ce8\u610f\u529b\u5e73\u8861\u673a\u5236\u6765\u8c03\u6574\u56fe\u50cf\u95f4\u7684\u6ce8\u610f\u529b\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u52a8\u6001\u6ce8\u610f\u529b\u5e73\u8861\u673a\u5236\u80fd\u663e\u8457\u51cf\u5c11\u591a\u56fe\u50cf\u5e7b\u89c9\uff0c\u5e76\u63d0\u9ad8\u8bed\u4e49\u7406\u89e3\u548c\u63a8\u7406\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6ce8\u610f\u529b\u5e73\u8861\u673a\u5236\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u591a\u56fe\u50cf\u573a\u666f\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u5e76\u63d0\u5347\u4e86\u8bed\u4e49\u6574\u5408\u548c\u63a8\u7406\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.00728", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00728", "abs": "https://arxiv.org/abs/2508.00728", "authors": ["Guanning Zeng", "Xiang Zhang", "Zirui Wang", "Haiyang Xu", "Zeyuan Chen", "Bingnan Li", "Zhuowen Tu"], "title": "YOLO-Count: Differentiable Object Counting for Text-to-Image Generation", "comment": "ICCV 2025", "summary": "We propose YOLO-Count, a differentiable open-vocabulary object counting model\nthat tackles both general counting challenges and enables precise quantity\ncontrol for text-to-image (T2I) generation. A core contribution is the\n'cardinality' map, a novel regression target that accounts for variations in\nobject size and spatial distribution. Leveraging representation alignment and a\nhybrid strong-weak supervision scheme, YOLO-Count bridges the gap between\nopen-vocabulary counting and T2I generation control. Its fully differentiable\narchitecture facilitates gradient-based optimization, enabling accurate object\ncount estimation and fine-grained guidance for generative models. Extensive\nexperiments demonstrate that YOLO-Count achieves state-of-the-art counting\naccuracy while providing robust and effective quantity control for T2I systems.", "AI": {"tldr": "YOLO-Count \u662f\u4e00\u79cd\u53ef\u5fae\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u8ba1\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u2018cardinality map\u2019\u548c\u6df7\u5408\u76d1\u7763\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u8ba1\u6570\u548cT2I\u751f\u6210\u7684\u7cbe\u786e\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u901a\u7528\u8ba1\u6570\u6311\u6218\u5e76\u4e3a\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u751f\u6210\u63d0\u4f9b\u7cbe\u786e\u7684\u6570\u91cf\u63a7\u5236\u3002", "method": "YOLO-Count \u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u2018cardinality map\u2019\u7684\u65b0\u578b\u56de\u5f52\u76ee\u6807\uff0c\u7ed3\u5408\u8868\u5f81\u5bf9\u9f50\u548c\u6df7\u5408\u5f3a\u5f31\u76d1\u7763\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u5f00\u653e\u8bcd\u6c47\u8ba1\u6570\u4e0eT2I\u751f\u6210\u63a7\u5236\u7684\u6865\u6881\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cYOLO-Count \u5728\u8ba1\u6570\u51c6\u786e\u6027\u4e0a\u8fbe\u5230\u4e86\u6700\u65b0\u6280\u672f\u6c34\u5e73\uff0c\u5e76\u4e3aT2I\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u6709\u6548\u7684\u6570\u91cf\u63a7\u5236\u3002", "conclusion": "YOLO-Count \u901a\u8fc7\u5176\u5b8c\u5168\u53ef\u5fae\u7684\u67b6\u6784\uff0c\u5728\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u8ba1\u6570\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u7cbe\u786e\u6570\u91cf\u63a7\u5236\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u65b0\u6280\u672f\u6c34\u5e73\u7684\u6210\u679c\u3002"}}
{"id": "2508.00744", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00744", "abs": "https://arxiv.org/abs/2508.00744", "authors": ["Adwait Chandorkar", "Hasan Tercan", "Tobias Meisen"], "title": "Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR", "comment": "accepted at the Embedded Vision Workshop ICCV 2025", "summary": "Recent advancements in LiDAR-based 3D object detection have significantly\naccelerated progress toward the realization of fully autonomous driving in\nreal-world environments. Despite achieving high detection performance, most of\nthe approaches still rely on a VGG-based or ResNet-based backbone for feature\nexploration, which increases the model complexity. Lightweight backbone design\nis well-explored for 2D object detection, but research on 3D object detection\nstill remains limited. In this work, we introduce Dense Backbone, a lightweight\nbackbone that combines the benefits of high processing speed, lightweight\narchitecture, and robust detection accuracy. We adapt multiple SoTA 3d object\ndetectors, such as PillarNet, with our backbone and show that with our\nbackbone, these models retain most of their detection capability at a\nsignificantly reduced computational cost. To our knowledge, this is the first\ndense-layer-based backbone tailored specifically for 3D object detection from\npoint cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%\nreduction in model parameters and a 28% reduction in latency with just a 2%\ndrop in detection accuracy on the nuScenes test set. Furthermore, Dense\nBackbone's plug-and-play design allows straightforward integration into\nexisting architectures, requiring no modifications to other network components.", "AI": {"tldr": "Dense Backbone\u662f\u4e00\u79cd\u4e13\u4e3a\u70b9\u4e91\u6570\u636e3D\u76ee\u6807\u68c0\u6d4b\u8bbe\u8ba1\u7684\u8f7b\u91cf\u7ea7\u9aa8\u5e72\u7f51\u7edc\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u5c3d\u7ba1LiDAR-based 3D\u76ee\u6807\u68c0\u6d4b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u4f9d\u8d56\u590d\u6742\u7684VGG\u6216ResNet\u9aa8\u5e72\u7f51\u7edc\uff0c\u589e\u52a0\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u3002\u8f7b\u91cf\u7ea7\u9aa8\u5e72\u7f51\u7edc\u57282D\u76ee\u6807\u68c0\u6d4b\u4e2d\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u57283D\u76ee\u6807\u68c0\u6d4b\u4e2d\u4ecd\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86Dense Backbone\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9aa8\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u4e86\u9ad8\u5904\u7406\u901f\u5ea6\u3001\u8f7b\u91cf\u7ea7\u67b6\u6784\u548c\u9c81\u68d2\u68c0\u6d4b\u7cbe\u5ea6\u3002\u901a\u8fc7\u9002\u914dPillarNet\u7b49SoTA 3D\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "result": "DensePillarNet\u5728nuScenes\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e8629%\u7684\u6a21\u578b\u53c2\u6570\u51cf\u5c11\u548c28%\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u4ec5\u5e26\u67652%\u7684\u68c0\u6d4b\u7cbe\u5ea6\u4e0b\u964d\u3002", "conclusion": "Dense Backbone\u4e3a3D\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u4e14\u6613\u4e8e\u96c6\u6210\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2508.00746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00746", "abs": "https://arxiv.org/abs/2508.00746", "authors": ["Regine Hartwig", "Dominik Muhle", "Riccardo Marin", "Daniel Cremers"], "title": "GECO: Geometrically Consistent Embedding with Lightspeed Inference", "comment": null, "summary": "Recent advances in feature learning have shown that self-supervised vision\nfoundation models can capture semantic correspondences but often lack awareness\nof underlying 3D geometry. GECO addresses this gap by producing geometrically\ncoherent features that semantically distinguish parts based on geometry (e.g.,\nleft/right eyes, front/back legs). We propose a training framework based on\noptimal transport, enabling supervision beyond keypoints, even under occlusions\nand disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%\nfaster than prior methods, while achieving state-of-the-art performance on\nPFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.\nFinally, we show that PCK alone is insufficient to capture geometric quality\nand introduce new metrics and insights for more geometry-aware feature\nlearning. Link to project page:\nhttps://reginehartwig.github.io/publications/geco/", "AI": {"tldr": "GECO\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u51e0\u4f55\u4e00\u81f4\u7279\u5f81\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u51e0\u4f55\u611f\u77e5\u548c\u8bed\u4e49\u5bf9\u5e94\u6027\u80fd\uff0c\u540c\u65f6\u8fd0\u884c\u6548\u7387\u6781\u9ad8\u3002", "motivation": "\u89e3\u51b3\u81ea\u76d1\u7763\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u6355\u6349\u8bed\u4e49\u5bf9\u5e94\u65f6\u7f3a\u4e4f3D\u51e0\u4f55\u611f\u77e5\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u652f\u6301\u5728\u906e\u6321\u548c\u53bb\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u76d1\u7763\u5b66\u4e60\uff0c\u8f7b\u91cf\u7ea7\u67b6\u6784\u5b9e\u73b0\u5b9e\u65f6\u5904\u7406\u3002", "result": "\u5728PFPascal\u3001APK\u548cCUB\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cPCK\u5206\u522b\u63d0\u53476.0%\u30016.2%\u548c4.1%\uff0c\u8fd0\u884c\u901f\u5ea6\u8fbe30fps\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb98.2%\u3002", "conclusion": "GECO\u901a\u8fc7\u5f15\u5165\u51e0\u4f55\u4e00\u81f4\u7684\u7279\u5f81\u5b66\u4e60\u548c\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u81ea\u76d1\u7763\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u57283D\u51e0\u4f55\u611f\u77e5\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2508.00750", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00750", "abs": "https://arxiv.org/abs/2508.00750", "authors": ["Prerana Ramkumar"], "title": "SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation", "comment": null, "summary": "Generative Adversarial Networks (GANs) have achieved realistic\nsuper-resolution (SR) of images however, they lack semantic consistency and\nper-pixel confidence, limiting their credibility in critical remote sensing\napplications such as disaster response, urban planning and agriculture. This\npaper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first\nSR framework designed for satellite imagery to integrate the ESRGAN,\nsegmentation loss via DeepLabv3 for class detail preservation and Monte Carlo\ndropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results\n(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This\nnovel model is valuable in satellite systems or UAVs that use wide\nfield-of-view (FoV) cameras, trading off spatial resolution for coverage. The\nmodular design allows integration in UAV data pipelines for on-board or\npost-processing SR to enhance imagery resulting due to motion blur, compression\nand sensor limitations. Further, the model is fine-tuned to evaluate its\nperformance on cross domain applications. The tests are conducted on two drone\nbased datasets which differ in altitude and imaging perspective. Performance\nevaluation of the fine-tuned models show a stronger adaptation to the Aerial\nMaritime Drone Dataset, whose imaging characteristics align with the training\ndata, highlighting the importance of domain-aware training in SR-applications.", "AI": {"tldr": "SU-ESRGAN\u662f\u9996\u4e2a\u4e3a\u536b\u661f\u56fe\u50cf\u8bbe\u8ba1\u7684\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u63d0\u5347\u9065\u611f\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3GAN\u5728\u8d85\u5206\u8fa8\u7387\u4e2d\u7f3a\u4e4f\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u50cf\u7d20\u7ea7\u7f6e\u4fe1\u5ea6\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5173\u952e\u9065\u611f\u5e94\u7528\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u7ed3\u5408ESRGAN\u3001DeepLabv3\u5206\u5272\u635f\u5931\u548c\u8499\u7279\u5361\u6d1bdropout\uff0c\u751f\u6210\u50cf\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\u5730\u56fe\u3002", "result": "\u5728PSNR\u3001SSIM\u3001LPIPS\u7b49\u6307\u6807\u4e0a\u4e0e\u57fa\u7ebfESRGAN\u76f8\u5f53\uff0c\u5c24\u5176\u5728\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "SU-ESRGAN\u5728\u536b\u661f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u65e0\u4eba\u673a\u548c\u536b\u661f\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u5f3a\u8c03\u4e86\u9886\u57df\u611f\u77e5\u8bad\u7ec3\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00777", "abs": "https://arxiv.org/abs/2508.00777", "authors": ["Zihan Wang", "Samira Ebrahimi Kahou", "Narges Armanfard"], "title": "Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning", "comment": "Accepted at BMVC 2025", "summary": "Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects\nin unseen categories by relying solely on generalizable features rather than\nrequiring any labeled examples of anomalies. However, existing ZSAD methods,\nwhether using fixed or learned prompts, struggle under domain shifts because\ntheir training data are derived from limited training domains and fail to\ngeneralize to new distributions. In this paper, we introduce PILOT, a framework\ndesigned to overcome these challenges through two key innovations: (1) a novel\ndual-branch prompt learning mechanism that dynamically integrates a pool of\nlearnable prompts with structured semantic attributes, enabling the model to\nadaptively weight the most relevant anomaly cues for each input image; and (2)\na label-free test-time adaptation strategy that updates the learnable prompt\nparameters using high-confidence pseudo-labels from unlabeled test data.\nExtensive experiments on 13 industrial and medical benchmarks demonstrate that\nPILOT achieves state-of-the-art performance in both anomaly detection and\nlocalization under domain shift.", "AI": {"tldr": "PILOT\u901a\u8fc7\u52a8\u6001\u53cc\u91cd\u5206\u652f\u63d0\u793a\u5b66\u4e60\u548c\u65e0\u6807\u7b7e\u6d4b\u8bd5\u65f6\u9002\u5e94\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u5728\u9886\u57df\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u9886\u57df\u504f\u79fb\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u5176\u8bad\u7ec3\u6570\u636e\u6765\u6e90\u6709\u9650\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u65b0\u5206\u5e03\u3002", "method": "\u91c7\u7528\u53cc\u91cd\u5206\u652f\u63d0\u793a\u5b66\u4e60\u673a\u5236\u548c\u57fa\u4e8e\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u7684\u65e0\u6807\u7b7e\u6d4b\u8bd5\u65f6\u9002\u5e94\u7b56\u7565\u3002", "result": "\u572813\u4e2a\u5de5\u4e1a\u548c\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPILOT\u5728\u9886\u57df\u504f\u79fb\u4e0b\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u6027\u80fd\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "PILOT\u6846\u67b6\u901a\u8fc7\u53cc\u91cd\u5206\u652f\u63d0\u793a\u5b66\u4e60\u548c\u65e0\u6807\u7b7e\u6d4b\u8bd5\u65f6\u9002\u5e94\u7b56\u7565\uff0c\u5728\u9886\u57df\u504f\u79fb\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2508.00822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00822", "abs": "https://arxiv.org/abs/2508.00822", "authors": ["Alexander Nikitas Dimopoulos", "Joseph Grasso"], "title": "Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning", "comment": null, "summary": "This study analyzes semantic segmentation performance across heterogeneously\nlabeled point-cloud datasets relevant to public safety applications, including\npre-incident planning systems derived from lidar scans. Using NIST's Point\nCloud City dataset (Enfield and Memphis collections), we investigate challenges\nin unifying differently labeled 3D data. Our methodology employs a graded\nschema with the KPConv architecture, evaluating performance through IoU metrics\non safety-relevant features. Results indicate performance variability:\ngeometrically large objects (e.g. stairs, windows) achieve higher segmentation\nperformance, suggesting potential for navigational context, while smaller\nsafety-critical features exhibit lower recognition rates. Performance is\nimpacted by class imbalance and the limited geometric distinction of smaller\nobjects in typical lidar scans, indicating limitations in detecting certain\nsafety-relevant features using current point-cloud methods. Key identified\nchallenges include insufficient labeled data, difficulties in unifying class\nlabels across datasets, and the need for standardization. Potential directions\ninclude automated labeling and multi-dataset learning strategies. We conclude\nthat reliable point-cloud semantic segmentation for public safety necessitates\nstandardized annotation protocols and improved labeling techniques to address\ndata heterogeneity and the detection of small, safety-critical elements.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u5f02\u6784\u6807\u6ce8\u70b9\u4e91\u6570\u636e\u96c6\u5728\u516c\u5171\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u53d1\u73b0\u5927\u578b\u7269\u4f53\u5206\u5272\u6548\u679c\u8f83\u597d\uff0c\u5c0f\u578b\u5b89\u5168\u5173\u952e\u7279\u5f81\u8bc6\u522b\u7387\u4f4e\uff0c\u9700\u6807\u51c6\u5316\u6807\u6ce8\u548c\u6539\u8fdb\u6280\u672f\u3002", "motivation": "\u7814\u7a76\u5f02\u6784\u6807\u6ce8\u70b9\u4e91\u6570\u636e\u96c6\u5728\u516c\u5171\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9884\u4e8b\u4ef6\u89c4\u5212\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u5206\u7ea7\u6807\u6ce8\u65b9\u6848\u548cKPConv\u67b6\u6784\uff0c\u901a\u8fc7IoU\u6307\u6807\u8bc4\u4f30\u5b89\u5168\u76f8\u5173\u7279\u5f81\u7684\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u51e0\u4f55\u8f83\u5927\u7684\u7269\u4f53\uff08\u5982\u697c\u68af\u3001\u7a97\u6237\uff09\u5206\u5272\u6027\u80fd\u8f83\u9ad8\uff0c\u800c\u5c0f\u578b\u5b89\u5168\u5173\u952e\u7279\u5f81\u7684\u8bc6\u522b\u7387\u8f83\u4f4e\uff0c\u6027\u80fd\u53d7\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u5178\u578b\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u4e2d\u5c0f\u7269\u4f53\u51e0\u4f55\u533a\u5206\u5ea6\u6709\u9650\u7684\u5f71\u54cd\u3002", "conclusion": "\u53ef\u9760\u7684\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u5728\u516c\u5171\u5b89\u5168\u9886\u57df\u9700\u8981\u6807\u51c6\u5316\u7684\u6807\u6ce8\u534f\u8bae\u548c\u6539\u8fdb\u7684\u6807\u6ce8\u6280\u672f\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u5f02\u8d28\u6027\u548c\u5c0f\u578b\u5b89\u5168\u5173\u952e\u5143\u7d20\u7684\u68c0\u6d4b\u95ee\u9898\u3002"}}
