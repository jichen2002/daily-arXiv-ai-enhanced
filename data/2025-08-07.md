<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 132]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.RO](#cs.RO) [Total: 16]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 9]
- [cs.DC](#cs.DC) [Total: 11]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Text2VR: Automated instruction Generation in Virtual Reality using Large language Models for Assembly Task](https://arxiv.org/abs/2508.03699)
*Subin Raj Peter*

Main category: cs.CV

TL;DR: 利用LLM自动生成VR培训指令，降低开发成本并提升培训效果。


<details>
  <summary>Details</summary>
Motivation: 解决VR培训应用开发中的时间、专业知识和资源挑战，降低开发成本并提升培训效果。

Method: 系统包含两个核心组件：LLM模块从文本中提取任务相关信息，智能模块将这些信息转化为VR环境中的动画演示和视觉提示。

Result: 提出的方法有效减少了开发开销，提高了培训内容的生成效率和适应性。

Conclusion: 利用大型语言模型（LLMs）自动生成虚拟指令，显著提升了VR培训的效率和可扩展性，适应了不断变化的工业需求。

Abstract: Virtual Reality (VR) has emerged as a powerful tool for workforce training,
offering immersive, interactive, and risk-free environments that enhance skill
acquisition, decision-making, and confidence. Despite its advantages,
developing VR applications for training remains a significant challenge due to
the time, expertise, and resources required to create accurate and engaging
instructional content. To address these limitations, this paper proposes a
novel approach that leverages Large Language Models (LLMs) to automate the
generation of virtual instructions from textual input. The system comprises two
core components: an LLM module that extracts task-relevant information from the
text, and an intelligent module that transforms this information into animated
demonstrations and visual cues within a VR environment. The intelligent module
receives input from the LLM module and interprets the extracted information.
Based on this, an instruction generator creates training content using relevant
data from a database. The instruction generator generates the instruction by
changing the color of virtual objects and creating animations to illustrate
tasks. This approach enhances training effectiveness and reduces development
overhead, making VR-based training more scalable and adaptable to evolving
industrial needs.

</details>


### [2] [Outlier Detection Algorithm for Circle Fitting](https://arxiv.org/abs/2508.03720)
*Ahmet Gökhan Poyraz*

Main category: cs.CV

TL;DR: 研究提出PCOD算法，通过极坐标转换和标准差比较检测异常值，显著提升工业圆拟合精度。


<details>
  <summary>Details</summary>
Motivation: 圆拟合方法在工业中广泛应用，但噪声点集会显著影响算法效果。本研究旨在通过异常值检测与去除算法提升圆拟合的准确性。

Method: 研究提出了基于极坐标的异常值检测（PCOD）算法，包括将点集转换为极坐标、计算局部和全局标准差，并通过比较局部均值与全局标准差来识别异常值。

Result: PCOD算法在工业垫圈零件的高精度直径测量中表现优异，优于其他十种圆拟合算法和五种异常值检测方法。

Conclusion: 该研究提出的PCOD算法在工业环境中显著提升了圆拟合的准确性，展示了其在工业应用中的潜力。

Abstract: Circle fitting methods are extensively utilized in various industries,
particularly in quality control processes and design applications. The
effectiveness of these algorithms can be significantly compromised when the
point sets to be predicted are noisy. To mitigate this issue, outlier detection
and removal algorithms are often applied before the circle fitting procedure.
This study introduces the Polar Coordinate-Based Outlier Detection (PCOD)
algorithm, which can be effectively employed in circle fitting applications. In
the proposed approach, the point set is first transformed into polar
coordinates, followed by the calculation of both local and global standard
deviations. Outliers are then identified by comparing local mean values with
the global standard deviation. The practicality and efficiency of the proposed
method are demonstrated by focusing on the high-precision diameter measurement
of industrial washer parts. Images from a machine vision system are processed
through preprocessing steps, including sub-pixel edge detection. The resulting
sub-pixel edge points are then cleaned using the proposed outlier detection and
removal algorithm, after which circle fitting is performed. A comparison is
made using ten different circle fitting algorithms and five distinct outlier
detection methods. The results indicate that the proposed method outperforms
the other approaches, delivering the best performance in terms of accuracy
within the dataset, thereby demonstrating its potential for enhancing circle
fitting applications in industrial environments.

</details>


### [3] [Enhancing Diameter Measurement Accuracy in Machine Vision Applications](https://arxiv.org/abs/2508.03721)
*Ahmet Gokhan Poyraz,Ahmet Emir Dirik,Hakan Gurkan,Mehmet Kacmaz*

Main category: cs.CV

TL;DR: 本研究提出两种方法（基于转换因子和基于像素）来提高相机测量系统的准确性，实验证明误差从13-114微米降至1-2微米。


<details>
  <summary>Details</summary>
Motivation: 在相机测量系统中，尽管使用了远心镜头等专业设备，机械和软件因素仍会导致测量误差，尤其是在测量不同直径的部件时。本研究旨在通过创新方法减少这些误差。

Method: 本研究提出了两种方法：1) 基于转换因子的方法，通过已知参考估计转换因子来计算未知部分的直径；2) 基于像素的方法，直接利用参考的像素直径信息估计直径。实验使用工业级相机和远心镜头，测试了玻璃样品（1-12毫米）和金属工件（3-24毫米）。

Result: 实验结果显示，测量误差从原来的13-114微米显著降低到1-2微米，证明了所提方法的有效性。

Conclusion: 通过使用已知参考部分的两种创新方法（基于转换因子的方法和基于像素的方法），本研究显著提高了相机测量系统的准确性，将测量误差从原来的13-114微米降低到1-2微米。

Abstract: In camera measurement systems, specialized equipment such as telecentric
lenses is often employed to measure parts with narrow tolerances. However,
despite the use of such equipment, measurement errors can occur due to
mechanical and software-related factors within the system. These errors are
particularly evident in applications where parts of different diameters are
measured using the same setup. This study proposes two innovative approaches to
enhance measurement accuracy using multiple known reference parts: a conversion
factor-based method and a pixel-based method. In the first approach, the
conversion factor is estimated from known references to calculate the diameter
(mm) of the unknown part. In the second approach, the diameter (mm) is directly
estimated using pixel-based diameter information from the references. The
experimental setup includes an industrial-grade camera and telecentric lenses.
Tests conducted on glass samples (1-12 mm) and metal workpieces (3-24 mm) show
that measurement errors, which originally ranged from 13-114 micrometers, were
reduced to 1-2 micrometers using the proposed methods. By utilizing only a few
known reference parts, the proposed approach enables high-accuracy measurement
of all parts within the camera's field of view. Additionally, this method
enhances the existing diameter measurement literature by significantly reducing
error rates and improving measurement reliability.

</details>


### [4] [Multimodal Video Emotion Recognition with Reliable Reasoning Priors](https://arxiv.org/abs/2508.03722)
*Zhepeng Wang,Yingjian Zhu,Guanghao Dong,Hongzhu Yi,Feng Chen,Xinming Wang,Jun Xie*

Main category: cs.CV

TL;DR: 研究通过Gemini生成的推理轨迹和平衡双对比学习，提升了多模态情感识别的性能，并解决了类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 探讨如何将MLLM的可信先验推理知识整合到多模态情感识别中，以提升性能并解决类别不平衡问题。

Method: 研究采用Gemini生成细粒度、模态可分离的推理轨迹，并在融合阶段注入为先验知识以丰富跨模态交互。同时，引入了平衡双对比学习（Balanced Dual-Contrastive Learning）以缓解类别不平衡问题。

Result: 在MER2024基准测试中，该先验增强框架实现了显著的性能提升。

Conclusion: 该研究通过将MLLM的可信先验推理知识与多模态情感识别相结合，展示了轻量级融合网络与MLLM推理的协同作用，实现了稳健且可扩展的情感识别性能。

Abstract: This study investigates the integration of trustworthy prior reasoning
knowledge from MLLMs into multimodal emotion recognition. We employ Gemini to
generate fine-grained, modality-separable reasoning traces, which are injected
as priors during the fusion stage to enrich cross-modal interactions. To
mitigate the pronounced class-imbalance in multimodal emotion recognition, we
introduce Balanced Dual-Contrastive Learning, a loss formulation that jointly
balances inter-class and intra-class distributions. Applied to the MER2024
benchmark, our prior-enhanced framework yields substantial performance gains,
demonstrating that the reliability of MLLM-derived reasoning can be
synergistically combined with the domain adaptability of lightweight fusion
networks for robust, scalable emotion recognition.

</details>


### [5] [From Waveforms to Pixels: A Survey on Audio-Visual Segmentation](https://arxiv.org/abs/2508.03724)
*Jia Li,Yapeng Tian*

Main category: cs.CV

TL;DR: 本文综述了音频-视觉分割（AVS）的研究进展，包括问题定义、方法比较和未来方向，旨在推动多模态感知领域的发展。


<details>
  <summary>Details</summary>
Motivation: 音频-视觉分割（AVS）通过结合视觉和音频模态，实现对视频中发声物体的精细分割，是多模态感知中的重要研究方向。本文旨在全面概述AVS领域，推动其进一步发展。

Method: 本文通过综述AVS领域的问题定义、基准数据集、评估指标和方法论进展，分析了多种方法，包括单模态和多模态编码架构、音频-视觉融合策略以及解码器设计。

Result: 本文对AVS方法在标准基准上的表现进行了广泛比较，总结了不同架构选择、融合策略和训练范式对性能的影响。

Conclusion: 本文总结了音频-视觉分割（AVS）领域的当前挑战，并提出了未来研究方向，包括改进时间推理和多模态融合、利用基础模型提升泛化能力、减少对标注数据的依赖，以及引入更高级的推理能力。

Abstract: Audio-Visual Segmentation (AVS) aims to identify and segment sound-producing
objects in videos by leveraging both visual and audio modalities. It has
emerged as a significant research area in multimodal perception, enabling
fine-grained object-level understanding. In this survey, we present a
comprehensive overview of the AVS field, covering its problem formulation,
benchmark datasets, evaluation metrics, and the progression of methodologies.
We analyze a wide range of approaches, including architectures for unimodal and
multimodal encoding, key strategies for audio-visual fusion, and various
decoder designs. Furthermore, we examine major training paradigms, from fully
supervised learning to weakly supervised and training-free methods. Notably, we
provide an extensive comparison of AVS methods across standard benchmarks,
highlighting the impact of different architectural choices, fusion strategies,
and training paradigms on performance. Finally, we outline the current
challenges, such as limited temporal modeling, modality bias toward vision,
lack of robustness in complex environments, and high computational demands, and
propose promising future directions, including improving temporal reasoning and
multimodal fusion, leveraging foundation models for better generalization and
few-shot learning, reducing reliance on labeled data through selfand weakly
supervised learning, and incorporating higher-level reasoning for more
intelligent AVS systems.

</details>


### [6] [A Large Language Model Powered Integrated Circuit Footprint Geometry Understanding](https://arxiv.org/abs/2508.03725)
*Yida Wang,Taiting Lu,Runze Liu,Lanqing Yang,Yifan Yang,Zhe Chen,Yuehai Wang,Yixin Liu,Kaiyuan Lin,Xiaomeng Chen,Dian Ding,Yijie Li,Yi-Chao Chen,Yincheng Jin,Mahanth Gowda*

Main category: cs.CV

TL;DR: 提出LLM4-IC8K框架，利用LLMs分阶段训练解决IC机械图纸几何标注问题，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于IC机械图纸的非结构化绘图和抽象标注，自动化解析和准确建模PCB封装几何标注极具挑战性，且目前缺乏直接自动化标注方法。

Method: 提出LLM4-IC8K框架，分两阶段训练LMMs：先在合成数据上学习基础几何推理，再在真实数据上微调。包含三个子任务：引脚数量感知、引脚中心坐标计算和引脚尺寸估计。

Result: LLM4-IC8K在ICGeo8K数据集上表现优于现有LMMs，验证了其有效性。

Conclusion: LLM4-IC8K框架通过结合合成数据和真实数据训练，显著提升了IC机械图纸的几何标注准确性，为自动化PCB设计提供了新思路。

Abstract: Printed-Circuit-board (PCB) footprint geometry labeling of integrated
circuits (IC) is essential in defining the physical interface between
components and the PCB layout, requiring exceptional visual perception
proficiency. However, due to the unstructured footprint drawing and abstract
diagram annotations, automated parsing and accurate footprint geometry modeling
remain highly challenging. Despite its importance, no methods currently exist
for automated package geometry labeling directly from IC mechanical drawings.
In this paper, we first investigate the visual perception performance of Large
Multimodal Models (LMMs) when solving IC footprint geometry understanding. Our
findings reveal that current LMMs severely suffer from inaccurate geometric
perception, which hinders their performance in solving the footprint geometry
labeling problem. To address these limitations, we propose LLM4-IC8K, a novel
framework that treats IC mechanical drawings as images and leverages LLMs for
structured geometric interpretation. To mimic the step-by-step reasoning
approach used by human engineers, LLM4-IC8K addresses three sub-tasks:
perceiving the number of pins, computing the center coordinates of each pin,
and estimating the dimensions of individual pins. We present a two-stage
framework that first trains LMMs on synthetically generated IC footprint
diagrams to learn fundamental geometric reasoning and then fine-tunes them on
real-world datasheet drawings to enhance robustness and accuracy in practical
scenarios. To support this, we introduce ICGeo8K, a multi-modal dataset with
8,608 labeled samples, including 4138 hand-crafted IC footprint samples and
4470 synthetically generated samples. Extensive experiments demonstrate that
our model outperforms state-of-the-art LMMs on the proposed benchmark.

</details>


### [7] [TIR-Diffusion: Diffusion-based Thermal Infrared Image Denoising via Latent and Wavelet Domain Optimization](https://arxiv.org/abs/2508.03727)
*Tai Hyoung Rhee,Dong-guw Lee,Ayoung Kim*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型的红外图像去噪方法，结合潜在空间和小波域优化，实验证明其优于现有方法且具备零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 热红外图像在机器人感知任务中潜力巨大，但常受固定模式噪声影响，导致目标检测和定位等任务复杂化，因此需要高效的去噪方法。

Method: 利用预训练的稳定扩散模型，通过结合潜在空间和小波域优化的新型损失函数进行微调，并采用级联细化阶段来增强细节。

Result: 实验结果表明，该方法在基准数据集上优于现有去噪方法，并在多样化的真实世界数据集中展现出强大的零样本泛化能力。

Conclusion: 该论文提出的基于扩散模型的红外图像去噪框架在基准数据集上表现出色，并展示了在真实世界复杂场景中的零样本泛化能力，证明了其在机器人应用中的实用性。

Abstract: Thermal infrared imaging exhibits considerable potentials for robotic
perception tasks, especially in environments with poor visibility or
challenging lighting conditions. However, TIR images typically suffer from
heavy non-uniform fixed-pattern noise, complicating tasks such as object
detection, localization, and mapping. To address this, we propose a
diffusion-based TIR image denoising framework leveraging latent-space
representations and wavelet-domain optimization. Utilizing a pretrained stable
diffusion model, our method fine-tunes the model via a novel loss function
combining latent-space and discrete wavelet transform (DWT) / dual-tree complex
wavelet transform (DTCWT) losses. Additionally, we implement a cascaded
refinement stage to enhance fine details, ensuring high-fidelity denoising
results. Experiments on benchmark datasets demonstrate superior performance of
our approach compared to state-of-the-art denoising methods. Furthermore, our
method exhibits robust zero-shot generalization to diverse and challenging
real-world TIR datasets, underscoring its effectiveness for practical robotic
deployment.

</details>


### [8] [StorySync: Training-Free Subject Consistency in Text-to-Image Generation via Region Harmonization](https://arxiv.org/abs/2508.03735)
*Gopalji Gaur,Mohammadreza Zolfaghari,Thomas Brox*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，通过掩码跨图像注意力共享和区域特征协调，高效生成视觉一致的主题，保持扩散模型的创意能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖微调或重新训练模型，计算成本高、耗时，且可能干扰模型的现有能力。因此，寻求一种无需训练的高效方法以保持主题一致性。

Method: 采用无需训练的方法，引入掩码跨图像注意力共享和区域特征协调，动态对齐批量图像中的主题特征并细化视觉相似细节。

Result: 实验结果表明，该方法在多种场景下成功生成了视觉一致的主题，同时保持了扩散模型的创意能力。

Conclusion: 本文提出了一种无需训练的高效方法，通过引入掩码跨图像注意力共享和区域特征协调，成功在多种场景下生成了视觉一致的主题，同时保持了扩散模型的创意能力。

Abstract: Generating a coherent sequence of images that tells a visual story, using
text-to-image diffusion models, often faces the critical challenge of
maintaining subject consistency across all story scenes. Existing approaches,
which typically rely on fine-tuning or retraining models, are computationally
expensive, time-consuming, and often interfere with the model's pre-existing
capabilities. In this paper, we follow a training-free approach and propose an
efficient consistent-subject-generation method. This approach works seamlessly
with pre-trained diffusion models by introducing masked cross-image attention
sharing to dynamically align subject features across a batch of images, and
Regional Feature Harmonization to refine visually similar details for improved
subject consistency. Experimental results demonstrate that our approach
successfully generates visually consistent subjects across a variety of
scenarios while maintaining the creative abilities of the diffusion model.

</details>


### [9] [Fusion of Pervasive RF Data with Spatial Images via Vision Transformers for Enhanced Mapping in Smart Cities](https://arxiv.org/abs/2508.03736)
*Rafayel Mkrtchyan,Armen Manukyan,Hrant Khachatrian,Theofanis P. Raptis*

Main category: cs.CV

TL;DR: 本文提出了一种结合开源地图和RF数据的深度学习方法，通过多模态融合显著提升了建筑映射的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统智能城市映射技术（如卫星图像、LiDAR扫描和手动标注）存在成本高、可访问性差和准确性不足的问题，而开源地图平台数据因人为错误和环境变化引入偏差，影响神经网络性能。

Method: 采用基于视觉Transformer的架构，统一处理RF和地图数据，捕获空间依赖性和结构先验，以提升映射精度。

Result: 在合成数据集上的评估显示，该方法在宏观IoU上达到65.3%，显著优于错误地图基线（40.1%）、文献中的RF-only方法（37.3%）和非AI融合基线（42.2%）。

Conclusion: 本文提出的基于DINOv2架构的深度学习方法通过结合开源地图平台数据和射频（RF）数据，显著提高了建筑映射的准确性，证明了多模态数据融合在智能城市环境映射中的有效性。

Abstract: Environment mapping is an important computing task for a wide range of smart
city applications, including autonomous navigation, wireless network operations
and extended reality environments. Conventional smart city mapping techniques,
such as satellite imagery, LiDAR scans, and manual annotations, often suffer
from limitations related to cost, accessibility and accuracy. Open-source
mapping platforms have been widely utilized in artificial intelligence
applications for environment mapping, serving as a source of ground truth.
However, human errors and the evolving nature of real-world environments
introduce biases that can negatively impact the performance of neural networks
trained on such data. In this paper, we present a deep learning-based approach
that integrates the DINOv2 architecture to improve building mapping by
combining maps from open-source platforms with radio frequency (RF) data
collected from multiple wireless user equipments and base stations. Our
approach leverages a vision transformer-based architecture to jointly process
both RF and map modalities within a unified framework, effectively capturing
spatial dependencies and structural priors for enhanced mapping accuracy. For
the evaluation purposes, we employ a synthetic dataset co-produced by Huawei.
We develop and train a model that leverages only aggregated path loss
information to tackle the mapping problem. We measure the results according to
three performance metrics which capture different qualities: (i) The Jaccard
index, also known as intersection over union (IoU), (ii) the Hausdorff
distance, and (iii) the Chamfer distance. Our design achieves a macro IoU of
65.3%, significantly surpassing (i) the erroneous maps baseline, which yields
40.1%, (ii) an RF-only method from the literature, which yields 37.3%, and
(iii) a non-AI fusion baseline that we designed which yields 42.2%.

</details>


### [10] [VQ-DeepISC: Vector Quantized-Enabled Digital Semantic Communication with Channel Adaptive Image Transmission](https://arxiv.org/abs/2508.03740)
*Jianqiao Chen,Tingting Zhu,Huishi Song,Nan Ma,Xiaodong Xu*

Main category: cs.CV

TL;DR: VQ-DeepISC, a digital semantic communication system, uses Swin Transformer and VQ modules for efficient feature transmission, achieving better reconstruction fidelity than benchmarks.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in digitizing semantic features while preserving continuity and context, ensuring robustness to channel degradation, which is addressed by the proposed system.

Method: The system employs a Swin Transformer backbone for hierarchical semantic feature extraction, VQ modules for discrete latent space projection, and an attention mechanism-driven channel adaptation module. It also uses KLD minimization and EMA for stable training and balanced codebook updates.

Result: Experimental results show the system's superior performance in reconstruction fidelity compared to existing methods.

Conclusion: The proposed VQ-DeepISC system demonstrates superior reconstruction fidelity over benchmark methods, showcasing its effectiveness in digital semantic communication.

Abstract: Discretization of semantic features enables interoperability between semantic
and digital communication systems, showing significant potential for practical
applications. The fundamental difficulty in digitizing semantic features stems
from the need to preserve continuity and context in inherently analog
representations during their compression into discrete symbols while ensuring
robustness to channel degradation. In this paper, we propose a vector quantized
(VQ)-enabled digital semantic communication system with channel adaptive image
transmission, named VQ-DeepISC. Guided by deep joint source-channel coding
(DJSCC), we first design a Swin Transformer backbone for hierarchical semantic
feature extraction, followed by VQ modules projecting features into discrete
latent spaces. Consequently, it enables efficient index-based transmission
instead of raw feature transmission. To further optimize this process, we
develop an attention mechanism-driven channel adaptation module to dynamically
optimize index transmission. Secondly, to counteract codebook collapse during
training process, we impose a distributional regularization by minimizing the
Kullback-Leibler divergence (KLD) between codeword usage frequencies and a
uniform prior. Meanwhile, exponential moving average (EMA) is employed to
stabilize training and ensure balanced feature coverage during codebook
updates. Finally, digital communication is implemented using quadrature phase
shift keying (QPSK) modulation alongside orthogonal frequency division
multiplexing (OFDM), adhering to the IEEE 802.11a standard. Experimental
results demonstrate superior reconstruction fidelity of the proposed system
over benchmark methods.

</details>


### [11] [Tobler's First Law in GeoAI: A Spatially Explicit Deep Learning Model for Terrain Feature Detection Under Weak Supervision](https://arxiv.org/abs/2508.03745)
*Wenwen Li,Chia-Yu Hsu,Maosheng Hu*

Main category: cs.CV

TL;DR: A weakly supervised deep learning model for geospatial object detection, leveraging spatial principles and attention maps, successfully applied to detect Martian craters and generalizable to other features.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenges in GeoAI, such as a lack of training data and the neglect of spatial principles in AI model design, which hinder the integration of AI with geospatial research.

Method: The method includes a spatially explicit model based on Tobler's first law of geography, incorporation of attention maps into the object detection pipeline, and a multistage training strategy to improve performance.

Result: The model successfully detects impact craters on Mars and generalizes to both natural and human-made features on the surfaces of Earth and other planets, reducing the need for extensive manual effort.

Conclusion: This research advances the theoretical and methodological foundations of GeoAI by developing a spatially explicit deep learning model for object detection, particularly of natural features, in a weakly supervised manner.

Abstract: Recent interest in geospatial artificial intelligence (GeoAI) has fostered a
wide range of applications using artificial intelligence (AI), especially deep
learning, for geospatial problem solving. However, major challenges such as a
lack of training data and the neglect of spatial principles and spatial effects
in AI model design remain, significantly hindering the in-depth integration of
AI with geospatial research. This paper reports our work in developing a deep
learning model that enables object detection, particularly of natural features,
in a weakly supervised manner. Our work makes three contributions: First, we
present a method of object detection using only weak labels. This is achieved
by developing a spatially explicit model based on Tobler's first law of
geography. Second, we incorporate attention maps into the object detection
pipeline and develop a multistage training strategy to improve performance.
Third, we apply this model to detect impact craters on Mars, a task that
previously required extensive manual effort. The model generalizes to both
natural and human-made features on the surfaces of Earth and other planets.
This research advances the theoretical and methodological foundations of GeoAI.

</details>


### [12] [Closed-Circuit Television Data as an Emergent Data Source for Urban Rail Platform Crowding Estimation](https://arxiv.org/abs/2508.03749)
*Riccardo Fiorista,Awad Abdelhalim,Anson F. Stewart,Gabriel L. Pincus,Ian Thistle,Jinhua Zhao*

Main category: cs.CV

TL;DR: 研究通过计算机视觉方法分析CCTV图像，提出了一种高效线性优化方法，证明CCTV数据可独立用于实时站台拥挤估计，提升运营响应能力。


<details>
  <summary>Details</summary>
Motivation: 准确估计城市轨道交通站台的占用情况可以提升交通机构的运营决策能力，从而改善安全性、运营效率和乘客体验，尤其是在拥挤情况下。然而，实时感知拥挤情况仍具挑战性，通常依赖于自动收费数据或工作人员观察等间接指标。

Method: 研究比较了三种最先进的计算机视觉方法（YOLOv11、RT-DETRv2和APGCC的目标检测与计数；通过自定义训练的Vision Transformer Crowd-ViT进行人群级别分类；以及使用DeepLabV3的语义分割），并提出了一种新颖的基于线性优化的高效方法来从分割图中提取计数，同时考虑图像对象深度和乘客在站台上的分布。

Result: 在包含超过600小时视频材料的隐私保护数据集上测试表明，计算机视觉方法在人群估计方面具有显著价值。

Conclusion: 研究表明，CCTV图像数据能够独立于其他数据源，为实时拥挤估计提供更精确的支持，从而有助于及时采取运营措施缓解站台拥挤问题。

Abstract: Accurately estimating urban rail platform occupancy can enhance transit
agencies' ability to make informed operational decisions, thereby improving
safety, operational efficiency, and customer experience, particularly in the
context of crowding. However, sensing real-time crowding remains challenging
and often depends on indirect proxies such as automatic fare collection data or
staff observations. Recently, Closed-Circuit Television (CCTV) footage has
emerged as a promising data source with the potential to yield accurate,
real-time occupancy estimates. The presented study investigates this potential
by comparing three state-of-the-art computer vision approaches for extracting
crowd-related features from platform CCTV imagery: (a) object detection and
counting using YOLOv11, RT-DETRv2, and APGCC; (b) crowd-level classification
via a custom-trained Vision Transformer, Crowd-ViT; and (c) semantic
segmentation using DeepLabV3. Additionally, we present a novel, highly
efficient linear-optimization-based approach to extract counts from the
generated segmentation maps while accounting for image object depth and, thus,
for passenger dispersion along a platform. Tested on a privacy-preserving
dataset created in collaboration with the Washington Metropolitan Area Transit
Authority (WMATA) that encompasses more than 600 hours of video material, our
results demonstrate that computer vision approaches can provide substantive
value for crowd estimation. This work demonstrates that CCTV image data,
independent of other data sources available to a transit agency, can enable
more precise real-time crowding estimation and, eventually, timely operational
responses for platform crowding mitigation.

</details>


### [13] [Modular Transformer Architecture for Precision Agriculture Imaging](https://arxiv.org/abs/2508.03751)
*Brian Gopalan,Nathalia Nascimento,Vishal Monga*

Main category: cs.CV

TL;DR: 该论文提出了一种质量感知的模块化深度学习框架，通过动态路由策略优化杂草分割，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决精准农业中无人机视频高效且准确的杂草分割需求，应对常见的图像退化问题。

Method: 提出了一种质量感知的模块化深度学习框架，通过分析图像质量（如模糊和噪声），将输入动态路由到经过优化的预处理和Transformer模型。

Result: 该系统在分割质量和计算效率上均优于现有的基于CNN的方法。

Conclusion: 该系统通过动态路由策略，显著提高了杂草分割的质量和计算效率，为农业深度学习的应用提供了重要进展。

Abstract: This paper addresses the critical need for efficient and accurate weed
segmentation from drone video in precision agriculture. A quality-aware modular
deep-learning framework is proposed that addresses common image degradation by
analyzing quality conditions-such as blur and noise-and routing inputs through
specialized pre-processing and transformer models optimized for each
degradation type. The system first analyzes drone images for noise and blur
using Mean Absolute Deviation and the Laplacian. Data is then dynamically
routed to one of three vision transformer models: a baseline for clean images,
a modified transformer with Fisher Vector encoding for noise reduction, or
another with an unrolled Lucy-Robinson decoder to correct blur. This novel
routing strategy allows the system to outperform existing CNN-based methods in
both segmentation quality and computational efficiency, demonstrating a
significant advancement in deep-learning applications for agriculture.

</details>


### [14] [Generating Synthetic Invoices via Layout-Preserving Content Replacement](https://arxiv.org/abs/2508.03754)
*Bevin V,Ananthakrishnan P V,Ragesh KR,Sanjay M,Vineeth S,Bibin Wilson*

Main category: cs.CV

TL;DR: 提出了一种结合OCR、LLM和图像修复技术的合成发票生成方法，解决了数据隐私和标注成本的挑战，为文档智能模型训练提供了大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在自动化发票处理中因隐私法规和手动标注成本高而难以获取大规模多样化数据集的问题。

Method: 方法包括三个步骤：1) 使用OCR提取源发票的文本内容和空间布局；2) 利用LLM生成上下文相关的合成内容替换部分数据字段；3) 使用图像修复技术擦除原文本并渲染新文本，保持原始布局和字体特征。

Result: 生成了一对输出：视觉上逼真的新发票图像和完美对齐的结构化数据文件（JSON），为训练更鲁棒和准确的文档智能模型提供了可扩展的自动化解决方案。

Conclusion: 本文提出了一种新颖的合成发票文档生成方法，通过结合OCR、LLM和图像修复技术，能够高效生成高保真的合成发票及其结构化数据，解决了数据隐私和标注成本的问题。

Abstract: The performance of machine learning models for automated invoice processing
is critically dependent on large-scale, diverse datasets. However, the
acquisition of such datasets is often constrained by privacy regulations and
the high cost of manual annotation. To address this, we present a novel
pipeline for generating high-fidelity, synthetic invoice documents and their
corresponding structured data. Our method first utilizes Optical Character
Recognition (OCR) to extract the text content and precise spatial layout from a
source invoice. Select data fields are then replaced with contextually
realistic, synthetic content generated by a large language model (LLM).
Finally, we employ an inpainting technique to erase the original text from the
image and render the new, synthetic text in its place, preserving the exact
layout and font characteristics. This process yields a pair of outputs: a
visually realistic new invoice image and a perfectly aligned structured data
file (JSON) reflecting the synthetic content. Our approach provides a scalable
and automated solution to amplify small, private datasets, enabling the
creation of large, varied corpora for training more robust and accurate
document intelligence models.

</details>


### [15] [Refine-IQA: Multi-Stage Reinforcement Finetuning for Perceptual Image Quality Assessment](https://arxiv.org/abs/2508.03763)
*Ziheng Jia,Jiaying Qian,Zicheng Zhang,Zijian Chen,Xiongkuo Min*

Main category: cs.CV

TL;DR: Refine-IQA通过多阶段RFT框架增强视觉质量感知和监督‘思考’过程，显著提升了IQA任务的性能和质量解释能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于RFT的IQA方法通常使用基于规则的输出奖励验证模型的输出，但未对‘思考’过程提供奖励监督，且直接在IQA任务上微调，未明确增强模型的低层视觉质量感知能力，这可能限制其性能上限。

Method: 提出多阶段RFT IQA框架（Refine-IQA），包括构建Refine-Perception-20K数据集和设计多任务奖励函数以增强模型的视觉质量感知（阶段1），以及针对质量评分任务引入概率差异奖励策略以监督‘思考’过程（阶段2）。

Result: Refine-IQA系列模型在感知和评分任务上表现优异，并激活了强大的‘思考’能力，在质量解释基准测试中也取得突出成绩。

Conclusion: Refine-IQA系列模型在感知和评分任务上表现出色，并激活了强大的‘思考’（质量解释）能力，在相应的质量解释基准测试中也取得了优异结果。

Abstract: Reinforcement fine-tuning (RFT) is a proliferating paradigm for LMM training.
Analogous to high-level reasoning tasks, RFT is similarly applicable to
low-level vision domains, including image quality assessment (IQA). Existing
RFT-based IQA methods typically use rule-based output rewards to verify the
model's rollouts but provide no reward supervision for the "think" process,
leaving its correctness and efficacy uncontrolled. Furthermore, these methods
typically fine-tune directly on downstream IQA tasks without explicitly
enhancing the model's native low-level visual quality perception, which may
constrain its performance upper bound. In response to these gaps, we propose
the multi-stage RFT IQA framework (Refine-IQA). In Stage-1, we build the
Refine-Perception-20K dataset (with 12 main distortions, 20,907
locally-distorted images, and over 55K RFT samples) and design multi-task
reward functions to strengthen the model's visual quality perception. In
Stage-2, targeting the quality scoring task, we introduce a probability
difference reward involved strategy for "think" process supervision. The
resulting Refine-IQA Series Models achieve outstanding performance on both
perception and scoring tasks-and, notably, our paradigm activates a robust
"think" (quality interpreting) capability that also attains exceptional results
on the corresponding quality interpreting benchmark.

</details>


### [16] [4D-PreNet: A Unified Preprocessing Framework for 4D-STEM Data Analysis](https://arxiv.org/abs/2508.03775)
*Mingyu Liu,Zian Mao,Zhu Liu,Haoran Zhang,Jintao Guo,Xiaoya He,Xi Huang,Shufen Chu,Chun Cheng,Jun Ding,Yujun Xie*

Main category: cs.CV

TL;DR: 4D-PreNet是一种基于深度学习的端到端预处理框架，通过联合去噪、中心校正和椭圆畸变校准，显著提升了4D-STEM数据的分析质量。


<details>
  <summary>Details</summary>
Motivation: 传统校正算法通常局限于特定材料且缺乏通用性，无法满足4D-STEM高通量数据采集的需求。

Method: 采用端到端的深度学习框架，结合注意力增强的U-Net和ResNet架构，进行去噪、中心校正和椭圆畸变校准。

Result: 4D-PreNet在去噪任务中将均方误差降低了50%，在中心检测任务中实现了亚像素级精度（平均误差低于0.04像素），显著优于传统算法。

Conclusion: 4D-PreNet 提供了一种通用的深度学习解决方案，有效解决了4D-STEM数据预处理中的噪声、中心漂移和椭圆畸变问题，显著提升了实时分析的可靠性和效率。

Abstract: Automated experimentation with real time data analysis in scanning
transmission electron microscopy (STEM) often require end-to-end framework. The
four-dimensional scanning transmission electron microscopy (4D-STEM) with
high-throughput data acquisition has been constrained by the critical
bottleneck results from data preprocessing. Pervasive noise, beam center drift,
and elliptical distortions during high-throughput acquisition inevitably
corrupt diffraction patterns, systematically biasing quantitative measurements.
Yet, conventional correction algorithms are often material-specific and fail to
provide a robust, generalizable solution. In this work, we present 4D-PreNet,
an end-to-end deep-learning pipeline that integrates attention-enhanced U-Net
and ResNet architectures to simultaneously perform denoising, center
correction, and elliptical distortion calibration. The network is trained on
large, simulated datasets encompassing a wide range of noise levels, drift
magnitudes, and distortion types, enabling it to generalize effectively to
experimental data acquired under varying conditions. Quantitative evaluations
demonstrate that our pipeline reduces mean squared error by up to 50% during
denoising and achieves sub-pixel center localization in the center detection
task, with average errors below 0.04 pixels. The outputs are bench-marked
against traditional algorithms, highlighting improvements in both noise
suppression and restoration of diffraction patterns, thereby facilitating
high-throughput, reliable 4D-STEM real-time analysis for automated
characterization.

</details>


### [17] [HPSv3: Towards Wide-Spectrum Human Preference Score](https://arxiv.org/abs/2508.03789)
*Yuhang Ma,Xiaoshi Wu,Keqiang Sun,Hongsheng Li*

Main category: cs.CV

TL;DR: HPSv3 是一个新的文本到图像生成模型评估指标，通过大规模数据集和优化方法显著提升了评估效果和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型评估指标存在数据覆盖有限、特征提取不优和损失函数效率低的问题，亟需一种更符合人类感知的评估方法。

Method: 1. 发布了 HPDv3 数据集，包含 1.08M 文本-图像对和 1.17M 标注的成对比较。2. 提出了基于 VLM 的偏好模型，使用不确定性感知的排序损失进行细粒度排名。3. 提出了 Chain-of-Human-Preference (CoHP) 方法，通过迭代优化提升图像质量。

Result: HPSv3 在广泛实验中表现出色，能够作为广泛的图像评估指标，而 CoHP 方法显著提升了生成图像的质量。

Conclusion: HPSv3 作为一个强大的图像评估指标，能够广泛覆盖不同质量的图像，而 CoHP 提供了一种无需额外数据即可提升生成图像质量的高效方法。

Abstract: Evaluating text-to-image generation models requires alignment with human
perception, yet existing human-centric metrics are constrained by limited data
coverage, suboptimal feature extraction, and inefficient loss functions. To
address these challenges, we introduce Human Preference Score v3 (HPSv3). (1)
We release HPDv3, the first wide-spectrum human preference dataset integrating
1.08M text-image pairs and 1.17M annotated pairwise comparisons from
state-of-the-art generative models and low to high-quality real-world images.
(2) We introduce a VLM-based preference model trained using an
uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose
Chain-of-Human-Preference (CoHP), an iterative image refinement method that
enhances quality without extra data, using HPSv3 to select the best image at
each step. Extensive experiments demonstrate that HPSv3 serves as a robust
metric for wide-spectrum image evaluation, and CoHP offers an efficient and
human-aligned approach to improve image generation quality. The code and
dataset are available at the HPSv3 Homepage.

</details>


### [18] [RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor Graph Optimization](https://arxiv.org/abs/2508.04335)
*Yanyan Li,Ze Yang,Keisuke Tateno,Federico Tombari Liang Zhao,Gim Hee Lee*

Main category: cs.CV

TL;DR: RiemanLine是一种基于黎曼流形的3D线统一最小表示方法，通过解耦全局和局部组件，显著提升了姿态估计和线重建的精度与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D线表示方法主要处理独立线，忽视了人造环境中普遍存在的结构规律性（如平行线组）。因此，需要一种统一的表示方法，既能处理独立线，又能高效编码平行线组。

Method: 提出了一种基于黎曼流形的统一最小表示方法RiemanLine，将线地标解耦为全局共享的消失方向和局部缩放的正常向量，减少了参数空间，并自然地嵌入了并行性。该方法进一步集成到因子图框架中，实现了全局方向对齐和局部重投影优化。

Result: 在ICL-NUIM、TartanAir和合成基准测试中，RiemanLine显著提高了姿态估计和线重建的准确性，同时减少了参数维度并改善了收敛稳定性。

Conclusion: RiemanLine通过将3D线解耦为全局和局部组件，在保持参数最小化的同时，显著提高了姿态估计和线重建的准确性，并改善了收敛稳定性。

Abstract: Minimal parametrization of 3D lines plays a critical role in camera
localization and structural mapping. Existing representations in robotics and
computer vision predominantly handle independent lines, overlooking structural
regularities such as sets of parallel lines that are pervasive in man-made
environments. This paper introduces \textbf{RiemanLine}, a unified minimal
representation for 3D lines formulated on Riemannian manifolds that jointly
accommodates both individual lines and parallel-line groups. Our key idea is to
decouple each line landmark into global and local components: a shared
vanishing direction optimized on the unit sphere $\mathcal{S}^2$, and scaled
normal vectors constrained on orthogonal subspaces, enabling compact encoding
of structural regularities. For $n$ parallel lines, the proposed representation
reduces the parameter space from $4n$ (orthonormal form) to $2n+2$, naturally
embedding parallelism without explicit constraints. We further integrate this
parameterization into a factor graph framework, allowing global direction
alignment and local reprojection optimization within a unified manifold-based
bundle adjustment. Extensive experiments on ICL-NUIM, TartanAir, and synthetic
benchmarks demonstrate that our method achieves significantly more accurate
pose estimation and line reconstruction, while reducing parameter
dimensionality and improving convergence stability.

</details>


### [19] [Deep learning framework for crater detection and identification on the Moon and Mars](https://arxiv.org/abs/2508.03920)
*Yihan Ma,Zeyang Yu,Rohitash Chandra*

Main category: cs.CV

TL;DR: 该论文利用深度学习模型（CNN、YOLO、ResNet）提出两阶段框架，用于陨石坑检测和识别，结果显示YOLO检测性能均衡，ResNet-50在大型陨石坑识别中表现优异。


<details>
  <summary>Details</summary>
Motivation: 陨石坑的空间分布和形态特征对行星表面组成、地质历史和撞击过程研究至关重要，深度学习模型的快速发展促进了自动化陨石坑检测的兴趣。

Method: 采用两阶段方法：第一阶段使用经典CNN、ResNet-50和YOLO进行陨石坑识别；第二阶段使用YOLO进行陨石坑定位。

Result: YOLO在陨石坑检测中表现均衡，ResNet-50在大型陨石坑识别中精度更高。

Conclusion: YOLO展示了最均衡的陨石坑检测性能，而ResNet-50在识别大型陨石坑时表现出高精度。

Abstract: Impact craters are among the most prominent geomorphological features on
planetary surfaces and are of substantial significance in planetary science
research. Their spatial distribution and morphological characteristics provide
critical information on planetary surface composition, geological history, and
impact processes. In recent years, the rapid advancement of deep learning
models has fostered significant interest in automated crater detection. In this
paper, we apply advancements in deep learning models for impact crater
detection and identification. We use novel models, including Convolutional
Neural Networks (CNNs) and variants such as YOLO and ResNet. We present a
framework that features a two-stage approach where the first stage features
crater identification using simple classic CNN, ResNet-50 and YOLO. In the
second stage, our framework employs YOLO-based detection for crater
localisation. Therefore, we detect and identify different types of craters and
present a summary report with remote sensing data for a selected region. We
consider selected regions for craters and identification from Mars and the Moon
based on remote sensing data. Our results indicate that YOLO demonstrates the
most balanced crater detection performance, while ResNet-50 excels in
identifying large craters with high precision.

</details>


### [20] [OmniDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment](https://arxiv.org/abs/2508.04611)
*Tongfan Guan,Jiaxin Guo,Chen Wang,Yun-Hui Liu*

Main category: cs.CV

TL;DR: OmniDepth通过统一单目与立体深度估计，利用双向对齐机制动态优化表示，显著提升深度估计精度，尤其在复杂场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 单目和立体深度估计各有优劣，但实践中二者常独立使用。OmniDepth旨在通过统一框架结合两者的优势，克服各自的局限性。

Method: OmniDepth采用迭代双向对齐机制，通过跨注意力动态同步单目上下文线索与立体假设表示，实现单目与立体表示的相互优化。

Result: OmniDepth在Middlebury和ETH3D数据集上零样本泛化误差降低超过40%，并在透明和反射表面等传统难题上取得显著改进。

Conclusion: OmniDepth通过统一框架结合单目和立体深度估计的优势，显著提升了零样本泛化能力，特别是在透明和反射表面等传统难题上表现突出。

Abstract: Monocular and stereo depth estimation offer complementary strengths:
monocular methods capture rich contextual priors but lack geometric precision,
while stereo approaches leverage epipolar geometry yet struggle with
ambiguities such as reflective or textureless surfaces. Despite post-hoc
synergies, these paradigms remain largely disjoint in practice. We introduce
OmniDepth, a unified framework that bridges both through iterative
bidirectional alignment of their latent representations. At its core, a novel
cross-attentive alignment mechanism dynamically synchronizes monocular
contextual cues with stereo hypothesis representations during stereo reasoning.
This mutual alignment resolves stereo ambiguities (e.g., specular surfaces) by
injecting monocular structure priors while refining monocular depth with stereo
geometry within a single network. Extensive experiments demonstrate
state-of-the-art results: \textbf{OmniDepth reduces zero-shot generalization
error by $\!>\!40\%$ on Middlebury and ETH3D}, while addressing longstanding
failures on transparent and reflective surfaces. By harmonizing multi-view
geometry with monocular context, OmniDepth enables robust 3D perception that
transcends modality-specific limitations. Codes available at
https://github.com/aeolusguan/OmniDepth.

</details>


### [21] [Point-Based Shape Representation Generation with a Correspondence-Preserving Diffusion Model](https://arxiv.org/abs/2508.03925)
*Shen Zhu,Yinzhu Jin,Ifrah Zawar,P. Thomas Fletcher*

Main category: cs.CV

TL;DR: 提出了一种扩散模型，用于生成具有点对应关系的点云形状，利用OASIS-3数据验证其有效性，并在下游任务中展示了应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法主要关注无序点云生成，忽略了点对应关系，而传统统计形状模型则关注点对应关系。本研究旨在填补这一空白，生成具有点对应关系的现实形状。

Method: 研究采用了一种扩散模型，专注于生成具有点对应关系的点云形状，利用OASIS-3数据集中的对应关系数据进行训练和验证。

Result: 模型生成的基于点的海马形状表示在现实性上优于现有方法，并在条件生成和反事实生成等下游任务中表现出色。

Conclusion: 该研究成功提出了一种能够生成具有点对应关系的点云形状的扩散模型，并在下游任务中展示了其应用潜力。

Abstract: We propose a diffusion model designed to generate point-based shape
representations with correspondences. Traditional statistical shape models have
considered point correspondences extensively, but current deep learning methods
do not take them into account, focusing on unordered point clouds instead.
Current deep generative models for point clouds do not address generating
shapes with point correspondences between generated shapes. This work aims to
formulate a diffusion model that is capable of generating realistic point-based
shape representations, which preserve point correspondences that are present in
the training data. Using shape representation data with correspondences derived
from Open Access Series of Imaging Studies 3 (OASIS-3), we demonstrate that our
correspondence-preserving model effectively generates point-based hippocampal
shape representations that are highly realistic compared to existing methods.
We further demonstrate the applications of our generative model by downstream
tasks, such as conditional generation of healthy and AD subjects and predicting
morphological changes of disease progression by counterfactual generation.

</details>


### [22] [Policy to Assist Iteratively Local Segmentation: Optimising Modality and Location Selection for Prostate Cancer Localisation](https://arxiv.org/abs/2508.03953)
*Xiangcen Wu,Shaheer U. Saeed,Yipei Wang,Ester Bonmati Coll,Yipeng Hu*

Main category: cs.CV

TL;DR: 提出了一种策略网络驱动的推荐系统，动态选择最佳成像模态和区域以优化前列腺癌分割，实验显示其性能超越标准方法，并可能辅助放射科医生。


<details>
  <summary>Details</summary>
Motivation: 放射科医生在阅读医学图像时需混合多种策略，包括检查不同模态和局部区域。为提高前列腺癌分割性能，需开发一种系统推荐最佳图像部分和模态。

Method: 提出了一种推荐系统，通过策略网络动态选择最佳成像模态和局部区域，迭代优化分割性能。训练过程中，预训练的分割网络模拟放射科医生的检查过程。

Result: 在1325例多参数MRI数据集上验证，该方法显著提升了标注效率和分割准确性，尤其在复杂病理情况下表现优异。

Conclusion: 该方法通过训练策略网络推荐最佳成像模态和感兴趣区域，提升了前列腺癌分割的准确性和效率，甚至超越了标准分割网络。有趣的是，策略网络独立开发了最优策略，可能与现有放射科医生指南不一致，这为交互式应用提供了潜力。

Abstract: Radiologists often mix medical image reading strategies, including inspection
of individual modalities and local image regions, using information at
different locations from different images independently as well as
concurrently. In this paper, we propose a recommend system to assist machine
learning-based segmentation models, by suggesting appropriate image portions
along with the best modality, such that prostate cancer segmentation
performance can be maximised. Our approach trains a policy network that assists
tumor localisation, by recommending both the optimal imaging modality and the
specific sections of interest for review. During training, a pre-trained
segmentation network mimics radiologist inspection on individual or variable
combinations of these imaging modalities and their sections - selected by the
policy network. Taking the locally segmented regions as an input for the next
step, this dynamic decision making process iterates until all cancers are best
localised. We validate our method using a data set of 1325 labelled
multiparametric MRI images from prostate cancer patients, demonstrating its
potential to improve annotation efficiency and segmentation accuracy,
especially when challenging pathology is present. Experimental results show
that our approach can surpass standard segmentation networks. Perhaps more
interestingly, our trained agent independently developed its own optimal
strategy, which may or may not be consistent with current radiologist
guidelines such as PI-RADS. This observation also suggests a promising
interactive application, in which the proposed policy networks assist human
radiologists.

</details>


### [23] [Scaling Up Audio-Synchronized Visual Animation: An Efficient Training Paradigm](https://arxiv.org/abs/2508.03955)
*Lin Zhang,Zefan Cai,Yufan Zhou,Shentong Mo,Jinhong Lin,Cheng-En Wu,Yibing Wei,Yijing Zhang,Ruiyi Zhang,Wen Xiao,Tong Sun,Junjie Hu,Pedro Morgado*

Main category: cs.CV

TL;DR: 提出两阶段训练范式，利用噪声视频预训练和小规模高质量数据微调，减少手动标注需求，并通过多特征条件化和窗口注意力增强同步效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的手动标注高质量视频，难以扩展到开放世界中的多样化音频-视频类别。

Method: 采用两阶段训练范式：第一阶段利用大量噪声视频进行预训练，学习多样但不完美的音频-视频对齐；第二阶段在小规模高质量手动标注视频上微调模型。通过多特征条件化和窗口注意力机制增强同步效果，并利用预训练的文本-视频生成器和音频编码器，仅引入1.9%的可训练参数。

Result: 提出了AVSync48基准测试，覆盖48个类别，是之前基准的3倍多样性。实验显示，方法将手动标注需求减少了10倍以上，并能泛化到开放类别。

Conclusion: 本研究提出了一种高效的两阶段训练范式，显著减少了对高质量手动标注视频的依赖，同时通过多特征条件化和窗口注意力机制增强了音频-视频同步效果。实验表明，该方法在减少人工标注需求的同时，能够泛化到更多开放类别。

Abstract: Recent advances in audio-synchronized visual animation enable control of
video content using audios from specific classes. However, existing methods
rely heavily on expensive manual curation of high-quality, class-specific
training videos, posing challenges to scaling up to diverse audio-video classes
in the open world. In this work, we propose an efficient two-stage training
paradigm to scale up audio-synchronized visual animation using abundant but
noisy videos. In stage one, we automatically curate large-scale videos for
pretraining, allowing the model to learn diverse but imperfect audio-video
alignments. In stage two, we finetune the model on manually curated
high-quality examples, but only at a small scale, significantly reducing the
required human effort. We further enhance synchronization by allowing each
frame to access rich audio context via multi-feature conditioning and window
attention. To efficiently train the model, we leverage pretrained text-to-video
generator and audio encoders, introducing only 1.9\% additional trainable
parameters to learn audio-conditioning capability without compromising the
generator's prior knowledge. For evaluation, we introduce AVSync48, a benchmark
with videos from 48 classes, which is 3$\times$ more diverse than previous
benchmarks. Extensive experiments show that our method significantly reduces
reliance on manual curation by over 10$\times$, while generalizing to many open
classes.

</details>


### [24] [RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification](https://arxiv.org/abs/2508.03967)
*Mamadou Keita,Wassim Hamidouche,Hessen Bougueffa Eutamene,Abdelmalik Taleb-Ahmed,Abdenour Hadid*

Main category: cs.CV

TL;DR: RAVID是首个基于视觉检索增强生成（RAG）的AI生成图像检测框架，通过动态检索和视觉语言模型融合，实现了高准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法在泛化性和鲁棒性方面表现不佳，且多依赖低层伪影或模型特定特征。RAVID旨在通过视觉检索增强生成（RAG）技术解决这些问题。

Method: RAVID结合了微调的CLIP图像编码器（RAVID CLIP）和视觉语言模型（VLM），通过检索相关图像并融合查询图像生成丰富输入，以提升检测性能。

Result: 在UniversalFakeDetect基准测试中，RAVID以93.85%的平均准确率取得最佳性能，且在图像退化条件下（如高斯模糊和JPEG压缩）表现优于传统方法。

Conclusion: RAVID通过动态检索相关图像并结合视觉语言模型（VLM），显著提升了AI生成图像检测的准确性和鲁棒性，成为当前最先进的解决方案。

Abstract: In this paper, we introduce RAVID, the first framework for AI-generated image
detection that leverages visual retrieval-augmented generation (RAG). While RAG
methods have shown promise in mitigating factual inaccuracies in foundation
models, they have primarily focused on text, leaving visual knowledge
underexplored. Meanwhile, existing detection methods, which struggle with
generalization and robustness, often rely on low-level artifacts and
model-specific features, limiting their adaptability. To address this, RAVID
dynamically retrieves relevant images to enhance detection. Our approach
utilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced with
category-related prompts to improve representation learning. We further
integrate a vision-language model (VLM) to fuse retrieved images with the
query, enriching the input and improving accuracy. Given a query image, RAVID
generates an embedding using RAVID CLIP, retrieves the most relevant images
from a database, and combines these with the query image to form an enriched
input for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on the
UniversalFakeDetect benchmark, which covers 19 generative models, show that
RAVID achieves state-of-the-art performance with an average accuracy of 93.85%.
RAVID also outperforms traditional methods in terms of robustness, maintaining
high accuracy even under image degradations such as Gaussian blur and JPEG
compression. Specifically, RAVID achieves an average accuracy of 80.27% under
degradation conditions, compared to 63.44% for the state-of-the-art model
C2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEG
compression scenarios. The code will be publicly available upon acceptance.

</details>


### [25] [Investigating the Impact of Large-Scale Pre-training on Nutritional Content Estimation from 2D Images](https://arxiv.org/abs/2508.03996)
*Michele Andrade,Guilherme A. L. Silva,Valéria Santos,Gladston Moreira,Eduardo Luz*

Main category: cs.CV

TL;DR: 研究探讨了预训练数据集对2D图像营养估计模型性能的影响，发现专有数据集（JFT-300M）预训练的模型表现最佳，而大规模公共数据集（COYO）预训练的效果不如预期。


<details>
  <summary>Details</summary>
Motivation: 从2D图像估计食物营养含量是一个具有重要健康意义的任务，但由于食物呈现、光照变化以及缺乏深度信息，这一任务极具挑战性。此外，现有方法依赖专有数据集进行大规模预训练，影响了可复现性。

Method: 本研究通过微调和评估在ImageNet和COYO两个大型公共数据集上预训练的Vision Transformer（ViT）模型，与基线CNN模型（InceptionV2和ResNet-50）及在专有JFT-300M数据集上预训练的最新方法进行比较。

Result: 在Nutrition5k数据集上的实验表明，使用JFT-300M预训练的模型显著优于使用公共数据集预训练的模型。出乎意料的是，在COYO数据集上预训练的模型表现不如在ImageNet上预训练的模型。

Conclusion: 预训练数据集的特征（包括规模、领域相关性和质量）对于2D营养估计中的有效迁移学习至关重要。

Abstract: Estimating the nutritional content of food from images is a critical task
with significant implications for health and dietary monitoring. This is
challenging, especially when relying solely on 2D images, due to the
variability in food presentation, lighting, and the inherent difficulty in
inferring volume and mass without depth information. Furthermore,
reproducibility in this domain is hampered by the reliance of state-of-the-art
methods on proprietary datasets for large-scale pre-training. In this paper, we
investigate the impact of large-scale pre-training datasets on the performance
of deep learning models for nutritional estimation using only 2D images. We
fine-tune and evaluate Vision Transformer (ViT) models pre-trained on two large
public datasets, ImageNet and COYO, comparing their performance against
baseline CNN models (InceptionV2 and ResNet-50) and a state-of-the-art method
pre-trained on the proprietary JFT-300M dataset. We conduct extensive
experiments on the Nutrition5k dataset, a large-scale collection of real-world
food plates with high-precision nutritional annotations. Our evaluation using
Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAE%) reveals
that models pre-trained on JFT-300M significantly outperform those pre-trained
on public datasets. Unexpectedly, the model pre-trained on the massive COYO
dataset performs worse than the model pre-trained on ImageNet for this specific
regression task, refuting our initial hypothesis. Our analysis provides
quantitative evidence highlighting the critical role of pre-training dataset
characteristics, including scale, domain relevance, and curation quality, for
effective transfer learning in 2D nutritional estimation.

</details>


### [26] [JanusNet: Hierarchical Slice-Block Shuffle and Displacement for Semi-Supervised 3D Multi-Organ Segmentation](https://arxiv.org/abs/2508.03997)
*Zheng Zhang,Tianzhuzi Tan,Guanchun Yin,Bo Zhang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: JanusNet是一种3D医学数据增强框架，通过全局建模解剖连续性和局部关注难分割区域，显著提升弱监督分割性能。


<details>
  <summary>Details</summary>
Motivation: 针对3D医学图像数据增强中随机混合块破坏解剖连续性和结构一致性的问题，提出一种更符合人体解剖信息的数据增强方法。

Method: JanusNet采用双阶段框架：1) Slice-Block Shuffle步骤沿随机轴对齐打乱同索引切片块，保留解剖上下文；2) Confidence-Guided Displacement步骤利用预测可靠性替换块，增强难分割区域的信号。

Result: 在Synapse和AMOS数据集上的实验表明，JanusNet显著优于现有方法，例如在仅20%标注数据的Synapse数据集上DSC提升了4%。

Conclusion: JanusNet通过全局建模解剖连续性和局部关注难分割区域，显著提升了弱监督医学图像分割的性能，特别是在标注数据有限的情况下。

Abstract: Limited by the scarcity of training samples and annotations, weakly
supervised medical image segmentation often employs data augmentation to
increase data diversity, while randomly mixing volumetric blocks has
demonstrated strong performance. However, this approach disrupts the inherent
anatomical continuity of 3D medical images along orthogonal axes, leading to
severe structural inconsistencies and insufficient training in challenging
regions, such as small-sized organs, etc. To better comply with and utilize
human anatomical information, we propose JanusNet}, a data augmentation
framework for 3D medical data that globally models anatomical continuity while
locally focusing on hard-to-segment regions. Specifically, our Slice-Block
Shuffle step performs aligned shuffling of same-index slice blocks across
volumes along a random axis, while preserving the anatomical context on planes
perpendicular to the perturbation axis. Concurrently, the Confidence-Guided
Displacement step uses prediction reliability to replace blocks within each
slice, amplifying signals from difficult areas. This dual-stage, axis-aligned
framework is plug-and-play, requiring minimal code changes for most
teacher-student schemes. Extensive experiments on the Synapse and AMOS datasets
demonstrate that JanusNet significantly surpasses state-of-the-art methods,
achieving, for instance, a 4% DSC gain on the Synapse dataset with only 20%
labeled data.

</details>


### [27] [CAD-Judge: Toward Efficient Morphological Grading and Verification for Text-to-CAD Generation](https://arxiv.org/abs/2508.04002)
*Zheyuan Zhou,Jiayi Han,Liang Du,Naiyu Fang,Lemiao Qiu,Shuyou Zhang*

Main category: cs.CV

TL;DR: CAD-Judge提出了一种高效的Text-to-CAD奖励系统，通过编译器模块优化生成和验证，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统Text-to-CAD系统中渲染速度慢、VLM部署成本高及奖励机制可能导致系统性能下降的问题。

Method: 采用Compiler-as-a-Judge Module（CJM）作为快速直接的奖励信号，结合前景理论优化模型对齐；引入agentic CAD生成方法和Compiler-as-a-Review Module（CRM）验证并优化生成的CAD模型。

Result: 在具有挑战性的CAD数据集上的广泛实验表明，该方法实现了最先进的性能，同时保持了卓越的效率。

Conclusion: CAD-Judge通过引入可验证的奖励系统和编译器模块（CJM和CRM），显著提升了Text-to-CAD系统的效率和鲁棒性，并在实验中展示了先进的性能。

Abstract: Computer-Aided Design (CAD) models are widely used across industrial design,
simulation, and manufacturing processes. Text-to-CAD systems aim to generate
editable, general-purpose CAD models from textual descriptions, significantly
reducing the complexity and entry barrier associated with traditional CAD
workflows. However, rendering CAD models can be slow, and deploying VLMs to
review CAD models can be expensive and may introduce reward hacking that
degrades the systems. To address these challenges, we propose CAD-Judge, a
novel, verifiable reward system for efficient and effective CAD preference
grading and grammatical validation. We adopt the Compiler-as-a-Judge Module
(CJM) as a fast, direct reward signal, optimizing model alignment by maximizing
generative utility through prospect theory. To further improve the robustness
of Text-to-CAD in the testing phase, we introduce a simple yet effective
agentic CAD generation approach and adopt the Compiler-as-a-Review Module
(CRM), which efficiently verifies the generated CAD models, enabling the system
to refine them accordingly. Extensive experiments on challenging CAD datasets
demonstrate that our method achieves state-of-the-art performance while
maintaining superior efficiency.

</details>


### [28] [$\text{S}^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation](https://arxiv.org/abs/2508.04016)
*Weilun Feng,Haotong Qin,Chuanguang Yang,Xiangqi Li,Han Yang,Yuqi Li,Zhulin An,Libo Huang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: S²Q-VDiT 是一种针对视频扩散模型的后训练量化框架，通过显著数据选择和稀疏令牌蒸馏，在保持性能的同时显著压缩模型并加速推理。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型（V-DMs）的长令牌序列导致高校准方差和学习挑战，需要一种高效的量化方法来解决这些问题。

Method: 提出了一种基于显著数据和稀疏令牌蒸馏的后训练量化框架 S²Q-VDiT，包括 Hessian-aware Salient Data Selection 和 Attention-guided Sparse Token Distillation。

Result: 在 W4A6 量化下，S²Q-VDiT 实现了无损性能，同时显著减少了模型大小并加速了推理。

Conclusion: S²Q-VDiT 是一种后训练量化框架，能够在 W4A6 量化下实现无损性能，同时提供 3.9 倍的模型压缩和 1.3 倍的推理加速。

Abstract: Diffusion transformers have emerged as the mainstream paradigm for video
generation models. However, the use of up to billions of parameters incurs
significant computational costs. Quantization offers a promising solution by
reducing memory usage and accelerating inference. Nonetheless, we observe that
the joint modeling of spatial and temporal information in video diffusion
models (V-DMs) leads to extremely long token sequences, which introduces high
calibration variance and learning challenges. To address these issues, we
propose \textbf{$\text{S}^2$Q-VDiT}, a post-training quantization framework for
V-DMs that leverages \textbf{S}alient data and \textbf{S}parse token
distillation. During the calibration phase, we identify that quantization
performance is highly sensitive to the choice of calibration data. To mitigate
this, we introduce \textit{Hessian-aware Salient Data Selection}, which
constructs high-quality calibration datasets by considering both diffusion and
quantization characteristics unique to V-DMs. To tackle the learning
challenges, we further analyze the sparse attention patterns inherent in V-DMs.
Based on this observation, we propose \textit{Attention-guided Sparse Token
Distillation}, which exploits token-wise attention distributions to emphasize
tokens that are more influential to the model's output. Under W4A6
quantization, $\text{S}^2$Q-VDiT achieves lossless performance while delivering
$3.9\times$ model compression and $1.3\times$ inference acceleration. Code will
be available at
\href{https://github.com/wlfeng0509/s2q-vdit}{https://github.com/wlfeng0509/s2q-vdit}.

</details>


### [29] [Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability](https://arxiv.org/abs/2508.04017)
*Haiqi Yang,Jinzhe Li,Gengxu Li,Yi Chang,Yuan Wu*

Main category: cs.CV

TL;DR: 研究提出了ISEval框架评估LMMs检测错误输入的能力，发现模型在无指导时表现不佳，且错误类型和模态信任度影响性能，强调了增强模型主动验证输入的必要性。


<details>
  <summary>Details</summary>
Motivation: 探索大型多模态模型是否能主动检测和审查错误输入，填补现有研究的空白。

Method: 研究引入了输入审查能力评估框架（ISEval），包含七类有缺陷的前提和三个评估指标，并对十种先进LMMs进行了广泛评估。

Result: 大多数模型在无指导情况下难以主动检测有缺陷的文本前提，表现依赖于明确提示；错误类型影响性能，模型在识别逻辑谬误时表现较好，但在表面语言错误和某些条件缺陷上表现较差；模型对模态的信任度不同。

Conclusion: 该研究强调了增强大型多模态模型（LMMs）主动验证输入有效性的迫切需求，并提供了减轻该问题的新见解。

Abstract: Large Multimodal Models (LMMs) have witnessed remarkable growth, showcasing
formidable capabilities in handling intricate multimodal tasks with exceptional
performance. Recent research has underscored the inclination of large language
models to passively accept defective inputs, often resulting in futile
reasoning on invalid prompts. However, the same critical question of whether
LMMs can actively detect and scrutinize erroneous inputs still remains
unexplored. To address this gap, we introduce the Input Scrutiny Ability
Evaluation Framework (ISEval), which encompasses seven categories of flawed
premises and three evaluation metrics. Our extensive evaluation of ten advanced
LMMs has identified key findings. Most models struggle to actively detect
flawed textual premises without guidance, which reflects a strong reliance on
explicit prompts for premise error identification. Error type affects
performance: models excel at identifying logical fallacies but struggle with
surface-level linguistic errors and certain conditional flaws. Modality trust
varies-Gemini 2.5 pro and Claude Sonnet 4 balance visual and textual info,
while aya-vision-8b over-rely on text in conflicts. These insights underscore
the urgent need to enhance LMMs' proactive verification of input validity and
shed novel insights into mitigating the problem. The code is available at
https://github.com/MLGroupJLU/LMM_ISEval.

</details>


### [30] [Prototype-Driven Structure Synergy Network for Remote Sensing Images Segmentation](https://arxiv.org/abs/2508.04022)
*Junyi Wang,Jinjiang Li,Guodong Fan,Yakun Ju,Xiang Fang,Alex C. Kot*

Main category: cs.CV

TL;DR: PDSSNet通过类语义和空间结构协同设计，解决了遥感图像分割的高类内差异和类间相似性问题，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决遥感图像语义分割中因高类内差异和高类间相似性导致的传统方法分割不完整问题，并提出类语义和空间结构的协同设计是关键。

Method: 提出了Prototype-Driven Structure Synergy Network (PDSSNet)，包含三个关键模块：Adaptive Prototype Extraction Module (APEM)用于提取无偏类原型，Semantic-Structure Coordination Module (SSCM)遵循语义优先、结构约束的原则，Channel Similarity Adjustment Module (CSAM)通过动态步长调整机制聚焦类间判别性特征。

Result: PDSSNet在实验中表现优于现有最先进方法，验证了其有效性。

Conclusion: PDSSNet通过结合类语义和空间结构的协同设计，显著提升了遥感图像语义分割的精度和完整性，超越了现有最先进方法。

Abstract: In the semantic segmentation of remote sensing images, acquiring complete
ground objects is critical for achieving precise analysis. However, this task
is severely hindered by two major challenges: high intra-class variance and
high inter-class similarity. Traditional methods often yield incomplete
segmentation results due to their inability to effectively unify class
representations and distinguish between similar features. Even emerging
class-guided approaches are limited by coarse class prototype representations
and a neglect of target structural information.
  Therefore, this paper proposes a Prototype-Driven Structure Synergy Network
(PDSSNet). The design of this network is based on a core concept, a complete
ground object is jointly defined by its invariant class semantics and its
variant spatial structure. To implement this, we have designed three key
modules. First, the Adaptive Prototype Extraction Module (APEM) ensures
semantic accuracy from the source by encoding the ground truth to extract
unbiased class prototypes. Subsequently, the designed Semantic-Structure
Coordination Module (SSCM) follows a hierarchical semantics-first,
structure-second principle. This involves first establishing a global semantic
cognition, then leveraging structural information to constrain and refine the
semantic representation, thereby ensuring the integrity of class information.
Finally, the Channel Similarity Adjustment Module (CSAM) employs a dynamic
step-size adjustment mechanism to focus on discriminative features between
classes.
  Extensive experiments demonstrate that PDSSNet outperforms state-of-the-art
methods. The source code is available at
https://github.com/wangjunyi-1/PDSSNet.

</details>


### [31] [Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval](https://arxiv.org/abs/2508.04028)
*Yifan Wang,Tao Wang,Chenwei Tang,Caiyang Yu,Zhengqing Zang,Mengmi Zhang,Shudong Huang,Jiancheng Lv*

Main category: cs.CV

TL;DR: DCAR是一种双重提示学习框架，通过动态调整提示向量和联合优化属性与类别特征，显著提升了下游图像-文本检索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的提示学习方法在图像分类等任务中表现优异，但在下游图像-文本检索(ITR)任务中面临挑战，尤其是在区分细粒度属性和相似子类别时。

Method: 提出了Dual prompt Learning with Joint Category-Attribute Reweighting (DCAR)框架，动态调整语义和视觉维度的提示向量，联合优化属性和类别特征。具体包括：属性级别基于文本-图像互信息相关动态更新属性描述权重；类别级别引入多视角负样本并通过类别匹配加权学习子类别区分。

Result: 在构建的Fine-class Described Retrieval Dataset (FDRD)上，DCAR表现优于现有基线，实现了最先进的性能。

Conclusion: DCAR框架通过双重提示学习和联合类别-属性重加权，显著提升了CLIP在下游ITR任务中的性能，并在FDRD数据集上达到了最先进的水平。

Abstract: Recently, prompt learning has demonstrated remarkable success in adapting
pre-trained Vision-Language Models (VLMs) to various downstream tasks such as
image classification. However, its application to the downstream Image-Text
Retrieval (ITR) task is more challenging. We find that the challenge lies in
discriminating both fine-grained attributes and similar subcategories of the
downstream data. To address this challenge, we propose Dual prompt Learning
with Joint Category-Attribute Reweighting (DCAR), a novel dual-prompt learning
framework to achieve precise image-text matching. The framework dynamically
adjusts prompt vectors from both semantic and visual dimensions to improve the
performance of CLIP on the downstream ITR task. Based on the prompt paradigm,
DCAR jointly optimizes attribute and class features to enhance fine-grained
representation learning. Specifically, (1) at the attribute level, it
dynamically updates the weights of attribute descriptions based on text-image
mutual information correlation; (2) at the category level, it introduces
negative samples from multiple perspectives with category-matching weighting to
learn subcategory distinctions. To validate our method, we construct the
Fine-class Described Retrieval Dataset (FDRD), which serves as a challenging
benchmark for ITR in downstream data domains. It covers over 1,500 downstream
fine categories and 230,000 image-caption pairs with detailed attribute
annotations. Extensive experiments on FDRD demonstrate that DCAR achieves
state-of-the-art performance over existing baselines.

</details>


### [32] [Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation](https://arxiv.org/abs/2508.04033)
*Hee-Yeun Kim,Byeonggyu Park,Byonghyok Choi,Hansang Cho,Byungkwan Kim,Soomok Lee,Mingu Jeon,Seung-Woo Seo,Seong-Woo Kim*

Main category: cs.CV

TL;DR: 论文提出了一种结合单目摄像头和2D雷达点云的NLoS行人定位方法，通过实时检测停放车辆和优化空间推理，有效提升了行人检测准确性，改善了道路安全。


<details>
  <summary>Details</summary>
Motivation: 解决因路边停车导致的NLoS盲区问题，尤其是行人突然从停放车辆间出现的情况，现有方法依赖预设空间信息或简单墙面反射假设，缺乏实时性和普适性。

Method: 该方法首先通过图像分割检测停放车辆，利用深度估计推断空间特征，再结合2D雷达点云数据优化空间推理，实现精确的行人定位。

Result: 实验结果表明，该方法在真实城市道路环境中显著提升了早期行人检测能力。

Conclusion: 论文提出的融合单目摄像头图像与2D雷达点云数据的NLoS行人定位框架，有效提升了城市道路环境中早期行人检测的准确性，从而改善了道路安全。

Abstract: The presence of Non-Line-of-Sight (NLoS) blind spots resulting from roadside
parking in urban environments poses a significant challenge to road safety,
particularly due to the sudden emergence of pedestrians. mmWave technology
leverages diffraction and reflection to observe NLoS regions, and recent
studies have demonstrated its potential for detecting obscured objects.
However, existing approaches predominantly rely on predefined spatial
information or assume simple wall reflections, thereby limiting their
generalizability and practical applicability. A particular challenge arises in
scenarios where pedestrians suddenly appear from between parked vehicles, as
these parked vehicles act as temporary spatial obstructions. Furthermore, since
parked vehicles are dynamic and may relocate over time, spatial information
obtained from satellite maps or other predefined sources may not accurately
reflect real-time road conditions, leading to erroneous sensor interpretations.
To address this limitation, we propose an NLoS pedestrian localization
framework that integrates monocular camera image with 2D radar point cloud
(PCD) data. The proposed method initially detects parked vehicles through image
segmentation, estimates depth to infer approximate spatial characteristics, and
subsequently refines this information using 2D radar PCD to achieve precise
spatial inference. Experimental evaluations conducted in real-world urban road
environments demonstrate that the proposed approach enhances early pedestrian
detection and contributes to improved road safety. Supplementary materials are
available at https://hiyeun.github.io/NLoS/.

</details>


### [33] [CORE-ReID V2: Advancing the Domain Adaptation for Object Re-Identification with Optimized Training and Ensemble Fusion](https://arxiv.org/abs/2508.04036)
*Trinh Quoc Nguyen,Oky Dicky Ardiansyah Prima,Syahid Al Irfan,Hindriyanto Dwi Purnomo,Radius Tanone*

Main category: cs.CV

TL;DR: CORE-ReID V2通过CycleGAN和集成融合机制（ECAB/SECAB）提升无监督域适应在ReID任务中的表现，支持轻量级骨干网络，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决无监督域适应（UDA）在Person ReID和Vehicle ReID中的挑战，并扩展至Object ReID。

Method: 在预训练阶段使用CycleGAN合成多样化数据，弥合不同领域间的图像特征差距；在微调阶段采用先进的集成融合机制（ECAB和SECAB），增强局部和全局特征表示，同时减少目标样本伪标签的模糊性。

Result: 在广泛使用的UDA Person ReID和Vehicle ReID数据集上，该框架在Mean Average Precision (mAP)和Rank-k Accuracy (Top-1, Top-5, Top-10)方面优于现有最优方法。

Conclusion: CORE-ReID V2不仅推动了基于UDA的Object ReID的边界，还为该领域的进一步研究和进步提供了坚实基础。

Abstract: This study presents CORE-ReID V2, an enhanced framework building upon
CORE-ReID. The new framework extends its predecessor by addressing Unsupervised
Domain Adaptation (UDA) challenges in Person ReID and Vehicle ReID, with
further applicability to Object ReID. During pre-training, CycleGAN is employed
to synthesize diverse data, bridging image characteristic gaps across different
domains. In the fine-tuning, an advanced ensemble fusion mechanism, consisting
of the Efficient Channel Attention Block (ECAB) and the Simplified Efficient
Channel Attention Block (SECAB), enhances both local and global feature
representations while reducing ambiguity in pseudo-labels for target samples.
Experimental results on widely used UDA Person ReID and Vehicle ReID datasets
demonstrate that the proposed framework outperforms state-of-the-art methods,
achieving top performance in Mean Average Precision (mAP) and Rank-k Accuracy
(Top-1, Top-5, Top-10). Moreover, the framework supports lightweight backbones
such as ResNet18 and ResNet34, ensuring both scalability and efficiency. Our
work not only pushes the boundaries of UDA-based Object ReID but also provides
a solid foundation for further research and advancements in this domain. Our
codes and models are available at
https://github.com/TrinhQuocNguyen/CORE-ReID-V2.

</details>


### [34] [ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds](https://arxiv.org/abs/2506.16991)
*Binbin Xiang,Maciej Wielgosz,Stefano Puliti,Kamil Král,Martin Krůček,Azim Missarov,Rasmus Astrup*

Main category: cs.CV

TL;DR: ForestFormer3D是一个新的端到端框架，用于精确的个体树和语义分割，通过新组件实现了最先进的性能，并在不同条件下表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理自然森林环境的复杂性和变异性时存在困难，需要一种更精确的个体树和语义分割方法。

Method: ForestFormer3D结合了ISA引导的查询点选择、基于分数的块合并策略和一对多关联机制，形成了一个统一的端到端框架。

Result: 模型在FOR-instanceV2数据集上实现了最先进的性能，并在未见过的测试集（Wytham woods和LAUTx）上表现良好。

Conclusion: ForestFormer3D在FOR-instanceV2数据集上实现了最先进的个体树分割性能，并在未见过的测试集上表现出良好的泛化能力，展示了其在不同森林条件和传感器模态下的鲁棒性。

Abstract: The segmentation of forest LiDAR 3D point clouds, including both individual
tree and semantic segmentation, is fundamental for advancing forest management
and ecological research. However, current approaches often struggle with the
complexity and variability of natural forest environments. We present
ForestFormer3D, a new unified and end-to-end framework designed for precise
individual tree and semantic segmentation. ForestFormer3D incorporates
ISA-guided query point selection, a score-based block merging strategy during
inference, and a one-to-many association mechanism for effective training. By
combining these new components, our model achieves state-of-the-art performance
for individual tree segmentation on the newly introduced FOR-instanceV2
dataset, which spans diverse forest types and regions. Additionally,
ForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx),
showcasing its robustness across different forest conditions and sensor
modalities. The FOR-instanceV2 dataset and the ForestFormer3D code are publicly
available at https://bxiang233.github.io/FF3D/.

</details>


### [35] [SPJFNet: Self-Mining Prior-Guided Joint Frequency Enhancement for Ultra-Efficient Dark Image Restoration](https://arxiv.org/abs/2508.04041)
*Tongshun Zhang,Pingling Liu,Zijian Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: SPJFNet通过自挖掘先验和双频率处理，高效解决了暗图像恢复的效率瓶颈，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有暗图像恢复方法存在效率瓶颈，主要源于外部先验依赖、多阶段增强管道的冗余操作以及频率域方法的全局计算需求。

Method: 提出了一种自挖掘指导模块（SMGM）和双频率指导框架（DFGF），结合无损小波分解和联合傅里叶增强，优化了频率域处理。

Result: SPJFNet在多个基准测试中超越了现有技术，同时显著提升了效率，减少了参数和计算复杂度。

Conclusion: SPJFNet通过自挖掘先验引导和双频率指导框架，显著提高了暗图像恢复的效率和性能，减少了模型复杂度和计算开销。

Abstract: Current dark image restoration methods suffer from severe efficiency
bottlenecks, primarily stemming from: (1) computational burden and error
correction costs associated with reliance on external priors (manual or
cross-modal); (2) redundant operations in complex multi-stage enhancement
pipelines; and (3) indiscriminate processing across frequency components in
frequency-domain methods, leading to excessive global computational demands. To
address these challenges, we propose an Efficient Self-Mining Prior-Guided
Joint Frequency Enhancement Network (SPJFNet). Specifically, we first introduce
a Self-Mining Guidance Module (SMGM) that generates lightweight endogenous
guidance directly from the network, eliminating dependence on external priors
and thereby bypassing error correction overhead while improving inference
speed. Second, through meticulous analysis of different frequency domain
characteristics, we reconstruct and compress multi-level operation chains into
a single efficient operation via lossless wavelet decomposition and joint
Fourier-based advantageous frequency enhancement, significantly reducing
parameters. Building upon this foundation, we propose a Dual-Frequency Guidance
Framework (DFGF) that strategically deploys specialized high/low frequency
branches (wavelet-domain high-frequency enhancement and Fourier-domain
low-frequency restoration), decoupling frequency processing to substantially
reduce computational complexity. Rigorous evaluation across multiple benchmarks
demonstrates that SPJFNet not only surpasses state-of-the-art performance but
also achieves significant efficiency improvements, substantially reducing model
complexity and computational overhead. Code is available at
https://github.com/bywlzts/SPJFNet.

</details>


### [36] [VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning](https://arxiv.org/abs/2508.04043)
*Yuheng Ji,Yipu Wang,Yuyang Liu,Xiaoshuai Hao,Yue Liu,Yuting Zhao,Huaihai Lyu,Xiaolong Zheng*

Main category: cs.CV

TL;DR: VisualTrans是一个针对真实世界人机交互场景设计的视觉转换推理基准测试，揭示了当前模型在动态推理中的不足，为未来研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 解决现有基准测试在仿真与现实差距、任务复杂性和推理覆盖范围上的不足，为真实世界的人机交互场景提供更全面的评估工具。

Method: 通过构建一个包含12种语义多样化操作任务的数据集，系统评估空间、程序和定量三个关键推理维度，并采用可扩展的数据构建流程，包括任务选择、图像对提取、自动化元数据注释和结构化问题生成。

Result: 评估显示，现有模型在静态空间任务中表现良好，但在动态、多步推理场景中存在显著不足，特别是在中间状态识别和转换序列规划方面。

Conclusion: VisualTrans基准测试揭示了现有视觉语言模型在动态、多步推理场景中的不足，特别是在中间状态识别和转换序列规划方面，为未来研究提供了明确方向。

Abstract: Visual transformation reasoning (VTR) is a vital cognitive capability that
empowers intelligent agents to understand dynamic scenes, model causal
relationships, and predict future states, and thereby guiding actions and
laying the foundation for advanced intelligent systems. However, existing
benchmarks suffer from a sim-to-real gap, limited task complexity, and
incomplete reasoning coverage, limiting their practical use in real-world
scenarios. To address these limitations, we introduce VisualTrans, the first
comprehensive benchmark specifically designed for VTR in real-world
human-object interaction scenarios. VisualTrans encompasses 12 semantically
diverse manipulation tasks and systematically evaluates three essential
reasoning dimensions - spatial, procedural, and quantitative - through 6
well-defined subtask types. The benchmark features 472 high-quality
question-answer pairs in various formats, including multiple-choice, open-ended
counting, and target enumeration. We introduce a scalable data construction
pipeline built upon first-person manipulation videos, which integrates task
selection, image pair extraction, automated metadata annotation with large
multimodal models, and structured question generation. Human verification
ensures the final benchmark is both high-quality and interpretable. Evaluations
of various state-of-the-art vision-language models show strong performance in
static spatial tasks. However, they reveal notable shortcomings in dynamic,
multi-step reasoning scenarios, particularly in areas like intermediate state
recognition and transformation sequence planning. These findings highlight
fundamental weaknesses in temporal modeling and causal reasoning, providing
clear directions for future research aimed at developing more capable and
generalizable VTR systems. The dataset and code are available at
https://github.com/WangYipu2002/VisualTrans.

</details>


### [37] [Iterative pseudo-labeling based adaptive copy-paste supervision for semi-supervised tumor segmentation](https://arxiv.org/abs/2508.04044)
*Qiangguo Jin,Hui Cui,Junbo Wang,Changming Sun,Yimiao He,Ping Xuan,Linlin Wang,Cong Cong,Leyi Wei,Ran Su*

Main category: cs.CV

TL;DR: IPA-CP是一种结合自适应增强和迭代伪标签的SSL方法，显著提升小体积肿瘤CT分割效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有SSL研究忽视小体积肿瘤分割及数据增强策略在标记和未标记数据中潜力未充分探索的问题。

Method: 提出了一种名为IPA-CP的方法，结合了双向不确定性自适应增强机制和迭代伪标签转换策略。

Result: 在内部和公共数据集上的实验表明，IPA-CP优于当前最先进的SSL方法。

Conclusion: IPA-CP框架在医学图像分割中表现优于现有SSL方法，技术贡献的有效性通过消融研究得到验证。

Abstract: Semi-supervised learning (SSL) has attracted considerable attention in
medical image processing. The latest SSL methods use a combination of
consistency regularization and pseudo-labeling to achieve remarkable success.
However, most existing SSL studies focus on segmenting large organs, neglecting
the challenging scenarios where there are numerous tumors or tumors of small
volume. Furthermore, the extensive capabilities of data augmentation
strategies, particularly in the context of both labeled and unlabeled data,
have yet to be thoroughly investigated. To tackle these challenges, we
introduce a straightforward yet effective approach, termed iterative
pseudo-labeling based adaptive copy-paste supervision (IPA-CP), for tumor
segmentation in CT scans. IPA-CP incorporates a two-way uncertainty based
adaptive augmentation mechanism, aiming to inject tumor uncertainties present
in the mean teacher architecture into adaptive augmentation. Additionally,
IPA-CP employs an iterative pseudo-label transition strategy to generate more
robust and informative pseudo labels for the unlabeled samples. Extensive
experiments on both in-house and public datasets show that our framework
outperforms state-of-the-art SSL methods in medical image segmentation.
Ablation study results demonstrate the effectiveness of our technical
contributions.

</details>


### [38] [Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation](https://arxiv.org/abs/2508.04049)
*Jiayi He,Xu Wang,Shengeng Tang,Yaxiong Wang,Lechao Cheng,Dan Guo*

Main category: cs.CV

TL;DR: 提出了一种新的手语视频生成范式，通过两阶段框架解耦手势语义和签者身份，实现了高质量的视频生成和签者个性化灵活性。


<details>
  <summary>Details</summary>
Motivation: 手语视频生成需要精确的语义控制下生成自然的手势动作和真实的外观，但面临两个关键挑战：过度的签者特定数据需求和较差的泛化能力。

Method: 提出了一种两阶段合成框架：首先构建一个签者无关的多模态手势词典，每个词条存储为身份无关的姿势、手势和3D网格序列；其次通过离散到连续的运动合成阶段将检索到的词条序列转换为时间连贯的运动轨迹，再通过身份感知的神经渲染生成任意签者的逼真视频。

Result: 实验证明，该方法能够实现高质量的视频合成，并在签者个性化方面展现出前所未有的灵活性。

Conclusion: 解耦手势语义和签者身份的两阶段合成框架不仅可行且具有优势，能够实现高质量的视频生成和前所未有的签者个性化灵活性。

Abstract: Sign language video generation requires producing natural signing motions
with realistic appearances under precise semantic control, yet faces two
critical challenges: excessive signer-specific data requirements and poor
generalization. We propose a new paradigm for sign language video generation
that decouples motion semantics from signer identity through a two-phase
synthesis framework. First, we construct a signer-independent multimodal motion
lexicon, where each gloss is stored as identity-agnostic pose, gesture, and 3D
mesh sequences, requiring only one recording per sign. This compact
representation enables our second key innovation: a discrete-to-continuous
motion synthesis stage that transforms retrieved gloss sequences into
temporally coherent motion trajectories, followed by identity-aware neural
rendering to produce photorealistic videos of arbitrary signers. Unlike prior
work constrained by signer-specific datasets, our method treats motion as a
first-class citizen: the learned latent pose dynamics serve as a portable
"choreography layer" that can be visually realized through different human
appearances. Extensive experiments demonstrate that disentangling motion from
identity is not just viable but advantageous - enabling both high-quality
synthesis and unprecedented flexibility in signer personalization.

</details>


### [39] [DOMR: Establishing Cross-View Segmentation via Dense Object Matching](https://arxiv.org/abs/2508.04050)
*Jitong Liao,Yulu Gao,Shaofei Huang,Jialin Gao,Jie Lei,Ronghua Liang,Si Liu*

Main category: cs.CV

TL;DR: DOMR框架通过密集对象匹配和掩码细化，显著提升了跨视角对象对应的性能。


<details>
  <summary>Details</summary>
Motivation: 跨视角对象对应是视觉理解中的关键但具有挑战性的任务，现有方法难以有效匹配不同视角下的对象。

Method: 提出了DOMR框架，包括Dense Object Matcher (DOM)模块和掩码细化头。DOM模块联合建模多个对象，利用位置和语义关系建立对应关系，而掩码细化头则提升掩码的完整性和准确性。

Result: 在Ego-Exo4D基准测试中，DOMR的平均IoU达到49.7%（Ego→Exo）和55.2%（Exo→Ego），分别比之前方法提升了5.8%和4.3%。

Conclusion: DOMR框架在跨视角对象对应任务中表现出色，通过结合密集对象匹配和掩码细化，显著提升了性能，验证了其有效性。

Abstract: Cross-view object correspondence involves matching objects between egocentric
(first-person) and exocentric (third-person) views. It is a critical yet
challenging task for visual understanding. In this work, we propose the Dense
Object Matching and Refinement (DOMR) framework to establish dense object
correspondences across views. The framework centers around the Dense Object
Matcher (DOM) module, which jointly models multiple objects. Unlike methods
that directly match individual object masks to image features, DOM leverages
both positional and semantic relationships among objects to find
correspondences. DOM integrates a proposal generation module with a dense
matching module that jointly encodes visual, spatial, and semantic cues,
explicitly constructing inter-object relationships to achieve dense matching
among objects. Furthermore, we combine DOM with a mask refinement head designed
to improve the completeness and accuracy of the predicted masks, forming the
complete DOMR framework. Extensive evaluations on the Ego-Exo4D benchmark
demonstrate that our approach achieves state-of-the-art performance with a mean
IoU of 49.7% on Ego$\to$Exo and 55.2% on Exo$\to$Ego. These results outperform
those of previous methods by 5.8% and 4.3%, respectively, validating the
effectiveness of our integrated approach for cross-view understanding.

</details>


### [40] [Towards Globally Predictable k-Space Interpolation: A White-box Transformer Approach](https://arxiv.org/abs/2508.04051)
*Chen Luo,Qiyu Jin,Taofeng Xie,Xuemei Wang,Huayu Wang,Congcong Liu,Liming Tang,Guoqing Chen,Zhuo-Xu Cui,Dong Liang*

Main category: cs.CV

TL;DR: 提出GPI-WT，一种基于全局可预测插值的白盒Transformer框架，用于k空间插值，显著提高准确性并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要利用局部可预测性而忽略了k空间的全局依赖性，Transformer在捕获长距离依赖性方面表现出色，但其缺乏可解释性。

Method: 通过将全局湮灭滤波器作为可学习参数，并将SLR模型的次梯度自然诱导为可学习的注意力机制，构建了首个专为加速MRI设计的白盒Transformer。

Result: 实验结果表明，GPI-WT在k空间插值准确性上显著优于现有方法，同时提供更好的可解释性。

Conclusion: GPI-WT 是一种基于全局可预测插值（GPI）的白盒Transformer框架，显著提高了k空间插值的准确性，并提供了优越的可解释性。

Abstract: Interpolating missing data in k-space is essential for accelerating imaging.
However, existing methods, including convolutional neural network-based deep
learning, primarily exploit local predictability while overlooking the inherent
global dependencies in k-space. Recently, Transformers have demonstrated
remarkable success in natural language processing and image analysis due to
their ability to capture long-range dependencies. This inspires the use of
Transformers for k-space interpolation to better exploit its global structure.
However, their lack of interpretability raises concerns regarding the
reliability of interpolated data. To address this limitation, we propose
GPI-WT, a white-box Transformer framework based on Globally Predictable
Interpolation (GPI) for k-space. Specifically, we formulate GPI from the
perspective of annihilation as a novel k-space structured low-rank (SLR) model.
The global annihilation filters in the SLR model are treated as learnable
parameters, and the subgradients of the SLR model naturally induce a learnable
attention mechanism. By unfolding the subgradient-based optimization algorithm
of SLR into a cascaded network, we construct the first white-box Transformer
specifically designed for accelerated MRI. Experimental results demonstrate
that the proposed method significantly outperforms state-of-the-art approaches
in k-space interpolation accuracy while providing superior interpretability.

</details>


### [41] [Uni-DocDiff: A Unified Document Restoration Model Based on Diffusion](https://arxiv.org/abs/2508.04055)
*Fangmin Zhao,Weichao Zeng,Zhenhang Li,Dongbao Yang,Binbin Li,Xiaojun Bi,Yu Zhou*

Main category: cs.CV

TL;DR: Uni-DocDiff 是一种基于扩散模型的统一文档修复方法，通过可学习任务提示和 Prior Pool 机制，实现高效多任务修复和扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决现有文档修复方法任务独立、系统复杂、扩展性差及任务间协同不足的问题。

Method: 提出 Uni-DocDiff，基于扩散模型，采用可学习的任务提示设计和 Prior Pool 机制，结合局部高频与全局低频特征，通过 Prior Fusion Module 自适应选择任务相关先验信息。

Result: 实验表明 Uni-DocDiff 性能与专业模型相当甚至更优，且能无缝适应新任务。

Conclusion: Uni-DocDiff 通过创新的任务提示设计和 Prior Pool 机制，实现了多任务文档修复的高效统一，性能优于专业模型，并具备良好的扩展性。

Abstract: Removing various degradations from damaged documents greatly benefits
digitization, downstream document analysis, and readability. Previous methods
often treat each restoration task independently with dedicated models, leading
to a cumbersome and highly complex document processing system. Although recent
studies attempt to unify multiple tasks, they often suffer from limited
scalability due to handcrafted prompts and heavy preprocessing, and fail to
fully exploit inter-task synergy within a shared architecture. To address the
aforementioned challenges, we propose Uni-DocDiff, a Unified and highly
scalable Document restoration model based on Diffusion. Uni-DocDiff develops a
learnable task prompt design, ensuring exceptional scalability across diverse
tasks. To further enhance its multi-task capabilities and address potential
task interference, we devise a novel \textbf{Prior \textbf{P}ool}, a simple yet
comprehensive mechanism that combines both local high-frequency features and
global low-frequency features. Additionally, we design the \textbf{Prior
\textbf{F}usion \textbf{M}odule (PFM)}, which enables the model to adaptively
select the most relevant prior information for each specific task. Extensive
experiments show that the versatile Uni-DocDiff achieves performance comparable
or even superior performance compared with task-specific expert models, and
simultaneously holds the task scalability for seamless adaptation to new tasks.

</details>


### [42] [TCSAFormer: Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation](https://arxiv.org/abs/2508.04058)
*Zunhui Xia,Hongxing Li,Libin Lan*

Main category: cs.CV

TL;DR: TCSAFormer通过压缩注意力和双分支前馈网络模块，高效解决了Transformer在医学图像分割中的计算复杂度和局部特征捕捉问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 针对Transformer方法在医学图像分割中计算复杂度高和局部上下文捕捉能力不足的问题，提出TCSAFormer以提升效率和精度。

Method: TCSAFormer 结合了压缩注意力模块（CA）和双分支前馈网络模块（DBFFN），CA模块通过剪枝和合并冗余标记来减少计算复杂度，DBFFN模块则增强了局部上下文和多尺度特征的捕捉能力。

Result: 在ISIC-2018、CVC-ClinicDB和Synapse数据集上的实验表明，TCSAFormer在保持较低计算开销的同时，性能优于现有SOTA方法。

Conclusion: TCSAFormer 通过引入压缩注意力模块和双分支前馈网络模块，显著降低了计算复杂度并提升了模型的特征表示能力，在多个医学图像分割数据集上实现了优于现有方法的性能。

Abstract: In recent years, transformer-based methods have achieved remarkable progress
in medical image segmentation due to their superior ability to capture
long-range dependencies. However, these methods typically suffer from two major
limitations. First, their computational complexity scales quadratically with
the input sequences. Second, the feed-forward network (FFN) modules in vanilla
Transformers typically rely on fully connected layers, which limits models'
ability to capture local contextual information and multiscale features
critical for precise semantic segmentation. To address these issues, we propose
an efficient medical image segmentation network, named TCSAFormer. The proposed
TCSAFormer adopts two key ideas. First, it incorporates a Compressed Attention
(CA) module, which combines token compression and pixel-level sparse attention
to dynamically focus on the most relevant key-value pairs for each query. This
is achieved by pruning globally irrelevant tokens and merging redundant ones,
significantly reducing computational complexity while enhancing the model's
ability to capture relationships between tokens. Second, it introduces a
Dual-Branch Feed-Forward Network (DBFFN) module as a replacement for the
standard FFN to capture local contextual features and multiscale information,
thereby strengthening the model's feature representation capability. We conduct
extensive experiments on three publicly available medical image segmentation
datasets: ISIC-2018, CVC-ClinicDB, and Synapse, to evaluate the segmentation
performance of TCSAFormer. Experimental results demonstrate that TCSAFormer
achieves superior performance compared to existing state-of-the-art (SOTA)
methods, while maintaining lower computational overhead, thus achieving an
optimal trade-off between efficiency and accuracy.

</details>


### [43] [Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models](https://arxiv.org/abs/2508.04059)
*Zhaochen Liu,Kaiwen Gao,Shuyi Liang,Bin Xiao,Limeng Qiao,Lin Ma,Tingting Jiang*

Main category: cs.CV

TL;DR: O-Bench是首个遮挡感知VQA基准测试，基于SA-1B构建1,365张图像和4,588个问答对。评估显示MLLMs与人类性能差距显著，并识别了三种失败模式。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）表现出色，但其在遮挡感知方面的性能尚未充分探索，因此需要专门的评估工具。

Method: 基于SA-1B，通过分层合成方法构建了1,365张语义连贯的遮挡场景图像，并标注了4,588个问答对，采用半自动工作流程。

Result: 评估22个代表性MLLMs发现，当前模型与人类性能存在显著差距，且模型规模或思维过程无法充分弥补。识别了三种典型失败模式。

Conclusion: O-Bench作为首个专门为遮挡感知设计的VQA基准测试，不仅提供了重要的评估工具，还激发了MLLMs在视觉智能方面的进一步发展。

Abstract: Occlusion perception, a critical foundation for human-level spatial
understanding, embodies the challenge of integrating visual recognition and
reasoning. Though multimodal large language models (MLLMs) have demonstrated
remarkable capabilities, their performance on occlusion perception remains
under-explored. To address this gap, we introduce O-Bench, the first visual
question answering (VQA) benchmark specifically designed for occlusion
perception. Based on SA-1B, we construct 1,365 images featuring semantically
coherent occlusion scenarios through a novel layered synthesis approach. Upon
this foundation, we annotate 4,588 question-answer pairs in total across five
tailored tasks, employing a reliable, semi-automatic workflow. Our extensive
evaluation of 22 representative MLLMs against the human baseline reveals a
significant performance gap between current MLLMs and humans, which, we find,
cannot be sufficiently bridged by model scaling or thinking process. We further
identify three typical failure patterns, including an overly conservative bias,
a fragile gestalt prediction, and a struggle with quantitative tasks. We
believe O-Bench can not only provide a vital evaluation tool for occlusion
perception, but also inspire the development of MLLMs for better visual
intelligence. Our benchmark will be made publicly available upon paper
publication.

</details>


### [44] [TNet: Terrace Convolutional Decoder Network for Remote Sensing Image Semantic Segmentation](https://arxiv.org/abs/2508.04061)
*Chengqian Dai,Yonghong Guo,Hongzhao Xiang,Yigui Luo*

Main category: cs.CV

TL;DR: TNet是一种新型解码网络，通过卷积逐步融合多分辨率特征，显著提升遥感图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有分割网络（如UNet）通常关注单尺度内的特征交互，而忽视了多分辨率间的全局上下文依赖。

Method: 提出了Terrace Convolutional Decoder Network（TNet），利用卷积和加法操作逐步将低分辨率特征（富含全局上下文）整合到高分辨率特征（富含局部细节）中。

Result: TNet-R在ISPRS Vaihingen（mIoU 85.35%）、ISPRS Potsdam（87.05%）和LoveDA（52.19%）上表现优异，同时保持高效计算。

Conclusion: TNet-R通过仅使用卷积和加法操作，在解码阶段逐步整合多分辨率特征，实现了全局与局部信息的自然融合，并在多个基准数据集上取得了竞争性性能。

Abstract: In remote sensing, most segmentation networks adopt the UNet architecture,
often incorporating modules such as Transformers or Mamba to enhance
global-local feature interactions within decoder stages. However, these
enhancements typically focus on intra-scale relationships and neglect the
global contextual dependencies across multiple resolutions. To address this
limitation, we introduce the Terrace Convolutional Decoder Network (TNet), a
simple yet effective architecture that leverages only convolution and addition
operations to progressively integrate low-resolution features (rich in global
context) into higher-resolution features (rich in local details) across
decoding stages. This progressive fusion enables the model to learn
spatially-aware convolutional kernels that naturally blend global and local
information in a stage-wise manner. We implement TNet with a ResNet-18 encoder
(TNet-R) and evaluate it on three benchmark datasets. TNet-R achieves
competitive performance with a mean Intersection-over-Union (mIoU) of 85.35\%
on ISPRS Vaihingen, 87.05\% on ISPRS Potsdam, and 52.19\% on LoveDA, while
maintaining high computational efficiency. Code is publicly available.

</details>


### [45] [Bridging Diffusion Models and 3D Representations: A 3D Consistent Super-Resolution Framework](https://arxiv.org/abs/2508.04090)
*Yi-Ting Chen,Ting-Hsuan Liao,Pengsheng Guo,Alexander Schwing,Jia-Bin Huang*

Main category: cs.CV

TL;DR: 3DSR通过3D高斯泼溅和2D超分辨率扩散模型，实现无需额外微调的3D一致超分辨率，提升视觉质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（如图像上采样或视频超分辨率）要么不考虑3D一致性，要么仅隐式纳入3D一致性的问题。

Method: 利用3D高斯泼溅的场景表示和现成的2D超分辨率扩散模型，确保视图间的3D一致性。

Result: 在MipNeRF360和LLFF数据上的评估显示，3DSR能够生成视觉吸引的高分辨率结果，同时保持3D重建的结构一致性。

Conclusion: 3DSR是一种基于3D高斯泼溅和现成扩散模型的2D超分辨率方法的新型3D超分辨率框架，能够在不需要额外微调的情况下提升视觉质量，并保持3D重建的结构一致性。

Abstract: We propose 3D Super Resolution (3DSR), a novel 3D Gaussian-splatting-based
super-resolution framework that leverages off-the-shelf diffusion-based 2D
super-resolution models. 3DSR encourages 3D consistency across views via the
use of an explicit 3D Gaussian-splatting-based scene representation. This makes
the proposed 3DSR different from prior work, such as image upsampling or the
use of video super-resolution, which either don't consider 3D consistency or
aim to incorporate 3D consistency implicitly. Notably, our method enhances
visual quality without additional fine-tuning, ensuring spatial coherence
within the reconstructed scene. We evaluate 3DSR on MipNeRF360 and LLFF data,
demonstrating that it produces high-resolution results that are visually
compelling, while maintaining structural consistency in 3D reconstructions.
Code will be released.

</details>


### [46] [DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D Gaussian Splatting](https://arxiv.org/abs/2508.04099)
*Zexu Huang,Min Xu,Stuart Perry*

Main category: cs.CV

TL;DR: DET-GS是一个针对3D高斯泼溅的深度和边缘感知正则化框架，通过分层几何监督和边缘保留技术，显著提升了稀疏视图条件下的重建质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图条件下，现有方法在几何重建中难以捕捉细粒度结构且对深度噪声敏感，传统平滑方法会破坏语义边界和纹理细节。

Method: 提出了DET-GS框架，包括分层几何深度监督、基于Canny边缘检测的边缘感知深度正则化，以及RGB引导的边缘保留总变差损失。

Result: 在稀疏视图新视角合成基准测试中，DET-GS在几何精度和视觉保真度上均显著优于现有方法。

Conclusion: DET-GS通过分层几何深度监督和边缘感知正则化框架，显著提升了稀疏视图条件下的几何重建精度和视觉保真度，优于现有方法。

Abstract: 3D Gaussian Splatting (3DGS) represents a significant advancement in the
field of efficient and high-fidelity novel view synthesis. Despite recent
progress, achieving accurate geometric reconstruction under sparse-view
conditions remains a fundamental challenge. Existing methods often rely on
non-local depth regularization, which fails to capture fine-grained structures
and is highly sensitive to depth estimation noise. Furthermore, traditional
smoothing methods neglect semantic boundaries and indiscriminately degrade
essential edges and textures, consequently limiting the overall quality of
reconstruction. In this work, we propose DET-GS, a unified depth and edge-aware
regularization framework for 3D Gaussian Splatting. DET-GS introduces a
hierarchical geometric depth supervision framework that adaptively enforces
multi-level geometric consistency, significantly enhancing structural fidelity
and robustness against depth estimation noise. To preserve scene boundaries, we
design an edge-aware depth regularization guided by semantic masks derived from
Canny edge detection. Furthermore, we introduce an RGB-guided edge-preserving
Total Variation loss that selectively smooths homogeneous regions while
rigorously retaining high-frequency details and textures. Extensive experiments
demonstrate that DET-GS achieves substantial improvements in both geometric
accuracy and visual fidelity, outperforming state-of-the-art (SOTA) methods on
sparse-view novel view synthesis benchmarks.

</details>


### [47] [NEARL-CLIP: Interacted Query Adaptation with Orthogonal Regularization for Medical Vision-Language Understanding](https://arxiv.org/abs/2508.04101)
*Zelin Peng,Yichen Zhao,Yu Huang,Piao Yang,Feilong Tang,Zhengqin Xu,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: NEARL-CLIP是一个跨模态交互框架，通过动态查询和正交技术提升医学图像分析性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分析中标注数据有限和视觉语言模型直接应用时的领域差距问题。

Method: 提出了NEARL-CLIP框架，包含USEformer和OCA两个关键组件，分别用于动态生成跨模态查询和正交分解新知识。

Result: NEARL-CLIP以仅1.46M可学习参数的参数高效方式，显著提升了模型性能。

Conclusion: NEARL-CLIP通过动态生成跨模态查询和正交技术，有效促进了多模态医学领域知识的交互与增强，显著提升了视觉语言模型在医学图像分析中的性能。

Abstract: Computer-aided medical image analysis is crucial for disease diagnosis and
treatment planning, yet limited annotated datasets restrict medical-specific
model development. While vision-language models (VLMs) like CLIP offer strong
generalization capabilities, their direct application to medical imaging
analysis is impeded by a significant domain gap. Existing approaches to bridge
this gap, including prompt learning and one-way modality interaction
techniques, typically focus on introducing domain knowledge to a single
modality. Although this may offer performance gains, it often causes modality
misalignment, thereby failing to unlock the full potential of VLMs. In this
paper, we propose \textbf{NEARL-CLIP} (i\underline{N}teracted qu\underline{E}ry
\underline{A}daptation with o\underline{R}thogona\underline{L} Regularization),
a novel cross-modality interaction VLM-based framework that contains two
contributions: (1) Unified Synergy Embedding Transformer (USEformer), which
dynamically generates cross-modality queries to promote interaction between
modalities, thus fostering the mutual enrichment and enhancement of multi-modal
medical domain knowledge; (2) Orthogonal Cross-Attention Adapter (OCA). OCA
introduces an orthogonality technique to decouple the new knowledge from
USEformer into two distinct components: the truly novel information and the
incremental knowledge. By isolating the learning process from the interference
of incremental knowledge, OCA enables a more focused acquisition of new
information, thereby further facilitating modality interaction and unleashing
the capability of VLMs. Notably, NEARL-CLIP achieves these two contributions in
a parameter-efficient style, which only introduces \textbf{1.46M} learnable
parameters.

</details>


### [48] [AR as an Evaluation Playground: Bridging Metrics and Visual Perception of Computer Vision Models](https://arxiv.org/abs/2508.04102)
*Ashkan Ganj,Yiqin Zhao,Tian Guo*

Main category: cs.CV

TL;DR: ARCADE是一个基于AR的平台，简化了CV模型的人类感知研究，展示了其在深度和光照估计模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 人类感知研究对理解CV模型性能至关重要，但现有方法复杂且难以扩展，AR技术为此提供了独特机会。

Method: 设计了ARCADE平台，支持跨平台AR数据收集、可插拔模型推理的自定义实验协议以及AR流媒体用于用户研究。

Result: ARCADE在深度和光照估计模型中验证了通过AR任务获取人类感知判断的可行性，并展示了其在多种部署和研究设置中的实用性。

Conclusion: ARCADE平台通过AR技术有效支持了人类感知研究，展示了其作为人本评估工具的灵活性和有效性。

Abstract: Human perception studies can provide complementary insights to qualitative
evaluation for understanding computer vision (CV) model performance. However,
conducting human perception studies remains a non-trivial task, it often
requires complex, end-to-end system setups that are time-consuming and
difficult to scale. In this paper, we explore the unique opportunity presented
by augmented reality (AR) for helping CV researchers to conduct perceptual
studies. We design ARCADE, an evaluation platform that allows researchers to
easily leverage AR's rich context and interactivity for human-centered CV
evaluation. Specifically, ARCADE supports cross-platform AR data collection,
custom experiment protocols via pluggable model inference, and AR streaming for
user studies. We demonstrate ARCADE using two types of CV models, depth and
lighting estimation and show that AR tasks can be effectively used to elicit
human perceptual judgments of model quality. We also evaluate the systems
usability and performance across different deployment and study settings,
highlighting its flexibility and effectiveness as a human-centered evaluation
platform.

</details>


### [49] [Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decode](https://arxiv.org/abs/2508.04107)
*Jingchao Wang,Zhijian Wu,Dingjiang Huang,Yefeng Zheng,Hong Wang*

Main category: cs.CV

TL;DR: MLLMSeg结合MLLM视觉编码器细节特征和LLM语义特征，通过DSFF和轻量解码器实现高效RES。


<details>
  <summary>Details</summary>
Motivation: 解决现有RES方法在性能和成本之间的权衡问题，避免使用参数繁重的SAM或牺牲准确性的轻量级方法。

Method: 提出了MLLMSeg框架，包括DSFF模块和轻量级掩码解码器，无需额外视觉编码器。

Result: MLLMSeg在实验中表现优于基于SAM和SAM-free的竞争者。

Conclusion: MLLMSeg框架通过充分利用MLLM视觉编码器中的视觉细节特征，结合DSFF模块和轻量级掩码解码器，在性能和成本之间取得了更好的平衡。

Abstract: Reference Expression Segmentation (RES) aims to segment image regions
specified by referring expressions and has become popular with the rise of
multimodal large models (MLLMs). While MLLMs excel in semantic understanding,
their token-generation paradigm struggles with pixel-level dense prediction.
Existing RES methods either couple MLLMs with the parameter-heavy Segment
Anything Model (SAM) with 632M network parameters or adopt SAM-free lightweight
pipelines that sacrifice accuracy. To address the trade-off between performance
and cost, we specifically propose MLLMSeg, a novel framework that fully
exploits the inherent visual detail features encoded in the MLLM vision encoder
without introducing an extra visual encoder. Besides, we propose a
detail-enhanced and semantic-consistent feature fusion module (DSFF) that fully
integrates the detail-related visual feature with the semantic-related feature
output by the large language model (LLM) of MLLM. Finally, we establish a
light-weight mask decoder with only 34M network parameters that optimally
leverages detailed spatial features from the visual encoder and semantic
features from the LLM to achieve precise mask prediction. Extensive experiments
demonstrate that our method generally surpasses both SAM-based and SAM-free
competitors, striking a better balance between performance and cost. Code is
available at https://github.com/jcwang0602/MLLMSeg.

</details>


### [50] [CLIPVehicle: A Unified Framework for Vision-based Vehicle Search](https://arxiv.org/abs/2508.04120)
*Likai Wang,Ruize Han,Xiangqun Zhang,Wei Feng*

Main category: cs.CV

TL;DR: CLIPVehicle通过双粒度对齐和多层次学习策略，实现了车辆检测与重识别的联合优化，性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法需先检测并存储所有车辆图像块再应用重识别模型，资源消耗大且不实用，因此需要一种能联合检测与重识别的端到端系统。

Method: 提出了一种名为CLIPVehicle的统一框架，包含双粒度语义区域对齐模块和多层次车辆识别学习策略，利用视觉语言模型（VLMs）进行车辆判别建模。

Result: 实验结果表明，CLIPVehicle在车辆重识别和人物搜索任务中均优于现有最先进方法。

Conclusion: CLIPVehicle框架通过双粒度语义区域对齐模块和多层次车辆识别学习策略，有效解决了车辆检测与重识别联合任务中的目标冲突问题，并在实验中超越了现有最先进方法。

Abstract: Vehicles, as one of the most common and significant objects in the real
world, the researches on which using computer vision technologies have made
remarkable progress, such as vehicle detection, vehicle re-identification, etc.
To search an interested vehicle from the surveillance videos, existing methods
first pre-detect and store all vehicle patches, and then apply vehicle
re-identification models, which is resource-intensive and not very practical.
In this work, we aim to achieve the joint detection and re-identification for
vehicle search. However, the conflicting objectives between detection that
focuses on shared vehicle commonness and re-identification that focuses on
individual vehicle uniqueness make it challenging for a model to learn in an
end-to-end system. For this problem, we propose a new unified framework, namely
CLIPVehicle, which contains a dual-granularity semantic-region alignment module
to leverage the VLMs (Vision-Language Models) for vehicle discrimination
modeling, and a multi-level vehicle identification learning strategy to learn
the identity representation from global, instance and feature levels. We also
construct a new benchmark, including a real-world dataset CityFlowVS, and two
synthetic datasets SynVS-Day and SynVS-All, for vehicle search. Extensive
experimental results demonstrate that our method outperforms the
state-of-the-art methods of both vehicle Re-ID and person search tasks.

</details>


### [51] [Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation](https://arxiv.org/abs/2508.04122)
*Maximilian Ulmer,Wout Boerdijk,Rudolph Triebel,Maximilian Durner*

Main category: cs.CV

TL;DR: OC-DiT是一种新型扩散模型，用于零样本实例分割，通过条件潜在扩散框架生成实例掩码，无需目标数据重新训练即可达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 设计OC-DiT用于对象中心预测，并应用于零样本实例分割，旨在通过扩散过程有效解耦对象实例。

Method: 提出了一种条件潜在扩散框架，通过将生成过程基于对象模板和图像特征在扩散模型的潜在空间中进行条件化，生成实例掩码。引入了两个模型变体：一个用于生成初始对象实例提案的粗粒度模型，以及一个并行细化所有提案的细化模型。

Result: 在多个挑战性真实世界基准测试中实现了最先进的性能，无需在目标数据上进行重新训练。

Conclusion: OC-DiT展示了扩散模型在实例分割任务中的潜力，无需在目标数据上重新训练即可在多个真实世界基准测试中达到最先进性能。

Abstract: This paper presents OC-DiT, a novel class of diffusion models designed for
object-centric prediction, and applies it to zero-shot instance segmentation.
We propose a conditional latent diffusion framework that generates instance
masks by conditioning the generative process on object templates and image
features within the diffusion model's latent space. This allows our model to
effectively disentangle object instances through the diffusion process, which
is guided by visual object descriptors and localized image cues. Specifically,
we introduce two model variants: a coarse model for generating initial object
instance proposals, and a refinement model that refines all proposals in
parallel. We train these models on a newly created, large-scale synthetic
dataset comprising thousands of high-quality object meshes. Remarkably, our
model achieves state-of-the-art performance on multiple challenging real-world
benchmarks, without requiring any retraining on target data. Through
comprehensive ablation studies, we demonstrate the potential of diffusion
models for instance segmentation tasks.

</details>


### [52] [DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image Segmentation](https://arxiv.org/abs/2508.04131)
*Zhaohong Huang,Yuxin Zhang,Mingbao Lin,Taojian Zhou,Guorong Cai,Rongrong Ji*

Main category: cs.CV

TL;DR: DS$^2$Net通过多视角深度监督和自适应损失，显著提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅单独监督粗粒度语义或细粒度细节特征，忽视了这两种特征在医学图像分析中的互补关系，导致性能受限。

Method: 提出了Detail-Semantic Deep Supervision Network (DS$^2$Net)，包含Detail Enhance Module (DEM)和Semantic Enhance Module (SEM)，分别监督低层细节和高层语义特征，并通过不确定性损失自适应调整监督强度。

Result: 在六个不同医学影像基准测试中，DS$^2$Net均优于现有方法。

Conclusion: DS$^2$Net通过结合细节和语义特征的深度监督，以及基于不确定性的监督损失，显著提升了医学图像分割的性能，并在多个基准测试中超越了现有最先进方法。

Abstract: Deep Supervision Networks exhibit significant efficacy for the medical
imaging community. Nevertheless, existing work merely supervises either the
coarse-grained semantic features or fine-grained detailed features in
isolation, which compromises the fact that these two types of features hold
vital relationships in medical image analysis. We advocate the powers of
complementary feature supervision for medical image segmentation, by proposing
a Detail-Semantic Deep Supervision Network (DS$^2$Net). DS$^2$Net navigates
both low-level detailed and high-level semantic feature supervision through
Detail Enhance Module (DEM) and Semantic Enhance Module (SEM). DEM and SEM
respectively harness low-level and high-level feature maps to create detail and
semantic masks for enhancing feature supervision. This is a novel shift from
single-view deep supervision to multi-view deep supervision. DS$^2$Net is also
equipped with a novel uncertainty-based supervision loss that adaptively
assigns the supervision strength of features within distinct scales based on
their uncertainty, thus circumventing the sub-optimal heuristic design that
typifies previous works. Through extensive experiments on six benchmarks
captured under either colonoscopy, ultrasound and microscope, we demonstrate
that DS$^2$Net consistently outperforms state-of-the-art methods for medical
image analysis.

</details>


### [53] [Excavate the potential of Single-Scale Features: A Decomposition Network for Water-Related Optical Image Enhancement](https://arxiv.org/abs/2508.04123)
*Zheng Cheng,Wenri Wang,Guangyong Chen,Yakun Ju,Yihua Cheng,Zhisong Liu,Yanda Meng,Jintao Song*

Main category: cs.CV

TL;DR: SSD-Net通过单尺度分解网络结合CNN与Transformer，简化水下图像增强，性能媲美多尺度方法。


<details>
  <summary>Details</summary>
Motivation: 现有水下图像增强方法过度依赖多尺度特征融合，但实验表明单尺度特征提取同样高效且能降低复杂度。

Method: 提出SSD-Net，采用不对称分解机制将输入图像解耦为干净层和退化层，结合PFDB和BFCB模块实现特征空间解耦与跨层交互。

Result: SSD-Net在性能上匹配或超越多尺度方法，显著减少计算复杂度。

Conclusion: SSD-Net通过单尺度特征分解网络，结合CNN和Transformer的优势，有效提升水下图像增强质量，简化了传统多尺度方法的复杂性。

Abstract: Underwater image enhancement (UIE) techniques aim to improve visual quality
of images captured in aquatic environments by addressing degradation issues
caused by light absorption and scattering effects, including color distortion,
blurring, and low contrast. Current mainstream solutions predominantly employ
multi-scale feature extraction (MSFE) mechanisms to enhance reconstruction
quality through multi-resolution feature fusion. However, our extensive
experiments demonstrate that high-quality image reconstruction does not
necessarily rely on multi-scale feature fusion. Contrary to popular belief, our
experiments show that single-scale feature extraction alone can match or
surpass the performance of multi-scale methods, significantly reducing
complexity. To comprehensively explore single-scale feature potential in
underwater enhancement, we propose an innovative Single-Scale Decomposition
Network (SSD-Net). This architecture introduces an asymmetrical decomposition
mechanism that disentangles input image into clean layer along with degradation
layer. The former contains scene-intrinsic information and the latter encodes
medium-induced interference. It uniquely combines CNN's local feature
extraction capabilities with Transformer's global modeling strengths through
two core modules: 1) Parallel Feature Decomposition Block (PFDB), implementing
dual-branch feature space decoupling via efficient attention operations and
adaptive sparse transformer; 2) Bidirectional Feature Communication Block
(BFCB), enabling cross-layer residual interactions for complementary feature
mining and fusion. This synergistic design preserves feature decomposition
independence while establishing dynamic cross-layer information pathways,
effectively enhancing degradation decoupling capacity.

</details>


### [54] [UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval](https://arxiv.org/abs/2508.04136)
*Hongyu Guo,Kuan Zhu,Xiangzhao Hao,Haiyun Guo,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: UniFGVC通过多模态检索和结构化文本描述，显著提升少样本细粒度视觉分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于预训练视觉语言模型的方法在少量样本下易过拟合和泛化能力弱的问题。

Method: 提出Category-Discriminative Visual Captioner (CDV-Captioner)生成结构化文本描述，结合多模态类别模板和现成的视觉/文本编码器实现检索。

Result: 在12个FGVC基准测试中表现优于现有基于CLIP的少样本方法及部分全监督MLLMs方法。

Conclusion: UniFGVC框架通过多模态检索的方式，在少量样本情况下显著提升了细粒度视觉分类的性能，且在多个基准测试中表现优于现有方法。

Abstract: Few-shot fine-grained visual classification (FGVC) aims to leverage limited
data to enable models to discriminate subtly distinct categories. Recent works
mostly finetuned the pre-trained visual language models to achieve performance
gain, yet suffering from overfitting and weak generalization. To deal with
this, we introduce UniFGVC, a universal training-free framework that
reformulates few-shot FGVC as multimodal retrieval. First, we propose the
Category-Discriminative Visual Captioner (CDV-Captioner) to exploit the
open-world knowledge of multimodal large language models (MLLMs) to generate a
structured text description that captures the fine-grained attribute features
distinguishing closely related classes. CDV-Captioner uses chain-of-thought
prompting and visually similar reference images to reduce hallucination and
enhance discrimination of generated captions. Using it we can convert each
image into an image-description pair, enabling more comprehensive feature
representation, and construct the multimodal category templates using few-shot
samples for the subsequent retrieval pipeline. Then, off-the-shelf vision and
text encoders embed query and template pairs, and FGVC is accomplished by
retrieving the nearest template in the joint space. UniFGVC ensures broad
compatibility with diverse MLLMs and encoders, offering reliable generalization
and adaptability across few-shot FGVC scenarios. Extensive experiments on 12
FGVC benchmarks demonstrate its consistent superiority over prior few-shot
CLIP-based methods and even several fully-supervised MLLMs-based approaches.

</details>


### [55] [Learning Using Privileged Information for Litter Detection](https://arxiv.org/abs/2508.04124)
*Matthias Bartolo,Konstantinos Makantasis,Dylan Seychell*

Main category: cs.CV

TL;DR: 提出了一种结合特权信息与深度学习的新方法，通过编码边界框信息为二进制掩码，显著提升垃圾检测准确性和效率，适用于多种场景。


<details>
  <summary>Details</summary>
Motivation: 随着全球垃圾污染的持续增加，开发高效的自动化垃圾检测工具成为重要挑战。

Method: 结合特权信息与深度学习目标检测，首次提出将边界框信息编码为二进制掩码以优化检测引导，并在五种广泛使用的目标检测模型上进行评估。

Result: 在SODA、BDW和UAVVaste数据集上的实验表明，该方法在所有模型中均实现了性能提升，且无需增加模型复杂度或额外层数，保持了计算效率和可扩展性。

Conclusion: 本研究提出了一种结合特权信息与深度学习目标检测的新方法，有效提高了垃圾检测的准确性和模型效率，为实际应用提供了平衡准确性与效率的实用解决方案。

Abstract: As litter pollution continues to rise globally, developing automated tools
capable of detecting litter effectively remains a significant challenge. This
study presents a novel approach that combines, for the first time, privileged
information with deep learning object detection to improve litter detection
while maintaining model efficiency. We evaluate our method across five widely
used object detection models, addressing challenges such as detecting small
litter and objects partially obscured by grass or stones. In addition to this,
a key contribution of our work can also be attributed to formulating a means of
encoding bounding box information as a binary mask, which can be fed to the
detection model to refine detection guidance. Through experiments on both
within-dataset evaluation on the renowned SODA dataset and cross-dataset
evaluation on the BDW and UAVVaste litter detection datasets, we demonstrate
consistent performance improvements across all models. Our approach not only
bolsters detection accuracy within the training sets but also generalises well
to other litter detection contexts. Crucially, these improvements are achieved
without increasing model complexity or adding extra layers, ensuring
computational efficiency and scalability. Our results suggest that this
methodology offers a practical solution for litter detection, balancing
accuracy and efficiency in real-world applications.

</details>


### [56] [Gather and Trace: Rethinking Video TextVQA from an Instance-oriented Perspective](https://arxiv.org/abs/2508.04197)
*Yan Zhang,Gangyan Zeng,Daiqing Wu,Huawen Shen,Binbin Li,Yu Zhou,Can Ma,Xiaojun Bi*

Main category: cs.CV

TL;DR: GAT模型通过上下文聚合和轨迹追踪模块，显著提升视频文本视觉问答的准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的帧级框架存在冗余文本实体和隐式关系建模的问题，导致准确性和效率受限。

Method: 提出GAT模型，包含上下文聚合实例收集模块和实例聚焦轨迹追踪模块，分别用于精确获取视频文本实例的阅读结果和捕捉文本的动态演化。

Result: 在多个公开数据集上验证了GAT的有效性和泛化能力，准确率和推理速度均显著优于现有方法。

Conclusion: GAT模型在视频文本视觉问答任务中显著优于现有方法，不仅在准确率上提升了3.86%，推理速度也比视频大型语言模型快十倍。

Abstract: Video text-based visual question answering (Video TextVQA) aims to answer
questions by explicitly reading and reasoning about the text involved in a
video. Most works in this field follow a frame-level framework which suffers
from redundant text entities and implicit relation modeling, resulting in
limitations in both accuracy and efficiency. In this paper, we rethink the
Video TextVQA task from an instance-oriented perspective and propose a novel
model termed GAT (Gather and Trace). First, to obtain accurate reading result
for each video text instance, a context-aggregated instance gathering module is
designed to integrate the visual appearance, layout characteristics, and
textual contents of the related entities into a unified textual representation.
Then, to capture dynamic evolution of text in the video flow, an
instance-focused trajectory tracing module is utilized to establish
spatio-temporal relationships between instances and infer the final answer.
Extensive experiments on several public Video TextVQA datasets validate the
effectiveness and generalization of our framework. GAT outperforms existing
Video TextVQA methods, video-language pretraining methods, and video large
language models in both accuracy and inference speed. Notably, GAT surpasses
the previous state-of-the-art Video TextVQA methods by 3.86\% in accuracy and
achieves ten times of faster inference speed than video large language models.
The source code is available at https://github.com/zhangyan-ucas/GAT.

</details>


### [57] [SVC 2025: the First Multimodal Deception Detection Challenge](https://arxiv.org/abs/2508.04129)
*Xun Lin,Xiaobao Guo,Taorui Wang,Yingjie Ma,Jiajian Huang,Jiayu Zhang,Junzhe Cao,Zitong Yu*

Main category: cs.CV

TL;DR: SVC 2025多模态欺骗检测挑战赛旨在评估跨领域泛化能力，推动更适应性强、可解释的欺骗检测系统发展，21支团队参与并提交结果。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于单一领域，忽视了领域转移导致的性能下降问题，需推动跨领域泛化能力的研究。

Method: 利用多模态数据（音频、视频、文本）设计模型，捕捉细微和隐性的欺骗线索，并评估模型在跨领域场景中的表现。

Result: 21支团队提交了最终结果，挑战赛成功促进了多模态学习领域的发展。

Conclusion: 通过SVC 2025多模态欺骗检测挑战赛，推动了跨领域泛化能力的研究，促进了更适应性强、可解释且实际可部署的欺骗检测系统的发展。

Abstract: Deception detection is a critical task in real-world applications such as
security screening, fraud prevention, and credibility assessment. While deep
learning methods have shown promise in surpassing human-level performance,
their effectiveness often depends on the availability of high-quality and
diverse deception samples. Existing research predominantly focuses on
single-domain scenarios, overlooking the significant performance degradation
caused by domain shifts. To address this gap, we present the SVC 2025
Multimodal Deception Detection Challenge, a new benchmark designed to evaluate
cross-domain generalization in audio-visual deception detection. Participants
are required to develop models that not only perform well within individual
domains but also generalize across multiple heterogeneous datasets. By
leveraging multimodal data, including audio, video, and text, this challenge
encourages the design of models capable of capturing subtle and implicit
deceptive cues. Through this benchmark, we aim to foster the development of
more adaptable, explainable, and practically deployable deception detection
systems, advancing the broader field of multimodal learning. By the conclusion
of the workshop competition, a total of 21 teams had submitted their final
results. https://sites.google.com/view/svc-mm25 for more information.

</details>


### [58] [ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs](https://arxiv.org/abs/2508.04201)
*Ben Zhang,LuLu Yu,Lei Gao,Jing Liu,QuanJiang Guo,Hui Gao*

Main category: cs.CV

TL;DR: ViFP是一个提升视觉语言模型推理可靠性的通用框架，通过子问题模板和多轮QA路径减少错误正例，显著提高准确率并降低FP率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于特定多步推理数据集和强化学习策略，训练成本高且泛化能力有限。ViFP旨在解决这些问题，提升视觉推理的可靠性。

Method: ViFP通过构建子问题模板和多轮QA路径来改进推理路径，动态分析推理路径一致性以识别潜在FP，并引入自适应的CoT机制来指导FP和非FP样本。

Result: 在A-OKVQA、OKVQA和FVQA数据集上，ViFP将准确率最高提升5.4%，并显著减少FP数量，验证了其在提升推理可靠性方面的优势。

Conclusion: ViFP通过构建基于视觉推理核心维度的子问题模板和多轮QA路径，显著提升了视觉语言模型的推理可靠性，减少了错误正例（FP）的发生，并在多个数据集上验证了其有效性。

Abstract: In visual-language model (VLM) reasoning, false positive(FP) reasoning occurs
when a model generates a correct answer but follows an incorrect reasoning
path. Existing methods based on specific multi-step reasoning datasets and
reinforcement learning strategies, leading to high training costs and limited
generalization. In this work, we propose ViFP, a general framework for
enhancing visual reasoning reliability. It improves both answer accuracy and
reasoning soundness by detecting FPs. ViFP tackles the limitations of dataset
dependency and poor generalization by constructing sub-question templates
grounded in the core dimensions of visual reasoning, such as object
localization, characteristic description, and object discovery. ViFP then
builds effective reasoning paths via multi-turn QA to improve reasoning
accuracy. Meanwhile, ViFP dynamically analyzes the consistency of reasoning
path to identify potential FPs, and introduces a targeted chain-of-thought
(CoT) mechanism that adaptively guides both FP and non-FP samples. Thereby
reducing logical errors in the reasoning path while preserving accuracy.
Finally, we introduce a reliability evaluation metric-VoC, which integrates
answer accuracy and the FP rate, providing a quantitative tool to assess
whether a VLM not only answers correctly, but also reasons reliably. Our
experiments on closed-source VLMs show that ViFP consistently improves
performance across three datasets: A-OKVQA, OKVQA, and FVQA. On A-OKVQA, ViFP
improves accuracy by up to 5.4%, surpassing the previous state-of-the-art by
4.3%, and significantly reduces the number of FPs, validating its benefits in
enhancing reasoning reliability.

</details>


### [59] [LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation](https://arxiv.org/abs/2508.04228)
*Kangrui Cen,Baixuan Zhao,Yi Xin,Siqi Luo,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: LayerT2V是一种分层视频生成方法，显著提升多对象运动控制，实验指标优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频（T2V）生成技术在多对象运动场景中表现不佳，尤其是在对象轨迹交叉时语义冲突严重，缺乏有效的多对象运动控制方法。

Method: 引入LayerT2V，一种通过逐层合成背景和前景对象来生成视频的首创方法，每个元素位于独立的“层”上，从而支持多对象合成。

Result: 实验证明LayerT2V在多对象复杂场景生成中优于现有技术，mIoU和AP50指标分别提升1.4倍和4.5倍。

Conclusion: LayerT2V通过分层生成视频的方法，显著提升了多对象运动场景下的生成质量与控制能力，实验结果表明其在mIoU和AP50指标上分别比现有最优方法提高了1.4倍和4.5倍。

Abstract: Controlling object motion trajectories in Text-to-Video (T2V) generation is a
challenging and relatively under-explored area, particularly in scenarios
involving multiple moving objects. Most community models and datasets in the
T2V domain are designed for single-object motion, limiting the performance of
current generative models in multi-object tasks. Additionally, existing motion
control methods in T2V either lack support for multi-object motion scenes or
experience severe performance degradation when object trajectories intersect,
primarily due to the semantic conflicts in colliding regions. To address these
limitations, we introduce LayerT2V, the first approach for generating video by
compositing background and foreground objects layer by layer. This layered
generation enables flexible integration of multiple independent elements within
a video, positioning each element on a distinct "layer" and thus facilitating
coherent multi-object synthesis while enhancing control over the generation
process. Extensive experiments demonstrate the superiority of LayerT2V in
generating complex multi-object scenarios, showcasing 1.4x and 4.5x
improvements in mIoU and AP50 metrics over state-of-the-art (SOTA) methods.
Project page and code are available at https://kr-panghu.github.io/LayerT2V/ .

</details>


### [60] [Segment Any Vehicle: Semantic and Visual Context Driven SAM and A Benchmark](https://arxiv.org/abs/2508.04260)
*Xiao Wang,Ziwen Wang,Wentao Wu,Anjie Wang,Jiashu Wu,Yantao Pan,Chenglong Li*

Main category: cs.CV

TL;DR: SAV框架结合知识图谱和上下文检索，解决了SAM在车辆部件分割中的限制，并发布了VehicleSeg10K数据集。


<details>
  <summary>Details</summary>
Motivation: 解决Segment Anything Model (SAM)在车辆部件分割任务中因缺乏语义标签和文本提示功能不可用而受限的问题。

Method: 提出SAV框架，包含SAM编码解码器、车辆部件知识图谱和上下文样本检索编码模块，利用结构化本体和视觉相似性提升分割性能。

Result: 在VehicleSeg10K数据集上进行了全面实验，证明了SAV框架的有效性，并发布了新的基准数据集。

Conclusion: SAV框架通过结合SAM编码解码器、车辆部件知识图谱和上下文检索模块，显著提升了车辆部件分割的准确性和泛化能力，并发布了VehicleSeg10K数据集为未来研究奠定基础。

Abstract: With the rapid advancement of autonomous driving, vehicle perception,
particularly detection and segmentation, has placed increasingly higher demands
on algorithmic performance. Pre-trained large segmentation models, especially
Segment Anything Model (SAM), have sparked significant interest and inspired
new research directions in artificial intelligence. However, SAM cannot be
directly applied to the fine-grained task of vehicle part segmentation, as its
text-prompted segmentation functionality is not publicly accessible, and the
mask regions generated by its default mode lack semantic labels, limiting its
utility in structured, category-specific segmentation tasks. To address these
limitations, we propose SAV, a novel framework comprising three core
components: a SAM-based encoder-decoder, a vehicle part knowledge graph, and a
context sample retrieval encoding module. The knowledge graph explicitly models
the spatial and geometric relationships among vehicle parts through a
structured ontology, effectively encoding prior structural knowledge.
Meanwhile, the context retrieval module enhances segmentation by identifying
and leveraging visually similar vehicle instances from training data, providing
rich contextual priors for improved generalization. Furthermore, we introduce a
new large-scale benchmark dataset for vehicle part segmentation, named
VehicleSeg10K, which contains 11,665 high-quality pixel-level annotations
across diverse scenes and viewpoints. We conduct comprehensive experiments on
this dataset and two other datasets, benchmarking multiple representative
baselines to establish a solid foundation for future research and comparison. %
Both the dataset and source code of this paper will be released upon
acceptance. Both the dataset and source code of this paper will be released on
https://github.com/Event-AHU/SAV

</details>


### [61] [IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene Generation with Precise Camera Control](https://arxiv.org/abs/2508.04147)
*Lijuan Liu,Wenfa Li,Dongbo Zhang,Shuo Wang,Shaohui Jiao*

Main category: cs.CV

TL;DR: IDC-Net是一个联合生成RGB图像和深度图的框架，通过几何感知扩散模型和Transformer块实现精确相机控制，生成的序列质量高且可直接用于3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将RGB和深度生成分开处理，导致几何一致性不足。IDC-Net旨在通过联合学习框架提升空间和几何对齐，实现更精确的相机控制。

Method: IDC-Net采用统一的几何感知扩散模型联合生成RGB图像和深度图，并设计了几何感知Transformer块以实现细粒度相机控制。

Result: 实验表明，IDC-Net在视觉质量和几何一致性上优于现有方法，生成的RGB-D序列可直接用于3D场景重建。

Conclusion: IDC-Net通过联合学习框架在RGB-D视频序列生成中实现了视觉质量和几何一致性的显著提升，生成的序列可直接用于下游3D场景重建任务，无需额外后处理。

Abstract: We present IDC-Net (Image-Depth Consistency Network), a novel framework
designed to generate RGB-D video sequences under explicit camera trajectory
control. Unlike approaches that treat RGB and depth generation separately,
IDC-Net jointly synthesizes both RGB images and corresponding depth maps within
a unified geometry-aware diffusion model. The joint learning framework
strengthens spatial and geometric alignment across frames, enabling more
precise camera control in the generated sequences. To support the training of
this camera-conditioned model and ensure high geometric fidelity, we construct
a camera-image-depth consistent dataset with metric-aligned RGB videos, depth
maps, and accurate camera poses, which provides precise geometric supervision
with notably improved inter-frame geometric consistency. Moreover, we introduce
a geometry-aware transformer block that enables fine-grained camera control,
enhancing control over the generated sequences. Extensive experiments show that
IDC-Net achieves improvements over state-of-the-art approaches in both visual
quality and geometric consistency of generated scene sequences. Notably, the
generated RGB-D sequences can be directly feed for downstream 3D Scene
reconstruction tasks without extra post-processing steps, showcasing the
practical benefits of our joint learning framework. See more at
https://idcnet-scene.github.io.

</details>


### [62] [ICM-Fusion: In-Context Meta-Optimized LoRA Fusion for Multi-Task Adaptation](https://arxiv.org/abs/2508.04153)
*Yihua Shao,Xiaofeng Lin,Xinwei Long,Siyu Chen,Minxi Yan,Yang Liu,Ziyang Yan,Ao Ma,Hao Tang,Jingcai Guo*

Main category: cs.CV

TL;DR: ICM-Fusion结合元学习和上下文适应，动态平衡优化方向，显著提升多任务适应和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有预训练LoRA融合方法中因权重冲突和领域遗忘导致的多任务适应和泛化能力不足的问题。

Method: 提出In-Context Meta LoRA Fusion (ICM-Fusion)框架，结合元学习和上下文适应，通过任务向量算术动态平衡优化方向，并利用Fusion VAE (F-VAE)重构融合LoRA。

Result: 实验证明ICM-Fusion适用于多种架构模型和任务，显著减少多任务损失并在少样本场景中实现任务增强。

Conclusion: ICM-Fusion通过结合元学习和上下文适应，显著降低了多任务损失，并在少样本场景中实现了任务增强。

Abstract: Enabling multi-task adaptation in pre-trained Low-Rank Adaptation (LoRA)
models is crucial for enhancing their generalization capabilities. Most
existing pre-trained LoRA fusion methods decompose weight matrices, sharing
similar parameters while merging divergent ones. However, this paradigm
inevitably induces inter-weight conflicts and leads to catastrophic domain
forgetting. While incremental learning enables adaptation to multiple tasks, it
struggles to achieve generalization in few-shot scenarios. Consequently, when
the weight data follows a long-tailed distribution, it can lead to forgetting
in the fused weights. To address this issue, we propose In-Context Meta LoRA
Fusion (ICM-Fusion), a novel framework that synergizes meta-learning with
in-context adaptation. The key innovation lies in our task vector arithmetic,
which dynamically balances conflicting optimization directions across domains
through learned manifold projections. ICM-Fusion obtains the optimal task
vector orientation for the fused model in the latent space by adjusting the
orientation of the task vectors. Subsequently, the fused LoRA is reconstructed
by a self-designed Fusion VAE (F-VAE) to realize multi-task LoRA generation. We
have conducted extensive experiments on visual and linguistic tasks, and the
experimental results demonstrate that ICM-Fusion can be adapted to a wide range
of architectural models and applied to various tasks. Compared to the current
pre-trained LoRA fusion method, ICM-Fusion fused LoRA can significantly reduce
the multi-tasking loss and can even achieve task enhancement in few-shot
scenarios.

</details>


### [63] [ProtoN: Prototype Node Graph Neural Network for Unconstrained Multi-Impression Ear Recognition](https://arxiv.org/abs/2508.04381)
*Santhoshkumar Peddi,Sadhvik Bathini,Arun Balasubramanian,Monalisa Sarma,Debasis Samanta*

Main category: cs.CV

TL;DR: ProtoN是一种基于图的少样本学习框架，通过PGNN和跨图原型对齐策略，显著提升了耳部识别的准确性和类间区分度。


<details>
  <summary>Details</summary>
Motivation: 耳部生物识别作为一种稳定且非接触的身份识别方式，其效果受限于标注数据的稀缺性和类内变异性。现有方法无法有效捕捉一致且具有判别性的表示。

Method: 提出了一个少样本学习框架ProtoN，采用基于图的方法处理多个身份印象，并使用Prototype Graph Neural Network（PGNN）层通过双路径消息传递机制优化表示。

Result: 在五个基准耳部数据集上的实验表明，ProtoN达到了最高99.60%的Rank-1识别准确率和低至0.025的EER。

Conclusion: ProtoN框架在有限数据条件下实现了最先进的性能，证明了其在少样本耳部识别中的有效性。

Abstract: Ear biometrics offer a stable and contactless modality for identity
recognition, yet their effectiveness remains limited by the scarcity of
annotated data and significant intra-class variability. Existing methods
typically extract identity features from individual impressions in isolation,
restricting their ability to capture consistent and discriminative
representations. To overcome these limitations, a few-shot learning framework,
ProtoN, is proposed to jointly process multiple impressions of an identity
using a graph-based approach. Each impression is represented as a node in a
class-specific graph, alongside a learnable prototype node that encodes
identity-level information. This graph is processed by a Prototype Graph Neural
Network (PGNN) layer, specifically designed to refine both impression and
prototype representations through a dual-path message-passing mechanism. To
further enhance discriminative power, the PGNN incorporates a cross-graph
prototype alignment strategy that improves class separability by enforcing
intra-class compactness while maintaining inter-class distinction.
Additionally, a hybrid loss function is employed to balance episodic and global
classification objectives, thereby improving the overall structure of the
embedding space. Extensive experiments on five benchmark ear datasets
demonstrate that ProtoN achieves state-of-the-art performance, with Rank-1
identification accuracy of up to 99.60% and an Equal Error Rate (EER) as low as
0.025, showing the effectiveness for few-shot ear recognition under limited
data conditions.

</details>


### [64] [Audio-Assisted Face Video Restoration with Temporal and Identity Complementary Learning](https://arxiv.org/abs/2508.04161)
*Yuqin Cao,Yixuan Gao,Wei Sun,Xiaohong Liu,Yulun Zhang,Xiongkuo Min*

Main category: cs.CV

TL;DR: GAVN is a novel audio-assisted network for face video restoration, leveraging temporal and identity features to outperform existing methods in tasks like deblurring and super-resolution.


<details>
  <summary>Details</summary>
Motivation: Existing face video restoration methods often ignore the correlation between visual and audio features, especially in mouth regions, limiting their effectiveness.

Method: GAVN employs a two-stage approach: capturing inter-frame temporal features in low-resolution space for coarse restoration, and extracting intra-frame identity features in high-resolution space with audio and face landmarks for detailed restoration.

Result: GAVN demonstrates superior performance in handling various video distortions, validated by experimental results.

Conclusion: GAVN outperforms existing state-of-the-art methods in face video restoration tasks like compression artifact removal, deblurring, and super-resolution.

Abstract: Face videos accompanied by audio have become integral to our daily lives,
while they often suffer from complex degradations. Most face video restoration
methods neglect the intrinsic correlations between the visual and audio
features, especially in mouth regions. A few audio-aided face video restoration
methods have been proposed, but they only focus on compression artifact
removal. In this paper, we propose a General Audio-assisted face Video
restoration Network (GAVN) to address various types of streaming video
distortions via identity and temporal complementary learning. Specifically,
GAVN first captures inter-frame temporal features in the low-resolution space
to restore frames coarsely and save computational cost. Then, GAVN extracts
intra-frame identity features in the high-resolution space with the assistance
of audio signals and face landmarks to restore more facial details. Finally,
the reconstruction module integrates temporal features and identity features to
generate high-quality face videos. Experimental results demonstrate that GAVN
outperforms the existing state-of-the-art methods on face video compression
artifact removal, deblurring, and super-resolution. Codes will be released upon
publication.

</details>


### [65] [Deep Learning-based Scalable Image-to-3D Facade Parser for Generating Thermal 3D Building Models](https://arxiv.org/abs/2508.04406)
*Yinan Yu,Alex Gonzalez-Caceres,Samuel Scheidegger,Sanjay Somanath,Alexander Hollberg*

Main category: cs.CV

TL;DR: SI3FP 是一种生成 LoD3 热模型的管道，结合计算机视觉和深度学习，可扩展且准确，适用于大规模能源翻新规划。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可扩展和准确识别 LoD3 热模型中的特征（如窗户）方面存在挑战，而早期翻新规划需要此类模型。

Method: SI3FP 通过计算机视觉和深度学习从图像中提取几何信息，直接在正交图像平面中建模几何基元，减少透视失真，并支持稀疏和密集数据源。

Result: 在典型瑞典住宅建筑上测试时，SI3FP 在窗户与墙壁比例估计中实现约5%的误差，满足了早期翻新分析的准确性要求。

Conclusion: SI3FP 提供了一种可扩展且准确的方法，用于生成 LoD3 热模型，支持大规模能源翻新规划，并在城市发展和规划中有广泛应用。

Abstract: Renovating existing buildings is essential for climate impact. Early-phase
renovation planning requires simulations based on thermal 3D models at Level of
Detail (LoD) 3, which include features like windows. However, scalable and
accurate identification of such features remains a challenge. This paper
presents the Scalable Image-to-3D Facade Parser (SI3FP), a pipeline that
generates LoD3 thermal models by extracting geometries from images using both
computer vision and deep learning. Unlike existing methods relying on
segmentation and projection, SI3FP directly models geometric primitives in the
orthographic image plane, providing a unified interface while reducing
perspective distortions. SI3FP supports both sparse (e.g., Google Street View)
and dense (e.g., hand-held camera) data sources. Tested on typical Swedish
residential buildings, SI3FP achieved approximately 5% error in window-to-wall
ratio estimates, demonstrating sufficient accuracy for early-stage renovation
analysis. The pipeline facilitates large-scale energy renovation planning and
has broader applications in urban development and planning.

</details>


### [66] [ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations](https://arxiv.org/abs/2508.04166)
*Subhankar Swain,Naquee Rizwan,Nayandeep Deb,Vishwajeet Singh Solanki,Vishwa Gangadhar S,Animesh Mukherjee*

Main category: cs.CV

TL;DR: 本研究构建了一个带有辅助元数据的模因数据集，并提出标签生成模块，显著提升了视觉语言模型在有害内容检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体在放大有害言论中扮演核心角色，而模因作为广泛使用的在线传播方式，常被用于传播有害内容。然而，数据可访问性的限制和数据集构建的高成本阻碍了强大的模因审核系统的开发。

Method: 本研究首先构建了一个包含6,300个真实世界模因帖子的数据集，并对其进行两阶段标注：（i）二元分类为有毒或正常，（ii）对有毒模因进行细粒度标注（仇恨、危险或冒犯）。此外，还提出了一个标签生成模块，为未标注的模因生成社会相关标签。

Result: 实验结果表明，结合社会相关标签显著提升了现有视觉语言模型在检测任务中的性能。

Conclusion: 本研究为多模态在线环境中的内容审核提供了一个新颖且可扩展的基础，通过引入带有辅助元数据的模因数据集和标签生成模块，显著提升了现有视觉语言模型在检测任务中的性能。

Abstract: The 2025 Global Risks Report identifies state-based armed conflict and
societal polarisation among the most pressing global threats, with social media
playing a central role in amplifying toxic discourse. Memes, as a widely used
mode of online communication, often serve as vehicles for spreading harmful
content. However, limitations in data accessibility and the high cost of
dataset curation hinder the development of robust meme moderation systems. To
address this challenge, in this work, we introduce a first-of-its-kind dataset
of 6,300 real-world meme-based posts annotated in two stages: (i) binary
classification into toxic and normal, and (ii) fine-grained labelling of toxic
memes as hateful, dangerous, or offensive. A key feature of this dataset is
that it is enriched with auxiliary metadata of socially relevant tags,
enhancing the context of each meme. In addition, we propose a tag generation
module that produces socially grounded tags, because most in-the-wild memes
often do not come with tags. Experimental results show that incorporating these
tags substantially enhances the performance of state-of-the-art VLMs detection
tasks. Our contributions offer a novel and scalable foundation for improved
content moderation in multimodal online environments.

</details>


### [67] [AD-FM: Multimodal LLMs for Anomaly Detection via Multi-Stage Reasoning and Fine-Grained Reward Optimization](https://arxiv.org/abs/2508.04175)
*Jingyi Liao,Yongyi Su,Rong-Cheng Tu,Zhao Jin,Wenhao Sun,Yiting Li,Dacheng Tao,Xun Xu,Xulei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种结合多阶段审慎推理和细粒度奖励的框架，解决了MLLM在专业异常检测中的适应问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GRPO的方法在专业异常检测中存在两个关键限制：模型产生统一响应时训练数据利用不足，以及对推理过程的监督不足，导致缺乏深思熟虑的分析。

Method: 引入多阶段审慎推理过程，从区域识别到聚焦检查，生成多样化的响应模式；开发细粒度奖励机制，结合分类准确性和定位监督，将二元反馈转化为连续信号。

Result: 在多个工业数据集上的综合评估显示，该方法在适应通用视觉语言模型到专业异常检测方面实现了显著的性能提升。

Conclusion: 本文提出的框架通过多阶段审慎推理过程和细粒度奖励机制，显著提升了通用视觉语言模型在专业异常检测领域的适应能力，有效弥补了通用MLLM能力与细粒度视觉判别需求之间的差距。

Abstract: While Multimodal Large Language Models (MLLMs) demonstrate remarkable
capabilities across diverse domains, their application to specialized anomaly
detection (AD) remains constrained by domain adaptation challenges. Existing
Group Relative Policy Optimization (GRPO) based approaches suffer from two
critical limitations: inadequate training data utilization when models produce
uniform responses, and insufficient supervision over reasoning processes that
encourage immediate binary decisions without deliberative analysis. We propose
a comprehensive framework addressing these limitations through two synergistic
innovations. First, we introduce a multi-stage deliberative reasoning process
that guides models from region identification to focused examination,
generating diverse response patterns essential for GRPO optimization while
enabling structured supervision over analytical workflows. Second, we develop a
fine-grained reward mechanism incorporating classification accuracy and
localization supervision, transforming binary feedback into continuous signals
that distinguish genuine analytical insight from spurious correctness.
Comprehensive evaluation across multiple industrial datasets demonstrates
substantial performance improvements in adapting general vision-language models
to specialized anomaly detection. Our method achieves superior accuracy with
efficient adaptation of existing annotations, effectively bridging the gap
between general-purpose MLLM capabilities and the fine-grained visual
discrimination required for detecting subtle manufacturing defects and
structural irregularities.

</details>


### [68] [Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model](https://arxiv.org/abs/2508.04472)
*Hongxu Chen,Zhen Wang,Taoran Mei,Lin Li,Bowei Zhu,Runshi Li,Long Chen*

Main category: cs.CV

TL;DR: ErasePro通过零残差约束和分层更新策略，优化文本到图像模型的概念擦除效果，解决了现有方法的不完全擦除和质量下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法存在不完全擦除（由于非零对齐残差）和生成质量下降（集中在少数深层参数更新）的局限性。

Method: ErasePro采用严格零残差约束的优化目标，并实施渐进式分层更新策略，从浅层到深层逐步转移目标概念特征至锚概念特征。

Result: 实验证明ErasePro在实例、艺术风格和裸露内容擦除等任务中表现优异，实现了更彻底的概念擦除并保持了生成质量。

Conclusion: ErasePro通过引入严格零残差约束和渐进式分层更新策略，有效解决了现有概念擦除方法的不完全擦除和生成质量下降问题，在不同擦除任务中均表现出色。

Abstract: Concept Erasure, which aims to prevent pretrained text-to-image models from
generating content associated with semantic-harmful concepts (i.e., target
concepts), is getting increased attention. State-of-the-art methods formulate
this task as an optimization problem: they align all target concepts with
semantic-harmless anchor concepts, and apply closed-form solutions to update
the model accordingly. While these closed-form methods are efficient, we argue
that existing methods have two overlooked limitations: 1) They often result in
incomplete erasure due to "non-zero alignment residual", especially when text
prompts are relatively complex. 2) They may suffer from generation quality
degradation as they always concentrate parameter updates in a few deep layers.
To address these issues, we propose a novel closed-form method ErasePro: it is
designed for more complete concept erasure and better preserving overall
generative quality. Specifically, ErasePro first introduces a strict
zero-residual constraint into the optimization objective, ensuring perfect
alignment between target and anchor concept features and enabling more complete
erasure. Secondly, it employs a progressive, layer-wise update strategy that
gradually transfers target concept features to those of the anchor concept from
shallow to deep layers. As the depth increases, the required parameter changes
diminish, thereby reducing deviations in sensitive deep layers and preserving
generative quality. Empirical results across different concept erasure tasks
(including instance, art style, and nudity erasure) have demonstrated the
effectiveness of our ErasePro.

</details>


### [69] [Uncertainty-Aware Spatial Color Correlation for Low-Light Image Enhancement](https://arxiv.org/abs/2508.04176)
*Jin Kuang,Dong Liu,Yukuang Zhang,Shengsheng Wang*

Main category: cs.CV

TL;DR: U2CLLIE是一个整合不确定性感知和因果关联建模的低光图像增强框架，显著提升极暗条件下的图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法主要关注架构创新，而忽略了在极暗条件下特征表示的内在不确定性，导致梯度退化和噪声主导严重影响模型可靠性和因果推理。

Method: 提出U2CLLIE框架，包含两个关键组件：(1) 不确定性感知双域去噪模块（UaD），利用高斯引导的自适应频域特征增强（G2AF）抑制频域噪声并优化熵驱动表示；(2) 分层因果感知框架，包括亮度增强网络（LEN）和两个非对称因果关联建模模块（NeCo和AsC）。

Result: 实验证明U2CLLIE在多个基准数据集上达到最先进性能，具有鲁棒性和强泛化能力。

Conclusion: U2CLLIE通过整合不确定性感知增强和空间-颜色因果关联建模，在多个基准数据集上实现了最先进的性能，表现出强大的泛化能力。

Abstract: Most existing low-light image enhancement approaches primarily focus on
architectural innovations, while often overlooking the intrinsic uncertainty
within feature representations particularly under extremely dark conditions
where degraded gradient and noise dominance severely impair model reliability
and causal reasoning. To address these issues, we propose U2CLLIE, a novel
framework that integrates uncertainty-aware enhancement and spatial-color
causal correlation modeling. From the perspective of entropy-based uncertainty,
our framework introduces two key components: (1) An Uncertainty-Aware
Dual-domain Denoise (UaD) Module, which leverages Gaussian-Guided Adaptive
Frequency Domain Feature Enhancement (G2AF) to suppress frequency-domain noise
and optimize entropy-driven representations. This module enhances spatial
texture extraction and frequency-domain noise suppression/structure refinement,
effectively mitigating gradient vanishing and noise dominance. (2) A
hierarchical causality-aware framework, where a Luminance Enhancement Network
(LEN) first performs coarse brightness enhancement on dark regions. Then,
during the encoder-decoder phase, two asymmetric causal correlation modeling
modules Neighborhood Correlation State Space (NeCo) and Adaptive Spatial-Color
Calibration (AsC) collaboratively construct hierarchical causal constraints.
These modules reconstruct and reinforce neighborhood structure and color
consistency in the feature space. Extensive experiments demonstrate that
U2CLLIE achieves state-of-the-art performance across multiple benchmark
datasets, exhibiting robust performance and strong generalization across
various scenes.

</details>


### [70] [Learning Robust Intervention Representations with Delta Embeddings](https://arxiv.org/abs/2508.04492)
*Panagiotis Alimisis,Christos Diou*

Main category: cs.CV

TL;DR: 本文提出 Causal Delta Embeddings 作为干预的稀疏、不变表示，显著提升 OOD 鲁棒性，无需额外监督。


<details>
  <summary>Details</summary>
Motivation: 尽管因果表示学习在提高模型泛化性和鲁棒性方面取得了进展，但对干预本身的表示研究较少。本文旨在通过专注于潜在空间中的干预表示来提升 OOD 鲁棒性。

Method: 提出了一种无需额外监督的框架，能够从图像对中学习因果表示，特别是通过 Causal Delta Embeddings 来表示干预。

Result: 在 Causal Triplet 挑战中的实验表明，Causal Delta Embeddings 在 OOD 设置中非常有效。

Conclusion: Causal Delta Embeddings 在 OOD 设置中表现出色，显著超越了合成和真实世界基准中的基线性能。

Abstract: Causal representation learning has attracted significant research interest
during the past few years, as a means for improving model generalization and
robustness. Causal representations of interventional image pairs, have the
property that only variables corresponding to scene elements affected by the
intervention / action are changed between the start state and the end state.
While most work in this area has focused on identifying and representing the
variables of the scene under a causal model, fewer efforts have focused on
representations of the interventions themselves. In this work, we show that an
effective strategy for improving out of distribution (OOD) robustness is to
focus on the representation of interventions in the latent space. Specifically,
we propose that an intervention can be represented by a Causal Delta Embedding
that is invariant to the visual scene and sparse in terms of the causal
variables it affects. Leveraging this insight, we propose a framework that is
capable of learning causal representations from image pairs, without any
additional supervision. Experiments in the Causal Triplet challenge demonstrate
that Causal Delta Embeddings are highly effective in OOD settings,
significantly exceeding baseline performance in both synthetic and real-world
benchmarks.

</details>


### [71] [Deeper Inside Deep ViT](https://arxiv.org/abs/2508.04181)
*Sungrae Hong*

Main category: cs.CV

TL;DR: 研究了ViT-22B在本地环境中的训练行为，解决了不稳定性问题，并探索了其在图像生成中的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模视觉模型（如ViT-22B）的研究提供了许多分析，但其实际应用价值尚未完全理解。

Method: 通过本地环境训练ViT-22B模型，分析其反应和训练行为，并对模型进行修改以稳定训练。探索ViT架构在图像生成任务中的应用。

Result: ViT-22B在相同参数规模下性能优于ViT，同时探索了其在图像生成任务中的适用性。

Conclusion: ViT-22B模型在相同参数规模下整体表现优于ViT，且在图像生成任务中展现了潜力。通过模型调整解决了训练不稳定的问题。

Abstract: There have been attempts to create large-scale structures in vision models
similar to LLM, such as ViT-22B. While this research has provided numerous
analyses and insights, our understanding of its practical utility remains
incomplete. Therefore, we examine how this model structure reacts and train in
a local environment. We also highlight the instability in training and make
some model modifications to stabilize it. The ViT-22B model, trained from
scratch, overall outperformed ViT in terms of performance under the same
parameter size. Additionally, we venture into the task of image generation,
which has not been attempted in ViT-22B. We propose an image generation
architecture using ViT and investigate which between ViT and ViT-22B is a more
suitable structure for image generation.

</details>


### [72] [RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning Framework for Explainable Deepfake Detection](https://arxiv.org/abs/2508.04524)
*Tianxiao Li,Zhenglin Huang,Haiquan Wen,Yiwei He,Shuchang Lyu,Baoyuan Wu,Guangliang Cheng*

Main category: cs.CV

TL;DR: RAIDX是一个结合RAG和GRPO的新型深度伪造检测框架，显著提升检测准确性和可解释性，无需大量人工标注。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测方法缺乏透明度，且依赖大量人工标注。RAIDX旨在解决这些问题，提供更准确和可解释的检测方案。

Method: RAIDX框架结合了检索增强生成（RAG）和群体相对策略优化（GRPO），利用外部知识提高检测准确性，并自动生成细粒度文本解释和显著性图。

Result: 在多个基准测试中，RAIDX在检测准确性和提供可解释性方面均达到最先进水平。

Conclusion: RAIDX框架通过整合RAG和GRPO技术，显著提升了深度伪造检测的准确性和可解释性，代表了该领域的重大进步。

Abstract: The rapid advancement of AI-generation models has enabled the creation of
hyperrealistic imagery, posing ethical risks through widespread misinformation.
Current deepfake detection methods, categorized as face specific detectors or
general AI-generated detectors, lack transparency by framing detection as a
classification task without explaining decisions. While several LLM-based
approaches offer explainability, they suffer from coarse-grained analyses and
dependency on labor-intensive annotations. This paper introduces RAIDX
(Retrieval-Augmented Image Deepfake Detection and Explainability), a novel
deepfake detection framework integrating Retrieval-Augmented Generation (RAG)
and Group Relative Policy Optimization (GRPO) to enhance detection accuracy and
decision explainability. Specifically, RAIDX leverages RAG to incorporate
external knowledge for improved detection accuracy and employs GRPO to
autonomously generate fine-grained textual explanations and saliency maps,
eliminating the need for extensive manual annotations. Experiments on multiple
benchmarks demonstrate RAIDX's effectiveness in identifying real or fake, and
providing interpretable rationales in both textual descriptions and saliency
maps, achieving state-of-the-art detection performance while advancing
transparency in deepfake identification. RAIDX represents the first unified
framework to synergize RAG and GRPO, addressing critical gaps in accuracy and
explainability. Our code and models will be publicly available.

</details>


### [73] [RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation](https://arxiv.org/abs/2508.04190)
*Fengyi Wu,Yimian Dai,Tianfang Zhang,Yixuan Ding,Jian Yang,Ming-Ming Cheng,Zhenming Peng*

Main category: cs.CV

TL;DR: RPCANet++结合RPCA与深度网络，提出了高效且可解释的稀疏对象分割框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统RPCA模型存在计算负担大、依赖精细调参和刚性先验等问题，限制了其在动态场景中的适应性。

Method: 提出了RPCANet++框架，包含背景近似模块（BAM）、对象提取模块（OEM）和图像恢复模块（IRM），并引入了记忆增强模块（MAM）和深度对比先验模块（DCPM）以提升性能。

Result: 在多种数据集上的实验表明，RPCANet++在各种成像场景下达到了最先进的性能。

Conclusion: RPCANet++通过结合RPCA的理论优势和深度网络的高效性，为稀疏对象分割设定了新的基准，提供了可靠且可解释的解决方案。

Abstract: Robust principal component analysis (RPCA) decomposes an observation matrix
into low-rank background and sparse object components. This capability has
enabled its application in tasks ranging from image restoration to
segmentation. However, traditional RPCA models suffer from computational
burdens caused by matrix operations, reliance on finely tuned hyperparameters,
and rigid priors that limit adaptability in dynamic scenarios. To solve these
limitations, we propose RPCANet++, a sparse object segmentation framework that
fuses the interpretability of RPCA with efficient deep architectures. Our
approach unfolds a relaxed RPCA model into a structured network comprising a
Background Approximation Module (BAM), an Object Extraction Module (OEM), and
an Image Restoration Module (IRM). To mitigate inter-stage transmission loss in
the BAM, we introduce a Memory-Augmented Module (MAM) to enhance background
feature preservation, while a Deep Contrast Prior Module (DCPM) leverages
saliency cues to expedite object extraction. Extensive experiments on diverse
datasets demonstrate that RPCANet++ achieves state-of-the-art performance under
various imaging scenarios. We further improve interpretability via visual and
numerical low-rankness and sparsity measurements. By combining the theoretical
strengths of RPCA with the efficiency of deep networks, our approach sets a new
baseline for reliable and interpretable sparse object segmentation. Codes are
available at our Project Webpage https://fengyiwu98.github.io/rpcanetx.

</details>


### [74] [MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning](https://arxiv.org/abs/2508.04549)
*Quang-Trung Truong,Yuk-Kwan Wong,Vo Hoang Kim Tuyen Dang,Rinaldi Gotama,Duc Thanh Nguyen,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 该论文针对海洋视频理解的挑战，提出了一个两阶段的字幕生成流程和综合基准，利用视频、文本和分割掩码提升分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视频字幕数据集通常集中在通用或人类中心领域，难以推广到海洋环境的复杂性，也无法深入了解海洋生物。

Method: 采用两阶段的海洋对象导向视频字幕生成流程，结合视频、文本和分割掩码的三元组，以及视频分割技术来检测显著对象转换。

Result: 提出的方法显著丰富了字幕内容的语义，并通过数据集和代码的发布促进了海洋视频理解领域的发展。

Conclusion: 论文提出了一种两阶段海洋对象导向的视频字幕生成流程，并引入了一个综合视频理解基准，利用视频、文本和分割掩码的三元组来促进视觉基础和字幕生成，从而提升海洋视频的理解、分析和生成能力。

Abstract: Marine videos present significant challenges for video understanding due to
the dynamics of marine objects and the surrounding environment, camera motion,
and the complexity of underwater scenes. Existing video captioning datasets,
typically focused on generic or human-centric domains, often fail to generalize
to the complexities of the marine environment and gain insights about marine
life. To address these limitations, we propose a two-stage marine
object-oriented video captioning pipeline. We introduce a comprehensive video
understanding benchmark that leverages the triplets of video, text, and
segmentation masks to facilitate visual grounding and captioning, leading to
improved marine video understanding and analysis, and marine video generation.
Additionally, we highlight the effectiveness of video splitting in order to
detect salient object transitions in scene changes, which significantly enrich
the semantics of captioning content. Our dataset and code have been released at
https://msc.hkustvgd.com.

</details>


### [75] [From Learning to Unlearning: Biomedical Security Protection in Multimodal Large Language Models](https://arxiv.org/abs/2508.04192)
*Dunyuan Xu,Xikai Yang,Yaoqian Li,Jinpeng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 提出了首个生物医学MLLMs遗忘基准MLLMU-Med，评估现有方法效果有限，为未来研究铺路。


<details>
  <summary>Details</summary>
Motivation: 生物医学MLLMs的训练样本可能包含隐私信息和错误知识，导致隐私泄露或错误输出，而完全重新训练成本高昂。机器遗忘成为一种解决方案，但缺乏相关评估数据集。

Method: 通过创新的数据生成流程，将合成的隐私数据和事实错误整合到训练集中，构建了MLLMU-Med基准，并提出了Unlearning Efficiency Score以量化遗忘性能。

Result: 评估了五种遗忘方法，发现它们在生物医学MLLMs中移除有害知识的效果有限，表明仍有改进空间。

Conclusion: 本文提出了首个针对生物医学多模态大语言模型（MLLMs）的安全保护遗忘基准MLLMU-Med，并揭示了现有遗忘方法在生物医学领域的局限性，为未来研究指明了方向。

Abstract: The security of biomedical Multimodal Large Language Models (MLLMs) has
attracted increasing attention. However, training samples easily contain
private information and incorrect knowledge that are difficult to detect,
potentially leading to privacy leakage or erroneous outputs after deployment.
An intuitive idea is to reprocess the training set to remove unwanted content
and retrain the model from scratch. Yet, this is impractical due to significant
computational costs, especially for large language models. Machine unlearning
has emerged as a solution to this problem, which avoids complete retraining by
selectively removing undesired knowledge derived from harmful samples while
preserving required capabilities on normal cases. However, there exist no
available datasets to evaluate the unlearning quality for security protection
in biomedical MLLMs. To bridge this gap, we propose the first benchmark
Multimodal Large Language Model Unlearning for BioMedicine (MLLMU-Med) built
upon our novel data generation pipeline that effectively integrates synthetic
private data and factual errors into the training set. Our benchmark targets
two key scenarios: 1) Privacy protection, where patient private information is
mistakenly included in the training set, causing models to unintentionally
respond with private data during inference; and 2) Incorrectness removal, where
wrong knowledge derived from unreliable sources is embedded into the dataset,
leading to unsafe model responses. Moreover, we propose a novel Unlearning
Efficiency Score that directly reflects the overall unlearning performance
across different subsets. We evaluate five unlearning approaches on MLLMU-Med
and find that these methods show limited effectiveness in removing harmful
knowledge from biomedical MLLMs, indicating significant room for improvement.
This work establishes a new pathway for further research in this promising
field.

</details>


### [76] [CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization](https://arxiv.org/abs/2508.04566)
*Jinxing Zhou,Ziheng Zhou,Yanghao Zhou,Yuxin Mao,Zhangling Duan,Dan Guo*

Main category: cs.CV

TL;DR: 本文提出了一种弱监督下的密集视听事件定位方法，通过跨模态显著锚点和多模块协作，显著提升了定位性能。


<details>
  <summary>Details</summary>
Motivation: 探索在弱监督设置下（仅提供视频级事件标签）的密集视听事件定位任务（W-DAVEL），以解决传统方法在缺乏时间边界标注时的局限性。

Method: 提出了一种包含'Mutual Event Agreement Evaluation'、'Cross-modal Salient Anchor Identification'和'Anchor-based Temporal Propagation'三个模块的方法，用于识别和利用跨模态显著锚点来增强事件语义编码。

Result: 在UnAV-100和ActivityNet1.3数据集上建立了W-DAVEL基准，实验表明该方法达到了最先进的性能。

Conclusion: 本文提出的方法在弱监督的密集视听事件定位（W-DAVEL）任务中表现优异，通过跨模态显著锚点和多模块协作，显著提升了事件定位的准确性。

Abstract: The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally
localize events in untrimmed videos that occur simultaneously in both the audio
and visual modalities. This paper explores DAVEL under a new and more
challenging weakly-supervised setting (W-DAVEL task), where only video-level
event labels are provided and the temporal boundaries of each event are
unknown. We address W-DAVEL by exploiting \textit{cross-modal salient anchors},
which are defined as reliable timestamps that are well predicted under weak
supervision and exhibit highly consistent event semantics across audio and
visual modalities. Specifically, we propose a \textit{Mutual Event Agreement
Evaluation} module, which generates an agreement score by measuring the
discrepancy between the predicted audio and visual event classes. Then, the
agreement score is utilized in a \textit{Cross-modal Salient Anchor
Identification} module, which identifies the audio and visual anchor features
through global-video and local temporal window identification mechanisms. The
anchor features after multimodal integration are fed into an
\textit{Anchor-based Temporal Propagation} module to enhance event semantic
encoding in the original temporal audio and visual features, facilitating
better temporal localization under weak supervision. We establish benchmarks
for W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive
experiments demonstrate that our method achieves state-of-the-art performance.

</details>


### [77] [X-SAM: From Segment Anything to Any Segmentation](https://arxiv.org/abs/2508.04655)
*Hao Wang,Limeng Qiao,Zequn Jie,Zhijian Huang,Chengjian Feng,Qingfang Zheng,Lin Ma,Xiangyuan Lan,Xiaodan Liang*

Main category: cs.CV

TL;DR: X-SAM 是一个多模态大语言模型框架，通过统一的分割任务和训练策略，显著提升了像素级感知能力，并在多个基准测试中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在像素级感知理解上存在不足，而 Segment Anything Model（SAM）在多掩模预测和类别特定分割任务中表现有限。X-SAM 旨在解决这些局限性，扩展分割范式。

Method: 提出了一种名为 X-SAM 的多模态大语言模型框架，引入了视觉接地（VGD）分割任务，并设计了一种支持跨数据集协同训练的统一训练策略。

Result: X-SAM 在广泛的图像分割基准测试中实现了最先进的性能，展示了其在多模态、像素级视觉理解上的高效性。

Conclusion: X-SAM 通过统一的框架和训练策略，显著提升了多模态大语言模型在像素级感知理解上的能力，并在多种图像分割基准测试中达到了最先进的性能。

Abstract: Large Language Models (LLMs) demonstrate strong capabilities in broad
knowledge representation, yet they are inherently deficient in pixel-level
perceptual understanding. Although the Segment Anything Model (SAM) represents
a significant advancement in visual-prompt-driven image segmentation, it
exhibits notable limitations in multi-mask prediction and category-specific
segmentation tasks, and it cannot integrate all segmentation tasks within a
unified model architecture. To address these limitations, we present X-SAM, a
streamlined Multimodal Large Language Model (MLLM) framework that extends the
segmentation paradigm from \textit{segment anything} to \textit{any
segmentation}. Specifically, we introduce a novel unified framework that
enables more advanced pixel-level perceptual comprehension for MLLMs.
Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD)
segmentation, which segments all instance objects with interactive visual
prompts and empowers MLLMs with visual grounded, pixel-wise interpretative
capabilities. To enable effective training on diverse data sources, we present
a unified training strategy that supports co-training across multiple datasets.
Experimental results demonstrate that X-SAM achieves state-of-the-art
performance on a wide range of image segmentation benchmarks, highlighting its
efficiency for multimodal, pixel-level visual understanding. Code is available
at https://github.com/wanghao9610/X-SAM.

</details>


### [78] [Bootstrap Deep Spectral Clustering with Optimal Transport](https://arxiv.org/abs/2508.04200)
*Wengang Guo,Wei Ye,Chunchun Chen,Xin Sun,Christian Böhm,Claudia Plant,Susanto Rahardja*

Main category: cs.CV

TL;DR: BootSC 是一种端到端的深度谱聚类模型，通过联合学习和优化技术显著提升了聚类性能。


<details>
  <summary>Details</summary>
Motivation: 解决谱聚类中存在的优化过程不连贯和表示能力有限的问题。

Method: 提出了一种深度谱聚类模型（BootSC），联合学习谱聚类的所有阶段（亲和矩阵构建、谱嵌入和k-means聚类），并通过最优传输监督和正交化技术优化模型。

Result: BootSC 在多个数据集上表现优异，例如在 ImageNet-Dogs 数据集上实现了 16% NMI 的提升。

Conclusion: BootSC 通过端到端的联合学习框架，显著提升了谱聚类的性能，并在多个数据集上实现了最先进的聚类效果。

Abstract: Spectral clustering is a leading clustering method. Two of its major
shortcomings are the disjoint optimization process and the limited
representation capacity. To address these issues, we propose a deep spectral
clustering model (named BootSC), which jointly learns all stages of spectral
clustering -- affinity matrix construction, spectral embedding, and $k$-means
clustering -- using a single network in an end-to-end manner. BootSC leverages
effective and efficient optimal-transport-derived supervision to bootstrap the
affinity matrix and the cluster assignment matrix. Moreover, a
semantically-consistent orthogonal re-parameterization technique is introduced
to orthogonalize spectral embeddings, significantly enhancing the
discrimination capability. Experimental results indicate that BootSC achieves
state-of-the-art clustering performance. For example, it accomplishes a notable
16\% NMI improvement over the runner-up method on the challenging ImageNet-Dogs
dataset. Our code is available at https://github.com/spdj2271/BootSC.

</details>


### [79] [YOLOv8-Based Deep Learning Model for Automated Poultry Disease Detection and Health Monitoring paper](https://arxiv.org/abs/2508.04658)
*Akhil Saketh Reddy Sabbella,Ch. Lakshmi Prachothan,Eswar Kumar Panta*

Main category: cs.CV

TL;DR: 利用YOLO v8深度学习模型实时检测鸡群疾病，提升养殖场管理效率与生物安全性。


<details>
  <summary>Details</summary>
Motivation: 传统鸡病检测依赖人工观察，效率低且易出错。研究旨在通过AI技术实现自动化、高效的疾病检测，减少经济损失。

Method: 研究采用YOLO v8深度学习模型，通过分析高分辨率鸡群照片，检测疾病迹象（如行为和外貌异常）。使用大量标注数据集训练算法，实现实时准确识别。

Result: 训练后的算法能准确实时识别病鸡，并及时向农场主发出警报。YOLO v8的实时特性为农场管理提供了可扩展且高效的解决方案。

Conclusion: 该AI技术通过YOLO v8模型实现了鸡群疾病的实时检测，提高了健康管理水平，减少了人工检查的需求，并增强了大规模养殖场的生物安全性。

Abstract: In the poultry industry, detecting chicken illnesses is essential to avoid
financial losses. Conventional techniques depend on manual observation, which
is laborious and prone to mistakes. Using YOLO v8 a deep learning model for
real-time object recognition. This study suggests an AI based approach, by
developing a system that analyzes high resolution chicken photos, YOLO v8
detects signs of illness, such as abnormalities in behavior and appearance. A
sizable, annotated dataset has been used to train the algorithm, which provides
accurate real-time identification of infected chicken and prompt warnings to
farm operators for prompt action. By facilitating early infection
identification, eliminating the need for human inspection, and enhancing
biosecurity in large-scale farms, this AI technology improves chicken health
management. The real-time features of YOLO v8 provide a scalable and effective
method for improving farm management techniques.

</details>


### [80] [HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models](https://arxiv.org/abs/2508.04663)
*Young D. Kwon,Rui Li,Sijia Li,Da Li,Sourav Bhattacharya,Stylianos I. Venieris*

Main category: cs.CV

TL;DR: HierarchicalPrune通过层次剪枝、权重保护和敏感度蒸馏技术，显著压缩扩散模型规模，适合设备端推理，且保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型参数量巨大（8-11B），在资源受限设备上推理面临挑战。

Method: HierarchicalPrune结合了三种技术：层次位置剪枝（Hierarchical Position Pruning）、位置权重保护（Positional Weight Preservation）和敏感度引导蒸馏（Sensitivity-Guided Distillation）。

Result: 结合INT4量化后，内存占用减少77.5-80.4%（如从15.8GB降至3.2GB），延迟降低27.9-38.0%，生成质量仅轻微下降（GenEval评分降低2.6%，HPSv2评分降低7%）。用户研究表明其感知质量与原模型相当。

Conclusion: HierarchicalPrune框架成功地将大规模扩散模型压缩至适合设备端推理的规模，同时保持输出图像质量，显著优于之前的工作。

Abstract: State-of-the-art text-to-image diffusion models (DMs) achieve remarkable
quality, yet their massive parameter scale (8-11B) poses significant challenges
for inferences on resource-constrained devices. In this paper, we present
HierarchicalPrune, a novel compression framework grounded in a key observation:
DM blocks exhibit distinct functional hierarchies, where early blocks establish
semantic structures while later blocks handle texture refinements.
HierarchicalPrune synergistically combines three techniques: (1) Hierarchical
Position Pruning, which identifies and removes less essential later blocks
based on position hierarchy; (2) Positional Weight Preservation, which
systematically protects early model portions that are essential for semantic
structural integrity; and (3) Sensitivity-Guided Distillation, which adjusts
knowledge-transfer intensity based on our discovery of block-wise sensitivity
variations. As a result, our framework brings billion-scale diffusion models
into a range more suitable for on-device inference, while preserving the
quality of the output images. Specifically, when combined with INT4 weight
quantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction
(e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on
server and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score
and 7% in HPSv2 score compared to the original model. Last but not least, our
comprehensive user study with 85 participants demonstrates that
HierarchicalPrune maintains perceptual quality comparable to the original model
while significantly outperforming prior works.

</details>


### [81] [Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network for Lung Disease Classification](https://arxiv.org/abs/2508.04205)
*Jianxun Yu,Ruiquan Ge,Zhipeng Wang,Cheng Yang,Chenyu Lin,Xianjun Fu,Jikui Liu,Ahmed Elazab,Changmiao Wang*

Main category: cs.CV

TL;DR: MMCAF-Net通过多模态多尺度交叉注意力融合，有效提升了医学影像和电子健康记录数据的融合质量，显著提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 医学疾病诊断面临小病变误诊等挑战，且医学影像与电子健康记录数据的维度差异导致有效对齐和融合困难。

Method: 提出了一种多模态多尺度交叉注意力融合网络（MMCAF-Net），采用特征金字塔结构和高效的3D多尺度卷积注意力模块，结合多尺度交叉注意力模块解决维度不一致问题。

Result: 在Lung-PET-CT-Dx数据集上的评估显示，MMCAF-Net显著提升了诊断准确性。

Conclusion: MMCAF-Net显著提高了医学疾病诊断的准确性，特别是在小病变的识别上，超越了现有最先进方法。

Abstract: The diagnosis of medical diseases faces challenges such as the misdiagnosis
of small lesions. Deep learning, particularly multimodal approaches, has shown
great potential in the field of medical disease diagnosis. However, the
differences in dimensionality between medical imaging and electronic health
record data present challenges for effective alignment and fusion. To address
these issues, we propose the Multimodal Multiscale Cross-Attention Fusion
Network (MMCAF-Net). This model employs a feature pyramid structure combined
with an efficient 3D multi-scale convolutional attention module to extract
lesion-specific features from 3D medical images. To further enhance multimodal
data integration, MMCAF-Net incorporates a multi-scale cross-attention module,
which resolves dimensional inconsistencies, enabling more effective feature
fusion. We evaluated MMCAF-Net on the Lung-PET-CT-Dx dataset, and the results
showed a significant improvement in diagnostic accuracy, surpassing current
state-of-the-art methods. The code is available at
https://github.com/yjx1234/MMCAF-Net

</details>


### [82] [What Holds Back Open-Vocabulary Segmentation?](https://arxiv.org/abs/2508.04211)
*Josip Šarić,Ivan Martinović,Matej Kristan,Siniša Šegvić*

Main category: cs.CV

TL;DR: 本文提出oracle组件解决开放词汇模型性能瓶颈问题，通过实证研究为未来方向提供见解。


<details>
  <summary>Details</summary>
Motivation: 当前的开放词汇方法在语言-图像预训练中存在性能瓶颈，导致性能停滞近两年。

Method: 利用groundtruth信息设计oracle组件，识别并解耦开放词汇模型中的瓶颈。

Result: 验证实验提供了重要的实证发现，揭示了开放词汇模型的失败原因，并提出了未来研究的潜在方向。

Conclusion: 本文通过提出新颖的oracle组件，成功识别并解耦了开放词汇模型中的瓶颈问题，为未来研究提供了重要的实证发现和方向。

Abstract: Standard segmentation setups are unable to deliver models that can recognize
concepts outside the training taxonomy. Open-vocabulary approaches promise to
close this gap through language-image pretraining on billions of image-caption
pairs. Unfortunately, we observe that the promise is not delivered due to
several bottlenecks that have caused the performance to plateau for almost two
years. This paper proposes novel oracle components that identify and decouple
these bottlenecks by taking advantage of the groundtruth information. The
presented validation experiments deliver important empirical findings that
provide a deeper insight into the failures of open-vocabulary models and
suggest prominent approaches to unlock the future research.

</details>


### [83] [SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition](https://arxiv.org/abs/2508.04224)
*Jiahui Li,Shengeng Tang,Jingxuan He,Gang Huang,Zhangye Wang,Yantao Pan,Lechao Cheng*

Main category: cs.CV

TL;DR: SplitGaussian通过分解静态和动态组件，解决了动态3D场景重建中的运动泄漏和几何失真问题，提升了渲染质量和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯泼溅的动态场景重建方法因静态和动态元素共享表示，导致运动泄漏、几何失真和时间闪烁。

Method: 提出SplitGaussian框架，将场景表示分解为静态和动态组件，解耦运动建模与背景几何，仅允许动态分支随时间变形。

Result: SplitGaussian在渲染质量、几何稳定性和运动分离方面优于现有最先进方法。

Conclusion: SplitGaussian通过显式分解静态和动态组件，显著提升了动态3D场景重建的渲染质量、几何稳定性和运动分离效果。

Abstract: Reconstructing dynamic 3D scenes from monocular video remains fundamentally
challenging due to the need to jointly infer motion, structure, and appearance
from limited observations. Existing dynamic scene reconstruction methods based
on Gaussian Splatting often entangle static and dynamic elements in a shared
representation, leading to motion leakage, geometric distortions, and temporal
flickering. We identify that the root cause lies in the coupled modeling of
geometry and appearance across time, which hampers both stability and
interpretability. To address this, we propose \textbf{SplitGaussian}, a novel
framework that explicitly decomposes scene representations into static and
dynamic components. By decoupling motion modeling from background geometry and
allowing only the dynamic branch to deform over time, our method prevents
motion artifacts in static regions while supporting view- and time-dependent
appearance refinement. This disentangled design not only enhances temporal
consistency and reconstruction fidelity but also accelerates convergence.
Extensive experiments demonstrate that SplitGaussian outperforms prior
state-of-the-art methods in rendering quality, geometric stability, and motion
separation.

</details>


### [84] [Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting](https://arxiv.org/abs/2508.04227)
*Yuyang Liu,Qiuhe Hong,Linlan Huang,Alexandra Gomez-Villa,Dipam Goswami,Xialei Liu,Joost van de Weijer,Yonghong Tian*

Main category: cs.CV

TL;DR: 该调查系统回顾了视觉语言模型的持续学习问题，提出了解决方案分类法，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在多模态任务中表现出色，但在非平稳数据中的持续学习能力仍面临挑战，尤其是跨模态对齐和泛化能力易受灾难性遗忘影响。

Method: 基于三个核心失败模式，提出了一个挑战驱动的分类法，将解决方案映射到目标问题：(1)多模态重放策略，(2)跨模态正则化，(3)参数高效适应。

Result: 提出了针对VLM-CL的解决方案分类法，并分析了当前评估协议、数据集和指标，指出了需要更好的基准测试。

Conclusion: 该调查旨在为开发终身视觉语言系统的研究人员提供一个全面且诊断性的参考。

Abstract: Vision-language models (VLMs) have achieved impressive performance across
diverse multimodal tasks by leveraging large-scale pre-training. However,
enabling them to learn continually from non-stationary data remains a major
challenge, as their cross-modal alignment and generalization capabilities are
particularly vulnerable to catastrophic forgetting. Unlike traditional unimodal
continual learning (CL), VLMs face unique challenges such as cross-modal
feature drift, parameter interference due to shared architectures, and
zero-shot capability erosion. This survey offers the first focused and
systematic review of continual learning for VLMs (VLM-CL). We begin by
identifying the three core failure modes that degrade performance in VLM-CL.
Based on these, we propose a challenge-driven taxonomy that maps solutions to
their target problems: (1) \textit{Multi-Modal Replay Strategies} address
cross-modal drift through explicit or implicit memory mechanisms; (2)
\textit{Cross-Modal Regularization} preserves modality alignment during
updates; and (3) \textit{Parameter-Efficient Adaptation} mitigates parameter
interference with modular or low-rank updates. We further analyze current
evaluation protocols, datasets, and metrics, highlighting the need for better
benchmarks that capture VLM-specific forgetting and compositional
generalization. Finally, we outline open problems and future directions,
including continual pre-training and compositional zero-shot learning. This
survey aims to serve as a comprehensive and diagnostic reference for
researchers developing lifelong vision-language systems. All resources are
available at:
https://github.com/YuyangSunshine/Awesome-Continual-learning-of-Vision-Language-Models.

</details>


### [85] [Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction](https://arxiv.org/abs/2508.04229)
*Yu Liu,Zhijie Liu,Xiao Ren,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出了一种结合行人运动意图的扩散模型，用于多模态轨迹预测，在ETH和UCY基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 行人轨迹预测对自动驾驶车辆的路径规划和运动控制至关重要，但现有扩散模型未能充分结合行人运动意图，限制了预测的准确性和可解释性。

Method: 提出了一种基于扩散的多模态轨迹预测模型，结合行人的运动意图（分为横向和纵向分量），并引入行人意图识别模块。采用高效的引导机制生成可解释的轨迹。

Result: 在ETH和UCY基准测试中，该方法表现优于现有先进方法。

Conclusion: 该方法在ETH和UCY基准测试中表现出色，证明了其预测行人轨迹的竞争力和有效性。

Abstract: Predicting pedestrian motion trajectories is critical for path planning and
motion control of autonomous vehicles. However, accurately forecasting crowd
trajectories remains a challenging task due to the inherently multimodal and
uncertain nature of human motion. Recent diffusion-based models have shown
promising results in capturing the stochasticity of pedestrian behavior for
trajectory prediction. However, few diffusion-based approaches explicitly
incorporate the underlying motion intentions of pedestrians, which can limit
the interpretability and precision of prediction models. In this work, we
propose a diffusion-based multimodal trajectory prediction model that
incorporates pedestrians' motion intentions into the prediction framework. The
motion intentions are decomposed into lateral and longitudinal components, and
a pedestrian intention recognition module is introduced to enable the model to
effectively capture these intentions. Furthermore, we adopt an efficient
guidance mechanism that facilitates the generation of interpretable
trajectories. The proposed framework is evaluated on two widely used human
trajectory prediction benchmarks, ETH and UCY, on which it is compared against
state-of-the-art methods. The experimental results demonstrate that our method
achieves competitive performance.

</details>


### [86] [DocVCE: Diffusion-based Visual Counterfactual Explanations for Document Image Classification](https://arxiv.org/abs/2508.04233)
*Saifullah Saifullah,Stefan Agne,Andreas Dengel,Sheraz Ahmed*

Main category: cs.CV

TL;DR: 本文提出DocVCE方法，利用生成式反事实解释提升文档图像分类模型的可解释性，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着黑盒AI决策系统在文档处理中的广泛应用，提高其透明度和可靠性变得至关重要。现有特征重要性图难以解释模型全局特征，因此需探索更直观的解释方法。

Method: 提出DocVCE方法，结合潜在扩散模型和分类器引导生成视觉反事实解释，并通过分层块细化寻找最接近目标事实图像的优化反事实。

Result: 在三个文档分类数据集（RVL-CDIP、Tobacco3482、DocLayNet）和三种模型（ResNet、ConvNeXt、DiT）上，通过有效性、接近性和真实性等标准验证了DocVCE的优越性。

Conclusion: 本研究通过引入DocVCE方法，成功填补了文档图像分类中生成式反事实解释的研究空白，为模型决策提供了直观且可操作的解释。

Abstract: As black-box AI-driven decision-making systems become increasingly widespread
in modern document processing workflows, improving their transparency and
reliability has become critical, especially in high-stakes applications where
biases or spurious correlations in decision-making could lead to serious
consequences. One vital component often found in such document processing
workflows is document image classification, which, despite its widespread use,
remains difficult to explain. While some recent works have attempted to explain
the decisions of document image classification models through
feature-importance maps, these maps are often difficult to interpret and fail
to provide insights into the global features learned by the model. In this
paper, we aim to bridge this research gap by introducing generative document
counterfactuals that provide meaningful insights into the model's
decision-making through actionable explanations. In particular, we propose
DocVCE, a novel approach that leverages latent diffusion models in combination
with classifier guidance to first generate plausible in-distribution visual
counterfactual explanations, and then performs hierarchical patch-wise
refinement to search for a refined counterfactual that is closest to the target
factual image. We demonstrate the effectiveness of our approach through a
rigorous qualitative and quantitative assessment on 3 different document
classification datasets -- RVL-CDIP, Tobacco3482, and DocLayNet -- and 3
different models -- ResNet, ConvNeXt, and DiT -- using well-established
evaluation criteria such as validity, closeness, and realism. To the best of
the authors' knowledge, this is the first work to explore generative
counterfactual explanations in document image analysis.

</details>


### [87] [A machine learning approach for image classification in synthetic aperture RADAR](https://arxiv.org/abs/2508.04234)
*Romina Gaburro,Patrick Healy,Shraddha Naidu,Clifford Nolan*

Main category: cs.CV

TL;DR: CNN在SAR数据中有效用于物体形状和冰类型分类，准确率≥75%，并探讨了天线高度对分类的影响。


<details>
  <summary>Details</summary>
Motivation: 解决SAR中地面物体的识别和分类问题，探索CNN在此类任务中的潜力。

Method: 采用单散射近似方法，结合模拟SAR数据和重建图像进行分类，并与真实SAR图像（来自Sentinel-1卫星）中的冰类型识别进行对比。

Result: 在模拟和真实SAR数据实验中均取得了较高的分类准确率（≥75%）。

Conclusion: 该论文展示了卷积神经网络（CNN）在合成孔径雷达（SAR）数据中用于几何和环境分类任务的有效性，并探讨了不同天线高度对分类能力的影响。

Abstract: We consider the problem in Synthetic Aperture RADAR (SAR) of identifying and
classifying objects located on the ground by means of Convolutional Neural
Networks (CNNs). Specifically, we adopt a single scattering approximation to
classify the shape of the object using both simulated SAR data and
reconstructed images from this data, and we compare the success of these
approaches. We then identify ice types in real SAR imagery from the satellite
Sentinel-1. In both experiments we achieve a promising high classification
accuracy ($\geq$75\%). Our results demonstrate the effectiveness of CNNs in
using SAR data for both geometric and environmental classification tasks. Our
investigation also explores the effect of SAR data acquisition at different
antenna heights on our ability to classify objects successfully.

</details>


### [88] [PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction](https://arxiv.org/abs/2508.04236)
*Muhua Zhu,Xinhao Jin,Chengbo Wang,Yongcong Zhang,Yifei Xue,Tie Ji,Yizhen Lao*

Main category: cs.CV

TL;DR: 提出PIS3R算法，通过深度3D重建和点条件扩散模块，有效解决大视差图像拼接问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有拼接方法在大视差场景下效果不佳的问题。

Method: 首先使用视觉几何基础的Transformer获取内参、外参及密集3D重建，然后通过重投影生成初始拼接图像，最后利用点条件图像扩散模块优化结果。

Result: 实验证明PIS3R在大视差图像拼接中表现优于现有方法，且在定性和定量评估中均取得更好结果。

Conclusion: PIS3R算法在图像拼接中表现出色，能够有效处理大视差场景，并在几何完整性和下游3D视觉任务中具有直接应用价值。

Abstract: Image stitching aim to align two images taken from different viewpoints into
one seamless, wider image. However, when the 3D scene contains depth variations
and the camera baseline is significant, noticeable parallax occurs-meaning the
relative positions of scene elements differ substantially between views. Most
existing stitching methods struggle to handle such images with large parallax
effectively. To address this challenge, in this paper, we propose an image
stitching solution called PIS3R that is robust to very large parallax based on
the novel concept of deep 3D reconstruction. First, we apply visual geometry
grounded transformer to two input images with very large parallax to obtain
both intrinsic and extrinsic parameters, as well as the dense 3D scene
reconstruction. Subsequently, we reproject reconstructed dense point cloud onto
a designated reference view using the recovered camera parameters, achieving
pixel-wise alignment and generating an initial stitched image. Finally, to
further address potential artifacts such as holes or noise in the initial
stitching, we propose a point-conditioned image diffusion module to obtain the
refined result.Compared with existing methods, our solution is very large
parallax tolerant and also provides results that fully preserve the geometric
integrity of all pixels in the 3D photogrammetric context, enabling direct
applicability to downstream 3D vision tasks such as SfM. Experimental results
demonstrate that the proposed algorithm provides accurate stitching results for
images with very large parallax, and outperforms the existing methods
qualitatively and quantitatively.

</details>


### [89] [From eye to AI: studying rodent social behavior in the era of machine Learning](https://arxiv.org/abs/2508.04255)
*Giuseppe Chindemi,Camilla Bellone,Benoit Girard*

Main category: cs.CV

TL;DR: 本文探讨了AI在啮齿类动物社交行为研究中的应用，分析了现有工具的优势与局限，并提出了解决常见障碍的实用方案。


<details>
  <summary>Details</summary>
Motivation: 传统方法存在偏见且难以捕捉啮齿类动物社交互动的复杂性，而结合计算机视觉、行为学和神经科学的现代方法能提供更全面的行为洞察。

Method: 本文讨论了分析啮齿类动物社交行为的主要步骤和可用工具，评估了它们的优势和局限性。

Result: 现代AI方法为啮齿类动物社交行为研究提供了多方面的见解，但其整合也带来了一些挑战。

Conclusion: 本文总结了AI在啮齿类动物社交行为研究中的优势与挑战，并提出了实用解决方案，旨在指导年轻研究者采用这些方法，并促进专家间关于这些工具在科学应用中不断变化需求的进一步讨论。

Abstract: The study of rodent social behavior has shifted in the last years from
relying on direct human observation to more nuanced approaches integrating
computational methods in artificial intelligence (AI) and machine learning.
While conventional approaches introduce bias and can fail to capture the
complexity of rodent social interactions, modern approaches bridging computer
vision, ethology and neuroscience provide more multifaceted insights into
behavior which are particularly relevant to social neuroscience. Despite these
benefits, the integration of AI into social behavior research also poses
several challenges. Here we discuss the main steps involved and the tools
available for analyzing rodent social behavior, examining their advantages and
limitations. Additionally, we suggest practical solutions to address common
hurdles, aiming to guide young researchers in adopting these methods and to
stimulate further discussion among experts regarding the evolving requirements
of these tools in scientific applications.

</details>


### [90] [Revisiting Continual Semantic Segmentation with Pre-trained Vision Models](https://arxiv.org/abs/2508.04267)
*Duzhen Zhang,Yong Ren,Wei Cong,Junhao Zheng,Qiaoyi Su,Shuncheng Jia,Zhong-Zhi Li,Xuanle Zhao,Ye Bai,Feilong Chen,Qi Tian,Tielin Zhang*

Main category: cs.CV

TL;DR: 研究表明PVM在DFT下抗遗忘能力被低估，提出的DFT*方法简单高效，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 重新评估DFT在CSS中的表现，揭示PVM固有的抗遗忘能力被低估，提出改进方法以解决分类器漂移问题。

Method: 通过系统性分析DFT在两个标准基准（Pascal VOC 2012和ADE20K）下的表现，探究PVM的抗遗忘能力，并基于特征空间分析提出DFT*。

Result: DFT*在十六种最先进的CSS方法中表现优异，且参数和训练时间大幅减少。

Conclusion: DFT*是一种简单但有效的改进方法，通过冻结PVM主干和先前学习的分类器，以及预分配未来分类器，显著提升了性能，同时减少了可训练参数和训练时间。

Abstract: Continual Semantic Segmentation (CSS) seeks to incrementally learn to segment
novel classes while preserving knowledge of previously encountered ones. Recent
advancements in CSS have been largely driven by the adoption of Pre-trained
Vision Models (PVMs) as backbones. Among existing strategies, Direct
Fine-Tuning (DFT), which sequentially fine-tunes the model across classes,
remains the most straightforward approach. Prior work often regards DFT as a
performance lower bound due to its presumed vulnerability to severe
catastrophic forgetting, leading to the development of numerous complex
mitigation techniques. However, we contend that this prevailing assumption is
flawed. In this paper, we systematically revisit forgetting in DFT across two
standard benchmarks, Pascal VOC 2012 and ADE20K, under eight CSS settings using
two representative PVM backbones: ResNet101 and Swin-B. Through a detailed
probing analysis, our findings reveal that existing methods significantly
underestimate the inherent anti-forgetting capabilities of PVMs. Even under
DFT, PVMs retain previously learned knowledge with minimal forgetting. Further
investigation of the feature space indicates that the observed forgetting
primarily arises from the classifier's drift away from the PVM, rather than
from degradation of the backbone representations. Based on this insight, we
propose DFT*, a simple yet effective enhancement to DFT that incorporates
strategies such as freezing the PVM backbone and previously learned
classifiers, as well as pre-allocating future classifiers. Extensive
experiments show that DFT* consistently achieves competitive or superior
performance compared to sixteen state-of-the-art CSS methods, while requiring
substantially fewer trainable parameters and less training time.

</details>


### [91] [PKSS-Align: Robust Point Cloud Registration on Pre-Kendall Shape Space](https://arxiv.org/abs/2508.04286)
*Chenlei Lv,Hui Huang*

Main category: cs.CV

TL;DR: PKSS-Align是一种鲁棒的点云配准方法，通过Pre-Kendall形状空间的形状特征相似性度量，有效处理多种干扰因素，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 点云配准对相似变换（平移、缩放、旋转）、噪声点和不完整几何结构敏感，非均匀尺度和缺陷部分增加了陷入局部最优的概率。

Method: PKSS-Align方法在Pre-Kendall形状空间（PKSS）上测量基于形状特征的相似性，无需点对点或点对面度量，直接生成变换矩阵。

Result: 该方法能够同时处理相似变换、非均匀密度、随机噪声点和缺陷部分的影响，无需数据训练和复杂特征编码。

Conclusion: 实验证明，PKSS-Align方法在点云配准任务中优于当前最先进的相关方法，具有高效性和实用性。

Abstract: Point cloud registration is a classical topic in the field of 3D Vision and
Computer Graphics. Generally, the implementation of registration is typically
sensitive to similarity transformations (translation, scaling, and rotation),
noisy points, and incomplete geometric structures. Especially, the non-uniform
scales and defective parts of point clouds increase probability of struck local
optima in registration task. In this paper, we propose a robust point cloud
registration PKSS-Align that can handle various influences, including
similarity transformations, non-uniform densities, random noisy points, and
defective parts. The proposed method measures shape feature-based similarity
between point clouds on the Pre-Kendall shape space (PKSS),
\textcolor{black}{which is a shape measurement-based scheme and doesn't require
point-to-point or point-to-plane metric.} The employed measurement can be
regarded as the manifold metric that is robust to various representations in
the Euclidean coordinate system. Benefited from the measurement, the
transformation matrix can be directly generated for point clouds with mentioned
influences at the same time. The proposed method does not require data training
and complex feature encoding. Based on a simple parallel acceleration, it can
achieve significant improvement for efficiency and feasibility in practice.
Experiments demonstrate that our method outperforms the relevant
state-of-the-art methods.

</details>


### [92] [MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction](https://arxiv.org/abs/2508.04297)
*Yaopeng Lou,Liao Shen,Tianqi Liu,Jiaqi Li,Zihao Huang,Huiqiang Sun,Zhiguo Cao*

Main category: cs.CV

TL;DR: MuRF通过结合MVS和MDE特征，提出投影采样机制和参考视图损失，实现了高效高质量的新视角合成，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决稀疏输入视图（包括小和大基线）下的新视角合成问题，并提升特征表示和渲染质量。

Method: MuRF整合了MVS和MDE特征，提出了投影采样机制进行深度深度融合，并引入参考视图损失优化几何和效率，利用3D高斯表示加速训练和推理。

Result: MuRF在DTU、RealEstate10K等数据集上实现了最先进的性能，并在LLFF和Mip-NeRF 360数据集上展示了零样本性能。

Conclusion: MuRF通过整合多视角立体视觉（MVS）和单目深度估计（MDE）特征，结合提出的投影采样机制和参考视图损失，实现了在各种基线设置下的高效高质量新视角合成，并在多个数据集上取得了最先进的性能。

Abstract: We present Multi-Baseline Gaussian Splatting (MuRF), a generalized
feed-forward approach for novel view synthesis that effectively handles diverse
baseline settings, including sparse input views with both small and large
baselines. Specifically, we integrate features from Multi-View Stereo (MVS) and
Monocular Depth Estimation (MDE) to enhance feature representations for
generalizable reconstruction. Next, We propose a projection-and-sampling
mechanism for deep depth fusion, which constructs a fine probability volume to
guide the regression of the feature map. Furthermore, We introduce a
reference-view loss to improve geometry and optimization efficiency. We
leverage 3D Gaussian representations to accelerate training and inference time
while enhancing rendering quality. MuRF achieves state-of-the-art performance
across multiple baseline settings and diverse scenarios ranging from simple
objects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also
demonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360
datasets.

</details>


### [93] [Length Matters: Length-Aware Transformer for Temporal Sentence Grounding](https://arxiv.org/abs/2508.04299)
*Yifan Wang,Ziyi Liu,Xiaolong Sun,Jiawei Wang,Hongmin Liu*

Main category: cs.CV

TL;DR: LATR通过分组查询和长度分类任务改进TSG，减少冗余预测，在多个基准测试中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: DETR-based模型在TSG任务中取得了显著进展，但缺乏显式监督导致查询角色重叠和冗余预测。因此，研究旨在通过使每个查询履行其指定角色来改进TSG。

Method: 提出了Length-Aware Transformer (LATR)，将查询分为三组，分别处理短、中、长时长的片段，并在训练中引入额外的长度分类任务，抑制不匹配长度的预测。

Result: LATR在三个公共基准测试中实现了最先进的性能，消融研究验证了各组件和长度先验的重要性。

Conclusion: LATR通过利用视频描述对的长度先验，成功提升了TSG任务的性能，并在三个公共基准测试中达到了最先进的水平。消融研究验证了方法中每个组件的贡献以及长度先验在TSG任务中的关键作用。

Abstract: Temporal sentence grounding (TSG) is a highly challenging task aiming to
localize the temporal segment within an untrimmed video corresponding to a
given natural language description. Benefiting from the design of learnable
queries, the DETR-based models have achieved substantial advancements in the
TSG task. However, the absence of explicit supervision often causes the learned
queries to overlap in roles, leading to redundant predictions. Therefore, we
propose to improve TSG by making each query fulfill its designated role,
leveraging the length priors of the video-description pairs. In this paper, we
introduce the Length-Aware Transformer (LATR) for TSG, which assigns different
queries to handle predictions based on varying temporal lengths. Specifically,
we divide all queries into three groups, responsible for segments with short,
middle, and long temporal durations, respectively. During training, an
additional length classification task is introduced. Predictions from queries
with mismatched lengths are suppressed, guiding each query to specialize in its
designated function. Extensive experiments demonstrate the effectiveness of our
LATR, achieving state-of-the-art performance on three public benchmarks.
Furthermore, the ablation studies validate the contribution of each component
of our method and the critical role of incorporating length priors into the TSG
task.

</details>


### [94] [A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks](https://arxiv.org/abs/2508.04316)
*Kun Gui,Hongliang Ren,Shang Shi,Jin Lu,Changqiu Yu,Quanjun Cao,Guomin Gu,Qi Xuan*

Main category: cs.CV

TL;DR: MAEPD是一种基于掩码自编码器的DAS信号识别基础模型，通过自监督预训练和视觉提示调优，显著提升模型泛化能力和效率。


<details>
  <summary>Details</summary>
Motivation: 解决DAS信号识别中因数据分布差异导致的模型泛化能力不足和标记数据短缺问题。

Method: 研究采用自监督的掩码重建任务预训练MAEPD模型，并通过视觉提示调优（VPT）进行下游任务微调，仅需调整少量参数。

Result: 在室内步态识别任务中，VPT-Deep方法达到96.94%的分类准确率，仅微调0.322%的参数，优于传统全微调方法，并在管道泄漏检测中表现稳健。

Conclusion: 该研究提出了一种基于掩码自编码器（MAEPD）的基础模型，用于解决分布式声学传感（DAS）信号识别中的泛化问题。实验证明，MAEPD在多个下游任务中表现出色，具有高效性和可扩展性。

Abstract: Distributed Acoustic Sensing (DAS) technology finds growing applications
across various domains. However, data distribution disparities due to
heterogeneous sensing environments pose challenges for data-driven artificial
intelligence (AI) models, limiting cross-domain generalization and facing a
shortage of labeled training data. To address these issues, this study proposes
a foundational model for DAS signal recognition based on a Masked Autoencoder,
named MAEPD. The MAEPD model is pretrained on a dataset of 635,860 samples,
encompassing DAS gait spatiotemporal signals, 2D GASF images for perimeter
security, 2D time-frequency images for pipeline leakage, and open-dataset
signals including whale vocalizations and seismic activities, using a
self-supervised mask reconstruction task to capture deep semantic features of
DAS signals. Visual Prompt Tuning (VPT) is employed for downstream recognition
tasks. This method freezes the pretrained backbone parameters and fine-tunes
only a small set of learnable visual prompt vectors inserted into the
Transformer encoder layers. Experiments on the NVIDIA GeForce RTX 4080 Super
platform validate MAEPD using indoor gait recognition as a downstream task. The
VPT-Deep approach achieves a classification accuracy of 96.94% with just 0.322%
of parameters fine-tuned, surpassing the traditional Full Fine Tuning (FFT)
method by 0.61% and reducing training time by 45%. The model also exhibits
robust performance in pipeline leakage detection, confirming the generality,
efficiency, and scalability of MAEPD as a foundational model. This approach
offers a novel paradigm for addressing the limited generalization of signal
recognition models in the DAS domain.

</details>


### [95] [TempFlow-GRPO: When Timing Matters for GRPO in Flow Models](https://arxiv.org/abs/2508.04324)
*Xiaoxuan He,Siming Fu,Yuke Zhao,Wanli Li,Jian Yang,Dacheng Yin,Fengyun Rao,Bo Zhang*

Main category: cs.CV

TL;DR: TempFlow-GRPO通过时间感知优化改进了文本到图像生成模型，解决了现有方法在强化学习中的信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 现有的流匹配模型在强化学习与人类偏好对齐方面表现不佳，主要原因是对时间均匀性的假设导致信用分配不精确。

Method: 提出了TempFlow-GRPO框架，包括轨迹分支机制和噪声感知权重方案，以捕捉和利用基于流生成的时间结构。

Result: TempFlow-GRPO在人类偏好对齐和标准文本到图像基准测试中实现了最先进的性能。

Conclusion: TempFlow-GRPO通过引入时间感知的优化机制，显著提升了基于流的文本到图像生成模型在人类偏好对齐和标准基准测试中的性能。

Abstract: Recent flow matching models for text-to-image generation have achieved
remarkable quality, yet their integration with reinforcement learning for human
preference alignment remains suboptimal, hindering fine-grained reward-based
optimization. We observe that the key impediment to effective GRPO training of
flow models is the temporal uniformity assumption in existing approaches:
sparse terminal rewards with uniform credit assignment fail to capture the
varying criticality of decisions across generation timesteps, resulting in
inefficient exploration and suboptimal convergence. To remedy this shortcoming,
we introduce \textbf{TempFlow-GRPO} (Temporal Flow GRPO), a principled GRPO
framework that captures and exploits the temporal structure inherent in
flow-based generation. TempFlow-GRPO introduces two key innovations: (i) a
trajectory branching mechanism that provides process rewards by concentrating
stochasticity at designated branching points, enabling precise credit
assignment without requiring specialized intermediate reward models; and (ii) a
noise-aware weighting scheme that modulates policy optimization according to
the intrinsic exploration potential of each timestep, prioritizing learning
during high-impact early stages while ensuring stable refinement in later
phases. These innovations endow the model with temporally-aware optimization
that respects the underlying generative dynamics, leading to state-of-the-art
performance in human preference alignment and standard text-to-image
benchmarks.

</details>


### [96] [RotatedMVPS: Multi-view Photometric Stereo with Rotated Natural Light](https://arxiv.org/abs/2508.04366)
*Songyun Yang,Yufei Han,Jilong Zhang,Kongming Liang,Peng Yu,Zhaowei Qu,Heng Guo*

Main category: cs.CV

TL;DR: RotatedMVPS通过光照一致性和数据先验提升自然光照下的形状和反射率恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有MVPS方法需在暗室环境下控制光照或忽略反射率和光照属性的恢复，限制了在自然光照场景和下游逆渲染任务中的应用。

Method: 通过确保不同相机和物体姿态下的光照一致性，减少复杂环境光的未知因素，并结合单视图光度立体法的数据先验，提升形状和反射率恢复的准确性。

Result: 实验结果表明，RotatedMVPS在自然光照条件下能有效恢复形状和反射率。

Conclusion: RotatedMVPS方法在合成和真实数据集上证明了其有效性，能够解决自然光照条件下的形状和反射率恢复问题。

Abstract: Multiview photometric stereo (MVPS) seeks to recover high-fidelity surface
shapes and reflectances from images captured under varying views and
illuminations. However, existing MVPS methods often require controlled darkroom
settings for varying illuminations or overlook the recovery of reflectances and
illuminations properties, limiting their applicability in natural illumination
scenarios and downstream inverse rendering tasks. In this paper, we propose
RotatedMVPS to solve shape and reflectance recovery under rotated natural
light, achievable with a practical rotation stage. By ensuring light
consistency across different camera and object poses, our method reduces the
unknowns associated with complex environment light. Furthermore, we integrate
data priors from off-the-shelf learning-based single-view photometric stereo
methods into our MVPS framework, significantly enhancing the accuracy of shape
and reflectance recovery. Experimental results on both synthetic and real-world
datasets demonstrate the effectiveness of our approach.

</details>


### [97] [TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding](https://arxiv.org/abs/2508.04369)
*Canhui Tang,Zifan Han,Hongbo Sun,Sanping Zhou,Xuchong Zhang,Xin Wei,Ye Yuan,Jinglin Xu,Hao Sun*

Main category: cs.CV

TL;DR: TSPO通过强化学习优化了MLLMs的长视频理解能力，解决了现有稀疏帧采样方法的局限性，并在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频MLLMs在处理长视频输入时存在上下文限制和训练成本高的问题，且现有的稀疏帧采样方法可能错过关键事件或受限于预训练模型的事件理解能力。

Method: 提出了Temporal Sampling Policy Optimization (TSPO)方法，包括可训练的事件感知时间代理和TSPO强化学习范式，以及长视频训练数据构建流水线。

Result: TSPO在多个长视频理解基准测试中实现了最先进的性能，并展示了在不同Video-MLLMs中的可迁移能力。

Conclusion: TSPO通过强化学习优化了MLLMs的长视频理解能力，并在多个基准测试中取得了最先进的性能，展示了其在不同Video-MLLMs中的可迁移能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
progress in vision-language tasks, yet they still face challenges when
processing long-duration video inputs. The limitation arises from MLLMs'
context limit and training costs, necessitating sparse frame sampling before
feeding videos into MLLMs. Existing video MLLMs adopt training-free uniform
sampling or keyframe search, which may miss critical events or be constrained
by the pre-trained models' event understanding capabilities. Meanwhile,
building a training-based method remains challenging due to the unsupervised
and non-differentiable nature of sparse frame sampling. To address these
problems, we propose Temporal Sampling Policy Optimization (TSPO), advancing
MLLMs' long-form video-language understanding via reinforcement learning.
Specifically, we first propose a trainable event-aware temporal agent, which
captures event-query correlation for performing probabilistic keyframe
selection. Then, we propose the TSPO reinforcement learning paradigm, which
models keyframe selection and language generation as a joint decision-making
process, enabling end-to-end group relative optimization with efficient
rule-based rewards. Furthermore, for the TSPO's training, we propose a long
video training data construction pipeline with comprehensive temporal data and
video Needle-in-a-Haystack data. Finally, we incorporate rule-based answering
accuracy and temporal locating reward mechanisms to optimize the temporal
sampling policy. Comprehensive experiments show that our TSPO achieves
state-of-the-art performance across multiple long video understanding
benchmarks, and shows transferable ability across different cutting-edge
Video-MLLMs.

</details>


### [98] [VisionTS++: Cross-Modal Time Series Foundation Model with Continual Pre-trained Visual Backbones](https://arxiv.org/abs/2508.04379)
*Lefei Shen,Mouxiang Chen,Xu Liu,Han Fu,Xiaoxue Ren,Jianling Sun,Zhuo Li,Chenghao Liu*

Main category: cs.CV

TL;DR: VisionTS++ 通过创新方法将视觉模型应用于时间序列预测，解决了模态差异问题，并在多个测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决视觉模型在时间序列预测中面临的三个关键差异：数据模态差异、多变量预测差异和概率预测差异。

Method: 提出了 VisionTS++，包括三个创新点：(1) 基于视觉模型的过滤机制，(2) 彩色多变量转换方法，(3) 多分位数预测方法。

Result: 在多个基准测试中，VisionTS++ 表现优异，MSE 降低了 6%-44%，并在 12 个概率预测设置中的 9 个排名第一。

Conclusion: VisionTS++ 通过创新的方法成功弥合了视觉模型与时间序列预测之间的关键差异，实现了跨模态知识转移的新范式，并在多个基准测试中取得了最先进的成果。

Abstract: Recent studies have revealed that vision models pre-trained on images can
perform well in time series forecasting by reformulating forecasting as an
image reconstruction task, suggesting their potential as universal time series
foundation models. However, effective cross-modal transfer from vision to time
series remains challenging due to three key discrepancies: (1) data-modality
gap between structured, bounded image data and unbounded, heterogeneous time
series; (2) multivariate-forecasting gap between standard RGB
three-channel-based vision models and the need to model time series with
arbitrary numbers of variates; and (3) probabilistic-forecasting gap between
the deterministic output formats of most vision models and the requirement for
uncertainty-aware probabilistic predictions. To bridge these gaps, we propose
VisionTS++, a vision-model-based TSFM that performs continual pre-training on
large-scale time series datasets, including 3 innovations: (1) a
vision-model-based filtering mechanism to identify high-quality time series
data, thereby mitigating modality gap and improving pre-training stability, (2)
a colorized multivariate conversion method that transforms multivariate time
series into multi-subfigure RGB images, capturing complex inter-variate
dependencies; and (3) a multi-quantile forecasting approach using parallel
reconstruction heads to generate forecasts of different quantile levels, thus
more flexibly approximating arbitrary output distributions without restrictive
prior distributional assumptions. Evaluated on both in-distribution and
out-of-distribution TSF benchmarks, \model achieves SOTA results, outperforming
specialized TSFMs by 6%-44% in MSE reduction and ranking first in 9 out of 12
probabilistic forecasting settings. Our work establishes a new paradigm for
cross-modal knowledge transfer, advancing the development of universal TSFMs.

</details>


### [99] [Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning](https://arxiv.org/abs/2508.04416)
*Haoji Zhang,Xin Gu,Jiawen Li,Chixiang Ma,Sule Bai,Chubin Zhang,Bowen Zhang,Zhichao Zhou,Dongliang He,Yansong Tang*

Main category: cs.CV

TL;DR: VITAL是一个端到端的代理视频推理框架，通过视觉工具箱和多模态链式思维推理提升长视频理解能力，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型(MLLMs)的视频推理能力对视频问答和时间定位等下游任务至关重要。现有基于文本的链式思维推理方法存在跨模态交互有限和幻觉增加的问题，尤其是在长视频或复杂推理链场景下。

Method: 提出了Video Intelligence via Tool-Augmented Learning (VITAL)框架，通过视觉工具箱密集采样视频帧，并生成多模态链式思维推理。构建了两个高质量多任务视频推理数据集MTVR-CoT-72k和MTVR-RL-110k，并提出Difficulty-aware Group Relative Policy Optimization算法(DGRPO)以解决多任务强化学习中的难度不平衡问题。

Result: 在11个具有挑战性的视频理解基准测试中，VITAL展示了先进的推理能力，在视频问答和时间定位任务中优于现有方法。

Conclusion: VITAL框架在视频问答和时间定位任务中表现出色，特别是在长视频场景下，超越了现有方法。所有代码、数据和模型权重将公开。

Abstract: The video reasoning ability of multimodal large language models (MLLMs) is
crucial for downstream tasks like video question answering and temporal
grounding. While recent approaches have explored text-based chain-of-thought
(CoT) reasoning for MLLMs, these methods often suffer from limited cross-modal
interaction and increased hallucination, especially with longer videos or
reasoning chains. To address these challenges, we propose Video Intelligence
via Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoning
framework. With a visual toolbox, the model can densely sample new video frames
on demand and generate multimodal CoT for precise long video reasoning. We
observe that temporal grounding and question answering are mutually beneficial
for video understanding tasks. Therefore, we construct two high-quality
multi-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning and
MTVR-RL-110k for reinforcement learning. Moreover, we propose a
Difficulty-aware Group Relative Policy Optimization algorithm (DGRPO) to
mitigate difficulty imbalance in multi-task reinforcement learning. Extensive
experiments on 11 challenging video understanding benchmarks demonstrate the
advanced reasoning ability of VITAL, outperforming existing methods in video
question answering and temporal grounding tasks, especially in long video
scenarios. All code, data and model weight will be made publicly available.

</details>


### [100] [Efficient Inter-Task Attention for Multitask Transformer Models](https://arxiv.org/abs/2508.04422)
*Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: 提出Deformable Inter-Task Self-Attention方法，显著降低多任务学习中Transformer的计算开销，提升任务性能。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在多任务学习中因注意力矩阵规模随任务数量呈二次增长而面临计算瓶颈，需解决硬件限制下的效率问题。

Method: 提出Deformable Inter-Task Self-Attention方法，通过更高效地聚合不同任务特征图的信息，减少计算复杂度。

Result: 在NYUD-v2和PASCAL-Context数据集上，实现了计算量和推理延迟的数量级减少，同时个别任务预测质量指标提升高达7.4%。

Conclusion: 论文提出了一种新颖的Deformable Inter-Task Self-Attention方法，有效解决了多任务学习中注意力矩阵计算复杂度高的问题，并在实验中显著降低了计算开销和推理延迟，同时提升了任务预测质量。

Abstract: In both Computer Vision and the wider Deep Learning field, the Transformer
architecture is well-established as state-of-the-art for many applications. For
Multitask Learning, however, where there may be many more queries necessary
compared to single-task models, its Multi-Head-Attention often approaches the
limits of what is computationally feasible considering practical hardware
limitations. This is due to the fact that the size of the attention matrix
scales quadratically with the number of tasks (assuming roughly equal numbers
of queries for all tasks). As a solution, we propose our novel Deformable
Inter-Task Self-Attention for Multitask models that enables the much more
efficient aggregation of information across the feature maps from different
tasks. In our experiments on the NYUD-v2 and PASCAL-Context datasets, we
demonstrate an order-of-magnitude reduction in both FLOPs count and inference
latency. At the same time, we also achieve substantial improvements by up to
7.4% in the individual tasks' prediction quality metrics.

</details>


### [101] [Composed Object Retrieval: Object-level Retrieval via Composed Expressions](https://arxiv.org/abs/2508.04424)
*Tong Wang,Guanyu Yang,Nian Liu,Zongyan Han,Jinxing Zhou,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: 提出 COR 任务和 CORE 模型，实现对象级精确检索与分割，在 COR127K 基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于组合图像检索（CIR）的方法仅能进行图像级匹配，无法定位特定对象，因此需要一种能够实现对象级精确检索与分割的新方法。

Method: 提出了 CORE 模型，该模型集成了参考区域编码、自适应视觉-文本交互和区域级对比学习。

Result: CORE 在 COR127K 基准上显著优于现有模型，为 COR 任务建立了简单有效的基线。

Conclusion: COR 提出了一种新的任务，通过结合参考对象和检索文本实现对象级精确检索与分割，CORE 模型在基准和新类别上显著优于现有模型，为细粒度多模态检索研究开辟了新方向。

Abstract: Retrieving fine-grained visual content based on user intent remains a
challenge in multi-modal systems. Although current Composed Image Retrieval
(CIR) methods combine reference images with retrieval texts, they are
constrained to image-level matching and cannot localize specific objects. To
this end, we propose Composed Object Retrieval (COR), a brand-new task that
goes beyond image-level retrieval to achieve object-level precision, allowing
the retrieval and segmentation of target objects based on composed expressions
combining reference objects and retrieval texts. COR presents significant
challenges in retrieval flexibility, which requires systems to identify
arbitrary objects satisfying composed expressions while avoiding semantically
similar but irrelevant negative objects within the same scene. We construct
COR127K, the first large-scale COR benchmark that contains 127,166 retrieval
triplets with various semantic transformations in 408 categories. We also
present CORE, a unified end-to-end model that integrates reference region
encoding, adaptive visual-textual interaction, and region-level contrastive
learning. Extensive experiments demonstrate that CORE significantly outperforms
existing models in both base and novel categories, establishing a simple and
effective baseline for this challenging task while opening new directions for
fine-grained multi-modal retrieval research.

</details>


### [102] [Benchmarking Foundation Models for Mitotic Figure Classification](https://arxiv.org/abs/2508.04441)
*Jonas Ammeling,Jonathan Ganz,Emely Rosbach,Ludwig Lausser,Christof A. Bertram,Katharina Breininger,Marc Aubreville*

Main category: cs.CV

TL;DR: Foundation models adapted with LoRA outperform linear probing, nearly matching full data performance with minimal training and reducing out-of-domain gaps, though traditional methods remain strong.


<details>
  <summary>Details</summary>
Motivation: Address the limited labeled data problem in pathology by leveraging self-supervised learning to train foundation models that generalize well with minimal training effort.

Method: Investigates data scaling laws on multiple foundation models, evaluates robustness to unseen tumor domains, and compares LoRA-adapted models with linear probing and end-to-end-trained baselines (CNNs and Vision Transformers).

Result: LoRA-adapted models achieve performance levels close to 100% data availability with only 10% of training data and nearly close the out-of-domain performance gap.

Conclusion: LoRA-adapted foundation models show superior performance compared to standard linear probing, nearly closing the out-of-domain performance gap, though traditional architectures with full fine-tuning remain competitive.

Abstract: The performance of deep learning models is known to scale with data quantity
and diversity. In pathology, as in many other medical imaging domains, the
availability of labeled images for a specific task is often limited.
Self-supervised learning techniques have enabled the use of vast amounts of
unlabeled data to train large-scale neural networks, i.e., foundation models,
that can address the limited data problem by providing semantically rich
feature vectors that can generalize well to new tasks with minimal training
effort increasing model performance and robustness. In this work, we
investigate the use of foundation models for mitotic figure classification. The
mitotic count, which can be derived from this classification task, is an
independent prognostic marker for specific tumors and part of certain tumor
grading systems. In particular, we investigate the data scaling laws on
multiple current foundation models and evaluate their robustness to unseen
tumor domains. Next to the commonly used linear probing paradigm, we also adapt
the models using low-rank adaptation (LoRA) of their attention mechanisms. We
compare all models against end-to-end-trained baselines, both CNNs and Vision
Transformers. Our results demonstrate that LoRA-adapted foundation models
provide superior performance to those adapted with standard linear probing,
reaching performance levels close to 100% data availability with only 10% of
training data. Furthermore, LoRA-adaptation of the most recent foundation
models almost closes the out-of-domain performance gap when evaluated on unseen
tumor domains. However, full fine-tuning of traditional architectures still
yields competitive performance.

</details>


### [103] [Boosting Visual Knowledge-Intensive Training for LVLMs Through Causality-Driven Visual Object Completion](https://arxiv.org/abs/2508.04453)
*Qingguo Hu,Ante Wang,Jia Song,Delai Qiu,Qingsong Liu,Jinsong Su*

Main category: cs.CV

TL;DR: 本文提出了一种基于因果关系的视觉对象补全任务（CVC）的自改进框架，显著提升了LVLMs在深度视觉感知任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）在需要深度视觉感知的任务中表现不佳，原因是现有指令调优语料库中视觉知识匮乏。本文旨在通过引入视觉知识密集型任务来解决这一问题。

Method: 本文提出了一种自改进框架，通过自动化实例构建管道生成丰富的视觉知识密集型任务实例，使LVLMs能够通过试错学习自我改进。

Result: 实验结果显示，该方法在四个具有挑战性的专门任务和四个广泛使用的综合基准测试中均取得了显著提升。特别是在专门任务中，使用LLaVA-1.5-7B和LLaVA-1.5-13B时，平均分别提升了5.4%和4.0%。

Conclusion: 通过引入基于因果关系的视觉对象补全任务（CVC），本文提出了一种自改进框架，显著提升了大型视觉语言模型（LVLMs）在需要深度视觉感知任务中的表现。实验证明，该方法在多个任务和基准测试中均取得了显著提升。

Abstract: Large Vision-Language Models (LVLMs) have experienced significant
advancements in recent years. However, their performance still falls short in
tasks requiring deep visual perception, such as identifying subtle differences
between images. A potential cause is the scarcity of visual knowledge in
popular instruction-tuning corpora, resulting in inadequate visual perception
and reasoning capabilities. To address this challenge, we introduce a
self-improvement framework grounded in a novel visual knowledge-intensive task,
\underline{C}ausality-driven \underline{V}isual object \underline{C}ompletion
(CVC). This task requires LVLMs to infer the masked object in an image based on
its \textit{causal} relationships with the other visible information. We first
obtain rich examples cheaply through our automated instance construction
pipeline, without relying on sophisticated LVLMs (\textit{e.g.}, GPT-4V) or
human assistance. Then, LVLMs effectively self-improve through trial and error
learning using these created instances. Our experiments demonstrate substantial
gains across four challenging specialized tasks and four widely-used
comprehensive benchmarks. Especially on specialized tasks, our method achieves
an average improvement of 5.4\% and 4.0\% compared to the corresponding
baselines when utilizing LLaVA-1.5-7B and LLaVA-1.5-13B, respectively. The code
is available at https://github.com/XMUDeepLIT/CVC.

</details>


### [104] [4DVD: Cascaded Dense-view Video Diffusion Model for High-quality 4D Content Generation](https://arxiv.org/abs/2508.04467)
*Shuzhou Yang,Xiaodong Cun,Xiaoyu Li,Yaowei Li,Jian Zhang*

Main category: cs.CV

TL;DR: 4DVD是一种级联视频扩散模型，通过解耦多视角布局生成和结构感知条件生成，实现了高质量的4D内容生成。


<details>
  <summary>Details</summary>
Motivation: 直接生成高维数据（如4D）具有高复杂性，因此提出了4DVD模型，通过解耦生成任务来简化复杂性并提升生成质量。

Method: 4DVD采用级联视频扩散模型，首先预测密集视角内容的布局，然后基于布局先验开发结构感知的时空生成分支，结合输入单目视频的精细外观内容生成高质量密集视角视频。

Result: 实验表明，4DVD在4D生成和新视角合成方面表现出最先进的性能。

Conclusion: 4DVD通过解耦多视角布局生成和结构感知条件生成两个子任务，有效统一了4D内容的生成，实现了在4D生成和新视角合成方面的最先进性能。

Abstract: Given the high complexity of directly generating high-dimensional data such
as 4D, we present 4DVD, a cascaded video diffusion model that generates 4D
content in a decoupled manner. Unlike previous multi-view video methods that
directly model 3D space and temporal features simultaneously with stacked cross
view/temporal attention modules, 4DVD decouples this into two subtasks: coarse
multi-view layout generation and structure-aware conditional generation, and
effectively unifies them. Specifically, given a monocular video, 4DVD first
predicts the dense view content of its layout with superior cross-view and
temporal consistency. Based on the produced layout priors, a structure-aware
spatio-temporal generation branch is developed, combining these coarse
structural priors with the exquisite appearance content of input monocular
video to generate final high-quality dense-view videos. Benefit from this,
explicit 4D representation~(such as 4D Gaussian) can be optimized accurately,
enabling wider practical application. To train 4DVD, we collect a dynamic 3D
object dataset, called D-Objaverse, from the Objaverse benchmark and render 16
videos with 21 frames for each object. Extensive experiments demonstrate our
state-of-the-art performance on both novel view synthesis and 4D generation.
Our project page is https://4dvd.github.io/

</details>


### [105] [FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding](https://arxiv.org/abs/2508.04469)
*Emmanuelle Bourigault,Pauline Bourigault*

Main category: cs.CV

TL;DR: FrEVL框架通过冻结预训练嵌入，高效支持视觉语言理解任务，性能接近SOTA且大幅降低计算和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型因高计算需求而难以部署的问题，探索冻结预训练嵌入是否能有效支持视觉语言理解。

Method: 提出了FrEVL框架，通过分析冻结预训练嵌入在判别性任务中的表现，并与下游任务需求对齐来评估其有效性。

Result: 冻结嵌入在标准基准测试中达到85%至95%的SOTA性能，仅需68.4M可训练参数，且能实现2.3倍加速和52%的能耗降低。

Conclusion: FrEVL框架证明了冻结预训练嵌入在视觉语言理解任务中的有效性，为实际部署提供了高效且低能耗的替代方案。

Abstract: The deployment of vision-language models remains constrained by substantial
computational requirements. We present \textbf{FrEVL}, a framework exploring
whether frozen pretrained embeddings can support effective vision-language
understanding. Our analysis reveals that frozen embeddings contain rich
information for discriminative tasks, achieving 85\% to 95\% of
state-of-the-art performance on standard benchmarks with only 68.4M trainable
parameters. This performance dichotomy reveals a critical insight: frozen
embedding effectiveness depends on alignment between pretraining objectives and
downstream task requirements. When accounting for end-to-end computation
including embedding extraction, FrEVL provides $2.3\times$ speedup with 52\%
lower energy consumption, making it suitable for scenarios with pre-computable
inputs or when deployment constraints outweigh marginal performance gains. Our
evaluation provides practitioners with guidance on when frozen embedding
approaches represent viable alternatives to full model deployment. We will
release our complete implementation and evaluation framework to facilitate
further research into efficient multi-modal understanding.

</details>


### [106] [QuantVSR: Low-Bit Post-Training Quantization for Real-World Video Super-Resolution](https://arxiv.org/abs/2508.04485)
*Bowen Chai,Zheng Chen,Libo Zhu,Wenbo Li,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: QuantVSR是一种低比特量化的视频超分辨率模型，通过STCA机制和LBA模块解决了量化挑战，性能接近全精度模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频超分辨率中表现优异，但处理速度慢和资源消耗大限制了其实际应用。量化是压缩模型的潜在解决方案，但视频超分辨率模型的时序特性和高保真要求使其量化具有挑战性。

Method: 提出了时空复杂度感知（STCA）机制，利用校准数据集测量每层的时空复杂度，并分配层特定的低秩全精度辅助分支。同时，提出了可学习偏置对齐（LBA）模块以减少量化误差。

Result: 在合成和真实数据集上的实验表明，QuantVSR性能与全精度模型相当，并显著优于其他低比特量化方法。

Conclusion: QuantVSR通过提出STCA机制和LBA模块，成功实现了低比特量化的视频超分辨率模型，性能接近全精度模型，并显著优于其他低比特量化方法。

Abstract: Diffusion models have shown superior performance in real-world video
super-resolution (VSR). However, the slow processing speeds and heavy resource
consumption of diffusion models hinder their practical application and
deployment. Quantization offers a potential solution for compressing the VSR
model. Nevertheless, quantizing VSR models is challenging due to their temporal
characteristics and high fidelity requirements. To address these issues, we
propose QuantVSR, a low-bit quantization model for real-world VSR. We propose a
spatio-temporal complexity aware (STCA) mechanism, where we first utilize the
calibration dataset to measure both spatial and temporal complexities for each
layer. Based on these statistics, we allocate layer-specific ranks to the
low-rank full-precision (FP) auxiliary branch. Subsequently, we jointly refine
the FP and low-bit branches to achieve simultaneous optimization. In addition,
we propose a learnable bias alignment (LBA) module to reduce the biased
quantization errors. Extensive experiments on synthetic and real-world datasets
demonstrate that our method obtains comparable performance with the FP model
and significantly outperforms recent leading low-bit quantization methods. Code
is available at: https://github.com/bowenchai/QuantVSR.

</details>


### [107] [MonoCloth: Reconstruction and Animation of Cloth-Decoupled Human Avatars from Monocular Videos](https://arxiv.org/abs/2508.04505)
*Daisheng Jin,Ying He*

Main category: cs.CV

TL;DR: MonoCloth 是一种从单目视频重建和动画化着装人体化身的新方法，通过部分分解和专用模块（如服装模拟）提升效果，并支持服装转移。


<details>
  <summary>Details</summary>
Motivation: 由于单目视频中几何信息有限且涉及复杂的非刚性运动，重建真实的3D人体化身具有挑战性。

Method: MonoCloth 采用基于部分的分解策略，将人体分为身体、面部、手部和服装，并针对不同部分设计了专门的模块，如服装模拟模块用于捕捉衣物形变。

Result: 实验结果表明，MonoCloth 在视觉重建质量和动画真实性方面优于现有方法。

Conclusion: MonoCloth 通过其基于部分的设计，不仅提高了重建和动画的真实性，还支持服装转移等附加任务，展现了其实用性和多功能性。

Abstract: Reconstructing realistic 3D human avatars from monocular videos is a
challenging task due to the limited geometric information and complex non-rigid
motion involved. We present MonoCloth, a new method for reconstructing and
animating clothed human avatars from monocular videos. To overcome the
limitations of monocular input, we introduce a part-based decomposition
strategy that separates the avatar into body, face, hands, and clothing. This
design reflects the varying levels of reconstruction difficulty and deformation
complexity across these components. Specifically, we focus on detailed geometry
recovery for the face and hands. For clothing, we propose a dedicated cloth
simulation module that captures garment deformation using temporal motion cues
and geometric constraints. Experimental results demonstrate that MonoCloth
improves both visual reconstruction quality and animation realism compared to
existing methods. Furthermore, thanks to its part-based design, MonoCloth also
supports additional tasks such as clothing transfer, underscoring its
versatility and practical utility.

</details>


### [108] [Skeleton Motion Words for Unsupervised Skeleton-Based Temporal Action Segmentation](https://arxiv.org/abs/2508.04513)
*Uzay Gökay,Federico Spurio,Dominik R. Bach,Juergen Gall*

Main category: cs.CV

TL;DR: 提出无监督骨架时序动作分割方法，通过时序自编码器和骨架运动词量化，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督时序动作分割方法主要针对视频数据，而骨架序列因其实际应用价值、鲁棒性和隐私保护特性未被充分探索。

Method: 使用序列到序列的时序自编码器，保持关节信息的解耦嵌入，并通过非重叠分块和量化生成骨架运动词。

Result: 在HuGaDB、LARa和BABEL三个数据集上评估，模型表现优于当前最先进的无监督方法。

Conclusion: 该论文提出了一种新颖的无监督骨架时序动作分割方法，通过序列到序列的时序自编码器和骨架运动词量化，显著优于现有方法。

Abstract: Current state-of-the-art methods for skeleton-based temporal action
segmentation are predominantly supervised and require annotated data, which is
expensive to collect. In contrast, existing unsupervised temporal action
segmentation methods have focused primarily on video data, while skeleton
sequences remain underexplored, despite their relevance to real-world
applications, robustness, and privacy-preserving nature. In this paper, we
propose a novel approach for unsupervised skeleton-based temporal action
segmentation. Our method utilizes a sequence-to-sequence temporal autoencoder
that keeps the information of the different joints disentangled in the
embedding space. Latent skeleton sequences are then divided into
non-overlapping patches and quantized to obtain distinctive skeleton motion
words, driving the discovery of semantically meaningful action clusters. We
thoroughly evaluate the proposed approach on three widely used skeleton-based
datasets, namely HuGaDB, LARa, and BABEL. The results demonstrate that our
model outperforms the current state-of-the-art unsupervised temporal action
segmentation methods. Code is available at https://github.com/bachlab/SMQ .

</details>


### [109] [No Masks Needed: Explainable AI for Deriving Segmentation from Classification](https://arxiv.org/abs/2508.04534)
*Mosong Ma,Tania Stathaki,Michalis Lazarou*

Main category: cs.CV

TL;DR: 提出一种针对医学图像的微调预训练模型方法，结合可解释AI，显著提升分割准确性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督分割方法在医学图像领域表现不佳，需要一种能够适应医学图像特性的新方法。

Method: 通过微调预训练模型并集成可解释AI生成相关性评分，优化医学图像分割过程。

Result: 在CBIS-DDSM、NuInsSeg和Kvasir-SEG等数据集上取得了优于传统方法的性能。

Conclusion: 本文提出了一种针对医学图像微调预训练模型的新方法，结合可解释AI生成相关性评分，显著提升了医学图像分割的准确性。

Abstract: Medical image segmentation is vital for modern healthcare and is a key
element of computer-aided diagnosis. While recent advancements in computer
vision have explored unsupervised segmentation using pre-trained models, these
methods have not been translated well to the medical imaging domain. In this
work, we introduce a novel approach that fine-tunes pre-trained models
specifically for medical images, achieving accurate segmentation with extensive
processing. Our method integrates Explainable AI to generate relevance scores,
enhancing the segmentation process. Unlike traditional methods that excel in
standard benchmarks but falter in medical applications, our approach achieves
improved results on datasets like CBIS-DDSM, NuInsSeg and Kvasir-SEG.

</details>


### [110] [TopKD: Top-scaled Knowledge Distillation](https://arxiv.org/abs/2508.04539)
*Qi Wang,Jinjia Zhou*

Main category: cs.CV

TL;DR: TopKD通过放大教师模型logit中的Top-K知识，提出了一种简单高效的蒸馏框架，显著提升性能并适用于多种架构。


<details>
  <summary>Details</summary>
Motivation: 当前知识蒸馏方法主要关注特征级知识转移，忽视了教师模型logit分布中的关键信息，尤其是Top-K知识。

Method: 提出了Top-scaled Knowledge Distillation (TopKD)框架，包含Top-K Scaling Module (TSM)和Top-K Decoupled Loss (TDL)两个核心组件，无需额外模块或架构修改。

Result: 在CIFAR-100、ImageNet等多个数据集上，TopKD均优于现有蒸馏方法，且在Vision Transformers上表现出色。

Conclusion: TopKD通过重新审视和利用Top-K知识，显著提升了基于logit的知识蒸馏效果，证明了logit在知识蒸馏中的巨大潜力。

Abstract: Recent advances in knowledge distillation (KD) predominantly emphasize
feature-level knowledge transfer, frequently overlooking critical information
embedded within the teacher's logit distributions. In this paper, we revisit
logit-based distillation and reveal an underexplored yet critical element:
Top-K knowledge. Motivated by this insight, we propose Top-scaled Knowledge
Distillation (TopKD), a simple, efficient, and architecture-agnostic framework
that significantly enhances logit-based distillation. TopKD consists of two
main components: (1) a Top-K Scaling Module (TSM), which adaptively amplifies
the most informative logits, and (2) a Top-K Decoupled Loss (TDL), which offers
targeted and effective supervision. Notably, TopKD integrates seamlessly into
existing KD methods without introducing extra modules or requiring
architectural changes. Extensive experiments on CIFAR-100, ImageNet, STL-10,
and Tiny-ImageNet demonstrate that TopKD consistently surpasses
state-of-the-art distillation methods. Moreover, our method demonstrates
substantial effectiveness when distilling Vision Transformers, underscoring its
versatility across diverse network architectures. These findings highlight the
significant potential of logits to advance knowledge distillation.

</details>


### [111] [InceptoFormer: A Multi-Signal Neural Framework for Parkinson's Disease Severity Evaluation from Gait](https://arxiv.org/abs/2508.04540)
*Safwen Naimi,Arij Said,Wassim Bouachir,Guillaume-Alexandre Bilodeau*

Main category: cs.CV

TL;DR: InceptoFormer 是一种结合 Inception1D 和 Transformer 的神经框架，用于 PD 严重程度评估，准确率达 96.6%，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 通过步态动力学分析评估帕金森病（PD）严重程度，现有方法在捕捉多尺度时间特征和长距离依赖关系方面存在不足。

Method: 提出了一种多信号神经框架 InceptoFormer，结合了 Inception1D（多尺度时间特征提取）和 Transformer（建模长距离依赖关系），并采用过采样策略解决类别不平衡问题。

Result: InceptoFormer 在 PD 严重程度评估中实现了 96.6% 的准确率，超越了现有最优方法。

Conclusion: InceptoFormer 通过结合 Inception1D 和 Transformer 架构，显著提升了帕金森病（PD）严重程度评估的分类性能，准确率达到 96.6%，优于现有方法。

Abstract: We present InceptoFormer, a multi-signal neural framework designed for
Parkinson's Disease (PD) severity evaluation via gait dynamics analysis. Our
architecture introduces a 1D adaptation of the Inception model, which we refer
to as Inception1D, along with a Transformer-based framework to stage PD
severity according to the Hoehn and Yahr (H&Y) scale. The Inception1D component
captures multi-scale temporal features by employing parallel 1D convolutional
filters with varying kernel sizes, thereby extracting features across multiple
temporal scales. The transformer component efficiently models long-range
dependencies within gait sequences, providing a comprehensive understanding of
both local and global patterns. To address the issue of class imbalance in PD
severity staging, we propose a data structuring and preprocessing strategy
based on oversampling to enhance the representation of underrepresented
severity levels. The overall design enables to capture fine-grained temporal
variations and global dynamics in gait signal, significantly improving
classification performance for PD severity evaluation. Through extensive
experimentation, InceptoFormer achieves an accuracy of 96.6%, outperforming
existing state-of-the-art methods in PD severity assessment. The source code
for our implementation is publicly available at
https://github.com/SafwenNaimi/InceptoFormer

</details>


### [112] [Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding](https://arxiv.org/abs/2508.04546)
*Minghang Zheng,Yuxin Peng,Benyuan Sun,Yi Yang,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种分层事件记忆框架，用于在线视频时间定位，通过事件提案和未来预测分支提升性能，并在多个数据集上取得最佳结果。


<details>
  <summary>Details</summary>
Motivation: 解决现有OnVTG模型缺乏有效事件建模和长期历史信息保留的问题，提升在线视频时间定位的性能。

Method: 采用基于事件的OnVTG框架，通过事件提案建模事件级信息，并引入分层事件记忆保留历史事件信息，同时提出未来预测分支以实现实时预测。

Result: 在TACoS、ActivityNet Captions和MAD数据集上实现了最先进性能。

Conclusion: 本文提出了一种分层事件记忆框架，用于在线视频时间定位（OnVTG），显著提升了模型性能，并在多个数据集上实现了最先进的结果。

Abstract: In this paper, we tackle the task of online video temporal grounding (OnVTG),
which requires the model to locate events related to a given text query within
a video stream. Unlike regular video temporal grounding, OnVTG requires the
model to make predictions without observing future frames. As online videos are
streaming inputs and can go on indefinitely, it is impractical and inefficient
to store all historical inputs. The existing OnVTG models employ memory to
store recent historical video frame features and predict scores indicating
whether the current frame corresponds to the start or end time of the target
event. However, these methods lack effective event modeling and cannot retain
long-term historical information, leading to low performance. To tackle these
challenges, we propose a hierarchical event memory for OnVTG. We propose an
event-based OnVTG framework that makes predictions based on event proposals
that model event-level information with various durations. To preserve
historically valuable event information, we introduce a hierarchical event
memory that retains historical events, allowing the model to access both recent
and long-term information. To enable the real-time prediction, we further
propose a future prediction branch that predicts whether the target event will
occur shortly and further regresses the start time of the event. We achieve
state-of-the-art performance on the TACoS, ActivityNet Captions, and MAD
datasets. Code is available at https://github.com/minghangz/OnVTG.

</details>


### [113] [Two-Way Garment Transfer: Unified Diffusion Framework for Dressing and Undressing Synthesis](https://arxiv.org/abs/2508.04551)
*Angang Zhang,Fang Deng,Hao Chen,Zhongjian Chen,Junyan Li*

Main category: cs.CV

TL;DR: TWGTM是首个统一框架，通过双向特征解耦和分阶段训练，同时解决虚拟试穿和试脱任务，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的虚拟试穿（VTON）和虚拟试脱（VTOFF）研究多将二者视为孤立任务，忽视了其互补对称性。本文旨在填补这一空白，提出统一框架以同时解决这两个任务。

Method: 本文提出了一种双向服装转移模型（TWGTM），通过双向特征解耦和双条件指导（潜在空间和像素空间），结合分阶段训练范式，解决了VTON和VTOFF任务的互补对称性问题。

Result: 在DressCode和VITON-HD数据集上的实验表明，TWGTM在VTON和VTOFF任务上均表现出色，具有竞争优势。

Conclusion: 本文提出的TWGTM框架通过双向特征解耦和分阶段训练范式，成功统一了虚拟试穿（VTON）和虚拟试脱（VTOFF）任务，验证了其在DressCode和VITON-HD数据集上的有效性和竞争力。

Abstract: While recent advances in virtual try-on (VTON) have achieved realistic
garment transfer to human subjects, its inverse task, virtual try-off (VTOFF),
which aims to reconstruct canonical garment templates from dressed humans,
remains critically underexplored and lacks systematic investigation. Existing
works predominantly treat them as isolated tasks: VTON focuses on garment
dressing while VTOFF addresses garment extraction, thereby neglecting their
complementary symmetry. To bridge this fundamental gap, we propose the Two-Way
Garment Transfer Model (TWGTM), to the best of our knowledge, the first unified
framework for joint clothing-centric image synthesis that simultaneously
resolves both mask-guided VTON and mask-free VTOFF through bidirectional
feature disentanglement. Specifically, our framework employs dual-conditioned
guidance from both latent and pixel spaces of reference images to seamlessly
bridge the dual tasks. On the other hand, to resolve the inherent mask
dependency asymmetry between mask-guided VTON and mask-free VTOFF, we devise a
phased training paradigm that progressively bridges this modality gap.
Extensive qualitative and quantitative experiments conducted across the
DressCode and VITON-HD datasets validate the efficacy and competitive edge of
our proposed approach.

</details>


### [114] [Augmentation-based Domain Generalization and Joint Training from Multiple Source Domains for Whole Heart Segmentation](https://arxiv.org/abs/2508.04552)
*Franz Thaler,Darko Stern,Gernot Plank,Martin Urschler*

Main category: cs.CV

TL;DR: 论文提出了一种结合CT和MR数据的全心脏分割方法，通过平衡训练和增强技术，显著提高了分割精度，为心脏数字孪生模型提供了可靠支持。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要更先进的方法来分析心脏及其子结构，以评估患者特定的心脏形态和病理。

Method: 采用了平衡联合训练方法，利用CT和MR数据，并结合强强度和空间增强技术以提高训练数据的多样性。

Result: 在CT数据上达到93.33% DSC和0.8388 mm ASSD，MR数据上达到89.30% DSC和1.2411 mm ASSD，性能优异。

Conclusion: 提出的全心脏分割方法在CT和MR数据上表现出色，展示了从患者特定心脏数字孪生模型中高效获取准确语义分割的潜力。

Abstract: As the leading cause of death worldwide, cardiovascular diseases motivate the
development of more sophisticated methods to analyze the heart and its
substructures from medical images like Computed Tomography (CT) and Magnetic
Resonance (MR). Semantic segmentations of important cardiac structures that
represent the whole heart are useful to assess patient-specific cardiac
morphology and pathology. Furthermore, accurate semantic segmentations can be
used to generate cardiac digital twin models which allows e.g.
electrophysiological simulation and personalized therapy planning. Even though
deep learning-based methods for medical image segmentation achieved great
advancements over the last decade, retaining good performance under domain
shift -- i.e. when training and test data are sampled from different data
distributions -- remains challenging. In order to perform well on domains known
at training-time, we employ a (1) balanced joint training approach that
utilizes CT and MR data in equal amounts from different source domains.
Further, aiming to alleviate domain shift towards domains only encountered at
test-time, we rely on (2) strong intensity and spatial augmentation techniques
to greatly diversify the available training data. Our proposed whole heart
segmentation method, a 5-fold ensemble with our contributions, achieves the
best performance for MR data overall and a performance similar to the best
performance for CT data when compared to a model trained solely on CT. With
93.33% DSC and 0.8388 mm ASSD for CT and 89.30% DSC and 1.2411 mm ASSD for MR
data, our method demonstrates great potential to efficiently obtain accurate
semantic segmentations from which patient-specific cardiac twin models can be
generated.

</details>


### [115] [One Model For All: Partial Diffusion for Unified Try-On and Try-Off in Any Pose](https://arxiv.org/abs/2508.04559)
*Jinxi Liu,Zijian He,Guangrun Wang,Guanbin Li,Liang Lin*

Main category: cs.CV

TL;DR: OMFA 是一种无需展示服装和掩码的扩散框架，支持任意姿态的虚拟试穿与试脱，通过部分扩散策略实现高效双向转换，实验效果领先。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的虚拟试穿方法依赖展示服装和分割掩码，且对姿态变化的适应性有限，限制了实际应用。OMFA 旨在解决这些问题，支持无需展示服装和任意姿态的虚拟试穿与试脱。

Method: OMFA 基于一种新颖的“部分扩散”策略，选择性对联合输入的各个组件（如服装、人物图像或面部）施加噪声和去噪，实现动态子任务控制和高效的服装-人物双向转换。

Result: 实验表明，OMFA 在虚拟试穿和试脱任务上均达到了最先进水平，支持从单一图像实现多视角和任意姿态的试穿。

Conclusion: OMFA 提供了一种实用的、可推广的虚拟服装合成解决方案，无需依赖展示服装和分割掩码，支持任意姿态，实现了最先进的虚拟试穿和试脱效果。

Abstract: Recent diffusion-based approaches have made significant advances in
image-based virtual try-on, enabling more realistic and end-to-end garment
synthesis. However, most existing methods remain constrained by their reliance
on exhibition garments and segmentation masks, as well as their limited ability
to handle flexible pose variations. These limitations reduce their practicality
in real-world scenarios-for instance, users cannot easily transfer garments
worn by one person onto another, and the generated try-on results are typically
restricted to the same pose as the reference image. In this paper, we introduce
\textbf{OMFA} (\emph{One Model For All}), a unified diffusion framework for
both virtual try-on and try-off that operates without the need for exhibition
garments and supports arbitrary poses. For example, OMFA enables removing
garments from a source person (try-off) and transferring them onto a target
person (try-on), while also allowing the generated target to appear in novel
poses-even without access to multi-pose images of that person. OMFA is built
upon a novel \emph{partial diffusion} strategy that selectively applies noise
and denoising to individual components of the joint input-such as the garment,
the person image, or the face-enabling dynamic subtask control and efficient
bidirectional garment-person transformation. The framework is entirely
mask-free and requires only a single portrait and a target pose as input,
making it well-suited for real-world applications. Additionally, by leveraging
SMPL-X-based pose conditioning, OMFA supports multi-view and arbitrary-pose
try-on from just one image. Extensive experiments demonstrate that OMFA
achieves state-of-the-art results on both try-on and try-off tasks, providing a
practical and generalizable solution for virtual garment synthesis. The project
page is here: https://onemodelforall.github.io/.

</details>


### [116] [Drone Detection with Event Cameras](https://arxiv.org/abs/2508.04564)
*Gabriele Magrini,Lorenzo Berlincioni,Luca Cultrera,Federico Becattini,Pietro Pala*

Main category: cs.CV

TL;DR: 本文探讨了事件相机在无人机检测中的优势，包括消除运动模糊和极端光照条件下的稳定检测，展示了其在反无人机系统中的潜力。


<details>
  <summary>Details</summary>
Motivation: 无人机扩散带来了重大的安全和安保挑战，而传统监视系统难以可靠检测这些目标。

Method: 本文综述了事件视觉领域的最新进展，包括数据表示方法和使用脉冲神经网络的高级处理流程。

Result: 事件相机几乎消除了运动模糊，并在极端光照条件下实现了稳定检测。

Conclusion: 事件相机技术为下一代可靠、低延迟且高效的反无人机系统提供了强大的基础。

Abstract: The diffusion of drones presents significant security and safety challenges.
Traditional surveillance systems, particularly conventional frame-based
cameras, struggle to reliably detect these targets due to their small size,
high agility, and the resulting motion blur and poor performance in challenging
lighting conditions. This paper surveys the emerging field of event-based
vision as a robust solution to these problems. Event cameras virtually
eliminate motion blur and enable consistent detection in extreme lighting.
Their sparse, asynchronous output suppresses static backgrounds, enabling
low-latency focus on motion cues. We review the state-of-the-art in event-based
drone detection, from data representation methods to advanced processing
pipelines using spiking neural networks. The discussion extends beyond simple
detection to cover more sophisticated tasks such as real-time tracking,
trajectory forecasting, and unique identification through propeller signature
analysis. By examining current methodologies, available datasets, and the
distinct advantages of the technology, this work demonstrates that event-based
vision provides a powerful foundation for the next generation of reliable,
low-latency, and efficient counter-UAV systems.

</details>


### [117] [TAlignDiff: Automatic Tooth Alignment assisted by Diffusion-based Transformation Learning](https://arxiv.org/abs/2508.04565)
*Yunbi Liu,Enqi Tang,Shiyu Li,Lei Ma,Juncheng Li,Shu Lou,Yongchu Pan,Qingshan Liu*

Main category: cs.CV

TL;DR: TAlignDiff是一种新型自动牙齿对齐方法，结合点云回归和扩散模型，通过双向反馈机制优化对齐效果，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖点对点几何约束预测转换矩阵，但忽略了这些矩阵与口腔解剖结构的关联及其分布特性。为此，提出TAlignDiff方法以更全面地捕捉这些特性。

Method: TAlignDiff由点云回归网络（PRN）和扩散基转换矩阵去噪模块（DTMD）组成。PRN通过几何约束损失进行点云级对齐学习，DTMD则从临床数据中学习转换矩阵的潜在分布。两者在一个统一框架中结合，实现几何约束与扩散优化的双向反馈。

Result: 大量消融和对比实验证明了TAlignDiff的有效性和优越性，尤其在牙齿对齐的自然性和准确性方面表现突出。

Conclusion: TAlignDiff结合了点云回归网络和扩散基转换矩阵去噪模块，通过双向反馈机制有效提升了牙齿对齐的准确性和自然性，展示了在正畸治疗中的潜力。

Abstract: Orthodontic treatment hinges on tooth alignment, which significantly affects
occlusal function, facial aesthetics, and patients' quality of life. Current
deep learning approaches predominantly concentrate on predicting transformation
matrices through imposing point-to-point geometric constraints for tooth
alignment. Nevertheless, these matrices are likely associated with the
anatomical structure of the human oral cavity and possess particular
distribution characteristics that the deterministic point-to-point geometric
constraints in prior work fail to capture. To address this, we introduce a new
automatic tooth alignment method named TAlignDiff, which is supported by
diffusion-based transformation learning. TAlignDiff comprises two main
components: a primary point cloud-based regression network (PRN) and a
diffusion-based transformation matrix denoising module (DTMD).
Geometry-constrained losses supervise PRN learning for point cloud-level
alignment. DTMD, as an auxiliary module, learns the latent distribution of
transformation matrices from clinical data. We integrate point cloud-based
transformation regression and diffusion-based transformation modeling into a
unified framework, allowing bidirectional feedback between geometric
constraints and diffusion refinement. Extensive ablation and comparative
experiments demonstrate the effectiveness and superiority of our method,
highlighting its potential in orthodontic treatment.

</details>


### [118] [Analyzing and Mitigating Object Hallucination: A Training Bias Perspective](https://arxiv.org/abs/2508.04567)
*Yifan Li,Kun Zhou,Wayne Xin Zhao,Lei Fang,Ji-Rong Wen*

Main category: cs.CV

TL;DR: 研究训练数据对大型视觉语言模型幻觉问题的影响，提出Obliviate方法通过偏置遗忘减少幻觉，效果显著且可扩展。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在视觉输入不一致时仍生成文本，即幻觉问题，研究训练数据在其中的作用。

Method: 引入POPEv2基准，通过掩码训练数据中的对象创建反事实图像，评估模型表现，并提出Obliviate方法，仅更新语言建模头以消除训练偏置。

Result: Obliviate方法在仅更新约2%参数的情况下，显著减少了对象幻觉，适用于不同模型规模和数据量。

Conclusion: Obliviate方法通过高效的偏置遗忘显著减少了大型视觉语言模型的对象幻觉问题，展示了良好的可扩展性和泛化能力。

Abstract: As scaling up training data has significantly improved the general multimodal
capabilities of Large Vision-Language Models (LVLMs), they still suffer from
the hallucination issue, generating text that is inconsistent with the visual
input. This phenomenon motivates us to systematically investigate the role of
training data in hallucination. We introduce a new benchmark, POPEv2, which
consists of counterfactual images collected from the training data of LVLMs
with certain objects masked. Through comprehensive evaluation on POPEv2, we
find that current LVLMs suffer from training bias: they fail to fully leverage
their training data and hallucinate more frequently on images seen during
training. Specifically, they perform poorly on counterfactual images, often
incorrectly answering ``Yes'' to questions about masked objects. To understand
this issue, we conduct probing experiments on the models' internal components,
revealing that this training bias is primarily located in the language modeling
(LM) head. Based on these findings, we propose Obliviate, an efficient and
lightweight unlearning method designed to mitigate object hallucination via
training bias unlearning. Obliviate identifies the discrepancy between
ground-truth labels and model outputs on the training data as a proxy for bias
and adopts a parameter- and data-efficient fine-tuning strategy that only
updates the LM head. Extensive experiments demonstrate the effectiveness of our
approach. While only reusing the training data and updating approximately 2\%
of the parameters, Obliviate significantly reduces hallucination across both
discriminative and generative tasks. Furthermore, it demonstrates strong
scalability with respect to both model size (2B to 72B) and training data
volume, and exhibits promising generalization to hallucination types beyond
object-level hallucination. Our code and data will be publicly released.

</details>


### [119] [DDTracking: A Deep Generative Framework for Diffusion MRI Tractography with Streamline Local-Global Spatiotemporal Modeling](https://arxiv.org/abs/2508.04568)
*Yijie Li,Wei Zhang,Xi Zhu,Ye Wu,Yogesh Rathi,Lauren J. O'Donnell,Fan Zhang*

Main category: cs.CV

TL;DR: DDTracking 是一种基于条件去噪扩散过程的深度生成框架，通过双路径编码网络和条件扩散模型，显著提升了纤维束成像的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前纤维束成像方法在长范围一致性和细节捕捉上的不足，DDTracking 提出了一种端到端可学习的框架，以提升纤维束成像的准确性和泛化能力。

Method: DDTracking 是一种新颖的深度生成框架，将流线传播建模为条件去噪扩散过程，并引入了双路径编码网络，联合建模局部空间编码和全局时间依赖性，以及条件扩散模型模块。

Result: 在 ISMRM Challenge 和 TractoInferno 两个基准测试中，DDTracking 显著优于当前最先进的纤维束成像方法，并展示了在不同数据集上的强泛化能力。

Conclusion: DDTracking 提供了解剖学上合理且稳健的纤维束成像，为广泛的 dMRI 应用提供了一个可扩展、适应性强且端到端可学习的解决方案。

Abstract: This paper presents DDTracking, a novel deep generative framework for
diffusion MRI tractography that formulates streamline propagation as a
conditional denoising diffusion process. In DDTracking, we introduce a
dual-pathway encoding network that jointly models local spatial encoding
(capturing fine-scale structural details at each streamline point) and global
temporal dependencies (ensuring long-range consistency across the entire
streamline). Furthermore, we design a conditional diffusion model module, which
leverages the learned local and global embeddings to predict streamline
propagation orientations for tractography in an end-to-end trainable manner. We
conduct a comprehensive evaluation across diverse, independently acquired dMRI
datasets, including both synthetic and clinical data. Experiments on two
well-established benchmarks with ground truth (ISMRM Challenge and
TractoInferno) demonstrate that DDTracking largely outperforms current
state-of-the-art tractography methods. Furthermore, our results highlight
DDTracking's strong generalizability across heterogeneous datasets, spanning
varying health conditions, age groups, imaging protocols, and scanner types.
Collectively, DDTracking offers anatomically plausible and robust tractography,
presenting a scalable, adaptable, and end-to-end learnable solution for broad
dMRI applications. Code is available at:
https://github.com/yishengpoxiao/DDtracking.git

</details>


### [120] [Knowledge to Sight: Reasoning over Visual Attributes via Knowledge Decomposition for Abnormality Grounding](https://arxiv.org/abs/2508.04572)
*Jun Li,Che Liu,Wenjia Bai,Mingxuan Liu,Rossella Arcucci,Cosmin I. Bercea,Julia A. Schnabel*

Main category: cs.CV

TL;DR: K2Sight通过结构化语义监督，以少量数据和紧凑模型实现医疗图像异常定位的高效训练和优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决通用视觉-语言模型在医疗领域表现不佳的问题，同时避免大规模领域预训练的高昂标注和计算成本。

Method: 提出K2Sight框架，将临床概念分解为可解释的视觉属性（如形状、密度和解剖位置），并通过简洁的指令式提示指导区域-文本对齐训练。

Result: 仅使用1.5%的数据训练的0.23B和2B参数模型，性能达到或优于7B+医疗VLM，mAP50提升高达9.82%。

Conclusion: K2Sight框架通过引入结构化语义监督，显著提升了医疗图像中异常定位的效率和性能，同时大幅减少了数据需求和计算资源。

Abstract: In this work, we address the problem of grounding abnormalities in medical
images, where the goal is to localize clinical findings based on textual
descriptions. While generalist Vision-Language Models (VLMs) excel in natural
grounding tasks, they often struggle in the medical domain due to rare,
compositional, and domain-specific terms that are poorly aligned with visual
patterns. Specialized medical VLMs address this challenge via large-scale
domain pretraining, but at the cost of substantial annotation and computational
resources. To overcome these limitations, we propose \textbf{Knowledge to Sight
(K2Sight)}, a framework that introduces structured semantic supervision by
decomposing clinical concepts into interpretable visual attributes, such as
shape, density, and anatomical location. These attributes are distilled from
domain ontologies and encoded into concise instruction-style prompts, which
guide region-text alignment during training. Unlike conventional report-level
supervision, our approach explicitly bridges domain knowledge and spatial
structure, enabling data-efficient training of compact models. We train compact
models with 0.23B and 2B parameters using only 1.5\% of the data required by
state-of-the-art medical VLMs. Despite their small size and limited training
data, these models achieve performance on par with or better than 7B+ medical
VLMs, with up to 9.82\% improvement in $mAP_{50}$. Code and models:
\href{https://lijunrio.github.io/K2Sight/}{\textcolor{SOTAPink}{https://lijunrio.github.io/K2Sight/}}.

</details>


### [121] [Visual Bias and Interpretability in Deep Learning for Dermatological Image Analysis](https://arxiv.org/abs/2508.04573)
*Enam Ahmed Taufik,Abdullah Khondoker,Antara Firoz Parsa,Seraj Al Mahmud Mostafa*

Main category: cs.CV

TL;DR: A deep learning framework for skin disease classification evaluates pre-processing techniques and models, finding DinoV2 with RGB pre-processing most effective (93% accuracy).


<details>
  <summary>Details</summary>
Motivation: The challenge of accurate skin disease classification due to high inter-class similarity, intra-class variability, and complex lesion textures motivates the need for improved deep learning-based CAD systems.

Method: The research systematically evaluates three image pre-processing techniques (standard RGB, CMY color space transformation, CLAHE) and benchmarks the performance of pre-trained CNN models (DenseNet201, EfficientNetB5) and transformer-based models (ViT, Swin Transformer, DinoV2 Large) using accuracy and F1-score metrics.

Result: DinoV2 with RGB pre-processing achieves the highest accuracy (up to 93%) and F1-scores, with Grad-CAM visualizations confirming precise lesion localization.

Conclusion: The study highlights the critical role of effective pre-processing and model selection in developing robust and explainable computer-aided diagnosis (CAD) systems for dermatology, with DinoV2 and RGB pre-processing achieving the highest performance.

Abstract: Accurate skin disease classification is a critical yet challenging task due
to high inter-class similarity, intra-class variability, and complex lesion
textures. While deep learning-based computer-aided diagnosis (CAD) systems have
shown promise in automating dermatological assessments, their performance is
highly dependent on image pre-processing and model architecture. This study
proposes a deep learning framework for multi-class skin disease classification,
systematically evaluating three image pre-processing techniques: standard RGB,
CMY color space transformation, and Contrast Limited Adaptive Histogram
Equalization (CLAHE). We benchmark the performance of pre-trained convolutional
neural networks (DenseNet201, Efficient-NetB5) and transformer-based models
(ViT, Swin Transformer, DinoV2 Large) using accuracy and F1-score as evaluation
metrics. Results show that DinoV2 with RGB pre-processing achieves the highest
accuracy (up to 93%) and F1-scores across all variants. Grad-CAM visualizations
applied to RGB inputs further reveal precise lesion localization, enhancing
interpretability. These findings underscore the importance of effective
pre-processing and model choice in building robust and explainable CAD systems
for dermatology.

</details>


### [122] [Face-voice Association in Multilingual Environments (FAME) 2026 Challenge Evaluation Plan](https://arxiv.org/abs/2508.04592)
*Marta Moscati,Ahmed Abdullah,Muhammad Saad Saeed,Shah Nawaz,Rohan Kumar Das,Muhammad Zaigham Zaheer,Junaid Mir,Muhammad Haroon Yousaf,Khalid Malik,Markus Schedl*

Main category: cs.CV

TL;DR: The FAME 2026 Challenge explores face-voice association in multilingual settings using the MAV-Celeb dataset, addressing the commonality of bilingual and multilingual communication.


<details>
  <summary>Details</summary>
Motivation: Half of the world's population is bilingual, and people often communicate in multilingual scenarios, highlighting the need to understand face-voice associations in such environments.

Method: The challenge utilizes the Multilingual Audio-Visual (MAV-Celeb) dataset and baseline models to investigate face-voice association under multilingual conditions.

Result: The report details the challenge, dataset, baseline models, and task specifics, setting the stage for future research in multimodal systems.

Conclusion: The FAME 2026 Challenge aims to explore face-voice association in multilingual environments, leveraging the MAV-Celeb dataset to address the unique challenges posed by bilingual and multilingual communication scenarios.

Abstract: The advancements of technology have led to the use of multimodal systems in
various real-world applications. Among them, audio-visual systems are among the
most widely used multimodal systems. In the recent years, associating face and
voice of a person has gained attention due to the presence of unique
correlation between them. The Face-voice Association in Multilingual
Environments (FAME) 2026 Challenge focuses on exploring face-voice association
under the unique condition of a multilingual scenario. This condition is
inspired from the fact that half of the world's population is bilingual and
most often people communicate under multilingual scenarios. The challenge uses
a dataset named Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice
association in multilingual environments. This report provides the details of
the challenge, dataset, baseline models, and task details for the FAME
Challenge.

</details>


### [123] [Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline](https://arxiv.org/abs/2508.04597)
*Linqing Zhao,Xiuwei Xu,Yirui Wang,Hao Wang,Wenzhao Zheng,Yansong Tang,Haibin Yan,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出基于3D高斯SLAM的在线3D重建方法，结合前馈预测模块和局部图渲染，显著提升速度，性能与SplaTAM相当，跟踪时间减少90%。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长序列处理或依赖慢速测试时优化和深度传感器方面存在不足，需要一种更高效且不依赖深度传感器的3D重建方法。

Method: 通过将深度估计器集成到RGB-D SLAM系统中，并利用3D高斯映射解决预测深度中几何细节不准确的问题。进一步结合前馈循环预测模块和局部图渲染技术，实现了快速网络推断替代慢速测试时优化。

Result: 在Replica和TUM-RGBD数据集上的实验结果表明，该方法性能与SplaTAM相当，同时跟踪时间减少了90%以上。

Conclusion: 本文提出了一种基于3D高斯SLAM的在线3D重建方法，结合前馈循环预测模块直接从光流推断相机姿态，显著提升了跟踪速度，并在Replica和TUM-RGBD数据集上展示了与SplaTAM相当的性能，同时减少了90%以上的跟踪时间。

Abstract: Incrementally recovering real-sized 3D geometry from a pose-free RGB stream
is a challenging task in 3D reconstruction, requiring minimal assumptions on
input data. Existing methods can be broadly categorized into end-to-end and
visual SLAM-based approaches, both of which either struggle with long sequences
or depend on slow test-time optimization and depth sensors. To address this, we
first integrate a depth estimator into an RGB-D SLAM system, but this approach
is hindered by inaccurate geometric details in predicted depth. Through further
investigation, we find that 3D Gaussian mapping can effectively solve this
problem. Building on this, we propose an online 3D reconstruction method using
3D Gaussian-based SLAM, combined with a feed-forward recurrent prediction
module to directly infer camera pose from optical flow. This approach replaces
slow test-time optimization with fast network inference, significantly
improving tracking speed. Additionally, we introduce a local graph rendering
technique to enhance robustness in feed-forward pose prediction. Experimental
results on the Replica and TUM-RGBD datasets, along with a real-world
deployment demonstration, show that our method achieves performance on par with
the state-of-the-art SplaTAM, while reducing tracking time by more than 90\%.

</details>


### [124] [How Does Bilateral Ear Symmetry Affect Deep Ear Features?](https://arxiv.org/abs/2508.04614)
*Kagan Ozturk,Deeksha Arun,Kevin W. Bowyer,Patrick Flynn*

Main category: cs.CV

TL;DR: 研究发现双边耳对称性对CNN-based耳部识别有重要影响，分别处理左右耳数据可提升性能，并提供了系统优化的实用建议。


<details>
  <summary>Details</summary>
Motivation: 探讨双边耳对称性对CNN-based耳部识别特征学习的影响，填补现有研究的空白。

Method: 开发了一个耳部侧边分类器来自动分类左右耳图像，并研究了在训练和测试阶段加入侧边信息的影响。通过跨数据集评估和消融实验，分析了对齐策略、输入尺寸及超参数设置的影响。

Result: 实验结果表明，分别处理左右耳数据能带来显著的性能提升，消融研究为系统优化提供了具体指导。

Conclusion: 研究发现，在训练和测试阶段分别处理左右耳数据能显著提升CNN-based耳部识别系统的性能，同时为大规模数据集上的系统优化提供了实用建议。

Abstract: Ear recognition has gained attention as a reliable biometric technique due to
the distinctive characteristics of human ears. With the increasing availability
of large-scale datasets, convolutional neural networks (CNNs) have been widely
adopted to learn features directly from raw ear images, outperforming
traditional hand-crafted methods. However, the effect of bilateral ear symmetry
on the features learned by CNNs has received little attention in recent
studies. In this paper, we investigate how bilateral ear symmetry influences
the effectiveness of CNN-based ear recognition. To this end, we first develop
an ear side classifier to automatically categorize ear images as either left or
right. We then explore the impact of incorporating this side information during
both training and test. Cross-dataset evaluations are conducted on five
datasets. Our results suggest that treating left and right ears separately
during training and testing can lead to notable performance improvements.
Furthermore, our ablation studies on alignment strategies, input sizes, and
various hyperparameter settings provide practical insights into training
CNN-based ear recognition systems on large-scale datasets to achieve higher
verification rates.

</details>


### [125] [FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging](https://arxiv.org/abs/2508.04625)
*Zichen Tang,Haihong E,Jiacheng Liu,Zhongjun Yang,Rongjin Li,Zihua Rong,Haoyang He,Zhuodi Hao,Xinyang Hu,Kun Ji,Ziyan Ma,Mengyuan Ji,Jun Zhang,Chenghao Ma,Qianhe Zheng,Yang Liu,Yiling Huang,Xinyi Hu,Qing Huang,Zijian Xie,Shiyao Peng*

Main category: cs.CV

TL;DR: FinMMR is a bilingual multimodal benchmark for evaluating MLLMs in financial numerical reasoning, featuring 4.3K questions and 8.7K images across 14 categories, with the best model scoring 53.0% on Hard problems.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a comprehensive bilingual multimodal benchmark for evaluating MLLMs in financial numerical reasoning tasks, especially in complex financial scenarios.

Method: The benchmark involves transforming existing financial reasoning benchmarks and constructing novel questions from Chinese financial research reports, resulting in 4.3K questions and 8.7K images across 14 categories.

Result: The best-performing MLLM achieves only 53.0% accuracy on Hard problems, indicating the challenging nature of FinMMR.

Conclusion: FinMMR is proposed as a benchmark to drive advancements in enhancing the reasoning capabilities of multimodal large language models (MLLMs) in real-world financial scenarios.

Abstract: We present FinMMR, a novel bilingual multimodal benchmark tailored to
evaluate the reasoning capabilities of multimodal large language models (MLLMs)
in financial numerical reasoning tasks. Compared to existing benchmarks, our
work introduces three significant advancements. (1) Multimodality: We
meticulously transform existing financial reasoning benchmarks, and construct
novel questions from the latest Chinese financial research reports. FinMMR
comprises 4.3K questions and 8.7K images spanning 14 categories, including
tables, bar charts, and ownership structure charts. (2) Comprehensiveness:
FinMMR encompasses 14 financial subdomains, including corporate finance,
banking, and industry analysis, significantly exceeding existing benchmarks in
financial domain knowledge breadth. (3) Challenge: Models are required to
perform multi-step precise numerical reasoning by integrating financial
knowledge with the understanding of complex financial images and text. The
best-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe
that FinMMR will drive advancements in enhancing the reasoning capabilities of
MLLMs in real-world scenarios.

</details>


### [126] [EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts](https://arxiv.org/abs/2508.04650)
*Kushin Mukherjee,Donghao Ren,Dominik Moritz,Yannick Assogba*

Main category: cs.CV

TL;DR: EncQA基准揭示了当前视觉语言模型在图表理解上的局限性，提出需针对性策略而非单纯扩大模型规模。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态视觉语言模型在图表理解基准上的进步未能全面覆盖视觉推理能力的广度。

Method: 引入了EncQA基准，包含2,076个合成问答对，覆盖6种视觉编码通道和8种任务，评估了9种最先进的多模态视觉语言模型。

Result: 评估显示，性能在不同编码和任务间差异显著，且模型规模对许多任务-编码对的性能提升有限。

Conclusion: 提升图表理解能力需要针对特定视觉推理差距的策略，而不仅仅是扩大模型或数据集规模。

Abstract: Multimodal vision-language models (VLMs) continue to achieve ever-improving
scores on chart understanding benchmarks. Yet, we find that this progress does
not fully capture the breadth of visual reasoning capabilities essential for
interpreting charts. We introduce EncQA, a novel benchmark informed by the
visualization literature, designed to provide systematic coverage of visual
encodings and analytic tasks that are crucial for chart understanding. EncQA
provides 2,076 synthetic question-answer pairs, enabling balanced coverage of
six visual encoding channels (position, length, area, color quantitative, color
nominal, and shape) and eight tasks (find extrema, retrieve value, find
anomaly, filter values, compute derived value exact, compute derived value
relative, correlate values, and correlate values relative). Our evaluation of 9
state-of-the-art VLMs reveals that performance varies significantly across
encodings within the same task, as well as across tasks. Contrary to
expectations, we observe that performance does not improve with model size for
many task-encoding pairs. Our results suggest that advancing chart
understanding requires targeted strategies addressing specific visual reasoning
gaps, rather than solely scaling up model or dataset size.

</details>


### [127] [PixCuboid: Room Layout Estimation from Multi-view Featuremetric Alignment](https://arxiv.org/abs/2508.04659)
*Gustav Hanning,Kalle Åström,Viktor Larsson*

Main category: cs.CV

TL;DR: PixCuboid是一种基于多视图对齐的优化方法，显著提升了立方体房间布局估计性能，并支持多房间扩展。


<details>
  <summary>Details</summary>
Motivation: 当前基于单视图的方法在房间布局估计中存在局限性，需要一种更灵活且性能优越的多视图对齐方法。

Method: 基于多视图对齐的密集深度特征优化方法，通过端到端训练学习特征图，实现大收敛盆地和平滑损失景观。

Result: 在ScanNet++和2D-3D-Semantics基准测试中显著优于现有方法，并展示了多房间估计的潜力。

Conclusion: PixCuboid通过端到端优化的多视图对齐方法，显著提升了立方体形状房间布局估计的性能，并展示了扩展到多房间估计的灵活性。

Abstract: Coarse room layout estimation provides important geometric cues for many
downstream tasks. Current state-of-the-art methods are predominantly based on
single views and often assume panoramic images. We introduce PixCuboid, an
optimization-based approach for cuboid-shaped room layout estimation, which is
based on multi-view alignment of dense deep features. By training with the
optimization end-to-end, we learn feature maps that yield large convergence
basins and smooth loss landscapes in the alignment. This allows us to
initialize the room layout using simple heuristics.
  For the evaluation we propose two new benchmarks based on ScanNet++ and
2D-3D-Semantics, with manually verified ground truth 3D cuboids. In thorough
experiments we validate our approach and significantly outperform the
competition. Finally, while our network is trained with single cuboids, the
flexibility of the optimization-based approach allow us to easily extend to
multi-room estimation, e.g. larger apartments or offices. Code and model
weights are available at https://github.com/ghanning/PixCuboid.

</details>


### [128] [ANPrompt: Anti-noise Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2508.04677)
*Yansheng Gao,Yufei Zheng,Jinghan Qu,Zixi Zhu,Yukuan Zhang,Shengsheng Wang*

Main category: cs.CV

TL;DR: ANPrompt是一种新型提示调优框架，通过噪声提示和抗噪声目标增强视觉语言模型在弱语义扰动下的鲁棒性，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有提示调优方法在弱语义扰动（如细微图像或文本噪声）下表现脆弱，影响模型对未见类别的泛化能力。

Method: ANPrompt通过融合原始和噪声扰动的文本嵌入构建弱噪声文本特征，聚类形成噪声提示，并与可学习提示令牌结合生成抗噪声提示，注入图像和文本编码器的深层。此外，通过计算噪声抵抗视觉提示原型（NRVPP）和引入弱语义噪声对齐损失（WALoss）来增强鲁棒性。

Result: 在11个基准测试中，ANPrompt在语义噪声鲁棒性和新类别泛化方面均优于现有方法。

Conclusion: ANPrompt显著提升了视觉语言模型在弱语义扰动下的鲁棒性，并在11个基准测试中表现优于现有方法。

Abstract: Prompt tuning has emerged as an efficient and effective technique for
adapting vision-language models (VLMs) with low computational overhead.
However, existing methods often overlook the vulnerability of prompt-tuned VLMs
to weak semantic perturbations-such as subtle image or text noise-that degrade
their generalization to unseen classes. To address this limitation, we propose
ANPrompt, a novel prompt tuning framework designed to enhance robustness under
such perturbations. ANPrompt first constructs weak noise text features by
fusing original and noise-perturbed text embeddings, which are then clustered
to form noise prompts. These noise prompts are integrated with learnable prompt
tokens to generate anti-noise prompts, which are injected into the deeper
layers of both image and text encoders. To further capture the noise-aware
visual semantics, ANPrompt computes the Noise-Resistant Visual Prompt Prototype
(NRVPP) by averaging the output prompt tokens from the vision encoder. Finally,
ANPrompt introduces alignment, robustness, and anti-noise objectives by
computing a Weak semantic noise Alignment Loss (WALoss) alongside the standard
cross-entropy and sim loss. Experiments across 11 benchmarks demonstrate that
ANPrompt consistently outperforms existing prompt tuning approaches, achieving
superior robustness to semantic noise and improved generalization to novel
categories.

</details>


### [129] [Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions](https://arxiv.org/abs/2508.04681)
*Liang Xu,Chengqun Yang,Zili Lin,Fei Xu,Yifan Liu,Congsheng Xu,Yiyi Zhang,Jie Qin,Xingdong Sheng,Yunhui Liu,Xin Jin,Yichao Yan,Wenjun Zeng,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出了InterVLA数据集，结合通用交互知识和第一人称视角，为AI助手在物理世界中的交互研究提供了新基准。


<details>
  <summary>Details</summary>
Motivation: 现有数据集通常只关注特定类别的交互，且忽视了AI助手基于第一人称视角的感知和行动。本文强调通用交互知识和第一人称视角的不可或缺性。

Method: 通过混合RGB-MoCap系统，助手和指导者按照GPT生成的脚本与多个对象和场景进行互动，创建了InterVLA数据集。

Result: 成功构建了InterVLA，这是第一个大规模的人-物-人交互数据集，包含11.4小时和120万帧多模态数据，涵盖2个第一人称和5个第三人称视频、精确的人/物运动和语音命令。

Conclusion: InterVLA数据集和基准测试将为未来在物理世界中构建AI代理的研究提供重要支持。

Abstract: Learning action models from real-world human-centric interaction datasets is
important towards building general-purpose intelligent assistants with
efficiency. However, most existing datasets only offer specialist interaction
category and ignore that AI assistants perceive and act based on first-person
acquisition. We urge that both the generalist interaction knowledge and
egocentric modality are indispensable. In this paper, we embed the
manual-assisted task into a vision-language-action framework, where the
assistant provides services to the instructor following egocentric vision and
commands. With our hybrid RGB-MoCap system, pairs of assistants and instructors
engage with multiple objects and the scene following GPT-generated scripts.
Under this setting, we accomplish InterVLA, the first large-scale
human-object-human interaction dataset with 11.4 hours and 1.2M frames of
multimodal data, spanning 2 egocentric and 5 exocentric videos, accurate
human/object motions and verbal commands. Furthermore, we establish novel
benchmarks on egocentric human motion estimation, interaction synthesis, and
interaction prediction with comprehensive analysis. We believe that our
InterVLA testbed and the benchmarks will foster future works on building AI
agents in the physical world.

</details>


### [130] [TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction](https://arxiv.org/abs/2508.04682)
*Zewei Zhou,Seth Z. Zhao,Tianhui Cai,Zhiyu Huang,Bolei Zhou,Jiaqi Ma*

Main category: cs.CV

TL;DR: TurboTrain提出了一种高效的多智能体训练框架，通过预训练和平衡学习策略简化流程并提升性能。


<details>
  <summary>Details</summary>
Motivation: 端到端训练多智能体系统在多任务性能提升方面具有优势，但现有方法需要大量手动设计和监控，训练过程复杂且耗时。

Method: TurboTrain包含两个关键组件：基于掩码重建学习的多智能体时空预训练方案和基于梯度冲突抑制的平衡多任务学习策略。

Result: 在真实世界协同驾驶数据集V2XPnP-Seq上的实验表明，TurboTrain进一步提升了最先进的多智能体感知与预测模型的性能。

Conclusion: TurboTrain通过创新的预训练方案和平衡的多任务学习策略，显著提升了多智能体感知与预测任务的性能，并简化了训练流程。

Abstract: End-to-end training of multi-agent systems offers significant advantages in
improving multi-task performance. However, training such models remains
challenging and requires extensive manual design and monitoring. In this work,
we introduce TurboTrain, a novel and efficient training framework for
multi-agent perception and prediction. TurboTrain comprises two key components:
a multi-agent spatiotemporal pretraining scheme based on masked reconstruction
learning and a balanced multi-task learning strategy based on gradient conflict
suppression. By streamlining the training process, our framework eliminates the
need for manually designing and tuning complex multi-stage training pipelines,
substantially reducing training time and improving performance. We evaluate
TurboTrain on a real-world cooperative driving dataset, V2XPnP-Seq, and
demonstrate that it further improves the performance of state-of-the-art
multi-agent perception and prediction models. Our results highlight that
pretraining effectively captures spatiotemporal multi-agent features and
significantly benefits downstream tasks. Moreover, the proposed balanced
multi-task learning strategy enhances detection and prediction.

</details>


### [131] [BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning](https://arxiv.org/abs/2508.04702)
*Ziyang Leng,Jiawei Yang,Zhicheng Ren,Bolei Zhou*

Main category: cs.CV

TL;DR: BEVCon通过对比学习提升BEV感知性能，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注BEV编码器和任务特定头部的优化，而BEV表征学习的潜力尚未充分探索。

Method: BEVCon引入了两个对比学习模块：实例特征对比模块用于优化BEV特征，视角对比模块用于增强图像主干网络。

Result: 在nuScenes数据集上，BEVCon实现了最高2.4% mAP的性能提升。

Conclusion: BEVCon通过对比学习模块显著提升了BEV感知的性能，强调了表征学习在BEV模型中的重要性。

Abstract: We present BEVCon, a simple yet effective contrastive learning framework
designed to improve Bird's Eye View (BEV) perception in autonomous driving. BEV
perception offers a top-down-view representation of the surrounding
environment, making it crucial for 3D object detection, segmentation, and
trajectory prediction tasks. While prior work has primarily focused on
enhancing BEV encoders and task-specific heads, we address the underexplored
potential of representation learning in BEV models. BEVCon introduces two
contrastive learning modules: an instance feature contrast module for refining
BEV features and a perspective view contrast module that enhances the image
backbone. The dense contrastive learning designed on top of detection losses
leads to improved feature representations across both the BEV encoder and the
backbone. Extensive experiments on the nuScenes dataset demonstrate that BEVCon
achieves consistent performance gains, achieving up to +2.4% mAP improvement
over state-of-the-art baselines. Our results highlight the critical role of
representation learning in BEV perception and offer a complementary avenue to
conventional task-specific optimizations.

</details>


### [132] [Occupancy Learning with Spatiotemporal Memory](https://arxiv.org/abs/2508.04705)
*Ziyang Leng,Jiawei Yang,Wenlong Yi,Bolei Zhou*

Main category: cs.CV

TL;DR: ST-Occ是一个学习时空特征的3D占用表示框架，通过时空记忆和注意力机制提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决多帧输入下3D占用预测的高处理成本和不确定性挑战，提升时空表示学习效率。

Method: 提出了ST-Occ框架，包含时空记忆和记忆注意力机制，用于高效学习时空特征并保持时间一致性。

Result: 实验显示，ST-Occ在3D占用预测任务中优于现有方法3 mIoU，并将时间不一致性降低了29%。

Conclusion: ST-Occ通过其时空记忆和注意力机制，显著提升了3D占用预测任务的性能，并在实验中表现出优于现有方法的性能。

Abstract: 3D occupancy becomes a promising perception representation for autonomous
driving to model the surrounding environment at a fine-grained scale. However,
it remains challenging to efficiently aggregate 3D occupancy over time across
multiple input frames due to the high processing cost and the uncertainty and
dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level
occupancy representation learning framework that effectively learns the
spatiotemporal feature with temporal consistency. ST-Occ consists of two core
designs: a spatiotemporal memory that captures comprehensive historical
information and stores it efficiently through a scene-level representation and
a memory attention that conditions the current occupancy representation on the
spatiotemporal memory with a model of uncertainty and dynamic awareness. Our
method significantly enhances the spatiotemporal representation learned for 3D
occupancy prediction tasks by exploiting the temporal dependency between
multi-frame inputs. Experiments show that our approach outperforms the
state-of-the-art methods by a margin of 3 mIoU and reduces the temporal
inconsistency by 29%.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [133] [Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation](https://arxiv.org/abs/2508.04418)
*Jinxing Zhou,Yanghao Zhou,Mingfei Han,Tong Wang,Xiaojun Chang,Hisham Cholakkal,Rao Muhammad Anwer*

Main category: cs.MM

TL;DR: TGS-Agent通过显式推理流程实现音频-视觉分割，无需像素级监督，性能最优。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖像素级监督和缺乏可解释性的问题，模仿人类推理过程实现显式参考理解。

Method: 提出TGS-Agent，将任务分解为Think-Ground-Segment流程，包括Ref-Thinker多模态推理、Grounding-DINO和SAM2的分割。

Result: 在Ref-AVSBench和R²-AVSBench上实现最优性能。

Conclusion: TGS-Agent通过Think-Ground-Segment流程实现了高效的音频-视觉分割，无需像素级监督，且在Ref-AVSBench和R²-AVSBench上达到最优性能。

Abstract: Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects
in audible videos based on given reference expressions. Prior works typically
rely on learning latent embeddings via multimodal fusion to prompt a tunable
SAM/SAM2 decoder for segmentation, which requires strong pixel-level
supervision and lacks interpretability. From a novel perspective of explicit
reference understanding, we propose TGS-Agent, which decomposes the task into a
Think-Ground-Segment process, mimicking the human reasoning procedure by first
identifying the referred object through multimodal analysis, followed by
coarse-grained grounding and precise segmentation. To this end, we first
propose Ref-Thinker, a multimodal language model capable of reasoning over
textual, visual, and auditory cues. We construct an instruction-tuning dataset
with explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The
object description inferred by Ref-Thinker is used as an explicit prompt for
Grounding-DINO and SAM2, which perform grounding and segmentation without
relying on pixel-level supervision. Additionally, we introduce
R\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and
reasoning-intensive references for better evaluating model generalization. Our
approach achieves state-of-the-art results on both standard Ref-AVSBench and
proposed R\textsuperscript{2}-AVSBench. Code will be available at
https://github.com/jasongief/TGS-Agent.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [134] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: MI9是首个专为代理型AI系统设计的运行时治理框架，通过六个实时控制组件解决传统治理无法覆盖的新风险，确保安全部署。


<details>
  <summary>Details</summary>
Motivation: 代理型AI系统在运行时表现出突发和意外行为，带来传统治理无法预见的新风险，需要专门的运行时治理框架。

Method: MI9通过六个集成组件实现实时控制：代理风险指数、代理语义遥测捕获、持续授权监控、基于有限状态机的一致性引擎、目标条件漂移检测和分级遏制策略。

Result: MI9在多样化场景中展示了系统性覆盖现有方法无法解决的治理挑战，为代理型AI的全面监督奠定了技术基础。

Conclusion: MI9框架为代理型AI系统的安全部署提供了全面的运行时治理基础，填补了传统治理方法的不足。

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [135] [Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety](https://arxiv.org/abs/2508.03864)
*Zhenyu Pan,Yiting Zhang,Yutong Zhang,Jianshu Zhang,Haozheng Luo,Yuwei Han,Dennis Wu,Hong-Yu Chen,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: Evo-MARL是一种新型多智能体强化学习框架，通过内化防御能力和对抗训练，显著提升多智能体系统的安全性和任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统（MAS）依赖外部安全模块（如专用安全代理）存在单点故障和有限保护的问题，需探索更高效且鲁棒的防御机制。

Method: Evo-MARL采用多智能体强化学习（MARL）框架，结合进化搜索和参数共享技术，共同演化攻击者和防御者，使每个代理在完成主要任务的同时具备防御能力。

Result: 实验表明，Evo-MARL将攻击成功率降低高达22%，同时在推理任务中提升准确性达5%。

Conclusion: Evo-MARL框架通过将防御能力内化到每个任务代理中，显著降低了攻击成功率并提升了任务准确性，实现了安全性与实用性的共同提升。

Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.

</details>


### [136] [MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework](https://arxiv.org/abs/2508.03929)
*Nguyen Viet Tuan Kiet,Dao Van Tung,Tran Cong Dao,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: MOTIF是一个基于蒙特卡洛树搜索的多智能体交互框架，通过轮换优化多个组件提升组合优化问题的求解性能，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅优化单一组件（如启发式评分函数），限制了创新空间。本文旨在通过多策略优化联合改进一组相互依赖的组件。

Method: 提出MOTIF框架，基于蒙特卡洛树搜索实现两个LLM智能体的轮换优化，每次优化一个组件并利用历史更新记录。

Result: 在多个组合优化问题领域，MOTIF均优于现有方法，验证了其有效性。

Conclusion: MOTIF框架通过多智能体交互和蒙特卡洛树搜索，显著提升了组合优化问题的求解性能，展示了自动化求解器设计的潜力。

Abstract: Designing effective algorithmic components remains a fundamental obstacle in
tackling NP-hard combinatorial optimization problems (COPs), where solvers
often rely on carefully hand-crafted strategies. Despite recent advances in
using large language models (LLMs) to synthesize high-quality components, most
approaches restrict the search to a single element - commonly a heuristic
scoring function - thus missing broader opportunities for innovation. In this
paper, we introduce a broader formulation of solver design as a multi-strategy
optimization problem, which seeks to jointly improve a set of interdependent
components under a unified objective. To address this, we propose
Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a
novel framework based on Monte Carlo Tree Search that facilitates turn-based
optimization between two LLM agents. At each turn, an agent improves one
component by leveraging the history of both its own and its opponent's prior
updates, promoting both competitive pressure and emergent cooperation. This
structured interaction broadens the search landscape and encourages the
discovery of diverse, high-performing solutions. Experiments across multiple
COP domains show that MOTIF consistently outperforms state-of-the-art methods,
highlighting the promise of turn-based, multi-agent prompting for fully
automated solver design.

</details>


### [137] [Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?](https://arxiv.org/abs/2508.03963)
*Zewen Liu,Juntong Ni,Xianfeng Tang,Max S. Y. Lau,Wei Jin*

Main category: cs.AI

TL;DR: 该论文提出了SymbolBench基准和结合LLM与遗传编程的框架，评估了LLM在时间序列数据符号推理中的能力，强调了领域知识、上下文对齐和推理结构的重要性。


<details>
  <summary>Details</summary>
Motivation: 揭示时间序列数据中隐藏的符号规律是科学发现和人工智能的核心挑战，而LLM在这方面的能力尚未充分探索。

Method: 引入SymbolBench基准，并提出了一个将LLM与遗传编程结合的统一框架，形成闭环符号推理系统。

Result: 实证结果揭示了当前模型的关键优势和局限性。

Conclusion: 结合领域知识、上下文对齐和推理结构对提升LLM在自动科学发现中的能力至关重要。

Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration
dating back to Kepler's discovery of planetary motion, remains a core challenge
in scientific discovery and artificial intelligence. While Large Language
Models show promise in structured reasoning tasks, their ability to infer
interpretable, context-aligned symbolic structures from time series data is
still underexplored. To systematically evaluate this capability, we introduce
SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning
over real-world time series across three tasks: multivariate symbolic
regression, Boolean network inference, and causal discovery. Unlike prior
efforts limited to simple algebraic equations, SymbolBench spans a diverse set
of symbolic forms with varying complexity. We further propose a unified
framework that integrates LLMs with genetic programming to form a closed-loop
symbolic reasoning system, where LLMs act both as predictors and evaluators.
Our empirical results reveal key strengths and limitations of current models,
highlighting the importance of combining domain knowledge, context alignment,
and reasoning structure to improve LLMs in automated scientific discovery.

</details>


### [138] [The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?](https://arxiv.org/abs/2508.03986)
*Yuan Xun,Xiaojun Jia,Xinwei Liu,Hua Zhang*

Main category: cs.AI

TL;DR: EmoAgent框架通过情感对抗提示劫持MLRMs推理路径，揭示其安全行为与情感认知的不对齐，并提出三个新指标量化风险。


<details>
  <summary>Details</summary>
Motivation: 发现MLRMs在深度思考阶段易受用户情感线索影响，导致安全协议被覆盖或安全检查失效。

Method: 提出EmoAgent框架，通过情感对抗提示劫持推理路径，并引入三个新指标（RRSS、RVNR、RAIC）量化风险。

Result: 实验证明EmoAgent有效，并揭示了模型安全行为中更深层的情感认知不对齐现象。

Conclusion: EmoAgent框架揭示了MLRMs在情感认知与安全行为之间的深层不对齐问题，现有基于内容的安全措施无法有效应对这些风险。

Abstract: We observe that MLRMs oriented toward human-centric service are highly
susceptible to user emotional cues during the deep-thinking stage, often
overriding safety protocols or built-in safety checks under high emotional
intensity. Inspired by this key insight, we propose EmoAgent, an autonomous
adversarial emotion-agent framework that orchestrates exaggerated affective
prompts to hijack reasoning pathways. Even when visual risks are correctly
identified, models can still produce harmful completions through emotional
misalignment. We further identify persistent high-risk failure modes in
transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning
masked behind seemingly safe responses. These failures expose misalignments
between internal inference and surface-level behavior, eluding existing
content-based safeguards. To quantify these risks, we introduce three metrics:
(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign
outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite
visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for
evaluating refusal unstability under prompt variants. Extensive experiments on
advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper
emotional cognitive misalignments in model safety behavior.

</details>


### [139] [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991)
*Chongyu Bao,Ruimin Dai,Yangbo Shen,Runyang Jian,Jinghan Zhang,Xiaolan Liu,Kunpeng Liu*

Main category: cs.AI

TL;DR: Proposes Galaxy, a framework for proactive, privacy-preserving IPAs using Cognition Forest, with KoRa and Kernel agents. Outperforms benchmarks, validated by experiments.


<details>
  <summary>Details</summary>
Motivation: Current IPAs lack proactive behaviors and self-evolution capabilities. The study aims to design an IPA that is proactive, privacy-preserving, and capable of self-evolution by leveraging LLM agents' cognitive architecture.

Method: The work introduces Cognition Forest, a semantic structure aligning cognitive modeling with system-level design, and Galaxy, a framework supporting multidimensional interactions and personalized capability generation. It implements two agents: KoRa for cognition-enhanced generative tasks and Kernel for meta-cognition-based self-evolution.

Result: Galaxy outperforms state-of-the-art benchmarks, with ablation studies and real-world interactions confirming its effectiveness in enabling proactive and self-evolving IPAs.

Conclusion: The proposed Galaxy framework, with its Cognition Forest semantic structure, effectively unifies cognitive architecture and system design, enabling proactive, privacy-preserving, and self-evolving IPAs. Experimental results and real-world cases validate its superiority over existing benchmarks.

Abstract: Intelligent personal assistants (IPAs) such as Siri and Google Assistant are
designed to enhance human capabilities and perform tasks on behalf of users.
The emergence of LLM agents brings new opportunities for the development of
IPAs. While responsive capabilities have been widely studied, proactive
behaviors remain underexplored. Designing an IPA that is proactive,
privacy-preserving, and capable of self-evolution remains a significant
challenge. Designing such IPAs relies on the cognitive architecture of LLM
agents. This work proposes Cognition Forest, a semantic structure designed to
align cognitive modeling with system-level design. We unify cognitive
architecture and system design into a self-reinforcing loop instead of treating
them separately. Based on this principle, we present Galaxy, a framework that
supports multidimensional interactions and personalized capability generation.
Two cooperative agents are implemented based on Galaxy: KoRa, a
cognition-enhanced generative agent that supports both responsive and proactive
skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's
self-evolution and privacy preservation. Experimental results show that Galaxy
outperforms multiple state-of-the-art benchmarks. Ablation studies and
real-world interaction cases validate the effectiveness of Galaxy.

</details>


### [140] [Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement](https://arxiv.org/abs/2508.04025)
*Chao Hao,Shuai Wang,Kaiwen Zhou*

Main category: cs.AI

TL;DR: RecAgent是一个不确定性感知的GUI代理，通过自适应感知和交互模块解决输入冗余和决策模糊问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决GUI代理在移动任务自动化中面临的输入冗余和决策模糊问题。

Method: RecAgent采用组件推荐机制减少感知不确定性，并通过交互模块在模糊情况下请求用户反馈以降低决策不确定性。

Result: 实验验证了RecAgent的有效性，并提出了ComplexAction数据集用于评估。

Conclusion: RecAgent通过自适应感知和交互模块有效解决了GUI代理中的输入冗余和决策模糊问题，其统一框架在复杂场景中表现出色。

Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile
tasks but still struggle with input redundancy and decision ambiguity. In this
paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses
these issues through adaptive perception. We distinguish two types of
uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input
redundancy and noise from comprehensive screen information, and (2) decision
uncertainty, arising from ambiguous tasks and complex reasoning. To reduce
perceptual uncertainty, RecAgent employs a component recommendation mechanism
that identifies and focuses on the most relevant UI elements. For decision
uncertainty, it uses an interactive module to request user feedback in
ambiguous situations, enabling intent-aware decisions. These components are
integrated into a unified framework that proactively reduces input complexity
and reacts to high-uncertainty cases via human-in-the-loop refinement.
Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate
the success rate of GUI agents in executing specified single-step actions
within complex scenarios. Extensive experiments validate the effectiveness of
our approach. The dataset and code will be available at
https://github.com/Fanye12/RecAgent.

</details>


### [141] [SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/abs/2508.04037)
*Liang Tang,Shuxian Li,Yuhao Cheng,Yukang Huo,Zhepeng Wang,Yiqiang Yan,Kaer Huang,Yanzhe Jing,Tiaonan Duan*

Main category: cs.AI

TL;DR: 提出自进化代理（SEA），通过创新数据生成、逐步强化学习和模型增强方法，实现了高效、轻量的计算机使用代理，性能优越且开源。


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用代理的性能远未达到实用水平，亟需一种高效、轻量且性能优越的解决方案。

Method: 提出了创新的数据生成、强化学习和模型增强方法：1. 自动生成可验证轨迹的流程；2. 高效的逐步强化学习以降低长时训练的计算需求；3. 无额外训练的模型增强方法，融合基础与规划能力。

Result: SEA仅需7B参数，性能优于同参数量模型，并接近更大模型。

Conclusion: 本文提出的自进化代理（SEA）在计算机使用任务中表现出色，性能优于同参数量的模型，并可与更大模型媲美。未来将开源模型权重和相关代码。

Abstract: Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.

</details>


### [142] [Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals](https://arxiv.org/abs/2508.04070)
*Ronja Mehlan,Claudia Hess,Quintus Stierstorfer,Kristina Schaaff*

Main category: cs.AI

TL;DR: 研究发现，基于职业目标的AI个性化学习内容能显著提升学习者的参与度和满意度，同时缩短学习时间，证明了其在教育中的实用价值。


<details>
  <summary>Details</summary>
Motivation: 探索基于生成式AI（GenAI）的职业目标内容适应如何影响学习者的参与度、满意度和学习效率。

Method: 采用混合方法实验，涉及4000多名学习者，一组接受基于职业目标量身定制的学习场景，另一组为对照组。

Result: 定量结果显示，个性化内容组的会话时长增加、满意度评分更高，学习时长略有减少；定性分析表明，学习者认为个性化材料更具激励性和实用性。

Conclusion: 研究表明，将学习内容与学习者的职业目标对齐具有重要价值，可扩展的AI个性化能够在学术知识与职场应用之间架起桥梁。

Abstract: As artificial intelligence becomes increasingly integrated into digital
learning environments, the personalization of learning content to reflect
learners' individual career goals offers promising potential to enhance
engagement and long-term motivation. In our study, we investigate how career
goal-based content adaptation in learning systems based on generative AI
(GenAI) influences learner engagement, satisfaction, and study efficiency. The
mixed-methods experiment involved more than 4,000 learners, with one group
receiving learning scenarios tailored to their career goals and a control
group. Quantitative results show increased session duration, higher
satisfaction ratings, and a modest reduction in study duration compared to
standard content. Qualitative analysis highlights that learners found the
personalized material motivating and practical, enabling deep cognitive
engagement and strong identification with the content. These findings
underscore the value of aligning educational content with learners' career
goals and suggest that scalable AI personalization can bridge academic
knowledge and workplace applicability.

</details>


### [143] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: 提出MAGRPO算法，通过MARL优化LLM在多智能体系统中的协作表现，实验验证其在写作和编码任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM主要在独立预训练基础上进行优化，缺乏针对多智能体协作的专门设计，且现有微调框架依赖个体奖励设计，复杂且难以鼓励协作。

Method: 研究将LLM协作建模为合作型多智能体强化学习（MARL）问题，并开发了多智能体、多回合算法MAGRPO，结合了现有RL和MARL技术。

Result: 实验表明，使用MAGRPO微调的MAS能够在写作和编码协作任务中高效生成高质量响应。

Conclusion: 该研究提出了一种新的多智能体协作优化方法MAGRPO，为LLM在多智能体系统中的协作提供了有效解决方案，并展示了其在写作和编码任务中的高效表现，同时指出了未来应用其他MARL方法于LLM的潜在挑战。

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


### [144] [KG-Augmented Executable CoT for Mathematical Coding](https://arxiv.org/abs/2508.04072)
*Xingyu Chen,Junxiu An,Jun Guo,Li Wang,Jingcai Guo*

Main category: cs.AI

TL;DR: KGA-ECoT框架通过知识图和可执行代码增强复杂推理任务，显著提升了数学推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在复杂推理任务（如数学推理和代码生成）中的局限性。

Method: KGA-ECoT通过将问题分解为结构化任务图，利用GraphRAG从数学库中精确检索知识，并生成可验证代码来确保计算准确性。

Result: 在多个数学推理基准测试中，KGA-ECoT显著优于现有提示方法，绝对准确率提升数个百分点至超过十个百分点。

Conclusion: KGA-ECoT被证明是一个强大且高度可泛化的框架，特别适用于复杂的数学推理任务。

Abstract: In recent years, large language models (LLMs) have excelled in natural
language processing tasks but face significant challenges in complex reasoning
tasks such as mathematical reasoning and code generation. To address these
limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a
novel framework that enhances code generation through knowledge graphs and
improves mathematical reasoning via executable code. KGA-ECoT decomposes
problems into a Structured Task Graph, leverages efficient GraphRAG for precise
knowledge retrieval from mathematical libraries, and generates verifiable code
to ensure computational accuracy. Evaluations on multiple mathematical
reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms
existing prompting methods, achieving absolute accuracy improvements ranging
from several to over ten percentage points. Further analysis confirms the
critical roles of GraphRAG in enhancing code quality and external code
execution in ensuring precision. These findings collectively establish KGA-ECoT
as a robust and highly generalizable framework for complex mathematical
reasoning tasks.

</details>


### [145] [GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement](https://arxiv.org/abs/2508.04080)
*Jinfan Tang,Kunming Wu,Ruifeng Gongxie,Yuya He,Yuankai Wu*

Main category: cs.AI

TL;DR: GeoSR是一种自优化代理框架，通过整合地理统计原则和空间结构化推理，提升LLMs在地理空间预测中的准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在无显式空间监督下展现出地理空间能力，但在空间一致性、多跳推理和地理偏差方面仍存在挑战，需要一种更有效的方法来提升其地理空间预测的准确性和公平性。

Method: 提出GeoSR，一种自优化的代理推理框架，将地理核心原则（如Tobler地理第一定律）嵌入迭代预测循环中，通过三个协作代理（变量选择、点选择、优化代理）逐步优化预测质量。

Result: 实验证明GeoSR在物理世界属性估计和社会经济预测等任务中，相比标准提示策略有显著改进。

Conclusion: GeoSR框架通过整合地理统计先验和空间结构化推理，显著提升了LLMs在空间一致性和多跳推理方面的表现，同时减少了地理偏差，实现了更准确和公平的地理空间预测。

Abstract: Recent studies have extended the application of large language models (LLMs)
to geographic problems, revealing surprising geospatial competence even without
explicit spatial supervision. However, LLMs still face challenges in spatial
consistency, multi-hop reasoning, and geographic bias. To address these issues,
we propose GeoSR, a self-refining agentic reasoning framework that embeds core
geographic principles -- most notably Tobler's First Law of Geography -- into
an iterative prediction loop. In GeoSR, the reasoning process is decomposed
into three collaborating agents: (1) a variable-selection agent that selects
relevant covariates from the same location; (2) a point-selection agent that
chooses reference predictions at nearby locations generated by the LLM in
previous rounds; and (3) a refine agent that coordinates the iterative
refinement process by evaluating prediction quality and triggering further
rounds when necessary. This agentic loop progressively improves prediction
quality by leveraging both spatial dependencies and inter-variable
relationships. We validate GeoSR on tasks ranging from physical-world property
estimation to socioeconomic prediction. Experimental results show consistent
improvements over standard prompting strategies, demonstrating that
incorporating geostatistical priors and spatially structured reasoning into
LLMs leads to more accurate and equitable geospatial predictions. The code of
GeoSR is available at https://github.com/JinfanTang/GeoSR.

</details>


### [146] [Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement](https://arxiv.org/abs/2508.04105)
*Karrtik Iyer,Manikandan Ravikiran,Prasanna Pendse,Shayan Mohanty*

Main category: cs.AI

TL;DR: 该论文提出语义熵作为衡量自动评分系统不确定性的方法，实验验证其与人类评分分歧的相关性，并展示其跨学科适用性。


<details>
  <summary>Details</summary>
Motivation: 现有自动评分系统在评分决策不确定或有争议时缺乏指示，因此需要一种方法来衡量评分者之间的分歧。

Method: 通过基于蕴含相似性的聚类和计算这些簇的熵，量化了论证的多样性，而不依赖于最终输出分数。

Result: 实验证明语义熵与评分者分歧相关，能够跨学科泛化，并对结构任务特征（如源依赖性）敏感。

Conclusion: 语义熵作为一种可解释的不确定性信号，能够支持更透明和可信的AI辅助评分工作流程。

Abstract: Automated grading systems can efficiently score short-answer responses, yet
they often fail to indicate when a grading decision is uncertain or potentially
contentious. We introduce semantic entropy, a measure of variability across
multiple GPT-4-generated explanations for the same student response, as a proxy
for human grader disagreement. By clustering rationales via entailment-based
similarity and computing entropy over these clusters, we quantify the diversity
of justifications without relying on final output scores. We address three
research questions: (1) Does semantic entropy align with human grader
disagreement? (2) Does it generalize across academic subjects? (3) Is it
sensitive to structural task features such as source dependency? Experiments on
the ASAP-SAS dataset show that semantic entropy correlates with rater
disagreement, varies meaningfully across subjects, and increases in tasks
requiring interpretive reasoning. Our findings position semantic entropy as an
interpretable uncertainty signal that supports more transparent and trustworthy
AI-assisted grading workflows.

</details>


### [147] [A Compositional Framework for On-the-Fly LTLf Synthesis](https://arxiv.org/abs/2508.04116)
*Yongkang Li,Shengping Xiao,Shufang Zhu,Jianwen Li,Geguang Pu*

Main category: cs.AI

TL;DR: 论文提出了一种组合式即时合成框架，结合两种现有方法的优点，有效处理大型LTLf公式合取问题，并在某些情况下优于现有求解器。


<details>
  <summary>Details</summary>
Motivation: 现有技术在构建DFA时要么在解决游戏前组合构建，利用自动机最小化缓解状态空间爆炸，要么在游戏求解过程中增量构建以避免完全DFA构建，但两者均非最佳。

Method: 引入了一种组合式即时合成框架，将组合应用于游戏求解过程中而非自动机（游戏竞技场）构建阶段，支持两种组合变体：在组合前进行剪枝以充分利用最小化，或在组合过程中进行剪枝以指导即时合成。

Result: 与最先进的合成求解器相比，该框架能够解决其他求解器无法处理的多个实例，详细分析表明两种组合变体各有独特优势。

Conclusion: 该论文提出了一种结合两种现有方法优点的组合式即时合成框架，有效处理实践中常见的大型LTLf公式合取问题，并在某些情况下表现优于现有合成求解器。

Abstract: Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can
be reduced to a two-player game over a Deterministic Finite Automaton (DFA) of
the LTLf specification. The primary challenge here is DFA construction, which
is 2EXPTIME-complete in the worst case. Existing techniques either construct
the DFA compositionally before solving the game, leveraging automata
minimization to mitigate state-space explosion, or build the DFA incrementally
during game solving to avoid full DFA construction. However, neither is
dominant. In this paper, we introduce a compositional on-the-fly synthesis
framework that integrates the strengths of both approaches, focusing on large
conjunctions of smaller LTLf formulas common in practice. This framework
applies composition during game solving instead of automata (game arena)
construction. While composing all intermediate results may be necessary in the
worst case, pruning these results simplifies subsequent compositions and
enables early detection of unrealizability. Specifically, the framework allows
two composition variants: pruning before composition to take full advantage of
minimization or pruning during composition to guide on-the-fly synthesis.
Compared to state-of-the-art synthesis solvers, our framework is able to solve
a notable number of instances that other solvers cannot handle. A detailed
analysis shows that both composition variants have unique merits.

</details>


### [148] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: AgREE, a zero-training agent-based framework, outperforms existing KGC methods by 13.7% for emerging entities via iterative retrieval and multi-step reasoning, with a new benchmark proposed.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of capturing comprehensive and up-to-date information about emerging entities in open-domain KGC, where existing methods fail due to reliance on static data or limited retrieval.

Method: The paper introduces AgREE, an agent-based framework combining iterative retrieval and multi-step reasoning to dynamically construct knowledge graph triplets.

Result: AgREE outperforms existing methods by up to 13.7%, especially for emerging entities, without requiring training data. A new evaluation methodology and benchmark are also proposed.

Conclusion: AgREE demonstrates the effectiveness of combining agent-based reasoning with strategic information retrieval for maintaining up-to-date knowledge graphs in dynamic environments.

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [149] [Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork](https://arxiv.org/abs/2508.04163)
*Hasra Dodampegama,Mohan Sridharan*

Main category: cs.AI

TL;DR: 结合知识驱动与数据驱动方法，提出新架构提升临时团队协作中AI代理的决策效率，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有临时团队协作方法依赖大量标注数据，缺乏透明性且难以快速更新知识，随着代理数量增加，决策复杂性导致协作效率下降。

Method: 通过非单调逻辑推理结合先验常识知识、快速学习与更新的行为预测模型，以及基于基础模型的抽象未来目标预测，构建了一个新的临时团队协作架构。

Result: 在VirtualHome仿真环境中，所提架构有效提升了临时团队协作中AI代理的决策能力。

Conclusion: 本文提出了一种结合知识驱动和数据驱动方法的架构，用于提升临时团队协作中AI代理的决策能力，实验验证了该架构在VirtualHome环境中的有效性。

Abstract: AI agents deployed in assistive roles often have to collaborate with other
agents (humans, AI systems) without prior coordination. Methods considered
state of the art for such ad hoc teamwork often pursue a data-driven approach
that needs a large labeled dataset of prior observations, lacks transparency,
and makes it difficult to rapidly revise existing knowledge in response to
changes. As the number of agents increases, the complexity of decision-making
makes it difficult to collaborate effectively. This paper advocates leveraging
the complementary strengths of knowledge-based and data-driven methods for
reasoning and learning for ad hoc teamwork. For any given goal, our
architecture enables each ad hoc agent to determine its actions through
non-monotonic logical reasoning with: (a) prior commonsense domain-specific
knowledge; (b) models learned and revised rapidly to predict the behavior of
other agents; and (c) anticipated abstract future goals based on generic
knowledge of similar situations in an existing foundation model. We
experimentally evaluate our architecture's capabilities in VirtualHome, a
realistic physics-based 3D simulation environment.

</details>


### [150] [Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities](https://arxiv.org/abs/2508.04235)
*Jiaying Zhu,Ziyang Zheng,Zhengyuan Shi,Yalun Cai,Qiang Xu*

Main category: cs.AI

TL;DR: CASCAD利用GNNs计算电路级条件概率，动态优化CDCL启发式策略，显著提升SAT求解效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法将电路转换为CNF并依赖通用SAT求解器，丢弃了丰富的结构和功能信息，导致性能不佳。

Method: 引入CASCAD，一种基于图神经网络（GNNs）计算电路级条件概率的新型电路感知SAT求解框架，动态指导CDCL启发式策略。

Result: 在逻辑等价检查基准测试中，CASCAD比基于CNF的最先进方法减少求解时间高达10倍，并通过概率引导的子句过滤策略额外减少23.5%的运行时间。

Conclusion: CASCAD框架通过保留电路级结构信息，显著提升了SAT求解效率，为未来SAT求解器和EDA工具设计提供了坚实基础。

Abstract: Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design
Automation. The standard workflow for solving CSAT problems converts circuits
into Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by
Conflict-Driven Clause Learning (CDCL). However, this process inherently
discards rich structural and functional information, leading to suboptimal
solver performance. To address this limitation, we introduce CASCAD, a novel
circuit-aware SAT solving framework that directly leverages circuit-level
conditional probabilities computed via Graph Neural Networks (GNNs). By
explicitly modeling gate-level conditional probabilities, CASCAD dynamically
guides two critical CDCL heuristics -- variable phase selection and clause
managementto significantly enhance solver efficiency. Extensive evaluations on
challenging real-world Logical Equivalence Checking (LEC) benchmarks
demonstrate that CASCAD reduces solving times by up to 10x compared to
state-of-the-art CNF-based approaches, achieving an additional 23.5% runtime
reduction via our probability-guided clause filtering strategy. Our results
underscore the importance of preserving circuit-level structural insights
within SAT solvers, providing a robust foundation for future improvements in
SAT-solving efficiency and EDA tool design.

</details>


### [151] [Large Language Model's Multi-Capability Alignment in Biomedical Domain](https://arxiv.org/abs/2508.04278)
*Wentao Wu,Linqing Chen,Hanmeng Zhong,Weilei Wang*

Main category: cs.AI

TL;DR: BalancedBio 是一个高效的生物医学 AI 框架，通过理论创新和实际验证，显著提升了性能、安全性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学领域中多能力集成的问题，确保 AI 在部署时的安全性和可靠性，同时提升性能和成本效益。

Method: 框架包括 Medical Knowledge Grounded Synthetic Generation (MKGSG) 和 Capability Aware Group Relative Policy Optimization，结合临床工作流约束和医学本体验证，确保事实准确性和安全性。数学分析证明了 Pareto-optimal 收敛。

Result: 在多个指标上达到最先进水平：领域专业知识（80.95% BIOMED-MMLU，提升 15.32%）、推理（61.94%，提升 7.75%）、指令遵循（67.95%，提升 6.44%）和集成（86.7%，提升 18.5%）。实际部署中实现了 78% 的成本降低、23% 的诊断准确性提升和 89% 的临床医生接受度。

Conclusion: BalancedBio 提供了一个理论基础的框架，用于参数高效的生物医学推理，实现了多能力集成，并确保了安全部署。通过理论证明和实际应用，展示了其在性能、安全和成本效益方面的显著优势。

Abstract: BalancedBio is a theoretically grounded framework for parameter-efficient
biomedical reasoning, addressing multi-capability integration in
domain-specific AI alignment. It establishes the Biomedical Multi-Capability
Convergence Theorem, proving orthogonal gradient spaces are essential to
prevent capability interference for safe deployment. Key innovations include:
(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending
Source2Synth with clinical workflow constraints and medical ontology validation
for factual accuracy and safety; and (2) Capability Aware Group Relative Policy
Optimization, deriving optimal hybrid reward weighting to maintain
orthogonality in RL, using a reward model with rule-based and model-based
scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal
convergence, preserving performance across capabilities. It achieves
state-of-the-art results in its parameter class: domain expertise (80.95%
BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction
following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety
guarantees include bounds on capability preservation and clinical accuracy.
Real-world deployment yields 78% cost reduction, 23% improved diagnostic
accuracy, and 89% clinician acceptance. This work provides a principled
methodology for biomedical AI alignment, enabling efficient reasoning with
essential safety and reliability, with the 0.5B model version to be released.

</details>


### [152] [Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling](https://arxiv.org/abs/2508.04282)
*Yongyi Wang,Lingfeng Li,Bozhou Chen,Ang Li,Hanyu Liu,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 本文提出了一个理论框架和方法论，用于构建可控难度的POMDP环境，以评估记忆增强强化学习算法，并提供了实证支持。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏对记忆模型挑战程度的可控性，而合成环境能精细操控动态特性，因此需要更详细和严格的评估方法。

Method: 提出了一个理论框架（基于MDS、转移不变性等概念），并利用线性过程动力学、状态聚合和奖励重新分配的方法构建了具有预定属性的定制POMDP。

Result: 开发了一系列基于理论洞察的POMDP环境，难度逐步增加，并进行了实证验证。

Conclusion: 本文阐明了记忆增强强化学习在解决POMDP问题中的挑战，并提供了分析和设计POMDP环境的指南，同时为RL任务中选择记忆模型提供了实证支持。

Abstract: Recent research has developed benchmarks for memory-augmented reinforcement
learning (RL) algorithms, providing Partially Observable Markov Decision
Process (POMDP) environments where agents depend on past observations to make
decisions. While many benchmarks incorporate sufficiently complex real-world
problems, they lack controllability over the degree of challenges posed to
memory models. In contrast, synthetic environments enable fine-grained
manipulation of dynamics, making them critical for detailed and rigorous
evaluation of memory-augmented RL. Our study focuses on POMDP synthesis with
three key contributions:
  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand
Structure (MDS), transition invariance, and related concepts; 2. A methodology
leveraging linear process dynamics, state aggregation, and reward
redistribution to construct customized POMDPs with predefined properties; 3.
Empirically validated series of POMDP environments with increasing difficulty
levels, designed based on our theoretical insights. Our work clarifies the
challenges of memory-augmented RL in solving POMDPs, provides guidelines for
analyzing and designing POMDP environments, and offers empirical support for
selecting memory models in RL tasks.

</details>


### [153] [Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models](https://arxiv.org/abs/2508.04339)
*Anran Xu,Jincheng Wang,Baigen Cai,Tao Wen*

Main category: cs.AI

TL;DR: DRN通过不确定性最小化改进逻辑推理，显著提升模型性能并展示零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在逻辑推理中因语义启发式与决定性证据冲突而失败的认知陷阱问题。

Method: 提出了Deliberative Reasoning Network (DRN)，通过将逻辑推理从概率最大化重新定义为不确定性最小化，并采用迭代证据合成过程来量化竞争假设的认知不确定性。

Result: 在LCR-1000基准测试中，DRN比标准基线提高了15.2%；与Mistral-7B集成后，在最具挑战性的问题上准确率从20%提升至80%。此外，DRN在TruthfulQA上零样本性能提升了23.6%。

Conclusion: DRN作为一种基础、可验证的系统2推理组件，为构建更可信的AI系统提供了可能。

Abstract: Large language models often fail at logical reasoning when semantic
heuristics conflict with decisive evidence - a phenomenon we term cognitive
traps. To address this fundamental limitation, we introduce the Deliberative
Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from
probability maximization to uncertainty minimization. Instead of asking "Which
answer is most likely?", DRN asks "Which hypothesis has the most internally
consistent evidence?". DRN achieves intrinsic interpretability by explicitly
tracking belief states and quantifying epistemic uncertainty for competing
hypotheses through an iterative evidence synthesis process. We validate our
approach through two complementary architectures - a bespoke discriminative
model that embodies the core uncertainty minimization principle, and a
lightweight verification module that enhances existing generative LLMs.
Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to
expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over
standard baselines. When integrated as a parameter-efficient verifier with
Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most
challenging problems. Critically, DRN demonstrates strong zero-shot
generalization, improving TruthfulQA performance by 23.6% without additional
training, indicating that uncertainty-driven deliberation learns transferable
reasoning principles. We position DRN as a foundational, verifiable System 2
reasoning component for building more trustworthy AI systems.

</details>


### [154] [OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing](https://arxiv.org/abs/2508.04361)
*Fuqing Bie,Shiyu Huang,Xijia Tao,Zhiqin Fang,Leyi Pan,Junzhe Chen,Min Ren,Liuyu Xiang,Zhaofeng He*

Main category: cs.AI

TL;DR: OmniPlay是一个多模态基准测试，揭示了当前模型在推理和规划上的脆弱性，提出了‘少即是多’的悖论，并呼吁研究聚焦于协同融合。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法测试通用基础模型（如Gemini和GPT-4o）在动态交互世界中的智能，静态基准缺乏代理性，而交互基准则存在模态瓶颈。

Method: 研究者提出了OmniPlay，一个包含五个游戏环境的诊断基准，这些环境系统性地创造了协同和冲突场景，迫使模型进行真正的跨模态推理。

Result: 对六个领先的多模态模型的全面评估揭示了关键的两极分化：它们在高保真记忆任务上表现超人类，但在需要稳健推理和策略规划的挑战中表现脆弱。

Conclusion: 研究发现，当前的多模态模型在记忆任务上表现超人类，但在需要强推理和策略规划的挑战中存在系统性缺陷。这表明，实现稳健的AGI需要超越单纯规模扩展，专注于协同融合机制的研究。

Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate
impressive multi-modal competence, existing evaluations fail to test their
intelligence in dynamic, interactive worlds. Static benchmarks lack agency,
while interactive benchmarks suffer from a severe modal bottleneck, typically
ignoring crucial auditory and temporal cues. To bridge this evaluation chasm,
we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,
but to probe the fusion and reasoning capabilities of agentic models across the
full sensory spectrum. Built on a core philosophy of modality interdependence,
OmniPlay comprises a suite of five game environments that systematically create
scenarios of both synergy and conflict, forcing agents to perform genuine
cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal
models reveals a critical dichotomy: they exhibit superhuman performance on
high-fidelity memory tasks but suffer from systemic failures in challenges
requiring robust reasoning and strategic planning. We demonstrate that this
fragility stems from brittle fusion mechanisms, which lead to catastrophic
performance degradation under modality conflict and uncover a counter-intuitive
"less is more" paradox, where removing sensory information can paradoxically
improve performance. Our findings suggest that the path toward robust AGI
requires a research focus beyond scaling to explicitly address synergistic
fusion. Our platform is available for anonymous review at
https://github.com/fuqingbie/omni-game-benchmark.

</details>


### [155] [Artificial Consciousness as Interface Representation](https://arxiv.org/abs/2508.04383)
*Robert Prentner*

Main category: cs.AI

TL;DR: 论文提出SLP测试框架，通过范畴论建模接口表示，将AI意识问题转化为实证检验标准。


<details>
  <summary>Details</summary>
Motivation: 由于定义和操作主观体验存在固有挑战，论文旨在为人工智能是否具备意识提供一个可实证检验的框架。

Method: 论文引入了SLP测试（主观语言、潜在涌现、现象结构），并利用范畴论对接口表示进行建模，将其视为关系基质与可观察行为之间的映射。

Result: SLP测试将主观体验操作化为功能接口而非物理系统的固有属性，为AI意识研究提供了新方法。

Conclusion: 该论文提出了一个框架，将人工智能意识问题转化为可实证检验的标准，通过SLP测试评估AI系统是否具备类似意识的属性。

Abstract: Whether artificial intelligence (AI) systems can possess consciousness is a
contentious question because of the inherent challenges of defining and
operationalizing subjective experience. This paper proposes a framework to
reframe the question of artificial consciousness into empirically tractable
tests. We introduce three evaluative criteria - S (subjective-linguistic), L
(latent-emergent), and P (phenomenological-structural) - collectively termed
SLP-tests, which assess whether an AI system instantiates interface
representations that facilitate consciousness-like properties. Drawing on
category theory, we model interface representations as mappings between
relational substrates (RS) and observable behaviors, akin to specific types of
abstraction layers. The SLP-tests collectively operationalize subjective
experience not as an intrinsic property of physical systems but as a functional
interface to a relational entity.

</details>


### [156] [GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning](https://arxiv.org/abs/2508.04389)
*Weitai Kang,Bin Lei,Gaowen Liu,Caiwen Ding,Yan Yan*

Main category: cs.AI

TL;DR: GuirlVG提出基于强化学习的GUI-VG方法，通过优化RFT组件和稳定技术，显著减少数据需求并超越SFT性能。


<details>
  <summary>Details</summary>
Motivation: 传统的监督微调（SFT）方法需要大量数据和训练成本，而随着MLLMs的进步，其必要性受到质疑。基于规则强化微调（RFT）的成功表明更高效的替代方案存在，但如何最优应用于GUI-VG尚待探索。

Method: GuirlVG采用基于强化学习的微调方法（RFT），结合对抗KL因子动态稳定训练，并探索了RFT的最佳训练配置。

Result: GuirlVG仅需5.2K训练样本，性能优于使用10M+样本的SFT方法，在多个基准测试中提升显著（ScreenSpot提升7.7%，ScreenSpotPro提升17.2%，ScreenSpotV2准确率达91.9%）。

Conclusion: GuirlVG通过系统实证研究和新颖的稳定技术，成功证明了基于强化学习的微调方法在GUI-VG任务中的高效性，显著优于传统监督微调方法。

Abstract: Graphical user interface visual grounding (GUI-VG), a core capability for GUI
agents, has primarily relied on supervised fine-tuning (SFT) of multimodal
large language models (MLLMs), which demands extensive data curation and
significant training costs. However, as MLLMs continue to advance and even
cover GUI domains during pretraining, the necessity of exhaustive SFT
post-training becomes increasingly questionable. Meanwhile, recent successes of
rule-based reinforcement fine-tuning (RFT) suggest a more efficient
alternative. Despite this promise, the optimal manner of applying RFT for
GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a
reinforcement learning-based GUI-VG method built on a systematic empirical
study and a novel stabilization technique. We find that naive application of
RFT underperforms the SFT baseline, motivating a deeper exploration. First, we
decompose RFT into its core components and analyze the optimal formulation of
each. Second, we propose a novel Adversarial KL Factor that dynamically
stabilizes training to mitigate reward over-optimization. Third, we further
explore the training configurations of RFT to enhance effectiveness. Extensive
experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT
methods trained on over 10M samples, achieving a 7.7% improvement on
ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on
ScreenSpotV2.

</details>


### [157] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: D2Snap, a DOM downsampling algorithm, matches GUI snapshot performance (67% vs. 65%) and outperforms it by 8% with higher token usage, proving DOM hierarchy's value for LLMs.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of application state serialization (snapshots) for web agents, highlighting the limitations of current GUI snapshots and the potential of DOM snapshots despite token size constraints.

Method: Proposes D2Snap, a DOM downsampling algorithm evaluated using a GPT-4o backend on tasks from the Online-Mind2Web dataset.

Result: D2Snap achieves a 67% success rate, matching GUI snapshots (65%) within the same token order (1e3), and outperforms them by 8% with slightly higher token usage.

Conclusion: DOM-inherent hierarchy is a strong UI feature for LLMs, and D2Snap demonstrates competitive performance against GUI snapshots within similar token constraints.

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [158] [\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices](https://arxiv.org/abs/2508.04428)
*Si Chen,Izzy Molnar,Ting Hua,Peiyu Li,Le Huy Khiem,G. Alex Ambrose,Jim Lang,Ronald Metoyer,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: SimInstruct通过LLMs模拟新手与专家互动，生成高质量教学对话，用于微调LLaMA模型，其表现优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 高质量的多轮教学对话数据稀缺，主要由于隐私问题和求助的脆弱性，SimInstruct旨在解决这一问题。

Method: 使用SimInstruct工具，通过LLMs模拟新手导师的不同教学挑战和人格特质，人类专家提供多轮反馈和教学支持。

Result: SimInstruct生成的对话在教学相关性和认知深度上与真实指导录音相当，专家反馈积极，且微调后的LLaMA模型在教学质量上优于GPT-4o。

Conclusion: SimInstruct工具通过模拟新手导师与人类专家互动，成功生成了高质量的教学对话数据，并用于微调LLaMA模型，其在教学质量上优于GPT-4o。

Abstract: High-quality, multi-turn instructional dialogues between novices and experts
are essential for developing AI systems that support teaching, learning, and
decision-making. These dialogues often involve scaffolding -- the process by
which an expert supports a novice's thinking through questions, feedback, and
step-by-step guidance. However, such data are scarce due to privacy concerns in
recording and the vulnerability inherent in help-seeking. We present
SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding
dialogues. Using teaching development coaching as an example domain,
SimInstruct simulates novice instructors via LLMs, varying their teaching
challenges and LLM's persona traits, while human experts provide multi-turn
feedback, reasoning, and instructional support. This design enables the
creation of realistic, pedagogically rich dialogues without requiring real
novice participants. Our results reveal that persona traits, such as
extroversion and introversion, meaningfully influence how experts engage.
Compared to real mentoring recordings, SimInstruct dialogues demonstrate
comparable pedagogical relevance and cognitive depth. Experts also reported the
process as engaging and reflective, improving both data quality and their own
professional insight. We further fine-tuned a LLaMA model to be an expert model
using the augmented dataset, which outperformed GPT-4o in instructional
quality. Our analysis highlights GPT-4o's limitations in weak reflective
questioning, overuse of generic praise, a condescending tone, and a tendency to
overwhelm novices with excessive suggestions.

</details>


### [159] [From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control](https://arxiv.org/abs/2508.04460)
*Rui Ha,Chaozhuo Li,Rui Pu,Sen Su*

Main category: cs.AI

TL;DR: MERA框架通过解耦推理和控制组件，优化大型推理模型的推理过程，提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在推理过程中缺乏内在调节机制，导致过度推理和计算资源浪费，限制了其实际应用。

Method: MERA框架采用接管式数据构建机制、结构化推理-控制分离和Control-Segment Policy Optimization（CSPO）方法，优化控制策略学习。

Result: 实验表明，采用MERA框架训练的模型在多个推理基准测试中提升了推理效率和准确性。

Conclusion: MERA框架通过解耦推理和控制组件，显著提升了大型推理模型（LRMs）的效率和准确性，为实际应用提供了可行的解决方案。

Abstract: Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex
reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step
reasoning, reflection, and backtracking, commonly referred to as "Aha Moments".
However, such emergent behaviors remain unregulated and uncontrolled, often
resulting in overthinking, where the model continues generating redundant
reasoning content even after reaching reliable conclusions. This leads to
excessive computational costs and increased latency, limiting the practical
deployment of LRMs. The root cause lies in the absence of intrinsic regulatory
mechanisms, as current models are unable to monitor and adaptively manage their
reasoning process to determine when to continue, backtrack, or terminate. To
address this issue, we propose the Meta-cognitive Reasoning Framework (MERA),
which explicitly decouples the thinking process into distinct reasoning and
control components, thereby enabling the independent optimization of control
strategies. Specifically, MERA incorporates a takeover-based data construction
mechanism that identifies critical decision points during reasoning and
delegates the creation of control signals to auxiliary LLMs, thereby enabling
the construction of high-quality reasoning-control data. Additionally, a
structured reasoning-control separation is implemented via supervised
fine-tuning, enabling the model to generate explicit traces and acquire initial
meta-cognitive control capabilities. Finally, MERA employs Control-Segment
Policy Optimization (CSPO), which combines segment-wise Group Relative Policy
Optimization (GRPO) with a control-masking mechanism to optimize control
behavior learning while minimizing interference from irrelevant content.
Experiments on various reasoning benchmarks demonstrate that models trained
with MERA enhance both reasoning efficiency and accuracy.

</details>


### [160] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: 本文全面调查了基于(M)LLM的OS Agents，涵盖其基础、构建方法、评估及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 实现像钢铁侠中的J.A.R.V.I.S一样多功能的人工智能助手。

Method: 详细回顾了构建OS Agents的方法论，重点关注领域特定的基础模型和代理框架。

Result: 总结了OS Agents的关键组件、能力、构建方法、评估协议和未来研究方向。

Conclusion: 本次调查旨在整合OS Agents的研究现状，为学术探索和工业发展提供指导。

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [161] [Argumentative Debates for Transparent Bias Detection [Technical Report]](https://arxiv.org/abs/2508.04511)
*Hamed Ayoobi,Nico Potyka,Anna Rapberger,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一种基于辩论的可解释偏见检测方法，通过正式和计算论证技术检测和缓解算法偏见，评估显示其在性能和透明度上均优于基线。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在社会中的广泛应用，数据或模型中的潜在偏见可能导致对特定群体的系统性不利。尽管已有多种公平性概念和算法方法被提出，但大多忽视了透明性。鉴于公平性的人为导向性质，可解释性和可说明性成为算法公平性的核心需求。

Method: 本文提出了一种基于辩论的偏见检测方法，利用受保护特征值在个体及其邻域内的比较，通过正式和计算论证技术构建辩论过程。

Result: 通过正式、定量和定性评估，本文的方法在性能上优于基线模型，同时在可解释性和可说明性方面表现出色。

Conclusion: 本文提出了一种新颖的可解释、可说明的偏见检测方法，该方法基于受保护特征值的辩论，并通过正式和计算论证技术进行构建。该方法在性能、可解释性和可说明性方面均表现出色，为算法公平性提供了透明且有效的解决方案。

Abstract: As the use of AI systems in society grows, addressing potential biases that
emerge from data or are learned by models is essential to prevent systematic
disadvantages against specific groups. Several notions of (un)fairness have
been proposed in the literature, alongside corresponding algorithmic methods
for detecting and mitigating unfairness, but, with very few exceptions, these
tend to ignore transparency. Instead, interpretability and explainability are
core requirements for algorithmic fairness, even more so than for other
algorithmic solutions, given the human-oriented nature of fairness. In this
paper, we contribute a novel interpretable, explainable method for bias
detection relying on debates about the presence of bias against individuals,
based on the values of protected features for the individuals and others in
their neighbourhoods. Our method builds upon techniques from formal and
computational argumentation, whereby debates result from arguing about biases
within and across neighbourhoods. We provide formal, quantitative, and
qualitative evaluations of our method, highlighting its strengths in
performance against baselines, as well as its interpretability and
explainability.

</details>


### [162] [SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset](https://arxiv.org/abs/2508.04563)
*Mei Jiang,Houping Yue,Bingdong Li,Hao Hao,Ying Qian,Bo Jiang,Aimin Zhou*

Main category: cs.AI

TL;DR: SID 是首个系统评估 LLMs 在多轮跨学科苏格拉底对话中高阶引导能力的基准，揭示了当前模型的局限性，并提供了推动更具教育意识的 LLMs 发展的工具。


<details>
  <summary>Details</summary>
Motivation: 现代教育的核心目标是培养学生解决复杂问题时的知识整合和迁移能力，跨学科 STEM 是实现这一目标的关键途径，但需要难以规模化的专家指导。LLMs 在这方面有潜力，但其真实指导能力因缺乏有效评估基准而不明确。

Method: 引入 SID 基准，包括一个大规模数据集（10,000 个对话轮次，涵盖 48 个复杂 STEM 项目）、新的注释模式（捕捉深层教学特征）和一套新的评估指标（如 X-SRG）。

Result: 基线实验证实，即使是最先进的 LLMs 也难以执行有效的引导对话来帮助学生实现知识整合和迁移。

Conclusion: SID 基准测试揭示了当前最先进的 LLMs 在执行有效的引导对话以促进学生知识整合和迁移方面的局限性，强调了该基准在推动更具教育意识的 LLMs 发展中的关键价值。

Abstract: Fostering students' abilities for knowledge integration and transfer in
complex problem-solving scenarios is a core objective of modern education, and
interdisciplinary STEM is a key pathway to achieve this, yet it requires expert
guidance that is difficult to scale. While LLMs offer potential in this regard,
their true capability for guided instruction remains unclear due to the lack of
an effective evaluation benchmark. To address this, we introduce SID, the first
benchmark designed to systematically evaluate the higher-order guidance
capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our
contributions include a large-scale dataset of 10,000 dialogue turns across 48
complex STEM projects, a novel annotation schema for capturing deep pedagogical
features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline
experiments confirm that even state-of-the-art LLMs struggle to execute
effective guided dialogues that lead students to achieve knowledge integration
and transfer. This highlights the critical value of our benchmark in driving
the development of more pedagogically-aware LLMs.

</details>


### [163] [ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges](https://arxiv.org/abs/2508.04576)
*Yue Zhou,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: 提出首个评估MPJ步骤级置信度可靠性的基准ConfProBench，通过对抗性扰动和新指标揭示当前模型的局限性，为未来研究提供基线。


<details>
  <summary>Details</summary>
Motivation: 现有MPJ评估基准忽略了步骤级置信度可靠性的关键问题，需填补这一空白以指导未来改进。

Method: 提出ConfProBench基准，通过三种对抗性扰动（同义词替换、句法变换和图像扰动）测试MPJ置信度的鲁棒性，并引入三个新评估指标（CRS、CSS、CCS）。

Result: 实验评估了14种先进MLLM，揭示了当前MPJ在置信度性能上的不足，并提供了支持未来研究的基线。

Conclusion: 当前的多模态大型语言模型（MLLM）基于过程判断（MPJ）在步骤级置信度可靠性方面存在局限性，ConfProBench为未来研究提供了评估基准和竞争基线。

Abstract: Reasoning is a critical capability of multimodal large language models
(MLLMs) for solving complex multimodal tasks, and judging the correctness of
reasoning steps is crucial for improving this capability. Recently, MLLM-based
process judges (MPJs) have been widely used to assess the correctness of
reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important
for identifying their limitations and guiding future improvements. However,
existing benchmarks for MPJs mainly focus on tasks such as step correctness
classification and reasoning process search, while overlooking a key aspect:
whether the confidence scores produced by MPJs at the step level are reliable.
To address this gap, we propose ConfProBench, the first comprehensive benchmark
designed to systematically evaluate the reliability of step-level confidence
scores generated by MPJs. Our benchmark constructs three types of adversarially
perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and
Image Perturbation, to test the robustness of MPJ confidence under
perturbations. In addition, we introduce three novel evaluation metrics:
Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and
Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and
calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including
both proprietary and open-source models. Experiments reveal limitations in
current MPJs' confidence performance and offer competitive baselines to support
future research.

</details>


### [164] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: SEAgent是一个自演进框架，通过课程生成和策略优化使计算机使用代理能自主适应陌生软件，成功率提升23.2%。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型（LVLM）在缺乏人工标注的陌生或专业软件中表现不佳，需一种自主进化框架以提升适应性。

Method: 提出了SEAgent框架，包括世界状态模型用于轨迹评估、课程生成器设计任务难度梯度，并通过对抗模仿失败动作和GRPO优化成功动作来更新策略。采用专家到通用训练策略整合个体经验。

Result: 在OS-World的五个新软件环境中，SEAgent的成功率从11.3%提升至34.5%，显著优于UI-TARS。

Conclusion: SEAgent框架通过自演进机制显著提升了计算机使用代理（CUA）在陌生软件环境中的表现，其成功率和自主进化能力均优于现有开源代理。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [165] [RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for Gaussian Splatting](https://arxiv.org/abs/2508.04078)
*Zhan Li,Huangying Zhan,Changyang Li,Qingan Yan,Yi Xu*

Main category: cs.GR

TL;DR: RLGS是一个强化学习框架，自动优化3DGS的超参数，提升渲染质量且无需修改现有系统。


<details>
  <summary>Details</summary>
Motivation: 3DGS中的超参数调整是一个劳动密集且依赖专家经验的过程，常导致重建不一致和结果次优，因此需要自动化解决方案。

Method: 提出了RLGS，一个即插即用的强化学习框架，通过轻量级策略模块动态调整关键超参数（如学习率和密集化阈值），无需修改现有3DGS流水线架构。

Result: RLGS在多个先进的3DGS变体（如Taming-3DGS和3DGS-MCMC）中展现出泛化能力，并在TNT数据集上将Taming-3DGS的PSNR提高了0.7dB。

Conclusion: RLGS提供了一个有效且通用的解决方案，用于自动化3D高斯溅射（3DGS）训练中的超参数调整，填补了强化学习在3DGS应用中的空白。

Abstract: Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive
and expert-driven process, often resulting in inconsistent reconstructions and
suboptimal results. We propose RLGS, a plug-and-play reinforcement learning
framework for adaptive hyperparameter tuning in 3DGS through lightweight policy
modules, dynamically adjusting critical hyperparameters such as learning rates
and densification thresholds. The framework is model-agnostic and seamlessly
integrates into existing 3DGS pipelines without architectural modifications. We
demonstrate its generalization ability across multiple state-of-the-art 3DGS
variants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness
across diverse datasets. RLGS consistently enhances rendering quality. For
example, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT)
dataset, under a fixed Gaussian budget, and continues to yield gains even when
baseline performance saturates. Our results suggest that RLGS provides an
effective and general solution for automating hyperparameter tuning in 3DGS
training, bridging a gap in applying reinforcement learning to 3DGS.

</details>


### [166] [Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research](https://arxiv.org/abs/2508.04326)
*Ke Li,Mana Masuda,Susanne Schmidt,Shohei Mori*

Main category: cs.GR

TL;DR: 本文系统调查了365篇RF文献，分析了RF在XR领域的应用现状及研究空白，为XR社区提供了研究导航。


<details>
  <summary>Details</summary>
Motivation: 尽管RF研究呈指数增长，但XR社区的RF相关贡献仍然稀少，本研究旨在填补这一研究空白。

Method: 我们收集了365篇与XR相关的RF文献，并详细分析了其中66篇论文，以回答研究问题。

Result: 调查揭示了RF在XR应用中的愿景、实现方式及剩余研究空白。

Conclusion: 通过本次系统调查，我们扩展并定位了XR特定的RF研究主题，为XR社区在RF研究的快速发展中提供了导航资源。

Abstract: The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS)
and Neural Radiance Fields (NeRF), has revolutionized interactive
photorealistic view synthesis and presents enormous opportunities for XR
research and applications. However, despite the exponential growth of RF
research, RF-related contributions to the XR community remain sparse. To better
understand this research gap, we performed a systematic survey of current RF
literature to analyze (i) how RF is envisioned for XR applications, (ii) how
they have already been implemented, and (iii) the remaining research gaps. We
collected 365 RF contributions related to XR from computer vision, computer
graphics, robotics, multimedia, human-computer interaction, and XR communities,
seeking to answer the above research questions. Among the 365 papers, we
performed an analysis of 66 papers that already addressed a detailed aspect of
RF research for XR. With this survey, we extended and positioned XR-specific RF
research topics in the broader RF research field and provide a helpful resource
for the XR community to navigate within the rapid development of RF research.

</details>


### [167] [Surf3R: Rapid Surface Reconstruction from Sparse RGB Views in Seconds](https://arxiv.org/abs/2508.04508)
*Haodong Zhu,Changbai Li,Yangyang Ren,Zichao Feng,Xuhui Liu,Hanlin Chen,Xiantong Zhen,Baochang Zhang*

Main category: cs.GR

TL;DR: Surf3R是一种无需相机校准的端到端3D重建方法，通过多分支架构和D-Normal正则化器，在10秒内完成高质量3D表面重建。


<details>
  <summary>Details</summary>
Motivation: 当前的多视角3D重建方法依赖精确的相机校准和姿态估计，需要复杂且耗时的预处理，限制了实际应用。

Method: Surf3R采用了一种端到端的前馈方法，通过多分支和多视角解码架构，结合分支处理、跨视角注意力和分支间融合技术，有效捕捉互补几何线索。此外，引入了基于3D高斯表示的D-Normal正则化器，联合优化3D几何形状。

Result: 实验结果表明，Surf3R能够在无需相机校准的情况下，在10秒内完成整个场景的3D表面重建。

Conclusion: Surf3R在ScanNet++和Replica数据集上实现了最先进的性能，展示了出色的泛化能力和效率。

Abstract: Current multi-view 3D reconstruction methods rely on accurate camera
calibration and pose estimation, requiring complex and time-intensive
pre-processing that hinders their practical deployment. To address this
challenge, we introduce Surf3R, an end-to-end feedforward approach that
reconstructs 3D surfaces from sparse views without estimating camera poses and
completes an entire scene in under 10 seconds. Our method employs a
multi-branch and multi-view decoding architecture in which multiple reference
views jointly guide the reconstruction process. Through the proposed
branch-wise processing, cross-view attention, and inter-branch fusion, the
model effectively captures complementary geometric cues without requiring
camera calibration. Moreover, we introduce a D-Normal regularizer based on an
explicit 3D Gaussian representation for surface reconstruction. It couples
surface normals with other geometric parameters to jointly optimize the 3D
geometry, significantly improving 3D consistency and surface detail accuracy.
Experimental results demonstrate that Surf3R achieves state-of-the-art
performance on multiple surface reconstruction metrics on ScanNet++ and Replica
datasets, exhibiting excellent generalization and efficiency.

</details>


### [168] [MienCap: Realtime Performance-Based Facial Animation with Live Mood Dynamics](https://arxiv.org/abs/2508.04687)
*Ye Pan,Ruisi Zhang,Jingying Wang,Nengfu Chen,Yilin Qiu,Yu Ding,Kenny Mitchell*

Main category: cs.GR

TL;DR: 结合传统动画技术与机器学习模型，提出非实时和实时系统以提升3D角色表情动画的真实性和效率，效果优于Faceware。


<details>
  <summary>Details</summary>
Motivation: 提升基于性能的动画表现，驱动具有真实感知的3D风格化角色。

Method: 结合传统的混合形状动画技术和多种机器学习模型，提出了非实时和实时的解决方案。非实时系统使用3D情感转移网络从2D人类图像生成风格化的3D角色参数；实时系统则通过混合形状适应网络生成几何一致且时间稳定的角色参数运动。

Result: 与商业产品Faceware相比，我们的系统在表达识别度、强度和吸引力方面的评分显著更高。

Conclusion: 我们的系统可以有效地提升基于性能的动画表现，为动画师提供更快速、准确的工具来创建所需的表情。

Abstract: Our purpose is to improve performance-based animation which can drive
believable 3D stylized characters that are truly perceptual. By combining
traditional blendshape animation techniques with multiple machine learning
models, we present both non-real time and real time solutions which drive
character expressions in a geometrically consistent and perceptually valid way.
For the non-real time system, we propose a 3D emotion transfer network makes
use of a 2D human image to generate a stylized 3D rig parameters. For the real
time system, we propose a blendshape adaption network which generates the
character rig parameter motions with geometric consistency and temporally
stability. We demonstrate the effectiveness of our system by comparing to a
commercial product Faceware. Results reveal that ratings of the recognition,
intensity, and attractiveness of expressions depicted for animated characters
via our systems are statistically higher than Faceware. Our results may be
implemented into the animation pipeline, and provide animators with a system
for creating the expressions they wish to use more quickly and accurately.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [169] [Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes](https://arxiv.org/abs/2508.03890)
*Sanghun Jung,Daehoon Gwak,Byron Boots,James Hays*

Main category: cs.RO

TL;DR: 论文提出一种基于神经过程的地形高程建模方法，结合语义特征和局部注意力机制，显著提升了实时性和准确性，适用于复杂越野环境。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如高斯过程和神经网络）无法满足实时性需求或低估地形突变，而NPs结合了贝叶斯不确定性估计与神经网络的高效性，为解决这一问题提供了新思路。

Method: 本研究采用神经过程（NPs）结合LiDAR和相机传感器的语义特征，提出了一种局部球查询注意力机制，显著降低了计算复杂度。

Result: 在包含不同地形特征的越野数据集上，该方法表现优于基线模型，计算复杂度降低了17%，同时保持了高程精度。

Conclusion: 该论文提出了一种基于神经过程（NPs）的方法，有效解决了地形高程建模中的实时性和准确性难题，特别是在复杂越野环境中。

Abstract: Terrain elevation modeling for off-road navigation aims to accurately
estimate changes in terrain geometry in real-time and quantify the
corresponding uncertainties. Having precise estimations and uncertainties plays
a crucial role in planning and control algorithms to explore safe and reliable
maneuver strategies. However, existing approaches, such as Gaussian Processes
(GPs) and neural network-based methods, often fail to meet these needs. They
are either unable to perform in real-time due to high computational demands,
underestimating sharp geometry changes, or harming elevation accuracy when
learned with uncertainties. Recently, Neural Processes (NPs) have emerged as a
promising approach that integrates the Bayesian uncertainty estimation of GPs
with the efficiency and flexibility of neural networks. Inspired by NPs, we
propose an effective NP-based method that precisely estimates sharp elevation
changes and quantifies the corresponding predictive uncertainty without losing
elevation accuracy. Our method leverages semantic features from LiDAR and
camera sensors to improve interpolation and extrapolation accuracy in
unobserved regions. Also, we introduce a local ball-query attention mechanism
to effectively reduce the computational complexity of global attention by 17\%
while preserving crucial local and spatial information. We evaluate our method
on off-road datasets having interesting geometric features, collected from
trails, deserts, and hills. Our results demonstrate superior performance over
baselines and showcase the potential of neural processes for effective and
expressive terrain modeling in complex off-road environments.

</details>


### [170] [Constraint-Preserving Data Generation for Visuomotor Policy Learning](https://arxiv.org/abs/2508.03944)
*Kevin Lin,Varun Ragunath,Andrew McAlinden,Aaditya Prasad,Jimmy Wu,Yuke Zhu,Jeannette Bohg*

Main category: cs.RO

TL;DR: CP-Gen通过单条专家轨迹生成多样化机器人演示数据，显著提升策略的零样本迁移和泛化能力，实验显示其平均成功率达77%。


<details>
  <summary>Details</summary>
Motivation: 大规模演示数据收集成本高且耗时，限制了机器人操纵技术的发展。CP-Gen旨在通过单条专家轨迹生成多样化数据，解决这一问题。

Method: CP-Gen首先将专家演示分解为自由空间运动和机器人技能，并将技能表示为关键点轨迹约束。通过采样任务相关物体的姿态和几何变换，生成新的演示数据，并优化机器人关节配置以跟踪变换后的关键点轨迹。

Result: 在16个仿真任务和4个真实世界任务中，CP-Gen训练的策略平均成功率达到77%，优于基线方法的50%。

Conclusion: CP-Gen方法通过单条专家轨迹生成多样化的机器人演示数据，显著提升了策略在真实世界中的零样本迁移能力和泛化能力，平均成功率从50%提升至77%。

Abstract: Large-scale demonstration data has powered key breakthroughs in robot
manipulation, but collecting that data remains costly and time-consuming. We
present Constraint-Preserving Data Generation (CP-Gen), a method that uses a
single expert trajectory to generate robot demonstrations containing novel
object geometries and poses. These generated demonstrations are used to train
closed-loop visuomotor policies that transfer zero-shot to the real world and
generalize across variations in object geometries and poses. Similar to prior
work using pose variations for data generation, CP-Gen first decomposes expert
demonstrations into free-space motions and robot skills. But unlike those
works, we achieve geometry-aware data generation by formulating robot skills as
keypoint-trajectory constraints: keypoints on the robot or grasped object must
track a reference trajectory defined relative to a task-relevant object. To
generate a new demonstration, CP-Gen samples pose and geometry transforms for
each task-relevant object, then applies these transforms to the object and its
associated keypoints or keypoint trajectories. We optimize robot joint
configurations so that the keypoints on the robot or grasped object track the
transformed keypoint trajectory, and then motion plan a collision-free path to
the first optimized joint configuration. Experiments on 16 simulation tasks and
four real-world tasks, featuring multi-stage, non-prehensile and
tight-tolerance manipulation, show that policies trained using CP-Gen achieve
an average success rate of 77%, outperforming the best baseline that achieves
an average of 50%.

</details>


### [171] [Optimization of sliding control parameters for a 3-dof robot arm using genetic algorithm (GA)](https://arxiv.org/abs/2508.04009)
*Vu Ngoc Son,Pham Van Cuong,Dao Thi My Linh,Le Tieu Nien*

Main category: cs.RO

TL;DR: 遗传算法优化SMC参数，提升机器人操纵器轨迹跟踪性能并减少抖振。


<details>
  <summary>Details</summary>
Motivation: 滑模控制（SMC）的有效性和鲁棒性依赖于参数选择，但参数选择困难且关键。

Method: 应用遗传算法（GA）优化滑模控制（SMC）参数，以满足机器人操纵器在不确定和受干扰条件下的轨迹跟踪需求。

Result: 仿真结果表明，遗传算法优化的SMC在跟踪能力和减少抖振效应方面优于传统方法。

Conclusion: 遗传算法优化的滑模控制（SMC）参数在机器人操纵器中表现出更好的跟踪能力和减少的抖振效应，相比传统SMC和模糊-SMC。

Abstract: This paper presents a method for optimizing the sliding mode control (SMC)
parameter for a robot manipulator applying a genetic algorithm (GA). The
objective of the SMC is to achieve precise and consistent tracking of the
trajectory of the robot manipulator under uncertain and disturbed conditions.
However, the system effectiveness and robustness depend on the choice of the
SMC parameters, which is a difficult and crucial task. To solve this problem, a
genetic algorithm is used to locate the optimal values of these parameters that
gratify the capability criteria. The proposed method is efficient compared with
the conventional SMC and Fuzzy-SMC. The simulation results show that the
genetic algorithm with SMC can achieve better tracking capability and reduce
the chattering effect.

</details>


### [172] [SCOUT: An in-vivo Methane Sensing System for Real-time Monitoring of Enteric Emissions in Cattle with ex-vivo Validation](https://arxiv.org/abs/2508.04056)
*Yuelin Deng,Hinayah Rojas de Oliveira,Richard M. Voyles,Upinder Kaur*

Main category: cs.RO

TL;DR: SCOUT系统通过闭环气体再循环设计实现了连续高分辨率瘤胃甲烷监测，数据保留率和测量精度显著优于传统方法，为可持续畜牧管理提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 现有环境采样方法存在数据保留率低、环境干扰大和时间分辨率有限的问题，阻碍了通过遗传选择和精准管理提升畜牧可持续性的进展。

Method: 开发了SCOUT系统，采用创新的闭环气体再循环设计，实现了连续高分辨率的瘤胃甲烷浓度监测，并与传统环境嗅探系统进行了跨平台验证。

Result: SCOUT系统表现出色，数据保留率达82%（传统方法仅17%），甲烷浓度测量值比环境方法高100-1000倍，并在生物相关的40分钟窗口内显示出强相关性（r = -0.564 ± 0.007）。高频监测揭示了行为与排放的新关联。

Conclusion: SCOUT系统代表了一项革命性进步，为基因组选择项目和可持续精准畜牧管理提供了准确、连续的排放表型分析。

Abstract: Accurate measurement of enteric methane emissions remains a critical
bottleneck for advancing livestock sustainability through genetic selection and
precision management. Existing ambient sampling approaches suffer from low data
retention rates, environmental interference, and limited temporal resolution.
We developed SCOUT (Smart Cannula-mounted Optical Unit for Trace-methane), the
first robust in-vivo sensing system enabling continuous, high-resolution
monitoring of ruminal methane concentrations through an innovative closed-loop
gas recirculation design. We conducted comprehensive validation with two
cannulated Simmental heifers under contrasting dietary treatments, with
cross-platform comparison against established ambient sniffer systems. SCOUT
achieved exceptional performance with 82% data retention compared to 17% for
conventional sniffer systems, while capturing methane concentrations 100-1000x
higher than ambient approaches. Cross-platform validation demonstrated strong
scale-dependent correlations, with optimal correlation strength (r = -0.564
$\pm$ 0.007) at biologically relevant 40-minute windows and 100% statistical
significance. High-frequency monitoring revealed novel behavior-emission
coupling, including rapid concentration changes (14.5 $\pm$ 11.3k ppm)
triggered by postural transitions within 15 minutes, insights previously
inaccessible through existing technologies. The SCOUT system represents a
transformative advancement, enabling accurate, continuous emission phenotyping
essential for genomic selection programs and sustainable precision livestock
management. This validation framework establishes new benchmarks for
agricultural sensor performance while generating unprecedented biological
insights into ruminal methane dynamics, contributing essential tools for
sustainable livestock production in climate-conscious agricultural systems.

</details>


### [173] [DRIVE: Dynamic Rule Inference and Verified Evaluation for Constraint-Aware Autonomous Driving](https://arxiv.org/abs/2508.04066)
*Longling Geng,Huangxing Li,Viktor Lado Naess,Mert Pilanci*

Main category: cs.RO

TL;DR: DRIVE通过动态规则推断和优化规划，实现了自动驾驶中对软约束的高效建模与验证，显著提升了轨迹质量和合规性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中软约束通常是隐式、上下文依赖且难以明确指定的，DRIVE旨在通过专家演示建模和评估人类驾驶约束，以提升安全性和社会合规性。

Method: DRIVE采用指数族似然建模估计状态转移的可行性，构建了概率化的软行为规则表示，并嵌入到基于凸优化的规划模块中。

Result: 实验证明，DRIVE在大型自然驾驶数据集上实现了0.0%的软约束违规率、更平滑的轨迹和更强的场景泛化能力。

Conclusion: DRIVE框架通过动态规则推断和验证评估，实现了在自动驾驶中对软约束的有效建模和验证，显著提高了轨迹的平滑性和泛化能力，同时保持了零软约束违规率。

Abstract: Understanding and adhering to soft constraints is essential for safe and
socially compliant autonomous driving. However, such constraints are often
implicit, context-dependent, and difficult to specify explicitly. In this work,
we present DRIVE, a novel framework for Dynamic Rule Inference and Verified
Evaluation that models and evaluates human-like driving constraints from expert
demonstrations. DRIVE leverages exponential-family likelihood modeling to
estimate the feasibility of state transitions, constructing a probabilistic
representation of soft behavioral rules that vary across driving contexts.
These learned rule distributions are then embedded into a convex
optimization-based planning module, enabling the generation of trajectories
that are not only dynamically feasible but also compliant with inferred human
preferences. Unlike prior approaches that rely on fixed constraint forms or
purely reward-based modeling, DRIVE offers a unified framework that tightly
couples rule inference with trajectory-level decision-making. It supports both
data-driven constraint generalization and principled feasibility verification.
We validate DRIVE on large-scale naturalistic driving datasets, including inD,
highD, and RoundD, and benchmark it against representative inverse constraint
learning and planning baselines. Experimental results show that DRIVE achieves
0.0% soft constraint violation rates, smoother trajectories, and stronger
generalization across diverse driving scenarios. Verified evaluations further
demonstrate the efficiency, explanability, and robustness of the framework for
real-world deployment.

</details>


### [174] [Industrial Robot Motion Planning with GPUs: Integration of cuRobo for Extended DOF Systems](https://arxiv.org/abs/2508.04146)
*Luai Abuelsamen,Harsh Rana,Ho-Wei Lu,Wenhan Tang,Swati Priyadarshini,Gabriel Gomes*

Main category: cs.RO

TL;DR: This paper integrates GPU-accelerated motion planning via NVIDIA's cuRobo into Vention's platform, enhancing trajectory generation and collision avoidance for industrial robots, with notable speed and robustness improvements.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of efficient motion planning in industrial robotics, especially for multi-axis systems in complex environments.

Method: Integration of GPU-accelerated motion planning using NVIDIA's cuRobo library, leveraging accurate CAD-based digital twins and real-time parallel optimization for rapid trajectory generation and dynamic collision avoidance.

Result: The system shows significant improvements in planning speed and robustness, particularly in pick-and-place tasks with robots equipped with additional degrees of freedom.

Conclusion: GPU-accelerated motion planning through NVIDIA's cuRobo library significantly improves planning speed and robustness, demonstrating its potential for scalable and adaptable deployment in industrial workflows.

Abstract: Efficient motion planning remains a key challenge in industrial robotics,
especially for multi-axis systems operating in complex environments. This paper
addresses that challenge by integrating GPU-accelerated motion planning through
NVIDIA's cuRobo library into Vention's modular automation platform. By
leveraging accurate CAD-based digital twins and real-time parallel
optimization, our system enables rapid trajectory generation and dynamic
collision avoidance for pick-and-place tasks. We demonstrate this capability on
robots equipped with additional degrees of freedom, including a 7th-axis
gantry, and benchmark performance across various scenarios. The results show
significant improvements in planning speed and robustness, highlighting the
potential of GPU-based planning pipelines for scalable, adaptable deployment in
modern industrial workflows.

</details>


### [175] [Improving Tactile Gesture Recognition with Optical Flow](https://arxiv.org/abs/2508.04338)
*Shaohong Zhong,Alessandro Albini,Giammarco Caroleo,Giorgio Cannata,Perla Maiolino*

Main category: cs.RO

TL;DR: 通过增强触觉图像中的动态信息（密集光流），提升了手势识别分类器的准确性，实验显示准确率提高了9%。


<details>
  <summary>Details</summary>
Motivation: 现有基于触觉图像的手势识别方法难以区分某些手势，因为这些手势在触觉图像上呈现相似但动态不同的特性。

Method: 提出了一种简单而有效的方法，通过计算密集光流来增强触觉图像中的动态信息，以区分手势的不同接触动态。

Result: 实验表明，使用增强后的触觉图像（包含光流信息）训练的分类器在手势识别任务中的准确率比标准触觉图像提高了9%。

Conclusion: 通过计算密集光流来突出触觉图像中的接触动态，显著提高了触觉手势识别的准确性，验证了该方法在提升分类器性能方面的有效性。

Abstract: Tactile gesture recognition systems play a crucial role in Human-Robot
Interaction (HRI) by enabling intuitive communication between humans and
robots. The literature mainly addresses this problem by applying machine
learning techniques to classify sequences of tactile images encoding the
pressure distribution generated when executing the gestures. However, some
gestures can be hard to differentiate based on the information provided by
tactile images alone. In this paper, we present a simple yet effective way to
improve the accuracy of a gesture recognition classifier. Our approach focuses
solely on processing the tactile images used as input by the classifier. In
particular, we propose to explicitly highlight the dynamics of the contact in
the tactile image by computing the dense optical flow. This additional
information makes it easier to distinguish between gestures that produce
similar tactile images but exhibit different contact dynamics. We validate the
proposed approach in a tactile gesture recognition task, showing that a
classifier trained on tactile images augmented with optical flow information
achieved a 9% improvement in gesture classification accuracy compared to one
trained on standard tactile images.

</details>


### [176] [Tactile Comfort: Lowering Heart Rate Through Interactions](https://arxiv.org/abs/2508.04372)
*Morten Roed Frederiksen,Kasper Støy,Maja Matarić*

Main category: cs.RO

TL;DR: 本文研究了一种便携式陪伴机器人，通过触觉游戏分散注意力以降低儿童心率，实验证明其效果显著。


<details>
  <summary>Details</summary>
Motivation: 探索一种无需事先训练即可提供放松技巧的便携式陪伴机器人，旨在验证其对用户心率的即时影响。

Method: 通过两项研究（一项14天的试点研究涉及两名8岁儿童，另一项主要研究涉及18名7-8岁儿童），采用被试内设计，测量儿童在使用和不使用机器人时的心率变化。

Result: 与不使用机器人相比，使用机器人时儿童的心率显著降低（p<0.01），表明其具有一致的镇静效果。

Conclusion: 触觉陪伴机器人有望提升放松技巧的治疗价值，因其能显著降低儿童的心率。

Abstract: Children diagnosed with anxiety disorders are taught a range of strategies to
navigate situations of heightened anxiety. Techniques such as deep breathing
and repetition of mantras are commonly employed, as they are known to be
calming and reduce elevated heart rates. Although these strategies are often
effective, their successful application relies on prior training of the
children for successful use when faced with challenging situations. This paper
investigates a pocket-sized companion robot designed to offer a relaxation
technique requiring no prior training, with a focus on immediate impact on the
user's heart rate. The robot utilizes a tactile game to divert the user's
attention, thereby promoting relaxation. We conducted two studies with children
who were not diagnosed with anxiety: a 14-day pilot study with two children
(age 8) and a main study with 18 children (ages 7-8). Both studies employed a
within-subjects design and focused on measuring heart rate during tactile
interaction with the robot and during non-use. Interacting with the robot was
found to significantly lower the study participants' heart rate (p$<$0.01)
compared to the non-use condition, indicating a consistent calming effect
across all participants. These results suggest that tactile companion robots
have the potential to enhance the therapeutic value of relaxation techniques.

</details>


### [177] [Incorporating Stochastic Models of Controller Behavior into Kinodynamic Efficiently Adaptive State Lattices for Mobile Robot Motion Planning in Off-Road Environments](https://arxiv.org/abs/2508.04384)
*Eric R. Damm,Eli S. Lancaster,Felix A. Sanchez,Kiana Bronder,Jason M. Gregory,Thomas M. Howard*

Main category: cs.RO

TL;DR: Incorporating stochastic controller behavior into KEASL improves collision prediction but may reduce planning success, tested on a Warthog UGV in off-road environments.


<details>
  <summary>Details</summary>
Motivation: Addressing errors in mobile robot motion planners due to real-world physics and uncertainty in lower-level controller behavior.

Method: Three methods of incorporating stochastic controller behavior into the recombinant search space of the KEASL planner were presented. Experiments were conducted on a Clearpath Robotics Warthog UGV in an off-road environment using two perception algorithms and an ablation study with simulated environments.

Result: Stochastic controller sampling in KEASL decreases predicted collision likelihood but may reduce planning success rates when compared to baseline planning with expanded obstacle footprints.

Conclusion: Incorporating stochastic controller sampling into KEASL leads to more conservative trajectories, reducing predicted collision likelihood but may lower planning success rates compared to baseline methods.

Abstract: Mobile robot motion planners rely on theoretical models to predict how the
robot will move through the world. However, when deployed on a physical robot,
these models are subject to errors due to real-world physics and uncertainty in
how the lower-level controller follows the planned trajectory. In this work, we
address this problem by presenting three methods of incorporating stochastic
controller behavior into the recombinant search space of the Kinodynamic
Efficiently Adaptive State Lattice (KEASL) planner. To demonstrate this work,
we analyze the results of experiments performed on a Clearpath Robotics Warthog
Unmanned Ground Vehicle (UGV) in an off-road, unstructured environment using
two different perception algorithms, and performed an ablation study using a
full spectrum of simulated environment map complexities. Analysis of the data
found that incorporating stochastic controller sampling into KEASL leads to
more conservative trajectories that decrease predicted collision likelihood
when compared to KEASL without sampling. When compared to baseline planning
with expanded obstacle footprints, the predicted likelihood of collisions
becomes more comparable, but reduces the planning success rate for baseline
search.

</details>


### [178] [Reliable and Real-Time Highway Trajectory Planning via Hybrid Learning-Optimization Frameworks](https://arxiv.org/abs/2508.04436)
*Yujia Lu,Chong Wei,Lu Ma*

Main category: cs.RO

TL;DR: 该论文提出一种混合轨迹规划框架，结合学习方法的适应性和优化方法的安全保证，在高速公路上实现高效、安全的自动驾驶，实验验证了其高成功率和实时性。


<details>
  <summary>Details</summary>
Motivation: 高速公路自动驾驶因环境快速变化和反应时间有限而具有高碰撞风险，需要可靠且高效的轨迹规划。

Method: 框架采用两层架构：上层使用图神经网络（GNN）预测人类类似纵向速度剖面，下层利用混合整数二次规划（MIQP）进行路径优化。下层路径优化模型引入离散化车辆几何的线性近似，显著降低计算复杂度，同时强制执行严格的时空非重叠约束以保证碰撞避免。

Result: 实验结果表明，该规划器在复杂真实世界紧急场景中生成高度平滑且无碰撞的轨迹，成功率达97%以上，平均规划时间为54毫秒。

Conclusion: 该论文提出的混合轨迹规划框架在复杂的真实世界紧急场景中能够生成高度平滑且无碰撞的轨迹，成功率达到97%以上，平均规划时间为54毫秒，证实了其实时能力。

Abstract: Autonomous highway driving presents a high collision risk due to
fast-changing environments and limited reaction time, necessitating reliable
and efficient trajectory planning. This paper proposes a hybrid trajectory
planning framework that integrates the adaptability of learning-based methods
with the formal safety guarantees of optimization-based approaches. The
framework features a two-layer architecture: an upper layer employing a graph
neural network (GNN) trained on real-world highway data to predict human-like
longitudinal velocity profiles, and a lower layer utilizing path optimization
formulated as a mixed-integer quadratic programming (MIQP) problem. The primary
contribution is the lower-layer path optimization model, which introduces a
linear approximation of discretized vehicle geometry to substantially reduce
computational complexity, while enforcing strict spatiotemporal non-overlapping
constraints to formally guarantee collision avoidance throughout the planning
horizon. Experimental results demonstrate that the planner generates highly
smooth, collision-free trajectories in complex real-world emergency scenarios,
achieving success rates exceeding 97% with average planning times of 54 ms,
thereby confirming real-time capability.

</details>


### [179] [Behaviorally Adaptive Multi-Robot Hazard Localization in Failure-Prone, Communication-Denied Environments](https://arxiv.org/abs/2508.04537)
*Alkesh K. Srivastava,Aamodh Suresh,Carlos Nieto-Granda*

Main category: cs.RO

TL;DR: 该论文提出了一种行为自适应路径规划框架（BAPP），用于高风险环境中的多机器人危险地图构建，通过两种算法优化信息收集和机器人存活率，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决高风险、易失败、通信受限环境下（如灾后区域、地下矿井、洞穴和行星表面）的多机器人自主危险地图构建挑战。

Method: 引入了基于行为熵（BE）的信息论规划框架，提出了行为自适应路径规划（BAPP）框架，包括BAPP-TID和BAPP-SIG两种算法。

Result: BAPP框架在单机器人和多机器人模拟中均优于基于香农熵和随机策略的方法，BAPP-TID加速熵减少，BAPP-SIG在信息增益损失最小的情况下提高机器人存活率。

Conclusion: 该研究强调了行为自适应规划在复杂、易失败环境中进行稳健、风险敏感探索的价值，通过BAPP框架在单机器人和多机器人部署中均表现出色。

Abstract: We address the challenge of multi-robot autonomous hazard mapping in
high-risk, failure-prone, communication-denied environments such as
post-disaster zones, underground mines, caves, and planetary surfaces. In these
missions, robots must explore and map hazards while minimizing the risk of
failure due to environmental threats or hardware limitations. We introduce a
behavior-adaptive, information-theoretic planning framework for multi-robot
teams grounded in the concept of Behavioral Entropy (BE), that generalizes
Shannon entropy (SE) to capture diverse human-like uncertainty evaluations.
Building on this formulation, we propose the Behavior-Adaptive Path Planning
(BAPP) framework, which modulates information gathering strategies via a
tunable risk-sensitivity parameter, and present two planning algorithms:
BAPP-TID for intelligent triggering of high-fidelity robots, and BAPP-SIG for
safe deployment under high risk. We provide theoretical insights on the
informativeness of the proposed BAPP framework and validate its effectiveness
through both single-robot and multi-robot simulations. Our results show that
the BAPP stack consistently outperforms Shannon-based and random strategies:
BAPP-TID accelerates entropy reduction, while BAPP-SIG improves robot
survivability with minimal loss in information gain. In multi-agent
deployments, BAPP scales effectively through spatial partitioning, mobile base
relocation, and role-aware heterogeneity. These findings underscore the value
of behavior-adaptive planning for robust, risk-sensitive exploration in
complex, failure-prone environments.

</details>


### [180] [$NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything](https://arxiv.org/abs/2508.04598)
*Lingfeng Zhang,Xiaoshuai Hao,Yingbo Tang,Haoxiang Fu,Xinyu Zheng,Pengwei Wang,Zhongyuan Wang,Wenbo Ding,Shanghang Zhang*

Main category: cs.RO

TL;DR: NavA^3是一个分层框架，通过全局和局部策略解决复杂环境中的高级指令导航问题，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有导航任务主要关注预定义对象或指令跟随，与真实场景中的复杂、开放需求存在差距，需要一种能理解高级指令并实现空间感知导航的方法。

Method: 提出了NavA^3分层框架，分为全局策略（利用Reasoning-VLM解析指令并整合全局3D场景视图）和局部策略（通过NaviAfford模型实现开放词汇对象定位和空间感知）。

Result: NavA^3在导航性能上达到SOTA，能成功完成不同机器人实体在真实环境中的长视野导航任务。

Conclusion: NavA^3框架通过结合全局和局部策略，显著提升了在复杂环境中理解和执行高级人类指令的能力，实现了通用具身导航的突破。

Abstract: Embodied navigation is a fundamental capability of embodied intelligence,
enabling robots to move and interact within physical environments. However,
existing navigation tasks primarily focus on predefined object navigation or
instruction following, which significantly differs from human needs in
real-world scenarios involving complex, open-ended scenes. To bridge this gap,
we introduce a challenging long-horizon navigation task that requires
understanding high-level human instructions and performing spatial-aware object
navigation in real-world environments. Existing embodied navigation methods
struggle with such tasks due to their limitations in comprehending high-level
human instructions and localizing objects with an open vocabulary. In this
paper, we propose $NavA^3$, a hierarchical framework divided into two stages:
global and local policies. In the global policy, we leverage the reasoning
capabilities of Reasoning-VLM to parse high-level human instructions and
integrate them with global 3D scene views. This allows us to reason and
navigate to regions most likely to contain the goal object. In the local
policy, we have collected a dataset of 1.0 million samples of spatial-aware
object affordances to train the NaviAfford model (PointingVLM), which provides
robust open-vocabulary object localization and spatial awareness for precise
goal identification and navigation in complex environments. Extensive
experiments demonstrate that $NavA^3$ achieves SOTA results in navigation
performance and can successfully complete longhorizon navigation tasks across
different robot embodiments in real-world settings, paving the way for
universal embodied navigation. The dataset and code will be made available.
Project website: https://NavigationA3.github.io/.

</details>


### [181] [RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case](https://arxiv.org/abs/2508.04642)
*Baihui Xiao,Chengjian Feng,Zhijian Huang,Feng yan,Yujie Zhong,Lin Ma*

Main category: cs.RO

TL;DR: RoboTron-Sim通过模拟高风险驾驶场景和新技术，显著提升了自动驾驶在复杂情况下的表现。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统因缺乏真实高风险场景数据而在关键情况下表现不佳的问题。

Method: 开发了HASS模拟数据集，包含13类高风险边缘案例；提出了SPE和I2E编码器，使多模态大语言模型能有效从HASS学习驾驶技能。

Result: 在nuScenes上的实验显示，RoboTron-Sim在挑战性场景中驾驶性能提升约50%，实现了开环规划的SOTA效果。

Conclusion: RoboTron-Sim通过模拟复杂场景显著提升了自动驾驶系统在真实世界高风险情况下的性能，达到了最先进的开环规划效果。

Abstract: Collecting real-world data for rare high-risk scenarios, long-tailed driving
events, and complex interactions remains challenging, leading to poor
performance of existing autonomous driving systems in these critical
situations. In this paper, we propose RoboTron-Sim that improves real-world
driving in critical situations by utilizing simulated hard cases. First, we
develop a simulated dataset called Hard-case Augmented Synthetic Scenarios
(HASS), which covers 13 high-risk edge-case categories, as well as balanced
environmental conditions such as day/night and sunny/rainy. Second, we
introduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder
(I2E Encoder) to enable multimodal large language models to effectively learn
real-world challenging driving skills from HASS, via adapting to environmental
deviations and hardware differences between real-world and simulated scenarios.
Extensive experiments on nuScenes show that RoboTron-Sim improves driving
performance in challenging scenarios by around 50%, achieving state-of-the-art
results in real-world open-loop planning. Qualitative results further
demonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk
driving scenarios. Project page: https://stars79689.github.io/RoboTron-Sim/

</details>


### [182] [Open Scene Graphs for Open-World Object-Goal Navigation](https://arxiv.org/abs/2508.04678)
*Joel Loo,Zhanxin Wu,David Hsu*

Main category: cs.RO

TL;DR: OSG Navigator是一个基于基础模型和Open Scene Graph的模块化系统，用于开放世界语义导航，实现了零样本适应和卓越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决开放世界语义导航的挑战，例如在自然语言指定的目标对象下搜索新环境。

Method: OSG Navigator是一个模块化系统，结合了基础模型和Open Scene Graph表示，后者作为空间记忆，通过OSG schemas分层组织空间信息。这些模板可以自动从环境的简单语义标签生成。

Result: 在Fetch和Spot机器人上的仿真和现实世界实验中，OSG Navigator在ObjectNav基准测试中达到了最先进的性能。

Conclusion: OSG Navigator通过Open Scene Graph表示和OSG schemas实现了在开放世界语义导航中的零样本适应，展示了在多样目标、环境和机器人平台上的卓越泛化能力。

Abstract: How can we build general-purpose robot systems for open-world semantic
navigation, e.g., searching a novel environment for a target object specified
in natural language? To tackle this challenge, we introduce OSG Navigator, a
modular system composed of foundation models, for open-world Object-Goal
Navigation (ObjectNav). Foundation models provide enormous semantic knowledge
about the world, but struggle to organise and maintain spatial information
effectively at scale. Key to OSG Navigator is the Open Scene Graph
representation, which acts as spatial memory for OSG Navigator. It organises
spatial information hierarchically using OSG schemas, which are templates, each
describing the common structure of a class of environments. OSG schemas can be
automatically generated from simple semantic labels of a given environment,
e.g., "home" or "supermarket". They enable OSG Navigator to adapt zero-shot to
new environment types. We conducted experiments using both Fetch and Spot
robots in simulation and in the real world, showing that OSG Navigator achieves
state-of-the-art performance on ObjectNav benchmarks and generalises zero-shot
over diverse goals, environments, and robot embodiments.

</details>


### [183] [From MAS to MARS: Coordination Failures and Reasoning Trade-offs in Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario](https://arxiv.org/abs/2508.04691)
*Yuanchen Bai,Zijian Ding,Shaoyue Wen,Xiang Chang,Angelique Taylor*

Main category: cs.RO

TL;DR: 本文通过两项研究探讨了多智能体框架在模拟现实世界多机器人医疗场景中的性能权衡，强调了自主性与稳定性的张力及边缘案例测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管存在先进的多智能体框架，但它们在机器人上的实际部署仍然有限，阻碍了多智能体机器人系统（MARS）研究的实际进展。本文旨在弥合这一差距。

Method: 在Study 1中使用CrewAI迭代优化系统的知识库，系统识别和分类无法仅通过提供上下文知识解决的协调失败；在Study 2中使用AutoGen评估重新设计的双向通信结构，并测量同一机器人团队设置中推理和非推理模型之间的权衡。

Result: 研究结果揭示了协调失败的类型（如工具访问违规、未能及时处理失败报告）以及推理和非推理模型在机器人团队中的性能权衡。

Conclusion: 本文通过两项研究强调了在现实世界多机器人医疗场景中，自主性和稳定性之间的张力，以及边缘案例测试对提高系统可靠性和安全性的重要性。

Abstract: Multi-agent robotic systems (MARS) build upon multi-agent systems by
integrating physical and task-related constraints, increasing the complexity of
action execution and agent coordination. However, despite the availability of
advanced multi-agent frameworks, their real-world deployment on robots remains
limited, hindering the advancement of MARS research in practice. To bridge this
gap, we conducted two studies to investigate performance trade-offs of
hierarchical multi-agent frameworks in a simulated real-world multi-robot
healthcare scenario. In Study 1, using CrewAI, we iteratively refine the
system's knowledge base, to systematically identify and categorize coordination
failures (e.g., tool access violations, lack of timely handling of failure
reports) not resolvable by providing contextual knowledge alone. In Study 2,
using AutoGen, we evaluate a redesigned bidirectional communication structure
and further measure the trade-offs between reasoning and non-reasoning models
operating within the same robotic team setting. Drawing from our empirical
findings, we emphasize the tension between autonomy and stability and the
importance of edge-case testing to improve system reliability and safety for
future real-world deployment. Supplementary materials, including codes, task
agent setup, trace outputs, and annotated examples of coordination failures and
reasoning behaviors, are available at:
https://byc-sophie.github.io/mas-to-mars/.

</details>


### [184] [Achieving Precise and Reliable Locomotion with Differentiable Simulation-Based System Identification](https://arxiv.org/abs/2508.04696)
*Vyacheslav Kovalev,Ekaterina Chaikovskaia,Egor Davydenko,Roman Gorbachev*

Main category: cs.RO

TL;DR: 提出了一种将系统辨识整合到强化学习训练循环中的新型控制框架，利用可微分仿真优化系统参数，仅需轨迹数据和控制输入，显著提升了轨迹跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 在双足运动中，准确的系统辨识对于减少轨迹漂移至关重要，尤其是在强化学习和基于模型的控制中。传统方法依赖直接扭矩测量，而本文方法仅需轨迹数据和控制输入，更具灵活性和可扩展性。

Method: 利用可微分仿真器MuJoCo-XLA优化系统参数，通过神经网络近似处理复杂非线性行为（如高级摩擦模型），确保仿真机器人行为与现实运动一致。

Result: 实验结果表明，该框架显著提升了轨迹跟踪性能。

Conclusion: 该框架不仅支持基本物理属性（如质量和惯性），还通过神经网络处理复杂系统非线性行为，为双足运动控制提供了灵活且可扩展的解决方案。

Abstract: Accurate system identification is crucial for reducing trajectory drift in
bipedal locomotion, particularly in reinforcement learning and model-based
control. In this paper, we present a novel control framework that integrates
system identification into the reinforcement learning training loop using
differentiable simulation. Unlike traditional approaches that rely on direct
torque measurements, our method estimates system parameters using only
trajectory data (positions, velocities) and control inputs. We leverage the
differentiable simulator MuJoCo-XLA to optimize system parameters, ensuring
that simulated robot behavior closely aligns with real-world motion. This
framework enables scalable and flexible parameter optimization. Accurate system
identification is crucial for reducing trajectory drift in bipedal locomotion,
particularly in reinforcement learning and model-based control. In this paper,
we present a novel control framework that integrates system identification into
the reinforcement learning training loop using differentiable simulation.
Unlike traditional approaches that rely on direct torque measurements, our
method estimates system parameters using only trajectory data (positions,
velocities) and control inputs. We leverage the differentiable simulator
MuJoCo-XLA to optimize system parameters, ensuring that simulated robot
behavior closely aligns with real-world motion. This framework enables scalable
and flexible parameter optimization. It supports fundamental physical
properties such as mass and inertia. Additionally, it handles complex system
nonlinear behaviors, including advanced friction models, through neural network
approximations. Experimental results show that our framework significantly
improves trajectory following.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [185] [A 60-Addition, Rank-23 Scheme for Exact 3x3 Matrix Multiplication](https://arxiv.org/abs/2508.03857)
*Joshua Stapleton*

Main category: cs.DS

TL;DR: 本文通过优化算法，将3x3非交换矩阵乘法的加法成本从61或62降至60，无需基变换，达到了新的技术水平。


<details>
  <summary>Details</summary>
Motivation: 探索如何进一步降低非交换矩阵乘法的计算成本，尤其是加法操作的开销。

Method: 通过优化算法，无需基变换，直接降低了加法成本。

Result: 将加法成本从61或62降至60，创下了新的记录。

Conclusion: 本文成功将3x3非交换矩阵乘法的加法成本从之前的61（Schwartz-Vaknin, 2023）和62（Martensson-Wagner, 2025）降低到60，且无需改变基，代表了当前的最新技术水平。

Abstract: We reduce the additive cost of general (non-commutative) 3x3 matrix
multiplication from the previous records of 61 (Schwartz-Vaknin, 2023) and 62
(Martensson-Wagner, 2025) to 60 without a change of basis. To our knowledge,
this represents a new state-of-the-art.

</details>


### [186] [Counting Distinct Square Substrings in Sublinear Time](https://arxiv.org/abs/2508.03930)
*Panagiotis Charalampopoulos,Manal Mohamed,Jakub Radoszewski,Wojciech Rytter,Tomasz Waleń,Wiktor Zuba*

Main category: cs.DS

TL;DR: 本文提出了一种在压缩字符串中计算不同平方数的子线性时间算法，填补了该领域的空白。


<details>
  <summary>Details</summary>
Motivation: 现有方法在压缩字符串中计算不同平方数的时间复杂度为线性，无法满足高效处理大规模数据的需求，因此需要开发更快的算法。

Method: 利用Crochemore等人描述的提取平方的技术，并结合压缩模型的新方法，如稀疏Lyndon根和层运行的组合性质，实现了O(n/logσn)时间复杂度的算法。

Result: 成功设计并实现了首个在压缩字符串中计算不同平方数的子线性时间算法，时间复杂度为O(n/logσn)。

Conclusion: 本文提出了一种在word-RAM模型中计算压缩字符串中不同平方数的子线性时间算法，填补了该领域的空白。

Abstract: We show that the number of distinct squares in a packed string of length $n$
over an alphabet of size $\sigma$ can be computed in $O(n/\log_\sigma n)$ time
in the word-RAM model. This paper is the first to introduce a sublinear-time
algorithm for counting squares in the packed setting. The packed representation
of a string of length $n$ over an alphabet of size $\sigma$ is given as a
sequence of $O(n/\log_\sigma n)$ machine words in the word-RAM model (a machine
word consists of $\omega \ge \log_2 n$ bits). Previously, it was known how to
count distinct squares in $O(n)$ time [Gusfield and Stoye, JCSS 2004], even for
a string over an integer alphabet [Crochemore et al., TCS 2014; Bannai et al.,
CPM 2017; Charalampopoulos et al., SPIRE 2020]. We use the techniques for
extracting squares from runs described by Crochemore et al. [TCS 2014].
However, the packed model requires novel approaches.
  We need an $O(n/\log_\sigma n)$-sized representation of all long-period runs
(runs with period $\Omega(\log_\sigma n)$) which allows for a sublinear-time
counting of the -- potentially linearly-many -- implied squares. The
long-period runs with a string period that is periodic itself (called layer
runs) are an obstacle, since their number can be $\Omega(n)$. The number of all
other long-period runs is $O(n/\log_\sigma n)$ and we can construct an implicit
representation of all long-period runs in $O(n/\log_\sigma n)$ time by
leveraging the insights of Amir et al. [ESA 2019]. We count squares in layer
runs by exploiting combinatorial properties of pyramidally-shaped groups of
layer runs. Another difficulty lies in computing the locations of Lyndon roots
of runs in packed strings, which is needed for grouping runs that may generate
equal squares. To overcome this difficulty, we introduce sparse-Lyndon roots
which are based on string synchronizers [Kempa and Kociumaka, STOC 2019].

</details>


### [187] [Exactly simulating stochastic chemical reaction networks in sub-constant time per reaction](https://arxiv.org/abs/2508.04079)
*Joshua Petrack,David Doty*

Main category: cs.DS

TL;DR: 提出首个亚线性时间复杂度的化学反应网络随机模拟算法，保持精确随机动力学，性能显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的Gillespie算法在模拟连续反应时需要线性时间，限制了其在大规模系统中的应用。本文旨在开发一种更高效的算法，以克服这一限制。

Method: 通过扩展Berenbrink等人用于模拟群体协议的算法，以一种非平凡的方式将其适配到更一般的化学反应网络场景中。

Result: 在合理假设下，新算法能在O(ℓ/√n)或O(ℓ/n^(2/5))时间内模拟ℓ个反应，具体取决于ℓ与n的关系。

Conclusion: 该论文提出了一种新的化学反应网络随机模拟算法，能够在保持精确随机动力学的同时，实现亚线性时间复杂度的模拟，显著优于传统的Gillespie算法。

Abstract: The model of chemical reaction networks is among the oldest and most widely
studied and used in natural science. The model describes reactions among
abstract chemical species, for instance $A + B \to C$, which indicates that if
a molecule of type $A$ interacts with a molecule of type $B$ (the reactants),
they may stick together to form a molecule of type $C$ (the product). The
standard algorithm for simulating (discrete, stochastic) chemical reaction
networks is the Gillespie algorithm [JPC 1977], which stochastically simulates
one reaction at a time, so to simulate $\ell$ consecutive reactions, it
requires total running time $\Omega(\ell)$.
  We give the first chemical reaction network stochastic simulation algorithm
that can simulate $\ell$ reactions, provably preserving the exact stochastic
dynamics (sampling from precisely the same distribution as the Gillespie
algorithm), yet using time provably sublinear in $\ell$. Under reasonable
assumptions, our algorithm can simulate $\ell$ reactions among $n$ total
molecules in time $O(\ell/\sqrt n)$ when $\ell \ge n^{5/4}$, and in time
$O(\ell/n^{2/5})$ when $n \le \ell \le n^{5/4}$. Our work adapts an algorithm
of Berenbrink, Hammer, Kaaser, Meyer, Penschuck, and Tran [ESA 2020] for
simulating the distributed computing model known as population protocols,
extending it (in a very nontrivial way) to the more general chemical reaction
network setting.
  We provide an implementation of our algorithm as a Python package, with the
core logic implemented in Rust, with remarkably fast performance in practice.

</details>


### [188] [Exact Matching in Matrix Multiplication Time](https://arxiv.org/abs/2508.04081)
*Ryotaro Sato,Yutaro Yamaguchi*

Main category: cs.DS

TL;DR: 本文利用矩阵特征多项式快速计算，改进了匹配问题的代数算法，展示了精确匹配问题的高效解法，并扩展至线性拟阵配对问题。


<details>
  <summary>Details</summary>
Motivation: 回顾并改进由Mulmuley、Vazirani和Vazirani（1987）提出的代数算法，以解决匹配及相关问题。

Method: 利用矩阵特征多项式的快速计算技术改进现有算法。

Result: 精确匹配问题可在与矩阵乘法相同的时间复杂度内以高概率解决。

Conclusion: 本文展示了精确匹配问题可以通过矩阵乘法的时间复杂度以高概率解决，并讨论了其在线性拟阵配对问题中的扩展。

Abstract: Initiated by Mulmuley, Vazirani, and Vazirani (1987), many algebraic
algorithms have been developed for matching and related problems. In this
paper, we review basic facts and discuss possible improvements with the aid of
fast computation of the characteristic polynomial of a matrix. In particular,
we show that the so-called exact matching problem can be solved with high
probability in asymptotically the same time order as matrix multiplication. We
also discuss its extension to the linear matroid parity problem.

</details>


### [189] [Approximation Algorithms for Scheduling Crowdsourcing Tasks in Mobile Social Networks](https://arxiv.org/abs/2508.04159)
*Chi-Yeh Chen*

Main category: cs.DS

TL;DR: 本文纠正了前人错误，提出了两种新算法优化移动社交网络的任务调度，分别达到1.5+ε和max{2.5,1+ε}的近似比。


<details>
  <summary>Details</summary>
Motivation: 解决移动社交网络中任务调度的近似比分析错误，并优化总加权完成时间。

Method: 通过分析Largest-Ratio-First算法的近似比，并引入随机化近似算法和确定性近似算法来优化总加权完成时间。

Result: 纠正了Zhang等人的分析错误，提出了两种新算法，随机算法达到1.5+ε的近似比，确定性算法在特定条件下可达1.5+ε。

Conclusion: 本文纠正了Zhang等人的近似比分析错误，并针对移动社交网络中的任务调度问题，提出了随机和确定性两种近似算法，分别达到了1.5+ε和max{2.5,1+ε}的近似比，且在特定条件下可达1.5+ε。

Abstract: This paper addresses the scheduling problem in mobile social networks. We
begin by proving that the approximation ratio analysis presented in the paper
by Zhang \textit{et al.} (IEEE Transactions on Mobile Computing, 2025) is
incorrect, and we provide the correct analysis results. Furthermore, when the
required service time for a task exceeds the total contact time between the
requester and the crowd worker, we demonstrate that the approximation ratio of
the Largest-Ratio-First task scheduling algorithm can reach $2 - \frac{1}{m}$.
Next, we introduce a randomized approximation algorithm to minimize mobile
social networks' total weighted completion time. This algorithm achieves an
expected approximation ratio of $1.5 + \epsilon$ for $\epsilon>0$. Finally, we
present a deterministic approximation algorithm that minimizes mobile social
networks' total weighted completion time. This deterministic algorithm achieves
an approximation ratio of $\max\left\{2.5,1+\epsilon\right\}$ for $\epsilon>0$.
Additionally, when the task's required service time or the total contact time
between the requester and the crowd worker is sufficiently large, this
algorithm can reach an approximation ratio of $1.5+\epsilon$ for $\epsilon>0$.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [190] [Empathy Guidelines for Improving Practitioner Well-being & Software Engineering Practices](https://arxiv.org/abs/2508.03846)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 本文提出了17条软件工程中共情的实践指南，并通过案例和优先级框架支持其落地，帮助团队从原则转向可持续行动。


<details>
  <summary>Details</summary>
Motivation: 共情在软件工程中被低估，但对团队协作、沟通和决策至关重要。本文旨在为从业者、团队和组织提供实用的共情实践指导。

Method: 基于先前研究，开发了17条共情指南，并通过实践案例、挑战分析和优先级框架支持其落地。

Result: 提出了17条共情指南，并设计了视觉优先级框架，帮助团队根据重要性、易实施性和采纳意愿分类指南。

Conclusion: 本文提出了17条可操作的共情指南，并探讨了其在实际应用中的挑战与解决策略。通过视觉优先级框架，帮助团队将共情原则转化为可持续行动。

Abstract: Empathy is a powerful yet often overlooked element in software engineering
(SE), supporting better teamwork, smoother communication, and effective
decision-making. In our previous study, we identified a range of practitioner
strategies for fostering empathy in SE contexts. Building on these insights,
this paper introduces 17 actionable empathy guidelines designed to support
practitioners, teams, and organisations. We also explore how these guidelines
can be implemented in practice by examining real-world applications,
challenges, and strategies to overcome them shared by software practitioners.
To support adoption, we present a visual prioritisation framework that
categorises the guidelines based on perceived importance, ease of
implementation, and willingness to adopt. The findings offer practical and
flexible suggestions for integrating empathy into everyday SE work, helping
teams move from principles to sustainable action.

</details>


### [191] [Evaluating Software Supply Chain Security in Research Software](https://arxiv.org/abs/2508.03856)
*Richard Hegewald,Rebecca Beyer*

Main category: cs.SE

TL;DR: 研究分析了3,248个研究软件仓库的安全状况，发现普遍较弱（平均3.5/10），并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究软件的安全性对确保科学结果的完整性和可重复性至关重要，但目前研究软件的安全性仍未被充分探索，且因其依赖开源组件和分布式开发实践，特别容易受到供应链攻击。

Method: 使用OpenSSF Scorecard分析了3,248个高质量、多数经过同行评审的研究软件仓库。

Result: 研究发现研究软件的安全状况普遍较弱，平均得分仅为3.5/10，且重要实践（如签名发布和分支保护）很少被实施。

Conclusion: 研究提出了可操作、低成本的建议，帮助研究团队提升软件安全性，减轻对科学完整性的潜在威胁。

Abstract: The security of research software is essential for ensuring the integrity and
reproducibility of scientific results. However, research software security is
still largely unexplored. Due to its dependence on open source components and
distributed development practices, research software is particularly vulnerable
to supply chain attacks. This study analyses 3,248 high-quality, largely
peer-reviewed research software repositories using the OpenSSF Scorecard. We
find a generally weak security posture with an average score of 3.5/10.
Important practices, such as signed releases and branch protection, are rarely
implemented. Finally, we present actionable, low-effort recommendations that
can help research teams improve software security and mitigate potential
threats to scientific integrity.

</details>


### [192] [From App Features to Explanation Needs: Analyzing Correlations and Predictive Potential](https://arxiv.org/abs/2508.03881)
*Martin Obaidi,Kushtrim Qengaj,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Elisa Schmid,Kurt Schneider*

Main category: cs.SE

TL;DR: 研究发现应用元数据难以准确预测用户解释需求，建议结合直接用户反馈来设计更用户友好的软件系统。


<details>
  <summary>Details</summary>
Motivation: 研究探讨是否可以根据应用属性预测用户评论中分类的解释需求，以便在开发早期和大规模需求挖掘中考虑这些需求。

Method: 通过分析4495条应用评论的金标准数据集（包含元数据如应用版本、评分、年龄限制、应用内购买等），并进行相关性分析和线性回归模型验证。

Result: 相关性分析显示应用属性与解释需求之间大多为弱关联，仅特定功能（如应用版本、评论数量、星级评分）有中等相关性。线性回归模型预测能力有限，验证结果一致。

Conclusion: 研究结果表明，解释需求高度依赖上下文，无法仅从应用元数据中准确推断。开发者和需求工程师应结合直接用户反馈来有效设计可解释和以用户为中心的软件系统。

Abstract: In today's digitized world, software systems must support users in
understanding both how to interact with a system and why certain behaviors
occur. This study investigates whether explanation needs, classified from user
reviews, can be predicted based on app properties, enabling early consideration
during development and large-scale requirements mining. We analyzed a gold
standard dataset of 4,495 app reviews enriched with metadata (e.g., app
version, ratings, age restriction, in-app purchases). Correlation analyses
identified mostly weak associations between app properties and explanation
needs, with moderate correlations only for specific features such as app
version, number of reviews, and star ratings. Linear regression models showed
limited predictive power, with no reliable forecasts across configurations.
Validation on a manually labeled dataset of 495 reviews confirmed these
findings. Categories such as Security & Privacy and System Behavior showed
slightly higher predictive potential, while Interaction and User Interface
remained most difficult to predict. Overall, our results highlight that
explanation needs are highly context-dependent and cannot be precisely inferred
from app metadata alone. Developers and requirements engineers should therefore
supplement metadata analysis with direct user feedback to effectively design
explainable and user-centered software systems.

</details>


### [193] [A Human Centric Requirements Engineering Framework for Assessing Github Copilot Output](https://arxiv.org/abs/2508.03922)
*Soroush Heydari*

Main category: cs.SE

TL;DR: 研究探讨了GitHub Copilot在满足人类需求方面的表现，提出了一个以人为中心的评估框架，并分析了其适应性和协作性。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架多关注技术方面（如代码正确性和效率），而忽略了影响AI助手成功融入软件开发工作流程的关键人为因素。

Method: 通过分析GitHub Copilot与用户的聊天界面交互，测量其适应不同用户专业水平的解释和代码生成能力，并评估其在促进协作编程体验中的有效性。

Result: 建立了一个明确指标的人类需求框架，用于评估GitHub Copilot聊天功能的适应性、协作性等质量。

Conclusion: 研究提出了一个以人为中心的需求框架，用于评估GitHub Copilot等AI编程助手在满足人类需求方面的表现，并讨论了测试结果对未来自动化编程中人类需求分析的启示。

Abstract: The rapid adoption of Artificial Intelligence(AI) programming assistants such
as GitHub Copilot introduces new challenges in how these software tools address
human needs. Many existing evaluation frameworks address technical aspects such
as code correctness and efficiency, but often overlook crucial human factors
that affect the successful integration of AI assistants in software development
workflows. In this study, I analyzed GitHub Copilot's interaction with users
through its chat interface, measured Copilot's ability to adapt explanations
and code generation to user expertise levels, and assessed its effectiveness in
facilitating collaborative programming experiences. I established a
human-centered requirements framework with clear metrics to evaluate these
qualities in GitHub Copilot chat. I discussed the test results and their
implications for future analysis of human requirements in automated
programming.

</details>


### [194] [Analyzing Prominent LLMs: An Empirical Study of Performance and Complexity in Solving LeetCode Problems](https://arxiv.org/abs/2508.03931)
*Everton Guimaraes,Nathalia Nascimento,Chandan Shivalingaiah,Asish Nelapati*

Main category: cs.SE

TL;DR: 研究比较了四种LLM在LeetCode问题上的表现，发现ChatGPT最稳定高效，Gemini在简单任务上表现最佳但复杂任务需更多尝试，为开发者选择模型提供了参考。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中的广泛应用，系统比较其性能对于优化实际应用至关重要。

Method: 本研究对ChatGPT、Copilot、Gemini和DeepSeek四种主流LLM在150道LeetCode问题（涵盖简单、中等和困难难度）上的表现进行了基准测试，生成Java和Python解决方案，并评估了执行时间、内存使用和算法复杂度。

Result: ChatGPT在执行时间和内存使用上表现一致高效；Copilot和DeepSeek在任务复杂度增加时表现不稳定；Gemini在简单任务上有效，但随着问题难度增加需要更多尝试。

Conclusion: 研究结果为开发者选择适合特定编码任务的LLM提供了实用指导，并深入了解了GPT类生成解决方案的性能和复杂性。

Abstract: Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek are
transforming software engineering by automating key tasks, including code
generation, testing, and debugging. As these models become integral to
development workflows, a systematic comparison of their performance is
essential for optimizing their use in real world applications. This study
benchmarks these four prominent LLMs on one hundred and fifty LeetCode problems
across easy, medium, and hard difficulties, generating solutions in Java and
Python. We evaluate each model based on execution time, memory usage, and
algorithmic complexity, revealing significant performance differences. ChatGPT
demonstrates consistent efficiency in execution time and memory usage, while
Copilot and DeepSeek show variability as task complexity increases. Gemini,
although effective on simpler tasks, requires more attempts as problem
difficulty rises. Our findings provide actionable insights into each model's
strengths and limitations, offering guidance for developers selecting LLMs for
specific coding tasks and providing insights on the performance and complexity
of GPT-like generated solutions.

</details>


### [195] [Model Compression vs. Adversarial Robustness: An Empirical Study on Language Models for Code](https://arxiv.org/abs/2508.03949)
*Md. Abdul Awal,Mrigank Rochan,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 压缩的代码语言模型在性能上接近未压缩模型，但在对抗攻击下鲁棒性显著下降，需在安全关键应用中谨慎使用。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的代码语言模型在各种软件分析任务中表现出色，但其高计算成本、慢推理速度和环境影响阻碍了其采用。模型压缩技术如剪枝、量化和知识蒸馏已被用于应对这些挑战，但这些策略在对抗场景下对压缩模型鲁棒性的影响尚不明确。

Method: 对三种广泛使用的代码语言模型的压缩版本进行了全面评估，使用了六种评估指标和四种经典对抗攻击方法。

Result: 压缩模型在性能上与未压缩模型相当，但在对抗攻击下鲁棒性显著降低。

Conclusion: 压缩模型在对抗攻击下表现出显著降低的鲁棒性，揭示了模型大小缩减与对抗鲁棒性之间的权衡，强调了在安全关键软件应用中部署压缩模型时需要谨慎考虑。

Abstract: Transformer-based language models for code have shown remarkable performance
in various software analytics tasks, but their adoption is hindered by high
computational costs, slow inference speeds, and substantial environmental
impact. Model compression techniques such as pruning, quantization, and
knowledge distillation have gained traction in addressing these challenges.
However, the impact of these strategies on the robustness of compressed
language models for code in adversarial scenarios remains poorly understood.
Understanding how these compressed models behave under adversarial attacks is
essential for their safe and effective deployment in real-world applications.
To bridge this knowledge gap, we conduct a comprehensive evaluation of how
common compression strategies affect the adversarial robustness of compressed
models. We assess the robustness of compressed versions of three widely used
language models for code across three software analytics tasks, using six
evaluation metrics and four commonly used classical adversarial attacks. Our
findings indicate that compressed models generally maintain comparable
performance to their uncompressed counterparts. However, when subjected to
adversarial attacks, compressed models exhibit significantly reduced
robustness. These results reveal a trade-off between model size reduction and
adversarial robustness, underscoring the need for careful consideration when
deploying compressed models in security-critical software applications. Our
study highlights the need for further research into compression strategies that
strike a balance between computational efficiency and adversarial robustness,
which is essential for deploying reliable language models for code in
real-world software applications.

</details>


### [196] [Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks](https://arxiv.org/abs/2508.04125)
*Sangwon Hyun,Hyunjun Kim,Jinhyuk Jang,Hyojin Choi,M. Ali Babar*

Main category: cs.SE

TL;DR: 研究通过实验发现3个关键HLI特征显著影响代码生成效率，提出5项改进原则和29种错误分类及缓解方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注函数级软件工程实践和已知提示模式，缺乏对涉及多类依赖等复杂现实工作流的探索，本研究旨在填补这一空白。

Method: 设计了包含两个项目级基准任务的实验，招募36名背景各异的参与者，通过特定提示模式与GPT助手交互完成任务，并分析屏幕录像和GPT聊天日志。

Result: 发现15个HLI特征中的3个显著影响代码生成效率，提出了5项提升HLI效率的指导原则，并分类了29种错误及缓解方案。

Conclusion: 本研究通过实验分析揭示了影响代码生成效率的三个关键HLI特征，提出了五项提升HLI效率的指导原则，并分类了29种HLI过程中可能出现的运行时和逻辑错误及缓解方案。

Abstract: The application of Large Language Models (LLMs) is growing in the productive
completion of Software Engineering tasks. Yet, studies investigating the
productive prompting techniques often employed a limited problem space,
primarily focusing on well-known prompting patterns and mainly targeting
function-level SE practices. We identify significant gaps in real-world
workflows that involve complexities beyond class-level (e.g., multi-class
dependencies) and different features that can impact Human-LLM Interactions
(HLIs) processes in code generation. To address these issues, we designed an
experiment that comprehensively analyzed the HLI features regarding the code
generation productivity. Our study presents two project-level benchmark tasks,
extending beyond function-level evaluations. We conducted a user study with 36
participants from diverse backgrounds, asking them to solve the assigned tasks
by interacting with the GPT assistant using specific prompting patterns. We
also examined the participants' experience and their behavioral features during
interactions by analyzing screen recordings and GPT chat logs. Our statistical
and empirical investigation revealed (1) that three out of 15 HLI features
significantly impacted the productivity in code generation; (2) five primary
guidelines for enhancing productivity for HLI processes; and (3) a taxonomy of
29 runtime and logic errors that can occur during HLI processes, along with
suggested mitigation plans.

</details>


### [197] [EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation](https://arxiv.org/abs/2508.04295)
*Chaofan Wang,Tingrui Yu,Jie Wang,Dong Chen,Wenrui Zhang,Yuling Shi,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: EvoC2Rust 是一个自动化框架，通过骨架引导策略将 C 项目转换为等效的 Rust 项目，结合了规则和 LLM 方法的优势，显著提升了转换的准确性、安全性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: Rust 的编译时安全特性使其成为安全关键系统的理想选择，但现有 C 到 Rust 的转换方法在小规模程序上表现有限，无法满足大规模项目的需求。

Method: EvoC2Rust 采用骨架引导的转换策略，分为三个阶段：分解 C 项目并生成可编译的 Rust 骨架、逐步翻译函数、通过 LLM 和静态分析修复编译错误。

Result: EvoC2Rust 在开源基准和工业项目上表现优异，语法和语义准确性分别提升 17.24% 和 14.32%，代码安全性比基于规则的工具高 96.79%，模块级编译和测试通过率分别达到 92.25% 和 89.53%。

Conclusion: EvoC2Rust 在项目级 C 到 Rust 的转换中表现出色，综合了基于规则和 LLM 方法的优势，显著提升了语法和语义准确性以及代码安全性。

Abstract: Rust's compile-time safety guarantees make it ideal for safety-critical
systems, creating demand for translating legacy C codebases to Rust. While
various approaches have emerged for this task, they face inherent trade-offs:
rule-based solutions face challenges in meeting code safety and idiomaticity
requirements, while LLM-based solutions often fail to generate semantically
equivalent Rust code, due to the heavy dependencies of modules across the
entire codebase. Recent studies have revealed that both solutions are limited
to small-scale programs. In this paper, we propose EvoC2Rust, an automated
framework for converting entire C projects to equivalent Rust ones. EvoC2Rust
employs a skeleton-guided translation strategy for project-level translation.
The pipeline consists of three evolutionary stages: 1) it first decomposes the
C project into functional modules, employs a feature-mapping-enhanced LLM to
transform definitions and macros and generates type-checked function stubs,
which form a compilable Rust skeleton; 2) it then incrementally translates the
function, replacing the corresponding stub placeholder; 3) finally, it repairs
compilation errors by integrating LLM and static analysis. Through evolutionary
augmentation, EvoC2Rust combines the advantages of both rule-based and
LLM-based solutions. Our evaluation on open-source benchmarks and six
industrial projects demonstrates EvoC2Rust's superior performance in
project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%
improvements in syntax and semantic accuracy over the LLM-based approaches,
along with a 96.79% higher code safety rate than the rule-based tools. At the
module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates
on industrial projects, even for complex codebases and long functions.

</details>


### [198] [Vanilla-Converter: A Tool for Converting Camunda 7 BPMN Models into Camunda 8 Models](https://arxiv.org/abs/2508.04352)
*Dragana Sunaric,Charlotte Verbruggen,Dominik Bork*

Main category: cs.SE

TL;DR: Vanilla-Converter是一个自动化工具，用于简化Camunda 7到8的BPMN模型迁移，支持多种元素并生成转换日志，已通过工业案例验证。


<details>
  <summary>Details</summary>
Motivation: 由于Camunda 7和8之间存在根本性差异，手动迁移过程复杂，因此需要自动化工具来简化这一过程。

Method: 开发了一个名为Vanilla-Converter的命令行工具，支持广泛的BPMN元素，自动转换模型并生成详细的转换日志。

Result: 工具能够将Camunda 7模型转换为有效且可执行的Camunda 8模型，并通过案例研究验证了其能力。

Conclusion: Vanilla-Converter成功地将Camunda 7的BPMN模型迁移至Camunda 8，并通过三个实际工业案例验证了其有效性。

Abstract: As organizations prepare for the end-of-life of Camunda 7, manual migration
remains complex due to fundamental differences between the two platforms. We
present Vanilla-Converter, a command-line tool that facilitates the migration
of BPMN models from Camunda 7 to Camunda 8. Vanilla-Converter automates the
transformation process, supports a wide range of BPMN elements, and produces a
transformed model and a detailed transformation log indicating automatic
changes and remaining manual conversion tasks. The tool's effectiveness is
demonstrated through three case studies with real industrially used Camunda 7
models, confirming its ability to convert these models into valid and
executable Camunda 8 models.

</details>


### [199] [Breaking New Ground in Software Defect Prediction: Introducing Practical and Actionable Metrics with Superior Predictive Power for Enhanced Decision-Making](https://arxiv.org/abs/2508.04408)
*Carlos Andrés Ramírez Cataño,Makoto Itoh*

Main category: cs.SE

TL;DR: 本文提出基于开发者编码习惯的软件缺陷预测框架，其性能优于传统代码和历史指标，并提升了模型的可解释性和实用性。


<details>
  <summary>Details</summary>
Motivation: 鉴于软件缺陷的根本原因常归因于人为错误，探索基于开发者编码习惯的非软件指标可能为缺陷预测提供关键指标。

Method: 首先提出了一个决定预测指标的框架，然后比较了这些指标与现有代码和提交历史指标的性能，并分析了每个指标的预测重要性。

Result: 在21个关键基础设施大规模开源软件项目上的分析表明：(1)提出了基于人为错误的方法级缺陷预测框架；(2)所提指标的平均预测性能优于现有最佳代码和历史指标；(3)新指标的平均重要性更高；(4)新指标显著提升了模型的可解释性和实用性。

Conclusion: 本文通过开发者编码习惯提出了一种基于人为错误的框架，显著提升了软件缺陷预测模型的可解释性、实用性和可操作性，为实践者提供了可操作的预测依据。

Abstract: Software defect prediction using code metrics has been extensively researched
over the past five decades. However, prediction harnessing non-software metrics
is under-researched. Considering that the root cause of software defects is
often attributed to human error, human factors theory might offer key
forecasting metrics for actionable insights. This paper explores automated
software defect prediction at the method level based on the developers' coding
habits. First, we propose a framework for deciding the metrics to conduct
predictions. Next, we compare the performance of our metrics to that of the
code and commit history metrics shown by research to achieve the highest
performance to date. Finally, we analyze the prediction importance of each
metric. As a result of our analyses of twenty-one critical infrastructure
large-scale open-source software projects, we have presented: (1) a human
error-based framework with metrics useful for defect prediction at method
level; (2) models using our proposed metrics achieve better average prediction
performance than the state-of-the-art code metrics and history measures; (3)
the prediction importance of all metrics distributes differently with each of
the novel metrics having better average importance than code and history
metrics; (4) the novel metrics dramatically enhance the explainability,
practicality, and actionability of software defect prediction models,
significantly advancing the field. We present a systematic approach to
forecasting defect-prone software methods via a human error framework. This
work empowers practitioners to act on predictions, empirically demonstrating
how developer coding habits contribute to defects in software systems.

</details>


### [200] [Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection](https://arxiv.org/abs/2508.04448)
*Damian Gnieciak,Tomasz Szandala*

Main category: cs.SE

TL;DR: 研究比较了六种自动化代码分析工具（三种基于规则和三种LLM），发现LLM在检测漏洞方面优于传统工具，但存在假阳性率高和定位不精确的问题，建议采用混合方法结合两者的优势。


<details>
  <summary>Details</summary>
Motivation: 现代软件依赖多种自动化测试和质量保证工具来防止错误、漏洞和潜在的安全问题。本研究旨在对六种自动化方法进行头对头、定量和定性的评估。

Method: 使用十个真实世界的C#项目，嵌入63个常见类别的漏洞（如SQL注入、硬编码秘密和过时依赖），测量经典检测准确度（精确度、召回率、F分数）、分析延迟和开发者验证真阳性所需的工作量。

Result: 基于语言的扫描器（GPT-4.1、Mistral Large和DeepSeek V3）的平均F-1分数（0.797、0.753和0.750）高于基于静态规则的工具（SonarQube、CodeQL和Snyk Code，分别为0.260、0.386和0.546）。语言模型的优势源于更高的召回率，但存在更高的假阳性率和定位不精确的问题。

Conclusion: 该研究建议采用混合管道：在开发早期使用语言模型进行广泛的上下文感知分类，同时保留基于确定性规则的扫描器进行高保证验证。开放的基准和JSON格式的结果框架为下一代自动化代码安全的可重复、以实践者为中心的研究奠定了基础。

Abstract: Modern software relies on a multitude of automated testing and quality
assurance tools to prevent errors, bugs and potential vulnerabilities. This
study sets out to provide a head-to-head, quantitative and qualitative
evaluation of six automated approaches: three industry-standard rule-based
static code-analysis tools (SonarQube, CodeQL and Snyk Code) and three
state-of-the-art large language models hosted on the GitHub Models platform
(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten
real-world C# projects that embed 63 vulnerabilities across common categories
such as SQL injection, hard-coded secrets and outdated dependencies, we measure
classical detection accuracy (precision, recall, F-score), analysis latency,
and the developer effort required to vet true positives. The language-based
scanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their
static counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'
advantage originates from superior recall, confirming an ability to reason
across broader code contexts. However, this benefit comes with substantial
trade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language
models mislocate issues at line-or-column granularity due to tokenisation
artefacts. Overall, language models successfully rival traditional static
analysers in finding real vulnerabilities. Still, their noisier output and
imprecise localisation limit their standalone use in safety-critical audits. We
therefore recommend a hybrid pipeline: employ language models early in
development for broad, context-aware triage, while reserving deterministic
rule-based scanners for high-assurance verification. The open benchmark and
JSON-based result harness released with this paper lay a foundation for
reproducible, practitioner-centric research into next-generation automated code
security.

</details>


### [201] [Manifestations of Empathy in Software Engineering: How, Why, and When It Matters](https://arxiv.org/abs/2508.04479)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 该研究通过访谈和调查探讨了同理心在软件工程中的表现、动机及影响因素，并提出了将其融入软件工程过程的实用建议。


<details>
  <summary>Details</summary>
Motivation: 先前的研究强调了同理心在软件工程中的重要性，但对同理心在实践中的表现、动机及影响因素的理解有限。

Method: 通过22次访谈和一项涉及116名软件从业者的大规模调查进行探索。

Result: 研究揭示了同理心在软件工程中的表达方式、驱动因素、被认为有用或无用的活动以及其他影响因素。

Conclusion: 该研究为软件工程实践者和研究者提供了关于如何有效将同理心融入软件工程过程的实用建议。

Abstract: Empathy plays a crucial role in software engineering (SE), influencing
collaboration, communication, and decision-making. While prior research has
highlighted the importance of empathy in SE, there is limited understanding of
how empathy manifests in SE practice, what motivates SE practitioners to
demonstrate empathy, and the factors that influence empathy in SE work. Our
study explores these aspects through 22 interviews and a large scale survey
with 116 software practitioners. Our findings provide insights into the
expression of empathy in SE, the drivers behind empathetic practices, SE
activities where empathy is perceived as useful or not, and the other factors
that influence empathy. In addition, we offer practical implications for SE
practitioners and researchers, offering a deeper understanding of how to
effectively integrate empathy into SE processes.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [202] [ARMS: Adaptive and Robust Memory Tiering System](https://arxiv.org/abs/2508.04417)
*Sujay Yadalam,Konstantinos Kanellis,Michael Swift,Shivaram Venkataraman*

Main category: cs.OS

TL;DR: ARMS通过自适应热/冷页面识别和迁移策略，无需调优阈值即实现高性能内存分层，性能接近最佳调优系统且显著优于未调优系统。


<details>
  <summary>Details</summary>
Motivation: 现有内存分层解决方案（如HeMem、Memtis、TPP）使用固定策略和预配置阈值进行数据放置和迁移决策，但缺乏通用性，调优可显著提升性能。

Method: 设计了基于短期和长期移动平均的热/冷页面识别机制，采用自适应迁移策略和带宽感知批量迁移调度器。

Result: ARMS在无需调优的情况下，性能接近最佳调优现有系统的3%以内，且在未调优情况下比现有系统快1.26倍至2.3倍。

Conclusion: ARMS（自适应和鲁棒内存分层系统）通过创新的热/冷页面识别机制、基于成本/效益分析的自适应迁移策略以及带宽感知的批量迁移调度器，无需可调阈值即可提供高性能，性能接近最佳调优的现有系统，且在未调优情况下显著优于现有系统。

Abstract: Memory tiering systems seek cost-effective memory scaling by adding multiple
tiers of memory. For maximum performance, frequently accessed (hot) data must
be placed close to the host in faster tiers and infrequently accessed (cold)
data can be placed in farther slower memory tiers. Existing tiering solutions
such as HeMem, Memtis, and TPP use rigid policies with pre-configured
thresholds to make data placement and migration decisions. We perform a
thorough evaluation of the threshold choices and show that there is no single
set of thresholds that perform well for all workloads and configurations, and
that tuning can provide substantial speedups. Our evaluation identified three
primary reasons why tuning helps: better hot/cold page identification, reduced
wasteful migrations, and more timely migrations.
  Based on this study, we designed ARMS - Adaptive and Robust Memory tiering
System - to provide high performance without tunable thresholds. We develop a
novel hot/cold page identification mechanism relying on short-term and
long-term moving averages, an adaptive migration policy based on cost/benefit
analysis, and a bandwidth-aware batched migration scheduler. Combined, these
approaches provide out-of-the-box performance within 3% the best tuned
performance of prior systems, and between 1.26x-2.3x better than prior systems
without tuning.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [203] [CASH: Context-Aware Smart Handover for Reliable UAV Connectivity on Aerial Corridors](https://arxiv.org/abs/2508.03862)
*Abdul Saboor,Zhuangzhuang Cui,Achiel Colpaert,Evgenii Vinogradov,Sofie Pollin*

Main category: cs.NI

TL;DR: CASH协议通过前瞻性评分机制减少无人机切换频率78%，优化UAM通信可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决高机动性空中走廊中频繁切换导致网络性能下降的问题。

Method: 提出了一种基于无人机轨迹的前瞻性评分机制的Context-Aware Smart Handover (CASH)协议，并在自定义模拟器中评估其性能。

Result: CASH将切换频率降低了78%，同时保持了低中断概率，并通过实验确定了基站密度和安全裕度的最优配置。

Conclusion: CASH协议通过前瞻性评分机制显著减少了切换频率，同时保持了低中断概率，为UAM通信提供了可靠解决方案。

Abstract: Urban Air Mobility (UAM) envisions aerial corridors for Unmanned Aerial
Vehicles (UAVs) to reduce ground traffic congestion by supporting 3D mobility,
such as air taxis. A key challenge in these high-mobility aerial corridors is
ensuring reliable connectivity, where frequent handovers can degrade network
performance. To resolve this, we present a Context-Aware Smart Handover (CASH)
protocol that uses a forward-looking scoring mechanism based on UAV trajectory
to make proactive handover decisions. We evaluate the performance of the
proposed CASH against existing handover protocols in a custom-built simulator.
Results show that CASH reduces handover frequency by up to 78% while
maintaining low outage probability. We then investigate the impact of base
station density and safety margin on handover performance, where their optimal
setups are empirically obtained to ensure reliable UAM communication.

</details>


### [204] [Confidence Driven Classification of Application Types in the Presence of Background Network](https://arxiv.org/abs/2508.03891)
*Eun Hun Choi,Jasleen Kaur,Vladas Pipiras,Nelson Gomes Rodrigues Antunes,Brendan Massey*

Main category: cs.NI

TL;DR: 研究提出了一种基于高斯混合模型的框架，用于改进网络流量分类器的置信度，以减少背景流量导致的误分类。


<details>
  <summary>Details</summary>
Motivation: 现有的网络流量分类器在现实数据中表现不佳，主要因为忽略了非应用特定的背景流量（如广告、分析、共享API等），导致分类混淆。

Method: 设计了一个基于高斯混合模型的分类框架，用于改进深度学习分类器的置信度指示。

Result: 提出的方法通过引入背景流量类别和改进置信度测量，有效减少了误分类，提升了分类可靠性。

Conclusion: 为了解决现实网络流量中非应用特定背景流量导致的分类混淆问题，研究提出了一种基于高斯混合模型的分类框架，通过改进深度学习分类器的置信度指示，实现了更可靠的分类。

Abstract: Accurately classifying the application types of network traffic using deep
learning models has recently gained popularity. However, we find that these
classifiers do not perform well on real-world traffic data due to the presence
of non-application-specific generic background traffic originating from
advertisements, analytics, shared APIs, and trackers. Unfortunately,
state-of-the-art application classifiers overlook such traffic in curated
datasets and only classify relevant application traffic. To address this issue,
when we label and train using an additional class for background traffic, it
leads to additional confusion between application and background traffic, as
the latter is heterogeneous and encompasses all traffic that is not relevant to
the application sessions. To avoid falsely classifying background traffic as
one of the relevant application types, a reliable confidence measure is
warranted, such that we can refrain from classifying uncertain samples.
Therefore, we design a Gaussian Mixture Model-based classification framework
that improves the indication of the deep learning classifier's confidence to
allow more reliable classification.

</details>


### [205] [Enabling Site-Specific Cellular Network Simulation Through Ray-Tracing-Driven ns-3](https://arxiv.org/abs/2508.04004)
*Tanguy Ropitault,Matteo Bordin,Paolo Testolina,Michele Polese,Pedram Johari,Nada Golmie,Tommaso Melodia*

Main category: cs.NI

TL;DR: 论文提出一种基于追踪的信道模型，扩展5G-LENA模块以支持站点特定的5G/6G仿真，为数字孪生应用提供高保真研究工具。


<details>
  <summary>Details</summary>
Motivation: 现有5G和6G系统评估主要依赖统计性3GPP信道模型，无法捕捉站点特定现象（如衍射、遮挡等），限制了高保真仿真和数字孪生应用的发展。

Method: 论文扩展了5G-LENA模块，采用基于外部射线追踪器或测量活动获得的多径分量（MPCs）来构建频域信道矩阵，并与现有的PHY/MAC堆栈无缝集成。

Result: 新模块实现了站点特定的信道建模，验证了其在波束导向和端到端指标分析中的能力，揭示了统计模型无法表现的关键性能拐点。

Conclusion: 该论文通过扩展5G-LENA模块，引入基于追踪的信道模型，为5G和6G系统级仿真提供了站点特定的几何保真度，为数字孪生应用奠定了基础。

Abstract: Evaluating cellular systems, from 5G New Radio (NR) and 5G-Advanced to 6G, is
challenging because the performance emerges from the tight coupling of
propagation, beam management, scheduling, and higher-layer interactions.
System-level simulation is therefore indispensable, yet the vast majority of
studies rely on the statistical 3GPP channel models. These are well suited to
capture average behavior across many statistical realizations, but cannot
reproduce site-specific phenomena such as corner diffraction, street-canyon
blockage, or deterministic line-of-sight conditions and
angle-of-departure/arrival relationships that drive directional links. This
paper extends 5G-LENA, an NR module for the system-level Network Simulator 3
(ns-3), with a trace-based channel model that processes the Multipath
Components (MPCs) obtained from external ray-tracers (e.g., Sionna Ray Tracer
(RT)) or measurement campaigns. Our module constructs frequency-domain channel
matrices and feeds them to the existing Physical (PHY)/Medium Access Control
(MAC) stack without any further modifications. The result is a geometry-based
channel model that remains fully compatible with the standard 3GPP
implementation in 5G-LENA, while delivering site-specific geometric fidelity.
This new module provides a key building block toward Digital Twin (DT)
capabilities by offering realistic site-specific channel modeling, unlocking
studies that require site awareness, including beam management, blockage
mitigation, and environment-aware sensing. We demonstrate its capabilities for
precise beam-steering validation and end-to-end metric analysis. In both cases,
the trace-driven engine exposes performance inflections that the statistical
model does not exhibit, confirming its value for high-fidelity system-level
cellular networks research and as a step toward DT applications.

</details>


### [206] [A Novel Hierarchical Co-Optimization Framework for Coordinated Task Scheduling and Power Dispatch in Computing Power Networks](https://arxiv.org/abs/2508.04015)
*Haoxiang Luo,Kun Yang,Qi Huang,Schahram Dustdar*

Main category: cs.NI

TL;DR: TSCO框架通过两阶段优化（Benders分解和DRL）协同管理电力调度和CPN任务，显著降低碳排放和成本，减少RES削减60%以上，并保持服务质量。


<details>
  <summary>Details</summary>
Motivation: 大规模人工智能和数据密集型应用的普及推动了计算能力网络（CPN）的发展，但其高能耗带来可持续性挑战。同时，电力系统因间歇性可再生能源（RES）的高渗透而面临不稳定性问题。本文旨在通过协同优化解决这两个挑战。

Method: 该框架采用两阶段优化方法，包括日前随机机组组合（SUC）阶段和实时运行阶段。前者使用Benders分解以提高计算效率，后者结合经济调度和由深度强化学习（DRL）代理管理的自适应CPN任务调度。

Result: 在IEEE 30节点系统中进行的仿真表明，TSCO框架显著优于基线方法，减少了碳排放和运营成本，同时将RES削减降低了60%以上，并保持了计算任务的服务质量。

Conclusion: 本文提出的TSCO框架通过协同管理电力系统调度和CPN任务调度，显著降低了碳排放和运营成本，同时减少了可再生能源的削减，并保持了计算任务的高服务质量。

Abstract: The proliferation of large-scale artificial intelligence and data-intensive
applications has spurred the development of Computing Power Networks (CPNs),
which promise to deliver ubiquitous and on-demand computational resources.
However, the immense energy consumption of these networks poses a significant
sustainability challenge. Simultaneously, power grids are grappling with the
instability introduced by the high penetration of intermittent renewable energy
sources (RES). This paper addresses these dual challenges through a novel
Two-Stage Co-Optimization (TSCO) framework that synergistically manages power
system dispatch and CPN task scheduling to achieve low-carbon operations. The
framework decomposes the complex, large-scale problem into a day-ahead
stochastic unit commitment (SUC) stage and a real-time operational stage. The
former is solved using Benders decomposition for computational tractability,
while in the latter, economic dispatch of generation assets is coupled with an
adaptive CPN task scheduling managed by a Deep Reinforcement Learning (DRL)
agent. This agent makes intelligent, carbon-aware decisions by responding to
dynamic grid conditions, including real-time electricity prices and marginal
carbon intensity. Through extensive simulations on an IEEE 30-bus system
integrated with a CPN, the TSCO framework is shown to significantly outperform
baseline approaches. Results demonstrate that the proposed framework reduces
total carbon emissions and operational costs, while simultaneously decreasing
RES curtailment by more than 60% and maintaining stringent Quality of Service
(QoS) for computational tasks.

</details>


### [207] [Metaverse Framework for Wireless Systems Management](https://arxiv.org/abs/2508.04150)
*Ilias Chrysovergis,Alexandros-Apostolos A. Boulogeorgos,Theodoros A. Tsiftsis,Dusit Niyato*

Main category: cs.NI

TL;DR: 该论文提出一个整合XR、DTs、AI、IoT、区块链和6G的元宇宙框架，用于无线系统的仿真与优化。


<details>
  <summary>Details</summary>
Motivation: 为无线系统的开发和管理提供一个动态、沉浸式的平台，探索和优化未来网络环境。

Method: 框架整合了扩展现实（XR）、数字孪生（DTs）、人工智能（AI）、物联网（IoT）、区块链和6G网络解决方案，通过XR实现可视化交互，DTs实现实时监控与优化，AI生成3D内容并提升决策能力，IoT提供实时传感器数据，区块链保障安全交互，5G/6G网络提供低延迟通信基础设施。

Result: 该框架成功集成了多种核心技术，为无线系统的仿真、开发和优化提供了强大工具。

Conclusion: 该论文提出了一个全面的元宇宙框架，旨在为无线系统的仿真、模拟和交互提供支持，集成了多种先进技术，为未来网络环境的发展提供了有价值的见解。

Abstract: This article introduces a comprehensive metaverse framework, which is
designed for the simulation, emulation, and interaction with wireless systems.
The proposed framework integrates core metaverse technologies such as extended
reality (XR), digital twins (DTs), artificial intelligence (AI), internet of
things (IoT), blockchain, and advanced 6G networking solutions to create a
dynamic, immersive platform for both system development and management. By
leveraging XR, users can visualize and engage with complex systems, while DTs
enable real-time monitoring and optimization. AI generates the
three-dimensional (3D) content, enhances decision-making and system
performance, whereas IoT devices provide real-time sensor data for boosting the
simulation accuracy. Additionally, blockchain ensures secure, decentralized
interactions, and 5G/6G networks offer the necessary infrastructure for
seamless, low-latency communication. This framework serves as a robust tool for
exploring, developing, and optimizing wireless systems, aiming to provide
valuable insights into the future of networked environments.

</details>


### [208] [DSNS: The Deep Space Network Simulator](https://arxiv.org/abs/2508.04317)
*Joshua Smailes,Filip Futera,Sebastian Köhler,Simon Birnbach,Martin Strohmeier,Ivan Martinovic*

Main category: cs.NI

TL;DR: DSNS是一个专注于大规模卫星网络的新型模拟器，优于现有工具，加速了协议开发和测试。


<details>
  <summary>Details</summary>
Motivation: 现有卫星和网络模拟工具在大规模卫星网络和星际互联网背景下变得不实用，阻碍了研究和创新。

Method: 开发了Deep Space Network Simulator (DSNS)，实现了现有协议和CCSDS推荐的DTN模拟参考场景，并评估了其可扩展性和保真度。

Result: DSNS在可扩展性和保真度方面优于现有工具，展示了其灵活性和可扩展性。

Conclusion: DSNS通过提供高保真度和可扩展性，加速了卫星网络协议开发和测试的迭代过程，为标准和卫星运营商提供了实际价值。

Abstract: Simulation tools are commonly used in the development and testing of new
protocols or new networks. However, as satellite networks start to grow to
encompass thousands of nodes, and as companies and space agencies begin to
realize the interplanetary internet, existing satellite and network simulation
tools have become impractical for use in this context.
  We therefore present the Deep Space Network Simulator (DSNS): a new network
simulator with a focus on large-scale satellite networks. We demonstrate its
improved capabilities compared to existing offerings, showcase its flexibility
and extensibility through an implementation of existing protocols and the DTN
simulation reference scenarios recommended by CCSDS, and evaluate its
scalability, showing that it exceeds existing tools while providing better
fidelity.
  DSNS provides concrete usefulness to both standards bodies and satellite
operators, enabling fast iteration on protocol development and testing of
parameters under highly realistic conditions. By removing roadblocks to
research and innovation, we can accelerate the development of upcoming
satellite networks and ensure that their communication is both fast and secure.

</details>


### [209] [Empowering Nanoscale Connectivity through Molecular Communication: A Case Study of Virus Infection](https://arxiv.org/abs/2508.04415)
*Xuan Chen,Yu Huang,Miaowen Wen,Shahid Mumtaz,Fatih Gulec,Anwer Al-Dulaimi,Andrew W. Eckford*

Main category: cs.NI

TL;DR: 本文探讨了分子通信在生物纳米物联网中用于流行病防控的潜力，提出了病毒传播模型、检测方法及突变识别策略，并通过模拟验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索分子通信（MC）在构建生物纳米物联网（IoBNT）以应对流行病防控挑战中的潜力，特别是在病毒传播建模、病毒/感染个体检测及病毒突变识别方面。

Method: 讨论了宏观和微观尺度下的MC通道以匹配病毒传播，研究了这两个尺度的检测方法，设计了病毒/感染个体的定位机制，并提出了一种识别病毒突变的策略，通过ORF3a蛋白作为基准进行模拟验证。

Result: 提出了针对病毒传播的MC模型、检测方法和定位机制，并通过模拟验证了病毒突变识别策略的有效性。

Conclusion: 本文总结了通过分子通信（MC）分析病毒传播，并利用MC中的信号处理技术对抗病毒传播的方法，同时提出了未来研究的开放性问题。

Abstract: The Internet of Bio-Nano Things (IoBNT), envisioned as a revolutionary
healthcare paradigm, shows promise for epidemic control. This paper explores
the potential of using molecular communication (MC) to address the challenges
in constructing IoBNT for epidemic prevention, specifically focusing on
modeling viral transmission, detecting the virus/infected individuals, and
identifying virus mutations. First, the MC channels in macroscale and
microscale scenarios are discussed to match viral transmission in both scales
separately. Besides, the detection methods for these two scales are also
studied, along with the localization mechanism designed for the virus/infected
individuals. Moreover, an identification strategy is proposed to determine
potential virus mutations, which is validated through simulation using the
ORF3a protein as a benchmark. Finally, open research issues are discussed. In
summary, this paper aims to analyze viral transmission through MC and combat
viral spread using signal processing techniques within MC.

</details>


### [210] [Policy Design in Zero-Trust Distributed Networks: Challenges and Solutions](https://arxiv.org/abs/2508.04526)
*Fannya R. Sandjaja,Ayesha A. Majeed,Abdullah Abdullah,Gyan Wickremasinghe,Karen Rafferty,Vishal Sharma*

Main category: cs.NI

TL;DR: 本文探讨了零信任分布式网络（ZTDN）策略设计的挑战，提出使用UPPAAL进行形式化验证，并强调了问责与责任在安全中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 传统安全架构在分布式攻击面前日益脆弱，尤其是在引入自主AI后，系统组件需要在分布式空间中更安全地管理。零信任架构（ZTA）虽为潜在解决方案，但其策略若未经验证可能导致未授权访问。因此，本文旨在探索ZTA策略设计的挑战与解决方案。

Method: 本文通过案例研究，使用UPPAAL工具对零信任分布式网络（ZTDN）的策略进行形式化验证，并分析了策略设计的挑战与解决方案。

Result: 研究发现，通过形式化验证工具（如UPPAAL）可以有效验证ZTDN策略的正确性，同时强调了问责与责任在系统安全中的重要性。

Conclusion: 本文强调了在分布式网络中设计零信任架构（ZTA）策略的重要性，并提出了通过形式化验证工具（如UPPAAL）来确保策略的正确性。同时，讨论了在系统安全中问责和责任的关键作用。

Abstract: Traditional security architectures are becoming more vulnerable to
distributed attacks due to significant dependence on trust. This will further
escalate when implementing agentic AI within the systems, as more components
must be secured over a similar distributed space. These scenarios can be
observed in consumer technologies, such as the dense Internet of things (IoT).
Here, zero-trust architecture (ZTA) can be seen as a potential solution, which
relies on a key principle of not giving users explicit trust, instead always
verifying their privileges whenever a request is made. However, the overall
security in ZTA is managed through its policies, and unverified policies can
lead to unauthorized access. Thus, this paper explores challenges and solutions
for ZTA policy design in the context of distributed networks, which is referred
to as zero-trust distributed networks (ZTDN). This is followed by a case-study
on formal verification of policies using UPPAAL. Subsequently, the importance
of accountability and responsibility in the system's security is discussed.

</details>


### [211] [CONVERGE: A Multi-Agent Vision-Radio Architecture for xApps](https://arxiv.org/abs/2508.04556)
*Filipe B. Teixeira,Carolina Simões,Paulo Fidalgo,Wagner Pedrosa,André Coelho,Manuel Ricardo,Luis M. Pessoa*

Main category: cs.NI

TL;DR: 论文提出了一种多智能体架构，集成实时无线电和视频感知信息，实验证明其能有效控制5G/6G RAN，延迟低于1毫秒。


<details>
  <summary>Details</summary>
Motivation: 随着高频无线链路的发展，视觉数据可以预测信道动态并帮助克服障碍，因此需要一种新的架构来集成无线电和视频感知信息。

Method: 采用多智能体方法，设计了一个新的视频功能，能够为xApps生成阻塞信息，实现集成感知与通信。

Result: 实验结果表明，感知信息的延迟保持在1毫秒以下，xApp能够成功利用无线电和视频感知信息实时控制5G/6G RAN。

Conclusion: 该论文提出的新型架构成功实现了实时无线电和视频感知信息的传输，并通过实验验证了其在5G/6G RAN实时控制中的有效性。

Abstract: Telecommunications and computer vision have evolved independently. With the
emergence of high-frequency wireless links operating mostly in line-of-sight,
visual data can help predict the channel dynamics by detecting obstacles and
help overcoming them through beamforming or handover techniques.
  This paper proposes a novel architecture for delivering real-time radio and
video sensing information to O-RAN xApps through a multi-agent approach, and
introduces a new video function capable of generating blockage information for
xApps, enabling Integrated Sensing and Communications. Experimental results
show that the delay of sensing information remains under 1\,ms and that an xApp
can successfully use radio and video sensing information to control the 5G/6G
RAN in real-time.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [212] [FlashCommunication V2: Bit Splitting and Spike Reserving for Any Bit Communication](https://arxiv.org/abs/2508.03760)
*Qingyuan Li,Bo Zhang,Hui Kang,Tianhao Xu,Yulei Qian,Yuchen Xie,Lin Ma*

Main category: cs.DC

TL;DR: FlashCommunication V2通过位分割和峰值保留技术，解决了低比特量化挑战，提升了通信效率，实现了显著的性能加速。


<details>
  <summary>Details</summary>
Motivation: 当前分布式训练和大语言模型部署中，通信瓶颈成为关键挑战，需要一种能够高效跨GPU传输的通信范式。

Method: 提出了位分割（bit splitting）和峰值保留（spike reserving）技术，前者将不规则位宽分解为基础单元，后者保留数值异常值为浮点数以缩小动态范围。

Result: 在NVLink和PCIe架构下，AllReduce和All2All通信分别实现了最高3.2倍和2倍的加速。

Conclusion: FlashCommunication V2通过创新的位分割和峰值保留技术，显著提升了通信系统的灵活性和资源利用率，在多种架构下实现了性能提升和开销减少。

Abstract: Nowadays, communication bottlenecks have emerged as a critical challenge in
the distributed training and deployment of large language models (LLMs). This
paper introduces FlashCommunication V2, a novel communication paradigm enabling
efficient cross-GPU transmission at arbitrary bit widths. Its core innovations
lie in the proposed bit splitting and spike reserving techniques, which address
the challenges of low-bit quantization. Bit splitting decomposes irregular bit
widths into basic units, ensuring compatibility with hardware capabilities and
thus enabling transmission at any bit width. Spike reserving, on the other
hand, retains numerical outliers (i.e., minima and maxima) as floating-point
numbers, which shrinks the dynamic numerical range and pushes the quantization
limits to 2-bit with acceptable losses. FlashCommunication V2 significantly
enhances the flexibility and resource utilization of communication systems.
Through meticulous software-hardware co-design, it delivers robust performance
and reduced overhead across both NVLink-based and PCIe-based architectures,
achieving a maximum 3.2$\times$ speedup in AllReduce and 2$\times$ in All2All
communication.

</details>


### [213] [Two-dimensional Sparse Parallelism for Large Scale Deep Learning Recommendation Model Training](https://arxiv.org/abs/2508.03854)
*Xin Zhang,Quanyu Zhu,Liangbei Xu,Zain Huda,Wang Zhou,Jin Fang,Dennis van der Staay,Yuxi Hu,Jade Nie,Jiyan Yang,Chunzhi Yang*

Main category: cs.DC

TL;DR: 针对DLRM的稀疏嵌入表问题，提出二维稀疏并行方法，结合数据并行和模型并行，显著提升训练效率并在4K GPUs上实现线性扩展。


<details>
  <summary>Details</summary>
Motivation: 深度学习推荐模型（DLRM）的稀疏嵌入表在处理大规模数据时面临内存限制、扩展性挑战等问题，传统全并行策略存在不平衡、滞后等问题。

Method: 提出了一种新颖的二维稀疏并行方法，结合了数据并行和模型并行，并开发了动量缩放的行级AdaGrad算法以减少性能损失。

Result: 实验证明该方法显著提升了训练效率，同时保持了模型性能的同等水平。

Conclusion: 提出的二维稀疏并行方法显著提升了训练效率，同时在4K GPUs上实现了近乎线性的训练速度扩展，为推荐模型训练设立了新的最先进基准。

Abstract: The increasing complexity of deep learning recommendation models (DLRM) has
led to a growing need for large-scale distributed systems that can efficiently
train vast amounts of data. In DLRM, the sparse embedding table is a crucial
component for managing sparse categorical features. Typically, these tables in
industrial DLRMs contain trillions of parameters, necessitating model
parallelism strategies to address memory constraints. However, as training
systems expand with massive GPUs, the traditional fully parallelism strategies
for embedding table post significant scalability challenges, including
imbalance and straggler issues, intensive lookup communication, and heavy
embedding activation memory. To overcome these limitations, we propose a novel
two-dimensional sparse parallelism approach. Rather than fully sharding tables
across all GPUs, our solution introduces data parallelism on top of model
parallelism. This enables efficient all-to-all communication and reduces peak
memory consumption. Additionally, we have developed the momentum-scaled
row-wise AdaGrad algorithm to mitigate performance losses associated with the
shift in training paradigms. Our extensive experiments demonstrate that the
proposed approach significantly enhances training efficiency while maintaining
model performance parity. It achieves nearly linear training speed scaling up
to 4K GPUs, setting a new state-of-the-art benchmark for recommendation model
training.

</details>


### [214] [Reputation-based partition scheme for IoT security](https://arxiv.org/abs/2508.03981)
*Zhikui Chen,Muhammad Zeeshan Haider,Naiwen Luo,Shuo Yu,Xu Yuan,Yaochen Zhang,Tayyaba Noreen*

Main category: cs.DC

TL;DR: RSPC是一种基于信誉的分区方案，通过优化分区管理和跨分区交易协议，提升了群智感知的安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 集中式管理的群智感知平台存在安全漏洞和可扩展性问题，需要一种有效的分区方案来解决这些问题。

Method: 提出了一种基于信誉的分区方案（RSPC），通过计算最优分区大小并结合节点信誉值进行分区管理，同时设计了四阶段确认协议处理跨分区交易。

Result: 实验表明，RSPC在可扩展性、低延迟和高吞吐量方面表现优异。

Conclusion: RSPC方案通过结合节点信誉值和分区管理，有效提升了群智感知的可扩展性、低延迟和高吞吐量。

Abstract: With the popularity of smart terminals, such as the Internet of Things,
crowdsensing is an emerging data aggregation paradigm, which plays a pivotal
role in data-driven applications. There are some key issues in the development
of crowdsensing such as platform security and privacy protection. As the
crowdsensing is usually managed by a centralized platform, centralized
management will bring various security vulnerabilities and scalability issues.
To solve these issues, an effective reputation-based partition scheme (RSPC) is
proposed in this article. The partition scheme calculates the optimal partition
size by combining the node reputation value and divides the node into several
disjoint partitions according to the node reputation value. By selecting the
appropriate partition size, RSPC provides a mechanism to ensure that each
partition is valid, as long as themaximum permissible threshold for the failed
node is observed. At the same time, the RSPC reorganizes the network
periodically to avoid partition attacks. In addition, for cross-partition
transactions, this paper innovatively proposes a four-stage confirmation
protocol to ensure the efficient and safe completion of cross-partition
transactions. Finally, experiments show that RSPC improves scalability, low
latency, and high throughput for crowdsensing.

</details>


### [215] [High-Performance and Power-Efficient Emulation of Matrix Multiplication using INT8 Matrix Engines](https://arxiv.org/abs/2508.03984)
*Yuki Uchino,Katsuhisa Ozaki,Toshiyuki Imamura*

Main category: cs.DC

TL;DR: 利用低精度矩阵引擎模拟高精度矩阵乘法，显著提升性能和能效。


<details>
  <summary>Details</summary>
Motivation: 由于低精度矩阵乘法在深度学习中的重要性，研究如何利用低精度矩阵引擎高效模拟高精度矩阵乘法。

Method: 通过利用低精度矩阵引擎，提出了模拟单精度和双精度通用矩阵乘法（SGEMM和DGEMM）的方法。

Result: 在GH200 Grace Hopper超级芯片上，提出的DGEMM模拟比原生DGEMM在大型问题上实现了1.4倍加速和43%的能效提升；SGEMM模拟实现了3.0倍加速和154%的能效提升。与传统模拟方法相比，性能提升超过2倍且能效更优。

Conclusion: 本研究提出的模拟方法在性能和能效上显著优于传统方法，为深度学习中的矩阵乘法提供了更高效的解决方案。

Abstract: Recent architectures integrate high-performance and power-efficient matrix
engines. These engines demonstrate remarkable performance in low-precision
matrix multiplication, which is crucial in deep learning. Several techniques
have been proposed to emulate single- and double-precision general
matrix-matrix multiplication (SGEMM and DGEMM, respectively) by leveraging such
low-precision matrix engines. In this study, we present emulation methods that
significantly outperforms conventional approaches. On a GH200 Grace Hopper
Superchip, the proposed DGEMM emulation achieves a 1.4x speedup and a 43\%
improvement in power efficiency compared to native DGEMM for sufficiently large
problems. The proposed SGEMM emulation achieves a 3.0x speedup and a 154\%
improvement in power efficiency compared to native SGEMM for sufficiently large
problems. Furthermore, compared to conventional emulation methods, the proposed
emulation achieves more than 2x higher performance and superior power
efficiency.

</details>


### [216] [Advanced DAG-Based Ranking (ADR) Protocol for Blockchain Scalability](https://arxiv.org/abs/2508.04000)
*Tayyaba Noreen,Qiufen Xia,Muhammad Zeeshan Haider*

Main category: cs.DC

TL;DR: 本文提出ADR协议，通过DAG结构和排名算法解决区块链的可扩展性和吞吐量问题，实验证明其性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前区块链系统存在吞吐量低、可扩展性差和高延迟的问题，限制了其在物联网等领域的应用。

Method: ADR协议采用三步法：节点验证、构建高级DAG账本和排名算法，以增强安全性和性能。

Result: 在Amazon EC2集群上的实验表明，ADR相比IOTA和ByteBall等现有DAG区块链，显著提升了交易吞吐量和网络活跃度。

Conclusion: ADR协议通过DAG结构和排名算法显著提高了区块链的吞吐量和可扩展性，适用于物联网应用。

Abstract: In the past decade, blockchain has emerged as a promising solution for
building secure distributed ledgers and has attracted significant attention.
However, current blockchain systems suffer from limited throughput, poor
scalability, and high latency. Due to limitations in consensus mechanisms,
especially in managing node identities, blockchain is often considered
unsuitable for applications such as the Internet of Things (IoT). This paper
proposes the Advanced DAG-based Ranking (ADR) protocol to enhance blockchain
scalability and throughput. ADR employs a directed acyclic graph (DAG)
structure where nodes are positioned based on their rankings. Unlike
traditional chains, ADR allows honest nodes to write blocks and verify
transactions using a DAG-based topology. The protocol follows a three-step
approach to secure the network against double-spending and enhance performance.
First, it verifies nodes using their public and private keys before granting
entry. Second, it builds an advanced DAG ledger enabling block production and
transaction validation. Third, a ranking algorithm filters out malicious nodes,
ranks the remaining nodes based on performance, and arranges them
topologically. This process increases throughput and ensures robust
scalability. We evaluated ADR on Amazon EC2 clusters with over 100 nodes,
including scenarios with injected malicious nodes. Simulation results
demonstrate that ADR significantly improves transaction throughput and network
liveness compared to existing DAG-based blockchains such as IOTA and ByteBall,
making it well-suited for IoT applications.

</details>


### [217] [High-Performance Statistical Computing (HPSC): Challenges, Opportunities, and Future Directions](https://arxiv.org/abs/2508.04013)
*Sameh Abdulah,Mary Lai O. Salvana,Ying Sun,David E. Keyes,Marc G. Genton*

Main category: cs.DC

TL;DR: 本文探讨了统计计算（SC）在高性能计算（HPC）领域的缺席问题，提出了通过社区和技术创新结合SC与HPC的愿景，并给出了发展HPSC社区的路线图。


<details>
  <summary>Details</summary>
Motivation: 统计计算（SC）社区开发的软件被广泛使用，但在高性能计算（HPC）领域（如Top500或Green500列表中的平台）中几乎缺席。本文旨在探讨如何弥合这一差距，推动统计方法与现代HPC技术的结合。

Method: 本文通过回顾统计计算（SC）的历史，分析其在现代HPC环境中的潜力与挑战，提出了一个愿景和路线图。

Result: 本文概述了HPSC社区的愿景、挑战和机遇，提出了一个促进HPSC社区发展的可能路线图。

Conclusion: 本文提出了一个可能的路线图，旨在促进高性能统计计算（HPSC）社区的繁荣发展，强调了统计计算（SC）和高性能计算（HPC）社区之间的紧密联系对推动快速、可扩展统计应用的重要性。

Abstract: We recognize the emergence of a statistical computing community focused on
working with large computing platforms and producing software and applications
that exemplify high-performance statistical computing (HPSC). The statistical
computing (SC) community develops software that is widely used across
disciplines. However, it remains largely absent from the high-performance
computing (HPC) landscape, particularly on platforms such as those featured on
the Top500 or Green500 lists. Many disciplines already participate in HPC,
mostly centered around simulation science, although data-focused efforts under
the artificial intelligence (AI) label are gaining popularity. Bridging this
gap requires both community adaptation and technical innovation to align
statistical methods with modern HPC technologies. We can accelerate progress in
fast and scalable statistical applications by building strong connections
between the SC and HPC communities. We present a brief history of SC, a vision
for how its strengths can contribute to statistical science in the HPC
environment (such as HPSC), the challenges that remain, and the opportunities
currently available, culminating in a possible roadmap toward a thriving HPSC
community.

</details>


### [218] [SelectiveShield: Lightweight Hybrid Defense Against Gradient Leakage in Federated Learning](https://arxiv.org/abs/2508.04265)
*Borui Li,Li Yan,Jianmin Liu*

Main category: cs.DC

TL;DR: SelectiveShield 是一个轻量级混合防御框架，通过选择性加密和差分隐私，有效保护联邦学习中的敏感数据，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在去中心化数据上的协作模型训练易受梯度泄漏攻击，现有防御机制（如差分隐私和同态加密）在异构环境中存在隐私、模型效用和系统开销之间的权衡问题。

Method: SelectiveShield 利用 Fisher 信息量化参数敏感性，通过协作协商协议确定需要保护的敏感参数，并结合自适应差分隐私噪声保护非关键参数。

Result: 实验表明，SelectiveShield 在保持强大模型效用的同时，显著降低了梯度泄漏风险。

Conclusion: SelectiveShield 是一个轻量级的混合防御框架，通过选择性同态加密和差分隐私的自适应集成，有效平衡了隐私保护、模型效用和系统开销，适用于现实世界的联邦学习部署。

Abstract: Federated Learning (FL) enables collaborative model training on decentralized
data but remains vulnerable to gradient leakage attacks that can reconstruct
sensitive user information. Existing defense mechanisms, such as differential
privacy (DP) and homomorphic encryption (HE), often introduce a trade-off
between privacy, model utility, and system overhead, a challenge that is
exacerbated in heterogeneous environments with non-IID data and varying client
capabilities. To address these limitations, we propose SelectiveShield, a
lightweight hybrid defense framework that adaptively integrates selective
homomorphic encryption and differential privacy. SelectiveShield leverages
Fisher information to quantify parameter sensitivity, allowing clients to
identify critical parameters locally. Through a collaborative negotiation
protocol, clients agree on a shared set of the most sensitive parameters for
protection via homomorphic encryption. Parameters that are uniquely important
to individual clients are retained locally, fostering personalization, while
non-critical parameters are protected with adaptive differential privacy noise.
Extensive experiments demonstrate that SelectiveShield maintains strong model
utility while significantly mitigating gradient leakage risks, offering a
practical and scalable defense mechanism for real-world federated learning
deployments.

</details>


### [219] [S2M3: Split-and-Share Multi-Modal Models for Distributed Multi-Task Inference on the Edge](https://arxiv.org/abs/2508.04271)
*JinYi Yoon,JiHo Lee,Ting He,Nakjung Choi,Bo Ji*

Main category: cs.DC

TL;DR: S2M3是一种边缘设备上的多模态多任务推理架构，通过模块拆分和共享显著降低资源使用和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决多模态AI服务在云端的带宽、延迟、隐私等问题，以及边缘设备上多任务支持的资源挑战。

Method: 提出S2M3，一种分割与共享的多模态架构，通过功能级模块拆分和共享来减少资源使用，并采用贪心模块级放置和每请求并行路由策略解决跨模型依赖。

Result: 实验表明，S2M3在单任务和多任务设置下分别减少内存使用高达50%和62%，推理延迟降低高达56.9%，且93.7%的情况下实现最优模块放置。

Conclusion: S2M3通过功能级模块拆分和共享，显著降低了边缘设备上的资源使用，同时保持了准确性，并在资源受限设备上实现了低延迟推理。

Abstract: With the advancement of Artificial Intelligence (AI) towards multiple
modalities (language, vision, speech, etc.), multi-modal models have
increasingly been used across various applications (e.g., visual question
answering or image generation/captioning). Despite the success of AI as a
service for multi-modal applications, it relies heavily on clouds, which are
constrained by bandwidth, latency, privacy concerns, and unavailability under
network or server failures. While on-device AI becomes popular, supporting
multiple tasks on edge devices imposes significant resource challenges. To
address this, we introduce S2M3, a split-and-share multi-modal architecture for
multi-task inference on edge devices. Inspired by the general-purpose nature of
multi-modal models, which are composed of multiple modules (encoder, decoder,
classifier, etc.), we propose to split multi-modal models at functional-level
modules; and then share common modules to reuse them across tasks, thereby
reducing resource usage. To address cross-model dependency arising from module
sharing, we propose a greedy module-level placement with per-request parallel
routing by prioritizing compute-intensive modules. Through experiments on a
testbed consisting of 14 multi-modal models across 5 tasks and 10 benchmarks,
we demonstrate that S2M3 can reduce memory usage by up to 50% and 62% in
single-task and multi-task settings, respectively, without sacrificing
accuracy. Furthermore, S2M3 achieves optimal placement in 89 out of 95
instances (93.7%) while reducing inference latency by up to 56.9% on
resource-constrained devices, compared to cloud AI.

</details>


### [220] [Optimizing Microgrid Composition for Sustainable Data Centers](https://arxiv.org/abs/2508.04284)
*Julius Irion,Philipp Wiesner,Jonathan Bader,Odej Kao*

Main category: cs.DC

TL;DR: 论文提出优化框架，结合Vessim和NREL的SAM模型，评估数据中心微电网的可持续性和可靠性，帮助规划决策。


<details>
  <summary>Details</summary>
Motivation: 随着计算能源需求增长和电网基础设施滞后，数据中心越来越多地采用微电网整合可再生能源和储能，但缺乏评估其长期可持续性和可靠性的工具。

Method: 扩展了计算和能源系统协同模拟器Vessim，结合NREL的SAM详细可再生能源生成模型，采用多视野黑盒优化方法。

Result: 开发了一个框架，模拟计算负载、现场可再生能源生产和储能之间的相互作用，捕捉运营和体现排放，探索高效的微电网组成。

Conclusion: 该论文提出了一个新颖的优化框架，用于评估数据中心微电网组件的长期可持续性和电力可靠性，帮助运营商做出更明智的能源系统规划决策。

Abstract: As computing energy demand continues to grow and electrical grid
infrastructure struggles to keep pace, an increasing number of data centers are
being planned with colocated microgrids that integrate on-site renewable
generation and energy storage. However, while existing research has examined
the tradeoffs between operational and embodied carbon emissions in the context
of renewable energy certificates, there is a lack of tools to assess how the
sizing and composition of microgrid components affects long-term sustainability
and power reliability.
  In this paper, we present a novel optimization framework that extends the
computing and energy system co-simulator Vessim with detailed renewable energy
generation models from the National Renewable Energy Laboratory's (NREL) System
Advisor Model (SAM). Our framework simulates the interaction between computing
workloads, on-site renewable production, and energy storage, capturing both
operational and embodied emissions. We use a multi-horizon black-box
optimization to explore efficient microgrid compositions and enable operators
to make more informed decisions when planning energy systems for data centers.

</details>


### [221] [Edge-assisted Parallel Uncertain Skyline Processing for Low-latency IoE Analysis](https://arxiv.org/abs/2508.04596)
*Chuan-Chi Lai,Yan-Lin Chen,Bo-Xin Liu,Chuan-Ming Liu*

Main category: cs.DC

TL;DR: EPUS算法通过边缘计算预处理IoE数据，利用skyline候选集剪枝减少传输量，显著降低延迟（二维＞50%），高维场景表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着IoE数据量激增，传统云计算环境中全数据传输至云端分析的成本和资源需求过高，边缘计算通过本地预处理可缓解此问题。

Method: 利用skyline候选集概念在并行边缘计算节点上剪枝不太可能成为skyline的数据，仅将必要信息发送至服务器更新全局skyline。

Result: 仿真结果表明，EPUS算法在二维数据上降低50%以上延迟，且在高维数据场景下优于其他现有方法。

Conclusion: 提出的EPUS算法在边缘计算环境中有效减少了数据传输量和云端计算资源需求，显著降低了处理延迟，特别是在高维数据场景下表现优于现有方法。

Abstract: Due to the Internet of Everything (IoE), data generated in our life become
larger. As a result, we need more effort to analyze the data and extract
valuable information. In the cloud computing environment, all data analysis is
done in the cloud, and the client only needs less computing power to handle
some simple tasks. However, with the rapid increase in data volume, sending all
data to the cloud via the Internet has become more expensive. The required
cloud computing resources have also become larger. To solve this problem, edge
computing is proposed. Edge is granted with more computation power to process
data before sending it to the cloud. Therefore, the data transmitted over the
Internet and the computing resources required by the cloud can be effectively
reduced. In this work, we proposed an Edge-assisted Parallel Uncertain Skyline
(EPUS) algorithm for emerging low-latency IoE analytic applications. We use the
concept of skyline candidate set to prune data that are less likely to become
the skyline data on the parallel edge computing nodes. With the candidate
skyline set, each edge computing node only sends the information required to
the server for updating the global skyline, which reduces the amount of data
that transfer over the internet. According to the simulation results, the
proposed method is better than two comparative methods, which reduces the
latency of processing two-dimensional data by more than 50%. For
high-dimensional data, the proposed EPUS method also outperforms the other
existing methods.

</details>


### [222] [Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in Cloud Computing](https://arxiv.org/abs/2508.04334)
*Noor Islam S. Mohammad*

Main category: cs.DC

TL;DR: 提出了一种结合深度RL和ACO的混合调度算法，显著提升了物联网云平台的性能指标。


<details>
  <summary>Details</summary>
Motivation: 物联网设备的快速增长产生了大量异构数据流，现有调度方法难以适应物联网云系统中的动态负载和网络变化。

Method: 论文提出了一种结合深度强化学习（RL）和蚁群优化（ACO）的混合调度算法。深度RL采用无模型策略梯度方法学习自适应任务分配策略，而ACO进行全局组合搜索以优化资源分配。

Result: 实验表明，该方法在大型合成物联网数据集上实现了18.4%的平均响应时间降低、12.7%的资源利用率提升和9.3%的能耗减少，同时确保了SLA合规性。

Conclusion: 该论文提出的结合深度强化学习和蚁群优化的混合调度算法，在满足SLA的同时显著提升了响应时间、资源利用率和能源效率，为下一代物联网云平台提供了一种有前景的解决方案。

Abstract: The rapid growth of Internet of Things (IoT) devices produces massive,
heterogeneous data streams, demanding scalable and efficient scheduling in
cloud environments to meet latency, energy, and Quality-of-Service (QoS)
requirements. Existing scheduling methods often lack adaptability to dynamic
workloads and network variability inherent in IoT-cloud systems. This paper
presents a novel hybrid scheduling algorithm combining deep Reinforcement
Learning (RL) and Ant Colony Optimization (ACO) to address these challenges.
The deep RL agent utilizes a model-free policy-gradient approach to learn
adaptive task allocation policies responsive to real-time workload fluctuations
and network states. Simultaneously, the ACO metaheuristic conducts a global
combinatorial search to optimize resource distribution, mitigate congestion,
and balance load across distributed cloud nodes. Extensive experiments on
large-scale synthetic IoT datasets, reflecting diverse workloads and QoS
constraints, demonstrate that the proposed method achieves up to 18.4%
reduction in average response time, 12.7% improvement in resource utilization,
and 9.3% decrease in energy consumption compared to leading heuristics and
RL-only baselines. Moreover, the algorithm ensures strict Service Level
Agreement (SLA) compliance through deadline-aware scheduling and dynamic
prioritization. The results confirm the effectiveness of integrating model-free
RL with swarm intelligence for scalable, energy-efficient IoT data scheduling,
offering a promising approach for next-generation IoT-cloud platforms.

</details>
