<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.RO](#cs.RO) [Total: 25]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/abs/2508.05689)
*Jinjia Peng,Zeze Tao,Huibing Wang,Meng Wang,Yang Wang*

Main category: cs.CV

TL;DR: ResPA是一种新型对抗攻击方法，通过优化扰动方向提升对抗样本的可迁移性，实验证明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于迁移的攻击方法忽视了扰动方向的影响，导致对抗样本的可迁移性有限。为了解决这一问题，研究提出了ResPA方法，旨在通过优化扰动方向提升对抗样本的可迁移性。

Method: 提出了一种名为ResPA的新型攻击方法，利用残差梯度作为扰动方向，引导对抗样本朝向损失函数的平坦区域。具体包括对输入梯度进行指数移动平均以获取参考梯度，并考虑当前梯度与参考梯度之间的残差来捕捉全局扰动方向的变化。

Result: 实验结果表明，ResPA的可迁移性优于现有的典型迁移攻击方法，且与现有输入变换方法结合后效果进一步提升。

Conclusion: ResPA方法通过考虑梯度变化和全局扰动方向，显著提升了对抗样本的可迁移性，且与现有输入变换方法结合后效果更佳。

Abstract: Deep neural networks are susceptible to adversarial examples while suffering
from incorrect predictions via imperceptible perturbations. Transfer-based
attacks create adversarial examples for surrogate models and transfer these
examples to target models under black-box scenarios. Recent studies reveal that
adversarial examples in flat loss landscapes exhibit superior transferability
to alleviate overfitting on surrogate models. However, the prior arts overlook
the influence of perturbation directions, resulting in limited transferability.
In this paper, we propose a novel attack method, named Residual Perturbation
Attack (ResPA), relying on the residual gradient as the perturbation direction
to guide the adversarial examples toward the flat regions of the loss function.
Specifically, ResPA conducts an exponential moving average on the input
gradients to obtain the first moment as the reference gradient, which
encompasses the direction of historical gradients. Instead of heavily relying
on the local flatness that stems from the current gradients as the perturbation
direction, ResPA further considers the residual between the current gradient
and the reference gradient to capture the changes in the global perturbation
direction. The experimental results demonstrate the better transferability of
ResPA than the existing typical transfer-based attack methods, while the
transferability can be further improved by combining ResPA with the current
input transformation methods. The code is available at
https://github.com/ZezeTao/ResPA.

</details>


### [2] [Generalized Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2508.05732)
*Pinxuan Li,Bing Cao,Changqing Zhang,Qinghua Hu*

Main category: cs.CV

TL;DR: 本文提出GOOD框架和KDE机制，通过通用知识模型提升少样本OOD检测的泛化能力，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有少样本OOD检测方法因泛化能力不足，在开放世界中的表现不一致。本文旨在通过引入通用知识模型和理论推导的GS平衡，解决这一问题。

Method: 提出了一个通用的少样本OOD检测框架（GOOD），通过辅助的通用知识模型（GKM）增强模型的通用知识，而非直接从少量数据中学习。采用知识动态嵌入（KDE）机制自适应调整通用知识的引导。

Result: 在真实世界的OOD基准测试中，GOOD框架表现出优越性能，验证了其有效性。

Conclusion: 本文提出的GOOD框架和KDE机制通过引入通用知识模型（GKM）和动态嵌入机制，显著提升了少样本OOD检测的泛化能力，实验验证了其优越性。

Abstract: Few-shot Out-of-Distribution (OOD) detection has emerged as a critical
research direction in machine learning for practical deployment. Most existing
Few-shot OOD detection methods suffer from insufficient generalization
capability for the open world. Due to the few-shot learning paradigm, the OOD
detection ability is often overfit to the limited training data itself, thus
degrading the performance on generalized data and performing inconsistently
across different scenarios. To address this challenge, we proposed a
Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general
knowledge of the OOD detection model with an auxiliary General Knowledge Model
(GKM), instead of directly learning from few-shot data. We proceed to reveal
the few-shot OOD detection from a generalization perspective and theoretically
derive the Generality-Specificity balance (GS-balance) for OOD detection, which
provably reduces the upper bound of generalization error with a general
knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)
mechanism to adaptively modulate the guidance of general knowledge. KDE
dynamically aligns the output distributions of the OOD detection model to the
general knowledge model based on the Generalized Belief (G-Belief) of GKM,
thereby boosting the GS-balance. Experiments on real-world OOD benchmarks
demonstrate our superiority. Codes will be available.

</details>


### [3] [HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing](https://arxiv.org/abs/2508.05899)
*Zixuan Bian,Ruohan Ren,Yue Yang,Chris Callison-Burch*

Main category: cs.CV

TL;DR: HOLODECK 2.0是一个文本驱动的3D场景生成框架，支持多样风格和交互式编辑，显著提升生成效率和质量。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景设计依赖大量人工，现有自动化方法难以生成开放域场景或支持灵活编辑。因此，直接从文本生成3D世界成为研究热点。

Method: HOLODECK 2.0 利用视觉语言模型（VLMs）识别和解析场景中的对象，并通过先进的3D生成模型生成高质量资产。通过迭代应用空间约束，实现语义连贯且物理合理的布局。

Result: 人类评估和基于CLIP的评估表明，HOLODECK 2.0在室内和开放域场景中均能生成与详细文本描述高度一致的高质量场景，优于基线方法。

Conclusion: HOLODECK 2.0 是一个先进的视觉语言引导框架，能够生成多样且风格丰富的3D场景，并通过交互式编辑功能适应人类反馈，显著提升了3D场景生成的效率和质量。

Abstract: 3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.

</details>


### [4] [UnGuide: Learning to Forget with LoRA-Guided Diffusion Models](https://arxiv.org/abs/2508.05755)
*Agnieszka Polowczyk,Alicja Polowczyk,Dawid Malarz,Artur Kasymov,Marcin Mazur,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: UnGuide 是一种新型机器遗忘方法，通过动态推理机制 UnGuidance 和 LoRA 适配器实现选择性遗忘，同时保持图像保真度和模型性能。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型的滥用风险增加，尤其是生成有害或误导性内容，因此需要一种有效的机器遗忘方法，在不影响整体性能的情况下移除特定知识或概念。

Method: UnGuide 结合了 UnGuidance（一种动态推理机制）和 LoRA 适配器，通过调制指导尺度在去噪过程的早期步骤中实现选择性遗忘。对于包含被遗忘概念的提示，LoRA 模块主导生成；对于无关提示，基础模型保持内容保真度。

Result: UnGuide 在概念移除任务中表现优异，能够精确控制遗忘过程并保留扩散模型的表达能力，优于现有的基于 LoRA 的方法。

Conclusion: UnGuide 通过动态推理机制 UnGuidance 实现了对预训练模型中有害或误导性内容的选择性遗忘，同时保持了模型的整体性能和图像保真度，优于现有的基于 LoRA 的方法。

Abstract: Recent advances in large-scale text-to-image diffusion models have heightened
concerns about their potential misuse, especially in generating harmful or
misleading content. This underscores the urgent need for effective machine
unlearning, i.e., removing specific knowledge or concepts from pretrained
models without compromising overall performance. One possible approach is
Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models
for targeted unlearning. However, LoRA often inadvertently alters unrelated
content, leading to diminished image fidelity and realism. To address this
limitation, we introduce UnGuide -- a novel approach which incorporates
UnGuidance, a dynamic inference mechanism that leverages Classifier-Free
Guidance (CFG) to exert precise control over the unlearning process. UnGuide
modulates the guidance scale based on the stability of a few first steps of
denoising processes, enabling selective unlearning by LoRA adapter. For prompts
containing the erased concept, the LoRA module predominates and is
counterbalanced by the base model; for unrelated prompts, the base model
governs generation, preserving content fidelity. Empirical results demonstrate
that UnGuide achieves controlled concept removal and retains the expressive
power of diffusion models, outperforming existing LoRA-based methods in both
object erasure and explicit content removal tasks.

</details>


### [5] [LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing](https://arxiv.org/abs/2508.06055)
*Wonjung Park,Suhyun Ahn,Jinah Park*

Main category: cs.CV

TL;DR: LV-Net通过联合模板和顶点分类，提高了侧脑室形状分析的准确性和鲁棒性，并在阿尔茨海默病研究中发现显著相关的子区域。


<details>
  <summary>Details</summary>
Motivation: 侧脑室形状分析作为神经疾病的生物标志物具有潜力，但由于个体间形状差异大和MRI分辨率限制导致的分割困难，仍面临挑战。

Method: 引入LV-Net框架，通过变形一个解剖学感知的联合侧脑室-海马模板网格，从脑MRI生成个性化的3D侧脑室网格。

Result: LV-Net在存在分割不完美的情况下仍能实现更高的重建精度，并在不同数据集中提供更可靠的形状描述符。

Conclusion: LV-Net通过结合解剖学关系的联合模板，提高了侧脑室形状分析的准确性，并在阿尔茨海默病分析中识别出与疾病显著相关的子区域。

Abstract: Lateral ventricle (LV) shape analysis holds promise as a biomarker for
neurological diseases; however, challenges remain due to substantial shape
variability across individuals and segmentation difficulties arising from
limited MRI resolution. We introduce LV-Net, a novel framework for producing
individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint
LV-hippocampus template mesh. By incorporating anatomical relationships
embedded within the joint template, LV-Net reduces boundary segmentation
artifacts and improves reconstruction robustness. In addition, by classifying
the vertices of the template mesh based on their anatomical adjacency, our
method enhances point correspondence across subjects, leading to more accurate
LV shape statistics. We demonstrate that LV-Net achieves superior
reconstruction accuracy, even in the presence of segmentation imperfections,
and delivers more reliable shape descriptors across diverse datasets. Finally,
we apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that
show significantly associations with the disease relative to cognitively normal
controls. The codes for LV shape modeling are available at
https://github.com/PWonjung/LV_Shape_Modeling.

</details>


### [6] [Improving Masked Style Transfer using Blended Partial Convolution](https://arxiv.org/abs/2508.05769)
*Seyed Hadi Seyed,Ayberk Cansever,David Hart*

Main category: cs.CV

TL;DR: 提出部分卷积风格转移网络，精准处理感兴趣区域风格转移，改进传统掩码方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的艺术风格转移方法通常应用于整个图像，而用户可能仅需对特定区域进行风格转移。标准的掩码后处理方式往往无法准确捕捉感兴趣区域的风格特征。

Method: 采用部分卷积风格转移网络和网络内部混合技术。

Result: 实验证明，该方法在SA-1B数据集上视觉和定量均优于传统方法。

Conclusion: 本文提出了一种基于部分卷积的风格转移网络，能够精确地将风格特征应用于感兴趣区域，并通过网络内部混合技术处理区域选择的不完美，从而在视觉和定量上改进了风格化效果。

Abstract: Artistic style transfer has long been possible with the advancements of
convolution- and transformer-based neural networks. Most algorithms apply the
artistic style transfer to the whole image, but individual users may only need
to apply a style transfer to a specific region in the image. The standard
practice is to simply mask the image after the stylization. This work shows
that this approach tends to improperly capture the style features in the region
of interest. We propose a partial-convolution-based style transfer network that
accurately applies the style features exclusively to the region of interest.
Additionally, we present network-internal blending techniques that account for
imperfections in the region selection. We show that this visually and
quantitatively improves stylization using examples from the SA-1B dataset. Code
is publicly available at https://github.com/davidmhart/StyleTransferMasked.

</details>


### [7] [MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss](https://arxiv.org/abs/2508.05772)
*Can Zhao,Pengfei Guo,Dong Yang,Yucheng Tang,Yufan He,Benjamin Simon,Mason Belue,Stephanie Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: MAISI-v2是首个加速的3D医学图像合成框架，通过rectified flow和contrastive loss解决了速度与条件一致性问题，达到SOTA质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在医学图像合成中的三大问题：通用性差、推理速度慢、输入条件对齐弱。

Method: 提出MAISI-v2框架，整合rectified flow加速生成，并引入region-specific contrastive loss增强条件一致性。

Result: MAISI-v2实现了33倍加速的潜在扩散模型生成，并展示了合成图像可用于数据增强的下游分割实验。

Conclusion: MAISI-v2通过整合rectified flow和引入region-specific contrastive loss，显著提升了生成速度与条件一致性，同时在医学图像合成中达到了SOTA质量。

Abstract: Medical image synthesis is an important topic for both clinical and research
applications. Recently, diffusion models have become a leading approach in this
area. Despite their strengths, many existing methods struggle with (1) limited
generalizability that only work for specific body regions or voxel spacings,
(2) slow inference, which is a common issue for diffusion models, and (3) weak
alignment with input conditions, which is a critical issue for medical imaging.
MAISI, a previously proposed framework, addresses generalizability issues but
still suffers from slow inference and limited condition consistency. In this
work, we present MAISI-v2, the first accelerated 3D medical image synthesis
framework that integrates rectified flow to enable fast and high quality
generation. To further enhance condition fidelity, we introduce a novel
region-specific contrastive loss to enhance the sensitivity to region of
interest. Our experiments show that MAISI-v2 can achieve SOTA image quality
with $33 \times$ acceleration for latent diffusion model. We also conducted a
downstream segmentation experiment to show that the synthetic images can be
used for data augmentation. We release our code, training details, model
weights, and a GUI demo to facilitate reproducibility and promote further
development within the community.

</details>


### [8] [Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks](https://arxiv.org/abs/2508.05783)
*Mengyu Li,Guoyao Shen,Chad W. Farris,Xin Zhang*

Main category: cs.CV

TL;DR: A framework for few-shot deployment of pretrained MRI transformers using MAE, achieving high accuracy in classification and segmentation with minimal supervision.


<details>
  <summary>Details</summary>
Motivation: Addresses the scarcity of annotated data in medical imaging by developing a practical framework for few-shot deployment of pretrained MRI transformers.

Method: Utilizes Masked Autoencoder (MAE) pretraining on a large-scale, multi-cohort brain MRI dataset and proposes MAE-FUnet for segmentation tasks.

Result: Achieves state-of-the-art accuracy in MRI sequence identification and outperforms baselines in skull stripping and multi-class anatomical segmentation under data-limited conditions.

Conclusion: The framework demonstrates efficiency, stability, and scalability, making it suitable for low-resource clinical environments and broader neuroimaging applications.

Abstract: Machine learning using transformers has shown great potential in medical
imaging, but its real-world applicability remains limited due to the scarcity
of annotated data. In this study, we propose a practical framework for the
few-shot deployment of pretrained MRI transformers in diverse brain imaging
tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a
large-scale, multi-cohort brain MRI dataset comprising over 31 million slices,
we obtain highly transferable latent representations that generalize well
across tasks and datasets. For high-level tasks such as classification, a
frozen MAE encoder combined with a lightweight linear head achieves
state-of-the-art accuracy in MRI sequence identification with minimal
supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a
hybrid architecture that fuses multiscale CNN features with pretrained MAE
embeddings. This model consistently outperforms other strong baselines in both
skull stripping and multi-class anatomical segmentation under data-limited
conditions. With extensive quantitative and qualitative evaluations, our
framework demonstrates efficiency, stability, and scalability, suggesting its
suitability for low-resource clinical environments and broader neuroimaging
applications.

</details>


### [9] [Optimization-Free Style Transfer for 3D Gaussian Splats](https://arxiv.org/abs/2508.05813)
*Raphael Du Sablon,David Hart*

Main category: cs.CV

TL;DR: 提出无需重建或优化的3D高斯溅射风格转移方法，通过表面图结构实现快速风格化，速度优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D高斯溅射风格转移方法需要重建、微调或优化特征提取网络的问题。

Method: 采用基于表面的前馈风格化方法，并在场景中将风格信息插值回单个溅射。

Result: 方法支持任意风格图像和3D高斯溅射，无需额外训练或优化，且速度显著优于现有方法。

Conclusion: 本文提出了一种无需重建或优化的3D高斯溅射风格转移方法，通过生成溅射表示的隐式表面图结构，实现了快速风格化，且在消费级硬件上速度低于2分钟。

Abstract: The task of style transfer for 3D Gaussian splats has been explored in many
previous works, but these require reconstructing or fine-tuning the splat while
incorporating style information or optimizing a feature extraction network on
the splat representation. We propose a reconstruction- and optimization-free
approach to stylizing 3D Gaussian splats. This is done by generating a graph
structure across the implicit surface of the splat representation. A
feed-forward, surface-based stylization method is then used and interpolated
back to the individual splats in the scene. This allows for any style image and
3D Gaussian splat to be used without any additional training or optimization.
This also allows for fast stylization of splats, achieving speeds under 2
minutes even on consumer-grade hardware. We demonstrate the quality results
this approach achieves and compare to other 3D Gaussian splat style transfer
methods. Code is publicly available at
https://github.com/davidmhart/FastSplatStyler.

</details>


### [10] [MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses](https://arxiv.org/abs/2508.05819)
*Jong-Ik Park,Carlee Joe-Wong,Gary K. Fedder*

Main category: cs.CV

TL;DR: MZEN是首个原生支持多缩放图像集的NeRF框架，通过改进相机模型和位姿策略，显著提升了工业检测中的细节捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有的NeRF方法在工业检测中无法捕捉到细微结构（如亚微米级缺陷或SEM芯片分析），且传感器分辨率和计算预算有限，添加放大图像会破坏多视角一致性。

Method: MZEN通过（i）在针孔相机模型中引入可学习的缩放标量来调整焦距，（ii）采用一种新颖的位姿策略：先解决广角图像以建立全局度量框架，然后通过缩放一致的裁剪和匹配程序将放大图像位姿初始化为最近的广角对应图像，最后进行联合优化。

Result: 在八个前向场景中，MZEN显著优于无位姿基线和高分辨率变体，PSNR提升高达28%，SSIM提升10%，LPIPS降低高达222%。

Conclusion: MZEN扩展了NeRF的应用范围，使其适用于工业检测场景，不仅保持了全局准确性，还能捕捉到对工业检测至关重要的微米级细节。

Abstract: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from
multiple 2D images, even those taken with unknown camera poses. However, they
still miss the fine-detailed structures that matter in industrial inspection,
e.g., detecting sub-micron defects on a production line or analyzing chips with
Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution
is fixed and compute budgets are tight, so the only way to expose fine
structure is to add zoom-in images; yet, this breaks the multi-view consistency
that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF
(MZEN), the first NeRF framework that natively handles multi-zoom image sets.
MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom
scalar that scales the focal length, and (ii) introduces a novel pose strategy:
wide-field images are solved first to establish a global metric frame, and
zoom-in images are then pose-primed to the nearest wide-field counterpart via a
zoom-consistent crop-and-match procedure before joint refinement. Across eight
forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of
micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently
outperforms pose-free baselines and even high-resolution variants, boosting
PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$.
MZEN, therefore, extends NeRF to real-world factory settings, preserving global
accuracy while capturing the micron-level details essential for industrial
inspection.

</details>


### [11] [TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios](https://arxiv.org/abs/2508.05829)
*Guoping Xu,Hua-Chieh Shao,You Zhang*

Main category: cs.CV

TL;DR: TSMS-SAM2通过多时间尺度采样和内存优化，提升了手术视频分割性能，在EndoVis数据集上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决SAM2在手术视频分析中因复杂运动动态和内存冗余导致的学习效率低下的问题。

Method: 提出了TSMS-SAM2框架，包含多时间尺度视频采样增强和内存分割与剪枝机制，以应对快速对象运动和内存冗余问题。

Result: 在EndoVis2017和EndoVis2018数据集上分别取得了95.24和86.73的最高平均Dice分数，优于现有的SAM-based和任务特定方法。

Conclusion: TSMS-SAM2框架通过多时间尺度视频采样增强和内存分割与剪枝机制，显著提升了手术视频中可提示视频对象分割与跟踪的性能，在EndoVis2017和EndoVis2018数据集上取得了最优结果。

Abstract: Promptable video object segmentation and tracking (VOST) has seen significant
advances with the emergence of foundation models like Segment Anything Model 2
(SAM2); however, their application in surgical video analysis remains
challenging due to complex motion dynamics and the redundancy of memory that
impedes effective learning. In this work, we propose TSMS-SAM2, a novel
framework that enhances promptable VOST in surgical videos by addressing
challenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2
introduces two key strategies: multi-temporal-scale video sampling augmentation
to improve robustness against motion variability, and a memory splitting and
pruning mechanism that organizes and filters past frame features for more
efficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018
datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,
respectively, outperforming prior SAM-based and task-specific methods.
Extensive ablation studies confirm the effectiveness of multiscale temporal
augmentation and memory splitting, highlighting the framework's potential for
robust, efficient segmentation in complex surgical scenarios. Our source code
will be available at https://github.com/apple1986/TSMS-SAM2.

</details>


### [12] [Temporal Cluster Assignment for Efficient Real-Time Video Segmentation](https://arxiv.org/abs/2508.05851)
*Ka-Wai Yung,Felix J. S. Bragman,Jialang Xu,Imanol Luengo,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: TCA通过时间一致性优化令牌聚类，提升视频分割效率，且无需微调。


<details>
  <summary>Details</summary>
Motivation: Swin Transformer在视频分割中计算成本高，传统令牌削减方法受限于窗口注意力机制，而现有训练免费令牌聚类方法未能利用时间冗余。

Method: 引入了Temporal Cluster Assignment (TCA)，一种轻量级且无需微调的策略，通过利用帧间时间一致性来优化令牌聚类。

Result: 在多个数据集上的广泛评估表明，TCA显著提升了现有聚类方法的精度-速度权衡。

Conclusion: TCA通过利用帧间时间一致性优化令牌聚类，显著提升了现有基于聚类方法的精度-速度权衡，并在自然和特定领域视频中表现出良好的泛化能力。

Abstract: Vision Transformers have substantially advanced the capabilities of
segmentation models across both image and video domains. Among them, the Swin
Transformer stands out for its ability to capture hierarchical, multi-scale
representations, making it a popular backbone for segmentation in videos.
However, despite its window-attention scheme, it still incurs a high
computational cost, especially in larger variants commonly used for dense
prediction in videos. This remains a major bottleneck for real-time,
resource-constrained applications. Whilst token reduction methods have been
proposed to alleviate this, the window-based attention mechanism of Swin
requires a fixed number of tokens per window, limiting the applicability of
conventional pruning techniques. Meanwhile, training-free token clustering
approaches have shown promise in image segmentation while maintaining window
consistency. Nevertheless, they fail to exploit temporal redundancy, missing a
key opportunity to further optimize video segmentation performance. We
introduce Temporal Cluster Assignment (TCA), a lightweight and effective,
fine-tuning-free strategy that enhances token clustering by leveraging temporal
coherence across frames. Instead of indiscriminately dropping redundant tokens,
TCA refines token clusters using temporal correlations, thereby retaining
fine-grained details while significantly reducing computation. Extensive
evaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical
video dataset show that TCA consistently boosts the accuracy-speed trade-off of
existing clustering-based methods. Our results demonstrate that TCA generalizes
competently across both natural and domain-specific videos.

</details>


### [13] [VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments](https://arxiv.org/abs/2508.05852)
*Kaiser Hamid,Khandakar Ashrafi Akbar,Nade Liang*

Main category: cs.CV

TL;DR: 提出视觉-语言框架，通过自然语言建模驾驶员动态视线，微调LLaVA模型，结果优于通用模型，为自动驾驶可解释AI提供新方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于静态RGB图像上单时刻注意力分配，缺乏对动态视线变化的建模。本研究旨在通过自然语言描述驾驶员视线行为，填补这一空白。

Method: 提出了一种基于视觉-语言框架的方法，通过自然语言建模驾驶员视线变化，利用few-shot和zero-shot学习在单RGB图像上进行训练。通过人类反馈优化BDD-A数据集的高质量标注，并微调LLaVA模型以对齐视觉感知与注意力中心场景理解。

Result: 微调后的模型在注意力转移检测和可解释性上优于通用视觉-语言模型，且在few-shot和one-shot训练场景下表现优异。

Conclusion: 该方法为自动驾驶中的下游任务（如行为预测、人机协作和多智能体协调）奠定了基础，为可解释AI提供了新方向。

Abstract: Driver visual attention prediction is a critical task in autonomous driving
and human-computer interaction (HCI) research. Most prior studies focus on
estimating attention allocation at a single moment in time, typically using
static RGB images such as driving scene pictures. In this work, we propose a
vision-language framework that models the changing landscape of drivers' gaze
through natural language, using few-shot and zero-shot learning on single RGB
images. We curate and refine high-quality captions from the BDD-A dataset using
human-in-the-loop feedback, then fine-tune LLaVA to align visual perception
with attention-centric scene understanding. Our approach integrates both
low-level cues and top-down context (e.g., route semantics, risk anticipation),
enabling language-based descriptions of gaze behavior. We evaluate performance
across training regimes (few shot, and one-shot) and introduce domain-specific
metrics for semantic alignment and response diversity. Results show that our
fine-tuned model outperforms general-purpose VLMs in attention shift detection
and interpretability. To our knowledge, this is among the first attempts to
generate driver visual attention allocation and shifting predictions in natural
language, offering a new direction for explainable AI in autonomous driving.
Our approach provides a foundation for downstream tasks such as behavior
forecasting, human-AI teaming, and multi-agent coordination.

</details>


### [14] [Multi-view Gaze Target Estimation](https://arxiv.org/abs/2508.05857)
*Qiaomu Miao,Vivek Raju Golani,Jingyi Xu,Progga Paromita Dutta,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: 提出了一种利用多视角相机提升视线目标估计（GTE）准确性的方法，通过整合多视角信息克服单视角方法的局限性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有单视角GTE方法在面对面部遮挡、目标模糊和视线外目标时表现不佳，因此需要一种多视角方法来提升准确性和适用性。

Method: 提出了一种整合多视角信息的方法，包括头部信息聚合（HIA）模块、基于不确定性的视线选择（UGS）模块和基于极线的场景注意力（ESA）模块。

Result: 该方法在性能上显著优于单视角基线方法，尤其是在第二视角能清晰捕捉人脸时。此外，该方法还能仅通过第二视角图像估计第一视角的视线目标。

Conclusion: 该方法通过多视角相机显著提升了视线目标估计（GTE）的准确性和适用性，解决了单视角方法在面部遮挡、目标模糊和视线外目标等挑战中的局限性。

Abstract: This paper presents a method that utilizes multiple camera views for the gaze
target estimation (GTE) task. The approach integrates information from
different camera views to improve accuracy and expand applicability, addressing
limitations in existing single-view methods that face challenges such as face
occlusion, target ambiguity, and out-of-view targets. Our method processes a
pair of camera views as input, incorporating a Head Information Aggregation
(HIA) module for leveraging head information from both views for more accurate
gaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the
most reliable gaze output, and an Epipolar-based Scene Attention (ESA) module
for cross-view background information sharing. This approach significantly
outperforms single-view baselines, especially when the second camera provides a
clear view of the person's face. Additionally, our method can estimate the gaze
target in the first view using the image of the person in the second view only,
a capability not possessed by single-view GTE methods. Furthermore, the paper
introduces a multi-view dataset for developing and evaluating multi-view GTE
methods. Data and code are available at
https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html

</details>


### [15] [ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates](https://arxiv.org/abs/2508.05898)
*Hamidreza Dastmalchi,Aijun An,Ali cheraghian*

Main category: cs.CV

TL;DR: ETTA提出了一种高效的测试时间适应方法，通过动态更新和自适应集成提升性能，超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的基于缓存的测试时间适应方法仅存储高置信度样本，限制了决策边界的泛化能力。

Method: ETTA采用递归更新模块动态整合所有测试样本，逐步优化决策边界，并通过自适应集成模块减少对提示的依赖。

Result: 在多个基准测试中，ETTA在计算复杂度和准确性上均优于现有方法。

Conclusion: ETTA通过引入递归更新模块和自适应集成模块，显著提升了预训练视觉语言模型在分布偏移下的零样本性能，同时保持了高效的计算和内存使用。

Abstract: Pretrained vision-language models (VLMs) like CLIP show strong zero-shot
performance but struggle with generalization under distribution shifts.
Test-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test
data in new domains. While some TTA methods rely on prompt-tuning,
training-free cache-based approaches are preferred for efficiency. However,
current cache-based TTA models store only a limited set of high-confidence
samples, restricting the decision boundary to these samples and ignoring the
influence of other incoming test data. To address this, we propose Efficient
Test-Time Adaptation (ETTA), introducing a Recursive Updating module that
integrates all incoming test samples, progressively refining the decision
boundary. This strategy mimics an unbounded cache, dynamically updating
contextual embeddings for improved accuracy with minimal memory and
computational overhead. ETTA also includes an Adaptive Ensemble module to
reduce prompt dependency in image-to-text scores by dynamically selecting
optimal prompts for each class. Furthermore, ETTA adaptively combines scores
from both modules based on confidence levels, leveraging their complementary
strengths. Extensive experiments on two benchmarks confirm that ETTA surpasses
the state-of-the-art TTA models in computational complexity and accuracy,
setting a new standard for effective, efficient test-time adaptation. The code
has been released at https://github.com/hamidreza-dastmalchi/ETTA.

</details>


### [16] [Robust Image Stitching with Optimal Plane](https://arxiv.org/abs/2508.05903)
*Lang Nie,Yuan Mei,Kang Liao,Yunqiu Xu,Chunyu Lin,Bin Xiao*

Main category: cs.CV

TL;DR: RopStitch是一种无监督深度图像拼接框架，通过双分支架构和虚拟最优平面概念，显著提升了拼接的鲁棒性和自然性。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像拼接方法在内容对齐和结构保持之间的矛盾，提升在多样化场景中的泛化能力。

Method: 采用双分支架构（预训练分支和可学习分支）结合虚拟最优平面概念，通过迭代系数预测器和最小语义失真约束来优化拼接效果。

Result: 在多个数据集上，RopStitch在场景鲁棒性和内容自然性方面显著优于现有方法。

Conclusion: RopStitch通过双分支架构和虚拟最优平面概念，显著提升了图像拼接的鲁棒性和自然性，并在多个数据集上验证了其优越性能。

Abstract: We present \textit{RopStitch}, an unsupervised deep image stitching framework
with both robustness and naturalness. To ensure the robustness of
\textit{RopStitch}, we propose to incorporate the universal prior of content
perception into the image stitching model by a dual-branch architecture. It
separately captures coarse and fine features and integrates them to achieve
highly generalizable performance across diverse unseen real-world scenes.
Concretely, the dual-branch model consists of a pretrained branch to capture
semantically invariant representations and a learnable branch to extract
fine-grained discriminative features, which are then merged into a whole by a
controllable factor at the correlation level. Besides, considering that content
alignment and structural preservation are often contradictory to each other, we
propose a concept of virtual optimal planes to relieve this conflict. To this
end, we model this problem as a process of estimating homography decomposition
coefficients, and design an iterative coefficient predictor and minimal
semantic distortion constraint to identify the optimal plane. This scheme is
finally incorporated into \textit{RopStitch} by warping both views onto the
optimal plane bidirectionally. Extensive experiments across various datasets
demonstrate that \textit{RopStitch} significantly outperforms existing methods,
particularly in scene robustness and content naturalness. The code is available
at {\color{red}https://github.com/MmelodYy/RopStitch}.

</details>


### [17] [Neural Field Representations of Mobile Computational Photography](https://arxiv.org/abs/2508.05907)
*Ilya Chugunov*

Main category: cs.CV

TL;DR: 该论文提出了一种利用神经场模型处理移动摄影数据的方法，实现了深度估计、图层分离和图像拼接等应用，性能优于现有技术且无需复杂预处理。


<details>
  <summary>Details</summary>
Motivation: 随着移动成像技术的快速发展，现代手机配备了多种成像技术和非视觉传感器，使其成为多功能计算成像平台。神经场模型能够在不依赖显式数据表示的情况下重建复杂场景，这为移动摄影数据的处理提供了新的可能性。

Method: 利用自正则化模型，通过随机梯度下降直接拟合智能手机的原始测量数据，解决具有挑战性的逆问题。

Result: 所提出的方法在无需复杂预处理或标记数据的情况下，优于现有最先进的方法。

Conclusion: 该论文展示了精心设计的神经场模型如何紧凑地表示复杂几何和光照效果，从而在移动摄影数据上实现深度估计、图层分离和图像拼接等应用，且无需依赖复杂的预处理步骤、标记的真实数据或机器学习先验。

Abstract: Over the past two decades, mobile imaging has experienced a profound
transformation, with cell phones rapidly eclipsing all other forms of digital
photography in popularity. Today's cell phones are equipped with a diverse
range of imaging technologies - laser depth ranging, multi-focal camera arrays,
and split-pixel sensors - alongside non-visual sensors such as gyroscopes,
accelerometers, and magnetometers. This, combined with on-board integrated
chips for image and signal processing, makes the cell phone a versatile
pocket-sized computational imaging platform. Parallel to this, we have seen in
recent years how neural fields - small neural networks trained to map
continuous spatial input coordinates to output signals - enable the
reconstruction of complex scenes without explicit data representations such as
pixel arrays or point clouds. In this thesis, I demonstrate how carefully
designed neural field models can compactly represent complex geometry and
lighting effects. Enabling applications such as depth estimation, layer
separation, and image stitching directly from collected in-the-wild mobile
photography data. These methods outperform state-of-the-art approaches without
relying on complex pre-processing steps, labeled ground truth data, or machine
learning priors. Instead, they leverage well-constructed, self-regularized
models that tackle challenging inverse problems through stochastic gradient
descent, fitting directly to raw measurements from a smartphone.

</details>


### [18] [Enhancing Construction Site Analysis and Understanding with 3D Segmentation](https://arxiv.org/abs/2508.05922)
*Sri Ramana Saketh Vasanthawada,Pengkun Liu,Pingbo Tang*

Main category: cs.CV

TL;DR: 研究评估了SAM和Mask3D在建筑环境中的表现，发现户外场景分割存在不足，提出了定制化分割流程的需求。


<details>
  <summary>Details</summary>
Motivation: 传统建筑进度监测方法资源密集且主要针对室内环境，难以应对建筑工地的复杂、杂乱和动态变化条件。计算机视觉方法为提高效率和可扩展性提供了可能。

Method: 本研究对两种先进的3D分割方法（SAM和Mask3D）在室内和户外建筑环境中的适应性进行了比较分析，评估了它们在复杂、动态变化条件下的性能。

Result: 研究发现，SAM和Mask3D在户外建筑场景中的表现存在局限性，突显了当前分割方法因缺乏户外基准测试的不足。

Conclusion: 本研究通过评估SAM和Mask3D在复杂建筑环境中的表现，揭示了当前分割方法在户外场景中的局限性，并提出了针对建筑工地数据定制化分割流程的必要性，推动了建筑监测技术向更自动化和精确化的方向发展。

Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting
the exploration of computer-vision-based methodologies for enhanced efficiency
and scalability. Traditional data acquisition methods, primarily focusing on
indoor environments, falter in construction site's complex, cluttered, and
dynamically changing conditions. This paper critically evaluates the
application of two advanced 3D segmentation methods, Segment Anything Model
(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained
initially on indoor datasets, both models' adaptability and performance are
assessed in real-world construction settings, highlighting the gap in current
segmentation approaches due to the absence of benchmarks for outdoor scenarios.
Through a comparative analysis, this study not only showcases the relative
effectiveness of SAM and Mask3D but also addresses the critical need for
tailored segmentation workflows capable of extracting actionable insights from
construction site data, thereby advancing the field towards more automated and
precise monitoring techniques.

</details>


### [19] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

TL;DR: SINGAD是一种自监督框架，通过3D高斯抛雪球引导的扩散模型和物理驱动的光交互建模，解决了单图像法线估计中的多视角几何不一致性和数据依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 解决单图像法线估计中缺乏空间维度信息的问题，以及现有扩散方法依赖数据驱动统计先验和缺乏光-表面交互显式建模导致的视角间法线方向冲突问题。

Method: SINGAD框架结合了3D高斯抛雪球引导的扩散模型、光交互驱动的3DGS重参数化模型、跨域特征融合模块和可微分3D重投影损失策略。

Result: 在Google Scanned Objects数据集上的定量评估表明，SINGAD在多个指标上优于现有最先进方法。

Conclusion: SINGAD框架通过整合物理驱动的光交互建模和基于可微分渲染的重投影策略，成功解决了多视角几何不一致性和数据依赖性问题，显著提升了单图像法线估计的性能。

Abstract: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.

</details>


### [20] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: Bifrost-1通过CLIP图像嵌入和轻量级ControlNet，高效桥接MLLMs与扩散模型，实现高保真图像生成，降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因直接训练LLMs或桥接LLMs与扩散模型而导致的高昂训练成本问题。

Method: 采用patch-level CLIP图像嵌入作为潜在变量，轻量级ControlNet适配扩散模型，并初始化MLLM的视觉生成分支以保持其多模态推理能力。

Result: Bifrost-1在视觉保真度和多模态理解方面表现优于或与现有方法相当，且训练计算成本显著降低。

Conclusion: Bifrost-1通过无缝集成预训练的多模态大语言模型（MLLMs）和扩散模型，实现了高保真可控图像生成，同时显著提高了训练效率。

Abstract: There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [21] [PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation](https://arxiv.org/abs/2508.05976)
*Zhihao Zhu,Yifan Zheng,Siyu Pan,Yaohui Jin,Yao Mu*

Main category: cs.CV

TL;DR: PASG框架通过自动基元提取和VLM驱动的语义锚定，解决了机器人操作中语义与几何特征的割裂问题，实现了细粒度的语义-功能理解。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中高层任务语义与低层几何特征之间的割裂问题，以及现有视觉语言模型(VLMs)在语义基础和动态语义-功能关系捕捉上的局限性。

Method: 提出了Primitive-Aware Semantic Grounding (PASG)框架，包括：(1) 通过几何特征聚合自动提取基元；(2) 使用VLM驱动的语义锚定动态耦合几何基元与功能；(3) 开发了一个空间-语义推理基准和微调的VLM (Qwen2.5VL-PA)。

Result: PASG在多样化场景的机器人操作任务中表现优异，性能接近手动标注的水平。

Conclusion: PASG通过将几何基元与任务语义相结合，为机器人操作提供了一个统一的范式，实现了对对象更细粒度的语义-功能理解。

Abstract: The fragmentation between high-level task semantics and low-level geometric
features remains a persistent challenge in robotic manipulation. While
vision-language models (VLMs) have shown promise in generating affordance-aware
visual representations, the lack of semantic grounding in canonical spaces and
reliance on manual annotations severely limit their ability to capture dynamic
semantic-affordance relationships. To address these, we propose Primitive-Aware
Semantic Grounding (PASG), a closed-loop framework that introduces: (1)
Automatic primitive extraction through geometric feature aggregation, enabling
cross-category detection of keypoints and axes; (2) VLM-driven semantic
anchoring that dynamically couples geometric primitives with functional
affordances and task-relevant description; (3) A spatial-semantic reasoning
benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's
effectiveness in practical robotic manipulation tasks across diverse scenarios,
achieving performance comparable to manual annotations. PASG achieves a
finer-grained semantic-affordance understanding of objects, establishing a
unified paradigm for bridging geometric primitives with task semantics in
robotic manipulation.

</details>


### [22] [AnimateScene: Camera-controllable Animation in Any Scene](https://arxiv.org/abs/2508.05982)
*Qingyang Liu,Bingjie Gao,Weiheng Huang,Jun Zhang,Zhongqian Sun,Yang Wei,Zelin Peng,Qianli Ma,Shuai Yang,Zhaohe Liao,Haonan Zhao,Li Niu*

Main category: cs.CV

TL;DR: AnimateScene 提出了一种统一框架，解决了4D人体动画与3D场景重建的无缝集成问题，包括位置放置、风格对齐和相机轨迹插入，生成高质量动态视频。


<details>
  <summary>Details</summary>
Motivation: 将4D人体动画与3D场景重建无缝集成面临多个挑战，如人体位置和比例的准确放置、避免不合理穿插、照明和风格的一致性，以及伴随相机运动的视角重建需求。

Method: 方法包括：1) 设计精确的位置放置模块以避免人体与场景的不合理穿插；2) 提出无需训练的样式对齐方法，使4D人体与背景的照明和风格相匹配；3) 设计联合后重建方法以插入相机轨迹。

Result: 实验表明，AnimateScene 能够生成具有高几何细节和时空一致性的动态场景视频，适用于多种相机和动作组合。

Conclusion: AnimateScene 通过统一的框架解决了将4D人体动画与3D场景重建无缝集成的挑战，包括准确的位置放置、风格对齐和相机轨迹插入，从而生成具有高几何细节和时空一致性的动态场景视频。

Abstract: 3D scene reconstruction and 4D human animation have seen rapid progress and
broad adoption in recent years. However, seamlessly integrating reconstructed
scenes with 4D human animation to produce visually engaging results remains
challenging. One key difficulty lies in placing the human at the correct
location and scale within the scene while avoiding unrealistic
interpenetration. Another challenge is that the human and the background may
exhibit different lighting and style, leading to unrealistic composites. In
addition, appealing character motion videos are often accompanied by camera
movements, which means that the viewpoints need to be reconstructed along a
specified trajectory. We present AnimateScene, which addresses the above issues
in a unified framework. First, we design an accurate placement module that
automatically determines a plausible 3D position for the human and prevents any
interpenetration within the scene during motion. Second, we propose a
training-free style alignment method that adapts the 4D human representation to
match the background's lighting and style, achieving coherent visual
integration. Finally, we design a joint post-reconstruction method for both the
4D human and the 3D scene that allows camera trajectories to be inserted,
enabling the final rendered video to feature visually appealing camera
movements. Extensive experiments show that AnimateScene generates dynamic scene
videos with high geometric detail and spatiotemporal coherence across various
camera and action combinations.

</details>


### [23] [ETA: Energy-based Test-time Adaptation for Depth Completion](https://arxiv.org/abs/2508.05989)
*Younjoon Chung,Hyoungseob Park,Patrick Rim,Xiaoran Zhang,Jihe He,Ziyao Zeng,Safa Cicek,Byung-Woo Hong,James S. Duncan,Alex Wong*

Main category: cs.CV

TL;DR: ETA是一种基于能量的测试时自适应方法，通过对抗扰动和能量模型优化预训练深度补全模型在新环境下的性能，显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 深度补全模型在从源数据转移到目标数据时，由于协变量偏移，预测输出常出现错误，需要一种无需预先了解目标分布的自适应方法。

Method: 利用对抗扰动探索数据空间，训练能量模型以评分深度预测的局部区域是否属于分布内或外，并在测试时更新预训练模型的参数以最小化能量。

Result: ETA在三个室内和三个室外数据集上的表现优于之前的最先进方法，室外平均提升6.94%，室内平均提升10.23%。

Conclusion: ETA方法通过最小化能量，有效将测试时的预测与源数据分布对齐，显著提升了深度补全模型在新环境条件下的性能。

Abstract: We propose a method for test-time adaptation of pretrained depth completion
models. Depth completion models, trained on some ``source'' data, often predict
erroneous outputs when transferred to ``target'' data captured in novel
environmental conditions due to a covariate shift. The crux of our method lies
in quantifying the likelihood of depth predictions belonging to the source data
distribution. The challenge is in the lack of access to out-of-distribution
(target) data prior to deployment. Hence, rather than making assumptions
regarding the target distribution, we utilize adversarial perturbations as a
mechanism to explore the data space. This enables us to train an energy model
that scores local regions of depth predictions as in- or out-of-distribution.
We update the parameters of pretrained depth completion models at test time to
minimize energy, effectively aligning test-time predictions to those of the
source distribution. We call our method ``Energy-based Test-time Adaptation'',
or ETA for short. We evaluate our method across three indoor and three outdoor
datasets, where ETA improve over the previous state-of-the-art method by an
average of 6.94% for outdoors and 10.23% for indoors. Project Page:
https://fuzzythecat.github.io/eta.

</details>


### [24] [Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision](https://arxiv.org/abs/2508.05990)
*Haichao Wang,Xinyue Xi,Jiangtao Wen,Yuxing Han*

Main category: cs.CV

TL;DR: 本文提出了一种高效视频计算机视觉系统，通过直接处理Bayer数据、快速块匹配和上下文感知细化，显著减少计算开销并保持性能。


<details>
  <summary>Details</summary>
Motivation: 视频计算机视觉系统的效率因视频内部的高时间冗余而受限，现有方法未能充分减少冗余且忽视了前端计算开销。

Method: 1. 移除图像信号处理器，直接将Bayer格式数据输入模型；2. 提出基于快速块匹配的运动估计算法，并引入运动矢量细化模块；3. 使用上下文感知块细化网络纠正大误差区域；4. 采用帧选择策略平衡准确性与效率。

Result: 在多个视频计算机视觉任务上的实验表明，该方法在保持轻微性能损失的同时实现了显著加速。

Conclusion: 本文提出的高效视频计算机视觉系统通过去除图像信号处理器、引入快速块匹配运动估计算法并结合上下文感知块细化网络，显著减少了计算开销，同时保持了较高的性能。

Abstract: The efficiency of video computer vision system remains a challenging task due
to the high temporal redundancy inside a video. Existing works have been
proposed for efficient vision computer vision. However, they do not fully
reduce the temporal redundancy and neglect the front end computation overhead.
In this paper, we propose an efficient video computer vision system. First,
image signal processor is removed and Bayer-format data is directly fed into
video computer vision models, thus saving the front end computation. Second,
instead of optical flow models and video codecs, a fast block matching-based
motion estimation algorithm is proposed specifically for efficient video
computer vision, with a MV refinement module. To correct the error,
context-aware block refinement network is introduced to refine regions with
large error. To further balance the accuracy and efficiency, a frame selection
strategy is employed. Experiments on multiple video computer vision tasks
demonstrate that our method achieves significant acceleration with slight
performance loss.

</details>


### [25] [ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge](https://arxiv.org/abs/2508.05991)
*Juewen Hu,Yexin Li,Jiulin Li,Shuo Chen,Pring Wong*

Main category: cs.CV

TL;DR: A novel multimodal emotion recognition framework using pre-trained models and advanced fusion strategies achieves significant performance improvement on MER2025-SEMI.


<details>
  <summary>Details</summary>
Motivation: To enhance human-computer interaction by addressing the challenge of multimodal emotion recognition, particularly under data scarcity conditions.

Method: The framework leverages large-scale pre-trained models for feature extraction from visual, audio, and textual modalities. It includes a dual-branch visual encoder, a context-enriched textual method, and a fusion strategy with self-attention mechanisms and residual connections. A multi-source labeling strategy refines noisy training labels.

Result: The approach outperforms the official baseline with a weighted F-score of 87.49% compared to 78.63%.

Conclusion: The proposed multimodal emotion recognition framework significantly improves performance on the MER2025-SEMI dataset, achieving a weighted F-score of 87.49%, validating its effectiveness.

Abstract: Emotion recognition plays a vital role in enhancing human-computer
interaction. In this study, we tackle the MER-SEMI challenge of the MER2025
competition by proposing a novel multimodal emotion recognition framework. To
address the issue of data scarcity, we leverage large-scale pre-trained models
to extract informative features from visual, audio, and textual modalities.
Specifically, for the visual modality, we design a dual-branch visual encoder
that captures both global frame-level features and localized facial
representations. For the textual modality, we introduce a context-enriched
method that employs large language models to enrich emotional cues within the
input text. To effectively integrate these multimodal features, we propose a
fusion strategy comprising two key components, i.e., self-attention mechanisms
for dynamic modality weighting, and residual connections to preserve original
representations. Beyond architectural design, we further refine noisy labels in
the training set by a multi-source labeling strategy. Our approach achieves a
substantial performance improvement over the official baseline on the
MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to
78.63%, thereby validating the effectiveness of the proposed framework.

</details>


### [26] [EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad](https://arxiv.org/abs/2508.05994)
*Huadong Wu,Yi Fu,Yunhao Li,Yuan Gao,Kang Du*

Main category: cs.CV

TL;DR: 论文提出了 EvoMakeup 框架和 MakeupQuad 数据集，解决了现有化妆编辑方法在细节和保真度上的不足，实现了高质量的化妆编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法由于缺乏结构化配对数据，导致化妆细节粗糙且难以同时保持身份和化妆保真度。

Method: 提出了 EvoMakeup，一个统一的训练框架，通过多阶段蒸馏减少图像退化，迭代提升数据和模型质量。

Result: 实验结果表明，EvoMakeup 在化妆保真度和身份保持方面优于现有方法，有效平衡了这两个方面。

Conclusion: EvoMakeup 方法在真实世界基准测试中表现优异，支持高保真、可控的多任务化妆编辑，包括全脸和部分参考编辑以及文本驱动的化妆编辑。

Abstract: Facial makeup editing aims to realistically transfer makeup from a reference
to a target face. Existing methods often produce low-quality results with
coarse makeup details and struggle to preserve both identity and makeup
fidelity, mainly due to the lack of structured paired data -- where source and
result share identity, and reference and result share identical makeup. To
address this, we introduce MakeupQuad, a large-scale, high-quality dataset with
non-makeup faces, references, edited results, and textual makeup descriptions.
Building on this, we propose EvoMakeup, a unified training framework that
mitigates image degradation during multi-stage distillation, enabling iterative
improvement of both data and model quality. Although trained solely on
synthetic data, EvoMakeup generalizes well and outperforms prior methods on
real-world benchmarks. It supports high-fidelity, controllable, multi-task
makeup editing -- including full-face and partial reference-based editing, as
well as text-driven makeup editing -- within a single model. Experimental
results demonstrate that our method achieves superior makeup fidelity and
identity preservation, effectively balancing both aspects. Code and dataset
will be released upon acceptance.

</details>


### [27] [GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.06113)
*Jian Wang,Chaokang Jiang,Haitao Xu*

Main category: cs.CV

TL;DR: GMF-Drive通过几何增强pillar编码和门控Mamba融合，以线性复杂度解决了transformer在自动驾驶中的效率问题，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于transformer的融合方法存在计算复杂度高和空间先验缺失的问题，限制了高分辨率特征的使用和BEV表示的有效建模。

Method: 提出了GMF-Drive框架，包含几何增强的pillar格式编码和门控Mamba融合架构（GM-Fusion），替代了传统的transformer，采用线性复杂度的状态空间模型（SSM）来高效捕捉长距离依赖。

Result: 在NAVSIM基准测试中，GMF-Drive显著优于DiffusionDrive，验证了其组件的有效性。

Conclusion: GMF-Drive通过几何增强的pillar格式和门控Mamba融合架构，显著提升了端到端自动驾驶的性能，超越了现有DiffusionDrive模型，并在NAVSIM基准测试中取得了最先进的成果。

Abstract: Diffusion-based models are redefining the state-of-the-art in end-to-end
autonomous driving, yet their performance is increasingly hampered by a
reliance on transformer-based fusion. These architectures face fundamental
limitations: quadratic computational complexity restricts the use of
high-resolution features, and a lack of spatial priors prevents them from
effectively modeling the inherent structure of Bird's Eye View (BEV)
representations. This paper introduces GMF-Drive (Gated Mamba Fusion for
Driving), an end-to-end framework that overcomes these challenges through two
principled innovations. First, we supersede the information-limited
histogram-based LiDAR representation with a geometrically-augmented pillar
format encoding shape descriptors and statistical features, preserving critical
3D geometric details. Second, we propose a novel hierarchical gated mamba
fusion (GM-Fusion) architecture that substitutes an expensive transformer with
a highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM
leverages directional sequencing and adaptive fusion mechanisms to capture
long-range dependencies with linear complexity, while explicitly respecting the
unique spatial properties of the driving scene. Extensive experiments on the
challenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new
state-of-the-art performance, significantly outperforming DiffusionDrive.
Comprehensive ablation studies validate the efficacy of each component,
demonstrating that task-specific SSMs can surpass a general-purpose transformer
in both performance and efficiency for autonomous driving.

</details>


### [28] [MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2508.06009)
*Jun Feng,Zixin Wang,Zhentao Zhang,Yue Guo,Zhihan Zhou,Xiuyi Chen,Zhenyang Li,Dawei Yin*

Main category: cs.CV

TL;DR: 研究提出MathReal数据集，评估多模态大语言模型在真实教育场景中的数学推理能力，发现现有模型表现不佳，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要基于干净或处理过的多模态输入，未涵盖真实K-12教育用户提供的图像，因此需要填补这一空白。

Method: 研究引入了MathReal数据集，包含2000个由手持移动设备在真实场景中捕获的数学问题图像，并设计了六种实验设置来系统评估MLLMs的性能。

Result: 研究发现，现有MLLMs在真实教育场景中的表现受到显著挑战，特别是在图像质量退化、视角变化和无关内容干扰等方面。

Conclusion: 现有的多模态大语言模型（MLLMs）在真实教育场景中的问题解决能力面临显著挑战，研究通过分析其表现和错误模式，为未来的改进提供了方向。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in visual mathematical reasoning across various existing
benchmarks. However, these benchmarks are predominantly based on clean or
processed multimodal inputs, without incorporating the images provided by
real-world Kindergarten through 12th grade (K-12) educational users. To address
this gap, we introduce MathReal, a meticulously curated dataset comprising
2,000 mathematical questions with images captured by handheld mobile devices in
authentic scenarios. Each question is an image, containing the question text
and visual element. We systematically classify the real images into three
primary categories: image quality degradation, perspective variation, and
irrelevant content interference, which are further delineated into 14
subcategories. Additionally, MathReal spans five core knowledge and ability
categories, which encompass three question types and are divided into three
difficulty levels. To comprehensively evaluate the multimodal mathematical
reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we
design six experimental settings that enable a systematic analysis of their
performance. Through extensive experimentation, we find that the
problem-solving abilities of existing MLLMs are significantly challenged in
realistic educational contexts. Based on this, we conduct a thorough analysis
of their performance and error patterns, providing insights into their
recognition, comprehension, and reasoning capabilities, and outlining
directions for future improvements. Data and code:
https://github.com/junfeng0288/MathReal.

</details>


### [29] [Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor](https://arxiv.org/abs/2508.06177)
*Dominik Brämer,Diana Kleingarn,Oliver Urbann*

Main category: cs.CV

TL;DR: 提出一种基于地板特征和图卷积网络的定位框架，显著提升定位精度和效率，解决绑架机器人问题。


<details>
  <summary>Details</summary>
Motivation: 传统定位方法（如激光雷达或QR码）在复杂环境中存在可扩展性和适应性不足的问题，需要更高效、精确的解决方案。

Method: 采用图表示地板特征，并利用图卷积网络（GCNs）进行定位，避免了传统图像特征比较的复杂性。

Result: 定位精度达到0.64厘米误差，且在每帧中成功解决绑架机器人问题，无需复杂滤波过程。

Conclusion: 该论文提出的基于地板特征和图卷积网络的定位框架，显著提高了机器人在复杂环境中的定位精度和效率，并成功解决了绑架机器人问题，为机器人导航开辟了新途径。

Abstract: Accurate localization represents a fundamental challenge in
  robotic navigation. Traditional methodologies, such as Lidar or QR-code based
systems, suffer from inherent scalability and adaptability con straints,
particularly in complex environments. In this work, we propose
  an innovative localization framework that harnesses flooring characteris tics
by employing graph-based representations and Graph Convolutional
  Networks (GCNs). Our method uses graphs to represent floor features,
  which helps localize the robot more accurately (0.64cm error) and more
  efficiently than comparing individual image features. Additionally, this
  approach successfully addresses the kidnapped robot problem in every
  frame without requiring complex filtering processes. These advancements
  open up new possibilities for robotic navigation in diverse environments.

</details>


### [30] [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/abs/2508.06014)
*Minsu Kim,Subin Jeon,In Cho,Mijin Yoo,Seon Joo Kim*

Main category: cs.CV

TL;DR: 提出了一种基于3DGS的管道方法，通过虚拟相机和视频扩散先验增强训练视图，显著提升了重建质量，支持高质量任意视角渲染。


<details>
  <summary>Details</summary>
Motivation: 现有方法在偏离训练轨迹的视角下渲染时存在伪影和缺失区域的问题，限制了场景的无缝探索。

Method: 采用信息增益驱动的虚拟相机放置策略以最大化场景覆盖，并结合视频扩散先验优化渲染结果，最后通过增强视图对3D高斯进行微调。

Result: 实验结果表明，该方法在Wild-Explore基准测试中优于现有3DGS方法，能够实现高质量、无伪影的任意视角渲染。

Conclusion: 本文提出的基于3D高斯泼溅（3DGS）的管道方法，通过生成额外的训练视图和利用视频扩散先验，显著提升了重建质量，实现了从任意视角的高质量、无伪影渲染。

Abstract: Recent advances in novel view synthesis (NVS) have enabled real-time
rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle
with artifacts and missing regions when rendering from viewpoints that deviate
from the training trajectory, limiting seamless scene exploration. To address
this, we propose a 3DGS-based pipeline that generates additional training views
to enhance reconstruction. We introduce an information-gain-driven virtual
camera placement strategy to maximize scene coverage, followed by video
diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with
these enhanced views significantly improves reconstruction quality. To evaluate
our method, we present Wild-Explore, a benchmark designed for challenging scene
exploration. Experiments demonstrate that our approach outperforms existing
3DGS-based methods, enabling high-quality, artifact-free rendering from
arbitrary viewpoints.
  https://exploregs.github.io

</details>


### [31] [Depth Jitter: Seeing through the Depth](https://arxiv.org/abs/2508.06227)
*Md Sazidur Rahman,David Cabecinhas,Ricard Marxer*

Main category: cs.CV

TL;DR: Depth-Jitter是一种新型深度增强技术，通过模拟自然深度变化提升模型泛化能力。实验证明其在深度敏感环境中能增强模型稳定性，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 传统增强技术忽视了深度感知变换，限制了模型在现实世界深度变化中的鲁棒性。本文旨在通过模拟自然深度变化来提升模型的泛化能力。

Method: Depth-Jitter通过自适应深度偏移（基于深度方差阈值）生成合成深度扰动，同时保持结构完整性。

Result: 在FathomNet和UTDAC2020数据集上的实验表明，Depth-Jitter能提升模型在不同深度条件下的稳定性。与传统增强策略（如ColorJitter）相比，虽然绝对性能不总是更优，但在深度敏感环境中表现更稳定。

Conclusion: Depth-Jitter作为一种新型的基于深度的增强技术，虽然在绝对性能上不总是优于传统方法，但在深度敏感环境中能持续提升模型的稳定性和泛化能力。这为深度感知增强在现实世界应用中的潜力提供了基础，并支持进一步研究深度学习策略。

Abstract: Depth information is essential in computer vision, particularly in underwater
imaging, robotics, and autonomous navigation. However, conventional
augmentation techniques overlook depth aware transformations, limiting model
robustness in real world depth variations. In this paper, we introduce
Depth-Jitter, a novel depth-based augmentation technique that simulates natural
depth variations to improve generalization. Our approach applies adaptive depth
offsetting, guided by depth variance thresholds, to generate synthetic depth
perturbations while preserving structural integrity. We evaluate Depth-Jitter
on two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on
model stability under diverse depth conditions. Extensive experiments compare
Depth-Jitter against traditional augmentation strategies such as ColorJitter,
analyzing performance across varying learning rates, encoders, and loss
functions. While Depth-Jitter does not always outperform conventional methods
in absolute performance, it consistently enhances model stability and
generalization in depth-sensitive environments. These findings highlight the
potential of depth-aware augmentation for real-world applications and provide a
foundation for further research into depth-based learning strategies. The
proposed technique is publicly available to support advancements in depth-aware
augmentation. The code is publicly available on
\href{https://github.com/mim-team/Depth-Jitter}{github}.

</details>


### [32] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: 开发扩散模型生成高保真图像以解决数据不平衡问题，提升多类深度神经网络分类性能，并公开模型促进研究。


<details>
  <summary>Details</summary>
Motivation: 由于可用数据稀少且粒子类型间严重不平衡，传统多类分类器在此类问题中效果有限。特别是对于无意中出现且数量较少的粒子类型（如硅油和气泡），数据获取更为困难。

Method: 采用最先进的扩散模型生成高保真图像，以增强训练数据集。通过大规模实验验证生成图像的有效性，并公开了模型和分类器。

Result: 实验表明，使用扩散生成的图像训练数据集可以显著提升分类性能，且无明显负面影响。

Conclusion: 本研究开发了一种先进的扩散模型，用于生成高保真图像以解决数据不平衡问题，有效提升了多类深度神经网络的训练效果。实验验证了生成样本在视觉质量和结构上与真实粒子图像的相似性，并公开了扩散模型和训练好的分类器以促进开放研究和重现性。

Abstract: Sub-visible particle analysis using flow imaging microscopy combined with
deep learning has proven effective in identifying particle types, enabling the
distinction of harmless components such as silicone oil from protein particles.
However, the scarcity of available data and severe imbalance between particle
types within datasets remain substantial hurdles when applying multi-class
classifiers to such problems, often forcing researchers to rely on less
effective methods. The aforementioned issue is particularly challenging for
particle types that appear unintentionally and in lower numbers, such as
silicone oil and air bubbles, as opposed to protein particles, where obtaining
large numbers of images through controlled settings is comparatively
straightforward. In this work, we develop a state-of-the-art diffusion model to
address data imbalance by generating high-fidelity images that can augment
training datasets, enabling the effective training of multi-class deep neural
networks. We validate this approach by demonstrating that the generated samples
closely resemble real particle images in terms of visual quality and structure.
To assess the effectiveness of using diffusion-generated images in training
datasets, we conduct large-scale experiments on a validation dataset comprising
500,000 protein particle images and demonstrate that this approach improves
classification performance with no negligible downside. Finally, to promote
open research and reproducibility, we publicly release both our diffusion
models and the trained multi-class deep neural network classifiers, along with
a straightforward interface for easy integration into future studies, at
https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [33] [Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts](https://arxiv.org/abs/2508.06032)
*Kiran Chhatre,Christopher Peters,Srikrishna Karanam*

Main category: cs.CV

TL;DR: Spectrum通过重新利用I2Tx扩散模型，实现了对人体和服装的细粒度解析，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人体解析方法通常使用固定的掩码类别，难以区分细粒度的服装类型；而开放词汇分割方法虽能泛化，但缺乏对详细人体解析的专业化表示。

Method: Spectrum利用经过调整的图像到纹理（I2Tx）扩散模型提取特征，并通过提示引导的接地生成语义有效的掩码，实现了对多样服装类别和身体部分的精确解析。

Result: Spectrum在跨数据集实验中表现出色，尤其在服装部分、身体部分、未见过的服装类别和全身掩码的分割任务中，均优于基线方法。

Conclusion: Spectrum提出了一种新颖的方法，通过重新利用图像到纹理（I2Tx）扩散模型，显著提升了人体解析的细粒度表现，尤其在服装和身体部分的详细分割上优于现有基线方法。

Abstract: Existing methods for human parsing into body parts and clothing often use
fixed mask categories with broad labels that obscure fine-grained clothing
types. Recent open-vocabulary segmentation approaches leverage pretrained
text-to-image (T2I) diffusion model features for strong zero-shot transfer, but
typically group entire humans into a single person category, failing to
distinguish diverse clothing or detailed body parts. To address this, we
propose Spectrum, a unified network for part-level pixel parsing (body parts
and clothing) and instance-level grouping. While diffusion-based
open-vocabulary models generalize well across tasks, their internal
representations are not specialized for detailed human parsing. We observe
that, unlike diffusion models with broad representations, image-driven 3D
texture generators maintain faithful correspondence to input images, enabling
stronger representations for parsing diverse clothing and body parts. Spectrum
introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --
obtained by fine-tuning a T2I model on 3D human texture maps -- for improved
alignment with body parts and clothing. From an input image, we extract
human-part internal features via the I2Tx diffusion model and generate
semantically valid masks aligned to diverse clothing categories through
prompt-guided grounding. Once trained, Spectrum produces semantic segmentation
maps for every visible body part and clothing category, ignoring standalone
garments or irrelevant objects, for any number of humans in the scene. We
conduct extensive cross-dataset experiments -- separately assessing body parts,
clothing parts, unseen clothing categories, and full-body masks -- and
demonstrate that Spectrum consistently outperforms baseline methods in
prompt-based segmentation.

</details>


### [34] [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/abs/2508.06033)
*Yiming Gong,Zhen Zhu,Minjia Zhang*

Main category: cs.CV

TL;DR: InstantEdit 是一种基于 RectifiedFlow 的快速文本引导图像编辑方法，通过多项创新技术实现高效且高质量的编辑，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导图像编辑方法在速度和效果上存在不足，InstantEdit 旨在通过改进框架和技术，实现更快速且高质量的编辑。

Method: InstantEdit 采用 RectifiedFlow 框架，结合 PerRFI 反转策略、Inversion Latent Injection 再生方法、Disentangled Prompt Guidance 技术和 Canny-conditioned ControlNet，实现高效图像编辑。

Result: 在 PIE 数据集上，InstantEdit 在速度和编辑质量上均优于现有方法，展示了其优越性。

Conclusion: InstantEdit 基于 RectifiedFlow 框架，通过 PerRFI 反转策略、Inversion Latent Injection 再生方法、Disentangled Prompt Guidance 技术和 Canny-conditioned ControlNet 的整合，实现了快速且高质量的文本引导图像编辑。在 PIE 数据集上的评估表明，其速度和效果均优于现有方法。

Abstract: We propose a fast text-guided image editing method called InstantEdit based
on the RectifiedFlow framework, which is structured as a few-step editing
process that preserves critical content while following closely to textual
instructions. Our approach leverages the straight sampling trajectories of
RectifiedFlow by introducing a specialized inversion strategy called PerRFI. To
maintain consistent while editable results for RectifiedFlow model, we further
propose a novel regeneration method, Inversion Latent Injection, which
effectively reuses latent information obtained during inversion to facilitate
more coherent and detailed regeneration. Additionally, we propose a
Disentangled Prompt Guidance technique to balance editability with detail
preservation, and integrate a Canny-conditioned ControlNet to incorporate
structural cues and suppress artifacts. Evaluation on the PIE image editing
dataset demonstrates that InstantEdit is not only fast but also achieves better
qualitative and quantitative results compared to state-of-the-art few-step
editing methods.

</details>


### [35] [More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment](https://arxiv.org/abs/2508.06036)
*Jun Xie,Yingjian Zhu,Feng Chen,Zhenghao Zhang,Xiaohui Fan,Hongzhu Yi,Xinming Wang,Chen Yu,Yue Bi,Zhaoran Zhao,Xiongjun Guan,Zhepeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种混合专家（MoE）情感识别系统，整合多种输入模态和伪标签策略，在MER2025-SEMI测试集上F1分数达0.8772，排名第二。


<details>
  <summary>Details</summary>
Motivation: 为了解决半监督学习赛道（MER-SEMI）中的情感识别问题，并利用未标记数据提升模型性能。

Method: 提出了一种基于“越多越好”原则的混合专家（MoE）情感识别系统框架，整合了多种输入模态作为独立专家，并引入了基于共识的伪标签策略和两阶段训练范式。

Result: 在MER2025-SEMI挑战数据集上，方法取得了0.8772的F1分数，排名第二。

Conclusion: 本文提出的方法在MER2025-SEMI挑战赛测试集上取得了0.8772的F1分数，排名第二。

Abstract: In this paper, we present our solution for the semi-supervised learning track
(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the
principle that "more is better," to construct a robust Mixture of Experts (MoE)
emotion recognition system. Our approach integrates a diverse range of input
modalities as independent experts, including novel signals such as knowledge
from large Vision-Language Models (VLMs) and temporal Action Unit (AU)
information. To effectively utilize unlabeled data, we introduce a
consensus-based pseudo-labeling strategy, generating high-quality labels from
the agreement between a baseline model and Gemini, which are then used in a
two-stage training paradigm. Finally, we employ a multi-expert voting ensemble
combined with a rule-based re-ranking process to correct prediction bias and
better align the outputs with human preferences. Evaluated on the MER2025-SEMI
challenge dataset, our method achieves an F1-score of 0.8772 on the test set,
ranking 2nd in the track. Our code is available at
https://github.com/zhuyjan/MER2025-MRAC25.

</details>


### [36] [Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)
*Huanyu Wang,Jushi Kai,Haoli Bai,Lu Hou,Bo Jiang,Ziwei He,Zhouhan Lin*

Main category: cs.CV

TL;DR: Fourier-VLM通过频域压缩视觉表示，降低计算开销，提升推理速度，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）中视觉标记数量大导致计算开销高，现有方法在性能或额外成本上存在妥协。

Method: 提出了一种基于二维离散余弦变换（DCT）的低通滤波方法，利用FFT高效计算，压缩视觉表示。

Result: 在多个基准测试中表现竞争性，推理FLOPs减少83.8%，生成速度提升31.2%。

Conclusion: Fourier-VLM通过频域压缩视觉表示，显著降低了计算开销和推理延迟，同时保持了竞争性的性能，展示了其高效性和实用性。

Abstract: Vision-Language Models (VLMs) typically replace the predefined image
placeholder token (<image>) in textual instructions with visual features from
an image encoder, forming the input to a backbone Large Language Model (LLM).
However, the large number of vision tokens significantly increases the context
length, leading to high computational overhead and inference latency. While
previous efforts mitigate this by selecting only important visual features or
leveraging learnable queries to reduce token count, they often compromise
performance or introduce substantial extra costs. In response, we propose
Fourier-VLM, a simple yet efficient method that compresses visual
representations in the frequency domain. Our approach is motivated by the
observation that vision features output from the vision encoder exhibit
concentrated energy in low-frequency components. Leveraging this, we apply a
low-pass filter to the vision features using a two-dimentional Discrete Cosine
Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier
Transform (FFT) operator with a time complexity of $\mathcal{O}(n\log n)$,
minimizing the extra computational cost while introducing no additional
parameters. Extensive experiments across various image-based benchmarks
demonstrate that Fourier-VLM achieves competitive performance with strong
generalizability across both LLaVA and Qwen-VL architectures. Crucially, it
reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%
compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.

</details>


### [37] [NEP: Autoregressive Image Editing via Next Editing Token Prediction](https://arxiv.org/abs/2508.06044)
*Huimin Wu,Xiaojian Ma,Haozhe Zhao,Yanpeng Zhao,Qing Li*

Main category: cs.CV

TL;DR: 提出NEP方法，通过自回归图像生成选择性编辑图像区域，避免了不必要的计算和偏差，实现了零样本编辑和测试时缩放，达到了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成整个目标图像而非选择性再生编辑区域，导致不必要的计算成本和未编辑区域的偏差，影响了编辑质量。

Method: 提出了一种基于自回归图像生成的下一个编辑令牌预测（NEP）方法，并预训练了一个任意顺序自回归文本到图像（T2I）模型，以实现零样本图像编辑。

Result: 该方法在广泛使用的图像编辑基准上达到了新的最先进水平，并支持通过零样本方式迭代优化生成的测试时缩放（TTS）。

Conclusion: 通过将图像编辑重新定义为基于自回归图像生成的下一个编辑令牌预测（NEP），该方法成功实现了仅对需要编辑的区域进行选择性再生，避免了不必要的计算成本和未编辑区域的偏差，从而在广泛使用的图像编辑基准上达到了新的最先进水平。

Abstract: Text-guided image editing involves modifying a source image based on a
language instruction and, typically, requires changes to only small local
regions. However, existing approaches generate the entire target image rather
than selectively regenerate only the intended editing areas. This results in
(1) unnecessary computational costs and (2) a bias toward reconstructing
non-editing regions, which compromises the quality of the intended edits. To
resolve these limitations, we propose to formulate image editing as Next
Editing-token Prediction (NEP) based on autoregressive image generation, where
only regions that need to be edited are regenerated, thus avoiding unintended
modification to the non-editing areas. To enable any-region editing, we propose
to pre-train an any-order autoregressive text-to-image (T2I) model. Once
trained, it is capable of zero-shot image editing and can be easily adapted to
NEP for image editing, which achieves a new state-of-the-art on widely used
image editing benchmarks. Moreover, our model naturally supports test-time
scaling (TTS) through iteratively refining its generation in a zero-shot
manner. The project page is: https://nep-bigai.github.io/

</details>


### [38] [VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning](https://arxiv.org/abs/2508.06051)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Jun Jia,Kaiwei Zhang,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: VQAThinker通过强化学习和多模态模型解决了视频质量评估的泛化和可解释性问题，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有VQA模型在泛化到分布外视频和可解释性方面存在局限，限制了其在实际场景中的应用。

Method: 提出了VQAThinker框架，利用大型多模态模型（LMMs）和强化学习（特别是GRPO算法）来模拟人类感知决策，并引入了三种VQA特定的奖励机制。

Result: VQAThinker在域内和OOD VQA基准测试中均达到最先进性能，并在视频质量理解任务中表现出优于现有可解释VQA模型和LMMs的能力。

Conclusion: 强化学习为仅使用分数级监督构建通用且可解释的视频质量评估模型提供了有效途径。

Abstract: Video quality assessment (VQA) aims to objectively quantify perceptual
quality degradation in alignment with human visual perception. Despite recent
advances, existing VQA models still suffer from two critical limitations:
\textit{poor generalization to out-of-distribution (OOD) videos} and
\textit{limited explainability}, which restrict their applicability in
real-world scenarios. To address these challenges, we propose
\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large
multimodal models (LMMs) with reinforcement learning to jointly model video
quality understanding and scoring, emulating human perceptual decision-making.
Specifically, we adopt group relative policy optimization (GRPO), a rule-guided
reinforcement learning algorithm that enables reasoning over video quality
under score-level supervision, and introduce three VQA-specific rewards: (1) a
\textbf{bell-shaped regression reward} that increases rapidly as the prediction
error decreases and becomes progressively less sensitive near the ground truth;
(2) a \textbf{pairwise ranking reward} that guides the model to correctly
determine the relative quality between video pairs; and (3) a \textbf{temporal
consistency reward} that encourages the model to prefer temporally coherent
videos over their perturbed counterparts. Extensive experiments demonstrate
that VQAThinker achieves state-of-the-art performance on both in-domain and OOD
VQA benchmarks, showing strong generalization for video quality scoring.
Furthermore, evaluations on video quality understanding tasks validate its
superiority in distortion attribution and quality description compared to
existing explainable VQA models and LMMs. These findings demonstrate that
reinforcement learning offers an effective pathway toward building
generalizable and explainable VQA models solely with score-level supervision.

</details>


### [39] [AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?](https://arxiv.org/abs/2508.06057)
*Mojtaba Valipour,Kelly Zheng,James Lowman,Spencer Szabados,Mike Gartner,Bobby Braswell*

Main category: cs.CV

TL;DR: 本文探讨了地球观测数据在AGI中的重要性，指出了现有基准的不足，并提出了一套新的任务集以改进模型评估。


<details>
  <summary>Details</summary>
Motivation: 尽管人工通用智能（AGI）在多模态数据处理方面取得了进展，但卫星光谱图像作为一种额外模态尚未得到应有的关注，其在提升AGI理解自然世界能力方面具有巨大潜力。

Method: 本文论证了地球观测数据对智能模型的有用性，并回顾了现有基准及其在评估基础模型在该领域泛化能力方面的局限性。

Result: 本文提出了一个全面的任务集，旨在为地球观测数据领域建立一个更有效的模型评估基准。

Conclusion: 本文强调了需要一个更全面的基准来评估地球观测模型，并提出了一套全面的任务集，以有效评估模型理解和交互地球观测数据的能力。

Abstract: Artificial General Intelligence (AGI) is closer than ever to becoming a
reality, sparking widespread enthusiasm in the research community to collect
and work with various modalities, including text, image, video, and audio.
Despite recent efforts, satellite spectral imagery, as an additional modality,
has yet to receive the attention it deserves. This area presents unique
challenges, but also holds great promise in advancing the capabilities of AGI
in understanding the natural world. In this paper, we argue why Earth
Observation data is useful for an intelligent model, and then we review
existing benchmarks and highlight their limitations in evaluating the
generalization ability of foundation models in this domain. This paper
emphasizes the need for a more comprehensive benchmark to evaluate earth
observation models. To facilitate this, we propose a comprehensive set of tasks
that a benchmark should encompass to effectively assess a model's ability to
understand and interact with Earth observation data.

</details>


### [40] [Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention](https://arxiv.org/abs/2508.06058)
*Shiyang Zhou,Haijin Zeng,Yunfan Lu,Yongyong Chen,Jie Liu,Jingyong Su*

Main category: cs.CV

TL;DR: TSANet是一种轻量级两阶段网络，通过状态空间增强的交叉注意力有效解决HybridEVS相机的去马赛克问题，性能优于现有方法且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 尽管HybridEVS相机在移动摄影中具有优势，但将Quad Bayer CFA传感器与无颜色信息的事件像素结合会导致去马赛克过程中的混叠和伪影问题。现有方法难以解决这些问题，尤其是在资源有限的移动设备上。

Method: TSANet是一个轻量级的两阶段网络，通过状态空间增强的交叉注意力处理事件像素修复和去马赛克。此外，引入了轻量级的Cross-Swin State Block，利用位置先验进行去马赛克，并通过线性复杂度的状态空间模型增强全局依赖。

Result: TSANet在七个不同数据集上PSNR和SSIM指标均优于DemosaicFormer，同时显著降低了参数和计算成本。

Conclusion: TSANet展示了在HybridEVS模拟和真实数据上的优秀去马赛克性能，同时保持轻量级模型，平均表现优于先前的最先进方法DemosaicFormer，并在参数和计算成本上分别减少了1.86倍和3.29倍。

Abstract: Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera
capture brightness changes as asynchronous "events" instead of frames, offering
advanced application on mobile photography. However, challenges arise from
combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels
lacking color information, resulting in aliasing and artifacts on the
demosaicing process before downstream application. Current methods struggle to
address these issues, especially on resource-limited mobile devices. In
response, we introduce \textbf{TSANet}, a lightweight \textbf{T}wo-stage
network via \textbf{S}tate space augmented cross-\textbf{A}ttention, which can
handle event pixels inpainting and demosaicing separately, leveraging the
benefits of dividing complex tasks into manageable subtasks. Furthermore, we
introduce a lightweight Cross-Swin State Block that uniquely utilizes
positional prior for demosaicing and enhances global dependencies through the
state space model with linear complexity. In summary, TSANet demonstrates
excellent demosaicing performance on both simulated and real data of HybridEVS
while maintaining a lightweight model, averaging better results than the
previous state-of-the-art method DemosaicFormer across seven diverse datasets
in both PSNR and SSIM, while respectively reducing parameter and computation
costs by $1.86\times$ and $3.29\times$. Our approach presents new possibilities
for efficient image demosaicing on mobile devices. Code is available in the
supplementary materials.

</details>


### [41] [Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection](https://arxiv.org/abs/2508.06063)
*Chao Hao,Zitong Yu,Xin Liu,Yuhao Wang,Weicheng Xie,Jingang Shi,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TL;DR: 提出SCJoint联合学习方案和SBSS策略，训练通用网络JoNet同时处理SOD和COD任务，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为SOD和COD的联合学习会混淆网络性能，但本文提出相反视角：通过正确学习方法，网络可同时具备两种能力。

Method: 提出SCJoint联合学习方案，通过插入少量任务特定可学习参数来解耦SOD和COD的解码过程分布差异，并采用SBSS策略平衡训练集质量和大小。

Result: 实验证明JoNet在SOD和COD任务上均表现优异，验证了方法的有效性。

Conclusion: SCJoint和SBSS策略有效解决了SOD和COD任务的联合学习问题，JoNet网络展示了同时捕获显著和伪装物体的能力，实验验证了方法的竞争力和有效性。

Abstract: Salient object detection (SOD) and camouflaged object detection (COD) are two
closely related but distinct computer vision tasks. Although both are
class-agnostic segmentation tasks that map from RGB space to binary space, the
former aims to identify the most salient objects in the image, while the latter
focuses on detecting perfectly camouflaged objects that blend into the
background in the image. These two tasks exhibit strong contradictory
attributes. Previous works have mostly believed that joint learning of these
two tasks would confuse the network, reducing its performance on both tasks.
However, here we present an opposite perspective: with the correct approach to
learning, the network can simultaneously possess the capability to find both
salient and camouflaged objects, allowing both tasks to benefit from joint
learning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,
assuming that the decoding processes of SOD and COD have different distribution
characteristics. The key to our method is to learn the respective means and
variances of the decoding processes for both tasks by inserting a minimal
amount of task-specific learnable parameters within a fully shared network
structure, thereby decoupling the contradictory attributes of the two tasks at
a minimal cost. Furthermore, we propose a saliency-based sampling strategy
(SBSS) to sample the training set of the SOD task to balance the training set
sizes of the two tasks. In addition, SBSS improves the training set quality and
shortens the training time. Based on the proposed SCJoint and SBSS, we train a
powerful generalist network, named JoNet, which has the ability to
simultaneously capture both ``salient" and ``camouflaged". Extensive
experiments demonstrate the competitive performance and effectiveness of our
proposed method. The code is available at https://github.com/linuxsino/JoNet.

</details>


### [42] [Can Large Models Fool the Eye? A New Turing Test for Biological Animation](https://arxiv.org/abs/2508.06072)
*Zijian Chen,Lirong Deng,Zhengyu Chen,Kaiwei Zhang,Qi Jia,Yuan Tian,Yucheng Zhu,Guangtao Zhai*

Main category: cs.CV

TL;DR: BioMotion Arena 是一个通过视觉动画评估LLMs和MLLMs的新框架，利用点光源成像和成对比较，揭示了模型在生物运动生成上的显著不足，提供了直观的性能反馈。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试要么基于静态数据集的真实评分，要么采用模糊的聊天机器人式人类偏好收集，无法为用户提供直观、即时的性能差异反馈。

Method: 采用点光源成像技术，利用生物运动变体的视觉感知特性，通过成对比较评估收集了超过45k票的人类投票数据，分析了53种主流LLMs和MLLMs在90种生物运动变体上的表现。

Result: 数据分析显示，众包的人类投票与专家评分高度一致，证明了BioMotion Arena在提供区分性反馈方面的优越性。超过90%的评估模型（包括先进的InternVL3和Claude-4系列）无法生成基本的人形点光源组，更不用说流畅且生物学合理的运动。

Conclusion: BioMotion Arena 作为一个新颖的评估框架，通过视觉动画有效展示了大型语言模型和多模态大型语言模型的性能差异，并提供了一个无需依赖真实数据的灵活评估方法。

Abstract: Evaluating the abilities of large models and manifesting their gaps are
challenging. Current benchmarks adopt either ground-truth-based score-form
evaluation on static datasets or indistinct textual chatbot-style human
preferences collection, which may not provide users with immediate, intuitive,
and perceptible feedback on performance differences. In this paper, we
introduce BioMotion Arena, a novel framework for evaluating large language
models (LLMs) and multimodal large language models (MLLMs) via visual
animation. Our methodology draws inspiration from the inherent visual
perception of motion patterns characteristic of living organisms that utilizes
point-light source imaging to amplify the performance discrepancies between
models. Specifically, we employ a pairwise comparison evaluation and collect
more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion
variants. Data analyses show that the crowd-sourced human votes are in good
agreement with those of expert raters, demonstrating the superiority of our
BioMotion Arena in offering discriminative feedback. We also find that over
90\% of evaluated models, including the cutting-edge open-source InternVL3 and
proprietary Claude-4 series, fail to produce fundamental humanoid point-light
groups, much less smooth and biologically plausible motions. This enables
BioMotion Arena to serve as a challenging benchmark for performance
visualization and a flexible evaluation framework without restrictions on
ground-truth.

</details>


### [43] [Towards MR-Based Trochleoplasty Planning](https://arxiv.org/abs/2508.06076)
*Michael Wehrli,Alicia Durrer,Paul Friedrich,Sidaty El Hadramy,Edwin Li,Luana Brahaj,Carol C. Hasler,Philippe C. Cattin*

Main category: cs.CV

TL;DR: 本文提出了一种从常规临床MR扫描生成高分辨率伪健康3D形态的流程，显著改善了TD患者的治疗效果，并减少了辐射暴露。


<details>
  <summary>Details</summary>
Motivation: 当前Trochlear Dysplasia（TD）治疗方法依赖低分辨率MR扫描和外科医生的直觉，导致手术结果不一致且微创技术应用有限。

Method: 1. 使用隐式神经表示（INR）计算各向同性的超分辨率MR体积。
2. 通过多标签定制训练网络分割股骨、胫骨、髌骨和腓骨。
3. 训练小波扩散模型（WDM）生成伪健康的滑车区域目标形态。

Result: 在25名TD患者中验证了方法，显著改善了滑车角度（SA）和滑车沟深度（TGD）。

Conclusion: 本文提出的方法通过生成高分辨率的伪健康目标形态，显著改善了Trochlear Dysplasia（TD）患者的治疗结果，减少了手术的不确定性，并降低了辐射暴露。

Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on
low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.
The surgeries are planned based on surgeons experience, have limited adoption
of minimally invasive techniques, and lead to inconsistent outcomes. We propose
a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy
target morphologies from conventional clinical MR scans. First, we compute an
isotropic super-resolved MR volume using an Implicit Neural Representation
(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label
custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to
generate pseudo-healthy target morphologies of the trochlear region. In
contrast to prior work producing pseudo-healthy low-resolution 3D MR images,
our approach enables the generation of sub-millimeter resolved 3D shapes
compatible for pre- and intraoperative use. These can serve as preoperative
blueprints for reshaping the femoral groove while preserving the native patella
articulation. Furthermore, and in contrast to other work, we do not require a
CT for our pipeline - reducing the amount of radiation. We evaluated our
approach on 25 TD patients and could show that our target morphologies
significantly improve the sulcus angle (SA) and trochlear groove depth (TGD).
The code and interactive visualization are available at
https://wehrlimi.github.io/sr-3d-planning/.

</details>


### [44] [DreamVE: Unified Instruction-based Image and Video Editing](https://arxiv.org/abs/2508.06080)
*Bin Xia,Jiyang Liu,Yuechen Zhang,Bohao Peng,Ruihang Chu,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: DreamVE是一个统一的基于指令的图像和视频编辑模型，采用两阶段训练策略，结合拼贴和生成模型数据合成方法，提升编辑性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的编辑方法，尤其是视频编辑，因训练数据有限而应用受限。

Method: 提出两阶段训练策略：先图像编辑后视频编辑，结合拼贴和生成模型数据合成方法，并设计高效编辑框架。

Result: DreamVE在关键编辑类型上表现优异，并通过生成模型数据微调提升属性编辑性能。

Conclusion: DreamVE通过统一训练策略和多样化数据合成，显著提升了基于指令的图像和视频编辑能力。

Abstract: Instruction-based editing holds vast potential due to its simple and
efficient interactive editing format. However, instruction-based editing,
particularly for video, has been constrained by limited training data,
hindering its practical application. To this end, we introduce DreamVE, a
unified model for instruction-based image and video editing. Specifically, We
propose a two-stage training strategy: first image editing, then video editing.
This offers two main benefits: (1) Image data scales more easily, and models
are more efficient to train, providing useful priors for faster and better
video editing training. (2) Unifying image and video generation is natural and
aligns with current trends. Moreover, we present comprehensive training data
synthesis pipelines, including collage-based and generative model-based data
synthesis. The collage-based data synthesis combines foreground objects and
backgrounds to generate diverse editing data, such as object manipulation,
background changes, and text modifications. It can easily generate billions of
accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE
on extensive collage-based data to achieve strong performance in key editing
types and enhance generalization and transfer capabilities. However,
collage-based data lacks some attribute editing cases, leading to a relative
drop in performance. In contrast, the generative model-based pipeline, despite
being hard to scale up, offers flexibility in handling attribute editing cases.
Therefore, we use generative model-based data to further fine-tune DreamVE.
Besides, we design an efficient and powerful editing framework for DreamVE. We
build on the SOTA T2V model and use a token concatenation with early drop
approach to inject source image guidance, ensuring strong consistency and
editability. The codes and models will be released.

</details>


### [45] [SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment](https://arxiv.org/abs/2508.06082)
*Yanxiao Sun,Jiafu Wu,Yun Cao,Chengming Xu,Yabiao Wang,Weijian Cao,Donghao Luo,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: SwiftVideo 结合轨迹保持和分布匹配，通过连续时间一致性蒸馏和双视角对齐，显著提升少步视频生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散或流的视频生成模型需要多次迭代采样，计算开销大；现有蒸馏方法在少步设置下性能下降或产生更多伪影。

Method: 提出了一种统一的稳定蒸馏框架 SwiftVideo，包括连续时间一致性蒸馏和双视角对齐（分布对齐与轨迹对齐）。

Result: 在 OpenVid-1M 基准测试中，SwiftVideo 在少步视频生成上显著优于其他方法，同时保持高质量生成。

Conclusion: SwiftVideo 通过结合轨迹保持和分布匹配策略，显著提升了少步视频生成的质量和效率，在 OpenVid-1M 基准测试中表现优于现有方法。

Abstract: Diffusion-based or flow-based models have achieved significant progress in
video synthesis but require multiple iterative sampling steps, which incurs
substantial computational overhead. While many distillation methods that are
solely based on trajectory-preserving or distribution-matching have been
developed to accelerate video generation models, these approaches often suffer
from performance breakdown or increased artifacts under few-step settings. To
address these limitations, we propose \textbf{\emph{SwiftVideo}}, a unified and
stable distillation framework that combines the advantages of
trajectory-preserving and distribution-matching strategies. Our approach
introduces continuous-time consistency distillation to ensure precise
preservation of ODE trajectories. Subsequently, we propose a dual-perspective
alignment that includes distribution alignment between synthetic and real data
along with trajectory alignment across different inference steps. Our method
maintains high-quality video generation while substantially reducing the number
of inference steps. Quantitative evaluations on the OpenVid-1M benchmark
demonstrate that our method significantly outperforms existing approaches in
few-step video generation.

</details>


### [46] [AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance](https://arxiv.org/abs/2508.06084)
*Weichen Zhang,Zhui Zhu,Ningbo Li,Kebin Liu,Yunhao Liu*

Main category: cs.CV

TL;DR: AdaptInfer通过动态文本引导和跨模态注意力优化，显著提升视觉语言模型推理效率，保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在推理过程中处理大量视觉令牌导致计算成本高，现有剪枝方法未能充分利用动态内部信号。

Method: 提出AdaptInfer框架，包括细粒度动态文本引导剪枝机制和基于跨模态注意力分析的剪枝策略，实现自适应视觉令牌剪枝。

Result: 实验表明，AdaptInfer在LLaVA-1.5-7B上将CUDA延迟降低61.3%，同时保持92.9%的平均准确率，并在相同令牌预算下超越现有最优方法。

Conclusion: AdaptInfer是一种轻量级即插即用框架，通过动态文本引导的剪枝机制和跨模态注意力分析，显著提升了视觉语言模型（VLMs）的推理效率，同时保持高准确率。

Abstract: Vision-language models (VLMs) have achieved impressive performance on
multimodal reasoning tasks such as visual question answering (VQA), but their
inference cost remains a significant challenge due to the large number of
vision tokens processed during the prefill stage. Existing pruning methods
often rely on directly using the attention patterns or static text prompt
guidance, failing to exploit the dynamic internal signals generated during
inference. To address these issues, we propose AdaptInfer, a plug-and-play
framework for adaptive vision token pruning in VLMs. First, we introduce a
fine-grained, dynamic text-guided pruning mechanism that reuses layer-wise
text-to-text attention maps to construct soft priors over text-token
importance, allowing more informed scoring of vision tokens at each stage.
Second, we perform an offline analysis of cross-modal attention shifts and
identify consistent inflection locations in inference, which inspire us to
propose a more principled and efficient pruning schedule. Our method is
lightweight and plug-and-play, also generalizable across multi-modal tasks.
Experimental results have verified the effectiveness of the proposed method.
For example, it reduces CUDA latency by 61.3\% while maintaining an average
accuracy of 92.9\% on vanilla LLaVA-1.5-7B. Under the same token budget,
AdaptInfer surpasses SOTA in accuracy.

</details>


### [47] [Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation](https://arxiv.org/abs/2508.06092)
*Yachun Mi,Yu Li,Yanting Li,Shixin Sun,Chen Hui,Tong Zhang,Yuanyuan Liu,Chenyue Song,Shaohui Liu*

Main category: cs.CV

TL;DR: Q-CLIP 是一种基于视觉语言模型的 VQA 框架，通过共享跨模态适配器和质量级别提示降低计算成本并提升性能，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前主流的 VQA 方法依赖于大规模分类数据集的预训练，但存在语义知识迁移不足和计算资源消耗巨大的问题。视觉语言模型（VLMs）在视觉任务中表现出强大的泛化能力，为 VQA 提供了新的可能性。

Method: Q-CLIP 通过共享跨模态适配器（SCMA）增强视觉和文本表示，仅需训练少量参数，同时引入五个可学习的质量级别提示以提升模型对视频质量变化的敏感性。此外，研究还探索了不同帧采样策略对 VQA 性能的影响。

Result: 实验结果表明，Q-CLIP 在多个 VQA 数据集上表现优异，且计算成本显著降低。基于帧差分的采样策略进一步提升了模型在不同数据集上的泛化性能。

Conclusion: Q-CLIP 是一种基于视觉语言模型（VLMs）的视频质量评估（VQA）框架，通过共享跨模态适配器（SCMA）和质量级别提示显著降低了计算成本，并在多个 VQA 数据集上表现出优异的性能。

Abstract: Accurate and efficient Video Quality Assessment (VQA) has long been a key
research challenge. Current mainstream VQA methods typically improve
performance by pretraining on large-scale classification datasets (e.g.,
ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this
strategy presents two significant challenges: (1) merely transferring semantic
knowledge learned from pretraining is insufficient for VQA, as video quality
depends on multiple factors (e.g., semantics, distortion, motion, aesthetics);
(2) pretraining on large-scale datasets demands enormous computational
resources, often dozens or even hundreds of times greater than training
directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown
remarkable generalization capabilities across a wide range of visual tasks, and
have begun to demonstrate promising potential in quality assessment. In this
work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP
enhances both visual and textual representations through a Shared Cross-Modal
Adapter (SCMA), which contains only a minimal number of trainable parameters
and is the only component that requires training. This design significantly
reduces computational cost. In addition, we introduce a set of five learnable
quality-level prompts to guide the VLMs in perceiving subtle quality
variations, thereby further enhancing the model's sensitivity to video quality.
Furthermore, we investigate the impact of different frame sampling strategies
on VQA performance, and find that frame-difference-based sampling leads to
better generalization performance across datasets. Extensive experiments
demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.

</details>


### [48] [E-React: Towards Emotionally Controlled Synthesis of Human Reactions](https://arxiv.org/abs/2508.06093)
*Chen Zhu,Buzhen Huang,Zijing Wu,Binghui Zuo,Yangang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种半监督情感先验与actor-reactor扩散模型结合的方法，用于生成情感驱动的多样化反应动作，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的人体动作生成框架未考虑情感影响，导致自然度不足，限制了其在交互任务中的应用。

Method: 通过半监督学习框架训练情感先验，并利用actor-reactor扩散模型生成考虑空间交互和情感响应的反应动作。

Result: 模型能够根据演员动作序列生成多样化的情感驱动反应，实验结果显示其优于现有方法。

Conclusion: 本文提出的基于半监督情感先验的actor-reactor扩散模型在情感驱动反应合成任务中表现优异，实验证明其优于现有方法。

Abstract: Emotion serves as an essential component in daily human interactions.
Existing human motion generation frameworks do not consider the impact of
emotions, which reduces naturalness and limits their application in interactive
tasks, such as human reaction synthesis. In this work, we introduce a novel
task: generating diverse reaction motions in response to different emotional
cues. However, learning emotion representation from limited motion data and
incorporating it into a motion generation framework remains a challenging
problem. To address the above obstacles, we introduce a semi-supervised emotion
prior in an actor-reactor diffusion model to facilitate emotion-driven reaction
synthesis. Specifically, based on the observation that motion clips within a
short sequence tend to share the same emotion, we first devise a
semi-supervised learning framework to train an emotion prior. With this prior,
we further train an actor-reactor diffusion model to generate reactions by
considering both spatial interaction and emotional response. Finally, given a
motion sequence of an actor, our approach can generate realistic reactions
under various emotional conditions. Experimental results demonstrate that our
model outperforms existing reaction generation methods. The code and data will
be made publicly available at https://ereact.github.io/

</details>


### [49] [UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization](https://arxiv.org/abs/2508.06101)
*Yachun Mi,Xingyang He,Shixin Sun,Yu Li,Yanting Li,Zhixuan Li,Jian Jin,Chen Hui,Shaohui Liu*

Main category: cs.CV

TL;DR: UGD-IML首次通过扩散模型统一IML和CIML任务，减少了数据依赖并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有IML方法依赖大规模标注数据和CIML方法流程复杂的问题。

Method: 提出了一种基于扩散模型的生成框架UGD-IML，通过类嵌入机制和参数共享设计，实现了IML和CIML任务的无缝切换。

Result: 在多个数据集上，UGD-IML在IML和CIML任务中的F1指标分别平均提升了9.66和4.36。

Conclusion: UGD-IML通过生成式扩散模型统一了IML和CIML任务，显著提升了检测性能，并在不确定性估计、可视化和鲁棒性方面表现出色。

Abstract: In the digital age, advanced image editing tools pose a serious threat to the
integrity of visual content, making image forgery detection and localization a
key research focus. Most existing Image Manipulation Localization (IML) methods
rely on discriminative learning and require large, high-quality annotated
datasets. However, current datasets lack sufficient scale and diversity,
limiting model performance in real-world scenarios. To overcome this, recent
studies have explored Constrained IML (CIML), which generates pixel-level
annotations through algorithmic supervision. However, existing CIML approaches
often depend on complex multi-stage pipelines, making the annotation process
inefficient. In this work, we propose a novel generative framework based on
diffusion models, named UGD-IML, which for the first time unifies both IML and
CIML tasks within a single framework. By learning the underlying data
distribution, generative diffusion models inherently reduce the reliance on
large-scale labeled datasets, allowing our approach to perform effectively even
under limited data conditions. In addition, by leveraging a class embedding
mechanism and a parameter-sharing design, our model seamlessly switches between
IML and CIML modes without extra components or training overhead. Furthermore,
the end-to-end design enables our model to avoid cumbersome steps in the data
annotation process. Extensive experimental results on multiple datasets
demonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and
4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the
proposed method also excels in uncertainty estimation, visualization and
robustness.

</details>


### [50] [Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention](https://arxiv.org/abs/2508.06107)
*Shree Mitra,Ritabrata Chakraborty,Nilkanta Sahu*

Main category: cs.CV

TL;DR: 该论文提出了一种自监督学习框架，通过渐进式空间掩蔽和注意力机制提升手写数学表达式识别性能，无需大量标注数据，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手写数学表达式识别（HMER）由于二维结构、符号尺度变化和复杂空间关系的挑战，需要大量标注数据。本文旨在通过自监督学习减少对标注数据的依赖，并提升模型性能。

Method: 论文提出了一种自监督学习框架，包括三个步骤：(1) 使用全局和局部对比损失预训练图像编码器；(2) 通过渐进式空间掩蔽策略训练自监督注意力网络；(3) 使用Transformer解码器进行监督微调以生成LATEX序列。

Result: 在CROHME基准测试上的实验表明，该方法优于现有的自监督和全监督基线方法，验证了渐进式注意力机制的有效性。

Conclusion: 该论文提出的自监督学习框架在无需昂贵标注数据的情况下，通过渐进式空间掩蔽策略和自监督注意力网络，显著提升了手写数学表达式识别的性能，优于现有的自监督和全监督基线方法。

Abstract: Recognizing handwritten mathematical expressions (HMER) is a challenging task
due to the inherent two-dimensional structure, varying symbol scales, and
complex spatial relationships among symbols. In this paper, we present a
self-supervised learning (SSL) framework for HMER that eliminates the need for
expensive labeled data. Our approach begins by pretraining an image encoder
using a combination of global and local contrastive loss, enabling the model to
learn both holistic and fine-grained representations. A key contribution of
this work is a novel self-supervised attention network, which is trained using
a progressive spatial masking strategy. This attention mechanism is designed to
learn semantically meaningful focus regions, such as operators, exponents, and
nested mathematical notation, without requiring any supervision. The
progressive masking curriculum encourages the network to become increasingly
robust to missing or occluded visual information, ultimately improving
structural understanding. Our complete pipeline consists of (1) self-supervised
pretraining of the encoder, (2) self-supervised attention learning, and (3)
supervised fine-tuning with a transformer decoder to generate LATEX sequences.
Extensive experiments on CROHME benchmarks demonstrate that our method
outperforms existing SSL and fully supervised baselines, validating the
effectiveness of our progressive attention mechanism in enhancing HMER
performance. Our codebase can be found here.

</details>


### [51] [MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment](https://arxiv.org/abs/2508.06104)
*Gui Zou,Chaofan Gan,Chern Hong Lim,Supavadee Aramvith,Weiyao Lin*

Main category: cs.CV

TL;DR: MCA框架通过多模态联合标签校正和多级自适应对齐，有效解决了2D-3D跨模态检索中的噪声标签问题，实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理噪声标签时通常在单个模态内独立划分样本，容易对损坏标签过拟合。为解决这一问题，需要一个更鲁棒的解决方案。

Method: 提出了MCA框架，包括多模态联合标签校正（MJC）机制和多级自适应对齐（MAA）策略。MJC利用多模态历史自预测联合建模模态预测一致性，实现可靠的标签细化；MAA通过多级自适应对齐增强跨模态特征语义和区分度。

Result: MCA在2D-3D跨模态检索中表现出色，特别是在噪声标签条件下，显著优于现有方法。

Conclusion: MCA框架在传统和现实噪声3D基准测试中均表现出色，实现了最先进的性能，证明了其通用性和有效性。

Abstract: With the increasing availability of 2D and 3D data, significant advancements
have been made in the field of cross-modal retrieval. Nevertheless, the
existence of imperfect annotations presents considerable challenges, demanding
robust solutions for 2D-3D cross-modal retrieval in the presence of noisy label
conditions. Existing methods generally address the issue of noise by dividing
samples independently within each modality, making them susceptible to
overfitting on corrupted labels. To address these issues, we propose a robust
2D-3D \textbf{M}ulti-level cross-modal adaptive \textbf{C}orrection and
\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal
Joint label Correction (MJC) mechanism that leverages multimodal historical
self-predictions to jointly model the modality prediction consistency, enabling
reliable label refinement. Additionally, we propose a Multi-level Adaptive
Alignment (MAA) strategy to effectively enhance cross-modal feature semantics
and discrimination across different levels. Extensive experiments demonstrate
the superiority of our method, MCA, which achieves state-of-the-art performance
on both conventional and realistic noisy 3D benchmarks, highlighting its
generality and effectiveness.

</details>


### [52] [FMCE-Net++: Feature Map Convergence Evaluation and Training](https://arxiv.org/abs/2508.06109)
*Zhibo Zhu,Renyu Huang,Lei He*

Main category: cs.CV

TL;DR: FMCE-Net++通过动态平衡损失优化，无需改动架构即可提升DNN性能，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决DNN内部表征不透明问题，弥补FMCE缺乏实验验证和闭环集成的局限性。

Method: 提出FMCE-Net++训练框架，集成预训练的FMCE-Net作为辅助头，生成FMCS预测，结合任务标签通过RAL监督骨干网络优化。

Result: 在MNIST、CIFAR-10等数据集上验证，FMCE-Net++显著提升模型性能（如ResNet-50/CIFAR-10准确率提升1.16个百分点）。

Conclusion: FMCE-Net++通过引入Representation Auxiliary Loss（RAL）动态平衡分类损失和特征收敛优化，无需修改架构或增加数据即可提升模型性能。

Abstract: Deep Neural Networks (DNNs) face interpretability challenges due to their
opaque internal representations. While Feature Map Convergence Evaluation
(FMCE) quantifies module-level convergence via Feature Map Convergence Scores
(FMCS), it lacks experimental validation and closed-loop integration. To
address this limitation, we propose FMCE-Net++, a novel training framework that
integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module
generates FMCS predictions, which, combined with task labels, jointly supervise
backbone optimization through a Representation Auxiliary Loss. The RAL
dynamically balances the primary classification loss and feature convergence
optimization via a tunable \Representation Abstraction Factor. Extensive
experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100
demonstrate that FMCE-Net++ consistently enhances model performance without
architectural modifications or additional data. Key experimental outcomes
include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp
(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate
state-of-the-art performance ceilings.

</details>


### [53] [Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation](https://arxiv.org/abs/2508.06136)
*YoungChan Choi,HengFei Wang,YiHua Cheng,Boeun Kim,Hyung Jin Chang,YoungGeun Choi,Sang-Il Choi*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯泼溅的3D眼球结构框架，通过明确建模眼球的旋转和平移以及自适应变形模块，生成高质量且多样化的视线方向图像，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经辐射场（NeRF）的视线重定向方法无法明确建模3D表示的旋转和平移，因此需要一种更直接的方法来生成更真实的视线方向图像。

Method: 引入了一种专用的3D眼球结构，使用3D高斯泼溅（3DGS）来明确表示眼球的旋转和平移，并提出了一个自适应变形模块以复制眼部周围的细微肌肉运动。

Result: 在ETH-XGaze数据集上的实验表明，该方法能够生成多样化的新视线图像，图像质量和视线估计准确性均优于现有方法。

Conclusion: 该论文提出的基于3D高斯泼溅的3D眼球结构框架在生成多样化的新视点图像方面表现出色，图像质量和视线估计准确性均优于现有最先进方法。

Abstract: We propose a novel 3D gaze redirection framework that leverages an explicit
3D eyeball structure. Existing gaze redirection methods are typically based on
neural radiance fields, which employ implicit neural representations via volume
rendering. Unlike these NeRF-based approaches, where the rotation and
translation of 3D representations are not explicitly modeled, we introduce a
dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian
Splatting (3DGS). Our method generates photorealistic images that faithfully
reproduce the desired gaze direction by explicitly rotating and translating the
3D eyeball structure. In addition, we propose an adaptive deformation module
that enables the replication of subtle muscle movements around the eyes.
Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our
framework is capable of generating diverse novel gaze images, achieving
superior image quality and gaze estimation accuracy compared to previous
state-of-the-art methods.

</details>


### [54] [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](https://arxiv.org/abs/2508.06169)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Changting Lin,Jianfeng Dong,Chaochao Chen,Xun Zhou,Meng Han*

Main category: cs.CV

TL;DR: UW-3DGS是一种基于3D高斯泼溅的水下3D重建框架，通过物理模型和不确定性修剪提升了重建质量，减少了伪影。


<details>
  <summary>Details</summary>
Motivation: 水下环境的光吸收、散射和浑浊问题导致传统方法（如NeRF）在几何和颜色保真度上表现不佳，需要更高效的解决方案。

Method: UW-3DGS采用可学习的水下图像形成模块和物理感知不确定性修剪分支，通过体素回归和不确定性评分优化高斯分布。

Result: 在SeaThru-NeRF和UWBundle数据集上，UW-3DGS实现了PSNR 27.604、SSIM 0.868和LPIPS 0.104，漂浮伪影减少约65%。

Conclusion: UW-3DGS通过创新的3D高斯泼溅框架和物理感知不确定性修剪，显著提升了水下3D场景重建的几何和颜色保真度，减少了漂浮伪影。

Abstract: Underwater 3D scene reconstruction faces severe challenges from light
absorption, scattering, and turbidity, which degrade geometry and color
fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF
extensions such as SeaThru-NeRF incorporate physics-based models, their MLP
reliance limits efficiency and spatial resolution in hazy environments. We
introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for
robust underwater reconstruction. Key innovations include: (1) a plug-and-play
learnable underwater image formation module using voxel-based regression for
spatially varying attenuation and backscatter; and (2) a Physics-Aware
Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating
Gaussians via uncertainty scoring, ensuring artifact-free geometry. The
pipeline operates in training and rendering stages. During training, noisy
Gaussians are optimized end-to-end with underwater parameters, guided by PAUP
pruning and scattering modeling. In rendering, refined Gaussians produce clean
Unattenuated Radiance Images (URIs) free from media effects, while learned
physics enable realistic Underwater Images (UWIs) with accurate light
transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior
performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on
SeaThru-NeRF, with ~65% reduction in floating artifacts.

</details>


### [55] [Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation](https://arxiv.org/abs/2508.06170)
*Ojonugwa Oluwafemi Ejiga Peter,Akingbola Oluwapemiisin,Amalahu Chetachi,Adeniran Opeyemi,Fahmi Khalifa,Md Mahmudur Rahman*

Main category: cs.CV

TL;DR: 研究提出了一种结合Faster R-CNN和SAM的自动化息肉检测框架，通过合成数据增强和多种分割模型评估，显著提升了检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜检查是结直肠癌早期诊断的关键工具，而结直肠癌是全球癌症相关死亡的主要原因之一。自动化息肉检测技术对于预防和早期发现结直肠癌至关重要。

Method: 研究采用了Faster R-CNN进行初始目标定位，并结合Segment Anything Model (SAM)优化分割掩码。此外，还评估了五种先进的分割模型（U-Net、PSPNet、FPN、LinkNet和MANet），以ResNet34为基础模型。

Result: Faster R-CNN的召回率为93.08%，精确率为88.97%，F1分数为90.98%。FPN在PSNR（7.205893）和SSIM（0.492381）上表现最佳，而UNet在召回率（84.85%）上领先，LinkNet在IoU（64.20%）和Dice分数（77.53%）上表现均衡。

Conclusion: 研究提出了一种新颖的多方向架构框架，用于自动化结肠镜图像中的息肉检测，有效解决了医疗数据集的规模限制和标注复杂性问题。结合Stable Diffusion增强的合成数据生成及检测与分割算法，该系统表现出色。

Abstract: Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,
which is one of the main causes of cancer-related mortality globally; hence, it
is deemed an essential technique for the prevention and early detection of
colorectal cancer. The research introduces a unique multidirectional
architectural framework to automate polyp detection within colonoscopy images
while helping resolve limited healthcare dataset sizes and annotation
complexities. The research implements a comprehensive system that delivers
synthetic data generation through Stable Diffusion enhancements together with
detection and segmentation algorithms. This detection approach combines Faster
R-CNN for initial object localization while the Segment Anything Model (SAM)
refines the segmentation masks. The faster R-CNN detection algorithm achieved a
recall of 93.08% combined with a precision of 88.97% and an F1 score of
90.98%.SAM is then used to generate the image mask. The research evaluated five
state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,
and MANet using ResNet34 as a base model. The results demonstrate the superior
performance of FPN with the highest scores of PSNR (7.205893) and SSIM
(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced
performance in IoU (64.20%) and Dice score (77.53%).

</details>


### [56] [SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.06115)
*Weichen Zhang,Kebin Liu,Fan Dang,Zhui Zhu,Xikai Sun,Yunhao Liu*

Main category: cs.CV

TL;DR: SynSeg是一种新型弱监督语义分割方法，通过多类别对比学习和特征重构框架，显著提升了开放词汇场景下的分割性能。


<details>
  <summary>Details</summary>
Motivation: 开放词汇场景下的语义分割面临语义类别广泛性和细粒度的挑战，现有弱监督方法依赖类别特定监督和不适合的特征构建方法，导致语义错位和性能不佳。

Method: 提出了一种新颖的弱监督方法SynSeg，包括多类别对比学习（MCCL）策略和特征协同结构（FSS）框架。MCCL通过结合类别内和类别间的对齐与分离来增强模型学习能力，FSS通过先验融合和语义激活图增强来重构判别性特征。

Result: SynSeg在多个基准数据集上表现优异，例如在VOC上比现有最优方法提高了4.5%，在Context上提高了8.9%，在Object上提高了2.6%，在City上提高了2.0%。

Conclusion: SynSeg通过多类别对比学习（MCCL）和特征协同结构（FSS）显著提升了开放词汇场景下的语义分割性能，实验证明其在多个基准数据集上优于现有最优方法。

Abstract: Semantic segmentation in open-vocabulary scenarios presents significant
challenges due to the wide range and granularity of semantic categories.
Existing weakly-supervised methods often rely on category-specific supervision
and ill-suited feature construction methods for contrastive learning, leading
to semantic misalignment and poor performance. In this work, we propose a novel
weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs
Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a
new feature reconstruction framework named Feature Synergy Structure (FSS).
Specifically, MCCL strategy robustly combines both intra- and inter-category
alignment and separation in order to make the model learn the knowledge of
correlations from different categories within the same image. Moreover, FSS
reconstructs discriminative features for contrastive learning through prior
fusion and semantic-activation-map enhancement, effectively avoiding the
foreground bias introduced by the visual encoder. In general, SynSeg
effectively improves the abilities in semantic localization and discrimination
under weak supervision. Extensive experiments on benchmarks demonstrate that
our method outperforms state-of-the-art (SOTA) performance. For instance,
SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% on
Context, 2.6\% on Object and 2.0\% on City.

</details>


### [57] [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/abs/2508.06202)
*Chang Che,Ziqi Wang,Pengwan Yang,Qi Wang,Hui Ma,Zenglin Shi*

Main category: cs.CV

TL;DR: LiLoRA 是一种针对 MLLMs 持续视觉指令调优的高效架构扩展方法，通过共享矩阵和低秩分解减少参数，缓解灾难性遗忘，提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决持续视觉指令调优中因架构扩展导致的大参数开销和可扩展性差的问题，同时缓解灾难性遗忘。

Method: LiLoRA 方法包括共享 LoRA 矩阵 A 以减少冗余，对矩阵 B 进行额外的低秩分解以最小化任务特定参数，并引入余弦正则化稳定性损失来保持共享表示的一致性。

Result: 在多样化的 CVIT 基准测试中，LiLoRA 在顺序任务学习中表现优异，显著提高了参数效率。

Conclusion: LiLoRA 是一种高效的结构扩展方法，通过共享 LoRA 矩阵 A 和低秩分解矩阵 B，显著减少了参数开销，同时通过余弦正则化稳定性损失保持共享表示的一致性，在持续视觉指令调优中表现优异。

Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language
Models (MLLMs) to incrementally learn new tasks over time. However, this
process is challenged by catastrophic forgetting, where performance on
previously learned tasks deteriorates as the model adapts to new ones. A common
approach to mitigate forgetting is architecture expansion, which introduces
task-specific modules to prevent interference. Yet, existing methods often
expand entire layers for each task, leading to significant parameter overhead
and poor scalability. To overcome these issues, we introduce LoRA in LoRA
(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in
MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,
applies an additional low-rank decomposition to matrix B to minimize
task-specific parameters, and incorporates a cosine-regularized stability loss
to preserve consistency in shared representations over time. Extensive
experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves
superior performance in sequential task learning while significantly improving
parameter efficiency compared to existing approaches.

</details>


### [58] [Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events](https://arxiv.org/abs/2508.06122)
*Ting-Shuo Yo,Shih-Hao Su,Chien-Ming Wu,Wei-Ting Chen,Jung-Lien Chu,Chiao-Wei Chang,Hung-Chi Kuo*

Main category: cs.CV

TL;DR: 研究比较了PCA、CAE和PT在卫星图像天气分类中的表现，发现CAE效果最佳，但缺乏物理解释性，建议未来开发物理信息版本的CAE。


<details>
  <summary>Details</summary>
Motivation: 研究旨在比较不同表示学习算法在卫星图像中对天气事件分类的效果，并探索潜在空间的分辨率和大小对分类性能的影响。

Method: 本研究应用了表示学习算法（包括主成分分析（PCA）、卷积自编码器（CAE）和预训练的残差网络（PT））对卫星图像进行分析，并评估了学习到的潜在空间在各种天气事件分类中的表现。

Result: 实验结果表明，CAE学习的潜在空间在所有分类任务中均表现出较高的威胁分数。PCA的分类命中率高但误报率也高。PT在识别热带气旋方面表现优异，但在其他任务中表现较差。此外，高分辨率数据集对深度学习算法（CAE和PT）的分类任务表现更优，而潜在空间尺寸小于128会导致误报率显著增加。

Conclusion: 尽管卷积自编码器（CAE）能够有效且高效地学习潜在空间，但学习到的表示缺乏与物理属性的直接联系。因此，开发一个物理信息版本的CAE是当前工作的一个有望展望。

Abstract: This study applied representation learning algorithms to satellite images and
evaluated the learned latent spaces with classifications of various weather
events. The algorithms investigated include the classical linear
transformation, i.e., principal component analysis (PCA), state-of-the-art deep
learning method, i.e., convolutional autoencoder (CAE), and a residual network
pre-trained with large image datasets (PT). The experiment results indicated
that the latent space learned by CAE consistently showed higher threat scores
for all classification tasks. The classifications with PCA yielded high hit
rates but also high false-alarm rates. In addition, the PT performed
exceptionally well at recognizing tropical cyclones but was inferior in other
tasks. Further experiments suggested that representations learned from
higher-resolution datasets are superior in all classification tasks for
deep-learning algorithms, i.e., CAE and PT. We also found that smaller latent
space sizes had minor impact on the classification task's hit rate. Still, a
latent space dimension smaller than 128 caused a significantly higher false
alarm rate. Though the CAE can learn latent spaces effectively and efficiently,
the interpretation of the learned representation lacks direct connections to
physical attributions. Therefore, developing a physics-informed version of CAE
can be a promising outlook for the current work.

</details>


### [59] [SIFThinker: Spatially-Aware Image Focus for Visual Reasoning](https://arxiv.org/abs/2508.06259)
*Zhangquan Chen,Ruihui Zhao,Chuwei Luo,Mingze Sun,Xinlei Yu,Yangyang Kang,Ruqi Huang*

Main category: cs.CV

TL;DR: SIFThinker是一个空间感知的'图像思维'框架，通过注意力校正和区域聚焦提升复杂视觉任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在复杂视觉任务中存在挑战，如空间理解和细粒度感知不足，需改进注意力机制以更精准聚焦提示相关区域。

Method: 提出SIFThinker框架，采用反向扩展前向推理策略生成交错的图像-文本思维链，并引入GRPO-SIF强化训练范式，整合深度信息视觉定位。

Result: SIFThinker在空间理解和细粒度视觉感知任务中优于现有方法，同时保持强大的通用能力。

Conclusion: SIFThinker通过结合深度增强的边界框和自然语言，显著提升了多模态大语言模型在复杂视觉任务中的表现，特别是在空间理解和细粒度感知方面。

Abstract: Current multimodal large language models (MLLMs) still face significant
challenges in complex visual tasks (e.g., spatial understanding, fine-grained
perception). Prior methods have tried to incorporate visual reasoning, however,
they fail to leverage attention correction with spatial cues to iteratively
refine their focus on prompt-relevant regions. In this paper, we introduce
SIFThinker, a spatially-aware "think-with-images" framework that mimics human
visual perception. Specifically, SIFThinker enables attention correcting and
image region focusing by interleaving depth-enhanced bounding boxes and natural
language. Our contributions are twofold: First, we introduce a
reverse-expansion-forward-inference strategy that facilitates the generation of
interleaved image-text chains of thought for process-level supervision, which
in turn leads to the construction of the SIF-50K dataset. Besides, we propose
GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual
grounding into a unified reasoning pipeline, teaching the model to dynamically
correct and focus on prompt-relevant regions. Extensive experiments demonstrate
that SIFThinker outperforms state-of-the-art methods in spatial understanding
and fine-grained visual perception, while maintaining strong general
capabilities, highlighting the effectiveness of our method.

</details>


### [60] [SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning](https://arxiv.org/abs/2508.06125)
*Lin Zhang,Xianfang Zeng,Kangcong Li,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: SC-Captioner利用强化学习框架和创新的奖励函数设计，显著提升了图像描述模型的自我纠正能力，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决图像描述模型在自我纠正方面的不足，并提升描述的准确性。

Method: 设计了基于场景图解析算法的奖励函数，通过分解预测和参考描述为对象、属性和关系集合，计算集合差异以识别添加和移除的元素，并匹配参考集合来计算奖励。

Result: 实验表明，SC-Captioner能够生成更优质的图像描述，显著优于现有方法。

Conclusion: SC-Captioner通过强化学习框架显著提升了图像描述模型的自我纠正能力，并在实验中证明了其优于直接偏好优化训练策略。

Abstract: We propose SC-Captioner, a reinforcement learning framework that enables the
self-correcting capability of image caption models. Our crucial technique lies
in the design of the reward function to incentivize accurate caption
corrections. Specifically, the predicted and reference captions are decomposed
into object, attribute, and relation sets using scene-graph parsing algorithms.
We calculate the set difference between sets of initial and self-corrected
captions to identify added and removed elements. These elements are matched
against the reference sets to calculate correctness bonuses for accurate
refinements and mistake punishments for wrong additions and removals, thereby
forming the final reward. For image caption quality assessment, we propose a
set of metrics refined from CAPTURE that alleviate its incomplete precision
evaluation and inefficient relation matching problems. Furthermore, we collect
a fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K
diverse images from COCO dataset. Experiments show that applying SC-Captioner
on large visual-language models can generate better image captions across
various scenarios, significantly outperforming the direct preference
optimization training strategy.

</details>


### [61] [Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.06318)
*Giacomo D'Amicantonio,Snehashis Majhi,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,François Bremond,Egor Bondarev*

Main category: cs.CV

TL;DR: GS-MoE通过专家混合和时序高斯散射损失，显著提升了弱监督视频异常检测的性能，在多个数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前弱监督视频异常检测模型在处理复杂真实世界事件时表现不佳，主要由于无法处理异常类型的多样性以及弱监督信号缺乏精确的时序信息。

Method: 提出了一种名为高斯散射引导的专家混合（GS-MoE）的新框架，该框架利用一组专家模型，每个模型专门捕捉特定类型的异常，并通过时序高斯散射损失增强弱监督信号。

Result: 在UCF-Crime数据集上达到了91.58%的AUC，并在XD-Violence和MSAD数据集上表现出色。

Conclusion: GS-MoE通过结合类别特定的专家模型和时序高斯散射损失，显著提升了弱监督视频异常检测的性能，并在多个数据集上达到了最先进水平。

Abstract: Video Anomaly Detection (VAD) is a challenging task due to the variability of
anomalous events and the limited availability of labeled data. Under the
Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided
during training, while predictions are made at the frame level. Although
state-of-the-art models perform well on simple anomalies (e.g., explosions),
they struggle with complex real-world events (e.g., shoplifting). This
difficulty stems from two key issues: (1) the inability of current models to
address the diversity of anomaly types, as they process all categories with a
shared model, overlooking category-specific features; and (2) the weak
supervision signal, which lacks precise temporal information, limiting the
ability to capture nuanced anomalous patterns blended with normal events. To
address these challenges, we propose Gaussian Splatting-guided Mixture of
Experts (GS-MoE), a novel framework that employs a set of expert models, each
specialized in capturing specific anomaly types. These experts are guided by a
temporal Gaussian splatting loss, enabling the model to leverage temporal
consistency and enhance weak supervision. The Gaussian splatting approach
encourages a more precise and comprehensive representation of anomalies by
focusing on temporal segments most likely to contain abnormal events. The
predictions from these specialized experts are integrated through a
mixture-of-experts mechanism to model complex relationships across diverse
anomaly patterns. Our approach achieves state-of-the-art performance, with a
91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on
XD-Violence and MSAD datasets. By leveraging category-specific expertise and
temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.

</details>


### [62] [SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures](https://arxiv.org/abs/2508.06127)
*Yi Qin,Rui Wang,Tao Huang,Tong Xiao,Liping Jing*

Main category: cs.CV

TL;DR: VeSCA是一种利用SAM编码器生成可转移对抗样本的新方法，通过参数化单纯复形和迭代顶点细化显著提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 评估SAM的可转移漏洞对下游应用的影响，以解决现有对抗攻击方法因领域间共同弱点探索不足而导致的有限可转移性问题。

Method: 提出了一种名为VeSCA的新方法，通过参数化单纯复形显式表征SAM与下游模型之间的共享脆弱区域，并通过迭代顶点细化来识别这些复形。采用轻量级领域再适应策略来弥合领域差异。

Result: VeSCa在五个领域特定数据集上的三种下游模型类别中，性能比现有最佳方法提高了12.7%。

Conclusion: 研究发现SAM的漏洞对下游模型构成风险，强调了开发更鲁棒基础模型的紧迫性。

Abstract: While the Segment Anything Model (SAM) transforms interactive segmentation
with zero-shot abilities, its inherent vulnerabilities present a single-point
risk, potentially leading to the failure of numerous downstream applications.
Proactively evaluating these transferable vulnerabilities is thus imperative.
Prior adversarial attacks on SAM often present limited transferability due to
insufficient exploration of common weakness across domains. To address this, we
propose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that
leverages only the encoder of SAM for generating transferable adversarial
examples. Specifically, it achieves this by explicitly characterizing the
shared vulnerable regions between SAM and downstream models through a
parametric simplicial complex. Our goal is to identify such complexes within
adversarially potent regions by iterative vertex-wise refinement. A lightweight
domain re-adaptation strategy is introduced to bridge domain divergence using
minimal reference data during the initialization of simplicial complex.
Ultimately, VeSCA generates consistently transferable adversarial examples
through random simplicial complex sampling. Extensive experiments demonstrate
that VeSCA achieves performance improved by 12.7% compared to state-of-the-art
methods across three downstream model categories across five domain-specific
datasets. Our findings further highlight the downstream model risks posed by
SAM's vulnerabilities and emphasize the urgency of developing more robust
foundation models.

</details>


### [63] [Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd](https://arxiv.org/abs/2508.06357)
*Aman Bhatta,Maria Dhakal,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: 該論文提出利用排名第一身份的額外註冊圖像訓練分類器，預測其是否為圖庫內/圖庫外身份，有效減少誤識別，並在不同圖像質量和人口統計群體中表現穩定。


<details>
  <summary>Details</summary>
Motivation: 解决一對多面部識別中排名第一結果可能為圖庫外身份的問題，減少誤識別和錯誤逮捕。

Method: 生成图库内和图库外的训练数据，提取排名第一身份的额外注册图像的排名作为特征向量，训练分类器进行预测。

Result: 實驗結果顯示，該方法在模糊、低分辨率、大氣湍流和太陽鏡等退化條件下均有效，且在不同人口統計群體中表現一致。

Conclusion: 该论文提出了一种新方法，通过利用排名第一身份的额外注册图像来预测该身份是否为图库内/图库外，从而减少误识别和错误逮捕。实验结果表明，该方法在多种图像质量下均有效，且在不同人口统计群体中表现一致。

Abstract: A central problem in one-to-many facial identification is that the person in
the probe image may or may not have enrolled image(s) in the gallery; that is,
may be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one
result is Out-of-gallery have mostly focused on finding a suitable threshold on
the similarity score. We take a new approach, using the additional enrolled
images of the identity with the rank-one result to predict if the rank-one
result is In-gallery / Out-of-gallery. Given a gallery of identities and
images, we generate In-gallery and Out-of-gallery training data by extracting
the ranks of additional enrolled images corresponding to the rank-one identity.
We then train a classifier to utilize this feature vector to predict whether a
rank-one result is In-gallery or Out-of-gallery. Using two different datasets
and four different matchers, we present experimental results showing that our
approach is viable for mugshot quality probe images, and also, importantly, for
probes degraded by blur, reduced resolution, atmospheric turbulence and
sunglasses. We also analyze results across demographic groups, and show that
In-gallery / Out-of-gallery classification accuracy is similar across
demographics. Our approach has the potential to provide an objective estimate
of whether a one-to-many facial identification is Out-of-gallery, and thereby
to reduce false positive identifications, wrongful arrests, and wasted
investigative time. Interestingly, comparing the results of older deep
CNN-based face matchers with newer ones suggests that the effectiveness of our
Out-of-gallery detection approach emerges only with matchers trained using
advanced margin-based loss functions.

</details>


### [64] [A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery](https://arxiv.org/abs/2508.06407)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: 本文研究了超分辨率与分类之间的关系，提出了一种通过优化损失函数同时提高图像质量和分类性能的新方法，显著提升了合成孔径雷达图像的分辨率和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像在视觉识别任务中至关重要，但传统超分辨率技术仅关注像素级指标，忽略了超分辨率图像保真度与下游分类性能之间的关系。本文旨在探索将分类目标直接集成到超分辨率过程中是否能进一步提高分类准确性。

Method: 本文提出了一种专门算法策略，通过优化损失函数来同时提高图像质量和分类性能，从而增强合成孔径雷达图像的分辨率。

Result: 所提出的方法不仅提高了图像质量（通过科学验证的图像质量指标衡量），还显著提升了分类准确性。

Conclusion: 本文提出了一种新颖的方法，通过优化同时考虑图像质量和分类性能的损失函数，提高了合成孔径雷达图像的分辨率，并在图像质量和分类准确性方面均取得了显著提升。

Abstract: High-resolution imagery plays a critical role in improving the performance of
visual recognition tasks such as classification, detection, and segmentation.
In many domains, including remote sensing and surveillance, low-resolution
images can limit the accuracy of automated analysis. To address this,
super-resolution (SR) techniques have been widely adopted to attempt to
reconstruct high-resolution images from low-resolution inputs. Related
traditional approaches focus solely on enhancing image quality based on
pixel-level metrics, leaving the relationship between super-resolved image
fidelity and downstream classification performance largely underexplored. This
raises a key question: can integrating classification objectives directly into
the super-resolution process further improve classification accuracy? In this
paper, we try to respond to this question by investigating the relationship
between super-resolution and classification through the deployment of a
specialised algorithmic strategy. We propose a novel methodology that increases
the resolution of synthetic aperture radar imagery by optimising loss functions
that account for both image quality and classification performance. Our
approach improves image quality, as measured by scientifically ascertained
image quality indicators, while also enhancing classification accuracy.

</details>


### [65] [DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera](https://arxiv.org/abs/2508.06139)
*Shaohua Pan,Xinyu Yi,Yan Zhou,Weihua Jian,Yuan Zhang,Pengfei Wan,Feng Xu*

Main category: cs.CV

TL;DR: 本文提出了一种结合稀疏IMU和单目相机的扩散模型方法，有效融合两种信号以实现实时人体动作捕捉，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 针对视觉信息可能因遮挡或离开视野而不可用的问题，以及IMU信号在传输稳定时的鲁棒性，设计了一种融合两种信号的统一框架。

Method: 提出了一种基于扩散模型的解决方案，将视觉信息作为整体条件嵌入，IMU测量与噪声体姿态逐帧拼接，构建扩散模型的序列输入。

Result: 实验表明，该方法在姿态估计方面优于先前工作，达到了最先进的性能。

Conclusion: 结合稀疏IMU和单目相机的新方法在实时人体动作捕捉中表现出色，通过扩散模型有效融合两种模态信号，实验证明了其设计和性能的优越性。

Abstract: Combining sparse IMUs and a monocular camera is a new promising setting to
perform real-time human motion capture. This paper proposes a diffusion-based
solution to learn human motion priors and fuse the two modalities of signals
together seamlessly in a unified framework. By delicately considering the
characteristics of the two signals, the sequential visual information is
considered as a whole and transformed into a condition embedding, while the
inertial measurement is concatenated with the noisy body pose frame by frame to
construct a sequential input for the diffusion model. Firstly, we observe that
the visual information may be unavailable in some frames due to occlusions or
subjects moving out of the camera view. Thus incorporating the sequential
visual features as a whole to get a single feature embedding is robust to the
occasional degenerations of visual information in those frames. On the other
hand, the IMU measurements are robust to occlusions and always stable when
signal transmission has no problem. So incorporating them frame-wisely could
better explore the temporal information for the system. Experiments have
demonstrated the effectiveness of the system design and its state-of-the-art
performance in pose estimation compared with the previous works. Our codes are
available for research at https://shaohua-pan.github.io/diffcap-page.

</details>


### [66] [SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models](https://arxiv.org/abs/2508.06142)
*Hanqing Wang,Yuan Tian,Mingyu Liu,Zhenhao Zhang,Xiangyang Zhu*

Main category: cs.CV

TL;DR: SDEval是首个动态评估MLLMs安全性的框架，通过文本、图像动态策略生成新样本，有效解决数据集过时和污染问题。


<details>
  <summary>Details</summary>
Motivation: 针对MLLMs输出安全性的问题，现有数据集易过时且易受污染，需要一种动态评估框架来调整安全基准的分布和复杂性。

Method: SDEval采用文本、图像及文本-图像动态策略，从原始基准生成新样本，探索了文本和图像动态对模型安全性的单独及联合影响。

Result: 实验表明，SDEval显著影响安全评估，缓解数据污染，并暴露了MLLMs的安全局限性。

Conclusion: SDEval作为一种动态评估框架，有效解决了多模态大语言模型（MLLMs）安全评估中的数据集过时和污染问题，并通过实验验证了其对安全评估的显著影响。

Abstract: In the rapidly evolving landscape of Multimodal Large Language Models
(MLLMs), the safety concerns of their outputs have earned significant
attention. Although numerous datasets have been proposed, they may become
outdated with MLLM advancements and are susceptible to data contamination
issues. To address these problems, we propose \textbf{SDEval}, the
\textit{first} safety dynamic evaluation framework to controllably adjust the
distribution and complexity of safety benchmarks. Specifically, SDEval mainly
adopts three dynamic strategies: text, image, and text-image dynamics to
generate new samples from original benchmarks. We first explore the individual
effects of text and image dynamics on model safety. Then, we find that
injecting text dynamics into images can further impact safety, and conversely,
injecting image dynamics into text also leads to safety risks. SDEval is
general enough to be applied to various existing safety and even capability
benchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and
capability benchmarks, MMBench and MMVet, show that SDEval significantly
influences safety evaluation, mitigates data contamination, and exposes safety
limitations of MLLMs. Code is available at https://github.com/hq-King/SDEval

</details>


### [67] [SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation](https://arxiv.org/abs/2508.06429)
*Guido Manni,Clemente Lauretti,Loredana Zollo,Paolo Soda*

Main category: cs.CV

TL;DR: 本文提出一种基于GAN的半监督学习框架，通过图像翻译和集成伪标签技术在低标签数据条件下显著提升医学影像分类性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像中效果显著，但受限于标签数据不足。本文旨在解决低标签数据条件下的分类问题。

Method: 本文介绍了一种新颖的基于GAN的半监督学习框架，包含三个专用神经网络（生成器、判别器和分类器）和一个三阶段训练框架，通过图像到图像翻译和集成伪标签技术有效利用未标记数据。

Result: 在11个MedMNIST数据集上的评估表明，该方法在所有设置（5、10、20和50-shot）中均优于现有方法，特别是在5-shot设置中表现最佳。

Conclusion: 本文提出的基于GAN的半监督学习框架在低标签数据条件下显著优于现有的六种最先进方法，特别是在极端5-shot设置中表现尤为突出，为医学影像应用提供了一种实用的解决方案。

Abstract: Deep learning has revolutionized medical imaging, but its effectiveness is
severely limited by insufficient labeled training data. This paper introduces a
novel GAN-based semi-supervised learning framework specifically designed for
low labeled-data regimes, evaluated across settings with 5 to 50 labeled
samples per class. Our approach integrates three specialized neural networks --
a generator for class-conditioned image translation, a discriminator for
authenticity assessment and classification, and a dedicated classifier --
within a three-phase training framework. The method alternates between
supervised training on limited labeled data and unsupervised learning that
leverages abundant unlabeled images through image-to-image translation rather
than generation from noise. We employ ensemble-based pseudo-labeling that
combines confidence-weighted predictions from the discriminator and classifier
with temporal consistency through exponential moving averaging, enabling
reliable label estimation for unlabeled data. Comprehensive evaluation across
eleven MedMNIST datasets demonstrates that our approach achieves statistically
significant improvements over six state-of-the-art GAN-based semi-supervised
methods, with particularly strong performance in the extreme 5-shot setting
where the scarcity of labeled data is most challenging. The framework maintains
its superiority across all evaluated settings (5, 10, 20, and 50 shots per
class). Our approach offers a practical solution for medical imaging
applications where annotation costs are prohibitive, enabling robust
classification performance even with minimal labeled data. Code is available at
https://github.com/GuidoManni/SPARSE.

</details>


### [68] [Text-guided Visual Prompt DINO for Generic Segmentation](https://arxiv.org/abs/2508.06146)
*Yuchen Guan,Chong Sun,Canmiao Fu,Zhipeng Huang,Chun Yuan,Chen Li*

Main category: cs.CV

TL;DR: Prompt-DINO 通过早期融合、顺序对齐查询和生成数据引擎，显著提升了开放世界多模态检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态视觉模型中晚期特征融合、混合提示开放世界分割的次优查询选择以及标题派生词汇限制的问题。

Method: 提出 Prompt-DINO 框架，包含三个关键创新：早期融合机制、顺序对齐查询选择和生成式数据引擎。

Result: Prompt-DINO 在开放世界检测基准上实现了最先进的性能，同时显著扩展了固定词汇限制之外的语义覆盖范围。

Conclusion: Prompt-DINO 提出了一种新的可扩展多模态检测和数据生成范式，显著扩展了开放世界场景中的语义覆盖范围。

Abstract: Recent advancements in multimodal vision models have highlighted limitations
in late-stage feature fusion and suboptimal query selection for hybrid prompts
open-world segmentation, alongside constraints from caption-derived
vocabularies. To address these challenges, we propose Prompt-DINO, a
text-guided visual Prompt DINO framework featuring three key innovations.
First, we introduce an early fusion mechanism that unifies text/visual prompts
and backbone features at the initial encoding stage, enabling deeper
cross-modal interactions to resolve semantic ambiguities. Second, we design
order-aligned query selection for DETR-based architectures, explicitly
optimizing the structural alignment between text and visual queries during
decoding to enhance semantic-spatial consistency. Third, we develop a
generative data engine powered by the Recognize Anything via Prompting (RAP)
model, which synthesizes 0.5B diverse training instances through a dual-path
cross-verification pipeline, reducing label noise by 80.5% compared to
conventional approaches. Extensive experiments demonstrate that Prompt-DINO
achieves state-of-the-art performance on open-world detection benchmarks while
significantly expanding semantic coverage beyond fixed-vocabulary constraints.
Our work establishes a new paradigm for scalable multimodal detection and data
generation in open-world scenarios. Data&Code are available at
https://github.com/WeChatCV/WeVisionOne.

</details>


### [69] [CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment](https://arxiv.org/abs/2508.06434)
*Shengzhu Yang,Jiawei Du,Shuai Lu,Weihang Zhang,Ningli Wang,Huiqi Li*

Main category: cs.CV

TL;DR: CLIPin是一种非对比学习插件，通过共享预投影器和参数妥协设计，提升CLIP模型的语义对齐能力，在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大规模自然图像-文本数据集通常存在语义对齐松散的问题，而医学数据集则存在内容多样性低的问题，这些问题影响了CLIP模型学习鲁棒和通用表示的能力。

Method: 提出了CLIPin，一种统一的非对比学习插件，设计了两个共享预投影器分别用于图像和文本模态，以参数妥协的方式整合对比和非对比学习。

Result: 在多种下游任务上的广泛实验证明了CLIPin的有效性和通用性，其代码已开源。

Conclusion: CLIPin作为一种即插即用的非对比学习插件，能够有效提升CLIP风格架构的多模态语义对齐能力，并在多种下游任务中展现出广泛的有效性和通用性。

Abstract: Large-scale natural image-text datasets, especially those automatically
collected from the web, often suffer from loose semantic alignment due to weak
supervision, while medical datasets tend to have high cross-modal correlation
but low content diversity. These properties pose a common challenge for
contrastive language-image pretraining (CLIP): they hinder the model's ability
to learn robust and generalizable representations. In this work, we propose
CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated
into CLIP-style architectures to improve multimodal semantic alignment,
providing stronger supervision and enhancing alignment robustness. Furthermore,
two shared pre-projectors are designed for image and text modalities
respectively to facilitate the integration of contrastive and non-contrastive
learning in a parameter-compromise manner. Extensive experiments on diverse
downstream tasks demonstrate the effectiveness and generality of CLIPin as a
plug-and-play component compatible with various contrastive frameworks. Code is
available at https://github.com/T6Yang/CLIPin.

</details>


### [70] [DSConv: Dynamic Splitting Convolution for Pansharpening](https://arxiv.org/abs/2508.06147)
*Xuanyu Liu,Bonan An*

Main category: cs.CV

TL;DR: 提出DSConv策略，通过动态分割卷积核和注意力机制提升图像融合性能，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像融合方法主要依赖标准卷积，少数采用自适应卷积，但缺乏对遥感图像像素间相关性的有效利用。

Method: 提出了一种名为DSConv的新策略，动态分割卷积核并结合注意力机制，选择感兴趣的位置，将原始卷积核分割为多个较小的卷积核。

Result: DSConv在实验中展现了最先进的性能，有效提升了图像融合的质量和效率。

Conclusion: DSConv在图像融合任务中表现出卓越的性能和泛化能力，通过动态分割卷积核结合注意力机制，显著提升了网络的优化和特征表示能力。

Abstract: Aiming to obtain a high-resolution image, pansharpening involves the fusion
of a multi-spectral image (MS) and a panchromatic image (PAN), the low-level
vision task remaining significant and challenging in contemporary research.
Most existing approaches rely predominantly on standard convolutions, few
making the effort to adaptive convolutions, which are effective owing to the
inter-pixel correlations of remote sensing images. In this paper, we propose a
novel strategy for dynamically splitting convolution kernels in conjunction
with attention, selecting positions of interest, and splitting the original
convolution kernel into multiple smaller kernels, named DSConv. The proposed
DSConv more effectively extracts features of different positions within the
receptive field, enhancing the network's generalization, optimization, and
feature representation capabilities. Furthermore, we innovate and enrich
concepts of dynamic splitting convolution and provide a novel network
architecture for pansharpening capable of achieving the tasks more efficiently,
building upon this methodology. Adequate fair experiments illustrate the
effectiveness and the state-of-the-art performance attained by
DSConv.Comprehensive and rigorous discussions proved the superiority and
optimal usage conditions of DSConv.

</details>


### [71] [Text Embedded Swin-UMamba for DeepLesion Segmentation](https://arxiv.org/abs/2508.06453)
*Ruida Cheng,Tejas Sudharshan Mathai,Pritam Mukherjee,Benjamin Hou,Qingqing Zhu,Zhiyong Lu,Matthew McAuliffe,Ronald M. Summers*

Main category: cs.CV

TL;DR: 研究通过整合文本特征的Swin-UMamba架构，在病灶分割任务中取得显著效果，Dice Score达82%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病（如淋巴瘤）的临床评估需要自动测量病灶，结合影像特征和放射报告中的描述可能提升分割效果。

Method: 研究探讨了将文本特征整合到Swin-UMamba架构中的可行性，使用了ULS23 DeepLesion数据集和放射报告的简短描述。

Result: 测试数据集上病灶分割的Dice Score达到82%，Hausdorff距离为6.58像素，显著优于LanGuideMedSeg、xLSTM-UNet和nnUNet模型。

Conclusion: 集成文本特征的Swin-UMamba架构在病灶分割任务中表现出色，显著优于现有方法，且公开了数据集和代码以供进一步研究。

Abstract: Segmentation of lesions on CT enables automatic measurement for clinical
assessment of chronic diseases (e.g., lymphoma). Integrating large language
models (LLMs) into the lesion segmentation workflow offers the potential to
combine imaging features with descriptions of lesion characteristics from the
radiology reports. In this study, we investigate the feasibility of integrating
text into the Swin-UMamba architecture for the task of lesion segmentation. The
publicly available ULS23 DeepLesion dataset was used along with short-form
descriptions of the findings from the reports. On the test dataset, a high Dice
Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for
lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior
approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <
0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by
1.74% and 0.22%, respectively. The dataset and code can be accessed at
https://github.com/ruida/LLM-Swin-UMamba

</details>


### [72] [VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation](https://arxiv.org/abs/2508.06152)
*Kaiyuan Jiang,Ruoxi Sun,Ying Cao,Yuqi Xu,Xinran Zhang,Junyan Guo,ChengSheng Deng*

Main category: cs.CV

TL;DR: VISTAR 是一个创新的文本到图像评估基准，结合脚本指标和分层加权问题方案，显著提升评估准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像评估指标存在局限性，VISTAR 旨在通过用户中心和混合评估方法解决这些问题。

Method: VISTAR 采用了两层混合范式：确定性脚本指标用于物理可量化属性，以及新颖的分层加权正负问题（HWPQ）方案用于评估抽象语义。

Result: VISTAR 在人类对齐方面表现优异（>75%），HWPQ 方案在抽象语义上达到 85.9% 的准确率，显著优于基线方法。

Conclusion: VISTAR 是一个用户中心的多维度文本到图像评估基准，通过混合范式显著提升了评估的准确性和实用性，为特定领域部署提供了可操作的指导。

Abstract: We present VISTAR, a user-centric, multi-dimensional benchmark for
text-to-image (T2I) evaluation that addresses the limitations of existing
metrics. VISTAR introduces a two-tier hybrid paradigm: it employs
deterministic, scriptable metrics for physically quantifiable attributes (e.g.,
text rendering, lighting) and a novel Hierarchical Weighted P/N Questioning
(HWPQ) scheme that uses constrained vision-language models to assess abstract
semantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study
with 120 experts, we defined seven user roles and nine evaluation angles to
construct the benchmark, which comprises 2,845 prompts validated by over 15,000
human pairwise comparisons. Our metrics achieve high human alignment (>75%),
with the HWPQ scheme reaching 85.9% accuracy on abstract semantics,
significantly outperforming VQA baselines. Comprehensive evaluation of
state-of-the-art models reveals no universal champion, as role-weighted scores
reorder rankings and provide actionable guidance for domain-specific
deployment. All resources are publicly released to foster reproducible T2I
assessment.

</details>


### [73] [WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion](https://arxiv.org/abs/2508.06485)
*Sofiane Bouaziz,Adel Hafiane,Raphael Canals,Rachid Nedjai*

Main category: cs.CV

TL;DR: WGAST 是一种新型深度学习框架，通过融合多源遥感数据实现每日10米分辨率LST估算，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 城市化、气候变化和农业压力增加了对精确及时环境监测的需求，而现有遥感系统在空间和时间分辨率之间存在权衡，亟需一种能够估算每日10米分辨率LST的方法。

Method: WGAST 采用条件生成对抗架构，包含特征提取、融合、LST重建和噪声抑制四个阶段，结合余弦相似性、归一化和时间注意力机制进行特征融合，并通过物理平均原则和PatchGAN判别器进行弱监督训练。

Result: WGAST 在定量和定性评估中均优于现有方法，平均减少RMSE 17.18%，提高SSIM 11.00%，并能有效捕捉细尺度热模式。

Conclusion: WGAST 是一种创新的端到端深度学习框架，通过弱监督生成对抗网络实现了每日10米分辨率的地表温度（LST）估算，显著优于现有方法，并在云干扰下表现出鲁棒性。

Abstract: Urbanization, climate change, and agricultural stress are increasing the
demand for precise and timely environmental monitoring. Land Surface
Temperature (LST) is a key variable in this context and is retrieved from
remote sensing satellites. However, these systems face a trade-off between
spatial and temporal resolution. While spatio-temporal fusion methods offer
promising solutions, few have addressed the estimation of daily LST at 10 m
resolution. In this study, we present WGAST, a Weakly-Supervised Generative
Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra
MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning
framework designed for this task. It adopts a conditional generative
adversarial architecture, with a generator composed of four stages: feature
extraction, fusion, LST reconstruction, and noise suppression. The first stage
employs a set of encoders to extract multi-level latent representations from
the inputs, which are then fused in the second stage using cosine similarity,
normalization, and temporal attention mechanisms. The third stage decodes the
fused features into high-resolution LST, followed by a Gaussian filter to
suppress high-frequency noise. Training follows a weakly supervised strategy
based on physical averaging principles and reinforced by a PatchGAN
discriminator. Experiments demonstrate that WGAST outperforms existing methods
in both quantitative and qualitative evaluations. Compared to the
best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves
SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and
effectively captures fine-scale thermal patterns, as validated against 33
ground-based sensors. The code is available at
https://github.com/Sofianebouaziz1/WGAST.git.

</details>


### [74] [An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06157)
*Xiaoxiao Yang,Meiliang Liu,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.CV

TL;DR: MPF-KANSC通过多平面融合和新型注意力机制，显著提升了阿尔茨海默病的早期诊断精度，并发现皮层下结构变化的右偏侧不对称性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法仅关注单一平面的结构磁共振成像（sMRI），难以准确捕捉脑部病理区域的复杂非线性关系，限制了萎缩特征的精确识别能力。

Method: 提出了MPF-KANSC框架，整合多平面融合（MPF）和Kolmogorov-Arnold网络引导的空间-通道注意力机制（KANSC），以更有效地学习和表示sMRI萎缩特征。

Result: 在ADNI数据集上的实验证实，MPF-KANSC在AD诊断中实现了更优的性能。

Conclusion: MPF-KANSC框架在阿尔茨海默病（AD）诊断中表现出优越性能，并揭示了AD进展中皮层下结构变化的右偏侧不对称性，增强了模型的可解释性。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder that
severely impairs cognitive function and quality of life. Timely intervention in
AD relies heavily on early and precise diagnosis, which remains challenging due
to the complex and subtle structural changes in the brain. Most existing deep
learning methods focus only on a single plane of structural magnetic resonance
imaging (sMRI) and struggle to accurately capture the complex and nonlinear
relationships among pathological regions of the brain, thus limiting their
ability to precisely identify atrophic features. To overcome these limitations,
we propose an innovative framework, MPF-KANSC, which integrates multi-plane
fusion (MPF) for combining features from the coronal, sagittal, and axial
planes, and a Kolmogorov-Arnold Network-guided spatial-channel attention
mechanism (KANSC) to more effectively learn and represent sMRI atrophy
features. Specifically, the proposed model enables parallel feature extraction
from multiple anatomical planes, thus capturing more comprehensive structural
information. The KANSC attention mechanism further leverages a more flexible
and accurate nonlinear function approximation technique, facilitating precise
identification and localization of disease-related abnormalities. Experiments
on the ADNI dataset confirm that the proposed MPF-KANSC achieves superior
performance in AD diagnosis. Moreover, our findings provide new evidence of
right-lateralized asymmetry in subcortical structural changes during AD
progression, highlighting the model's promising interpretability.

</details>


### [75] [Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment](https://arxiv.org/abs/2508.06160)
*Zhenbang Du,Yonggan Fu,Lifu Wang,Jiayi Qian,Xiao Luo,Yingyan,Lin*

Main category: cs.CV

TL;DR: PostDiff是一种无需训练的后训练框架，通过混合分辨率去噪和模块缓存优化扩散模型效率，实验证明降低每步推理成本比减少去噪步骤更有效。


<details>
  <summary>Details</summary>
Motivation: 研究在无需微调的后训练设置下，减少去噪步骤或降低每步推理成本哪种方法更有效，以优化扩散模型在资源有限平台上的部署。

Method: 提出了PostDiff框架，包括混合分辨率去噪方案和混合模块缓存策略，以减少输入级别和模块级别的冗余。

Result: 实验表明，PostDiff显著提升了扩散模型的效率与保真度平衡，且降低每步推理成本通常比减少去噪步骤更有效。

Conclusion: PostDiff通过减少去噪步骤和降低每步推理成本，显著提升了扩散模型在资源有限平台上的效率与生成保真度之间的平衡。

Abstract: Diffusion models have shown remarkable success across generative tasks, yet
their high computational demands challenge deployment on resource-limited
platforms. This paper investigates a critical question for compute-optimal
diffusion model deployment: Under a post-training setting without fine-tuning,
is it more effective to reduce the number of denoising steps or to use a
cheaper per-step inference? Intuitively, reducing the number of denoising steps
increases the variability of the distributions across steps, making the model
more sensitive to compression. In contrast, keeping more denoising steps makes
the differences smaller, preserving redundancy, and making post-training
compression more feasible. To systematically examine this, we propose PostDiff,
a training-free framework for accelerating pre-trained diffusion models by
reducing redundancy at both the input level and module level in a post-training
manner. At the input level, we propose a mixed-resolution denoising scheme
based on the insight that reducing generation resolution in early denoising
steps can enhance low-frequency components and improve final generation
fidelity. At the module level, we employ a hybrid module caching strategy to
reuse computations across denoising steps. Extensive experiments and ablation
studies demonstrate that (1) PostDiff can significantly improve the
fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to
boost efficiency while maintaining decent generation fidelity, reducing
per-step inference cost is often more effective than reducing the number of
denoising steps. Our code is available at
https://github.com/GATECH-EIC/PostDiff.

</details>


### [76] [MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration](https://arxiv.org/abs/2508.06189)
*Cheng Liu,Daou Zhang,Tingxu Liu,Yuhan Wang,Jinyang Chen,Yuexuan Li,Xinying Xiao,Chenbo Xin,Ziru Wang,Weichao Wu*

Main category: cs.CV

TL;DR: 提出MA-CBP框架，通过多智能体异步协作预测犯罪行为，结合实时视频流和长短期上下文推理，实现高效风险预警。


<details>
  <summary>Details</summary>
Motivation: 随着城市化加速，公共场所的犯罪行为对社会安全构成严重威胁。传统基于特征识别的异常检测方法难以从历史信息中捕捉高级行为语义，而基于大语言模型的生成方法往往无法满足实时性要求。

Method: 提出MA-CBP，一个基于多智能体异步协作的犯罪行为预测框架，将实时视频流转换为帧级语义描述，构建因果一致的历史摘要，并融合相邻图像帧进行长短期上下文联合推理。

Result: 实验结果表明，该方法在多个数据集上实现了优越性能。

Conclusion: MA-CBP框架在多个数据集上表现出色，为城市公共安全场景中的风险预警提供了有前景的解决方案。

Abstract: With the acceleration of urbanization, criminal behavior in public scenes
poses an increasingly serious threat to social security. Traditional anomaly
detection methods based on feature recognition struggle to capture high-level
behavioral semantics from historical information, while generative approaches
based on Large Language Models (LLMs) often fail to meet real-time
requirements. To address these challenges, we propose MA-CBP, a criminal
behavior prediction framework based on multi-agent asynchronous collaboration.
This framework transforms real-time video streams into frame-level semantic
descriptions, constructs causally consistent historical summaries, and fuses
adjacent image frames to perform joint reasoning over long- and short-term
contexts. The resulting behavioral decisions include key elements such as event
subjects, locations, and causes, enabling early warning of potential criminal
activity. In addition, we construct a high-quality criminal behavior dataset
that provides multi-scale language supervision, including frame-level,
summary-level, and event-level semantic annotations. Experimental results
demonstrate that our method achieves superior performance on multiple datasets
and offers a promising solution for risk warning in urban public safety
scenarios.

</details>


### [77] [A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet](https://arxiv.org/abs/2508.06191)
*Ruixiang Tang,Jianglong Qin,Mingda Zhang,Yan Song,Yi Wu,Wei Wu*

Main category: cs.CV

TL;DR: DBIF-AUNet通过双分支交互融合注意力模型显著提升胸水CT图像分割精度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 胸水CT图像的语义分割面临灰度相似、边缘模糊和形态多变等挑战，现有方法难以应对多样化的图像变化和复杂边缘。

Method: 提出了双分支交互融合注意力模型（DBIF-AUNet），包括双域特征解耦模块（DDFD）和分支交互注意力融合模块（BIAF），并采用嵌套深度监督机制和分层自适应混合损失。

Result: 在1,622张胸水CT图像上的验证中，DBIF-AUNet的IoU和Dice分数分别达到80.1%和89.0%，优于U-Net++和Swin-UNet。

Conclusion: DBIF-AUNet模型在胸水CT图像的语义分割中表现出色，显著提升了分割精度，优于当前最先进的医学图像分割模型。

Abstract: Pleural effusion semantic segmentation can significantly enhance the accuracy
and timeliness of clinical diagnosis and treatment by precisely identifying
disease severity and lesion areas. Currently, semantic segmentation of pleural
effusion CT images faces multiple challenges. These include similar gray levels
between effusion and surrounding tissues, blurred edges, and variable
morphology. Existing methods often struggle with diverse image variations and
complex edges, primarily because direct feature concatenation causes semantic
gaps. To address these challenges, we propose the Dual-Branch Interactive
Fusion Attention model (DBIF-AUNet). This model constructs a densely nested
skip-connection network and innovatively refines the Dual-Domain Feature
Disentanglement module (DDFD). The DDFD module orthogonally decouples the
functions of dual-domain modules to achieve multi-scale feature complementarity
and enhance characteristics at different levels. Concurrently, we design a
Branch Interaction Attention Fusion module (BIAF) that works synergistically
with the DDFD. This module dynamically weights and fuses global, local, and
frequency band features, thereby improving segmentation robustness.
Furthermore, we implement a nested deep supervision mechanism with hierarchical
adaptive hybrid loss to effectively address class imbalance. Through validation
on 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet
achieved IoU and Dice scores of 80.1% and 89.0% respectively. These results
outperform state-of-the-art medical image segmentation models U-Net++ and
Swin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant
optimization in segmentation accuracy for complex pleural effusion CT images.

</details>


### [78] [AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection](https://arxiv.org/abs/2508.06203)
*Zhaopeng Gu,Bingke Zhu,Guibo Zhu,Yingying Chen,Wei Ge,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: AnomalyMoE是一个基于MoE架构的通用异常检测框架，通过分层设计和专家模块，显著提升泛化能力，在多个领域数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法通常高度专业化，泛化能力有限。为了克服这一限制，提出一种通用的异常检测框架。

Method: 提出AnomalyMoE框架，基于Mixture-of-Experts (MoE)架构，分解异常检测问题为三个语义层次（局部结构异常、组件级语义异常和全局逻辑异常），并采用三个专家网络分别处理。引入EIR模块促进专家多样性，ESB模块确保专家全面利用。

Result: 在8个具有挑战性的数据集上，AnomalyMoE显著优于各领域的专用方法，确立了新的最先进性能。

Conclusion: AnomalyMoE通过其分层设计和专家模块，显著提升了异常检测的泛化能力，并在多个领域的数据集上实现了最先进的性能。

Abstract: Anomaly detection is a critical task across numerous domains and modalities,
yet existing methods are often highly specialized, limiting their
generalizability. These specialized models, tailored for specific anomaly types
like textural defects or logical errors, typically exhibit limited performance
when deployed outside their designated contexts. To overcome this limitation,
we propose AnomalyMoE, a novel and universal anomaly detection framework based
on a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the
complex anomaly detection problem into three distinct semantic hierarchies:
local structural anomalies, component-level semantic anomalies, and global
logical anomalies. AnomalyMoE correspondingly employs three dedicated expert
networks at the patch, component, and global levels, and is specialized in
reconstructing features and identifying deviations at its designated semantic
level. This hierarchical design allows a single model to concurrently
understand and detect a wide spectrum of anomalies. Furthermore, we introduce
an Expert Information Repulsion (EIR) module to promote expert diversity and an
Expert Selection Balancing (ESB) module to ensure the comprehensive utilization
of all experts. Experiments on 8 challenging datasets spanning industrial
imaging, 3D point clouds, medical imaging, video surveillance, and logical
anomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art
performance, significantly outperforming specialized methods in their
respective domains.

</details>


### [79] [PA-HOI: A Physics-Aware Human and Object Interaction Dataset](https://arxiv.org/abs/2508.06205)
*Ruiyan Wang,Lin Zuo,Zonghao Lin,Qiang Wang,Zhengxue Cheng,Rong Xie,Jun Ling,Li Song*

Main category: cs.CV

TL;DR: PA-HOI数据集填补了HOI研究中物体物理属性影响人类运动的空白，包含562个互动序列，验证了其在运动生成中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有HOI数据集关注功能细节，常忽略物体物理属性对人类长期运动的影响，PA-HOI旨在填补这一空白。

Method: 引入了PA-HOI运动捕捉数据集，包含562个不同性别受试者与35个3D物体互动的运动序列，这些物体在大小、形状和重量上各不相同。

Result: 数据集突出了物体物理属性对人类运动动态的影响，如姿势、移动速度等，并通过与现有运动生成方法的整合验证了其实际应用能力。

Conclusion: PA-HOI数据集通过扩展现有数据集的覆盖范围，显著提升了理解物体物理属性对人类运动影响的深度，并验证了其在运动生成方法中的实际应用价值。

Abstract: The Human-Object Interaction (HOI) task explores the dynamic interactions
between humans and objects in physical environments, providing essential
biomechanical and cognitive-behavioral foundations for fields such as robotics,
virtual reality, and human-computer interaction. However, existing HOI data
sets focus on details of affordance, often neglecting the influence of physical
properties of objects on human long-term motion. To bridge this gap, we
introduce the PA-HOI Motion Capture dataset, which highlights the impact of
objects' physical attributes on human motion dynamics, including human posture,
moving velocity, and other motion characteristics. The dataset comprises 562
motion sequences of human-object interactions, with each sequence performed by
subjects of different genders interacting with 35 3D objects that vary in size,
shape, and weight. This dataset stands out by significantly extending the scope
of existing ones for understanding how the physical attributes of different
objects influence human posture, speed, motion scale, and interacting
strategies. We further demonstrate the applicability of the PA-HOI dataset by
integrating it with existing motion generation methods, validating its capacity
to transfer realistic physical awareness.

</details>


### [80] [Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning](https://arxiv.org/abs/2508.06218)
*Zhiyan Bo,Laura C. Coates,Bartlomiej W. Papiez*

Main category: cs.CV

TL;DR: 提出基于双手指X光的两阶段管道，通过注意力多实例学习和区域提取方案，实现高效的SvdH评分预测，性能接近放射科医生水平。


<details>
  <summary>Details</summary>
Motivation: 解决手动SvdH评分在常规临床实践中因复杂性而难以广泛应用的问题，提高评分的效率。

Method: 采用基于注意力的多实例学习方法，结合两种区域提取方案（异常区域采样和疾病相关关节裁剪）生成图像级特征进行预测。

Result: 最佳个体模型PCC为0.943，RMSE为15.73；集成学习后PCC提升至0.945，RMSE降至15.57，接近放射科医生水平（PCC=0.97，RMSE=18.75）。

Conclusion: 该研究提出的两阶段管道在SvdH评分预测上达到了与经验丰富的放射科医生相当的最新性能，且能有效识别与RA进展相关的解剖结构。

Abstract: The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials
to quantify radiographic damage in Rheumatoid Arthritis (RA), but its
complexity has limited its adoption in routine clinical practice. To address
the inefficiency of manual scoring, this work proposes a two-stage pipeline for
interpretable image-level SvdH score prediction using dual-hand radiographs.
Our approach extracts disease-relevant image regions and integrates them using
attention-based multiple instance learning to generate image-level features for
prediction. We propose two region extraction schemes: 1) sampling image tiles
most likely to contain abnormalities, and 2) cropping patches containing
disease-relevant joints. With Scheme 2, our best individual score prediction
model achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root
mean squared error (RMSE) of 15.73. Ensemble learning further boosted
prediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving
state-of-the-art performance that is comparable to that of experienced
radiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively
identified and made decisions based on anatomical structures which clinicians
consider relevant to RA progression.

</details>


### [81] [TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images](https://arxiv.org/abs/2508.06224)
*Guoyu Zhou,Jing Zhang,Yi Yan,Hui Zhang,Li Zhuo*

Main category: cs.CV

TL;DR: TEFormer结合纹理感知和边缘引导机制，提升城市遥感图像语义分割精度，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 城市遥感图像中的地理空间对象常因纹理差异小、空间结构相似及边缘形态复杂而导致语义模糊和误分类，需一种能同时处理纹理和边缘信息的方法。

Method: 提出了一种纹理感知和边缘引导的Transformer（TEFormer），包括纹理感知模块（TaM）、边缘引导的三分支解码器（Eg3Head）和边缘引导特征融合模块（EgFFM）。

Result: TEFormer在Potsdam、Vaihingen和LoveDA数据集上分别达到了88.57%、81.46%和53.55%的mIoU，验证了其有效性。

Conclusion: TEFormer通过整合纹理感知和边缘引导机制，显著提升了城市遥感图像的语义分割精度，特别是在处理纹理差异小和边缘复杂的对象时表现出色。

Abstract: Semantic segmentation of urban remote sensing images (URSIs) is crucial for
applications such as urban planning and environmental monitoring. However,
geospatial objects often exhibit subtle texture differences and similar spatial
structures, which can easily lead to semantic ambiguity and misclassification.
Moreover, challenges such as irregular object shapes, blurred boundaries, and
overlapping spatial distributions of semantic objects contribute to complex and
diverse edge morphologies, further complicating accurate segmentation. To
tackle these issues, we propose a texture-aware and edge-guided Transformer
(TEFormer) that integrates texture awareness and edge-guidance mechanisms for
semantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is
designed to capture fine-grained texture differences between visually similar
categories to enhance semantic discrimination. Then, an edge-guided tri-branch
decoder (Eg3Head) is constructed to preserve local edges and details for
multiscale context-awareness. Finally, an edge-guided feature fusion module
(EgFFM) is to fuse contextual and detail information with edge information to
realize refined semantic segmentation. Extensive experiments show that TEFormer
achieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and
LoveDA datasets, respectively, shows the effectiveness in URSI semantic
segmentation.

</details>


### [82] [Towards Unified Image Deblurring using a Mixture-of-Experts Decoder](https://arxiv.org/abs/2508.06228)
*Daniel Feijoo,Paula Garrido-Mellado,Jaesung Rim,Alvaro Garcia,Marcos V. Conde*

Main category: cs.CV

TL;DR: 提出首个全能去模糊方法，通过MoE模块动态处理多种模糊类型，性能媲美专用模型且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有去模糊方法针对特定模糊类型设计，缺乏泛化能力，需多个模型覆盖不同模糊类型，实际应用中不实用。

Method: 采用混合专家（MoE）解码模块，动态识别模糊类型并路由图像特征，实现端到端的精确恢复。

Result: 提出的统一方法在性能上与专用模型相当，并在未见过的模糊场景中展现出卓越的鲁棒性和泛化能力。

Conclusion: 本文提出了一种统一的图像去模糊方法，通过混合专家（MoE）解码模块动态路由图像特征，实现了对多种模糊类型的高效恢复，并在性能和泛化能力上表现出色。

Abstract: Image deblurring, removing blurring artifacts from images, is a fundamental
task in computational photography and low-level computer vision. Existing
approaches focus on specialized solutions tailored to particular blur types,
thus, these solutions lack generalization. This limitation in current methods
implies requiring multiple models to cover several blur types, which is not
practical in many real scenarios. In this paper, we introduce the first
all-in-one deblurring method capable of efficiently restoring images affected
by diverse blur degradations, including global motion, local motion, blur in
low-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)
decoding module, which dynamically routes image features based on the
recognized blur degradation, enabling precise and efficient restoration in an
end-to-end manner. Our unified approach not only achieves performance
comparable to dedicated task-specific models, but also demonstrates remarkable
robustness and generalization capabilities on unseen blur degradation
scenarios.

</details>


### [83] [Deepfake Detection that Generalizes Across Benchmarks](https://arxiv.org/abs/2508.06248)
*Andrii Yermakov,Jan Cech,Jiri Matas,Mario Fritz*

Main category: cs.CV

TL;DR: LNCLIP-DF通过仅微调CLIP模型的Layer Normalization参数，实现了深度伪造检测的高效泛化，在13个数据集上达到最先进性能，并揭示了训练数据配对和时间无关的泛化关键因素。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测器在面对未见过的篡改技术时泛化能力不足，现有方法往往通过增加架构复杂性来适应，但本研究旨在证明通过参数高效的微调即可实现鲁棒泛化。

Method: 提出了一种名为LNCLIP-DF的方法，仅微调预训练CLIP视觉编码器的Layer Normalization参数（占总参数的0.03%），并通过L2归一化和潜在空间增强来强化超球形特征流形。

Result: 在13个基准数据集（2019-2025）上的广泛评估显示，LNCLIP-DF在平均跨数据集AUROC上优于更复杂的近期方法。研究发现：1）在同一源视频的成对真实-伪造数据上训练对避免捷径学习和提升泛化至关重要；2）学术数据集的检测难度并未随时间严格增加，基于旧数据训练的模型仍表现出强泛化能力。

Conclusion: 该研究证明，通过对预训练的CLIP模型进行参数高效的微调（仅调整Layer Normalization参数），可以实现深度伪造检测器的鲁棒泛化能力。LNCLIP-DF方法在计算效率高且可复现的情况下，达到了最先进的性能。

Abstract: The generalization of deepfake detectors to unseen manipulation techniques
remains a challenge for practical deployment. Although many approaches adapt
foundation models by introducing significant architectural complexity, this
work demonstrates that robust generalization is achievable through a
parameter-efficient adaptation of a pre-trained CLIP vision encoder. The
proposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters
(0.03% of the total) and enhances generalization by enforcing a hyperspherical
feature manifold using L2 normalization and latent space augmentations.
  We conducted an extensive evaluation on 13 benchmark datasets spanning from
2019 to 2025. The proposed method achieves state-of-the-art performance,
outperforming more complex, recent approaches in average cross-dataset AUROC.
Our analysis yields two primary findings for the field: 1) training on paired
real-fake data from the same source video is essential for mitigating shortcut
learning and improving generalization, and 2) detection difficulty on academic
datasets has not strictly increased over time, with models trained on older,
diverse datasets showing strong generalization capabilities.
  This work delivers a computationally efficient and reproducible method,
proving that state-of-the-art generalization is attainable by making targeted,
minimal changes to a pre-trained CLIP model. The code will be made publicly
available upon acceptance.

</details>


### [84] [FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing](https://arxiv.org/abs/2508.06256)
*Barış Büyüktaş,Jonas Klotz,Begüm Demir*

Main category: cs.CV

TL;DR: FedX通过解释引导的剪枝减少联邦学习的通信开销，在多标签和单标签场景分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在遥感图像分类任务中面临通信开销大的挑战，传统方法频繁交换大模型更新导致效率低下。

Method: 提出FedX策略，利用基于反向传播的解释方法估计模型组件的重要性，剪枝最不相关的组件，生成稀疏全局模型以减少通信开销。

Result: 实验结果表明，FedX显著减少了共享模型参数的数量，同时提升了全局模型的泛化能力，优于未剪枝模型和其他先进剪枝方法。

Conclusion: FedX通过解释引导的剪枝策略有效减少了联邦学习中的通信开销，同时保持了模型性能，并在多标签和单标签场景分类任务中表现出优越的泛化能力。

Abstract: Federated learning (FL) enables the collaborative training of deep neural
networks across decentralized data archives (i.e., clients), where each client
stores data locally and only shares model updates with a central server. This
makes FL a suitable learning paradigm for remote sensing (RS) image
classification tasks, where data centralization may be restricted due to legal
and privacy constraints. However, a key challenge in applying FL to RS tasks is
the communication overhead caused by the frequent exchange of large model
updates between clients and the central server. To address this issue, in this
paper we propose a novel strategy (denoted as FedX) that uses
explanation-guided pruning to reduce communication overhead by minimizing the
size of the transmitted models without compromising performance. FedX leverages
backpropagation-based explanation methods to estimate the task-specific
importance of model components and prunes the least relevant ones at the
central server. The resulting sparse global model is then sent to clients,
substantially reducing communication overhead. We evaluate FedX on multi-label
scene classification using the BigEarthNet-S2 dataset and single-label scene
classification using the EuroSAT dataset. Experimental results show the success
of FedX in significantly reducing the number of shared model parameters while
enhancing the generalization capability of the global model, compared to both
unpruned model and state-of-the-art pruning methods. The code of FedX will be
available at https://git.tu-berlin.de/rsim/FedX.

</details>


### [85] [XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur MRI Segmentation](https://arxiv.org/abs/2508.06258)
*Byunghyun Ko,Anning Tian,Jeongkyu Lee*

Main category: cs.CV

TL;DR: XAG-Net是一种新型2.5D U-Net架构，通过跨切片注意力和跳跃注意力门控提升股骨MRI分割精度，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D和3D深度学习的股骨MRI分割方法存在局限性，影响了骨科诊断和手术规划的准确性。

Method: 本研究提出了一种新型的2.5D U-Net架构XAG-Net，结合了像素级的跨切片注意力（CSA）和跳跃注意力门控（AG）机制，以增强切片间上下文建模和切片内特征细化。

Result: XAG-Net在股骨分割精度上超越了基线2D、2.5D和3D U-Net模型，同时保持了计算效率。

Conclusion: XAG-Net被证实为一种高效且准确的股骨MRI分割框架，通过CSA和AG模块的引入显著提升了分割精度。

Abstract: Accurate segmentation of femur structures from Magnetic Resonance Imaging
(MRI) is critical for orthopedic diagnosis and surgical planning but remains
challenging due to the limitations of existing 2D and 3D deep learning-based
segmentation approaches. In this study, we propose XAG-Net, a novel 2.5D
U-Net-based architecture that incorporates pixel-wise cross-slice attention
(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice
contextual modeling and intra-slice feature refinement. Unlike previous
CSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent
slices at each spatial location for fine-grained inter-slice modeling.
Extensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and
3D U-Net models in femur segmentation accuracy while maintaining computational
efficiency. Ablation studies further validate the critical role of the CSA and
AG modules, establishing XAG-Net as a promising framework for efficient and
accurate femur MRI segmentation.

</details>


### [86] [Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding](https://arxiv.org/abs/2508.06317)
*Jian Hu,Zixu Cheng,Shaogang Gong,Isabel Guan,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.CV

TL;DR: 提出URPA方法，通过伪标签和置信度加权，实现无标注跨域视频时间定位，计算高效且适合实时应用。


<details>
  <summary>Details</summary>
Motivation: 解决GRPO依赖标注数据和不适合实时部署的问题，提出一种数据高效的无标注跨域时间定位方法。

Method: 引入Uncertainty-quantified Rollout Policy Adaptation (URPA)，通过GRPO rollout生成多个候选预测，形成伪标签，并基于方差估计置信度，加权训练奖励。

Result: 在六个跨域设置下的三个数据集上实验表明，URPA仅需少量未标注目标视频即可实现良好泛化。

Conclusion: URPA方法在无需目标域标注的情况下，通过少量未标注视频实现了跨域视频时间定位，且计算和存储开销低，适合实时部署。

Abstract: Video Temporal Grounding (TG) aims to temporally locate video segments
matching a natural language description (a query) in a long video. While
Vision-Language Models (VLMs) are effective at holistic semantic matching, they
often struggle with fine-grained temporal localisation. Recently, Group
Relative Policy Optimisation (GRPO) reformulates the inference process as a
reinforcement learning task, enabling fine-grained grounding and achieving
strong in-domain performance. However, GRPO relies on labelled data, making it
unsuitable in unlabelled domains. Moreover, because videos are large and
expensive to store and process, performing full-scale adaptation introduces
prohibitive latency and computational overhead, making it impractical for
real-time deployment. To overcome both problems, we introduce a Data-Efficient
Unlabelled Cross-domain Temporal Grounding method, from which a model is first
trained on a labelled source domain, then adapted to a target domain using only
a small number of unlabelled videos from the target domain. This approach
eliminates the need for target annotation and keeps both computational and
storage overhead low enough to run in real time. Specifically, we introduce.
Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain
knowledge transfer in learning video temporal grounding without target labels.
URPA generates multiple candidate predictions using GRPO rollouts, averages
them to form a pseudo label, and estimates confidence from the variance across
these rollouts. This confidence then weights the training rewards, guiding the
model to focus on reliable supervision. Experiments on three datasets across
six cross-domain settings show that URPA generalises well using only a few
unlabelled target videos. Codes will be released once published.

</details>


### [87] [Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?](https://arxiv.org/abs/2508.06327)
*Xin Ci Wong,Duygu Sarikaya,Kieran Zucker,Marc De Kamps,Nishant Ravikumar*

Main category: cs.CV

TL;DR: 提出扩散模型生成合成心脏MR图像，解决领域偏移问题，显著提升多中心分割性能。


<details>
  <summary>Details</summary>
Motivation: 心脏MR影像因设备和协议差异易受领域偏移影响，导致AI模型在实际场景中性能下降。传统数据增强或迁移学习方法存在局限，合成数据因解剖结构约束效果有限。

Method: 使用扩散模型（DM）在源域上训练，生成与参考相似的合成心脏MR图像，保持空间和结构保真度。评估了2D/3D nnU-Net和普通U-Net分割网络在合成数据上的表现，探索了领域泛化和领域适应策略。

Result: 合成数据训练显著提升了未见目标域的分割性能（Welch's t-test, p < 0.01），优于仅使用真实数据训练的模型。

Conclusion: 该研究提出的扩散模型（DM）方法有效解决了心脏磁共振（MR）影像分析中的领域偏移问题，显著提高了未见目标域数据的分割性能，无需依赖迁移学习或在线训练。

Abstract: Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain
shift due to variations in imaging devices and acquisition protocols. This
challenge limits the deployment of trained AI models in real-world scenarios,
where performance degrades on unseen domains. Traditional solutions involve
increasing the size of the dataset through ad-hoc image augmentation or
additional online training/transfer learning, which have several limitations.
Synthetic data offers a promising alternative, but anatomical/structural
consistency constraints limit the effectiveness of generative models in
creating image-label pairs. To address this, we propose a diffusion model (DM)
trained on a source domain that generates synthetic cardiac MR images that
resemble a given reference. The synthetic data maintains spatial and structural
fidelity, ensuring similarity to the source domain and compatibility with the
segmentation mask. We assess the utility of our generative approach in
multi-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and
vanilla U-Net segmentation networks. We explore domain generalisation, where,
domain-invariant segmentation models are trained on synthetic source domain
data, and domain adaptation, where, we shift target domain data towards the
source domain using the DM. Both strategies significantly improved segmentation
performance on data from an unseen target domain, in terms of surface-based
metrics (Welch's t-test, p < 0.01), compared to training segmentation models on
real data alone. The proposed method ameliorates the need for transfer learning
or online training to address domain shift challenges in cardiac MR image
analysis, especially useful in data-scarce settings.

</details>


### [88] [ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction](https://arxiv.org/abs/2508.06335)
*Patrick Takenaka,Johannes Maucher,Marco F. Huber*

Main category: cs.CV

TL;DR: 改进ViPro模型，使其无需初始真实状态即可无监督推断状态，并扩展Orbits数据集至3D。


<details>
  <summary>Details</summary>
Motivation: 预测未来视频帧是一个具有挑战性的任务，有许多下游应用。先前的工作表明，程序性知识能够帮助深度模型处理复杂的动态设置，但ViPro模型假设了一个给定的初始符号状态作为真实值。这种方法导致模型学习了一个捷径，未能真正连接观测环境和预测的符号状态，从而在观测噪声较大时无法准确估计状态。

Method: 在ViPro模型的基础上，增加了若干改进，使其能够从观测中正确推断状态，而不需要在开始时提供完整的真实状态。

Result: 改进后的ViPro模型能够在无监督的情况下正确从观测中推断状态，无需提供完整的初始真实状态。此外，还扩展了Orbits数据集，增加了3D变体。

Conclusion: 本研究通过改进ViPro模型，使其能够在无监督的情况下正确从观测中推断状态，无需提供完整的初始真实状态。此外，还扩展了Orbits数据集，增加了3D变体，以更接近真实世界场景。

Abstract: Predicting future video frames is a challenging task with many downstream
applications. Previous work has shown that procedural knowledge enables deep
models for complex dynamical settings, however their model ViPro assumed a
given ground truth initial symbolic state. We show that this approach led to
the model learning a shortcut that does not actually connect the observed
environment with the predicted symbolic state, resulting in the inability to
estimate states given an observation if previous states are noisy. In this
work, we add several improvements to ViPro that enables the model to correctly
infer states from observations without providing a full ground truth state in
the beginning. We show that this is possible in an unsupervised manner, and
extend the original Orbits dataset with a 3D variant to close the gap to real
world scenarios.

</details>


### [89] [Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities](https://arxiv.org/abs/2508.06342)
*Kieran Elrod,Katherine Flanigan,Mario Bergés*

Main category: cs.CV

TL;DR: 研究表明，街道景观图像可推断社交互动类型与建成环境变量之间的关系，支持城市社交性研究。


<details>
  <summary>Details</summary>
Motivation: 现有定量研究主要关注行人数量而非社交互动的质量，假设街道景观图像包含潜在社交信息，并可通过社会科学理论提取和解释。

Method: 使用多模态大型语言模型分析2,998张街道景观图像，并结合Mehta的社会性分类法（被动、短暂和持久社交性），通过线性回归模型控制天气、时间和行人数量等因素，验证社会性指标与城市级地方依恋分数及环境预测因子（如绿色、天空和水景指数）之间的相关性。

Result: 天空视野指数与所有三种社交性类型相关，绿色视野指数预测持久社交性，地方依恋与短暂社交性呈正相关。

Conclusion: 街道景观图像可以作为一种可扩展且保护隐私的工具，用于研究城市社交性，支持跨文化理论测试和基于证据的设计，以创建社交活跃的城市。

Abstract: Designing socially active streets has long been a goal of urban planning, yet
existing quantitative research largely measures pedestrian volume rather than
the quality of social interactions. We hypothesize that street view imagery --
an inexpensive data source with global coverage -- contains latent social
information that can be extracted and interpreted through established social
science theory. As a proof of concept, we analyzed 2,998 street view images
from 15 cities using a multimodal large language model guided by Mehta's
taxonomy of passive, fleeting, and enduring sociability -- one illustrative
example of a theory grounded in urban design that could be substituted or
complemented by other sociological frameworks. We then used linear regression
models, controlling for factors like weather, time of day, and pedestrian
counts, to test whether the inferred sociability measures correlate with
city-level place attachment scores from the World Values Survey and with
environmental predictors (e.g., green, sky, and water view indices) derived
from individual street view images. Results aligned with long-standing urban
planning theory: the sky view index was associated with all three sociability
types, the green view index predicted enduring sociability, and place
attachment was positively associated with fleeting sociability. These results
provide preliminary evidence that street view images can be used to infer
relationships between specific types of social interactions and built
environment variables. Further research could establish street view imagery as
a scalable, privacy-preserving tool for studying urban sociability, enabling
cross-cultural theory testing and evidence-based design of socially vibrant
cities.

</details>


### [90] [Aligning Effective Tokens with Video Anomaly in Large Language Models](https://arxiv.org/abs/2508.06350)
*Yingxian Chen,Jiahui Liu,Ruifan Di,Yanwei Li,Chirui Chang,Shizhen Zhao,Wilton W. T. Fok,Xiaojuan Qi,Yik-Chung Wu*

Main category: cs.CV

TL;DR: VA-GPT是一种新型MLLM，通过SETS和TETG模块优化视频异常事件的分析，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在处理视频异常事件时因空间和时间稀疏性导致冗余信息干扰，效果不佳。

Method: 提出了VA-GPT模型，包含Spatial Effective Token Selection (SETS)和Temporal Effective Token Generation (TETG)模块，用于高效对齐视觉编码器和LLM之间的有效token。

Result: VA-GPT在多个基准测试中表现优于现有方法，特别是在跨域评估中基于XD-Violence数据集的表现突出。

Conclusion: VA-GPT通过SETS和TETG模块有效解决了视频异常事件的空间和时间稀疏性问题，显著提升了异常事件的总结和定位能力，并在多个基准测试中优于现有方法。

Abstract: Understanding abnormal events in videos is a vital and challenging task that
has garnered significant attention in a wide range of applications. Although
current video understanding Multi-modal Large Language Models (MLLMs) are
capable of analyzing general videos, they often struggle to handle anomalies
due to the spatial and temporal sparsity of abnormal events, where the
redundant information always leads to suboptimal outcomes. To address these
challenges, exploiting the representation and generalization capabilities of
Vison Language Models (VLMs) and Large Language Models (LLMs), we propose
VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in
various videos. Our approach efficiently aligns effective tokens between visual
encoders and LLMs through two key proposed modules: Spatial Effective Token
Selection (SETS) and Temporal Effective Token Generation (TETG). These modules
enable our model to effectively capture and analyze both spatial and temporal
information associated with abnormal events, resulting in more accurate
responses and interactions. Furthermore, we construct an instruction-following
dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a
cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed
method outperforms existing state-of-the-art methods on various benchmarks.

</details>


### [91] [An Implemention of Two-Phase Image Segmentation using the Split Bregman Method](https://arxiv.org/abs/2508.06351)
*Olakunle S. Abawonse,Günay Doğan*

Main category: cs.CV

TL;DR: 本文实现了一种基于改进Chan-Vese能量模型的两阶段图像分割算法，通过split Bregman方法高效优化，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目标是实现一种高效的两阶段图像分割算法，将图像分为前景和背景区域，同时保证区域边界平滑。

Method: 采用Chan-Vese能量模型的改进版本，利用split Bregman方法高效最小化能量，实现两阶段分割。

Result: 实验结果表明，该方法在不同图像和参数范围内均能有效实现分割。

Conclusion: 本文详细实现了Goldstein、Bresson和Osher提出的两阶段图像分割算法，并通过实验验证了其在不同参数下的性能。

Abstract: In this paper, we describe an implementation of the two-phase image
segmentation algorithm proposed by Goldstein, Bresson, Osher in
\cite{gold:bre}. This algorithm partitions the domain of a given 2d image into
foreground and background regions, and each pixel of the image is assigned
membership to one of these two regions. The underlying assumption for the
segmentation model is that the pixel values of the input image can be
summarized by two distinct average values, and that the region boundaries are
smooth. Accordingly, the model is defined as an energy in which the variable is
a region membership function to assign pixels to either region, originally
proposed by Chan and Vese in \cite{chan:vese}. This energy is the sum of image
data terms in the regions and a length penalty for region boundaries.
Goldstein, Bresson, Osher modify the energy of Chan-Vese in \cite{gold:bre} so
that their new energy can be minimized efficiently using the split Bregman
method to produce an equivalent two-phase segmentation. We provide a detailed
implementation of this method \cite{gold:bre}, and document its performance
with several images over a range of algorithm parameters.

</details>


### [92] [Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning](https://arxiv.org/abs/2508.06382)
*Xiangyu Wu,Feng Yu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: TaAM-CPT 是一种仅需文本数据的通用表示模型，通过模态提示池和对齐编码器支持无限模态扩展，并在多模态任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法严重依赖大量特定模态的标注数据或仅针对单一模态定制，本研究旨在解决这一问题。

Method: TaAM-CPT 包含模态提示池、文本构建和预训练模型的模态对齐文本编码器，通过设计模态内和模态间学习目标来协调不同模态的学习。

Result: TaAM-CPT 在视频分类、图像分类和音频分类等多种模态任务上取得了领先结果，且无需任何特定模态的标注数据。

Conclusion: TaAM-CPT 是一种可扩展的方法，能够仅使用文本数据构建面向无限模态的通用表示模型，并在多种模态任务上取得领先结果。

Abstract: The integration of prompt tuning with multimodal learning has shown
significant generalization abilities for various downstream tasks. Despite
advancements, existing methods heavily depend on massive modality-specific
labeled data (e.g., video, audio, and image), or are customized for a single
modality. In this study, we present Text as Any-Modality by Consistent Prompt
Tuning (TaAM-CPT), a scalable approach for constructing a general
representation model toward unlimited modalities using solely text data.
TaAM-CPT comprises modality prompt pools, text construction, and
modality-aligned text encoders from pre-trained models, which allows for
extending new modalities by simply adding prompt pools and modality-aligned
text encoders. To harmonize the learning across different modalities, TaAM-CPT
designs intra- and inter-modal learning objectives, which can capture category
details within modalities while maintaining semantic consistency across
different modalities. Benefiting from its scalable architecture and pre-trained
models, TaAM-CPT can be seamlessly extended to accommodate unlimited
modalities. Remarkably, without any modality-specific labeled data, TaAM-CPT
achieves leading results on diverse datasets spanning various modalities,
including video classification, image classification, and audio classification.
The code is available at https://github.com/Jinx630/TaAM-CPT.

</details>


### [93] [FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation](https://arxiv.org/abs/2508.06392)
*Wenbin Teng,Gonglin Chen,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: FVGen通过蒸馏视频扩散模型，实现了快速新颖视图合成，采样时间减少90%，适用于稀疏视图3D重建。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在使用视频扩散模型（VDMs）进行稀疏视图3D重建时采样速度慢的问题。

Method: 提出了一种新颖的视频扩散模型蒸馏方法，利用生成对抗网络（GANs）和软化的反向KL散度最小化，将多步去噪教师模型蒸馏为少步去噪学生模型。

Result: 在真实数据集上的实验表明，FVGen在保持（或提升）视觉质量的同时，将采样时间减少了90%以上，显著提升了时间效率。

Conclusion: FVGen 通过创新的视频扩散模型蒸馏方法，显著提升了新颖视图合成的速度，同时保持了视觉质量，为稀疏视图下的3D重建任务提供了高效解决方案。

Abstract: Recent progress in 3D reconstruction has enabled realistic 3D models from
dense image captures, yet challenges persist with sparse views, often leading
to artifacts in unseen areas. Recent works leverage Video Diffusion Models
(VDMs) to generate dense observations, filling the gaps when only sparse views
are available for 3D reconstruction tasks. A significant limitation of these
methods is their slow sampling speed when using VDMs. In this paper, we present
FVGen, a novel framework that addresses this challenge by enabling fast novel
view synthesis using VDMs in as few as four sampling steps. We propose a novel
video diffusion model distillation method that distills a multi-step denoising
teacher model into a few-step denoising student model using Generative
Adversarial Networks (GANs) and softened reverse KL-divergence minimization.
Extensive experiments on real-world datasets show that, compared to previous
works, our framework generates the same number of novel views with similar (or
even better) visual quality while reducing sampling time by more than 90%.
FVGen significantly improves time efficiency for downstream reconstruction
tasks, particularly when working with sparse input views (more than 2) where
pre-trained VDMs need to be run multiple times to achieve better spatial
coverage.

</details>


### [94] [Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification](https://arxiv.org/abs/2508.06420)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: 提出两种新算法M2m_f和M2m_u，有效解决SAR船只分类中的长尾问题，显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决SAR船只分类中长尾数据集导致的类别不平衡问题。

Method: 提出了两种新颖的算法M2m_f和M2m_u，并在OpenSARShip和FuSARShip数据集上使用ViT、VGG16和ResNet50作为特征提取器进行测试。

Result: 新方法在FuSARShip和OpenSARShip数据集上平均F1分数分别提高了8.82%和4.44%。

Conclusion: 所提出的M2m_f和M2m_u算法在SAR船只分类中有效解决了长尾数据集问题，显著提升了分类性能。

Abstract: SAR ship classification faces the challenge of long-tailed datasets, which
complicates the classification of underrepresented classes. Oversampling
methods have proven effective in addressing class imbalance in optical data. In
this paper, we evaluated the effect of oversampling in the feature space for
SAR ship classification. We propose two novel algorithms inspired by the
Major-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two
public datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three
state-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.
Additionally, we also analyzed the impact of oversampling methods on different
class sizes. The results demonstrated the effectiveness of our novel methods
over the original M2m and baselines, with an average F1-score increase of 8.82%
for FuSARShip and 4.44% for OpenSARShip.

</details>


### [95] [MotionSwap](https://arxiv.org/abs/2508.06430)
*Om Patil,Jinesh Modi,Suryabha Mukhopadhyay,Meghaditya Giri,Chhavi Malhotra*

Main category: cs.CV

TL;DR: 本文改进了SimSwap框架，通过引入注意力机制和动态优化策略，显著提升了面部交换的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提升现有面部交换技术在高保真度、身份保持和视觉质量方面的表现。

Method: 本文改进了SimSwap框架，主要方法包括在生成器架构中集成自注意力和交叉注意力机制、动态损失权重以及余弦退火学习率调度。

Result: 实验结果表明，改进后的模型在身份相似性、FID分数和视觉质量上均显著优于基线模型，消融研究验证了各改进的重要性。

Conclusion: 本文提出了SimSwap框架的改进版本，通过引入自注意力和交叉注意力机制、动态损失权重以及余弦退火学习率调度，显著提升了身份保持、属性一致性和视觉质量。未来工作方向包括集成StyleGAN3、改进唇部同步、融合3D面部建模以及增强视频应用的时间一致性。

Abstract: Face swapping technology has gained significant attention in both academic
research and commercial applications. This paper presents our implementation
and enhancement of SimSwap, an efficient framework for high fidelity face
swapping. We introduce several improvements to the original model, including
the integration of self and cross-attention mechanisms in the generator
architecture, dynamic loss weighting, and cosine annealing learning rate
scheduling. These enhancements lead to significant improvements in identity
preservation, attribute consistency, and overall visual quality.
  Our experimental results, spanning 400,000 training iterations, demonstrate
progressive improvements in generator and discriminator performance. The
enhanced model achieves better identity similarity, lower FID scores, and
visibly superior qualitative results compared to the baseline. Ablation studies
confirm the importance of each architectural and training improvement. We
conclude by identifying key future directions, such as integrating StyleGAN3,
improving lip synchronization, incorporating 3D facial modeling, and
introducing temporal consistency for video-based applications.

</details>


### [96] [TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.06452)
*Mattia Litrico,Mario Valerio Giuffrida,Sebastiano Battiato,Devis Tuia*

Main category: cs.CV

TL;DR: TRUST是一种利用语言模态鲁棒性指导视觉模型适应的新UDA方法，通过伪标签生成、不确定性估计和多模态对比学习，显著提升了复杂域偏移下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂域偏移（如地理偏移）中现有无监督域适应方法因背景和对象外观显著差异而效果不佳的问题。

Method: TRUST通过生成目标样本的伪标签，并引入基于CLIP相似度分数的归一化不确定性估计策略，重新加权分类损失以减少低质量伪标签的负面影响。此外，提出了一种多模态软对比学习损失，通过利用字幕指导目标图像的对比训练，对齐视觉和语言特征空间。

Result: TRUST在DomainNet和GeoNet数据集上取得了最先进的性能。

Conclusion: TRUST方法在经典和复杂域偏移（如DomainNet和GeoNet）上优于现有方法，设立了新的技术标杆。

Abstract: Recent unsupervised domain adaptation (UDA) methods have shown great success
in addressing classical domain shifts (e.g., synthetic-to-real), but they still
suffer under complex shifts (e.g. geographical shift), where both the
background and object appearances differ significantly across domains. Prior
works showed that the language modality can help in the adaptation process,
exhibiting more robustness to such complex shifts. In this paper, we introduce
TRUST, a novel UDA approach that exploits the robustness of the language
modality to guide the adaptation of a vision model. TRUST generates
pseudo-labels for target samples from their captions and introduces a novel
uncertainty estimation strategy that uses normalised CLIP similarity scores to
estimate the uncertainty of the generated pseudo-labels. Such estimated
uncertainty is then used to reweight the classification loss, mitigating the
adverse effects of wrong pseudo-labels obtained from low-quality captions. To
further increase the robustness of the vision model, we propose a multimodal
soft-contrastive learning loss that aligns the vision and language feature
spaces, by leveraging captions to guide the contrastive training of the vision
model on target images. In our contrastive loss, each pair of images acts as
both a positive and a negative pair and their feature representations are
attracted and repulsed with a strength proportional to the similarity of their
captions. This solution avoids the need for hardly determining positive and
negative pairs, which is critical in the UDA setting. Our approach outperforms
previous methods, setting the new state-of-the-art on classical (DomainNet) and
complex (GeoNet) domain shifts. The code will be available upon acceptance.

</details>


### [97] [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/abs/2508.06492)
*Yuwei Yang,Zeyu Zhang,Yunzhong Hou,Zhuowan Li,Gaowen Liu,Ali Payani,Yuan-Sen Ting,Liang Zheng*

Main category: cs.CV

TL;DR: 通过模块化和多样化图表生成流程，本研究提升了MLLMs的图表理解能力，并发布了包含丰富图表和问答对的ECD数据集。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态大语言模型在图表理解任务上表现不佳（成功率30%-50%），且合成图表与真实图表的相似度不足限制了模型训练效果。

Method: 设计了一个五步数据合成流程，包括分离数据和函数创建、条件生成多子图、视觉多样化、过滤低质量数据以及使用GPT-4生成问答对。

Result: 生成的ECD数据集包含1万+图表图像和30万+问答对，覆盖25个主题和250+图表类型组合，显著提升了多种MLLMs在真实和合成测试集上的性能。

Conclusion: 本研究通过模块化和多样化的图表生成方法，显著提升了多模态大语言模型（MLLMs）在图表理解任务中的表现，并通过引入有效图表数据集（ECD）为未来研究提供了高质量资源。

Abstract: Being able to effectively read scientific plots, or chart understanding, is a
central part toward building effective agents for science. However, existing
multimodal large language models (MLLMs), especially open-source ones, are
still falling behind with a typical success rate of 30%-50% on challenging
benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are
often restricted by their inadequate similarity to the real charts, which could
compromise model training and performance on complex real-world charts. In this
study, we show that modularizing chart generation and diversifying visual
details improves chart understanding capabilities. In particular, we design a
five-step data synthesis pipeline, where we separate data and function creation
for single plot generation, condition the generation of later subplots on
earlier ones for multi-subplot figures, visually diversify the generated
figures, filter out low quality data, and finally generate the question-answer
(QA) pairs with GPT-4o. This approach allows us to streamline the generation of
fine-tuning datasets and introduce the effective chart dataset (ECD), which
contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring
250+ chart type combinations with high visual complexity. We show that ECD
consistently improves the performance of various MLLMs on a range of real-world
and synthetic test sets. Code, data and models are available at:
https://github.com/yuweiyang-anu/ECD.

</details>


### [98] [LightSwitch: Multi-view Relighting with Material-guided Diffusion](https://arxiv.org/abs/2508.06494)
*Yehonathan Litman,Fernando De la Torre,Shubham Tulsiani*

Main category: cs.CV

TL;DR: LightSwitch 是一种新颖的材质重光照扩散框架，通过整合多视图和材质信息，高效且一致地重光照多样材质物体，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的2D重光照生成先验未能充分利用可推断的主题内在属性或无法大规模考虑多视图数据，导致重光照效果不佳。因此，需要一种能够整合这些属性的新方法。

Method: LightSwitch 结合了多视图和材质信息线索，以及可扩展的去噪方案，以一致且高效地对具有多样材质组成的物体的密集多视图数据进行重光照。

Result: LightSwitch 的2D重光照预测质量超过了之前直接从图像进行重光照的最先进先验，且在合成和真实物体上的重光照表现优异，仅需2分钟即可完成。

Conclusion: LightSwitch 提出了一种新颖的微调材质重光照扩散框架，能够高效地对任意数量的输入图像进行目标光照条件下的重光照，并在合成和真实物体上表现出色，匹配或超越了现有最先进的扩散逆渲染方法。

Abstract: Recent approaches for 3D relighting have shown promise in integrating 2D
image relighting generative priors to alter the appearance of a 3D
representation while preserving the underlying structure. Nevertheless,
generative priors used for 2D relighting that directly relight from an input
image do not take advantage of intrinsic properties of the subject that can be
inferred or cannot consider multi-view data at scale, leading to subpar
relighting. In this paper, we propose Lightswitch, a novel finetuned
material-relighting diffusion framework that efficiently relights an arbitrary
number of input images to a target lighting condition while incorporating cues
from inferred intrinsic properties. By using multi-view and material
information cues together with a scalable denoising scheme, our method
consistently and efficiently relights dense multi-view data of objects with
diverse material compositions. We show that our 2D relighting prediction
quality exceeds previous state-of-the-art relighting priors that directly
relight from images. We further demonstrate that LightSwitch matches or
outperforms state-of-the-art diffusion inverse rendering methods in relighting
synthetic and real objects in as little as 2 minutes.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [99] [DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models](https://arxiv.org/abs/2508.05685)
*Yara Bahram,Mohammadhadi Shateri,Eric Granger*

Main category: cs.GR

TL;DR: DogFit通过领域感知引导偏移和轻量级条件机制，优化扩散模型迁移学习中的保真度与多样性权衡，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型迁移到较小目标领域时，单纯微调导致的泛化性能差问题，同时避免现有测试时引导方法的高计算成本。

Method: 提出了Domain-guided Fine-tuning（DogFit）方法，通过在训练损失中注入领域感知引导偏移，并采用轻量级条件机制编码引导强度值。此外，研究了训练中引导偏移的最佳放置和时机，提出了两种简单的调度策略（late-start和cut-off）。

Result: 在DiT和SiT骨干网络上，跨六个不同目标领域的实验显示，DogFit在FID和FDDINOV2指标上优于现有引导方法，且采样TFLOPS减少高达2倍。

Conclusion: DogFit方法在扩散模型迁移学习中表现出色，通过内部化引导行为，优化了保真度与多样性的权衡，同时减少了计算开销。

Abstract: Transfer learning of diffusion models to smaller target domains is
challenging, as naively fine-tuning the model often results in poor
generalization. Test-time guidance methods help mitigate this by offering
controllable improvements in image fidelity through a trade-off with sample
diversity. However, this benefit comes at a high computational cost, typically
requiring dual forward passes during sampling. We propose the Domain-guided
Fine-tuning (DogFit) method, an effective guidance mechanism for diffusion
transfer learning that maintains controllability without incurring additional
computational overhead. DogFit injects a domain-aware guidance offset into the
training loss, effectively internalizing the guided behavior during the
fine-tuning process. The domain-aware design is motivated by our observation
that during fine-tuning, the unconditional source model offers a stronger
marginal estimate than the target model. To support efficient controllable
fidelity-diversity trade-offs at inference, we encode the guidance strength
value as an additional model input through a lightweight conditioning
mechanism. We further investigate the optimal placement and timing of the
guidance offset during training and propose two simple scheduling strategies,
i.e., late-start and cut-off, which improve generation quality and training
stability. Experiments on DiT and SiT backbones across six diverse target
domains show that DogFit can outperform prior guidance methods in transfer
learning in terms of FID and FDDINOV2 while requiring up to 2x fewer sampling
TFLOPS.

</details>


### [100] [Exploring Interactive Simulation of Grass Display Color Characteristic Based on Real-World Conditions](https://arxiv.org/abs/2508.06086)
*Kojiro Tanaka,Keiichi Sato,Masahiko Mikawa,Makoto Fujisawa*

Main category: cs.GR

TL;DR: 本文提出了一种虚拟环境中的交互式草色变化特性模拟方法，解决了传统方法耗时和高成本的问题，结果显示其准确且高效。


<details>
  <summary>Details</summary>
Motivation: 传统方法每次光照或视角变化都需在实际设备上进行实验，耗时且成本高；现有模拟方法虽已开始研究，但仍需数小时进行一次测量，存在显著问题。

Method: 探索了一种基于虚拟环境的交互式草色变化特性模拟方法，通过模拟多种视角和环境下的草色特性进行评估。

Result: 模拟结果显示，该方法能够较准确地模拟实际草色特性，且在速度和准确性上与之前的研究相当。

Conclusion: 本文提出的方法在虚拟环境中模拟草色变化特性，结果显示其能够较准确地模拟实际草色特性，且在速度和准确性上与之前的研究相当。

Abstract: Recent research has focused on incorporating media into living environments
via color-controlled materials and image display. In particular, grass-based
displays have drawn attention as landscape-friendly interactive interfaces. To
develop the grass display, it is important to obtain the grass color change
characteristics that depend on the real environment. However, conventional
methods require experiments on actual equipment every time the lighting or
viewpoint changes, which is time-consuming and costly. Although research has
begun on simulating grass colors, this approach still faces significant issues
as it takes many hours for a single measurement. In this paper, we explore an
interactive simulation of a grass display color change characteristic based on
real-world conditions in a virtual environment. We evaluated our method's
accuracy by simulating grass color characteristics across multiple viewpoints
and environments, and then compared the results against prior work. The results
indicated that our method tended to simulate the grass color characteristics
similar to the actual characteristics and showed the potential to do so more
quickly and with comparable accuracy to the previous study.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [101] [Empirical Evaluation of AI-Assisted Software Package Selection: A Knowledge Graph Approach](https://arxiv.org/abs/2508.05693)
*Siamak Farshidi,Amir Saberhabibi,Behbod Eskafi,Niloofar Nikfarjam,Sadegh Eskandari,Slinger Jansen,Michel Chaudron,Bedir Tekinerdogan*

Main category: cs.SE

TL;DR: 该研究提出了一种基于MCDM和数据驱动的框架PySelect，用于解决开源生态系统（如Python）中软件包选择问题，通过整合多源数据和AI辅助意图建模，显著提升了推荐质量和用户体验。


<details>
  <summary>Details</summary>
Motivation: 开源生态系统（如Python）中第三方软件包选择困难，原因在于替代方案众多且缺乏透明的比较证据。生成式AI工具在开发流程中的应用常忽视依赖评估、过度强调流行度而非适用性，且缺乏可复现性，导致项目在透明度、长期可靠性、可维护性和架构决策方面存在风险。

Method: 研究将软件包选择问题建模为MCDM问题，并开发了一个数据驱动的技术评估框架。自动化数据管道从GitHub、PyPI和Stack Overflow收集并整合软件元数据、使用趋势、漏洞信息和开发者情感。这些数据通过决策模型结构化，PySelect系统利用大语言模型解释用户意图并查询模型以识别上下文合适的软件包。

Result: 研究通过798,669个Python脚本和16,887个GitHub仓库的数据评估，以及基于技术接受模型的用户研究，证明了数据提取的高精度、推荐质量优于生成式AI基线，用户对系统有用性和易用性评价积极。

Conclusion: 该研究提出了一种基于多标准决策（MCDM）原则、经验数据和AI辅助意图建模的可扩展、可解释且可复现的框架，支持基于证据的软件选择。PySelect决策支持系统通过整合软件元数据、使用趋势、漏洞信息和开发者情感，显著提升了推荐质量。

Abstract: Selecting third-party software packages in open-source ecosystems like Python
is challenging due to the large number of alternatives and limited transparent
evidence for comparison. Generative AI tools are increasingly used in
development workflows, but their suggestions often overlook dependency
evaluation, emphasize popularity over suitability, and lack reproducibility.
This creates risks for projects that require transparency, long-term
reliability, maintainability, and informed architectural decisions. This study
formulates software package selection as a Multi-Criteria Decision-Making
(MCDM) problem and proposes a data-driven framework for technology evaluation.
Automated data pipelines continuously collect and integrate software metadata,
usage trends, vulnerability information, and developer sentiment from GitHub,
PyPI, and Stack Overflow. These data are structured into a decision model
representing relationships among packages, domain features, and quality
attributes. The framework is implemented in PySelect, a decision support system
that uses large language models to interpret user intent and query the model to
identify contextually appropriate packages. The approach is evaluated using
798,669 Python scripts from 16,887 GitHub repositories and a user study based
on the Technology Acceptance Model. Results show high data extraction
precision, improved recommendation quality over generative AI baselines, and
positive user evaluations of usefulness and ease of use. This work introduces a
scalable, interpretable, and reproducible framework that supports
evidence-based software selection using MCDM principles, empirical data, and
AI-assisted intent modeling.

</details>


### [102] [Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning](https://arxiv.org/abs/2508.05710)
*Jia Fu,Xinyu Yang,Hongzhi Zhang,Yahui Liu,Jingyuan Zhang,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.SE

TL;DR: Klear-CodeTest通过G-V框架和多层沙盒系统，解决了高质量测试案例合成的难题，提升了LLMs的训练效果。


<details>
  <summary>Details</summary>
Motivation: 高质量的测试案例对训练大型语言模型（LLMs）至关重要，但合成高质量测试案例仍是一个未解决的难题。

Method: 采用Generator-Validation（G-V）框架，结合一致性验证机制和多层安全沙盒系统，生成包括常规和边界案例的全面测试用例。

Result: 实验证明，该方法显著提高了模型性能和训练稳定性，数据集和沙盒系统已公开。

Conclusion: Klear-CodeTest通过其Generator-Validation框架和多层安全沙盒系统，显著提升了代码强化学习中的测试案例质量和模型性能，其源代码和数据集已开源。

Abstract: Precise, correct feedback is crucial for effectively training large language
models (LLMs) in code reinforcement learning. However, synthesizing
high-quality test cases remains a profoundly challenging and unsolved problem.
In this work, we present Klear-CodeTest, a comprehensive test case synthesis
framework featuring rigorous verification to ensure quality and reliability of
test cases. Our approach achieves broad coverage of programming problems via a
novel Generator-Validation (G-V) framework, ensuring correctness through a
consistency validation mechanism that verifies outputs against gold solutions.
The proposed G-V framework generates comprehensive test cases including both
regular and corner cases, enhancing test coverage and discriminative power for
solution correctness assessment in code reinforcement learning. In addition, we
design a multi-layered security sandbox system optimized for online
verification platforms, guaranteeing safe and reliable code execution. Through
comprehensive experiments, we demonstrate the effectiveness of our curated
dataset, showing significant improvements in model performance and training
stability. The source codes, curated dataset and sandbox system are available
at: https://github.com/Kwai-Klear/CodeTest.

</details>


### [103] [Utilizing Composer Packages to Accelerate Laravel-Based Project Development Among Students: A Pedagogical and Practical Framework](https://arxiv.org/abs/2508.05747)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.SE

TL;DR: 本文探讨了如何在大学Laravel课程中利用Composer及其包加速开发，同时强调教育者需引导学生理解工具的原理和适用场景，以平衡效率与深度学习。


<details>
  <summary>Details</summary>
Motivation: 针对学生在有限时间内完成Laravel项目的困难，本文旨在通过引入Composer及其包，减少开发工作量，同时培养专业软件实践能力。

Method: 本文通过概念性分析，介绍了Composer及其精选包的使用，并结合实际和教学考量，展示了如何策略性地利用这些工具。

Result: 研究表明，合理使用Composer包能显著提升开发效率并增强学生的行业准备度，但需注意避免过度依赖和包冲突。

Conclusion: 本文强调了在学术环境中有效整合Composer包的重要性，既能加速开发，又能确保学生理解其背后的原理和最佳实践。教育者需通过精心设计的教学计划，引导学生批判性地评估工具的使用，以支持深度学习而非替代。

Abstract: Laravel has emerged as a foundational framework in university web development
curricula. However, despite its scaffolding capabilities, students often
struggle to complete projects within limited academic timelines. This
conceptual paper introduces Composer, PHP's standard dependency manager, and
categorizes a curated selection of Composer packages that significantly reduce
development effort while fostering professional software practices. Grounded in
practical and pedagogical considerations, the paper illustrates how educators
and learners can strategically leverage these tools to build typical academic
or personal Laravel-based systems. Central to this approach is maintaining code
quality and reinforcing conceptual understanding. The paper also addresses
potential risks such as package conflicts and over-reliance on tools, providing
best-practice recommendations to mitigate them. While the goal is to accelerate
development, the deeper objective is to reinforce professional workflows and
industry readiness. Exposure to Composer packages enhances curriculum relevance
and smooths the transition from academia to the workplace. However, effective
integration requires deliberate instructional design aligned with learning
objectives. Without guidance, students may treat packages as black boxes. Thus,
educators must teach not only how to use these tools, but also when and why,
encouraging critical evaluation of their utility and limitations. This ensures
that practical convenience supports rather than supplants deep learning.

</details>


### [104] [AI-Guided Exploration of Large-Scale Codebases](https://arxiv.org/abs/2508.05799)
*Yoseph Berhanu Alebachew*

Main category: cs.SE

TL;DR: 该论文提出了一种结合逆向工程和LLM引导的混合方法，用于提升代码理解的交互性和适应性，原型实现展示了可行性。


<details>
  <summary>Details</summary>
Motivation: 传统工具如静态可视化和逆向工程技术缺乏交互性、适应性和上下文信息的整合，而LLMs的新进展为代码探索提供了新机会，但其缺乏基础且与结构化视图的整合限制了效果。

Method: 提出了一种混合方法，结合确定性逆向工程与LLM引导的意图感知视觉探索。系统整合了UML可视化、动态用户界面、历史上下文和协作功能。

Result: 原型实现证明了该方法的可行性，未来工作包括实证评估、扩展到多语言系统及探索GUI驱动的LLM交互模型。

Conclusion: 本研究为智能、交互式环境奠定了基础，这些环境与开发者的认知和协作工作流程相契合。

Abstract: Understanding large-scale, complex software systems is a major challenge for
developers, who spend a significant portion of their time on program
comprehension. Traditional tools such as static visualizations and reverse
engineering techniques provide structural insights but often lack
interactivity, adaptability, and integration with contextual information.
Recent advancements in large language models (LLMs) offer new opportunities to
enhance code exploration workflows, yet their lack of grounding and integration
with structured views limits their effectiveness. This work introduces a hybrid
approach that integrates deterministic reverse engineering with LLM-guided,
intent-aware visual exploration. The proposed system combines UML-based
visualization, dynamic user interfaces, historical context, and collaborative
features into an adaptive tool for code comprehension. By interpreting user
queries and interaction patterns, the LLM helps developers navigate and
understand complex codebases more effectively. A prototype implementation for
Java demonstrates the feasibility of this approach. Future work includes
empirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM
interaction models. This research lays the groundwork for intelligent,
interactive environments that align with developer cognition and collaborative
workflows.

</details>


### [105] [Enhancing Software Vulnerability Detection Through Adaptive Test Input Generation Using Genetic Algorithm](https://arxiv.org/abs/2508.05923)
*Yanusha Mehendran,Maolin Tang,Yi Lu*

Main category: cs.SE

TL;DR: 提出一种结合遗传算法和自适应学习的测试输入生成方法，显著提升漏洞检测覆盖率和效果。


<details>
  <summary>Details</summary>
Motivation: 传统漏洞检测方法已无法应对日益复杂的软件系统，需要更高效、自适应的检测手段。

Method: 采用遗传算法（包括交叉操作符）和自适应反馈机制，动态生成并优化测试输入。

Result: 在九个开源JSON处理库的评估中，该方法在类、方法、行、指令和分支覆盖率上分别平均提升了39.8%、62.4%、105.0%、114.0%和166.0%。

Conclusion: 该研究提出的基于遗传算法和自适应学习的测试输入生成方法显著提高了软件漏洞检测的覆盖率，尤其在处理复杂软件时表现出色。

Abstract: Software vulnerabilities continue to undermine the reliability and security
of modern systems, particularly as software complexity outpaces the
capabilities of traditional detection methods. This study introduces a genetic
algorithm-based method for test input generation that innovatively integrates
genetic operators and adaptive learning to enhance software vulnerability
detection. A key contribution is the application of the crossover operator,
which facilitates exploration by searching across a broader space of potential
test inputs. Complementing this, an adaptive feedback mechanism continuously
learns from the system's execution behavior and dynamically guides input
generation toward promising areas of the input space. Rather than relying on
fixed or randomly selected inputs, the approach evolves a population of
structurally valid test cases using feedback-driven selection, enabling deeper
and more effective code traversal. This strategic integration of exploration
and exploitation ensures that both diverse and targeted test inputs are
developed over time. Evaluation was conducted across nine open-source
JSON-processing libraries. The proposed method achieved substantial
improvements in coverage compared to a benchmark evolutionary fuzzing method,
with average gains of 39.8% in class coverage, 62.4% in method coverage, 105.0%
in line coverage, 114.0% in instruction coverage, and 166.0% in branch
coverage. These results highlight the method's capacity to detect deeper and
more complex vulnerabilities, offering a scalable and adaptive solution to
software security testing.

</details>


### [106] [A Survey on Task Scheduling in Carbon-Aware Container Orchestration](https://arxiv.org/abs/2508.05949)
*Jialin Yang,Zainab Saad,Jiajun Wu,Xiaoguang Niu,Henry Leung,Steve Drew*

Main category: cs.SE

TL;DR: 论文综述了Kubernetes调度策略，分类为硬件和软件中心，并提出了一个关注环境可持续性的云任务调度分类法，为下一代云计算系统提供了设计见解。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是应对大规模软件生态系统和云数据中心能源需求的激增，尤其是大型语言模型的密集训练和部署，导致能源消耗和碳足迹达到前所未有的水平，因此需要更高效的任务调度和基础设施编排来减少碳排放。

Method: 论文采用的方法是对各种Kubernetes调度策略进行系统综述，将其分为硬件中心和软件中心两类，并根据所使用的算法进行分组。

Result: 论文的结果是提出了一个全面的云任务调度研究分类法，分析了新兴研究趋势和开放挑战。

Conclusion: 该论文的结论是，通过系统性地分析和分类Kubernetes调度策略，并重点关注环境可持续性，为下一代云计算系统设计可持续的调度解决方案提供了关键见解。

Abstract: The soaring energy demands of large-scale software ecosystems and cloud data
centers, accelerated by the intensive training and deployment of large language
models, have driven energy consumption and carbon footprint to unprecedented
levels. In response, both industry and academia are increasing efforts to
reduce the carbon emissions associated with cloud computing through more
efficient task scheduling and infrastructure orchestration. In this work, we
present a systematic review of various Kubernetes scheduling strategies,
categorizing them into hardware-centric and software-centric, annotating each
with its sustainability objectives, and grouping them according to the
algorithms they use. We propose a comprehensive taxonomy for cloud task
scheduling studies, with a particular focus on the environmental sustainability
aspect. We analyze emerging research trends and open challenges, and our
findings provide critical insight into the design of sustainable scheduling
solutions for next-generation cloud computing systems.

</details>


### [107] [Impact-driven Context Filtering For Cross-file Code Completion](https://arxiv.org/abs/2508.05970)
*Yanzhou Li,Shangqing Liu,Kangjie Chen,Tianwei Zhang,Yang Liu*

Main category: cs.SE

TL;DR: CODEFILTER 通过自适应过滤检索到的上下文，提高了仓库级代码补全的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 为了理解检索到的跨文件上下文对代码补全的贡献，并解决检索到的代码块中只有少数对补全有正面贡献甚至可能降低性能的问题。

Method: 提出了一种基于似然的度量来评估每个检索代码块对补全的影响，并构建了一个仓库级数据集，标注每个检索块的正面、中性或负面相关性。随后提出了自适应检索上下文过滤框架 CODEFILTER。

Result: 在 RepoEval 和 CrossCodeLongEval 基准测试中，CODEFILTER 在不进行过滤操作的方法中一致提高了补全准确性，并显著减少了输入提示的长度，提高了计算效率。

Conclusion: CODEFILTER 显著提高了仓库级代码补全的准确性、效率和可归因性，展示了其潜力。

Abstract: Retrieval-augmented generation (RAG) has recently demonstrated considerable
potential for repository-level code completion, as it integrates cross-file
knowledge with in-file preceding code to provide comprehensive contexts for
generation. To better understand the contribution of the retrieved cross-file
contexts, we introduce a likelihood-based metric to evaluate the impact of each
retrieved code chunk on the completion. Our analysis reveals that, despite
retrieving numerous chunks, only a small subset positively contributes to the
completion, while some chunks even degrade performance. To address this issue,
we leverage this metric to construct a repository-level dataset where each
retrieved chunk is labeled as positive, neutral, or negative based on its
relevance to the target completion. We then propose an adaptive retrieval
context filtering framework, CODEFILTER, trained on this dataset to mitigate
the harmful effects of negative retrieved contexts in code completion.
Extensive evaluation on the RepoEval and CrossCodeLongEval benchmarks
demonstrates that CODEFILTER consistently improves completion accuracy compared
to approaches without filtering operations across various tasks. Additionally,
CODEFILTER significantly reduces the length of the input prompt, enhancing
computational efficiency while exhibiting strong generalizability across
different models. These results underscore the potential of CODEFILTER to
enhance the accuracy, efficiency, and attributability of repository-level code
completion.

</details>


### [108] [Position: Intelligent Coding Systems Should Write Programs with Justifications](https://arxiv.org/abs/2508.06017)
*Xiangzhe Xu,Shiwei Feng,Zian Su,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: 智能编码系统需生成代码并提供合理化解释。神经符号方法被提出，以解决现有方法的局限性，确保解释的认知对齐和语义忠实性。


<details>
  <summary>Details</summary>
Motivation: AI驱动的编码器的不透明决策引发了信任和可用性问题，尤其是非专家用户无法检查底层实现。

Method: 采用神经符号方法进行合理化解释生成，其中符号约束在训练过程中指导模型行为，并通过神经表示丰富程序语义，在推理时实现自动一致性检查。

Result: 提出了两个关键的合理化解释属性——认知对齐和语义忠实性，并指出了现有方法（包括形式验证、静态分析和事后可解释性）的局限性。

Conclusion: 智能编码系统应不仅生成代码，还应提供清晰、一致的合理化解释，以弥合模型推理与用户理解之间的鸿沟。

Abstract: Intelligent coding systems are transforming software development by enabling
users to specify code behavior in natural language. However, the opaque
decision-making of AI-driven coders raises trust and usability concerns,
particularly for non-expert users who cannot inspect low-level implementations.
We argue that these systems should not only generate code but also produce
clear, consistent justifications that bridge model reasoning and user
understanding. To this end, we identify two critical justification
properties-cognitive alignment and semantic faithfulness-and highlight the
limitations of existing methods, including formal verification, static
analysis, and post-hoc explainability. We advocate exploring neuro-symbolic
approaches for justification generation, where symbolic constraints guide model
behavior during training and program semantics are enriched through neural
representations, enabling automated consistency checks at inference time.

</details>


### [109] [Understanding Inconsistent State Update Vulnerabilities in Smart Contracts](https://arxiv.org/abs/2508.06192)
*Lantian Li,Yuyu Chen,Jingwen Wu,Yue Pan,Zhongxing Yu*

Main category: cs.SE

TL;DR: 本文首次大规模实证研究智能合约中的不一致状态更新漏洞，总结漏洞特征并开发检测工具，验证了研究结果的实用性。


<details>
  <summary>Details</summary>
Motivation: 智能合约的状态更新过程可能存在问题，尤其是由于未同步修改等原因导致的不一致状态更新漏洞，现有工具难以有效识别，亟需系统性研究以指导实践。

Method: 通过系统调查352个真实智能合约项目中的116个不一致状态更新漏洞，总结其根本原因、修复策略和利用方法，并基于研究发现开发了一个概念验证检查器。

Result: 研究发现11项重要结论，开发的概念验证检查器在64个GitHub项目中有效检测到问题，其中19个项目所有者确认了问题。

Conclusion: 本研究首次大规模实证分析了智能合约中的不一致状态更新漏洞，为开发者、研究人员和工具构建者提供了重要见解，并开发了一个概念验证检查器，证明了研究成果的实际应用价值。

Abstract: Smart contracts enable contract terms to be automatically executed and
verified on the blockchain, and recent years have witnessed numerous
applications of them in areas such as financial institutions and supply chains.
The execution logic of a smart contract is closely related to the contract
state, and thus the correct and safe execution of the contract depends heavily
on the precise control and update of the contract state. However, the contract
state update process can have issues. In particular, inconsistent state update
issues can arise for reasons such as unsynchronized modifications. Inconsistent
state update bugs have been exploited by attackers many times, but existing
detection tools still have difficulty in effectively identifying them. This
paper conducts the first large-scale empirical study about inconsistent state
update vulnerabilities (that is, inconsistent state update bugs that are
exploitable) in smart contracts, aiming to shed light for developers,
researchers, tool builders, and language or library designers in order to avoid
inconsistent state update vulnerabilities. We systematically investigate 116
inconsistent state update vulnerabilities in 352 real-world smart contract
projects, summarizing their root causes, fix strategies, and exploitation
methods. Our study provides 11 original and important findings, and we also
give the implications of our findings. To illustrate the potential benefits of
our research, we also develop a proof-of-concept checker based on one of our
findings. The checker effectively detects issues in 64 popular GitHub projects,
and 19 project owners have confirmed the detected issues at the time of
writing. The result demonstrates the usefulness and importance of our findings
for avoiding inconsistent state update vulnerabilities in smart contracts.

</details>


### [110] [Improving the Developer Experience with a Low-Code Process Modelling Language](https://arxiv.org/abs/2508.06299)
*Henrique Henriques,Hugo Lourenço,Vasco Amaral,Miguel Goulão*

Main category: cs.SE

TL;DR: 通过改进业务流程技术（BPT）的可用性，新版本显著提升了开发者的体验，包括语义透明度、回答正确率和可用性评分。


<details>
  <summary>Details</summary>
Motivation: OutSystems平台的DSL之一，业务流程技术（BPT），采用率低且存在可用性问题，这影响了其采用率，并带来了语言维护成本。

Method: 结合访谈、使用“Physics of Notation”对BPT进行批判性审查，以及使用系统可用性量表（SUS）和NASA任务负荷指数（TLX）对BPT进行实证评估，开发了新版本的BPT。

Result: 对25名专业软件工程师的评估显示，新版本的语义透明度从31%提高到69%，回答正确率从51%提高到89%，SUS评分从42.25提高到64.78，TLX评分从36.50降低到20.78。这些差异具有统计学意义。

Conclusion: 这些结果表明，新版本的BPT显著改善了之前版本的开发者体验。最终用户的OutSystems背景对最终的具体语法选择和实现的可用性指标有重要影响。

Abstract: Context: The OutSystems Platform is a development environment composed of
several DSLs, used to specify, quickly build, and validate web and mobile
applications. The DSLs allow users to model different perspectives such as
interfaces and data models, define custom business logic and construct process
models. Problem: The DSL for process modelling (Business Process Technology
(BPT)), has a low adoption rate and is perceived as having usability problems
hampering its adoption. This is problematic given the language maintenance
costs. Method: We used a combination of interviews, a critical review of BPT
using the "Physics of Notation" and empirical evaluations of BPT using the
System Usability Scale (SUS) and the NASA Task Load indeX (TLX), to develop a
new version of BPT, taking these inputs and Outsystems' engineers' culture into
account. Results: Evaluations conducted with 25 professional software engineers
showed an increase of the semantic transparency on the new version, from 31% to
69%, an increase in the correctness of responses, from 51% to 89%, an increase
in the SUS score, from 42.25 to 64.78, and a decrease of the TLX score, from
36.50 to 20.78. These differences were statistically significant. Conclusions:
These results suggest that the new version of BPT significantly improved the
developer experience of the previous version. The end users' background with
OutSystems had a relevant impact on the final concrete syntax choices and
achieved usability indicators.

</details>


### [111] [Execution-Feedback Driven Test Generation from SWE Issues](https://arxiv.org/abs/2508.06365)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: 论文提出e-Otter++，利用新颖执行反馈技术自动生成软件工程问题重现测试，显著提升生成测试质量。


<details>
  <summary>Details</summary>
Motivation: 大多数软件工程问题缺乏有效的重现测试，而现有方法因代码缺失或错误难以生成高质量的测试。

Method: 引入了一种新颖的执行反馈利用技术，绕过了测试生成中代码缺失或错误的问题。

Result: 在TDD-Bench Verified基准测试中，e-Otter++的平均失败转通过率达到63%，显著优于现有技术。

Conclusion: e-Otter++ 在自动生成软件工程问题的重现测试方面取得了显著进展，平均失败转通过率为63%，为该领域的技术进步提供了有力支持。

Abstract: A software engineering issue (SWE issue) is easier to resolve when
accompanied by a reproduction test. Unfortunately, most issues do not come with
functioning reproduction tests, so this paper explores how to generate them
automatically. The primary challenge in this setting is that the code to be
tested is either missing or wrong, as evidenced by the existence of the issue
in the first place. This has held back test generation for this setting:
without the correct code to execute, it is difficult to leverage execution
feedback to generate good tests. This paper introduces novel techniques for
leveraging execution feedback to get around this problem, implemented in a new
reproduction test generator called e-Otter++. Experiments show that e-Otter++
represents a leap ahead in the state-of-the-art for this problem, generating
tests with an average fail-to-pass rate of 63% on the TDD-Bench Verified
benchmark.

</details>


### [112] [What Builds Effective In-Context Examples for Code Generation?](https://arxiv.org/abs/2508.06414)
*Dongze Li,Songqiang Chen,Jialun Cao,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 本文通过消融研究发现，变量和函数的适当命名对ICL代码生成至关重要，性能下降高达30%。LLMs偏好语义上有意义的名称而非格式，并难以从相似代码中提取可泛化的问题解决洞察力。


<details>
  <summary>Details</summary>
Motivation: 尽管基于代码示例的ICL方法在提高LLMs的代码生成能力方面表现出显著效果，但ICL提供的代码示例中哪些具体特征（如标识符命名风格、代码格式、解决方案洞察力）对其有效性有显著贡献尚不明确。

Method: 本文通过控制消融研究，系统地调查了各种代码特征对ICL与代码示例的影响。

Result: 研究发现，变量和函数的适当命名对有效代码生成至关重要，消除这些命名会导致性能下降高达30个百分点。此外，LLMs更倾向于语义上有意义的标识符名称而非格式约定，并对标识符的冗长性有语言特定的偏好。此外，LLMs在从相似代码解决方案中提取可泛化的问题解决洞察力方面表现不佳。

Conclusion: 本文的结论是，适当的变量和函数命名对代码生成至关重要，而当前的LLMs在从相似代码解决方案中提取可泛化的问题解决洞察力方面存在困难。这些发现为优化代码生成中的ICL系统提供了宝贵见解，并突出了基于反射学习的代码生成任务中的基本挑战。

Abstract: In-Context Learning (ICL) has emerged as a promising solution to enhance the
code generation capabilities of Large Language Models (LLMs), which
incorporates code examples inside the prompt to let LLMs learn from
demonstrations. However, despite the substantial effectiveness of the code
example-based ICL approach, the specific features (e.g., identifier naming
styles, code formatting, solution insight) within the ICL-provided code
examples that significantly contribute to the ICL's effectiveness remain
unclear. This paper systematically investigates the impact of various code
features on ICL with code examples through controlled ablation studies. Our
findings reveal that the appropriate naming of variables and functions is
crucial for effective code generation, with their elimination leading to
performance decreases of up to 30 percentage points. We further demonstrate
that LLMs prioritize semantically meaningful identifier names over formatting
conventions, with language-specific preferences regarding identifier verbosity.
Additionally, our investigation into ICL's potential for enhancing reflection
and inference capabilities reveals that current LLMs struggle to extract
generalizable problem-solving insights from similar code solutions, despite
being capable of utilizing direct information effectively. These findings are
expected to provide valuable insights for optimizing ICL systems in code
generation applications and highlight fundamental challenges in
reflection-based learning for code generation tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [113] [GPU-Accelerated Barrier-Rate Guided MPPI Control for Tractor-Trailer Systems](https://arxiv.org/abs/2508.05773)
*Keyvan Majd,Hardik Parwana,Bardh Hoxha,Steven Hong,Hideki Okamoto,Georgios Fainekos*

Main category: cs.RO

TL;DR: BR-MPPI通过嵌入CBF约束改进MPPI，在复杂环境中高效生成鲁棒轨迹，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决铰接式车辆在拥挤空间中的倒车和机动问题，特别是在行人存在的环境中。

Method: BR-MPPI将控制屏障函数（CBF）约束直接嵌入路径积分更新中，通过引导重要性采样分布朝向无碰撞、动态可行的轨迹，增强了MPPI的探索能力和轨迹鲁棒性。

Result: 在高保真CarMaker模拟器中，BR-MPPI在单GPU上以超过100Hz的频率计算控制输入（针对8个障碍物的场景），并在停车间距方面优于标准MPPI和带碰撞成本的MPPI基线。

Conclusion: BR-MPPI方法在复杂环境中表现出色，计算效率高，且在保持停车间距方面优于传统MPPI方法。

Abstract: Articulated vehicles such as tractor-trailers, yard trucks, and similar
platforms must often reverse and maneuver in cluttered spaces where pedestrians
are present. We present how Barrier-Rate guided Model Predictive Path Integral
(BR-MPPI) control can solve navigation in such challenging environments.
BR-MPPI embeds Control Barrier Function (CBF) constraints directly into the
path-integral update. By steering the importance-sampling distribution toward
collision-free, dynamically feasible trajectories, BR-MPPI enhances the
exploration strength of MPPI and improves robustness of resulting trajectories.
The method is evaluated in the high-fidelity CarMaker simulator on a 12 [m]
tractor-trailer tasked with reverse and forward parking in a parking lot.
BR-MPPI computes control inputs in above 100 [Hz] on a single GPU (for
scenarios with eight obstacles) and maintains better parking clearance than a
standard MPPI baseline and an MPPI with collision cost baseline.

</details>


### [114] [Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction](https://arxiv.org/abs/2508.05838)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: 结合SAM、YOLOv5和PPO的智能体在AI2-THOR中显著提升了对象交互能力，实验结果显示多项性能指标大幅提升。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过整合视觉基础模型与强化学习来提升智能体在模拟环境中的对象交互能力。

Method: 结合Segment Anything Model (SAM)、YOLOv5和PPO算法，在AI2-THOR模拟环境中进行实验。

Result: 实验结果显示，平均累积奖励提升68%，对象交互成功率提高52.5%，导航效率提升33%。

Conclusion: 整合视觉基础模型与强化学习能显著提升模拟环境中对象的交互能力，为复杂机器人任务提供了新思路。

Abstract: This paper presents a novel approach that integrates vision foundation models
with reinforcement learning to enhance object interaction capabilities in
simulated environments. By combining the Segment Anything Model (SAM) and
YOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the
AI2-THOR simulation environment, we enable the agent to perceive and interact
with objects more effectively. Our comprehensive experiments, conducted across
four diverse indoor kitchen settings, demonstrate significant improvements in
object interaction success rates and navigation efficiency compared to a
baseline agent without advanced perception. The results show a 68% increase in
average cumulative reward, a 52.5% improvement in object interaction success
rate, and a 33% increase in navigation efficiency. These findings highlight the
potential of integrating foundation models with reinforcement learning for
complex robotic tasks, paving the way for more sophisticated and capable
autonomous agents.

</details>


### [115] [Modular Vacuum-Based Fixturing System for Adaptive Disassembly Workspace Integration](https://arxiv.org/abs/2508.05936)
*Haohui Pan,Takuya Kiyokawa,Tomoki Ishikura,Shingo Hamada,Genichiro Matsuda,Kensuke Harada*

Main category: cs.RO

TL;DR: 论文提出了一种模块化真空吸附夹具系统，结合气球型软夹持器和稳定性感知规划框架，显著提高了复杂曲面家电拆卸的成功率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 小型家用电器的复杂曲面几何形状使得传统刚性夹具难以有效拆卸，因此需要一种能够适应任意形状表面并提供稳定支撑的解决方案。

Method: 开发了一个稳定性感知的规划框架，包括目标物体底面采样、基于几何连续性的候选接触点筛选，以及基于凸包的静态稳定性评估。

Result: 实验结果表明，该方法在不同数量和配置的气球夹持器下均能提供更优的物体放置质量和稳定性，且在螺丝拆卸任务中的成功率显著高于传统刚性夹具。

Conclusion: 论文提出了一种基于模块化真空吸附的夹具系统，结合商业化的气球型软夹持器，能够适应复杂曲面并提供稳定支撑。实验证明，该方法在螺丝拆卸任务中比传统刚性夹具具有更高的成功率和稳定性。

Abstract: The disassembly of small household appliances poses significant challenges
due to their complex and curved geometries, which render traditional rigid
fixtures inadequate. In this paper, we propose a modular vacuum-based fixturing
system that leverages commercially available balloon-type soft grippers to
conform to arbitrarily shaped surfaces and provide stable support during
screw-removal tasks. To enable a reliable deployment of the system, we develop
a stability-aware planning framework that samples the bottom surface of the
target object, filters candidate contact points based on geometric continuity,
and evaluates support configurations using convex hull-based static stability
criteria. We compare the quality of object placement under different numbers
and configurations of balloon hands. In addition, real-world experiments were
conducted to compare the success rates of traditional rigid fixtures with our
proposed system. The results demonstrate that our method consistently achieves
higher success rates and superior placement stability during screw removal
tasks.

</details>


### [116] [Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts](https://arxiv.org/abs/2508.05937)
*Gen Sako,Takuya Kiyokawa,Kensuke Harada,Tomoki Ishikura,Naoya Miyaji,Genichiro Matsuda*

Main category: cs.RO

TL;DR: 研究提出了一种示能引导的远程操作系统，通过虚拟环境可视化和混合控制策略，有效解决了机器人非破坏性拆卸配合件的挑战，实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 由于内部结构可见性有限和灵活操作的需求，机器人非破坏性拆卸配合件仍具有挑战性。

Method: 研究开发了一个基于示能引导的远程操作系统，结合虚拟环境中的可行抓取姿态和拆卸方向可视化，以及混合位置和阻抗控制的控制器，用于双臂固定和拆卸任务。

Result: 实际实验验证了系统的有效性，显示任务成功率提高且对象姿态偏差减少。

Conclusion: 所提出的系统通过直观的人机协作和混合控制策略，有效提高了机器人非破坏性拆卸的成功率，并减少了对象姿态偏差。

Abstract: Robotic non-destructive disassembly of mating parts remains challenging due
to the need for flexible manipulation and the limited visibility of internal
structures. This study presents an affordance-guided teleoperation system that
enables intuitive human demonstrations for dual-arm fix-and-disassemble tasks
for mating parts. The system visualizes feasible grasp poses and disassembly
directions in a virtual environment, both derived from the object's geometry,
to address occlusions and structural complexity. To prevent excessive position
tracking under load when following the affordance, we integrate a hybrid
controller that combines position and impedance control into the teleoperated
disassembly arm. Real-world experiments validate the effectiveness of the
proposed system, showing improved task success rates and reduced object pose
deviation.

</details>


### [117] [Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution](https://arxiv.org/abs/2508.05941)
*Zhanyi Sun,Shuran Song*

Main category: cs.RO

TL;DR: LPB introduces a robust visuomotor policy framework using latent embeddings as barriers, separating safe and unsafe states, improving robustness and data efficiency without human intervention.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of visuomotor policies trained via behavior cloning to covariate shift, where small deviations from expert trajectories can lead to failure, without relying on labor-intensive human corrections or synthetic data augmentation.

Method: LPB decouples expert imitation and out-of-distribution (OOD) recovery into two modules: a base diffusion policy trained on expert data and a dynamics model trained on both expert and suboptimal policy data. The dynamics model predicts future latent states to ensure they remain within the expert distribution.

Result: Simulated and real-world experiments demonstrate that LPB improves policy robustness and data efficiency, achieving reliable manipulation with limited expert data and no additional human correction.

Conclusion: Latent Policy Barrier (LPB) significantly enhances the robustness and data efficiency of visuomotor policies, enabling reliable manipulation from limited expert data without additional human intervention.

Abstract: Visuomotor policies trained via behavior cloning are vulnerable to covariate
shift, where small deviations from expert trajectories can compound into
failure. Common strategies to mitigate this issue involve expanding the
training distribution through human-in-the-loop corrections or synthetic data
augmentation. However, these approaches are often labor-intensive, rely on
strong task assumptions, or compromise the quality of imitation. We introduce
Latent Policy Barrier, a framework for robust visuomotor policy learning.
Inspired by Control Barrier Functions, LPB treats the latent embeddings of
expert demonstrations as an implicit barrier separating safe, in-distribution
states from unsafe, out-of-distribution (OOD) ones. Our approach decouples the
role of precise expert imitation and OOD recovery into two separate modules: a
base diffusion policy solely on expert data, and a dynamics model trained on
both expert and suboptimal policy rollout data. At inference time, the dynamics
model predicts future latent states and optimizes them to stay within the
expert distribution. Both simulated and real-world experiments show that LPB
improves both policy robustness and data efficiency, enabling reliable
manipulation from limited expert data and without additional human correction
or annotation.

</details>


### [118] [Social and Telepresence Robots for Accessibility and Inclusion in Small Museums](https://arxiv.org/abs/2508.05946)
*Nello Balossino,Rossana Damiano,Cristina Gena,Alberto Lillo,Anna Maria Marras,Claudio Mattutino,Antonio Pizzo,Alessia Prin,Fabiana Vernero*

Main category: cs.RO

TL;DR: ROBSO-PM项目利用社交机器人提升小型博物馆的可访问性，解决感知、文化和认知障碍，案例包括三个博物馆，机器人用于导览和远程访问。


<details>
  <summary>Details</summary>
Motivation: 许多博物馆，尤其是低人口密度地区的博物馆，存在感知、文化和认知方面的可访问性障碍。该项目旨在通过技术手段改善这一问题。

Method: 项目采用社交机器人和远程社交机器人作为工具，在三个案例博物馆中实施：都灵圣裹尸布博物馆、Champlas du Col狂欢节博物馆和Pragelato阿尔卑斯民族服饰与传统博物馆。机器人被用作导览工具和远程访问工具。

Result: 项目探索了机器人在导览和远程访问中的应用，重点关注讲故事、机器人个性、共情、个性化以及远程协作等研究主题。

Conclusion: ROBSO-PM项目通过社交机器人和远程社交机器人技术，成功提升了小型博物馆的可访问性，特别是在感知、文化和认知层面。

Abstract: There are still many museums that present accessibility barriers,
particularly regarding perceptual, cultural, and cognitive aspects. This is
especially evident in low-density population areas. The aim of the ROBSO-PM
project is to improve the accessibility of small museums through the use of
social robots and social telepresence robots, focusing on three museums as case
studies: the Museum of the Holy Shroud in Turin, a small but globally known
institution, and two lesser known mountain museums: the Museum of the Champlas
du Col Carnival and the Pragelato Museum of Alpine Peoples' Costumes and
Traditions. The project explores two main applications for robots: as guides
supporting inclusive visits for foreign or disabled visitors, and as
telepresence tools allowing people with limited mobility to access museums
remotely. From a research perspective, key topics include storytelling, robot
personality, empathy, personalization, and, in the case of telepresence,
collaboration between the robot and the person, with clearly defined roles and
autonomy.

</details>


### [119] [Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles](https://arxiv.org/abs/2508.05972)
*Shaoting Liu,Zhou Liu*

Main category: cs.RO

TL;DR: 论文提出了一种扰动感知规划框架，通过实时扰动估计和动态安全边界调整，提升了双模车辆在复杂环境中的轨迹规划鲁棒性，实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高双模车辆在环境扰动下的轨迹规划鲁棒性，结合空中和地面移动的优势，解决复杂环境中的导航问题。

Method: 该框架结合了实时扰动估计、路径搜索和轨迹优化，并引入了一个扰动自适应的安全边界调整机制，动态调整车辆的可行动态边界以确保轨迹可行性。

Result: 实验和基准比较验证了该方法的有效性和鲁棒性，在跟踪精度、任务效率和能源性能方面均有显著提升。

Conclusion: 该论文提出的扰动感知规划框架通过实时扰动估计和动态安全边界调整机制，显著提升了双模车辆在复杂环境中的轨迹规划鲁棒性，实验验证了其在跟踪精度、任务效率和能源性能方面的优越表现。

Abstract: Air-land bimodal vehicles provide a promising solution for navigating complex
environments by combining the flexibility of aerial locomotion with the energy
efficiency of ground mobility. To enhance the robustness of trajectory planning
under environmental disturbances, this paper presents a disturbance-aware
planning framework that incorporates real-time disturbance estimation into both
path searching and trajectory optimization. A key component of the framework is
a disturbance-adaptive safety boundary adjustment mechanism, which dynamically
modifies the vehicle's feasible dynamic boundaries based on estimated
disturbances to ensure trajectory feasibility. Leveraging the dynamics model of
the bimodal vehicle, the proposed approach achieves adaptive and reliable
motion planning across different terrains and operating conditions. A series of
real-world experiments and benchmark comparisons on a custom-built platform
validate the effectiveness and robustness of the method, demonstrating
improvements in tracking accuracy, task efficiency, and energy performance
under both ground and aerial disturbances.

</details>


### [120] [ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian Inference](https://arxiv.org/abs/2508.06053)
*Kaixuan Wu,Yuanzhuo Xu,Zejun Zhang,Weiping Zhu,Steve Drew,Xiaoguang Niu*

Main category: cs.RO

TL;DR: ReNiL是一个贝叶斯深度学习框架，通过IPDPs和ASLE技术，实现了高效、准确且不确定性感知的行人惯性定位，适用于移动和物联网服务。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的方法依赖于固定的滑动窗口集成，难以适应不同的运动尺度和节奏，且产生的不确定性不一致，限制了实际应用。ReNiL旨在解决这些问题。

Method: ReNiL引入了惯性定位需求点（IPDPs）来估计在上下文有意义的路径点上的运动，支持任意尺度的IMU序列推理。它结合了运动感知的方向滤波器与任意尺度拉普拉斯估计器（ASLE），后者是一个双任务网络，混合了基于块的自监督与贝叶斯回归。

Result: 在RoNIN-ds和新的WUDataset上，ReNiL在位移精度和不确定性一致性方面达到了最先进的水平，超越了TLIO、CTIN、iMoT和RoNIN变体，同时减少了计算量。

Conclusion: ReNiL作为一种贝叶斯深度学习框架，在行人惯性定位中表现出色，提供了准确、高效且不确定性感知的定位解决方案，适用于移动和物联网服务。

Abstract: Pedestrian inertial localization is key for mobile and IoT services because
it provides infrastructure-free positioning. Yet most learning-based methods
depend on fixed sliding-window integration, struggle to adapt to diverse motion
scales and cadences, and yield inconsistent uncertainty, limiting real-world
use. We present ReNiL, a Bayesian deep-learning framework for accurate,
efficient, and uncertainty-aware pedestrian localization. ReNiL introduces
Inertial Positioning Demand Points (IPDPs) to estimate motion at contextually
meaningful waypoints instead of dense tracking, and supports inference on IMU
sequences at any scale so cadence can match application needs. It couples a
motion-aware orientation filter with an Any-Scale Laplace Estimator (ASLE), a
dual-task network that blends patch-based self-supervision with Bayesian
regression. By modeling displacements with a Laplace distribution, ReNiL
provides homogeneous Euclidean uncertainty that integrates cleanly with other
sensors. A Bayesian inference chain links successive IPDPs into consistent
trajectories. On RoNIN-ds and a new WUDataset covering indoor and outdoor
motion from 28 participants, ReNiL achieves state-of-the-art displacement
accuracy and uncertainty consistency, outperforming TLIO, CTIN, iMoT, and RoNIN
variants while reducing computation. Application studies further show
robustness and practicality for mobile and IoT localization, making ReNiL a
scalable, uncertainty-aware foundation for next-generation positioning.

</details>


### [121] [Incremental Language Understanding for Online Motion Planning of Robot Manipulators](https://arxiv.org/abs/2508.06095)
*Mitchell Abrams,Thies Oelerich,Christian Hartl-Nesic,Andreas Kugi,Matthias Scheutz*

Main category: cs.RO

TL;DR: 论文提出了一种增量解析器，结合在线运动规划，使机器人能实时适应动态语言输入，提升了人机协作的流畅性。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导的机器人运动规划方法通常假设指令完全明确，导致在需要纠正或澄清时出现低效的停止和重新规划行为。

Method: 引入了一种基于推理的增量解析器，结合在线运动规划算法，支持动态语言输入的实时处理和动作调整。

Result: 在真实人机交互场景中验证了系统对目标姿态、约束或任务目标的在线调整能力，证明了其优越性。

Conclusion: 该论文提出了一种新颖的基于推理的增量解析器，结合在线运动规划算法，显著提升了机器人在动态语言输入下的实时适应能力，实现了更自然流畅的人机协作。

Abstract: Human-robot interaction requires robots to process language incrementally,
adapting their actions in real-time based on evolving speech input. Existing
approaches to language-guided robot motion planning typically assume fully
specified instructions, resulting in inefficient stop-and-replan behavior when
corrections or clarifications occur. In this paper, we introduce a novel
reasoning-based incremental parser which integrates an online motion planning
algorithm within the cognitive architecture. Our approach enables continuous
adaptation to dynamic linguistic input, allowing robots to update motion plans
without restarting execution. The incremental parser maintains multiple
candidate parses, leveraging reasoning mechanisms to resolve ambiguities and
revise interpretations when needed. By combining symbolic reasoning with online
motion planning, our system achieves greater flexibility in handling speech
corrections and dynamically changing constraints. We evaluate our framework in
real-world human-robot interaction scenarios, demonstrating online adaptions of
goal poses, constraints, or task objectives. Our results highlight the
advantages of integrating incremental language understanding with real-time
motion planning for natural and fluid human-robot collaboration. The
experiments are demonstrated in the accompanying video at
www.acin.tuwien.ac.at/42d5.

</details>


### [122] [Bounding Distributional Shifts in World Modeling through Novelty Detection](https://arxiv.org/abs/2508.06096)
*Eric Jing,Abdeslam Boularias*

Main category: cs.RO

TL;DR: 提出一种基于变分自编码器的新颖性检测方法，增强模型预测控制的鲁棒性，实验证明其在数据效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉世界模型对训练质量敏感，需近乎完整的动作和状态空间覆盖以避免推理时发散，因此需要一种更鲁棒的规划方法。

Method: 提出了一种结合变分自编码器的新颖性检测机制，将其集成到基于DINO-WM架构的模型预测控制策略中。

Result: 在具有挑战性的模拟机器人环境中，该方法在数据效率方面优于现有最先进解决方案。

Conclusion: 使用变分自编码器作为新颖性检测器，显著提高了模型在规划过程中的鲁棒性，特别是在数据效率方面优于现有最先进方法。

Abstract: Recent work on visual world models shows significant promise in latent state
dynamics obtained from pre-trained image backbones. However, most of the
current approaches are sensitive to training quality, requiring near-complete
coverage of the action and state space during training to prevent divergence
during inference. To make a model-based planning algorithm more robust to the
quality of the learned world model, we propose in this work to use a
variational autoencoder as a novelty detector to ensure that proposed action
trajectories during planning do not cause the learned model to deviate from the
training data distribution. To evaluate the effectiveness of this approach, a
series of experiments in challenging simulated robot environments was carried
out, with the proposed method incorporated into a model-predictive control
policy loop extending the DINO-WM architecture. The results clearly show that
the proposed method improves over state-of-the-art solutions in terms of data
efficiency.

</details>


### [123] [Beyond Constant Parameters: Hyper Prediction Models and HyperMPC](https://arxiv.org/abs/2508.06181)
*Jan Węgrzynowski,Piotr Kicki,Grzegorz Czechmanowski,Maciej Krupka,Krzysztof Walas*

Main category: cs.RO

TL;DR: 提出 HyperPM，通过时变参数神经网络改进 MPC 的动力学模型，显著减少长期预测误差，并在实际应用中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的 MPC 动力学模型受限于计算复杂性和状态表示，需改进以提升预测能力。

Method: 提出 Hyper Prediction Model (HyperPM)，通过神经网络学习时变模型参数，将未建模动态投影到时间依赖的动力学模型上。

Result: 在 F1TENTH 自动驾驶赛车等挑战性系统中，HyperPM 显著减少了长期预测误差，且在 MPC 框架中表现优于现有技术。

Conclusion: HyperPM 方法在 MPC 框架下显著降低了长期预测误差，并优于现有技术。

Abstract: Model Predictive Control (MPC) is among the most widely adopted and reliable
methods for robot control, relying critically on an accurate dynamics model.
However, existing dynamics models used in the gradient-based MPC are limited by
computational complexity and state representation. To address this limitation,
we propose the Hyper Prediction Model (HyperPM) - a novel approach in which we
project the unmodeled dynamics onto a time-dependent dynamics model. This
time-dependency is captured through time-varying model parameters, whose
evolution over the MPC prediction horizon is learned using a neural network.
Such formulation preserves the computational efficiency and robustness of the
base model while equipping it with the capacity to anticipate previously
unmodeled phenomena. We evaluated the proposed approach on several challenging
systems, including real-world F1TENTH autonomous racing, and demonstrated that
it significantly reduces long-horizon prediction errors. Moreover, when
integrated within the MPC framework (HyperMPC), our method consistently
outperforms existing state-of-the-art techniques.

</details>


### [124] [Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model](https://arxiv.org/abs/2508.06206)
*Hanqing Wang,Shaoyang Wang,Yiming Zhong,Zemin Yang,Jiamin Wang,Zhiqing Cui,Jiahao Yuan,Yifan Han,Mingyu Liu,Yuexin Ma*

Main category: cs.RO

TL;DR: Affordance-R1是首个整合GRPO与推理的赋能推理框架，通过强化学习和高质量数据集实现了零样本泛化和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型因缺乏链式思维（CoT）推理能力，忽视了不同对象间的赋能共享，限制了外域泛化和显式推理能力。

Method: 提出了Affordance-R1框架，包含复杂的赋能函数（格式、感知和认知奖励）以指导优化方向，并构建了高质量赋能中心推理数据集ReasonAff支持训练。

Result: Affordance-R1在强化学习训练下实现了零样本泛化和测试时推理能力，实验表明其优于现有方法并具备开放世界泛化能力。

Conclusion: Affordance-R1通过整合认知链式思维（CoT）引导的群体相对策略优化（GRPO）和强化学习范式，显著提升了外域（OOD）泛化和显式推理能力，成为首个将GRPO与推理结合的赋能推理框架。

Abstract: Affordance grounding focuses on predicting the specific regions of objects
that are associated with the actions to be performed by robots. It plays a
vital role in the fields of human-robot interaction, human-object interaction,
embodied manipulation, and embodied perception. Existing models often neglect
the affordance shared among different objects because they lack the
Chain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD)
generalization and explicit reasoning capabilities. To address these
challenges, we propose Affordance-R1, the first unified affordance grounding
framework that integrates cognitive CoT guided Group Relative Policy
Optimization (GRPO) within a reinforcement learning paradigm. Specifically, we
designed a sophisticated affordance function, which contains format,
perception, and cognition rewards to effectively guide optimization directions.
Furthermore, we constructed a high-quality affordance-centric reasoning
dataset, ReasonAff, to support training. Trained exclusively via reinforcement
learning with GRPO and without explicit reasoning data, Affordance-R1 achieves
robust zero-shot generalization and exhibits emergent test-time reasoning
capabilities. Comprehensive experiments demonstrate that our model outperforms
well-established methods and exhibits open-world generalization. To the best of
our knowledge, Affordance-R1 is the first to integrate GRPO-based RL with
reasoning into affordance reasoning. The code of our method and our dataset is
released on https://github.com/hq-King/Affordance-R1.

</details>


### [125] [Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization](https://arxiv.org/abs/2508.06207)
*Andrea Dal Prete,Seyram Ofori,Chan Yon Sin,Ashwin Narayan,Francesco Braghin,Marta Gandolla,Haoyong Yu*

Main category: cs.RO

TL;DR: 该研究通过优化空间和自适应控制策略，显著减少了背部肌肉的激活峰值，提升了外骨骼的效果和用户体验。


<details>
  <summary>Details</summary>
Motivation: 背部外骨骼可以减少肌肉骨骼的负担，但其效果取决于支持调节和自适应控制。本研究旨在解决两个挑战：定义最优支持策略和开发基于载荷估计的自适应控制。

Method: 研究引入了基于肌肉活动减少、感知不适和用户偏好的优化空间，构建了识别最优策略的函数。基于这些见解，开发了一个基于视觉的自适应控制流程，通过增强外骨骼的情境理解来实时估计载荷。

Result: 实验结果显示，与静态控制相比，自适应调节将背部肌肉的峰值激活减少了23%，同时保留了用户偏好并最小化了不适感。验证阶段显示准确性超过80%，并在所有指标上有所改善。

Conclusion: 研究验证了所提出的框架，并凸显了智能、情境感知控制在工业外骨骼中的潜力。

Abstract: Back exoskeletons can reduce musculoskeletal strain, but their effectiveness
depends on support modulation and adaptive control. This study addresses two
challenges: defining optimal support strategies and developing adaptive control
based on payload estimation. We introduce an optimization space based on muscle
activity reduction, perceived discomfort, and user preference, constructing
functions to identify optimal strategies. Experiments with 12 subjects revealed
optimal operating regions, highlighting the need for dynamic modulation. Based
on these insights, we developed a vision-based adaptive control pipeline that
estimates payloads in real-time by enhancing exoskeleton contextual
understanding, minimising latency and enabling support adaptation within the
defined optimisation space. Validation with 12 more subjects showed over 80%
accuracy and improvements across all metrics. Compared to static control,
adaptive modulation reduced peak back muscle activation by up to 23% while
preserving user preference and minimising discomfort. These findings validate
the proposed framework and highlight the potential of intelligent,
context-aware control in industrial exoskeletons.

</details>


### [126] [REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance](https://arxiv.org/abs/2508.06229)
*Zihao Xu,Ce Hao,Chunzheng Wang,Kuankuan Sima,Fan Shi,Jin Song Dong*

Main category: cs.RO

TL;DR: REBot是一种实时反射性避障控制框架，通过有限状态机整合避障和恢复策略，显著提升四足机器人在动态环境中的避障性能。


<details>
  <summary>Details</summary>
Motivation: 现有动态避障方法依赖导航轨迹重规划，反应时间不足，无法应对快速接近的障碍物。四足机器人需要瞬时低延迟的反射性避障能力。

Method: REBot采用有限状态机整合避障策略和恢复策略，并通过精心设计的学习课程、正则化和自适应奖励实现实时反射性避障。

Result: 通过大量仿真和实际实验验证，REBot在避障成功率、能效和应对快速移动障碍物的鲁棒性方面均有显著提升。

Conclusion: REBot框架通过有限状态机整合避障和恢复策略，显著提高了四足机器人在动态避障任务中的成功率、能效和鲁棒性。

Abstract: Dynamic obstacle avoidance (DOA) is critical for quadrupedal robots operating
in environments with moving obstacles or humans. Existing approaches typically
rely on navigation-based trajectory replanning, which assumes sufficient
reaction time and leading to fails when obstacles approach rapidly. In such
scenarios, quadrupedal robots require reflexive evasion capabilities to perform
instantaneous, low-latency maneuvers. This paper introduces Reflexive Evasion
Robot (REBot), a control framework that enables quadrupedal robots to achieve
real-time reflexive obstacle avoidance. REBot integrates an avoidance policy
and a recovery policy within a finite-state machine. With carefully designed
learning curricula and by incorporating regularization and adaptive rewards,
REBot achieves robust evasion and rapid stabilization in instantaneous DOA
tasks. We validate REBot through extensive simulations and real-world
experiments, demonstrating notable improvements in avoidance success rates,
energy efficiency, and robustness to fast-moving obstacles. Videos and appendix
are available on https://rebot-2025.github.io/.

</details>


### [127] [ADPro: a Test-time Adaptive Diffusion Policy for Robot Manipulation via Manifold and Initial Noise Constraints](https://arxiv.org/abs/2508.06266)
*Zezeng Li,Rui Yang,Ruochen Chen,ZhongXuan Luo,Liming Chen*

Main category: cs.RO

TL;DR: ADP通过几何约束和初始化优化，提升扩散策略在机器人操作中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略在动作生成中忽略了几何和控制结构的先验知识，限制了其性能。

Method: 提出自适应扩散策略（ADP），通过几何流形约束和解析引导初始化优化扩散过程。

Result: 在RLBench、CALVIN和真实数据集上，ADP（ADPro实现）成功率和采样效率显著提升，执行速度提高25%，成功率提升9%。

Conclusion: ADP通过引入几何流形约束和解析引导初始化，显著提升了扩散策略在机器人操作中的成功率和效率，无需重新训练即可适应新任务和环境。

Abstract: Diffusion policies have recently emerged as a powerful class of visuomotor
controllers for robot manipulation, offering stable training and expressive
multi-modal action modeling. However, existing approaches typically treat
action generation as an unconstrained denoising process, ignoring valuable a
priori knowledge about geometry and control structure. In this work, we propose
the Adaptive Diffusion Policy (ADP), a test-time adaptation method that
introduces two key inductive biases into the diffusion. First, we embed a
geometric manifold constraint that aligns denoising updates with task-relevant
subspaces, leveraging the fact that the relative pose between the end-effector
and target scene provides a natural gradient direction, and guiding denoising
along the geodesic path of the manipulation manifold. Then, to reduce
unnecessary exploration and accelerate convergence, we propose an analytically
guided initialization: rather than sampling from an uninformative prior, we
compute a rough registration between the gripper and target scenes to propose a
structured initial noisy action. ADP is compatible with pre-trained diffusion
policies and requires no retraining, enabling test-time adaptation that tailors
the policy to specific tasks, thereby enhancing generalization across novel
tasks and environments. Experiments on RLBench, CALVIN, and real-world dataset
show that ADPro, an implementation of ADP, improves success rates,
generalization, and sampling efficiency, achieving up to 25% faster execution
and 9% points over strong diffusion baselines.

</details>


### [128] [EcBot: Data-Driven Energy Consumption Open-Source MATLAB Library for Manipulators](https://arxiv.org/abs/2508.06276)
*Juan Heredia,Christian Schlette,Mikkel Baun Kjærgaard*

Main category: cs.RO

TL;DR: 开源Matlab库通过数据驱动方法提升机械臂电力估算精度，验证结果显示训练集RMSE 1.42-2.80W，测试集1.45-5.25W。


<details>
  <summary>Details</summary>
Motivation: 现有机械臂电力估算模型主要针对传统工业机器人，且准确性不足，因此需要一种更通用且精确的解决方案。

Method: 开发了一个基于Matlab的开源库，利用Denavit-Hartenberg参数、质量、质心以及实际运行数据（如关节位置、速度、加速度、电力及时间戳）自动生成EC模型。

Result: 在来自三家制造商的四种轻型机器人上验证，训练集RMSE为1.42-2.80W，测试集为1.45-5.25W。

Conclusion: 提出的开源Matlab库通过数据驱动方法有效提高了机械臂电力估算的准确性，填补了现有模型的不足。

Abstract: Existing literature proposes models for estimating the electrical power of
manipulators, yet two primary limitations prevail. First, most models are
predominantly tested using traditional industrial robots. Second, these models
often lack accuracy. To address these issues, we introduce an open source
Matlab-based library designed to automatically generate \ac{ec} models for
manipulators. The necessary inputs for the library are Denavit-Hartenberg
parameters, link masses, and centers of mass. Additionally, our model is
data-driven and requires real operational data, including joint positions,
velocities, accelerations, electrical power, and corresponding timestamps. We
validated our methodology by testing on four lightweight robots sourced from
three distinct manufacturers: Universal Robots, Franka Emika, and Kinova. The
model underwent testing, and the results demonstrated an RMSE ranging from 1.42
W to 2.80 W for the training dataset and from 1.45 W to 5.25 W for the testing
dataset.

</details>


### [129] [Mitigating Undesired Conditions in Flexible Production with Product-Process-Resource Asset Knowledge Graphs](https://arxiv.org/abs/2508.06278)
*Petr Novak,Stefan Biffl,Marek Obitko,Petr Kadera*

Main category: cs.RO

TL;DR: PPR-AKG模型结合语义技术和LLMs，优化了工业CPPS的不良条件处理和资源分配。


<details>
  <summary>Details</summary>
Motivation: 解决工业4.0环境下CPPS因灵活性导致的不良条件分析和传统质量保证机制失效的问题。

Method: 基于ISA-95和VDI-3682的PPR模型，开发了PPR-AKG语义模型，并结合大型语言模型（LLMs）提供自然语言交互接口。

Result: 在电动汽车电池再制造案例中，PPR-AKG有效支持了资源分配和不良条件的识别与缓解。

Conclusion: PPR-AKG模型通过结合语义技术和大型语言模型，为工业CPPS提供了灵活的资源分配和不良条件处理能力，显著提升了生产效率和操作便捷性。

Abstract: Contemporary industrial cyber-physical production systems (CPPS) composed of
robotic workcells face significant challenges in the analysis of undesired
conditions due to the flexibility of Industry 4.0 that disrupts traditional
quality assurance mechanisms. This paper presents a novel industry-oriented
semantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG),
which is designed to analyze and mitigate undesired conditions in flexible
CPPS. Built on top of the well-proven Product-Process-Resource (PPR) model
originating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses
shortcomings of conventional model-driven engineering for CPPS, particularly
inadequate undesired condition and error handling representation. The
integration of semantic technologies with large language models (LLMs) provides
intuitive interfaces for factory operators, production planners, and engineers
to interact with the entire model using natural language. Evaluation with the
use case addressing electric vehicle battery remanufacturing demonstrates that
the PPR-AKG approach efficiently supports resource allocation based on
explicitly represented capabilities as well as identification and mitigation of
undesired conditions in production. The key contributions include (1) a
holistic PPR-AKG model capturing multi-dimensional production knowledge, and
(2) the useful combination of the PPR-AKG with LLM-based chatbots for human
interaction.

</details>


### [130] [Situationally-aware Path Planning Exploiting 3D Scene Graphs](https://arxiv.org/abs/2508.06283)
*Saad Ejaz,Marco Giberna,Muhammad Shaheer,Jose Andres Millan-Romera,Ali Tourani,Paul Kremer,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: S-Path利用3D场景图语义结构，通过两阶段规划和重规划机制，显著提升路径规划效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景图的结构未被充分利用以提高路径规划效率和可解释性，因此提出S-Path来填补这一空白。

Method: S-Path采用两阶段规划流程：首先在语义图上进行搜索生成高层路径，再分解为独立子问题并行求解，并引入重规划机制优化未来规划效率。

Result: 实验表明，S-Path平均减少5.7倍规划时间，在复杂场景中超越经典采样规划器。

Conclusion: S-Path通过利用室内3D场景图的度量语义结构，显著提升了路径规划效率，同时保持了与经典采样规划器相当的路径最优性，在复杂场景中表现更优。

Abstract: 3D Scene Graphs integrate both metric and semantic information, yet their
structure remains underutilized for improving path planning efficiency and
interpretability. In this work, we present S-Path, a situationally-aware path
planner that leverages the metric-semantic structure of indoor 3D Scene Graphs
to significantly enhance planning efficiency. S-Path follows a two-stage
process: it first performs a search over a semantic graph derived from the
scene graph to yield a human-understandable high-level path. This also
identifies relevant regions for planning, which later allows the decomposition
of the problem into smaller, independent subproblems that can be solved in
parallel. We also introduce a replanning mechanism that, in the event of an
infeasible path, reuses information from previously solved subproblems to
update semantic heuristics and prioritize reuse to further improve the
efficiency of future planning attempts. Extensive experiments on both
real-world and simulated environments show that S-Path achieves average
reductions of 5.7x in planning time while maintaining comparable path
optimality to classical sampling-based planners and surpassing them in complex
scenarios, making it an efficient and interpretable path planner for
environments represented by indoor 3D Scene Graphs.

</details>


### [131] [Real-Time 3D Vision-Language Embedding Mapping](https://arxiv.org/abs/2508.06291)
*Christian Rauch,Björn Ellensohn,Linus Nwankwo,Vedant Dave,Elmar Rueckert*

Main category: cs.RO

TL;DR: 该研究提出了一种实时、高精度的语义3D表示方法，结合局部嵌入掩蔽和置信度加权3D集成，适用于多种机器人任务，实验证明其有效性和实时性。


<details>
  <summary>Details</summary>
Motivation: 高精度的语义3D表示对许多机器人任务至关重要，但现有方法在实时性和准确性上存在不足。

Method: 提出了一种简单而有效的方法，将视觉语言模型的2D嵌入集成到高精度的3D表示中，包括局部嵌入掩蔽策略和置信度加权的3D集成。

Result: 实验证明，该方法在多房间全局和局部物体级别上均能准确表示语义概念，显著提高了感兴趣对象的定位精度，并满足实时性要求。

Conclusion: 该研究提出的方法通过结合局部嵌入掩蔽策略和置信度加权的3D集成，实现了实时、高精度的语义3D表示，适用于多种机器人任务。

Abstract: A metric-accurate semantic 3D representation is essential for many robotic
tasks. This work proposes a simple, yet powerful, way to integrate the 2D
embeddings of a Vision-Language Model in a metric-accurate 3D representation at
real-time. We combine a local embedding masking strategy, for a more distinct
embedding distribution, with a confidence-weighted 3D integration for more
reliable 3D embeddings. The resulting metric-accurate embedding representation
is task-agnostic and can represent semantic concepts on a global multi-room, as
well as on a local object-level. This enables a variety of interactive robotic
applications that require the localisation of objects-of-interest via natural
language. We evaluate our approach on a variety of real-world sequences and
demonstrate that these strategies achieve a more accurate object-of-interest
localisation while improving the runtime performance in order to meet our
real-time constraints. We further demonstrate the versatility of our approach
in a variety of interactive handheld, mobile robotics and manipulation tasks,
requiring only raw image data.

</details>


### [132] [Evaluating Robot Program Performance with Power Consumption Driven Metrics in Lightweight Industrial Robots](https://arxiv.org/abs/2508.06295)
*Juan Heredia,Emil Stubbe Kolvig-Raun,Sune Lundo Sorensen,Mikkel Baun Kjaergaard*

Main category: cs.RO

TL;DR: 本研究提出了一种通过电力功率曲线评估机器人程序性能的新框架，使用标准化指标比较不同策略，为优化编程和提升工业目标提供见解。


<details>
  <summary>Details</summary>
Motivation: 传统的基于CPU的机器人代码性能分析方法忽视了代码对机器人行为的物理影响，本研究旨在从具体化角度出发，通过电力功率曲线评估机器人程序的性能。

Method: 研究引入了一套标准化指标（能源利用系数、能源转换指标和可靠性系数），结合机器人磨损指标，通过分析UR5e机器人在机器照料任务中的电力消耗模式，比较了四种不同策略的程序。

Result: 研究结果表明，提出的指标能够直接比较和分类不同的机器人程序，揭示了每种策略的优缺点，为优化编程实践提供了依据。

Conclusion: 通过从具体化角度分析机器人的电力功率曲线，本研究提出的框架能够更全面地评估机器人程序的性能，为优化编程实践提供可行见解，从而提升能源效率和可靠性，支持可持续制造和成本降低等工业目标。

Abstract: The code performance of industrial robots is typically analyzed through CPU
metrics, which overlook the physical impact of code on robot behavior. This
study introduces a novel framework for assessing robot program performance from
an embodiment perspective by analyzing the robot's electrical power profile.
Our approach diverges from conventional CPU based evaluations and instead
leverages a suite of normalized metrics, namely, the energy utilization
coefficient, the energy conversion metric, and the reliability coefficient, to
capture how efficiently and reliably energy is used during task execution.
Complementing these metrics, the established robot wear metric provides further
insight into long term reliability. Our approach is demonstrated through an
experimental case study in machine tending, comparing four programs with
diverse strategies using a UR5e robot. The proposed metrics directly compare
and categorize different robot programs, regardless of the specific task, by
linking code performance to its physical manifestation through power
consumption patterns. Our results reveal the strengths and weaknesses of each
strategy, offering actionable insights for optimizing robot programming
practices. Enhancing energy efficiency and reliability through this embodiment
centric approach not only improves individual robot performance but also
supports broader industrial objectives such as sustainable manufacturing and
cost reduction.

</details>


### [133] [Surrogate-Enhanced Modeling and Adaptive Modular Control of All-Electric Heavy-Duty Robotic Manipulators](https://arxiv.org/abs/2508.06313)
*Amir Hossein Barjini,Mohammad Bahari,Mahdi Hejrati,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文提出了一种结合替代增强致动器模型和VDC架构的控制框架，实现了全电动重型机械臂的高精度模块化实时控制。


<details>
  <summary>Details</summary>
Motivation: 为全电动重型机械臂（HDRM）提供一个模块化、实时的控制解决方案，以支持其在下一代移动工作机械中的应用。

Method: 提出了一个统一的系统级建模和控制框架，结合了替代增强的致动器模型和扩展的虚拟分解控制（VDC）架构，并通过自然适应律进行增强。

Result: 在多域模拟和实验验证中，提出的自适应模块控制器实现了亚厘米级的笛卡尔跟踪精度。

Conclusion: 本文表明，将增强替代模型嵌入VDC方法可以实现全电动重型机械臂的模块化实时控制，支持其在下一代移动工作机械中的部署。

Abstract: This paper presents a unified system-level modeling and control framework for
an all-electric heavy-duty robotic manipulator (HDRM) driven by
electromechanical linear actuators (EMLAs). A surrogate-enhanced actuator
model, combining integrated electromechanical dynamics with a neural network
trained on a dedicated testbed, is integrated into an extended virtual
decomposition control (VDC) architecture augmented by a natural adaptation law.
The derived analytical HDRM model supports a hierarchical control structure
that seamlessly maps high-level force and velocity objectives to real-time
actuator commands, accompanied by a Lyapunov-based stability proof. In
multi-domain simulations of both cubic and a custom planar triangular
trajectory, the proposed adaptive modular controller achieves sub-centimeter
Cartesian tracking accuracy. Experimental validation of the same 1-DoF platform
under realistic load emulation confirms the efficacy of the proposed control
strategy. These findings demonstrate that a surrogate-enhanced EMLA model
embedded in the VDC approach can enable modular, real-time control of an
all-electric HDRM, supporting its deployment in next-generation mobile working
machines.

</details>


### [134] [Towards Balanced Behavior Cloning from Imbalanced Datasets](https://arxiv.org/abs/2508.06319)
*Sagar Parekh,Heramb Nemlekar,Dylan P. Losey*

Main category: cs.RO

TL;DR: 本文研究模仿学习中数据不平衡问题，提出元梯度重平衡算法，实验证明其能有效提升学习性能。


<details>
  <summary>Details</summary>
Motivation: 人类演示的数据集不可避免地存在不平衡，现有方法默认平等对待每个数据元素，导致学习算法偏向高频行为，而非复杂的多任务演示。

Method: 分析了不平衡数据导致不平衡策略的问题，探索了重新平衡离线数据集的算法，并引入了元梯度重平衡算法。

Result: 实验表明，数据集重平衡能提升模仿学习算法的整体性能。

Conclusion: 本文提出了一种新颖的元梯度重平衡算法，解决了现有方法的主要局限性，实验证明数据集重平衡能提升下游学习性能，无需额外数据收集。

Abstract: Robots should be able to learn complex behaviors from human demonstrations.
In practice, these human-provided datasets are inevitably imbalanced: i.e., the
human demonstrates some subtasks more frequently than others. State-of-the-art
methods default to treating each element of the human's dataset as equally
important. So if -- for instance -- the majority of the human's data focuses on
reaching a goal, and only a few state-action pairs move to avoid an obstacle,
the learning algorithm will place greater emphasis on goal reaching. More
generally, misalignment between the relative amounts of data and the importance
of that data causes fundamental problems for imitation learning approaches. In
this paper we analyze and develop learning methods that automatically account
for mixed datasets. We formally prove that imbalanced data leads to imbalanced
policies when each state-action pair is weighted equally; these policies
emulate the most represented behaviors, and not the human's complex, multi-task
demonstrations. We next explore algorithms that rebalance offline datasets
(i.e., reweight the importance of different state-action pairs) without human
oversight. Reweighting the dataset can enhance the overall policy performance.
However, there is no free lunch: each method for autonomously rebalancing
brings its own pros and cons. We formulate these advantages and disadvantages,
helping other researchers identify when each type of approach is most
appropriate. We conclude by introducing a novel meta-gradient rebalancing
algorithm that addresses the primary limitations behind existing approaches.
Our experiments show that dataset rebalancing leads to better downstream
learning, improving the performance of general imitation learning algorithms
without requiring additional data collection. See our project website:
https://collab.me.vt.edu/data_curation/.

</details>


### [135] [L2Calib: $SE(3)$-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience](https://arxiv.org/abs/2508.06330)
*Baorun Li,Chengrui Zhu,Siyi Du,Bingran Chen,Jie Ren,Wenfei Wang,Yong Liu,Jiajun Lv*

Main category: cs.RO

TL;DR: 提出基于强化学习的外标定框架，通过Bingham分布和轨迹对齐奖励实现高精度标定，无需结构化目标或高质量初始外参，适用于多种机器人平台。


<details>
  <summary>Details</summary>
Motivation: 现有外标定方法依赖结构化目标或完全激励数据，限制了实际应用；在线标定因弱激励导致估计不可靠，亟需一种更鲁棒且通用的解决方案。

Method: 采用强化学习（RL）框架，将外标定问题建模为决策问题，利用Bingham分布建模3D旋转以确保优化稳定性，并设计轨迹对齐奖励机制和自动化数据选择模块。

Result: 在无人机（UAV）、无人地面车辆（UGV）和手持平台上的实验表明，该方法优于传统优化方法，能在弱激励条件下实现高精度标定。

Conclusion: 该论文提出的基于强化学习的外标定框架通过直接优化SE(3)外参，显著提高了标定精度，尤其在弱激励条件下表现优异，且无需高质量初始外参，简化了部署流程。

Abstract: Extrinsic calibration is essential for multi-sensor fusion, existing methods
rely on structured targets or fully-excited data, limiting real-world
applicability. Online calibration further suffers from weak excitation, leading
to unreliable estimates. To address these limitations, we propose a
reinforcement learning (RL)-based extrinsic calibration framework that
formulates extrinsic calibration as a decision-making problem, directly
optimizes $SE(3)$ extrinsics to enhance odometry accuracy. Our approach
leverages a probabilistic Bingham distribution to model 3D rotations, ensuring
stable optimization while inherently retaining quaternion symmetry. A
trajectory alignment reward mechanism enables robust calibration without
structured targets by quantitatively evaluating estimated tightly-coupled
trajectory against a reference trajectory. Additionally, an automated data
selection module filters uninformative samples, significantly improving
efficiency and scalability for large-scale datasets. Extensive experiments on
UAVs, UGVs, and handheld platforms demonstrate that our method outperforms
traditional optimization-based approaches, achieving high-precision calibration
even under weak excitation conditions. Our framework simplifies deployment on
diverse robotic platforms by eliminating the need for high-quality initial
extrinsics and enabling calibration from routine operating data. The code is
available at https://github.com/APRIL-ZJU/learn-to-calibrate.

</details>


### [136] [V*: An Efficient Motion Planning Algorithm for Autonomous Vehicles](https://arxiv.org/abs/2508.06404)
*Abdullah Zareh Andaryan,Michael G. H. Bell,Mohsen Ramezani,Glenn Geers*

Main category: cs.RO

TL;DR: V* 是一种基于图的运动规划器，通过动态图生成和几何剪枝策略，直接在时空速度格子中生成动态可行的无碰撞轨迹，适用于自动驾驶车辆在结构化环境中的导航。


<details>
  <summary>Details</summary>
Motivation: 为了解决结构化环境中自动驾驶车辆导航的挑战，特别是生成时间最优、动态可行的无碰撞轨迹，传统方法往往在空间搜索和动态可行性之间存在脱节或依赖后处理平滑。

Method: V* 采用六边形离散化策略和动态图生成技术，将速度和方向直接纳入搜索扩展过程。此外，通过数学建模转向动力学和几何剪枝策略，确保动态可行性。

Result: 仿真研究表明，V* 在复杂和动态环境中能够有效避免冲突、主动避让，并生成安全高效的轨迹，具备时间推理能力。

Conclusion: V* 是一种创新的基于图的运动规划器，能够直接在时空速度格子中集成速度和方向作为状态变量，生成时间最优且动态可行的无碰撞轨迹。通过动态图生成和几何剪枝策略，V* 确保了轨迹的物理可实现性，无需进一步优化。

Abstract: Autonomous vehicle navigation in structured environments requires planners
capable of generating time-optimal, collision-free trajectories that satisfy
dynamic and kinematic constraints. We introduce V*, a graph-based motion
planner that represents speed and direction as explicit state variables within
a discretised space-time-velocity lattice. Unlike traditional methods that
decouple spatial search from dynamic feasibility or rely on post-hoc smoothing,
V* integrates both motion dimensions directly into graph construction through
dynamic graph generation during search expansion. To manage the complexity of
high-dimensional search, we employ a hexagonal discretisation strategy and
provide formal mathematical proofs establishing optimal waypoint spacing and
minimal node redundancy under constrained heading transitions for
velocity-aware motion planning. We develop a mathematical formulation for
transient steering dynamics in the kinematic bicycle model, modelling steering
angle convergence with exponential behaviour, and deriving the relationship for
convergence rate parameters. This theoretical foundation, combined with
geometric pruning strategies that eliminate expansions leading to infeasible
steering configurations, enables V* to evaluate dynamically admissible
manoeuvres, ensuring each trajectory is physically realisable without further
refinement. We further demonstrate V*'s performance in simulation studies with
cluttered and dynamic environments involving moving obstacles, showing its
ability to avoid conflicts, yield proactively, and generate safe, efficient
trajectories with temporal reasoning capabilities for waiting behaviours and
dynamic coordination.

</details>


### [137] [Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation](https://arxiv.org/abs/2508.06426)
*Youguang Xing,Xu Luo,Junlin Xie,Lianli Gao,Hengtao Shen,Jingkuan Song*

Main category: cs.RO

TL;DR: 研究发现通用机器人策略泛化受限的主要原因是捷径学习，改进数据集收集或增强策略可提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探究通用机器人策略在训练数据分布之外泛化能力受限的根本原因。

Method: 通过理论和实证分析，识别了导致捷径学习的两个主要原因：子数据集内部多样性不足和子数据集之间的分布差异导致的数据集碎片化。

Result: 揭示了捷径学习是泛化能力受限的关键障碍，并提出了改进数据集收集和增强的解决方案。

Conclusion: 研究发现，通过改进数据集收集策略或采用精心设计的机器人数据增强方法，可以有效减少捷径学习，提升通用机器人策略的泛化能力。

Abstract: Generalist robot policies trained on large-scale datasets such as Open
X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.
However, they often struggle to generalize beyond the distribution of their
training data. In this paper, we investigate the underlying cause of this
limited generalization capability. We identify shortcut learning -- the
reliance on task-irrelevant features -- as a key impediment to generalization.
Through comprehensive theoretical and empirical analysis, we uncover two
primary contributors to shortcut learning: (1) limited diversity within
individual sub-datasets, and (2) significant distributional disparities across
sub-datasets, leading to dataset fragmentation. These issues arise from the
inherent structure of large-scale datasets like OXE, which are typically
composed of multiple sub-datasets collected independently across varied
environments and embodiments. Our findings provide critical insights into
dataset collection strategies that can reduce shortcut learning and enhance the
generalization ability of generalist robot policies. Moreover, in scenarios
where acquiring new large-scale data is impractical, we demonstrate that
carefully selected robotic data augmentation strategies can effectively reduce
shortcut learning in existing offline datasets, thereby improving
generalization capabilities of generalist robot policies, e.g., $\pi_0$, in
both simulation and real-world environments. More information at
https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [138] [Debiasing Polynomial and Fourier Regression](https://arxiv.org/abs/2508.05920)
*Chris Camaño,Raphael A. Meyer,Kevin Shu*

Main category: cs.DS

TL;DR: 论文提出了一种基于随机矩阵理论的去偏多项式回归方法，实现了无偏估计和接近最优的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有随机化算法在恢复最优多项式时存在偏差，需要一种无偏且样本复杂度接近最优的方法。

Method: 通过设计一个与概率分布μ相关的随机复矩阵，并利用其特征值进行函数评估，提出了一种无偏的估计器。

Result: 提出的方法在实验中优于独立同分布杠杆得分采样，并能去偏现有周期函数近似方法。

Conclusion: 该论文提出了一种基于随机矩阵理论的去偏方法，用于近似未知函数，该方法在样本复杂度和去偏效果上均表现优异。

Abstract: We study the problem of approximating an unknown function
$f:\mathbb{R}\to\mathbb{R}$ by a degree-$d$ polynomial using as few function
evaluations as possible, where error is measured with respect to a probability
distribution $\mu$. Existing randomized algorithms achieve near-optimal sample
complexities to recover a $ (1+\varepsilon) $-optimal polynomial but produce
biased estimates of the best polynomial approximation, which is undesirable.
  We propose a simple debiasing method based on a connection between polynomial
regression and random matrix theory. Our method involves evaluating
$f(\lambda_1),\ldots,f(\lambda_{d+1})$ where $\lambda_1,\ldots,\lambda_{d+1}$
are the eigenvalues of a suitably designed random complex matrix tailored to
the distribution $\mu$. Our estimator is unbiased, has near-optimal sample
complexity, and experimentally outperforms iid leverage score sampling.
  Additionally, our techniques enable us to debias existing methods for
approximating a periodic function with a truncated Fourier series with
near-optimal sample complexity.

</details>


### [139] [A Structural Linear-Time Algorithm for Computing the Tutte Decomposition](https://arxiv.org/abs/2508.06212)
*Romain Bourneuf,Tim Planken*

Main category: cs.DS

TL;DR: 本文提出了一种线性时间算法，基于完全嵌套2-分离计算Tutte-分解，并引入了稳定性概念，对2-连通图的结构进行了新分析。


<details>
  <summary>Details</summary>
Motivation: 扩展块割树的概念，通过Tutte-分解将2-连通图分解为其三连通分量，并基于Cunningham和Edmonds的结构特征，设计更简单的算法。

Method: 首先计算所有完全嵌套的2-分离，然后基于这些分离构建Tutte-分解。

Result: 提出了一种线性时间算法，用于计算Tutte-分解，并得出了关于完全嵌套2-分离结构的新结果。

Conclusion: 本文提出了一种基于完全嵌套2-分离的Tutte-分解概念简单算法，能够在线性时间内计算Tutte-分解。此外，还通过新颖的稳定性概念，对2-连通图中完全嵌套2-分离的结构提出了新的见解。

Abstract: The block-cut tree decomposes a connected graph along its cutvertices,
displaying its 2-connected components. The Tutte-decomposition extends this
idea to 2-separators in 2-connected graphs, yielding a canonical
tree-decomposition that decomposes the graph into its triconnected components.
In 1973, Hopcroft and Tarjan introduced a linear-time algorithm to compute the
Tutte-decomposition. Cunningham and Edmonds later established a structural
characterization of the Tutte-decomposition via totally-nested 2-separations.
We present a conceptually simple algorithm based on this characterization,
which computes the Tutte-decomposition in linear time. Our algorithm first
computes all totally-nested 2-separations and then builds the
Tutte-decomposition from them.
  Along the way, we derive new structural results on the structure of
totally-nested 2-separations in 2-connected graphs using a novel notion of
stability, which may be of independent interest.

</details>


### [140] [The Beauty of Anisotropic Mesh Refinement: Omnitrees for Efficient Dyadic Discretizations](https://arxiv.org/abs/2508.06316)
*Theresa Pollinger,Masado Ishii,Jens Domke*

Main category: cs.DS

TL;DR: Omnitrees是一种各向异性的数据结构，比octrees更高效，尤其适用于高维问题。


<details>
  <summary>Details</summary>
Motivation: 传统的octrees在各向异性问题中效率低下，因为它们在感兴趣区域强制进行各向同性细化，导致分辨率浪费。

Method: 本文提出了omnitrees作为octrees和各向同性数据结构的各向异性扩展，允许仅在最关键的维度上进行细化。

Result: 在4,166个三维物体的二进制形状表示问题上，omnitrees将平均收敛速度提高了1.5倍，并以更少的存储达到相同的误差界限。

Conclusion: Omnitrees作为一种各向异性的数据结构，显著提高了自适应网格细化（AMR）的效率，尤其是在高维问题中。

Abstract: Structured adaptive mesh refinement (AMR), commonly implemented via quadtrees
and octrees, underpins a wide range of applications including databases,
computer graphics, physics simulations, and machine learning. However, octrees
enforce isotropic refinement in regions of interest, which can be especially
inefficient for problems that are intrinsically anisotropic--much resolution is
spent where little information is gained. This paper presents omnitrees as an
anisotropic generalization of octrees and related data structures. Omnitrees
allow to refine only the locally most important dimensions, providing tree
structures that are less deep than bintrees and less wide than octrees. As a
result, the convergence of the AMR schemes can be increased by up to a factor
of the dimensionality d for very anisotropic problems, quickly offsetting their
modest increase in storage overhead. We validate this finding on the problem of
binary shape representation across 4,166 three-dimensional objects: Omnitrees
increase the mean convergence rate by 1.5x, require less storage to achieve
equivalent error bounds, and maximize the information density of the stored
function faster than octrees. These advantages are projected to be even
stronger for higher-dimensional problems. We provide a first validation by
introducing a time-dependent rotation to create four-dimensional
representations, and discuss the properties of their 4-d octree and omnitree
approximations. Overall, omnitree discretizations can make existing AMR
approaches more efficient, and open up new possibilities for high-dimensional
applications.

</details>


### [141] [A Simple PTAS for Weighted $k$-means and Sensor Coverage](https://arxiv.org/abs/2508.06460)
*Akash Pareek,Supratim Shit*

Main category: cs.DS

TL;DR: 提出了一种不依赖核心集的加权k均值PTAS算法，运行高效，并改进了传感器覆盖问题的近似精度。


<details>
  <summary>Details</summary>
Motivation: 现有加权k均值算法多依赖核心集，缺乏简单直接的PTAS方案，且传感器覆盖问题的最优近似比仅为O(log k)。

Method: 基于加权D²采样技术，扩展了Jaiswal等人对非加权情况的框架，算法运行时间为n·d·2^O(k²/ε)。

Result: 算法在(1+ε)因子内逼近最优聚类成本，运行时间高效，并为传感器覆盖问题提供了(1+ε)近似解。

Conclusion: 本文提出了一种针对加权k均值问题的简单PTAS算法，不依赖于核心集，扩展了Jaiswal等人的框架，并应用于传感器覆盖问题，显著提升了近似精度。

Abstract: Clustering is a fundamental technique in data analysis, with the $k$-means
being one of the widely studied objectives due to its simplicity and broad
applicability. In many practical scenarios, data points come with associated
weights that reflect their importance, frequency, or confidence. Given a
weighted point set $P \subset R^d$, where each point $p \in P$ has a positive
weight $w_p$, the goal is to compute a set of $k$ centers $C = \{ c_1, c_2,
\ldots, c_k \} \subset R^d$ that minimizes the weighted clustering cost:
$\Delta_w(P,C) = \sum_{p \in P} w_p \cdot d(p,C)^2$, where $d(p,C)$ denotes the
Euclidean distance from $p$ to its nearest center in $C$. Although most
existing coreset-based algorithms for $k$-means extend naturally to the
weighted setting and provide a PTAS, no prior work has offered a simple,
coreset-free PTAS designed specifically for the weighted $k$-means problem.
  In this paper, we present a simple PTAS for weighted $k$-means that does not
rely on coresets. Building upon the framework of Jaiswal, Kumar, and Sen (2012)
for the unweighted case, we extend the result to the weighted setting by using
the weighted $D^2$-sampling technique. Our algorithm runs in time $n d \cdot
2^{O\left(\frac{k^2}{\epsilon}\right)}$ and outputs a set of $k$ centers whose
total clustering cost is within a $(1 + \epsilon)$-factor of the optimal cost.
As a key application of the weighted $k$-means, we obtain a PTAS for the sensor
coverage problem, which can also be viewed as a continuous locational
optimization problem. For this problem, the best-known result prior to our work
was an $O(\log k)$-approximation by Deshpande (2014), whereas our algorithm
guarantees a $(1 + \epsilon)$-approximation to the optimal coverage cost even
before applying refinement steps like Lloyd desent.

</details>


### [142] [On the Parallel Complexity of Identifying Groups and Quasigroups via Decompositions](https://arxiv.org/abs/2508.06478)
*Dan Johnson,Michael Levet,Petr Vojtěchovský,Brett Widholm*

Main category: cs.DS

TL;DR: 本文改进了有限群和拟群同构测试的算法复杂度上界，利用分解方法和并行计算技术，提出了多项新结果。


<details>
  <summary>Details</summary>
Motivation: 研究有限群和拟群的同构测试计算复杂度，改进现有算法复杂度上界。

Method: 利用分解方法（如直积分解和仿射分解）和Weisfeiler--Leman算法，结合并行计算技术（如$\textsf{AC}^{3}$）。

Result: 1. 对于特定群类$\mathcal{C}$，同构问题在$\textsf{L}$中；2. 更一般的群类，提出了$\textsf{AC}^{3}$标号算法；3. 中心拟群的同构测试在$\textsf{NC}$中。

Conclusion: 本文通过研究有限群和拟群的同构测试计算复杂度，利用分解方法，提出了改进的算法复杂度上界，包括在$\textsf{L}$和$\textsf{AC}^{3}$中的结果，以及中心拟群的$\textsf{NC}$算法。

Abstract: In this paper, we investigate the computational complexity of isomorphism
testing for finite groups and quasigroups, given by their multiplication
tables. We crucially take advantage of their various decompositions to show the
following:
  - We first consider the class $\mathcal{C}$ of groups that admit direct
product decompositions, where each indecompsable factor is $O(1)$-generated,
and either perfect or centerless. We show any group in $\mathcal{C}$ is
identified by the $O(1)$-dimensional count-free Weisfeiler--Leman (WL)
algorithm with $O(\log \log n)$ rounds, and the $O(1)$-dimensional counting WL
algorithm with $O(1)$ rounds. Consequently, the isomorphism problem for
$\mathcal{C}$ is in $\textsf{L}$. The previous upper bound for this class was
$\textsf{TC}^{1}$, using $O(\log n)$ rounds of the $O(1)$-dimensional counting
WL (Grochow and Levet, FCT 2023).
  - We next consider more generally, the class of groups where each
indecomposable factor is $O(1)$-generated. We exhibit an $\textsf{AC}^{3}$
canonical labeling procedure for this class. Here, we accomplish this by
showing that in the multiplication table model, the direct product
decomposition can be computed in $\textsf{AC}^{3}$, parallelizing the work of
Kayal and Nezhmetdinov (ICALP 2009).
  - Isomorphism testing between a central quasigroup $G$ and an arbitrary
quasigroup $H$ is in $\textsf{NC}$. Here, we take advantage of the fact that
central quasigroups admit an affine decomposition in terms of an underlying
Abelian group. Only the trivial bound of $n^{\log(n)+O(1)}$-time was previously
known for isomorphism testing of central quasigroups.

</details>


### [143] [Does block size matter in randomized block Krylov low-rank approximation?](https://arxiv.org/abs/2508.06486)
*Tyler Chen,Ethan N. Epperly,Raphael A. Meyer,Christopher Musco,Akash Rao*

Main category: cs.DS

TL;DR: 本文解决了随机块Krylov迭代在任意块大小下的高效秩-$k$近似问题，填补了理论与实践的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的研究在块大小$b=1$或$b=k$时，可以高效地获得秩-$k$近似，但在$1\ll b\ll k$时，理论性能与实践表现存在差距。本文旨在解决这一差距。

Method: 本文采用随机块Krylov迭代方法，并通过新的随机块Krylov矩阵最小奇异值界限进行分析。

Result: 证明了在任何块大小$1\le b\le k$下，随机块Krylov迭代均可高效地获得$(1 + \varepsilon)$因子的近似秩-$k$近似。

Conclusion: 本文通过证明随机块Krylov迭代可以在任何块大小$1\le b\le k$下，使用$\tilde O(k/\sqrt{\varepsilon})$次矩阵-向量乘积，得到$(1 + \varepsilon)$因子的近似秩-$k$近似，解决了理论与实践的差距。

Abstract: We study the problem of computing a rank-$k$ approximation of a matrix using
randomized block Krylov iteration. Prior work has shown that, for block size $b
= 1$ or $b = k$, a $(1 + \varepsilon)$-factor approximation to the best
rank-$k$ approximation can be obtained after $\tilde O(k/\sqrt{\varepsilon})$
matrix-vector products with the target matrix. On the other hand, when $b$ is
between $1$ and $k$, the best known bound on the number of matrix-vector
products scales with $b(k-b)$, which could be as large as $O(k^2)$.
Nevertheless, in practice, the performance of block Krylov methods is often
optimized by choosing a block size $1 \ll b \ll k$. We resolve this
theory-practice gap by proving that randomized block Krylov iteration produces
a $(1 + \varepsilon)$-factor approximate rank-$k$ approximation using $\tilde
O(k/\sqrt{\varepsilon})$ matrix-vector products for any block size $1\le b\le
k$. Our analysis relies on new bounds for the minimum singular value of a
random block Krylov matrix, which may be of independent interest. Similar
bounds are central to recent breakthroughs on faster algorithms for sparse
linear systems [Peng & Vempala, SODA 2021; Nie, STOC 2022].

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [144] [Accelerating Data Chunking in Deduplication Systems using Vector Instructions](https://arxiv.org/abs/2508.05797)
*Sreeharsha Udayashankar,Abdelrahman Baba,Samer Al-Kiswany*

Main category: cs.DC

TL;DR: VectorCDC利用向量CPU指令加速无哈希CDC算法，显著提升吞吐量且不影响去重效果。


<details>
  <summary>Details</summary>
Motivation: 现有的CDC算法因需要完整扫描文件而速度慢，成为数据去重系统的主要性能瓶颈。

Method: 采用向量CPU指令（如SSE/AVX）来加速无哈希CDC算法。

Result: 在Intel、AMD、ARM和IBM CPU上，VectorCDC的吞吐量比现有向量加速技术高8.35倍至26.2倍。

Conclusion: VectorCDC通过利用向量CPU指令（如SSE/AVX）显著提升了无哈希CDC算法的处理速度，且在多种CPU架构上均表现优异，同时不影响去重空间节省效果。

Abstract: Content-defined Chunking (CDC) algorithms dictate the overall space savings
that deduplication systems achieve. However, due to their need to scan each
file in its entirety, they are slow and often the main performance bottleneck
within data deduplication. We present VectorCDC, a method to accelerate
hashless CDC algorithms using vector CPU instructions, such as SSE / AVX. Our
evaluation shows that VectorCDC is effective on Intel, AMD, ARM, and IBM CPUs,
achieving 8.35x - 26.2x higher throughput than existing vector-accelerated
techniques without affecting the deduplication space savings.

</details>


### [145] [A Dynamic Approach to Load Balancing in Cloud Infrastructure: Enhancing Energy Efficiency and Resource Utilization](https://arxiv.org/abs/2508.05821)
*Shadman Sakib,Ajay Katangur,Rahul Dubey*

Main category: cs.DC

TL;DR: 论文提出了一种基于分数的动态负载均衡器（SBDLB），显著提高了云计算的资源利用率和效率，降低了成本和能耗。


<details>
  <summary>Details</summary>
Motivation: 云计算中负载均衡是保持性能、防止过载和确保用户体验的关键部分。然而，动态管理服务器资源和保持工作负载平衡仍然是一个主要挑战。

Method: 论文提出了一种新颖的基于分数的动态负载均衡器（SBDLB），根据实时性能指标将工作负载分配给虚拟机。方法在CloudSim 7G平台上进行了全面测试，并与节流负载均衡策略进行了比较。

Result: SBDLB在动态适应工作负载波动和优化资源使用方面表现出色，平均响应时间提高了34%和37%，数据中心处理时间平均减少了13%，并在24小时模拟中降低了15%的运营成本。

Conclusion: 论文提出了一种基于分数的动态负载均衡器（SBDLB），在云环境中显著提高了资源利用率和系统效率，同时降低了运营成本和能源消耗。

Abstract: Cloud computing has grown rapidly in recent years, mainly due to the sharp
increase in data transferred over the internet. This growth makes load
balancing a key part of cloud systems, as it helps distribute user requests
across servers to maintain performance, prevent overload, and ensure a smooth
user experience. Despite its importance, managing server resources and keeping
workloads balanced over time remains a major challenge in cloud environments.
This paper introduces a novel Score-Based Dynamic Load Balancer (SBDLB) that
allocates workloads to virtual machines based on real-time performance metrics.
The objective is to enhance resource utilization and overall system efficiency.
The method was thoroughly tested using the CloudSim 7G platform, comparing its
performance against the throttled load balancing strategy. Evaluations were
conducted across a variety of workloads and scenarios, demonstrating the
SBDLB's ability to adapt dynamically to workload fluctuations while optimizing
resource usage. The proposed method outperformed the throttled strategy,
improving average response times by 34% and 37% in different scenarios. It also
reduced data center processing times by an average of 13%. Over a 24-hour
simulation, the method decreased operational costs by 15%, promoting a more
energy-efficient and sustainable cloud infrastructure through reduced energy
consumption.

</details>


### [146] [Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML Next To Your Data](https://arxiv.org/abs/2508.05904)
*Brandon Baker,Elliott Brossard,Chenwei Xie,Zihao Ye,Deen Liu,Yijun Xie,Arthur Zwiegincew,Nitya Kumar Sharma,Gaurav Jain,Eugene Retunsky,Mike Halcrow,Derek Denny-Brown,Istvan Cseri,Tyler Akidau,Yuxiong He*

Main category: cs.DC

TL;DR: Snowpark是Snowflake推出的一个托管式解决方案，支持数据工程和AI/ML工作负载，通过弹性架构和核心创新实现了高性能、强安全性和易用性。


<details>
  <summary>Details</summary>
Motivation: Snowflake旨在通过Snowpark提供一个支持数据工程和AI/ML工作负载的托管式解决方案，以推动其AI Data Cloud愿景的实现。

Method: 详细介绍了Snowpark的架构，包括其弹性扩展能力、与Snowflake核心计算基础设施的无缝集成、安全沙箱的使用，以及查询初始化延迟减少、工作负载调度优化和数据倾斜管理等核心创新。

Result: 展示了Snowpark在提高性能、增强安全性和治理、以及简化使用方面的设计目标，并通过实际案例证明了其在大规模数据工程和AI/ML任务中的高效性和有效性。

Conclusion: Snowpark通过其高性能、强安全性和易用性，成功支持了大规模数据工程和AI/ML任务，展示了其在Snowflake AI Data Cloud愿景中的关键作用。

Abstract: Snowflake revolutionized data analytics with an elastic architecture that
decouples compute and storage, enabling scalable solutions supporting data
architectures like data lake, data warehouse, data lakehouse, and data mesh.
Building on this foundation, Snowflake has advanced its AI Data Cloud vision by
introducing Snowpark, a managed turnkey solution that supports data engineering
and AI and ML workloads using Python and other programming languages.
  This paper outlines Snowpark's design objectives towards high performance,
strong security and governance, and ease of use. We detail the architecture of
Snowpark, highlighting its elastic scalability and seamless integration with
Snowflake core compute infrastructure. This includes leveraging Snowflake
control plane for distributed computing and employing a secure sandbox for
isolating Snowflake SQL workloads from Snowpark executions. Additionally, we
present core innovations in Snowpark that drive further performance
enhancements, such as query initialization latency reduction through Python
package caching, improved workload scheduling for customized workloads, and
data skew management via efficient row redistribution. Finally, we showcase
real-world case studies that illustrate Snowpark's efficiency and effectiveness
for large-scale data engineering and AI and ML tasks.

</details>


### [147] [KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training](https://arxiv.org/abs/2508.06001)
*Kai Zhang,Peng Wang,Sai Bi,Jianming Zhang,Yuanjun Xiong*

Main category: cs.DC

TL;DR: KnapFormer是一个结合工作负载平衡和序列并行性的框架，用于优化Diffusion Transformers的分布式训练，显著提升效率并减少工作负载差异。


<details>
  <summary>Details</summary>
Motivation: 为了解决在混合分辨率和图像-视频联合训练中，由于变长文本输入和视觉令牌数量不均导致的工作负载不平衡问题，KnapFormer旨在通过优化工作负载分配和序列并行性来提升训练效率。

Method: KnapFormer首先收集所有节点的序列长度元数据，解决全局背包问题以最小化每GPU工作负载的方差，同时考虑了序列并行性的影响。通过集成DeepSpeed-Ulysees的序列并行性和使用半经验工作负载模型，实现了低通信开销和小于1%的工作负载差异。

Result: KnapFormer在实际训练中实现了小于1%的工作负载差异，消除了滞后效应，并在FLUX等先进扩散模型的训练中实现了2倍至3倍的加速。

Conclusion: KnapFormer通过结合工作负载平衡和序列并行性，显著提升了Diffusion Transformers（DiT）的分布式训练效率，消除了滞后效应，并在混合分辨率和图像-视频联合数据训练中实现了2倍至3倍的加速。

Abstract: We present KnapFormer, an efficient and versatile framework to combine
workload balancing and sequence parallelism in distributed training of
Diffusion Transformers (DiT). KnapFormer builds on the insight that strong
synergy exists between sequence parallelism and the need to address the
significant token imbalance across ranks. This imbalance arises from
variable-length text inputs and varying visual token counts in mixed-resolution
and image-video joint training. KnapFormer redistributes tokens by first
gathering sequence length metadata across all ranks in a balancing group and
solving a global knapsack problem. The solver aims to minimize the variances of
total workload per-GPU, while accounting for the effect of sequence
parallelism. By integrating DeepSpeed-Ulysees-based sequence parallelism in the
load-balancing decision process and utilizing a simple semi-empirical workload
model, KnapFormers achieves minimal communication overhead and less than 1%
workload discrepancy in real-world training workloads with sequence length
varying from a few hundred to tens of thousands. It eliminates straggler
effects and achieves 2x to 3x speedup when training state-of-the-art diffusion
models like FLUX on mixed-resolution and image-video joint data corpora. We
open-source the KnapFormer implementation at
https://github.com/Kai-46/KnapFormer/

</details>


### [148] [EC2MoE: Adaptive End-Cloud Pipeline Collaboration Enabling Scalable Mixture-of-Experts Inference](https://arxiv.org/abs/2508.06024)
*Zheming Yang,Yunqing Hu,Sheng Sun,Wen Ji*

Main category: cs.DC

TL;DR: EC2MoE通过端云协同和硬件感知优化，显著提升MoE模型的推理效率和可扩展性，实验显示吞吐量提升2.2-5.1倍，延迟降低53-67%。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型在异构端云环境中部署时面临的专家调度、通信开销和资源异构性等挑战。

Method: 设计了硬件感知的轻量级组门网络和端云协同的管道优化机制，包括低秩压缩的编码器-解码器结构和路由感知的启发式管道调度算法。

Result: 实验表明，EC2MoE能将吞吐量提升2.2至5.1倍，端到端延迟降低53%至67%，同时在动态负载和网络环境下保持良好可扩展性。

Conclusion: EC2MoE框架通过端云协同的管道优化和硬件感知的轻量级门控网络，显著提升了MoE模型在异构环境中的推理效率和可扩展性，同时保持高准确性。

Abstract: The Mixture-of-Experts (MoE) paradigm has emerged as a promising solution to
scale up model capacity while maintaining inference efficiency. However,
deploying MoE models across heterogeneous end-cloud environments poses new
challenges in expert scheduling, communication overhead, and resource
heterogeneity. In this paper, we propose EC2MoE, an adaptive framework for
scalable MoE inference via end-cloud pipeline collaboration. First, we design a
hardware-aware lightweight group gate network that enhances expert selection
and computational efficiency. By incorporating a hardware-aware local expert
selection mechanism, the system adaptively filters candidate experts based on
real-time device profiles. A lightweight group gate module then integrates
local and global gating outputs to achieve high-quality expert routing with
minimal overhead. Second, we develop a pipeline optimization mechanism based on
endcloud collaboration to accelerate MoE inference. This includes an
encoder-decoder structure based on low-rank compression, which reduces
transmission and computation costs. And a route-aware heuristic pipeline
scheduling algorithm that dynamically allocates inference stages across devices
according to workload and network topology. Extensive experiments show that
EC2MoE can increase throughput by 2.2x to 5.1x and reduce end-to-end latency by
53% to 67% while maintaining high accuracy compared to state-of-the-art
methods. It also maintains good scalability under dynamic load and network
environments.

</details>


### [149] [KV Cache Compression for Inference Efficiency in LLMs: A Review](https://arxiv.org/abs/2508.06297)
*Yanyu Liu,Jingying Fu,Sixiang Liu,Yitian Zou,You Fu,Jiehan Zhou,Shouhua Zhang*

Main category: cs.DC

TL;DR: 本文综述了KV缓存优化技术，分析了其效果和挑战，并探讨了未来研究方向，以提升大型语言模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速发展，推理上下文长度不断增加，导致对键值（KV）缓存的需求呈指数级增长，引发了显著的内存瓶颈，限制了模型的推理效率和可扩展性。

Method: 本文系统性地回顾了当前的KV缓存优化技术，包括选择性令牌策略、量化和注意力压缩等压缩策略，并评估了它们的有效性、权衡和应用场景。

Result: 本文全面分析了现有KV缓存优化方法对内存使用和推理速度的影响，并指出了现有方法的局限性和挑战，如与不同模型和任务的兼容性问题。

Conclusion: 本文强调，优化KV缓存对于提升大型语言模型的推理效率和可扩展性至关重要，并指出了未来研究方向，如混合优化技术和软硬件协同设计。

Abstract: Withtherapid advancement of large language models (LLMs), the context length
for inference has been continuously increasing, leading to an exponential
growth in the demand for Key-Value (KV) caching. This has resulted in a
significant memory bottleneck, limiting the inference efficiency and
scalability of the models. Therefore, optimizing the KV cache during inference
is crucial for enhancing performance and efficiency. This review systematically
examines current KV cache optimization techniques, including compression
strategies such as selective token strategies, quantization, and attention
compression. We evaluate the effectiveness, trade-offs, and application
scenarios of these methods, providing a comprehensive analysis of their impact
on memory usage and inference speed. We focus on identifying the limitations
and challenges of existing methods, such as compatibility issues with different
models and tasks. Additionally, this review highlights future research
directions, including hybrid optimization techniques, adaptive dynamic
strategies, and software-hardware co-design. These approaches aim to improve
inference efficiency and promote the practical application of large language
models.

</details>


### [150] [Performant Unified GPU Kernels for Portable Singular Value Computation Across Hardware and Precision](https://arxiv.org/abs/2508.06339)
*Evelyne Ringoot,Rabab Alomairy,Valentin Churavy,Alan Edelman*

Main category: cs.DC

TL;DR: 论文提出了一种基于Julia的便携式GPU加速SVD算法，支持多种GPU架构和数据类型，性能优异且无需牺牲便携性。


<details>
  <summary>Details</summary>
Motivation: 奇异值分解（SVD）是科学计算和机器学习中的基本数值工具，尤其在大型机器学习管道（如大型语言模型）中，其重要性日益增加。现有实现缺乏对Apple Metal GPU和半精度的支持。

Method: 算法基于经典的两阶段QR约简，包括将矩阵逐步约简为带状形式和双对角形式。利用Julia的多重分派和元编程能力，与GPUArrays和KernelAbstractions框架集成，提供统一的类型和硬件无关功能。

Result: 在多种GPU后端和数据类型上，统一函数的性能优于大多数线性代数库（如MAGMA、SLATE、rocSOLVER、oneMKL），对于大于1024x1024的矩阵，性能达到cuSOLVER的80%-90%。

Conclusion: 该论文展示了在Julia中实现的便携式、GPU加速的QR奇异值计算算法，其性能在多种GPU架构和数据类型上表现优异，证明了便携性无需牺牲性能。

Abstract: This paper presents a portable, GPU-accelerated implementation of a QR-based
singular value computation algorithm in Julia. The singular value ecomposition
(SVD) is a fundamental numerical tool in scientific computing and machine
learning, providing optimal low-rank matrix approximations. Its importance has
increased even more in large-scale machine learning pipelines, including large
language models (LLMs), where it enables low-rank adaptation (LoRA). The
implemented algorithm is based on the classic two-stage QR reduction,
consisting of successive matrix reduction to band form and bidiagonal form. Our
implementation leverages Julia's multiple dispatch and metaprogramming
capabilities, integrating with the GPUArrays and KernelAbstractions frameworks
to provide a unified type and hardware-agnostic function. It supports diverse
GPU architectures and data types, and is, to our knowledge, the first
GPU-accelerated singular value implementation to support Apple Metal GPUs and
half precision. Performance results on multiple GPU backends and data types
demonstrate that portability does not require sacrificing performance: the
unified function outperforms most linear algebra libraries (MAGMA, SLATE,
rocSOLVER, oneMKL) for matrix sizes larger than 1024x1024, and achieves 80%-90%
of the performance of cuSOLVER for large matrices.

</details>


### [151] [Blockchain-Enabled Federated Learning](https://arxiv.org/abs/2508.06406)
*Murtaza Rangwala,Venugopal K R,Rajkumar Buyya*

Main category: cs.DC

TL;DR: BCFL通过区块链技术解决联邦学习中的信任与隐私问题，采用四维分类法分析架构，并通过案例验证其在实际应用中的可行性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决协作AI系统中的信任、隐私和协调等基本挑战。

Method: 通过系统化的四维分类法（协调结构、共识机制、存储架构和信任模型）对BCFL系统进行全面架构分析，并通过TrustMesh框架的案例研究展示实际实现。

Result: BCFL系统在医疗联盟、金融服务和物联网安全等实际应用中验证了其可行性，实现了分布式图像分类训练，并在高度非独立同分布数据下保持透明性和容错性。

Conclusion: 区块链赋能的联邦学习（BCFL）系统在实际部署中表现出与集中式方法相当的性能，同时提供了更强的安全保证，并实现了无信任协作智能的新模型。

Abstract: Blockchain-enabled federated learning (BCFL) addresses fundamental challenges
of trust, privacy, and coordination in collaborative AI systems. This chapter
provides comprehensive architectural analysis of BCFL systems through a
systematic four-dimensional taxonomy examining coordination structures,
consensus mechanisms, storage architectures, and trust models. We analyze
design patterns from blockchain-verified centralized coordination to fully
decentralized peer-to-peer networks, evaluating trade-offs in scalability,
security, and performance. Through detailed examination of consensus mechanisms
designed for federated learning contexts, including Proof of Quality and Proof
of Federated Learning, we demonstrate how computational work can be repurposed
from arbitrary cryptographic puzzles to productive machine learning tasks. The
chapter addresses critical storage challenges by examining multi-tier
architectures that balance blockchain's transaction constraints with neural
networks' large parameter requirements while maintaining cryptographic
integrity. A technical case study of the TrustMesh framework illustrates
practical implementation considerations in BCFL systems through distributed
image classification training, demonstrating effective collaborative learning
across IoT devices with highly non-IID data distributions while maintaining
complete transparency and fault tolerance. Analysis of real-world deployments
across healthcare consortiums, financial services, and IoT security
applications validates the practical viability of BCFL systems, achieving
performance comparable to centralized approaches while providing enhanced
security guarantees and enabling new models of trustless collaborative
intelligence.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [152] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: AEPO框架通过自适应探索优化，解决了MLLMs在GUI任务中语义对齐的探索瓶颈，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在强化学习可验证奖励（RLVR）中，低效的探索限制了语义对齐能力，导致模型难以学习复杂的语义关联。

Method: 采用多答案生成策略和基于效率第一原理的自适应探索奖励函数（AER）来优化策略。

Result: AEPO训练的模型（InfiGUI-G1-3B和InfiGUI-G1-7B）在多个GUI基准测试中表现优异，相对基线模型提升高达9.0%。

Conclusion: 论文提出的AEPO框架通过自适应探索策略优化，显著提升了多模态大语言模型在GUI任务中的语义对齐能力，并在多个基准测试中取得了最先进的结果。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


### [153] [A Framework for Inherently Safer AGI through Language-Mediated Active Inference](https://arxiv.org/abs/2508.05766)
*Bo Wen*

Main category: cs.AI

TL;DR: 该论文提出了一种结合主动推理与大型语言模型的新框架，旨在通过自然语言表示和分层设计实现AGI的固有安全性。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全方法（如事后可解释性和奖励工程）存在根本性局限，需要一种将安全保障整合到AGI核心设计中的新方法。

Method: 提出了一种架构，将安全保障整合到系统的核心设计中，通过透明的信念表示和分层价值对齐。该框架利用自然语言作为表示和操作信念的媒介，实现直接的人类监督并保持计算的可处理性。架构实现了一个多智能体系统，智能体根据主动推理原则自组织，偏好和安全约束通过分层的马尔可夫毯流动。

Result: 提出了一个结合主动推理与LLMs的框架，通过自然语言表示信念、分层价值对齐和模块化智能体结构确保安全性。

Conclusion: 该论文提出了一种通过结合主动推理原则与大型语言模型（LLMs）来开发安全的人工通用智能（AGI）的新框架，为AGI的发展提供了一条本质上更安全的路径。

Abstract: This paper proposes a novel framework for developing safe Artificial General
Intelligence (AGI) by combining Active Inference principles with Large Language
Models (LLMs). We argue that traditional approaches to AI safety, focused on
post-hoc interpretability and reward engineering, have fundamental limitations.
We present an architecture where safety guarantees are integrated into the
system's core design through transparent belief representations and
hierarchical value alignment. Our framework leverages natural language as a
medium for representing and manipulating beliefs, enabling direct human
oversight while maintaining computational tractability. The architecture
implements a multi-agent system where agents self-organize according to Active
Inference principles, with preferences and safety constraints flowing through
hierarchical Markov blankets. We outline specific mechanisms for ensuring
safety, including: (1) explicit separation of beliefs and preferences in
natural language, (2) bounded rationality through resource-aware free energy
minimization, and (3) compositional safety through modular agent structures.
The paper concludes with a research agenda centered on the Abstraction and
Reasoning Corpus (ARC) benchmark, proposing experiments to validate our
framework's safety properties. Our approach offers a path toward AGI
development that is inherently safer, rather than retrofitted with safety
measures.

</details>


### [154] [Whither symbols in the era of advanced neural networks?](https://arxiv.org/abs/2508.05776)
*Thomas L. Griffiths,Brenden M. Lake,R. Thomas McCoy,Ellie Pavlick,Taylor W. Webb*

Main category: cs.AI

TL;DR: 本文指出神经网络的能力削弱了人类思维必须基于符号系统的论点，并提出了新的研究议程。


<details>
  <summary>Details</summary>
Motivation: 探讨人类思维是否必须以符号系统为基础，以及神经网络在模拟人类认知过程中的作用。

Method: 通过分析现代神经网络和人工智能系统的能力，与人类思维的符号系统理论进行对比。

Result: 研究发现神经网络能够表现出类似人类思维的组合、创新和快速学习能力，这削弱了人类认知必须基于符号系统的论点。

Conclusion: 本文提出了一种新的研究议程，探讨人类思维的符号基础，并指出神经网络的能力削弱了人类认知过程必须基于符号系统的论点。

Abstract: Some of the strongest evidence that human minds should be thought about in
terms of symbolic systems has been the way they combine ideas, produce novelty,
and learn quickly. We argue that modern neural networks -- and the artificial
intelligence systems built upon them -- exhibit similar abilities. This
undermines the argument that the cognitive processes and representations used
by human minds are symbolic, although the fact that these neural networks are
typically trained on data generated by symbolic systems illustrates that such
systems play an important role in characterizing the abstract problems that
human minds have to solve. This argument leads us to offer a new agenda for
research on the symbolic basis of human thought.

</details>


### [155] [Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making](https://arxiv.org/abs/2508.05792)
*Kausik Lakkaraju,Siva Likitha Valluru,Biplav Srivastava*

Main category: cs.AI

TL;DR: H-XAI是一个结合因果评分和传统XAI方法的统一框架，支持交互式、多方法的解释过程，满足不同利益相关者在个体和整体模型层面的需求。


<details>
  <summary>Details</summary>
Motivation: 当前XAI方法主要服务于开发者，侧重于模型输出的合理性证明，而非满足多样化的利益相关者需求。H-XAI旨在通过一个统一的框架，支持解释作为一个交互式、多方法的过程，以更全面地满足不同需求。

Method: H-XAI整合了因果评分方法和传统XAI方法，支持交互式、多方法的解释过程，允许利益相关者提问、测试假设，并比较模型行为与自动构建的随机和偏见基线。

Result: 通过两个案例研究（二元信用风险分类和金融时间序列预测）的六个场景，展示了H-XAI的通用性。

Conclusion: H-XAI框架通过结合因果评分和传统XAI方法，填补了现有解释性AI方法的空白，能够满足不同利益相关者在个体决策和整体模型层面的需求。

Abstract: Current eXplainable AI (XAI) methods largely serve developers, often focusing
on justifying model outputs rather than supporting diverse stakeholder needs. A
recent shift toward Evaluative AI reframes explanation as a tool for hypothesis
testing, but still focuses primarily on operational organizations. We introduce
Holistic-XAI (H-XAI), a unified framework that integrates causal rating methods
with traditional XAI methods to support explanation as an interactive,
multi-method process. H-XAI allows stakeholders to ask a series of questions,
test hypotheses, and compare model behavior against automatically constructed
random and biased baselines. It combines instance-level and global
explanations, adapting to each stakeholder's goals, whether understanding
individual decisions, assessing group-level bias, or evaluating robustness
under perturbations. We demonstrate the generality of our approach through two
case studies spanning six scenarios: binary credit risk classification and
financial time-series forecasting. H-XAI fills critical gaps left by existing
XAI methods by combining causal ratings and post-hoc explanations to answer
stakeholder-specific questions at both the individual decision level and the
overall model level.

</details>


### [156] [Safety of Embodied Navigation: A Survey](https://arxiv.org/abs/2508.05855)
*Zixia Wang,Jia Hu,Ronghui Mu*

Main category: cs.AI

TL;DR: 这篇综述全面分析了具身导航的安全性，涵盖攻击、防御和评估方法，探讨了未解决问题和未来方向，旨在开发更安全可靠的系统。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的进步和具身AI的发展，具身导航在关键应用中的集成引发了重大安全问题，需确保其在动态现实环境中的安全性。

Method: 通过全面分析具身导航的安全性，涵盖攻击策略、防御机制和评估方法，并对现有安全挑战、缓解技术、数据集和指标进行综合考察。

Result: 综述提供了对具身导航安全性的多角度分析，探讨了未解决的问题和未来研究方向，包括潜在攻击方法、缓解策略、更可靠的评估技术和验证框架的实施。

Conclusion: 这篇综述旨在为未来研究提供有价值的见解，以开发更安全、更可靠的具身导航系统，并对增强社会安全和提高工业效率具有广泛影响。

Abstract: As large language models (LLMs) continue to advance and gain influence, the
development of embodied AI has accelerated, drawing significant attention,
particularly in navigation scenarios. Embodied navigation requires an agent to
perceive, interact with, and adapt to its environment while moving toward a
specified target in unfamiliar settings. However, the integration of embodied
navigation into critical applications raises substantial safety concerns. Given
their deployment in dynamic, real-world environments, ensuring the safety of
such systems is critical. This survey provides a comprehensive analysis of
safety in embodied navigation from multiple perspectives, encompassing attack
strategies, defense mechanisms, and evaluation methodologies. Beyond conducting
a comprehensive examination of existing safety challenges, mitigation
technologies, and various datasets and metrics that assess effectiveness and
robustness, we explore unresolved issues and future research directions in
embodied navigation safety. These include potential attack methods, mitigation
strategies, more reliable evaluation techniques, and the implementation of
verification frameworks. By addressing these critical gaps, this survey aims to
provide valuable insights that can guide future research toward the development
of safer and more reliable embodied navigation systems. Furthermore, the
findings of this study have broader implications for enhancing societal safety
and increasing industrial efficiency.

</details>


### [157] [Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning](https://arxiv.org/abs/2508.05888)
*Sahil Bansal,Sai Shruthi Sistla,Aarti Arikatala,Sebastian Schreiber*

Main category: cs.AI

TL;DR: 提出基于知识图谱的工具检索框架，通过建模工具间关系提升多步骤任务中的检索准确性，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在处理多步骤用户请求时因仅依赖查询与工具描述的相似性而导致的检索准确性限制问题。

Method: 提出了一种基于知识图谱（KG）的工具检索框架，利用1-hop ego工具图的集成来建模工具之间的直接和间接联系。

Result: 在微平均Complete Recall指标上，基于工具图的方法达到91.85%的工具覆盖率，优于非KG基线（89.26%）。

Conclusion: 知识图谱（KG）在工具检索中提供了补充性信号，特别是在需要顺序工具组合的查询中，验证了其优于传统相似性匹配方法的假设。

Abstract: Effective tool retrieval is essential for AI agents to select from a vast
array of tools when identifying and planning actions in the context of complex
user queries. Despite its central role in planning, this aspect remains
underexplored in the literature. Traditional approaches rely primarily on
similarities between user queries and tool descriptions, which significantly
limits retrieval accuracy, specifically when handling multi-step user requests.
To address these limitations, we propose a Knowledge Graph (KG)-based tool
retrieval framework that captures the semantic relationships between tools and
their functional dependencies. Our retrieval algorithm leverages ensembles of
1-hop ego tool graphs to model direct and indirect connections between tools,
enabling more comprehensive and contextual tool selection for multi-step tasks.
We evaluate our approach on a synthetically generated internal dataset across
six defined user classes, extending previous work on coherent dialogue
synthesis and too retrieval benchmarks. Results demonstrate that our tool
graph-based method achieves 91.85% tool coverage on the micro-average Complete
Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid
retrieval, the strongest non-KG baseline in our experiments. These findings
support our hypothesis that the structural information in the KG provides
complementary signals to pure similarity matching, particularly for queries
requiring sequential tool composition.

</details>


### [158] [Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making](https://arxiv.org/abs/2508.05996)
*Kaitao Chen,Mianxin Liu,Daoming Zong,Chaoyue Ding,Shaohao Rui,Yankai Jiang,Mu Zhou,Xiaosong Wang*

Main category: cs.AI

TL;DR: 提出MedOrch框架，通过LLM中介协调多VLM智能体协作，提升医疗多模态决策性能，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有多智能体研究在医疗多模态场景中因VLM指令跟随和自省能力不足导致的协作挑战。

Method: 提出MedOrch框架，利用LLM中介协调多个VLM专家智能体，实现多模态协作。

Result: 在五个医疗视觉问答基准测试中验证了协作性能优于单个智能体。

Conclusion: MedOrch框架通过LLM中介引导的多智能体协作，显著提升了医疗多模态决策的性能，且无需额外模型训练。

Abstract: Complex medical decision-making involves cooperative workflows operated by
different clinicians. Designing AI multi-agent systems can expedite and augment
human-level clinical decision-making. Existing multi-agent researches primarily
focus on language-only tasks, yet their extension to multimodal scenarios
remains challenging. A blind combination of diverse vision-language models
(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are
less capable in instruction following and importantly self-reflection, compared
to large language models (LLMs) of comparable sizes. This disparity largely
constrains VLMs' ability in cooperative workflows. In this study, we propose
MedOrch, a mediator-guided multi-agent collaboration framework for medical
multimodal decision-making. MedOrch employs an LLM-based mediator agent that
enables multiple VLM-based expert agents to exchange and reflect on their
outputs towards collaboration. We utilize multiple open-source general-purpose
and domain-specific VLMs instead of costly GPT-series models, revealing the
strength of heterogeneous models. We show that the collaboration within
distinct VLM-based agents can surpass the capabilities of any individual agent.
We validate our approach on five medical vision question answering benchmarks,
demonstrating superior collaboration performance without model training. Our
findings underscore the value of mediator-guided multi-agent collaboration in
advancing medical multimodal intelligence. Our code will be made publicly
available.

</details>


### [159] [Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning](https://arxiv.org/abs/2508.06042)
*Daechul Ahn,San Kim,Jonghyun Choi*

Main category: cs.AI

TL;DR: HIMA是一种分层多代理框架，通过模仿学习和元级编排提升LLM在动态任务（如StarCraftII）中的表现，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法在处理动态、长期任务（如实时战略游戏StarCraftII）时表现不佳，需要一种更有效的解决方案。

Method: 提出了一种分层多代理框架HIMA，使用专门的模仿学习代理在战略规划器（SP）下工作，通过专家示范学习特定策略，并由SP协调这些提议形成适应性计划。

Result: HIMA在战略清晰性、适应性和计算效率方面优于现有技术，验证了其有效性。

Conclusion: HIMA框架通过结合专业模仿学习模块和元级编排，展示了在动态、长期任务中开发更强大、通用AI代理的潜力。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive action
sequence prediction capabilities but often struggle with dynamic, long-horizon
tasks such as real-time strategic games. In a game such as StarCraftII (SC2),
agents need to manage resource constraints and adapt to evolving battlefield
situations in a partially observable environment. This often overwhelms
exisiting LLM-based approaches. To address these challenges, we propose a
hierarchical multi-agent framework that employs specialized imitation learning
agents under a meta-controller called Strategic Planner (SP). By expert
demonstrations, each specialized agent learns a distinctive strategy, such as
aerial support or defensive maneuvers, and produces coherent, structured
multistep action sequences. The SP then orchestrates these proposals into a
single, environmentally adaptive plan that ensures local decisions aligning
with long-term strategies. We call this HIMA (Hierarchical Imitation
Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that
encompasses all race match combinations in SC2. Our empirical results show that
HIMA outperforms state of the arts in strategic clarity, adaptability, and
computational efficiency, underscoring the potential of combining specialized
imitation modules with meta-level orchestration to develop more robust,
general-purpose AI agents.

</details>


### [160] [LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences](https://arxiv.org/abs/2508.06060)
*Sankarshan Damle,Boi Faltings*

Main category: cs.AI

TL;DR: 本文提出一个框架，利用参与式预算评估LLMs的资源分配和推理能力，发现提示设计是关键，LLMs在非结构化输入处理中具有潜力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在结构化资源分配任务中的能力，并解决现有基准因数据污染和静态性导致的评估难题。

Method: 提出了一个双重用途框架，利用参与式预算（PB）作为LLM资源分配的实践场景和评估其推理能力的自适应基准。通过三种提示策略（贪心选择、直接优化和启发式爬山优化）测试LLMs在可行性约束下选择项目子集的能力。

Result: LLMs的分配结果与效用最大化基准进行了对比，并测试了其从自然语言输入或元数据中推断结构化偏好的能力。结果表明，提示设计对LLMs的表现至关重要，且其在非结构化输入中提取偏好的能力具有实际应用价值。

Conclusion: 大语言模型（LLMs）在机制设计和处理非结构化输入方面展现出潜力，提示设计对其资源分配能力有显著影响。

Abstract: Large Language Models (LLMs) are increasingly expected to handle complex
decision-making tasks, yet their ability to perform structured resource
allocation remains underexplored. Evaluating their reasoning is also difficult
due to data contamination and the static nature of existing benchmarks. We
present a dual-purpose framework leveraging Participatory Budgeting (PB) both
as (i) a practical setting for LLM-based resource allocation and (ii) an
adaptive benchmark for evaluating their reasoning capabilities. We task LLMs
with selecting project subsets under feasibility (e.g., budget) constraints via
three prompting strategies: greedy selection, direct optimization, and a
hill-climbing-inspired refinement. We benchmark LLMs' allocations against a
utility-maximizing oracle. Interestingly, we also test whether LLMs can infer
structured preferences from natural-language voter input or metadata, without
explicit votes. By comparing allocations based on inferred preferences to those
from ground-truth votes, we evaluate LLMs' ability to extract preferences from
open-ended input. Our results underscore the role of prompt design and show
that LLMs hold promise for mechanism design with unstructured inputs.

</details>


### [161] [Don't Forget Imagination!](https://arxiv.org/abs/2508.06062)
*Evgenii E. Vityaev,Andrei Mantsivoda*

Main category: cs.AI

TL;DR: 论文呼吁重视认知想象力在AI中的作用，并提出语义模型作为模拟工具，强调其在推理和决策中的重要性。


<details>
  <summary>Details</summary>
Motivation: 认知想象力在人类思维中扮演关键角色，但目前AI领域对其重视不足，限制了AI的能力。

Method: 提出语义模型，这是一种基于概率因果关系的数学模型，能够学习和确保想象上下文的一致性。

Result: 语义模型能够模拟认知想象力，通过玻璃盒方法实现上下文的整体性和一致性。

Conclusion: 认知想象力作为人工智能下一个突破点的重要性被低估，语义模型作为一种新方法，能够模拟认知想象力，有望推动AI发展。

Abstract: Cognitive imagination is a type of imagination that plays a key role in human
thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to
mentally visualize coherent and holistic systems of concepts and causal links
that serve as semantic contexts for reasoning, decision making and prediction.
Our position is that the role of cognitive imagination is still greatly
underestimated, and this creates numerous problems and diminishes the current
capabilities of AI. For instance, when reasoning, humans rely on imaginary
contexts to retrieve background info. They also constantly return to the
context for semantic verification that their reasoning is still reasonable.
Thus, reasoning without imagination is blind. This paper is a call for greater
attention to cognitive imagination as the next promising breakthrough in
artificial intelligence. As an instrument for simulating cognitive imagination,
we propose semantic models -- a new approach to mathematical models that can
learn, like neural networks, and are based on probabilistic causal
relationships. Semantic models can simulate cognitive imagination because they
ensure the consistency of imaginary contexts and implement a glass-box approach
that allows the context to be manipulated as a holistic and coherent system of
interrelated facts glued together with causal relations.

</details>


### [162] [A Generic Complete Anytime Beam Search for Optimal Decision Tree](https://arxiv.org/abs/2508.06064)
*Harold Silvère Kiossou,Siegfried Nijssen,Pierre Schaus*

Main category: cs.AI

TL;DR: CA-DL8.5是一种通用的实时决策树学习框架，通过模块化设计整合多种启发式，LDS表现最佳，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有实时扩展方法（如LDS-DL8.5、Top-k-DL8.5和Blossom）缺乏系统比较，难以评估其相对有效性。

Method: 提出了一种通用的、完整的、实时的束搜索算法CA-DL8.5，扩展了DL8.5框架，并通过模块化设计整合多种启发式和松弛机制。

Result: 实验表明，基于LDS启发式的CA-DL8.5在标准分类基准上表现最佳，优于其他变体和Blossom算法。

Conclusion: CA-DL8.5算法通过模块化设计整合多种启发式和松弛机制，显著提升了决策树学习的实时性能，尤其是在LDS启发式下表现最佳，同时保持了完整性和最优性保证。

Abstract: Finding an optimal decision tree that minimizes classification error is known
to be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic
programming guarantee optimality, they often suffer from poor anytime behavior
-- meaning they struggle to find high-quality decision trees quickly when the
search is stopped before completion -- due to unbalanced search space
exploration. To address this, several anytime extensions of exact methods have
been proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not
been systematically compared, making it difficult to assess their relative
effectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and
anytime beam search algorithm that extends the DL8.5 framework and unifies some
existing anytime strategies. In particular, CA-DL8.5 generalizes previous
approaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various
heuristics and relaxation mechanisms through a modular design. The algorithm
reuses DL8.5's efficient branch-and-bound pruning and trie-based caching,
combined with a restart-based beam search that gradually relaxes pruning
criteria to improve solution quality over time. Our contributions are twofold:
(1) We introduce this new generic framework for exact and anytime decision tree
learning, enabling the incorporation of diverse heuristics and search
strategies; (2) We conduct a rigorous empirical comparison of several
instantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k
heuristics -- using an anytime evaluation metric called the primal gap
integral. Experimental results on standard classification benchmarks show that
CA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime
performance, outperforming both other CA-DL8.5 variants and the Blossom
algorithm while maintaining completeness and optimality guarantees.

</details>


### [163] [ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception](https://arxiv.org/abs/2508.06074)
*Siyi Lu,Run Liu,Dongsheng Yang,Lei He*

Main category: cs.AI

TL;DR: 本文提出结合BEV感知和DRL的	exttt{ME$^3$-BEV}框架，显著提升自动驾驶性能，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在复杂环境感知和实时决策方面面临挑战，传统模块化方法存在错误传播和协调问题，而端到端学习系统则存在计算瓶颈。本文旨在通过结合BEV感知和DRL，解决这些问题。

Method: 本文提出了一种新颖的自动驾驶方法，结合了深度强化学习（DRL）和鸟瞰图（BEV）感知。具体包括	exttt{Mamba-BEV}模型（一种高效的时空特征提取网络）和基于此的	exttt{ME$^3$-BEV}框架，用于端到端DRL。

Result: 在CARLA模拟器上的广泛实验表明，	exttt{ME$^3$-BEV}在碰撞率和轨迹准确性等多个指标上优于现有模型。

Conclusion: 本文提出的	exttt{ME$^3$-BEV}框架通过结合BEV感知和DRL，显著提升了自动驾驶系统在动态城市驾驶场景中的性能，并在CARLA模拟器上验证了其优越性。

Abstract: Autonomous driving systems face significant challenges in perceiving complex
environments and making real-time decisions. Traditional modular approaches,
while offering interpretability, suffer from error propagation and coordination
issues, whereas end-to-end learning systems can simplify the design but face
computational bottlenecks. This paper presents a novel approach to autonomous
driving using deep reinforcement learning (DRL) that integrates bird's-eye view
(BEV) perception for enhanced real-time decision-making. We introduce the
\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction
network that combines BEV-based perception with the Mamba framework for
temporal feature modeling. This integration allows the system to encode vehicle
surroundings and road features in a unified coordinate system and accurately
model long-range dependencies. Building on this, we propose the
\texttt{ME$^3$-BEV} framework, which utilizes the \texttt{Mamba-BEV} model as a
feature input for end-to-end DRL, achieving superior performance in dynamic
urban driving scenarios. We further enhance the interpretability of the model
by visualizing high-dimensional features through semantic segmentation,
providing insight into the learned representations. Extensive experiments on
the CARLA simulator demonstrate that \texttt{ME$^3$-BEV} outperforms existing
models across multiple metrics, including collision rate and trajectory
accuracy, offering a promising solution for real-time autonomous driving.

</details>


### [164] [Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2](https://arxiv.org/abs/2508.06091)
*Stan P Hauke,Przemysław Andrzej Wałęga*

Main category: cs.AI

TL;DR: 本文解决了GNNs与C2逻辑表达能力关系的开放性问题，证明前者严格超越后者，适用于多种图类型。


<details>
  <summary>Details</summary>
Motivation: 研究图神经网络（GNNs）的表达能力与逻辑语言之间的关系，解决Barceló等人提出的关于aggregate-combine-readout GNNs是否完全由C2逻辑特征化的开放性问题。

Method: 通过理论证明，展示了aggregate-combine-readout GNNs在逻辑表达能力上超越C2的能力。

Result: 证明了aggregate-combine-readout GNNs的逻辑表达能力严格超过C2，适用于无向和有向图。

Conclusion: 本文解决了Barceló等人提出的开放性问题，证明了aggregate-combine-readout GNNs的逻辑表达能力严格超过C2，这一结果对图神经网络和无限逻辑的表达能力研究具有重要意义。

Abstract: In recent years, there has been growing interest in understanding the
expressive power of graph neural networks (GNNs) by relating them to logical
languages. This research has been been initialised by an influential result of
Barcel\'o et al. (2020), who showed that the graded modal logic (or a guarded
fragment of the logic C2), characterises the logical expressiveness of
aggregate-combine GNNs. As a ``challenging open problem'' they left the
question whether full C2 characterises the logical expressiveness of
aggregate-combine-readout GNNs. This question has remained unresolved despite
several attempts. In this paper, we solve the above open problem by proving
that the logical expressiveness of aggregate-combine-readout GNNs strictly
exceeds that of C2. This result holds over both undirected and directed graphs.
Beyond its implications for GNNs, our work also leads to purely logical
insights on the expressive power of infinitary logics.

</details>


### [165] [PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion](https://arxiv.org/abs/2508.06110)
*Yiran Rex Ma*

Main category: cs.AI

TL;DR: PanelTR通过LLM代理科学家的结构化科学方法，在零样本下表推理任务中表现优异，超越普通LLMs并媲美监督模型。


<details>
  <summary>Details</summary>
Motivation: 表推理任务通常依赖标注数据或复杂的数据增强，限制了灵活性和泛化能力；LLMs虽多功能但性能不及简单监督模型。

Method: PanelTR框架利用LLM代理科学家，通过个体调查、自我审查和协作同行评审讨论的结构化科学方法进行表推理。

Result: 在四个基准测试中，PanelTR优于普通LLMs，并与完全监督模型相媲美，且无需依赖训练数据。

Conclusion: PanelTR展示了结构化科学方法在零样本环境下处理复杂任务（如表推理）的有效性，具备灵活的语义理解能力。

Abstract: Table reasoning, including tabular QA and fact verification, often depends on
annotated data or complex data augmentation, limiting flexibility and
generalization. LLMs, despite their versatility, often underperform compared to
simple supervised models. To approach these issues, we introduce PanelTR, a
framework utilizing LLM agent scientists for robust table reasoning through a
structured scientific approach. PanelTR's workflow involves agent scientists
conducting individual investigations, engaging in self-review, and
participating in collaborative peer-review discussions. This process, driven by
five scientist personas, enables semantic-level transfer without relying on
data augmentation or parametric optimization. Experiments across four
benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully
supervised models, all while remaining independent of training data. Our
findings indicate that structured scientific methodology can effectively handle
complex tasks beyond table reasoning with flexible semantic understanding in a
zero-shot context.

</details>


### [166] [SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges](https://arxiv.org/abs/2508.06111)
*Dewi S. W. Gould,Bruno Mlodozeniec,Samuel F. Brown*

Main category: cs.AI

TL;DR: SKATE是一种自动化、可扩展的LLM评估框架，通过模型相互生成和解决任务来客观评估能力，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法需要大量领域专业知识，难以随着模型快速进化而扩展。SKATE旨在提供一种自动化、无需人工干预的评估方法。

Method: 引入SKATE评估框架，让大型语言模型（LLMs）通过生成和解决可验证任务相互竞争。采用基于TrueSkill的排名系统评估六个前沿LLMs。

Result: 研究发现：（1）较弱模型能可靠区分和评分更强模型，（2）基于LLM的系统表现出自我偏好行为，（3）SKATE能自动揭示模型间的细粒度能力差异。

Conclusion: SKATE框架是朝着通用、可扩展评估框架迈出的重要一步，能够跟上大型语言模型的快速发展。

Abstract: Evaluating the capabilities and risks of foundation models is paramount, yet
current methods demand extensive domain expertise, hindering their scalability
as these models rapidly evolve. We introduce SKATE: a novel evaluation
framework in which large language models (LLMs) compete by generating and
solving verifiable tasks for one another. Our core insight is to treat
evaluation as a game: models act as both task-setters and solvers, incentivized
to create questions which highlight their own strengths while exposing others'
weaknesses. SKATE offers several key advantages, balancing scalability,
open-endedness, and objectivity. It is fully automated, data-free, and
scalable, requiring no human input or domain expertise. By using verifiable
tasks rather than LLM judges, scoring is objective. Unlike domain-limited
programmatically-generated benchmarks (e.g. chess-playing or spatial
reasoning), having LLMs creatively pose challenges enables open-ended and
scalable evaluation. As a proof of concept, we introduce LLM-set
code-output-prediction (COP) challenges as a verifiable and extensible
framework in which to test our approach. Using a TrueSkill-based ranking
system, we evaluate six frontier LLMs and find that: (1) weaker models can
reliably differentiate and score stronger ones, (2) LLM-based systems are
capable of self-preferencing behavior, generating questions that align with
their own capabilities, and (3) SKATE automatically surfaces fine-grained
capability differences between models. Our findings are an important step
towards general, scalable evaluation frameworks which can keep pace with LLM
progress.

</details>


### [167] [Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem](https://arxiv.org/abs/2508.06129)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux*

Main category: cs.AI

TL;DR: 该研究通过机器学习方法分析VRP解决方案的结构特征，利用可解释AI技术揭示特征重要性，为元启发式算法设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 传统元启发式算法依赖人工设计，而机器学习方法能够利用组合优化中解决方案的结构特征，从而帮助设计更高效的算法。

Method: 通过使用多种分类器模型进行敏感性分析，并利用可解释AI技术理解模型决策过程。

Result: 研究发现特征重要性虽各有不同，但某些特征始终是强预测因子，并提出了一个统一框架来排名不同情景下的特征影响。

Conclusion: 该研究强调了特征重要性分析作为开发车辆路径问题（VRP）元启发式算法指导机制的潜力。

Abstract: The Vehicle Routing Problem (VRP) is a complex optimization problem with
numerous real-world applications, mostly solved using metaheuristic algorithms
due to its $\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely
on human-crafted designs developed through empirical studies. However, recent
research shows that machine learning methods can be used the structural
characteristics of solutions in combinatorial optimization, thereby aiding in
designing more efficient algorithms, particularly for solving VRP. Building on
this advancement, this study extends the previous research by conducting a
sensitivity analysis using multiple classifier models that are capable of
predicting the quality of VRP solutions. Hence, by leveraging explainable AI,
this research is able to extend the understanding of how these models make
decisions. Finally, our findings indicate that while feature importance varies,
certain features consistently emerge as strong predictors. Furthermore, we
propose a unified framework able of ranking feature impact across different
scenarios to illustrate this finding. These insights highlight the potential of
feature importance analysis as a foundation for developing a guidance mechanism
of metaheuristic algorithms for solving the VRP.

</details>


### [168] [Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications](https://arxiv.org/abs/2508.06145)
*Byeonghun Bang,Jongsuk Yoon,Dong-Jin Chang,Seho Park,Yong Oh Lee*

Main category: cs.AI

TL;DR: 本研究通过RAG框架增强LLMs处理药物禁忌的能力，显著提升了模型准确性，为医疗决策提供了更可靠的信息。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在医疗保健领域的应用面临挑战，尤其是在需要准确可靠信息的药物禁忌领域。本研究旨在提升LLMs处理禁忌问题的能力。

Method: 本研究采用Retrieval Augmented Generation（RAG）流程，结合OpenAI的GPT-4o-mini作为基础模型，使用text-embedding-3-small模型生成嵌入，并通过Langchain协调混合检索系统与重新排序。数据来源为公共数据库中的药物使用审查（DUR）数据。

Result: 在整合RAG流程后，模型准确性显著提升，年龄组、妊娠和伴随药物使用的禁忌相关准确率分别达到0.94、0.87和0.89。

Conclusion: 通过RAG框架增强大型语言模型（LLMs）可以显著减少处方和药物摄入决策中的不确定性，提供更精确可靠的药物禁忌信息。

Abstract: The versatility of large language models (LLMs) has been explored across
various sectors, but their application in healthcare poses challenges,
particularly in the domain of pharmaceutical contraindications where accurate
and reliable information is required. This study enhances the capability of
LLMs to address contraindications effectively by implementing a Retrieval
Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base
model, and the text-embedding-3-small model for embeddings, our approach
integrates Langchain to orchestrate a hybrid retrieval system with re-ranking.
This system leverages Drug Utilization Review (DUR) data from public databases,
focusing on contraindications for specific age groups, pregnancy, and
concomitant drug use. The dataset includes 300 question-answer pairs across
three categories, with baseline model accuracy ranging from 0.49 to 0.57.
Post-integration of the RAG pipeline, we observed a significant improvement in
model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications
related to age groups, pregnancy, and concomitant drug use, respectively. The
results indicate that augmenting LLMs with a RAG framework can substantially
reduce uncertainty in prescription and drug intake decisions by providing more
precise and reliable drug contraindication information.

</details>


### [169] [Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution](https://arxiv.org/abs/2508.06225)
*Zailong Tian,Zhuoheng Han,Yanzhe Chen,Haozhe Xu,Xi Yang,richeng xuan,Hongfeng Wang,Lizi Liao*

Main category: cs.AI

TL;DR: 论文提出以置信度驱动、风险感知的LLM评估系统，通过TH-Score和LLM-as-a-Fuser框架解决现有方法的过自信问题，提升评估的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge方法主要关注准确性，忽视了校准置信度的重要性，导致实际部署中的可靠性问题。论文旨在解决这一问题，推动更可信赖的自适应评估。

Method: 论文提出了TH-Score作为衡量置信度-准确性对齐的新指标，并设计了LLM-as-a-Fuser这一集成框架，将LLM转变为可靠、风险感知的评估器。

Result: 实验表明，提出的方法显著改善了校准效果，实现了优于现有基线的可靠性和准确性。

Conclusion: 该论文主张从以准确性为中心的评估转向以置信度驱动、风险感知的LLM-as-a-Judge系统，强调良好校准的置信度对于可信赖和自适应评估的必要性。通过引入TH-Score和LLM-as-a-Fuser框架，显著提高了校准效果和可靠性。

Abstract: Large Language Models (LLMs) are widely used as automated judges, where
practical value depends on both accuracy and trustworthy, risk-aware judgments.
Existing approaches predominantly focus on accuracy, overlooking the necessity
of well-calibrated confidence, which is vital for adaptive and reliable
evaluation pipelines. In this work, we advocate a shift from accuracy-centric
evaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing
the necessity of well-calibrated confidence for trustworthy and adaptive
evaluation. We systematically identify the **Overconfidence Phenomenon** in
current LLM-as-a-Judges, where predicted confidence significantly overstates
actual correctness, undermining reliability in practical deployment. To
quantify this phenomenon, we introduce **TH-Score**, a novel metric measuring
confidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an
ensemble framework that transforms LLMs into reliable, risk-aware evaluators.
Extensive experiments demonstrate that our approach substantially improves
calibration and enables adaptive, confidence-driven evaluation pipelines,
achieving superior reliability and accuracy compared to existing baselines.

</details>


### [170] [GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines](https://arxiv.org/abs/2508.06226)
*Yumeng Fu,Jiayin Zhu,Lingling Zhang,Bo Zhao,Shaoxuan Ma,Yushun Zhang,Yanrui Wu,Wenjun Wu*

Main category: cs.AI

TL;DR: GeoLaux 基准填补了现有几何问题评估的空白，通过五维评估策略揭示了 MLLMs 在长步骤推理和辅助线构建中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估 MLLMs 几何技能的基准忽视了辅助线构建和细粒度过程评估，无法充分评估 MLLMs 的长步骤推理能力。

Method: 提出了 GeoLaux 基准，包含 2,186 个几何问题，设计了五维评估策略（答案正确性、过程正确性、过程质量、辅助线影响和错误原因）。

Result: 实验发现：1）模型在长步骤推理中性能显著下降；2）MLLMs 在证明题中倾向于走捷径；3）模型缺乏辅助线意识，增强该能力对几何推理提升尤为有益。

Conclusion: GeoLaux 基准不仅用于评估 MLLMs 的长步骤几何推理能力，还为其能力提升提供了指导。数据集和代码将公开发布。

Abstract: Geometry problem solving (GPS) requires models to master diagram
comprehension, logical reasoning, knowledge application, numerical computation,
and auxiliary line construction. This presents a significant challenge for
Multimodal Large Language Models (MLLMs). However, existing benchmarks for
evaluating MLLM geometry skills overlook auxiliary line construction and lack
fine-grained process evaluation, making them insufficient for assessing MLLMs'
long-step reasoning abilities. To bridge these gaps, we present the GeoLaux
benchmark, comprising 2,186 geometry problems, incorporating both calculation
and proving questions. Notably, the problems require an average of 6.51
reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary
line construction. Building on the dataset, we design a novel five-dimensional
evaluation strategy assessing answer correctness, process correctness, process
quality, auxiliary line impact, and error causes. Extensive experiments on 13
leading MLLMs (including thinking models and non-thinking models) yield three
pivotal findings: First, models exhibit substantial performance degradation in
extended reasoning steps (nine models demonstrate over 50% performance drop).
Second, compared to calculation problems, MLLMs tend to take shortcuts when
solving proving problems. Third, models lack auxiliary line awareness, and
enhancing this capability proves particularly beneficial for overall geometry
reasoning improvement. These findings establish GeoLaux as both a benchmark for
evaluating MLLMs' long-step geometric reasoning with auxiliary lines and a
guide for capability advancement. Our dataset and code are included in
supplementary materials and will be released.

</details>


### [171] [Learning Logical Rules using Minimum Message Length](https://arxiv.org/abs/2508.06230)
*Ruben Sharma,Sebastijan Dumančić,Ross D. King,Andrew Cropper*

Main category: cs.AI

TL;DR: 该研究提出了一种贝叶斯归纳逻辑编程方法，通过平衡假设复杂度和数据拟合度，显著优于现有方法，并在多个领域验证了其高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 统一概率和逻辑学习是AI领域的关键挑战。

Method: 通过先验概率和似然函数平衡假设复杂度和数据拟合度，优先选择更通用的程序。

Result: 实验结果表明，该方法数据效率高，对示例平衡不敏感，甚至能从纯正例中学习。

Conclusion: 该论文提出的贝叶斯归纳逻辑编程方法在多个领域（如游戏和药物设计）中显著优于先前的方法，特别是在学习最小描述长度程序方面。

Abstract: Unifying probabilistic and logical learning is a key challenge in AI. We
introduce a Bayesian inductive logic programming approach that learns minimum
message length programs from noisy data. Our approach balances hypothesis
complexity and data fit through priors, which explicitly favour more general
programs, and a likelihood that favours accurate programs. Our experiments on
several domains, including game playing and drug design, show that our method
significantly outperforms previous methods, notably those that learn minimum
description length programs. Our results also show that our approach is
data-efficient and insensitive to example balance, including the ability to
learn from exclusively positive examples.

</details>


### [172] [Symmetry breaking for inductive logic programming](https://arxiv.org/abs/2508.06263)
*Andrew Cropper,David M. Cerna,Matti Järvisalo*

Main category: cs.AI

TL;DR: 提出一种打破假设空间对称性的方法，显著提升归纳逻辑编程效率，实验显示求解时间从一小时缩短至17秒。


<details>
  <summary>Details</summary>
Motivation: 归纳逻辑编程中假设空间巨大且存在大量逻辑等效假设，导致搜索效率低下。

Method: 提出了一种在假设空间中打破对称性的方法，并在答案集编程中实现了这一想法。

Result: 在多个领域（包括视觉推理和游戏玩法）的实验中，该方法将求解时间从超过一小时大幅减少到仅17秒。

Conclusion: 通过打破假设空间中的对称性，该方法显著提高了归纳逻辑编程的效率，将求解时间从超过一小时缩短至仅17秒。

Abstract: The goal of inductive logic programming is to search for a hypothesis that
generalises training data and background knowledge. The challenge is searching
vast hypothesis spaces, which is exacerbated because many logically equivalent
hypotheses exist. To address this challenge, we introduce a method to break
symmetries in the hypothesis space. We implement our idea in answer set
programming. Our experiments on multiple domains, including visual reasoning
and game playing, show that our approach can reduce solving times from over an
hour to just 17 seconds.

</details>


### [173] [LLM Robustness Leaderboard v1 --Technical report](https://arxiv.org/abs/2508.06296)
*Pierre Peigné - Lefebvre,Quentin Feuillade-Montixi,Tom David,Nicolas Miailhe*

Main category: cs.AI

TL;DR: PRISM Eval BET工具通过动态对抗优化实现了对多数LLM的高效攻击，揭示了模型间的巨大鲁棒性差异，并提出了细粒度评估方法。


<details>
  <summary>Details</summary>
Motivation: 评估当前最先进LLM的鲁棒性，揭示其普遍脆弱性，并开发更精准的评估工具和方法。

Method: 通过动态对抗优化（Dynamic Adversarial Optimization）进行自动化红队测试，提出细粒度鲁棒性指标，并进行原始级漏洞分析。

Result: PRISM Eval BET工具对41个最先进LLM中的37个实现了100%的攻击成功率，攻击难度在不同模型间差异超过300倍。

Conclusion: 该报告展示了PRISM Eval BET工具在自动化红队测试中的高效性，揭示了当前LLM普遍存在的脆弱性，并提出了细粒度的鲁棒性评估方法，为分布式鲁棒性评估提供了实用路径。

Abstract: This technical report accompanies the LLM robustness leaderboard published by
PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior
Elicitation Tool (BET), an AI system performing automated red-teaming through
Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)
against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we
propose a fine-grained robustness metric estimating the average number of
attempts required to elicit harmful behaviors, revealing that attack difficulty
varies by over 300-fold across models despite universal vulnerability. We
introduce primitive-level vulnerability analysis to identify which jailbreaking
techniques are most effective for specific hazard categories. Our collaborative
evaluation with trusted third parties from the AI Safety Network demonstrates
practical pathways for distributed robustness assessment across the community.

</details>


### [174] [A "good regulator theorem" for embodied agents](https://arxiv.org/abs/2508.06326)
*Nathaniel Virgo,Martin Biehl,Manuel Baltieri,Matteo Capucci*

Main category: cs.AI

TL;DR: 论文提出了一种新的模型概念，通过观察者视角解释代理的‘信念’更新，扩展了Conant和Ashby定理的适用范围。


<details>
  <summary>Details</summary>
Motivation: 探讨Conant和Ashby定理在人工生命系统中的局限性，并提出一种更普遍的模型概念。

Method: 通过理论分析和定理证明，展示了代理在执行调节任务时，观察者可以将其解释为具有环境‘信念’并更新。

Result: 证明了无论系统是在经典控制理论设置中调节环境，还是调节自身内部状态，观察者均可赋予其模型。

Conclusion: 该论文提出了一个比Conant和Ashby定理更广泛适用的模型概念，即通过观察者视角解释代理的‘信念’更新，从而重新定义了模型的存在方式。

Abstract: In a classic paper, Conant and Ashby claimed that "every good regulator of a
system must be a model of that system." Artificial Life has produced many
examples of systems that perform tasks with apparently no model in sight; these
suggest Conant and Ashby's theorem doesn't easily generalise beyond its
restricted setup. Nevertheless, here we show that a similar intuition can be
fleshed out in a different way: whenever an agent is able to perform a
regulation task, it is possible for an observer to interpret it as having
"beliefs" about its environment, which it "updates" in response to sensory
input. This notion of belief updating provides a notion of model that is more
sophisticated than Conant and Ashby's, as well as a theorem that is more
broadly applicable. However, it necessitates a change in perspective, in that
the observer plays an essential role in the theory: models are not a mere
property of the system but are imposed on it from outside. Our theorem holds
regardless of whether the system is regulating its environment in a classic
control theory setup, or whether it's regulating its own internal state; the
model is of its environment either way. The model might be trivial, however,
and this is how the apparent counterexamples are resolved.

</details>


### [175] [AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games](https://arxiv.org/abs/2508.06348)
*Mille Mei Zhen Loo,Gert Luzkov,Paolo Burelli*

Main category: cs.AI

TL;DR: 提出AntiCheatPT_256模型，基于Transformer检测CS2作弊行为，使用公开数据集CS2CD，准确率89.17%，AUC 93.36%。


<details>
  <summary>Details</summary>
Motivation: 在线游戏中的作弊行为破坏了游戏体验，现有反作弊系统（如VAC）难以在不侵入用户系统的情况下应对不断演变的作弊方法。

Method: 采用基于Transformer的机器学习模型AntiCheatPT_256，使用CS2CD数据集（795场比赛）生成90,707个上下文窗口并进行数据增强以解决类别不平衡问题。

Result: 模型在未增强测试集上达到89.17%的准确率和93.36%的AUC。

Conclusion: AntiCheatPT_256模型在Counter-Strike 2中表现出色，准确率为89.17%，AUC为93.36%，为数据驱动的作弊检测研究提供了可靠基准。

Abstract: Cheating in online video games compromises the integrity of gaming
experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face
significant challenges in keeping pace with evolving cheating methods without
imposing invasive measures on users' systems. This paper presents
AntiCheatPT\_256, a transformer-based machine learning model designed to detect
cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we
introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using
this dataset, 90,707 context windows were created and subsequently augmented to
address class imbalance. The transformer model, trained on these windows,
achieved an accuracy of 89.17\% and an AUC of 93.36\% on an unaugmented test
set. This approach emphasizes reproducibility and real-world applicability,
offering a robust baseline for future research in data-driven cheat detection.

</details>


### [176] [From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI](https://arxiv.org/abs/2508.06352)
*Christian Meske,Justin Brenne,Erdi Uenal,Sabahat Oelcer,Ayseguel Doganguen*

Main category: cs.AI

TL;DR: 论文提出‘解释性AI’作为XAI的补充范式，通过生成式AI能力支持人类理解。实证验证显示用户偏好上下文敏感的多模态解释，为跨领域用户中心AI解释方法奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI（XAI）方法过于关注算法透明度，提供的解释抽象且缺乏适应性，难以支持终端用户的理解。因此，论文提出‘解释性AI’作为补充范式，旨在通过生成式AI能力提升人类决策支持。

Method: 论文采用快速情境设计方法，与医疗保健专业人员合作，通过实证验证了用户对上下文敏感的多模态解释的偏好。

Result: 实证验证表明，用户一致偏好上下文敏感的多模态解释，而非技术透明度。

Conclusion: 论文提出了‘解释性AI’作为补充范式，强调通过生成式AI能力作为人类理解力的解释伙伴，而非仅提供算法透明度。研究发现用户更偏好上下文敏感的多模态解释，而非技术透明度，这为跨领域和文化背景的用户中心AI解释方法建立了全面研究议程。

Abstract: Current explainable AI (XAI) approaches prioritize algorithmic transparency
and present explanations in abstract, non-adaptive formats that often fail to
support meaningful end-user understanding. This paper introduces "Explanatory
AI" as a complementary paradigm that leverages generative AI capabilities to
serve as explanatory partners for human understanding rather than providers of
algorithmic transparency. While XAI reveals algorithmic decision processes for
model validation, Explanatory AI addresses contextual reasoning to support
human decision-making in sociotechnical contexts. We develop a definition and
systematic eight-dimensional conceptual model distinguishing Explanatory AI
through narrative communication, adaptive personalization, and progressive
disclosure principles. Empirical validation through Rapid Contextual Design
methodology with healthcare professionals demonstrates that users consistently
prefer context-sensitive, multimodal explanations over technical transparency.
Our findings reveal the practical urgency for AI systems designed for human
comprehension rather than algorithmic introspection, establishing a
comprehensive research agenda for advancing user-centered AI explanation
approaches across diverse domains and cultural contexts.

</details>


### [177] [Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned](https://arxiv.org/abs/2508.06368)
*Claudia dAmato,Giuseppe Rubini,Francesco Didio,Donato Francioso,Fatima Zahra Amara,Nicola Fanizzi*

Main category: cs.AI

TL;DR: 本文开发了针对女性暴力案件的法律知识图谱，结合自底向上方法和大型语言模型，提升法律信息可访问性，支持预测性司法。


<details>
  <summary>Details</summary>
Motivation: 法律决策需要全面且详细的立法背景知识及最新案例信息，而现有法律领域的KGs较少。本文旨在填补这一空白，开发针对女性暴力案件的法律KG。

Method: 论文提出了两种互补的自动化法律KG构建方法：一种针对法律领域的系统化自底向上方法，以及一种利用大型语言模型的新解决方案。

Result: 通过结构化数据提取、本体开发和语义增强，构建了针对女性暴力案件的KGs，并通过能力问题验证了其有效性。

Conclusion: 构建的法律知识图谱（KGs）能显著提升法律信息的可访问性，支持复杂查询，并为预测性司法机器学习工具提供知识基础。

Abstract: Legal decision-making process requires the availability of comprehensive and
detailed legislative background knowledge and up-to-date information on legal
cases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a
valuable tool to facilitate access to legal information, to be queried and
exploited for the purpose, and to enable advanced reasoning and machine
learning applications. Indeed, legal KGs may act as knowledge intensive
component to be used by pre-dictive machine learning solutions supporting the
decision process of the legal expert. Nevertheless, a few KGs can be found in
the legal domain. To fill this gap, we developed a legal KG targeting legal
cases of violence against women, along with clear adopted methodologies.
Specifically, the paper introduces two complementary approaches for automated
legal KG construction; a systematic bottom-up approach, customized for the
legal domain, and a new solution leveraging Large Language Models. Starting
from legal sentences publicly available from the European Court of Justice, the
solutions integrate structured data extraction, ontology development, and
semantic enrichment to produce KGs tailored for legal cases involving violence
against women. After analyzing and comparing the results of the two approaches,
the developed KGs are validated via suitable competency questions. The obtained
KG may be impactful for multiple purposes: can improve the accessibility to
legal information both to humans and machine, can enable complex queries and
may constitute an important knowledge component to be possibly exploited by
machine learning tools tailored for predictive justice.

</details>


### [178] [The Fair Game: Auditing & Debiasing AI Algorithms Over Time](https://arxiv.org/abs/2508.06443)
*Debabrota Basu,Udvas Das*

Main category: cs.AI

TL;DR: 论文提出“Fair Game”动态机制，利用强化学习持续调整机器学习算法的公平性目标，填补现有Fair ML在动态社会环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的公平机器学习（Fair ML）方法通常基于观察性定义，这些定义在现实中往往相互冲突且只能在已知真实情况或事后部署时使用。因此，需要一个动态机制来填补Fair ML在动态社会环境中理想与现实之间的差距。

Method: “Fair Game”机制将审计员和去偏算法置于一个围绕机器学习算法的循环中，利用强化学习（RL）来动态调整算法的公平性目标。RL通过与环境的交互获取反馈，并据此调整未来的决策。

Result: “Fair Game”提供了一个独特的框架，通过仅修改审计员及其量化的不同偏见，可以随时间调整公平性目标，从而模拟社会伦理和法律框架的演变。

Conclusion: 论文提出了一个名为“Fair Game”的动态机制，通过结合审计员和去偏算法，利用强化学习（RL）来持续适应和调整机器学习算法的公平性目标，从而在动态社会环境中实现公平的机器学习系统。

Abstract: An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify
different types of bias (also known as unfairness) exhibited in the predictions
of ML algorithms, and to design new algorithms to mitigate them. Often, the
definitions of bias used in the literature are observational, i.e. they use the
input and output of a pre-trained algorithm to quantify a bias under concern.
In reality,these definitions are often conflicting in nature and can only be
deployed if either the ground truth is known or only in retrospect after
deploying the algorithm. Thus,there is a gap between what we want Fair ML to
achieve and what it does in a dynamic social environment. Hence, we propose an
alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions
of an ML algorithm and to adapt its predictions as the society interacts with
the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing
algorithm in a loop around an ML algorithm. The "Fair Game" puts these two
components in a loop by leveraging Reinforcement Learning (RL). RL algorithms
interact with an environment to take decisions, which yields new observations
(also known as data/feedback) from the environment and in turn, adapts future
decisions. RL is already used in algorithms with pre-fixed long-term fairness
goals. "Fair Game" provides a unique framework where the fairness goals can be
adapted over time by only modifying the auditor and the different biases it
quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and
legal frameworks in the society by creating an auditor which sends feedback to
a debiasing algorithm deployed around an ML system. This allows us to develop a
flexible and adaptive-over-time framework to build Fair ML systems pre- and
post-deployment.

</details>


### [179] [What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting](https://arxiv.org/abs/2508.06454)
*Joshua Caiata,Ben Armstrong,Kate Larson*

Main category: cs.AI

TL;DR: 本文提出数据驱动框架评估投票规则公理违反频率，发现神经网络在最小化公理违反上优于传统规则，支持社会选择领域的数据驱动研究。


<details>
  <summary>Details</summary>
Motivation: 委员会选择问题在许多背景下和应用中普遍存在，社会选择研究社区越来越关注识别不同多赢家投票规则满足哪些属性。

Method: 提出一个数据驱动的框架，评估不同偏好分布下投票规则违反公理的频率，并分析多赢家投票规则与其公理性能之间的关系。

Result: 神经网络作为投票规则，在最小化公理违反方面可以优于传统规则。

Conclusion: 数据驱动的方法可以为设计新的投票系统提供信息，并支持社会选择领域继续开展数据驱动的研究。

Abstract: Committee-selection problems arise in many contexts and applications, and
there has been increasing interest within the social choice research community
on identifying which properties are satisfied by different multi-winner voting
rules. In this work, we propose a data-driven framework to evaluate how
frequently voting rules violate axioms across diverse preference distributions
in practice, shifting away from the binary perspective of axiom satisfaction
given by worst-case analysis. Using this framework, we analyze the relationship
between multi-winner voting rules and their axiomatic performance under several
preference distributions. We then show that neural networks, acting as voting
rules, can outperform traditional rules in minimizing axiom violations. Our
results suggest that data-driven approaches to social choice can inform the
design of new voting systems and support the continuation of data-driven
research in social choice.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [180] [Hierarchical Placement Learning for Network Slice Provisioning](https://arxiv.org/abs/2508.06432)
*Jesutofunmi Ajayi,Antonio Di Maio,Torsten Braun*

Main category: cs.NI

TL;DR: 该论文提出了一种基于分层老虎机的解决方案，用于优化边缘网络切片配置，显著提高了请求接受率并降低了资源使用。


<details>
  <summary>Details</summary>
Motivation: 解决边缘移动网络中切片配置的挑战，以最大化请求接受率并最小化节点资源利用率。

Method: 采用两级分层老虎机方法，学习可扩展的在线放置策略。

Result: 在两种真实网络拓扑上的模拟显示，该方法在特定场景下比基线方法降低了5%的平均节点资源利用率，并多接受了25%以上的切片请求。

Conclusion: 研究提出了一种分层多臂老虎机解决方案，用于优化边缘移动网络中的切片配置，有效提高了请求接受率并降低了节点资源利用率。

Abstract: In this work, we aim to address the challenge of slice provisioning in
edge-based mobile networks. We propose a solution that learns a service
function chain placement policy for Network Slice Requests, to maximize the
request acceptance rate, while minimizing the average node resource
utilization. To do this, we consider a Hierarchical Multi-Armed Bandit problem
and propose a two-level hierarchical bandit solution which aims to learn a
scalable placement policy that optimizes the stated objectives in an online
manner. Simulations on two real network topologies show that our proposed
approach achieves 5% average node resource utilization while admitting over 25%
more slice requests in certain scenarios, compared to baseline methods.

</details>


### [181] [An Online Multi-dimensional Knapsack Approach for Slice Admission Control](https://arxiv.org/abs/2508.06468)
*Jesutofunmi Ajayi,Antonio Di Maio,Torsten Braun,Dimitrios Xenakis*

Main category: cs.NI

TL;DR: 网络切片在线准入控制问题被建模为多维背包问题，提出的策略显著提升了收入并优化了资源利用。


<details>
  <summary>Details</summary>
Motivation: 解决切片使能网络中服务提供的不确定性，特别是在有限网络资源需求共享方面的挑战，以最大化长期收入。

Method: 将切片准入控制问题建模为在线多维背包问题，提出了两种基于预留的策略及其算法，并通过蒙特卡洛模拟评估性能。

Result: 模拟结果显示，提出的在线策略比先到先服务贪婪策略增加了基础设施提供商高达12.9%的收入，同时平均资源消耗减少了1.7%。

Conclusion: 通过网络切片技术的在线多维背包问题建模，提出的两种基于预留的策略及其算法在长期收入最大化方面表现出色，尤其在租户经济不平等增加时，能够为基础设施提供商带来更高的收入。

Abstract: Network Slicing has emerged as a powerful technique to enable cost-effective,
multi-tenant communications and services over a shared physical mobile network
infrastructure. One major challenge of service provisioning in slice-enabled
networks is the uncertainty in the demand for the limited network resources
that must be shared among existing slices and potentially new Network Slice
Requests. In this paper, we consider admission control of Network Slice
Requests in an online setting, with the goal of maximizing the long-term
revenue received from admitted requests. We model the Slice Admission Control
problem as an Online Multidimensional Knapsack Problem and present two
reservation-based policies and their algorithms, which have a competitive
performance for Online Multidimensional Knapsack Problems. Through Monte Carlo
simulations, we evaluate the performance of our online admission control method
in terms of average revenue gained by the Infrastructure Provider, system
resource utilization, and the ratio of accepted slice requests. We compare our
approach with those of the online First Come First Serve greedy policy. The
simulation's results prove that our proposed online policies increase revenues
for Infrastructure Providers by up to 12.9 % while reducing the average
resource consumption by up to 1.7% In particular, when the tenants' economic
inequality increases, an Infrastructure Provider who adopts our proposed online
admission policies gains higher revenues compared to an Infrastructure Provider
who adopts First Come First Serve.

</details>
