<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 94]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.RO](#cs.RO) [Total: 30]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.NI](#cs.NI) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder](https://arxiv.org/abs/2508.10918)
*Samantha Aziz,Oleg Komogortsev*

Main category: cs.CV

TL;DR: 提出一种潜在噪声自编码器机制，保护视线数据隐私并保持其可用性，实现了优于现有方法的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 解决用户在不同游戏会话中被未经同意重新识别的隐私问题，同时确保视线数据在良性任务中的可用性。

Method: 采用潜在噪声自编码器技术，平衡隐私保护与数据实用性，评估了生物特征识别和视线预测任务中的隐私-效用权衡。

Result: 显著降低了生物特征可识别性，且效用损失最小，同时保留了适合下游使用的生理学上合理的视线模式。

Conclusion: 该研究通过潜在噪声自编码器机制，在保护用户隐私的同时，保留了视线数据的可用性，为视线基系统提供了实用且有效的隐私保护方案。

Abstract: We present a privacy-enhancing mechanism for gaze signals using a
latent-noise autoencoder that prevents users from being re-identified across
play sessions without their consent, while retaining the usability of the data
for benign tasks. We evaluate privacy-utility trade-offs across biometric
identification and gaze prediction tasks, showing that our approach
significantly reduces biometric identifiability with minimal utility
degradation. Unlike prior methods in this direction, our framework retains
physiologically plausible gaze patterns suitable for downstream use, which
produces favorable privacy-utility trade-off. This work advances privacy in
gaze-based systems by providing a usable and effective mechanism for protecting
sensitive gaze data.

</details>


### [2] [A Survey on Video Temporal Grounding with Multimodal Large Language Model](https://arxiv.org/abs/2508.10922)
*Jianlong Wu,Wei Liu,Ye Liu,Meng Liu,Liqiang Nie,Zhouchen Lin,Chang Wen Chen*

Main category: cs.CV

TL;DR: 本文系统综述了基于多模态大语言模型的视频时间定位方法，通过三维分类法分析其架构、训练和特征处理，并总结了现有局限和未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管视频语言理解领域已有广泛综述，但针对VTG-MLLMs的全面调查仍较少，本文旨在填补这一空白。

Method: 通过三维分类法系统性地回顾当前VTG-MLLMs的研究：1) MLLMs的功能角色；2) 训练范式；3) 视频特征处理技术。

Result: VTG-MLLMs不仅在性能上具有竞争力，还在零样本、多任务和多领域设置中展现出卓越的泛化能力。

Conclusion: 本文总结了视频时间定位（VTG）领域的最新进展，特别是基于多模态大语言模型（MLLMs）的方法，并提出了未来研究方向。

Abstract: The recent advancement in video temporal grounding (VTG) has significantly
enhanced fine-grained video understanding, primarily driven by multimodal large
language models (MLLMs). With superior multimodal comprehension and reasoning
abilities, VTG approaches based on MLLMs (VTG-MLLMs) are gradually surpassing
traditional fine-tuned methods. They not only achieve competitive performance
but also excel in generalization across zero-shot, multi-task, and multi-domain
settings. Despite extensive surveys on general video-language understanding,
comprehensive reviews specifically addressing VTG-MLLMs remain scarce. To fill
this gap, this survey systematically examines current research on VTG-MLLMs
through a three-dimensional taxonomy: 1) the functional roles of MLLMs,
highlighting their architectural significance; 2) training paradigms, analyzing
strategies for temporal reasoning and task adaptation; and 3) video feature
processing techniques, which determine spatiotemporal representation
effectiveness. We further discuss benchmark datasets, evaluation protocols, and
summarize empirical findings. Finally, we identify existing limitations and
propose promising research directions. For additional resources and details,
readers are encouraged to visit our repository at
https://github.com/ki-lw/Awesome-MLLMs-for-Video-Temporal-Grounding.

</details>


### [3] [VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By \underline{V}alue \underline{S}ign \underline{F}lip](https://arxiv.org/abs/2508.10931)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: VSF通过翻转负提示的注意力值符号，有效提升负提示遵循能力，适用于多种模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法（如CFG、NASA和NAG）在负提示引导方面的不足，特别是在少步扩散和流匹配图像生成模型中。

Method: VSF（Value Sign Flip）是一种简单高效的方法，通过翻转负提示的注意力值符号来动态抑制不需要的内容，适用于MMDiT-style架构和基于交叉注意力的模型。

Result: 实验结果表明，VSF在复杂提示对的数据集上表现优异，显著优于现有方法，且在静态图像和视频生成任务中均保持竞争力。

Conclusion: VSF方法通过动态翻转负提示的注意力值符号，显著提高了负提示的遵循能力，同时在少步模型和非少步模型中均表现出色，且保持了图像质量。

Abstract: We introduce Value Sign Flip (VSF), a simple and efficient method for
incorporating negative prompt guidance in few-step diffusion and flow-matching
image generation models. Unlike existing approaches such as classifier-free
guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by
flipping the sign of attention values from negative prompts. Our method
requires only small computational overhead and integrates effectively with
MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as
cross-attention-based models like Wan. We validate VSF on challenging datasets
with complex prompt pairs and demonstrate superior performance in both static
image and video generation tasks. Experimental results show that VSF
significantly improves negative prompt adherence compared to prior methods in
few-step models, and even CFG in non-few-step models, while maintaining
competitive image quality. Code and ComfyUI node are available in
https://github.com/weathon/VSF/tree/main.

</details>


### [4] [ViPE: Video Pose Engine for 3D Geometric Perception](https://arxiv.org/abs/2508.10934)
*Jiahui Huang,Qunjie Zhou,Hesam Rabeti,Aleksandr Korovko,Huan Ling,Xuanchi Ren,Tianchang Shen,Jun Gao,Dmitry Slepichev,Chen-Hsuan Lin,Jiawei Ren,Kevin Xie,Joydeep Biswas,Laura Leal-Taixe,Sanja Fidler*

Main category: cs.CV

TL;DR: ViPE是一种高效视频处理引擎，能估计相机参数和深度图，适用于多种场景，性能优于现有方法，并标注了大规模视频数据集。


<details>
  <summary>Details</summary>
Motivation: 解决从野外视频中获取一致且精确的3D标注的挑战，以支持空间AI系统的训练需求。

Method: ViPE是一种高效估计相机内参、相机运动和密集近度量深度图的视频处理引擎，适用于多种场景和相机模型。

Result: ViPE在TUM/KITTI序列上优于现有未校准位姿估计基线18%/50%，并在单GPU上以3-5FPS运行。

Conclusion: ViPE及其标注的数据集的开源旨在加速空间AI系统的发展。

Abstract: Accurate 3D geometric perception is an important prerequisite for a wide
range of spatial AI systems. While state-of-the-art methods depend on
large-scale training data, acquiring consistent and precise 3D annotations from
in-the-wild videos remains a key challenge. In this work, we introduce ViPE, a
handy and versatile video processing engine designed to bridge this gap. ViPE
efficiently estimates camera intrinsics, camera motion, and dense, near-metric
depth maps from unconstrained raw videos. It is robust to diverse scenarios,
including dynamic selfie videos, cinematic shots, or dashcams, and supports
various camera models such as pinhole, wide-angle, and 360{\deg} panoramas. We
have benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing
uncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and
runs at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to
annotate a large-scale collection of videos. This collection includes around
100K real-world internet videos, 1M high-quality AI-generated videos, and 2K
panoramic videos, totaling approximately 96M frames -- all annotated with
accurate camera poses and dense depth maps. We open-source ViPE and the
annotated dataset with the hope of accelerating the development of spatial AI
systems.

</details>


### [5] [Relative Pose Regression with Pose Auto-Encoders: Enhancing Accuracy and Data Efficiency for Retail Applications](https://arxiv.org/abs/2508.10933)
*Yoli Shavit,Yosi Keller*

Main category: cs.CV

TL;DR: 提出一种基于PAE的RPR方法，优化APR定位精度，减少数据需求，适用于零售环境。


<details>
  <summary>Details</summary>
Motivation: 现代零售环境中，精准的相机定位对提升顾客体验、优化库存管理和实现自主操作至关重要。结合视觉和空间场景先验的方法能提高定位精度。

Method: 扩展PAEs至RPR任务，并提出一种无需额外存储图像或姿态数据的重新定位方案，通过PAE-based RPR优化APR预测。

Result: 在室内基准测试中，该方法显著提升了APR定位精度，且仅需30%的训练数据即可达到竞争性能。

Conclusion: 该方法通过结合PAE-based RPR显著提升了APR定位精度，且在数据量有限的情况下仍保持竞争力，降低了零售部署的数据收集负担。

Abstract: Accurate camera localization is crucial for modern retail environments,
enabling enhanced customer experiences, streamlined inventory management, and
autonomous operations. While Absolute Pose Regression (APR) from a single image
offers a promising solution, approaches that incorporate visual and spatial
scene priors tend to achieve higher accuracy. Camera Pose Auto-Encoders (PAEs)
have recently been introduced to embed such priors into APR. In this work, we
extend PAEs to the task of Relative Pose Regression (RPR) and propose a novel
re-localization scheme that refines APR predictions using PAE-based RPR,
without requiring additional storage of images or pose data. We first introduce
PAE-based RPR and establish its effectiveness by comparing it with image-based
RPR models of equivalent architectures. We then demonstrate that our refinement
strategy, driven by a PAE-based RPR, enhances APR localization accuracy on
indoor benchmarks. Notably, our method is shown to achieve competitive
performance even when trained with only 30% of the data, substantially reducing
the data collection burden for retail deployment. Our code and pre-trained
models are available at: https://github.com/yolish/camera-pose-auto-encoders

</details>


### [6] [HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model](https://arxiv.org/abs/2508.10935)
*Qi Liu,Yabei Li,Hongsong Wang,Lei He*

Main category: cs.CV

TL;DR: HQ-OV3D是一个专注于生成和精炼高质量伪标签的开放词汇3D检测框架，通过几何一致性和去噪机制显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统封闭式3D检测框架无法满足自动驾驶等开放世界应用的需求，现有开放词汇3D检测方法的几何质量（尤其是边界框精度）常被忽视。

Method: HQ-OV3D框架包含两个关键组件：Intra-Modality Cross-Validated (IMCV) Proposal Generator和Annotated-Class Assisted (ACA) Denoiser，前者利用跨模态几何一致性生成高质量初始3D提案，后者通过基于DDIM的去噪机制逐步精炼3D提案。

Result: 与最先进方法相比，使用HQ-OV3D生成的伪标签训练在新类别上实现了7.37%的mAP提升。

Conclusion: HQ-OV3D框架不仅作为独立的开放词汇3D检测器表现出色，还能作为高质量伪标签生成器，适用于现有开放词汇检测或注释流程。

Abstract: Traditional closed-set 3D detection frameworks fail to meet the demands of
open-world applications like autonomous driving. Existing open-vocabulary 3D
detection methods typically adopt a two-stage pipeline consisting of
pseudo-label generation followed by semantic alignment. While vision-language
models (VLMs) recently have dramatically improved the semantic accuracy of
pseudo-labels, their geometric quality, particularly bounding box precision,
remains commonly neglected.To address this issue, we propose a High Box Quality
Open-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and
refine high-quality pseudo-labels for open-vocabulary classes. The framework
comprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal
Generator that utilizes cross-modality geometric consistency to generate
high-quality initial 3D proposals, and an Annotated-Class Assisted (ACA)
Denoiser that progressively refines 3D proposals by leveraging geometric priors
from annotated categories through a DDIM-based denoising mechanism.Compared to
the state-of-the-art method, training with pseudo-labels generated by our
approach achieves a 7.37% improvement in mAP on novel classes, demonstrating
the superior quality of the pseudo-labels produced by our framework. HQ-OV3D
can serve not only as a strong standalone open-vocabulary 3D detector but also
as a plug-in high-quality pseudo-label generator for existing open-vocabulary
detection or annotation pipelines.

</details>


### [7] [Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction](https://arxiv.org/abs/2508.10936)
*Cheng Chen,Hao Huang,Saurabh Bagchi*

Main category: cs.CV

TL;DR: 提出一种基于稀疏3D语义高斯泼溅的协作感知方法，显著提升性能并减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于密集3D体素或2D平面特征的协作感知方法在通信成本、深度估计或额外监督方面的局限性。

Method: 利用稀疏3D语义高斯泼溅技术，通过共享和融合中间高斯基元，实现了跨代理的邻居融合、几何与语义的联合编码以及稀疏的以对象为中心的消息传递。

Result: 实验表明，该方法在mIoU和IoU上分别比单代理感知和基线协作方法提升了+8.42/+3.28和+5.11/+22.41分，且在减少传输高斯数量时仍能保持性能优势。

Conclusion: 该论文提出的基于稀疏3D语义高斯泼溅的协作3D语义占用预测方法，在减少通信开销的同时显著提升了性能，适用于有限通信预算的场景。

Abstract: Collaborative perception enables connected vehicles to share information,
overcoming occlusions and extending the limited sensing range inherent in
single-agent (non-collaborative) systems. Existing vision-only methods for 3D
semantic occupancy prediction commonly rely on dense 3D voxels, which incur
high communication costs, or 2D planar features, which require accurate depth
estimation or additional supervision, limiting their applicability to
collaborative scenarios. To address these challenges, we propose the first
approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D
semantic occupancy prediction. By sharing and fusing intermediate Gaussian
primitives, our method provides three benefits: a neighborhood-based
cross-agent fusion that removes duplicates and suppresses noisy or inconsistent
Gaussians; a joint encoding of geometry and semantics in each primitive, which
reduces reliance on depth supervision and allows simple rigid alignment; and
sparse, object-centric messages that preserve structural information while
reducing communication volume. Extensive experiments demonstrate that our
approach outperforms single-agent perception and baseline collaborative methods
by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU,
respectively. When further reducing the number of transmitted Gaussians, our
method still achieves a +1.9 improvement in mIoU, using only 34.6%
communication volume, highlighting robust performance under limited
communication budgets.

</details>


### [8] [Personalized Face Super-Resolution with Identity Decoupling and Fitting](https://arxiv.org/abs/2508.10937)
*Jiarui Yang,Hang Guo,Wen Huang,Tao Dai,Shutao Xia*

Main category: cs.CV

TL;DR: IDFSR通过身份解耦和拟合技术，在极端退化场景下提升了面部超分辨率的ID一致性和感知质量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在极端退化场景（如缩放比例>8倍）下，传统模型难以重建真实且ID一致的面部图像，导致生成的图像缺乏真实的ID约束。IDFSR旨在解决这一问题。

Method: IDFSR方法包括三个关键设计：1）掩码低分辨率（LR）图像中的面部区域以消除不可靠的ID线索；2）将参考图像对齐到LR输入以提供风格指导；3）利用从真实（GT）图像中提取的ID嵌入进行细粒度ID建模和个性化适配。

Result: 广泛的定量评估和视觉比较表明，IDFSR在极端退化条件下显著优于现有方法，尤其是在ID一致性方面表现优异。

Conclusion: 该论文提出的IDFSR方法在极端退化场景下显著提升了面部超分辨率（FSR）的身份（ID）一致性和感知质量，通过身份解耦和拟合技术有效缓解了幻觉效应。

Abstract: In recent years, face super-resolution (FSR) methods have achieved remarkable
progress, generally maintaining high image fidelity and identity (ID)
consistency under standard settings. However, in extreme degradation scenarios
(e.g., scale $> 8\times$), critical attributes and ID information are often
severely lost in the input image, making it difficult for conventional models
to reconstruct realistic and ID-consistent faces. Existing methods tend to
generate hallucinated faces under such conditions, producing restored images
lacking authentic ID constraints. To address this challenge, we propose a novel
FSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID
restoration under large scaling factors while mitigating hallucination effects.
Our approach involves three key designs: 1) \textbf{Masking} the facial region
in the low-resolution (LR) image to eliminate unreliable ID cues; 2)
\textbf{Warping} a reference image to align with the LR input, providing style
guidance; 3) Leveraging \textbf{ID embeddings} extracted from ground truth (GT)
images for fine-grained ID modeling and personalized adaptation. We first
pretrain a diffusion-based model to explicitly decouple style and ID by forcing
it to reconstruct masked LR face regions using both style and identity
embeddings. Subsequently, we freeze most network parameters and perform
lightweight fine-tuning of the ID embedding using a small set of target ID
images. This embedding encodes fine-grained facial attributes and precise ID
information, significantly improving both ID consistency and perceptual
quality. Extensive quantitative evaluations and visual comparisons demonstrate
that the proposed IDFSR substantially outperforms existing approaches under
extreme degradation, particularly achieving superior performance on ID
consistency.

</details>


### [9] [Deep Learning for Automated Identification of Vietnamese Timber Species: A Tool for Ecological Monitoring and Conservation](https://arxiv.org/abs/2508.10938)
*Tianyu Song,Van-Doan Duong,Thi-Phuong Le,Ton Viet Ta*

Main category: cs.CV

TL;DR: 研究利用深度学习模型（尤其是ShuffleNetV2）实现了越南常见木材物种的高精度自动化分类，为生态监测和森林管理提供了高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统木材分类方法依赖宏观和微观检查，耗时且需专业知识。本研究旨在通过深度学习自动化分类，提升效率和准确性。

Method: 研究构建了一个自定义图像数据集，并评估了五种卷积神经网络架构（ResNet50、EfficientNet、MobileViT、MobileNetV3和ShuffleNetV2）的性能。

Result: ShuffleNetV2在20次独立运行中平均准确率达到99.29%，F1分数为99.35%，表现出最佳的分类性能和计算效率平衡。

Conclusion: 研究表明，轻量级深度学习模型（如ShuffleNetV2）在资源受限环境中能够实现高精度的木材物种实时识别，为生态信息学领域提供了可扩展的图像分类解决方案。

Abstract: Accurate identification of wood species plays a critical role in ecological
monitoring, biodiversity conservation, and sustainable forest management.
Traditional classification approaches relying on macroscopic and microscopic
inspection are labor-intensive and require expert knowledge. In this study, we
explore the application of deep learning to automate the classification of ten
wood species commonly found in Vietnam. A custom image dataset was constructed
from field-collected wood samples, and five state-of-the-art convolutional
neural network architectures--ResNet50, EfficientNet, MobileViT, MobileNetV3,
and ShuffleNetV2--were evaluated. Among these, ShuffleNetV2 achieved the best
balance between classification performance and computational efficiency, with
an average accuracy of 99.29\% and F1-score of 99.35\% over 20 independent
runs. These results demonstrate the potential of lightweight deep learning
models for real-time, high-accuracy species identification in
resource-constrained environments. Our work contributes to the growing field of
ecological informatics by providing scalable, image-based solutions for
automated wood classification and forest biodiversity assessment.

</details>


### [10] [NIRMAL Pooling: An Adaptive Max Pooling Approach with Non-linear Activation for Enhanced Image Classification](https://arxiv.org/abs/2508.10940)
*Nirmal Gaud,Krishna Kumar Jha,Jhimli Adhikari,Adhini Nasarin P S,Joydeep Das,Samarth S Deshpande,Nitasha Barara,Vaduguru Venkata Ramya,Santu Saha,Mehmet Tarik Baran,Sarangi Venkateshwarlu,Anusha M D,Surej Mouli,Preeti Katiyar,Vipin Kumar Chaudhary*

Main category: cs.CV

TL;DR: NIRMAL Pooling是一种新型CNN池化层，结合自适应最大池化和ReLU激活，提升了图像分类性能，尤其在复杂数据集上表现更优。


<details>
  <summary>Details</summary>
Motivation: 为了提高CNN在图像分类任务中的特征表达能力和鲁棒性，开发一种更灵活可靠的池化方法。

Method: 提出NIRMAL Pooling层，结合自适应最大池化和ReLU激活函数，动态调整池化参数以适应输出维度。

Result: 在MNIST Digits、MNIST Fashion和CIFAR-10数据集上，NIRMAL Pooling分别达到99.25%、91.59%和70.49%的测试准确率，均优于传统最大池化。

Conclusion: NIRMAL Pooling作为一种新型的CNN池化层，通过结合自适应最大池化和非线性激活函数，在图像分类任务中展现了优于传统最大池化的性能，特别是在复杂数据集上。

Abstract: This paper presents NIRMAL Pooling, a novel pooling layer for Convolutional
Neural Networks (CNNs) that integrates adaptive max pooling with non-linear
activation function for image classification tasks. The acronym NIRMAL stands
for Non-linear Activation, Intermediate Aggregation, Reduction, Maximum,
Adaptive, and Localized. By dynamically adjusting pooling parameters based on
desired output dimensions and applying a Rectified Linear Unit (ReLU)
activation post-pooling, NIRMAL Pooling improves robustness and feature
expressiveness. We evaluated its performance against standard Max Pooling on
three benchmark datasets: MNIST Digits, MNIST Fashion, and CIFAR-10. NIRMAL
Pooling achieves test accuracies of 99.25% (vs. 99.12% for Max Pooling) on
MNIST Digits, 91.59% (vs. 91.44%) on MNIST Fashion, and 70.49% (vs. 68.87%) on
CIFAR-10, demonstrating consistent improvements, particularly on complex
datasets. This work highlights the potential of NIRMAL Pooling to enhance CNN
performance in diverse image recognition tasks, offering a flexible and
reliable alternative to traditional pooling methods.

</details>


### [11] [Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram](https://arxiv.org/abs/2508.10942)
*Liming Xu,Dave Towey,Andrew P. French,Steve Benford*

Main category: cs.CV

TL;DR: 研究提出了一种新的特征描述符（形状方向直方图）用于检测Artcodes，实验验证了其可行性和有效性，为拓扑对象检测提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 随着智能手机和VR/AR技术的普及，日常环境中可能出现更多与虚拟元素连接的装饰性对象。识别这些对象（如Artcodes）的存在是触发后续交互的第一步。

Method: 提出了形状方向直方图（shape of orientation histogram）作为新的特征描述符，用于描述Artcode的通用拓扑结构，并构建了Artcode检测系统进行实验评估。

Result: 实验结果表明，所提出的特征向量能够有效表示拓扑结构，且系统在Artcode检测任务中表现良好。

Conclusion: 本研究提出了基于形状方向直方图的新特征描述符，用于检测Artcodes，实验证明了该方法的可行性和系统的有效性。尽管这是初步尝试，但它为拓扑对象检测开辟了新的交互机会和应用潜力。

Abstract: The increasing ubiquity of smartphones and resurgence of VR/AR techniques, it
is expected that our everyday environment may soon be decorating with objects
connecting with virtual elements. Alerting to the presence of these objects is
therefore the first step for motivating follow-up further inspection and
triggering digital material attached to the objects. This work studies a
special kind of these objects -- Artcodes -- a human-meaningful and
machine-readable decorative markers that camouflage themselves with freeform
appearance by encoding information into their topology. We formulate this
problem of recongising the presence of Artcodes as Artcode proposal detection,
a distinct computer vision task that classifies topologically similar but
geometrically and semantically different objects as a same class. To deal with
this problem, we propose a new feature descriptor, called the shape of
orientation histogram, to describe the generic topological structure of an
Artcode. We collect datasets and conduct comprehensive experiments to evaluate
the performance of the Artcode detection proposer built upon this new feature
vector. Our experimental results show the feasibility of the proposed feature
vector for representing topological structures and the effectiveness of the
system for detecting Artcode proposals. Although this work is an initial
attempt to develop a feature-based system for detecting topological objects
like Artcodes, it would open up new interaction opportunities and spark
potential applications of topological object detection.

</details>


### [12] [Analysis of the Compaction Behavior of Textile Reinforcements in Low-Resolution In-Situ CT Scans via Machine-Learning and Descriptor-Based Methods](https://arxiv.org/abs/2508.10943)
*Christian Düreth,Jan Condé-Wolter,Marek Danczak,Karsten Tittmann,Jörn Jaschinski,Andreas Hornig,Maik Gude*

Main category: cs.CV

TL;DR: 该研究通过低分辨率CT和3D-UNet量化纺织增强材料的嵌套行为，为复合材料预成型体的结构分析提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 理解纺织增强复合材料的多尺度结构对于预测建模至关重要，而嵌套行为是影响其机械性能（如刚度、渗透性和损伤容限）的关键因素。

Method: 研究采用低分辨率计算断层扫描（CT）进行原位压实实验，结合定制的3D-UNet模型进行语义分割，并使用两点相关函数S2分析空间结构。

Result: 模型实现了最小平均交并比（IoU）0.822和F1分数0.902，并通过两点相关函数成功提取了平均层厚度和嵌套度，结果与显微图像验证高度一致。

Conclusion: 该研究提出了一种量化干纺织增强材料在压实过程中嵌套行为的框架，并通过低分辨率CT扫描实现了对基质、纬纱和填充相的语义分割。该方法为从工业相关CT数据中提取关键几何特征提供了可靠途径，并为复合预成型体的逆向建模和基于描述符的结构分析奠定了基础。

Abstract: A detailed understanding of material structure across multiple scales is
essential for predictive modeling of textile-reinforced composites. Nesting --
characterized by the interlocking of adjacent fabric layers through local
interpenetration and misalignment of yarns -- plays a critical role in defining
mechanical properties such as stiffness, permeability, and damage tolerance.
This study presents a framework to quantify nesting behavior in dry textile
reinforcements under compaction using low-resolution computed tomography (CT).
In-situ compaction experiments were conducted on various stacking
configurations, with CT scans acquired at 20.22 $\mu$m per voxel resolution. A
tailored 3D{-}UNet enabled semantic segmentation of matrix, weft, and fill
phases across compaction stages corresponding to fiber volume contents of
50--60 %. The model achieved a minimum mean Intersection-over-Union of 0.822
and an $F1$ score of 0.902. Spatial structure was subsequently analyzed using
the two-point correlation function $S_2$, allowing for probabilistic extraction
of average layer thickness and nesting degree. The results show strong
agreement with micrograph-based validation. This methodology provides a robust
approach for extracting key geometrical features from industrially relevant CT
data and establishes a foundation for reverse modeling and descriptor-based
structural analysis of composite preforms.

</details>


### [13] [ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks](https://arxiv.org/abs/2508.10956)
*Abhishek Kolari,Mohammadhossein Khojasteh,Yifan Jiang,Floris den Hengst,Filip Ilievski*

Main category: cs.CV

TL;DR: ORBIT基准测试揭示了视觉语言模型在对象属性推理上的不足，尤其是在复杂任务中，呼吁开发更强大的推理模型和标准化评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉问答基准测试在对象属性推理上存在局限性，未能有效区分感知与推理，且图像类别和推理任务缺乏代表性。受人类对象分类启发，研究旨在填补这一空白。

Method: 研究团队开发了一个系统性的评估框架ORBIT，包含三种代表性图像类型、三个复杂性递增的推理层级和四个对象属性维度，并通过1,080个计数问题对12种最先进的视觉语言模型进行了零样本测试。

Result: 实验结果显示，最佳模型在ORBIT基准测试中仅达到40%的准确率，远低于人类水平，尤其在真实图像、反事实推理和高计数任务中表现较差。

Conclusion: ORBIT基准测试揭示了当前视觉语言模型在对象属性推理上的显著局限性，尤其是在真实图像和复杂推理任务中表现不佳，强调了开发可扩展基准测试和改进模型推理能力的必要性。

Abstract: While vision-language models (VLMs) have made remarkable progress on many
popular visual question answering (VQA) benchmarks, it remains unclear whether
they abstract and reason over depicted objects. Inspired by human object
categorisation, object property reasoning involves identifying and recognising
low-level details and higher-level abstractions. While current VQA benchmarks
consider a limited set of object property attributes like size, they typically
blend perception and reasoning, and lack representativeness in terms of
reasoning and image categories. To this end, we introduce a systematic
evaluation framework with images of three representative types, three reasoning
levels of increasing complexity, and four object property dimensions driven by
prior work on commonsense reasoning. We develop a procedure to instantiate this
benchmark into ORBIT, a multi-level reasoning VQA benchmark for object
properties comprising 360 images paired with a total of 1,080 count-based
questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings
reveal significant limitations compared to humans, with the best-performing
model only reaching 40\% accuracy. VLMs struggle particularly with realistic
(photographic) images, counterfactual reasoning about physical and functional
properties, and higher counts. ORBIT points to the need to develop methods for
scalable benchmarking, generalize annotation guidelines, and explore additional
reasoning VLMs. We make the ORBIT benchmark and the experimental code available
to support such endeavors.

</details>


### [14] [iWatchRoad: Scalable Detection and Geospatial Visualization of Potholes for Smart Cities](https://arxiv.org/abs/2508.10945)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoad 是一个端到端系统，通过 dashcam 视频和 YOLO 模型实时检测和映射道路坑洞，适用于印度等发展中国家的道路管理。


<details>
  <summary>Details</summary>
Motivation: 印度多样且维护不足的道路上的坑洞对道路安全和车辆寿命构成严重威胁，需要一种自动化、高效的检测和映射系统。

Method: 利用 dashcam 视频创建了一个包含 7,000 多帧的自注释数据集，并微调了 Ultralytics YOLO 模型进行实时坑洞检测。结合自定义 OCR 模块提取时间戳并与 GPS 日志同步，实现精确的地理标记。数据存储在数据库中并通过 OSM 用户友好的网络界面可视化。

Result: iWatchRoad 在具有挑战性的条件下提高了检测准确性，并提供了政府兼容的输出，用于道路评估和维护规划。

Conclusion: iWatchRoad 提供了一种成本效益高、硬件效率高且可扩展的解决方案，用于自动化检测和映射道路坑洞，特别适用于发展中国家如印度的道路管理。

Abstract: Potholes on the roads are a serious hazard and maintenance burden. This poses
a significant threat to road safety and vehicle longevity, especially on the
diverse and under-maintained roads of India. In this paper, we present a
complete end-to-end system called iWatchRoad for automated pothole detection,
Global Positioning System (GPS) tagging, and real time mapping using
OpenStreetMap (OSM). We curated a large, self-annotated dataset of over 7,000
frames captured across various road types, lighting conditions, and weather
scenarios unique to Indian environments, leveraging dashcam footage. This
dataset is used to fine-tune, Ultralytics You Only Look Once (YOLO) model to
perform real time pothole detection, while a custom Optical Character
Recognition (OCR) module was employed to extract timestamps directly from video
frames. The timestamps are synchronized with GPS logs to geotag each detected
potholes accurately. The processed data includes the potholes' details and
frames as metadata is stored in a database and visualized via a user friendly
web interface using OSM. iWatchRoad not only improves detection accuracy under
challenging conditions but also provides government compatible outputs for road
assessment and maintenance planning through the metadata visible on the
website. Our solution is cost effective, hardware efficient, and scalable,
offering a practical tool for urban and rural road management in developing
regions, making the system automated. iWatchRoad is available at
https://smlab.niser.ac.in/project/iwatchroad

</details>


### [15] [Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision](https://arxiv.org/abs/2508.10972)
*Rosiana Natalie,Wenqian Xu,Ruei-Che Chang,Rada Mihalcea,Anhong Guo*

Main category: cs.CV

TL;DR: VLMs在模拟低视力视觉感知时，组合视觉信息和示例图像响应能显著提升一致性（0.70），但额外示例效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型（VLMs）在无障碍领域模拟低视力个体视觉感知的能力，填补此前研究的空白。

Method: 通过调查40名低视力参与者，收集他们的视觉信息和图像感知响应，构建基准数据集。使用这些数据为GPT-4o生成提示，模拟每个参与者的代理，并评估VLM生成响应与原始答案的一致性。

Result: VLMs在最小提示下倾向于推断超出指定视觉能力，导致一致性较低（0.59）。仅提供视觉信息或示例图像响应时一致性仍低（0.59），而组合两者则显著提高一致性（0.70）。

Conclusion: VLMs在模拟低视力个体的视觉感知时，当提示中包含视觉信息和示例图像响应的组合时，能够显著提高与参与者原始答案的一致性（0.70）。单个结合开放式和多选题响应的示例已足以显著提升性能，而额外示例的收益有限。

Abstract: Advances in vision language models (VLMs) have enabled the simulation of
general human behavior through their reasoning and problem solving
capabilities. However, prior research has not investigated such simulation
capabilities in the accessibility domain. In this paper, we evaluate the extent
to which VLMs can simulate the vision perception of low vision individuals when
interpreting images. We first compile a benchmark dataset through a survey
study with 40 low vision participants, collecting their brief and detailed
vision information and both open-ended and multiple-choice image perception and
recognition responses to up to 25 images. Using these responses, we construct
prompts for VLMs (GPT-4o) to create simulated agents of each participant,
varying the included information on vision information and example image
responses. We evaluate the agreement between VLM-generated responses and
participants' original answers. Our results indicate that VLMs tend to infer
beyond the specified vision ability when given minimal prompts, resulting in
low agreement (0.59). The agreement between the agent' and participants'
responses remains low when only either the vision information (0.59) or example
image responses (0.59) are provided, whereas a combination of both
significantly increase the agreement (0.70, p < 0.0001). Notably, a single
example combining both open-ended and multiple-choice responses, offers
significant performance improvements over either alone (p < 0.0001), while
additional examples provided minimal benefits (p > 0.05).

</details>


### [16] [IPG: Incremental Patch Generation for Generalized Adversarial Patch Training](https://arxiv.org/abs/2508.10946)
*Wonho Lee,Hyunsik Na,Jisu Lee,Daeseon Choi*

Main category: cs.CV

TL;DR: IPG是一种高效的对抗补丁生成方法，比现有方法快11.1倍，适用于多种高风险AI应用场景。


<details>
  <summary>Details</summary>
Motivation: 对抗补丁对AI模型（尤其是计算机视觉任务）的鲁棒性构成重大挑战，现有方法效率不足。

Method: 提出了增量补丁生成（IPG）方法，通过实验和消融研究（如YOLO特征分布可视化和对抗训练结果）验证其有效性。

Result: IPG生成对抗补丁的效率比现有方法高11.1倍，同时保持可比攻击性能，并能覆盖更广泛的模型漏洞。

Conclusion: IPG展现了在对抗补丁防御和实际应用中的巨大潜力，特别是在需要AI模型在高风险动态环境中保持韧性的领域。

Abstract: The advent of adversarial patches poses a significant challenge to the
robustness of AI models, particularly in the domain of computer vision tasks
such as object detection. In contradistinction to traditional adversarial
examples, these patches target specific regions of an image, resulting in the
malfunction of AI models. This paper proposes Incremental Patch Generation
(IPG), a method that generates adversarial patches up to 11.1 times more
efficiently than existing approaches while maintaining comparable attack
performance. The efficacy of IPG is demonstrated by experiments and ablation
studies including YOLO's feature distribution visualization and adversarial
training results, which show that it produces well-generalized patches that
effectively cover a broader range of model vulnerabilities. Furthermore,
IPG-generated datasets can serve as a robust knowledge foundation for
constructing a robust model, enabling structured representation, advanced
reasoning, and proactive defenses in AI security ecosystems. The findings of
this study suggest that IPG has considerable potential for future utilization
not only in adversarial patch defense but also in real-world applications such
as autonomous vehicles, security systems, and medical imaging, where AI models
must remain resilient to adversarial attacks in dynamic and high-stakes
environments.

</details>


### [17] [MedAtlas: Evaluating LLMs for Multi-Round, Multi-Task Medical Reasoning Across Diverse Imaging Modalities and Clinical Text](https://arxiv.org/abs/2508.10947)
*Ronghao Xu,Zhen Huang,Yangbo Wei,Xiaoqian Zhou,Zikang Xu,Ting Liu,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: MedAtlas是一个新型基准框架，用于评估大语言模型在真实医疗推理任务中的表现，解决了现有基准的局限性，并提出了新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 解决现有医学多模态基准在单图像、单轮任务上的局限性，缺乏多模态医学图像整合，无法捕捉临床实践中固有的纵向和多模态交互特性。

Method: 引入了MedAtlas这一新型基准框架，支持多轮对话、多模态医学图像交互、多任务集成和高临床保真度，涵盖四种核心任务。

Result: 基准测试结果显示，现有多模态模型在多阶段临床推理中存在显著性能差距。

Conclusion: MedAtlas 建立了一个具有挑战性的评估平台，旨在推动稳健且可信赖的医疗AI的发展。

Abstract: Artificial intelligence has demonstrated significant potential in clinical
decision-making; however, developing models capable of adapting to diverse
real-world scenarios and performing complex diagnostic reasoning remains a
major challenge. Existing medical multi-modal benchmarks are typically limited
to single-image, single-turn tasks, lacking multi-modal medical image
integration and failing to capture the longitudinal and multi-modal interactive
nature inherent to clinical practice. To address this gap, we introduce
MedAtlas, a novel benchmark framework designed to evaluate large language
models on realistic medical reasoning tasks. MedAtlas is characterized by four
key features: multi-turn dialogue, multi-modal medical image interaction,
multi-task integration, and high clinical fidelity. It supports four core
tasks: open-ended multi-turn question answering, closed-ended multi-turn
question answering, multi-image joint reasoning, and comprehensive disease
diagnosis. Each case is derived from real diagnostic workflows and incorporates
temporal interactions between textual medical histories and multiple imaging
modalities, including CT, MRI, PET, ultrasound, and X-ray, requiring models to
perform deep integrative reasoning across images and clinical texts. MedAtlas
provides expert-annotated gold standards for all tasks. Furthermore, we propose
two novel evaluation metrics: Round Chain Accuracy and Error Propagation
Resistance. Benchmark results with existing multi-modal models reveal
substantial performance gaps in multi-stage clinical reasoning. MedAtlas
establishes a challenging evaluation platform to advance the development of
robust and trustworthy medical AI.

</details>


### [18] [From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement](https://arxiv.org/abs/2508.10950)
*Xinyi Wang,Michael Barnett,Frederique Boonstra,Yael Barnett,Mariano Cabezas,Arkiev D'Souza,Matthew C. Kiernan,Kain Kyle,Meng Law,Lynette Masters,Zihao Tang,Stephen Tisch,Sicong Tu,Anneke Van Der Walt,Dongang Wang,Fernando Calamante,Weidong Cai,Chenyu Wang*

Main category: cs.CV

TL;DR: FastFOD-Net 是一种高效的深度学习框架，用于增强扩散 MRI 中的纤维方向分布（FODs），在健康和疾病人群中验证，显著提升了临床应用的可行性和信任度。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习技术在扩散 MRI 建模中取得了进展，但现有方法主要在健康受试者上评估，这限制了其临床应用。FastFOD-Net 旨在解决这一局限性，并在健康对照和六种神经系统疾病中进行验证。

Method: FastFOD-Net 是一个加速的端到端深度学习框架，用于增强 FODs，具有卓越的性能和临床使用的训练/推理效率（比其前身快 60 倍）。

Result: FastFOD-Net 在临床评估中表现出色，能够对真实世界的临床扩散 MRI 数据进行稳健分析，性能与高质量研究采集相当。

Conclusion: FastFOD-Net 的临床验证展示了其在加速临床神经科学研究、提升疾病区分能力、改善连接组应用的可解释性以及降低测量误差方面的潜力，有助于推动深度学习在扩散 MRI 增强中的广泛应用和临床信任。

Abstract: Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling
technique that represents complex white matter fiber configurations, and a key
step for subsequent brain tractography and connectome analysis. Its reliability
and accuracy, however, heavily rely on the quality of the MRI acquisition and
the subsequent estimation of the FODs at each voxel. Generating reliable FODs
from widely available clinical protocols with single-shell and
low-angular-resolution acquisitions remains challenging but could potentially
be addressed with recent advances in deep learning-based enhancement
techniques. Despite advancements, existing methods have predominantly been
assessed on healthy subjects, which have proved to be a major hurdle for their
clinical adoption. In this work, we validate a newly optimized enhancement
framework, FastFOD-Net, across healthy controls and six neurological disorders.
This accelerated end-to-end deep learning framework enhancing FODs with
superior performance and delivering training/inference efficiency for clinical
use ($60\times$ faster comparing to its predecessor). With the most
comprehensive clinical evaluation to date, our work demonstrates the potential
of FastFOD-Net in accelerating clinical neuroscience research, empowering
diffusion MRI analysis for disease differentiation, improving interpretability
in connectome applications, and reducing measurement errors to lower sample
size requirements. Critically, this work will facilitate the more widespread
adoption of, and build clinical trust in, deep learning based methods for
diffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of
real-world, clinical diffusion MRI data, comparable to that achievable with
high-quality research acquisitions.

</details>


### [19] [A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations](https://arxiv.org/abs/2508.11141)
*Bin Ma,Yifei Zhang,Yongjin Xian,Qi Li,Linna Zhou,Gongxun Miao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于对比学习的跨模态谣言检测方案MICC，通过多尺度图像和上下文相关性探索，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的谣言检测方法往往忽视图像内容以及不同视觉尺度下上下文与图像之间的内在关系，导致关键信息丢失。

Method: 设计了一个SCLIP编码器生成统一的语义嵌入，引入了跨模态多尺度对齐模块，并设计了一个尺度感知融合网络来整合多尺度图像特征与全局文本特征。

Result: 在两个真实世界数据集上的实验结果表明，该方法在谣言检测方面显著优于现有最先进方法。

Conclusion: 该论文提出的MICC算法通过对比学习和多尺度图像与上下文相关性探索，显著提升了谣言检测的性能，显示出其在实践应用中的有效性和潜力。

Abstract: Existing rumor detection methods often neglect the content within images as
well as the inherent relationships between contexts and images across different
visual scales, thereby resulting in the loss of critical information pertinent
to rumor identification. To address these issues, this paper presents a novel
cross-modal rumor detection scheme based on contrastive learning, namely the
Multi-scale Image and Context Correlation exploration algorithm (MICC).
Specifically, we design an SCLIP encoder to generate unified semantic
embeddings for text and multi-scale image patches through contrastive
pretraining, enabling their relevance to be measured via dot-product
similarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is
introduced to identify image regions most relevant to the textual semantics,
guided by mutual information maximization and the information bottleneck
principle, through a Top-K selection strategy based on a cross-modal relevance
matrix constructed between the text and multi-scale image patches. Moreover, a
scale-aware fusion network is designed to integrate the highly correlated
multi-scale image features with global text features by assigning adaptive
weights to image regions based on their semantic importance and cross-modal
relevance. The proposed methodology has been extensively evaluated on two
real-world datasets. The experimental results demonstrate that it achieves a
substantial performance improvement over existing state-of-the-art approaches
in rumor detection, highlighting its effectiveness and potential for practical
applications.

</details>


### [20] [Empowering Multimodal LLMs with External Tools: A Comprehensive Survey](https://arxiv.org/abs/2508.10955)
*Wenbin An,Jiahao Nie,Yaqiang Wu,Feng Tian,Shijian Lu,Qinghua Zheng*

Main category: cs.CV

TL;DR: 本文综述了利用外部工具（如API、专家模型和知识库）增强多模态大语言模型（MLLMs）性能的方法，探讨了其在数据获取、任务表现、模型评估中的作用，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在多模态任务中取得了巨大成功，但多模态数据的质量有限、复杂下游任务表现不佳以及评估协议不足等问题限制了其可靠性和广泛适用性。受人类利用外部工具增强推理和问题解决能力的启发，本文探讨了如何通过外部工具克服这些挑战。

Method: 通过对利用外部工具增强MLLM性能的全面调查，本文围绕四个关键维度展开讨论：高质量多模态数据的获取与标注、复杂下游任务性能的提升、MLLMs的全面准确评估，以及工具增强MLLMs的当前局限与未来方向。

Result: 本文系统性地总结了外部工具如何促进高质量多模态数据的获取与标注、提升MLLMs在复杂任务中的表现、实现更全面的评估，并指出了当前局限与未来方向。

Conclusion: 本文强调了外部工具在提升多模态大语言模型（MLLMs）能力方面的变革潜力，并展望了其未来发展和应用方向。

Abstract: By integrating the perception capabilities of multimodal encoders with the
generative power of Large Language Models (LLMs), Multimodal Large Language
Models (MLLMs), exemplified by GPT-4V, have achieved great success in various
multimodal tasks, pointing toward a promising pathway to artificial general
intelligence. Despite this progress, the limited quality of multimodal data,
poor performance on many complex downstream tasks, and inadequate evaluation
protocols continue to hinder the reliability and broader applicability of MLLMs
across diverse domains. Inspired by the human ability to leverage external
tools for enhanced reasoning and problem-solving, augmenting MLLMs with
external tools (e.g., APIs, expert models, and knowledge bases) offers a
promising strategy to overcome these challenges. In this paper, we present a
comprehensive survey on leveraging external tools to enhance MLLM performance.
Our discussion is structured along four key dimensions about external tools:
(1) how they can facilitate the acquisition and annotation of high-quality
multimodal data; (2) how they can assist in improving MLLM performance on
challenging downstream tasks; (3) how they enable comprehensive and accurate
evaluation of MLLMs; (4) the current limitations and future directions of
tool-augmented MLLMs. Through this survey, we aim to underscore the
transformative potential of external tools in advancing MLLM capabilities,
offering a forward-looking perspective on their development and applications.
The project page of this paper is publicly available
athttps://github.com/Lackel/Awesome-Tools-for-MLLMs.

</details>


### [21] [Better Supervised Fine-tuning for VQA: Integer-Only Loss](https://arxiv.org/abs/2508.11170)
*Baihong Qian,Haotian Fan,Wenjie Liao,Yunqiu Wang,Tao Li,Junhui Cui*

Main category: cs.CV

TL;DR: IOVQA通过整数标签和目标掩码策略优化VLM，显著提升视频质量评估性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉质量评估中存在结果不精确和损失计算效率低的问题，限制了模型对关键评估指标的关注。

Method: 提出IOVQA方法，包括整数标签构造和目标掩码策略，用于微调VLM。

Result: 实验结果表明，该方法在VQA任务中显著提升了模型的性能，在VQualA 2025 GenAI-Bench AIGC视频质量评估挑战中排名第三。

Conclusion: 该研究通过IOVQA方法显著提升了VLM在视频质量评估任务中的准确性和一致性，为优化定量评估场景中的VLM提供了有效思路。

Abstract: With the rapid advancement of vision language models(VLM), their ability to
assess visual content based on specific criteria and dimensions has become
increasingly critical for applications such as video-theme consistency
assessment and visual quality scoring. However, existing methods often suffer
from imprecise results and inefficient loss calculation, which limit the focus
of the model on key evaluation indicators. To address this, we propose
IOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to
enhance their performance in video quality assessment tasks. The key innovation
of IOVQA lies in its label construction and its targeted loss calculation
mechanism. Specifically, during dataset curation, we constrain the model's
output to integers within the range of [10,50], ensuring numerical stability,
and convert decimal Overall_MOS to integer before using them as labels. We also
introduce a target-mask strategy: when computing the loss, only the first
two-digit-integer of the label is unmasked, forcing the model to learn the
critical components of the numerical evaluation. After fine-tuning the
Qwen2.5-VL model using the constructed dataset, experimental results
demonstrate that the proposed method significantly improves the model's
accuracy and consistency in the VQA task, ranking 3rd in VQualA 2025
GenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work
highlights the effectiveness of merely leaving integer labels during
fine-tuning, providing an effective idea for optimizing VLMs in quantitative
evaluation scenarios.

</details>


### [22] [CSNR and JMIM Based Spectral Band Selection for Reducing Metamerism in Urban Driving](https://arxiv.org/abs/2508.10962)
*Jiarong Li,Imad Ali Shah,Diarmaid Geever,Fiachra Collins,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: 该研究利用高光谱成像（HSI）克服RGB图像中的同色异谱问题，通过波段选择策略显著提高弱势道路使用者（VRU）的可检测性，为自动驾驶系统提供更安全的感知基础。


<details>
  <summary>Details</summary>
Motivation: 保护弱势道路使用者（VRU）是汽车感知系统的关键安全挑战，尤其是在由同色异谱现象引起的视觉模糊下，这种现象在RGB图像中使不同材料看起来相似。

Method: 我们提出了一种波段选择策略，结合信息论技术（联合互信息最大化、相关性分析）与图像质量度量（对比信噪比）的新应用，以识别最具光谱信息的波段。

Result: 使用H-City数据集，我们识别了三个信息丰富的波段（497 nm、607 nm和895 nm，±27 nm），并重建了伪彩色图像与共配准的RGB进行比较。定量结果显示VRU与背景的差异性和感知可分离性增加。所选HSI波段在差异性（欧几里得、SAM、T²）和感知（CIE ΔE）指标上分别提高了70.24%、528.46%、1206.83%和246.62%，显著优于RGB，并确认了同色异谱混淆的显著减少。

Conclusion: 通过提供光谱优化的输入，我们的方法增强了VRU的可分离性，为高级驾驶辅助系统（ADAS）和自动驾驶（AD）的下游感知任务奠定了坚实基础，最终有助于提高道路安全。

Abstract: Protecting Vulnerable Road Users (VRU) is a critical safety challenge for
automotive perception systems, particularly under visual ambiguity caused by
metamerism, a phenomenon where distinct materials appear similar in RGB
imagery. This work investigates hyperspectral imaging (HSI) to overcome this
limitation by capturing unique material signatures beyond the visible spectrum,
especially in the Near-Infrared (NIR). To manage the inherent
high-dimensionality of HSI data, we propose a band selection strategy that
integrates information theory techniques (joint mutual information
maximization, correlation analysis) with a novel application of an image
quality metric (contrast signal-to-noise ratio) to identify the most spectrally
informative bands. Using the Hyperspectral City V2 (H-City) dataset, we
identify three informative bands (497 nm, 607 nm, and 895 nm, $\pm$27 nm) and
reconstruct pseudo-color images for comparison with co-registered RGB.
Quantitative results demonstrate increased dissimilarity and perceptual
separability of VRU from the background. The selected HSI bands yield
improvements of 70.24%, 528.46%, 1206.83%, and 246.62% for dissimilarity
(Euclidean, SAM, $T^2$) and perception (CIE $\Delta E$) metrics, consistently
outperforming RGB and confirming a marked reduction in metameric confusion. By
providing a spectrally optimized input, our method enhances VRU separability,
establishing a robust foundation for downstream perception tasks in Advanced
Driver Assistance Systems (ADAS) and Autonomous Driving (AD), ultimately
contributing to improved road safety.

</details>


### [23] [EVCtrl: Efficient Control Adapter for Visual Generation](https://arxiv.org/abs/2508.10963)
*Zixiang Yang,Yue Ma,Yinhan Zhang,Shanhui Mo,Dongrui Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: EVCtrl是一种轻量级控制适配器，通过时空双缓存策略减少冗余计算，显著提升图像和视频控制生成效率，速度提升2倍以上且质量几乎无损。


<details>
  <summary>Details</summary>
Motivation: 当前ControlNet虽然提供了精确的时空控制，但其辅助分支显著增加了延迟，并在非控制区域和去噪步骤中引入了冗余计算，尤其在视频生成中更为明显。为了解决这一问题，作者提出了EVCtrl。

Method: 提出了一种时空双缓存策略：在空间冗余方面，通过分析DiT-ControlNet各层对细粒度控制的响应，将网络划分为全局和局部功能区域，利用局部感知缓存专注于真正需要控制信号的区域；在时间冗余方面，选择性忽略不必要的去噪步骤以提高效率。

Result: 在CogVideo-Controlnet和Wan2.1-Controlnet等基准测试中，EVCtrl分别实现了2.16倍和2.05倍的速度提升，且生成质量几乎无下降。

Conclusion: EVCtrl作为一种轻量级、即插即用的控制适配器，通过时空双缓存策略显著减少了冗余计算，在不重新训练模型的情况下实现了高效的图像和视频控制生成。该方法在多个基准测试中表现出色，速度提升显著且生成质量几乎无下降。

Abstract: Visual generation includes both image and video generation, training
probabilistic models to create coherent, diverse, and semantically faithful
content from scratch. While early research focused on unconditional sampling,
practitioners now demand controllable generation that allows precise
specification of layout, pose, motion, or style. While ControlNet grants
precise spatial-temporal control, its auxiliary branch markedly increases
latency and introduces redundant computation in both uncontrolled regions and
denoising steps, especially for video. To address this problem, we introduce
EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead
without retraining the model. Specifically, we propose a spatio-temporal dual
caching strategy for sparse control information. For spatial redundancy, we
first profile how each layer of DiT-ControlNet responds to fine-grained
control, then partition the network into global and local functional zones. A
locality-aware cache focuses computation on the local zones that truly need the
control signal, skipping the bulk of redundant computation in global regions.
For temporal redundancy, we selectively omit unnecessary denoising steps to
improve efficiency. Extensive experiments on CogVideo-Controlnet,
Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image
and video control generation without the need for training. For example, it
achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and
Wan2.1-Controlnet, respectively, with almost no degradation in generation
quality.Codes are available in the supplementary materials.

</details>


### [24] [Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?](https://arxiv.org/abs/2508.11011)
*Xuezheng Chen,Zhengbo Zou*

Main category: cs.CV

TL;DR: 论文提出了ConstructionSite 10k数据集，用于评估和训练VLM在建筑安全检查中的表现，结果显示现有模型在零样本和小样本设置中表现良好，但仍需进一步训练。


<details>
  <summary>Details</summary>
Motivation: 现有的VLM在建筑安全检查中的应用受限于小型监督数据集，缺乏开放的全面评估数据集。

Method: 提出了ConstructionSite 10k数据集，包含10,000张建筑工地图像，标注了三个互相关联的任务：图像描述、安全规则违反视觉问答（VQA）和建筑元素视觉定位。

Result: 评估显示，当前最先进的预训练VLM在零样本和小样本设置中表现出显著的泛化能力，但仍需额外训练以适应实际建筑工地。

Conclusion: ConstructionSite 10k数据集为研究人员提供了一个有价值的基准，用于训练和评估自己的VLM模型，并推动建筑安全检查领域的进步。

Abstract: Construction safety inspections typically involve a human inspector
identifying safety concerns on-site. With the rise of powerful Vision Language
Models (VLMs), researchers are exploring their use for tasks such as detecting
safety rule violations from on-site images. However, there is a lack of open
datasets to comprehensively evaluate and further fine-tune VLMs in construction
safety inspection. Current applications of VLMs use small, supervised datasets,
limiting their applicability in tasks they are not directly trained for. In
this paper, we propose the ConstructionSite 10k, featuring 10,000 construction
site images with annotations for three inter-connected tasks, including image
captioning, safety rule violation visual question answering (VQA), and
construction element visual grounding. Our subsequent evaluation of current
state-of-the-art large pre-trained VLMs shows notable generalization abilities
in zero-shot and few-shot settings, while additional training is needed to make
them applicable to actual construction sites. This dataset allows researchers
to train and evaluate their own VLMs with new architectures and techniques,
providing a valuable benchmark for construction safety inspection.

</details>


### [25] [Can Multi-modal (reasoning) LLMs detect document manipulation?](https://arxiv.org/abs/2508.11021)
*Zisheng Liang,Kidus Zewde,Rudra Pratap Singh,Disha Patil,Zexi Chen,Jiayu Xue,Yao Yao,Yifei Chen,Qinzhe Liu,Simiao Ren*

Main category: cs.CV

TL;DR: 本研究评估了多模态LLMs在文档欺诈检测中的效果，发现顶级模型在零样本泛化上优于传统方法，但模型大小与准确性相关性有限，强调任务特定微调的重要性。


<details>
  <summary>Details</summary>
Motivation: 文档欺诈对依赖安全可验证文件的行业构成重大威胁，需要强大的检测机制。

Method: 通过提示优化和对模型推理过程的详细分析，评估了这些模型识别欺诈细微指标的能力，如篡改文本、格式不对齐和不一致的交易金额。

Result: 结果显示，表现最佳的多模态LLMs在零样本泛化方面表现优异，优于传统方法，而一些视觉LLMs表现出不一致或较差的表现。模型大小和高级推理能力与检测准确性的相关性有限，表明任务特定的微调至关重要。

Conclusion: 本研究强调了多模态大型语言模型在提升文档欺诈检测系统中的潜力，并为未来可解释和可扩展的欺诈缓解策略研究奠定了基础。

Abstract: Document fraud poses a significant threat to industries reliant on secure and
verifiable documentation, necessitating robust detection mechanisms. This study
investigates the efficacy of state-of-the-art multi-modal large language models
(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,
Grok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and
3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against
each other and prior work on document fraud detection techniques using a
standard dataset with real transactional documents. Through prompt optimization
and detailed analysis of the models' reasoning processes, we evaluate their
ability to identify subtle indicators of fraud, such as tampered text,
misaligned formatting, and inconsistent transactional sums. Our results reveal
that top-performing multi-modal LLMs demonstrate superior zero-shot
generalization, outperforming conventional methods on out-of-distribution
datasets, while several vision LLMs exhibit inconsistent or subpar performance.
Notably, model size and advanced reasoning capabilities show limited
correlation with detection accuracy, suggesting task-specific fine-tuning is
critical. This study underscores the potential of multi-modal LLMs in enhancing
document fraud detection systems and provides a foundation for future research
into interpretable and scalable fraud mitigation strategies.

</details>


### [26] [Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception](https://arxiv.org/abs/2508.11256)
*Junjie Wang,Keyu Chen,Yulin Li,Bin Chen,Hengshuang Zhao,Xiaojuan Qi,Zhuotao Tian*

Main category: cs.CV

TL;DR: DeCLIP通过解耦自注意力模块增强CLIP的局部特征表示能力，显著提升开放词汇密集感知任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有密集视觉感知任务受限于预定义类别，而CLIP等视觉语言模型在密集感知中表现不佳，主要因其局部特征表示能力有限。

Method: 提出了DeCLIP框架，通过解耦自注意力模块获取“内容”和“上下文”特征，并分别通过视觉基础模型和扩散模型增强语义相关性和空间一致性。

Result: DeCLIP在多种任务（如2D检测、分割、3D实例分割等）中均达到最先进性能。

Conclusion: DeCLIP通过解耦自注意力模块并分别增强内容和上下文特征，显著提升了CLIP在密集感知任务中的性能，为开放词汇密集感知建立了坚实基础。

Abstract: Dense visual perception tasks have been constrained by their reliance on
predefined categories, limiting their applicability in real-world scenarios
where visual concepts are unbounded. While Vision-Language Models (VLMs) like
CLIP have shown promise in open-vocabulary tasks, their direct application to
dense perception often leads to suboptimal performance due to limitations in
local feature representation. In this work, we present our observation that
CLIP's image tokens struggle to effectively aggregate information from
spatially or semantically related regions, resulting in features that lack
local discriminability and spatial consistency. To address this issue, we
propose DeCLIP, a novel framework that enhances CLIP by decoupling the
self-attention module to obtain ``content'' and ``context'' features
respectively. \revise{The context features are enhanced by jointly distilling
semantic correlations from Vision Foundation Models (VFMs) and object integrity
cues from diffusion models, thereby enhancing spatial consistency. In parallel,
the content features are aligned with image crop representations and
constrained by region correlations from VFMs to improve local discriminability.
Extensive experiments demonstrate that DeCLIP establishes a solid foundation
for open-vocabulary dense perception, consistently achieving state-of-the-art
performance across a broad spectrum of tasks, including 2D detection and
segmentation, 3D instance segmentation, video instance segmentation, and 6D
object pose estimation.} Code is available at
https://github.com/xiaomoguhz/DeCLIP

</details>


### [27] [MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation](https://arxiv.org/abs/2508.11032)
*Yanwu Yang,Guinan Su,Jiesi Hu,Francesco Sammarco,Jonas Geiping,Thomas Wolfers*

Main category: cs.CV

TL;DR: MedSAMix是一种无需训练的模型合并方法，通过结合通用和专用模型优势，显著提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像分割模型（如MedSAM）受限于数据异质性、标注稀缺和分布偏移，难以广泛泛化。MedSAMix旨在结合通用和专用模型的优势，克服这些限制。

Method: 提出了一种基于零阶优化的自动层合并方法，并开发了单任务优化和多目标优化两种策略，以满足不同场景的需求。

Result: 在25个医学分割任务上的评估显示，MedSAMix在专用任务上提升了6.67%，在多任务评估中提升了4.37%，有效减少了模型偏差。

Conclusion: MedSAMix通过整合通用模型（如SAM）和专用模型（如MedSAM）的优势，提出了一种无需训练即可合并模型的方法，显著提升了医学图像分割任务的性能，特别是在领域专用准确性和泛化能力方面。

Abstract: Universal medical image segmentation models have emerged as a promising
paradigm due to their strong generalizability across diverse tasks, showing
great potential for a wide range of clinical applications. This potential has
been partly driven by the success of general-purpose vision models such as the
Segment Anything Model (SAM), which has inspired the development of various
fine-tuned variants for medical segmentation tasks. However, fine-tuned
variants like MedSAM are trained on comparatively limited medical imaging data
that often suffers from heterogeneity, scarce annotations, and distributional
shifts. These challenges limit their ability to generalize across a wide range
of medical segmentation tasks. In this regard, we propose MedSAMix, a
training-free model merging method that integrates the strengths of both
generalist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical
image segmentation. In contrast to traditional model merging approaches that
rely on manual configuration and often result in suboptimal outcomes, we
propose a zero-order optimization method to automatically discover optimal
layer-wise merging solutions. Furthermore, for clinical applications, we
develop two regimes to meet the demand of domain-specificity and
generalizability in different scenarios by single-task optimization and
multi-objective optimization respectively. Extensive evaluations on 25 medical
segmentation tasks demonstrate that MedSAMix effectively mitigates model bias
and consistently improves performance in both domain-specific accuracy and
generalization, achieving improvements of 6.67% on specialized tasks and 4.37%
on multi-task evaluations.

</details>


### [28] [Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset](https://arxiv.org/abs/2508.11058)
*Wentao Mo,Qingchao Chen,Yuxin Peng,Siyuan Huang,Yang Liu*

Main category: cs.CV

TL;DR: 提出MV-ScanQA和TripAlign数据集解决3D VL学习中的多视角推理问题，LEGO方法在多个任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有3D VL数据集在远距离对象和多视角推理方面存在局限，阻碍了深度、多视角3D场景理解模型的发展。

Method: 提出了LEGO基线方法，利用TripAlign数据集将预训练的2D LVLMs知识迁移到3D领域。

Result: MV-ScanQA和TripAlign数据集的引入显著提升了多视角组合推理能力，LEGO方法在多个基准测试中表现优异。

Conclusion: LEGO模型在TripAlign数据集上预训练后，不仅在MV-ScanQA上表现优异，还在现有3D密集描述和问答基准测试中达到了最先进水平。

Abstract: The advancement of 3D vision-language (3D VL) learning is hindered by several
limitations in existing 3D VL datasets: they rarely necessitate reasoning
beyond a close range of objects in single viewpoint, and annotations often link
instructions to single objects, missing richer contextual alignments between
multiple objects. This significantly curtails the development of models capable
of deep, multi-view 3D scene understanding over distant objects. To address
these challenges, we introduce MV-ScanQA, a novel 3D question answering dataset
where 68% of questions explicitly require integrating information from multiple
views (compared to less than 7% in existing datasets), thereby rigorously
testing multi-view compositional reasoning. To facilitate the training of
models for such demanding scenarios, we present TripAlign dataset, a
large-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D
view, set of 3D objects, text> triplets that explicitly aligns groups of
contextually related objects with text, providing richer, view-grounded
multi-object multimodal alignment signals than previous single-object
annotations. We further develop LEGO, a baseline method for the multi-view
reasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D
LVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign
achieves state-of-the-art performance not only on the proposed MV-ScanQA, but
also on existing benchmarks for 3D dense captioning and question answering.
Datasets and code are available at
https://matthewdm0816.github.io/tripalign-mvscanqa.

</details>


### [29] [Vision-Language Models display a strong gender bias](https://arxiv.org/abs/2508.11262)
*Aiswarya Konavoor,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CV

TL;DR: 研究揭示视觉语言模型在职业和活动描述中隐含性别偏见，提出评估框架并发现显著关联模式。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在检索和零样本迁移中表现优异，但其对齐过程可能隐含并放大社会刻板印象，现有准确性指标难以捕捉这种微妙偏差。

Method: 研究通过计算人脸图像与描述性短语的嵌入之间的余弦相似度，定义了基于性别的关联分数，并利用自助法估计置信区间和零模型验证结果。

Result: 研究发现对比视觉语言空间中存在显著的性别关联模式，不同类别的职业和活动描述与特定性别群体的关联强度存在差异。

Conclusion: 该研究提出了一个评估对比视觉语言模型中性别偏见的稳健框架，并揭示了模型在职业和活动描述中存在的性别关联模式。

Abstract: Vision-language models (VLM) align images and text in a shared representation
space that is useful for retrieval and zero-shot transfer. Yet, this alignment
can encode and amplify social stereotypes in subtle ways that are not obvious
from standard accuracy metrics. In this study, we test whether the contrastive
vision-language encoder exhibits gender-linked associations when it places
embeddings of face images near embeddings of short phrases that describe
occupations and activities. We assemble a dataset of 220 face photographs split
by perceived binary gender and a set of 150 unique statements distributed
across six categories covering emotional labor, cognitive labor, domestic
labor, technical labor, professional roles, and physical labor. We compute
unit-norm image embeddings for every face and unit-norm text embeddings for
every statement, then define a statement-level association score as the
difference between the mean cosine similarity to the male set and the mean
cosine similarity to the female set, where positive values indicate stronger
association with the male set and negative values indicate stronger association
with the female set. We attach bootstrap confidence intervals by resampling
images within each gender group, aggregate by category with a separate
bootstrap over statements, and run a label-swap null model that estimates the
level of mean absolute association we would expect if no gender structure were
present. The outcome is a statement-wise and category-wise map of gender
associations in a contrastive vision-language space, accompanied by
uncertainty, simple sanity checks, and a robust gender bias evaluation
framework.

</details>


### [30] [Data-Driven Abdominal Phenotypes of Type 2 Diabetes in Lean, Overweight, and Obese Cohorts](https://arxiv.org/abs/2508.11063)
*Lucas W. Remedios,Chloe Choe,Trent M. Schwartz,Dingjie Su,Gaurav Rudravaram,Chenyu Gao,Aravind R. Krishnan,Adam M. Saunders,Michael E. Kim,Shunxing Bao,Alvin C. Powers,Bennett A. Landman,John Virostko*

Main category: cs.CV

TL;DR: 研究利用AI分析腹部CT扫描，发现2型糖尿病的腹部特征在不同体重类别中一致，包括脂肪性骨骼肌、较多内脏脂肪和较小胰腺等。


<details>
  <summary>Details</summary>
Motivation: 尽管高BMI是2型糖尿病的已知风险因素，但该疾病在某些瘦弱成年人中的存在和在其他肥胖者中的缺失表明，详细的身体组成可能揭示2型糖尿病的腹部表型。利用AI，可以从3D临床影像中大规模提取腹部结构的详细测量，从而有机会利用大规模临床数据实证定义与2型糖尿病风险和保护相关的身体组成特征。

Method: 通过分割将腹部扫描转化为可解释的测量集合，使用交叉验证的随机森林分类2型糖尿病，通过SHAP分析测量特征对模型估计风险或保护的影响，通过聚类从SHAP中共享模型决策模式，并链接回解剖学差异。

Result: 随机森林模型的平均AUC为0.72-0.74。在每个组中发现了共享的2型糖尿病特征：脂肪性骨骼肌、年龄较大、内脏和皮下脂肪较多，以及较小或脂肪过多的胰腺。单变量逻辑回归确认了每个亚组中14-18个前20预测因子的方向（p < 0.05）。

Conclusion: 研究结果表明，腹部特征对2型糖尿病的驱动作用在不同体重类别中可能是一致的。

Abstract: Purpose: Although elevated BMI is a well-known risk factor for type 2
diabetes, the disease's presence in some lean adults and absence in others with
obesity suggests that detailed body composition may uncover abdominal
phenotypes of type 2 diabetes. With AI, we can now extract detailed
measurements of size, shape, and fat content from abdominal structures in 3D
clinical imaging at scale. This creates an opportunity to empirically define
body composition signatures linked to type 2 diabetes risk and protection using
large-scale clinical data. Approach: To uncover BMI-specific diabetic abdominal
patterns from clinical CT, we applied our design four times: once on the full
cohort (n = 1,728) and once on lean (n = 497), overweight (n = 611), and obese
(n = 620) subgroups separately. Briefly, our experimental design transforms
abdominal scans into collections of explainable measurements through
segmentation, classifies type 2 diabetes through a cross-validated random
forest, measures how features contribute to model-estimated risk or protection
through SHAP analysis, groups scans by shared model decision patterns
(clustering from SHAP) and links back to anatomical differences
(classification). Results: The random-forests achieved mean AUCs of 0.72-0.74.
There were shared type 2 diabetes signatures in each group; fatty skeletal
muscle, older age, greater visceral and subcutaneous fat, and a smaller or
fat-laden pancreas. Univariate logistic regression confirmed the direction of
14-18 of the top 20 predictors within each subgroup (p < 0.05). Conclusions:
Our findings suggest that abdominal drivers of type 2 diabetes may be
consistent across weight classes.

</details>


### [31] [Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering](https://arxiv.org/abs/2508.11272)
*Jun Li,Kai Li,Shaoguo Liu,Tingting Gao*

Main category: cs.CV

TL;DR: PMTFR框架通过Pyramid Patcher和Training-Free Refinement提升监督式CIR性能，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在CIR任务中面临需要额外训练排名模型或仅适用于零样本CIR的局限性，PMTFR旨在通过无训练细化范式解决这些挑战。

Method: 提出了包含Pyramid Matching Model with Training-Free Refinement (PMTFR)的框架，利用Pyramid Patcher模块增强视觉信息理解，并通过从CoT数据中提取表示注入LVLMs以实现无训练细化。

Result: 在CIR基准测试中，PMTFR在监督式CIR任务中表现优于现有最先进方法。

Conclusion: PMTFR框架通过Pyramid Patcher模块和Training-Free Refinement范式，显著提升了监督式CIR任务的性能，超越了现有最先进方法。

Abstract: Composed Image Retrieval (CIR) presents a significant challenge as it
requires jointly understanding a reference image and a modified textual
instruction to find relevant target images. Some existing methods attempt to
use a two-stage approach to further refine retrieval results. However, this
often requires additional training of a ranking model. Despite the success of
Chain-of-Thought (CoT) techniques in reducing training costs for language
models, their application in CIR tasks remains limited -- compressing visual
information into text or relying on elaborate prompt designs. Besides, existing
works only utilize it for zero-shot CIR, as it is challenging to achieve
satisfactory results in supervised CIR with a well-trained model. In this work,
we proposed a framework that includes the Pyramid Matching Model with
Training-Free Refinement (PMTFR) to address these challenges. Through a simple
but effective module called Pyramid Patcher, we enhanced the Pyramid Matching
Model's understanding of visual information at different granularities.
Inspired by representation engineering, we extracted representations from COT
data and injected them into the LVLMs. This approach allowed us to obtain
refined retrieval scores in the Training-Free Refinement paradigm without
relying on explicit textual reasoning, further enhancing performance. Extensive
experiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art
methods in supervised CIR tasks. The code will be made public.

</details>


### [32] [HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing](https://arxiv.org/abs/2508.11106)
*Xinjie Gao,Bi'an Du,Wei Hu*

Main category: cs.CV

TL;DR: HierOctFusion通过分层生成和部分感知机制，高效生成精细3D结构，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法将3D对象视为整体，忽略语义部分层次结构，且高分辨率建模计算成本高，而真实对象具有稀疏性和层次性。

Method: 提出HierOctFusion，一种基于部分感知的多尺度八叉树扩散模型，结合跨注意力条件机制和分层特征交互。

Result: 实验表明，HierOctFusion在形状质量和效率上优于现有方法。

Conclusion: HierOctFusion通过多尺度八叉树扩散模型和跨注意力条件机制，显著提升了3D内容生成的精细度和效率，优于现有方法。

Abstract: 3D content generation remains a fundamental yet challenging task due to the
inherent structural complexity of 3D data. While recent octree-based diffusion
models offer a promising balance between efficiency and quality through
hierarchical generation, they often overlook two key insights: 1) existing
methods typically model 3D objects as holistic entities, ignoring their
semantic part hierarchies and limiting generalization; and 2) holistic
high-resolution modeling is computationally expensive, whereas real-world
objects are inherently sparse and hierarchical, making them well-suited for
layered generation. Motivated by these observations, we propose HierOctFusion,
a part-aware multi-scale octree diffusion model that enhances hierarchical
feature interaction for generating fine-grained and sparse object structures.
Furthermore, we introduce a cross-attention conditioning mechanism that injects
part-level information into the generation process, enabling semantic features
to propagate effectively across hierarchical levels from parts to the whole.
Additionally, we construct a 3D dataset with part category annotations using a
pre-trained segmentation model to facilitate training and evaluation.
Experiments demonstrate that HierOctFusion achieves superior shape quality and
efficiency compared to prior methods.

</details>


### [33] [UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring](https://arxiv.org/abs/2508.11115)
*Haotang Li,Zhenyu Qi,Sen He,Kebin Peng,Sheng Tan,Yili Ren,Tomas Cerny,Jiyue Zhao,Zi Wang*

Main category: cs.CV

TL;DR: UWB-PostureGuard是一种基于UWB的隐私保护坐姿监测系统，通过PoseGBDT模型实现高精度监测，适用于现实环境。


<details>
  <summary>Details</summary>
Motivation: 传统姿势监测解决方案存在隐私问题和用户不适，需要一种隐私保护且无接触的监测方法。

Method: 利用商业UWB设备，通过全面的特征工程提取多种人体工学坐姿特征，并开发PoseGBDT以有效捕捉姿势模式的时间依赖性。

Result: 在10名参与者和19种不同姿势的广泛实际评估中，系统表现出色，准确率达到99.11%，并对环境变量保持鲁棒性。

Conclusion: UWB-PostureGuard系统通过超宽带（UWB）技术提供了一种可扩展、保护隐私的移动健康解决方案，以低成本改善了生活质量。

Abstract: Improper sitting posture during prolonged computer use has become a
significant public health concern. Traditional posture monitoring solutions
face substantial barriers, including privacy concerns with camera-based systems
and user discomfort with wearable sensors. This paper presents
UWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that
advances mobile technologies for preventive health management through
continuous, contactless monitoring of ergonomic sitting posture. Our system
leverages commercial UWB devices, utilizing comprehensive feature engineering
to extract multiple ergonomic sitting posture features. We develop PoseGBDT to
effectively capture temporal dependencies in posture patterns, addressing
limitations of traditional frame-wise classification approaches. Extensive
real-world evaluation across 10 participants and 19 distinct postures
demonstrates exceptional performance, achieving 99.11% accuracy while
maintaining robustness against environmental variables such as clothing
thickness, additional devices, and furniture configurations. Our system
provides a scalable, privacy-preserving mobile health solution on existing
platforms for proactive ergonomic management, improving quality of life at low
costs.

</details>


### [34] [Leveraging the RETFound foundation model for optic disc segmentation in retinal images](https://arxiv.org/abs/2508.11354)
*Zhenyi Zhao,Muthu Rama Krishnan Mookiah,Emanuele Trucco*

Main category: cs.CV

TL;DR: RETFound基础模型首次适配于视盘分割任务，仅需少量示例训练头部网络，即在多个数据集上达到约96% Dice系数，性能超越现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 探索RETFound基础模型在视盘分割这一视网膜图像分析基础任务中的应用潜力。

Method: 通过训练一个头部网络，仅使用少量任务特定示例，将RETFound适配于视盘分割任务。

Result: 在四个公共数据集（IDRID、Drishti-GS、RIM-ONE-r3、REFUGE）和一个私有数据集（GoDARTS）上，Dice系数达到约96%，性能优于大多数最先进基线。

Conclusion: RETFound作为基础模型在视盘分割任务中表现出色，不仅超越了特定任务的最先进基线网络，还在内部验证、领域泛化和领域适应方面展现了卓越性能。

Abstract: RETFound is a well-known foundation model (FM) developed for fundus camera
and optical coherence tomography images. It has shown promising performance
across multiple datasets in diagnosing diseases, both eye-specific and
systemic, from retinal images. However, to our best knowledge, it has not been
used for other tasks. We present the first adaptation of RETFound for optic
disc segmentation, a ubiquitous and foundational task in retinal image
analysis. The resulting segmentation system outperforms state-of-the-art,
segmentation-specific baseline networks after training a head with only a very
modest number of task-specific examples. We report and discuss results with
four public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private
dataset, GoDARTS, achieving about 96% Dice consistently across all datasets.
Overall, our method obtains excellent performance in internal verification,
domain generalization and domain adaptation, and exceeds most of the
state-of-the-art baseline results. We discuss the results in the framework of
the debate about FMs as alternatives to task-specific architectures. The code
is available at: [link to be added after the paper is accepted]

</details>


### [35] [Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation](https://arxiv.org/abs/2508.11134)
*Bing Liu,Le Wang,Hao Liu,Mingming Liu*

Main category: cs.CV

TL;DR: 提出RBDM模型，实现有雾和无雾图像的双向转换，性能优越。


<details>
  <summary>Details</summary>
Motivation: 当前深度去雾方法仅关注从有雾图像中去除雾，缺乏在有雾和无雾图像之间进行转换的能力。

Method: 提出了基于残差的高效双向扩散模型（RBDM），设计了双马尔可夫链来有效转移残差并促进双向平滑过渡，通过在单个时间步扰动有雾和无雾图像并预测扰动数据中的噪声来同时学习条件分布。

Result: RBDM在合成和真实世界数据集上表现优于或至少与现有最先进方法相当。

Conclusion: RBDM成功实现了在仅有15个采样步骤的情况下，在无雾和有雾图像之间进行大小无关的双向转换，并在合成和真实世界数据集上表现优于或至少与现有最先进方法相当。

Abstract: Current deep dehazing methods only focus on removing haze from hazy images,
lacking the capability to translate between hazy and haze-free images. To
address this issue, we propose a residual-based efficient bidirectional
diffusion model (RBDM) that can model the conditional distributions for both
dehazing and haze generation. Firstly, we devise dual Markov chains that can
effectively shift the residuals and facilitate bidirectional smooth transitions
between them. Secondly, the RBDM perturbs the hazy and haze-free images at
individual timesteps and predicts the noise in the perturbed data to
simultaneously learn the conditional distributions. Finally, to enhance
performance on relatively small datasets and reduce computational costs, our
method introduces a unified score function learned on image patches instead of
entire images. Our RBDM successfully implements size-agnostic bidirectional
transitions between haze-free and hazy images with only 15 sampling steps.
Extensive experiments demonstrate that the proposed method achieves superior or
at least comparable performance to state-of-the-art methods on both synthetic
and real-world datasets.

</details>


### [36] [Does the Skeleton-Recall Loss Really Work?](https://arxiv.org/abs/2508.11374)
*Devansh Arora,Nitin Kumar,Sukrit Gupta*

Main category: cs.CV

TL;DR: 本研究分析了SRL损失的理论梯度并在多个数据集上测试其性能，发现其并未优于传统方法，揭示了拓扑保持损失函数的局限性。


<details>
  <summary>Details</summary>
Motivation: 图像分割是计算机视觉中的重要任务，针对薄管状结构的有效分割需要定制模型架构和损失函数。拓扑保持损失函数（如SRL）声称能生成更精确的分割掩码，但本研究质疑其实际效果。

Method: 对SRL损失的梯度进行了理论分析，并在多个管状数据集上比较了SRL与传统基线模型的性能。

Result: 研究发现，基于SRL的分割模型性能并未超过传统基线模型。

Conclusion: 本研究通过理论分析和实证证据，批判性地评估了基于拓扑的损失函数（如SRL）的局限性，为开发更有效的复杂管状结构分割模型提供了有价值的见解。

Abstract: Image segmentation is an important and widely performed task in computer
vision. Accomplishing effective image segmentation in diverse settings often
requires custom model architectures and loss functions. A set of models that
specialize in segmenting thin tubular structures are topology
preservation-based loss functions. These models often utilize a pixel
skeletonization process claimed to generate more precise segmentation masks of
thin tubes and better capture the structures that other models often miss. One
such model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\cite
{kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark
tubular datasets. In this work, we performed a theoretical analysis of the
gradients for the SRL loss. Upon comparing the performance of the proposed
method on some of the tubular datasets (used in the original work, along with
some additional datasets), we found that the performance of SRL-based
segmentation models did not exceed traditional baseline models. By providing
both a theoretical explanation and empirical evidence, this work critically
evaluates the limitations of topology-based loss functions, offering valuable
insights for researchers aiming to develop more effective segmentation models
for complex tubular structures.

</details>


### [37] [G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration](https://arxiv.org/abs/2508.11379)
*Ramil Khafizov,Artem Komarichev,Ruslan Rakhimov,Peter Wonka,Evgeny Burnaev*

Main category: cs.CV

TL;DR: G-CUT3R通过整合多模态先验信息优化3D重建，性能显著提升且保持输入灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有前馈方法仅依赖输入图像，而G-CUT3R利用实际场景中常见的辅助数据（如深度、相机校准或位置）来提升重建效果。

Method: 提出了一种轻量级修改CUT3R的方法，为每种模态设计专用编码器提取特征，并通过零卷积与RGB图像令牌融合。

Result: 在多个基准测试中，G-CUT3R表现出显著性能提升，证明了其有效利用先验信息的能力。

Conclusion: G-CUT3R通过整合先验信息显著提升了3D场景重建的性能，同时保持了与不同输入模态的兼容性。

Abstract: We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene
reconstruction that enhances the CUT3R model by integrating prior information.
Unlike existing feed-forward methods that rely solely on input images, our
method leverages auxiliary data, such as depth, camera calibrations, or camera
positions, commonly available in real-world scenarios. We propose a lightweight
modification to CUT3R, incorporating a dedicated encoder for each modality to
extract features, which are fused with RGB image tokens via zero convolution.
This flexible design enables seamless integration of any combination of prior
information during inference. Evaluated across multiple benchmarks, including
3D reconstruction and other multi-view tasks, our approach demonstrates
significant performance improvements, showing its ability to effectively
utilize available priors while maintaining compatibility with varying input
modalities.

</details>


### [38] [LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction](https://arxiv.org/abs/2508.11153)
*Maoquan Zhang,Bisser Raytchev,Xiujuan Sun*

Main category: cs.CV

TL;DR: LEARN是一个布局感知的扩散框架，专为STEM教育生成教学对齐的插图，通过布局条件和语义训练减少认知负荷，支持中高级推理。


<details>
  <summary>Details</summary>
Motivation: 设计LEARN框架旨在为STEM教育生成教学对齐的插图，以支持布鲁姆分类学中的中高级推理，并减少认知负荷。

Method: 通过布局条件生成、对比视觉语义训练和提示调制，LEARN生成连贯的视觉序列。

Result: LEARN能够生成空间组织且故事驱动的叙事，对抗短媒体引发的注意力碎片化，促进持续的概念聚焦。

Conclusion: LEARN作为首个结合布局叙事、语义结构学习和认知支架的生成方法，为教育领域的生成AI开辟了新方向。代码和数据集将公开以促进未来研究和实际应用。

Abstract: LEARN is a layout-aware diffusion framework designed to generate
pedagogically aligned illustrations for STEM education. It leverages a curated
BookCover dataset that provides narrative layouts and structured visual cues,
enabling the model to depict abstract and sequential scientific concepts with
strong semantic alignment. Through layout-conditioned generation, contrastive
visual-semantic training, and prompt modulation, LEARN produces coherent visual
sequences that support mid-to-high-level reasoning in line with Bloom's
taxonomy while reducing extraneous cognitive load as emphasized by Cognitive
Load Theory. By fostering spatially organized and story-driven narratives, the
framework counters fragmented attention often induced by short-form media and
promotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates
potential for integration with multimodal systems and curriculum-linked
knowledge graphs to create adaptive, exploratory educational content. As the
first generative approach to unify layout-based storytelling, semantic
structure learning, and cognitive scaffolding, LEARN represents a novel
direction for generative AI in education. The code and dataset will be released
to facilitate future research and practical deployment.

</details>


### [39] [Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models](https://arxiv.org/abs/2508.11165)
*Bing Liu,Le Wang,Mingming Liu,Hao Liu,Rui Yao,Yong Zhou,Peng Liu,Tongqiang Xia*

Main category: cs.CV

TL;DR: 提出EM-B3DM方法，通过两阶段学习方案和细节增强模块，显著提升图像去雾性能。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法在处理真实世界雾霾图像时存在困难，尤其是浓雾场景，主要原因在于缺乏真实世界配对数据和鲁棒先验。

Method: 提出了一种基于期望最大化和双向布朗桥扩散模型（EM-B3DM）的两阶段学习方案，包括EM算法解耦联合分布和利用预训练模型及大规模未配对数据提升性能。

Result: EM-B3DM在合成和真实世界数据集上均表现出优异性能。

Conclusion: EM-B3DM在合成和真实世界数据集上均表现出优于或至少可与现有最先进方法相媲美的性能。

Abstract: Existing dehazing methods deal with real-world haze images with difficulty,
especially scenes with thick haze. One of the main reasons is the lack of
real-world paired data and robust priors. To avoid the costly collection of
paired hazy and clear images, we propose an efficient semi-supervised image
dehazing method via Expectation-Maximization and Bidirectional Brownian Bridge
Diffusion Models (EM-B3DM) with a two-stage learning scheme. In the first
stage, we employ the EM algorithm to decouple the joint distribution of paired
hazy and clear images into two conditional distributions, which are then
modeled using a unified Brownian Bridge diffusion model to directly capture the
structural and content-related correlations between hazy and clear images. In
the second stage, we leverage the pre-trained model and large-scale unpaired
hazy and clear images to further improve the performance of image dehazing.
Additionally, we introduce a detail-enhanced Residual Difference Convolution
block (RDC) to capture gradient-level information, significantly enhancing the
model's representation capability. Extensive experiments demonstrate that our
EM-B3DM achieves superior or at least comparable performance to
state-of-the-art methods on both synthetic and real-world datasets.

</details>


### [40] [VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images](https://arxiv.org/abs/2508.11167)
*Jianhong Han,Yupei Wang,Liang Chen*

Main category: cs.CV

TL;DR: VG-DETR是一种基于视觉基础模型的半监督框架，用于源自由遥感目标检测，通过伪标签挖掘和双级对齐提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决源自由目标检测（SFOD）在遥感图像中因伪标签噪声和复杂背景导致的训练崩溃问题，同时利用少量标注目标数据提升模型性能。

Method: VG-DETR采用视觉基础模型引导的伪标签挖掘策略和双级VFM引导对齐方法，通过对比学习和特征图相似性匹配增强特征表示的鲁棒性。

Result: 实验证明VG-DETR在源自由遥感检测任务中表现优异，有效提升了伪标签质量和特征提取能力。

Conclusion: VG-DETR通过整合视觉基础模型（VFM）和半监督框架，显著提高了源自由目标检测在遥感图像中的性能，尤其是在处理伪标签噪声和特征提取方面。

Abstract: Unsupervised domain adaptation methods have been widely explored to bridge
domain gaps. However, in real-world remote-sensing scenarios, privacy and
transmission constraints often preclude access to source domain data, which
limits their practical applicability. Recently, Source-Free Object Detection
(SFOD) has emerged as a promising alternative, aiming at cross-domain
adaptation without relying on source data, primarily through a self-training
paradigm. Despite its potential, SFOD frequently suffers from training collapse
caused by noisy pseudo-labels, especially in remote sensing imagery with dense
objects and complex backgrounds. Considering that limited target domain
annotations are often feasible in practice, we propose a Vision
foundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised
framework for SFOD in remote sensing images. VG-DETR integrates a Vision
Foundation Model (VFM) into the training pipeline in a "free lunch" manner,
leveraging a small amount of labeled target data to mitigate pseudo-label noise
while improving the detector's feature-extraction capability. Specifically, we
introduce a VFM-guided pseudo-label mining strategy that leverages the VFM's
semantic priors to further assess the reliability of the generated
pseudo-labels. By recovering potentially correct predictions from
low-confidence outputs, our strategy improves pseudo-label quality and
quantity. In addition, a dual-level VFM-guided alignment method is proposed,
which aligns detector features with VFM embeddings at both the instance and
image levels. Through contrastive learning among fine-grained prototypes and
similarity matching between feature maps, this dual-level alignment further
enhances the robustness of feature representations against domain gaps.
Extensive experiments demonstrate that VG-DETR achieves superior performance in
source-free remote sensing detection tasks.

</details>


### [41] [Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation](https://arxiv.org/abs/2508.11446)
*Daniel Airinei,Elena Burceanu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出仅依赖视觉的高效室内导航深度学习方法，含数据集和Android应用。


<details>
  <summary>Details</summary>
Motivation: 解决室内导航中GPS信号弱、现有解决方案复杂且难以部署的问题。

Method: 基于新颖的图基路径生成方法，结合可解释的数据增强和课程学习，使数据收集、标注和训练过程尽可能自动化、高效且鲁棒。

Result: 提出了一个大型购物中心视频数据集，每帧标注了朝向不同目标的下一个正确方向，并开发了易于使用的Android应用。

Conclusion: 该论文提出了一种仅依赖视觉输入的高效、实时且易于部署的深度学习室内导航方法，避免了传统方法对特殊传感器、标记或场景地图的依赖，并提供了公开的数据集和Android应用。

Abstract: Indoor navigation is a difficult task, as it generally comes with poor GPS
access, forcing solutions to rely on other sources of information. While
significant progress continues to be made in this area, deployment to
production applications is still lacking, given the complexity and additional
requirements of current solutions. Here, we introduce an efficient, real-time
and easily deployable deep learning approach, based on visual input only, that
can predict the direction towards a target from images captured by a mobile
device. Our technical approach, based on a novel graph-based path generation
method, combined with explainable data augmentation and curriculum learning,
includes contributions that make the process of data collection, annotation and
training, as automatic as possible, efficient and robust. On the practical
side, we introduce a novel largescale dataset, with video footage inside a
relatively large shopping mall, in which each frame is annotated with the
correct next direction towards different specific target destinations.
Different from current methods, ours relies solely on vision, avoiding the need
of special sensors, additional markers placed along the path, knowledge of the
scene map or internet access. We also created an easy to use application for
Android, which we plan to make publicly available. We make all our data and
code available along with visual demos on our project site

</details>


### [42] [Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models](https://arxiv.org/abs/2508.11499)
*Erez Meoded*

Main category: cs.CV

TL;DR: 研究通过TrOCR模型和新型数据增强技术，显著提升了16世纪拉丁手稿的识别准确率，展示了领域特定方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决历史手写文本识别中的转录稀缺、语言变异和手写风格多样性问题，以释放档案文件的文化和学术价值。

Method: 应用TrOCR模型，结合目标图像预处理和多种数据增强技术（包括四种新方法），并评估集成学习策略。

Result: 在Gwalther数据集上，最佳单模型增强（Elastic）的CER为1.86，而top-5投票集成的CER为1.60，分别比最佳TrOCR_BASE结果和先前最优结果提升了50%和42%。

Conclusion: 结果表明，针对历史手写特性的领域特定数据增强和集成策略显著提升了HTR性能，为历史手稿的数字化提供了有效方法。

Abstract: Historical handwritten text recognition (HTR) is essential for unlocking the
cultural and scholarly value of archival documents, yet digitization is often
hindered by scarce transcriptions, linguistic variation, and highly diverse
handwriting styles. In this study, we apply TrOCR, a state-of-the-art
transformer-based HTR model, to 16th-century Latin manuscripts authored by
Rudolf Gwalther. We investigate targeted image preprocessing and a broad suite
of data augmentation techniques, introducing four novel augmentation methods
designed specifically for historical handwriting characteristics. We also
evaluate ensemble learning approaches to leverage the complementary strengths
of augmentation-trained models. On the Gwalther dataset, our best single-model
augmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a
top-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative
improvement over the best reported TrOCR_BASE result and a 42% improvement over
the previous state of the art. These results highlight the impact of
domain-specific augmentations and ensemble strategies in advancing HTR
performance for historical manuscripts.

</details>


### [43] [Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery](https://arxiv.org/abs/2508.11173)
*Ruobing Jiang,Yang Liu,Haobing Liu,Yanwei Yu,Chunyang Wang*

Main category: cs.CV

TL;DR: IDOD通过独立多样性增强、联合新颖性发现和正交性增量模块，有效解决持续类别发现中的问题，在细粒度数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 持续类别发现（CCD）面临无标签数据中新类别数量未知、灾难性遗忘及错误积累等挑战，现有方法无法有效平衡新类别发现与分类，且存储开销大。

Method: IDOD主要包括独立多样性增强模块、联合新颖性发现模块和正交性增量模块，分别用于避免分类特征过度集中、减少错误积累影响以及通过正交原型降低存储开销。

Result: 实验结果表明，IDOD在细粒度数据集上优于现有技术。

Conclusion: IDOD方法在细粒度数据集上表现优于现有技术，通过独立多样性增强、联合新颖性发现和正交性增量模块，有效解决了持续类别发现中的矛盾与错误积累问题。

Abstract: Continuous category discovery (CCD) aims to automatically discover novel
categories in continuously arriving unlabeled data. This is a challenging
problem considering that there is no number of categories and labels in the
newly arrived data, while also needing to mitigate catastrophic forgetting.
Most CCD methods cannot handle the contradiction between novel class discovery
and classification well. They are also prone to accumulate errors in the
process of gradually discovering novel classes. Moreover, most of them use
knowledge distillation and data replay to prevent forgetting, occupying more
storage space. To address these limitations, we propose Independence-based
Diversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes
independent enrichment of diversity module, joint discovery of novelty module,
and continuous increment by orthogonality module. In independent enrichment,
the backbone is trained separately using contrastive loss to avoid it focusing
only on features for classification. Joint discovery transforms multi-stage
novel class discovery into single-stage, reducing error accumulation impact.
Continuous increment by orthogonality module generates mutually orthogonal
prototypes for classification and prevents forgetting with lower space overhead
via representative representation replay. Experimental results show that on
challenging fine-grained datasets, our method outperforms the state-of-the-art
methods.

</details>


### [44] [Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning](https://arxiv.org/abs/2508.11176)
*Yumiao Zhao,Bo Jiang,Yuhe Ding,Xiao Wang,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: LatHAdapter是一种新型适配器，利用双曲空间和潜在语义层次结构优化视觉-语言模型的少样本分类性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有适配器方法通过嵌入空间中的显式空间邻近性对齐视觉和文本模态，未能捕捉类别与图像样本之间的一对多关联，且在未知类别和图像之间建立准确关联方面存在困难。

Method: LatHAdapter引入可学习的‘属性’提示作为对齐类别和图像的桥梁，并在双曲空间中投影类别、属性提示和图像，利用层次正则化学习它们的潜在语义层次结构。

Result: 在四个具有挑战性的少样本任务上的广泛实验表明，LatHAdapter在性能上持续优于许多其他微调方法，特别是在适应已知类别和泛化到未知类别方面。

Conclusion: LatHAdapter通过利用下游训练数据的潜在语义层次结构，为适配器学习过程提供了更丰富、更细粒度的指导，显著提升了在已知类别适应和未知类别泛化方面的性能。

Abstract: Adapter-based approaches have garnered attention for fine-tuning pre-trained
Vision-Language Models (VLMs) on few-shot classification tasks. These methods
strive to develop a lightweight module that better aligns visual and (category)
textual representations, thereby enhancing performance on downstream few-shot
learning tasks. However, existing adapters generally learn/align (category)
textual-visual modalities via explicit spatial proximity in the underlying
embedding space, which i) fails to capture the inherent one-to-many
associations between categories and image samples and ii) struggles to
establish accurate associations between the unknown categories and images. To
address these issues, inspired by recent works on hyperbolic learning, we
develop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs
on downstream few-shot classification tasks. The core of LatHAdapter is to
exploit the latent semantic hierarchy of downstream training data and employ it
to provide richer, fine-grained guidance for the adapter learning process.
Specifically, LatHAdapter first introduces some learnable `attribute' prompts
as the bridge to align categories and images. Then, it projects the categories,
attribute prompts, and images within each batch in a hyperbolic space, and
employs hierarchical regularization to learn the latent semantic hierarchy of
them, thereby fully modeling the inherent one-to-many associations among
categories, learnable attributes, and image samples. Extensive experiments on
four challenging few-shot tasks show that the proposed LatHAdapter consistently
outperforms many other fine-tuning approaches, particularly in adapting known
classes and generalizing to unknown classes.

</details>


### [45] [Versatile Video Tokenization with Generative 2D Gaussian Splatting](https://arxiv.org/abs/2508.11183)
*Zhenghao Chen,Zicong Chen,Lei Liu,Yiming Wu,Dong Xu*

Main category: cs.CV

TL;DR: GVT通过生成式2D高斯和时空划分策略，提升了视频标记化的时空适应性，在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频标记化方法多为固定网格和补丁方式，空间上对低信息区域过度编码，时间上难以有效区分静态和动态内容。GVT旨在解决这些问题。

Method: GVT采用生成式2D高斯平铺策略（2DGS）和时空高斯嵌入机制（STGE），通过高斯集划分（GSP）策略将2D高斯分为静态和动态集，以提升时空适应性。

Result: GVT在UCF101、Kinetics和DAVIS数据集上的实验表明，其在视频重建、动作识别和压缩任务中表现优异。

Conclusion: GVT在视频重建质量上达到了最先进的水平，在动作识别任务中超越了基线MAGVIT-v2，并在压缩性能上表现相当。

Abstract: Video tokenization procedure is critical for a wide range of video processing
tasks. Most existing approaches directly transform video into fixed-grid and
patch-wise tokens, which exhibit limited versatility. Spatially, uniformly
allocating a fixed number of tokens often leads to over-encoding in
low-information regions. Temporally, reducing redundancy remains challenging
without explicitly distinguishing between static and dynamic content. In this
work, we propose the Gaussian Video Transformer (GVT), a versatile video
tokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We
first extract latent rigid features from a video clip and represent them with a
set of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian
Embedding (STGE) mechanism in a feed-forward manner. Such generative 2D
Gaussians not only enhance spatial adaptability by assigning higher (resp.,
lower) rendering weights to regions with higher (resp., lower) information
content during rasterization, but also improve generalization by avoiding
per-video optimization.To enhance the temporal versatility, we introduce a
Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into
static and dynamic sets, which explicitly model static content shared across
different time-steps and dynamic content specific to each time-step, enabling a
compact representation.We primarily evaluate GVT on the video reconstruction,
while also assessing its performance on action recognition and compression
using the UCF101, Kinetics, and DAVIS datasets. Extensive experiments
demonstrate that GVT achieves a state-of-the-art video reconstruction quality,
outperforms the baseline MAGVIT-v2 in action recognition, and delivers
comparable compression performance.

</details>


### [46] [Controlling Multimodal LLMs via Reward-guided Decoding](https://arxiv.org/abs/2508.11616)
*Oscar Mañas,Pierluca D'Oro,Koustuv Sinha,Adriana Romero-Soriano,Michal Drozdzal,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 提出了一种通过奖励模型引导MLLMs解码的方法，提升了视觉接地的可控性和性能。


<details>
  <summary>Details</summary>
Motivation: 为了适应多样化的用户需求，需要提升MLLMs的可控性和视觉接地能力。

Method: 构建了两个独立的奖励模型，分别控制对象精度和召回率，并在解码过程中动态调整奖励函数的权重和搜索范围。

Result: 在标准对象幻觉基准测试中，该方法显著提升了可控性，并优于现有幻觉缓解方法。

Conclusion: 该方法通过奖励引导解码显著提升了MLLMs的可控性，并在视觉接地任务中表现优于现有方法。

Abstract: As Multimodal Large Language Models (MLLMs) gain widespread applicability, it
is becoming increasingly desirable to adapt them for diverse user needs. In
this paper, we study the adaptation of MLLMs through controlled decoding. To
achieve this, we introduce the first method for reward-guided decoding of MLLMs
and demonstrate its application in improving their visual grounding. Our method
involves building reward models for visual grounding and using them to guide
the MLLM's decoding process. Concretely, we build two separate reward models to
independently control the degree of object precision and recall in the model's
output. Our approach enables on-the-fly controllability of an MLLM's inference
process in two ways: first, by giving control over the relative importance of
each reward function during decoding, allowing a user to dynamically trade off
object precision for recall in image captioning tasks; second, by giving
control over the breadth of the search during decoding, allowing the user to
control the trade-off between the amount of test-time compute and the degree of
visual grounding. We evaluate our method on standard object hallucination
benchmarks, showing that it provides significant controllability over MLLM
inference, while consistently outperforming existing hallucination mitigation
methods.

</details>


### [47] [CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector](https://arxiv.org/abs/2508.11185)
*Abhinav Kumar,Yuliang Guo,Zhihao Zhang,Xinyu Huang,Liu Ren,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文提出CHARM3R，通过整合两种深度估计方法，显著提升了单目3D检测器在相机高度变化下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的单目3D检测器在相机高度变化时表现不佳，这一问题尚未得到充分研究。本文旨在解决这一不足，提升模型在未见相机高度下的泛化能力。

Method: 作者首先系统分析了相机高度变化对现有单目3D模型的影响，发现深度估计是主要影响因素。随后，他们提出CHARM3R模型，通过平均回归和基于地面的深度估计来缓解这一问题。

Result: CHARM3R在未见相机高度下的泛化能力提升了45%以上，并在CARLA数据集上实现了最先进的性能。

Conclusion: CHARM3R通过整合回归和基于地面的深度估计，显著提升了单目3D检测器在未见相机高度下的泛化能力，在CARLA数据集上达到了最先进的性能。

Abstract: Monocular 3D object detectors, while effective on data from one ego camera
height, struggle with unseen or out-of-distribution camera heights. Existing
methods often rely on Plucker embeddings, image transformations or data
augmentation. This paper takes a step towards this understudied problem by
first investigating the impact of camera height variations on state-of-the-art
(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset
with multiple camera heights, we observe that depth estimation is a primary
factor influencing performance under height variations. We mathematically prove
and also empirically observe consistent negative and positive trends in mean
depth error of regressed and ground-based depth models, respectively, under
camera height changes. To mitigate this, we propose Camera Height Robust
Monocular 3D Detector (CHARM3R), which averages both depth estimates within the
model. CHARM3R improves generalization to unseen camera heights by more than
$45\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at
https://github.com/abhi1kumar/CHARM3R

</details>


### [48] [Is ChatGPT-5 Ready for Mammogram VQA?](https://arxiv.org/abs/2508.11628)
*Qiang Li,Shansong Wang,Mingzhe Hu,Mojtaba Safari,Zachary Eidex,Xiaofeng Yang*

Main category: cs.CV

TL;DR: GPT-5在乳腺X光视觉问答任务中表现优于GPT-4o，但仍需改进以匹配人类专家水平。


<details>
  <summary>Details</summary>
Motivation: 探索通用大语言模型（LLMs）在乳腺X光视觉问答任务中的潜力，以支持乳腺癌筛查。

Method: 在四个公开乳腺X光数据集（EMBED、InBreast、CMMD、CBIS-DDSM）上系统评估GPT-5和GPT-4o模型，涵盖BI-RADS评估、异常检测和恶性分类任务。

Result: GPT-5在多个任务中表现最佳，但落后于人类专家和领域专用模型。例如，在EMBED数据集中，其密度分类准确率为56.8%，恶性分类为52.8%。

Conclusion: GPT-5在乳腺X光视觉问答任务中展现出潜力，但其性能仍不足以支持高风险的临床影像应用，需进一步领域适应和优化。

Abstract: Mammogram visual question answering (VQA) integrates image interpretation
with clinical reasoning and has potential to support breast cancer screening.
We systematically evaluated the GPT-5 family and GPT-4o model on four public
mammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,
abnormality detection, and malignancy classification tasks. GPT-5 consistently
was the best performing model but lagged behind both human experts and
domain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores
among GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),
calcification (63.5%), and malignancy (52.8%) classification. On InBreast, it
attained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%
malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection
and 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS
accuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared
with human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and
specificity (52.3%). While GPT-5 exhibits promising capabilities for screening
tasks, its performance remains insufficient for high-stakes clinical imaging
applications without targeted domain adaptation and optimization. However, the
tremendous improvements in performance from GPT-4o to GPT-5 show a promising
trend in the potential for general large language models (LLMs) to assist with
mammography VQA tasks.

</details>


### [49] [Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark](https://arxiv.org/abs/2508.11192)
*Lavisha Aggarwal,Vikas Bahirwani,Lin Li,Andrea Colaco*

Main category: cs.CV

TL;DR: 本文提出了一种自动转换单人教学视频为任务指导对话的方法，并构建了HowToDIV数据集，为未来研究提供了基准。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界任务辅助中对话-视频数据集的稀缺问题。

Method: 利用大型语言模型自动将单人教学视频转换为两人对话，并与视频片段对齐。

Result: 构建了包含507个对话、6636个问答对和24小时视频片段的HowToDIV数据集。

Conclusion: 本文提出了一种将单人教学视频转化为任务指导对话的方法，并构建了HowToDIV数据集，为未来研究提供了基准。

Abstract: Many everyday tasks ranging from fixing appliances, cooking recipes to car
maintenance require expert knowledge, especially when tasks are complex and
multi-step. Despite growing interest in AI agents, there is a scarcity of
dialogue-video datasets grounded for real world task assistance. In this paper,
we propose a simple yet effective approach that transforms single-person
instructional videos into task-guidance two-person dialogues, aligned with fine
grained steps and video-clips. Our fully automatic approach, powered by large
language models, offers an efficient alternative to the substantial cost and
effort required for human-assisted data collection. Using this technique, we
build HowToDIV, a large-scale dataset containing 507 conversations, 6636
question-answer pairs and 24 hours of videoclips across diverse tasks in
cooking, mechanics, and planting. Each session includes multi-turn conversation
where an expert teaches a novice user how to perform a task step by step, while
observing user's surrounding through a camera and microphone equipped wearable
device. We establish the baseline benchmark performance on HowToDIV dataset
through Gemma-3 model for future research on this new task of dialogues for
procedural-task assistance.

</details>


### [50] [UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning](https://arxiv.org/abs/2508.11196)
*Jiajin Guan,Haibo Mei,Bonan Zhang,Dan Liu,Yuanshuang Fu,Yue Zhang*

Main category: cs.CV

TL;DR: UAV-VL-R1是一种专为无人机航拍设计的轻量级视觉语言模型，通过混合训练方法显著提升性能，支持实时部署。


<details>
  <summary>Details</summary>
Motivation: 当前通用视觉语言模型在无人机航拍图像任务中表现不佳，主要由于高分辨率、复杂空间语义和实时性要求。

Method: 采用混合训练方法，结合监督微调（SFT）和多阶段强化学习（RL），利用GRPO算法提升结构化推理能力。

Result: UAV-VL-R1在零样本准确率上比基线模型高48.17%，甚至优于其36倍大的变体，且内存占用低，支持实时部署。

Conclusion: UAV-VL-R1是一种轻量级的视觉语言模型，专为无人机航拍图像设计，通过结合监督微调（SFT）和多阶段强化学习（RL）训练方法，显著提升了在航拍图像任务中的性能。实验表明，该模型在零样本准确率上优于基线模型，并支持在资源受限的无人机平台上实时部署。

Abstract: Recent advances in vision-language models (VLMs) have demonstrated strong
generalization in natural image tasks. However, their performance often
degrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features
high resolution, complex spatial semantics, and strict real-time constraints.
These challenges limit the applicability of general-purpose VLMs to structured
aerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a
lightweight VLM explicitly designed for aerial visual reasoning. It is trained
using a hybrid method that combines supervised fine-tuning (SFT) and
multi-stage reinforcement learning (RL). We leverage the group relative policy
optimization (GRPO) algorithm to promote structured and interpretable reasoning
through rule-guided rewards and intra-group policy alignment. To support model
training and evaluation, we introduce a high-resolution visual question
answering dataset named HRVQA-VL, which consists of 50,019 annotated samples
covering eight UAV-relevant reasoning tasks, including object counting,
transportation recognition, and spatial scene inference. Experimental results
show that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the
Qwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which
is 36x larger, on multiple tasks. Ablation studies reveal that while SFT
improves semantic alignment, it may reduce reasoning diversity in mathematical
tasks. GRPO-based RL compensates for this limitation by enhancing logical
flexibility and the robustness of inference. Additionally, UAV-VL-R1 requires
only 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with
INT8, supporting real-time deployment on resource-constrained UAV platforms.

</details>


### [51] [A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network](https://arxiv.org/abs/2508.11212)
*Zhangjian Ji,Wenjin Zhang,Shaotong Qiao,Kai Feng,Yuhua Qian*

Main category: cs.CV

TL;DR: 本文提出两阶段知识蒸馏框架，结合结构损失和渐进式图卷积网络，显著提升轻量级人体姿态估计性能，尤其在复杂场景下表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的人体姿态估计方法需要大量计算资源，而传统知识蒸馏框架未能充分利用人体关节的上下文信息，因此需要一种更高效的轻量级解决方案。

Method: 第一阶段蒸馏引入人体关节结构损失以挖掘关节间结构信息；第二阶段蒸馏利用图像引导的渐进式图卷积网络（IGP-GCN）细化初始姿态，并通过教师模型的最终输出姿态进行监督训练。

Result: 在COCO关键点和CrowdPose数据集上的实验表明，该方法优于现有的大多数先进方法，尤其在更复杂的CrowdPose数据集上性能提升更为显著。

Conclusion: 本文提出了一种新颖的从粗到细的两阶段知识蒸馏框架，用于轻量级人体姿态估计，通过在两个阶段中分别利用结构损失和渐进式图卷积网络，显著提升了模型的性能。

Abstract: Human pose estimation has been widely applied in the human-centric
understanding and generation, but most existing state-of-the-art human pose
estimation methods require heavy computational resources for accurate
predictions. In order to obtain an accurate, robust yet lightweight human pose
estimator, one feasible way is to transfer pose knowledge from a powerful
teacher model to a less-parameterized student model by knowledge distillation.
However, the traditional knowledge distillation framework does not fully
explore the contextual information among human joints. Thus, in this paper, we
propose a novel coarse-to-fine two-stage knowledge distillation framework for
human pose estimation. In the first-stage distillation, we introduce the human
joints structure loss to mine the structural information among human joints so
as to transfer high-level semantic knowledge from the teacher model to the
student model. In the second-stage distillation, we utilize an Image-Guided
Progressive Graph Convolutional Network (IGP-GCN) to refine the initial human
pose obtained from the first-stage distillation and supervise the training of
the IGP-GCN in the progressive way by the final output pose of teacher model.
The extensive experiments on the benchmark dataset: COCO keypoint and CrowdPose
datasets, show that our proposed method performs favorably against lots of the
existing state-of-the-art human pose estimation methods, especially for the
more complex CrowdPose dataset, the performance improvement of our model is
more significant.

</details>


### [52] [A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving](https://arxiv.org/abs/2508.11218)
*Jialin Li,Shuqi Wu,Ning Wang*

Main category: cs.CV

TL;DR: UMM是一个轻量级不确定性模态建模框架，通过多模态令牌映射、模态增强和跨模态学习，有效解决自动驾驶中行人重识别的模态不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 由于自动驾驶中行人重识别面临输入模态不确定或缺失的挑战，且现有大规模预训练模型计算开销大，无法在资源受限环境中部署，因此提出UMM框架。

Method: UMM框架整合了多模态令牌映射器、合成模态增强策略和跨模态线索交互学习器，利用CLIP的视觉-语言对齐能力高效融合多模态输入。

Result: 实验结果表明，UMM在不确定模态条件下实现了强鲁棒性、泛化能力和计算效率。

Conclusion: UMM框架在不确定模态条件下展现出强大的鲁棒性、泛化能力和计算效率，为自动驾驶场景中的行人重识别提供了可扩展且实用的解决方案。

Abstract: Re-Identification (ReID) is a critical technology in intelligent perception
systems, especially within autonomous driving, where onboard cameras must
identify pedestrians across views and time in real-time to support safe
navigation and trajectory prediction. However, the presence of uncertain or
missing input modalities--such as RGB, infrared, sketches, or textual
descriptions--poses significant challenges to conventional ReID approaches.
While large-scale pre-trained models offer strong multimodal semantic modeling
capabilities, their computational overhead limits practical deployment in
resource-constrained environments. To address these challenges, we propose a
lightweight Uncertainty Modal Modeling (UMM) framework, which integrates a
multimodal token mapper, synthetic modality augmentation strategy, and
cross-modal cue interactive learner. Together, these components enable unified
feature representation, mitigate the impact of missing modalities, and extract
complementary information across different data types. Additionally, UMM
leverages CLIP's vision-language alignment ability to fuse multimodal inputs
efficiently without extensive finetuning. Experimental results demonstrate that
UMM achieves strong robustness, generalization, and computational efficiency
under uncertain modality conditions, offering a scalable and practical solution
for pedestrian re-identification in autonomous driving scenarios.

</details>


### [53] [FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation](https://arxiv.org/abs/2508.11255)
*MengChao Wang,Qiang Wang,Fan Jiang,Mu Xu*

Main category: cs.CV

TL;DR: 介绍了Talking-Critic和TLPO框架，通过多维奖励模型和专家模块融合，显著提升了音频驱动肖像动画的多维偏好对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多维人类偏好（如运动自然度、唇同步准确度和视觉质量）的对齐上存在困难，且缺乏大规模高质量的多维偏好标注数据集。

Method: 提出了TLPO框架，通过将偏好分解为专家模块并在时间步和网络层间融合，实现多维度的精细优化。

Result: 实验表明，Talking-Critic在人类偏好评分对齐上显著优于现有方法，TLPO在多个维度上均实现了显著提升。

Conclusion: TLPO框架在唇同步准确度、运动自然度和视觉质量上均显著优于基线模型，展示了在多维偏好对齐方面的优越性能。

Abstract: Recent advances in audio-driven portrait animation have demonstrated
impressive capabilities. However, existing methods struggle to align with
fine-grained human preferences across multiple dimensions, such as motion
naturalness, lip-sync accuracy, and visual quality. This is due to the
difficulty of optimizing among competing preference objectives, which often
conflict with one another, and the scarcity of large-scale, high-quality
datasets with multidimensional preference annotations. To address these, we
first introduce Talking-Critic, a multimodal reward model that learns
human-aligned reward functions to quantify how well generated videos satisfy
multidimensional expectations. Leveraging this model, we curate Talking-NSQ, a
large-scale multidimensional human preference dataset containing 410K
preference pairs. Finally, we propose Timestep-Layer adaptive multi-expert
Preference Optimization (TLPO), a novel framework for aligning diffusion-based
portrait animation models with fine-grained, multidimensional preferences. TLPO
decouples preferences into specialized expert modules, which are then fused
across timesteps and network layers, enabling comprehensive, fine-grained
enhancement across all dimensions without mutual interference. Experiments
demonstrate that Talking-Critic significantly outperforms existing methods in
aligning with human preference ratings. Meanwhile, TLPO achieves substantial
improvements over baseline models in lip-sync accuracy, motion naturalness, and
visual quality, exhibiting superior performance in both qualitative and
quantitative evaluations. Ours project page:
https://fantasy-amap.github.io/fantasy-talking2/

</details>


### [54] [Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds](https://arxiv.org/abs/2508.11265)
*Pei He,Lingling Li,Licheng Jiao,Ronghua Shang,Fang Liu,Shuang Wang,Xu Liu,Wenping Ma*

Main category: cs.CV

TL;DR: 论文提出了一种类别级几何学习框架（CGE和GCL），通过感知细粒度几何特性和对齐类别级几何嵌入，提升了3D语义分割的领域泛化能力，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过增强点云的数据分布来缓解领域偏移，但忽略了类别级分布和对齐问题。论文旨在探索领域不变的几何特征以提高泛化能力。

Method: 论文提出了类别级几何嵌入（CGE）和几何一致性学习（GCL）框架，通过感知点云特征的细粒度几何特性并模拟潜在3D分布，实现类别级几何嵌入的对齐。

Result: 实验结果表明，该方法在领域泛化的点云分割任务中具有较高的分割精度。

Conclusion: 该论文提出的类别级几何学习框架（CGE和GCL）在领域泛化的3D语义分割中表现出色，与现有方法相比具有竞争力的分割精度。

Abstract: Domain generalization in 3D segmentation is a critical challenge in deploying
models to unseen environments. Current methods mitigate the domain shift by
augmenting the data distribution of point clouds. However, the model learns
global geometric patterns in point clouds while ignoring the category-level
distribution and alignment. In this paper, a category-level geometry learning
framework is proposed to explore the domain-invariant geometric features for
domain generalized 3D semantic segmentation. Specifically, Category-level
Geometry Embedding (CGE) is proposed to perceive the fine-grained geometric
properties of point cloud features, which constructs the geometric properties
of each class and couples geometric embedding to semantic learning. Secondly,
Geometric Consistent Learning (GCL) is proposed to simulate the latent 3D
distribution and align the category-level geometric embeddings, allowing the
model to focus on the geometric invariant information to improve
generalization. Experimental results verify the effectiveness of the proposed
method, which has very competitive segmentation accuracy compared with the
state-of-the-art domain generalized point cloud methods.

</details>


### [55] [Probing the Representational Power of Sparse Autoencoders in Vision Models](https://arxiv.org/abs/2508.11277)
*Matthew Lyle Olson,Musashi Hinck,Neale Ratzlaff,Changbai Li,Phillip Howard,Vasudev Lal,Shao-Yen Tseng*

Main category: cs.CV

TL;DR: 本研究评估了稀疏自编码器（SAE）在视觉模型中的表现，发现其在提升可解释性、泛化能力和可控性方面具有显著潜力，尤其是在视觉嵌入模型、扩散模型和多模态LLM中。


<details>
  <summary>Details</summary>
Motivation: 尽管稀疏自编码器（SAE）在语言模型中广受欢迎，但在视觉领域的研究仍不足。本研究旨在填补这一空白，探索SAE在视觉模型中的应用潜力。

Method: 通过广泛的基于图像的任务，对三种视觉模型架构（视觉嵌入模型、多模态LLM和扩散模型）中SAE的表征能力进行了全面评估。

Result: 实验结果表明，SAE特征具有语义意义，能够提升分布外泛化能力，并在三种视觉模型架构中实现可控生成。具体而言，在视觉嵌入模型中，SAE特征可用于OOD检测并恢复底层模型的本体结构；在扩散模型中，SAE支持通过文本编码器操作实现语义引导，并开发了自动化流程以发现人类可解释属性；在多模态LLM中，SAE特征揭示了跨视觉和语言模态的共享表示。

Conclusion: 本研究为视觉模型中稀疏自编码器（SAE）的评估奠定了基础，展示了其在视觉领域中提升可解释性、泛化能力和可控性的强大潜力。

Abstract: Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting
the hidden states of large language models (LLMs). By learning to reconstruct
activations from a sparse bottleneck layer, SAEs discover interpretable
features from the high-dimensional internal representations of LLMs. Despite
their popularity with language models, SAEs remain understudied in the visual
domain. In this work, we provide an extensive evaluation the representational
power of SAEs for vision models using a broad range of image-based tasks. Our
experimental results demonstrate that SAE features are semantically meaningful,
improve out-of-distribution generalization, and enable controllable generation
across three vision model architectures: vision embedding models, multi-modal
LMMs and diffusion models. In vision embedding models, we find that learned SAE
features can be used for OOD detection and provide evidence that they recover
the ontological structure of the underlying model. For diffusion models, we
demonstrate that SAEs enable semantic steering through text encoder
manipulation and develop an automated pipeline for discovering
human-interpretable attributes. Finally, we conduct exploratory experiments on
multi-modal LLMs, finding evidence that SAE features reveal shared
representations across vision and language modalities. Our study provides a
foundation for SAE evaluation in vision models, highlighting their strong
potential improving interpretability, generalization, and steerability in the
visual domain.

</details>


### [56] [Unifying Scale-Aware Depth Prediction and Perceptual Priors for Monocular Endoscope Pose Estimation and Tissue Reconstruction](https://arxiv.org/abs/2508.11282)
*Muzammil Khan,Enzo Kerkhof,Matteo Fusaglia,Koert Kuhlmann,Theo Ruers,Françoise J. Siepel*

Main category: cs.CV

TL;DR: 提出了一种统一框架，结合尺度感知深度预测和时间约束感知细化，显著提升了单目内窥镜手术中的组织重建精度。


<details>
  <summary>Details</summary>
Motivation: 单目内窥镜姿态估计和组织重建面临深度模糊、组织变形、运动不一致、纹理保真度低和视野受限等挑战，需要一种统一框架来克服这些限制。

Method: 该框架整合了尺度感知深度预测与时间约束的感知细化，包括MAPIS-Depth模块（利用Depth Pro和Depth Anything进行深度预测）和L-BFGS-B优化，以及WEMA-RTDL模块（优化旋转和平移）。最后通过截断符号距离函数体积融合和行进立方体提取3D表面网格。

Result: 框架在HEVD和SCARED数据集上通过消融和对比分析验证了其鲁棒性和优越性。

Conclusion: 该框架在HEVD和SCARED数据集上的评估表明，其在单目内窥镜组织重建方面优于现有最先进方法，具有鲁棒性和优越性。

Abstract: Accurate endoscope pose estimation and 3D tissue surface reconstruction
significantly enhances monocular minimally invasive surgical procedures by
enabling accurate navigation and improved spatial awareness. However, monocular
endoscope pose estimation and tissue reconstruction face persistent challenges,
including depth ambiguity, physiological tissue deformation, inconsistent
endoscope motion, limited texture fidelity, and a restricted field of view. To
overcome these limitations, a unified framework for monocular endoscopic tissue
reconstruction that integrates scale-aware depth prediction with
temporally-constrained perceptual refinement is presented. This framework
incorporates a novel MAPIS-Depth module, which leverages Depth Pro for robust
initialisation and Depth Anything for efficient per-frame depth prediction, in
conjunction with L-BFGS-B optimisation, to generate pseudo-metric depth
estimates. These estimates are temporally refined by computing pixel
correspondences using RAFT and adaptively blending flow-warped frames based on
LPIPS perceptual similarity, thereby reducing artefacts arising from
physiological tissue deformation and motion. To ensure accurate registration of
the synthesised pseudo-RGBD frames from MAPIS-Depth, a novel WEMA-RTDL module
is integrated, optimising both rotation and translation. Finally, truncated
signed distance function-based volumetric fusion and marching cubes are applied
to extract a comprehensive 3D surface mesh. Evaluations on HEVD and SCARED,
with ablation and comparative analyses, demonstrate the framework's robustness
and superiority over state-of-the-art methods.

</details>


### [57] [TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation](https://arxiv.org/abs/2508.11284)
*Yilin Mi,Qixin Yan,Zheng-Peng Duan,Chunle Guo,Hubery Yin,Hao Liu,Chen Li,Chongyi Li*

Main category: cs.CV

TL;DR: TimeMachine是一种新型扩散框架，通过多交叉注意力模块和ACG模块实现精确年龄编辑，同时构建HFFA数据集以提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在面部图像编辑方面取得了显著进展，但如何在保持个人身份的同时实现细粒度年龄编辑仍是一个挑战。

Method: 提出了一种基于扩散的框架TimeMachine，通过将高精度年龄信息注入多交叉注意力模块，明确分离年龄相关和身份相关特征。此外，还提出了Age Classifier Guidance (ACG)模块，在潜在空间直接预测年龄，并构建了HFFA数据集。

Result: 实验结果表明，TimeMachine在细粒度年龄编辑和身份一致性保持方面表现优异。

Conclusion: TimeMachine在保持身份一致性的同时，实现了细粒度年龄编辑的最先进性能。

Abstract: With the advancement of generative models, facial image editing has made
significant progress. However, achieving fine-grained age editing while
preserving personal identity remains a challenging task.In this paper, we
propose TimeMachine, a novel diffusion-based framework that achieves accurate
age editing while keeping identity features unchanged. To enable fine-grained
age editing, we inject high-precision age information into the multi-cross
attention module, which explicitly separates age-related and identity-related
features. This design facilitates more accurate disentanglement of age
attributes, thereby allowing precise and controllable manipulation of facial
aging.Furthermore, we propose an Age Classifier Guidance (ACG) module that
predicts age directly in the latent space, instead of performing denoising
image reconstruction during training. By employing a lightweight module to
incorporate age constraints, this design enhances age editing accuracy by
modest increasing training cost. Additionally, to address the lack of
large-scale, high-quality facial age datasets, we construct a HFFA dataset
(High-quality Fine-grained Facial-Age dataset) which contains one million
high-resolution images labeled with identity and facial attributes.
Experimental results demonstrate that TimeMachine achieves state-of-the-art
performance in fine-grained age editing while preserving identity consistency.

</details>


### [58] [Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study](https://arxiv.org/abs/2508.11301)
*Jiarong Li,Imad Ali Shah,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: 研究通过HSI和优化波段选择（CSNR-JMIM）提升了行人分割性能，IoU和F1分数均有显著提高。


<details>
  <summary>Details</summary>
Motivation: 由于RGB成像中的同色异谱现象，行人与背景在视觉上难以区分，导致汽车感知系统中的行人分割面临关键安全挑战。

Method: 比较了标准RGB与两种降维方法（PCA和CSNR-JMIM），评估了三种语义分割模型（U-Net、DeepLabV3+和SegFormer）。

Result: CSNR-JMIM在行人分割中平均IoU提高了1.44%，F1分数提高了2.18%；在骑行者分割中也有类似提升。

Conclusion: 本研究通过最优HSI波段选择展示了稳健的行人分割，显示出在安全关键的汽车应用中的巨大潜力。

Abstract: Pedestrian segmentation in automotive perception systems faces critical
safety challenges due to metamerism in RGB imaging, where pedestrians and
backgrounds appear visually indistinguishable.. This study investigates the
potential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation
in urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We
compared standard RGB against two dimensionality-reduction approaches by
converting 128-channel HSI data into three-channel representations: Principal
Component Analysis (PCA) and optimal band selection using Contrast
Signal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM).
Three semantic segmentation models were evaluated: U-Net, DeepLabV3+, and
SegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements
of 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian
segmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25%
F1-score improvements. These improved performance results from enhanced
spectral discrimination of optimally selected HSI bands effectively reducing
false positives. This study demonstrates robust pedestrian segmentation through
optimal HSI band selection, showing significant potential for safety-critical
automotive applications.

</details>


### [59] [Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval](https://arxiv.org/abs/2508.11313)
*Weijia Liu,Jiuxin Cao,Bo Miao,Zhiheng Fu,Xuelin Zhu,Jiawei Ge,Bo Liu,Mehwish Nasim,Ajmal Mian*

Main category: cs.CV

TL;DR: DRNet 通过去噪-检索范式（TCD + TRF 模块）过滤无关视频片段并净化多模态表示，显著提升了视频时刻检索性能且具有通用性。


<details>
  <summary>Details</summary>
Motivation: 现有VMR方法编码所有视频片段（包括无关片段）会破坏多模态对齐并阻碍优化，因此需要一种能够过滤无关片段的新范式。

Method: 提出了 Denoise-then-Retrieve Network (DRNet)，包含 Text-Conditioned Denoising (TCD) 和 Text-Reconstruction Feedback (TRF) 模块，通过动态识别无关视频片段并净化多模态表示来实现精准检索。

Result: 在 Charades-STA 和 QVHighlights 数据集上的实验表明，DRNet 在所有指标上均优于现有最先进方法。

Conclusion: DRNet 提出的去噪-检索范式不仅显著提升了视频时刻检索的性能，还展示了其与其他先进VMR模型的无缝集成能力。

Abstract: Current text-driven Video Moment Retrieval (VMR) methods encode all video
clips, including irrelevant ones, disrupting multimodal alignment and hindering
optimization. To this end, we propose a denoise-then-retrieve paradigm that
explicitly filters text-irrelevant clips from videos and then retrieves the
target moment using purified multimodal representations. Following this
paradigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising
Text-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF)
modules. TCD integrates cross-attention and structured state space blocks to
dynamically identify noisy clips and produce a noise mask to purify multimodal
video representations. TRF further distills a single query embedding from
purified video representations and aligns it with the text embedding, serving
as auxiliary supervision for denoising during training. Finally, we perform
conditional retrieval using text embeddings on purified video representations
for accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that
our approach surpasses state-of-the-art methods on all metrics. Furthermore,
our denoise-then-retrieve paradigm is adaptable and can be seamlessly
integrated into advanced VMR models to boost performance.

</details>


### [60] [Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models](https://arxiv.org/abs/2508.11317)
*Yuchen Zhou,Jiayu Tang,Shuo Yang,Xiaoyan Xiao,Yuqin Dai,Wenhao Yang,Chao Gou,Xiaobo Xia,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 该论文提出LogicBench基准和LogicCLIP框架，系统性诊断并提升VLM的逻辑理解能力，实验显示其显著优于现有方法且不影响通用性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在逻辑理解方面存在显著不足，导致在实际应用中的可靠性受限。

Method: 提出LogicCLIP，一种通过逻辑感知数据生成和对比学习策略（包括粗粒度对齐、细粒度多目标选择和逻辑结构感知目标）来增强VLM逻辑敏感性的训练框架。

Result: LogicCLIP在所有LogicBench领域中显著提升了逻辑理解能力，且不影响通用视觉语言基准的性能。

Conclusion: LogicBench和LogicCLIP将成为推动VLM逻辑能力发展的重要资源。

Abstract: Vision-Language Models (VLMs), exemplified by CLIP, have emerged as
foundational for multimodal intelligence. However, their capacity for logical
understanding remains significantly underexplored, resulting in critical
''logical blindspots'' that limit their reliability in practical applications.
To systematically diagnose this, we introduce LogicBench, a comprehensive
benchmark with over 50,000 vision-language pairs across 9 logical categories
and 4 diverse scenarios: images, videos, anomaly detection, and medical
diagnostics. Our evaluation reveals that existing VLMs, even the
state-of-the-art ones, fall at over 40 accuracy points below human performance,
particularly in challenging tasks like Causality and Conditionality,
highlighting their reliance on surface semantics over critical logical
structures. To bridge this gap, we propose LogicCLIP, a novel training
framework designed to boost VLMs' logical sensitivity through advancements in
both data generation and optimization objectives. LogicCLIP utilizes
logic-aware data generation and a contrastive learning strategy that combines
coarse-grained alignment, a fine-grained multiple-choice objective, and a novel
logical structure-aware objective. Extensive experiments demonstrate
LogicCLIP's substantial improvements in logical comprehension across all
LogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP
retains, and often surpasses, competitive performance on general
vision-language benchmarks, demonstrating that the enhanced logical
understanding does not come at the expense of general alignment. We believe
that LogicBench and LogicCLIP will be important resources for advancing VLM
logical capabilities.

</details>


### [61] [Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking](https://arxiv.org/abs/2508.11323)
*Haonan Zhang,Xinyao Wang,Boxi Wu,Tu Zheng,Wang Yunhua,Zheng Yang*

Main category: cs.CV

TL;DR: DSC-Track通过动态场景中的空间线索一致性提升3D多目标跟踪性能，实验验证其高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在拥挤环境或检测不准确时表现不佳，主要原因是忽略了物体间的几何关系或受到无关物体的干扰。本文提出通过识别和匹配稳定的空间模式（线索一致性）来解决这一问题。

Method: 1. 使用点对特征（PPF）设计统一的时空编码器，学习具有区分性的轨迹嵌入并抑制干扰；2. 引入线索一致性变换器模块，显式对齐历史轨迹与当前检测的特征表示；3. 动态更新机制保留显著的时空信息以支持稳定在线跟踪。

Result: 在nuScenes和Waymo Open Datasets上的实验表明，DSC-Track具有高效性和鲁棒性。例如，在nuScenes验证集和测试集上分别达到73.2%和70.3%的AMOTA，实现了最先进的性能。

Conclusion: 本文提出的DSC-Track方法通过利用动态场景中的空间线索一致性，显著提升了3D多目标跟踪的性能，尤其在拥挤环境或检测不准确的情况下表现优异。

Abstract: 3D multi-object tracking is a critical and challenging task in the field of
autonomous driving. A common paradigm relies on modeling individual object
motion, e.g., Kalman filters, to predict trajectories. While effective in
simple scenarios, this approach often struggles in crowded environments or with
inaccurate detections, as it overlooks the rich geometric relationships between
objects. This highlights the need to leverage spatial cues. However, existing
geometry-aware methods can be susceptible to interference from irrelevant
objects, leading to ambiguous features and incorrect associations. To address
this, we propose focusing on cue-consistency: identifying and matching stable
spatial patterns over time. We introduce the Dynamic Scene Cue-Consistency
Tracker (DSC-Track) to implement this principle. Firstly, we design a unified
spatiotemporal encoder using Point Pair Features (PPF) to learn discriminative
trajectory embeddings while suppressing interference. Secondly, our
cue-consistency transformer module explicitly aligns consistent feature
representations between historical tracks and current detections. Finally, a
dynamic update mechanism preserves salient spatiotemporal information for
stable online tracking. Extensive experiments on the nuScenes and Waymo Open
Datasets validate the effectiveness and robustness of our approach. On the
nuScenes benchmark, for instance, our method achieves state-of-the-art
performance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,
respectively.

</details>


### [62] [Noise Matters: Optimizing Matching Noise for Diffusion Classifiers](https://arxiv.org/abs/2508.11330)
*Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: NoOp通过学习匹配噪声优化扩散分类器的稳定性，避免了数百次采样的计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散分类器（DC）因噪声不稳定性而需要集成数百次采样结果，导致分类速度显著下降。为缓解这一问题，论文探索了噪声在DC中的作用，并提出了NoOp方法。

Method: NoOp通过频率匹配和空间匹配两个原则优化噪声：1）优化一个数据集特定的参数化噪声；2）训练一个元网络生成图像特定的噪声偏移。优化的噪声和噪声偏移之和用于替代随机噪声。

Result: 在多个数据集上的广泛实验表明，NoOp能有效缓解噪声不稳定性，提升分类性能的稳定性。

Conclusion: 论文提出了一种名为NoOp的噪声优化方法，通过学习匹配噪声来解决扩散分类器（DC）中的噪声不稳定性问题，显著提高了分类性能的稳定性。

Abstract: Although today's pretrained discriminative vision-language models (e.g.,
CLIP) have demonstrated strong perception abilities, such as zero-shot image
classification, they also suffer from the bag-of-words problem and spurious
bias. To mitigate these problems, some pioneering studies leverage powerful
generative models (e.g., pretrained diffusion models) to realize generalizable
image classification, dubbed Diffusion Classifier (DC). Specifically, by
randomly sampling a Gaussian noise, DC utilizes the differences of denoising
effects with different category conditions to classify categories.
Unfortunately, an inherent and notorious weakness of existing DCs is noise
instability: different random sampled noises lead to significant performance
changes. To achieve stable classification performance, existing DCs always
ensemble the results of hundreds of sampled noises, which significantly reduces
the classification speed. To this end, we firstly explore the role of noise in
DC, and conclude that: there are some ``good noises'' that can relieve the
instability. Meanwhile, we argue that these good noises should meet two
principles: Frequency Matching and Spatial Matching. Regarding both principles,
we propose a novel Noise Optimization method to learn matching (i.e., good)
noise for DCs: NoOp. For frequency matching, NoOp first optimizes a
dataset-specific noise: Given a dataset and a timestep t, optimize one randomly
initialized parameterized noise. For Spatial Matching, NoOp trains a
Meta-Network that adopts an image as input and outputs image-specific noise
offset. The sum of optimized noise and noise offset will be used in DC to
replace random noise. Extensive ablations on various datasets demonstrated the
effectiveness of NoOp.

</details>


### [63] [GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition](https://arxiv.org/abs/2508.11334)
*Md Asgor Hossain Reaj,Rajan Das Gupta,Md Yeasin Rahat,Nafiz Fahad,Md Jawadul Hasan,Tze Hui Liew*

Main category: cs.CV

TL;DR: GANDiff FR 是一个结合 GAN 和扩散模型的框架，用于精确控制面部属性以减少偏见，显著提升了公平性。


<details>
  <summary>Details</summary>
Motivation: 为了测量、解释和减少偏见，并确保可重复性，研究团队开发了 GANDiff FR 框架。

Method: GANDiff FR 结合了基于 StyleGAN3 的身份保留生成和基于扩散的属性控制，能够精细操控姿态（约30度）、光照（四个方向）和表情（五个级别）。

Result: 在合成10,000张人口统计平衡的面孔后，AdaFace 将组间 TPR 差异减少了60%（2.5% vs. 6.3%），光照占剩余偏见的42%。跨数据集评估显示合成到真实的强迁移性（r 0.85）。

Conclusion: GANDiff FR 通过结合 StyleGAN3 和扩散模型，实现了对人口统计和环境因素的精确控制，显著减少了面部识别中的偏见，并为公平性审计提供了可重复的标准。

Abstract: We introduce GANDiff FR, the first synthetic framework that precisely
controls demographic and environmental factors to measure, explain, and reduce
bias with reproducible rigor. GANDiff FR unifies StyleGAN3-based
identity-preserving generation with diffusion-based attribute control, enabling
fine-grained manipulation of pose around 30 degrees, illumination (four
directions), and expression (five levels) under ceteris paribus conditions. We
synthesize 10,000 demographically balanced faces across five cohorts validated
for realism via automated detection (98.2%) and human review (89%) to isolate
and quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under
matched operating points shows AdaFace reduces inter-group TPR disparity by 60%
(2.5% vs. 6.3%), with illumination accounting for 42% of residual bias.
Cross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong
synthetic-to-real transfer (r 0.85). Despite around 20% computational overhead
relative to pure GANs, GANDiff FR yields three times more attribute-conditioned
variants, establishing a reproducible, regulation-aligned (EU AI Act) standard
for fairness auditing. Code and data are released to support transparent,
scalable bias evaluation.

</details>


### [64] [Index-Aligned Query Distillation for Transformer-based Incremental Object Detection](https://arxiv.org/abs/2508.11339)
*Mingxiao Ma,Shunyao Zhu,Guoliang Kang*

Main category: cs.CV

TL;DR: IAQD通过索引对齐查询蒸馏，解决了transformer增量检测中的知识遗忘问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 在基于transformer的增量目标检测中，传统匈牙利匹配方法可能导致知识遗忘，因此需要更有效的蒸馏方法。

Method: 提出了名为索引对齐查询蒸馏（IAQD）的新蒸馏方法，通过建立相同索引的查询对应关系，并在关键查询上执行索引对齐蒸馏。

Result: IAQD显著保留了之前的语义和空间编码能力，同时不影响新类别的学习，实验证明其性能优于现有方法。

Conclusion: IAQD方法通过索引对齐的查询蒸馏，有效缓解了基于transformer的增量目标检测模型中的知识遗忘问题，并在代表性基准测试中取得了最先进的性能。

Abstract: Incremental object detection (IOD) aims to continuously expand the capability
of a model to detect novel categories while preserving its performance on
previously learned ones. When adopting a transformer-based detection model to
perform IOD, catastrophic knowledge forgetting may inevitably occur, meaning
the detection performance on previously learned categories may severely
degenerate. Previous typical methods mainly rely on knowledge distillation (KD)
to mitigate the catastrophic knowledge forgetting of transformer-based
detection models. Specifically, they utilize Hungarian Matching to build a
correspondence between the queries of the last-phase and current-phase
detection models and align the classifier and regressor outputs between matched
queries to avoid knowledge forgetting. However, we observe that in IOD task,
Hungarian Matching is not a good choice. With Hungarian Matching, the query of
the current-phase model may match different queries of the last-phase model at
different iterations during KD. As a result, the knowledge encoded in each
query may be reshaped towards new categories, leading to the forgetting of
previously encoded knowledge of old categories. Based on our observations, we
propose a new distillation approach named Index-Aligned Query Distillation
(IAQD) for transformer-based IOD. Beyond using Hungarian Matching, IAQD
establishes a correspondence between queries of the previous and current phase
models that have the same index. Moreover, we perform index-aligned
distillation only on partial queries which are critical for the detection of
previous categories. In this way, IAQD largely preserves the previous semantic
and spatial encoding capabilities without interfering with the learning of new
categories. Extensive experiments on representative benchmarks demonstrate that
IAQD effectively mitigates knowledge forgetting, achieving new state-of-the-art
performance.

</details>


### [65] [Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification](https://arxiv.org/abs/2508.11340)
*Yuanlin Liu,Zhihan Zhou,Mingqiang Wei,Youyi Song*

Main category: cs.CV

TL;DR: 提出“主动标记”算法，通过分类器不确定性选择最具标记价值的宫颈细胞图像，显著降低人力成本，提升数据分类效率。


<details>
  <summary>Details</summary>
Motivation: 现有的自动分类方法需要代表性训练数据集，但构建这样的数据集通常需要高昂甚至难以承受的人力成本。

Method: 提出了一种称为“主动标记”的新算法，该算法基于分类器对未标记宫颈细胞图像的不确定性，高效选择最具标记价值的图像。

Result: 广泛的实证结果表明，该方法在提升训练数据集代表性方面有效，并显著优化了人力成本的使用。

Conclusion: 该方法通过有效利用未标记宫颈细胞图像的分类器不确定性，显著降低了构建代表性训练数据集的人力成本，为数据高效的宫颈细胞分类开辟了新途径。

Abstract: Information on the number and category of cervical cells is crucial for the
diagnosis of cervical cancer. However, existing classification methods capable
of automatically measuring this information require the training dataset to be
representative, which consumes an expensive or even unaffordable human cost. We
herein propose active labeling that enables us to construct a representative
training dataset using a much smaller human cost for data-efficient cervical
cell classification. This cost-effective method efficiently leverages the
classifier's uncertainty on the unlabeled cervical cell images to accurately
select images that are most beneficial to label. With a fast estimation of the
uncertainty, this new algorithm exhibits its validity and effectiveness in
enhancing the representative ability of the constructed training dataset. The
extensive empirical results confirm its efficacy again in navigating the usage
of human cost, opening the avenue for data-efficient cervical cell
classification.

</details>


### [66] [Semantically Guided Adversarial Testing of Vision Models Using Language Models](https://arxiv.org/abs/2508.11341)
*Katarzyna Filus,Jorge M. Cruz-Duarte*

Main category: cs.CV

TL;DR: 本文提出了一种语义引导的对抗目标选择框架，利用预训练模型的跨模态知识转移，优于现有方法，尤其在远距离类别关系上，适用于构建标准化对抗基准。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击策略通常依赖随机性、模型预测或静态语义资源，限制了可解释性、可重复性或灵活性。

Method: 提出了一种基于语义引导的对抗目标选择框架，利用预训练语言和视觉语言模型的跨模态知识转移。评估了BERT、TinyLLAMA和CLIP等模型作为相似性源，选择与真实标签最相关和最不相关的标签，形成最佳和最差对抗场景。

Result: 实验表明，这些模型能持续提供实用的对抗目标，并超越静态词汇数据库（如WordNet），尤其在远距离类别关系上。静态目标标签测试可作为相似性源有效性的初步评估。

Conclusion: 预训练模型适用于构建可解释、标准化和可扩展的对抗性基准，适用于不同架构和数据集。

Abstract: In targeted adversarial attacks on vision models, the selection of the target
label is a critical yet often overlooked determinant of attack success. This
target label corresponds to the class that the attacker aims to force the model
to predict. Now, existing strategies typically rely on randomness, model
predictions, or static semantic resources, limiting interpretability,
reproducibility, or flexibility. This paper then proposes a semantics-guided
framework for adversarial target selection using the cross-modal knowledge
transfer from pretrained language and vision-language models. We evaluate
several state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity
sources to select the most and least semantically related labels with respect
to the ground truth, forming best- and worst-case adversarial scenarios. Our
experiments on three vision models and five attack methods reveal that these
models consistently render practical adversarial targets and surpass static
lexical databases, such as WordNet, particularly for distant class
relationships. We also observe that static testing of target labels offers a
preliminary assessment of the effectiveness of similarity sources, \textit{a
priori} testing. Our results corroborate the suitability of pretrained models
for constructing interpretable, standardized, and scalable adversarial
benchmarks across architectures and datasets.

</details>


### [67] [HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model](https://arxiv.org/abs/2508.11350)
*Zhenhao Zhang,Hanqing Wang,Xiangyu Zeng,Ziyu Cheng,Jiaxin Liu,Haoyu Yan,Zhirui Liu,Kaiyang Ji,Tianxiang Gui,Ke Hu,Kangyi Chen,Yahao Fan,Mokai Pan*

Main category: cs.CV

TL;DR: HOID-R1结合CoT-SFT和GRPO，在RL框架中提升HOI检测性能，并通过'MLLM-as-a-judge'减少幻觉，实现开放世界泛化。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇HOI检测方法过度依赖大型语言模型，忽视了其3D空间理解能力，需要更全面的解决方案。

Method: 集成了链式思维（CoT）引导的监督微调（SFT）和组相对策略优化（GRPO），并引入'MLLM-as-a-judge'机制以减少幻觉。

Result: HOID-R1在HOI检测基准测试中达到最先进性能，并在新场景的开放世界泛化中优于现有方法。

Conclusion: HOID-R1通过结合CoT引导的SFT和GRPO，在RL框架中显著提升了HOI检测的性能，并在开放世界泛化中表现优异。

Abstract: Understanding and recognizing human-object interaction (HOI) is a pivotal
application in AR/VR and robotics. Recent open-vocabulary HOI detection
approaches depend exclusively on large language models for richer textual
prompts, neglecting their inherent 3D spatial understanding capabilities. To
address this shortcoming, we introduce HOID-R1, the first HOI detection
framework that integrates chain-of-thought (CoT) guided supervised fine-tuning
(SFT) with group relative policy optimization (GRPO) within a reinforcement
learning (RL) paradigm. Specifically, we initially apply SFT to imbue the model
with essential reasoning capabilities, forcing the model to articulate its
thought process in the output. Subsequently, we integrate GRPO to leverage
multi-reward signals for policy optimization, thereby enhancing alignment
across diverse modalities. To mitigate hallucinations in the CoT reasoning, we
introduce an "MLLM-as-a-judge" mechanism that supervises the CoT outputs,
further improving generalization. Extensive experiments show that HOID-R1
achieves state-of-the-art performance on HOI detection benchmarks and
outperforms existing methods in open-world generalization to novel scenarios.

</details>


### [68] [Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition](https://arxiv.org/abs/2508.11376)
*Durgesh Mishra,Rishabh Uikey*

Main category: cs.CV

TL;DR: A unified knowledge distillation method combining instance-level and relational losses improves face recognition model performance on edge devices, even surpassing teacher models in accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional knowledge distillation methods often fail to capture both fine-grained instance-level details and complex relational structures, leading to suboptimal performance in face recognition models for edge devices.

Method: The approach integrates two novel loss functions: Instance-Level Embedding Distillation and Relation-Based Pairwise Similarity Distillation, utilizing dynamic hard mining and memory bank mechanisms.

Result: The unified framework outperforms state-of-the-art distillation methods across multiple benchmark face recognition datasets.

Conclusion: The proposed unified knowledge distillation framework effectively combines instance-level and relational information, outperforming traditional methods and even enabling the student model to surpass the teacher's accuracy in some cases.

Abstract: Knowledge Distillation is crucial for optimizing face recognition models for
deployment in computationally limited settings, such as edge devices.
Traditional KD methods, such as Raw L2 Feature Distillation or Feature
Consistency loss, often fail to capture both fine-grained instance-level
details and complex relational structures, leading to suboptimal performance.
We propose a unified approach that integrates two novel loss functions,
Instance-Level Embedding Distillation and Relation-Based Pairwise Similarity
Distillation. Instance-Level Embedding Distillation focuses on aligning
individual feature embeddings by leveraging a dynamic hard mining strategy,
thereby enhancing learning from challenging examples. Relation-Based Pairwise
Similarity Distillation captures relational information through pairwise
similarity relationships, employing a memory bank mechanism and a sample mining
strategy. This unified framework ensures both effective instance-level
alignment and preservation of geometric relationships between samples, leading
to a more comprehensive distillation process. Our unified framework outperforms
state-of-the-art distillation methods across multiple benchmark face
recognition datasets, as demonstrated by extensive experimental evaluations.
Interestingly, when using strong teacher networks compared to the student, our
unified KD enables the student to even surpass the teacher's accuracy.

</details>


### [69] [RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator](https://arxiv.org/abs/2508.11409)
*Zhiming Liu,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: RMFAT是一种轻量级循环框架，通过多尺度特征和时间扭曲模块高效恢复大气湍流视频，显著提升清晰度和推理速度。


<details>
  <summary>Details</summary>
Motivation: 大气湍流严重降低视频质量，现有基于Transformer和3D架构的方法计算成本高，难以实时部署。

Method: RMFAT采用轻量级循环框架，仅需两帧输入，通过多尺度特征编码和解码以及时间扭曲模块来恢复视频帧。

Result: RMFAT在清晰度恢复（SSIM提升近9%）和推理速度（运行时减少四倍以上）上均优于现有方法。

Conclusion: RMFAT提出了一种轻量级循环框架，显著降低了计算负担，同时通过多尺度特征编码和解码以及时间扭曲模块，提升了空间细节和时间一致性。在合成和真实世界的大气湍流数据集上的实验表明，RMFAT在清晰度恢复和推理速度上均优于现有方法。

Abstract: Atmospheric turbulence severely degrades video quality by introducing
distortions such as geometric warping, blur, and temporal flickering, posing
significant challenges to both visual clarity and temporal consistency. Current
state-of-the-art methods are based on transformer and 3D architectures and
require multi-frame input, but their large computational cost and memory usage
limit real-time deployment, especially in resource-constrained scenarios. In
this work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric
Turbulence Mitigator, designed for efficient and temporally consistent video
restoration under AT conditions. RMFAT adopts a lightweight recurrent framework
that restores each frame using only two inputs at a time, significantly
reducing temporal window size and computational burden. It further integrates
multi-scale feature encoding and decoding with temporal warping modules at both
encoder and decoder stages to enhance spatial detail and temporal coherence.
Extensive experiments on synthetic and real-world atmospheric turbulence
datasets demonstrate that RMFAT not only outperforms existing methods in terms
of clarity restoration (with nearly a 9\% improvement in SSIM) but also
achieves significantly improved inference speed (more than a fourfold reduction
in runtime), making it particularly suitable for real-time atmospheric
turbulence suppression tasks.

</details>


### [70] [SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models](https://arxiv.org/abs/2508.11411)
*Fabian H. Reith,Jannik Franzen,Dinesh R. Palli,J. Lorenz Rumberger,Dagmar Kainmueller*

Main category: cs.CV

TL;DR: SelfAdapt通过无标签自适应方法提升Cellpose在细胞分割中的性能，最高提升29.64%，并兼容监督微调模型。


<details>
  <summary>Details</summary>
Motivation: 通用模型如Cellpose在跨领域数据上性能下降，而监督微调需要标注数据，但标注可能不易获取。

Method: 基于学生-教师增强一致性训练，引入L2-SP正则化和无标签停止标准。

Result: 在LiveCell和TissueNet数据集上，AP0.5相对基线Cellpose最高提升29.64%，且无监督自适应可进一步提升监督微调模型的性能。

Conclusion: SelfAdapt作为一种无需标签的预训练细胞分割模型自适应方法，显著提升了Cellpose在LiveCell和TissueNet数据集上的性能，并可作为Cellpose框架的易用扩展。

Abstract: Deep neural networks have become the go-to method for biomedical instance
segmentation. Generalist models like Cellpose demonstrate state-of-the-art
performance across diverse cellular data, though their effectiveness often
degrades on domains that differ from their training data. While supervised
fine-tuning can address this limitation, it requires annotated data that may
not be readily available. We propose SelfAdapt, a method that enables the
adaptation of pre-trained cell segmentation models without the need for labels.
Our approach builds upon student-teacher augmentation consistency training,
introducing L2-SP regularization and label-free stopping criteria. We evaluate
our method on the LiveCell and TissueNet datasets, demonstrating relative
improvements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we
show that our unsupervised adaptation can further improve models that were
previously fine-tuned with supervision. We release SelfAdapt as an easy-to-use
extension of the Cellpose framework. The code for our method is publicly
available at https: //github.com/Kainmueller-Lab/self_adapt.

</details>


### [71] [Training-free Dimensionality Reduction via Feature Truncation: Enhancing Efficiency in Privacy-preserving Multi-Biometric Systems](https://arxiv.org/abs/2508.11419)
*Florian Bayer,Maximilian Russo,Christian Rathgeb*

Main category: cs.CV

TL;DR: 研究通过多模态融合减少生物特征模板尺寸，实验表明模板尺寸可减少67%且不影响识别精度。


<details>
  <summary>Details</summary>
Motivation: 生物特征识别广泛使用，提取模板的隐私和安全性成为关键问题。生物特征模板保护方案，尤其是使用同态加密的方案，由于增加了计算负担，带来了显著的挑战。

Method: 在内部虚拟多生物特征数据库上进行实验，该数据库基于DNN提取的面部、指纹和虹膜特征，使用FRGC、MCYT和CASIA数据库。评估方法包括（i）可解释且易于在加密下实现，（ii）无需训练，（iii）具有泛化能力。

Result: 通过减少特征向量的维度，可以在同态加密（HE）域中进行更高效的加密处理，同时保持生物特征准确性和安全性，达到或超过单生物特征识别的水平。

Conclusion: 通过融合多模态特征向量，模板尺寸可以减少67%，同时保持与最佳单模态识别相当的等错误率（EER）。

Abstract: Biometric recognition is widely used, making the privacy and security of
extracted templates a critical concern. Biometric Template Protection schemes,
especially those utilizing Homomorphic Encryption, introduce significant
computational challenges due to increased workload. Recent advances in deep
neural networks have enabled state-of-the-art feature extraction for face,
fingerprint, and iris modalities. The ubiquity and affordability of biometric
sensors further facilitate multi-modal fusion, which can enhance security by
combining features from different modalities. This work investigates the
biometric performance of reduced multi-biometric template sizes. Experiments
are conducted on an in-house virtual multi-biometric database, derived from
DNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT,
and CASIA databases. The evaluated approaches are (i) explainable and
straightforward to implement under encryption, (ii) training-free, and (iii)
capable of generalization. Dimensionality reduction of feature vectors leads to
fewer operations in the Homomorphic Encryption (HE) domain, enabling more
efficient encrypted processing while maintaining biometric accuracy and
security at a level equivalent to or exceeding single-biometric recognition.
Our results demonstrate that, by fusing feature vectors from multiple
modalities, template size can be reduced by 67 % with no loss in Equal Error
Rate (EER) compared to the best-performing single modality.

</details>


### [72] [ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving](https://arxiv.org/abs/2508.11428)
*Jingyu Li,Bozhou Zhang,Xin Jin,Jiankang Deng,Xiatian Zhu,Li Zhang*

Main category: cs.CV

TL;DR: ImagiDrive整合VLM和DWM，通过迭代优化驾驶决策，在自动驾驶任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要多模态理解和预测能力，VLM和DWM各有优势但整合不足，ImagiDrive旨在弥补这一空白。

Method: 提出了ImagiDrive框架，结合VLM驱动的行为和DWM驱动的场景生成，通过早期停止机制和轨迹选择策略优化效率和准确性。

Result: 在nuScenes和NAVSIM数据集上的实验验证了ImagiDrive的鲁棒性和优越性。

Conclusion: ImagiDrive通过整合VLM和DWM，展示了在自动驾驶任务中的优越性能，尤其在开环和闭环条件下均优于现有方法。

Abstract: Autonomous driving requires rich contextual comprehension and precise
predictive reasoning to navigate dynamic and complex environments safely.
Vision-Language Models (VLMs) and Driving World Models (DWMs) have
independently emerged as powerful recipes addressing different aspects of this
challenge. VLMs provide interpretability and robust action prediction through
their ability to understand multi-modal context, while DWMs excel in generating
detailed and plausible future driving scenarios essential for proactive
planning. Integrating VLMs with DWMs is an intuitive, promising, yet
understudied strategy to exploit the complementary strengths of accurate
behavioral prediction and realistic scene generation. Nevertheless, this
integration presents notable challenges, particularly in effectively connecting
action-level decisions with high-fidelity pixel-level predictions and
maintaining computational efficiency. In this paper, we propose ImagiDrive, a
novel end-to-end autonomous driving framework that integrates a VLM-based
driving agent with a DWM-based scene imaginer to form a unified
imagination-and-planning loop. The driving agent predicts initial driving
trajectories based on multi-modal inputs, guiding the scene imaginer to
generate corresponding future scenarios. These imagined scenarios are
subsequently utilized to iteratively refine the driving agent's planning
decisions. To address efficiency and predictive accuracy challenges inherent in
this integration, we introduce an early stopping mechanism and a trajectory
selection strategy. Extensive experimental validation on the nuScenes and
NAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over
previous alternatives under both open-loop and closed-loop conditions.

</details>


### [73] [Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting](https://arxiv.org/abs/2508.11431)
*Simona Kocour,Assia Benbihi,Torsten Sattler*

Main category: cs.CV

TL;DR: 本研究提出了一个评估3D物体移除后语义残留的框架和数据集，发现现有方法在复杂场景中存在不足。


<details>
  <summary>Details</summary>
Motivation: 理解物体移除后保留的语义信息对隐私保护的3D重建和可编辑场景表示至关重要。

Method: 引入了一个新颖的基准和评估框架，用于测量3D高斯泼溅中物体移除后的语义残留。实验涵盖了多样化的室内外场景，并发布了Remove360数据集。

Result: 实验表明，当前方法在视觉几何缺失的情况下仍能保留语义信息，但存在局限性。

Conclusion: 当前3D物体移除技术在处理真实世界复杂场景时存在关键局限性，需要更强大的解决方案。

Abstract: Understanding what semantic information persists after object removal is
critical for privacy-preserving 3D reconstruction and editable scene
representations. In this work, we introduce a novel benchmark and evaluation
framework to measure semantic residuals, the unintended semantic traces left
behind, after object removal in 3D Gaussian Splatting. We conduct experiments
across a diverse set of indoor and outdoor scenes, showing that current methods
can preserve semantic information despite the absence of visual geometry. We
also release Remove360, a dataset of pre/post-removal RGB images and
object-level masks captured in real-world environments. While prior datasets
have focused on isolated object instances, Remove360 covers a broader and more
complex range of indoor and outdoor scenes, enabling evaluation of object
removal in the context of full-scene representations. Given ground truth images
of a scene before and after object removal, we assess whether we can truly
eliminate semantic presence, and if downstream models can still infer what was
removed. Our findings reveal critical limitations in current 3D object removal
techniques and underscore the need for more robust solutions capable of
handling real-world complexity. The evaluation framework is available at
github.com/spatial-intelligence-ai/Remove360.git. Data are available at
huggingface.co/datasets/simkoc/Remove360.

</details>


### [74] [MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation](https://arxiv.org/abs/2508.11433)
*Qian Liang,Yujia Wu,Kuncheng Li,Jiwei Wei,Shiyuan He,Jinyu Guo,Ning Xie*

Main category: cs.CV

TL;DR: MM-R1通过跨模态思维链推理和GRPO优化，实现了统一MLLMs在个性化图像生成中的高效零样本应用。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs方法通常是特定主题的，需要为每个新主题进行数据密集的微调过程，限制了其可扩展性。因此，需要一种更高效的方法来解锁统一MLLMs在个性化图像生成中的潜力。

Method: MM-R1通过视觉推理和生成过程的集成，包括（1）基于用户提供的图像和上下文线索的主题概念定位，（2）基于提取的主题表示和用户提示的个性化图像生成。此外，采用分组奖励近端策略优化（GRPO）进一步增强推理能力。

Result: 实验证明，MM-R1能够在零样本情况下生成具有高主题保真度和强文本对齐的个性化图像。

Conclusion: MM-R1框架通过跨模态思维链（X-CoT）推理策略，成功释放了统一多模态大语言模型（MLLMs）在个性化图像生成中的潜力，实现了高主题保真度和强文本对齐的零样本生成。

Abstract: Multimodal Large Language Models (MLLMs) with unified architectures excel
across a wide range of vision-language tasks, yet aligning them with
personalized image generation remains a significant challenge. Existing methods
for MLLMs are frequently subject-specific, demanding a data-intensive
fine-tuning process for every new subject, which limits their scalability. In
this paper, we introduce MM-R1, a framework that integrates a cross-modal
Chain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of
unified MLLMs for personalized image generation. Specifically, we structure
personalization as an integrated visual reasoning and generation process: (1)
grounding subject concepts by interpreting and understanding user-provided
images and contextual cues, and (2) generating personalized images conditioned
on both the extracted subject representations and user prompts. To further
enhance the reasoning capability, we adopt Grouped Reward Proximal Policy
Optimization (GRPO) to explicitly align the generation. Experiments demonstrate
that MM-R1 unleashes the personalization capability of unified MLLMs to
generate images with high subject fidelity and strong text alignment in a
zero-shot manner.

</details>


### [75] [Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge](https://arxiv.org/abs/2508.11464)
*Xiaoya Zhu,Yibing Nan,Shiguo Lian*

Main category: cs.CV

TL;DR: 本文利用Swin Transformer V2-B网络和数据增强技术，成功检测Deepfake图像并在竞赛中获奖。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，Deepfake技术带来了大量AI生成内容，同时也对数字安全提出了前所未有的挑战。

Method: 采用Swin Transformer V2-B分类网络，结合在线数据增强和离线样本生成方法。

Result: 在Deepfake图像检测竞赛中获得优异奖项。

Conclusion: 本文通过Swin Transformer V2-B分类网络及数据增强方法，在Deepfake图像检测竞赛中取得了优异奖项。

Abstract: With the rapid development of technology in the field of AI, deepfake
technology has emerged as a double-edged sword. It has not only created a large
amount of AI-generated content but also posed unprecedented challenges to
digital security. The task of the competition is to determine whether a face
image is a Deepfake image and output its probability score of being a Deepfake
image. In the image track competition, our approach is based on the Swin
Transformer V2-B classification network. And online data augmentation and
offline sample generation methods are employed to enrich the diversity of
training samples and increase the generalization ability of the model. Finally,
we got the award of excellence in Deepfake image detection.

</details>


### [76] [CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement Membrane Segmentation](https://arxiv.org/abs/2508.11469)
*Hongjin Fang,Daniel Reisenbüchler,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: CoFi是一种快速高效的粗到精细少样本分割流程，用于EM图像中的GBM分割，减轻标注和计算负担，同时实现高精度和速度。


<details>
  <summary>Details</summary>
Motivation: 准确分割电子显微镜图像中的GBM对于量化膜厚度和支持各种肾脏疾病的诊断至关重要。虽然监督深度学习方法实现了高分割精度，但其对大量像素级标注的依赖使其在临床工作流程中不切实际。少样本学习可以减少标注负担，但往往难以捕捉GBM分析所需的精细结构细节。

Method: CoFi首先使用仅三张标注图像训练一个轻量级神经网络生成初始粗分割掩模，然后通过形态感知修剪自动处理生成高质量点提示，用于指导SAM细化分割。

Result: 所提出的方法在GBM分割中表现出色，Dice系数达到74.54%，推理速度为1.9 FPS。

Conclusion: CoFi不仅减轻了传统方法的标注和计算负担，还实现了准确可靠的GBM分割结果，其速度和标注效率使其非常适合研究，并具有在肾脏病理学临床应用中强大的潜力。

Abstract: Accurate segmentation of the glomerular basement membrane (GBM) in electron
microscopy (EM) images is fundamental for quantifying membrane thickness and
supporting the diagnosis of various kidney diseases. While supervised deep
learning approaches achieve high segmentation accuracy, their reliance on
extensive pixel-level annotation renders them impractical for clinical
workflows. Few-shot learning can reduce this annotation burden but often
struggles to capture the fine structural details necessary for GBM analysis. In
this study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot
segmentation pipeline designed for GBM delineation in EM images. CoFi first
trains a lightweight neural network using only three annotated images to
produce an initial coarse segmentation mask. This mask is then automatically
processed to generate high-quality point prompts with morphology-aware pruning,
which are subsequently used to guide SAM in refining the segmentation. The
proposed method achieved exceptional GBM segmentation performance, with a Dice
coefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that
CoFi not only alleviates the annotation and computational burdens associated
with conventional methods, but also achieves accurate and reliable segmentation
results. The pipeline's speed and annotation efficiency make it well-suited for
research and hold strong potential for clinical applications in renal
pathology. The pipeline is publicly available at:
https://github.com/ddrrnn123/CoFi.

</details>


### [77] [TACR-YOLO: A Real-time Detection Framework for Abnormal Human Behaviors Enhanced with Coordinate and Task-Aware Representations](https://arxiv.org/abs/2508.11478)
*Xinyi Yin,Wenbo Yuan,Xuecheng Wu,Liangyu Fu,Danlei Huang*

Main category: cs.CV

TL;DR: TACR-YOLO 是一种新型实时异常行为检测框架，通过多个模块改进 YOLO 方法，在 PABD 数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 特殊场景下的异常人类行为检测（AHBD）日益重要，但现有 YOLO 方法在小目标检测、任务冲突和多尺度融合方面存在挑战。

Method: 提出了 TACR-YOLO 框架，包括 Coordinate Attention Module（增强小目标检测）、Task-Aware Attention Module（处理分类-回归冲突）、Strengthen Neck Network（优化多尺度融合），并优化了 Anchor Box 大小和 DIoU-Loss。

Result: 在 PABD 数据集上，TACR-YOLO 达到 91.92% mAP，速度和鲁棒性表现优异。

Conclusion: TACR-YOLO 在特殊场景下的异常行为检测中提供了新的见解，并推动了该领域的进展。

Abstract: Abnormal Human Behavior Detection (AHBD) under special scenarios is becoming
increasingly crucial. While YOLO-based detection methods excel in real-time
tasks, they remain hindered by challenges including small objects, task
conflicts, and multi-scale fusion in AHBD. To tackle them, we propose
TACR-YOLO, a new real-time framework for AHBD. We introduce a Coordinate
Attention Module to enhance small object detection, a Task-Aware Attention
Module to deal with classification-regression conflicts, and a Strengthen Neck
Network for refined multi-scale fusion, respectively. In addition, we optimize
Anchor Box sizes using K-means clustering and deploy DIoU-Loss to improve
bounding box regression. The Personnel Anomalous Behavior Detection (PABD)
dataset, which includes 8,529 samples across four behavior categories, is also
presented. Extensive experimental results indicate that TACR-YOLO achieves
91.92% mAP on PABD, with competitive speed and robustness. Ablation studies
highlight the contribution of each improvement. This work provides new insights
for abnormal behavior detection under special scenarios, advancing its
progress.

</details>


### [78] [OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring](https://arxiv.org/abs/2508.11482)
*Ruoxin Xiong,Yanyu Wang,Jiannan Cai,Kaijian Liu,Yuansheng Zhu,Pingbo Tang,Nora El-Gohary*

Main category: cs.CV

TL;DR: 研究系统综述了建筑行业的视觉数据集，提出了开源目录和FAIR原则下的未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 建筑行业对视觉数据的依赖日益增加，但现有数据集在规模、数据模态、标注质量和代表性上差异较大，缺乏系统性综述，限制了AI应用的进一步发展。

Method: 通过广泛搜索学术数据库和开放数据平台，收集了51个公开可用的视觉数据集（2005-2024年），并使用结构化数据模式进行分类。

Result: 提出了OpenConstruction开源目录，总结了现有数据集的局限性，并基于FAIR原则提出了未来发展的战略优先级。

Conclusion: 本研究通过系统综述和分类现有建筑视觉数据集，提出了OpenConstruction开源目录，并基于FAIR原则提出了未来数据基础设施的发展路线图，支持建筑行业数据驱动方法的发展。

Abstract: The construction industry increasingly relies on visual data to support
Artificial Intelligence (AI) and Machine Learning (ML) applications for site
monitoring. High-quality, domain-specific datasets, comprising images, videos,
and point clouds, capture site geometry and spatiotemporal dynamics, including
the location and interaction of objects, workers, and materials. However,
despite growing interest in leveraging visual datasets, existing resources vary
widely in sizes, data modalities, annotation quality, and representativeness of
real-world construction conditions. A systematic review to categorize their
data characteristics and application contexts is still lacking, limiting the
community's ability to fully understand the dataset landscape, identify
critical gaps, and guide future directions toward more effective, reliable, and
scalable AI applications in construction. To address this gap, this study
conducts an extensive search of academic databases and open-data platforms,
yielding 51 publicly available visual datasets that span the 2005-2024 period.
These datasets are categorized using a structured data schema covering (i) data
fundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and
point cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)
downstream application domains (e.g., progress tracking). This study
synthesizes these findings into an open-source catalog, OpenConstruction,
supporting data-driven method development. Furthermore, the study discusses
several critical limitations in the existing construction dataset landscape and
presents a roadmap for future data infrastructure anchored in the Findability,
Accessibility, Interoperability, and Reusability (FAIR) principles. By
reviewing the current landscape and outlining strategic priorities, this study
supports the advancement of data-centric solutions in the construction sector.

</details>


### [79] [CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models](https://arxiv.org/abs/2508.11484)
*Xiaoxue Wu,Bingjie Gao,Yu Qiao,Yaohui Wang,Xinyuan Chen*

Main category: cs.CV

TL;DR: CineTrans是一个用于生成具有电影风格转换的多镜头视频的新框架，通过掩码控制机制和专用数据集提升了生成质量和连贯性。


<details>
  <summary>Details</summary>
Motivation: 尽管视频合成技术取得了显著进展，但多镜头视频生成的研究仍处于初级阶段，现有模型的镜头转换能力有限且不稳定。

Method: 构建了一个多镜头视频文本数据集Cine250K，并利用扩散模型中的注意力图与镜头边界的对应关系，设计了一种基于掩码的控制机制，实现任意位置的转换。

Result: CineTrans能够生成具有电影风格转换的连贯多镜头视频，避免了不稳定的过渡或简单拼接，并在所有评估标准上显著优于现有基线。

Conclusion: CineTrans通过引入基于掩码的控制机制和专用评估指标，显著提升了多镜头视频生成的连贯性和电影风格转换的质量，优于现有基线方法。

Abstract: Despite significant advances in video synthesis, research into multi-shot
video generation remains in its infancy. Even with scaled-up models and massive
datasets, the shot transition capabilities remain rudimentary and unstable,
largely confining generated videos to single-shot sequences. In this work, we
introduce CineTrans, a novel framework for generating coherent multi-shot
videos with cinematic, film-style transitions. To facilitate insights into the
film editing style, we construct a multi-shot video-text dataset Cine250K with
detailed shot annotations. Furthermore, our analysis of existing video
diffusion models uncovers a correspondence between attention maps in the
diffusion model and shot boundaries, which we leverage to design a mask-based
control mechanism that enables transitions at arbitrary positions and transfers
effectively in a training-free setting. After fine-tuning on our dataset with
the mask mechanism, CineTrans produces cinematic multi-shot sequences while
adhering to the film editing style, avoiding unstable transitions or naive
concatenations. Finally, we propose specialized evaluation metrics for
transition control, temporal consistency and overall quality, and demonstrate
through extensive experiments that CineTrans significantly outperforms existing
baselines across all criteria.

</details>


### [80] [Automated Building Heritage Assessment Using Street-Level Imagery](https://arxiv.org/abs/2508.11486)
*Kristina Dabrock,Tim Johansson,Anna Donarelli,Mikael Mangold,Noah Pflugradt,Jann Michael Weinand,Jochen Linßen*

Main category: cs.CV

TL;DR: 研究利用GPT和机器学习评估建筑文化遗产价值，验证结果显示结合登记数据效果更佳，支持高效能源改造。


<details>
  <summary>Details</summary>
Motivation: 传统建筑文化遗产价值评估耗时且成本高，需要更高效的方法。

Method: 利用GPT检测建筑立面图像的文化遗产价值，并结合建筑登记数据训练机器学习模型进行分类。

Result: 结合GPT和登记数据的模型宏F1得分为0.71，仅使用GPT数据的得分为0.60。

Conclusion: 该方法可以提高数据库质量，支持在大型能源改造中综合考虑文化遗产价值。

Abstract: Detailed data is required to quantify energy conservation measures in
buildings, such as envelop retrofits, without compromising cultural heritage.
Novel artificial intelligence tools may improve efficiency in identifying
heritage values in buildings compared to costly and time-consuming traditional
inventories. In this study, the large language model GPT was used to detect
various aspects of cultural heritage value in fa\c{c}ade images. Using this
data and building register data as features, machine learning models were
trained to classify multi-family and non-residential buildings in Stockholm,
Sweden. Validation against an expert-created inventory shows a macro F1-score
of 0.71 using a combination of register data and features retrieved from GPT,
and a score of 0.60 using only GPT-derived data. The presented methodology can
contribute to a higher-quality database and thus support careful energy
efficiency measures and integrated consideration of heritage value in
large-scale energetic refurbishment scenarios.

</details>


### [81] [Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.11488)
*Bozhou Zhang,Jingyu Li,Nan Song,Li Zhang*

Main category: cs.CV

TL;DR: VeteranAD是一种端到端自动驾驶框架，通过感知-规划一体化设计，利用规划目标引导感知，显著提升驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法遵循感知-规划范式，但感知和规划是顺序执行的。本文提出感知-规划一体化设计，通过规划目标引导感知，提升规划性能。

Method: 提出了VeteranAD框架，采用感知-规划耦合设计，通过多模式锚定轨迹作为规划先验，感知模块专门收集这些轨迹上的交通元素。采用自回归策略逐步预测未来轨迹，并在每一步聚焦相关区域进行针对性感知。

Result: 在NAVSIM和Bench2Drive数据集上的实验表明，VeteranAD达到了最先进的性能。

Conclusion: VeteranAD通过将感知整合到规划过程中，释放了规划导向端到端方法的潜力，实现了更准确可靠的驾驶行为。

Abstract: End-to-end autonomous driving has achieved remarkable advancements in recent
years. Existing methods primarily follow a perception-planning paradigm, where
perception and planning are executed sequentially within a fully differentiable
framework for planning-oriented optimization. We further advance this paradigm
through a perception-in-plan framework design, which integrates perception into
the planning process. This design facilitates targeted perception guided by
evolving planning objectives over time, ultimately enhancing planning
performance. Building on this insight, we introduce VeteranAD, a coupled
perception and planning framework for end-to-end autonomous driving. By
incorporating multi-mode anchored trajectories as planning priors, the
perception module is specifically designed to gather traffic elements along
these trajectories, enabling comprehensive and targeted perception. Planning
trajectories are then generated based on both the perception results and the
planning priors. To make perception fully serve planning, we adopt an
autoregressive strategy that progressively predicts future trajectories while
focusing on relevant regions for targeted perception at each step. With this
simple yet effective design, VeteranAD fully unleashes the potential of
planning-oriented end-to-end methods, leading to more accurate and reliable
driving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasets
demonstrate that our VeteranAD achieves state-of-the-art performance.

</details>


### [82] [Hierarchical Graph Feature Enhancement with Adaptive Frequency Modulation for Visual Recognition](https://arxiv.org/abs/2508.11497)
*Feiyue Zhao,Zhichao Zhang*

Main category: cs.CV

TL;DR: HGFE框架通过图推理增强CNN的结构感知和特征表示，在多个视觉任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: CNN对规则网格结构的依赖限制了其对复杂拓扑关系和非局部语义的建模能力，需要增强结构感知和特征表示。

Method: 提出了分层图特征增强（HGFE）框架，包含局部空间依赖的窗口内图卷积和全局语义关系的窗口间超节点交互，并引入自适应频率调制模块。

Result: 在CIFAR-100、PASCAL VOC、VisDrone、CrackSeg和CarParts等多个数据集上验证了HGFE的有效性。

Conclusion: HGFE框架通过结合图推理和CNN，显著提升了视觉任务中的结构感知和特征表示能力，并在多个数据集上验证了其有效性。

Abstract: Convolutional neural networks (CNNs) have
  demonstrated strong performance in visual recognition tasks,
  but their inherent reliance on regular grid structures limits
  their capacity to model complex topological relationships and
  non-local semantics within images. To address this limita tion, we propose
the hierarchical graph feature enhancement
  (HGFE), a novel framework that integrates graph-based rea soning into CNNs to
enhance both structural awareness and
  feature representation. HGFE builds two complementary levels
  of graph structures: intra-window graph convolution to cap ture local spatial
dependencies and inter-window supernode
  interactions to model global semantic relationships. Moreover,
  we introduce an adaptive frequency modulation module that
  dynamically balances low-frequency and high-frequency signal
  propagation, preserving critical edge and texture information
  while mitigating over-smoothing. The proposed HGFE module
  is lightweight, end-to-end trainable, and can be seamlessly
  integrated into standard CNN backbone networks. Extensive
  experiments on CIFAR-100 (classification), PASCAL VOC,
  and VisDrone (detection), as well as CrackSeg and CarParts
  (segmentation), validated the effectiveness of the HGFE in
  improving structural representation and enhancing overall
  recognition performance.

</details>


### [83] [AIM: Amending Inherent Interpretability via Self-Supervised Masking](https://arxiv.org/abs/2508.11502)
*Eyad Alshami,Shashank Agnihotri,Bernt Schiele,Margret Keuper*

Main category: cs.CV

TL;DR: AIM通过自监督掩码提升DNNs对真实特征的利用，无需额外标注即可提高模型的解释性和准确性，在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNNs）常同时使用真实和虚假特征，AIM旨在促进网络优先利用真实特征，提升模型的解释性和泛化能力。

Method: AIM利用多阶段编码特征指导自监督、样本特定的特征掩码过程，无需额外标注即可训练出高性能且内在可解释的模型。

Result: 在包括ImageNet100、HardImageNet、ImageWoof以及细粒度分类数据集（如Waterbirds、TravelingBirds、CUB-200）上，AIM在解释性（EPG得分）和准确性上均显著优于基线。

Conclusion: AIM方法通过自监督掩码过程显著提升了模型的解释性和准确性，使其能够更有效地利用真实特征而非虚假特征，从而在多种数据集和架构上实现了一致的性能提升。

Abstract: It has been observed that deep neural networks (DNNs) often use both genuine
as well as spurious features. In this work, we propose "Amending Inherent
Interpretability via Self-Supervised Masking" (AIM), a simple yet interestingly
effective method that promotes the network's utilization of genuine features
over spurious alternatives without requiring additional annotations. In
particular, AIM uses features at multiple encoding stages to guide a
self-supervised, sample-specific feature-masking process. As a result, AIM
enables the training of well-performing and inherently interpretable models
that faithfully summarize the decision process. We validate AIM across a
diverse range of challenging datasets that test both out-of-distribution
generalization and fine-grained visual understanding. These include
general-purpose classification benchmarks such as ImageNet100, HardImageNet,
and ImageWoof, as well as fine-grained classification datasets such as
Waterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual
benefits: interpretability improvements, as measured by the Energy Pointing
Game (EPG) score, and accuracy gains over strong baselines. These consistent
gains across domains and architectures provide compelling evidence that AIM
promotes the use of genuine and meaningful features that directly contribute to
improved generalization and human-aligned interpretability.

</details>


### [84] [A Real-time Concrete Crack Detection and Segmentation Model Based on YOLOv11](https://arxiv.org/abs/2508.11517)
*Shaoze Huang,Qi Liu,Chao Chen,Yuhang Chen*

Main category: cs.CV

TL;DR: 提出YOLOv11-KW-TA-FP模型，通过动态卷积、注意力机制和损失函数优化，显著提升裂缝检测性能，适用于自动化基础设施检测。


<details>
  <summary>Details</summary>
Motivation: 长江三角洲地区交通基础设施的快速老化使得高效混凝土裂缝检测变得尤为重要，而传统人工检测效率低下且现有深度学习模型在复杂背景下小目标裂缝检测性能不佳。

Method: 本研究提出了一种三阶段优化框架：(1) 在骨干网络中嵌入动态KernelWarehouse卷积（KWConv），通过动态核共享机制增强特征表示；(2) 在特征金字塔中引入三重注意力机制（TA），以加强通道-空间交互建模；(3) 设计FP-IoU损失函数，实现自适应边界框回归惩罚。

Result: 实验验证表明，增强模型在基线基础上性能显著提升，达到91.3%的精确率、76.6%的召回率和86.4%的mAP@50。消融研究证实了所提出模块的协同效果，鲁棒性测试表明在数据稀缺和噪声干扰下性能稳定。

Conclusion: 本研究提出了一种基于YOLOv11n架构的多任务混凝土裂缝检测与分割模型YOLOv11-KW-TA-FP，通过动态KernelWarehouse卷积、三重注意力机制和FP-IoU损失函数的三阶段优化框架，显著提升了模型性能，为自动化基础设施检测提供了高效的计算机视觉解决方案，具有重要的工程实用价值。

Abstract: Accelerated aging of transportation infrastructure in the rapidly developing
Yangtze River Delta region necessitates efficient concrete crack detection, as
crack deterioration critically compromises structural integrity and regional
economic growth. To overcome the limitations of inefficient manual inspection
and the suboptimal performance of existing deep learning models, particularly
for small-target crack detection within complex backgrounds, this paper
proposes YOLOv11-KW-TA-FP, a multi-task concrete crack detection and
segmentation model based on the YOLOv11n architecture. The proposed model
integrates a three-stage optimization framework: (1) Embedding dynamic
KernelWarehouse convolution (KWConv) within the backbone network to enhance
feature representation through a dynamic kernel sharing mechanism; (2)
Incorporating a triple attention mechanism (TA) into the feature pyramid to
strengthen channel-spatial interaction modeling; and (3) Designing an FP-IoU
loss function to facilitate adaptive bounding box regression penalization.
Experimental validation demonstrates that the enhanced model achieves
significant performance improvements over the baseline, attaining 91.3%
precision, 76.6% recall, and 86.4% mAP@50. Ablation studies confirm the
synergistic efficacy of the proposed modules. Furthermore, robustness tests
indicate stable performance under conditions of data scarcity and noise
interference. This research delivers an efficient computer vision solution for
automated infrastructure inspection, exhibiting substantial practical
engineering value.

</details>


### [85] [Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction](https://arxiv.org/abs/2508.11531)
*Shilei Wang,Gong Cheng,Pujian Lai,Dong Gao,Junwei Han*

Main category: cs.CV

TL;DR: MST通过轻量级SSE和CSI模块增强多状态特征表示，显著提升跟踪性能且保持高效，GOT-10K数据集AO分数提升4.5%。


<details>
  <summary>Details</summary>
Motivation: 现有高效跟踪器因计算复杂度和模型参数减少导致特征表示能力不足，限制了其在复杂环境中的跟踪准确性。

Method: 提出Multi-State Tracker (MST)，结合轻量级state-specific enhancement (SSE)和cross-state interaction (CSI)模块，通过multi-state generation (MSG)生成多状态特征并优化。

Result: MST在多个数据集上表现优异，计算开销仅0.1 GFLOPs和0.66 M参数，运行效率高，GOT-10K数据集AO分数提升4.5%。

Conclusion: MST通过轻量级SSE和CSI模块显著提升了特征表示能力，同时保持了低计算开销，在多个数据集上超越了以往的高效跟踪器，特别是在GOT-10K数据集上AO分数提升了4.5%。

Abstract: Efficient trackers achieve faster runtime by reducing computational
complexity and model parameters. However, this efficiency often compromises the
expense of weakened feature representation capacity, thus limiting their
ability to accurately capture target states using single-layer features. To
overcome this limitation, we propose Multi-State Tracker (MST), which utilizes
highly lightweight state-specific enhancement (SSE) to perform specialized
enhancement on multi-state features produced by multi-state generation (MSG)
and aggregates them in an interactive and adaptive manner using cross-state
interaction (CSI). This design greatly enhances feature representation while
incurring minimal computational overhead, leading to improved tracking
robustness in complex environments. Specifically, the MSG generates multiple
state representations at multiple stages during feature extraction, while SSE
refines them to highlight target-specific features. The CSI module facilitates
information exchange between these states and ensures the integration of
complementary features. Notably, the introduced SSE and CSI modules adopt a
highly lightweight hidden state adaptation-based state space duality (HSA-SSD)
design, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.
Experimental results demonstrate that MST outperforms all previous efficient
trackers across multiple datasets, significantly improving tracking accuracy
and robustness. In particular, it shows excellent runtime performance, with an
AO score improvement of 4.5\% over the previous SOTA efficient tracker HCAT on
the GOT-10K dataset. The code is available at https://github.com/wsumel/MST.

</details>


### [86] [An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture](https://arxiv.org/abs/2508.11532)
*Jingsong Xia,Yue Yin,Xiuhan Li*

Main category: cs.CV

TL;DR: 该研究提出了一种改进的ConvNeXt-Tiny医学图像分类方法，通过双全局池化和SEVector模块优化特征提取，结合特征平滑损失提升分类性能，在CPU环境下实现了89.10%的准确率。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的计算环境中实现高效且高精度的医学图像分类具有挑战性，因此需要优化现有方法以提升性能。

Method: 通过结构优化和损失函数设计，引入了双全局池化特征融合策略和轻量级通道注意力模块SEVector，并结合特征平滑损失函数。

Result: 在仅使用CPU的条件下，该方法在10个训练周期内达到了89.10%的最高分类准确率，并表现出稳定的收敛趋势。

Conclusion: 该研究提出的基于改进ConvNeXt-Tiny架构的医学图像分类方法，在资源受限环境下显著提升了分类性能，为医学影像分析模型的部署和推广提供了高效解决方案。

Abstract: Intelligent analysis of medical imaging plays a crucial role in assisting
clinical diagnosis. However, achieving efficient and high-accuracy image
classification in resource-constrained computational environments remains
challenging. This study proposes a medical image classification method based on
an improved ConvNeXt-Tiny architecture. Through structural optimization and
loss function design, the proposed method enhances feature extraction
capability and classification performance while reducing computational
complexity. Specifically, the method introduces a dual global pooling (Global
Average Pooling and Global Max Pooling) feature fusion strategy into the
ConvNeXt-Tiny backbone to simultaneously preserve global statistical features
and salient response information. A lightweight channel attention module,
termed Squeeze-and-Excitation Vector (SEVector), is designed to improve the
adaptive allocation of channel weights while minimizing parameter overhead.
Additionally, a Feature Smoothing Loss is incorporated into the loss function
to enhance intra-class feature consistency and suppress intra-class variance.
Under CPU-only conditions (8 threads), the method achieves a maximum
classification accuracy of 89.10% on the test set within 10 training epochs,
exhibiting a stable convergence trend in loss values. Experimental results
demonstrate that the proposed method effectively improves medical image
classification performance in resource-limited settings, providing a feasible
and efficient solution for the deployment and promotion of medical imaging
analysis models.

</details>


### [87] [Reinforcing Video Reasoning Segmentation to Think Before It Segments](https://arxiv.org/abs/2508.11538)
*Sitong Gong,Lu Zhang,Yunzhi Zhuge,Xu Jia,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: Veason-R1是一种专为视频推理分割设计的LVLM，通过GRPO和CoT训练，显著提升性能并增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推理过程中解释性有限，且由于时空推理不足导致性能欠佳。受强化学习突破启发，提出Veason-R1以提升分割中的结构化推理能力。

Method: Veason-R1是一种专为VRS设计的LVLM，采用Group Relative Policy Optimization (GRPO)和Chain-of-Thought (CoT)初始化进行训练。首先通过高质量CoT数据训练Veason-SFT模型，然后通过GRPO优化推理链。

Result: Veason-R1在多个基准测试中达到最先进水平，显著超越现有方法（如ReVOS中J&F提升1.3，ReasonVOS中提升10.0），并展示了对幻觉的鲁棒性（R提升8.8）。

Conclusion: Veason-R1通过GRPO和CoT初始化的训练方法，在视频推理分割任务中实现了最先进的性能，显著超越现有方法，并展示了对抗幻觉的鲁棒性。

Abstract: Video reasoning segmentation (VRS) endeavors to delineate referred objects in
videos guided by implicit instructions that encapsulate human intent and
temporal logic. Previous approaches leverage large vision language models
(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.
However, this paradigm suffers from limited interpretability during inference
and suboptimal performance due to inadequate spatiotemporal reasoning. Drawing
inspiration from seminal breakthroughs in reinforcement learning, we introduce
Veason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in
segmentation. Veason-R1 is trained through Group Relative Policy Optimization
(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we
curate high-quality CoT training data to instill structured reasoning
trajectories, bridging video-level semantics and frame-level spatial grounding,
yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO
fine-tuning encourages efficient exploration of the reasoning space by
optimizing reasoning chains. To this end, we incorporate a holistic reward
mechanism that synergistically enhances spatial alignment and temporal
consistency, bolstering keyframe localization and fine-grained grounding.
Comprehensive empirical evaluations demonstrate that Veason-R1 achieves
state-of-the-art performance on multiple benchmarks, surpassing prior art by
significant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),
while exhibiting robustness to hallucinations (+8.8 R). Our code and model
weights will be available at Veason-R1.

</details>


### [88] [Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model](https://arxiv.org/abs/2508.11550)
*Zuo Zuo,Jiahao Dong,Yanyun Qu,Zongze Wu*

Main category: cs.CV

TL;DR: AAG是一种基于Stable Diffusion的无训练异常生成框架，通过CAE和SAE技术生成逼真异常，解决了工业异常检测中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测中数据稀缺是一个长期挑战，现有异常生成方法缺乏保真度或需要额外训练数据，因此提出了无需训练的AAG框架。

Method: AAG基于Stable Diffusion，通过Cross-Attention Enhancement (CAE)和Self-Attention Enhancement (SAE)技术，实现了在特定区域内生成逼真异常的同时保持其他区域内容不变。

Result: 在MVTec AD和VisA数据集上的广泛实验表明，AAG在异常生成方面具有高效性，且生成的异常图像能提升下游异常检测任务的性能。

Conclusion: AAG框架通过结合Stable Diffusion的强大生成能力，提出了一种无需训练的异常生成方法，有效解决了工业异常检测中的数据稀缺问题，并在多个数据集上验证了其高效性和实用性。

Abstract: Industrial anomaly detection (AD) plays a significant role in manufacturing
where a long-standing challenge is data scarcity. A growing body of works have
emerged to address insufficient anomaly data via anomaly generation. However,
these anomaly generation methods suffer from lack of fidelity or need to be
trained with extra data. To this end, we propose a training-free anomaly
generation framework dubbed AAG, which is based on Stable Diffusion (SD)'s
strong generation ability for effective anomaly image generation. Given a
normal image, mask and a simple text prompt, AAG can generate realistic and
natural anomalies in the specific regions and simultaneously keep contents in
other regions unchanged. In particular, we propose Cross-Attention Enhancement
(CAE) to re-engineer the cross-attention mechanism within Stable Diffusion
based on the given mask. CAE increases the similarity between visual tokens in
specific regions and text embeddings, which guides these generated visual
tokens in accordance with the text description. Besides, generated anomalies
need to be more natural and plausible with object in given image. We propose
Self-Attention Enhancement (SAE) which improves similarity between each normal
visual token and anomaly visual tokens. SAE ensures that generated anomalies
are coherent with original pattern. Extensive experiments on MVTec AD and VisA
datasets demonstrate effectiveness of AAG in anomaly generation and its
utility. Furthermore, anomaly images generated by AAG can bolster performance
of various downstream anomaly inspection tasks.

</details>


### [89] [TrajSV: A Trajectory-based Model for Sports Video Representations and Applications](https://arxiv.org/abs/2508.11569)
*Zheng Wang,Shihao Xu,Wei Shi*

Main category: cs.CV

TL;DR: TrajSV是一个基于轨迹的体育视频分析框架，通过无监督学习和Transformer模块显著提升了视频检索、动作识别和字幕生成的性能。


<details>
  <summary>Details</summary>
Motivation: 体育分析领域存在数据不可用、缺乏有效的轨迹框架和监督标签不足等问题，TrajSV旨在解决这些挑战。

Method: TrajSV包含数据预处理、Clip Representation Network (CRNet)和Video Representation Network (VRNet)三个模块，采用轨迹增强的Transformer模块和无监督学习优化视频和片段表示。

Result: 在三种体育视频数据集上，TrajSV在视频检索中提升近70%，动作识别中17个类别中9个达到最佳，视频字幕生成提升近20%。

Conclusion: TrajSV框架通过轨迹增强的Transformer模块和三重对比损失优化，在体育视频检索、动作识别和视频字幕生成等应用中表现出色，显著提升了性能。

Abstract: Sports analytics has received significant attention from both academia and
industry in recent years. Despite the growing interest and efforts in this
field, several issues remain unresolved, including (1) data unavailability, (2)
lack of an effective trajectory-based framework, and (3) requirement for
sufficient supervision labels. In this paper, we present TrajSV, a
trajectory-based framework that addresses various issues in existing studies.
TrajSV comprises three components: data preprocessing, Clip Representation
Network (CRNet), and Video Representation Network (VRNet). The data
preprocessing module extracts player and ball trajectories from sports
broadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to
learn clip representations based on these trajectories. Additionally, VRNet
learns video representations by aggregating clip representations and visual
features with an encoder-decoder architecture. Finally, a triple contrastive
loss is introduced to optimize both video and clip representations in an
unsupervised manner. The experiments are conducted on three broadcast video
datasets to verify the effectiveness of TrajSV for three types of sports (i.e.,
soccer, basketball, and volleyball) with three downstream applications (i.e.,
sports video retrieval, action spotting, and video captioning). The results
demonstrate that TrajSV achieves state-of-the-art performance in sports video
retrieval, showcasing a nearly 70% improvement. It outperforms baselines in
action spotting, achieving state-of-the-art results in 9 out of 17 action
categories, and demonstrates a nearly 20% improvement in video captioning.
Additionally, we introduce a deployed system along with the three applications
based on TrajSV.

</details>


### [90] [Causality Matters: How Temporal Information Emerges in Video Language Models](https://arxiv.org/abs/2508.11576)
*Yumeng Shi,Quanyu Long,Yin Wu,Wenya Wang*

Main category: cs.CV

TL;DR: 本文研究发现视频语言模型的时间推理通过帧间注意力逐步合成，并提出两种效率策略，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管视频语言模型在多模态理解方面取得了进展，但时间理解（如事件顺序、持续时间和跨时间关系）仍是核心挑战。本文旨在揭示时间信息在模型中的整合机制。

Method: 通过分析实验追踪时间信息在模型中的整合方式，发现时间线索通过帧间注意力逐步合成，并在最后一帧中聚合。基于此，提出了分阶段跨模态注意力和时间退出机制。

Result: 去除或修改视频输入中的位置编码对时间理解的性能影响很小，而反转帧序列会导致显著下降。提出的两种策略在基准测试中验证了其有效性。

Conclusion: 本文揭示了视频语言模型中时间推理的因果信息路径，并提出了两种效率导向的策略，验证了其在两个基准测试中的有效性。

Abstract: Video language models (VideoLMs) have made significant progress in multimodal
understanding. However, temporal understanding, which involves identifying
event order, duration, and relationships across time, still remains a core
challenge. Prior works emphasize positional encodings (PEs) as a key mechanism
for encoding temporal structure. Surprisingly, we find that removing or
modifying PEs in video inputs yields minimal degradation in the performance of
temporal understanding. In contrast, reversing the frame sequence while
preserving the original PEs causes a substantial drop. To explain this
behavior, we conduct substantial analysis experiments to trace how temporal
information is integrated within the model. We uncover a causal information
pathway: temporal cues are progressively synthesized through inter-frame
attention, aggregated in the final frame, and subsequently integrated into the
query tokens. This emergent mechanism shows that temporal reasoning emerges
from inter-visual token interactions under the constraints of causal attention,
which implicitly encodes temporal structure. Based on these insights, we
propose two efficiency-oriented strategies: staged cross-modal attention and a
temporal exit mechanism for early token truncation. Experiments on two
benchmarks validate the effectiveness of both approaches. To the best of our
knowledge, this is the first work to systematically investigate video temporal
understanding in VideoLMs, offering insights for future model improvement.

</details>


### [91] [DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring](https://arxiv.org/abs/2508.11591)
*Durga Joshi,Chandi Witharana,Robert Fahey,Thomas Worthley,Zhe Zhu,Diego Cerrai*

Main category: cs.CV

TL;DR: 该研究利用车载摄像头视频数据，通过单目深度估计和几何三角测量，开发了一种低成本、实时的对象级结构评估和地理定位框架，适用于城市植被和基础设施监测。


<details>
  <summary>Details</summary>
Motivation: 传统遥感方法如LiDAR和图像成本高且不实时，本研究旨在利用消费者级视频数据提供一种快速、实时且经济高效的解决方案，用于对象级监测植被风险和基础设施暴露。

Method: 结合单目深度估计、深度误差校正和几何三角测量，开发了一个端到端管道，从车载摄像头的街级视频流中生成准确的空间和结构数据。深度图首先使用最先进的单目深度模型估计，然后通过梯度提升回归框架进行细化以校正低估，特别是对于远处物体。

Result: 深度校正模型表现出强大的预测性能（R2 = 0.92，MAE = 0.31），显著减少了15米以外的偏差。对象位置通过基于GPS的三角测量估计，对象高度通过针孔相机几何计算。在低速车辆和内部摄像头条件下，地理定位误差平均为2.83米，高度估计的平均绝对误差（MAE）树木为2.09米，杆为0.88米。

Conclusion: 该研究提出了一种低成本、可复制的框架，利用车载摄像头视频数据进行实时、对象级的结构评估和地理定位，为城市植被和基础设施监测提供了快速、实时且经济高效的解决方案，尤其适用于公用事业公司和城市规划者。

Abstract: Our study introduces a novel, low-cost, and reproducible framework for
real-time, object-level structural assessment and geolocation of roadside
vegetation and infrastructure with commonly available but underutilized
dashboard camera (dashcam) video data. We developed an end-to-end pipeline that
combines monocular depth estimation, depth error correction, and geometric
triangulation to generate accurate spatial and structural data from
street-level video streams from vehicle-mounted dashcams. Depth maps were first
estimated using a state-of-the-art monocular depth model, then refined via a
gradient-boosted regression framework to correct underestimations, particularly
for distant objects. The depth correction model achieved strong predictive
performance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly
reducing bias beyond 15 m. Further, object locations were estimated using
GPS-based triangulation, while object heights were calculated using pin hole
camera geometry. Our method was evaluated under varying conditions of camera
placement and vehicle speed. Low-speed vehicle with inside camera gave the
highest accuracy, with mean geolocation error of 2.83 m, and mean absolute
error (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To
the best of our knowledge, it is the first framework to combine monocular depth
modeling, triangulated GPS-based geolocation, and real-time structural
assessment for urban vegetation and infrastructure using consumer-grade video
data. Our approach complements conventional RS methods, such as LiDAR and image
by offering a fast, real-time, and cost-effective solution for object-level
monitoring of vegetation risks and infrastructure exposure, making it
especially valuable for utility companies, and urban planners aiming for
scalable and frequent assessments in dynamic urban environments.

</details>


### [92] [CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion](https://arxiv.org/abs/2508.11603)
*Zhe Zhu,Honghua Chen,Peng Li,Mingqiang Wei*

Main category: cs.CV

TL;DR: CoreEditor通过对应约束注意力机制和选择性编辑，解决了多视图一致性问题，提升了文本驱动3D编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视图信息交换上缺乏显式控制，导致编辑不足和细节模糊。

Method: 提出了CoreEditor框架，采用对应约束的注意力机制结合语义相似性，增强多视图一致性，并设计选择性编辑流程。

Result: 实验表明，CoreEditor能生成高质量、3D一致的编辑结果，细节更清晰。

Conclusion: CoreEditor通过引入对应约束的注意力机制和选择性编辑流程，显著提升了文本驱动3D编辑的质量和一致性，优于现有方法。

Abstract: Text-driven 3D editing seeks to modify 3D scenes according to textual
descriptions, and most existing approaches tackle this by adapting pre-trained
2D image editors to multi-view inputs. However, without explicit control over
multi-view information exchange, they often fail to maintain cross-view
consistency, leading to insufficient edits and blurry details. We introduce
CoreEditor, a novel framework for consistent text-to-3D editing. The key
innovation is a correspondence-constrained attention mechanism that enforces
precise interactions between pixels expected to remain consistent throughout
the diffusion denoising process. Beyond relying solely on geometric alignment,
we further incorporate semantic similarity estimated during denoising, enabling
more reliable correspondence modeling and robust multi-view editing. In
addition, we design a selective editing pipeline that allows users to choose
preferred results from multiple candidates, offering greater flexibility and
user control. Extensive experiments show that CoreEditor produces high-quality,
3D-consistent edits with sharper details, significantly outperforming prior
methods.

</details>


### [93] [LoRAtorio: An intrinsic approach to LoRA Skill Composition](https://arxiv.org/abs/2508.11624)
*Niki Foteinopoulou,Ignas Budvytis,Stephan Liwicki*

Main category: cs.CV

TL;DR: LoRAtorio是一种无需训练的多LoRA组合框架，通过空间感知权重矩阵和修改的分类器无关引导，显著提升了开放环境中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放环境中难以有效组合多个LoRA适配器，特别是在所需技能的数量和性质未知的情况下。LoRAtorio的提出基于两个关键观察：窄域训练的LoRA适配器产生的去噪输出与基础模型存在差异，以及在分布外操作时LoRA输出更接近基础模型行为。

Method: 该方法在潜在空间中通过划分空间补丁并计算每个补丁预测噪声与基础模型之间的余弦相似度，构建空间感知权重矩阵，指导LoRA输出的加权聚合。此外，还提出了对分类器无关引导的修改，以解决领域漂移问题。

Result: LoRAtorio在ClipScore上实现了高达1.3%的提升，在GPT-4V成对评估中获得了72.43%的胜率，并在多种潜在扩散模型中展现了良好的泛化能力。

Conclusion: LoRAtorio通过利用内在模型行为，提出了一种无需训练的多LoRA组合框架，显著提升了在开放环境中的性能表现，并在多种潜在扩散模型中展现了良好的泛化能力。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted technique in
text-to-image diffusion models, enabling the personalisation of visual concepts
such as characters, styles, and objects. However, existing approaches struggle
to effectively compose multiple LoRA adapters, particularly in open-ended
settings where the number and nature of required skills are not known in
advance. In this work, we present LoRAtorio, a novel train-free framework for
multi-LoRA composition that leverages intrinsic model behaviour. Our method is
motivated by two key observations: (1) LoRA adapters trained on narrow domains
produce denoised outputs that diverge from the base model, and (2) when
operating out-of-distribution, LoRA outputs show behaviour closer to the base
model than when conditioned in distribution. The balance between these two
observations allows for exceptional performance in the single LoRA scenario,
which nevertheless deteriorates when multiple LoRAs are loaded. Our method
operates in the latent space by dividing it into spatial patches and computing
cosine similarity between each patch's predicted noise and that of the base
model. These similarities are used to construct a spatially-aware weight
matrix, which guides a weighted aggregation of LoRA outputs. To address domain
drift, we further propose a modification to classifier-free guidance that
incorporates the base model's unconditional score into the composition. We
extend this formulation to a dynamic module selection setting, enabling
inference-time selection of relevant LoRA adapters from a large pool. LoRAtorio
achieves state-of-the-art performance, showing up to a 1.3% improvement in
ClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises
effectively to multiple latent diffusion models.

</details>


### [94] [Thyme: Think Beyond Images](https://arxiv.org/abs/2508.11630)
*Yi-Fan Zhang,Xingyu Lu,Shukang Yin,Chaoyou Fu,Wei Chen,Xiao Hu,Bin Wen,Kaiyu Jiang,Changyi Liu,Tianke Zhang,Haonan Fan,Kaibing Chen,Jiankang Chen,Haojie Ding,Kaiyu Tang,Zhang Zhang,Liang Wang,Fan Yang,Tingting Gao,Guorui Zhou*

Main category: cs.CV

TL;DR: Thyme 是一种新范式，通过代码生成和执行增强 MLLMs 的图像处理和推理能力，两阶段训练策略（SFT+RL）显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前开源模型在图像操作和逻辑推理能力上不及专有模型（如 O3），因此需要一种新方法，通过代码增强多模态模型的推理和操作能力。

Method: 采用两阶段训练策略：初始 SFT 阶段在 50 万样本的精选数据集上训练代码生成能力，随后通过 RL 阶段（使用 GRPO-ATS 算法）优化决策能力。

Result: 在近 20 个基准测试中，Thyme 表现出显著且一致的性能提升，尤其是在高分辨率感知和复杂推理任务中。

Conclusion: Thyme 提出了一种新颖的范式，通过自主生成和执行可执行代码，使 MLLMs 超越现有的“图像思维”方法，显著提升了在高分辨率感知和复杂推理任务中的性能。

Abstract: Following OpenAI's introduction of the ``thinking with images'' concept,
recent efforts have explored stimulating the use of visual information in the
reasoning process to enhance model performance in perception and reasoning
tasks. However, to the best of our knowledge, no open-source work currently
offers a feature set as rich as proprietary models (O3), which can perform
diverse image manipulations and simultaneously enhance logical reasoning
capabilities through code. In this paper, we make a preliminary attempt in this
direction by introducing Thyme (Think Beyond Images), a novel paradigm for
enabling MLLMs to transcend existing ``think with images'' approaches by
autonomously generating and executing diverse image processing and
computational operations via executable code. This approach not only
facilitates a rich, on-the-fly set of image manipulations (e.g., cropping,
rotation, contrast enhancement) but also allows for mathematical computations,
all while maintaining high autonomy in deciding when and how to apply these
operations. We activate this capability through a two-stage training strategy:
an initial SFT on a curated dataset of 500K samples to teach code generation,
followed by a RL phase to refine decision-making. For the RL stage, we manually
collect and design high-resolution question-answer pairs to increase the
learning difficulty, and we propose GRPO-ATS (Group Relative Policy
Optimization with Adaptive Temperature Sampling), an algorithm that applies
distinct temperatures to text and code generation to balance reasoning
exploration with code execution precision. We conduct extensive experimental
analysis and ablation studies. Comprehensive evaluations on nearly 20
benchmarks show that Thyme yields significant and consistent performance gains,
particularly in challenging high-resolution perception and complex reasoning
tasks.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [95] [A Gentle Wakeup Call: Symmetry Breaking with Less Collision Cost](https://arxiv.org/abs/2508.11006)
*Umesh Biswas,Maxwell Young*

Main category: cs.DS

TL;DR: Aim-High算法优化了唤醒问题中的延迟和碰撞成本，适用于静态和动态场景。


<details>
  <summary>Details</summary>
Motivation: 现有唤醒算法在延迟方面表现良好，但忽视了碰撞带来的显著延迟，导致总延迟主要由碰撞贡献。

Method: 提出并分析了一种随机唤醒算法Aim-High，适用于静态和动态版本的唤醒问题。

Result: Aim-High算法在足够大的$C$和有限错误下，实现了近$O(\sqrt{C})$的延迟和碰撞成本；否则在静态和动态设置下分别达到$O(\texttt{poly}{(\log n)})$和$O(n\,\texttt{poly}{(\log n)})$。

Conclusion: 本研究设计的Aim-High算法在静态和动态版本的唤醒问题中，均能实现近$O(\sqrt{C})$的延迟和碰撞成本，或在特定条件下达到$O(\texttt{poly}{(\log n)})$或$O(n\,\texttt{poly}{(\log n)})$的性能。

Abstract: The wakeup problem addresses the fundamental challenge of symmetry breaking.
There are $n$ devices sharing a time-slotted multiple access channel. In any
fixed slot, if a single device sends a packet, it succeeds; however, if two or
more devices send, then there is a collision and none of the corresponding
packets succeed. For the static version of wakeup, all packets are initially
active (i.e., can send and listen on the channel); for the dynamic version, the
packets become active at arbitrary times. In both versions, the goal is to
successfully send a single packet.
  Prior results on wakeup have largely focused on the number of slots until the
first success; that is, the latency. However, in many modern systems,
collisions introduce significant delay, an aspect that current wakeup
algorithms do not address. For instance, while existing results for static
wakeup have polylogarithmic-in-$n$ latency, they can incur additional latency
that is {\it linear} in the cost of a collision $C$. Thus, the total latency is
large and dominated by the contributions from collisions.
  Here, we design and analyze a randomized wakeup algorithm, Aim-High. For
sufficiently large $C$ and with bounded error, Aim-High has latency and
expected collision cost that is nearly $O(\sqrt{C})$ for both the static and
dynamic versions. Otherwise, the latency and expected collision cost are
$O(\texttt{poly}{(\log n)})$ for the static setting, and
$O(n\,\texttt{poly}{(\log n)})$ for the dynamic setting. We also establish
lower bounds that complement these results.

</details>


### [96] [Sampling tree-weighted partitions without sampling trees](https://arxiv.org/abs/2508.11130)
*Sarah Cannon,Wesley Pegden,Jamie Tucker-Foltz*

Main category: cs.DS

TL;DR: 提出直接采样平衡树加权2分区的新算法，避免生成树采样瓶颈，在特定平面图上实现线性时间复杂度，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 计算重划分析中对平衡树加权k分区的需求推动了研究，现有方法因需先采样生成树而存在计算瓶颈。

Method: 通过直接采样平衡树加权2分区的新算法，避免传统方法中先采样生成树的瓶颈。算法在特定平面图类上实现线性时间复杂度O(n)。

Result: 新算法在典型平面图上实现O(n)时间复杂度，比现有随机树生成方法更快（O(n log² n)近似采样，O(n^(1 + log log log n / log log n))精确采样）。

Conclusion: 本文提出了一种直接采样平衡树加权2分区的新算法，无需先采样生成树，且接受率和拒绝率与传统方法相同。该算法在典型的地理数据网络结构上具有线性时间复杂度O(n)，显著快于现有随机树生成方法。

Abstract: This paper gives a new algorithm for sampling tree-weighted partitions of a
large class of planar graphs. Formally, the tree-weighted distribution on
$k$-partitions of a graph weights $k$-partitions proportional to the product of
the number of spanning trees of each partition class. Recent work on problems
in computational redistricting analysis has driven special interest in the
conditional distribution where all partition classes have the same size
(balanced partitions). One class of Markov chains in wide use aims to sample
from balanced tree-weighted $k$-partitions using a sampler for balanced
tree-weighted 2-partitions. Previous implementations of this 2-partition
sampler would draw a random spanning tree and check whether it contains an edge
whose removal produces a balanced 2-component forest; if it does, this
2-partition is accepted, otherwise the algorithm rejects and repeats. In
practice, this is a significant computational bottleneck.
  We show that in fact it is possible to sample from the balanced tree-weighted
2-partition distribution directly, without first sampling a spanning tree; the
acceptance and rejection rates are the same as in previous samplers. We prove
that on a wide class of planar graphs encompassing network structures typically
arising from the geographic data used in computational redistricting, our
algorithm takes expected linear time $O(n)$. Notably, this is asymptotically
faster than the best known method to generate random trees, which is $O(n
\log^2 n)$ for approximate sampling and $O(n^{1 + \log \log \log n / \log \log
n})$ for exact sampling. Additionally, we show that a variant of our algorithm
also gives a speedup to $O(n \log n)$ for exact sampling of uniformly random
trees on these families of graphs, improving the bounds for both exact and
approximate sampling.

</details>


### [97] [Face-hitting dominating sets in planar graphs: Alternative proof and linear-time algorithm](https://arxiv.org/abs/2508.11444)
*Therese Biedl*

Main category: cs.DS

TL;DR: 本文提出了一种新的构造性证明，使用线性时间算法将平面图划分为两个支配且面碰的集合，避免了复杂的4色定理依赖。


<details>
  <summary>Details</summary>
Motivation: 原证明依赖于4色定理且非算法化，难以实现。本文旨在提供一种更简单、更易实现的构造性证明。

Method: 通过将图分解为2连通分量、寻找耳分解以及在3正则平面图中计算完美匹配，这些均已知有线性时间算法。

Result: 成功构造性地证明了每个n顶点平面图G可以在线性时间内划分为两个支配且面碰的集合。

Conclusion: 本文提供了一种新的构造性证明，表明每个n顶点平面图G在相同限制条件下可以划分为两个支配且面碰的集合。该方法避免了复杂的算法实现，如4色定理，转而使用线性时间算法即可完成。

Abstract: In a recent paper, Francis, Illickan, Jose and Rajendraprasad showed that
every $n$-vertex plane graph $G$ has (under some natural restrictions) a
vertex-partition into two sets $V_1$ and $V_2$ such that each $V_i$ is
\emph{dominating} (every vertex of $G$ contains a vertex of $V_i$ in its closed
neighbourhood) and \emph{face-hitting} (every face of $G$ is incident to a
vertex of $V_i$). Their proof works by considering a supergraph $G'$ of $G$
that has certain properties, and among all such graphs, taking one that has the
fewest edges. As such, their proof is not algorithmic. Their proof also relies
on the 4-color theorem, for which a quadratic-time algorithm exists, but it
would not be easy to implement.
  In this paper, we give a new proof that every $n$-vertex plane graph $G$ has
(under the same restrictions) a vertex-partition into two dominating
face-hitting sets. Our proof is constructive, and requires nothing more
complicated than splitting a graph into 2-connected components, finding an ear
decomposition, and computing a perfect matching in a 3-regular plane graph. For
all these problems, linear-time algorithms are known and so we can find the
vertex-partition in linear time.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [98] [Developing and Validating a High-Throughput Robotic System for the Accelerated Development of Porous Membranes](https://arxiv.org/abs/2508.10973)
*Hongchen Wang,Sima Zeinali Danalou,Jiahao Zhu,Kenneth Sulimro,Chaewon Lim,Smita Basak,Aimee Tai,Usan Siriwardana,Jason Hattrick-Simpers,Jay Werber*

Main category: cs.RO

TL;DR: 开发了一个自动化平台，用于通过NIPS技术高效、一致地制造和表征多孔聚合物膜，验证了其在数据驱动优化中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统多孔聚合物膜的开发过程劳动密集且依赖试错，需要一个自动化平台提高效率和一致性。

Method: 通过非溶剂诱导相分离（NIPS）技术，集成了自动溶液制备、刀片浇铸、受控浸没和压缩测试的系统，实现了对制造参数的精确控制。

Result: 自动化系统成功复制了聚合物浓度和环境湿度对膜性能的影响，验证了其在高通量实验中的有效性。

Conclusion: 开发的自动化平台为多孔聚合物膜的数据驱动优化提供了可扩展且可重复的基础，适合集成到自驱动实验室工作流程中。

Abstract: The development of porous polymeric membranes remains a labor-intensive
process, often requiring extensive trial and error to identify optimal
fabrication parameters. In this study, we present a fully automated platform
for membrane fabrication and characterization via nonsolvent-induced phase
separation (NIPS). The system integrates automated solution preparation, blade
casting, controlled immersion, and compression testing, allowing precise
control over fabrication parameters such as polymer concentration and ambient
humidity. The modular design allows parallel processing and reproducible
handling of samples, reducing experimental time and increasing consistency.
Compression testing is introduced as a sensitive mechanical characterization
method for estimating membrane stiffness and as a proxy to infer porosity and
intra-sample uniformity through automated analysis of stress-strain curves. As
a proof of concept to demonstrate the effectiveness of the system, NIPS was
carried out with polysulfone, the green solvent PolarClean, and water as the
polymer, solvent, and nonsolvent, respectively. Experiments conducted with the
automated system reproduced expected effects of polymer concentration and
ambient humidity on membrane properties, namely increased stiffness and
uniformity with increasing polymer concentration and humidity variations in
pore morphology and mechanical response. The developed automated platform
supports high-throughput experimentation and is well-suited for integration
into self-driving laboratory workflows, offering a scalable and reproducible
foundation for data-driven optimization of porous polymeric membranes through
NIPS.

</details>


### [99] [Robust Online Calibration for UWB-Aided Visual-Inertial Navigation with Bias Correction](https://arxiv.org/abs/2508.10999)
*Yizhi Zhou,Jie Xu,Jiawei Xia,Zechen Hu,Weizi Li,Xuan Wang*

Main category: cs.RO

TL;DR: 提出了一种鲁棒在线校准框架，通过显式处理机器人定位不确定性和SKF在线细化，显著提升UWB锚点校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有UWB锚点校准方法存在两个主要问题：1）忽略初始化阶段的机器人定位误差，影响校准鲁棒性；2）校准结果对UWB锚点初始位置猜测高度敏感。本研究旨在解决这些问题，提升实际应用中的校准性能。

Method: 提出了一种紧密耦合的Schmidt卡尔曼滤波器（SKF）在线细化方法，显式地将机器人定位不确定性纳入校准过程。

Result: 仿真和实际实验表明，该方法在准确性和鲁棒性上优于现有方法。

Conclusion: 本研究提出了一种新颖的鲁棒在线校准框架，通过显式考虑机器人定位不确定性并采用紧密耦合的Schmidt卡尔曼滤波器（SKF）在线细化方法，显著提高了超宽带（UWB）锚点校准的准确性和鲁棒性。仿真和实际实验验证了该方法的有效性。

Abstract: This paper presents a novel robust online calibration framework for
Ultra-Wideband (UWB) anchors in UWB-aided Visual-Inertial Navigation Systems
(VINS). Accurate anchor positioning, a process known as calibration, is crucial
for integrating UWB ranging measurements into state estimation. While several
prior works have demonstrated satisfactory results by using robot-aided systems
to autonomously calibrate UWB systems, there are still some limitations: 1)
these approaches assume accurate robot localization during the initialization
step, ignoring localization errors that can compromise calibration robustness,
and 2) the calibration results are highly sensitive to the initial guess of the
UWB anchors' positions, reducing the practical applicability of these methods
in real-world scenarios. Our approach addresses these challenges by explicitly
incorporating the impact of robot localization uncertainties into the
calibration process, ensuring robust initialization. To further enhance the
robustness of the calibration results against initialization errors, we propose
a tightly-coupled Schmidt Kalman Filter (SKF)-based online refinement method,
making the system suitable for practical applications. Simulations and
real-world experiments validate the improved accuracy and robustness of our
approach.

</details>


### [100] [3D FlowMatch Actor: Unified 3D Policy for Single- and Dual-Arm Manipulation](https://arxiv.org/abs/2508.11002)
*Nikolaos Gkanatsios,Jiahe Xu,Matthew Bronars,Arsalan Mousavian,Tsung-Wei Ke,Katerina Fragkiadaki*

Main category: cs.RO

TL;DR: 3DFA是一种结合流匹配和3D视觉表示的机器人操作策略，训练和推理速度提升30倍，性能显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有3D扩散策略训练和推理速度慢的问题，同时保持或提升性能。

Method: 3DFA结合流匹配进行轨迹预测和3D预训练视觉场景表示，利用3D相对注意力机制在动作去噪过程中优化动作和视觉令牌的交互。

Result: 3DFA在训练和推理速度上比现有方法快30倍，在双手机器人操作基准测试中性能提升41.4%，在单手机器人操作中直接在74个RLBench任务上取得最佳成绩。

Conclusion: 3DFA通过结合流匹配和3D预训练视觉场景表示，显著提升了机器人操作的效率和性能，成为新的技术标杆。

Abstract: We present 3D FlowMatch Actor (3DFA), a 3D policy architecture for robot
manipulation that combines flow matching for trajectory prediction with 3D
pretrained visual scene representations for learning from demonstration. 3DFA
leverages 3D relative attention between action and visual tokens during action
denoising, building on prior work in 3D diffusion-based single-arm policy
learning. Through a combination of flow matching and targeted system-level and
architectural optimizations, 3DFA achieves over 30x faster training and
inference than previous 3D diffusion-based policies, without sacrificing
performance. On the bimanual PerAct2 benchmark, it establishes a new state of
the art, outperforming the next-best method by an absolute margin of 41.4%. In
extensive real-world evaluations, it surpasses strong baselines with up to
1000x more parameters and significantly more pretraining. In unimanual
settings, it sets a new state of the art on 74 RLBench tasks by directly
predicting dense end-effector trajectories, eliminating the need for motion
planning. Comprehensive ablation studies underscore the importance of our
design choices for both policy effectiveness and efficiency.

</details>


### [101] [GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning](https://arxiv.org/abs/2508.11049)
*Kelin Yu,Sheng Zhang,Harshit Soora,Furong Huang,Heng Huang,Pratap Tokekar,Ruohan Gao*

Main category: cs.RO

TL;DR: GenFlowRL通过从多样化的跨体现数据集中提取生成的流来形成奖励，学习通用且稳健的策略，在多个操作任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型在机器人学习中存在对生成数据质量的依赖以及对精细操作的困难。视频强化学习虽能提升策略稳健性，但仍受限于视频生成的不确定性和大规模机器人数据集收集的挑战。

Method: GenFlowRL从多样化的跨体现数据集中训练生成的流，并从中提取形状奖励。这种方法利用低维、以对象为中心的特征来学习通用且稳健的策略。

Result: 在10个操作任务的实验中，GenFlowRL在模拟和真实世界的跨体现评估中均表现出色，能够有效利用从生成的对象中心流中提取的操作特征。

Conclusion: GenFlowRL通过从多样化的跨体现数据集中提取生成的流来形成奖励，从而能够利用低维、以对象为中心的特征学习通用且稳健的策略。在10个操作任务的实验中，无论是在模拟还是真实世界的跨体现评估中，GenFlowRL均表现优异。

Abstract: Recent advances have shown that video generation models can enhance robot
learning by deriving effective robot actions through inverse dynamics. However,
these methods heavily depend on the quality of generated data and struggle with
fine-grained manipulation due to the lack of environment feedback. While
video-based reinforcement learning improves policy robustness, it remains
constrained by the uncertainty of video generation and the challenges of
collecting large-scale robot datasets for training diffusion models. To address
these limitations, we propose GenFlowRL, which derives shaped rewards from
generated flow trained from diverse cross-embodiment datasets. This enables
learning generalizable and robust policies from diverse demonstrations using
low-dimensional, object-centric features. Experiments on 10 manipulation tasks,
both in simulation and real-world cross-embodiment evaluations, demonstrate
that GenFlowRL effectively leverages manipulation features extracted from
generated object-centric flow, consistently achieving superior performance
across diverse and challenging scenarios. Our Project Page:
https://colinyu1.github.io/genflowrl

</details>


### [102] [Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance](https://arxiv.org/abs/2508.11093)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: 该论文提出通过增强GUIDER框架，结合VLM和LLM形成语义先验，优化机器人导航和操作能力，以适应人机协作中的实时意图变化。


<details>
  <summary>Details</summary>
Motivation: 人机协作需要机器人快速推断用户意图、提供透明推理并协助用户实现目标。

Method: 提出通过视觉语言模型（VLM）和纯文本语言模型（LLM）增强GUIDER框架，形成语义先验，筛选基于任务提示的对象和位置。视觉流程（YOLO用于目标检测，Segment Anything Model用于实例分割）将候选对象输入VLM，VLM根据操作员提示评分相关性；同时，纯文本LLM对检测到的对象标签进行排名。这些分数加权GUIDER的导航和操作层，选择上下文相关目标并抑制无关对象。

Result: 一旦综合信念超过阈值，自主性变化发生，使机器人能够导航到目标区域并检索所需对象，同时适应操作员意图的任何变化。

Conclusion: 未来工作将在Isaac Sim中使用Franka Emika机械臂和Ridgeback底座评估系统，重点关注实时辅助功能。

Abstract: Human-robot collaboration requires robots to quickly infer user intent,
provide transparent reasoning, and assist users in achieving their goals. Our
recent work introduced GUIDER, our framework for inferring navigation and
manipulation intents. We propose augmenting GUIDER with a vision-language model
(VLM) and a text-only language model (LLM) to form a semantic prior that
filters objects and locations based on the mission prompt. A vision pipeline
(YOLO for object detection and the Segment Anything Model for instance
segmentation) feeds candidate object crops into the VLM, which scores their
relevance given an operator prompt; in addition, the list of detected object
labels is ranked by a text-only LLM. These scores weight the existing
navigation and manipulation layers of GUIDER, selecting context-relevant
targets while suppressing unrelated objects. Once the combined belief exceeds a
threshold, autonomy changes occur, enabling the robot to navigate to the
desired area and retrieve the desired object, while adapting to any changes in
the operator's intent. Future work will evaluate the system on Isaac Sim using
a Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.

</details>


### [103] [Robot Policy Evaluation for Sim-to-Real Transfer: A Benchmarking Perspective](https://arxiv.org/abs/2508.11117)
*Xuning Yang,Clemens Eppner,Jonathan Tremblay,Dieter Fox,Stan Birchfield,Fabio Ramos*

Main category: cs.RO

TL;DR: 论文讨论了通用机器人操作策略基准设计的挑战，提出了高视觉保真度模拟、任务复杂性评估和性能对齐量化等改进模拟到现实转移的方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉的机器人模拟基准在机器人操作研究中取得了显著进展，但机器人本质上是现实世界的问题，通用策略在现实应用中的评估滞后。

Method: 1) 利用高视觉保真度模拟改进模拟到现实转移；2) 通过系统增加任务复杂性和场景扰动评估策略的鲁棒性；3) 量化现实世界性能与其模拟对应物之间的性能对齐。

Result: 论文提出了改进模拟到现实策略转移的基准设计方法，并建议通过高保真模拟、任务复杂性增加和性能对齐量化来评估策略。

Conclusion: 论文提出了设计通用机器人操作策略基准的挑战和需求，并提出了三个具体建议以改进模拟到现实的策略转移效果。

Abstract: Current vision-based robotics simulation benchmarks have significantly
advanced robotic manipulation research. However, robotics is fundamentally a
real-world problem, and evaluation for real-world applications has lagged
behind in evaluating generalist policies. In this paper, we discuss challenges
and desiderata in designing benchmarks for generalist robotic manipulation
policies for the goal of sim-to-real policy transfer. We propose 1) utilizing
high visual-fidelity simulation for improved sim-to-real transfer, 2)
evaluating policies by systematically increasing task complexity and scenario
perturbation to assess robustness, and 3) quantifying performance alignment
between real-world performance and its simulation counterparts.

</details>


### [104] [Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC](https://arxiv.org/abs/2508.11129)
*Ryan M. Bena,Gilbert Bahati,Blake Werner,Ryan K. Cosner,Lizhi Yang,Aaron D. Ames*

Main category: cs.RO

TL;DR: 该论文提出了一种基于Poisson安全函数和CBFs的MPC算法，用于腿式机器人在动态环境中的安全轨迹规划，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化和动态变化环境中腿式机器人轨迹规划的安全关键问题，特别是处理其可操纵的非对称几何形状。

Method: 通过将静态Dirichlet问题重新表述为参数化的移动边界值问题，扩展了Poisson安全函数的理论框架，并利用Minkowski集合操作将域提升到考虑机器人几何形状的配置空间。

Result: 在多种安全关键场景中成功实现了实时预测安全过滤器，验证了方法的有效性和通用性。

Conclusion: 该论文提出了一种基于控制屏障函数（CBFs）的几何感知安全约束的非线性模型预测控制（MPC）算法，成功应用于人形和四足机器人的实时安全关键场景，验证了Poisson安全函数的通用性和CBF约束模型预测控制器的优势。

Abstract: Autonomous navigation through unstructured and dynamically-changing
environments is a complex task that continues to present many challenges for
modern roboticists. In particular, legged robots typically possess manipulable
asymmetric geometries which must be considered during safety-critical
trajectory planning. This work proposes a predictive safety filter: a nonlinear
model predictive control (MPC) algorithm for online trajectory generation with
geometry-aware safety constraints based on control barrier functions (CBFs).
Critically, our method leverages Poisson safety functions to numerically
synthesize CBF constraints directly from perception data. We extend the
theoretical framework for Poisson safety functions to incorporate temporal
changes in the domain by reformulating the static Dirichlet problem for
Poisson's equation as a parameterized moving boundary value problem.
Furthermore, we employ Minkowski set operations to lift the domain into a
configuration space that accounts for robot geometry. Finally, we implement our
real-time predictive safety filter on humanoid and quadruped robots in various
safety-critical scenarios. The results highlight the versatility of Poisson
safety functions, as well as the benefit of CBF constrained model predictive
safety-critical controllers.

</details>


### [105] [Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward](https://arxiv.org/abs/2508.11143)
*Jiarui Yang,Bin Zhu,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.RO

TL;DR: AC3是一种新型强化学习框架，通过稳定actor和critic的学习机制，有效解决了稀疏奖励下连续动作序列学习的难题，实验证明其在多任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在稀疏奖励的长期机器人操作任务中表现不佳，直接学习连续动作块存在稳定性和数据效率的挑战。

Method: AC3采用非对称更新规则训练actor，仅从成功轨迹中学习，并使用intra-chunk $n$-step returns和自监督模块稳定critic的更新。

Result: 在BiGym和RLBench基准测试的25个任务中，AC3仅需少量演示和简单模型架构即可在多数任务上取得更高的成功率。

Conclusion: AC3框架通过引入针对性的稳定机制，成功解决了在稀疏奖励环境下学习连续动作序列的挑战，显著提高了长期机器人操作任务的成功率。

Abstract: Existing reinforcement learning (RL) methods struggle with long-horizon
robotic manipulation tasks, particularly those involving sparse rewards. While
action chunking is a promising paradigm for robotic manipulation, using RL to
directly learn continuous action chunks in a stable and data-efficient manner
remains a critical challenge. This paper introduces AC3 (Actor-Critic for
Continuous Chunks), a novel RL framework that learns to generate
high-dimensional, continuous action sequences. To make this learning process
stable and data-efficient, AC3 incorporates targeted stabilization mechanisms
for both the actor and the critic. First, to ensure reliable policy
improvement, the actor is trained with an asymmetric update rule, learning
exclusively from successful trajectories. Second, to enable effective value
learning despite sparse rewards, the critic's update is stabilized using
intra-chunk $n$-step returns and further enriched by a self-supervised module
providing intrinsic rewards at anchor points aligned with each action chunk. We
conducted extensive experiments on 25 tasks from the BiGym and RLBench
benchmarks. Results show that by using only a few demonstrations and a simple
model architecture, AC3 achieves superior success rates on most tasks,
validating its effective design.

</details>


### [106] [Visuomotor Grasping with World Models for Surgical Robots](https://arxiv.org/abs/2508.11200)
*Hongbin Lin,Bin Li,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: GASv2是一个基于视觉运动学习的手术抓取框架，通过仿真训练和真实部署，实现了65%的成功率，并展示了强大的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动化手术机器人抓取任务可以减少外科医生的工作负担，并提高效率、安全性和一致性。现有方法在泛化性、鲁棒性和处理可变形物体方面存在局限。

Method: GASv2采用基于世界模型的架构和手术感知管道进行视觉观察，结合混合控制系统实现安全执行。通过领域随机化在仿真中训练策略，并在真实机器人上部署。

Result: GASv2在幻影和离体手术环境中均实现了65%的成功率，能够泛化到未见过的物体和夹爪，并适应多种干扰。

Conclusion: GASv2框架在手术机器人抓取任务中表现出色，成功率达到65%，能够泛化到未见过的物体和夹爪，并适应多种干扰，展示了强大的性能、通用性和鲁棒性。

Abstract: Grasping is a fundamental task in robot-assisted surgery (RAS), and
automating it can reduce surgeon workload while enhancing efficiency, safety,
and consistency beyond teleoperated systems. Most prior approaches rely on
explicit object pose tracking or handcrafted visual features, limiting their
generalization to novel objects, robustness to visual disturbances, and the
ability to handle deformable objects. Visuomotor learning offers a promising
alternative, but deploying it in RAS presents unique challenges, such as low
signal-to-noise ratio in visual observations, demands for high safety and
millimeter-level precision, as well as the complex surgical environment. This
paper addresses three key challenges: (i) sim-to-real transfer of visuomotor
policies to ex vivo surgical scenes, (ii) visuomotor learning using only a
single stereo camera pair -- the standard RAS setup, and (iii) object-agnostic
grasping with a single policy that generalizes to diverse, unseen surgical
objects without retraining or task-specific models. We introduce Grasp Anything
for Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.
GASv2 leverages a world-model-based architecture and a surgical perception
pipeline for visual observations, combined with a hybrid control system for
safe execution. We train the policy in simulation using domain randomization
for sim-to-real transfer and deploy it on a real robot in both phantom-based
and ex vivo surgical settings, using only a single pair of endoscopic cameras.
Extensive experiments show our policy achieves a 65% success rate in both
settings, generalizes to unseen objects and grippers, and adapts to diverse
disturbances, demonstrating strong performance, generality, and robustness.

</details>


### [107] [Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation](https://arxiv.org/abs/2508.11204)
*Hongbin Lin,Juan Rojas,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: 论文提出了一种结合非等距对称性结构和MEA数据增强的方法，显著提升了机器人操作中的采样效率，并通过实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 采样效率对于实际机器人操作中的视觉运动学习至关重要。尽管任务对称性作为一种有前途的归纳偏差被提出以提高效率，但大多数现有工作仅限于等距对称性。本文旨在探索非等距对称性，以放松这些限制。

Method: 提出了一种新的部分可观测马尔可夫决策过程（POMDP）公式，结合非等距对称性结构，并设计了Multi-Group Equivariance Augmentation（MEA）数据增强方法。同时，采用了一种基于体素的视觉表示方法以保持平移等变性。

Result: 通过仿真和实际机器人实验，证明了所提方法在两个操作领域中的有效性。

Conclusion: 该论文通过引入非等距对称性结构和MEA数据增强方法，显著提升了视觉运动学习在机器人操作中的采样效率，并通过广泛的仿真和实际机器人实验验证了其有效性。

Abstract: Sampling efficiency is critical for deploying visuomotor learning in
real-world robotic manipulation. While task symmetry has emerged as a promising
inductive bias to improve efficiency, most prior work is limited to isometric
symmetries -- applying the same group transformation to all task objects across
all timesteps. In this work, we explore non-isometric symmetries, applying
multiple independent group transformations across spatial and temporal
dimensions to relax these constraints. We introduce a novel formulation of the
partially observable Markov decision process (POMDP) that incorporates the
non-isometric symmetry structures, and propose a simple yet effective data
augmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate
MEA with offline reinforcement learning to enhance sampling efficiency, and
introduce a voxel-based visual representation that preserves translational
equivariance. Extensive simulation and real-robot experiments across two
manipulation domains demonstrate the effectiveness of our approach.

</details>


### [108] [Embodied Edge Intelligence Meets Near Field Communication: Concept, Design, and Verification](https://arxiv.org/abs/2508.11232)
*Guoliang Li,Xibin Jin,Yujie Wan,Chenxuan Liu,Tong Zhang,Shuai Wang,Chengzhong Xu*

Main category: cs.RO

TL;DR: 本文提出将EEI与NFC集成为NEEI范式，解决了实时推理和大模型计算需求的矛盾，并通过两种关键技术优化性能，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 由于大型模型的计算需求巨大，实现具身人工智能面临挑战。EEI虽能提供近端计算支持，但需要更高的频谱效率、通信安全和减少用户间干扰。NFC凭借其硬件基础成为理想解决方案，但集成EEI和NFC也带来了新的挑战。

Method: 提出了两种关键技术：EEI辅助NFC场景下的无线电友好型实体规划，以及NFC辅助EEI场景下的视图引导波束聚焦。此外，还探讨了如何通过机会性协作导航实现资源高效的NEEI。

Result: 实验结果表明，所提出的技术在性能上优于多种基准方法。

Conclusion: 本文提出了一种新的NEEI范式，通过将EEI与NFC集成，解决了实时推理和大模型计算需求之间的矛盾，并通过实验验证了所提技术的优越性。

Abstract: Realizing embodied artificial intelligence is challenging due to the huge
computation demands of large models (LMs). To support LMs while ensuring
real-time inference, embodied edge intelligence (EEI) is a promising paradigm,
which leverages an LM edge to provide computing powers in close proximity to
embodied robots. Due to embodied data exchange, EEI requires higher spectral
efficiency, enhanced communication security, and reduced inter-user
interference. To meet these requirements, near-field communication (NFC), which
leverages extremely large antenna arrays as its hardware foundation, is an
ideal solution. Therefore, this paper advocates the integration of EEI and NFC,
resulting in a near-field EEI (NEEI) paradigm. However, NEEI also introduces
new challenges that cannot be adequately addressed by isolated EEI or NFC
designs, creating research opportunities for joint optimization of both
functionalities. To this end, we propose radio-friendly embodied planning for
EEI-assisted NFC scenarios and view-guided beam-focusing for NFC-assisted EEI
scenarios. We also elaborate how to realize resource-efficient NEEI through
opportunistic collaborative navigation. Experimental results are provided to
confirm the superiority of the proposed techniques compared with various
benchmarks.

</details>


### [109] [Tactile Robotics: An Outlook](https://arxiv.org/abs/2508.11261)
*Shan Luo,Nathan F. Lepora,Wenzhen Yuan,Kaspar Althoefer,Gordon Cheng,Ravinder Dahiya*

Main category: cs.RO

TL;DR: 本文综述了触觉机器人技术的现状与挑战，提出了跨领域创新的潜在解决方案。


<details>
  <summary>Details</summary>
Motivation: 触觉感知对机器人实现与人类密切共存和互动的应用至关重要，因此触觉传感技术的研究具有重要价值。

Method: 文章回顾了多种触觉传感技术，包括压阻、压电、电容、磁性和光学传感器，并探讨了模拟工具在生成大规模触觉数据集以支持传感器设计和算法开发中的作用。

Result: 多种触觉传感技术的开发及其与其他感知模态（如视觉）的集成，以及主动触觉感知策略的结合，显著扩展了该领域的应用范围。

Conclusion: 本文提出了一种全面的方法来推动触觉机器人技术的变革性进展，并探讨了当前技术面临的挑战及潜在解决方案，以激发跨领域创新。

Abstract: Robotics research has long sought to give robots the ability to perceive the
physical world through touch in an analogous manner to many biological systems.
Developing such tactile capabilities is important for numerous emerging
applications that require robots to co-exist and interact closely with humans.
Consequently, there has been growing interest in tactile sensing, leading to
the development of various technologies, including piezoresistive and
piezoelectric sensors, capacitive sensors, magnetic sensors, and optical
tactile sensors. These diverse approaches utilise different transduction
methods and materials to equip robots with distributed sensing capabilities,
enabling more effective physical interactions. These advances have been
supported in recent years by simulation tools that generate large-scale tactile
datasets to support sensor designs and algorithms to interpret and improve the
utility of tactile data. The integration of tactile sensing with other
modalities, such as vision, as well as with action strategies for active
tactile perception highlights the growing scope of this field. To further the
transformative progress in tactile robotics, a holistic approach is essential.
In this outlook article, we examine several challenges associated with the
current state of the art in tactile robotics and explore potential solutions to
inspire innovations across multiple domains, including manufacturing,
healthcare, recycling and agriculture.

</details>


### [110] [Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation](https://arxiv.org/abs/2508.11275)
*Masaki Murooka,Iori Kumagai,Mitsuharu Morisawa,Fumio Kanehiro*

Main category: cs.RO

TL;DR: 提出可微可达性地图，通过连续优化高效解决人形机器人运动规划问题。


<details>
  <summary>Details</summary>
Motivation: 为了降低人形机器人运动生成的计算成本，并实现更高效的运动规划。

Method: 使用神经网络或支持向量机从机器人末端执行器的位姿集合中学习可微可达性地图，并将其作为约束条件应用于连续优化问题。

Result: 该方法在足步规划、多接触运动规划和操作-移动规划等多种任务中表现出高效性。

Conclusion: 该论文提出了一种可微可达性地图的新方法，有效降低了人形机器人运动生成的计算成本，并通过连续优化解决了多种运动规划问题。

Abstract: To reduce the computational cost of humanoid motion generation, we introduce
a new approach to representing robot kinematic reachability: the differentiable
reachability map. This map is a scalar-valued function defined in the task
space that takes positive values only in regions reachable by the robot's
end-effector. A key feature of this representation is that it is continuous and
differentiable with respect to task-space coordinates, enabling its direct use
as constraints in continuous optimization for humanoid motion planning. We
describe a method to learn such differentiable reachability maps from a set of
end-effector poses generated using a robot's kinematic model, using either a
neural network or a support vector machine as the learning model. By
incorporating the learned reachability map as a constraint, we formulate
humanoid motion generation as a continuous optimization problem. We demonstrate
that the proposed approach efficiently solves various motion planning problems,
including footstep planning, multi-contact motion planning, and
loco-manipulation planning for humanoid robots.

</details>


### [111] [Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent](https://arxiv.org/abs/2508.11286)
*Che Rin Yu,Daewon Chae,Dabin Seo,Sangwon Lee,Hyeongwoo Im,Jinkyu Kim*

Main category: cs.RO

TL;DR: 论文提出了一种基于场景图比较的主动重新规划框架，通过轻量级推理模块在子任务边界检测和纠正不匹配，实验证明其能提前预防失败，提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 人类能够根据环境状态调整行动，而许多自主机器人缺乏这种适应性，导致在过时假设下执行动作并最终失败。现有的重新规划方法通常在失败后响应，而主动重新规划有望提前预防失败，但当前解决方案依赖手动设计的规则和大量监督。

Method: 通过比较当前RGB-D观测构建的场景图与从成功演示中提取的参考图，当当前场景与参考轨迹不一致时，激活轻量级推理模块来诊断不匹配并调整计划。

Result: 在AI2-THOR模拟器中的实验表明，该方法在执行失败发生前检测语义和空间不匹配，显著提高了任务成功率和鲁棒性。

Conclusion: 该论文提出了一种主动重新规划框架，通过在子任务边界检测和纠正失败，显著提高了任务的成功率和鲁棒性。

Abstract: When humans perform everyday tasks, we naturally adjust our actions based on
the current state of the environment. For instance, if we intend to put
something into a drawer but notice it is closed, we open it first. However,
many autonomous robots lack this adaptive awareness. They often follow
pre-planned actions that may overlook subtle yet critical changes in the scene,
which can result in actions being executed under outdated assumptions and
eventual failure. While replanning is critical for robust autonomy, most
existing methods respond only after failures occur, when recovery may be
inefficient or infeasible. While proactive replanning holds promise for
preventing failures in advance, current solutions often rely on manually
designed rules and extensive supervision. In this work, we present a proactive
replanning framework that detects and corrects failures at subtask boundaries
by comparing scene graphs constructed from current RGB-D observations against
reference graphs extracted from successful demonstrations. When the current
scene fails to align with reference trajectories, a lightweight reasoning
module is activated to diagnose the mismatch and adjust the plan. Experiments
in the AI2-THOR simulator demonstrate that our approach detects semantic and
spatial mismatches before execution failures occur, significantly improving
task success and robustness.

</details>


### [112] [A Recursive Total Least Squares Solution for Bearing-Only Target Motion Analysis and Circumnavigation](https://arxiv.org/abs/2508.11289)
*Lin Li,Xueming Liu,Zhoujingzi Qiu,Tianjiang Hu,Qingrui Zhang*

Main category: cs.RO

TL;DR: 提出RTLS方法和环绕导航控制器，解决了bearing-only TMA的可观测性和收敛性问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: bearing-only TMA因测量模型非线性和缺乏距离信息而面临可观测性和估计器收敛性挑战。

Method: 提出了一种基于递归总体最小二乘法（RTLS）的在线目标定位与跟踪方法，并结合了环绕导航控制器以增强系统可观测性。

Result: 仿真和实验证明，该方法在准确性和稳定性上优于伪线性卡尔曼滤波（PLKF）等现有方法。

Conclusion: The proposed RTLS方法和circumnavigation控制器显著提高了bearing-only TMA的性能，在准确性和稳定性上优于现有方法。

Abstract: Bearing-only Target Motion Analysis (TMA) is a promising technique for
passive tracking in various applications as a bearing angle is easy to measure.
Despite its advantages, bearing-only TMA is challenging due to the nonlinearity
of the bearing measurement model and the lack of range information, which
impairs observability and estimator convergence. This paper addresses these
issues by proposing a Recursive Total Least Squares (RTLS) method for online
target localization and tracking using mobile observers. The RTLS approach,
inspired by previous results on Total Least Squares (TLS), mitigates biases in
position estimation and improves computational efficiency compared to
pseudo-linear Kalman filter (PLKF) methods. Additionally, we propose a
circumnavigation controller to enhance system observability and estimator
convergence by guiding the mobile observer in orbit around the target.
Extensive simulations and experiments are performed to demonstrate the
effectiveness and robustness of the proposed method. The proposed algorithm is
also compared with the state-of-the-art approaches, which confirms its superior
performance in terms of both accuracy and stability.

</details>


### [113] [Pedestrian Dead Reckoning using Invariant Extended Kalman Filter](https://arxiv.org/abs/2508.11396)
*Jingran Zhang,Zhengzhang Yan,Yiming Chen,Zeqiang He,Jiahao Chen*

Main category: cs.RO

TL;DR: 本文提出了一种低成本惯性行人航位推算方法，适用于GPS缺失环境中的双足机器人，通过InEKF实现，实验证明其优于标准EKF且更易调整。


<details>
  <summary>Details</summary>
Motivation: 解决GPS缺失环境下双足机器人的定位问题，提供一种成本效益高的解决方案。

Method: 采用基于矩阵Lie群的不变扩展卡尔曼滤波器（InEKF），通过在站立足上的惯性测量单元（IMU）执行伪测量来提供创新。

Result: 实验结果表明，InEKF在运动捕捉基准实验、大规模多层行走实验和双足机器人实验中均优于标准EKF。

Conclusion: 本文提出了一种成本效益高的惯性行人航位推算方法，适用于GPS缺失环境中的双足机器人。通过实验验证，该方法在真实机器人系统中具有可行性，且InEKF比标准EKF更容易调整。

Abstract: This paper presents a cost-effective inertial pedestrian dead reckoning
method for the bipedal robot in the GPS-denied environment. Each time when the
inertial measurement unit (IMU) is on the stance foot, a stationary
pseudo-measurement can be executed to provide innovation to the IMU measurement
based prediction. The matrix Lie group based theoretical development of the
adopted invariant extended Kalman filter (InEKF) is set forth for tutorial
purpose. Three experiments are conducted to compare between InEKF and standard
EKF, including motion capture benchmark experiment, large-scale multi-floor
walking experiment, and bipedal robot experiment, as an effort to show our
method's feasibility in real-world robot system. In addition, a sensitivity
analysis is included to show that InEKF is much easier to tune than EKF.

</details>


### [114] [An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration](https://arxiv.org/abs/2508.11404)
*Junyeon Kim,Tianshu Ruan,Cesar Alan Contreras,Manolis Chiou*

Main category: cs.RO

TL;DR: AI辅助的移动机器人检查在核设施中比人工方法更高效、准确且减少工作负荷。


<details>
  <summary>Details</summary>
Motivation: 传统核设施结构人工检查存在安全风险、认知负荷高及人为误差问题，AI与机器人技术的进步为更安全、高效、准确的检查方法提供了可能。

Method: 本研究采用AI辅助视觉裂纹检测技术，集成到移动Jackal机器人平台，进行实验验证。

Result: 实验结果表明，HRC能提升检查准确性并降低操作员工作负荷。

Conclusion: HRC结合AI视觉裂纹检测的移动Jackal机器人平台在核设施结构检查中展现出比传统人工方法更高的准确性和更低的工作负荷。

Abstract: Structural inspection in nuclear facilities is vital for maintaining
operational safety and integrity. Traditional methods of manual inspection pose
significant challenges, including safety risks, high cognitive demands, and
potential inaccuracies due to human limitations. Recent advancements in
Artificial Intelligence (AI) and robotic technologies have opened new
possibilities for safer, more efficient, and accurate inspection methodologies.
Specifically, Human-Robot Collaboration (HRC), leveraging robotic platforms
equipped with advanced detection algorithms, promises significant improvements
in inspection outcomes and reductions in human workload. This study explores
the effectiveness of AI-assisted visual crack detection integrated into a
mobile Jackal robot platform. The experiment results indicate that HRC enhances
inspection accuracy and reduces operator workload, resulting in potential
superior performance outcomes compared to traditional manual methods.

</details>


### [115] [Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing](https://arxiv.org/abs/2508.11406)
*Benjamin Alt,Mareike Picklum,Sorin Arion,Franklin Kenghagho Kenfack,Michael Beetz*

Main category: cs.RO

TL;DR: 该论文提出了一种语义执行追踪框架和云平台AICOR VRB，旨在实现可重复、透明的机器人驱动科学研究。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是推动自主机器人进行科学实验，使其不仅精确可重复，而且开放、可信和透明。

Method: 论文提出了两种关键技术：一是语义执行追踪框架，记录传感器数据及语义注释的机器人信念状态；二是AICOR虚拟研究建筑（VRB），一个基于云的平台，用于大规模共享、复制和验证机器人任务执行。

Result: 论文的成果是开发了两种工具，确保了自动化实验的透明性和可复制性，并为自主系统参与科学发现奠定了基础。

Conclusion: 该论文的结论是通过语义执行追踪框架和AICOR虚拟研究建筑（VRB）平台，为自主机器人参与科学发现奠定了基础，实现了可重复、机器人驱动的科学研究。

Abstract: We envision a future in which autonomous robots conduct scientific
experiments in ways that are not only precise and repeatable, but also open,
trustworthy, and transparent. To realize this vision, we present two key
contributions: a semantic execution tracing framework that logs sensor data
together with semantically annotated robot belief states, ensuring that
automated experimentation is transparent and replicable; and the AICOR Virtual
Research Building (VRB), a cloud-based platform for sharing, replicating, and
validating robot task executions at scale. Together, these tools enable
reproducible, robot-driven science by integrating deterministic execution,
semantic memory, and open knowledge representation, laying the foundation for
autonomous systems to participate in scientific discovery.

</details>


### [116] [EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback](https://arxiv.org/abs/2508.11453)
*Jiayue Jin,Lang Qian,Jingyu Zhang,Chuanyu Ju,Liang Song*

Main category: cs.RO

TL;DR: EvoPSF是一种在线进化框架，通过规划状态反馈和自监督学习提升自动驾驶系统对新环境的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统多为离线训练，缺乏在线适应机制，导致在真实场景中面对未见过变化时泛化能力下降。

Method: 提出了一种基于规划状态反馈的在线进化框架EvoPSF，利用规划器的不确定性作为触发信号，通过自监督损失对关键对象进行针对性模型更新。

Result: 在nuScenes数据集的跨区域和损坏变体上实验表明，EvoPSF在挑战性条件下持续提升规划性能。

Conclusion: EvoPSF通过在线进化框架显著提升了自动驾驶系统在复杂环境中的规划性能，特别是在面对新环境或变化时表现出更强的适应性和鲁棒性。

Abstract: Recent years have witnessed remarkable progress in autonomous driving, with
systems evolving from modular pipelines to end-to-end architectures. However,
most existing methods are trained offline and lack mechanisms to adapt to new
environments during deployment. As a result, their generalization ability
diminishes when faced with unseen variations in real-world driving scenarios.
In this paper, we break away from the conventional "train once, deploy forever"
paradigm and propose EvoPSF, a novel online Evolution framework for autonomous
driving based on Planning-State Feedback. We argue that planning failures are
primarily caused by inaccurate object-level motion predictions, and such
failures are often reflected in the form of increased planner uncertainty. To
address this, we treat planner uncertainty as a trigger for online evolution,
using it as a diagnostic signal to initiate targeted model updates. Rather than
performing blind updates, we leverage the planner's agent-agent attention to
identify the specific objects that the ego vehicle attends to most, which are
primarily responsible for the planning failures. For these critical objects, we
compute a targeted self-supervised loss by comparing their predicted waypoints
from the prediction module with their actual future positions, selected from
the perception module's outputs with high confidence scores. This loss is then
backpropagated to adapt the model online. As a result, our method improves the
model's robustness to environmental changes, leads to more precise motion
predictions, and therefore enables more accurate and stable planning behaviors.
Experiments on both cross-region and corrupted variants of the nuScenes dataset
demonstrate that EvoPSF consistently improves planning performance under
challenging conditions.

</details>


### [117] [OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation](https://arxiv.org/abs/2508.11479)
*Tatiana Zemskova,Aleksei Staroverov,Dmitry Yudin,Aleksandr Panov*

Main category: cs.RO

TL;DR: OVSegDT通过语义分支和熵自适应损失调制，提升了开放词汇目标导航的性能，减少了碰撞，并在未见类别上达到与已见类别相当的表现，且无需额外传感器或大型模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有端到端策略在小规模模拟器数据集上的过拟合问题，以及由此导致的泛化能力差和不安全行为（频繁碰撞）。

Method: 提出OVSegDT，一种轻量级Transformer策略，包含两个关键组件：语义分支（含目标二进制掩码编码器和辅助分割损失函数）和熵自适应损失调制（一种根据策略熵动态平衡模仿与强化信号的样本级调度器）。

Result: 在HM3D-OVON数据集上，模型在未见类别上的表现与已见类别相当，且实现了最先进的结果（40.1% SR，20.9% SPL on val unseen），同时将训练样本复杂度降低了33%，碰撞次数减少了一半。

Conclusion: OVSegDT通过结合语义分支和熵自适应损失调制，显著提升了开放词汇目标导航的性能，减少了碰撞次数，并在未见类别上达到了与已见类别相当的表现，且无需深度、测距或大型视觉语言模型。

Abstract: Open-vocabulary Object Goal Navigation requires an embodied agent to reach
objects described by free-form language, including categories never seen during
training. Existing end-to-end policies overfit small simulator datasets,
achieving high success on training scenes but failing to generalize and
exhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a
lightweight transformer policy that tackles these issues with two synergistic
components. The first component is the semantic branch, which includes an
encoder for the target binary mask and an auxiliary segmentation loss function,
grounding the textual goal and providing precise spatial cues. The second
component consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample
scheduler that continuously balances imitation and reinforcement signals
according to the policy entropy, eliminating brittle manual phase switches.
These additions cut the sample complexity of training by 33%, and reduce
collision count in two times while keeping inference cost low (130M parameters,
RGB-only input). On HM3D-OVON, our model matches the performance on unseen
categories to that on seen ones and establishes state-of-the-art results (40.1%
SR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language
models. Code is available at https://github.com/CognitiveAISystems/OVSegDT.

</details>


### [118] [i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping](https://arxiv.org/abs/2508.11485)
*Hailiang Tang,Tisheng Zhang,Liqiang Wang,Xin Ding,Man Yuan,Zhiyu Xiang,Jujin Chen,Yuhan Bian,Shuangyan Liu,Yuqing Wang,Guan Wang,Xiaoji Niu*

Main category: cs.RO

TL;DR: i2Nav-Robot是一个多传感器融合的大规模UGV数据集，解决了现有数据集的不足，提供高精度导航和地图构建支持。


<details>
  <summary>Details</summary>
Motivation: 当前UGV数据集在传感器配置、时间同步、地面真实和场景多样性方面存在不足，无法满足导航和地图构建技术发展的需求。

Method: 集成多模态传感器（包括最新的固态LiDAR、4D雷达、立体相机等），并通过在线硬件同步和离线校准获取精确时间戳。数据集包含10个大规模序列，覆盖多样化的室内外场景。

Result: i2Nav-Robot数据集覆盖17060米，提供厘米级精度的位置地面真实数据，并被多个开源系统验证其优越性。

Conclusion: i2Nav-Robot数据集通过多模态传感器集成和高精度地面真实数据，显著提升了UGV导航和地图构建的研究水平，并被多个开源系统验证其数据质量优越。

Abstract: Accurate and reliable navigation is crucial for autonomous unmanned ground
vehicle (UGV). However, current UGV datasets fall short in meeting the demands
for advancing navigation and mapping techniques due to limitations in sensor
configuration, time synchronization, ground truth, and scenario diversity. To
address these challenges, we present i2Nav-Robot, a large-scale dataset
designed for multi-sensor fusion navigation and mapping in indoor-outdoor
environments. We integrate multi-modal sensors, including the newest front-view
and 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras,
odometer, global navigation satellite system (GNSS) receiver, and inertial
measurement units (IMU) on an omnidirectional wheeled robot. Accurate
timestamps are obtained through both online hardware synchronization and
offline calibration for all sensors. The dataset comprises ten larger-scale
sequences covering diverse UGV operating scenarios, such as outdoor streets,
and indoor parking lots, with a total length of about 17060 meters.
High-frequency ground truth, with centimeter-level accuracy for position, is
derived from post-processing integrated navigation methods using a
navigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more
than ten open-sourced multi-sensor fusion systems, and it has proven to have
superior data quality.

</details>


### [119] [Relative Position Matters: Trajectory Prediction and Planning with Polar Representation](https://arxiv.org/abs/2508.11492)
*Bozhou Zhang,Nan Song,Bingzhao Gao,Li Zhang*

Main category: cs.RO

TL;DR: Polaris是一种基于极坐标系的新方法，通过更直观地建模距离和方向变化，显著提升了自动驾驶中的轨迹预测和规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在笛卡尔坐标系中对自车和周围交通元素的关系建模不够理想，无法自然捕捉不同元素基于距离和方向的动态影响。

Method: 采用了极坐标系（半径和角度）来表示位置，并设计了专门的编码和细化模块，以更直观地建模空间变化和相对关系。

Result: 在Argoverse 2和nuPlan等具有挑战性的预测和规划基准测试中，Polaris表现出色。

Conclusion: Polaris在极坐标系下的表现优于传统的笛卡尔坐标系方法，在轨迹预测和规划任务中实现了最先进的性能。

Abstract: Trajectory prediction and planning in autonomous driving are highly
challenging due to the complexity of predicting surrounding agents' movements
and planning the ego agent's actions in dynamic environments. Existing methods
encode map and agent positions and decode future trajectories in Cartesian
coordinates. However, modeling the relationships between the ego vehicle and
surrounding traffic elements in Cartesian space can be suboptimal, as it does
not naturally capture the varying influence of different elements based on
their relative distances and directions. To address this limitation, we adopt
the Polar coordinate system, where positions are represented by radius and
angle. This representation provides a more intuitive and effective way to model
spatial changes and relative relationships, especially in terms of distance and
directional influence. Based on this insight, we propose Polaris, a novel
method that operates entirely in Polar coordinates, distinguishing itself from
conventional Cartesian-based approaches. By leveraging the Polar
representation, this method explicitly models distance and direction variations
and captures relative relationships through dedicated encoding and refinement
modules, enabling more structured and spatially aware trajectory prediction and
planning. Extensive experiments on the challenging prediction (Argoverse 2) and
planning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art
performance.

</details>


### [120] [Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language](https://arxiv.org/abs/2508.11498)
*Agnes Bressan de Almeida,Joao Aires Correa Fernandes Marsicano*

Main category: cs.RO

TL;DR: Swarm in Blocks 2.0是一个基于块状语言的无人机群编程工具，简化了复杂操作，适合初学者和教育用途。


<details>
  <summary>Details</summary>
Motivation: 随着无人机群在配送、农业和监控等领域的应用增加，管理复杂性也随之上升，尤其是对初学者而言。Atena团队开发此工具旨在降低使用门槛，无需ROS或编程的深入知识。

Method: 基于Clover平台，采用块状语言构建高级接口，支持循环和条件结构等功能。

Result: Swarm in Blocks 2.0进一步优化了平台，以用户友好的方式解决了群管理的复杂性。

Conclusion: Swarm in Blocks 2.0有效简化了无人机群编程，使其对初学者更友好，同时扩展了编程教育的可能性。

Abstract: Swarm in Blocks, originally developed for CopterHack 2022, is a high-level
interface that simplifies drone swarm programming using a block-based language.
Building on the Clover platform, this tool enables users to create
functionalities like loops and conditional structures by assembling code
blocks. In 2023, we introduced Swarm in Blocks 2.0, further refining the
platform to address the complexities of swarm management in a user-friendly
way. As drone swarm applications grow in areas like delivery, agriculture, and
surveillance, the challenge of managing them, especially for beginners, has
also increased. The Atena team developed this interface to make swarm handling
accessible without requiring extensive knowledge of ROS or programming. The
block-based approach not only simplifies swarm control but also expands
educational opportunities in programming.

</details>


### [121] [Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media](https://arxiv.org/abs/2508.11503)
*Andrej Orsula,Matthieu Geist,Miguel Olivares-Mendez,Carol Martinez*

Main category: cs.RO

TL;DR: 本文提出一个完整的模拟到现实框架，通过程序生成的环境训练强化学习代理，并在物理轮式漫游车上验证其零样本性能，为未来太空探索提供可靠导航系统。


<details>
  <summary>Details</summary>
Motivation: 可靠的自主导航对于未来太空探索至关重要，但学习型控制器的部署受到模拟到现实差距的阻碍。

Method: 利用大规模并行仿真在程序生成的环境中训练强化学习代理，并将其零样本转移到物理轮式漫游车上。

Result: 实验表明，具有程序多样性的代理在零样本性能上优于静态场景训练的代理，而高保真粒子物理的微调在低速精度上仅有微小提升。

Conclusion: 本研究建立了一个经过验证的工作流程，用于创建可靠的学习型导航系统，标志着在最终前沿部署自主机器人的关键一步。

Abstract: Reliable autonomous navigation across the unstructured terrains of distant
planetary surfaces is a critical enabler for future space exploration. However,
the deployment of learning-based controllers is hindered by the inherent
sim-to-real gap, particularly for the complex dynamics of wheel interactions
with granular media. This work presents a complete sim-to-real framework for
developing and validating robust control policies for dynamic waypoint tracking
on such challenging surfaces. We leverage massively parallel simulation to
train reinforcement learning agents across a vast distribution of procedurally
generated environments with randomized physics. These policies are then
transferred zero-shot to a physical wheeled rover operating in a lunar-analogue
facility. Our experiments systematically compare multiple reinforcement
learning algorithms and action smoothing filters to identify the most effective
combinations for real-world deployment. Crucially, we provide strong empirical
evidence that agents trained with procedural diversity achieve superior
zero-shot performance compared to those trained on static scenarios. We also
analyze the trade-offs of fine-tuning with high-fidelity particle physics,
which offers minor gains in low-speed precision at a significant computational
cost. Together, these contributions establish a validated workflow for creating
reliable learning-based navigation systems, marking a critical step towards
deploying autonomous robots in the final frontier.

</details>


### [122] [A Comparative Study of Floating-Base Space Parameterizations for Agile Whole-Body Motion Planning](https://arxiv.org/abs/2508.11520)
*Evangelos Tsiatsianas,Chairi Kiourt,Konstantinos Chatzilygeroudis*

Main category: cs.RO

TL;DR: 比较不同浮动基参数化方法在敏捷运动优化中的表现，提出基于SE(3)切空间的新方法，验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决敏捷全身运动生成中浮动基空间参数化选择缺乏明确指导的问题，特别是在复杂接触动力学场景下。

Method: 采用直接转录法进行轨迹优化，系统比较了多种常见的浮动基空间参数化方法，并引入了一种基于SE(3)切空间的新参数化方法。

Result: 实验表明，基于SE(3)切空间的参数化方法在性能上表现优异，且无需专门的流形优化技术即可使用成熟的数值求解器。

Conclusion: 本文通过比较不同参数化方法在敏捷运动轨迹优化中的表现，提出了一种基于SE(3)切空间的新颖参数化方法，为选择适合的浮动基表示提供了有价值的参考。

Abstract: Automatically generating agile whole-body motions for legged and humanoid
robots remains a fundamental challenge in robotics. While numerous trajectory
optimization approaches have been proposed, there is no clear guideline on how
the choice of floating-base space parameterization affects performance,
especially for agile behaviors involving complex contact dynamics. In this
paper, we present a comparative study of different parameterizations for direct
transcription-based trajectory optimization of agile motions in legged systems.
We systematically evaluate several common choices under identical optimization
settings to ensure a fair comparison. Furthermore, we introduce a novel
formulation based on the tangent space of SE(3) for representing the robot's
floating-base pose, which, to our knowledge, has not received attention from
the literature. This approach enables the use of mature off-the-shelf numerical
solvers without requiring specialized manifold optimization techniques. We hope
that our experiments and analysis will provide meaningful insights for
selecting the appropriate floating-based representation for agile whole-body
motion generation.

</details>


### [123] [MultiPark: Multimodal Parking Transformer with Next-Segment Prediction](https://arxiv.org/abs/2508.11537)
*Han Zheng,Zikang Zhou,Guli Zhang,Zhepei Wang,Kaixuan Wang,Peiliang Li,Shaojie Shen,Ming Yang,Tong Qin*

Main category: cs.RO

TL;DR: MultiPark是一种新型自回归变换器，通过数据高效预测和可学习查询解决多模态停车问题，并在实际应用中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有模仿学习（IL）方法在无车道开放空间中忽略停车行为多模态性的问题，以及IL固有的因果混淆问题。

Method: 提出了MultiPark，一种用于多模态停车的自回归变换器，引入了数据高效的下一个段预测范式，并设计了可学习的停车查询。

Result: MultiPark在各种场景下实现了最先进的性能。

Conclusion: MultiPark在真实世界数据集上表现出色，并在实际停车环境中验证了其鲁棒性。

Abstract: Parking accurately and safely in highly constrained spaces remains a critical
challenge. Unlike structured driving environments, parking requires executing
complex maneuvers such as frequent gear shifts and steering saturation. Recent
attempts to employ imitation learning (IL) for parking have achieved promising
results. However, existing works ignore the multimodal nature of parking
behavior in lane-free open space, failing to derive multiple plausible
solutions under the same situation. Notably, IL-based methods encompass
inherent causal confusion, so enabling a neural network to generalize across
diverse parking scenarios is particularly difficult. To address these
challenges, we propose MultiPark, an autoregressive transformer for multimodal
parking. To handle paths filled with abrupt turning points, we introduce a
data-efficient next-segment prediction paradigm, enabling spatial
generalization and temporal extrapolation. Furthermore, we design learnable
parking queries factorized into gear, longitudinal, and lateral components,
parallelly decoding diverse parking behaviors. To mitigate causal confusion in
IL, our method employs target-centric pose and ego-centric collision as
outcome-oriented loss across all modalities beyond pure imitation loss.
Evaluations on real-world datasets demonstrate that MultiPark achieves
state-of-the-art performance across various scenarios. We deploy MultiPark on a
production vehicle, further confirming our approach's robustness in real-world
parking environments.

</details>


### [124] [Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads](https://arxiv.org/abs/2508.11547)
*Martin Jiroušek,Tomáš Báča,Martin Saska*

Main category: cs.RO

TL;DR: 论文提出仅用标准机载传感器跟踪无人机负载位置的框架，仿真和实验验证其性能接近真实测量，且鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 解决无人机携带电缆悬挂负载位置跟踪的问题，强调实际部署和最小硬件需求，避免依赖运动捕捉系统、额外机载摄像头或仪器化负载。

Method: 系统建模了飞行器和负载的完整耦合动力学，并集成了线性卡尔曼滤波器进行状态估计、模型预测轮廓控制规划器和增量模型预测控制器。

Result: 仿真显示系统性能接近基于地面真实测量的控制，仅轻微下降（<6%），且对负载参数变化表现出强鲁棒性。实地实验进一步验证了框架的实用性。

Conclusion: 该论文提出的框架在仅使用标准机载传感器的情况下，实现了与基于地面真实测量的控制相当的性能，验证了其在实际户外环境中的适用性和可靠性。

Abstract: This paper addresses the problem of tracking the position of a
cable-suspended payload carried by an unmanned aerial vehicle, with a focus on
real-world deployment and minimal hardware requirements. In contrast to many
existing approaches that rely on motion-capture systems, additional onboard
cameras, or instrumented payloads, we propose a framework that uses only
standard onboard sensors--specifically, real-time kinematic global navigation
satellite system measurements and data from the onboard inertial measurement
unit--to estimate and control the payload's position. The system models the
full coupled dynamics of the aerial vehicle and payload, and integrates a
linear Kalman filter for state estimation, a model predictive contouring
control planner, and an incremental model predictive controller. The control
architecture is designed to remain effective despite sensing limitations and
estimation uncertainty. Extensive simulations demonstrate that the proposed
system achieves performance comparable to control based on ground-truth
measurements, with only minor degradation (< 6%). The system also shows strong
robustness to variations in payload parameters. Field experiments further
validate the framework, confirming its practical applicability and reliable
performance in outdoor environments using only off-the-shelf aerial vehicle
hardware.

</details>


### [125] [Nominal Evaluation Of Automatic Multi-Sections Control Potential In Comparison To A Simpler One- Or Two-Sections Alternative With Predictive Spray Switching](https://arxiv.org/abs/2508.11573)
*Mogens Plessen*

Main category: cs.RO

TL;DR: 研究比较了传统ASC与简化喷洒控制方法，提出了一种低成本、传感器自由的替代方案，适用于复杂农田场景。


<details>
  <summary>Details</summary>
Motivation: 传统自动段控制（ASC）需要复杂传感器且受多种不确定性因素影响，研究旨在探索一种更简单、低成本的替代方案。

Method: 比较了自动多段控制方法与提出的一或两段替代方法，结合了两种路径规划和喷洒逻辑，并在10个真实农田场景中评估了三种段配置（48段、2段和单段控制）。

Result: 研究表明，提出的简化方法在传感器自由的情况下仍能有效满足喷洒需求，并在路径规划和喷洒逻辑组合中表现优异。

Conclusion: 研究提出了一种更简单的自动多段控制方法，相比传统ASC方法，能够在传感器自由的情况下实现低成本喷洒，同时满足路径长度最小化、中等重叠等需求。

Abstract: Automatic Section Control (ASC) is a long-standing trend for spraying in
agriculture. It promises to minimise spray overlap areas. The core idea is to
(i) switch off spray nozzles on areas that have already been sprayed, and (ii)
to dynamically adjust nozzle flow rates along the boom bar that holds the spray
nozzles when velocities of boom sections vary during turn maneuvers. ASC is not
possible without sensors, in particular for accurate positioning data. Spraying
and the movement of modern wide boom bars are highly dynamic processes. In
addition, many uncertainty factors have an effect such as cross wind drift,
boom height, nozzle clogging in open-field conditions, and so forth. In view of
this complexity, the natural question arises if a simpler alternative exist.
Therefore, an Automatic Multi-Sections Control method is compared to a proposed
simpler one- or two-sections alternative that uses predictive spray switching.
The comparison is provided under nominal conditions. Agricultural spraying is
intrinsically linked to area coverage path planning and spray switching logic.
Combinations of two area coverage path planning and switching logics as well as
three sections-setups are compared. The three sections-setups differ by
controlling 48 sections, 2 sections or controlling all nozzles uniformly with
the same control signal as one single section. Methods are evaluated on 10
diverse real-world field examples, including non-convex field contours,
freeform mainfield lanes and multiple obstacle areas. A preferred method is
suggested that (i) minimises area coverage pathlength, (ii) offers intermediate
overlap, (iii) is suitable for manual driving by following a pre-planned
predictive spray switching logic for an area coverage path plan, and (iv) and
in contrast to ASC can be implemented sensor-free and therefore at low cost.

</details>


### [126] [Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks](https://arxiv.org/abs/2508.11584)
*Jakub Łucki,Jonathan Becktor,Georgios Georgakis,Robert Royce,Shehryar Khattak*

Main category: cs.RO

TL;DR: VPEngine是一个模块化框架，通过共享基础模型和多任务并行处理头，提升了机器人平台上视觉多任务的效率，实现了3倍速度提升和实时性能。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的机器人平台上部署多个机器学习模型时出现的计算冗余、内存占用大和集成复杂的问题。

Method: VPEngine采用共享基础模型骨干（如DINOv2）提取图像表示，并通过多任务并行处理头实现高效计算，避免了不必要的GPU-CPU内存传输。结合CUDA MPS技术，实现了高效的GPU利用和恒定内存占用。

Result: VPEngine在NVIDIA Jetson Orin AGX上实现了端到端实时性能（≥50 Hz），并通过TensorRT优化模型展示了高达3倍的速度提升。

Conclusion: VPEngine框架通过共享基础模型骨干和多任务并行处理头，显著提升了资源受限机器人平台上的视觉多任务处理效率，实现了高达3倍的速度提升，并保持了动态任务优先级调整的能力。

Abstract: Deploying multiple machine learning models on resource-constrained robotic
platforms for different perception tasks often results in redundant
computations, large memory footprints, and complex integration challenges. In
response, this work presents Visual Perception Engine (VPEngine), a modular
framework designed to enable efficient GPU usage for visual multitasking while
maintaining extensibility and developer accessibility. Our framework
architecture leverages a shared foundation model backbone that extracts image
representations, which are efficiently shared, without any unnecessary GPU-CPU
memory transfers, across multiple specialized task-specific model heads running
in parallel. This design eliminates the computational redundancy inherent in
feature extraction component when deploying traditional sequential models while
enabling dynamic task prioritization based on application demands. We
demonstrate our framework's capabilities through an example implementation
using DINOv2 as the foundation model with multiple task (depth, object
detection and semantic segmentation) heads, achieving up to 3x speedup compared
to sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine
offers efficient GPU utilization and maintains a constant memory footprint
while allowing per-task inference frequencies to be adjusted dynamically during
runtime. The framework is written in Python and is open source with ROS2 C++
(Humble) bindings for ease of use by the robotics community across diverse
robotic platforms. Our example implementation demonstrates end-to-end real-time
performance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized
models.

</details>


### [127] [Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation](https://arxiv.org/abs/2508.11588)
*Benjamin Walt,Jordan Westphal,Girish Krishnan*

Main category: cs.RO

TL;DR: 研究通过集成多传感器和随机森林分类器，实现了农业采摘中抓取状态的100%准确分类，提升了采摘效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 农业环境因复杂性、杂乱性和遮挡性带来独特挑战，需精确理解抓取状态以实现高效采摘，因此需选择合适的传感器和建模技术以获取可靠反馈。

Method: 研究评估了IMU、红外反射、张力、触觉传感器和RGB摄像头的贡献，并比较了随机森林和LSTM网络两种分类模型的性能。

Result: 在实验室控制环境中训练并在实际樱桃番茄植株上测试的随机森林分类器，对抓取状态的分类准确率达100%，显著优于基线性能。

Conclusion: 通过集成IMU和张力传感器的最小可行组合，结合随机森林分类器，该研究显著提高了农业采摘操作的效率和可靠性，实现了对抓取状态（滑动、抓取失败和成功采摘）的100%准确分类。

Abstract: Effective and efficient agricultural manipulation and harvesting depend on
accurately understanding the current state of the grasp. The agricultural
environment presents unique challenges due to its complexity, clutter, and
occlusion. Additionally, fruit is physically attached to the plant, requiring
precise separation during harvesting. Selecting appropriate sensors and
modeling techniques is critical for obtaining reliable feedback and correctly
identifying grasp states. This work investigates a set of key sensors, namely
inertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile
sensors, and RGB cameras, integrated into a compliant gripper to classify grasp
states. We evaluate the individual contribution of each sensor and compare the
performance of two widely used classification models: Random Forest and Long
Short-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest
classifier, trained in a controlled lab environment and tested on real cherry
tomato plants, achieved 100% accuracy in identifying slip, grasp failure, and
successful picks, marking a substantial improvement over baseline performance.
Furthermore, we identify a minimal viable sensor combination, namely IMU and
tension sensors that effectively classifies grasp states. This classifier
enables the planning of corrective actions based on real-time feedback, thereby
enhancing the efficiency and reliability of fruit harvesting operations.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [128] [LayoutRectifier: An Optimization-based Post-processing for Graphic Design Layout Generation](https://arxiv.org/abs/2508.11177)
*I-Chao Shen,Ariel Shamir,Takeo Igarashi*

Main category: cs.GR

TL;DR: LayoutRectifier 是一种优化方法，通过两阶段优化减少自动生成布局的瑕疵，提升布局质量且无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法生成的布局常存在瑕疵（如未对齐、重叠、不满足包含关系），影响了设计质量。

Method: 提出了一种基于优化的方法 LayoutRectifier，包括离散搜索以利用网格系统减少未对齐问题，以及引入新的盒子包含函数来调整元素位置和大小以避免重叠和促进包含。

Result: 在内容无关和内容感知的布局生成任务中，LayoutRectifier 生成了更高质量的布局，更适合下游图形设计任务。

Conclusion: LayoutRectifier 通过两阶段优化方法有效减少了自动生成布局中的瑕疵，如未对齐、重叠和不满足的包含关系，且无需额外训练即可提升布局质量。

Abstract: Recent deep learning methods can generate diverse graphic design layouts
efficiently. However, these methods often create layouts with flaws, such as
misalignment, unwanted overlaps, and unsatisfied containment. To tackle this
issue, we propose an optimization-based method called LayoutRectifier, which
gracefully rectifies auto-generated graphic design layouts to reduce these
flaws while minimizing deviation from the generated layout. The core of our
method is a two-stage optimization. First, we utilize grid systems, which
professional designers commonly use to organize elements, to mitigate
misalignments through discrete search. Second, we introduce a novel box
containment function designed to adjust the positions and sizes of the layout
elements, preventing unwanted overlapping and promoting desired containment. We
evaluate our method on content-agnostic and content-aware layout generation
tasks and achieve better-quality layouts that are more suitable for downstream
graphic design tasks. Our method complements learning-based layout generation
methods and does not require additional training.

</details>


### [129] [StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation](https://arxiv.org/abs/2508.11203)
*Seungmi Lee,Kwan Yun,Junyong Noh*

Main category: cs.GR

TL;DR: StyleMM是一个通过文本描述生成风格化3D人脸模型的新框架，利用扩散模型和图像训练实现一致风格迁移，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够根据用户定义的文本描述生成风格化3D可变形模型（3DMM）的框架。

Method: 基于预训练的网格变形网络和纹理生成器，通过文本引导的图像到图像翻译生成风格化目标图像，并在训练中保持关键面部属性。

Result: StyleMM能够前馈生成具有一致顶点连接性和可动画性的风格化面部网格，并允许对形状、表情和纹理参数进行显式控制。

Conclusion: StyleMM通过定量和定性评估证明，在面部多样性和风格化能力方面优于现有方法，支持代码和视频公开。

Abstract: We introduce StyleMM, a novel framework that can construct a stylized 3D
Morphable Model (3DMM) based on user-defined text descriptions specifying a
target style. Building upon a pre-trained mesh deformation network and a
texture generator for original 3DMM-based realistic human faces, our approach
fine-tunes these models using stylized facial images generated via text-guided
image-to-image (i2i) translation with a diffusion model, which serve as
stylization targets for the rendered mesh. To prevent undesired changes in
identity, facial alignment, or expressions during i2i translation, we introduce
a stylization method that explicitly preserves the facial attributes of the
source image. By maintaining these critical attributes during image
stylization, the proposed approach ensures consistent 3D style transfer across
the 3DMM parameter space through image-based training. Once trained, StyleMM
enables feed-forward generation of stylized face meshes with explicit control
over shape, expression, and texture parameters, producing meshes with
consistent vertex connectivity and animatability. Quantitative and qualitative
evaluations demonstrate that our approach outperforms state-of-the-art methods
in terms of identity-level facial diversity and stylization capability. The
code and videos are available at
[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).

</details>


### [130] [SPG: Style-Prompting Guidance for Style-Specific Content Creation](https://arxiv.org/abs/2508.11476)
*Qian Liang,Zichong Chen,Yang Zhou,Hui Huang*

Main category: cs.GR

TL;DR: SPG是一种新的采样策略，通过风格噪声向量引导扩散过程，实现风格控制和语义保真，兼容性强且效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型在控制输出视觉风格方面存在挑战，需要一种既能保持语义保真度又能实现风格一致性的方法。

Method: 提出Style-Prompting Guidance (SPG)，通过构建风格噪声向量并利用其与无条件噪声的方向偏差来引导扩散过程，结合Classifier-Free Guidance (CFG)实现语义和风格一致性。

Result: SPG在实验中表现出色，优于现有技术，且兼容ControlNet和IPAdapter等可控框架。

Conclusion: SPG是一种简单、鲁棒且兼容性强的采样策略，能够有效控制文本到图像扩散模型的输出风格，同时保持语义保真度。

Abstract: Although recent text-to-image (T2I) diffusion models excel at aligning
generated images with textual prompts, controlling the visual style of the
output remains a challenging task. In this work, we propose Style-Prompting
Guidance (SPG), a novel sampling strategy for style-specific image generation.
SPG constructs a style noise vector and leverages its directional deviation
from unconditional noise to guide the diffusion process toward the target style
distribution. By integrating SPG with Classifier-Free Guidance (CFG), our
method achieves both semantic fidelity and style consistency. SPG is simple,
robust, and compatible with controllable frameworks like ControlNet and
IPAdapter, making it practical and widely applicable. Extensive experiments
demonstrate the effectiveness and generality of our approach compared to
state-of-the-art methods. Code is available at
https://github.com/Rumbling281441/SPG.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [131] [EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI Training](https://arxiv.org/abs/2508.11035)
*Hasibul Jamil,MD S Q Zulkar Nine,Tevfik Kosar*

Main category: cs.DC

TL;DR: EMLIO是一种高效的机器学习I/O服务，通过优化数据加载延迟和能源消耗，显著提升大规模深度学习工作负载的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习工作负载因数据集超出本地存储容量和GPU计算超越网络及磁盘延迟而面临I/O瓶颈，现有系统忽略了I/O的能源成本。

Method: EMLIO在存储节点上部署轻量级数据服务守护进程，通过TCP流式传输和乱序预取技术，与客户端的GPU加速预处理无缝集成。

Result: 在本地磁盘、LAN和WAN环境下，EMLIO比现有最优加载器快8.6倍，能源消耗降低10.9倍，且性能和能源消耗不受网络距离影响。

Conclusion: EMLIO提供了一种可扩展的能源感知I/O架构，适用于下一代AI云计算环境。

Abstract: Large-scale deep learning workloads increasingly suffer from I/O bottlenecks
as datasets grow beyond local storage capacities and GPU compute outpaces
network and disk latencies. While recent systems optimize data-loading time,
they overlook the energy cost of I/O - a critical factor at large scale. We
introduce EMLIO, an Efficient Machine Learning I/O service that jointly
minimizes end-to-end data-loading latency T and I/O energy consumption E across
variable-latency networked storage. EMLIO deploys a lightweight data-serving
daemon on storage nodes that serializes and batches raw samples, streams them
over TCP with out-of-order prefetching, and integrates seamlessly with
GPU-accelerated (NVIDIA DALI) preprocessing on the client side. In exhaustive
evaluations over local disk, LAN (0.05 ms & 10 ms RTT), and WAN (30 ms RTT)
environments, EMLIO delivers up to 8.6X faster I/O and 10.9X lower energy use
compared to state-of-the-art loaders, while maintaining constant performance
and energy profiles irrespective of network distance. EMLIO's service-based
architecture offers a scalable blueprint for energy-aware I/O in
next-generation AI clouds.

</details>


### [132] [Element and Everything Tokens: Two-Tier Architecture for Mobilizing Alternative Assets](https://arxiv.org/abs/2508.11266)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.DC

TL;DR: 论文提出了一种两层代币化架构，通过Element Tokens和Everything Tokens增强复杂资产的流动性和透明度，实现高价值项目的分割和交易。


<details>
  <summary>Details</summary>
Motivation: 传统框架下，大型、异构的资产（如矿山、发电厂或基础设施项目）难以交易或分割，导致流动性不足和透明度低。

Method: 引入Element Tokens和Everything Tokens的概念，设计了一个双向可转换系统，并通过套利机制保持组合代币价格与净资产价值的对齐。

Result: 通过能源和工业领域的示例，证明了该架构能够将高价值项目分割并像股票或ETF一样交易，从而降低进入门槛、提高价格发现和灵活融资。

Conclusion: 论文提出了一种新颖的两层代币化架构，通过Element Tokens和Everything Tokens的概念，增强了复杂资产的流动性和透明度，为投资者和资产所有者带来了诸多好处。

Abstract: Alternative assets such as mines, power plants, or infrastructure projects
are often large, heterogeneous bundles of resources, rights, and outputs whose
value is difficult to trade or fractionalize under traditional frameworks. This
paper proposes a novel two-tier tokenization architecture to enhance the
liquidity and transparency of such complex assets. We introduce the concepts of
Element Tokens and Everything Tokens: elemental tokens represent standardized,
fully collateralized components of an asset (e.g., outputs, rights, or
credits), while an everything token represents the entire asset as a fixed
combination of those elements. The architecture enables both fine-grained
partial ownership and integrated whole-asset ownership through a system of
two-way convertibility. We detail the design and mechanics of this system,
including an arbitrage mechanism that keeps the price of the composite token
aligned with the net asset value of its constituents. Through illustrative
examples in the energy and industrial sectors, we demonstrate that our approach
allows previously illiquid, high-value projects to be fractionalized and traded
akin to stocks or exchange-traded funds (ETFs). We discuss the benefits for
investors and asset owners, such as lower entry barriers, improved price
discovery, and flexible financing, as well as the considerations for
implementation and regulation.

</details>


### [133] [Inter-APU Communication on AMD MI300A Systems via Infinity Fabric: a Deep Dive](https://arxiv.org/abs/2508.11298)
*Gabin Schieffer,Jacob Wahlgren,Ruimin Shi,Edgar A. León,Roger Pearce,Maya Gokhale,Ivy Peng*

Main category: cs.DC

TL;DR: 研究评估了AMD MI300A APU在多APU系统中的通信效率，提出了优化设计选择，并通过实际应用验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着GPU加速器计算性能的不断提升，HPC应用程序需要高效的数据移动以维持性能。AMD MI300A APU将CPU、GPU和高带宽内存（HBM）集成在一个物理封装中，为解决CPU-GPU数据移动问题提供了解决方案。

Method: 设计了特定的基准测试来评估GPU的直接内存访问、显式的APU间数据移动以及多APU集体通信。比较了HIP API、MPI例程和GPU专用RCCL库的效率。

Result: 研究结果突出了在多APU AMD MI300A系统中优化APU间通信的关键设计选择。通过优化两个实际HPC应用程序，验证了这些选择的有效性。

Conclusion: 研究强调了在多APU AMD MI300A系统中优化APU间通信的关键设计选择，包括编程接口、分配器和数据移动。通过优化两个实际HPC应用程序（Quicksilver和CloverLeaf），验证了这些选择的有效性。

Abstract: The ever-increasing compute performance of GPU accelerators drives up the
need for efficient data movements within HPC applications to sustain
performance. Proposed as a solution to alleviate CPU-GPU data movement, AMD
MI300A Accelerated Processing Unit (APU) combines CPU, GPU, and high-bandwidth
memory (HBM) within a single physical package. Leadership supercomputers, such
as El Capitan, group four APUs within a single compute node, using Infinity
Fabric Interconnect. In this work, we design specific benchmarks to evaluate
direct memory access from the GPU, explicit inter-APU data movement, and
collective multi-APU communication. We also compare the efficiency of HIP APIs,
MPI routines, and the GPU-specialized RCCL library. Our results highlight key
design choices for optimizing inter-APU communication on multi-APU AMD MI300A
systems with Infinity Fabric, including programming interfaces, allocators, and
data movement. Finally, we optimize two real HPC applications, Quicksilver and
CloverLeaf, and evaluate them on a four MI100A APU system.

</details>


### [134] [Space-efficient population protocols for exact majority in general graphs](https://arxiv.org/abs/2508.11384)
*Joel Rybicki,Jakob Solnerzik,Olivier Stietel,Robin Vacus*

Main category: cs.DC

TL;DR: 本文改进了精确多数共识在一般图中的上下界，提供了参数化上界和状态复杂度分析，在规则扩展图中匹配最优空间复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究精确多数共识在一般图中的性能边界，以改进现有协议的时间复杂度和空间复杂度。

Method: 研究采用了基于图的随机游走松弛时间和度不平衡参数化的方法，设计了一种协议，该协议在期望和高概率下稳定，并分析了其状态复杂度。

Result: 提出了新的上界和下界，特别是在规则扩展图中展示了与最优空间复杂度的匹配，并提供了一个恒定状态协议的新上界。

Conclusion: 本文改进了对一般图中精确多数共识问题的上下界分析，提供了新的参数化上界，并展示了在规则扩展图中与最优空间复杂度的匹配。

Abstract: We study exact majority consensus in the population protocol model. In this
model, the system is described by a graph $G = (V,E)$ with $n$ nodes, and in
each time step, a scheduler samples uniformly at random a pair of adjacent
nodes to interact. In the exact majority consensus task, each node is given a
binary input, and the goal is to design a protocol that almost surely reaches a
stable configuration, where all nodes output the majority input value.
  We give improved upper and lower bounds for the exact majority in general
graphs. First, we give asymptotically tight time lower bounds for general
(unbounded space) protocols. Second, we obtain new upper bounds parameterized
by the relaxation time $\tau_{\mathsf{rel}}$ of the random walk on $G$ induced
by the scheduler and the degree imbalance $\Delta/\delta$ of $G$. Specifically,
we give a protocol that stabilizes in $O\left( \tfrac{\Delta}{\delta}
\tau_{\mathsf{rel}} \log^2 n \right)$ steps in expectation and with high
probability and uses $O\left( \log n \cdot \left(
\log\left(\tfrac{\Delta}{\delta}\right) + \log
\left(\tfrac{\tau_{\mathsf{rel}}}{n}\right) \right) \right)$ states in any
graph with minimum degree at least $\delta$ and maximum degree at most
$\Delta$.
  For regular expander graphs, this matches the optimal space complexity of
$\Theta(\log n)$ for fast protocols in complete graphs [Alistarh et al., SODA
2016 and Doty et al., FOCS 2022] with a nearly optimal stabilization time of
$O(n \log^2 n)$ steps. Finally, we give a new upper bound of
$O(\tau_{\mathsf{rel}} \cdot n \log n)$ for the stabilization time of a
constant-state protocol.

</details>


### [135] [Time, Fences and the Ordering of Events in TSO](https://arxiv.org/abs/2508.11415)
*Raïssa Nataf,Yoram Moses*

Main category: cs.DC

TL;DR: 该研究提出了一个语义框架，用于在TSO内存模型下精确确定同步操作的必要性，通过引入‘occurs-before’关系并分析同步操作的作用，为理解TSO模型中的信息流和因果关系提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 在TSO内存模型下，硬件优化（如存储缓冲）虽然提升了多处理器效率，但增加了正确性推理的复杂性。现有方法通常依赖同步操作（如内存屏障或原子操作），但这些操作会带来性能开销。本研究旨在精确描述何时需要这些同步操作。

Method: 研究引入了一种新颖的TSO特定的‘occurs-before’关系，并基于此关系开发了一个语义框架。通过分析内存屏障和原子操作在创建‘occurs-before’链中的作用，研究确定了这些同步操作的必要性。

Result: 研究发现，确保事件时间顺序的唯一方法是通过执行创建‘occurs-before’链。通过分析同步操作在创建这些链中的作用，研究明确了这些操作的必要性。此外，研究推广了先前关于共享内存对象线性化实现的下界。

Conclusion: 该研究提出了一个语义框架，用于精确描述在TSO（Total Store Order）内存模型下何时需要同步操作（如内存屏障或原子操作）。通过引入一种新颖的TSO特定的‘occurs-before’关系，研究证明了确保事件时间顺序的唯一方法是通过执行创建‘occurs-before’链。这一结果为理解TSO模型中的信息流和因果关系提供了理论基础，并推广了先前关于共享内存对象线性化实现的下界。

Abstract: The Total Store Order (TSO) is arguably the most widely used relaxed memory
model in multiprocessor architectures, widely implemented, for example in
Intel's x86 and x64 platforms. It allows processes to delay the visibility of
writes through store buffering. While this supports hardware-level
optimizations and makes a significant contribution to multiprocessor
efficiency, it complicates reasoning about correctness, as executions may
violate sequential consistency. Ensuring correct behavior often requires
inserting synchronization primitives such as memory fences ($F$) or atomic
read-modify-write ($RMW$) operations, but this approach can incur significant
performance costs. In this work, we develop a semantic framework that precisely
characterizes when such synchronization is necessary under TSO. We introduce a
novel TSO-specific occurs-before relation, which adapts Lamport's celebrated
happens-before relation from asynchronous message-passing systems to the TSO
setting. Our main result is a theorem that proves that the only way to ensure
that two events that take place at different sites are temporally ordered is by
having the execution create an occurs-before chain between the events. By
studying the role of fences and $RMW$s in creating occurs-before chains, we are
then able to capture cases in which these costly synchronization operations are
unavoidable. Since proper real-time ordering of events is a fundamental aspect
of consistency conditions such as Linearizability, our analysis provides a
sound theoretical understanding of essential aspects of the TSO model. In
particular, we are able to generalize prior lower bounds for linearizable
implementations of shared memory objects. Our results capture the structure of
information flow and causality in the TSO model by extending the standard
communication-based reasoning from asynchronous systems to the TSO memory
model.

</details>


### [136] [Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method](https://arxiv.org/abs/2508.11467)
*Shifang Liu,Huiyuan Li,Hongjiao Sheng,Haoyuan Gui,Xiaoyu Zhang*

Main category: cs.DC

TL;DR: 提出了一种完全在GPU上执行的SVD算法，通过消除CPU-GPU数据传输和优化计算流程，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统SVD方法在异构系统中因面板分解速度慢和频繁的CPU-GPU数据传输而受限，尽管GPU计算能力有所提升。

Method: 引入了一种基于GPU的双对角分治（BDC）方法，重新设计了SVD计算的算法和数据布局，完全在GPU上执行面板级计算和尾随矩阵更新，并通过整合计算优化BLAS利用率。

Result: 在AMD MI210和NVIDIA V100 GPU上，相比rocSOLVER/cuSOLVER和MAGMA，分别实现了最高1293.64x/7.47x和14.10x/12.38x的加速。

Conclusion: 提出的GPU中心SVD算法通过消除CPU-GPU数据传输和优化BLAS利用率，显著提升了计算性能，实验证明在AMD和NVIDIA GPU上均取得了显著的加速效果。

Abstract: Singular Value Decomposition (SVD) is a fundamental matrix factorization
technique in linear algebra, widely applied in numerous matrix-related
problems. However, traditional SVD approaches are hindered by slow panel
factorization and frequent CPU-GPU data transfers in heterogeneous systems,
despite advancements in GPU computational capabilities. In this paper, we
introduce a GPU-centered SVD algorithm, incorporating a novel GPU-based
bidiagonal divide-and-conquer (BDC) method. We reformulate the algorithm and
data layout of different steps for SVD computation, performing all panel-level
computations and trailing matrix updates entirely on GPU to eliminate CPU-GPU
data transfers. Furthermore, we integrate related computations to optimize BLAS
utilization, thereby increasing arithmetic intensity and fully leveraging the
computational capabilities of GPUs. Additionally, we introduce a newly
developed GPU-based BDC algorithm that restructures the workflow to eliminate
matrix-level CPU-GPU data transfers and enable asynchronous execution between
the CPU and GPU. Experimental results on AMD MI210 and NVIDIA V100 GPUs
demonstrate that our proposed method achieves speedups of up to 1293.64x/7.47x
and 14.10x/12.38x compared to rocSOLVER/cuSOLVER and MAGMA, respectively.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [137] [Grounding Rule-Based Argumentation Using Datalog](https://arxiv.org/abs/2508.10976)
*Martin Diller,Sarah Alice Gaggl,Philipp Hanisch,Giuseppina Monterosso,Fritz Rauschenbach*

Main category: cs.AI

TL;DR: 提出了一种智能基础化方法，将ASPIC+一阶规则转换为Datalog程序，有效控制基础化规模并保持推理正确性，实证评估证明其可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多仅支持命题规则，而一阶规则的基础化可能导致输入理论规模指数级增长，缺乏针对ASPIC+的专用解决方案。

Method: 将一阶ASPIC+实例翻译为Datalog程序，利用Datalog引擎获取基础替换，并对ASPIC+形式化进行特定简化以避免无关规则的基础化。

Result: 通过原型实现的实证评估展示了该方法的可扩展性。

Conclusion: 本文提出了一种智能基础化程序，用于处理ASPIC+中的一阶规则，通过将ASPIC+实例转换为Datalog程序并利用Datalog引擎进行查询，有效控制了基础化规模，同时保持了推理过程的正确性。

Abstract: ASPIC+ is one of the main general frameworks for rule-based argumentation for
AI. Although first-order rules are commonly used in ASPIC+ examples, most
existing approaches to reason over rule-based argumentation only support
propositional rules. To enable reasoning over first-order instances, a
preliminary grounding step is required. As groundings can lead to an
exponential increase in the size of the input theories, intelligent procedures
are needed. However, there is a lack of dedicated solutions for ASPIC+.
Therefore, we propose an intelligent grounding procedure that keeps the size of
the grounding manageable while preserving the correctness of the reasoning
process. To this end, we translate the first-order ASPIC+ instance into a
Datalog program and query a Datalog engine to obtain ground substitutions to
perform the grounding of rules and contraries. Additionally, we propose
simplifications specific to the ASPIC+ formalism to avoid grounding of rules
that have no influence on the reasoning process. Finally, we performed an
empirical evaluation of a prototypical implementation to show scalability.

</details>


### [138] [From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching](https://arxiv.org/abs/2508.11070)
*Zahra Khotanlou,Kate Larson,Amir-Hossein Karimi*

Main category: cs.AI

TL;DR: 本文提出了一种多主体算法追索框架，通过三层优化解决了多对多交互问题，实验证明其能高效提升社会福利。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的算法追索往往涉及多个交互的利益相关者，而现有研究主要关注单一个体和单一模型场景。个体福利方法忽视了现实系统的多主体性质，其中个体需要竞争有限资源。

Method: 模型将多对多交互建模为一个带容量限制的加权二分图匹配问题，匹配由追索成本和提供者容量共同指导。提出了三层优化框架：基本容量匹配、最优容量再分配以最小化福利差距，以及平衡福利最大化和容量调整成本的成本感知优化。

Result: 在合成和真实数据集上的实验验证表明，该框架能够在系统设置最小修改的情况下，使多对多算法追索实现接近最优的福利。

Conclusion: 本文提出了一种新颖的多主体算法追索框架，通过三层优化（容量匹配、最优容量再分配和成本感知优化），实现了从个体推荐到系统级设计的扩展，提高了社会福利同时保持个体可操作性。

Abstract: Decision makers are increasingly relying on machine learning in sensitive
situations. In such settings, algorithmic recourse aims to provide individuals
with actionable and minimally costly steps to reverse unfavorable AI-driven
decisions. While existing research predominantly focuses on single-individual
(i.e., seeker) and single-model (i.e., provider) scenarios, real-world
applications often involve multiple interacting stakeholders. Optimizing
outcomes for seekers under an individual welfare approach overlooks the
inherently multi-agent nature of real-world systems, where individuals interact
and compete for limited resources. To address this, we introduce a novel
framework for multi-agent algorithmic recourse that accounts for multiple
recourse seekers and recourse providers. We model this many-to-many interaction
as a capacitated weighted bipartite matching problem, where matches are guided
by both recourse cost and provider capacity. Edge weights, reflecting recourse
costs, are optimized for social welfare while quantifying the welfare gap
between individual welfare and this collectively feasible outcome. We propose a
three-layer optimization framework: (1) basic capacitated matching, (2) optimal
capacity redistribution to minimize the welfare gap, and (3) cost-aware
optimization balancing welfare maximization with capacity adjustment costs.
Experimental validation on synthetic and real-world datasets demonstrates that
our framework enables the many-to-many algorithmic recourse to achieve
near-optimal welfare with minimum modification in system settings. This work
extends algorithmic recourse from individual recommendations to system-level
design, providing a tractable path toward higher social welfare while
maintaining individual actionability.

</details>


### [139] [Learn to optimize for automatic proton PBS treatment planning for H&N cancers](https://arxiv.org/abs/2508.11085)
*Qingqing Wang,Liqiang Xiao,Chang Chang*

Main category: cs.AI

TL;DR: 提出数据驱动逆向优化器结合PPO框架，自动生成高质量质子治疗计划，显著提升效率与效果。


<details>
  <summary>Details</summary>
Motivation: 解决传统质子PBS治疗计划中人工调整参数和逆向优化的高耗时问题。

Method: 采用L2O方法和Transformer框架处理长上下文，结合PPO策略网络调整目标参数。

Result: L2O逆向优化器效率提升36.41%，效果提升22.97%，生成的计划质量优于或等同于人工计划。

Conclusion: 该研究提出了一种数据驱动的逆向优化器，并结合PPO框架自动生成高质量治疗计划，显著提高了效率和效果。

Abstract: Proton PBS treatment planning for H&N cancers involves numerous conflicting
objectives, requiring significant effort from human planners to balance and
satisfy multiple clinical goals during planning. To achieve this,
experience-demanding objective parameter adjustment and computationally
expensive inverse optimization are performed iteratively. Extensive efforts
have been made to automatically adjust objective parameters, but the most
time-consuming component, i.e., inverse optimization, still relies heavily on
theory-driven approaches. We propose a data-driven inverse optimizer and
integrate it into a PPO-based automatic treatment planning framework to
automatically generate high-quality plans within a clinical acceptable planning
time. The inverse optimizer is a L2O method that predicts update steps by
learning from the task-specific data distribution. For the first time, we
integrate techniques designed for long-context processing, originally developed
for LLMs, into a Transformer-based L2O framework to address the scalability
issue of existing L2O methods. The PPO framework functions as an outer-loop
virtual planner, autonomously adjusting objective parameters through a policy
network, and the dose predictor is used to initialize objective parameters. The
inner-loop L2O inverse optimizer computes machine-deliverable MU values based
on objectives refined by the PPO policy network. 97 patients are collected in
this study, and compared with L-BFGSB, our L2O-based inverse optimizer improves
the effectiveness and efficiency by 22.97% and 36.41%, respectively. In
conjunction with the PPO-based learned virtual planner, plans generated by our
framework within an average of 2.55 hours show improved or comparable OAR
sparing with superior target coverage for patients with different prescription
dose levels, number of target volumes, beam angles, etc., compared with
human-generated plans.

</details>


### [140] [On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation](https://arxiv.org/abs/2508.11182)
*Matti Berthold,Lydia Blümel,Anna Rapberger*

Main category: cs.AI

TL;DR: 本文研究了非平坦ABA中强和弱可接受性，展示了它们的模块化性质和局限性。


<details>
  <summary>Details</summary>
Motivation: 拓宽对ABA中可接受性概念的理解，特别是在非平坦ABA中，填补强可接受性研究的空白。

Method: 使用抽象双极集基论证框架（BSAFs）作为形式化工具，扩展了对平坦ABA中弱可接受性的研究，并首次在ABA中引入了强可接受性。

Result: 证明了在非平坦ABA中，强和弱可接受性语义保持模块化性质，但也揭示了它们与标准可接受性语义共有的局限性。

Conclusion: 本文探讨了在基于假设的论证（ABA）中强和弱可接受性的替代概念，并展示了在非平坦ABA中这些概念的模块化性质及其局限性。

Abstract: In this work, we broaden the investigation of admissibility notions in the
context of assumption-based argumentation (ABA). More specifically, we study
two prominent alternatives to the standard notion of admissibility from
abstract argumentation, namely strong and weak admissibility, and introduce the
respective preferred, complete and grounded semantics for general (sometimes
called non-flat) ABA. To do so, we use abstract bipolar set-based argumentation
frameworks (BSAFs) as formal playground since they concisely capture the
relations between assumptions and are expressive enough to represent general
non-flat ABA frameworks, as recently shown. While weak admissibility has been
recently investigated for a restricted fragment of ABA in which assumptions
cannot be derived (flat ABA), strong admissibility has not been investigated
for ABA so far. We introduce strong admissibility for ABA and investigate
desirable properties. We furthermore extend the recent investigations of weak
admissibility in the flat ABA fragment to the non-flat case. We show that the
central modularization property is maintained under classical, strong, and weak
admissibility. We also show that strong and weakly admissible semantics in
non-flat ABA share some of the shortcomings of standard admissible semantics
and discuss ways to address these.

</details>


### [141] [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
*Youcheng Huang,Bowen Qin,Chen Huang,Duanyu Feng,Xi Yang,Wenqiang Lei*

Main category: cs.AI

TL;DR: LRMs在信息不足时无法主动询问，研究揭示了其局限性并探讨了监督微调的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅评估LRMs在定义良好的问题上的解决能力，而忽略了其在信息不足时主动询问的能力。

Method: 提出了一个新的数据集，包含两种类型的不完整问题，并系统评估了LRMs在这些问题上的表现。

Result: 发现LRMs在主动询问信息方面表现不佳，并揭示了其过度思考和幻觉行为。

Conclusion: 论文强调了开发具有真正智能的LRMs的重要性，而不仅仅是问题解决能力。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving
abilities in mathematics, as evaluated by existing benchmarks exclusively on
well-defined problems. However, such evaluation setup constitutes a critical
gap, since a genuine intelligent agent should not only solve problems (as a
math quiz solver), but also be able~to ask for information when the problems
lack sufficient information, enabling proactivity in responding users'
requests. To bridge such gap, we proposes a new dataset consisting of two types
of incomplete problems with diverse contexts. Based on the dataset, our
systematical evaluation of LRMs reveals their inability in proactively asking
for information. In addition, we uncover the behaviors related to overthinking
and hallucination of LRMs, and highlight the potential and challenges of
supervised fine-tuning in learning such ability. We hope to provide new
insights in developing LRMs with genuine intelligence, rather than just solving
problems.

</details>


### [142] [SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.11347)
*Yifei Li,Lingling Zhang,Hang Yan,Tianzhe Zhao,Zihan Ma,Muye Huang,Jun Liu*

Main category: cs.AI

TL;DR: SAGE是一种自适应嵌入维度的动态知识图谱嵌入框架，通过动态蒸馏机制有效平衡新旧知识，实验表明其在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统KG嵌入方法主要针对静态图，而现实KG是动态演化的，现有CKGE方法未能充分考虑更新规模的变化。

Method: SAGE提出了一种基于更新规模确定嵌入维度的框架，并通过动态蒸馏机制平衡新旧知识的整合。

Result: 在七个基准测试中，SAGE显著优于现有基线方法，MRR提升1.38%，H@1提升1.25%，H@10提升1.6%。

Conclusion: SAGE框架通过自适应嵌入维度在CKGE中实现了最佳性能，证明了在动态知识图谱嵌入中考虑更新规模的重要性。

Abstract: Traditional knowledge graph (KG) embedding methods aim to represent entities
and relations in a low-dimensional space, primarily focusing on static graphs.
However, real-world KGs are dynamically evolving with the constant addition of
entities, relations and facts. To address such dynamic nature of KGs, several
continual knowledge graph embedding (CKGE) methods have been developed to
efficiently update KG embeddings to accommodate new facts while maintaining
learned knowledge. As KGs grow at different rates and scales in real-world
scenarios, existing CKGE methods often fail to consider the varying scales of
updates and lack systematic evaluation throughout the entire update process. In
this paper, we propose SAGE, a scale-aware gradual evolution framework for
CKGE. Specifically, SAGE firstly determine the embedding dimensions based on
the update scales and expand the embedding space accordingly. The Dynamic
Distillation mechanism is further employed to balance the preservation of
learned knowledge and the incorporation of new facts. We conduct extensive
experiments on seven benchmarks, and the results show that SAGE consistently
outperforms existing baselines, with a notable improvement of 1.38% in MRR,
1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with
methods using fixed embedding dimensions show that SAGE achieves optimal
performance on every snapshot, demonstrating the importance of adaptive
embedding dimensions in CKGE. The codes of SAGE are publicly available at:
https://github.com/lyfxjtu/Dynamic-Embedding.

</details>


### [143] [CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks](https://arxiv.org/abs/2508.11360)
*Songqin Nong,Jingxuan Xu,Sheng Zhou,Jianfeng Chen,Xiaoxuan Tang,Tao Jiang,Wenhao Xu*

Main category: cs.AI

TL;DR: CRAFT-GUI通过课程学习和精细化奖励设计，显著提升了GUI任务中的强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视GUI任务难度的差异，且奖励信号过于粗糙，导致学习效率低下。

Method: 提出了基于GRPO的课程学习框架CRAFT-GUI，设计了结合规则信号和模型评估的奖励函数。

Result: 在公开基准Android Control上提升5.6%，内部基准上提升10.3%。

Conclusion: 该研究通过结合强化学习和课程学习，在GUI交互任务中显著提升了性能，验证了方法的有效性。

Abstract: As autonomous agents become adept at understanding and interacting with
graphical user interface (GUI) environments, a new era of automated task
execution is emerging. Recent studies have demonstrated that Reinforcement
Learning (RL) can effectively enhance agents' performance in dynamic
interactive GUI environments. However, these methods face two key limitations:
(1) they overlook the significant variation in difficulty across different GUI
tasks by treating the entire training data as a uniform set, which hampers the
agent's ability to adapt its learning process; and (2) most approaches collapse
task-specific nuances into a single, coarse reward, leaving the agent with a
uniform signal that yields inefficient policy updates. To address these
limitations, we propose CRAFT-GUI, a curriculum learning framework based on
Group Relative Policy Optimization (GRPO) that explicitly accounts for the
varying difficulty across trajectories. To enable more fine-grained policy
optimization, we design a reward function that combines simple rule-based
signals with model-judged evaluation, providing richer and more nuanced
feedback during training. Experimental results demonstrate that our method
achieves significant improvements over previous state-of-the-art approaches,
outperforming them by 5.6% on public benchmarks Android Control and 10.3% on
our internal online benchmarks, respectively. These findings empirically
validate the effectiveness of integrating reinforcement learning with
curriculum learning in GUI interaction tasks.

</details>


### [144] [AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager](https://arxiv.org/abs/2508.11416)
*Xuhua Zhao,Yuxuan Xie,Caihua Chen,Yuxiang Sun*

Main category: cs.AI

TL;DR: 研究评估了LLM代理在不确定供应链管理中的决策行为，发现其存在与人类相似的偏见，并提出了减轻偏见的策略，强调了在库存决策中谨慎部署LLMs的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在数学推理和长期规划能力方面取得了进展，但其在不确定环境下做出库存决策的能力及决策偏见（如框架效应等）尚未充分探索，这引发了对其解决现实问题能力的担忧。

Method: 通过引入AIM-Bench这一新颖的基准测试，评估了LLM代理在不确定供应链管理场景中的决策行为，包括一系列多样化的库存补充实验。

Result: 研究发现不同LLMs通常表现出与人类相似的决策偏见程度，并探讨了减轻这些偏见的策略，如认知反思和信息共享的实施。

Conclusion: 研究强调了在库存决策中部署大型语言模型（LLMs）时需谨慎考虑潜在偏见的重要性，并提出了减轻人类决策偏见的策略，为开发以人为中心的供应链决策支持系统铺平了道路。

Abstract: Recent advances in mathematical reasoning and the long-term planning
capabilities of large language models (LLMs) have precipitated the development
of agents, which are being increasingly leveraged in business operations
processes. Decision models to optimize inventory levels are one of the core
elements of operations management. However, the capabilities of the LLM agent
in making inventory decisions in uncertain contexts, as well as the
decision-making biases (e.g. framing effect, etc.) of the agent, remain largely
unexplored. This prompts concerns regarding the capacity of LLM agents to
effectively address real-world problems, as well as the potential implications
of biases that may be present. To address this gap, we introduce AIM-Bench, a
novel benchmark designed to assess the decision-making behaviour of LLM agents
in uncertain supply chain management scenarios through a diverse series of
inventory replenishment experiments. Our results reveal that different LLMs
typically exhibit varying degrees of decision bias that are similar to those
observed in human beings. In addition, we explored strategies to mitigate the
pull-to-centre effect and the bullwhip effect, namely cognitive reflection and
implementation of information sharing. These findings underscore the need for
careful consideration of the potential biases in deploying LLMs in Inventory
decision-making scenarios. We hope that these insights will pave the way for
mitigating human decision bias and developing human-centred decision support
systems for supply chains.

</details>


### [145] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: Inclusion Arena是一个实时排行榜，通过用户反馈和创新的排名机制评估LLMs和MLLMs在实际应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试和排行榜多依赖静态数据集或通用领域提示，难以反映模型在真实应用中的表现。

Method: 采用Bradley-Terry模型，并引入Placement Matches和Proximity Sampling两项创新机制，以优化模型排名。

Result: Inclusion Arena提供了可靠且稳定的排名，数据传递性更高，并能有效减少恶意操纵风险。

Conclusion: Inclusion Arena通过结合实时用户反馈和创新的排名机制，为LLMs和MLLMs提供了更贴近实际应用的评估平台，加速了面向用户优化的模型开发。

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


### [146] [Landmark-Assisted Monte Carlo Planning](https://arxiv.org/abs/2508.11493)
*David H. Chan,Mark Roberts,Dana S. Nau*

Main category: cs.AI

TL;DR: 该论文提出了概率性地标的概念，并调整UCT算法以利用地标作为子目标分解MDP，实验证明地标能显著提升在线概率规划的性能。


<details>
  <summary>Details</summary>
Motivation: 地标在经典规划中促成了重大进展，但在随机领域中很少使用。

Method: 我们形式化了概率性地标，并调整了UCT算法以利用它们作为子目标来分解MDP；调整的核心是在贪婪地标达成与最终目标达成之间取得平衡。

Result: 基准领域的结果表明，精心选择的地标可以显著提高UCT在在线概率规划中的性能，而贪婪与长期目标达成的最佳平衡是问题依赖的。

Conclusion: 研究结果表明，地标可以为解决MDP的随时算法提供有益的指导。

Abstract: Landmarks$\unicode{x2013}$conditions that must be satisfied at some point in
every solution plan$\unicode{x2013}$have contributed to major advancements in
classical planning, but they have seldom been used in stochastic domains. We
formalize probabilistic landmarks and adapt the UCT algorithm to leverage them
as subgoals to decompose MDPs; core to the adaptation is balancing between
greedy landmark achievement and final goal achievement. Our results in
benchmark domains show that well-chosen landmarks can significantly improve the
performance of UCT in online probabilistic planning, while the best balance of
greedy versus long-term goal achievement is problem-dependent. The results
suggest that landmarks can provide helpful guidance for anytime algorithms
solving MDPs.

</details>


### [147] [Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models](https://arxiv.org/abs/2508.11524)
*Wenkai Yu,Jianhang Tang,Yang Zhang,Shanjiang Tang,Kebing Jin,Hankz Hankui Zhuo*

Main category: cs.AI

TL;DR: 提出结合问题分解的LLM辅助规划器，LLM4Predict在解决大规模规划问题时优于LLM4Inspire。


<details>
  <summary>Details</summary>
Motivation: 解决大规模规划问题中的状态空间爆炸问题，并探索如何有效结合LLM与领域特定知识以确保有效规划。

Method: 提出了一种结合问题分解的LLM辅助规划器，包括LLM4Inspire（提供启发式指导）和LLM4Predict（推断中间条件）。

Result: 实验结果表明，LLM在剪枝搜索空间时能有效定位可行解，LLM4Predict表现优于LLM4Inspire。

Conclusion: LLM4Predict（结合领域特定知识的LLM）在解决大规模规划问题时表现出色，相比仅提供通用知识的LLM4Inspire更具潜力。

Abstract: Addressing large-scale planning problems has become one of the central
challenges in the planning community, deriving from the state-space explosion
caused by growing objects and actions. Recently, researchers have explored the
effectiveness of leveraging Large Language Models (LLMs) to generate helpful
actions and states to prune the search space. However, prior works have largely
overlooked integrating LLMs with domain-specific knowledge to ensure valid
plans. In this paper, we propose a novel LLM-assisted planner integrated with
problem decomposition, which first decomposes large planning problems into
multiple simpler sub-tasks. Then we explore two novel paradigms to utilize
LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where
LLM4Inspire provides heuristic guidance according to general knowledge and
LLM4Predict employs domain-specific knowledge to infer intermediate conditions.
We empirically validate the effectiveness of our planner across multiple
domains, demonstrating the ability of search space partition when solving
large-scale planning problems. The experimental results show that LLMs
effectively locate feasible solutions when pruning the search space, where
infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds
particular promise compared with LLM4Inspire, which offers general knowledge
within LLMs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [148] [The Impact of Large Language Models (LLMs) on Code Review Process](https://arxiv.org/abs/2508.11034)
*Antonio Collante,Samuel Abedu,SayedHassan Khatoonabadi,Ahmad Abdellatif,Ebube Alor,Emad Shihab*

Main category: cs.SE

TL;DR: GPT辅助显著提升GitHub PR流程效率，减少解决时间60%，审查时间33%，等待时间87%，主要用于代码优化、错误修复和文档更新。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究探讨了任务特定应用，但LLM辅助对代码审查流程各阶段效率的影响仍未充分探索。

Method: 研究通过半自动启发式方法（结合关键词检测、正则表达式过滤和手动验证）识别GPT辅助的PR，并应用统计建模（包括多元线性回归和Mann-Whitney U检验）评估差异。

Result: GPT辅助的PR中位数解决时间减少了60%（9小时 vs 23小时），审查时间减少33%，等待接受时间减少87%。开发者主要将GPT用于代码优化（60%）、错误修复（26%）和文档更新（12%）。

Conclusion: 研究揭示了GPT在代码审查过程中的显著影响，为软件团队提供了可操作的见解，以优化工作流程和促进协作。

Abstract: Large language models (LLMs) have recently gained prominence in the field of
software development, significantly boosting productivity and simplifying
teamwork. Although prior studies have examined task-specific applications, the
phase-specific effects of LLM assistance on the efficiency of code review
processes remain underexplored. This research investigates the effect of GPT on
GitHub pull request (PR) workflows, with a focus on reducing resolution time,
optimizing phase-specific performance, and assisting developers. We curated a
dataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted
PRs using a semi-automated heuristic approach that combines keyword-based
detection, regular expression filtering, and manual verification until
achieving 95% labeling accuracy. We then applied statistical modeling,
including multiple linear regression and Mann-Whitney U test, to evaluate
differences between GPT-assisted and non-assisted PRs, both at the overall
resolution level and across distinct review phases. Our research has revealed
that early adoption of GPT can substantially boost the effectiveness of the PR
process, leading to considerable time savings at various stages. Our findings
suggest that GPT-assisted PRs reduced median resolution time by more than 60%
(9 hours compared to 23 hours for non-assisted PRs). We discovered that
utilizing GPT can reduce the review time by 33% and the waiting time before
acceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we
discovered that developers predominantly use GPT for code optimization (60%),
bug fixing (26%), and documentation updates (12%). This research sheds light on
the impact of the GPT model on the code review process, offering actionable
insights for software teams seeking to enhance workflows and promote seamless
collaboration.

</details>


### [149] [Diffusion is a code repair operator and generator](https://arxiv.org/abs/2508.11110)
*Mukul Singh,Gust Verbruggen,Vu Le,Sumit Gulwani*

Main category: cs.SE

TL;DR: 代码扩散模型后期行为可用于最后一英里修复和生成训练数据，实验验证了其在多个领域的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索代码扩散模型在后期步骤中表现出的行为是否可用于最后一英里修复，以提升代码修复的效率和生成训练数据的能力。

Method: 通过向损坏的代码片段添加噪声并恢复扩散过程，以及从扩散过程中采样中间程序和最终程序来生成训练数据。

Result: 在Python、Excel和PowerShell三个领域进行了实验，验证了方法的有效性和潜在应用。

Conclusion: 代码扩散模型在后期步骤中展现出的行为类似于最后一英里修复，这为利用预训练模型进行代码修复和生成训练数据提供了新的可能性。

Abstract: Code diffusion models generate code by iteratively removing noise from the
latent representation of a code snippet. During later steps of the diffusion
process, when the code snippet has almost converged, differences between
discrete representations of these snippets look like last-mile repairs applied
to broken or incomplete code. We evaluate the extent to which this resemblance
can be exploited to leverage pre-trained code diffusion models for the problem
of last-mile repair by considering two applications with significant potential.
First, we can leverage the diffusion model for last-mile repair by adding noise
to a broken code snippet and resuming the diffusion process. Second, we can
leverage the diffusion model to generate arbitrary amount of training data for
last-mile repair tasks (that are computationally more efficient) by sampling an
intermediate program (input) and the final program (output) from the diffusion
process. We perform experiments on 3 domains (Python, Excel and PowerShell) to
evaluate applications, as well as analyze properties.

</details>


### [150] [AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities](https://arxiv.org/abs/2508.11126)
*Huanting Wang,Jingzhi Gong,Huawei Zhang,Zheng Wang*

Main category: cs.SE

TL;DR: 本文综述了AI代理编程的最新进展，提出了分类法和核心技术，并指出了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理编程这一新兴领域的迅速发展，有必要定义其范围、巩固其技术基础并识别开放的研究挑战。

Method: 我们介绍了代理行为和系统架构的分类法，并研究了包括规划、内存和上下文管理、工具集成和执行监控在内的核心技术。

Result: 我们的研究发现了几个关键挑战，包括处理长上下文的局限性、跨任务缺乏持久内存，以及与人类开发者协作的安全性和对齐问题。

Conclusion: 本调查旨在为构建下一代智能且可信赖的AI编程代理的研究和开发提供基础，通过综合近期进展并概述未来方向。

Abstract: AI agentic programming is an emerging paradigm in which large language models
(LLMs) autonomously plan, execute, and interact with external tools like
compilers, debuggers, and version control systems to iteratively perform
complex software development tasks. Unlike conventional code generation tools,
agentic systems are capable of decomposing high-level goals, coordinating
multi-step processes, and adapting their behavior based on intermediate
feedback. These capabilities are transforming the software development
practice. As this emerging field evolves rapidly, there is a need to define its
scope, consolidate its technical foundations, and identify open research
challenges. This survey provides a comprehensive and timely review of AI
agentic programming. We introduce a taxonomy of agent behaviors and system
architectures, and examine core techniques including planning, memory and
context management, tool integration, and execution monitoring. We also analyze
existing benchmarks and evaluation methodologies used to assess coding agent
performance. Our study identifies several key challenges, including limitations
in handling long context, a lack of persistent memory across tasks, and
concerns around safety, alignment with user intent, and collaboration with
human developers. We discuss emerging opportunities to improve the reliability,
adaptability, and transparency of agentic systems. By synthesizing recent
advances and outlining future directions, this survey aims to provide a
foundation for research and development in building the next generation of
intelligent and trustworthy AI coding agents.

</details>


### [151] [From Feedback to Failure: Automated Android Performance Issue Reproduction](https://arxiv.org/abs/2508.11147)
*Zhengquan Li,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: RevPerf利用Google Play评论和多方面检测方法，成功复现70%的性能问题。


<details>
  <summary>Details</summary>
Motivation: 移动应用性能对用户体验至关重要，但性能问题在开发环境中难以检测和诊断。

Method: RevPerf结合相关评论和提示工程，丰富原始评论中的性能问题细节，并通过执行代理生成和执行命令来复现问题。系统还采用多方面的检测方法，如监控Android日志、GUI变化和系统资源利用率。

Result: 实验结果表明，RevPerf在构建并手动验证的数据集上，成功复现了70%的性能问题。

Conclusion: RevPerf是一种有效的性能问题复现工具，通过利用Google Play的评论和多种检测方法，成功复现了70%的性能问题。

Abstract: Mobile application performance is a vital factor for user experience. Yet,
performance issues are notoriously difficult to detect within development
environments, where their manifestations are often less conspicuous and
diagnosis proves more challenging. To address this limitation, we propose
RevPerf, an advanced performance issue reproduction tool that leverages app
reviews from Google Play to acquire pertinent information. RevPerf employs
relevant reviews and prompt engineering to enrich the original review with
performance issue details. An execution agent is then employed to generate and
execute commands to reproduce the issue. After executing all necessary steps,
the system incorporates multifaceted detection methods to identify performance
issues by monitoring Android logs, GUI changes, and system resource utilization
during the reproduction process. Experimental results demonstrate that our
proposed framework achieves a 70\% success rate in reproducing performance
issues on the dataset we constructed and manually validated.

</details>


### [152] [PTMPicker: Facilitating Efficient Pretrained Model Selection for Application Developers](https://arxiv.org/abs/2508.11179)
*Pei Liu,Terry Zhuo,Jiawei Deng,Zhenchang Xing,Qinghua Lu,Xiaoning Du,Hongyu Zhan*

Main category: cs.SE

TL;DR: PTMPicker 是一种结构化方法，用于更准确地匹配预训练模型与用户需求，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于关键词的模型搜索方法难以全面捕捉用户意图，尤其在考虑偏见缓解、硬件要求或许可证合规等因素时。

Method: PTMPicker 通过定义结构化模板表示PTM和用户需求，计算功能相关属性的嵌入相似度，并使用精心设计的提示评估特殊约束。

Result: 实验表明，PTMPicker 在合成的模型搜索请求上表现良好，85%的请求在前10个候选模型中成功匹配。

Conclusion: PTMPicker 能够有效帮助用户选择合适的预训练模型，85%的请求在前10个候选模型中成功找到合适的PTM。

Abstract: The rapid emergence of pretrained models (PTMs) has attracted significant
attention from both Deep Learning (DL) researchers and downstream application
developers. However, selecting appropriate PTMs remains challenging because
existing methods typically rely on keyword-based searches in which the keywords
are often derived directly from function descriptions. This often fails to
fully capture user intent and makes it difficult to identify suitable models
when developers also consider factors such as bias mitigation, hardware
requirements, or license compliance. To address the limitations of
keyword-based model search, we propose PTMPicker to accurately identify
suitable PTMs. We first define a structured template composed of common and
essential attributes for PTMs and then PTMPicker represents both candidate
models and user-intended features (i.e., model search requests) in this unified
format. To determine whether candidate models satisfy user requirements, it
computes embedding similarities for function-related attributes and uses
well-crafted prompts to evaluate special constraints such as license compliance
and hardware requirements. We scraped a total of 543,949 pretrained models from
Hugging Face to prepare valid candidates for selection. PTMPicker then
represented them in the predefined structured format by extracting their
associated descriptions. Guided by the extracted metadata, we synthesized a
total of 15,207 model search requests with carefully designed prompts, as no
such search requests are readily available. Experiments on the curated PTM
dataset and the synthesized model search requests show that PTMPicker can help
users effectively identify models,with 85% of the sampled requests successfully
locating appropriate PTMs within the top-10 ranked candidates.

</details>


### [153] [ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal](https://arxiv.org/abs/2508.11222)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Jiashui Wang,Xinlei Ying,Long Liu,Wenhai Wang*

Main category: cs.SE

TL;DR: ORFuzz是首个用于系统检测和分析LLM过度拒绝的进化测试框架，其生成的测试案例和基准集显著优于现有方法，为提升LLM可靠性提供了有力工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）日益表现出过度拒绝行为，即错误拒绝良性查询，这严重削弱了其可靠性和可用性。当前测试方法存在明显不足，需要更有效的解决方案。

Method: ORFuzz整合了三个核心组件：(1) 安全类别感知的种子选择，(2) 使用推理LLM进行自适应变异器优化，(3) OR-Judge，一个经过验证的人类对齐判断模型。

Result: ORFuzz生成的多样化、验证过的过度拒绝实例的平均率为6.98%，是领先基准的两倍以上。ORFuzzSet包含1,855个高度可转移的测试案例，平均过度拒绝率为63.56%，显著优于现有数据集。

Conclusion: ORFuzz和ORFuzzSet为开发更可靠和可信赖的基于LLM的软件系统提供了强大的自动化测试框架和宝贵的社区资源。

Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously
rejecting benign queries due to overly conservative safety measures - a
critical functional flaw that undermines their reliability and usability.
Current methods for testing this behavior are demonstrably inadequate,
suffering from flawed benchmarks and limited test generation capabilities, as
highlighted by our empirical user study. To the best of our knowledge, this
paper introduces the first evolutionary testing framework, ORFuzz, for the
systematic detection and analysis of LLM over-refusals. ORFuzz uniquely
integrates three core components: (1) safety category-aware seed selection for
comprehensive test coverage, (2) adaptive mutator optimization using reasoning
LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge
model validated to accurately reflect user perception of toxicity and refusal.
Our extensive evaluations demonstrate that ORFuzz generates diverse, validated
over-refusal instances at a rate (6.98% average) more than double that of
leading baselines, effectively uncovering vulnerabilities. Furthermore,
ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly
transferable test cases that achieves a superior 63.56% average over-refusal
rate across 10 diverse LLMs, significantly outperforming existing datasets.
ORFuzz and ORFuzzSet provide a robust automated testing framework and a
valuable community resource, paving the way for developing more reliable and
trustworthy LLM-based software systems.

</details>


### [154] [Hallucination in LLM-Based Code Generation: An Automotive Case Study](https://arxiv.org/abs/2508.11257)
*Marc Pavel,Nenad Petrovic,Lukasz Mazur,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: 论文研究了代码生成中的幻觉现象，发现即使先进模型也常出错，需更多上下文才能正确生成代码，强调安全关键领域需有效缓解技术。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在代码生成任务中展现出巨大潜力，但由于幻觉现象（输出看似合理但实际错误、不可验证或无意义），其实际应用受到限制。本研究旨在探讨代码生成中的幻觉现象，特别是在汽车领域。

Method: 通过案例研究评估了多种代码LLM在不同提示复杂度下的表现，从简单的一行提示到包含Covesa车辆信号规范（VSS）和代码骨架的丰富上下文提示。

Result: 评估发现，即使是先进的模型（如GPT-4.1、Codex和GPT-4o）也频繁出现语法违规、无效引用错误和API知识冲突。只有在提供最丰富上下文的提示时，GPT-4.1和GPT-4o才能生成正确解决方案。

Conclusion: 论文强调了在安全关键领域（如汽车软件系统）中，需要有效的缓解技术以确保LLM生成代码的安全可靠使用。

Abstract: Large Language Models (LLMs) have shown significant potential in automating
code generation tasks offering new opportunities across software engineering
domains. However, their practical application remains limited due to
hallucinations - outputs that appear plausible but are factually incorrect,
unverifiable or nonsensical. This paper investigates hallucination phenomena in
the context of code generation with a specific focus on the automotive domain.
A case study is presented that evaluates multiple code LLMs for three different
prompting complexities ranging from a minimal one-liner prompt to a prompt with
Covesa Vehicle Signal Specifications (VSS) as additional context and finally to
a prompt with an additional code skeleton. The evaluation reveals a high
frequency of syntax violations, invalid reference errors and API knowledge
conflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the
evaluated models, only GPT-4.1 and GPT-4o were able to produce a correct
solution when given the most context-rich prompt. Simpler prompting strategies
failed to yield a working result, even after multiple refinement iterations.
These findings highlight the need for effective mitigation techniques to ensure
the safe and reliable use of LLM generated code, especially in safety-critical
domains such as automotive software systems.

</details>


### [155] [Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and Reasoning](https://arxiv.org/abs/2508.11305)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: 研究构建了全面的日志代码缺陷分类法，并评估了LLMs在检测这些缺陷时的表现。结果表明，LLMs在缺乏背景知识时表现不佳，但整合适当知识后性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 日志代码在系统调试、性能分析和监控中至关重要，但其缺陷可能导致日志失效或误读。尽管已有研究识别了一些日志缺陷模式，但缺乏系统性和全面性。此外，大型语言模型（LLMs）在代码相关任务中展现出潜力，但其在日志代码缺陷检测中的应用尚未充分探索。

Method: 研究首先通过系统分析构建了一个全面的日志代码缺陷分类法，包含7种缺陷模式和14种详细场景。随后，研究者构建了一个包含164个开发者验证的真实日志缺陷的基准数据集（\dataset），并提出了一个自动化框架，利用多种提示策略和上下文信息来评估LLMs在检测和推理日志代码缺陷方面的能力。

Result: 实验结果显示，LLMs在仅基于源代码的情况下普遍难以准确检测和推理日志代码缺陷。然而，通过整合适当的背景知识（如缺陷模式的详细场景），检测准确性可提升10.9%。

Conclusion: 研究结果表明，尽管大型语言模型（LLMs）在仅基于源代码的情况下难以准确检测和推理日志代码缺陷，但通过整合适当的背景知识（如缺陷模式的详细场景）可以显著提升检测准确性（10.9%）。这为实践者提供了避免常见缺陷模式的可操作指导，并为改进基于LLM的日志代码缺陷检测推理奠定了基础。

Abstract: Logging code is written by developers to capture system runtime behavior and
plays a vital role in debugging, performance analysis, and system monitoring.
However, defects in logging code can undermine the usefulness of logs and lead
to misinterpretations. Although prior work has identified several logging
defect patterns and provided valuable insights into logging practices, these
studies often focus on a narrow range of defect patterns derived from limited
sources (e.g., commit histories) and lack a systematic and comprehensive
analysis. Moreover, large language models (LLMs) have demonstrated promising
generalization and reasoning capabilities across a variety of code-related
tasks, yet their potential for detecting logging code defects remains largely
unexplored.
  In this paper, we derive a comprehensive taxonomy of logging code defects,
which encompasses seven logging code defect patterns with 14 detailed
scenarios. We further construct a benchmark dataset, \dataset, consisting of
164 developer-verified real-world logging defects. Then we propose an automated
framework that leverages various prompting strategies and contextual
information to evaluate LLMs' capability in detecting and reasoning logging
code defects. Experimental results reveal that LLMs generally struggle to
accurately detect and reason logging code defects based on the source code
only. However, incorporating proper knowledge (e.g., detailed scenarios of
defect patterns) can lead to 10.9\% improvement in detection accuracy. Overall,
our findings provide actionable guidance for practitioners to avoid common
defect patterns and establish a foundation for improving LLM-based reasoning in
logging code defect detection.

</details>


### [156] [TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation](https://arxiv.org/abs/2508.11468)
*Zhihao Gong,Zeyu Sun,Dong Huang,Qingyuan Liang,Jie M. Zhang,Dan Hao*

Main category: cs.SE

TL;DR: TRACY是首个评估LLM翻译代码执行效率的基准，揭示即使顶级模型在效率方面表现不佳，强调未来需同时优化正确性和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）显著提高了代码翻译的正确性，但执行效率这一关键维度仍被忽视。

Method: 通过LLM驱动的两阶段流程构建TRACY基准：首先生成一套压力测试以放大性能差异，随后进行效率导向的任务修剪阶段，以隔离区分效率的任务。

Result: TRACY包含1,011个跨C++、Java和Python的代码翻译任务，每个任务平均有22.1个已验证的参考翻译和10个计算密集型测试。评估显示，即使是顶级LLM也难以持续生成高效的代码翻译，算法缺陷和资源处理不当是最有害的问题。

Conclusion: 本文强调了在未来基于LLM的代码翻译中，同时优化正确性和效率的必要性。

Abstract: Automatic code translation is a fundamental task in modern software
development. While the advent of Large Language Models (LLMs) has significantly
improved the correctness of code translation, the critical dimension of
execution efficiency remains overlooked. To address this gap, we introduce
TRACY, the first comprehensive benchmark designed to evaluate the execution
efficiency of LLM-translated code. TRACY is constructed through an LLM-driven
two-stage pipeline: an initial stage generates a suite of stress tests to
amplify performance differences, followed by an efficiency-oriented task
pruning stage that isolates the efficiency-distinguishing tasks. The resulting
benchmark comprises 1,011 code translation tasks across C++, Java, and Python,
each accompanied by an average of 22.1 verified reference translations and 10
computationally demanding tests. Our extensive evaluation of 26 representative
LLMs reveals that even top-tier LLMs struggle to consistently produce efficient
code translations. For instance, Claude-4-think, the leading model for
correctness, ranks eighth overall when time efficiency is taken into account,
surpassed by several smaller open-source models. We further pinpoint that
algorithmic flaws and improper resource handling are the most detrimental,
causing a median time slowdown of 5.6$\times$ and memory increase of
12.0$\times$, respectively. Our work underscores the necessity of jointly
optimizing for correctness and efficiency in future LLM-based code translation.

</details>


### [157] [Temporal Network Analysis of Microservice Architectural Degradation](https://arxiv.org/abs/2508.11571)
*Alexander Bakhtin*

Main category: cs.SE

TL;DR: 本文探讨了从微服务系统获取时间网络并分析的挑战，受限于数据规模（7个时间实例和42个微服务），部分分析方法无法应用。


<details>
  <summary>Details</summary>
Motivation: 微服务架构可以建模为微服务之间相互调用的网络，即服务依赖图。网络科学提供了研究此类网络的方法，尤其是时间网络分析可以分析随时间演变的网络。

Method: 采用时间网络分析方法，研究微服务架构中的服务依赖图。

Result: 获取的最完整时间网络包含7个时间实例和42个微服务，这限制了潜在的分析方法。

Conclusion: 本文讨论了从微服务系统中获取时间网络并利用时间网络方法分析这些网络的挑战，指出当前获取的最完整时间网络仅包含7个时间实例和42个微服务，限制了可应用的分析方法。

Abstract: Microservice architecture can be modeled as a network of microservices making
calls to each other, commonly known as the service dependency graph. Network
Science can provide methods to study such networks. In particular, temporal
network analysis is a branch of Network Science that analyzes networks evolving
with time. In microservice systems, temporal networks can arise if we examine
the architecture of the system across releases or monitor a deployed system
using tracing.
  In this research summary paper, I discuss the challenges in obtaining
temporal networks from microservice systems and analyzing them with the
temporal network methods. In particular, the most complete temporal network
that we could obtain contains 7 time instances and 42 microservices, which
limits the potential analysis that could be applied.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [158] [CrossTrace: Efficient Cross-Thread and Cross-Service Span Correlation in Distributed Tracing for Microservices](https://arxiv.org/abs/2508.11342)
*Linh-An Phan,MingXue Wang,Guangyu Wu,Wang Dawei,Chen Liqun,Li Jin*

Main category: cs.NI

TL;DR: CrossTrace 是一种无需修改代码的分布式追踪方案，通过贪婪算法和 eBPF 技术高效解决跨度关联问题，适用于微服务调试。


<details>
  <summary>Details</summary>
Motivation: 分布式追踪在微服务应用中至关重要，但现有零代码方案在跨度关联上面临性能、安全性和计算开销的挑战。

Method: CrossTrace 采用贪婪算法从延迟模式推断服务内跨度关系，并通过 eBPF 在 TCP 包头中嵌入跨度标识符以实现服务间关联。

Result: 评估显示，CrossTrace 能在几秒内以超过 90% 的准确率关联数千个跨度，适合生产环境部署。

Conclusion: CrossTrace 是一种实用且高效的分布式追踪解决方案，适用于无需修改源代码的微服务应用调试，通过创新的贪婪算法和 eBPF 技术实现了高准确性和安全性。

Abstract: Distributed tracing has become an essential technique for debugging and
troubleshooting modern microservice-based applications, enabling software
engineers to detect performance bottlenecks, identify failures, and gain
insights into system behavior. However, implementing distributed tracing in
large-scale applications remains challenging due to the need for extensive
instrumentation. To reduce this burden, zero-code instrumentation solutions,
such as those based on eBPF, have emerged, allowing span data to be collected
without modifying application code. Despite this promise, span correlation, the
process of establishing causal relationships between spans, remains a critical
challenge in zero-code approaches. Existing solutions often rely on thread
affinity, compromise system security by requiring the kernel integrity mode to
be disabled, or incur significant computational overhead due to complex
inference algorithms. This paper presents CrossTrace, a practical and efficient
distributed tracing solution designed to support the debugging of microservice
applications without requiring source code modifications. CrossTrace employs a
greedy algorithm to infer intra-service span relationships from delay patterns,
eliminating reliance on thread identifiers. For inter-service correlation,
CrossTrace embeds span identifiers into TCP packet headers via eBPF, enabling
secure and efficient correlation compromising system security policies.
Evaluation results show that CrossTrace can correlate thousands of spans within
seconds with over 90% accuracy, making it suitable for production deployment
and valuable for microservice observability and diagnosis.

</details>


### [159] [Optimizing ROS 2 Communication for Wireless Robotic Systems](https://arxiv.org/abs/2508.11366)
*Sanghoon Lee,Taehun Kim,Jiyeong Chae,Kyung-Joon Park*

Main category: cs.NI

TL;DR: 论文分析了ROS 2的DDS在无线环境下传输大负载的性能问题，提出了无需修改协议的优化框架，显著提升了传输效率。


<details>
  <summary>Details</summary>
Motivation: ROS 2作为领先的开源机器人中间件，其默认的DDS通信栈在无线环境下传输大负载时性能显著下降，但原因尚未被深入探索。

Method: 通过深入分析ROS 2的DDS栈在网络层的表现，识别了IP碎片化、重传时机和缓冲区拥塞三个关键问题，并提出了基于链路和负载特性的通信参数优化框架。

Result: 在各种无线场景下的实验表明，该框架在现有DDS模式失败的情况下成功传输了大负载，同时保持了低端到端延迟。

Conclusion: 提出的轻量级DDS优化框架在无需修改协议或增加组件的情况下，显著提升了ROS 2在无线环境下传输大负载的性能，保持了低延迟。

Abstract: Wireless transmission of large payloads, such as high-resolution images and
LiDAR point clouds, is a major bottleneck in ROS 2, the leading open-source
robotics middleware. The default Data Distribution Service (DDS) communication
stack in ROS 2 exhibits significant performance degradation over lossy wireless
links. Despite the widespread use of ROS 2, the underlying causes of these
wireless communication challenges remain unexplored. In this paper, we present
the first in-depth network-layer analysis of ROS 2's DDS stack under wireless
conditions with large payloads. We identify the following three key issues:
excessive IP fragmentation, inefficient retransmission timing, and congestive
buffer bursts. To address these issues, we propose a lightweight and fully
compatible DDS optimization framework that tunes communication parameters based
on link and payload characteristics. Our solution can be seamlessly applied
through the standard ROS 2 application interface via simple XML-based QoS
configuration, requiring no protocol modifications, no additional components,
and virtually no integration efforts. Extensive experiments across various
wireless scenarios demonstrate that our framework successfully delivers large
payloads in conditions where existing DDS modes fail, while maintaining low
end-to-end latency.

</details>


### [160] [D2Q Synchronizer: Distributed SDN Synchronization for Time Sensitive Applications](https://arxiv.org/abs/2508.11475)
*Ioannis Panitsas,Akrit Mudvari,Leandros Tassiulas*

Main category: cs.NI

TL;DR: D2Q Synchronizer是一种基于强化学习的算法，用于优化分布式SDN中的任务卸载，显著降低网络成本并满足QoS要求。


<details>
  <summary>Details</summary>
Motivation: 分布式SDN控制器需要同步以维护全局网络状态，但现有同步策略大多未考虑网络和用户性能的联合优化。

Method: 提出了一种基于强化学习的算法D2Q Synchronizer，通过策略性地将时间敏感任务卸载到成本效益高的边缘服务器来最小化长期网络成本。

Result: 评估结果表明，D2Q Synchronizer在减少网络成本方面优于启发式和其他学习策略，分别降低了至少45%和10%的成本。

Conclusion: D2Q Synchronizer通过强化学习算法，在动态和多域SDN网络中显著降低了网络成本，同时确保了所有用户任务的QoS要求。

Abstract: In distributed Software-Defined Networking (SDN), distributed SDN controllers
require synchronization to maintain a global network state. Despite the
availability of synchronization policies for distributed SDN architectures,
most policies do not consider joint optimization of network and user
performance. In this work, we propose a reinforcement learning-based algorithm
called D2Q Synchronizer, to minimize long-term network costs by strategically
offloading time-sensitive tasks to cost-effective edge servers while satisfying
the latency requirements for all tasks. Evaluation results demonstrate the
superiority of our synchronizer compared to heuristic and other learning
policies in literature, by reducing network costs by at least 45% and 10%,
respectively, while ensuring the QoS requirements for all user tasks across
dynamic and multi-domain SDN networks.

</details>


### [161] [Intelligent Edge Resource Provisioning for Scalable Digital Twins of Autonomous Vehicles](https://arxiv.org/abs/2508.11574)
*Mohammad Sajid Shahriar,Suresh Subramaniam,Motoharu Matsuura,Hiroshi Hasegawa,Shih-Chun Lin*

Main category: cs.NI

TL;DR: 论文提出了一种集成数字孪生和移动边缘计算的分布式架构，通过智能任务配置算法提升交通服务的低延迟和稳健性。


<details>
  <summary>Details</summary>
Motivation: 下一代网络为智能交通系统（ITS）提供了巨大潜力，尤其是通过数字孪生（DTs）的集成。然而，如何通过高效的计算资源管理确保DTs的无间断运行仍是一个开放挑战。

Method: 开发了一种网络感知的可扩展协作任务配置算法，用于训练自主代理，并在现实的联网自动驾驶车辆（CAV）交通模拟中进行评估。

Result: 提出的框架显著增强了DT操作的稳健性和可扩展性，同步错误降低至5%，边缘计算资源利用率高达99.5%。

Conclusion: 该论文提出的分布式计算架构显著提升了数字孪生操作的稳健性和可扩展性，同步错误降低至5%，边缘计算资源利用率高达99.5%。

Abstract: The next generation networks offers significant potential to advance
Intelligent Transportation Systems (ITS), particularly through the integration
of Digital Twins (DTs). However, ensuring the uninterrupted operation of DTs
through efficient computing resource management remains an open challenge. This
paper introduces a distributed computing archi tecture that integrates DTs and
Mobile Edge Computing (MEC) within a software-defined vehicular networking
framework to enable intelligent, low-latency transportation services. A network
aware scalable collaborative task provisioning algorithm is de veloped to train
an autonomous agent, which is evaluated using a realistic connected autonomous
vehicle (CAV) traffic simulation. The proposed framework significantly enhances
the robustness and scalability of DT operations by reducing synchronization
errors to as low as 5% while achieving up to 99.5% utilization of edge
computing resources.

</details>
