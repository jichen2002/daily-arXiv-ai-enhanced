{"id": "2508.11035", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11035", "abs": "https://arxiv.org/abs/2508.11035", "authors": ["Hasibul Jamil", "MD S Q Zulkar Nine", "Tevfik Kosar"], "title": "EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI Training", "comment": "SC25 Sustainable Supercomputing Workshop", "summary": "Large-scale deep learning workloads increasingly suffer from I/O bottlenecks\nas datasets grow beyond local storage capacities and GPU compute outpaces\nnetwork and disk latencies. While recent systems optimize data-loading time,\nthey overlook the energy cost of I/O - a critical factor at large scale. We\nintroduce EMLIO, an Efficient Machine Learning I/O service that jointly\nminimizes end-to-end data-loading latency T and I/O energy consumption E across\nvariable-latency networked storage. EMLIO deploys a lightweight data-serving\ndaemon on storage nodes that serializes and batches raw samples, streams them\nover TCP with out-of-order prefetching, and integrates seamlessly with\nGPU-accelerated (NVIDIA DALI) preprocessing on the client side. In exhaustive\nevaluations over local disk, LAN (0.05 ms & 10 ms RTT), and WAN (30 ms RTT)\nenvironments, EMLIO delivers up to 8.6X faster I/O and 10.9X lower energy use\ncompared to state-of-the-art loaders, while maintaining constant performance\nand energy profiles irrespective of network distance. EMLIO's service-based\narchitecture offers a scalable blueprint for energy-aware I/O in\nnext-generation AI clouds.", "AI": {"tldr": "EMLIO\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u673a\u5668\u5b66\u4e60I/O\u670d\u52a1\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u52a0\u8f7d\u5ef6\u8fdf\u548c\u80fd\u6e90\u6d88\u8017\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u548c\u80fd\u6548\u3002", "motivation": "\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u56e0\u6570\u636e\u96c6\u8d85\u51fa\u672c\u5730\u5b58\u50a8\u5bb9\u91cf\u548cGPU\u8ba1\u7b97\u8d85\u8d8a\u7f51\u7edc\u53ca\u78c1\u76d8\u5ef6\u8fdf\u800c\u9762\u4e34I/O\u74f6\u9888\uff0c\u73b0\u6709\u7cfb\u7edf\u5ffd\u7565\u4e86I/O\u7684\u80fd\u6e90\u6210\u672c\u3002", "method": "EMLIO\u5728\u5b58\u50a8\u8282\u70b9\u4e0a\u90e8\u7f72\u8f7b\u91cf\u7ea7\u6570\u636e\u670d\u52a1\u5b88\u62a4\u8fdb\u7a0b\uff0c\u901a\u8fc7TCP\u6d41\u5f0f\u4f20\u8f93\u548c\u4e71\u5e8f\u9884\u53d6\u6280\u672f\uff0c\u4e0e\u5ba2\u6237\u7aef\u7684GPU\u52a0\u901f\u9884\u5904\u7406\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u672c\u5730\u78c1\u76d8\u3001LAN\u548cWAN\u73af\u5883\u4e0b\uff0cEMLIO\u6bd4\u73b0\u6709\u6700\u4f18\u52a0\u8f7d\u5668\u5feb8.6\u500d\uff0c\u80fd\u6e90\u6d88\u8017\u964d\u4f4e10.9\u500d\uff0c\u4e14\u6027\u80fd\u548c\u80fd\u6e90\u6d88\u8017\u4e0d\u53d7\u7f51\u7edc\u8ddd\u79bb\u5f71\u54cd\u3002", "conclusion": "EMLIO\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u80fd\u6e90\u611f\u77e5I/O\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u4e0b\u4e00\u4ee3AI\u4e91\u8ba1\u7b97\u73af\u5883\u3002"}}
{"id": "2508.11266", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11266", "abs": "https://arxiv.org/abs/2508.11266", "authors": ["Ailiya Borjigin", "Cong He", "Charles CC Lee", "Wei Zhou"], "title": "Element and Everything Tokens: Two-Tier Architecture for Mobilizing Alternative Assets", "comment": "8 Pages, Submitted to RASSE 2025", "summary": "Alternative assets such as mines, power plants, or infrastructure projects\nare often large, heterogeneous bundles of resources, rights, and outputs whose\nvalue is difficult to trade or fractionalize under traditional frameworks. This\npaper proposes a novel two-tier tokenization architecture to enhance the\nliquidity and transparency of such complex assets. We introduce the concepts of\nElement Tokens and Everything Tokens: elemental tokens represent standardized,\nfully collateralized components of an asset (e.g., outputs, rights, or\ncredits), while an everything token represents the entire asset as a fixed\ncombination of those elements. The architecture enables both fine-grained\npartial ownership and integrated whole-asset ownership through a system of\ntwo-way convertibility. We detail the design and mechanics of this system,\nincluding an arbitrage mechanism that keeps the price of the composite token\naligned with the net asset value of its constituents. Through illustrative\nexamples in the energy and industrial sectors, we demonstrate that our approach\nallows previously illiquid, high-value projects to be fractionalized and traded\nakin to stocks or exchange-traded funds (ETFs). We discuss the benefits for\ninvestors and asset owners, such as lower entry barriers, improved price\ndiscovery, and flexible financing, as well as the considerations for\nimplementation and regulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u5c42\u4ee3\u5e01\u5316\u67b6\u6784\uff0c\u901a\u8fc7Element Tokens\u548cEverything Tokens\u589e\u5f3a\u590d\u6742\u8d44\u4ea7\u7684\u6d41\u52a8\u6027\u548c\u900f\u660e\u5ea6\uff0c\u5b9e\u73b0\u9ad8\u4ef7\u503c\u9879\u76ee\u7684\u5206\u5272\u548c\u4ea4\u6613\u3002", "motivation": "\u4f20\u7edf\u6846\u67b6\u4e0b\uff0c\u5927\u578b\u3001\u5f02\u6784\u7684\u8d44\u4ea7\uff08\u5982\u77ff\u5c71\u3001\u53d1\u7535\u5382\u6216\u57fa\u7840\u8bbe\u65bd\u9879\u76ee\uff09\u96be\u4ee5\u4ea4\u6613\u6216\u5206\u5272\uff0c\u5bfc\u81f4\u6d41\u52a8\u6027\u4e0d\u8db3\u548c\u900f\u660e\u5ea6\u4f4e\u3002", "method": "\u5f15\u5165Element Tokens\u548cEverything Tokens\u7684\u6982\u5ff5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53cc\u5411\u53ef\u8f6c\u6362\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u5957\u5229\u673a\u5236\u4fdd\u6301\u7ec4\u5408\u4ee3\u5e01\u4ef7\u683c\u4e0e\u51c0\u8d44\u4ea7\u4ef7\u503c\u7684\u5bf9\u9f50\u3002", "result": "\u901a\u8fc7\u80fd\u6e90\u548c\u5de5\u4e1a\u9886\u57df\u7684\u793a\u4f8b\uff0c\u8bc1\u660e\u4e86\u8be5\u67b6\u6784\u80fd\u591f\u5c06\u9ad8\u4ef7\u503c\u9879\u76ee\u5206\u5272\u5e76\u50cf\u80a1\u7968\u6216ETF\u4e00\u6837\u4ea4\u6613\uff0c\u4ece\u800c\u964d\u4f4e\u8fdb\u5165\u95e8\u69db\u3001\u63d0\u9ad8\u4ef7\u683c\u53d1\u73b0\u548c\u7075\u6d3b\u878d\u8d44\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u5c42\u4ee3\u5e01\u5316\u67b6\u6784\uff0c\u901a\u8fc7Element Tokens\u548cEverything Tokens\u7684\u6982\u5ff5\uff0c\u589e\u5f3a\u4e86\u590d\u6742\u8d44\u4ea7\u7684\u6d41\u52a8\u6027\u548c\u900f\u660e\u5ea6\uff0c\u4e3a\u6295\u8d44\u8005\u548c\u8d44\u4ea7\u6240\u6709\u8005\u5e26\u6765\u4e86\u8bf8\u591a\u597d\u5904\u3002"}}
{"id": "2508.11298", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11298", "abs": "https://arxiv.org/abs/2508.11298", "authors": ["Gabin Schieffer", "Jacob Wahlgren", "Ruimin Shi", "Edgar A. Le\u00f3n", "Roger Pearce", "Maya Gokhale", "Ivy Peng"], "title": "Inter-APU Communication on AMD MI300A Systems via Infinity Fabric: a Deep Dive", "comment": null, "summary": "The ever-increasing compute performance of GPU accelerators drives up the\nneed for efficient data movements within HPC applications to sustain\nperformance. Proposed as a solution to alleviate CPU-GPU data movement, AMD\nMI300A Accelerated Processing Unit (APU) combines CPU, GPU, and high-bandwidth\nmemory (HBM) within a single physical package. Leadership supercomputers, such\nas El Capitan, group four APUs within a single compute node, using Infinity\nFabric Interconnect. In this work, we design specific benchmarks to evaluate\ndirect memory access from the GPU, explicit inter-APU data movement, and\ncollective multi-APU communication. We also compare the efficiency of HIP APIs,\nMPI routines, and the GPU-specialized RCCL library. Our results highlight key\ndesign choices for optimizing inter-APU communication on multi-APU AMD MI300A\nsystems with Infinity Fabric, including programming interfaces, allocators, and\ndata movement. Finally, we optimize two real HPC applications, Quicksilver and\nCloverLeaf, and evaluate them on a four MI100A APU system.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86AMD MI300A APU\u5728\u591aAPU\u7cfb\u7edf\u4e2d\u7684\u901a\u4fe1\u6548\u7387\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u5e94\u7528\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740GPU\u52a0\u901f\u5668\u8ba1\u7b97\u6027\u80fd\u7684\u4e0d\u65ad\u63d0\u5347\uff0cHPC\u5e94\u7528\u7a0b\u5e8f\u9700\u8981\u9ad8\u6548\u7684\u6570\u636e\u79fb\u52a8\u4ee5\u7ef4\u6301\u6027\u80fd\u3002AMD MI300A APU\u5c06CPU\u3001GPU\u548c\u9ad8\u5e26\u5bbd\u5185\u5b58\uff08HBM\uff09\u96c6\u6210\u5728\u4e00\u4e2a\u7269\u7406\u5c01\u88c5\u4e2d\uff0c\u4e3a\u89e3\u51b3CPU-GPU\u6570\u636e\u79fb\u52a8\u95ee\u9898\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u7279\u5b9a\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30GPU\u7684\u76f4\u63a5\u5185\u5b58\u8bbf\u95ee\u3001\u663e\u5f0f\u7684APU\u95f4\u6570\u636e\u79fb\u52a8\u4ee5\u53ca\u591aAPU\u96c6\u4f53\u901a\u4fe1\u3002\u6bd4\u8f83\u4e86HIP API\u3001MPI\u4f8b\u7a0b\u548cGPU\u4e13\u7528RCCL\u5e93\u7684\u6548\u7387\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u5728\u591aAPU AMD MI300A\u7cfb\u7edf\u4e2d\u4f18\u5316APU\u95f4\u901a\u4fe1\u7684\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u3002\u901a\u8fc7\u4f18\u5316\u4e24\u4e2a\u5b9e\u9645HPC\u5e94\u7528\u7a0b\u5e8f\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u9009\u62e9\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u591aAPU AMD MI300A\u7cfb\u7edf\u4e2d\u4f18\u5316APU\u95f4\u901a\u4fe1\u7684\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff0c\u5305\u62ec\u7f16\u7a0b\u63a5\u53e3\u3001\u5206\u914d\u5668\u548c\u6570\u636e\u79fb\u52a8\u3002\u901a\u8fc7\u4f18\u5316\u4e24\u4e2a\u5b9e\u9645HPC\u5e94\u7528\u7a0b\u5e8f\uff08Quicksilver\u548cCloverLeaf\uff09\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u9009\u62e9\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11384", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11384", "abs": "https://arxiv.org/abs/2508.11384", "authors": ["Joel Rybicki", "Jakob Solnerzik", "Olivier Stietel", "Robin Vacus"], "title": "Space-efficient population protocols for exact majority in general graphs", "comment": null, "summary": "We study exact majority consensus in the population protocol model. In this\nmodel, the system is described by a graph $G = (V,E)$ with $n$ nodes, and in\neach time step, a scheduler samples uniformly at random a pair of adjacent\nnodes to interact. In the exact majority consensus task, each node is given a\nbinary input, and the goal is to design a protocol that almost surely reaches a\nstable configuration, where all nodes output the majority input value.\n  We give improved upper and lower bounds for the exact majority in general\ngraphs. First, we give asymptotically tight time lower bounds for general\n(unbounded space) protocols. Second, we obtain new upper bounds parameterized\nby the relaxation time $\\tau_{\\mathsf{rel}}$ of the random walk on $G$ induced\nby the scheduler and the degree imbalance $\\Delta/\\delta$ of $G$. Specifically,\nwe give a protocol that stabilizes in $O\\left( \\tfrac{\\Delta}{\\delta}\n\\tau_{\\mathsf{rel}} \\log^2 n \\right)$ steps in expectation and with high\nprobability and uses $O\\left( \\log n \\cdot \\left(\n\\log\\left(\\tfrac{\\Delta}{\\delta}\\right) + \\log\n\\left(\\tfrac{\\tau_{\\mathsf{rel}}}{n}\\right) \\right) \\right)$ states in any\ngraph with minimum degree at least $\\delta$ and maximum degree at most\n$\\Delta$.\n  For regular expander graphs, this matches the optimal space complexity of\n$\\Theta(\\log n)$ for fast protocols in complete graphs [Alistarh et al., SODA\n2016 and Doty et al., FOCS 2022] with a nearly optimal stabilization time of\n$O(n \\log^2 n)$ steps. Finally, we give a new upper bound of\n$O(\\tau_{\\mathsf{rel}} \\cdot n \\log n)$ for the stabilization time of a\nconstant-state protocol.", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86\u7cbe\u786e\u591a\u6570\u5171\u8bc6\u5728\u4e00\u822c\u56fe\u4e2d\u7684\u4e0a\u4e0b\u754c\uff0c\u63d0\u4f9b\u4e86\u53c2\u6570\u5316\u4e0a\u754c\u548c\u72b6\u6001\u590d\u6742\u5ea6\u5206\u6790\uff0c\u5728\u89c4\u5219\u6269\u5c55\u56fe\u4e2d\u5339\u914d\u6700\u4f18\u7a7a\u95f4\u590d\u6742\u5ea6\u3002", "motivation": "\u7814\u7a76\u7cbe\u786e\u591a\u6570\u5171\u8bc6\u5728\u4e00\u822c\u56fe\u4e2d\u7684\u6027\u80fd\u8fb9\u754c\uff0c\u4ee5\u6539\u8fdb\u73b0\u6709\u534f\u8bae\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u57fa\u4e8e\u56fe\u7684\u968f\u673a\u6e38\u8d70\u677e\u5f1b\u65f6\u95f4\u548c\u5ea6\u4e0d\u5e73\u8861\u53c2\u6570\u5316\u7684\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u534f\u8bae\uff0c\u8be5\u534f\u8bae\u5728\u671f\u671b\u548c\u9ad8\u6982\u7387\u4e0b\u7a33\u5b9a\uff0c\u5e76\u5206\u6790\u4e86\u5176\u72b6\u6001\u590d\u6742\u5ea6\u3002", "result": "\u63d0\u51fa\u4e86\u65b0\u7684\u4e0a\u754c\u548c\u4e0b\u754c\uff0c\u7279\u522b\u662f\u5728\u89c4\u5219\u6269\u5c55\u56fe\u4e2d\u5c55\u793a\u4e86\u4e0e\u6700\u4f18\u7a7a\u95f4\u590d\u6742\u5ea6\u7684\u5339\u914d\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6052\u5b9a\u72b6\u6001\u534f\u8bae\u7684\u65b0\u4e0a\u754c\u3002", "conclusion": "\u672c\u6587\u6539\u8fdb\u4e86\u5bf9\u4e00\u822c\u56fe\u4e2d\u7cbe\u786e\u591a\u6570\u5171\u8bc6\u95ee\u9898\u7684\u4e0a\u4e0b\u754c\u5206\u6790\uff0c\u63d0\u4f9b\u4e86\u65b0\u7684\u53c2\u6570\u5316\u4e0a\u754c\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u89c4\u5219\u6269\u5c55\u56fe\u4e2d\u4e0e\u6700\u4f18\u7a7a\u95f4\u590d\u6742\u5ea6\u7684\u5339\u914d\u3002"}}
{"id": "2508.11006", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.11006", "abs": "https://arxiv.org/abs/2508.11006", "authors": ["Umesh Biswas", "Maxwell Young"], "title": "A Gentle Wakeup Call: Symmetry Breaking with Less Collision Cost", "comment": null, "summary": "The wakeup problem addresses the fundamental challenge of symmetry breaking.\nThere are $n$ devices sharing a time-slotted multiple access channel. In any\nfixed slot, if a single device sends a packet, it succeeds; however, if two or\nmore devices send, then there is a collision and none of the corresponding\npackets succeed. For the static version of wakeup, all packets are initially\nactive (i.e., can send and listen on the channel); for the dynamic version, the\npackets become active at arbitrary times. In both versions, the goal is to\nsuccessfully send a single packet.\n  Prior results on wakeup have largely focused on the number of slots until the\nfirst success; that is, the latency. However, in many modern systems,\ncollisions introduce significant delay, an aspect that current wakeup\nalgorithms do not address. For instance, while existing results for static\nwakeup have polylogarithmic-in-$n$ latency, they can incur additional latency\nthat is {\\it linear} in the cost of a collision $C$. Thus, the total latency is\nlarge and dominated by the contributions from collisions.\n  Here, we design and analyze a randomized wakeup algorithm, Aim-High. For\nsufficiently large $C$ and with bounded error, Aim-High has latency and\nexpected collision cost that is nearly $O(\\sqrt{C})$ for both the static and\ndynamic versions. Otherwise, the latency and expected collision cost are\n$O(\\texttt{poly}{(\\log n)})$ for the static setting, and\n$O(n\\,\\texttt{poly}{(\\log n)})$ for the dynamic setting. We also establish\nlower bounds that complement these results.", "AI": {"tldr": "Aim-High\u7b97\u6cd5\u4f18\u5316\u4e86\u5524\u9192\u95ee\u9898\u4e2d\u7684\u5ef6\u8fdf\u548c\u78b0\u649e\u6210\u672c\uff0c\u9002\u7528\u4e8e\u9759\u6001\u548c\u52a8\u6001\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u5524\u9192\u7b97\u6cd5\u5728\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5ffd\u89c6\u4e86\u78b0\u649e\u5e26\u6765\u7684\u663e\u8457\u5ef6\u8fdf\uff0c\u5bfc\u81f4\u603b\u5ef6\u8fdf\u4e3b\u8981\u7531\u78b0\u649e\u8d21\u732e\u3002", "method": "\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u4e00\u79cd\u968f\u673a\u5524\u9192\u7b97\u6cd5Aim-High\uff0c\u9002\u7528\u4e8e\u9759\u6001\u548c\u52a8\u6001\u7248\u672c\u7684\u5524\u9192\u95ee\u9898\u3002", "result": "Aim-High\u7b97\u6cd5\u5728\u8db3\u591f\u5927\u7684$C$\u548c\u6709\u9650\u9519\u8bef\u4e0b\uff0c\u5b9e\u73b0\u4e86\u8fd1$O(\\sqrt{C})$\u7684\u5ef6\u8fdf\u548c\u78b0\u649e\u6210\u672c\uff1b\u5426\u5219\u5728\u9759\u6001\u548c\u52a8\u6001\u8bbe\u7f6e\u4e0b\u5206\u522b\u8fbe\u5230$O(\\texttt{poly}{(\\log n)})$\u548c$O(n\\,\\texttt{poly}{(\\log n)})$\u3002", "conclusion": "\u672c\u7814\u7a76\u8bbe\u8ba1\u7684Aim-High\u7b97\u6cd5\u5728\u9759\u6001\u548c\u52a8\u6001\u7248\u672c\u7684\u5524\u9192\u95ee\u9898\u4e2d\uff0c\u5747\u80fd\u5b9e\u73b0\u8fd1$O(\\sqrt{C})$\u7684\u5ef6\u8fdf\u548c\u78b0\u649e\u6210\u672c\uff0c\u6216\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8fbe\u5230$O(\\texttt{poly}{(\\log n)})$\u6216$O(n\\,\\texttt{poly}{(\\log n)})$\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11342", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11342", "abs": "https://arxiv.org/abs/2508.11342", "authors": ["Linh-An Phan", "MingXue Wang", "Guangyu Wu", "Wang Dawei", "Chen Liqun", "Li Jin"], "title": "CrossTrace: Efficient Cross-Thread and Cross-Service Span Correlation in Distributed Tracing for Microservices", "comment": null, "summary": "Distributed tracing has become an essential technique for debugging and\ntroubleshooting modern microservice-based applications, enabling software\nengineers to detect performance bottlenecks, identify failures, and gain\ninsights into system behavior. However, implementing distributed tracing in\nlarge-scale applications remains challenging due to the need for extensive\ninstrumentation. To reduce this burden, zero-code instrumentation solutions,\nsuch as those based on eBPF, have emerged, allowing span data to be collected\nwithout modifying application code. Despite this promise, span correlation, the\nprocess of establishing causal relationships between spans, remains a critical\nchallenge in zero-code approaches. Existing solutions often rely on thread\naffinity, compromise system security by requiring the kernel integrity mode to\nbe disabled, or incur significant computational overhead due to complex\ninference algorithms. This paper presents CrossTrace, a practical and efficient\ndistributed tracing solution designed to support the debugging of microservice\napplications without requiring source code modifications. CrossTrace employs a\ngreedy algorithm to infer intra-service span relationships from delay patterns,\neliminating reliance on thread identifiers. For inter-service correlation,\nCrossTrace embeds span identifiers into TCP packet headers via eBPF, enabling\nsecure and efficient correlation compromising system security policies.\nEvaluation results show that CrossTrace can correlate thousands of spans within\nseconds with over 90% accuracy, making it suitable for production deployment\nand valuable for microservice observability and diagnosis.", "AI": {"tldr": "CrossTrace \u662f\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u4ee3\u7801\u7684\u5206\u5e03\u5f0f\u8ffd\u8e2a\u65b9\u6848\uff0c\u901a\u8fc7\u8d2a\u5a6a\u7b97\u6cd5\u548c eBPF \u6280\u672f\u9ad8\u6548\u89e3\u51b3\u8de8\u5ea6\u5173\u8054\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5fae\u670d\u52a1\u8c03\u8bd5\u3002", "motivation": "\u5206\u5e03\u5f0f\u8ffd\u8e2a\u5728\u5fae\u670d\u52a1\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u96f6\u4ee3\u7801\u65b9\u6848\u5728\u8de8\u5ea6\u5173\u8054\u4e0a\u9762\u4e34\u6027\u80fd\u3001\u5b89\u5168\u6027\u548c\u8ba1\u7b97\u5f00\u9500\u7684\u6311\u6218\u3002", "method": "CrossTrace \u91c7\u7528\u8d2a\u5a6a\u7b97\u6cd5\u4ece\u5ef6\u8fdf\u6a21\u5f0f\u63a8\u65ad\u670d\u52a1\u5185\u8de8\u5ea6\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7 eBPF \u5728 TCP \u5305\u5934\u4e2d\u5d4c\u5165\u8de8\u5ea6\u6807\u8bc6\u7b26\u4ee5\u5b9e\u73b0\u670d\u52a1\u95f4\u5173\u8054\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0cCrossTrace \u80fd\u5728\u51e0\u79d2\u5185\u4ee5\u8d85\u8fc7 90% \u7684\u51c6\u786e\u7387\u5173\u8054\u6570\u5343\u4e2a\u8de8\u5ea6\uff0c\u9002\u5408\u751f\u4ea7\u73af\u5883\u90e8\u7f72\u3002", "conclusion": "CrossTrace \u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u8ffd\u8e2a\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u65e0\u9700\u4fee\u6539\u6e90\u4ee3\u7801\u7684\u5fae\u670d\u52a1\u5e94\u7528\u8c03\u8bd5\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8d2a\u5a6a\u7b97\u6cd5\u548c eBPF \u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2508.11177", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.11177", "abs": "https://arxiv.org/abs/2508.11177", "authors": ["I-Chao Shen", "Ariel Shamir", "Takeo Igarashi"], "title": "LayoutRectifier: An Optimization-based Post-processing for Graphic Design Layout Generation", "comment": "11 pages, Pacific Graphics 2025", "summary": "Recent deep learning methods can generate diverse graphic design layouts\nefficiently. However, these methods often create layouts with flaws, such as\nmisalignment, unwanted overlaps, and unsatisfied containment. To tackle this\nissue, we propose an optimization-based method called LayoutRectifier, which\ngracefully rectifies auto-generated graphic design layouts to reduce these\nflaws while minimizing deviation from the generated layout. The core of our\nmethod is a two-stage optimization. First, we utilize grid systems, which\nprofessional designers commonly use to organize elements, to mitigate\nmisalignments through discrete search. Second, we introduce a novel box\ncontainment function designed to adjust the positions and sizes of the layout\nelements, preventing unwanted overlapping and promoting desired containment. We\nevaluate our method on content-agnostic and content-aware layout generation\ntasks and achieve better-quality layouts that are more suitable for downstream\ngraphic design tasks. Our method complements learning-based layout generation\nmethods and does not require additional training.", "AI": {"tldr": "LayoutRectifier \u662f\u4e00\u79cd\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\u51cf\u5c11\u81ea\u52a8\u751f\u6210\u5e03\u5c40\u7684\u7455\u75b5\uff0c\u63d0\u5347\u5e03\u5c40\u8d28\u91cf\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u751f\u6210\u7684\u5e03\u5c40\u5e38\u5b58\u5728\u7455\u75b5\uff08\u5982\u672a\u5bf9\u9f50\u3001\u91cd\u53e0\u3001\u4e0d\u6ee1\u8db3\u5305\u542b\u5173\u7cfb\uff09\uff0c\u5f71\u54cd\u4e86\u8bbe\u8ba1\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5 LayoutRectifier\uff0c\u5305\u62ec\u79bb\u6563\u641c\u7d22\u4ee5\u5229\u7528\u7f51\u683c\u7cfb\u7edf\u51cf\u5c11\u672a\u5bf9\u9f50\u95ee\u9898\uff0c\u4ee5\u53ca\u5f15\u5165\u65b0\u7684\u76d2\u5b50\u5305\u542b\u51fd\u6570\u6765\u8c03\u6574\u5143\u7d20\u4f4d\u7f6e\u548c\u5927\u5c0f\u4ee5\u907f\u514d\u91cd\u53e0\u548c\u4fc3\u8fdb\u5305\u542b\u3002", "result": "\u5728\u5185\u5bb9\u65e0\u5173\u548c\u5185\u5bb9\u611f\u77e5\u7684\u5e03\u5c40\u751f\u6210\u4efb\u52a1\u4e2d\uff0cLayoutRectifier \u751f\u6210\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u5e03\u5c40\uff0c\u66f4\u9002\u5408\u4e0b\u6e38\u56fe\u5f62\u8bbe\u8ba1\u4efb\u52a1\u3002", "conclusion": "LayoutRectifier \u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u81ea\u52a8\u751f\u6210\u5e03\u5c40\u4e2d\u7684\u7455\u75b5\uff0c\u5982\u672a\u5bf9\u9f50\u3001\u91cd\u53e0\u548c\u4e0d\u6ee1\u8db3\u7684\u5305\u542b\u5173\u7cfb\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u5e03\u5c40\u8d28\u91cf\u3002"}}
{"id": "2508.11034", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11034", "abs": "https://arxiv.org/abs/2508.11034", "authors": ["Antonio Collante", "Samuel Abedu", "SayedHassan Khatoonabadi", "Ahmad Abdellatif", "Ebube Alor", "Emad Shihab"], "title": "The Impact of Large Language Models (LLMs) on Code Review Process", "comment": null, "summary": "Large language models (LLMs) have recently gained prominence in the field of\nsoftware development, significantly boosting productivity and simplifying\nteamwork. Although prior studies have examined task-specific applications, the\nphase-specific effects of LLM assistance on the efficiency of code review\nprocesses remain underexplored. This research investigates the effect of GPT on\nGitHub pull request (PR) workflows, with a focus on reducing resolution time,\noptimizing phase-specific performance, and assisting developers. We curated a\ndataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted\nPRs using a semi-automated heuristic approach that combines keyword-based\ndetection, regular expression filtering, and manual verification until\nachieving 95% labeling accuracy. We then applied statistical modeling,\nincluding multiple linear regression and Mann-Whitney U test, to evaluate\ndifferences between GPT-assisted and non-assisted PRs, both at the overall\nresolution level and across distinct review phases. Our research has revealed\nthat early adoption of GPT can substantially boost the effectiveness of the PR\nprocess, leading to considerable time savings at various stages. Our findings\nsuggest that GPT-assisted PRs reduced median resolution time by more than 60%\n(9 hours compared to 23 hours for non-assisted PRs). We discovered that\nutilizing GPT can reduce the review time by 33% and the waiting time before\nacceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we\ndiscovered that developers predominantly use GPT for code optimization (60%),\nbug fixing (26%), and documentation updates (12%). This research sheds light on\nthe impact of the GPT model on the code review process, offering actionable\ninsights for software teams seeking to enhance workflows and promote seamless\ncollaboration.", "AI": {"tldr": "GPT\u8f85\u52a9\u663e\u8457\u63d0\u5347GitHub PR\u6d41\u7a0b\u6548\u7387\uff0c\u51cf\u5c11\u89e3\u51b3\u65f6\u95f460%\uff0c\u5ba1\u67e5\u65f6\u95f433%\uff0c\u7b49\u5f85\u65f6\u95f487%\uff0c\u4e3b\u8981\u7528\u4e8e\u4ee3\u7801\u4f18\u5316\u3001\u9519\u8bef\u4fee\u590d\u548c\u6587\u6863\u66f4\u65b0\u3002", "motivation": "\u5c3d\u7ba1\u5148\u524d\u7814\u7a76\u63a2\u8ba8\u4e86\u4efb\u52a1\u7279\u5b9a\u5e94\u7528\uff0c\u4f46LLM\u8f85\u52a9\u5bf9\u4ee3\u7801\u5ba1\u67e5\u6d41\u7a0b\u5404\u9636\u6bb5\u6548\u7387\u7684\u5f71\u54cd\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u534a\u81ea\u52a8\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08\u7ed3\u5408\u5173\u952e\u8bcd\u68c0\u6d4b\u3001\u6b63\u5219\u8868\u8fbe\u5f0f\u8fc7\u6ee4\u548c\u624b\u52a8\u9a8c\u8bc1\uff09\u8bc6\u522bGPT\u8f85\u52a9\u7684PR\uff0c\u5e76\u5e94\u7528\u7edf\u8ba1\u5efa\u6a21\uff08\u5305\u62ec\u591a\u5143\u7ebf\u6027\u56de\u5f52\u548cMann-Whitney U\u68c0\u9a8c\uff09\u8bc4\u4f30\u5dee\u5f02\u3002", "result": "GPT\u8f85\u52a9\u7684PR\u4e2d\u4f4d\u6570\u89e3\u51b3\u65f6\u95f4\u51cf\u5c11\u4e8660%\uff089\u5c0f\u65f6 vs 23\u5c0f\u65f6\uff09\uff0c\u5ba1\u67e5\u65f6\u95f4\u51cf\u5c1133%\uff0c\u7b49\u5f85\u63a5\u53d7\u65f6\u95f4\u51cf\u5c1187%\u3002\u5f00\u53d1\u8005\u4e3b\u8981\u5c06GPT\u7528\u4e8e\u4ee3\u7801\u4f18\u5316\uff0860%\uff09\u3001\u9519\u8bef\u4fee\u590d\uff0826%\uff09\u548c\u6587\u6863\u66f4\u65b0\uff0812%\uff09\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86GPT\u5728\u4ee3\u7801\u5ba1\u67e5\u8fc7\u7a0b\u4e2d\u7684\u663e\u8457\u5f71\u54cd\uff0c\u4e3a\u8f6f\u4ef6\u56e2\u961f\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u4ee5\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b\u548c\u4fc3\u8fdb\u534f\u4f5c\u3002"}}
{"id": "2508.10973", "categories": ["cs.RO", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.10973", "abs": "https://arxiv.org/abs/2508.10973", "authors": ["Hongchen Wang", "Sima Zeinali Danalou", "Jiahao Zhu", "Kenneth Sulimro", "Chaewon Lim", "Smita Basak", "Aimee Tai", "Usan Siriwardana", "Jason Hattrick-Simpers", "Jay Werber"], "title": "Developing and Validating a High-Throughput Robotic System for the Accelerated Development of Porous Membranes", "comment": null, "summary": "The development of porous polymeric membranes remains a labor-intensive\nprocess, often requiring extensive trial and error to identify optimal\nfabrication parameters. In this study, we present a fully automated platform\nfor membrane fabrication and characterization via nonsolvent-induced phase\nseparation (NIPS). The system integrates automated solution preparation, blade\ncasting, controlled immersion, and compression testing, allowing precise\ncontrol over fabrication parameters such as polymer concentration and ambient\nhumidity. The modular design allows parallel processing and reproducible\nhandling of samples, reducing experimental time and increasing consistency.\nCompression testing is introduced as a sensitive mechanical characterization\nmethod for estimating membrane stiffness and as a proxy to infer porosity and\nintra-sample uniformity through automated analysis of stress-strain curves. As\na proof of concept to demonstrate the effectiveness of the system, NIPS was\ncarried out with polysulfone, the green solvent PolarClean, and water as the\npolymer, solvent, and nonsolvent, respectively. Experiments conducted with the\nautomated system reproduced expected effects of polymer concentration and\nambient humidity on membrane properties, namely increased stiffness and\nuniformity with increasing polymer concentration and humidity variations in\npore morphology and mechanical response. The developed automated platform\nsupports high-throughput experimentation and is well-suited for integration\ninto self-driving laboratory workflows, offering a scalable and reproducible\nfoundation for data-driven optimization of porous polymeric membranes through\nNIPS.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u7528\u4e8e\u901a\u8fc7NIPS\u6280\u672f\u9ad8\u6548\u3001\u4e00\u81f4\u5730\u5236\u9020\u548c\u8868\u5f81\u591a\u5b54\u805a\u5408\u7269\u819c\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6570\u636e\u9a71\u52a8\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u591a\u5b54\u805a\u5408\u7269\u819c\u7684\u5f00\u53d1\u8fc7\u7a0b\u52b3\u52a8\u5bc6\u96c6\u4e14\u4f9d\u8d56\u8bd5\u9519\uff0c\u9700\u8981\u4e00\u4e2a\u81ea\u52a8\u5316\u5e73\u53f0\u63d0\u9ad8\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "method": "\u901a\u8fc7\u975e\u6eb6\u5242\u8bf1\u5bfc\u76f8\u5206\u79bb\uff08NIPS\uff09\u6280\u672f\uff0c\u96c6\u6210\u4e86\u81ea\u52a8\u6eb6\u6db2\u5236\u5907\u3001\u5200\u7247\u6d47\u94f8\u3001\u53d7\u63a7\u6d78\u6ca1\u548c\u538b\u7f29\u6d4b\u8bd5\u7684\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5236\u9020\u53c2\u6570\u7684\u7cbe\u786e\u63a7\u5236\u3002", "result": "\u81ea\u52a8\u5316\u7cfb\u7edf\u6210\u529f\u590d\u5236\u4e86\u805a\u5408\u7269\u6d53\u5ea6\u548c\u73af\u5883\u6e7f\u5ea6\u5bf9\u819c\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u9ad8\u901a\u91cf\u5b9e\u9a8c\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5f00\u53d1\u7684\u81ea\u52a8\u5316\u5e73\u53f0\u4e3a\u591a\u5b54\u805a\u5408\u7269\u819c\u7684\u6570\u636e\u9a71\u52a8\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u91cd\u590d\u7684\u57fa\u7840\uff0c\u9002\u5408\u96c6\u6210\u5230\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2508.10918", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.10918", "abs": "https://arxiv.org/abs/2508.10918", "authors": ["Samantha Aziz", "Oleg Komogortsev"], "title": "Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder", "comment": "IJCB 2025; 11 pages, 7 figures", "summary": "We present a privacy-enhancing mechanism for gaze signals using a\nlatent-noise autoencoder that prevents users from being re-identified across\nplay sessions without their consent, while retaining the usability of the data\nfor benign tasks. We evaluate privacy-utility trade-offs across biometric\nidentification and gaze prediction tasks, showing that our approach\nsignificantly reduces biometric identifiability with minimal utility\ndegradation. Unlike prior methods in this direction, our framework retains\nphysiologically plausible gaze patterns suitable for downstream use, which\nproduces favorable privacy-utility trade-off. This work advances privacy in\ngaze-based systems by providing a usable and effective mechanism for protecting\nsensitive gaze data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6f5c\u5728\u566a\u58f0\u81ea\u7f16\u7801\u5668\u673a\u5236\uff0c\u4fdd\u62a4\u89c6\u7ebf\u6570\u636e\u9690\u79c1\u5e76\u4fdd\u6301\u5176\u53ef\u7528\u6027\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u5728\u4e0d\u540c\u6e38\u620f\u4f1a\u8bdd\u4e2d\u88ab\u672a\u7ecf\u540c\u610f\u91cd\u65b0\u8bc6\u522b\u7684\u9690\u79c1\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u89c6\u7ebf\u6570\u636e\u5728\u826f\u6027\u4efb\u52a1\u4e2d\u7684\u53ef\u7528\u6027\u3002", "method": "\u91c7\u7528\u6f5c\u5728\u566a\u58f0\u81ea\u7f16\u7801\u5668\u6280\u672f\uff0c\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u4e0e\u6570\u636e\u5b9e\u7528\u6027\uff0c\u8bc4\u4f30\u4e86\u751f\u7269\u7279\u5f81\u8bc6\u522b\u548c\u89c6\u7ebf\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\u3002", "result": "\u663e\u8457\u964d\u4f4e\u4e86\u751f\u7269\u7279\u5f81\u53ef\u8bc6\u522b\u6027\uff0c\u4e14\u6548\u7528\u635f\u5931\u6700\u5c0f\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u9002\u5408\u4e0b\u6e38\u4f7f\u7528\u7684\u751f\u7406\u5b66\u4e0a\u5408\u7406\u7684\u89c6\u7ebf\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6f5c\u5728\u566a\u58f0\u81ea\u7f16\u7801\u5668\u673a\u5236\uff0c\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u540c\u65f6\uff0c\u4fdd\u7559\u4e86\u89c6\u7ebf\u6570\u636e\u7684\u53ef\u7528\u6027\uff0c\u4e3a\u89c6\u7ebf\u57fa\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u3002"}}
{"id": "2508.10976", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10976", "abs": "https://arxiv.org/abs/2508.10976", "authors": ["Martin Diller", "Sarah Alice Gaggl", "Philipp Hanisch", "Giuseppina Monterosso", "Fritz Rauschenbach"], "title": "Grounding Rule-Based Argumentation Using Datalog", "comment": null, "summary": "ASPIC+ is one of the main general frameworks for rule-based argumentation for\nAI. Although first-order rules are commonly used in ASPIC+ examples, most\nexisting approaches to reason over rule-based argumentation only support\npropositional rules. To enable reasoning over first-order instances, a\npreliminary grounding step is required. As groundings can lead to an\nexponential increase in the size of the input theories, intelligent procedures\nare needed. However, there is a lack of dedicated solutions for ASPIC+.\nTherefore, we propose an intelligent grounding procedure that keeps the size of\nthe grounding manageable while preserving the correctness of the reasoning\nprocess. To this end, we translate the first-order ASPIC+ instance into a\nDatalog program and query a Datalog engine to obtain ground substitutions to\nperform the grounding of rules and contraries. Additionally, we propose\nsimplifications specific to the ASPIC+ formalism to avoid grounding of rules\nthat have no influence on the reasoning process. Finally, we performed an\nempirical evaluation of a prototypical implementation to show scalability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u57fa\u7840\u5316\u65b9\u6cd5\uff0c\u5c06ASPIC+\u4e00\u9636\u89c4\u5219\u8f6c\u6362\u4e3aDatalog\u7a0b\u5e8f\uff0c\u6709\u6548\u63a7\u5236\u57fa\u7840\u5316\u89c4\u6a21\u5e76\u4fdd\u6301\u63a8\u7406\u6b63\u786e\u6027\uff0c\u5b9e\u8bc1\u8bc4\u4f30\u8bc1\u660e\u5176\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u4ec5\u652f\u6301\u547d\u9898\u89c4\u5219\uff0c\u800c\u4e00\u9636\u89c4\u5219\u7684\u57fa\u7840\u5316\u53ef\u80fd\u5bfc\u81f4\u8f93\u5165\u7406\u8bba\u89c4\u6a21\u6307\u6570\u7ea7\u589e\u957f\uff0c\u7f3a\u4e4f\u9488\u5bf9ASPIC+\u7684\u4e13\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u4e00\u9636ASPIC+\u5b9e\u4f8b\u7ffb\u8bd1\u4e3aDatalog\u7a0b\u5e8f\uff0c\u5229\u7528Datalog\u5f15\u64ce\u83b7\u53d6\u57fa\u7840\u66ff\u6362\uff0c\u5e76\u5bf9ASPIC+\u5f62\u5f0f\u5316\u8fdb\u884c\u7279\u5b9a\u7b80\u5316\u4ee5\u907f\u514d\u65e0\u5173\u89c4\u5219\u7684\u57fa\u7840\u5316\u3002", "result": "\u901a\u8fc7\u539f\u578b\u5b9e\u73b0\u7684\u5b9e\u8bc1\u8bc4\u4f30\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u57fa\u7840\u5316\u7a0b\u5e8f\uff0c\u7528\u4e8e\u5904\u7406ASPIC+\u4e2d\u7684\u4e00\u9636\u89c4\u5219\uff0c\u901a\u8fc7\u5c06ASPIC+\u5b9e\u4f8b\u8f6c\u6362\u4e3aDatalog\u7a0b\u5e8f\u5e76\u5229\u7528Datalog\u5f15\u64ce\u8fdb\u884c\u67e5\u8be2\uff0c\u6709\u6548\u63a7\u5236\u4e86\u57fa\u7840\u5316\u89c4\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u8fc7\u7a0b\u7684\u6b63\u786e\u6027\u3002"}}
{"id": "2508.11415", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11415", "abs": "https://arxiv.org/abs/2508.11415", "authors": ["Ra\u00efssa Nataf", "Yoram Moses"], "title": "Time, Fences and the Ordering of Events in TSO", "comment": null, "summary": "The Total Store Order (TSO) is arguably the most widely used relaxed memory\nmodel in multiprocessor architectures, widely implemented, for example in\nIntel's x86 and x64 platforms. It allows processes to delay the visibility of\nwrites through store buffering. While this supports hardware-level\noptimizations and makes a significant contribution to multiprocessor\nefficiency, it complicates reasoning about correctness, as executions may\nviolate sequential consistency. Ensuring correct behavior often requires\ninserting synchronization primitives such as memory fences ($F$) or atomic\nread-modify-write ($RMW$) operations, but this approach can incur significant\nperformance costs. In this work, we develop a semantic framework that precisely\ncharacterizes when such synchronization is necessary under TSO. We introduce a\nnovel TSO-specific occurs-before relation, which adapts Lamport's celebrated\nhappens-before relation from asynchronous message-passing systems to the TSO\nsetting. Our main result is a theorem that proves that the only way to ensure\nthat two events that take place at different sites are temporally ordered is by\nhaving the execution create an occurs-before chain between the events. By\nstudying the role of fences and $RMW$s in creating occurs-before chains, we are\nthen able to capture cases in which these costly synchronization operations are\nunavoidable. Since proper real-time ordering of events is a fundamental aspect\nof consistency conditions such as Linearizability, our analysis provides a\nsound theoretical understanding of essential aspects of the TSO model. In\nparticular, we are able to generalize prior lower bounds for linearizable\nimplementations of shared memory objects. Our results capture the structure of\ninformation flow and causality in the TSO model by extending the standard\ncommunication-based reasoning from asynchronous systems to the TSO memory\nmodel.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bed\u4e49\u6846\u67b6\uff0c\u7528\u4e8e\u5728TSO\u5185\u5b58\u6a21\u578b\u4e0b\u7cbe\u786e\u786e\u5b9a\u540c\u6b65\u64cd\u4f5c\u7684\u5fc5\u8981\u6027\uff0c\u901a\u8fc7\u5f15\u5165\u2018occurs-before\u2019\u5173\u7cfb\u5e76\u5206\u6790\u540c\u6b65\u64cd\u4f5c\u7684\u4f5c\u7528\uff0c\u4e3a\u7406\u89e3TSO\u6a21\u578b\u4e2d\u7684\u4fe1\u606f\u6d41\u548c\u56e0\u679c\u5173\u7cfb\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u5728TSO\u5185\u5b58\u6a21\u578b\u4e0b\uff0c\u786c\u4ef6\u4f18\u5316\uff08\u5982\u5b58\u50a8\u7f13\u51b2\uff09\u867d\u7136\u63d0\u5347\u4e86\u591a\u5904\u7406\u5668\u6548\u7387\uff0c\u4f46\u589e\u52a0\u4e86\u6b63\u786e\u6027\u63a8\u7406\u7684\u590d\u6742\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u540c\u6b65\u64cd\u4f5c\uff08\u5982\u5185\u5b58\u5c4f\u969c\u6216\u539f\u5b50\u64cd\u4f5c\uff09\uff0c\u4f46\u8fd9\u4e9b\u64cd\u4f5c\u4f1a\u5e26\u6765\u6027\u80fd\u5f00\u9500\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cbe\u786e\u63cf\u8ff0\u4f55\u65f6\u9700\u8981\u8fd9\u4e9b\u540c\u6b65\u64cd\u4f5c\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684TSO\u7279\u5b9a\u7684\u2018occurs-before\u2019\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u6b64\u5173\u7cfb\u5f00\u53d1\u4e86\u4e00\u4e2a\u8bed\u4e49\u6846\u67b6\u3002\u901a\u8fc7\u5206\u6790\u5185\u5b58\u5c4f\u969c\u548c\u539f\u5b50\u64cd\u4f5c\u5728\u521b\u5efa\u2018occurs-before\u2019\u94fe\u4e2d\u7684\u4f5c\u7528\uff0c\u7814\u7a76\u786e\u5b9a\u4e86\u8fd9\u4e9b\u540c\u6b65\u64cd\u4f5c\u7684\u5fc5\u8981\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u786e\u4fdd\u4e8b\u4ef6\u65f6\u95f4\u987a\u5e8f\u7684\u552f\u4e00\u65b9\u6cd5\u662f\u901a\u8fc7\u6267\u884c\u521b\u5efa\u2018occurs-before\u2019\u94fe\u3002\u901a\u8fc7\u5206\u6790\u540c\u6b65\u64cd\u4f5c\u5728\u521b\u5efa\u8fd9\u4e9b\u94fe\u4e2d\u7684\u4f5c\u7528\uff0c\u7814\u7a76\u660e\u786e\u4e86\u8fd9\u4e9b\u64cd\u4f5c\u7684\u5fc5\u8981\u6027\u3002\u6b64\u5916\uff0c\u7814\u7a76\u63a8\u5e7f\u4e86\u5148\u524d\u5173\u4e8e\u5171\u4eab\u5185\u5b58\u5bf9\u8c61\u7ebf\u6027\u5316\u5b9e\u73b0\u7684\u4e0b\u754c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bed\u4e49\u6846\u67b6\uff0c\u7528\u4e8e\u7cbe\u786e\u63cf\u8ff0\u5728TSO\uff08Total Store Order\uff09\u5185\u5b58\u6a21\u578b\u4e0b\u4f55\u65f6\u9700\u8981\u540c\u6b65\u64cd\u4f5c\uff08\u5982\u5185\u5b58\u5c4f\u969c\u6216\u539f\u5b50\u64cd\u4f5c\uff09\u3002\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684TSO\u7279\u5b9a\u7684\u2018occurs-before\u2019\u5173\u7cfb\uff0c\u7814\u7a76\u8bc1\u660e\u4e86\u786e\u4fdd\u4e8b\u4ef6\u65f6\u95f4\u987a\u5e8f\u7684\u552f\u4e00\u65b9\u6cd5\u662f\u901a\u8fc7\u6267\u884c\u521b\u5efa\u2018occurs-before\u2019\u94fe\u3002\u8fd9\u4e00\u7ed3\u679c\u4e3a\u7406\u89e3TSO\u6a21\u578b\u4e2d\u7684\u4fe1\u606f\u6d41\u548c\u56e0\u679c\u5173\u7cfb\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u63a8\u5e7f\u4e86\u5148\u524d\u5173\u4e8e\u5171\u4eab\u5185\u5b58\u5bf9\u8c61\u7ebf\u6027\u5316\u5b9e\u73b0\u7684\u4e0b\u754c\u3002"}}
{"id": "2508.11130", "categories": ["cs.DS", "cs.DM", "math.CO"], "pdf": "https://arxiv.org/pdf/2508.11130", "abs": "https://arxiv.org/abs/2508.11130", "authors": ["Sarah Cannon", "Wesley Pegden", "Jamie Tucker-Foltz"], "title": "Sampling tree-weighted partitions without sampling trees", "comment": null, "summary": "This paper gives a new algorithm for sampling tree-weighted partitions of a\nlarge class of planar graphs. Formally, the tree-weighted distribution on\n$k$-partitions of a graph weights $k$-partitions proportional to the product of\nthe number of spanning trees of each partition class. Recent work on problems\nin computational redistricting analysis has driven special interest in the\nconditional distribution where all partition classes have the same size\n(balanced partitions). One class of Markov chains in wide use aims to sample\nfrom balanced tree-weighted $k$-partitions using a sampler for balanced\ntree-weighted 2-partitions. Previous implementations of this 2-partition\nsampler would draw a random spanning tree and check whether it contains an edge\nwhose removal produces a balanced 2-component forest; if it does, this\n2-partition is accepted, otherwise the algorithm rejects and repeats. In\npractice, this is a significant computational bottleneck.\n  We show that in fact it is possible to sample from the balanced tree-weighted\n2-partition distribution directly, without first sampling a spanning tree; the\nacceptance and rejection rates are the same as in previous samplers. We prove\nthat on a wide class of planar graphs encompassing network structures typically\narising from the geographic data used in computational redistricting, our\nalgorithm takes expected linear time $O(n)$. Notably, this is asymptotically\nfaster than the best known method to generate random trees, which is $O(n\n\\log^2 n)$ for approximate sampling and $O(n^{1 + \\log \\log \\log n / \\log \\log\nn})$ for exact sampling. Additionally, we show that a variant of our algorithm\nalso gives a speedup to $O(n \\log n)$ for exact sampling of uniformly random\ntrees on these families of graphs, improving the bounds for both exact and\napproximate sampling.", "AI": {"tldr": "\u63d0\u51fa\u76f4\u63a5\u91c7\u6837\u5e73\u8861\u6811\u52a0\u67432\u5206\u533a\u7684\u65b0\u7b97\u6cd5\uff0c\u907f\u514d\u751f\u6210\u6811\u91c7\u6837\u74f6\u9888\uff0c\u5728\u7279\u5b9a\u5e73\u9762\u56fe\u4e0a\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u8ba1\u7b97\u91cd\u5212\u5206\u6790\u4e2d\u5bf9\u5e73\u8861\u6811\u52a0\u6743k\u5206\u533a\u7684\u9700\u6c42\u63a8\u52a8\u4e86\u7814\u7a76\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u9700\u5148\u91c7\u6837\u751f\u6210\u6811\u800c\u5b58\u5728\u8ba1\u7b97\u74f6\u9888\u3002", "method": "\u901a\u8fc7\u76f4\u63a5\u91c7\u6837\u5e73\u8861\u6811\u52a0\u67432\u5206\u533a\u7684\u65b0\u7b97\u6cd5\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5148\u91c7\u6837\u751f\u6210\u6811\u7684\u74f6\u9888\u3002\u7b97\u6cd5\u5728\u7279\u5b9a\u5e73\u9762\u56fe\u7c7b\u4e0a\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6O(n)\u3002", "result": "\u65b0\u7b97\u6cd5\u5728\u5178\u578b\u5e73\u9762\u56fe\u4e0a\u5b9e\u73b0O(n)\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u6bd4\u73b0\u6709\u968f\u673a\u6811\u751f\u6210\u65b9\u6cd5\u66f4\u5feb\uff08O(n log\u00b2 n)\u8fd1\u4f3c\u91c7\u6837\uff0cO(n^(1 + log log log n / log log n))\u7cbe\u786e\u91c7\u6837\uff09\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u91c7\u6837\u5e73\u8861\u6811\u52a0\u67432\u5206\u533a\u7684\u65b0\u7b97\u6cd5\uff0c\u65e0\u9700\u5148\u91c7\u6837\u751f\u6210\u6811\uff0c\u4e14\u63a5\u53d7\u7387\u548c\u62d2\u7edd\u7387\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u540c\u3002\u8be5\u7b97\u6cd5\u5728\u5178\u578b\u7684\u5730\u7406\u6570\u636e\u7f51\u7edc\u7ed3\u6784\u4e0a\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6O(n)\uff0c\u663e\u8457\u5feb\u4e8e\u73b0\u6709\u968f\u673a\u6811\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2508.11366", "categories": ["cs.NI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11366", "abs": "https://arxiv.org/abs/2508.11366", "authors": ["Sanghoon Lee", "Taehun Kim", "Jiyeong Chae", "Kyung-Joon Park"], "title": "Optimizing ROS 2 Communication for Wireless Robotic Systems", "comment": "10 pages, 8 figures", "summary": "Wireless transmission of large payloads, such as high-resolution images and\nLiDAR point clouds, is a major bottleneck in ROS 2, the leading open-source\nrobotics middleware. The default Data Distribution Service (DDS) communication\nstack in ROS 2 exhibits significant performance degradation over lossy wireless\nlinks. Despite the widespread use of ROS 2, the underlying causes of these\nwireless communication challenges remain unexplored. In this paper, we present\nthe first in-depth network-layer analysis of ROS 2's DDS stack under wireless\nconditions with large payloads. We identify the following three key issues:\nexcessive IP fragmentation, inefficient retransmission timing, and congestive\nbuffer bursts. To address these issues, we propose a lightweight and fully\ncompatible DDS optimization framework that tunes communication parameters based\non link and payload characteristics. Our solution can be seamlessly applied\nthrough the standard ROS 2 application interface via simple XML-based QoS\nconfiguration, requiring no protocol modifications, no additional components,\nand virtually no integration efforts. Extensive experiments across various\nwireless scenarios demonstrate that our framework successfully delivers large\npayloads in conditions where existing DDS modes fail, while maintaining low\nend-to-end latency.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86ROS 2\u7684DDS\u5728\u65e0\u7ebf\u73af\u5883\u4e0b\u4f20\u8f93\u5927\u8d1f\u8f7d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65e0\u9700\u4fee\u6539\u534f\u8bae\u7684\u4f18\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f20\u8f93\u6548\u7387\u3002", "motivation": "ROS 2\u4f5c\u4e3a\u9886\u5148\u7684\u5f00\u6e90\u673a\u5668\u4eba\u4e2d\u95f4\u4ef6\uff0c\u5176\u9ed8\u8ba4\u7684DDS\u901a\u4fe1\u6808\u5728\u65e0\u7ebf\u73af\u5883\u4e0b\u4f20\u8f93\u5927\u8d1f\u8f7d\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4f46\u539f\u56e0\u5c1a\u672a\u88ab\u6df1\u5165\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u6df1\u5165\u5206\u6790ROS 2\u7684DDS\u6808\u5728\u7f51\u7edc\u5c42\u7684\u8868\u73b0\uff0c\u8bc6\u522b\u4e86IP\u788e\u7247\u5316\u3001\u91cd\u4f20\u65f6\u673a\u548c\u7f13\u51b2\u533a\u62e5\u585e\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u94fe\u8def\u548c\u8d1f\u8f7d\u7279\u6027\u7684\u901a\u4fe1\u53c2\u6570\u4f18\u5316\u6846\u67b6\u3002", "result": "\u5728\u5404\u79cd\u65e0\u7ebf\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u73b0\u6709DDS\u6a21\u5f0f\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u4f20\u8f93\u4e86\u5927\u8d1f\u8f7d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7DDS\u4f18\u5316\u6846\u67b6\u5728\u65e0\u9700\u4fee\u6539\u534f\u8bae\u6216\u589e\u52a0\u7ec4\u4ef6\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86ROS 2\u5728\u65e0\u7ebf\u73af\u5883\u4e0b\u4f20\u8f93\u5927\u8d1f\u8f7d\u7684\u6027\u80fd\uff0c\u4fdd\u6301\u4e86\u4f4e\u5ef6\u8fdf\u3002"}}
{"id": "2508.11203", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "51-04", "I.3.8; I.4.9"], "pdf": "https://arxiv.org/pdf/2508.11203", "abs": "https://arxiv.org/abs/2508.11203", "authors": ["Seungmi Lee", "Kwan Yun", "Junyong Noh"], "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation", "comment": "Pacific graphics 2025, CGF, 15 pages", "summary": "We introduce StyleMM, a novel framework that can construct a stylized 3D\nMorphable Model (3DMM) based on user-defined text descriptions specifying a\ntarget style. Building upon a pre-trained mesh deformation network and a\ntexture generator for original 3DMM-based realistic human faces, our approach\nfine-tunes these models using stylized facial images generated via text-guided\nimage-to-image (i2i) translation with a diffusion model, which serve as\nstylization targets for the rendered mesh. To prevent undesired changes in\nidentity, facial alignment, or expressions during i2i translation, we introduce\na stylization method that explicitly preserves the facial attributes of the\nsource image. By maintaining these critical attributes during image\nstylization, the proposed approach ensures consistent 3D style transfer across\nthe 3DMM parameter space through image-based training. Once trained, StyleMM\nenables feed-forward generation of stylized face meshes with explicit control\nover shape, expression, and texture parameters, producing meshes with\nconsistent vertex connectivity and animatability. Quantitative and qualitative\nevaluations demonstrate that our approach outperforms state-of-the-art methods\nin terms of identity-level facial diversity and stylization capability. The\ncode and videos are available at\n[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).", "AI": {"tldr": "StyleMM\u662f\u4e00\u4e2a\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u751f\u6210\u98ce\u683c\u53163D\u4eba\u8138\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u56fe\u50cf\u8bad\u7ec3\u5b9e\u73b0\u4e00\u81f4\u98ce\u683c\u8fc1\u79fb\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u6839\u636e\u7528\u6237\u5b9a\u4e49\u7684\u6587\u672c\u63cf\u8ff0\u751f\u6210\u98ce\u683c\u53163D\u53ef\u53d8\u5f62\u6a21\u578b\uff083DMM\uff09\u7684\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u7f51\u683c\u53d8\u5f62\u7f51\u7edc\u548c\u7eb9\u7406\u751f\u6210\u5668\uff0c\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u751f\u6210\u98ce\u683c\u5316\u76ee\u6807\u56fe\u50cf\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u4fdd\u6301\u5173\u952e\u9762\u90e8\u5c5e\u6027\u3002", "result": "StyleMM\u80fd\u591f\u524d\u9988\u751f\u6210\u5177\u6709\u4e00\u81f4\u9876\u70b9\u8fde\u63a5\u6027\u548c\u53ef\u52a8\u753b\u6027\u7684\u98ce\u683c\u5316\u9762\u90e8\u7f51\u683c\uff0c\u5e76\u5141\u8bb8\u5bf9\u5f62\u72b6\u3001\u8868\u60c5\u548c\u7eb9\u7406\u53c2\u6570\u8fdb\u884c\u663e\u5f0f\u63a7\u5236\u3002", "conclusion": "StyleMM\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8bc1\u660e\uff0c\u5728\u9762\u90e8\u591a\u6837\u6027\u548c\u98ce\u683c\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u4ee3\u7801\u548c\u89c6\u9891\u516c\u5f00\u3002"}}
{"id": "2508.11110", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11110", "abs": "https://arxiv.org/abs/2508.11110", "authors": ["Mukul Singh", "Gust Verbruggen", "Vu Le", "Sumit Gulwani"], "title": "Diffusion is a code repair operator and generator", "comment": "12 pages", "summary": "Code diffusion models generate code by iteratively removing noise from the\nlatent representation of a code snippet. During later steps of the diffusion\nprocess, when the code snippet has almost converged, differences between\ndiscrete representations of these snippets look like last-mile repairs applied\nto broken or incomplete code. We evaluate the extent to which this resemblance\ncan be exploited to leverage pre-trained code diffusion models for the problem\nof last-mile repair by considering two applications with significant potential.\nFirst, we can leverage the diffusion model for last-mile repair by adding noise\nto a broken code snippet and resuming the diffusion process. Second, we can\nleverage the diffusion model to generate arbitrary amount of training data for\nlast-mile repair tasks (that are computationally more efficient) by sampling an\nintermediate program (input) and the final program (output) from the diffusion\nprocess. We perform experiments on 3 domains (Python, Excel and PowerShell) to\nevaluate applications, as well as analyze properties.", "AI": {"tldr": "\u4ee3\u7801\u6269\u6563\u6a21\u578b\u540e\u671f\u884c\u4e3a\u53ef\u7528\u4e8e\u6700\u540e\u4e00\u82f1\u91cc\u4fee\u590d\u548c\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u4e2a\u9886\u57df\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u4ee3\u7801\u6269\u6563\u6a21\u578b\u5728\u540e\u671f\u6b65\u9aa4\u4e2d\u8868\u73b0\u51fa\u7684\u884c\u4e3a\u662f\u5426\u53ef\u7528\u4e8e\u6700\u540e\u4e00\u82f1\u91cc\u4fee\u590d\uff0c\u4ee5\u63d0\u5347\u4ee3\u7801\u4fee\u590d\u7684\u6548\u7387\u548c\u751f\u6210\u8bad\u7ec3\u6570\u636e\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5411\u635f\u574f\u7684\u4ee3\u7801\u7247\u6bb5\u6dfb\u52a0\u566a\u58f0\u5e76\u6062\u590d\u6269\u6563\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u4ece\u6269\u6563\u8fc7\u7a0b\u4e2d\u91c7\u6837\u4e2d\u95f4\u7a0b\u5e8f\u548c\u6700\u7ec8\u7a0b\u5e8f\u6765\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728Python\u3001Excel\u548cPowerShell\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6f5c\u5728\u5e94\u7528\u3002", "conclusion": "\u4ee3\u7801\u6269\u6563\u6a21\u578b\u5728\u540e\u671f\u6b65\u9aa4\u4e2d\u5c55\u73b0\u51fa\u7684\u884c\u4e3a\u7c7b\u4f3c\u4e8e\u6700\u540e\u4e00\u82f1\u91cc\u4fee\u590d\uff0c\u8fd9\u4e3a\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u4ee3\u7801\u4fee\u590d\u548c\u751f\u6210\u8bad\u7ec3\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.10999", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10999", "abs": "https://arxiv.org/abs/2508.10999", "authors": ["Yizhi Zhou", "Jie Xu", "Jiawei Xia", "Zechen Hu", "Weizi Li", "Xuan Wang"], "title": "Robust Online Calibration for UWB-Aided Visual-Inertial Navigation with Bias Correction", "comment": null, "summary": "This paper presents a novel robust online calibration framework for\nUltra-Wideband (UWB) anchors in UWB-aided Visual-Inertial Navigation Systems\n(VINS). Accurate anchor positioning, a process known as calibration, is crucial\nfor integrating UWB ranging measurements into state estimation. While several\nprior works have demonstrated satisfactory results by using robot-aided systems\nto autonomously calibrate UWB systems, there are still some limitations: 1)\nthese approaches assume accurate robot localization during the initialization\nstep, ignoring localization errors that can compromise calibration robustness,\nand 2) the calibration results are highly sensitive to the initial guess of the\nUWB anchors' positions, reducing the practical applicability of these methods\nin real-world scenarios. Our approach addresses these challenges by explicitly\nincorporating the impact of robot localization uncertainties into the\ncalibration process, ensuring robust initialization. To further enhance the\nrobustness of the calibration results against initialization errors, we propose\na tightly-coupled Schmidt Kalman Filter (SKF)-based online refinement method,\nmaking the system suitable for practical applications. Simulations and\nreal-world experiments validate the improved accuracy and robustness of our\napproach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u5728\u7ebf\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5904\u7406\u673a\u5668\u4eba\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\u548cSKF\u5728\u7ebf\u7ec6\u5316\uff0c\u663e\u8457\u63d0\u5347UWB\u951a\u70b9\u6821\u51c6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709UWB\u951a\u70b9\u6821\u51c6\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5ffd\u7565\u521d\u59cb\u5316\u9636\u6bb5\u7684\u673a\u5668\u4eba\u5b9a\u4f4d\u8bef\u5dee\uff0c\u5f71\u54cd\u6821\u51c6\u9c81\u68d2\u6027\uff1b2\uff09\u6821\u51c6\u7ed3\u679c\u5bf9UWB\u951a\u70b9\u521d\u59cb\u4f4d\u7f6e\u731c\u6d4b\u9ad8\u5ea6\u654f\u611f\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6821\u51c6\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u5bc6\u8026\u5408\u7684Schmidt\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08SKF\uff09\u5728\u7ebf\u7ec6\u5316\u65b9\u6cd5\uff0c\u663e\u5f0f\u5730\u5c06\u673a\u5668\u4eba\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165\u6821\u51c6\u8fc7\u7a0b\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9c81\u68d2\u5728\u7ebf\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u8003\u8651\u673a\u5668\u4eba\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\u5e76\u91c7\u7528\u7d27\u5bc6\u8026\u5408\u7684Schmidt\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08SKF\uff09\u5728\u7ebf\u7ec6\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d85\u5bbd\u5e26\uff08UWB\uff09\u951a\u70b9\u6821\u51c6\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.10922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10922", "abs": "https://arxiv.org/abs/2508.10922", "authors": ["Jianlong Wu", "Wei Liu", "Ye Liu", "Meng Liu", "Liqiang Nie", "Zhouchen Lin", "Chang Wen Chen"], "title": "A Survey on Video Temporal Grounding with Multimodal Large Language Model", "comment": "20 pages,6 figures,survey", "summary": "The recent advancement in video temporal grounding (VTG) has significantly\nenhanced fine-grained video understanding, primarily driven by multimodal large\nlanguage models (MLLMs). With superior multimodal comprehension and reasoning\nabilities, VTG approaches based on MLLMs (VTG-MLLMs) are gradually surpassing\ntraditional fine-tuned methods. They not only achieve competitive performance\nbut also excel in generalization across zero-shot, multi-task, and multi-domain\nsettings. Despite extensive surveys on general video-language understanding,\ncomprehensive reviews specifically addressing VTG-MLLMs remain scarce. To fill\nthis gap, this survey systematically examines current research on VTG-MLLMs\nthrough a three-dimensional taxonomy: 1) the functional roles of MLLMs,\nhighlighting their architectural significance; 2) training paradigms, analyzing\nstrategies for temporal reasoning and task adaptation; and 3) video feature\nprocessing techniques, which determine spatiotemporal representation\neffectiveness. We further discuss benchmark datasets, evaluation protocols, and\nsummarize empirical findings. Finally, we identify existing limitations and\npropose promising research directions. For additional resources and details,\nreaders are encouraged to visit our repository at\nhttps://github.com/ki-lw/Awesome-MLLMs-for-Video-Temporal-Grounding.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u7ef4\u5206\u7c7b\u6cd5\u5206\u6790\u5176\u67b6\u6784\u3001\u8bad\u7ec3\u548c\u7279\u5f81\u5904\u7406\uff0c\u5e76\u603b\u7ed3\u4e86\u73b0\u6709\u5c40\u9650\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u9891\u8bed\u8a00\u7406\u89e3\u9886\u57df\u5df2\u6709\u5e7f\u6cdb\u7efc\u8ff0\uff0c\u4f46\u9488\u5bf9VTG-MLLMs\u7684\u5168\u9762\u8c03\u67e5\u4ecd\u8f83\u5c11\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u4e09\u7ef4\u5206\u7c7b\u6cd5\u7cfb\u7edf\u6027\u5730\u56de\u987e\u5f53\u524dVTG-MLLMs\u7684\u7814\u7a76\uff1a1) MLLMs\u7684\u529f\u80fd\u89d2\u8272\uff1b2) \u8bad\u7ec3\u8303\u5f0f\uff1b3) \u89c6\u9891\u7279\u5f81\u5904\u7406\u6280\u672f\u3002", "result": "VTG-MLLMs\u4e0d\u4ec5\u5728\u6027\u80fd\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u8fd8\u5728\u96f6\u6837\u672c\u3001\u591a\u4efb\u52a1\u548c\u591a\u9886\u57df\u8bbe\u7f6e\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\uff08VTG\uff09\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.11070", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11070", "abs": "https://arxiv.org/abs/2508.11070", "authors": ["Zahra Khotanlou", "Kate Larson", "Amir-Hossein Karimi"], "title": "From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching", "comment": null, "summary": "Decision makers are increasingly relying on machine learning in sensitive\nsituations. In such settings, algorithmic recourse aims to provide individuals\nwith actionable and minimally costly steps to reverse unfavorable AI-driven\ndecisions. While existing research predominantly focuses on single-individual\n(i.e., seeker) and single-model (i.e., provider) scenarios, real-world\napplications often involve multiple interacting stakeholders. Optimizing\noutcomes for seekers under an individual welfare approach overlooks the\ninherently multi-agent nature of real-world systems, where individuals interact\nand compete for limited resources. To address this, we introduce a novel\nframework for multi-agent algorithmic recourse that accounts for multiple\nrecourse seekers and recourse providers. We model this many-to-many interaction\nas a capacitated weighted bipartite matching problem, where matches are guided\nby both recourse cost and provider capacity. Edge weights, reflecting recourse\ncosts, are optimized for social welfare while quantifying the welfare gap\nbetween individual welfare and this collectively feasible outcome. We propose a\nthree-layer optimization framework: (1) basic capacitated matching, (2) optimal\ncapacity redistribution to minimize the welfare gap, and (3) cost-aware\noptimization balancing welfare maximization with capacity adjustment costs.\nExperimental validation on synthetic and real-world datasets demonstrates that\nour framework enables the many-to-many algorithmic recourse to achieve\nnear-optimal welfare with minimum modification in system settings. This work\nextends algorithmic recourse from individual recommendations to system-level\ndesign, providing a tractable path toward higher social welfare while\nmaintaining individual actionability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4e3b\u4f53\u7b97\u6cd5\u8ffd\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u5c42\u4f18\u5316\u89e3\u51b3\u4e86\u591a\u5bf9\u591a\u4ea4\u4e92\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u9ad8\u6548\u63d0\u5347\u793e\u4f1a\u798f\u5229\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u7b97\u6cd5\u8ffd\u7d22\u5f80\u5f80\u6d89\u53ca\u591a\u4e2a\u4ea4\u4e92\u7684\u5229\u76ca\u76f8\u5173\u8005\uff0c\u800c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u4e2a\u4f53\u548c\u5355\u4e00\u6a21\u578b\u573a\u666f\u3002\u4e2a\u4f53\u798f\u5229\u65b9\u6cd5\u5ffd\u89c6\u4e86\u73b0\u5b9e\u7cfb\u7edf\u7684\u591a\u4e3b\u4f53\u6027\u8d28\uff0c\u5176\u4e2d\u4e2a\u4f53\u9700\u8981\u7ade\u4e89\u6709\u9650\u8d44\u6e90\u3002", "method": "\u6a21\u578b\u5c06\u591a\u5bf9\u591a\u4ea4\u4e92\u5efa\u6a21\u4e3a\u4e00\u4e2a\u5e26\u5bb9\u91cf\u9650\u5236\u7684\u52a0\u6743\u4e8c\u5206\u56fe\u5339\u914d\u95ee\u9898\uff0c\u5339\u914d\u7531\u8ffd\u7d22\u6210\u672c\u548c\u63d0\u4f9b\u8005\u5bb9\u91cf\u5171\u540c\u6307\u5bfc\u3002\u63d0\u51fa\u4e86\u4e09\u5c42\u4f18\u5316\u6846\u67b6\uff1a\u57fa\u672c\u5bb9\u91cf\u5339\u914d\u3001\u6700\u4f18\u5bb9\u91cf\u518d\u5206\u914d\u4ee5\u6700\u5c0f\u5316\u798f\u5229\u5dee\u8ddd\uff0c\u4ee5\u53ca\u5e73\u8861\u798f\u5229\u6700\u5927\u5316\u548c\u5bb9\u91cf\u8c03\u6574\u6210\u672c\u7684\u6210\u672c\u611f\u77e5\u4f18\u5316\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5728\u7cfb\u7edf\u8bbe\u7f6e\u6700\u5c0f\u4fee\u6539\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u591a\u5bf9\u591a\u7b97\u6cd5\u8ffd\u7d22\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u798f\u5229\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u4e3b\u4f53\u7b97\u6cd5\u8ffd\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u5c42\u4f18\u5316\uff08\u5bb9\u91cf\u5339\u914d\u3001\u6700\u4f18\u5bb9\u91cf\u518d\u5206\u914d\u548c\u6210\u672c\u611f\u77e5\u4f18\u5316\uff09\uff0c\u5b9e\u73b0\u4e86\u4ece\u4e2a\u4f53\u63a8\u8350\u5230\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u7684\u6269\u5c55\uff0c\u63d0\u9ad8\u4e86\u793e\u4f1a\u798f\u5229\u540c\u65f6\u4fdd\u6301\u4e2a\u4f53\u53ef\u64cd\u4f5c\u6027\u3002"}}
{"id": "2508.11467", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.11467", "abs": "https://arxiv.org/abs/2508.11467", "authors": ["Shifang Liu", "Huiyuan Li", "Hongjiao Sheng", "Haoyuan Gui", "Xiaoyu Zhang"], "title": "Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method", "comment": null, "summary": "Singular Value Decomposition (SVD) is a fundamental matrix factorization\ntechnique in linear algebra, widely applied in numerous matrix-related\nproblems. However, traditional SVD approaches are hindered by slow panel\nfactorization and frequent CPU-GPU data transfers in heterogeneous systems,\ndespite advancements in GPU computational capabilities. In this paper, we\nintroduce a GPU-centered SVD algorithm, incorporating a novel GPU-based\nbidiagonal divide-and-conquer (BDC) method. We reformulate the algorithm and\ndata layout of different steps for SVD computation, performing all panel-level\ncomputations and trailing matrix updates entirely on GPU to eliminate CPU-GPU\ndata transfers. Furthermore, we integrate related computations to optimize BLAS\nutilization, thereby increasing arithmetic intensity and fully leveraging the\ncomputational capabilities of GPUs. Additionally, we introduce a newly\ndeveloped GPU-based BDC algorithm that restructures the workflow to eliminate\nmatrix-level CPU-GPU data transfers and enable asynchronous execution between\nthe CPU and GPU. Experimental results on AMD MI210 and NVIDIA V100 GPUs\ndemonstrate that our proposed method achieves speedups of up to 1293.64x/7.47x\nand 14.10x/12.38x compared to rocSOLVER/cuSOLVER and MAGMA, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u5728GPU\u4e0a\u6267\u884c\u7684SVD\u7b97\u6cd5\uff0c\u901a\u8fc7\u6d88\u9664CPU-GPU\u6570\u636e\u4f20\u8f93\u548c\u4f18\u5316\u8ba1\u7b97\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfSVD\u65b9\u6cd5\u5728\u5f02\u6784\u7cfb\u7edf\u4e2d\u56e0\u9762\u677f\u5206\u89e3\u901f\u5ea6\u6162\u548c\u9891\u7e41\u7684CPU-GPU\u6570\u636e\u4f20\u8f93\u800c\u53d7\u9650\uff0c\u5c3d\u7ba1GPU\u8ba1\u7b97\u80fd\u529b\u6709\u6240\u63d0\u5347\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u7684\u53cc\u5bf9\u89d2\u5206\u6cbb\uff08BDC\uff09\u65b9\u6cd5\uff0c\u91cd\u65b0\u8bbe\u8ba1\u4e86SVD\u8ba1\u7b97\u7684\u7b97\u6cd5\u548c\u6570\u636e\u5e03\u5c40\uff0c\u5b8c\u5168\u5728GPU\u4e0a\u6267\u884c\u9762\u677f\u7ea7\u8ba1\u7b97\u548c\u5c3e\u968f\u77e9\u9635\u66f4\u65b0\uff0c\u5e76\u901a\u8fc7\u6574\u5408\u8ba1\u7b97\u4f18\u5316BLAS\u5229\u7528\u7387\u3002", "result": "\u5728AMD MI210\u548cNVIDIA V100 GPU\u4e0a\uff0c\u76f8\u6bd4rocSOLVER/cuSOLVER\u548cMAGMA\uff0c\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad81293.64x/7.47x\u548c14.10x/12.38x\u7684\u52a0\u901f\u3002", "conclusion": "\u63d0\u51fa\u7684GPU\u4e2d\u5fc3SVD\u7b97\u6cd5\u901a\u8fc7\u6d88\u9664CPU-GPU\u6570\u636e\u4f20\u8f93\u548c\u4f18\u5316BLAS\u5229\u7528\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728AMD\u548cNVIDIA GPU\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u3002"}}
{"id": "2508.11444", "categories": ["cs.DS", "math.CO"], "pdf": "https://arxiv.org/pdf/2508.11444", "abs": "https://arxiv.org/abs/2508.11444", "authors": ["Therese Biedl"], "title": "Face-hitting dominating sets in planar graphs: Alternative proof and linear-time algorithm", "comment": null, "summary": "In a recent paper, Francis, Illickan, Jose and Rajendraprasad showed that\nevery $n$-vertex plane graph $G$ has (under some natural restrictions) a\nvertex-partition into two sets $V_1$ and $V_2$ such that each $V_i$ is\n\\emph{dominating} (every vertex of $G$ contains a vertex of $V_i$ in its closed\nneighbourhood) and \\emph{face-hitting} (every face of $G$ is incident to a\nvertex of $V_i$). Their proof works by considering a supergraph $G'$ of $G$\nthat has certain properties, and among all such graphs, taking one that has the\nfewest edges. As such, their proof is not algorithmic. Their proof also relies\non the 4-color theorem, for which a quadratic-time algorithm exists, but it\nwould not be easy to implement.\n  In this paper, we give a new proof that every $n$-vertex plane graph $G$ has\n(under the same restrictions) a vertex-partition into two dominating\nface-hitting sets. Our proof is constructive, and requires nothing more\ncomplicated than splitting a graph into 2-connected components, finding an ear\ndecomposition, and computing a perfect matching in a 3-regular plane graph. For\nall these problems, linear-time algorithms are known and so we can find the\nvertex-partition in linear time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6784\u9020\u6027\u8bc1\u660e\uff0c\u4f7f\u7528\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u5c06\u5e73\u9762\u56fe\u5212\u5206\u4e3a\u4e24\u4e2a\u652f\u914d\u4e14\u9762\u78b0\u7684\u96c6\u5408\uff0c\u907f\u514d\u4e86\u590d\u6742\u76844\u8272\u5b9a\u7406\u4f9d\u8d56\u3002", "motivation": "\u539f\u8bc1\u660e\u4f9d\u8d56\u4e8e4\u8272\u5b9a\u7406\u4e14\u975e\u7b97\u6cd5\u5316\uff0c\u96be\u4ee5\u5b9e\u73b0\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u6613\u5b9e\u73b0\u7684\u6784\u9020\u6027\u8bc1\u660e\u3002", "method": "\u901a\u8fc7\u5c06\u56fe\u5206\u89e3\u4e3a2\u8fde\u901a\u5206\u91cf\u3001\u5bfb\u627e\u8033\u5206\u89e3\u4ee5\u53ca\u57283\u6b63\u5219\u5e73\u9762\u56fe\u4e2d\u8ba1\u7b97\u5b8c\u7f8e\u5339\u914d\uff0c\u8fd9\u4e9b\u5747\u5df2\u77e5\u6709\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u3002", "result": "\u6210\u529f\u6784\u9020\u6027\u5730\u8bc1\u660e\u4e86\u6bcf\u4e2an\u9876\u70b9\u5e73\u9762\u56feG\u53ef\u4ee5\u5728\u7ebf\u6027\u65f6\u95f4\u5185\u5212\u5206\u4e3a\u4e24\u4e2a\u652f\u914d\u4e14\u9762\u78b0\u7684\u96c6\u5408\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6784\u9020\u6027\u8bc1\u660e\uff0c\u8868\u660e\u6bcf\u4e2an\u9876\u70b9\u5e73\u9762\u56feG\u5728\u76f8\u540c\u9650\u5236\u6761\u4ef6\u4e0b\u53ef\u4ee5\u5212\u5206\u4e3a\u4e24\u4e2a\u652f\u914d\u4e14\u9762\u78b0\u7684\u96c6\u5408\u3002\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u590d\u6742\u7684\u7b97\u6cd5\u5b9e\u73b0\uff0c\u59824\u8272\u5b9a\u7406\uff0c\u8f6c\u800c\u4f7f\u7528\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u5373\u53ef\u5b8c\u6210\u3002"}}
{"id": "2508.11475", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11475", "abs": "https://arxiv.org/abs/2508.11475", "authors": ["Ioannis Panitsas", "Akrit Mudvari", "Leandros Tassiulas"], "title": "D2Q Synchronizer: Distributed SDN Synchronization for Time Sensitive Applications", "comment": null, "summary": "In distributed Software-Defined Networking (SDN), distributed SDN controllers\nrequire synchronization to maintain a global network state. Despite the\navailability of synchronization policies for distributed SDN architectures,\nmost policies do not consider joint optimization of network and user\nperformance. In this work, we propose a reinforcement learning-based algorithm\ncalled D2Q Synchronizer, to minimize long-term network costs by strategically\noffloading time-sensitive tasks to cost-effective edge servers while satisfying\nthe latency requirements for all tasks. Evaluation results demonstrate the\nsuperiority of our synchronizer compared to heuristic and other learning\npolicies in literature, by reducing network costs by at least 45% and 10%,\nrespectively, while ensuring the QoS requirements for all user tasks across\ndynamic and multi-domain SDN networks.", "AI": {"tldr": "D2Q Synchronizer\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u5206\u5e03\u5f0fSDN\u4e2d\u7684\u4efb\u52a1\u5378\u8f7d\uff0c\u663e\u8457\u964d\u4f4e\u7f51\u7edc\u6210\u672c\u5e76\u6ee1\u8db3QoS\u8981\u6c42\u3002", "motivation": "\u5206\u5e03\u5f0fSDN\u63a7\u5236\u5668\u9700\u8981\u540c\u6b65\u4ee5\u7ef4\u62a4\u5168\u5c40\u7f51\u7edc\u72b6\u6001\uff0c\u4f46\u73b0\u6709\u540c\u6b65\u7b56\u7565\u5927\u591a\u672a\u8003\u8651\u7f51\u7edc\u548c\u7528\u6237\u6027\u80fd\u7684\u8054\u5408\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b97\u6cd5D2Q Synchronizer\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u5730\u5c06\u65f6\u95f4\u654f\u611f\u4efb\u52a1\u5378\u8f7d\u5230\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8fb9\u7f18\u670d\u52a1\u5668\u6765\u6700\u5c0f\u5316\u957f\u671f\u7f51\u7edc\u6210\u672c\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cD2Q Synchronizer\u5728\u51cf\u5c11\u7f51\u7edc\u6210\u672c\u65b9\u9762\u4f18\u4e8e\u542f\u53d1\u5f0f\u548c\u5176\u4ed6\u5b66\u4e60\u7b56\u7565\uff0c\u5206\u522b\u964d\u4f4e\u4e86\u81f3\u5c1145%\u548c10%\u7684\u6210\u672c\u3002", "conclusion": "D2Q Synchronizer\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u52a8\u6001\u548c\u591a\u57dfSDN\u7f51\u7edc\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u7f51\u7edc\u6210\u672c\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u6240\u6709\u7528\u6237\u4efb\u52a1\u7684QoS\u8981\u6c42\u3002"}}
{"id": "2508.11476", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11476", "abs": "https://arxiv.org/abs/2508.11476", "authors": ["Qian Liang", "Zichong Chen", "Yang Zhou", "Hui Huang"], "title": "SPG: Style-Prompting Guidance for Style-Specific Content Creation", "comment": "Accepted to the Journal track of Pacific Graphics 2025", "summary": "Although recent text-to-image (T2I) diffusion models excel at aligning\ngenerated images with textual prompts, controlling the visual style of the\noutput remains a challenging task. In this work, we propose Style-Prompting\nGuidance (SPG), a novel sampling strategy for style-specific image generation.\nSPG constructs a style noise vector and leverages its directional deviation\nfrom unconditional noise to guide the diffusion process toward the target style\ndistribution. By integrating SPG with Classifier-Free Guidance (CFG), our\nmethod achieves both semantic fidelity and style consistency. SPG is simple,\nrobust, and compatible with controllable frameworks like ControlNet and\nIPAdapter, making it practical and widely applicable. Extensive experiments\ndemonstrate the effectiveness and generality of our approach compared to\nstate-of-the-art methods. Code is available at\nhttps://github.com/Rumbling281441/SPG.", "AI": {"tldr": "SPG\u662f\u4e00\u79cd\u65b0\u7684\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u98ce\u683c\u566a\u58f0\u5411\u91cf\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u98ce\u683c\u63a7\u5236\u548c\u8bed\u4e49\u4fdd\u771f\uff0c\u517c\u5bb9\u6027\u5f3a\u4e14\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u63a7\u5236\u8f93\u51fa\u89c6\u89c9\u98ce\u683c\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u53c8\u80fd\u5b9e\u73b0\u98ce\u683c\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faStyle-Prompting Guidance (SPG)\uff0c\u901a\u8fc7\u6784\u5efa\u98ce\u683c\u566a\u58f0\u5411\u91cf\u5e76\u5229\u7528\u5176\u4e0e\u65e0\u6761\u4ef6\u566a\u58f0\u7684\u65b9\u5411\u504f\u5dee\u6765\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u7ed3\u5408Classifier-Free Guidance (CFG)\u5b9e\u73b0\u8bed\u4e49\u548c\u98ce\u683c\u4e00\u81f4\u6027\u3002", "result": "SPG\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u517c\u5bb9ControlNet\u548cIPAdapter\u7b49\u53ef\u63a7\u6846\u67b6\u3002", "conclusion": "SPG\u662f\u4e00\u79cd\u7b80\u5355\u3001\u9c81\u68d2\u4e14\u517c\u5bb9\u6027\u5f3a\u7684\u91c7\u6837\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u63a7\u5236\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u8f93\u51fa\u98ce\u683c\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2508.11126", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11126", "abs": "https://arxiv.org/abs/2508.11126", "authors": ["Huanting Wang", "Jingzhi Gong", "Huawei Zhang", "Zheng Wang"], "title": "AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities", "comment": null, "summary": "AI agentic programming is an emerging paradigm in which large language models\n(LLMs) autonomously plan, execute, and interact with external tools like\ncompilers, debuggers, and version control systems to iteratively perform\ncomplex software development tasks. Unlike conventional code generation tools,\nagentic systems are capable of decomposing high-level goals, coordinating\nmulti-step processes, and adapting their behavior based on intermediate\nfeedback. These capabilities are transforming the software development\npractice. As this emerging field evolves rapidly, there is a need to define its\nscope, consolidate its technical foundations, and identify open research\nchallenges. This survey provides a comprehensive and timely review of AI\nagentic programming. We introduce a taxonomy of agent behaviors and system\narchitectures, and examine core techniques including planning, memory and\ncontext management, tool integration, and execution monitoring. We also analyze\nexisting benchmarks and evaluation methodologies used to assess coding agent\nperformance. Our study identifies several key challenges, including limitations\nin handling long context, a lack of persistent memory across tasks, and\nconcerns around safety, alignment with user intent, and collaboration with\nhuman developers. We discuss emerging opportunities to improve the reliability,\nadaptability, and transparency of agentic systems. By synthesizing recent\nadvances and outlining future directions, this survey aims to provide a\nfoundation for research and development in building the next generation of\nintelligent and trustworthy AI coding agents.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86AI\u4ee3\u7406\u7f16\u7a0b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u6cd5\u548c\u6838\u5fc3\u6280\u672f\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u7f16\u7a0b\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u7684\u8fc5\u901f\u53d1\u5c55\uff0c\u6709\u5fc5\u8981\u5b9a\u4e49\u5176\u8303\u56f4\u3001\u5de9\u56fa\u5176\u6280\u672f\u57fa\u7840\u5e76\u8bc6\u522b\u5f00\u653e\u7684\u7814\u7a76\u6311\u6218\u3002", "method": "\u6211\u4eec\u4ecb\u7ecd\u4e86\u4ee3\u7406\u884c\u4e3a\u548c\u7cfb\u7edf\u67b6\u6784\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u7814\u7a76\u4e86\u5305\u62ec\u89c4\u5212\u3001\u5185\u5b58\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u3001\u5de5\u5177\u96c6\u6210\u548c\u6267\u884c\u76d1\u63a7\u5728\u5185\u7684\u6838\u5fc3\u6280\u672f\u3002", "result": "\u6211\u4eec\u7684\u7814\u7a76\u53d1\u73b0\u4e86\u51e0\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u5c40\u9650\u6027\u3001\u8de8\u4efb\u52a1\u7f3a\u4e4f\u6301\u4e45\u5185\u5b58\uff0c\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u5f00\u53d1\u8005\u534f\u4f5c\u7684\u5b89\u5168\u6027\u548c\u5bf9\u9f50\u95ee\u9898\u3002", "conclusion": "\u672c\u8c03\u67e5\u65e8\u5728\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u667a\u80fd\u4e14\u53ef\u4fe1\u8d56\u7684AI\u7f16\u7a0b\u4ee3\u7406\u7684\u7814\u7a76\u548c\u5f00\u53d1\u63d0\u4f9b\u57fa\u7840\uff0c\u901a\u8fc7\u7efc\u5408\u8fd1\u671f\u8fdb\u5c55\u5e76\u6982\u8ff0\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2508.11002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11002", "abs": "https://arxiv.org/abs/2508.11002", "authors": ["Nikolaos Gkanatsios", "Jiahe Xu", "Matthew Bronars", "Arsalan Mousavian", "Tsung-Wei Ke", "Katerina Fragkiadaki"], "title": "3D FlowMatch Actor: Unified 3D Policy for Single- and Dual-Arm Manipulation", "comment": null, "summary": "We present 3D FlowMatch Actor (3DFA), a 3D policy architecture for robot\nmanipulation that combines flow matching for trajectory prediction with 3D\npretrained visual scene representations for learning from demonstration. 3DFA\nleverages 3D relative attention between action and visual tokens during action\ndenoising, building on prior work in 3D diffusion-based single-arm policy\nlearning. Through a combination of flow matching and targeted system-level and\narchitectural optimizations, 3DFA achieves over 30x faster training and\ninference than previous 3D diffusion-based policies, without sacrificing\nperformance. On the bimanual PerAct2 benchmark, it establishes a new state of\nthe art, outperforming the next-best method by an absolute margin of 41.4%. In\nextensive real-world evaluations, it surpasses strong baselines with up to\n1000x more parameters and significantly more pretraining. In unimanual\nsettings, it sets a new state of the art on 74 RLBench tasks by directly\npredicting dense end-effector trajectories, eliminating the need for motion\nplanning. Comprehensive ablation studies underscore the importance of our\ndesign choices for both policy effectiveness and efficiency.", "AI": {"tldr": "3DFA\u662f\u4e00\u79cd\u7ed3\u5408\u6d41\u5339\u914d\u548c3D\u89c6\u89c9\u8868\u793a\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u63d0\u534730\u500d\uff0c\u6027\u80fd\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u67093D\u6269\u6563\u7b56\u7565\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u3002", "method": "3DFA\u7ed3\u5408\u6d41\u5339\u914d\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u548c3D\u9884\u8bad\u7ec3\u89c6\u89c9\u573a\u666f\u8868\u793a\uff0c\u5229\u75283D\u76f8\u5bf9\u6ce8\u610f\u529b\u673a\u5236\u5728\u52a8\u4f5c\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4f18\u5316\u52a8\u4f5c\u548c\u89c6\u89c9\u4ee4\u724c\u7684\u4ea4\u4e92\u3002", "result": "3DFA\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb30\u500d\uff0c\u5728\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u63d0\u534741.4%\uff0c\u5728\u5355\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u76f4\u63a5\u572874\u4e2aRLBench\u4efb\u52a1\u4e0a\u53d6\u5f97\u6700\u4f73\u6210\u7ee9\u3002", "conclusion": "3DFA\u901a\u8fc7\u7ed3\u5408\u6d41\u5339\u914d\u548c3D\u9884\u8bad\u7ec3\u89c6\u89c9\u573a\u666f\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u6210\u4e3a\u65b0\u7684\u6280\u672f\u6807\u6746\u3002"}}
{"id": "2508.10931", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10931", "abs": "https://arxiv.org/abs/2508.10931", "authors": ["Wenqi Guo", "Shan Du"], "title": "VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By \\underline{V}alue \\underline{S}ign \\underline{F}lip", "comment": null, "summary": "We introduce Value Sign Flip (VSF), a simple and efficient method for\nincorporating negative prompt guidance in few-step diffusion and flow-matching\nimage generation models. Unlike existing approaches such as classifier-free\nguidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by\nflipping the sign of attention values from negative prompts. Our method\nrequires only small computational overhead and integrates effectively with\nMMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as\ncross-attention-based models like Wan. We validate VSF on challenging datasets\nwith complex prompt pairs and demonstrate superior performance in both static\nimage and video generation tasks. Experimental results show that VSF\nsignificantly improves negative prompt adherence compared to prior methods in\nfew-step models, and even CFG in non-few-step models, while maintaining\ncompetitive image quality. Code and ComfyUI node are available in\nhttps://github.com/weathon/VSF/tree/main.", "AI": {"tldr": "VSF\u901a\u8fc7\u7ffb\u8f6c\u8d1f\u63d0\u793a\u7684\u6ce8\u610f\u529b\u503c\u7b26\u53f7\uff0c\u6709\u6548\u63d0\u5347\u8d1f\u63d0\u793a\u9075\u5faa\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\uff08\u5982CFG\u3001NASA\u548cNAG\uff09\u5728\u8d1f\u63d0\u793a\u5f15\u5bfc\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5c11\u6b65\u6269\u6563\u548c\u6d41\u5339\u914d\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u3002", "method": "VSF\uff08Value Sign Flip\uff09\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ffb\u8f6c\u8d1f\u63d0\u793a\u7684\u6ce8\u610f\u529b\u503c\u7b26\u53f7\u6765\u52a8\u6001\u6291\u5236\u4e0d\u9700\u8981\u7684\u5185\u5bb9\uff0c\u9002\u7528\u4e8eMMDiT-style\u67b6\u6784\u548c\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVSF\u5728\u590d\u6742\u63d0\u793a\u5bf9\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u9759\u6001\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "VSF\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u7ffb\u8f6c\u8d1f\u63d0\u793a\u7684\u6ce8\u610f\u529b\u503c\u7b26\u53f7\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d1f\u63d0\u793a\u7684\u9075\u5faa\u80fd\u529b\uff0c\u540c\u65f6\u5728\u5c11\u6b65\u6a21\u578b\u548c\u975e\u5c11\u6b65\u6a21\u578b\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u4fdd\u6301\u4e86\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2508.11085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11085", "abs": "https://arxiv.org/abs/2508.11085", "authors": ["Qingqing Wang", "Liqiang Xiao", "Chang Chang"], "title": "Learn to optimize for automatic proton PBS treatment planning for H&N cancers", "comment": "27 pages, 4 figures", "summary": "Proton PBS treatment planning for H&N cancers involves numerous conflicting\nobjectives, requiring significant effort from human planners to balance and\nsatisfy multiple clinical goals during planning. To achieve this,\nexperience-demanding objective parameter adjustment and computationally\nexpensive inverse optimization are performed iteratively. Extensive efforts\nhave been made to automatically adjust objective parameters, but the most\ntime-consuming component, i.e., inverse optimization, still relies heavily on\ntheory-driven approaches. We propose a data-driven inverse optimizer and\nintegrate it into a PPO-based automatic treatment planning framework to\nautomatically generate high-quality plans within a clinical acceptable planning\ntime. The inverse optimizer is a L2O method that predicts update steps by\nlearning from the task-specific data distribution. For the first time, we\nintegrate techniques designed for long-context processing, originally developed\nfor LLMs, into a Transformer-based L2O framework to address the scalability\nissue of existing L2O methods. The PPO framework functions as an outer-loop\nvirtual planner, autonomously adjusting objective parameters through a policy\nnetwork, and the dose predictor is used to initialize objective parameters. The\ninner-loop L2O inverse optimizer computes machine-deliverable MU values based\non objectives refined by the PPO policy network. 97 patients are collected in\nthis study, and compared with L-BFGSB, our L2O-based inverse optimizer improves\nthe effectiveness and efficiency by 22.97% and 36.41%, respectively. In\nconjunction with the PPO-based learned virtual planner, plans generated by our\nframework within an average of 2.55 hours show improved or comparable OAR\nsparing with superior target coverage for patients with different prescription\ndose levels, number of target volumes, beam angles, etc., compared with\nhuman-generated plans.", "AI": {"tldr": "\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u9006\u5411\u4f18\u5316\u5668\u7ed3\u5408PPO\u6846\u67b6\uff0c\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u8d28\u5b50\u6cbb\u7597\u8ba1\u5212\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u4e0e\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8d28\u5b50PBS\u6cbb\u7597\u8ba1\u5212\u4e2d\u4eba\u5de5\u8c03\u6574\u53c2\u6570\u548c\u9006\u5411\u4f18\u5316\u7684\u9ad8\u8017\u65f6\u95ee\u9898\u3002", "method": "\u91c7\u7528L2O\u65b9\u6cd5\u548cTransformer\u6846\u67b6\u5904\u7406\u957f\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408PPO\u7b56\u7565\u7f51\u7edc\u8c03\u6574\u76ee\u6807\u53c2\u6570\u3002", "result": "L2O\u9006\u5411\u4f18\u5316\u5668\u6548\u7387\u63d0\u534736.41%\uff0c\u6548\u679c\u63d0\u534722.97%\uff0c\u751f\u6210\u7684\u8ba1\u5212\u8d28\u91cf\u4f18\u4e8e\u6216\u7b49\u540c\u4e8e\u4eba\u5de5\u8ba1\u5212\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u9006\u5411\u4f18\u5316\u5668\uff0c\u5e76\u7ed3\u5408PPO\u6846\u67b6\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u6cbb\u7597\u8ba1\u5212\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2508.11574", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11574", "abs": "https://arxiv.org/abs/2508.11574", "authors": ["Mohammad Sajid Shahriar", "Suresh Subramaniam", "Motoharu Matsuura", "Hiroshi Hasegawa", "Shih-Chun Lin"], "title": "Intelligent Edge Resource Provisioning for Scalable Digital Twins of Autonomous Vehicles", "comment": null, "summary": "The next generation networks offers significant potential to advance\nIntelligent Transportation Systems (ITS), particularly through the integration\nof Digital Twins (DTs). However, ensuring the uninterrupted operation of DTs\nthrough efficient computing resource management remains an open challenge. This\npaper introduces a distributed computing archi tecture that integrates DTs and\nMobile Edge Computing (MEC) within a software-defined vehicular networking\nframework to enable intelligent, low-latency transportation services. A network\naware scalable collaborative task provisioning algorithm is de veloped to train\nan autonomous agent, which is evaluated using a realistic connected autonomous\nvehicle (CAV) traffic simulation. The proposed framework significantly enhances\nthe robustness and scalability of DT operations by reducing synchronization\nerrors to as low as 5% while achieving up to 99.5% utilization of edge\ncomputing resources.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u6570\u5b57\u5b6a\u751f\u548c\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u7684\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u901a\u8fc7\u667a\u80fd\u4efb\u52a1\u914d\u7f6e\u7b97\u6cd5\u63d0\u5347\u4ea4\u901a\u670d\u52a1\u7684\u4f4e\u5ef6\u8fdf\u548c\u7a33\u5065\u6027\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u7f51\u7edc\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08ITS\uff09\u63d0\u4f9b\u4e86\u5de8\u5927\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\uff08DTs\uff09\u7684\u96c6\u6210\u3002\u7136\u800c\uff0c\u5982\u4f55\u901a\u8fc7\u9ad8\u6548\u7684\u8ba1\u7b97\u8d44\u6e90\u7ba1\u7406\u786e\u4fddDTs\u7684\u65e0\u95f4\u65ad\u8fd0\u884c\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7f51\u7edc\u611f\u77e5\u7684\u53ef\u6269\u5c55\u534f\u4f5c\u4efb\u52a1\u914d\u7f6e\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u81ea\u4e3b\u4ee3\u7406\uff0c\u5e76\u5728\u73b0\u5b9e\u7684\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08CAV\uff09\u4ea4\u901a\u6a21\u62df\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u589e\u5f3a\u4e86DT\u64cd\u4f5c\u7684\u7a33\u5065\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u540c\u6b65\u9519\u8bef\u964d\u4f4e\u81f35%\uff0c\u8fb9\u7f18\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u9ad8\u8fbe99.5%\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b57\u5b6a\u751f\u64cd\u4f5c\u7684\u7a33\u5065\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u540c\u6b65\u9519\u8bef\u964d\u4f4e\u81f35%\uff0c\u8fb9\u7f18\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u9ad8\u8fbe99.5%\u3002"}}
{"id": "2508.10934", "categories": ["cs.CV", "cs.GR", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.10934", "abs": "https://arxiv.org/abs/2508.10934", "authors": ["Jiahui Huang", "Qunjie Zhou", "Hesam Rabeti", "Aleksandr Korovko", "Huan Ling", "Xuanchi Ren", "Tianchang Shen", "Jun Gao", "Dmitry Slepichev", "Chen-Hsuan Lin", "Jiawei Ren", "Kevin Xie", "Joydeep Biswas", "Laura Leal-Taixe", "Sanja Fidler"], "title": "ViPE: Video Pose Engine for 3D Geometric Perception", "comment": "Paper website: https://research.nvidia.com/labs/toronto-ai/vipe/", "summary": "Accurate 3D geometric perception is an important prerequisite for a wide\nrange of spatial AI systems. While state-of-the-art methods depend on\nlarge-scale training data, acquiring consistent and precise 3D annotations from\nin-the-wild videos remains a key challenge. In this work, we introduce ViPE, a\nhandy and versatile video processing engine designed to bridge this gap. ViPE\nefficiently estimates camera intrinsics, camera motion, and dense, near-metric\ndepth maps from unconstrained raw videos. It is robust to diverse scenarios,\nincluding dynamic selfie videos, cinematic shots, or dashcams, and supports\nvarious camera models such as pinhole, wide-angle, and 360{\\deg} panoramas. We\nhave benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing\nuncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and\nruns at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to\nannotate a large-scale collection of videos. This collection includes around\n100K real-world internet videos, 1M high-quality AI-generated videos, and 2K\npanoramic videos, totaling approximately 96M frames -- all annotated with\naccurate camera poses and dense depth maps. We open-source ViPE and the\nannotated dataset with the hope of accelerating the development of spatial AI\nsystems.", "AI": {"tldr": "ViPE\u662f\u4e00\u79cd\u9ad8\u6548\u89c6\u9891\u5904\u7406\u5f15\u64ce\uff0c\u80fd\u4f30\u8ba1\u76f8\u673a\u53c2\u6570\u548c\u6df1\u5ea6\u56fe\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6807\u6ce8\u4e86\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u4ece\u91ce\u5916\u89c6\u9891\u4e2d\u83b7\u53d6\u4e00\u81f4\u4e14\u7cbe\u786e\u76843D\u6807\u6ce8\u7684\u6311\u6218\uff0c\u4ee5\u652f\u6301\u7a7a\u95f4AI\u7cfb\u7edf\u7684\u8bad\u7ec3\u9700\u6c42\u3002", "method": "ViPE\u662f\u4e00\u79cd\u9ad8\u6548\u4f30\u8ba1\u76f8\u673a\u5185\u53c2\u3001\u76f8\u673a\u8fd0\u52a8\u548c\u5bc6\u96c6\u8fd1\u5ea6\u91cf\u6df1\u5ea6\u56fe\u7684\u89c6\u9891\u5904\u7406\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u548c\u76f8\u673a\u6a21\u578b\u3002", "result": "ViPE\u5728TUM/KITTI\u5e8f\u5217\u4e0a\u4f18\u4e8e\u73b0\u6709\u672a\u6821\u51c6\u4f4d\u59ff\u4f30\u8ba1\u57fa\u7ebf18%/50%\uff0c\u5e76\u5728\u5355GPU\u4e0a\u4ee53-5FPS\u8fd0\u884c\u3002", "conclusion": "ViPE\u53ca\u5176\u6807\u6ce8\u7684\u6570\u636e\u96c6\u7684\u5f00\u6e90\u65e8\u5728\u52a0\u901f\u7a7a\u95f4AI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.11147", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2508.11147", "abs": "https://arxiv.org/abs/2508.11147", "authors": ["Zhengquan Li", "Zhenhao Li", "Zishuo Ding"], "title": "From Feedback to Failure: Automated Android Performance Issue Reproduction", "comment": "10page, 8 figures", "summary": "Mobile application performance is a vital factor for user experience. Yet,\nperformance issues are notoriously difficult to detect within development\nenvironments, where their manifestations are often less conspicuous and\ndiagnosis proves more challenging. To address this limitation, we propose\nRevPerf, an advanced performance issue reproduction tool that leverages app\nreviews from Google Play to acquire pertinent information. RevPerf employs\nrelevant reviews and prompt engineering to enrich the original review with\nperformance issue details. An execution agent is then employed to generate and\nexecute commands to reproduce the issue. After executing all necessary steps,\nthe system incorporates multifaceted detection methods to identify performance\nissues by monitoring Android logs, GUI changes, and system resource utilization\nduring the reproduction process. Experimental results demonstrate that our\nproposed framework achieves a 70\\% success rate in reproducing performance\nissues on the dataset we constructed and manually validated.", "AI": {"tldr": "RevPerf\u5229\u7528Google Play\u8bc4\u8bba\u548c\u591a\u65b9\u9762\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6210\u529f\u590d\u73b070%\u7684\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u79fb\u52a8\u5e94\u7528\u6027\u80fd\u5bf9\u7528\u6237\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6027\u80fd\u95ee\u9898\u5728\u5f00\u53d1\u73af\u5883\u4e2d\u96be\u4ee5\u68c0\u6d4b\u548c\u8bca\u65ad\u3002", "method": "RevPerf\u7ed3\u5408\u76f8\u5173\u8bc4\u8bba\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u4e30\u5bcc\u539f\u59cb\u8bc4\u8bba\u4e2d\u7684\u6027\u80fd\u95ee\u9898\u7ec6\u8282\uff0c\u5e76\u901a\u8fc7\u6267\u884c\u4ee3\u7406\u751f\u6210\u548c\u6267\u884c\u547d\u4ee4\u6765\u590d\u73b0\u95ee\u9898\u3002\u7cfb\u7edf\u8fd8\u91c7\u7528\u591a\u65b9\u9762\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5982\u76d1\u63a7Android\u65e5\u5fd7\u3001GUI\u53d8\u5316\u548c\u7cfb\u7edf\u8d44\u6e90\u5229\u7528\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRevPerf\u5728\u6784\u5efa\u5e76\u624b\u52a8\u9a8c\u8bc1\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u6210\u529f\u590d\u73b0\u4e8670%\u7684\u6027\u80fd\u95ee\u9898\u3002", "conclusion": "RevPerf\u662f\u4e00\u79cd\u6709\u6548\u7684\u6027\u80fd\u95ee\u9898\u590d\u73b0\u5de5\u5177\uff0c\u901a\u8fc7\u5229\u7528Google Play\u7684\u8bc4\u8bba\u548c\u591a\u79cd\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6210\u529f\u590d\u73b0\u4e8670%\u7684\u6027\u80fd\u95ee\u9898\u3002"}}
{"id": "2508.11049", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11049", "abs": "https://arxiv.org/abs/2508.11049", "authors": ["Kelin Yu", "Sheng Zhang", "Harshit Soora", "Furong Huang", "Heng Huang", "Pratap Tokekar", "Ruohan Gao"], "title": "GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning", "comment": "Published at ICCV 2025", "summary": "Recent advances have shown that video generation models can enhance robot\nlearning by deriving effective robot actions through inverse dynamics. However,\nthese methods heavily depend on the quality of generated data and struggle with\nfine-grained manipulation due to the lack of environment feedback. While\nvideo-based reinforcement learning improves policy robustness, it remains\nconstrained by the uncertainty of video generation and the challenges of\ncollecting large-scale robot datasets for training diffusion models. To address\nthese limitations, we propose GenFlowRL, which derives shaped rewards from\ngenerated flow trained from diverse cross-embodiment datasets. This enables\nlearning generalizable and robust policies from diverse demonstrations using\nlow-dimensional, object-centric features. Experiments on 10 manipulation tasks,\nboth in simulation and real-world cross-embodiment evaluations, demonstrate\nthat GenFlowRL effectively leverages manipulation features extracted from\ngenerated object-centric flow, consistently achieving superior performance\nacross diverse and challenging scenarios. Our Project Page:\nhttps://colinyu1.github.io/genflowrl", "AI": {"tldr": "GenFlowRL\u901a\u8fc7\u4ece\u591a\u6837\u5316\u7684\u8de8\u4f53\u73b0\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u751f\u6210\u7684\u6d41\u6765\u5f62\u6210\u5956\u52b1\uff0c\u5b66\u4e60\u901a\u7528\u4e14\u7a33\u5065\u7684\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u5b58\u5728\u5bf9\u751f\u6210\u6570\u636e\u8d28\u91cf\u7684\u4f9d\u8d56\u4ee5\u53ca\u5bf9\u7cbe\u7ec6\u64cd\u4f5c\u7684\u56f0\u96be\u3002\u89c6\u9891\u5f3a\u5316\u5b66\u4e60\u867d\u80fd\u63d0\u5347\u7b56\u7565\u7a33\u5065\u6027\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u89c6\u9891\u751f\u6210\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u96c6\u6536\u96c6\u7684\u6311\u6218\u3002", "method": "GenFlowRL\u4ece\u591a\u6837\u5316\u7684\u8de8\u4f53\u73b0\u6570\u636e\u96c6\u4e2d\u8bad\u7ec3\u751f\u6210\u7684\u6d41\uff0c\u5e76\u4ece\u4e2d\u63d0\u53d6\u5f62\u72b6\u5956\u52b1\u3002\u8fd9\u79cd\u65b9\u6cd5\u5229\u7528\u4f4e\u7ef4\u3001\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u7279\u5f81\u6765\u5b66\u4e60\u901a\u7528\u4e14\u7a33\u5065\u7684\u7b56\u7565\u3002", "result": "\u572810\u4e2a\u64cd\u4f5c\u4efb\u52a1\u7684\u5b9e\u9a8c\u4e2d\uff0cGenFlowRL\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u8de8\u4f53\u73b0\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u4ece\u751f\u6210\u7684\u5bf9\u8c61\u4e2d\u5fc3\u6d41\u4e2d\u63d0\u53d6\u7684\u64cd\u4f5c\u7279\u5f81\u3002", "conclusion": "GenFlowRL\u901a\u8fc7\u4ece\u591a\u6837\u5316\u7684\u8de8\u4f53\u73b0\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u751f\u6210\u7684\u6d41\u6765\u5f62\u6210\u5956\u52b1\uff0c\u4ece\u800c\u80fd\u591f\u5229\u7528\u4f4e\u7ef4\u3001\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u7279\u5f81\u5b66\u4e60\u901a\u7528\u4e14\u7a33\u5065\u7684\u7b56\u7565\u3002\u572810\u4e2a\u64cd\u4f5c\u4efb\u52a1\u7684\u5b9e\u9a8c\u4e2d\uff0c\u65e0\u8bba\u662f\u5728\u6a21\u62df\u8fd8\u662f\u771f\u5b9e\u4e16\u754c\u7684\u8de8\u4f53\u73b0\u8bc4\u4f30\u4e2d\uff0cGenFlowRL\u5747\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.10933", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.10933", "abs": "https://arxiv.org/abs/2508.10933", "authors": ["Yoli Shavit", "Yosi Keller"], "title": "Relative Pose Regression with Pose Auto-Encoders: Enhancing Accuracy and Data Efficiency for Retail Applications", "comment": "Accepted to ICCVW 2025", "summary": "Accurate camera localization is crucial for modern retail environments,\nenabling enhanced customer experiences, streamlined inventory management, and\nautonomous operations. While Absolute Pose Regression (APR) from a single image\noffers a promising solution, approaches that incorporate visual and spatial\nscene priors tend to achieve higher accuracy. Camera Pose Auto-Encoders (PAEs)\nhave recently been introduced to embed such priors into APR. In this work, we\nextend PAEs to the task of Relative Pose Regression (RPR) and propose a novel\nre-localization scheme that refines APR predictions using PAE-based RPR,\nwithout requiring additional storage of images or pose data. We first introduce\nPAE-based RPR and establish its effectiveness by comparing it with image-based\nRPR models of equivalent architectures. We then demonstrate that our refinement\nstrategy, driven by a PAE-based RPR, enhances APR localization accuracy on\nindoor benchmarks. Notably, our method is shown to achieve competitive\nperformance even when trained with only 30% of the data, substantially reducing\nthe data collection burden for retail deployment. Our code and pre-trained\nmodels are available at: https://github.com/yolish/camera-pose-auto-encoders", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8ePAE\u7684RPR\u65b9\u6cd5\uff0c\u4f18\u5316APR\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u51cf\u5c11\u6570\u636e\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u96f6\u552e\u73af\u5883\u3002", "motivation": "\u73b0\u4ee3\u96f6\u552e\u73af\u5883\u4e2d\uff0c\u7cbe\u51c6\u7684\u76f8\u673a\u5b9a\u4f4d\u5bf9\u63d0\u5347\u987e\u5ba2\u4f53\u9a8c\u3001\u4f18\u5316\u5e93\u5b58\u7ba1\u7406\u548c\u5b9e\u73b0\u81ea\u4e3b\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u7ed3\u5408\u89c6\u89c9\u548c\u7a7a\u95f4\u573a\u666f\u5148\u9a8c\u7684\u65b9\u6cd5\u80fd\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "method": "\u6269\u5c55PAEs\u81f3RPR\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u989d\u5916\u5b58\u50a8\u56fe\u50cf\u6216\u59ff\u6001\u6570\u636e\u7684\u91cd\u65b0\u5b9a\u4f4d\u65b9\u6848\uff0c\u901a\u8fc7PAE-based RPR\u4f18\u5316APR\u9884\u6d4b\u3002", "result": "\u5728\u5ba4\u5185\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86APR\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4e14\u4ec5\u970030%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fbe\u5230\u7ade\u4e89\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408PAE-based RPR\u663e\u8457\u63d0\u5347\u4e86APR\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4e14\u5728\u6570\u636e\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u964d\u4f4e\u4e86\u96f6\u552e\u90e8\u7f72\u7684\u6570\u636e\u6536\u96c6\u8d1f\u62c5\u3002"}}
{"id": "2508.11182", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11182", "abs": "https://arxiv.org/abs/2508.11182", "authors": ["Matti Berthold", "Lydia Bl\u00fcmel", "Anna Rapberger"], "title": "On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation", "comment": null, "summary": "In this work, we broaden the investigation of admissibility notions in the\ncontext of assumption-based argumentation (ABA). More specifically, we study\ntwo prominent alternatives to the standard notion of admissibility from\nabstract argumentation, namely strong and weak admissibility, and introduce the\nrespective preferred, complete and grounded semantics for general (sometimes\ncalled non-flat) ABA. To do so, we use abstract bipolar set-based argumentation\nframeworks (BSAFs) as formal playground since they concisely capture the\nrelations between assumptions and are expressive enough to represent general\nnon-flat ABA frameworks, as recently shown. While weak admissibility has been\nrecently investigated for a restricted fragment of ABA in which assumptions\ncannot be derived (flat ABA), strong admissibility has not been investigated\nfor ABA so far. We introduce strong admissibility for ABA and investigate\ndesirable properties. We furthermore extend the recent investigations of weak\nadmissibility in the flat ABA fragment to the non-flat case. We show that the\ncentral modularization property is maintained under classical, strong, and weak\nadmissibility. We also show that strong and weakly admissible semantics in\nnon-flat ABA share some of the shortcomings of standard admissible semantics\nand discuss ways to address these.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u975e\u5e73\u5766ABA\u4e2d\u5f3a\u548c\u5f31\u53ef\u63a5\u53d7\u6027\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u7684\u6a21\u5757\u5316\u6027\u8d28\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u62d3\u5bbd\u5bf9ABA\u4e2d\u53ef\u63a5\u53d7\u6027\u6982\u5ff5\u7684\u7406\u89e3\uff0c\u7279\u522b\u662f\u5728\u975e\u5e73\u5766ABA\u4e2d\uff0c\u586b\u8865\u5f3a\u53ef\u63a5\u53d7\u6027\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u62bd\u8c61\u53cc\u6781\u96c6\u57fa\u8bba\u8bc1\u6846\u67b6\uff08BSAFs\uff09\u4f5c\u4e3a\u5f62\u5f0f\u5316\u5de5\u5177\uff0c\u6269\u5c55\u4e86\u5bf9\u5e73\u5766ABA\u4e2d\u5f31\u53ef\u63a5\u53d7\u6027\u7684\u7814\u7a76\uff0c\u5e76\u9996\u6b21\u5728ABA\u4e2d\u5f15\u5165\u4e86\u5f3a\u53ef\u63a5\u53d7\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u975e\u5e73\u5766ABA\u4e2d\uff0c\u5f3a\u548c\u5f31\u53ef\u63a5\u53d7\u6027\u8bed\u4e49\u4fdd\u6301\u6a21\u5757\u5316\u6027\u8d28\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u5b83\u4eec\u4e0e\u6807\u51c6\u53ef\u63a5\u53d7\u6027\u8bed\u4e49\u5171\u6709\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u57fa\u4e8e\u5047\u8bbe\u7684\u8bba\u8bc1\uff08ABA\uff09\u4e2d\u5f3a\u548c\u5f31\u53ef\u63a5\u53d7\u6027\u7684\u66ff\u4ee3\u6982\u5ff5\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u975e\u5e73\u5766ABA\u4e2d\u8fd9\u4e9b\u6982\u5ff5\u7684\u6a21\u5757\u5316\u6027\u8d28\u53ca\u5176\u5c40\u9650\u6027\u3002"}}
{"id": "2508.11179", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11179", "abs": "https://arxiv.org/abs/2508.11179", "authors": ["Pei Liu", "Terry Zhuo", "Jiawei Deng", "Zhenchang Xing", "Qinghua Lu", "Xiaoning Du", "Hongyu Zhan"], "title": "PTMPicker: Facilitating Efficient Pretrained Model Selection for Application Developers", "comment": null, "summary": "The rapid emergence of pretrained models (PTMs) has attracted significant\nattention from both Deep Learning (DL) researchers and downstream application\ndevelopers. However, selecting appropriate PTMs remains challenging because\nexisting methods typically rely on keyword-based searches in which the keywords\nare often derived directly from function descriptions. This often fails to\nfully capture user intent and makes it difficult to identify suitable models\nwhen developers also consider factors such as bias mitigation, hardware\nrequirements, or license compliance. To address the limitations of\nkeyword-based model search, we propose PTMPicker to accurately identify\nsuitable PTMs. We first define a structured template composed of common and\nessential attributes for PTMs and then PTMPicker represents both candidate\nmodels and user-intended features (i.e., model search requests) in this unified\nformat. To determine whether candidate models satisfy user requirements, it\ncomputes embedding similarities for function-related attributes and uses\nwell-crafted prompts to evaluate special constraints such as license compliance\nand hardware requirements. We scraped a total of 543,949 pretrained models from\nHugging Face to prepare valid candidates for selection. PTMPicker then\nrepresented them in the predefined structured format by extracting their\nassociated descriptions. Guided by the extracted metadata, we synthesized a\ntotal of 15,207 model search requests with carefully designed prompts, as no\nsuch search requests are readily available. Experiments on the curated PTM\ndataset and the synthesized model search requests show that PTMPicker can help\nusers effectively identify models,with 85% of the sampled requests successfully\nlocating appropriate PTMs within the top-10 ranked candidates.", "AI": {"tldr": "PTMPicker \u662f\u4e00\u79cd\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u5339\u914d\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u7528\u6237\u9700\u6c42\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5173\u952e\u8bcd\u7684\u6a21\u578b\u641c\u7d22\u65b9\u6cd5\u96be\u4ee5\u5168\u9762\u6355\u6349\u7528\u6237\u610f\u56fe\uff0c\u5c24\u5176\u5728\u8003\u8651\u504f\u89c1\u7f13\u89e3\u3001\u786c\u4ef6\u8981\u6c42\u6216\u8bb8\u53ef\u8bc1\u5408\u89c4\u7b49\u56e0\u7d20\u65f6\u3002", "method": "PTMPicker \u901a\u8fc7\u5b9a\u4e49\u7ed3\u6784\u5316\u6a21\u677f\u8868\u793aPTM\u548c\u7528\u6237\u9700\u6c42\uff0c\u8ba1\u7b97\u529f\u80fd\u76f8\u5173\u5c5e\u6027\u7684\u5d4c\u5165\u76f8\u4f3c\u5ea6\uff0c\u5e76\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bc4\u4f30\u7279\u6b8a\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPTMPicker \u5728\u5408\u6210\u7684\u6a21\u578b\u641c\u7d22\u8bf7\u6c42\u4e0a\u8868\u73b0\u826f\u597d\uff0c85%\u7684\u8bf7\u6c42\u5728\u524d10\u4e2a\u5019\u9009\u6a21\u578b\u4e2d\u6210\u529f\u5339\u914d\u3002", "conclusion": "PTMPicker \u80fd\u591f\u6709\u6548\u5e2e\u52a9\u7528\u6237\u9009\u62e9\u5408\u9002\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c85%\u7684\u8bf7\u6c42\u5728\u524d10\u4e2a\u5019\u9009\u6a21\u578b\u4e2d\u6210\u529f\u627e\u5230\u5408\u9002\u7684PTM\u3002"}}
{"id": "2508.11093", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11093", "abs": "https://arxiv.org/abs/2508.11093", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "title": "Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance", "comment": "Accepted at Human-Centered Robot Autonomy for Human-Robot Teams\n  (HuRoboT) at IEEE RO-MAN 2025, Eindhoven, the Netherlands", "summary": "Human-robot collaboration requires robots to quickly infer user intent,\nprovide transparent reasoning, and assist users in achieving their goals. Our\nrecent work introduced GUIDER, our framework for inferring navigation and\nmanipulation intents. We propose augmenting GUIDER with a vision-language model\n(VLM) and a text-only language model (LLM) to form a semantic prior that\nfilters objects and locations based on the mission prompt. A vision pipeline\n(YOLO for object detection and the Segment Anything Model for instance\nsegmentation) feeds candidate object crops into the VLM, which scores their\nrelevance given an operator prompt; in addition, the list of detected object\nlabels is ranked by a text-only LLM. These scores weight the existing\nnavigation and manipulation layers of GUIDER, selecting context-relevant\ntargets while suppressing unrelated objects. Once the combined belief exceeds a\nthreshold, autonomy changes occur, enabling the robot to navigate to the\ndesired area and retrieve the desired object, while adapting to any changes in\nthe operator's intent. Future work will evaluate the system on Isaac Sim using\na Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u589e\u5f3aGUIDER\u6846\u67b6\uff0c\u7ed3\u5408VLM\u548cLLM\u5f62\u6210\u8bed\u4e49\u5148\u9a8c\uff0c\u4f18\u5316\u673a\u5668\u4eba\u5bfc\u822a\u548c\u64cd\u4f5c\u80fd\u529b\uff0c\u4ee5\u9002\u5e94\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5b9e\u65f6\u610f\u56fe\u53d8\u5316\u3002", "motivation": "\u4eba\u673a\u534f\u4f5c\u9700\u8981\u673a\u5668\u4eba\u5feb\u901f\u63a8\u65ad\u7528\u6237\u610f\u56fe\u3001\u63d0\u4f9b\u900f\u660e\u63a8\u7406\u5e76\u534f\u52a9\u7528\u6237\u5b9e\u73b0\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u7eaf\u6587\u672c\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u589e\u5f3aGUIDER\u6846\u67b6\uff0c\u5f62\u6210\u8bed\u4e49\u5148\u9a8c\uff0c\u7b5b\u9009\u57fa\u4e8e\u4efb\u52a1\u63d0\u793a\u7684\u5bf9\u8c61\u548c\u4f4d\u7f6e\u3002\u89c6\u89c9\u6d41\u7a0b\uff08YOLO\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\uff0cSegment Anything Model\u7528\u4e8e\u5b9e\u4f8b\u5206\u5272\uff09\u5c06\u5019\u9009\u5bf9\u8c61\u8f93\u5165VLM\uff0cVLM\u6839\u636e\u64cd\u4f5c\u5458\u63d0\u793a\u8bc4\u5206\u76f8\u5173\u6027\uff1b\u540c\u65f6\uff0c\u7eaf\u6587\u672cLLM\u5bf9\u68c0\u6d4b\u5230\u7684\u5bf9\u8c61\u6807\u7b7e\u8fdb\u884c\u6392\u540d\u3002\u8fd9\u4e9b\u5206\u6570\u52a0\u6743GUIDER\u7684\u5bfc\u822a\u548c\u64cd\u4f5c\u5c42\uff0c\u9009\u62e9\u4e0a\u4e0b\u6587\u76f8\u5173\u76ee\u6807\u5e76\u6291\u5236\u65e0\u5173\u5bf9\u8c61\u3002", "result": "\u4e00\u65e6\u7efc\u5408\u4fe1\u5ff5\u8d85\u8fc7\u9608\u503c\uff0c\u81ea\u4e3b\u6027\u53d8\u5316\u53d1\u751f\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5bfc\u822a\u5230\u76ee\u6807\u533a\u57df\u5e76\u68c0\u7d22\u6240\u9700\u5bf9\u8c61\uff0c\u540c\u65f6\u9002\u5e94\u64cd\u4f5c\u5458\u610f\u56fe\u7684\u4efb\u4f55\u53d8\u5316\u3002", "conclusion": "\u672a\u6765\u5de5\u4f5c\u5c06\u5728Isaac Sim\u4e2d\u4f7f\u7528Franka Emika\u673a\u68b0\u81c2\u548cRidgeback\u5e95\u5ea7\u8bc4\u4f30\u7cfb\u7edf\uff0c\u91cd\u70b9\u5173\u6ce8\u5b9e\u65f6\u8f85\u52a9\u529f\u80fd\u3002"}}
{"id": "2508.11252", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11252", "abs": "https://arxiv.org/abs/2508.11252", "authors": ["Youcheng Huang", "Bowen Qin", "Chen Huang", "Duanyu Feng", "Xi Yang", "Wenqiang Lei"], "title": "Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information", "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving\nabilities in mathematics, as evaluated by existing benchmarks exclusively on\nwell-defined problems. However, such evaluation setup constitutes a critical\ngap, since a genuine intelligent agent should not only solve problems (as a\nmath quiz solver), but also be able~to ask for information when the problems\nlack sufficient information, enabling proactivity in responding users'\nrequests. To bridge such gap, we proposes a new dataset consisting of two types\nof incomplete problems with diverse contexts. Based on the dataset, our\nsystematical evaluation of LRMs reveals their inability in proactively asking\nfor information. In addition, we uncover the behaviors related to overthinking\nand hallucination of LRMs, and highlight the potential and challenges of\nsupervised fine-tuning in learning such ability. We hope to provide new\ninsights in developing LRMs with genuine intelligence, rather than just solving\nproblems.", "AI": {"tldr": "LRMs\u5728\u4fe1\u606f\u4e0d\u8db3\u65f6\u65e0\u6cd5\u4e3b\u52a8\u8be2\u95ee\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u5176\u5c40\u9650\u6027\u5e76\u63a2\u8ba8\u4e86\u76d1\u7763\u5fae\u8c03\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4ec5\u8bc4\u4f30LRMs\u5728\u5b9a\u4e49\u826f\u597d\u7684\u95ee\u9898\u4e0a\u7684\u89e3\u51b3\u80fd\u529b\uff0c\u800c\u5ffd\u7565\u4e86\u5176\u5728\u4fe1\u606f\u4e0d\u8db3\u65f6\u4e3b\u52a8\u8be2\u95ee\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e24\u79cd\u7c7b\u578b\u7684\u4e0d\u5b8c\u6574\u95ee\u9898\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86LRMs\u5728\u8fd9\u4e9b\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0LRMs\u5728\u4e3b\u52a8\u8be2\u95ee\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u8fc7\u5ea6\u601d\u8003\u548c\u5e7b\u89c9\u884c\u4e3a\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u5f00\u53d1\u5177\u6709\u771f\u6b63\u667a\u80fd\u7684LRMs\u7684\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002"}}
{"id": "2508.11222", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11222", "abs": "https://arxiv.org/abs/2508.11222", "authors": ["Haonan Zhang", "Dongxia Wang", "Yi Liu", "Kexin Chen", "Jiashui Wang", "Xinlei Ying", "Long Liu", "Wenhai Wang"], "title": "ORFuzz: Fuzzing the \"Other Side\" of LLM Safety -- Testing Over-Refusal", "comment": null, "summary": "Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously\nrejecting benign queries due to overly conservative safety measures - a\ncritical functional flaw that undermines their reliability and usability.\nCurrent methods for testing this behavior are demonstrably inadequate,\nsuffering from flawed benchmarks and limited test generation capabilities, as\nhighlighted by our empirical user study. To the best of our knowledge, this\npaper introduces the first evolutionary testing framework, ORFuzz, for the\nsystematic detection and analysis of LLM over-refusals. ORFuzz uniquely\nintegrates three core components: (1) safety category-aware seed selection for\ncomprehensive test coverage, (2) adaptive mutator optimization using reasoning\nLLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge\nmodel validated to accurately reflect user perception of toxicity and refusal.\nOur extensive evaluations demonstrate that ORFuzz generates diverse, validated\nover-refusal instances at a rate (6.98% average) more than double that of\nleading baselines, effectively uncovering vulnerabilities. Furthermore,\nORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly\ntransferable test cases that achieves a superior 63.56% average over-refusal\nrate across 10 diverse LLMs, significantly outperforming existing datasets.\nORFuzz and ORFuzzSet provide a robust automated testing framework and a\nvaluable community resource, paving the way for developing more reliable and\ntrustworthy LLM-based software systems.", "AI": {"tldr": "ORFuzz\u662f\u9996\u4e2a\u7528\u4e8e\u7cfb\u7edf\u68c0\u6d4b\u548c\u5206\u6790LLM\u8fc7\u5ea6\u62d2\u7edd\u7684\u8fdb\u5316\u6d4b\u8bd5\u6846\u67b6\uff0c\u5176\u751f\u6210\u7684\u6d4b\u8bd5\u6848\u4f8b\u548c\u57fa\u51c6\u96c6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u63d0\u5347LLM\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65e5\u76ca\u8868\u73b0\u51fa\u8fc7\u5ea6\u62d2\u7edd\u884c\u4e3a\uff0c\u5373\u9519\u8bef\u62d2\u7edd\u826f\u6027\u67e5\u8be2\uff0c\u8fd9\u4e25\u91cd\u524a\u5f31\u4e86\u5176\u53ef\u9760\u6027\u548c\u53ef\u7528\u6027\u3002\u5f53\u524d\u6d4b\u8bd5\u65b9\u6cd5\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "ORFuzz\u6574\u5408\u4e86\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u5b89\u5168\u7c7b\u522b\u611f\u77e5\u7684\u79cd\u5b50\u9009\u62e9\uff0c(2) \u4f7f\u7528\u63a8\u7406LLM\u8fdb\u884c\u81ea\u9002\u5e94\u53d8\u5f02\u5668\u4f18\u5316\uff0c(3) OR-Judge\uff0c\u4e00\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u4eba\u7c7b\u5bf9\u9f50\u5224\u65ad\u6a21\u578b\u3002", "result": "ORFuzz\u751f\u6210\u7684\u591a\u6837\u5316\u3001\u9a8c\u8bc1\u8fc7\u7684\u8fc7\u5ea6\u62d2\u7edd\u5b9e\u4f8b\u7684\u5e73\u5747\u7387\u4e3a6.98%\uff0c\u662f\u9886\u5148\u57fa\u51c6\u7684\u4e24\u500d\u4ee5\u4e0a\u3002ORFuzzSet\u5305\u542b1,855\u4e2a\u9ad8\u5ea6\u53ef\u8f6c\u79fb\u7684\u6d4b\u8bd5\u6848\u4f8b\uff0c\u5e73\u5747\u8fc7\u5ea6\u62d2\u7edd\u7387\u4e3a63.56%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u3002", "conclusion": "ORFuzz\u548cORFuzzSet\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u548c\u53ef\u4fe1\u8d56\u7684\u57fa\u4e8eLLM\u7684\u8f6f\u4ef6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u6846\u67b6\u548c\u5b9d\u8d35\u7684\u793e\u533a\u8d44\u6e90\u3002"}}
{"id": "2508.11117", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11117", "abs": "https://arxiv.org/abs/2508.11117", "authors": ["Xuning Yang", "Clemens Eppner", "Jonathan Tremblay", "Dieter Fox", "Stan Birchfield", "Fabio Ramos"], "title": "Robot Policy Evaluation for Sim-to-Real Transfer: A Benchmarking Perspective", "comment": "2025 Robot: Science and Systems (RSS) Workshop on Robot Evaluation\n  for the Real World", "summary": "Current vision-based robotics simulation benchmarks have significantly\nadvanced robotic manipulation research. However, robotics is fundamentally a\nreal-world problem, and evaluation for real-world applications has lagged\nbehind in evaluating generalist policies. In this paper, we discuss challenges\nand desiderata in designing benchmarks for generalist robotic manipulation\npolicies for the goal of sim-to-real policy transfer. We propose 1) utilizing\nhigh visual-fidelity simulation for improved sim-to-real transfer, 2)\nevaluating policies by systematically increasing task complexity and scenario\nperturbation to assess robustness, and 3) quantifying performance alignment\nbetween real-world performance and its simulation counterparts.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u57fa\u51c6\u8bbe\u8ba1\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u6a21\u62df\u3001\u4efb\u52a1\u590d\u6742\u6027\u8bc4\u4f30\u548c\u6027\u80fd\u5bf9\u9f50\u91cf\u5316\u7b49\u6539\u8fdb\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u6a21\u62df\u57fa\u51c6\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u7814\u7a76\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u673a\u5668\u4eba\u672c\u8d28\u4e0a\u662f\u73b0\u5b9e\u4e16\u754c\u7684\u95ee\u9898\uff0c\u901a\u7528\u7b56\u7565\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u8bc4\u4f30\u6ede\u540e\u3002", "method": "1) \u5229\u7528\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u6a21\u62df\u6539\u8fdb\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\uff1b2) \u901a\u8fc7\u7cfb\u7edf\u589e\u52a0\u4efb\u52a1\u590d\u6742\u6027\u548c\u573a\u666f\u6270\u52a8\u8bc4\u4f30\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff1b3) \u91cf\u5316\u73b0\u5b9e\u4e16\u754c\u6027\u80fd\u4e0e\u5176\u6a21\u62df\u5bf9\u5e94\u7269\u4e4b\u95f4\u7684\u6027\u80fd\u5bf9\u9f50\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u6539\u8fdb\u6a21\u62df\u5230\u73b0\u5b9e\u7b56\u7565\u8f6c\u79fb\u7684\u57fa\u51c6\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5efa\u8bae\u901a\u8fc7\u9ad8\u4fdd\u771f\u6a21\u62df\u3001\u4efb\u52a1\u590d\u6742\u6027\u589e\u52a0\u548c\u6027\u80fd\u5bf9\u9f50\u91cf\u5316\u6765\u8bc4\u4f30\u7b56\u7565\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u8bbe\u8ba1\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u57fa\u51c6\u7684\u6311\u6218\u548c\u9700\u6c42\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u5177\u4f53\u5efa\u8bae\u4ee5\u6539\u8fdb\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u7b56\u7565\u8f6c\u79fb\u6548\u679c\u3002"}}
{"id": "2508.10935", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10935", "abs": "https://arxiv.org/abs/2508.10935", "authors": ["Qi Liu", "Yabei Li", "Hongsong Wang", "Lei He"], "title": "HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model", "comment": null, "summary": "Traditional closed-set 3D detection frameworks fail to meet the demands of\nopen-world applications like autonomous driving. Existing open-vocabulary 3D\ndetection methods typically adopt a two-stage pipeline consisting of\npseudo-label generation followed by semantic alignment. While vision-language\nmodels (VLMs) recently have dramatically improved the semantic accuracy of\npseudo-labels, their geometric quality, particularly bounding box precision,\nremains commonly neglected.To address this issue, we propose a High Box Quality\nOpen-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and\nrefine high-quality pseudo-labels for open-vocabulary classes. The framework\ncomprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal\nGenerator that utilizes cross-modality geometric consistency to generate\nhigh-quality initial 3D proposals, and an Annotated-Class Assisted (ACA)\nDenoiser that progressively refines 3D proposals by leveraging geometric priors\nfrom annotated categories through a DDIM-based denoising mechanism.Compared to\nthe state-of-the-art method, training with pseudo-labels generated by our\napproach achieves a 7.37% improvement in mAP on novel classes, demonstrating\nthe superior quality of the pseudo-labels produced by our framework. HQ-OV3D\ncan serve not only as a strong standalone open-vocabulary 3D detector but also\nas a plug-in high-quality pseudo-label generator for existing open-vocabulary\ndetection or annotation pipelines.", "AI": {"tldr": "HQ-OV3D\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u751f\u6210\u548c\u7cbe\u70bc\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u7684\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u53bb\u566a\u673a\u5236\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5c01\u95ed\u5f0f3D\u68c0\u6d4b\u6846\u67b6\u65e0\u6cd5\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u7b49\u5f00\u653e\u4e16\u754c\u5e94\u7528\u7684\u9700\u6c42\uff0c\u73b0\u6709\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u65b9\u6cd5\u7684\u51e0\u4f55\u8d28\u91cf\uff08\u5c24\u5176\u662f\u8fb9\u754c\u6846\u7cbe\u5ea6\uff09\u5e38\u88ab\u5ffd\u89c6\u3002", "method": "HQ-OV3D\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1aIntra-Modality Cross-Validated (IMCV) Proposal Generator\u548cAnnotated-Class Assisted (ACA) Denoiser\uff0c\u524d\u8005\u5229\u7528\u8de8\u6a21\u6001\u51e0\u4f55\u4e00\u81f4\u6027\u751f\u6210\u9ad8\u8d28\u91cf\u521d\u59cb3D\u63d0\u6848\uff0c\u540e\u8005\u901a\u8fc7\u57fa\u4e8eDDIM\u7684\u53bb\u566a\u673a\u5236\u9010\u6b65\u7cbe\u70bc3D\u63d0\u6848\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f7f\u7528HQ-OV3D\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u8bad\u7ec3\u5728\u65b0\u7c7b\u522b\u4e0a\u5b9e\u73b0\u4e867.37%\u7684mAP\u63d0\u5347\u3002", "conclusion": "HQ-OV3D\u6846\u67b6\u4e0d\u4ec5\u4f5c\u4e3a\u72ec\u7acb\u7684\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u5668\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u751f\u6210\u5668\uff0c\u9002\u7528\u4e8e\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u6216\u6ce8\u91ca\u6d41\u7a0b\u3002"}}
{"id": "2508.11347", "categories": ["cs.AI", "cs.LG", "I.2.4; I.2.6; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.11347", "abs": "https://arxiv.org/abs/2508.11347", "authors": ["Yifei Li", "Lingling Zhang", "Hang Yan", "Tianzhe Zhao", "Zihan Ma", "Muye Huang", "Jun Liu"], "title": "SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding", "comment": "10 pages, 5 figures, Accepted at KDD 2025, code available at\n  https://github.com/lyfxjtu/Dynamic-Embedding", "summary": "Traditional knowledge graph (KG) embedding methods aim to represent entities\nand relations in a low-dimensional space, primarily focusing on static graphs.\nHowever, real-world KGs are dynamically evolving with the constant addition of\nentities, relations and facts. To address such dynamic nature of KGs, several\ncontinual knowledge graph embedding (CKGE) methods have been developed to\nefficiently update KG embeddings to accommodate new facts while maintaining\nlearned knowledge. As KGs grow at different rates and scales in real-world\nscenarios, existing CKGE methods often fail to consider the varying scales of\nupdates and lack systematic evaluation throughout the entire update process. In\nthis paper, we propose SAGE, a scale-aware gradual evolution framework for\nCKGE. Specifically, SAGE firstly determine the embedding dimensions based on\nthe update scales and expand the embedding space accordingly. The Dynamic\nDistillation mechanism is further employed to balance the preservation of\nlearned knowledge and the incorporation of new facts. We conduct extensive\nexperiments on seven benchmarks, and the results show that SAGE consistently\noutperforms existing baselines, with a notable improvement of 1.38% in MRR,\n1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with\nmethods using fixed embedding dimensions show that SAGE achieves optimal\nperformance on every snapshot, demonstrating the importance of adaptive\nembedding dimensions in CKGE. The codes of SAGE are publicly available at:\nhttps://github.com/lyfxjtu/Dynamic-Embedding.", "AI": {"tldr": "SAGE\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u5d4c\u5165\u7ef4\u5ea6\u7684\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u84b8\u998f\u673a\u5236\u6709\u6548\u5e73\u8861\u65b0\u65e7\u77e5\u8bc6\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfKG\u5d4c\u5165\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u56fe\uff0c\u800c\u73b0\u5b9eKG\u662f\u52a8\u6001\u6f14\u5316\u7684\uff0c\u73b0\u6709CKGE\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u66f4\u65b0\u89c4\u6a21\u7684\u53d8\u5316\u3002", "method": "SAGE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u66f4\u65b0\u89c4\u6a21\u786e\u5b9a\u5d4c\u5165\u7ef4\u5ea6\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u84b8\u998f\u673a\u5236\u5e73\u8861\u65b0\u65e7\u77e5\u8bc6\u7684\u6574\u5408\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAGE\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0cMRR\u63d0\u53471.38%\uff0cH@1\u63d0\u53471.25%\uff0cH@10\u63d0\u53471.6%\u3002", "conclusion": "SAGE\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u5d4c\u5165\u7ef4\u5ea6\u5728CKGE\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5728\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u4e2d\u8003\u8651\u66f4\u65b0\u89c4\u6a21\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.11257", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11257", "abs": "https://arxiv.org/abs/2508.11257", "authors": ["Marc Pavel", "Nenad Petrovic", "Lukasz Mazur", "Vahid Zolfaghari", "Fengjunjie Pan", "Alois Knoll"], "title": "Hallucination in LLM-Based Code Generation: An Automotive Case Study", "comment": null, "summary": "Large Language Models (LLMs) have shown significant potential in automating\ncode generation tasks offering new opportunities across software engineering\ndomains. However, their practical application remains limited due to\nhallucinations - outputs that appear plausible but are factually incorrect,\nunverifiable or nonsensical. This paper investigates hallucination phenomena in\nthe context of code generation with a specific focus on the automotive domain.\nA case study is presented that evaluates multiple code LLMs for three different\nprompting complexities ranging from a minimal one-liner prompt to a prompt with\nCovesa Vehicle Signal Specifications (VSS) as additional context and finally to\na prompt with an additional code skeleton. The evaluation reveals a high\nfrequency of syntax violations, invalid reference errors and API knowledge\nconflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the\nevaluated models, only GPT-4.1 and GPT-4o were able to produce a correct\nsolution when given the most context-rich prompt. Simpler prompting strategies\nfailed to yield a working result, even after multiple refinement iterations.\nThese findings highlight the need for effective mitigation techniques to ensure\nthe safe and reliable use of LLM generated code, especially in safety-critical\ndomains such as automotive software systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u53d1\u73b0\u5373\u4f7f\u5148\u8fdb\u6a21\u578b\u4e5f\u5e38\u51fa\u9519\uff0c\u9700\u66f4\u591a\u4e0a\u4e0b\u6587\u624d\u80fd\u6b63\u786e\u751f\u6210\u4ee3\u7801\uff0c\u5f3a\u8c03\u5b89\u5168\u5173\u952e\u9886\u57df\u9700\u6709\u6548\u7f13\u89e3\u6280\u672f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u5e7b\u89c9\u73b0\u8c61\uff08\u8f93\u51fa\u770b\u4f3c\u5408\u7406\u4f46\u5b9e\u9645\u9519\u8bef\u3001\u4e0d\u53ef\u9a8c\u8bc1\u6216\u65e0\u610f\u4e49\uff09\uff0c\u5176\u5b9e\u9645\u5e94\u7528\u53d7\u5230\u9650\u5236\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u7279\u522b\u662f\u5728\u6c7d\u8f66\u9886\u57df\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u4ee3\u7801LLM\u5728\u4e0d\u540c\u63d0\u793a\u590d\u6742\u5ea6\u4e0b\u7684\u8868\u73b0\uff0c\u4ece\u7b80\u5355\u7684\u4e00\u884c\u63d0\u793a\u5230\u5305\u542bCovesa\u8f66\u8f86\u4fe1\u53f7\u89c4\u8303\uff08VSS\uff09\u548c\u4ee3\u7801\u9aa8\u67b6\u7684\u4e30\u5bcc\u4e0a\u4e0b\u6587\u63d0\u793a\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u6a21\u578b\uff08\u5982GPT-4.1\u3001Codex\u548cGPT-4o\uff09\u4e5f\u9891\u7e41\u51fa\u73b0\u8bed\u6cd5\u8fdd\u89c4\u3001\u65e0\u6548\u5f15\u7528\u9519\u8bef\u548cAPI\u77e5\u8bc6\u51b2\u7a81\u3002\u53ea\u6709\u5728\u63d0\u4f9b\u6700\u4e30\u5bcc\u4e0a\u4e0b\u6587\u7684\u63d0\u793a\u65f6\uff0cGPT-4.1\u548cGPT-4o\u624d\u80fd\u751f\u6210\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982\u6c7d\u8f66\u8f6f\u4ef6\u7cfb\u7edf\uff09\u4e2d\uff0c\u9700\u8981\u6709\u6548\u7684\u7f13\u89e3\u6280\u672f\u4ee5\u786e\u4fddLLM\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u53ef\u9760\u4f7f\u7528\u3002"}}
{"id": "2508.11129", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11129", "abs": "https://arxiv.org/abs/2508.11129", "authors": ["Ryan M. Bena", "Gilbert Bahati", "Blake Werner", "Ryan K. Cosner", "Lizhi Yang", "Aaron D. Ames"], "title": "Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC", "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots", "summary": "Autonomous navigation through unstructured and dynamically-changing\nenvironments is a complex task that continues to present many challenges for\nmodern roboticists. In particular, legged robots typically possess manipulable\nasymmetric geometries which must be considered during safety-critical\ntrajectory planning. This work proposes a predictive safety filter: a nonlinear\nmodel predictive control (MPC) algorithm for online trajectory generation with\ngeometry-aware safety constraints based on control barrier functions (CBFs).\nCritically, our method leverages Poisson safety functions to numerically\nsynthesize CBF constraints directly from perception data. We extend the\ntheoretical framework for Poisson safety functions to incorporate temporal\nchanges in the domain by reformulating the static Dirichlet problem for\nPoisson's equation as a parameterized moving boundary value problem.\nFurthermore, we employ Minkowski set operations to lift the domain into a\nconfiguration space that accounts for robot geometry. Finally, we implement our\nreal-time predictive safety filter on humanoid and quadruped robots in various\nsafety-critical scenarios. The results highlight the versatility of Poisson\nsafety functions, as well as the benefit of CBF constrained model predictive\nsafety-critical controllers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePoisson\u5b89\u5168\u51fd\u6570\u548cCBFs\u7684MPC\u7b97\u6cd5\uff0c\u7528\u4e8e\u817f\u5f0f\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b89\u5168\u8f68\u8ff9\u89c4\u5212\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u548c\u52a8\u6001\u53d8\u5316\u73af\u5883\u4e2d\u817f\u5f0f\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\u7684\u5b89\u5168\u5173\u952e\u95ee\u9898\uff0c\u7279\u522b\u662f\u5904\u7406\u5176\u53ef\u64cd\u7eb5\u7684\u975e\u5bf9\u79f0\u51e0\u4f55\u5f62\u72b6\u3002", "method": "\u901a\u8fc7\u5c06\u9759\u6001Dirichlet\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u53c2\u6570\u5316\u7684\u79fb\u52a8\u8fb9\u754c\u503c\u95ee\u9898\uff0c\u6269\u5c55\u4e86Poisson\u5b89\u5168\u51fd\u6570\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u5229\u7528Minkowski\u96c6\u5408\u64cd\u4f5c\u5c06\u57df\u63d0\u5347\u5230\u8003\u8651\u673a\u5668\u4eba\u51e0\u4f55\u5f62\u72b6\u7684\u914d\u7f6e\u7a7a\u95f4\u3002", "result": "\u5728\u591a\u79cd\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u6210\u529f\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9884\u6d4b\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\u7684\u51e0\u4f55\u611f\u77e5\u5b89\u5168\u7ea6\u675f\u7684\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7b97\u6cd5\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u4eba\u5f62\u548c\u56db\u8db3\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u9a8c\u8bc1\u4e86Poisson\u5b89\u5168\u51fd\u6570\u7684\u901a\u7528\u6027\u548cCBF\u7ea6\u675f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.10936", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10936", "abs": "https://arxiv.org/abs/2508.10936", "authors": ["Cheng Chen", "Hao Huang", "Saurabh Bagchi"], "title": "Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction", "comment": null, "summary": "Collaborative perception enables connected vehicles to share information,\novercoming occlusions and extending the limited sensing range inherent in\nsingle-agent (non-collaborative) systems. Existing vision-only methods for 3D\nsemantic occupancy prediction commonly rely on dense 3D voxels, which incur\nhigh communication costs, or 2D planar features, which require accurate depth\nestimation or additional supervision, limiting their applicability to\ncollaborative scenarios. To address these challenges, we propose the first\napproach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D\nsemantic occupancy prediction. By sharing and fusing intermediate Gaussian\nprimitives, our method provides three benefits: a neighborhood-based\ncross-agent fusion that removes duplicates and suppresses noisy or inconsistent\nGaussians; a joint encoding of geometry and semantics in each primitive, which\nreduces reliance on depth supervision and allows simple rigid alignment; and\nsparse, object-centric messages that preserve structural information while\nreducing communication volume. Extensive experiments demonstrate that our\napproach outperforms single-agent perception and baseline collaborative methods\nby +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU,\nrespectively. When further reducing the number of transmitted Gaussians, our\nmethod still achieves a +1.9 improvement in mIoU, using only 34.6%\ncommunication volume, highlighting robust performance under limited\ncommunication budgets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f3D\u8bed\u4e49\u9ad8\u65af\u6cfc\u6e85\u7684\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u5bc6\u96c63D\u4f53\u7d20\u62162D\u5e73\u9762\u7279\u5f81\u7684\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\u5728\u901a\u4fe1\u6210\u672c\u3001\u6df1\u5ea6\u4f30\u8ba1\u6216\u989d\u5916\u76d1\u7763\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u7a00\u758f3D\u8bed\u4e49\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u901a\u8fc7\u5171\u4eab\u548c\u878d\u5408\u4e2d\u95f4\u9ad8\u65af\u57fa\u5143\uff0c\u5b9e\u73b0\u4e86\u8de8\u4ee3\u7406\u7684\u90bb\u5c45\u878d\u5408\u3001\u51e0\u4f55\u4e0e\u8bed\u4e49\u7684\u8054\u5408\u7f16\u7801\u4ee5\u53ca\u7a00\u758f\u7684\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6d88\u606f\u4f20\u9012\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728mIoU\u548cIoU\u4e0a\u5206\u522b\u6bd4\u5355\u4ee3\u7406\u611f\u77e5\u548c\u57fa\u7ebf\u534f\u4f5c\u65b9\u6cd5\u63d0\u5347\u4e86+8.42/+3.28\u548c+5.11/+22.41\u5206\uff0c\u4e14\u5728\u51cf\u5c11\u4f20\u8f93\u9ad8\u65af\u6570\u91cf\u65f6\u4ecd\u80fd\u4fdd\u6301\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u7a00\u758f3D\u8bed\u4e49\u9ad8\u65af\u6cfc\u6e85\u7684\u534f\u4f5c3D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\uff0c\u5728\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u6709\u9650\u901a\u4fe1\u9884\u7b97\u7684\u573a\u666f\u3002"}}
{"id": "2508.11360", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11360", "abs": "https://arxiv.org/abs/2508.11360", "authors": ["Songqin Nong", "Jingxuan Xu", "Sheng Zhou", "Jianfeng Chen", "Xiaoxuan Tang", "Tao Jiang", "Wenhao Xu"], "title": "CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks", "comment": null, "summary": "As autonomous agents become adept at understanding and interacting with\ngraphical user interface (GUI) environments, a new era of automated task\nexecution is emerging. Recent studies have demonstrated that Reinforcement\nLearning (RL) can effectively enhance agents' performance in dynamic\ninteractive GUI environments. However, these methods face two key limitations:\n(1) they overlook the significant variation in difficulty across different GUI\ntasks by treating the entire training data as a uniform set, which hampers the\nagent's ability to adapt its learning process; and (2) most approaches collapse\ntask-specific nuances into a single, coarse reward, leaving the agent with a\nuniform signal that yields inefficient policy updates. To address these\nlimitations, we propose CRAFT-GUI, a curriculum learning framework based on\nGroup Relative Policy Optimization (GRPO) that explicitly accounts for the\nvarying difficulty across trajectories. To enable more fine-grained policy\noptimization, we design a reward function that combines simple rule-based\nsignals with model-judged evaluation, providing richer and more nuanced\nfeedback during training. Experimental results demonstrate that our method\nachieves significant improvements over previous state-of-the-art approaches,\noutperforming them by 5.6% on public benchmarks Android Control and 10.3% on\nour internal online benchmarks, respectively. These findings empirically\nvalidate the effectiveness of integrating reinforcement learning with\ncurriculum learning in GUI interaction tasks.", "AI": {"tldr": "CRAFT-GUI\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u548c\u7cbe\u7ec6\u5316\u5956\u52b1\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86GUI\u4efb\u52a1\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6GUI\u4efb\u52a1\u96be\u5ea6\u7684\u5dee\u5f02\uff0c\u4e14\u5956\u52b1\u4fe1\u53f7\u8fc7\u4e8e\u7c97\u7cd9\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eGRPO\u7684\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6CRAFT-GUI\uff0c\u8bbe\u8ba1\u4e86\u7ed3\u5408\u89c4\u5219\u4fe1\u53f7\u548c\u6a21\u578b\u8bc4\u4f30\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6Android Control\u4e0a\u63d0\u53475.6%\uff0c\u5185\u90e8\u57fa\u51c6\u4e0a\u63d0\u534710.3%\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5728GUI\u4ea4\u4e92\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11305", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11305", "abs": "https://arxiv.org/abs/2508.11305", "authors": ["Xin Wang", "Zhenhao Li", "Zishuo Ding"], "title": "Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and Reasoning", "comment": null, "summary": "Logging code is written by developers to capture system runtime behavior and\nplays a vital role in debugging, performance analysis, and system monitoring.\nHowever, defects in logging code can undermine the usefulness of logs and lead\nto misinterpretations. Although prior work has identified several logging\ndefect patterns and provided valuable insights into logging practices, these\nstudies often focus on a narrow range of defect patterns derived from limited\nsources (e.g., commit histories) and lack a systematic and comprehensive\nanalysis. Moreover, large language models (LLMs) have demonstrated promising\ngeneralization and reasoning capabilities across a variety of code-related\ntasks, yet their potential for detecting logging code defects remains largely\nunexplored.\n  In this paper, we derive a comprehensive taxonomy of logging code defects,\nwhich encompasses seven logging code defect patterns with 14 detailed\nscenarios. We further construct a benchmark dataset, \\dataset, consisting of\n164 developer-verified real-world logging defects. Then we propose an automated\nframework that leverages various prompting strategies and contextual\ninformation to evaluate LLMs' capability in detecting and reasoning logging\ncode defects. Experimental results reveal that LLMs generally struggle to\naccurately detect and reason logging code defects based on the source code\nonly. However, incorporating proper knowledge (e.g., detailed scenarios of\ndefect patterns) can lead to 10.9\\% improvement in detection accuracy. Overall,\nour findings provide actionable guidance for practitioners to avoid common\ndefect patterns and establish a foundation for improving LLM-based reasoning in\nlogging code defect detection.", "AI": {"tldr": "\u7814\u7a76\u6784\u5efa\u4e86\u5168\u9762\u7684\u65e5\u5fd7\u4ee3\u7801\u7f3a\u9677\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86LLMs\u5728\u68c0\u6d4b\u8fd9\u4e9b\u7f3a\u9677\u65f6\u7684\u8868\u73b0\u3002\u7ed3\u679c\u8868\u660e\uff0cLLMs\u5728\u7f3a\u4e4f\u80cc\u666f\u77e5\u8bc6\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u6574\u5408\u9002\u5f53\u77e5\u8bc6\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u65e5\u5fd7\u4ee3\u7801\u5728\u7cfb\u7edf\u8c03\u8bd5\u3001\u6027\u80fd\u5206\u6790\u548c\u76d1\u63a7\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u7f3a\u9677\u53ef\u80fd\u5bfc\u81f4\u65e5\u5fd7\u5931\u6548\u6216\u8bef\u8bfb\u3002\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u8bc6\u522b\u4e86\u4e00\u4e9b\u65e5\u5fd7\u7f3a\u9677\u6a21\u5f0f\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u548c\u5168\u9762\u6027\u3002\u6b64\u5916\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u65e5\u5fd7\u4ee3\u7801\u7f3a\u9677\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u9996\u5148\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u65e5\u5fd7\u4ee3\u7801\u7f3a\u9677\u5206\u7c7b\u6cd5\uff0c\u5305\u542b7\u79cd\u7f3a\u9677\u6a21\u5f0f\u548c14\u79cd\u8be6\u7ec6\u573a\u666f\u3002\u968f\u540e\uff0c\u7814\u7a76\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b164\u4e2a\u5f00\u53d1\u8005\u9a8c\u8bc1\u7684\u771f\u5b9e\u65e5\u5fd7\u7f3a\u9677\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff08\\dataset\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5229\u7528\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u8bc4\u4f30LLMs\u5728\u68c0\u6d4b\u548c\u63a8\u7406\u65e5\u5fd7\u4ee3\u7801\u7f3a\u9677\u65b9\u9762\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLLMs\u5728\u4ec5\u57fa\u4e8e\u6e90\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u666e\u904d\u96be\u4ee5\u51c6\u786e\u68c0\u6d4b\u548c\u63a8\u7406\u65e5\u5fd7\u4ee3\u7801\u7f3a\u9677\u3002\u7136\u800c\uff0c\u901a\u8fc7\u6574\u5408\u9002\u5f53\u7684\u80cc\u666f\u77e5\u8bc6\uff08\u5982\u7f3a\u9677\u6a21\u5f0f\u7684\u8be6\u7ec6\u573a\u666f\uff09\uff0c\u68c0\u6d4b\u51c6\u786e\u6027\u53ef\u63d0\u534710.9%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ec5\u57fa\u4e8e\u6e90\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u96be\u4ee5\u51c6\u786e\u68c0\u6d4b\u548c\u63a8\u7406\u65e5\u5fd7\u4ee3\u7801\u7f3a\u9677\uff0c\u4f46\u901a\u8fc7\u6574\u5408\u9002\u5f53\u7684\u80cc\u666f\u77e5\u8bc6\uff08\u5982\u7f3a\u9677\u6a21\u5f0f\u7684\u8be6\u7ec6\u573a\u666f\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u6027\uff0810.9%\uff09\u3002\u8fd9\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u907f\u514d\u5e38\u89c1\u7f3a\u9677\u6a21\u5f0f\u7684\u53ef\u64cd\u4f5c\u6307\u5bfc\uff0c\u5e76\u4e3a\u6539\u8fdb\u57fa\u4e8eLLM\u7684\u65e5\u5fd7\u4ee3\u7801\u7f3a\u9677\u68c0\u6d4b\u63a8\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.11143", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11143", "abs": "https://arxiv.org/abs/2508.11143", "authors": ["Jiarui Yang", "Bin Zhu", "Jingjing Chen", "Yu-Gang Jiang"], "title": "Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward", "comment": null, "summary": "Existing reinforcement learning (RL) methods struggle with long-horizon\nrobotic manipulation tasks, particularly those involving sparse rewards. While\naction chunking is a promising paradigm for robotic manipulation, using RL to\ndirectly learn continuous action chunks in a stable and data-efficient manner\nremains a critical challenge. This paper introduces AC3 (Actor-Critic for\nContinuous Chunks), a novel RL framework that learns to generate\nhigh-dimensional, continuous action sequences. To make this learning process\nstable and data-efficient, AC3 incorporates targeted stabilization mechanisms\nfor both the actor and the critic. First, to ensure reliable policy\nimprovement, the actor is trained with an asymmetric update rule, learning\nexclusively from successful trajectories. Second, to enable effective value\nlearning despite sparse rewards, the critic's update is stabilized using\nintra-chunk $n$-step returns and further enriched by a self-supervised module\nproviding intrinsic rewards at anchor points aligned with each action chunk. We\nconducted extensive experiments on 25 tasks from the BiGym and RLBench\nbenchmarks. Results show that by using only a few demonstrations and a simple\nmodel architecture, AC3 achieves superior success rates on most tasks,\nvalidating its effective design.", "AI": {"tldr": "AC3\u662f\u4e00\u79cd\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7a33\u5b9aactor\u548ccritic\u7684\u5b66\u4e60\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u4e0b\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\u5b66\u4e60\u7684\u96be\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7a00\u758f\u5956\u52b1\u7684\u957f\u671f\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u76f4\u63a5\u5b66\u4e60\u8fde\u7eed\u52a8\u4f5c\u5757\u5b58\u5728\u7a33\u5b9a\u6027\u548c\u6570\u636e\u6548\u7387\u7684\u6311\u6218\u3002", "method": "AC3\u91c7\u7528\u975e\u5bf9\u79f0\u66f4\u65b0\u89c4\u5219\u8bad\u7ec3actor\uff0c\u4ec5\u4ece\u6210\u529f\u8f68\u8ff9\u4e2d\u5b66\u4e60\uff0c\u5e76\u4f7f\u7528intra-chunk $n$-step returns\u548c\u81ea\u76d1\u7763\u6a21\u5757\u7a33\u5b9acritic\u7684\u66f4\u65b0\u3002", "result": "\u5728BiGym\u548cRLBench\u57fa\u51c6\u6d4b\u8bd5\u768425\u4e2a\u4efb\u52a1\u4e2d\uff0cAC3\u4ec5\u9700\u5c11\u91cf\u6f14\u793a\u548c\u7b80\u5355\u6a21\u578b\u67b6\u6784\u5373\u53ef\u5728\u591a\u6570\u4efb\u52a1\u4e0a\u53d6\u5f97\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002", "conclusion": "AC3\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u9488\u5bf9\u6027\u7684\u7a33\u5b9a\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u5b66\u4e60\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u671f\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2508.10937", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10937", "abs": "https://arxiv.org/abs/2508.10937", "authors": ["Jiarui Yang", "Hang Guo", "Wen Huang", "Tao Dai", "Shutao Xia"], "title": "Personalized Face Super-Resolution with Identity Decoupling and Fitting", "comment": null, "summary": "In recent years, face super-resolution (FSR) methods have achieved remarkable\nprogress, generally maintaining high image fidelity and identity (ID)\nconsistency under standard settings. However, in extreme degradation scenarios\n(e.g., scale $> 8\\times$), critical attributes and ID information are often\nseverely lost in the input image, making it difficult for conventional models\nto reconstruct realistic and ID-consistent faces. Existing methods tend to\ngenerate hallucinated faces under such conditions, producing restored images\nlacking authentic ID constraints. To address this challenge, we propose a novel\nFSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID\nrestoration under large scaling factors while mitigating hallucination effects.\nOur approach involves three key designs: 1) \\textbf{Masking} the facial region\nin the low-resolution (LR) image to eliminate unreliable ID cues; 2)\n\\textbf{Warping} a reference image to align with the LR input, providing style\nguidance; 3) Leveraging \\textbf{ID embeddings} extracted from ground truth (GT)\nimages for fine-grained ID modeling and personalized adaptation. We first\npretrain a diffusion-based model to explicitly decouple style and ID by forcing\nit to reconstruct masked LR face regions using both style and identity\nembeddings. Subsequently, we freeze most network parameters and perform\nlightweight fine-tuning of the ID embedding using a small set of target ID\nimages. This embedding encodes fine-grained facial attributes and precise ID\ninformation, significantly improving both ID consistency and perceptual\nquality. Extensive quantitative evaluations and visual comparisons demonstrate\nthat the proposed IDFSR substantially outperforms existing approaches under\nextreme degradation, particularly achieving superior performance on ID\nconsistency.", "AI": {"tldr": "IDFSR\u901a\u8fc7\u8eab\u4efd\u89e3\u8026\u548c\u62df\u5408\u6280\u672f\uff0c\u5728\u6781\u7aef\u9000\u5316\u573a\u666f\u4e0b\u63d0\u5347\u4e86\u9762\u90e8\u8d85\u5206\u8fa8\u7387\u7684ID\u4e00\u81f4\u6027\u548c\u611f\u77e5\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u6781\u7aef\u9000\u5316\u573a\u666f\uff08\u5982\u7f29\u653e\u6bd4\u4f8b>8\u500d\uff09\u4e0b\uff0c\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u91cd\u5efa\u771f\u5b9e\u4e14ID\u4e00\u81f4\u7684\u9762\u90e8\u56fe\u50cf\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u56fe\u50cf\u7f3a\u4e4f\u771f\u5b9e\u7684ID\u7ea6\u675f\u3002IDFSR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "IDFSR\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a1\uff09\u63a9\u7801\u4f4e\u5206\u8fa8\u7387\uff08LR\uff09\u56fe\u50cf\u4e2d\u7684\u9762\u90e8\u533a\u57df\u4ee5\u6d88\u9664\u4e0d\u53ef\u9760\u7684ID\u7ebf\u7d22\uff1b2\uff09\u5c06\u53c2\u8003\u56fe\u50cf\u5bf9\u9f50\u5230LR\u8f93\u5165\u4ee5\u63d0\u4f9b\u98ce\u683c\u6307\u5bfc\uff1b3\uff09\u5229\u7528\u4ece\u771f\u5b9e\uff08GT\uff09\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684ID\u5d4c\u5165\u8fdb\u884c\u7ec6\u7c92\u5ea6ID\u5efa\u6a21\u548c\u4e2a\u6027\u5316\u9002\u914d\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9a\u91cf\u8bc4\u4f30\u548c\u89c6\u89c9\u6bd4\u8f83\u8868\u660e\uff0cIDFSR\u5728\u6781\u7aef\u9000\u5316\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728ID\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684IDFSR\u65b9\u6cd5\u5728\u6781\u7aef\u9000\u5316\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u9762\u90e8\u8d85\u5206\u8fa8\u7387\uff08FSR\uff09\u7684\u8eab\u4efd\uff08ID\uff09\u4e00\u81f4\u6027\u548c\u611f\u77e5\u8d28\u91cf\uff0c\u901a\u8fc7\u8eab\u4efd\u89e3\u8026\u548c\u62df\u5408\u6280\u672f\u6709\u6548\u7f13\u89e3\u4e86\u5e7b\u89c9\u6548\u5e94\u3002"}}
{"id": "2508.11416", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11416", "abs": "https://arxiv.org/abs/2508.11416", "authors": ["Xuhua Zhao", "Yuxuan Xie", "Caihua Chen", "Yuxiang Sun"], "title": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager", "comment": null, "summary": "Recent advances in mathematical reasoning and the long-term planning\ncapabilities of large language models (LLMs) have precipitated the development\nof agents, which are being increasingly leveraged in business operations\nprocesses. Decision models to optimize inventory levels are one of the core\nelements of operations management. However, the capabilities of the LLM agent\nin making inventory decisions in uncertain contexts, as well as the\ndecision-making biases (e.g. framing effect, etc.) of the agent, remain largely\nunexplored. This prompts concerns regarding the capacity of LLM agents to\neffectively address real-world problems, as well as the potential implications\nof biases that may be present. To address this gap, we introduce AIM-Bench, a\nnovel benchmark designed to assess the decision-making behaviour of LLM agents\nin uncertain supply chain management scenarios through a diverse series of\ninventory replenishment experiments. Our results reveal that different LLMs\ntypically exhibit varying degrees of decision bias that are similar to those\nobserved in human beings. In addition, we explored strategies to mitigate the\npull-to-centre effect and the bullwhip effect, namely cognitive reflection and\nimplementation of information sharing. These findings underscore the need for\ncareful consideration of the potential biases in deploying LLMs in Inventory\ndecision-making scenarios. We hope that these insights will pave the way for\nmitigating human decision bias and developing human-centred decision support\nsystems for supply chains.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86LLM\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u4f9b\u5e94\u94fe\u7ba1\u7406\u4e2d\u7684\u51b3\u7b56\u884c\u4e3a\uff0c\u53d1\u73b0\u5176\u5b58\u5728\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u4e86\u51cf\u8f7b\u504f\u89c1\u7684\u7b56\u7565\uff0c\u5f3a\u8c03\u4e86\u5728\u5e93\u5b58\u51b3\u7b56\u4e2d\u8c28\u614e\u90e8\u7f72LLMs\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u6570\u5b66\u63a8\u7406\u548c\u957f\u671f\u89c4\u5212\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u505a\u51fa\u5e93\u5b58\u51b3\u7b56\u7684\u80fd\u529b\u53ca\u51b3\u7b56\u504f\u89c1\uff08\u5982\u6846\u67b6\u6548\u5e94\u7b49\uff09\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u5176\u89e3\u51b3\u73b0\u5b9e\u95ee\u9898\u80fd\u529b\u7684\u62c5\u5fe7\u3002", "method": "\u901a\u8fc7\u5f15\u5165AIM-Bench\u8fd9\u4e00\u65b0\u9896\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86LLM\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u4f9b\u5e94\u94fe\u7ba1\u7406\u573a\u666f\u4e2d\u7684\u51b3\u7b56\u884c\u4e3a\uff0c\u5305\u62ec\u4e00\u7cfb\u5217\u591a\u6837\u5316\u7684\u5e93\u5b58\u8865\u5145\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540cLLMs\u901a\u5e38\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u51b3\u7b56\u504f\u89c1\u7a0b\u5ea6\uff0c\u5e76\u63a2\u8ba8\u4e86\u51cf\u8f7b\u8fd9\u4e9b\u504f\u89c1\u7684\u7b56\u7565\uff0c\u5982\u8ba4\u77e5\u53cd\u601d\u548c\u4fe1\u606f\u5171\u4eab\u7684\u5b9e\u65bd\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5e93\u5b58\u51b3\u7b56\u4e2d\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65f6\u9700\u8c28\u614e\u8003\u8651\u6f5c\u5728\u504f\u89c1\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u51cf\u8f7b\u4eba\u7c7b\u51b3\u7b56\u504f\u89c1\u7684\u7b56\u7565\uff0c\u4e3a\u5f00\u53d1\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4f9b\u5e94\u94fe\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.11468", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11468", "abs": "https://arxiv.org/abs/2508.11468", "authors": ["Zhihao Gong", "Zeyu Sun", "Dong Huang", "Qingyuan Liang", "Jie M. Zhang", "Dan Hao"], "title": "TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation", "comment": null, "summary": "Automatic code translation is a fundamental task in modern software\ndevelopment. While the advent of Large Language Models (LLMs) has significantly\nimproved the correctness of code translation, the critical dimension of\nexecution efficiency remains overlooked. To address this gap, we introduce\nTRACY, the first comprehensive benchmark designed to evaluate the execution\nefficiency of LLM-translated code. TRACY is constructed through an LLM-driven\ntwo-stage pipeline: an initial stage generates a suite of stress tests to\namplify performance differences, followed by an efficiency-oriented task\npruning stage that isolates the efficiency-distinguishing tasks. The resulting\nbenchmark comprises 1,011 code translation tasks across C++, Java, and Python,\neach accompanied by an average of 22.1 verified reference translations and 10\ncomputationally demanding tests. Our extensive evaluation of 26 representative\nLLMs reveals that even top-tier LLMs struggle to consistently produce efficient\ncode translations. For instance, Claude-4-think, the leading model for\ncorrectness, ranks eighth overall when time efficiency is taken into account,\nsurpassed by several smaller open-source models. We further pinpoint that\nalgorithmic flaws and improper resource handling are the most detrimental,\ncausing a median time slowdown of 5.6$\\times$ and memory increase of\n12.0$\\times$, respectively. Our work underscores the necessity of jointly\noptimizing for correctness and efficiency in future LLM-based code translation.", "AI": {"tldr": "TRACY\u662f\u9996\u4e2a\u8bc4\u4f30LLM\u7ffb\u8bd1\u4ee3\u7801\u6267\u884c\u6548\u7387\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u5373\u4f7f\u9876\u7ea7\u6a21\u578b\u5728\u6548\u7387\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5f3a\u8c03\u672a\u6765\u9700\u540c\u65f6\u4f18\u5316\u6b63\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u7ffb\u8bd1\u7684\u6b63\u786e\u6027\uff0c\u4f46\u6267\u884c\u6548\u7387\u8fd9\u4e00\u5173\u952e\u7ef4\u5ea6\u4ecd\u88ab\u5ffd\u89c6\u3002", "method": "\u901a\u8fc7LLM\u9a71\u52a8\u7684\u4e24\u9636\u6bb5\u6d41\u7a0b\u6784\u5efaTRACY\u57fa\u51c6\uff1a\u9996\u5148\u751f\u6210\u4e00\u5957\u538b\u529b\u6d4b\u8bd5\u4ee5\u653e\u5927\u6027\u80fd\u5dee\u5f02\uff0c\u968f\u540e\u8fdb\u884c\u6548\u7387\u5bfc\u5411\u7684\u4efb\u52a1\u4fee\u526a\u9636\u6bb5\uff0c\u4ee5\u9694\u79bb\u533a\u5206\u6548\u7387\u7684\u4efb\u52a1\u3002", "result": "TRACY\u5305\u542b1,011\u4e2a\u8de8C++\u3001Java\u548cPython\u7684\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u5e73\u5747\u670922.1\u4e2a\u5df2\u9a8c\u8bc1\u7684\u53c2\u8003\u7ffb\u8bd1\u548c10\u4e2a\u8ba1\u7b97\u5bc6\u96c6\u578b\u6d4b\u8bd5\u3002\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u9876\u7ea7LLM\u4e5f\u96be\u4ee5\u6301\u7eed\u751f\u6210\u9ad8\u6548\u7684\u4ee3\u7801\u7ffb\u8bd1\uff0c\u7b97\u6cd5\u7f3a\u9677\u548c\u8d44\u6e90\u5904\u7406\u4e0d\u5f53\u662f\u6700\u6709\u5bb3\u7684\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5728\u672a\u6765\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u7ffb\u8bd1\u4e2d\uff0c\u540c\u65f6\u4f18\u5316\u6b63\u786e\u6027\u548c\u6548\u7387\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.11200", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11200", "abs": "https://arxiv.org/abs/2508.11200", "authors": ["Hongbin Lin", "Bin Li", "Kwok Wai Samuel Au"], "title": "Visuomotor Grasping with World Models for Surgical Robots", "comment": null, "summary": "Grasping is a fundamental task in robot-assisted surgery (RAS), and\nautomating it can reduce surgeon workload while enhancing efficiency, safety,\nand consistency beyond teleoperated systems. Most prior approaches rely on\nexplicit object pose tracking or handcrafted visual features, limiting their\ngeneralization to novel objects, robustness to visual disturbances, and the\nability to handle deformable objects. Visuomotor learning offers a promising\nalternative, but deploying it in RAS presents unique challenges, such as low\nsignal-to-noise ratio in visual observations, demands for high safety and\nmillimeter-level precision, as well as the complex surgical environment. This\npaper addresses three key challenges: (i) sim-to-real transfer of visuomotor\npolicies to ex vivo surgical scenes, (ii) visuomotor learning using only a\nsingle stereo camera pair -- the standard RAS setup, and (iii) object-agnostic\ngrasping with a single policy that generalizes to diverse, unseen surgical\nobjects without retraining or task-specific models. We introduce Grasp Anything\nfor Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.\nGASv2 leverages a world-model-based architecture and a surgical perception\npipeline for visual observations, combined with a hybrid control system for\nsafe execution. We train the policy in simulation using domain randomization\nfor sim-to-real transfer and deploy it on a real robot in both phantom-based\nand ex vivo surgical settings, using only a single pair of endoscopic cameras.\nExtensive experiments show our policy achieves a 65% success rate in both\nsettings, generalizes to unseen objects and grippers, and adapts to diverse\ndisturbances, demonstrating strong performance, generality, and robustness.", "AI": {"tldr": "GASv2\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8fd0\u52a8\u5b66\u4e60\u7684\u624b\u672f\u6293\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u4eff\u771f\u8bad\u7ec3\u548c\u771f\u5b9e\u90e8\u7f72\uff0c\u5b9e\u73b0\u4e8665%\u7684\u6210\u529f\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u81ea\u52a8\u5316\u624b\u672f\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u53ef\u4ee5\u51cf\u5c11\u5916\u79d1\u533b\u751f\u7684\u5de5\u4f5c\u8d1f\u62c5\uff0c\u5e76\u63d0\u9ad8\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u4e00\u81f4\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u3001\u9c81\u68d2\u6027\u548c\u5904\u7406\u53ef\u53d8\u5f62\u7269\u4f53\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "GASv2\u91c7\u7528\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u67b6\u6784\u548c\u624b\u672f\u611f\u77e5\u7ba1\u9053\u8fdb\u884c\u89c6\u89c9\u89c2\u5bdf\uff0c\u7ed3\u5408\u6df7\u5408\u63a7\u5236\u7cfb\u7edf\u5b9e\u73b0\u5b89\u5168\u6267\u884c\u3002\u901a\u8fc7\u9886\u57df\u968f\u673a\u5316\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\u3002", "result": "GASv2\u5728\u5e7b\u5f71\u548c\u79bb\u4f53\u624b\u672f\u73af\u5883\u4e2d\u5747\u5b9e\u73b0\u4e8665%\u7684\u6210\u529f\u7387\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u548c\u5939\u722a\uff0c\u5e76\u9002\u5e94\u591a\u79cd\u5e72\u6270\u3002", "conclusion": "GASv2\u6846\u67b6\u5728\u624b\u672f\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6210\u529f\u7387\u8fbe\u523065%\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u548c\u5939\u722a\uff0c\u5e76\u9002\u5e94\u591a\u79cd\u5e72\u6270\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u3001\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.10938", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10938", "abs": "https://arxiv.org/abs/2508.10938", "authors": ["Tianyu Song", "Van-Doan Duong", "Thi-Phuong Le", "Ton Viet Ta"], "title": "Deep Learning for Automated Identification of Vietnamese Timber Species: A Tool for Ecological Monitoring and Conservation", "comment": null, "summary": "Accurate identification of wood species plays a critical role in ecological\nmonitoring, biodiversity conservation, and sustainable forest management.\nTraditional classification approaches relying on macroscopic and microscopic\ninspection are labor-intensive and require expert knowledge. In this study, we\nexplore the application of deep learning to automate the classification of ten\nwood species commonly found in Vietnam. A custom image dataset was constructed\nfrom field-collected wood samples, and five state-of-the-art convolutional\nneural network architectures--ResNet50, EfficientNet, MobileViT, MobileNetV3,\nand ShuffleNetV2--were evaluated. Among these, ShuffleNetV2 achieved the best\nbalance between classification performance and computational efficiency, with\nan average accuracy of 99.29\\% and F1-score of 99.35\\% over 20 independent\nruns. These results demonstrate the potential of lightweight deep learning\nmodels for real-time, high-accuracy species identification in\nresource-constrained environments. Our work contributes to the growing field of\necological informatics by providing scalable, image-based solutions for\nautomated wood classification and forest biodiversity assessment.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5c24\u5176\u662fShuffleNetV2\uff09\u5b9e\u73b0\u4e86\u8d8a\u5357\u5e38\u89c1\u6728\u6750\u7269\u79cd\u7684\u9ad8\u7cbe\u5ea6\u81ea\u52a8\u5316\u5206\u7c7b\uff0c\u4e3a\u751f\u6001\u76d1\u6d4b\u548c\u68ee\u6797\u7ba1\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u6728\u6750\u5206\u7c7b\u65b9\u6cd5\u4f9d\u8d56\u5b8f\u89c2\u548c\u5fae\u89c2\u68c0\u67e5\uff0c\u8017\u65f6\u4e14\u9700\u4e13\u4e1a\u77e5\u8bc6\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u81ea\u52a8\u5316\u5206\u7c7b\uff0c\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u81ea\u5b9a\u4e49\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e94\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08ResNet50\u3001EfficientNet\u3001MobileViT\u3001MobileNetV3\u548cShuffleNetV2\uff09\u7684\u6027\u80fd\u3002", "result": "ShuffleNetV2\u572820\u6b21\u72ec\u7acb\u8fd0\u884c\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523099.29%\uff0cF1\u5206\u6570\u4e3a99.35%\uff0c\u8868\u73b0\u51fa\u6700\u4f73\u7684\u5206\u7c7b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u5e73\u8861\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982ShuffleNetV2\uff09\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u80fd\u591f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u6728\u6750\u7269\u79cd\u5b9e\u65f6\u8bc6\u522b\uff0c\u4e3a\u751f\u6001\u4fe1\u606f\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u56fe\u50cf\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11452", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11452", "abs": "https://arxiv.org/abs/2508.11452", "authors": ["Kangyu Wang", "Hongliang He", "Lin Liu", "Ruiqi Liang", "Zhenzhong Lan", "Jianguo Li"], "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps", "comment": "Our platform is publicly accessible at\n  https://doraemon.alipay.com/model-ranking", "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking.", "AI": {"tldr": "Inclusion Arena\u662f\u4e00\u4e2a\u5b9e\u65f6\u6392\u884c\u699c\uff0c\u901a\u8fc7\u7528\u6237\u53cd\u9988\u548c\u521b\u65b0\u7684\u6392\u540d\u673a\u5236\u8bc4\u4f30LLMs\u548cMLLMs\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u548c\u6392\u884c\u699c\u591a\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\u6216\u901a\u7528\u9886\u57df\u63d0\u793a\uff0c\u96be\u4ee5\u53cd\u6620\u6a21\u578b\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528Bradley-Terry\u6a21\u578b\uff0c\u5e76\u5f15\u5165Placement Matches\u548cProximity Sampling\u4e24\u9879\u521b\u65b0\u673a\u5236\uff0c\u4ee5\u4f18\u5316\u6a21\u578b\u6392\u540d\u3002", "result": "Inclusion Arena\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u7a33\u5b9a\u7684\u6392\u540d\uff0c\u6570\u636e\u4f20\u9012\u6027\u66f4\u9ad8\uff0c\u5e76\u80fd\u6709\u6548\u51cf\u5c11\u6076\u610f\u64cd\u7eb5\u98ce\u9669\u3002", "conclusion": "Inclusion Arena\u901a\u8fc7\u7ed3\u5408\u5b9e\u65f6\u7528\u6237\u53cd\u9988\u548c\u521b\u65b0\u7684\u6392\u540d\u673a\u5236\uff0c\u4e3aLLMs\u548cMLLMs\u63d0\u4f9b\u4e86\u66f4\u8d34\u8fd1\u5b9e\u9645\u5e94\u7528\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u52a0\u901f\u4e86\u9762\u5411\u7528\u6237\u4f18\u5316\u7684\u6a21\u578b\u5f00\u53d1\u3002"}}
{"id": "2508.11571", "categories": ["cs.SE", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.11571", "abs": "https://arxiv.org/abs/2508.11571", "authors": ["Alexander Bakhtin"], "title": "Temporal Network Analysis of Microservice Architectural Degradation", "comment": null, "summary": "Microservice architecture can be modeled as a network of microservices making\ncalls to each other, commonly known as the service dependency graph. Network\nScience can provide methods to study such networks. In particular, temporal\nnetwork analysis is a branch of Network Science that analyzes networks evolving\nwith time. In microservice systems, temporal networks can arise if we examine\nthe architecture of the system across releases or monitor a deployed system\nusing tracing.\n  In this research summary paper, I discuss the challenges in obtaining\ntemporal networks from microservice systems and analyzing them with the\ntemporal network methods. In particular, the most complete temporal network\nthat we could obtain contains 7 time instances and 42 microservices, which\nlimits the potential analysis that could be applied.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4ece\u5fae\u670d\u52a1\u7cfb\u7edf\u83b7\u53d6\u65f6\u95f4\u7f51\u7edc\u5e76\u5206\u6790\u7684\u6311\u6218\uff0c\u53d7\u9650\u4e8e\u6570\u636e\u89c4\u6a21\uff087\u4e2a\u65f6\u95f4\u5b9e\u4f8b\u548c42\u4e2a\u5fae\u670d\u52a1\uff09\uff0c\u90e8\u5206\u5206\u6790\u65b9\u6cd5\u65e0\u6cd5\u5e94\u7528\u3002", "motivation": "\u5fae\u670d\u52a1\u67b6\u6784\u53ef\u4ee5\u5efa\u6a21\u4e3a\u5fae\u670d\u52a1\u4e4b\u95f4\u76f8\u4e92\u8c03\u7528\u7684\u7f51\u7edc\uff0c\u5373\u670d\u52a1\u4f9d\u8d56\u56fe\u3002\u7f51\u7edc\u79d1\u5b66\u63d0\u4f9b\u4e86\u7814\u7a76\u6b64\u7c7b\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u65f6\u95f4\u7f51\u7edc\u5206\u6790\u53ef\u4ee5\u5206\u6790\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u7f51\u7edc\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u7f51\u7edc\u5206\u6790\u65b9\u6cd5\uff0c\u7814\u7a76\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u7684\u670d\u52a1\u4f9d\u8d56\u56fe\u3002", "result": "\u83b7\u53d6\u7684\u6700\u5b8c\u6574\u65f6\u95f4\u7f51\u7edc\u5305\u542b7\u4e2a\u65f6\u95f4\u5b9e\u4f8b\u548c42\u4e2a\u5fae\u670d\u52a1\uff0c\u8fd9\u9650\u5236\u4e86\u6f5c\u5728\u7684\u5206\u6790\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u8ba8\u8bba\u4e86\u4ece\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u83b7\u53d6\u65f6\u95f4\u7f51\u7edc\u5e76\u5229\u7528\u65f6\u95f4\u7f51\u7edc\u65b9\u6cd5\u5206\u6790\u8fd9\u4e9b\u7f51\u7edc\u7684\u6311\u6218\uff0c\u6307\u51fa\u5f53\u524d\u83b7\u53d6\u7684\u6700\u5b8c\u6574\u65f6\u95f4\u7f51\u7edc\u4ec5\u5305\u542b7\u4e2a\u65f6\u95f4\u5b9e\u4f8b\u548c42\u4e2a\u5fae\u670d\u52a1\uff0c\u9650\u5236\u4e86\u53ef\u5e94\u7528\u7684\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2508.11204", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11204", "abs": "https://arxiv.org/abs/2508.11204", "authors": ["Hongbin Lin", "Juan Rojas", "Kwok Wai Samuel Au"], "title": "Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation", "comment": null, "summary": "Sampling efficiency is critical for deploying visuomotor learning in\nreal-world robotic manipulation. While task symmetry has emerged as a promising\ninductive bias to improve efficiency, most prior work is limited to isometric\nsymmetries -- applying the same group transformation to all task objects across\nall timesteps. In this work, we explore non-isometric symmetries, applying\nmultiple independent group transformations across spatial and temporal\ndimensions to relax these constraints. We introduce a novel formulation of the\npartially observable Markov decision process (POMDP) that incorporates the\nnon-isometric symmetry structures, and propose a simple yet effective data\naugmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate\nMEA with offline reinforcement learning to enhance sampling efficiency, and\nintroduce a voxel-based visual representation that preserves translational\nequivariance. Extensive simulation and real-robot experiments across two\nmanipulation domains demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u975e\u7b49\u8ddd\u5bf9\u79f0\u6027\u7ed3\u6784\u548cMEA\u6570\u636e\u589e\u5f3a\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u91c7\u6837\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u91c7\u6837\u6548\u7387\u5bf9\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u89c6\u89c9\u8fd0\u52a8\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u4efb\u52a1\u5bf9\u79f0\u6027\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u9014\u7684\u5f52\u7eb3\u504f\u5dee\u88ab\u63d0\u51fa\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u5927\u591a\u6570\u73b0\u6709\u5de5\u4f5c\u4ec5\u9650\u4e8e\u7b49\u8ddd\u5bf9\u79f0\u6027\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u975e\u7b49\u8ddd\u5bf9\u79f0\u6027\uff0c\u4ee5\u653e\u677e\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u516c\u5f0f\uff0c\u7ed3\u5408\u975e\u7b49\u8ddd\u5bf9\u79f0\u6027\u7ed3\u6784\uff0c\u5e76\u8bbe\u8ba1\u4e86Multi-Group Equivariance Augmentation\uff08MEA\uff09\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f53\u7d20\u7684\u89c6\u89c9\u8868\u793a\u65b9\u6cd5\u4ee5\u4fdd\u6301\u5e73\u79fb\u7b49\u53d8\u6027\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u4e24\u4e2a\u64cd\u4f5c\u9886\u57df\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u975e\u7b49\u8ddd\u5bf9\u79f0\u6027\u7ed3\u6784\u548cMEA\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8fd0\u52a8\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u91c7\u6837\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u5e7f\u6cdb\u7684\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.10940", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10940", "abs": "https://arxiv.org/abs/2508.10940", "authors": ["Nirmal Gaud", "Krishna Kumar Jha", "Jhimli Adhikari", "Adhini Nasarin P S", "Joydeep Das", "Samarth S Deshpande", "Nitasha Barara", "Vaduguru Venkata Ramya", "Santu Saha", "Mehmet Tarik Baran", "Sarangi Venkateshwarlu", "Anusha M D", "Surej Mouli", "Preeti Katiyar", "Vipin Kumar Chaudhary"], "title": "NIRMAL Pooling: An Adaptive Max Pooling Approach with Non-linear Activation for Enhanced Image Classification", "comment": "6 pages, 2 figures", "summary": "This paper presents NIRMAL Pooling, a novel pooling layer for Convolutional\nNeural Networks (CNNs) that integrates adaptive max pooling with non-linear\nactivation function for image classification tasks. The acronym NIRMAL stands\nfor Non-linear Activation, Intermediate Aggregation, Reduction, Maximum,\nAdaptive, and Localized. By dynamically adjusting pooling parameters based on\ndesired output dimensions and applying a Rectified Linear Unit (ReLU)\nactivation post-pooling, NIRMAL Pooling improves robustness and feature\nexpressiveness. We evaluated its performance against standard Max Pooling on\nthree benchmark datasets: MNIST Digits, MNIST Fashion, and CIFAR-10. NIRMAL\nPooling achieves test accuracies of 99.25% (vs. 99.12% for Max Pooling) on\nMNIST Digits, 91.59% (vs. 91.44%) on MNIST Fashion, and 70.49% (vs. 68.87%) on\nCIFAR-10, demonstrating consistent improvements, particularly on complex\ndatasets. This work highlights the potential of NIRMAL Pooling to enhance CNN\nperformance in diverse image recognition tasks, offering a flexible and\nreliable alternative to traditional pooling methods.", "AI": {"tldr": "NIRMAL Pooling\u662f\u4e00\u79cd\u65b0\u578bCNN\u6c60\u5316\u5c42\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u6700\u5927\u6c60\u5316\u548cReLU\u6fc0\u6d3b\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8CNN\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u7279\u5f81\u8868\u8fbe\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u5f00\u53d1\u4e00\u79cd\u66f4\u7075\u6d3b\u53ef\u9760\u7684\u6c60\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faNIRMAL Pooling\u5c42\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u6700\u5927\u6c60\u5316\u548cReLU\u6fc0\u6d3b\u51fd\u6570\uff0c\u52a8\u6001\u8c03\u6574\u6c60\u5316\u53c2\u6570\u4ee5\u9002\u5e94\u8f93\u51fa\u7ef4\u5ea6\u3002", "result": "\u5728MNIST Digits\u3001MNIST Fashion\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\uff0cNIRMAL Pooling\u5206\u522b\u8fbe\u523099.25%\u300191.59%\u548c70.49%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u5747\u4f18\u4e8e\u4f20\u7edf\u6700\u5927\u6c60\u5316\u3002", "conclusion": "NIRMAL Pooling\u4f5c\u4e3a\u4e00\u79cd\u65b0\u578b\u7684CNN\u6c60\u5316\u5c42\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u9002\u5e94\u6700\u5927\u6c60\u5316\u548c\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u4f18\u4e8e\u4f20\u7edf\u6700\u5927\u6c60\u5316\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u3002"}}
{"id": "2508.11493", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11493", "abs": "https://arxiv.org/abs/2508.11493", "authors": ["David H. Chan", "Mark Roberts", "Dana S. Nau"], "title": "Landmark-Assisted Monte Carlo Planning", "comment": "To be published in the Proceedings of the 28th European Conference on\n  Artificial Intelligence", "summary": "Landmarks$\\unicode{x2013}$conditions that must be satisfied at some point in\nevery solution plan$\\unicode{x2013}$have contributed to major advancements in\nclassical planning, but they have seldom been used in stochastic domains. We\nformalize probabilistic landmarks and adapt the UCT algorithm to leverage them\nas subgoals to decompose MDPs; core to the adaptation is balancing between\ngreedy landmark achievement and final goal achievement. Our results in\nbenchmark domains show that well-chosen landmarks can significantly improve the\nperformance of UCT in online probabilistic planning, while the best balance of\ngreedy versus long-term goal achievement is problem-dependent. The results\nsuggest that landmarks can provide helpful guidance for anytime algorithms\nsolving MDPs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u6982\u7387\u6027\u5730\u6807\u7684\u6982\u5ff5\uff0c\u5e76\u8c03\u6574UCT\u7b97\u6cd5\u4ee5\u5229\u7528\u5730\u6807\u4f5c\u4e3a\u5b50\u76ee\u6807\u5206\u89e3MDP\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5730\u6807\u80fd\u663e\u8457\u63d0\u5347\u5728\u7ebf\u6982\u7387\u89c4\u5212\u7684\u6027\u80fd\u3002", "motivation": "\u5730\u6807\u5728\u7ecf\u5178\u89c4\u5212\u4e2d\u4fc3\u6210\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u5728\u968f\u673a\u9886\u57df\u4e2d\u5f88\u5c11\u4f7f\u7528\u3002", "method": "\u6211\u4eec\u5f62\u5f0f\u5316\u4e86\u6982\u7387\u6027\u5730\u6807\uff0c\u5e76\u8c03\u6574\u4e86UCT\u7b97\u6cd5\u4ee5\u5229\u7528\u5b83\u4eec\u4f5c\u4e3a\u5b50\u76ee\u6807\u6765\u5206\u89e3MDP\uff1b\u8c03\u6574\u7684\u6838\u5fc3\u662f\u5728\u8d2a\u5a6a\u5730\u6807\u8fbe\u6210\u4e0e\u6700\u7ec8\u76ee\u6807\u8fbe\u6210\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "result": "\u57fa\u51c6\u9886\u57df\u7684\u7ed3\u679c\u8868\u660e\uff0c\u7cbe\u5fc3\u9009\u62e9\u7684\u5730\u6807\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8UCT\u5728\u5728\u7ebf\u6982\u7387\u89c4\u5212\u4e2d\u7684\u6027\u80fd\uff0c\u800c\u8d2a\u5a6a\u4e0e\u957f\u671f\u76ee\u6807\u8fbe\u6210\u7684\u6700\u4f73\u5e73\u8861\u662f\u95ee\u9898\u4f9d\u8d56\u7684\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5730\u6807\u53ef\u4ee5\u4e3a\u89e3\u51b3MDP\u7684\u968f\u65f6\u7b97\u6cd5\u63d0\u4f9b\u6709\u76ca\u7684\u6307\u5bfc\u3002"}}
{"id": "2508.11232", "categories": ["cs.RO", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11232", "abs": "https://arxiv.org/abs/2508.11232", "authors": ["Guoliang Li", "Xibin Jin", "Yujie Wan", "Chenxuan Liu", "Tong Zhang", "Shuai Wang", "Chengzhong Xu"], "title": "Embodied Edge Intelligence Meets Near Field Communication: Concept, Design, and Verification", "comment": "9 pages, 6 figures, to appear in IEEE Network", "summary": "Realizing embodied artificial intelligence is challenging due to the huge\ncomputation demands of large models (LMs). To support LMs while ensuring\nreal-time inference, embodied edge intelligence (EEI) is a promising paradigm,\nwhich leverages an LM edge to provide computing powers in close proximity to\nembodied robots. Due to embodied data exchange, EEI requires higher spectral\nefficiency, enhanced communication security, and reduced inter-user\ninterference. To meet these requirements, near-field communication (NFC), which\nleverages extremely large antenna arrays as its hardware foundation, is an\nideal solution. Therefore, this paper advocates the integration of EEI and NFC,\nresulting in a near-field EEI (NEEI) paradigm. However, NEEI also introduces\nnew challenges that cannot be adequately addressed by isolated EEI or NFC\ndesigns, creating research opportunities for joint optimization of both\nfunctionalities. To this end, we propose radio-friendly embodied planning for\nEEI-assisted NFC scenarios and view-guided beam-focusing for NFC-assisted EEI\nscenarios. We also elaborate how to realize resource-efficient NEEI through\nopportunistic collaborative navigation. Experimental results are provided to\nconfirm the superiority of the proposed techniques compared with various\nbenchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06EEI\u4e0eNFC\u96c6\u6210\u4e3aNEEI\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u5b9e\u65f6\u63a8\u7406\u548c\u5927\u6a21\u578b\u8ba1\u7b97\u9700\u6c42\u7684\u77db\u76fe\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u5173\u952e\u6280\u672f\u4f18\u5316\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\u5de8\u5927\uff0c\u5b9e\u73b0\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u9762\u4e34\u6311\u6218\u3002EEI\u867d\u80fd\u63d0\u4f9b\u8fd1\u7aef\u8ba1\u7b97\u652f\u6301\uff0c\u4f46\u9700\u8981\u66f4\u9ad8\u7684\u9891\u8c31\u6548\u7387\u3001\u901a\u4fe1\u5b89\u5168\u548c\u51cf\u5c11\u7528\u6237\u95f4\u5e72\u6270\u3002NFC\u51ed\u501f\u5176\u786c\u4ef6\u57fa\u7840\u6210\u4e3a\u7406\u60f3\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u96c6\u6210EEI\u548cNFC\u4e5f\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5173\u952e\u6280\u672f\uff1aEEI\u8f85\u52a9NFC\u573a\u666f\u4e0b\u7684\u65e0\u7ebf\u7535\u53cb\u597d\u578b\u5b9e\u4f53\u89c4\u5212\uff0c\u4ee5\u53caNFC\u8f85\u52a9EEI\u573a\u666f\u4e0b\u7684\u89c6\u56fe\u5f15\u5bfc\u6ce2\u675f\u805a\u7126\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u673a\u4f1a\u6027\u534f\u4f5c\u5bfc\u822a\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u7684NEEI\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6280\u672f\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u591a\u79cd\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684NEEI\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06EEI\u4e0eNFC\u96c6\u6210\uff0c\u89e3\u51b3\u4e86\u5b9e\u65f6\u63a8\u7406\u548c\u5927\u6a21\u578b\u8ba1\u7b97\u9700\u6c42\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6280\u672f\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.10942", "categories": ["cs.CV", "cs.HC", "cs.MM", "I.4.10; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.10942", "abs": "https://arxiv.org/abs/2508.10942", "authors": ["Liming Xu", "Dave Towey", "Andrew P. French", "Steve Benford"], "title": "Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram", "comment": "This work is an extension of an ACM MM'17 workshop paper (Xu et al,\n  2017), which was completed in late 2017 and early 2018 during the first\n  author's doctoral studies at the University of Nottingham. This paper\n  includes 42 pages, 25 figures, 7 tables, and 13,536 words", "summary": "The increasing ubiquity of smartphones and resurgence of VR/AR techniques, it\nis expected that our everyday environment may soon be decorating with objects\nconnecting with virtual elements. Alerting to the presence of these objects is\ntherefore the first step for motivating follow-up further inspection and\ntriggering digital material attached to the objects. This work studies a\nspecial kind of these objects -- Artcodes -- a human-meaningful and\nmachine-readable decorative markers that camouflage themselves with freeform\nappearance by encoding information into their topology. We formulate this\nproblem of recongising the presence of Artcodes as Artcode proposal detection,\na distinct computer vision task that classifies topologically similar but\ngeometrically and semantically different objects as a same class. To deal with\nthis problem, we propose a new feature descriptor, called the shape of\norientation histogram, to describe the generic topological structure of an\nArtcode. We collect datasets and conduct comprehensive experiments to evaluate\nthe performance of the Artcode detection proposer built upon this new feature\nvector. Our experimental results show the feasibility of the proposed feature\nvector for representing topological structures and the effectiveness of the\nsystem for detecting Artcode proposals. Although this work is an initial\nattempt to develop a feature-based system for detecting topological objects\nlike Artcodes, it would open up new interaction opportunities and spark\npotential applications of topological object detection.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7279\u5f81\u63cf\u8ff0\u7b26\uff08\u5f62\u72b6\u65b9\u5411\u76f4\u65b9\u56fe\uff09\u7528\u4e8e\u68c0\u6d4bArtcodes\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0c\u4e3a\u62d3\u6251\u5bf9\u8c61\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u624b\u673a\u548cVR/AR\u6280\u672f\u7684\u666e\u53ca\uff0c\u65e5\u5e38\u73af\u5883\u4e2d\u53ef\u80fd\u51fa\u73b0\u66f4\u591a\u4e0e\u865a\u62df\u5143\u7d20\u8fde\u63a5\u7684\u88c5\u9970\u6027\u5bf9\u8c61\u3002\u8bc6\u522b\u8fd9\u4e9b\u5bf9\u8c61\uff08\u5982Artcodes\uff09\u7684\u5b58\u5728\u662f\u89e6\u53d1\u540e\u7eed\u4ea4\u4e92\u7684\u7b2c\u4e00\u6b65\u3002", "method": "\u63d0\u51fa\u4e86\u5f62\u72b6\u65b9\u5411\u76f4\u65b9\u56fe\uff08shape of orientation histogram\uff09\u4f5c\u4e3a\u65b0\u7684\u7279\u5f81\u63cf\u8ff0\u7b26\uff0c\u7528\u4e8e\u63cf\u8ff0Artcode\u7684\u901a\u7528\u62d3\u6251\u7ed3\u6784\uff0c\u5e76\u6784\u5efa\u4e86Artcode\u68c0\u6d4b\u7cfb\u7edf\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7279\u5f81\u5411\u91cf\u80fd\u591f\u6709\u6548\u8868\u793a\u62d3\u6251\u7ed3\u6784\uff0c\u4e14\u7cfb\u7edf\u5728Artcode\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f62\u72b6\u65b9\u5411\u76f4\u65b9\u56fe\u7684\u65b0\u7279\u5f81\u63cf\u8ff0\u7b26\uff0c\u7528\u4e8e\u68c0\u6d4bArtcodes\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002\u5c3d\u7ba1\u8fd9\u662f\u521d\u6b65\u5c1d\u8bd5\uff0c\u4f46\u5b83\u4e3a\u62d3\u6251\u5bf9\u8c61\u68c0\u6d4b\u5f00\u8f9f\u4e86\u65b0\u7684\u4ea4\u4e92\u673a\u4f1a\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.11524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11524", "abs": "https://arxiv.org/abs/2508.11524", "authors": ["Wenkai Yu", "Jianhang Tang", "Yang Zhang", "Shanjiang Tang", "Kebing Jin", "Hankz Hankui Zhuo"], "title": "Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models", "comment": null, "summary": "Addressing large-scale planning problems has become one of the central\nchallenges in the planning community, deriving from the state-space explosion\ncaused by growing objects and actions. Recently, researchers have explored the\neffectiveness of leveraging Large Language Models (LLMs) to generate helpful\nactions and states to prune the search space. However, prior works have largely\noverlooked integrating LLMs with domain-specific knowledge to ensure valid\nplans. In this paper, we propose a novel LLM-assisted planner integrated with\nproblem decomposition, which first decomposes large planning problems into\nmultiple simpler sub-tasks. Then we explore two novel paradigms to utilize\nLLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where\nLLM4Inspire provides heuristic guidance according to general knowledge and\nLLM4Predict employs domain-specific knowledge to infer intermediate conditions.\nWe empirically validate the effectiveness of our planner across multiple\ndomains, demonstrating the ability of search space partition when solving\nlarge-scale planning problems. The experimental results show that LLMs\neffectively locate feasible solutions when pruning the search space, where\ninfusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds\nparticular promise compared with LLM4Inspire, which offers general knowledge\nwithin LLMs.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u95ee\u9898\u5206\u89e3\u7684LLM\u8f85\u52a9\u89c4\u5212\u5668\uff0cLLM4Predict\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u89c4\u5212\u95ee\u9898\u65f6\u4f18\u4e8eLLM4Inspire\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u89c4\u5212\u95ee\u9898\u4e2d\u7684\u72b6\u6001\u7a7a\u95f4\u7206\u70b8\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u6709\u6548\u7ed3\u5408LLM\u4e0e\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u4ee5\u786e\u4fdd\u6709\u6548\u89c4\u5212\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u95ee\u9898\u5206\u89e3\u7684LLM\u8f85\u52a9\u89c4\u5212\u5668\uff0c\u5305\u62ecLLM4Inspire\uff08\u63d0\u4f9b\u542f\u53d1\u5f0f\u6307\u5bfc\uff09\u548cLLM4Predict\uff08\u63a8\u65ad\u4e2d\u95f4\u6761\u4ef6\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLM\u5728\u526a\u679d\u641c\u7d22\u7a7a\u95f4\u65f6\u80fd\u6709\u6548\u5b9a\u4f4d\u53ef\u884c\u89e3\uff0cLLM4Predict\u8868\u73b0\u4f18\u4e8eLLM4Inspire\u3002", "conclusion": "LLM4Predict\uff08\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u7684LLM\uff09\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u89c4\u5212\u95ee\u9898\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u6bd4\u4ec5\u63d0\u4f9b\u901a\u7528\u77e5\u8bc6\u7684LLM4Inspire\u66f4\u5177\u6f5c\u529b\u3002"}}
{"id": "2508.11261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11261", "abs": "https://arxiv.org/abs/2508.11261", "authors": ["Shan Luo", "Nathan F. Lepora", "Wenzhen Yuan", "Kaspar Althoefer", "Gordon Cheng", "Ravinder Dahiya"], "title": "Tactile Robotics: An Outlook", "comment": "20 pages, 2 figures, accepted to IEEE Transactions on Robotics", "summary": "Robotics research has long sought to give robots the ability to perceive the\nphysical world through touch in an analogous manner to many biological systems.\nDeveloping such tactile capabilities is important for numerous emerging\napplications that require robots to co-exist and interact closely with humans.\nConsequently, there has been growing interest in tactile sensing, leading to\nthe development of various technologies, including piezoresistive and\npiezoelectric sensors, capacitive sensors, magnetic sensors, and optical\ntactile sensors. These diverse approaches utilise different transduction\nmethods and materials to equip robots with distributed sensing capabilities,\nenabling more effective physical interactions. These advances have been\nsupported in recent years by simulation tools that generate large-scale tactile\ndatasets to support sensor designs and algorithms to interpret and improve the\nutility of tactile data. The integration of tactile sensing with other\nmodalities, such as vision, as well as with action strategies for active\ntactile perception highlights the growing scope of this field. To further the\ntransformative progress in tactile robotics, a holistic approach is essential.\nIn this outlook article, we examine several challenges associated with the\ncurrent state of the art in tactile robotics and explore potential solutions to\ninspire innovations across multiple domains, including manufacturing,\nhealthcare, recycling and agriculture.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89e6\u89c9\u673a\u5668\u4eba\u6280\u672f\u7684\u73b0\u72b6\u4e0e\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u8de8\u9886\u57df\u521b\u65b0\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e6\u89c9\u611f\u77e5\u5bf9\u673a\u5668\u4eba\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u5bc6\u5207\u5171\u5b58\u548c\u4e92\u52a8\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u89e6\u89c9\u4f20\u611f\u6280\u672f\u7684\u7814\u7a76\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "\u6587\u7ae0\u56de\u987e\u4e86\u591a\u79cd\u89e6\u89c9\u4f20\u611f\u6280\u672f\uff0c\u5305\u62ec\u538b\u963b\u3001\u538b\u7535\u3001\u7535\u5bb9\u3001\u78c1\u6027\u548c\u5149\u5b66\u4f20\u611f\u5668\uff0c\u5e76\u63a2\u8ba8\u4e86\u6a21\u62df\u5de5\u5177\u5728\u751f\u6210\u5927\u89c4\u6a21\u89e6\u89c9\u6570\u636e\u96c6\u4ee5\u652f\u6301\u4f20\u611f\u5668\u8bbe\u8ba1\u548c\u7b97\u6cd5\u5f00\u53d1\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u591a\u79cd\u89e6\u89c9\u4f20\u611f\u6280\u672f\u7684\u5f00\u53d1\u53ca\u5176\u4e0e\u5176\u4ed6\u611f\u77e5\u6a21\u6001\uff08\u5982\u89c6\u89c9\uff09\u7684\u96c6\u6210\uff0c\u4ee5\u53ca\u4e3b\u52a8\u89e6\u89c9\u611f\u77e5\u7b56\u7565\u7684\u7ed3\u5408\uff0c\u663e\u8457\u6269\u5c55\u4e86\u8be5\u9886\u57df\u7684\u5e94\u7528\u8303\u56f4\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u65b9\u6cd5\u6765\u63a8\u52a8\u89e6\u89c9\u673a\u5668\u4eba\u6280\u672f\u7684\u53d8\u9769\u6027\u8fdb\u5c55\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f53\u524d\u6280\u672f\u9762\u4e34\u7684\u6311\u6218\u53ca\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u6fc0\u53d1\u8de8\u9886\u57df\u521b\u65b0\u3002"}}
{"id": "2508.10943", "categories": ["cs.CV", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.10943", "abs": "https://arxiv.org/abs/2508.10943", "authors": ["Christian D\u00fcreth", "Jan Cond\u00e9-Wolter", "Marek Danczak", "Karsten Tittmann", "J\u00f6rn Jaschinski", "Andreas Hornig", "Maik Gude"], "title": "Analysis of the Compaction Behavior of Textile Reinforcements in Low-Resolution In-Situ CT Scans via Machine-Learning and Descriptor-Based Methods", "comment": "submitted to Elsevier Composite Part C: Open Access\n  (JCOMC-D-25-00212), 16 pages, 8 Figures, and 3 Tables", "summary": "A detailed understanding of material structure across multiple scales is\nessential for predictive modeling of textile-reinforced composites. Nesting --\ncharacterized by the interlocking of adjacent fabric layers through local\ninterpenetration and misalignment of yarns -- plays a critical role in defining\nmechanical properties such as stiffness, permeability, and damage tolerance.\nThis study presents a framework to quantify nesting behavior in dry textile\nreinforcements under compaction using low-resolution computed tomography (CT).\nIn-situ compaction experiments were conducted on various stacking\nconfigurations, with CT scans acquired at 20.22 $\\mu$m per voxel resolution. A\ntailored 3D{-}UNet enabled semantic segmentation of matrix, weft, and fill\nphases across compaction stages corresponding to fiber volume contents of\n50--60 %. The model achieved a minimum mean Intersection-over-Union of 0.822\nand an $F1$ score of 0.902. Spatial structure was subsequently analyzed using\nthe two-point correlation function $S_2$, allowing for probabilistic extraction\nof average layer thickness and nesting degree. The results show strong\nagreement with micrograph-based validation. This methodology provides a robust\napproach for extracting key geometrical features from industrially relevant CT\ndata and establishes a foundation for reverse modeling and descriptor-based\nstructural analysis of composite preforms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f4e\u5206\u8fa8\u7387CT\u548c3D-UNet\u91cf\u5316\u7eba\u7ec7\u589e\u5f3a\u6750\u6599\u7684\u5d4c\u5957\u884c\u4e3a\uff0c\u4e3a\u590d\u5408\u6750\u6599\u9884\u6210\u578b\u4f53\u7684\u7ed3\u6784\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3\u7eba\u7ec7\u589e\u5f3a\u590d\u5408\u6750\u6599\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784\u5bf9\u4e8e\u9884\u6d4b\u5efa\u6a21\u81f3\u5173\u91cd\u8981\uff0c\u800c\u5d4c\u5957\u884c\u4e3a\u662f\u5f71\u54cd\u5176\u673a\u68b0\u6027\u80fd\uff08\u5982\u521a\u5ea6\u3001\u6e17\u900f\u6027\u548c\u635f\u4f24\u5bb9\u9650\uff09\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4f4e\u5206\u8fa8\u7387\u8ba1\u7b97\u65ad\u5c42\u626b\u63cf\uff08CT\uff09\u8fdb\u884c\u539f\u4f4d\u538b\u5b9e\u5b9e\u9a8c\uff0c\u7ed3\u5408\u5b9a\u5236\u76843D-UNet\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff0c\u5e76\u4f7f\u7528\u4e24\u70b9\u76f8\u5173\u51fd\u6570S2\u5206\u6790\u7a7a\u95f4\u7ed3\u6784\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u5c0f\u5e73\u5747\u4ea4\u5e76\u6bd4\uff08IoU\uff090.822\u548cF1\u5206\u65700.902\uff0c\u5e76\u901a\u8fc7\u4e24\u70b9\u76f8\u5173\u51fd\u6570\u6210\u529f\u63d0\u53d6\u4e86\u5e73\u5747\u5c42\u539a\u5ea6\u548c\u5d4c\u5957\u5ea6\uff0c\u7ed3\u679c\u4e0e\u663e\u5fae\u56fe\u50cf\u9a8c\u8bc1\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u5e72\u7eba\u7ec7\u589e\u5f3a\u6750\u6599\u5728\u538b\u5b9e\u8fc7\u7a0b\u4e2d\u5d4c\u5957\u884c\u4e3a\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u4f4e\u5206\u8fa8\u7387CT\u626b\u63cf\u5b9e\u73b0\u4e86\u5bf9\u57fa\u8d28\u3001\u7eac\u7eb1\u548c\u586b\u5145\u76f8\u7684\u8bed\u4e49\u5206\u5272\u3002\u8be5\u65b9\u6cd5\u4e3a\u4ece\u5de5\u4e1a\u76f8\u5173CT\u6570\u636e\u4e2d\u63d0\u53d6\u5173\u952e\u51e0\u4f55\u7279\u5f81\u63d0\u4f9b\u4e86\u53ef\u9760\u9014\u5f84\uff0c\u5e76\u4e3a\u590d\u5408\u9884\u6210\u578b\u4f53\u7684\u9006\u5411\u5efa\u6a21\u548c\u57fa\u4e8e\u63cf\u8ff0\u7b26\u7684\u7ed3\u6784\u5206\u6790\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.10956", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10956", "abs": "https://arxiv.org/abs/2508.10956", "authors": ["Abhishek Kolari", "Mohammadhossein Khojasteh", "Yifan Jiang", "Floris den Hengst", "Filip Ilievski"], "title": "ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks", "comment": null, "summary": "While vision-language models (VLMs) have made remarkable progress on many\npopular visual question answering (VQA) benchmarks, it remains unclear whether\nthey abstract and reason over depicted objects. Inspired by human object\ncategorisation, object property reasoning involves identifying and recognising\nlow-level details and higher-level abstractions. While current VQA benchmarks\nconsider a limited set of object property attributes like size, they typically\nblend perception and reasoning, and lack representativeness in terms of\nreasoning and image categories. To this end, we introduce a systematic\nevaluation framework with images of three representative types, three reasoning\nlevels of increasing complexity, and four object property dimensions driven by\nprior work on commonsense reasoning. We develop a procedure to instantiate this\nbenchmark into ORBIT, a multi-level reasoning VQA benchmark for object\nproperties comprising 360 images paired with a total of 1,080 count-based\nquestions. Experiments with 12 state-of-the-art VLMs in zero-shot settings\nreveal significant limitations compared to humans, with the best-performing\nmodel only reaching 40\\% accuracy. VLMs struggle particularly with realistic\n(photographic) images, counterfactual reasoning about physical and functional\nproperties, and higher counts. ORBIT points to the need to develop methods for\nscalable benchmarking, generalize annotation guidelines, and explore additional\nreasoning VLMs. We make the ORBIT benchmark and the experimental code available\nto support such endeavors.", "AI": {"tldr": "ORBIT\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8c61\u5c5e\u6027\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u547c\u5401\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u63a8\u7406\u6a21\u578b\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u5728\u5bf9\u8c61\u5c5e\u6027\u63a8\u7406\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u672a\u80fd\u6709\u6548\u533a\u5206\u611f\u77e5\u4e0e\u63a8\u7406\uff0c\u4e14\u56fe\u50cf\u7c7b\u522b\u548c\u63a8\u7406\u4efb\u52a1\u7f3a\u4e4f\u4ee3\u8868\u6027\u3002\u53d7\u4eba\u7c7b\u5bf9\u8c61\u5206\u7c7b\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u6846\u67b6ORBIT\uff0c\u5305\u542b\u4e09\u79cd\u4ee3\u8868\u6027\u56fe\u50cf\u7c7b\u578b\u3001\u4e09\u4e2a\u590d\u6742\u6027\u9012\u589e\u7684\u63a8\u7406\u5c42\u7ea7\u548c\u56db\u4e2a\u5bf9\u8c61\u5c5e\u6027\u7ef4\u5ea6\uff0c\u5e76\u901a\u8fc71,080\u4e2a\u8ba1\u6570\u95ee\u9898\u5bf912\u79cd\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u96f6\u6837\u672c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6700\u4f73\u6a21\u578b\u5728ORBIT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4ec5\u8fbe\u523040%\u7684\u51c6\u786e\u7387\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\uff0c\u5c24\u5176\u5728\u771f\u5b9e\u56fe\u50cf\u3001\u53cd\u4e8b\u5b9e\u63a8\u7406\u548c\u9ad8\u8ba1\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "ORBIT\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8c61\u5c5e\u6027\u63a8\u7406\u4e0a\u7684\u663e\u8457\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u771f\u5b9e\u56fe\u50cf\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u53ef\u6269\u5c55\u57fa\u51c6\u6d4b\u8bd5\u548c\u6539\u8fdb\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.11275", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11275", "abs": "https://arxiv.org/abs/2508.11275", "authors": ["Masaki Murooka", "Iori Kumagai", "Mitsuharu Morisawa", "Fumio Kanehiro"], "title": "Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation", "comment": null, "summary": "To reduce the computational cost of humanoid motion generation, we introduce\na new approach to representing robot kinematic reachability: the differentiable\nreachability map. This map is a scalar-valued function defined in the task\nspace that takes positive values only in regions reachable by the robot's\nend-effector. A key feature of this representation is that it is continuous and\ndifferentiable with respect to task-space coordinates, enabling its direct use\nas constraints in continuous optimization for humanoid motion planning. We\ndescribe a method to learn such differentiable reachability maps from a set of\nend-effector poses generated using a robot's kinematic model, using either a\nneural network or a support vector machine as the learning model. By\nincorporating the learned reachability map as a constraint, we formulate\nhumanoid motion generation as a continuous optimization problem. We demonstrate\nthat the proposed approach efficiently solves various motion planning problems,\nincluding footstep planning, multi-contact motion planning, and\nloco-manipulation planning for humanoid robots.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u5fae\u53ef\u8fbe\u6027\u5730\u56fe\uff0c\u901a\u8fc7\u8fde\u7eed\u4f18\u5316\u9ad8\u6548\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u964d\u4f4e\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8fd0\u52a8\u89c4\u5212\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6216\u652f\u6301\u5411\u91cf\u673a\u4ece\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u4f4d\u59ff\u96c6\u5408\u4e2d\u5b66\u4e60\u53ef\u5fae\u53ef\u8fbe\u6027\u5730\u56fe\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u7ea6\u675f\u6761\u4ef6\u5e94\u7528\u4e8e\u8fde\u7eed\u4f18\u5316\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8db3\u6b65\u89c4\u5212\u3001\u591a\u63a5\u89e6\u8fd0\u52a8\u89c4\u5212\u548c\u64cd\u4f5c-\u79fb\u52a8\u89c4\u5212\u7b49\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u53ef\u8fbe\u6027\u5730\u56fe\u7684\u65b0\u65b9\u6cd5\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u901a\u8fc7\u8fde\u7eed\u4f18\u5316\u89e3\u51b3\u4e86\u591a\u79cd\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002"}}
{"id": "2508.10945", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10945", "abs": "https://arxiv.org/abs/2508.10945", "authors": ["Rishi Raj Sahoo", "Surbhi Saswati Mohanty", "Subhankar Mishra"], "title": "iWatchRoad: Scalable Detection and Geospatial Visualization of Potholes for Smart Cities", "comment": "Under review", "summary": "Potholes on the roads are a serious hazard and maintenance burden. This poses\na significant threat to road safety and vehicle longevity, especially on the\ndiverse and under-maintained roads of India. In this paper, we present a\ncomplete end-to-end system called iWatchRoad for automated pothole detection,\nGlobal Positioning System (GPS) tagging, and real time mapping using\nOpenStreetMap (OSM). We curated a large, self-annotated dataset of over 7,000\nframes captured across various road types, lighting conditions, and weather\nscenarios unique to Indian environments, leveraging dashcam footage. This\ndataset is used to fine-tune, Ultralytics You Only Look Once (YOLO) model to\nperform real time pothole detection, while a custom Optical Character\nRecognition (OCR) module was employed to extract timestamps directly from video\nframes. The timestamps are synchronized with GPS logs to geotag each detected\npotholes accurately. The processed data includes the potholes' details and\nframes as metadata is stored in a database and visualized via a user friendly\nweb interface using OSM. iWatchRoad not only improves detection accuracy under\nchallenging conditions but also provides government compatible outputs for road\nassessment and maintenance planning through the metadata visible on the\nwebsite. Our solution is cost effective, hardware efficient, and scalable,\noffering a practical tool for urban and rural road management in developing\nregions, making the system automated. iWatchRoad is available at\nhttps://smlab.niser.ac.in/project/iwatchroad", "AI": {"tldr": "iWatchRoad \u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u901a\u8fc7 dashcam \u89c6\u9891\u548c YOLO \u6a21\u578b\u5b9e\u65f6\u68c0\u6d4b\u548c\u6620\u5c04\u9053\u8def\u5751\u6d1e\uff0c\u9002\u7528\u4e8e\u5370\u5ea6\u7b49\u53d1\u5c55\u4e2d\u56fd\u5bb6\u7684\u9053\u8def\u7ba1\u7406\u3002", "motivation": "\u5370\u5ea6\u591a\u6837\u4e14\u7ef4\u62a4\u4e0d\u8db3\u7684\u9053\u8def\u4e0a\u7684\u5751\u6d1e\u5bf9\u9053\u8def\u5b89\u5168\u548c\u8f66\u8f86\u5bff\u547d\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u9ad8\u6548\u7684\u68c0\u6d4b\u548c\u6620\u5c04\u7cfb\u7edf\u3002", "method": "\u5229\u7528 dashcam \u89c6\u9891\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 7,000 \u591a\u5e27\u7684\u81ea\u6ce8\u91ca\u6570\u636e\u96c6\uff0c\u5e76\u5fae\u8c03\u4e86 Ultralytics YOLO \u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u5751\u6d1e\u68c0\u6d4b\u3002\u7ed3\u5408\u81ea\u5b9a\u4e49 OCR \u6a21\u5757\u63d0\u53d6\u65f6\u95f4\u6233\u5e76\u4e0e GPS \u65e5\u5fd7\u540c\u6b65\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u5730\u7406\u6807\u8bb0\u3002\u6570\u636e\u5b58\u50a8\u5728\u6570\u636e\u5e93\u4e2d\u5e76\u901a\u8fc7 OSM \u7528\u6237\u53cb\u597d\u7684\u7f51\u7edc\u754c\u9762\u53ef\u89c6\u5316\u3002", "result": "iWatchRoad \u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u653f\u5e9c\u517c\u5bb9\u7684\u8f93\u51fa\uff0c\u7528\u4e8e\u9053\u8def\u8bc4\u4f30\u548c\u7ef4\u62a4\u89c4\u5212\u3002", "conclusion": "iWatchRoad \u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u3001\u786c\u4ef6\u6548\u7387\u9ad8\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u68c0\u6d4b\u548c\u6620\u5c04\u9053\u8def\u5751\u6d1e\uff0c\u7279\u522b\u9002\u7528\u4e8e\u53d1\u5c55\u4e2d\u56fd\u5bb6\u5982\u5370\u5ea6\u7684\u9053\u8def\u7ba1\u7406\u3002"}}
{"id": "2508.10972", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.10972", "abs": "https://arxiv.org/abs/2508.10972", "authors": ["Rosiana Natalie", "Wenqian Xu", "Ruei-Che Chang", "Rada Mihalcea", "Anhong Guo"], "title": "Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision", "comment": null, "summary": "Advances in vision language models (VLMs) have enabled the simulation of\ngeneral human behavior through their reasoning and problem solving\ncapabilities. However, prior research has not investigated such simulation\ncapabilities in the accessibility domain. In this paper, we evaluate the extent\nto which VLMs can simulate the vision perception of low vision individuals when\ninterpreting images. We first compile a benchmark dataset through a survey\nstudy with 40 low vision participants, collecting their brief and detailed\nvision information and both open-ended and multiple-choice image perception and\nrecognition responses to up to 25 images. Using these responses, we construct\nprompts for VLMs (GPT-4o) to create simulated agents of each participant,\nvarying the included information on vision information and example image\nresponses. We evaluate the agreement between VLM-generated responses and\nparticipants' original answers. Our results indicate that VLMs tend to infer\nbeyond the specified vision ability when given minimal prompts, resulting in\nlow agreement (0.59). The agreement between the agent' and participants'\nresponses remains low when only either the vision information (0.59) or example\nimage responses (0.59) are provided, whereas a combination of both\nsignificantly increase the agreement (0.70, p < 0.0001). Notably, a single\nexample combining both open-ended and multiple-choice responses, offers\nsignificant performance improvements over either alone (p < 0.0001), while\nadditional examples provided minimal benefits (p > 0.05).", "AI": {"tldr": "VLMs\u5728\u6a21\u62df\u4f4e\u89c6\u529b\u89c6\u89c9\u611f\u77e5\u65f6\uff0c\u7ec4\u5408\u89c6\u89c9\u4fe1\u606f\u548c\u793a\u4f8b\u56fe\u50cf\u54cd\u5e94\u80fd\u663e\u8457\u63d0\u5347\u4e00\u81f4\u6027\uff080.70\uff09\uff0c\u4f46\u989d\u5916\u793a\u4f8b\u6548\u679c\u6709\u9650\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u65e0\u969c\u788d\u9886\u57df\u6a21\u62df\u4f4e\u89c6\u529b\u4e2a\u4f53\u89c6\u89c9\u611f\u77e5\u7684\u80fd\u529b\uff0c\u586b\u8865\u6b64\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8c03\u67e540\u540d\u4f4e\u89c6\u529b\u53c2\u4e0e\u8005\uff0c\u6536\u96c6\u4ed6\u4eec\u7684\u89c6\u89c9\u4fe1\u606f\u548c\u56fe\u50cf\u611f\u77e5\u54cd\u5e94\uff0c\u6784\u5efa\u57fa\u51c6\u6570\u636e\u96c6\u3002\u4f7f\u7528\u8fd9\u4e9b\u6570\u636e\u4e3aGPT-4o\u751f\u6210\u63d0\u793a\uff0c\u6a21\u62df\u6bcf\u4e2a\u53c2\u4e0e\u8005\u7684\u4ee3\u7406\uff0c\u5e76\u8bc4\u4f30VLM\u751f\u6210\u54cd\u5e94\u4e0e\u539f\u59cb\u7b54\u6848\u7684\u4e00\u81f4\u6027\u3002", "result": "VLMs\u5728\u6700\u5c0f\u63d0\u793a\u4e0b\u503e\u5411\u4e8e\u63a8\u65ad\u8d85\u51fa\u6307\u5b9a\u89c6\u89c9\u80fd\u529b\uff0c\u5bfc\u81f4\u4e00\u81f4\u6027\u8f83\u4f4e\uff080.59\uff09\u3002\u4ec5\u63d0\u4f9b\u89c6\u89c9\u4fe1\u606f\u6216\u793a\u4f8b\u56fe\u50cf\u54cd\u5e94\u65f6\u4e00\u81f4\u6027\u4ecd\u4f4e\uff080.59\uff09\uff0c\u800c\u7ec4\u5408\u4e24\u8005\u5219\u663e\u8457\u63d0\u9ad8\u4e00\u81f4\u6027\uff080.70\uff09\u3002", "conclusion": "VLMs\u5728\u6a21\u62df\u4f4e\u89c6\u529b\u4e2a\u4f53\u7684\u89c6\u89c9\u611f\u77e5\u65f6\uff0c\u5f53\u63d0\u793a\u4e2d\u5305\u542b\u89c6\u89c9\u4fe1\u606f\u548c\u793a\u4f8b\u56fe\u50cf\u54cd\u5e94\u7684\u7ec4\u5408\u65f6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u4e0e\u53c2\u4e0e\u8005\u539f\u59cb\u7b54\u6848\u7684\u4e00\u81f4\u6027\uff080.70\uff09\u3002\u5355\u4e2a\u7ed3\u5408\u5f00\u653e\u5f0f\u548c\u591a\u9009\u9898\u54cd\u5e94\u7684\u793a\u4f8b\u5df2\u8db3\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u800c\u989d\u5916\u793a\u4f8b\u7684\u6536\u76ca\u6709\u9650\u3002"}}
{"id": "2508.11286", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11286", "abs": "https://arxiv.org/abs/2508.11286", "authors": ["Che Rin Yu", "Daewon Chae", "Dabin Seo", "Sangwon Lee", "Hyeongwoo Im", "Jinkyu Kim"], "title": "Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent", "comment": null, "summary": "When humans perform everyday tasks, we naturally adjust our actions based on\nthe current state of the environment. For instance, if we intend to put\nsomething into a drawer but notice it is closed, we open it first. However,\nmany autonomous robots lack this adaptive awareness. They often follow\npre-planned actions that may overlook subtle yet critical changes in the scene,\nwhich can result in actions being executed under outdated assumptions and\neventual failure. While replanning is critical for robust autonomy, most\nexisting methods respond only after failures occur, when recovery may be\ninefficient or infeasible. While proactive replanning holds promise for\npreventing failures in advance, current solutions often rely on manually\ndesigned rules and extensive supervision. In this work, we present a proactive\nreplanning framework that detects and corrects failures at subtask boundaries\nby comparing scene graphs constructed from current RGB-D observations against\nreference graphs extracted from successful demonstrations. When the current\nscene fails to align with reference trajectories, a lightweight reasoning\nmodule is activated to diagnose the mismatch and adjust the plan. Experiments\nin the AI2-THOR simulator demonstrate that our approach detects semantic and\nspatial mismatches before execution failures occur, significantly improving\ntask success and robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u573a\u666f\u56fe\u6bd4\u8f83\u7684\u4e3b\u52a8\u91cd\u65b0\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u63a8\u7406\u6a21\u5757\u5728\u5b50\u4efb\u52a1\u8fb9\u754c\u68c0\u6d4b\u548c\u7ea0\u6b63\u4e0d\u5339\u914d\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u63d0\u524d\u9884\u9632\u5931\u8d25\uff0c\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u6839\u636e\u73af\u5883\u72b6\u6001\u8c03\u6574\u884c\u52a8\uff0c\u800c\u8bb8\u591a\u81ea\u4e3b\u673a\u5668\u4eba\u7f3a\u4e4f\u8fd9\u79cd\u9002\u5e94\u6027\uff0c\u5bfc\u81f4\u5728\u8fc7\u65f6\u5047\u8bbe\u4e0b\u6267\u884c\u52a8\u4f5c\u5e76\u6700\u7ec8\u5931\u8d25\u3002\u73b0\u6709\u7684\u91cd\u65b0\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u5728\u5931\u8d25\u540e\u54cd\u5e94\uff0c\u800c\u4e3b\u52a8\u91cd\u65b0\u89c4\u5212\u6709\u671b\u63d0\u524d\u9884\u9632\u5931\u8d25\uff0c\u4f46\u5f53\u524d\u89e3\u51b3\u65b9\u6848\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u89c4\u5219\u548c\u5927\u91cf\u76d1\u7763\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5f53\u524dRGB-D\u89c2\u6d4b\u6784\u5efa\u7684\u573a\u666f\u56fe\u4e0e\u4ece\u6210\u529f\u6f14\u793a\u4e2d\u63d0\u53d6\u7684\u53c2\u8003\u56fe\uff0c\u5f53\u5f53\u524d\u573a\u666f\u4e0e\u53c2\u8003\u8f68\u8ff9\u4e0d\u4e00\u81f4\u65f6\uff0c\u6fc0\u6d3b\u8f7b\u91cf\u7ea7\u63a8\u7406\u6a21\u5757\u6765\u8bca\u65ad\u4e0d\u5339\u914d\u5e76\u8c03\u6574\u8ba1\u5212\u3002", "result": "\u5728AI2-THOR\u6a21\u62df\u5668\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6267\u884c\u5931\u8d25\u53d1\u751f\u524d\u68c0\u6d4b\u8bed\u4e49\u548c\u7a7a\u95f4\u4e0d\u5339\u914d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u91cd\u65b0\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5b50\u4efb\u52a1\u8fb9\u754c\u68c0\u6d4b\u548c\u7ea0\u6b63\u5931\u8d25\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.10946", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.10946", "abs": "https://arxiv.org/abs/2508.10946", "authors": ["Wonho Lee", "Hyunsik Na", "Jisu Lee", "Daeseon Choi"], "title": "IPG: Incremental Patch Generation for Generalized Adversarial Patch Training", "comment": null, "summary": "The advent of adversarial patches poses a significant challenge to the\nrobustness of AI models, particularly in the domain of computer vision tasks\nsuch as object detection. In contradistinction to traditional adversarial\nexamples, these patches target specific regions of an image, resulting in the\nmalfunction of AI models. This paper proposes Incremental Patch Generation\n(IPG), a method that generates adversarial patches up to 11.1 times more\nefficiently than existing approaches while maintaining comparable attack\nperformance. The efficacy of IPG is demonstrated by experiments and ablation\nstudies including YOLO's feature distribution visualization and adversarial\ntraining results, which show that it produces well-generalized patches that\neffectively cover a broader range of model vulnerabilities. Furthermore,\nIPG-generated datasets can serve as a robust knowledge foundation for\nconstructing a robust model, enabling structured representation, advanced\nreasoning, and proactive defenses in AI security ecosystems. The findings of\nthis study suggest that IPG has considerable potential for future utilization\nnot only in adversarial patch defense but also in real-world applications such\nas autonomous vehicles, security systems, and medical imaging, where AI models\nmust remain resilient to adversarial attacks in dynamic and high-stakes\nenvironments.", "AI": {"tldr": "IPG\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5bf9\u6297\u8865\u4e01\u751f\u6210\u65b9\u6cd5\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb11.1\u500d\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9ad8\u98ce\u9669AI\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u5bf9\u6297\u8865\u4e01\u5bf9AI\u6a21\u578b\uff08\u5c24\u5176\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff09\u7684\u9c81\u68d2\u6027\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u589e\u91cf\u8865\u4e01\u751f\u6210\uff08IPG\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff08\u5982YOLO\u7279\u5f81\u5206\u5e03\u53ef\u89c6\u5316\u548c\u5bf9\u6297\u8bad\u7ec3\u7ed3\u679c\uff09\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "IPG\u751f\u6210\u5bf9\u6297\u8865\u4e01\u7684\u6548\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad811.1\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u653b\u51fb\u6027\u80fd\uff0c\u5e76\u80fd\u8986\u76d6\u66f4\u5e7f\u6cdb\u7684\u6a21\u578b\u6f0f\u6d1e\u3002", "conclusion": "IPG\u5c55\u73b0\u4e86\u5728\u5bf9\u6297\u8865\u4e01\u9632\u5fa1\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981AI\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u52a8\u6001\u73af\u5883\u4e2d\u4fdd\u6301\u97e7\u6027\u7684\u9886\u57df\u3002"}}
{"id": "2508.11289", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11289", "abs": "https://arxiv.org/abs/2508.11289", "authors": ["Lin Li", "Xueming Liu", "Zhoujingzi Qiu", "Tianjiang Hu", "Qingrui Zhang"], "title": "A Recursive Total Least Squares Solution for Bearing-Only Target Motion Analysis and Circumnavigation", "comment": "Accepted by 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), 6 Pages", "summary": "Bearing-only Target Motion Analysis (TMA) is a promising technique for\npassive tracking in various applications as a bearing angle is easy to measure.\nDespite its advantages, bearing-only TMA is challenging due to the nonlinearity\nof the bearing measurement model and the lack of range information, which\nimpairs observability and estimator convergence. This paper addresses these\nissues by proposing a Recursive Total Least Squares (RTLS) method for online\ntarget localization and tracking using mobile observers. The RTLS approach,\ninspired by previous results on Total Least Squares (TLS), mitigates biases in\nposition estimation and improves computational efficiency compared to\npseudo-linear Kalman filter (PLKF) methods. Additionally, we propose a\ncircumnavigation controller to enhance system observability and estimator\nconvergence by guiding the mobile observer in orbit around the target.\nExtensive simulations and experiments are performed to demonstrate the\neffectiveness and robustness of the proposed method. The proposed algorithm is\nalso compared with the state-of-the-art approaches, which confirms its superior\nperformance in terms of both accuracy and stability.", "AI": {"tldr": "\u63d0\u51faRTLS\u65b9\u6cd5\u548c\u73af\u7ed5\u5bfc\u822a\u63a7\u5236\u5668\uff0c\u89e3\u51b3\u4e86bearing-only TMA\u7684\u53ef\u89c2\u6d4b\u6027\u548c\u6536\u655b\u6027\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "bearing-only TMA\u56e0\u6d4b\u91cf\u6a21\u578b\u975e\u7ebf\u6027\u548c\u7f3a\u4e4f\u8ddd\u79bb\u4fe1\u606f\u800c\u9762\u4e34\u53ef\u89c2\u6d4b\u6027\u548c\u4f30\u8ba1\u5668\u6536\u655b\u6027\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9012\u5f52\u603b\u4f53\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08RTLS\uff09\u7684\u5728\u7ebf\u76ee\u6807\u5b9a\u4f4d\u4e0e\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86\u73af\u7ed5\u5bfc\u822a\u63a7\u5236\u5668\u4ee5\u589e\u5f3a\u7cfb\u7edf\u53ef\u89c2\u6d4b\u6027\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u4f2a\u7ebf\u6027\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08PLKF\uff09\u7b49\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "The proposed RTLS\u65b9\u6cd5\u548ccircumnavigation\u63a7\u5236\u5668\u663e\u8457\u63d0\u9ad8\u4e86bearing-only TMA\u7684\u6027\u80fd\uff0c\u5728\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.10947", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10947", "abs": "https://arxiv.org/abs/2508.10947", "authors": ["Ronghao Xu", "Zhen Huang", "Yangbo Wei", "Xiaoqian Zhou", "Zikang Xu", "Ting Liu", "Zihang Jiang", "S. Kevin Zhou"], "title": "MedAtlas: Evaluating LLMs for Multi-Round, Multi-Task Medical Reasoning Across Diverse Imaging Modalities and Clinical Text", "comment": null, "summary": "Artificial intelligence has demonstrated significant potential in clinical\ndecision-making; however, developing models capable of adapting to diverse\nreal-world scenarios and performing complex diagnostic reasoning remains a\nmajor challenge. Existing medical multi-modal benchmarks are typically limited\nto single-image, single-turn tasks, lacking multi-modal medical image\nintegration and failing to capture the longitudinal and multi-modal interactive\nnature inherent to clinical practice. To address this gap, we introduce\nMedAtlas, a novel benchmark framework designed to evaluate large language\nmodels on realistic medical reasoning tasks. MedAtlas is characterized by four\nkey features: multi-turn dialogue, multi-modal medical image interaction,\nmulti-task integration, and high clinical fidelity. It supports four core\ntasks: open-ended multi-turn question answering, closed-ended multi-turn\nquestion answering, multi-image joint reasoning, and comprehensive disease\ndiagnosis. Each case is derived from real diagnostic workflows and incorporates\ntemporal interactions between textual medical histories and multiple imaging\nmodalities, including CT, MRI, PET, ultrasound, and X-ray, requiring models to\nperform deep integrative reasoning across images and clinical texts. MedAtlas\nprovides expert-annotated gold standards for all tasks. Furthermore, we propose\ntwo novel evaluation metrics: Round Chain Accuracy and Error Propagation\nResistance. Benchmark results with existing multi-modal models reveal\nsubstantial performance gaps in multi-stage clinical reasoning. MedAtlas\nestablishes a challenging evaluation platform to advance the development of\nrobust and trustworthy medical AI.", "AI": {"tldr": "MedAtlas\u662f\u4e00\u4e2a\u65b0\u578b\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u533b\u7597\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u533b\u5b66\u591a\u6a21\u6001\u57fa\u51c6\u5728\u5355\u56fe\u50cf\u3001\u5355\u8f6e\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u7f3a\u4e4f\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u6574\u5408\uff0c\u65e0\u6cd5\u6355\u6349\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u56fa\u6709\u7684\u7eb5\u5411\u548c\u591a\u6a21\u6001\u4ea4\u4e92\u7279\u6027\u3002", "method": "\u5f15\u5165\u4e86MedAtlas\u8fd9\u4e00\u65b0\u578b\u57fa\u51c6\u6846\u67b6\uff0c\u652f\u6301\u591a\u8f6e\u5bf9\u8bdd\u3001\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u4ea4\u4e92\u3001\u591a\u4efb\u52a1\u96c6\u6210\u548c\u9ad8\u4e34\u5e8a\u4fdd\u771f\u5ea6\uff0c\u6db5\u76d6\u56db\u79cd\u6838\u5fc3\u4efb\u52a1\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u5728\u591a\u9636\u6bb5\u4e34\u5e8a\u63a8\u7406\u4e2d\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "MedAtlas \u5efa\u7acb\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u65e8\u5728\u63a8\u52a8\u7a33\u5065\u4e14\u53ef\u4fe1\u8d56\u7684\u533b\u7597AI\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.11396", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11396", "abs": "https://arxiv.org/abs/2508.11396", "authors": ["Jingran Zhang", "Zhengzhang Yan", "Yiming Chen", "Zeqiang He", "Jiahao Chen"], "title": "Pedestrian Dead Reckoning using Invariant Extended Kalman Filter", "comment": null, "summary": "This paper presents a cost-effective inertial pedestrian dead reckoning\nmethod for the bipedal robot in the GPS-denied environment. Each time when the\ninertial measurement unit (IMU) is on the stance foot, a stationary\npseudo-measurement can be executed to provide innovation to the IMU measurement\nbased prediction. The matrix Lie group based theoretical development of the\nadopted invariant extended Kalman filter (InEKF) is set forth for tutorial\npurpose. Three experiments are conducted to compare between InEKF and standard\nEKF, including motion capture benchmark experiment, large-scale multi-floor\nwalking experiment, and bipedal robot experiment, as an effort to show our\nmethod's feasibility in real-world robot system. In addition, a sensitivity\nanalysis is included to show that InEKF is much easier to tune than EKF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u60ef\u6027\u884c\u4eba\u822a\u4f4d\u63a8\u7b97\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eGPS\u7f3a\u5931\u73af\u5883\u4e2d\u7684\u53cc\u8db3\u673a\u5668\u4eba\uff0c\u901a\u8fc7InEKF\u5b9e\u73b0\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u6807\u51c6EKF\u4e14\u66f4\u6613\u8c03\u6574\u3002", "motivation": "\u89e3\u51b3GPS\u7f3a\u5931\u73af\u5883\u4e0b\u53cc\u8db3\u673a\u5668\u4eba\u7684\u5b9a\u4f4d\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u77e9\u9635Lie\u7fa4\u7684\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08InEKF\uff09\uff0c\u901a\u8fc7\u5728\u7ad9\u7acb\u8db3\u4e0a\u7684\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u6267\u884c\u4f2a\u6d4b\u91cf\u6765\u63d0\u4f9b\u521b\u65b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cInEKF\u5728\u8fd0\u52a8\u6355\u6349\u57fa\u51c6\u5b9e\u9a8c\u3001\u5927\u89c4\u6a21\u591a\u5c42\u884c\u8d70\u5b9e\u9a8c\u548c\u53cc\u8db3\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u5747\u4f18\u4e8e\u6807\u51c6EKF\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u60ef\u6027\u884c\u4eba\u822a\u4f4d\u63a8\u7b97\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eGPS\u7f3a\u5931\u73af\u5883\u4e2d\u7684\u53cc\u8db3\u673a\u5668\u4eba\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u4e14InEKF\u6bd4\u6807\u51c6EKF\u66f4\u5bb9\u6613\u8c03\u6574\u3002"}}
{"id": "2508.10950", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10950", "abs": "https://arxiv.org/abs/2508.10950", "authors": ["Xinyi Wang", "Michael Barnett", "Frederique Boonstra", "Yael Barnett", "Mariano Cabezas", "Arkiev D'Souza", "Matthew C. Kiernan", "Kain Kyle", "Meng Law", "Lynette Masters", "Zihao Tang", "Stephen Tisch", "Sicong Tu", "Anneke Van Der Walt", "Dongang Wang", "Fernando Calamante", "Weidong Cai", "Chenyu Wang"], "title": "From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement", "comment": "24 pages, 5 figures", "summary": "Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling\ntechnique that represents complex white matter fiber configurations, and a key\nstep for subsequent brain tractography and connectome analysis. Its reliability\nand accuracy, however, heavily rely on the quality of the MRI acquisition and\nthe subsequent estimation of the FODs at each voxel. Generating reliable FODs\nfrom widely available clinical protocols with single-shell and\nlow-angular-resolution acquisitions remains challenging but could potentially\nbe addressed with recent advances in deep learning-based enhancement\ntechniques. Despite advancements, existing methods have predominantly been\nassessed on healthy subjects, which have proved to be a major hurdle for their\nclinical adoption. In this work, we validate a newly optimized enhancement\nframework, FastFOD-Net, across healthy controls and six neurological disorders.\nThis accelerated end-to-end deep learning framework enhancing FODs with\nsuperior performance and delivering training/inference efficiency for clinical\nuse ($60\\times$ faster comparing to its predecessor). With the most\ncomprehensive clinical evaluation to date, our work demonstrates the potential\nof FastFOD-Net in accelerating clinical neuroscience research, empowering\ndiffusion MRI analysis for disease differentiation, improving interpretability\nin connectome applications, and reducing measurement errors to lower sample\nsize requirements. Critically, this work will facilitate the more widespread\nadoption of, and build clinical trust in, deep learning based methods for\ndiffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of\nreal-world, clinical diffusion MRI data, comparable to that achievable with\nhigh-quality research acquisitions.", "AI": {"tldr": "FastFOD-Net \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u6269\u6563 MRI \u4e2d\u7684\u7ea4\u7ef4\u65b9\u5411\u5206\u5e03\uff08FODs\uff09\uff0c\u5728\u5065\u5eb7\u548c\u75be\u75c5\u4eba\u7fa4\u4e2d\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e34\u5e8a\u5e94\u7528\u7684\u53ef\u884c\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5728\u6269\u6563 MRI \u5efa\u6a21\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5728\u5065\u5eb7\u53d7\u8bd5\u8005\u4e0a\u8bc4\u4f30\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u5e94\u7528\u3002FastFOD-Net \u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5065\u5eb7\u5bf9\u7167\u548c\u516d\u79cd\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "method": "FastFOD-Net \u662f\u4e00\u4e2a\u52a0\u901f\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a FODs\uff0c\u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\u548c\u4e34\u5e8a\u4f7f\u7528\u7684\u8bad\u7ec3/\u63a8\u7406\u6548\u7387\uff08\u6bd4\u5176\u524d\u8eab\u5feb 60 \u500d\uff09\u3002", "result": "FastFOD-Net \u5728\u4e34\u5e8a\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u6269\u6563 MRI \u6570\u636e\u8fdb\u884c\u7a33\u5065\u5206\u6790\uff0c\u6027\u80fd\u4e0e\u9ad8\u8d28\u91cf\u7814\u7a76\u91c7\u96c6\u76f8\u5f53\u3002", "conclusion": "FastFOD-Net \u7684\u4e34\u5e8a\u9a8c\u8bc1\u5c55\u793a\u4e86\u5176\u5728\u52a0\u901f\u4e34\u5e8a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u3001\u63d0\u5347\u75be\u75c5\u533a\u5206\u80fd\u529b\u3001\u6539\u5584\u8fde\u63a5\u7ec4\u5e94\u7528\u7684\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u964d\u4f4e\u6d4b\u91cf\u8bef\u5dee\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u6df1\u5ea6\u5b66\u4e60\u5728\u6269\u6563 MRI \u589e\u5f3a\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u548c\u4e34\u5e8a\u4fe1\u4efb\u3002"}}
{"id": "2508.11141", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11141", "abs": "https://arxiv.org/abs/2508.11141", "authors": ["Bin Ma", "Yifei Zhang", "Yongjin Xian", "Qi Li", "Linna Zhou", "Gongxun Miao"], "title": "A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations", "comment": null, "summary": "Existing rumor detection methods often neglect the content within images as\nwell as the inherent relationships between contexts and images across different\nvisual scales, thereby resulting in the loss of critical information pertinent\nto rumor identification. To address these issues, this paper presents a novel\ncross-modal rumor detection scheme based on contrastive learning, namely the\nMulti-scale Image and Context Correlation exploration algorithm (MICC).\nSpecifically, we design an SCLIP encoder to generate unified semantic\nembeddings for text and multi-scale image patches through contrastive\npretraining, enabling their relevance to be measured via dot-product\nsimilarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is\nintroduced to identify image regions most relevant to the textual semantics,\nguided by mutual information maximization and the information bottleneck\nprinciple, through a Top-K selection strategy based on a cross-modal relevance\nmatrix constructed between the text and multi-scale image patches. Moreover, a\nscale-aware fusion network is designed to integrate the highly correlated\nmulti-scale image features with global text features by assigning adaptive\nweights to image regions based on their semantic importance and cross-modal\nrelevance. The proposed methodology has been extensively evaluated on two\nreal-world datasets. The experimental results demonstrate that it achieves a\nsubstantial performance improvement over existing state-of-the-art approaches\nin rumor detection, highlighting its effectiveness and potential for practical\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u8de8\u6a21\u6001\u8c23\u8a00\u68c0\u6d4b\u65b9\u6848MICC\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u56fe\u50cf\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8c23\u8a00\u68c0\u6d4b\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u56fe\u50cf\u5185\u5bb9\u4ee5\u53ca\u4e0d\u540c\u89c6\u89c9\u5c3a\u5ea6\u4e0b\u4e0a\u4e0b\u6587\u4e0e\u56fe\u50cf\u4e4b\u95f4\u7684\u5185\u5728\u5173\u7cfb\uff0c\u5bfc\u81f4\u5173\u952e\u4fe1\u606f\u4e22\u5931\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2aSCLIP\u7f16\u7801\u5668\u751f\u6210\u7edf\u4e00\u7684\u8bed\u4e49\u5d4c\u5165\uff0c\u5f15\u5165\u4e86\u8de8\u6a21\u6001\u591a\u5c3a\u5ea6\u5bf9\u9f50\u6a21\u5757\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5c3a\u5ea6\u611f\u77e5\u878d\u5408\u7f51\u7edc\u6765\u6574\u5408\u591a\u5c3a\u5ea6\u56fe\u50cf\u7279\u5f81\u4e0e\u5168\u5c40\u6587\u672c\u7279\u5f81\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8c23\u8a00\u68c0\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684MICC\u7b97\u6cd5\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u591a\u5c3a\u5ea6\u56fe\u50cf\u4e0e\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8c23\u8a00\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u663e\u793a\u51fa\u5176\u5728\u5b9e\u8df5\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2508.11404", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11404", "abs": "https://arxiv.org/abs/2508.11404", "authors": ["Junyeon Kim", "Tianshu Ruan", "Cesar Alan Contreras", "Manolis Chiou"], "title": "An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration", "comment": null, "summary": "Structural inspection in nuclear facilities is vital for maintaining\noperational safety and integrity. Traditional methods of manual inspection pose\nsignificant challenges, including safety risks, high cognitive demands, and\npotential inaccuracies due to human limitations. Recent advancements in\nArtificial Intelligence (AI) and robotic technologies have opened new\npossibilities for safer, more efficient, and accurate inspection methodologies.\nSpecifically, Human-Robot Collaboration (HRC), leveraging robotic platforms\nequipped with advanced detection algorithms, promises significant improvements\nin inspection outcomes and reductions in human workload. This study explores\nthe effectiveness of AI-assisted visual crack detection integrated into a\nmobile Jackal robot platform. The experiment results indicate that HRC enhances\ninspection accuracy and reduces operator workload, resulting in potential\nsuperior performance outcomes compared to traditional manual methods.", "AI": {"tldr": "AI\u8f85\u52a9\u7684\u79fb\u52a8\u673a\u5668\u4eba\u68c0\u67e5\u5728\u6838\u8bbe\u65bd\u4e2d\u6bd4\u4eba\u5de5\u65b9\u6cd5\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u51cf\u5c11\u5de5\u4f5c\u8d1f\u8377\u3002", "motivation": "\u4f20\u7edf\u6838\u8bbe\u65bd\u7ed3\u6784\u4eba\u5de5\u68c0\u67e5\u5b58\u5728\u5b89\u5168\u98ce\u9669\u3001\u8ba4\u77e5\u8d1f\u8377\u9ad8\u53ca\u4eba\u4e3a\u8bef\u5dee\u95ee\u9898\uff0cAI\u4e0e\u673a\u5668\u4eba\u6280\u672f\u7684\u8fdb\u6b65\u4e3a\u66f4\u5b89\u5168\u3001\u9ad8\u6548\u3001\u51c6\u786e\u7684\u68c0\u67e5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528AI\u8f85\u52a9\u89c6\u89c9\u88c2\u7eb9\u68c0\u6d4b\u6280\u672f\uff0c\u96c6\u6210\u5230\u79fb\u52a8Jackal\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHRC\u80fd\u63d0\u5347\u68c0\u67e5\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u64cd\u4f5c\u5458\u5de5\u4f5c\u8d1f\u8377\u3002", "conclusion": "HRC\u7ed3\u5408AI\u89c6\u89c9\u88c2\u7eb9\u68c0\u6d4b\u7684\u79fb\u52a8Jackal\u673a\u5668\u4eba\u5e73\u53f0\u5728\u6838\u8bbe\u65bd\u7ed3\u6784\u68c0\u67e5\u4e2d\u5c55\u73b0\u51fa\u6bd4\u4f20\u7edf\u4eba\u5de5\u65b9\u6cd5\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u4f4e\u7684\u5de5\u4f5c\u8d1f\u8377\u3002"}}
{"id": "2508.10955", "categories": ["cs.CV", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.10955", "abs": "https://arxiv.org/abs/2508.10955", "authors": ["Wenbin An", "Jiahao Nie", "Yaqiang Wu", "Feng Tian", "Shijian Lu", "Qinghua Zheng"], "title": "Empowering Multimodal LLMs with External Tools: A Comprehensive Survey", "comment": "21 pages, 361 references", "summary": "By integrating the perception capabilities of multimodal encoders with the\ngenerative power of Large Language Models (LLMs), Multimodal Large Language\nModels (MLLMs), exemplified by GPT-4V, have achieved great success in various\nmultimodal tasks, pointing toward a promising pathway to artificial general\nintelligence. Despite this progress, the limited quality of multimodal data,\npoor performance on many complex downstream tasks, and inadequate evaluation\nprotocols continue to hinder the reliability and broader applicability of MLLMs\nacross diverse domains. Inspired by the human ability to leverage external\ntools for enhanced reasoning and problem-solving, augmenting MLLMs with\nexternal tools (e.g., APIs, expert models, and knowledge bases) offers a\npromising strategy to overcome these challenges. In this paper, we present a\ncomprehensive survey on leveraging external tools to enhance MLLM performance.\nOur discussion is structured along four key dimensions about external tools:\n(1) how they can facilitate the acquisition and annotation of high-quality\nmultimodal data; (2) how they can assist in improving MLLM performance on\nchallenging downstream tasks; (3) how they enable comprehensive and accurate\nevaluation of MLLMs; (4) the current limitations and future directions of\ntool-augmented MLLMs. Through this survey, we aim to underscore the\ntransformative potential of external tools in advancing MLLM capabilities,\noffering a forward-looking perspective on their development and applications.\nThe project page of this paper is publicly available\nathttps://github.com/Lackel/Awesome-Tools-for-MLLMs.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5229\u7528\u5916\u90e8\u5de5\u5177\uff08\u5982API\u3001\u4e13\u5bb6\u6a21\u578b\u548c\u77e5\u8bc6\u5e93\uff09\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u6570\u636e\u83b7\u53d6\u3001\u4efb\u52a1\u8868\u73b0\u3001\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u591a\u6a21\u6001\u6570\u636e\u7684\u8d28\u91cf\u6709\u9650\u3001\u590d\u6742\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u4e0d\u4f73\u4ee5\u53ca\u8bc4\u4f30\u534f\u8bae\u4e0d\u8db3\u7b49\u95ee\u9898\u9650\u5236\u4e86\u5176\u53ef\u9760\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002\u53d7\u4eba\u7c7b\u5229\u7528\u5916\u90e8\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u542f\u53d1\uff0c\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5916\u90e8\u5de5\u5177\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5bf9\u5229\u7528\u5916\u90e8\u5de5\u5177\u589e\u5f3aMLLM\u6027\u80fd\u7684\u5168\u9762\u8c03\u67e5\uff0c\u672c\u6587\u56f4\u7ed5\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\u5c55\u5f00\u8ba8\u8bba\uff1a\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u7684\u83b7\u53d6\u4e0e\u6807\u6ce8\u3001\u590d\u6742\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u63d0\u5347\u3001MLLMs\u7684\u5168\u9762\u51c6\u786e\u8bc4\u4f30\uff0c\u4ee5\u53ca\u5de5\u5177\u589e\u5f3aMLLMs\u7684\u5f53\u524d\u5c40\u9650\u4e0e\u672a\u6765\u65b9\u5411\u3002", "result": "\u672c\u6587\u7cfb\u7edf\u6027\u5730\u603b\u7ed3\u4e86\u5916\u90e8\u5de5\u5177\u5982\u4f55\u4fc3\u8fdb\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u7684\u83b7\u53d6\u4e0e\u6807\u6ce8\u3001\u63d0\u5347MLLMs\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3001\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u5c40\u9650\u4e0e\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5916\u90e8\u5de5\u5177\u5728\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u80fd\u529b\u65b9\u9762\u7684\u53d8\u9769\u6f5c\u529b\uff0c\u5e76\u5c55\u671b\u4e86\u5176\u672a\u6765\u53d1\u5c55\u548c\u5e94\u7528\u65b9\u5411\u3002"}}
{"id": "2508.11406", "categories": ["cs.RO", "cs.AI", "68T40", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.11406", "abs": "https://arxiv.org/abs/2508.11406", "authors": ["Benjamin Alt", "Mareike Picklum", "Sorin Arion", "Franklin Kenghagho Kenfack", "Michael Beetz"], "title": "Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing", "comment": "8 pages, 6 figures, submitted to the 1st IROS Workshop on Embodied AI\n  and Robotics for Future Scientific Discovery", "summary": "We envision a future in which autonomous robots conduct scientific\nexperiments in ways that are not only precise and repeatable, but also open,\ntrustworthy, and transparent. To realize this vision, we present two key\ncontributions: a semantic execution tracing framework that logs sensor data\ntogether with semantically annotated robot belief states, ensuring that\nautomated experimentation is transparent and replicable; and the AICOR Virtual\nResearch Building (VRB), a cloud-based platform for sharing, replicating, and\nvalidating robot task executions at scale. Together, these tools enable\nreproducible, robot-driven science by integrating deterministic execution,\nsemantic memory, and open knowledge representation, laying the foundation for\nautonomous systems to participate in scientific discovery.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u6267\u884c\u8ffd\u8e2a\u6846\u67b6\u548c\u4e91\u5e73\u53f0AICOR VRB\uff0c\u65e8\u5728\u5b9e\u73b0\u53ef\u91cd\u590d\u3001\u900f\u660e\u7684\u673a\u5668\u4eba\u9a71\u52a8\u79d1\u5b66\u7814\u7a76\u3002", "motivation": "\u8bba\u6587\u7684\u52a8\u673a\u662f\u63a8\u52a8\u81ea\u4e3b\u673a\u5668\u4eba\u8fdb\u884c\u79d1\u5b66\u5b9e\u9a8c\uff0c\u4f7f\u5176\u4e0d\u4ec5\u7cbe\u786e\u53ef\u91cd\u590d\uff0c\u800c\u4e14\u5f00\u653e\u3001\u53ef\u4fe1\u548c\u900f\u660e\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u5173\u952e\u6280\u672f\uff1a\u4e00\u662f\u8bed\u4e49\u6267\u884c\u8ffd\u8e2a\u6846\u67b6\uff0c\u8bb0\u5f55\u4f20\u611f\u5668\u6570\u636e\u53ca\u8bed\u4e49\u6ce8\u91ca\u7684\u673a\u5668\u4eba\u4fe1\u5ff5\u72b6\u6001\uff1b\u4e8c\u662fAICOR\u865a\u62df\u7814\u7a76\u5efa\u7b51\uff08VRB\uff09\uff0c\u4e00\u4e2a\u57fa\u4e8e\u4e91\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u5171\u4eab\u3001\u590d\u5236\u548c\u9a8c\u8bc1\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u3002", "result": "\u8bba\u6587\u7684\u6210\u679c\u662f\u5f00\u53d1\u4e86\u4e24\u79cd\u5de5\u5177\uff0c\u786e\u4fdd\u4e86\u81ea\u52a8\u5316\u5b9e\u9a8c\u7684\u900f\u660e\u6027\u548c\u53ef\u590d\u5236\u6027\uff0c\u5e76\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u53c2\u4e0e\u79d1\u5b66\u53d1\u73b0\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u8bba\u6587\u7684\u7ed3\u8bba\u662f\u901a\u8fc7\u8bed\u4e49\u6267\u884c\u8ffd\u8e2a\u6846\u67b6\u548cAICOR\u865a\u62df\u7814\u7a76\u5efa\u7b51\uff08VRB\uff09\u5e73\u53f0\uff0c\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u53c2\u4e0e\u79d1\u5b66\u53d1\u73b0\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u53ef\u91cd\u590d\u3001\u673a\u5668\u4eba\u9a71\u52a8\u7684\u79d1\u5b66\u7814\u7a76\u3002"}}
{"id": "2508.11170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11170", "abs": "https://arxiv.org/abs/2508.11170", "authors": ["Baihong Qian", "Haotian Fan", "Wenjie Liao", "Yunqiu Wang", "Tao Li", "Junhui Cui"], "title": "Better Supervised Fine-tuning for VQA: Integer-Only Loss", "comment": null, "summary": "With the rapid advancement of vision language models(VLM), their ability to\nassess visual content based on specific criteria and dimensions has become\nincreasingly critical for applications such as video-theme consistency\nassessment and visual quality scoring. However, existing methods often suffer\nfrom imprecise results and inefficient loss calculation, which limit the focus\nof the model on key evaluation indicators. To address this, we propose\nIOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to\nenhance their performance in video quality assessment tasks. The key innovation\nof IOVQA lies in its label construction and its targeted loss calculation\nmechanism. Specifically, during dataset curation, we constrain the model's\noutput to integers within the range of [10,50], ensuring numerical stability,\nand convert decimal Overall_MOS to integer before using them as labels. We also\nintroduce a target-mask strategy: when computing the loss, only the first\ntwo-digit-integer of the label is unmasked, forcing the model to learn the\ncritical components of the numerical evaluation. After fine-tuning the\nQwen2.5-VL model using the constructed dataset, experimental results\ndemonstrate that the proposed method significantly improves the model's\naccuracy and consistency in the VQA task, ranking 3rd in VQualA 2025\nGenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work\nhighlights the effectiveness of merely leaving integer labels during\nfine-tuning, providing an effective idea for optimizing VLMs in quantitative\nevaluation scenarios.", "AI": {"tldr": "IOVQA\u901a\u8fc7\u6574\u6570\u6807\u7b7e\u548c\u76ee\u6807\u63a9\u7801\u7b56\u7565\u4f18\u5316VLM\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u4e2d\u5b58\u5728\u7ed3\u679c\u4e0d\u7cbe\u786e\u548c\u635f\u5931\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u5173\u952e\u8bc4\u4f30\u6307\u6807\u7684\u5173\u6ce8\u3002", "method": "\u63d0\u51faIOVQA\u65b9\u6cd5\uff0c\u5305\u62ec\u6574\u6570\u6807\u7b7e\u6784\u9020\u548c\u76ee\u6807\u63a9\u7801\u7b56\u7565\uff0c\u7528\u4e8e\u5fae\u8c03VLM\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728VQA\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728VQualA 2025 GenAI-Bench AIGC\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6311\u6218\u4e2d\u6392\u540d\u7b2c\u4e09\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7IOVQA\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86VLM\u5728\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u4e3a\u4f18\u5316\u5b9a\u91cf\u8bc4\u4f30\u573a\u666f\u4e2d\u7684VLM\u63d0\u4f9b\u4e86\u6709\u6548\u601d\u8def\u3002"}}
{"id": "2508.11453", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11453", "abs": "https://arxiv.org/abs/2508.11453", "authors": ["Jiayue Jin", "Lang Qian", "Jingyu Zhang", "Chuanyu Ju", "Liang Song"], "title": "EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback", "comment": null, "summary": "Recent years have witnessed remarkable progress in autonomous driving, with\nsystems evolving from modular pipelines to end-to-end architectures. However,\nmost existing methods are trained offline and lack mechanisms to adapt to new\nenvironments during deployment. As a result, their generalization ability\ndiminishes when faced with unseen variations in real-world driving scenarios.\nIn this paper, we break away from the conventional \"train once, deploy forever\"\nparadigm and propose EvoPSF, a novel online Evolution framework for autonomous\ndriving based on Planning-State Feedback. We argue that planning failures are\nprimarily caused by inaccurate object-level motion predictions, and such\nfailures are often reflected in the form of increased planner uncertainty. To\naddress this, we treat planner uncertainty as a trigger for online evolution,\nusing it as a diagnostic signal to initiate targeted model updates. Rather than\nperforming blind updates, we leverage the planner's agent-agent attention to\nidentify the specific objects that the ego vehicle attends to most, which are\nprimarily responsible for the planning failures. For these critical objects, we\ncompute a targeted self-supervised loss by comparing their predicted waypoints\nfrom the prediction module with their actual future positions, selected from\nthe perception module's outputs with high confidence scores. This loss is then\nbackpropagated to adapt the model online. As a result, our method improves the\nmodel's robustness to environmental changes, leads to more precise motion\npredictions, and therefore enables more accurate and stable planning behaviors.\nExperiments on both cross-region and corrupted variants of the nuScenes dataset\ndemonstrate that EvoPSF consistently improves planning performance under\nchallenging conditions.", "AI": {"tldr": "EvoPSF\u662f\u4e00\u79cd\u5728\u7ebf\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u89c4\u5212\u72b6\u6001\u53cd\u9988\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5bf9\u65b0\u73af\u5883\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u591a\u4e3a\u79bb\u7ebf\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u5728\u7ebf\u9002\u5e94\u673a\u5236\uff0c\u5bfc\u81f4\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9762\u5bf9\u672a\u89c1\u8fc7\u53d8\u5316\u65f6\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5212\u72b6\u6001\u53cd\u9988\u7684\u5728\u7ebf\u8fdb\u5316\u6846\u67b6EvoPSF\uff0c\u5229\u7528\u89c4\u5212\u5668\u7684\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u89e6\u53d1\u4fe1\u53f7\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u635f\u5931\u5bf9\u5173\u952e\u5bf9\u8c61\u8fdb\u884c\u9488\u5bf9\u6027\u6a21\u578b\u66f4\u65b0\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u7684\u8de8\u533a\u57df\u548c\u635f\u574f\u53d8\u4f53\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cEvoPSF\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u6301\u7eed\u63d0\u5347\u89c4\u5212\u6027\u80fd\u3002", "conclusion": "EvoPSF\u901a\u8fc7\u5728\u7ebf\u8fdb\u5316\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u89c4\u5212\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u65b0\u73af\u5883\u6216\u53d8\u5316\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.10962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10962", "abs": "https://arxiv.org/abs/2508.10962", "authors": ["Jiarong Li", "Imad Ali Shah", "Diarmaid Geever", "Fiachra Collins", "Enda Ward", "Martin Glavin", "Edward Jones", "Brian Deegan"], "title": "CSNR and JMIM Based Spectral Band Selection for Reducing Metamerism in Urban Driving", "comment": "Under Review at IEEE OJITS, July, 2025", "summary": "Protecting Vulnerable Road Users (VRU) is a critical safety challenge for\nautomotive perception systems, particularly under visual ambiguity caused by\nmetamerism, a phenomenon where distinct materials appear similar in RGB\nimagery. This work investigates hyperspectral imaging (HSI) to overcome this\nlimitation by capturing unique material signatures beyond the visible spectrum,\nespecially in the Near-Infrared (NIR). To manage the inherent\nhigh-dimensionality of HSI data, we propose a band selection strategy that\nintegrates information theory techniques (joint mutual information\nmaximization, correlation analysis) with a novel application of an image\nquality metric (contrast signal-to-noise ratio) to identify the most spectrally\ninformative bands. Using the Hyperspectral City V2 (H-City) dataset, we\nidentify three informative bands (497 nm, 607 nm, and 895 nm, $\\pm$27 nm) and\nreconstruct pseudo-color images for comparison with co-registered RGB.\nQuantitative results demonstrate increased dissimilarity and perceptual\nseparability of VRU from the background. The selected HSI bands yield\nimprovements of 70.24%, 528.46%, 1206.83%, and 246.62% for dissimilarity\n(Euclidean, SAM, $T^2$) and perception (CIE $\\Delta E$) metrics, consistently\noutperforming RGB and confirming a marked reduction in metameric confusion. By\nproviding a spectrally optimized input, our method enhances VRU separability,\nestablishing a robust foundation for downstream perception tasks in Advanced\nDriver Assistance Systems (ADAS) and Autonomous Driving (AD), ultimately\ncontributing to improved road safety.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u514b\u670dRGB\u56fe\u50cf\u4e2d\u7684\u540c\u8272\u5f02\u8c31\u95ee\u9898\uff0c\u901a\u8fc7\u6ce2\u6bb5\u9009\u62e9\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u7684\u53ef\u68c0\u6d4b\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u66f4\u5b89\u5168\u7684\u611f\u77e5\u57fa\u7840\u3002", "motivation": "\u4fdd\u62a4\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u662f\u6c7d\u8f66\u611f\u77e5\u7cfb\u7edf\u7684\u5173\u952e\u5b89\u5168\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7531\u540c\u8272\u5f02\u8c31\u73b0\u8c61\u5f15\u8d77\u7684\u89c6\u89c9\u6a21\u7cca\u4e0b\uff0c\u8fd9\u79cd\u73b0\u8c61\u5728RGB\u56fe\u50cf\u4e2d\u4f7f\u4e0d\u540c\u6750\u6599\u770b\u8d77\u6765\u76f8\u4f3c\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6ce2\u6bb5\u9009\u62e9\u7b56\u7565\uff0c\u7ed3\u5408\u4fe1\u606f\u8bba\u6280\u672f\uff08\u8054\u5408\u4e92\u4fe1\u606f\u6700\u5927\u5316\u3001\u76f8\u5173\u6027\u5206\u6790\uff09\u4e0e\u56fe\u50cf\u8d28\u91cf\u5ea6\u91cf\uff08\u5bf9\u6bd4\u4fe1\u566a\u6bd4\uff09\u7684\u65b0\u5e94\u7528\uff0c\u4ee5\u8bc6\u522b\u6700\u5177\u5149\u8c31\u4fe1\u606f\u7684\u6ce2\u6bb5\u3002", "result": "\u4f7f\u7528H-City\u6570\u636e\u96c6\uff0c\u6211\u4eec\u8bc6\u522b\u4e86\u4e09\u4e2a\u4fe1\u606f\u4e30\u5bcc\u7684\u6ce2\u6bb5\uff08497 nm\u3001607 nm\u548c895 nm\uff0c\u00b127 nm\uff09\uff0c\u5e76\u91cd\u5efa\u4e86\u4f2a\u5f69\u8272\u56fe\u50cf\u4e0e\u5171\u914d\u51c6\u7684RGB\u8fdb\u884c\u6bd4\u8f83\u3002\u5b9a\u91cf\u7ed3\u679c\u663e\u793aVRU\u4e0e\u80cc\u666f\u7684\u5dee\u5f02\u6027\u548c\u611f\u77e5\u53ef\u5206\u79bb\u6027\u589e\u52a0\u3002\u6240\u9009HSI\u6ce2\u6bb5\u5728\u5dee\u5f02\u6027\uff08\u6b27\u51e0\u91cc\u5f97\u3001SAM\u3001T\u00b2\uff09\u548c\u611f\u77e5\uff08CIE \u0394E\uff09\u6307\u6807\u4e0a\u5206\u522b\u63d0\u9ad8\u4e8670.24%\u3001528.46%\u30011206.83%\u548c246.62%\uff0c\u663e\u8457\u4f18\u4e8eRGB\uff0c\u5e76\u786e\u8ba4\u4e86\u540c\u8272\u5f02\u8c31\u6df7\u6dc6\u7684\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u5149\u8c31\u4f18\u5316\u7684\u8f93\u5165\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u589e\u5f3a\u4e86VRU\u7684\u53ef\u5206\u79bb\u6027\uff0c\u4e3a\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff08ADAS\uff09\u548c\u81ea\u52a8\u9a7e\u9a76\uff08AD\uff09\u7684\u4e0b\u6e38\u611f\u77e5\u4efb\u52a1\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u6700\u7ec8\u6709\u52a9\u4e8e\u63d0\u9ad8\u9053\u8def\u5b89\u5168\u3002"}}
{"id": "2508.11479", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11479", "abs": "https://arxiv.org/abs/2508.11479", "authors": ["Tatiana Zemskova", "Aleksei Staroverov", "Dmitry Yudin", "Aleksandr Panov"], "title": "OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation", "comment": null, "summary": "Open-vocabulary Object Goal Navigation requires an embodied agent to reach\nobjects described by free-form language, including categories never seen during\ntraining. Existing end-to-end policies overfit small simulator datasets,\nachieving high success on training scenes but failing to generalize and\nexhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a\nlightweight transformer policy that tackles these issues with two synergistic\ncomponents. The first component is the semantic branch, which includes an\nencoder for the target binary mask and an auxiliary segmentation loss function,\ngrounding the textual goal and providing precise spatial cues. The second\ncomponent consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample\nscheduler that continuously balances imitation and reinforcement signals\naccording to the policy entropy, eliminating brittle manual phase switches.\nThese additions cut the sample complexity of training by 33%, and reduce\ncollision count in two times while keeping inference cost low (130M parameters,\nRGB-only input). On HM3D-OVON, our model matches the performance on unseen\ncategories to that on seen ones and establishes state-of-the-art results (40.1%\nSR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language\nmodels. Code is available at https://github.com/CognitiveAISystems/OVSegDT.", "AI": {"tldr": "OVSegDT\u901a\u8fc7\u8bed\u4e49\u5206\u652f\u548c\u71b5\u81ea\u9002\u5e94\u635f\u5931\u8c03\u5236\uff0c\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u5bfc\u822a\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u78b0\u649e\uff0c\u5e76\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u8fbe\u5230\u4e0e\u5df2\u89c1\u7c7b\u522b\u76f8\u5f53\u7684\u8868\u73b0\uff0c\u4e14\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u6216\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7aef\u5230\u7aef\u7b56\u7565\u5728\u5c0f\u89c4\u6a21\u6a21\u62df\u5668\u6570\u636e\u96c6\u4e0a\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u4ee5\u53ca\u7531\u6b64\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u4e0d\u5b89\u5168\u884c\u4e3a\uff08\u9891\u7e41\u78b0\u649e\uff09\u3002", "method": "\u63d0\u51faOVSegDT\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7Transformer\u7b56\u7565\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u8bed\u4e49\u5206\u652f\uff08\u542b\u76ee\u6807\u4e8c\u8fdb\u5236\u63a9\u7801\u7f16\u7801\u5668\u548c\u8f85\u52a9\u5206\u5272\u635f\u5931\u51fd\u6570\uff09\u548c\u71b5\u81ea\u9002\u5e94\u635f\u5931\u8c03\u5236\uff08\u4e00\u79cd\u6839\u636e\u7b56\u7565\u71b5\u52a8\u6001\u5e73\u8861\u6a21\u4eff\u4e0e\u5f3a\u5316\u4fe1\u53f7\u7684\u6837\u672c\u7ea7\u8c03\u5ea6\u5668\uff09\u3002", "result": "\u5728HM3D-OVON\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u7684\u8868\u73b0\u4e0e\u5df2\u89c1\u7c7b\u522b\u76f8\u5f53\uff0c\u4e14\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0840.1% SR\uff0c20.9% SPL on val unseen\uff09\uff0c\u540c\u65f6\u5c06\u8bad\u7ec3\u6837\u672c\u590d\u6742\u5ea6\u964d\u4f4e\u4e8633%\uff0c\u78b0\u649e\u6b21\u6570\u51cf\u5c11\u4e86\u4e00\u534a\u3002", "conclusion": "OVSegDT\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u5206\u652f\u548c\u71b5\u81ea\u9002\u5e94\u635f\u5931\u8c03\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u5bfc\u822a\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u78b0\u649e\u6b21\u6570\uff0c\u5e76\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u8fbe\u5230\u4e86\u4e0e\u5df2\u89c1\u7c7b\u522b\u76f8\u5f53\u7684\u8868\u73b0\uff0c\u4e14\u65e0\u9700\u6df1\u5ea6\u3001\u6d4b\u8ddd\u6216\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2508.10963", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10963", "abs": "https://arxiv.org/abs/2508.10963", "authors": ["Zixiang Yang", "Yue Ma", "Yinhan Zhang", "Shanhui Mo", "Dongrui Liu", "Linfeng Zhang"], "title": "EVCtrl: Efficient Control Adapter for Visual Generation", "comment": null, "summary": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials.", "AI": {"tldr": "EVCtrl\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63a7\u5236\u9002\u914d\u5668\uff0c\u901a\u8fc7\u65f6\u7a7a\u53cc\u7f13\u5b58\u7b56\u7565\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u548c\u89c6\u9891\u63a7\u5236\u751f\u6210\u6548\u7387\uff0c\u901f\u5ea6\u63d0\u53472\u500d\u4ee5\u4e0a\u4e14\u8d28\u91cf\u51e0\u4e4e\u65e0\u635f\u3002", "motivation": "\u5f53\u524dControlNet\u867d\u7136\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u65f6\u7a7a\u63a7\u5236\uff0c\u4f46\u5176\u8f85\u52a9\u5206\u652f\u663e\u8457\u589e\u52a0\u4e86\u5ef6\u8fdf\uff0c\u5e76\u5728\u975e\u63a7\u5236\u533a\u57df\u548c\u53bb\u566a\u6b65\u9aa4\u4e2d\u5f15\u5165\u4e86\u5197\u4f59\u8ba1\u7b97\uff0c\u5c24\u5176\u5728\u89c6\u9891\u751f\u6210\u4e2d\u66f4\u4e3a\u660e\u663e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86EVCtrl\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u7a7a\u53cc\u7f13\u5b58\u7b56\u7565\uff1a\u5728\u7a7a\u95f4\u5197\u4f59\u65b9\u9762\uff0c\u901a\u8fc7\u5206\u6790DiT-ControlNet\u5404\u5c42\u5bf9\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u54cd\u5e94\uff0c\u5c06\u7f51\u7edc\u5212\u5206\u4e3a\u5168\u5c40\u548c\u5c40\u90e8\u529f\u80fd\u533a\u57df\uff0c\u5229\u7528\u5c40\u90e8\u611f\u77e5\u7f13\u5b58\u4e13\u6ce8\u4e8e\u771f\u6b63\u9700\u8981\u63a7\u5236\u4fe1\u53f7\u7684\u533a\u57df\uff1b\u5728\u65f6\u95f4\u5197\u4f59\u65b9\u9762\uff0c\u9009\u62e9\u6027\u5ffd\u7565\u4e0d\u5fc5\u8981\u7684\u53bb\u566a\u6b65\u9aa4\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5728CogVideo-Controlnet\u548cWan2.1-Controlnet\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEVCtrl\u5206\u522b\u5b9e\u73b0\u4e862.16\u500d\u548c2.05\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4e14\u751f\u6210\u8d28\u91cf\u51e0\u4e4e\u65e0\u4e0b\u964d\u3002", "conclusion": "EVCtrl\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u63a7\u5236\u9002\u914d\u5668\uff0c\u901a\u8fc7\u65f6\u7a7a\u53cc\u7f13\u5b58\u7b56\u7565\u663e\u8457\u51cf\u5c11\u4e86\u5197\u4f59\u8ba1\u7b97\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u56fe\u50cf\u548c\u89c6\u9891\u63a7\u5236\u751f\u6210\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901f\u5ea6\u63d0\u5347\u663e\u8457\u4e14\u751f\u6210\u8d28\u91cf\u51e0\u4e4e\u65e0\u4e0b\u964d\u3002"}}
{"id": "2508.11485", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11485", "abs": "https://arxiv.org/abs/2508.11485", "authors": ["Hailiang Tang", "Tisheng Zhang", "Liqiang Wang", "Xin Ding", "Man Yuan", "Zhiyu Xiang", "Jujin Chen", "Yuhan Bian", "Shuangyan Liu", "Yuqing Wang", "Guan Wang", "Xiaoji Niu"], "title": "i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping", "comment": "10 pages, 12 figures", "summary": "Accurate and reliable navigation is crucial for autonomous unmanned ground\nvehicle (UGV). However, current UGV datasets fall short in meeting the demands\nfor advancing navigation and mapping techniques due to limitations in sensor\nconfiguration, time synchronization, ground truth, and scenario diversity. To\naddress these challenges, we present i2Nav-Robot, a large-scale dataset\ndesigned for multi-sensor fusion navigation and mapping in indoor-outdoor\nenvironments. We integrate multi-modal sensors, including the newest front-view\nand 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras,\nodometer, global navigation satellite system (GNSS) receiver, and inertial\nmeasurement units (IMU) on an omnidirectional wheeled robot. Accurate\ntimestamps are obtained through both online hardware synchronization and\noffline calibration for all sensors. The dataset comprises ten larger-scale\nsequences covering diverse UGV operating scenarios, such as outdoor streets,\nand indoor parking lots, with a total length of about 17060 meters.\nHigh-frequency ground truth, with centimeter-level accuracy for position, is\nderived from post-processing integrated navigation methods using a\nnavigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more\nthan ten open-sourced multi-sensor fusion systems, and it has proven to have\nsuperior data quality.", "AI": {"tldr": "i2Nav-Robot\u662f\u4e00\u4e2a\u591a\u4f20\u611f\u5668\u878d\u5408\u7684\u5927\u89c4\u6a21UGV\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u5bfc\u822a\u548c\u5730\u56fe\u6784\u5efa\u652f\u6301\u3002", "motivation": "\u5f53\u524dUGV\u6570\u636e\u96c6\u5728\u4f20\u611f\u5668\u914d\u7f6e\u3001\u65f6\u95f4\u540c\u6b65\u3001\u5730\u9762\u771f\u5b9e\u548c\u573a\u666f\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5bfc\u822a\u548c\u5730\u56fe\u6784\u5efa\u6280\u672f\u53d1\u5c55\u7684\u9700\u6c42\u3002", "method": "\u96c6\u6210\u591a\u6a21\u6001\u4f20\u611f\u5668\uff08\u5305\u62ec\u6700\u65b0\u7684\u56fa\u6001LiDAR\u30014D\u96f7\u8fbe\u3001\u7acb\u4f53\u76f8\u673a\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u5728\u7ebf\u786c\u4ef6\u540c\u6b65\u548c\u79bb\u7ebf\u6821\u51c6\u83b7\u53d6\u7cbe\u786e\u65f6\u95f4\u6233\u3002\u6570\u636e\u96c6\u5305\u542b10\u4e2a\u5927\u89c4\u6a21\u5e8f\u5217\uff0c\u8986\u76d6\u591a\u6837\u5316\u7684\u5ba4\u5185\u5916\u573a\u666f\u3002", "result": "i2Nav-Robot\u6570\u636e\u96c6\u8986\u76d617060\u7c73\uff0c\u63d0\u4f9b\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684\u4f4d\u7f6e\u5730\u9762\u771f\u5b9e\u6570\u636e\uff0c\u5e76\u88ab\u591a\u4e2a\u5f00\u6e90\u7cfb\u7edf\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "i2Nav-Robot\u6570\u636e\u96c6\u901a\u8fc7\u591a\u6a21\u6001\u4f20\u611f\u5668\u96c6\u6210\u548c\u9ad8\u7cbe\u5ea6\u5730\u9762\u771f\u5b9e\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86UGV\u5bfc\u822a\u548c\u5730\u56fe\u6784\u5efa\u7684\u7814\u7a76\u6c34\u5e73\uff0c\u5e76\u88ab\u591a\u4e2a\u5f00\u6e90\u7cfb\u7edf\u9a8c\u8bc1\u5176\u6570\u636e\u8d28\u91cf\u4f18\u8d8a\u3002"}}
{"id": "2508.11492", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11492", "abs": "https://arxiv.org/abs/2508.11492", "authors": ["Bozhou Zhang", "Nan Song", "Bingzhao Gao", "Li Zhang"], "title": "Relative Position Matters: Trajectory Prediction and Planning with Polar Representation", "comment": null, "summary": "Trajectory prediction and planning in autonomous driving are highly\nchallenging due to the complexity of predicting surrounding agents' movements\nand planning the ego agent's actions in dynamic environments. Existing methods\nencode map and agent positions and decode future trajectories in Cartesian\ncoordinates. However, modeling the relationships between the ego vehicle and\nsurrounding traffic elements in Cartesian space can be suboptimal, as it does\nnot naturally capture the varying influence of different elements based on\ntheir relative distances and directions. To address this limitation, we adopt\nthe Polar coordinate system, where positions are represented by radius and\nangle. This representation provides a more intuitive and effective way to model\nspatial changes and relative relationships, especially in terms of distance and\ndirectional influence. Based on this insight, we propose Polaris, a novel\nmethod that operates entirely in Polar coordinates, distinguishing itself from\nconventional Cartesian-based approaches. By leveraging the Polar\nrepresentation, this method explicitly models distance and direction variations\nand captures relative relationships through dedicated encoding and refinement\nmodules, enabling more structured and spatially aware trajectory prediction and\nplanning. Extensive experiments on the challenging prediction (Argoverse 2) and\nplanning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art\nperformance.", "AI": {"tldr": "Polaris\u662f\u4e00\u79cd\u57fa\u4e8e\u6781\u5750\u6807\u7cfb\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u66f4\u76f4\u89c2\u5730\u5efa\u6a21\u8ddd\u79bb\u548c\u65b9\u5411\u53d8\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8f68\u8ff9\u9884\u6d4b\u548c\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u4e2d\u5bf9\u81ea\u8f66\u548c\u5468\u56f4\u4ea4\u901a\u5143\u7d20\u7684\u5173\u7cfb\u5efa\u6a21\u4e0d\u591f\u7406\u60f3\uff0c\u65e0\u6cd5\u81ea\u7136\u6355\u6349\u4e0d\u540c\u5143\u7d20\u57fa\u4e8e\u8ddd\u79bb\u548c\u65b9\u5411\u7684\u52a8\u6001\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u4e86\u6781\u5750\u6807\u7cfb\uff08\u534a\u5f84\u548c\u89d2\u5ea6\uff09\u6765\u8868\u793a\u4f4d\u7f6e\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u7f16\u7801\u548c\u7ec6\u5316\u6a21\u5757\uff0c\u4ee5\u66f4\u76f4\u89c2\u5730\u5efa\u6a21\u7a7a\u95f4\u53d8\u5316\u548c\u76f8\u5bf9\u5173\u7cfb\u3002", "result": "\u5728Argoverse 2\u548cnuPlan\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u9884\u6d4b\u548c\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPolaris\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Polaris\u5728\u6781\u5750\u6807\u7cfb\u4e0b\u7684\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u7684\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u65b9\u6cd5\uff0c\u5728\u8f68\u8ff9\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11011", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11011", "abs": "https://arxiv.org/abs/2508.11011", "authors": ["Xuezheng Chen", "Zhengbo Zou"], "title": "Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?", "comment": null, "summary": "Construction safety inspections typically involve a human inspector\nidentifying safety concerns on-site. With the rise of powerful Vision Language\nModels (VLMs), researchers are exploring their use for tasks such as detecting\nsafety rule violations from on-site images. However, there is a lack of open\ndatasets to comprehensively evaluate and further fine-tune VLMs in construction\nsafety inspection. Current applications of VLMs use small, supervised datasets,\nlimiting their applicability in tasks they are not directly trained for. In\nthis paper, we propose the ConstructionSite 10k, featuring 10,000 construction\nsite images with annotations for three inter-connected tasks, including image\ncaptioning, safety rule violation visual question answering (VQA), and\nconstruction element visual grounding. Our subsequent evaluation of current\nstate-of-the-art large pre-trained VLMs shows notable generalization abilities\nin zero-shot and few-shot settings, while additional training is needed to make\nthem applicable to actual construction sites. This dataset allows researchers\nto train and evaluate their own VLMs with new architectures and techniques,\nproviding a valuable benchmark for construction safety inspection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ConstructionSite 10k\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3VLM\u5728\u5efa\u7b51\u5b89\u5168\u68c0\u67e5\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684VLM\u5728\u5efa\u7b51\u5b89\u5168\u68c0\u67e5\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u5c0f\u578b\u76d1\u7763\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u5f00\u653e\u7684\u5168\u9762\u8bc4\u4f30\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86ConstructionSite 10k\u6570\u636e\u96c6\uff0c\u5305\u542b10,000\u5f20\u5efa\u7b51\u5de5\u5730\u56fe\u50cf\uff0c\u6807\u6ce8\u4e86\u4e09\u4e2a\u4e92\u76f8\u5173\u8054\u7684\u4efb\u52a1\uff1a\u56fe\u50cf\u63cf\u8ff0\u3001\u5b89\u5168\u89c4\u5219\u8fdd\u53cd\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u548c\u5efa\u7b51\u5143\u7d20\u89c6\u89c9\u5b9a\u4f4d\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3VLM\u5728\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4ecd\u9700\u989d\u5916\u8bad\u7ec3\u4ee5\u9002\u5e94\u5b9e\u9645\u5efa\u7b51\u5de5\u5730\u3002", "conclusion": "ConstructionSite 10k\u6570\u636e\u96c6\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u81ea\u5df1\u7684VLM\u6a21\u578b\uff0c\u5e76\u63a8\u52a8\u5efa\u7b51\u5b89\u5168\u68c0\u67e5\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2508.11498", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11498", "abs": "https://arxiv.org/abs/2508.11498", "authors": ["Agnes Bressan de Almeida", "Joao Aires Correa Fernandes Marsicano"], "title": "Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language", "comment": null, "summary": "Swarm in Blocks, originally developed for CopterHack 2022, is a high-level\ninterface that simplifies drone swarm programming using a block-based language.\nBuilding on the Clover platform, this tool enables users to create\nfunctionalities like loops and conditional structures by assembling code\nblocks. In 2023, we introduced Swarm in Blocks 2.0, further refining the\nplatform to address the complexities of swarm management in a user-friendly\nway. As drone swarm applications grow in areas like delivery, agriculture, and\nsurveillance, the challenge of managing them, especially for beginners, has\nalso increased. The Atena team developed this interface to make swarm handling\naccessible without requiring extensive knowledge of ROS or programming. The\nblock-based approach not only simplifies swarm control but also expands\neducational opportunities in programming.", "AI": {"tldr": "Swarm in Blocks 2.0\u662f\u4e00\u4e2a\u57fa\u4e8e\u5757\u72b6\u8bed\u8a00\u7684\u65e0\u4eba\u673a\u7fa4\u7f16\u7a0b\u5de5\u5177\uff0c\u7b80\u5316\u4e86\u590d\u6742\u64cd\u4f5c\uff0c\u9002\u5408\u521d\u5b66\u8005\u548c\u6559\u80b2\u7528\u9014\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u7fa4\u5728\u914d\u9001\u3001\u519c\u4e1a\u548c\u76d1\u63a7\u7b49\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u7ba1\u7406\u590d\u6742\u6027\u4e5f\u968f\u4e4b\u4e0a\u5347\uff0c\u5c24\u5176\u662f\u5bf9\u521d\u5b66\u8005\u800c\u8a00\u3002Atena\u56e2\u961f\u5f00\u53d1\u6b64\u5de5\u5177\u65e8\u5728\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\uff0c\u65e0\u9700ROS\u6216\u7f16\u7a0b\u7684\u6df1\u5165\u77e5\u8bc6\u3002", "method": "\u57fa\u4e8eClover\u5e73\u53f0\uff0c\u91c7\u7528\u5757\u72b6\u8bed\u8a00\u6784\u5efa\u9ad8\u7ea7\u63a5\u53e3\uff0c\u652f\u6301\u5faa\u73af\u548c\u6761\u4ef6\u7ed3\u6784\u7b49\u529f\u80fd\u3002", "result": "Swarm in Blocks 2.0\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u5e73\u53f0\uff0c\u4ee5\u7528\u6237\u53cb\u597d\u7684\u65b9\u5f0f\u89e3\u51b3\u4e86\u7fa4\u7ba1\u7406\u7684\u590d\u6742\u6027\u3002", "conclusion": "Swarm in Blocks 2.0\u6709\u6548\u7b80\u5316\u4e86\u65e0\u4eba\u673a\u7fa4\u7f16\u7a0b\uff0c\u4f7f\u5176\u5bf9\u521d\u5b66\u8005\u66f4\u53cb\u597d\uff0c\u540c\u65f6\u6269\u5c55\u4e86\u7f16\u7a0b\u6559\u80b2\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.11021", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11021", "abs": "https://arxiv.org/abs/2508.11021", "authors": ["Zisheng Liang", "Kidus Zewde", "Rudra Pratap Singh", "Disha Patil", "Zexi Chen", "Jiayu Xue", "Yao Yao", "Yifei Chen", "Qinzhe Liu", "Simiao Ren"], "title": "Can Multi-modal (reasoning) LLMs detect document manipulation?", "comment": "arXiv admin note: text overlap with arXiv:2503.20084", "summary": "Document fraud poses a significant threat to industries reliant on secure and\nverifiable documentation, necessitating robust detection mechanisms. This study\ninvestigates the efficacy of state-of-the-art multi-modal large language models\n(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,\nGrok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and\n3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against\neach other and prior work on document fraud detection techniques using a\nstandard dataset with real transactional documents. Through prompt optimization\nand detailed analysis of the models' reasoning processes, we evaluate their\nability to identify subtle indicators of fraud, such as tampered text,\nmisaligned formatting, and inconsistent transactional sums. Our results reveal\nthat top-performing multi-modal LLMs demonstrate superior zero-shot\ngeneralization, outperforming conventional methods on out-of-distribution\ndatasets, while several vision LLMs exhibit inconsistent or subpar performance.\nNotably, model size and advanced reasoning capabilities show limited\ncorrelation with detection accuracy, suggesting task-specific fine-tuning is\ncritical. This study underscores the potential of multi-modal LLMs in enhancing\ndocument fraud detection systems and provides a foundation for future research\ninto interpretable and scalable fraud mitigation strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u6a21\u6001LLMs\u5728\u6587\u6863\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u9876\u7ea7\u6a21\u578b\u5728\u96f6\u6837\u672c\u6cdb\u5316\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u6a21\u578b\u5927\u5c0f\u4e0e\u51c6\u786e\u6027\u76f8\u5173\u6027\u6709\u9650\uff0c\u5f3a\u8c03\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u6587\u6863\u6b3a\u8bc8\u5bf9\u4f9d\u8d56\u5b89\u5168\u53ef\u9a8c\u8bc1\u6587\u4ef6\u7684\u884c\u4e1a\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u9700\u8981\u5f3a\u5927\u7684\u68c0\u6d4b\u673a\u5236\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u4f18\u5316\u548c\u5bf9\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6a21\u578b\u8bc6\u522b\u6b3a\u8bc8\u7ec6\u5fae\u6307\u6807\u7684\u80fd\u529b\uff0c\u5982\u7be1\u6539\u6587\u672c\u3001\u683c\u5f0f\u4e0d\u5bf9\u9f50\u548c\u4e0d\u4e00\u81f4\u7684\u4ea4\u6613\u91d1\u989d\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u8868\u73b0\u6700\u4f73\u7684\u591a\u6a21\u6001LLMs\u5728\u96f6\u6837\u672c\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u800c\u4e00\u4e9b\u89c6\u89c9LLMs\u8868\u73b0\u51fa\u4e0d\u4e00\u81f4\u6216\u8f83\u5dee\u7684\u8868\u73b0\u3002\u6a21\u578b\u5927\u5c0f\u548c\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u4e0e\u68c0\u6d4b\u51c6\u786e\u6027\u7684\u76f8\u5173\u6027\u6709\u9650\uff0c\u8868\u660e\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u5347\u6587\u6863\u6b3a\u8bc8\u68c0\u6d4b\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u6b3a\u8bc8\u7f13\u89e3\u7b56\u7565\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.11256", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11256", "abs": "https://arxiv.org/abs/2508.11256", "authors": ["Junjie Wang", "Keyu Chen", "Yulin Li", "Bin Chen", "Hengshuang Zhao", "Xiaojuan Qi", "Zhuotao Tian"], "title": "Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception", "comment": "arXiv admin note: text overlap with arXiv:2505.04410", "summary": "Dense visual perception tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense perception often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. \\revise{The context features are enhanced by jointly distilling\nsemantic correlations from Vision Foundation Models (VFMs) and object integrity\ncues from diffusion models, thereby enhancing spatial consistency. In parallel,\nthe content features are aligned with image crop representations and\nconstrained by region correlations from VFMs to improve local discriminability.\nExtensive experiments demonstrate that DeCLIP establishes a solid foundation\nfor open-vocabulary dense perception, consistently achieving state-of-the-art\nperformance across a broad spectrum of tasks, including 2D detection and\nsegmentation, 3D instance segmentation, video instance segmentation, and 6D\nobject pose estimation.} Code is available at\nhttps://github.com/xiaomoguhz/DeCLIP", "AI": {"tldr": "DeCLIP\u901a\u8fc7\u89e3\u8026\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3aCLIP\u7684\u5c40\u90e8\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u5f00\u653e\u8bcd\u6c47\u5bc6\u96c6\u611f\u77e5\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5bc6\u96c6\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u800cCLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5bc6\u96c6\u611f\u77e5\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u5176\u5c40\u90e8\u7279\u5f81\u8868\u793a\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86DeCLIP\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u83b7\u53d6\u201c\u5185\u5bb9\u201d\u548c\u201c\u4e0a\u4e0b\u6587\u201d\u7279\u5f81\uff0c\u5e76\u5206\u522b\u901a\u8fc7\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u589e\u5f3a\u8bed\u4e49\u76f8\u5173\u6027\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "result": "DeCLIP\u5728\u591a\u79cd\u4efb\u52a1\uff08\u59822D\u68c0\u6d4b\u3001\u5206\u5272\u30013D\u5b9e\u4f8b\u5206\u5272\u7b49\uff09\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "DeCLIP\u901a\u8fc7\u89e3\u8026\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u5e76\u5206\u522b\u589e\u5f3a\u5185\u5bb9\u548c\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86CLIP\u5728\u5bc6\u96c6\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u5f00\u653e\u8bcd\u6c47\u5bc6\u96c6\u611f\u77e5\u5efa\u7acb\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2508.11503", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11503", "abs": "https://arxiv.org/abs/2508.11503", "authors": ["Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez", "Carol Martinez"], "title": "Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media", "comment": "The source code is available at\n  https://github.com/AndrejOrsula/space_robotics_bench", "summary": "Reliable autonomous navigation across the unstructured terrains of distant\nplanetary surfaces is a critical enabler for future space exploration. However,\nthe deployment of learning-based controllers is hindered by the inherent\nsim-to-real gap, particularly for the complex dynamics of wheel interactions\nwith granular media. This work presents a complete sim-to-real framework for\ndeveloping and validating robust control policies for dynamic waypoint tracking\non such challenging surfaces. We leverage massively parallel simulation to\ntrain reinforcement learning agents across a vast distribution of procedurally\ngenerated environments with randomized physics. These policies are then\ntransferred zero-shot to a physical wheeled rover operating in a lunar-analogue\nfacility. Our experiments systematically compare multiple reinforcement\nlearning algorithms and action smoothing filters to identify the most effective\ncombinations for real-world deployment. Crucially, we provide strong empirical\nevidence that agents trained with procedural diversity achieve superior\nzero-shot performance compared to those trained on static scenarios. We also\nanalyze the trade-offs of fine-tuning with high-fidelity particle physics,\nwhich offers minor gains in low-speed precision at a significant computational\ncost. Together, these contributions establish a validated workflow for creating\nreliable learning-based navigation systems, marking a critical step towards\ndeploying autonomous robots in the final frontier.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5b8c\u6574\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u6846\u67b6\uff0c\u901a\u8fc7\u7a0b\u5e8f\u751f\u6210\u7684\u73af\u5883\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u5e76\u5728\u7269\u7406\u8f6e\u5f0f\u6f2b\u6e38\u8f66\u4e0a\u9a8c\u8bc1\u5176\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u592a\u7a7a\u63a2\u7d22\u63d0\u4f9b\u53ef\u9760\u5bfc\u822a\u7cfb\u7edf\u3002", "motivation": "\u53ef\u9760\u7684\u81ea\u4e3b\u5bfc\u822a\u5bf9\u4e8e\u672a\u6765\u592a\u7a7a\u63a2\u7d22\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b66\u4e60\u578b\u63a7\u5236\u5668\u7684\u90e8\u7f72\u53d7\u5230\u6a21\u62df\u5230\u73b0\u5b9e\u5dee\u8ddd\u7684\u963b\u788d\u3002", "method": "\u5229\u7528\u5927\u89c4\u6a21\u5e76\u884c\u4eff\u771f\u5728\u7a0b\u5e8f\u751f\u6210\u7684\u73af\u5883\u4e2d\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u5e76\u5c06\u5176\u96f6\u6837\u672c\u8f6c\u79fb\u5230\u7269\u7406\u8f6e\u5f0f\u6f2b\u6e38\u8f66\u4e0a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5177\u6709\u7a0b\u5e8f\u591a\u6837\u6027\u7684\u4ee3\u7406\u5728\u96f6\u6837\u672c\u6027\u80fd\u4e0a\u4f18\u4e8e\u9759\u6001\u573a\u666f\u8bad\u7ec3\u7684\u4ee3\u7406\uff0c\u800c\u9ad8\u4fdd\u771f\u7c92\u5b50\u7269\u7406\u7684\u5fae\u8c03\u5728\u4f4e\u901f\u7cbe\u5ea6\u4e0a\u4ec5\u6709\u5fae\u5c0f\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7528\u4e8e\u521b\u5efa\u53ef\u9760\u7684\u5b66\u4e60\u578b\u5bfc\u822a\u7cfb\u7edf\uff0c\u6807\u5fd7\u7740\u5728\u6700\u7ec8\u524d\u6cbf\u90e8\u7f72\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2508.11032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11032", "abs": "https://arxiv.org/abs/2508.11032", "authors": ["Yanwu Yang", "Guinan Su", "Jiesi Hu", "Francesco Sammarco", "Jonas Geiping", "Thomas Wolfers"], "title": "MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation", "comment": null, "summary": "Universal medical image segmentation models have emerged as a promising\nparadigm due to their strong generalizability across diverse tasks, showing\ngreat potential for a wide range of clinical applications. This potential has\nbeen partly driven by the success of general-purpose vision models such as the\nSegment Anything Model (SAM), which has inspired the development of various\nfine-tuned variants for medical segmentation tasks. However, fine-tuned\nvariants like MedSAM are trained on comparatively limited medical imaging data\nthat often suffers from heterogeneity, scarce annotations, and distributional\nshifts. These challenges limit their ability to generalize across a wide range\nof medical segmentation tasks. In this regard, we propose MedSAMix, a\ntraining-free model merging method that integrates the strengths of both\ngeneralist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical\nimage segmentation. In contrast to traditional model merging approaches that\nrely on manual configuration and often result in suboptimal outcomes, we\npropose a zero-order optimization method to automatically discover optimal\nlayer-wise merging solutions. Furthermore, for clinical applications, we\ndevelop two regimes to meet the demand of domain-specificity and\ngeneralizability in different scenarios by single-task optimization and\nmulti-objective optimization respectively. Extensive evaluations on 25 medical\nsegmentation tasks demonstrate that MedSAMix effectively mitigates model bias\nand consistently improves performance in both domain-specific accuracy and\ngeneralization, achieving improvements of 6.67% on specialized tasks and 4.37%\non multi-task evaluations.", "AI": {"tldr": "MedSAMix\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u901a\u7528\u548c\u4e13\u7528\u6a21\u578b\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\uff08\u5982MedSAM\uff09\u53d7\u9650\u4e8e\u6570\u636e\u5f02\u8d28\u6027\u3001\u6807\u6ce8\u7a00\u7f3a\u548c\u5206\u5e03\u504f\u79fb\uff0c\u96be\u4ee5\u5e7f\u6cdb\u6cdb\u5316\u3002MedSAMix\u65e8\u5728\u7ed3\u5408\u901a\u7528\u548c\u4e13\u7528\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f6\u9636\u4f18\u5316\u7684\u81ea\u52a8\u5c42\u5408\u5e76\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u5355\u4efb\u52a1\u4f18\u5316\u548c\u591a\u76ee\u6807\u4f18\u5316\u4e24\u79cd\u7b56\u7565\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u7684\u9700\u6c42\u3002", "result": "\u572825\u4e2a\u533b\u5b66\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cMedSAMix\u5728\u4e13\u7528\u4efb\u52a1\u4e0a\u63d0\u5347\u4e866.67%\uff0c\u5728\u591a\u4efb\u52a1\u8bc4\u4f30\u4e2d\u63d0\u5347\u4e864.37%\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u6a21\u578b\u504f\u5dee\u3002", "conclusion": "MedSAMix\u901a\u8fc7\u6574\u5408\u901a\u7528\u6a21\u578b\uff08\u5982SAM\uff09\u548c\u4e13\u7528\u6a21\u578b\uff08\u5982MedSAM\uff09\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5408\u5e76\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9886\u57df\u4e13\u7528\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u3002"}}
{"id": "2508.11520", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.11520", "abs": "https://arxiv.org/abs/2508.11520", "authors": ["Evangelos Tsiatsianas", "Chairi Kiourt", "Konstantinos Chatzilygeroudis"], "title": "A Comparative Study of Floating-Base Space Parameterizations for Agile Whole-Body Motion Planning", "comment": "8 pages, 2 figures, 4 tables, Accepted at Humanoids 2025", "summary": "Automatically generating agile whole-body motions for legged and humanoid\nrobots remains a fundamental challenge in robotics. While numerous trajectory\noptimization approaches have been proposed, there is no clear guideline on how\nthe choice of floating-base space parameterization affects performance,\nespecially for agile behaviors involving complex contact dynamics. In this\npaper, we present a comparative study of different parameterizations for direct\ntranscription-based trajectory optimization of agile motions in legged systems.\nWe systematically evaluate several common choices under identical optimization\nsettings to ensure a fair comparison. Furthermore, we introduce a novel\nformulation based on the tangent space of SE(3) for representing the robot's\nfloating-base pose, which, to our knowledge, has not received attention from\nthe literature. This approach enables the use of mature off-the-shelf numerical\nsolvers without requiring specialized manifold optimization techniques. We hope\nthat our experiments and analysis will provide meaningful insights for\nselecting the appropriate floating-based representation for agile whole-body\nmotion generation.", "AI": {"tldr": "\u6bd4\u8f83\u4e0d\u540c\u6d6e\u52a8\u57fa\u53c2\u6570\u5316\u65b9\u6cd5\u5728\u654f\u6377\u8fd0\u52a8\u4f18\u5316\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u57fa\u4e8eSE(3)\u5207\u7a7a\u95f4\u7684\u65b0\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u654f\u6377\u5168\u8eab\u8fd0\u52a8\u751f\u6210\u4e2d\u6d6e\u52a8\u57fa\u7a7a\u95f4\u53c2\u6570\u5316\u9009\u62e9\u7f3a\u4e4f\u660e\u786e\u6307\u5bfc\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u63a5\u89e6\u52a8\u529b\u5b66\u573a\u666f\u4e0b\u3002", "method": "\u91c7\u7528\u76f4\u63a5\u8f6c\u5f55\u6cd5\u8fdb\u884c\u8f68\u8ff9\u4f18\u5316\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u591a\u79cd\u5e38\u89c1\u7684\u6d6e\u52a8\u57fa\u7a7a\u95f4\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8eSE(3)\u5207\u7a7a\u95f4\u7684\u65b0\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eSE(3)\u5207\u7a7a\u95f4\u7684\u53c2\u6570\u5316\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u65e0\u9700\u4e13\u95e8\u7684\u6d41\u5f62\u4f18\u5316\u6280\u672f\u5373\u53ef\u4f7f\u7528\u6210\u719f\u7684\u6570\u503c\u6c42\u89e3\u5668\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u53c2\u6570\u5316\u65b9\u6cd5\u5728\u654f\u6377\u8fd0\u52a8\u8f68\u8ff9\u4f18\u5316\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSE(3)\u5207\u7a7a\u95f4\u7684\u65b0\u9896\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u4e3a\u9009\u62e9\u9002\u5408\u7684\u6d6e\u52a8\u57fa\u8868\u793a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2508.11058", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.11058", "abs": "https://arxiv.org/abs/2508.11058", "authors": ["Wentao Mo", "Qingchao Chen", "Yuxin Peng", "Siyuan Huang", "Yang Liu"], "title": "Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset", "comment": "Accepeted to ACM MM 25", "summary": "The advancement of 3D vision-language (3D VL) learning is hindered by several\nlimitations in existing 3D VL datasets: they rarely necessitate reasoning\nbeyond a close range of objects in single viewpoint, and annotations often link\ninstructions to single objects, missing richer contextual alignments between\nmultiple objects. This significantly curtails the development of models capable\nof deep, multi-view 3D scene understanding over distant objects. To address\nthese challenges, we introduce MV-ScanQA, a novel 3D question answering dataset\nwhere 68% of questions explicitly require integrating information from multiple\nviews (compared to less than 7% in existing datasets), thereby rigorously\ntesting multi-view compositional reasoning. To facilitate the training of\nmodels for such demanding scenarios, we present TripAlign dataset, a\nlarge-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D\nview, set of 3D objects, text> triplets that explicitly aligns groups of\ncontextually related objects with text, providing richer, view-grounded\nmulti-object multimodal alignment signals than previous single-object\nannotations. We further develop LEGO, a baseline method for the multi-view\nreasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D\nLVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign\nachieves state-of-the-art performance not only on the proposed MV-ScanQA, but\nalso on existing benchmarks for 3D dense captioning and question answering.\nDatasets and code are available at\nhttps://matthewdm0816.github.io/tripalign-mvscanqa.", "AI": {"tldr": "\u63d0\u51faMV-ScanQA\u548cTripAlign\u6570\u636e\u96c6\u89e3\u51b33D VL\u5b66\u4e60\u4e2d\u7684\u591a\u89c6\u89d2\u63a8\u7406\u95ee\u9898\uff0cLEGO\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73b0\u67093D VL\u6570\u636e\u96c6\u5728\u8fdc\u8ddd\u79bb\u5bf9\u8c61\u548c\u591a\u89c6\u89d2\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u963b\u788d\u4e86\u6df1\u5ea6\u3001\u591a\u89c6\u89d23D\u573a\u666f\u7406\u89e3\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86LEGO\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5229\u7528TripAlign\u6570\u636e\u96c6\u5c06\u9884\u8bad\u7ec3\u76842D LVLMs\u77e5\u8bc6\u8fc1\u79fb\u52303D\u9886\u57df\u3002", "result": "MV-ScanQA\u548cTripAlign\u6570\u636e\u96c6\u7684\u5f15\u5165\u663e\u8457\u63d0\u5347\u4e86\u591a\u89c6\u89d2\u7ec4\u5408\u63a8\u7406\u80fd\u529b\uff0cLEGO\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LEGO\u6a21\u578b\u5728TripAlign\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u540e\uff0c\u4e0d\u4ec5\u5728MV-ScanQA\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fd8\u5728\u73b0\u67093D\u5bc6\u96c6\u63cf\u8ff0\u548c\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2508.11262", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11262", "abs": "https://arxiv.org/abs/2508.11262", "authors": ["Aiswarya Konavoor", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Vision-Language Models display a strong gender bias", "comment": null, "summary": "Vision-language models (VLM) align images and text in a shared representation\nspace that is useful for retrieval and zero-shot transfer. Yet, this alignment\ncan encode and amplify social stereotypes in subtle ways that are not obvious\nfrom standard accuracy metrics. In this study, we test whether the contrastive\nvision-language encoder exhibits gender-linked associations when it places\nembeddings of face images near embeddings of short phrases that describe\noccupations and activities. We assemble a dataset of 220 face photographs split\nby perceived binary gender and a set of 150 unique statements distributed\nacross six categories covering emotional labor, cognitive labor, domestic\nlabor, technical labor, professional roles, and physical labor. We compute\nunit-norm image embeddings for every face and unit-norm text embeddings for\nevery statement, then define a statement-level association score as the\ndifference between the mean cosine similarity to the male set and the mean\ncosine similarity to the female set, where positive values indicate stronger\nassociation with the male set and negative values indicate stronger association\nwith the female set. We attach bootstrap confidence intervals by resampling\nimages within each gender group, aggregate by category with a separate\nbootstrap over statements, and run a label-swap null model that estimates the\nlevel of mean absolute association we would expect if no gender structure were\npresent. The outcome is a statement-wise and category-wise map of gender\nassociations in a contrastive vision-language space, accompanied by\nuncertainty, simple sanity checks, and a robust gender bias evaluation\nframework.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u804c\u4e1a\u548c\u6d3b\u52a8\u63cf\u8ff0\u4e2d\u9690\u542b\u6027\u522b\u504f\u89c1\uff0c\u63d0\u51fa\u8bc4\u4f30\u6846\u67b6\u5e76\u53d1\u73b0\u663e\u8457\u5173\u8054\u6a21\u5f0f\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u7d22\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5bf9\u9f50\u8fc7\u7a0b\u53ef\u80fd\u9690\u542b\u5e76\u653e\u5927\u793e\u4f1a\u523b\u677f\u5370\u8c61\uff0c\u73b0\u6709\u51c6\u786e\u6027\u6307\u6807\u96be\u4ee5\u6355\u6349\u8fd9\u79cd\u5fae\u5999\u504f\u5dee\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u8ba1\u7b97\u4eba\u8138\u56fe\u50cf\u4e0e\u63cf\u8ff0\u6027\u77ed\u8bed\u7684\u5d4c\u5165\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u5b9a\u4e49\u4e86\u57fa\u4e8e\u6027\u522b\u7684\u5173\u8054\u5206\u6570\uff0c\u5e76\u5229\u7528\u81ea\u52a9\u6cd5\u4f30\u8ba1\u7f6e\u4fe1\u533a\u95f4\u548c\u96f6\u6a21\u578b\u9a8c\u8bc1\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u7a7a\u95f4\u4e2d\u5b58\u5728\u663e\u8457\u7684\u6027\u522b\u5173\u8054\u6a21\u5f0f\uff0c\u4e0d\u540c\u7c7b\u522b\u7684\u804c\u4e1a\u548c\u6d3b\u52a8\u63cf\u8ff0\u4e0e\u7279\u5b9a\u6027\u522b\u7fa4\u4f53\u7684\u5173\u8054\u5f3a\u5ea6\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u6027\u522b\u504f\u89c1\u7684\u7a33\u5065\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u804c\u4e1a\u548c\u6d3b\u52a8\u63cf\u8ff0\u4e2d\u5b58\u5728\u7684\u6027\u522b\u5173\u8054\u6a21\u5f0f\u3002"}}
{"id": "2508.11537", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11537", "abs": "https://arxiv.org/abs/2508.11537", "authors": ["Han Zheng", "Zikang Zhou", "Guli Zhang", "Zhepei Wang", "Kaixuan Wang", "Peiliang Li", "Shaojie Shen", "Ming Yang", "Tong Qin"], "title": "MultiPark: Multimodal Parking Transformer with Next-Segment Prediction", "comment": null, "summary": "Parking accurately and safely in highly constrained spaces remains a critical\nchallenge. Unlike structured driving environments, parking requires executing\ncomplex maneuvers such as frequent gear shifts and steering saturation. Recent\nattempts to employ imitation learning (IL) for parking have achieved promising\nresults. However, existing works ignore the multimodal nature of parking\nbehavior in lane-free open space, failing to derive multiple plausible\nsolutions under the same situation. Notably, IL-based methods encompass\ninherent causal confusion, so enabling a neural network to generalize across\ndiverse parking scenarios is particularly difficult. To address these\nchallenges, we propose MultiPark, an autoregressive transformer for multimodal\nparking. To handle paths filled with abrupt turning points, we introduce a\ndata-efficient next-segment prediction paradigm, enabling spatial\ngeneralization and temporal extrapolation. Furthermore, we design learnable\nparking queries factorized into gear, longitudinal, and lateral components,\nparallelly decoding diverse parking behaviors. To mitigate causal confusion in\nIL, our method employs target-centric pose and ego-centric collision as\noutcome-oriented loss across all modalities beyond pure imitation loss.\nEvaluations on real-world datasets demonstrate that MultiPark achieves\nstate-of-the-art performance across various scenarios. We deploy MultiPark on a\nproduction vehicle, further confirming our approach's robustness in real-world\nparking environments.", "AI": {"tldr": "MultiPark\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u901a\u8fc7\u6570\u636e\u9ad8\u6548\u9884\u6d4b\u548c\u53ef\u5b66\u4e60\u67e5\u8be2\u89e3\u51b3\u591a\u6a21\u6001\u505c\u8f66\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\uff08IL\uff09\u65b9\u6cd5\u5728\u65e0\u8f66\u9053\u5f00\u653e\u7a7a\u95f4\u4e2d\u5ffd\u7565\u505c\u8f66\u884c\u4e3a\u591a\u6a21\u6001\u6027\u7684\u95ee\u9898\uff0c\u4ee5\u53caIL\u56fa\u6709\u7684\u56e0\u679c\u6df7\u6dc6\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86MultiPark\uff0c\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u505c\u8f66\u7684\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u5f15\u5165\u4e86\u6570\u636e\u9ad8\u6548\u7684\u4e0b\u4e00\u4e2a\u6bb5\u9884\u6d4b\u8303\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u53ef\u5b66\u4e60\u7684\u505c\u8f66\u67e5\u8be2\u3002", "result": "MultiPark\u5728\u5404\u79cd\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MultiPark\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u5b9e\u9645\u505c\u8f66\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11063", "abs": "https://arxiv.org/abs/2508.11063", "authors": ["Lucas W. Remedios", "Chloe Choe", "Trent M. Schwartz", "Dingjie Su", "Gaurav Rudravaram", "Chenyu Gao", "Aravind R. Krishnan", "Adam M. Saunders", "Michael E. Kim", "Shunxing Bao", "Alvin C. Powers", "Bennett A. Landman", "John Virostko"], "title": "Data-Driven Abdominal Phenotypes of Type 2 Diabetes in Lean, Overweight, and Obese Cohorts", "comment": null, "summary": "Purpose: Although elevated BMI is a well-known risk factor for type 2\ndiabetes, the disease's presence in some lean adults and absence in others with\nobesity suggests that detailed body composition may uncover abdominal\nphenotypes of type 2 diabetes. With AI, we can now extract detailed\nmeasurements of size, shape, and fat content from abdominal structures in 3D\nclinical imaging at scale. This creates an opportunity to empirically define\nbody composition signatures linked to type 2 diabetes risk and protection using\nlarge-scale clinical data. Approach: To uncover BMI-specific diabetic abdominal\npatterns from clinical CT, we applied our design four times: once on the full\ncohort (n = 1,728) and once on lean (n = 497), overweight (n = 611), and obese\n(n = 620) subgroups separately. Briefly, our experimental design transforms\nabdominal scans into collections of explainable measurements through\nsegmentation, classifies type 2 diabetes through a cross-validated random\nforest, measures how features contribute to model-estimated risk or protection\nthrough SHAP analysis, groups scans by shared model decision patterns\n(clustering from SHAP) and links back to anatomical differences\n(classification). Results: The random-forests achieved mean AUCs of 0.72-0.74.\nThere were shared type 2 diabetes signatures in each group; fatty skeletal\nmuscle, older age, greater visceral and subcutaneous fat, and a smaller or\nfat-laden pancreas. Univariate logistic regression confirmed the direction of\n14-18 of the top 20 predictors within each subgroup (p < 0.05). Conclusions:\nOur findings suggest that abdominal drivers of type 2 diabetes may be\nconsistent across weight classes.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528AI\u5206\u6790\u8179\u90e8CT\u626b\u63cf\uff0c\u53d1\u73b02\u578b\u7cd6\u5c3f\u75c5\u7684\u8179\u90e8\u7279\u5f81\u5728\u4e0d\u540c\u4f53\u91cd\u7c7b\u522b\u4e2d\u4e00\u81f4\uff0c\u5305\u62ec\u8102\u80aa\u6027\u9aa8\u9abc\u808c\u3001\u8f83\u591a\u5185\u810f\u8102\u80aa\u548c\u8f83\u5c0f\u80f0\u817a\u7b49\u3002", "motivation": "\u5c3d\u7ba1\u9ad8BMI\u662f2\u578b\u7cd6\u5c3f\u75c5\u7684\u5df2\u77e5\u98ce\u9669\u56e0\u7d20\uff0c\u4f46\u8be5\u75be\u75c5\u5728\u67d0\u4e9b\u7626\u5f31\u6210\u5e74\u4eba\u4e2d\u7684\u5b58\u5728\u548c\u5728\u5176\u4ed6\u80a5\u80d6\u8005\u4e2d\u7684\u7f3a\u5931\u8868\u660e\uff0c\u8be6\u7ec6\u7684\u8eab\u4f53\u7ec4\u6210\u53ef\u80fd\u63ed\u793a2\u578b\u7cd6\u5c3f\u75c5\u7684\u8179\u90e8\u8868\u578b\u3002\u5229\u7528AI\uff0c\u53ef\u4ee5\u4ece3D\u4e34\u5e8a\u5f71\u50cf\u4e2d\u5927\u89c4\u6a21\u63d0\u53d6\u8179\u90e8\u7ed3\u6784\u7684\u8be6\u7ec6\u6d4b\u91cf\uff0c\u4ece\u800c\u6709\u673a\u4f1a\u5229\u7528\u5927\u89c4\u6a21\u4e34\u5e8a\u6570\u636e\u5b9e\u8bc1\u5b9a\u4e49\u4e0e2\u578b\u7cd6\u5c3f\u75c5\u98ce\u9669\u548c\u4fdd\u62a4\u76f8\u5173\u7684\u8eab\u4f53\u7ec4\u6210\u7279\u5f81\u3002", "method": "\u901a\u8fc7\u5206\u5272\u5c06\u8179\u90e8\u626b\u63cf\u8f6c\u5316\u4e3a\u53ef\u89e3\u91ca\u7684\u6d4b\u91cf\u96c6\u5408\uff0c\u4f7f\u7528\u4ea4\u53c9\u9a8c\u8bc1\u7684\u968f\u673a\u68ee\u6797\u5206\u7c7b2\u578b\u7cd6\u5c3f\u75c5\uff0c\u901a\u8fc7SHAP\u5206\u6790\u6d4b\u91cf\u7279\u5f81\u5bf9\u6a21\u578b\u4f30\u8ba1\u98ce\u9669\u6216\u4fdd\u62a4\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u805a\u7c7b\u4eceSHAP\u4e2d\u5171\u4eab\u6a21\u578b\u51b3\u7b56\u6a21\u5f0f\uff0c\u5e76\u94fe\u63a5\u56de\u89e3\u5256\u5b66\u5dee\u5f02\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u7684\u5e73\u5747AUC\u4e3a0.72-0.74\u3002\u5728\u6bcf\u4e2a\u7ec4\u4e2d\u53d1\u73b0\u4e86\u5171\u4eab\u76842\u578b\u7cd6\u5c3f\u75c5\u7279\u5f81\uff1a\u8102\u80aa\u6027\u9aa8\u9abc\u808c\u3001\u5e74\u9f84\u8f83\u5927\u3001\u5185\u810f\u548c\u76ae\u4e0b\u8102\u80aa\u8f83\u591a\uff0c\u4ee5\u53ca\u8f83\u5c0f\u6216\u8102\u80aa\u8fc7\u591a\u7684\u80f0\u817a\u3002\u5355\u53d8\u91cf\u903b\u8f91\u56de\u5f52\u786e\u8ba4\u4e86\u6bcf\u4e2a\u4e9a\u7ec4\u4e2d14-18\u4e2a\u524d20\u9884\u6d4b\u56e0\u5b50\u7684\u65b9\u5411\uff08p < 0.05\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8179\u90e8\u7279\u5f81\u5bf92\u578b\u7cd6\u5c3f\u75c5\u7684\u9a71\u52a8\u4f5c\u7528\u5728\u4e0d\u540c\u4f53\u91cd\u7c7b\u522b\u4e2d\u53ef\u80fd\u662f\u4e00\u81f4\u7684\u3002"}}
{"id": "2508.11272", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11272", "abs": "https://arxiv.org/abs/2508.11272", "authors": ["Jun Li", "Kai Li", "Shaoguo Liu", "Tingting Gao"], "title": "Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering", "comment": null, "summary": "Composed Image Retrieval (CIR) presents a significant challenge as it\nrequires jointly understanding a reference image and a modified textual\ninstruction to find relevant target images. Some existing methods attempt to\nuse a two-stage approach to further refine retrieval results. However, this\noften requires additional training of a ranking model. Despite the success of\nChain-of-Thought (CoT) techniques in reducing training costs for language\nmodels, their application in CIR tasks remains limited -- compressing visual\ninformation into text or relying on elaborate prompt designs. Besides, existing\nworks only utilize it for zero-shot CIR, as it is challenging to achieve\nsatisfactory results in supervised CIR with a well-trained model. In this work,\nwe proposed a framework that includes the Pyramid Matching Model with\nTraining-Free Refinement (PMTFR) to address these challenges. Through a simple\nbut effective module called Pyramid Patcher, we enhanced the Pyramid Matching\nModel's understanding of visual information at different granularities.\nInspired by representation engineering, we extracted representations from COT\ndata and injected them into the LVLMs. This approach allowed us to obtain\nrefined retrieval scores in the Training-Free Refinement paradigm without\nrelying on explicit textual reasoning, further enhancing performance. Extensive\nexperiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art\nmethods in supervised CIR tasks. The code will be made public.", "AI": {"tldr": "PMTFR\u6846\u67b6\u901a\u8fc7Pyramid Patcher\u548cTraining-Free Refinement\u63d0\u5347\u76d1\u7763\u5f0fCIR\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728CIR\u4efb\u52a1\u4e2d\u9762\u4e34\u9700\u8981\u989d\u5916\u8bad\u7ec3\u6392\u540d\u6a21\u578b\u6216\u4ec5\u9002\u7528\u4e8e\u96f6\u6837\u672cCIR\u7684\u5c40\u9650\u6027\uff0cPMTFR\u65e8\u5728\u901a\u8fc7\u65e0\u8bad\u7ec3\u7ec6\u5316\u8303\u5f0f\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542bPyramid Matching Model with Training-Free Refinement (PMTFR)\u7684\u6846\u67b6\uff0c\u5229\u7528Pyramid Patcher\u6a21\u5757\u589e\u5f3a\u89c6\u89c9\u4fe1\u606f\u7406\u89e3\uff0c\u5e76\u901a\u8fc7\u4eceCoT\u6570\u636e\u4e2d\u63d0\u53d6\u8868\u793a\u6ce8\u5165LVLMs\u4ee5\u5b9e\u73b0\u65e0\u8bad\u7ec3\u7ec6\u5316\u3002", "result": "\u5728CIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPMTFR\u5728\u76d1\u7763\u5f0fCIR\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "PMTFR\u6846\u67b6\u901a\u8fc7Pyramid Patcher\u6a21\u5757\u548cTraining-Free Refinement\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76d1\u7763\u5f0fCIR\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2508.11547", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11547", "abs": "https://arxiv.org/abs/2508.11547", "authors": ["Martin Jirou\u0161ek", "Tom\u00e1\u0161 B\u00e1\u010da", "Martin Saska"], "title": "Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads", "comment": null, "summary": "This paper addresses the problem of tracking the position of a\ncable-suspended payload carried by an unmanned aerial vehicle, with a focus on\nreal-world deployment and minimal hardware requirements. In contrast to many\nexisting approaches that rely on motion-capture systems, additional onboard\ncameras, or instrumented payloads, we propose a framework that uses only\nstandard onboard sensors--specifically, real-time kinematic global navigation\nsatellite system measurements and data from the onboard inertial measurement\nunit--to estimate and control the payload's position. The system models the\nfull coupled dynamics of the aerial vehicle and payload, and integrates a\nlinear Kalman filter for state estimation, a model predictive contouring\ncontrol planner, and an incremental model predictive controller. The control\narchitecture is designed to remain effective despite sensing limitations and\nestimation uncertainty. Extensive simulations demonstrate that the proposed\nsystem achieves performance comparable to control based on ground-truth\nmeasurements, with only minor degradation (< 6%). The system also shows strong\nrobustness to variations in payload parameters. Field experiments further\nvalidate the framework, confirming its practical applicability and reliable\nperformance in outdoor environments using only off-the-shelf aerial vehicle\nhardware.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4ec5\u7528\u6807\u51c6\u673a\u8f7d\u4f20\u611f\u5668\u8ddf\u8e2a\u65e0\u4eba\u673a\u8d1f\u8f7d\u4f4d\u7f6e\u7684\u6846\u67b6\uff0c\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\u63a5\u8fd1\u771f\u5b9e\u6d4b\u91cf\uff0c\u4e14\u9c81\u68d2\u6027\u5f3a\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u643a\u5e26\u7535\u7f06\u60ac\u6302\u8d1f\u8f7d\u4f4d\u7f6e\u8ddf\u8e2a\u7684\u95ee\u9898\uff0c\u5f3a\u8c03\u5b9e\u9645\u90e8\u7f72\u548c\u6700\u5c0f\u786c\u4ef6\u9700\u6c42\uff0c\u907f\u514d\u4f9d\u8d56\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u3001\u989d\u5916\u673a\u8f7d\u6444\u50cf\u5934\u6216\u4eea\u5668\u5316\u8d1f\u8f7d\u3002", "method": "\u7cfb\u7edf\u5efa\u6a21\u4e86\u98de\u884c\u5668\u548c\u8d1f\u8f7d\u7684\u5b8c\u6574\u8026\u5408\u52a8\u529b\u5b66\uff0c\u5e76\u96c6\u6210\u4e86\u7ebf\u6027\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\u3001\u6a21\u578b\u9884\u6d4b\u8f6e\u5ed3\u63a7\u5236\u89c4\u5212\u5668\u548c\u589e\u91cf\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u3002", "result": "\u4eff\u771f\u663e\u793a\u7cfb\u7edf\u6027\u80fd\u63a5\u8fd1\u57fa\u4e8e\u5730\u9762\u771f\u5b9e\u6d4b\u91cf\u7684\u63a7\u5236\uff0c\u4ec5\u8f7b\u5fae\u4e0b\u964d\uff08<6%\uff09\uff0c\u4e14\u5bf9\u8d1f\u8f7d\u53c2\u6570\u53d8\u5316\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002\u5b9e\u5730\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5728\u4ec5\u4f7f\u7528\u6807\u51c6\u673a\u8f7d\u4f20\u611f\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u57fa\u4e8e\u5730\u9762\u771f\u5b9e\u6d4b\u91cf\u7684\u63a7\u5236\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u6237\u5916\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.11106", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11106", "abs": "https://arxiv.org/abs/2508.11106", "authors": ["Xinjie Gao", "Bi'an Du", "Wei Hu"], "title": "HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing", "comment": null, "summary": "3D content generation remains a fundamental yet challenging task due to the\ninherent structural complexity of 3D data. While recent octree-based diffusion\nmodels offer a promising balance between efficiency and quality through\nhierarchical generation, they often overlook two key insights: 1) existing\nmethods typically model 3D objects as holistic entities, ignoring their\nsemantic part hierarchies and limiting generalization; and 2) holistic\nhigh-resolution modeling is computationally expensive, whereas real-world\nobjects are inherently sparse and hierarchical, making them well-suited for\nlayered generation. Motivated by these observations, we propose HierOctFusion,\na part-aware multi-scale octree diffusion model that enhances hierarchical\nfeature interaction for generating fine-grained and sparse object structures.\nFurthermore, we introduce a cross-attention conditioning mechanism that injects\npart-level information into the generation process, enabling semantic features\nto propagate effectively across hierarchical levels from parts to the whole.\nAdditionally, we construct a 3D dataset with part category annotations using a\npre-trained segmentation model to facilitate training and evaluation.\nExperiments demonstrate that HierOctFusion achieves superior shape quality and\nefficiency compared to prior methods.", "AI": {"tldr": "HierOctFusion\u901a\u8fc7\u5206\u5c42\u751f\u6210\u548c\u90e8\u5206\u611f\u77e5\u673a\u5236\uff0c\u9ad8\u6548\u751f\u6210\u7cbe\u7ec63D\u7ed3\u6784\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c063D\u5bf9\u8c61\u89c6\u4e3a\u6574\u4f53\uff0c\u5ffd\u7565\u8bed\u4e49\u90e8\u5206\u5c42\u6b21\u7ed3\u6784\uff0c\u4e14\u9ad8\u5206\u8fa8\u7387\u5efa\u6a21\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u771f\u5b9e\u5bf9\u8c61\u5177\u6709\u7a00\u758f\u6027\u548c\u5c42\u6b21\u6027\u3002", "method": "\u63d0\u51faHierOctFusion\uff0c\u4e00\u79cd\u57fa\u4e8e\u90e8\u5206\u611f\u77e5\u7684\u591a\u5c3a\u5ea6\u516b\u53c9\u6811\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u8de8\u6ce8\u610f\u529b\u6761\u4ef6\u673a\u5236\u548c\u5206\u5c42\u7279\u5f81\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHierOctFusion\u5728\u5f62\u72b6\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HierOctFusion\u901a\u8fc7\u591a\u5c3a\u5ea6\u516b\u53c9\u6811\u6269\u6563\u6a21\u578b\u548c\u8de8\u6ce8\u610f\u529b\u6761\u4ef6\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5185\u5bb9\u751f\u6210\u7684\u7cbe\u7ec6\u5ea6\u548c\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.11573", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11573", "abs": "https://arxiv.org/abs/2508.11573", "authors": ["Mogens Plessen"], "title": "Nominal Evaluation Of Automatic Multi-Sections Control Potential In Comparison To A Simpler One- Or Two-Sections Alternative With Predictive Spray Switching", "comment": "14 pages plus 7 pages appendix with additional figures, 18 main\n  figures, 3 tables", "summary": "Automatic Section Control (ASC) is a long-standing trend for spraying in\nagriculture. It promises to minimise spray overlap areas. The core idea is to\n(i) switch off spray nozzles on areas that have already been sprayed, and (ii)\nto dynamically adjust nozzle flow rates along the boom bar that holds the spray\nnozzles when velocities of boom sections vary during turn maneuvers. ASC is not\npossible without sensors, in particular for accurate positioning data. Spraying\nand the movement of modern wide boom bars are highly dynamic processes. In\naddition, many uncertainty factors have an effect such as cross wind drift,\nboom height, nozzle clogging in open-field conditions, and so forth. In view of\nthis complexity, the natural question arises if a simpler alternative exist.\nTherefore, an Automatic Multi-Sections Control method is compared to a proposed\nsimpler one- or two-sections alternative that uses predictive spray switching.\nThe comparison is provided under nominal conditions. Agricultural spraying is\nintrinsically linked to area coverage path planning and spray switching logic.\nCombinations of two area coverage path planning and switching logics as well as\nthree sections-setups are compared. The three sections-setups differ by\ncontrolling 48 sections, 2 sections or controlling all nozzles uniformly with\nthe same control signal as one single section. Methods are evaluated on 10\ndiverse real-world field examples, including non-convex field contours,\nfreeform mainfield lanes and multiple obstacle areas. A preferred method is\nsuggested that (i) minimises area coverage pathlength, (ii) offers intermediate\noverlap, (iii) is suitable for manual driving by following a pre-planned\npredictive spray switching logic for an area coverage path plan, and (iv) and\nin contrast to ASC can be implemented sensor-free and therefore at low cost.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edfASC\u4e0e\u7b80\u5316\u55b7\u6d12\u63a7\u5236\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u4f20\u611f\u5668\u81ea\u7531\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u519c\u7530\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u6bb5\u63a7\u5236\uff08ASC\uff09\u9700\u8981\u590d\u6742\u4f20\u611f\u5668\u4e14\u53d7\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u56e0\u7d20\u5f71\u54cd\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u4f4e\u6210\u672c\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u6bd4\u8f83\u4e86\u81ea\u52a8\u591a\u6bb5\u63a7\u5236\u65b9\u6cd5\u4e0e\u63d0\u51fa\u7684\u4e00\u6216\u4e24\u6bb5\u66ff\u4ee3\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u4e24\u79cd\u8def\u5f84\u89c4\u5212\u548c\u55b7\u6d12\u903b\u8f91\uff0c\u5e76\u572810\u4e2a\u771f\u5b9e\u519c\u7530\u573a\u666f\u4e2d\u8bc4\u4f30\u4e86\u4e09\u79cd\u6bb5\u914d\u7f6e\uff0848\u6bb5\u30012\u6bb5\u548c\u5355\u6bb5\u63a7\u5236\uff09\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u63d0\u51fa\u7684\u7b80\u5316\u65b9\u6cd5\u5728\u4f20\u611f\u5668\u81ea\u7531\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u6709\u6548\u6ee1\u8db3\u55b7\u6d12\u9700\u6c42\uff0c\u5e76\u5728\u8def\u5f84\u89c4\u5212\u548c\u55b7\u6d12\u903b\u8f91\u7ec4\u5408\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u7684\u81ea\u52a8\u591a\u6bb5\u63a7\u5236\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edfASC\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4f20\u611f\u5668\u81ea\u7531\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4f4e\u6210\u672c\u55b7\u6d12\uff0c\u540c\u65f6\u6ee1\u8db3\u8def\u5f84\u957f\u5ea6\u6700\u5c0f\u5316\u3001\u4e2d\u7b49\u91cd\u53e0\u7b49\u9700\u6c42\u3002"}}
{"id": "2508.11115", "categories": ["cs.CV", "cs.HC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11115", "abs": "https://arxiv.org/abs/2508.11115", "authors": ["Haotang Li", "Zhenyu Qi", "Sen He", "Kebin Peng", "Sheng Tan", "Yili Ren", "Tomas Cerny", "Jiyue Zhao", "Zi Wang"], "title": "UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring", "comment": null, "summary": "Improper sitting posture during prolonged computer use has become a\nsignificant public health concern. Traditional posture monitoring solutions\nface substantial barriers, including privacy concerns with camera-based systems\nand user discomfort with wearable sensors. This paper presents\nUWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that\nadvances mobile technologies for preventive health management through\ncontinuous, contactless monitoring of ergonomic sitting posture. Our system\nleverages commercial UWB devices, utilizing comprehensive feature engineering\nto extract multiple ergonomic sitting posture features. We develop PoseGBDT to\neffectively capture temporal dependencies in posture patterns, addressing\nlimitations of traditional frame-wise classification approaches. Extensive\nreal-world evaluation across 10 participants and 19 distinct postures\ndemonstrates exceptional performance, achieving 99.11% accuracy while\nmaintaining robustness against environmental variables such as clothing\nthickness, additional devices, and furniture configurations. Our system\nprovides a scalable, privacy-preserving mobile health solution on existing\nplatforms for proactive ergonomic management, improving quality of life at low\ncosts.", "AI": {"tldr": "UWB-PostureGuard\u662f\u4e00\u79cd\u57fa\u4e8eUWB\u7684\u9690\u79c1\u4fdd\u62a4\u5750\u59ff\u76d1\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7PoseGBDT\u6a21\u578b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u76d1\u6d4b\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u73af\u5883\u3002", "motivation": "\u4f20\u7edf\u59ff\u52bf\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u9690\u79c1\u95ee\u9898\u548c\u7528\u6237\u4e0d\u9002\uff0c\u9700\u8981\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u4e14\u65e0\u63a5\u89e6\u7684\u76d1\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5546\u4e1aUWB\u8bbe\u5907\uff0c\u901a\u8fc7\u5168\u9762\u7684\u7279\u5f81\u5de5\u7a0b\u63d0\u53d6\u591a\u79cd\u4eba\u4f53\u5de5\u5b66\u5750\u59ff\u7279\u5f81\uff0c\u5e76\u5f00\u53d1PoseGBDT\u4ee5\u6709\u6548\u6355\u6349\u59ff\u52bf\u6a21\u5f0f\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u572810\u540d\u53c2\u4e0e\u8005\u548c19\u79cd\u4e0d\u540c\u59ff\u52bf\u7684\u5e7f\u6cdb\u5b9e\u9645\u8bc4\u4f30\u4e2d\uff0c\u7cfb\u7edf\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u8fbe\u523099.11%\uff0c\u5e76\u5bf9\u73af\u5883\u53d8\u91cf\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "UWB-PostureGuard\u7cfb\u7edf\u901a\u8fc7\u8d85\u5bbd\u5e26\uff08UWB\uff09\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u79fb\u52a8\u5065\u5eb7\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u4f4e\u6210\u672c\u6539\u5584\u4e86\u751f\u6d3b\u8d28\u91cf\u3002"}}
{"id": "2508.11354", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11354", "abs": "https://arxiv.org/abs/2508.11354", "authors": ["Zhenyi Zhao", "Muthu Rama Krishnan Mookiah", "Emanuele Trucco"], "title": "Leveraging the RETFound foundation model for optic disc segmentation in retinal images", "comment": null, "summary": "RETFound is a well-known foundation model (FM) developed for fundus camera\nand optical coherence tomography images. It has shown promising performance\nacross multiple datasets in diagnosing diseases, both eye-specific and\nsystemic, from retinal images. However, to our best knowledge, it has not been\nused for other tasks. We present the first adaptation of RETFound for optic\ndisc segmentation, a ubiquitous and foundational task in retinal image\nanalysis. The resulting segmentation system outperforms state-of-the-art,\nsegmentation-specific baseline networks after training a head with only a very\nmodest number of task-specific examples. We report and discuss results with\nfour public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private\ndataset, GoDARTS, achieving about 96% Dice consistently across all datasets.\nOverall, our method obtains excellent performance in internal verification,\ndomain generalization and domain adaptation, and exceeds most of the\nstate-of-the-art baseline results. We discuss the results in the framework of\nthe debate about FMs as alternatives to task-specific architectures. The code\nis available at: [link to be added after the paper is accepted]", "AI": {"tldr": "RETFound\u57fa\u7840\u6a21\u578b\u9996\u6b21\u9002\u914d\u4e8e\u89c6\u76d8\u5206\u5272\u4efb\u52a1\uff0c\u4ec5\u9700\u5c11\u91cf\u793a\u4f8b\u8bad\u7ec3\u5934\u90e8\u7f51\u7edc\uff0c\u5373\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u7ea696% Dice\u7cfb\u6570\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22RETFound\u57fa\u7840\u6a21\u578b\u5728\u89c6\u76d8\u5206\u5272\u8fd9\u4e00\u89c6\u7f51\u819c\u56fe\u50cf\u5206\u6790\u57fa\u7840\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u5934\u90e8\u7f51\u7edc\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\u4efb\u52a1\u7279\u5b9a\u793a\u4f8b\uff0c\u5c06RETFound\u9002\u914d\u4e8e\u89c6\u76d8\u5206\u5272\u4efb\u52a1\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff08IDRID\u3001Drishti-GS\u3001RIM-ONE-r3\u3001REFUGE\uff09\u548c\u4e00\u4e2a\u79c1\u6709\u6570\u636e\u96c6\uff08GoDARTS\uff09\u4e0a\uff0cDice\u7cfb\u6570\u8fbe\u5230\u7ea696%\uff0c\u6027\u80fd\u4f18\u4e8e\u5927\u591a\u6570\u6700\u5148\u8fdb\u57fa\u7ebf\u3002", "conclusion": "RETFound\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u5728\u89c6\u76d8\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u7279\u5b9a\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u57fa\u7ebf\u7f51\u7edc\uff0c\u8fd8\u5728\u5185\u90e8\u9a8c\u8bc1\u3001\u9886\u57df\u6cdb\u5316\u548c\u9886\u57df\u9002\u5e94\u65b9\u9762\u5c55\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.11584", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11584", "abs": "https://arxiv.org/abs/2508.11584", "authors": ["Jakub \u0141ucki", "Jonathan Becktor", "Georgios Georgakis", "Robert Royce", "Shehryar Khattak"], "title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks", "comment": "6 pages, 6 figures, 2 tables", "summary": "Deploying multiple machine learning models on resource-constrained robotic\nplatforms for different perception tasks often results in redundant\ncomputations, large memory footprints, and complex integration challenges. In\nresponse, this work presents Visual Perception Engine (VPEngine), a modular\nframework designed to enable efficient GPU usage for visual multitasking while\nmaintaining extensibility and developer accessibility. Our framework\narchitecture leverages a shared foundation model backbone that extracts image\nrepresentations, which are efficiently shared, without any unnecessary GPU-CPU\nmemory transfers, across multiple specialized task-specific model heads running\nin parallel. This design eliminates the computational redundancy inherent in\nfeature extraction component when deploying traditional sequential models while\nenabling dynamic task prioritization based on application demands. We\ndemonstrate our framework's capabilities through an example implementation\nusing DINOv2 as the foundation model with multiple task (depth, object\ndetection and semantic segmentation) heads, achieving up to 3x speedup compared\nto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine\noffers efficient GPU utilization and maintains a constant memory footprint\nwhile allowing per-task inference frequencies to be adjusted dynamically during\nruntime. The framework is written in Python and is open source with ROS2 C++\n(Humble) bindings for ease of use by the robotics community across diverse\nrobotic platforms. Our example implementation demonstrates end-to-end real-time\nperformance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized\nmodels.", "AI": {"tldr": "VPEngine\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u57fa\u7840\u6a21\u578b\u548c\u591a\u4efb\u52a1\u5e76\u884c\u5904\u7406\u5934\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u89c6\u89c9\u591a\u4efb\u52a1\u7684\u6548\u7387\uff0c\u5b9e\u73b0\u4e863\u500d\u901f\u5ea6\u63d0\u5347\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u90e8\u7f72\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\u51fa\u73b0\u7684\u8ba1\u7b97\u5197\u4f59\u3001\u5185\u5b58\u5360\u7528\u5927\u548c\u96c6\u6210\u590d\u6742\u7684\u95ee\u9898\u3002", "method": "VPEngine\u91c7\u7528\u5171\u4eab\u57fa\u7840\u6a21\u578b\u9aa8\u5e72\uff08\u5982DINOv2\uff09\u63d0\u53d6\u56fe\u50cf\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u591a\u4efb\u52a1\u5e76\u884c\u5904\u7406\u5934\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\uff0c\u907f\u514d\u4e86\u4e0d\u5fc5\u8981\u7684GPU-CPU\u5185\u5b58\u4f20\u8f93\u3002\u7ed3\u5408CUDA MPS\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684GPU\u5229\u7528\u548c\u6052\u5b9a\u5185\u5b58\u5360\u7528\u3002", "result": "VPEngine\u5728NVIDIA Jetson Orin AGX\u4e0a\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u5b9e\u65f6\u6027\u80fd\uff08\u226550 Hz\uff09\uff0c\u5e76\u901a\u8fc7TensorRT\u4f18\u5316\u6a21\u578b\u5c55\u793a\u4e86\u9ad8\u8fbe3\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "VPEngine\u6846\u67b6\u901a\u8fc7\u5171\u4eab\u57fa\u7840\u6a21\u578b\u9aa8\u5e72\u548c\u591a\u4efb\u52a1\u5e76\u884c\u5904\u7406\u5934\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u53d7\u9650\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u89c6\u89c9\u591a\u4efb\u52a1\u5904\u7406\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe3\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u4fdd\u6301\u4e86\u52a8\u6001\u4efb\u52a1\u4f18\u5148\u7ea7\u8c03\u6574\u7684\u80fd\u529b\u3002"}}
{"id": "2508.11134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11134", "abs": "https://arxiv.org/abs/2508.11134", "authors": ["Bing Liu", "Le Wang", "Hao Liu", "Mingming Liu"], "title": "Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation", "comment": "7 pages, 5 figures, 2025 ICME Accepted", "summary": "Current deep dehazing methods only focus on removing haze from hazy images,\nlacking the capability to translate between hazy and haze-free images. To\naddress this issue, we propose a residual-based efficient bidirectional\ndiffusion model (RBDM) that can model the conditional distributions for both\ndehazing and haze generation. Firstly, we devise dual Markov chains that can\neffectively shift the residuals and facilitate bidirectional smooth transitions\nbetween them. Secondly, the RBDM perturbs the hazy and haze-free images at\nindividual timesteps and predicts the noise in the perturbed data to\nsimultaneously learn the conditional distributions. Finally, to enhance\nperformance on relatively small datasets and reduce computational costs, our\nmethod introduces a unified score function learned on image patches instead of\nentire images. Our RBDM successfully implements size-agnostic bidirectional\ntransitions between haze-free and hazy images with only 15 sampling steps.\nExtensive experiments demonstrate that the proposed method achieves superior or\nat least comparable performance to state-of-the-art methods on both synthetic\nand real-world datasets.", "AI": {"tldr": "\u63d0\u51faRBDM\u6a21\u578b\uff0c\u5b9e\u73b0\u6709\u96fe\u548c\u65e0\u96fe\u56fe\u50cf\u7684\u53cc\u5411\u8f6c\u6362\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u53bb\u96fe\u65b9\u6cd5\u4ec5\u5173\u6ce8\u4ece\u6709\u96fe\u56fe\u50cf\u4e2d\u53bb\u9664\u96fe\uff0c\u7f3a\u4e4f\u5728\u6709\u96fe\u548c\u65e0\u96fe\u56fe\u50cf\u4e4b\u95f4\u8fdb\u884c\u8f6c\u6362\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6b8b\u5dee\u7684\u9ad8\u6548\u53cc\u5411\u6269\u6563\u6a21\u578b\uff08RBDM\uff09\uff0c\u8bbe\u8ba1\u4e86\u53cc\u9a6c\u5c14\u53ef\u592b\u94fe\u6765\u6709\u6548\u8f6c\u79fb\u6b8b\u5dee\u5e76\u4fc3\u8fdb\u53cc\u5411\u5e73\u6ed1\u8fc7\u6e21\uff0c\u901a\u8fc7\u5728\u5355\u4e2a\u65f6\u95f4\u6b65\u6270\u52a8\u6709\u96fe\u548c\u65e0\u96fe\u56fe\u50cf\u5e76\u9884\u6d4b\u6270\u52a8\u6570\u636e\u4e2d\u7684\u566a\u58f0\u6765\u540c\u65f6\u5b66\u4e60\u6761\u4ef6\u5206\u5e03\u3002", "result": "RBDM\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u81f3\u5c11\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "RBDM\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u4ec5\u670915\u4e2a\u91c7\u6837\u6b65\u9aa4\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u65e0\u96fe\u548c\u6709\u96fe\u56fe\u50cf\u4e4b\u95f4\u8fdb\u884c\u5927\u5c0f\u65e0\u5173\u7684\u53cc\u5411\u8f6c\u6362\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u81f3\u5c11\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002"}}
{"id": "2508.11374", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11374", "abs": "https://arxiv.org/abs/2508.11374", "authors": ["Devansh Arora", "Nitin Kumar", "Sukrit Gupta"], "title": "Does the Skeleton-Recall Loss Really Work?", "comment": null, "summary": "Image segmentation is an important and widely performed task in computer\nvision. Accomplishing effective image segmentation in diverse settings often\nrequires custom model architectures and loss functions. A set of models that\nspecialize in segmenting thin tubular structures are topology\npreservation-based loss functions. These models often utilize a pixel\nskeletonization process claimed to generate more precise segmentation masks of\nthin tubes and better capture the structures that other models often miss. One\nsuch model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\\cite\n{kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark\ntubular datasets. In this work, we performed a theoretical analysis of the\ngradients for the SRL loss. Upon comparing the performance of the proposed\nmethod on some of the tubular datasets (used in the original work, along with\nsome additional datasets), we found that the performance of SRL-based\nsegmentation models did not exceed traditional baseline models. By providing\nboth a theoretical explanation and empirical evidence, this work critically\nevaluates the limitations of topology-based loss functions, offering valuable\ninsights for researchers aiming to develop more effective segmentation models\nfor complex tubular structures.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86SRL\u635f\u5931\u7684\u7406\u8bba\u68af\u5ea6\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u5176\u6027\u80fd\uff0c\u53d1\u73b0\u5176\u5e76\u672a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u62d3\u6251\u4fdd\u6301\u635f\u5931\u51fd\u6570\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u56fe\u50cf\u5206\u5272\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u91cd\u8981\u4efb\u52a1\uff0c\u9488\u5bf9\u8584\u7ba1\u72b6\u7ed3\u6784\u7684\u6709\u6548\u5206\u5272\u9700\u8981\u5b9a\u5236\u6a21\u578b\u67b6\u6784\u548c\u635f\u5931\u51fd\u6570\u3002\u62d3\u6251\u4fdd\u6301\u635f\u5931\u51fd\u6570\uff08\u5982SRL\uff09\u58f0\u79f0\u80fd\u751f\u6210\u66f4\u7cbe\u786e\u7684\u5206\u5272\u63a9\u7801\uff0c\u4f46\u672c\u7814\u7a76\u8d28\u7591\u5176\u5b9e\u9645\u6548\u679c\u3002", "method": "\u5bf9SRL\u635f\u5931\u7684\u68af\u5ea6\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u5e76\u5728\u591a\u4e2a\u7ba1\u72b6\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u4e86SRL\u4e0e\u4f20\u7edf\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8eSRL\u7684\u5206\u5272\u6a21\u578b\u6027\u80fd\u5e76\u672a\u8d85\u8fc7\u4f20\u7edf\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc1\u636e\uff0c\u6279\u5224\u6027\u5730\u8bc4\u4f30\u4e86\u57fa\u4e8e\u62d3\u6251\u7684\u635f\u5931\u51fd\u6570\uff08\u5982SRL\uff09\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u590d\u6742\u7ba1\u72b6\u7ed3\u6784\u5206\u5272\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.11588", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11588", "abs": "https://arxiv.org/abs/2508.11588", "authors": ["Benjamin Walt", "Jordan Westphal", "Girish Krishnan"], "title": "Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation", "comment": null, "summary": "Effective and efficient agricultural manipulation and harvesting depend on\naccurately understanding the current state of the grasp. The agricultural\nenvironment presents unique challenges due to its complexity, clutter, and\nocclusion. Additionally, fruit is physically attached to the plant, requiring\nprecise separation during harvesting. Selecting appropriate sensors and\nmodeling techniques is critical for obtaining reliable feedback and correctly\nidentifying grasp states. This work investigates a set of key sensors, namely\ninertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile\nsensors, and RGB cameras, integrated into a compliant gripper to classify grasp\nstates. We evaluate the individual contribution of each sensor and compare the\nperformance of two widely used classification models: Random Forest and Long\nShort-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest\nclassifier, trained in a controlled lab environment and tested on real cherry\ntomato plants, achieved 100% accuracy in identifying slip, grasp failure, and\nsuccessful picks, marking a substantial improvement over baseline performance.\nFurthermore, we identify a minimal viable sensor combination, namely IMU and\ntension sensors that effectively classifies grasp states. This classifier\nenables the planning of corrective actions based on real-time feedback, thereby\nenhancing the efficiency and reliability of fruit harvesting operations.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u96c6\u6210\u591a\u4f20\u611f\u5668\u548c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u4e86\u519c\u4e1a\u91c7\u6458\u4e2d\u6293\u53d6\u72b6\u6001\u7684100%\u51c6\u786e\u5206\u7c7b\uff0c\u63d0\u5347\u4e86\u91c7\u6458\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u519c\u4e1a\u73af\u5883\u56e0\u590d\u6742\u6027\u3001\u6742\u4e71\u6027\u548c\u906e\u6321\u6027\u5e26\u6765\u72ec\u7279\u6311\u6218\uff0c\u9700\u7cbe\u786e\u7406\u89e3\u6293\u53d6\u72b6\u6001\u4ee5\u5b9e\u73b0\u9ad8\u6548\u91c7\u6458\uff0c\u56e0\u6b64\u9700\u9009\u62e9\u5408\u9002\u7684\u4f20\u611f\u5668\u548c\u5efa\u6a21\u6280\u672f\u4ee5\u83b7\u53d6\u53ef\u9760\u53cd\u9988\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86IMU\u3001\u7ea2\u5916\u53cd\u5c04\u3001\u5f20\u529b\u3001\u89e6\u89c9\u4f20\u611f\u5668\u548cRGB\u6444\u50cf\u5934\u7684\u8d21\u732e\uff0c\u5e76\u6bd4\u8f83\u4e86\u968f\u673a\u68ee\u6797\u548cLSTM\u7f51\u7edc\u4e24\u79cd\u5206\u7c7b\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u5728\u5b9e\u9a8c\u5ba4\u63a7\u5236\u73af\u5883\u4e2d\u8bad\u7ec3\u5e76\u5728\u5b9e\u9645\u6a31\u6843\u756a\u8304\u690d\u682a\u4e0a\u6d4b\u8bd5\u7684\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\uff0c\u5bf9\u6293\u53d6\u72b6\u6001\u7684\u5206\u7c7b\u51c6\u786e\u7387\u8fbe100%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210IMU\u548c\u5f20\u529b\u4f20\u611f\u5668\u7684\u6700\u5c0f\u53ef\u884c\u7ec4\u5408\uff0c\u7ed3\u5408\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\uff0c\u8be5\u7814\u7a76\u663e\u8457\u63d0\u9ad8\u4e86\u519c\u4e1a\u91c7\u6458\u64cd\u4f5c\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6293\u53d6\u72b6\u6001\uff08\u6ed1\u52a8\u3001\u6293\u53d6\u5931\u8d25\u548c\u6210\u529f\u91c7\u6458\uff09\u7684100%\u51c6\u786e\u5206\u7c7b\u3002"}}
{"id": "2508.11379", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11379", "abs": "https://arxiv.org/abs/2508.11379", "authors": ["Ramil Khafizov", "Artem Komarichev", "Ruslan Rakhimov", "Peter Wonka", "Evgeny Burnaev"], "title": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration", "comment": null, "summary": "We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene\nreconstruction that enhances the CUT3R model by integrating prior information.\nUnlike existing feed-forward methods that rely solely on input images, our\nmethod leverages auxiliary data, such as depth, camera calibrations, or camera\npositions, commonly available in real-world scenarios. We propose a lightweight\nmodification to CUT3R, incorporating a dedicated encoder for each modality to\nextract features, which are fused with RGB image tokens via zero convolution.\nThis flexible design enables seamless integration of any combination of prior\ninformation during inference. Evaluated across multiple benchmarks, including\n3D reconstruction and other multi-view tasks, our approach demonstrates\nsignificant performance improvements, showing its ability to effectively\nutilize available priors while maintaining compatibility with varying input\nmodalities.", "AI": {"tldr": "G-CUT3R\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u5148\u9a8c\u4fe1\u606f\u4f18\u53163D\u91cd\u5efa\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u4e14\u4fdd\u6301\u8f93\u5165\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709\u524d\u9988\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u8f93\u5165\u56fe\u50cf\uff0c\u800cG-CUT3R\u5229\u7528\u5b9e\u9645\u573a\u666f\u4e2d\u5e38\u89c1\u7684\u8f85\u52a9\u6570\u636e\uff08\u5982\u6df1\u5ea6\u3001\u76f8\u673a\u6821\u51c6\u6216\u4f4d\u7f6e\uff09\u6765\u63d0\u5347\u91cd\u5efa\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4fee\u6539CUT3R\u7684\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u79cd\u6a21\u6001\u8bbe\u8ba1\u4e13\u7528\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u96f6\u5377\u79ef\u4e0eRGB\u56fe\u50cf\u4ee4\u724c\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cG-CUT3R\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u5229\u7528\u5148\u9a8c\u4fe1\u606f\u7684\u80fd\u529b\u3002", "conclusion": "G-CUT3R\u901a\u8fc7\u6574\u5408\u5148\u9a8c\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e863D\u573a\u666f\u91cd\u5efa\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u4e0d\u540c\u8f93\u5165\u6a21\u6001\u7684\u517c\u5bb9\u6027\u3002"}}
{"id": "2508.11153", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11153", "abs": "https://arxiv.org/abs/2508.11153", "authors": ["Maoquan Zhang", "Bisser Raytchev", "Xiujuan Sun"], "title": "LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction", "comment": "The International Conference on Neural Information Processing\n  (ICONIP) 2025", "summary": "LEARN is a layout-aware diffusion framework designed to generate\npedagogically aligned illustrations for STEM education. It leverages a curated\nBookCover dataset that provides narrative layouts and structured visual cues,\nenabling the model to depict abstract and sequential scientific concepts with\nstrong semantic alignment. Through layout-conditioned generation, contrastive\nvisual-semantic training, and prompt modulation, LEARN produces coherent visual\nsequences that support mid-to-high-level reasoning in line with Bloom's\ntaxonomy while reducing extraneous cognitive load as emphasized by Cognitive\nLoad Theory. By fostering spatially organized and story-driven narratives, the\nframework counters fragmented attention often induced by short-form media and\npromotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates\npotential for integration with multimodal systems and curriculum-linked\nknowledge graphs to create adaptive, exploratory educational content. As the\nfirst generative approach to unify layout-based storytelling, semantic\nstructure learning, and cognitive scaffolding, LEARN represents a novel\ndirection for generative AI in education. The code and dataset will be released\nto facilitate future research and practical deployment.", "AI": {"tldr": "LEARN\u662f\u4e00\u4e2a\u5e03\u5c40\u611f\u77e5\u7684\u6269\u6563\u6846\u67b6\uff0c\u4e13\u4e3aSTEM\u6559\u80b2\u751f\u6210\u6559\u5b66\u5bf9\u9f50\u7684\u63d2\u56fe\uff0c\u901a\u8fc7\u5e03\u5c40\u6761\u4ef6\u548c\u8bed\u4e49\u8bad\u7ec3\u51cf\u5c11\u8ba4\u77e5\u8d1f\u8377\uff0c\u652f\u6301\u4e2d\u9ad8\u7ea7\u63a8\u7406\u3002", "motivation": "\u8bbe\u8ba1LEARN\u6846\u67b6\u65e8\u5728\u4e3aSTEM\u6559\u80b2\u751f\u6210\u6559\u5b66\u5bf9\u9f50\u7684\u63d2\u56fe\uff0c\u4ee5\u652f\u6301\u5e03\u9c81\u59c6\u5206\u7c7b\u5b66\u4e2d\u7684\u4e2d\u9ad8\u7ea7\u63a8\u7406\uff0c\u5e76\u51cf\u5c11\u8ba4\u77e5\u8d1f\u8377\u3002", "method": "\u901a\u8fc7\u5e03\u5c40\u6761\u4ef6\u751f\u6210\u3001\u5bf9\u6bd4\u89c6\u89c9\u8bed\u4e49\u8bad\u7ec3\u548c\u63d0\u793a\u8c03\u5236\uff0cLEARN\u751f\u6210\u8fde\u8d2f\u7684\u89c6\u89c9\u5e8f\u5217\u3002", "result": "LEARN\u80fd\u591f\u751f\u6210\u7a7a\u95f4\u7ec4\u7ec7\u4e14\u6545\u4e8b\u9a71\u52a8\u7684\u53d9\u4e8b\uff0c\u5bf9\u6297\u77ed\u5a92\u4f53\u5f15\u53d1\u7684\u6ce8\u610f\u529b\u788e\u7247\u5316\uff0c\u4fc3\u8fdb\u6301\u7eed\u7684\u6982\u5ff5\u805a\u7126\u3002", "conclusion": "LEARN\u4f5c\u4e3a\u9996\u4e2a\u7ed3\u5408\u5e03\u5c40\u53d9\u4e8b\u3001\u8bed\u4e49\u7ed3\u6784\u5b66\u4e60\u548c\u8ba4\u77e5\u652f\u67b6\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u4e3a\u6559\u80b2\u9886\u57df\u7684\u751f\u6210AI\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.11165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11165", "abs": "https://arxiv.org/abs/2508.11165", "authors": ["Bing Liu", "Le Wang", "Mingming Liu", "Hao Liu", "Rui Yao", "Yong Zhou", "Peng Liu", "Tongqiang Xia"], "title": "Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models", "comment": "10 pages, 4 figures", "summary": "Existing dehazing methods deal with real-world haze images with difficulty,\nespecially scenes with thick haze. One of the main reasons is the lack of\nreal-world paired data and robust priors. To avoid the costly collection of\npaired hazy and clear images, we propose an efficient semi-supervised image\ndehazing method via Expectation-Maximization and Bidirectional Brownian Bridge\nDiffusion Models (EM-B3DM) with a two-stage learning scheme. In the first\nstage, we employ the EM algorithm to decouple the joint distribution of paired\nhazy and clear images into two conditional distributions, which are then\nmodeled using a unified Brownian Bridge diffusion model to directly capture the\nstructural and content-related correlations between hazy and clear images. In\nthe second stage, we leverage the pre-trained model and large-scale unpaired\nhazy and clear images to further improve the performance of image dehazing.\nAdditionally, we introduce a detail-enhanced Residual Difference Convolution\nblock (RDC) to capture gradient-level information, significantly enhancing the\nmodel's representation capability. Extensive experiments demonstrate that our\nEM-B3DM achieves superior or at least comparable performance to\nstate-of-the-art methods on both synthetic and real-world datasets.", "AI": {"tldr": "\u63d0\u51faEM-B3DM\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5b66\u4e60\u65b9\u6848\u548c\u7ec6\u8282\u589e\u5f3a\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u53bb\u96fe\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53bb\u96fe\u65b9\u6cd5\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u96fe\u973e\u56fe\u50cf\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u6d53\u96fe\u573a\u666f\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u914d\u5bf9\u6570\u636e\u548c\u9c81\u68d2\u5148\u9a8c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u671f\u671b\u6700\u5927\u5316\u548c\u53cc\u5411\u5e03\u6717\u6865\u6269\u6563\u6a21\u578b\uff08EM-B3DM\uff09\u7684\u4e24\u9636\u6bb5\u5b66\u4e60\u65b9\u6848\uff0c\u5305\u62ecEM\u7b97\u6cd5\u89e3\u8026\u8054\u5408\u5206\u5e03\u548c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u53ca\u5927\u89c4\u6a21\u672a\u914d\u5bf9\u6570\u636e\u63d0\u5347\u6027\u80fd\u3002", "result": "EM-B3DM\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "EM-B3DM\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u4e8e\u6216\u81f3\u5c11\u53ef\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11167", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11167", "abs": "https://arxiv.org/abs/2508.11167", "authors": ["Jianhong Han", "Yupei Wang", "Liang Chen"], "title": "VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images", "comment": "Manuscript submitted to IEEE TGRS", "summary": "Unsupervised domain adaptation methods have been widely explored to bridge\ndomain gaps. However, in real-world remote-sensing scenarios, privacy and\ntransmission constraints often preclude access to source domain data, which\nlimits their practical applicability. Recently, Source-Free Object Detection\n(SFOD) has emerged as a promising alternative, aiming at cross-domain\nadaptation without relying on source data, primarily through a self-training\nparadigm. Despite its potential, SFOD frequently suffers from training collapse\ncaused by noisy pseudo-labels, especially in remote sensing imagery with dense\nobjects and complex backgrounds. Considering that limited target domain\nannotations are often feasible in practice, we propose a Vision\nfoundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised\nframework for SFOD in remote sensing images. VG-DETR integrates a Vision\nFoundation Model (VFM) into the training pipeline in a \"free lunch\" manner,\nleveraging a small amount of labeled target data to mitigate pseudo-label noise\nwhile improving the detector's feature-extraction capability. Specifically, we\nintroduce a VFM-guided pseudo-label mining strategy that leverages the VFM's\nsemantic priors to further assess the reliability of the generated\npseudo-labels. By recovering potentially correct predictions from\nlow-confidence outputs, our strategy improves pseudo-label quality and\nquantity. In addition, a dual-level VFM-guided alignment method is proposed,\nwhich aligns detector features with VFM embeddings at both the instance and\nimage levels. Through contrastive learning among fine-grained prototypes and\nsimilarity matching between feature maps, this dual-level alignment further\nenhances the robustness of feature representations against domain gaps.\nExtensive experiments demonstrate that VG-DETR achieves superior performance in\nsource-free remote sensing detection tasks.", "AI": {"tldr": "VG-DETR\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u534a\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u6e90\u81ea\u7531\u9065\u611f\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u6316\u6398\u548c\u53cc\u7ea7\u5bf9\u9f50\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6e90\u81ea\u7531\u76ee\u6807\u68c0\u6d4b\uff08SFOD\uff09\u5728\u9065\u611f\u56fe\u50cf\u4e2d\u56e0\u4f2a\u6807\u7b7e\u566a\u58f0\u548c\u590d\u6742\u80cc\u666f\u5bfc\u81f4\u7684\u8bad\u7ec3\u5d29\u6e83\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528\u5c11\u91cf\u6807\u6ce8\u76ee\u6807\u6570\u636e\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "VG-DETR\u91c7\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u6316\u6398\u7b56\u7565\u548c\u53cc\u7ea7VFM\u5f15\u5bfc\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u7279\u5f81\u56fe\u76f8\u4f3c\u6027\u5339\u914d\u589e\u5f3a\u7279\u5f81\u8868\u793a\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eVG-DETR\u5728\u6e90\u81ea\u7531\u9065\u611f\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f2a\u6807\u7b7e\u8d28\u91cf\u548c\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "conclusion": "VG-DETR\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFM\uff09\u548c\u534a\u76d1\u7763\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6e90\u81ea\u7531\u76ee\u6807\u68c0\u6d4b\u5728\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u4f2a\u6807\u7b7e\u566a\u58f0\u548c\u7279\u5f81\u63d0\u53d6\u65b9\u9762\u3002"}}
{"id": "2508.11446", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11446", "abs": "https://arxiv.org/abs/2508.11446", "authors": ["Daniel Airinei", "Elena Burceanu", "Marius Leordeanu"], "title": "Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation", "comment": "Accepted at the International Conference on Computer Vision Workshops\n  2025", "summary": "Indoor navigation is a difficult task, as it generally comes with poor GPS\naccess, forcing solutions to rely on other sources of information. While\nsignificant progress continues to be made in this area, deployment to\nproduction applications is still lacking, given the complexity and additional\nrequirements of current solutions. Here, we introduce an efficient, real-time\nand easily deployable deep learning approach, based on visual input only, that\ncan predict the direction towards a target from images captured by a mobile\ndevice. Our technical approach, based on a novel graph-based path generation\nmethod, combined with explainable data augmentation and curriculum learning,\nincludes contributions that make the process of data collection, annotation and\ntraining, as automatic as possible, efficient and robust. On the practical\nside, we introduce a novel largescale dataset, with video footage inside a\nrelatively large shopping mall, in which each frame is annotated with the\ncorrect next direction towards different specific target destinations.\nDifferent from current methods, ours relies solely on vision, avoiding the need\nof special sensors, additional markers placed along the path, knowledge of the\nscene map or internet access. We also created an easy to use application for\nAndroid, which we plan to make publicly available. We make all our data and\ncode available along with visual demos on our project site", "AI": {"tldr": "\u63d0\u51fa\u4ec5\u4f9d\u8d56\u89c6\u89c9\u7684\u9ad8\u6548\u5ba4\u5185\u5bfc\u822a\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u542b\u6570\u636e\u96c6\u548cAndroid\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u5ba4\u5185\u5bfc\u822a\u4e2dGPS\u4fe1\u53f7\u5f31\u3001\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u590d\u6742\u4e14\u96be\u4ee5\u90e8\u7f72\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u65b0\u9896\u7684\u56fe\u57fa\u8def\u5f84\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u53ef\u89e3\u91ca\u7684\u6570\u636e\u589e\u5f3a\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u4f7f\u6570\u636e\u6536\u96c6\u3001\u6807\u6ce8\u548c\u8bad\u7ec3\u8fc7\u7a0b\u5c3d\u53ef\u80fd\u81ea\u52a8\u5316\u3001\u9ad8\u6548\u4e14\u9c81\u68d2\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u578b\u8d2d\u7269\u4e2d\u5fc3\u89c6\u9891\u6570\u636e\u96c6\uff0c\u6bcf\u5e27\u6807\u6ce8\u4e86\u671d\u5411\u4e0d\u540c\u76ee\u6807\u7684\u4e0b\u4e00\u4e2a\u6b63\u786e\u65b9\u5411\uff0c\u5e76\u5f00\u53d1\u4e86\u6613\u4e8e\u4f7f\u7528\u7684Android\u5e94\u7528\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f9d\u8d56\u89c6\u89c9\u8f93\u5165\u7684\u9ad8\u6548\u3001\u5b9e\u65f6\u4e14\u6613\u4e8e\u90e8\u7f72\u7684\u6df1\u5ea6\u5b66\u4e60\u5ba4\u5185\u5bfc\u822a\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u7279\u6b8a\u4f20\u611f\u5668\u3001\u6807\u8bb0\u6216\u573a\u666f\u5730\u56fe\u7684\u4f9d\u8d56\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5f00\u7684\u6570\u636e\u96c6\u548cAndroid\u5e94\u7528\u3002"}}
{"id": "2508.11499", "categories": ["cs.CV", "cs.AI", "cs.DL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11499", "abs": "https://arxiv.org/abs/2508.11499", "authors": ["Erez Meoded"], "title": "Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models", "comment": null, "summary": "Historical handwritten text recognition (HTR) is essential for unlocking the\ncultural and scholarly value of archival documents, yet digitization is often\nhindered by scarce transcriptions, linguistic variation, and highly diverse\nhandwriting styles. In this study, we apply TrOCR, a state-of-the-art\ntransformer-based HTR model, to 16th-century Latin manuscripts authored by\nRudolf Gwalther. We investigate targeted image preprocessing and a broad suite\nof data augmentation techniques, introducing four novel augmentation methods\ndesigned specifically for historical handwriting characteristics. We also\nevaluate ensemble learning approaches to leverage the complementary strengths\nof augmentation-trained models. On the Gwalther dataset, our best single-model\naugmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a\ntop-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative\nimprovement over the best reported TrOCR_BASE result and a 42% improvement over\nthe previous state of the art. These results highlight the impact of\ndomain-specific augmentations and ensemble strategies in advancing HTR\nperformance for historical manuscripts.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7TrOCR\u6a21\u578b\u548c\u65b0\u578b\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e8616\u4e16\u7eaa\u62c9\u4e01\u624b\u7a3f\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u9886\u57df\u7279\u5b9a\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u89e3\u51b3\u5386\u53f2\u624b\u5199\u6587\u672c\u8bc6\u522b\u4e2d\u7684\u8f6c\u5f55\u7a00\u7f3a\u3001\u8bed\u8a00\u53d8\u5f02\u548c\u624b\u5199\u98ce\u683c\u591a\u6837\u6027\u95ee\u9898\uff0c\u4ee5\u91ca\u653e\u6863\u6848\u6587\u4ef6\u7684\u6587\u5316\u548c\u5b66\u672f\u4ef7\u503c\u3002", "method": "\u5e94\u7528TrOCR\u6a21\u578b\uff0c\u7ed3\u5408\u76ee\u6807\u56fe\u50cf\u9884\u5904\u7406\u548c\u591a\u79cd\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u5305\u62ec\u56db\u79cd\u65b0\u65b9\u6cd5\uff09\uff0c\u5e76\u8bc4\u4f30\u96c6\u6210\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728Gwalther\u6570\u636e\u96c6\u4e0a\uff0c\u6700\u4f73\u5355\u6a21\u578b\u589e\u5f3a\uff08Elastic\uff09\u7684CER\u4e3a1.86\uff0c\u800ctop-5\u6295\u7968\u96c6\u6210\u7684CER\u4e3a1.60\uff0c\u5206\u522b\u6bd4\u6700\u4f73TrOCR_BASE\u7ed3\u679c\u548c\u5148\u524d\u6700\u4f18\u7ed3\u679c\u63d0\u5347\u4e8650%\u548c42%\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u9488\u5bf9\u5386\u53f2\u624b\u5199\u7279\u6027\u7684\u9886\u57df\u7279\u5b9a\u6570\u636e\u589e\u5f3a\u548c\u96c6\u6210\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86HTR\u6027\u80fd\uff0c\u4e3a\u5386\u53f2\u624b\u7a3f\u7684\u6570\u5b57\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.11173", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11173", "abs": "https://arxiv.org/abs/2508.11173", "authors": ["Ruobing Jiang", "Yang Liu", "Haobing Liu", "Yanwei Yu", "Chunyang Wang"], "title": "Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery", "comment": "Accepted by CIKM 2025. 10 pages, 5 figures,", "summary": "Continuous category discovery (CCD) aims to automatically discover novel\ncategories in continuously arriving unlabeled data. This is a challenging\nproblem considering that there is no number of categories and labels in the\nnewly arrived data, while also needing to mitigate catastrophic forgetting.\nMost CCD methods cannot handle the contradiction between novel class discovery\nand classification well. They are also prone to accumulate errors in the\nprocess of gradually discovering novel classes. Moreover, most of them use\nknowledge distillation and data replay to prevent forgetting, occupying more\nstorage space. To address these limitations, we propose Independence-based\nDiversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes\nindependent enrichment of diversity module, joint discovery of novelty module,\nand continuous increment by orthogonality module. In independent enrichment,\nthe backbone is trained separately using contrastive loss to avoid it focusing\nonly on features for classification. Joint discovery transforms multi-stage\nnovel class discovery into single-stage, reducing error accumulation impact.\nContinuous increment by orthogonality module generates mutually orthogonal\nprototypes for classification and prevents forgetting with lower space overhead\nvia representative representation replay. Experimental results show that on\nchallenging fine-grained datasets, our method outperforms the state-of-the-art\nmethods.", "AI": {"tldr": "IDOD\u901a\u8fc7\u72ec\u7acb\u591a\u6837\u6027\u589e\u5f3a\u3001\u8054\u5408\u65b0\u9896\u6027\u53d1\u73b0\u548c\u6b63\u4ea4\u6027\u589e\u91cf\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u6301\u7eed\u7c7b\u522b\u53d1\u73b0\u4e2d\u7684\u95ee\u9898\uff0c\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6301\u7eed\u7c7b\u522b\u53d1\u73b0\uff08CCD\uff09\u9762\u4e34\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u65b0\u7c7b\u522b\u6570\u91cf\u672a\u77e5\u3001\u707e\u96be\u6027\u9057\u5fd8\u53ca\u9519\u8bef\u79ef\u7d2f\u7b49\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5e73\u8861\u65b0\u7c7b\u522b\u53d1\u73b0\u4e0e\u5206\u7c7b\uff0c\u4e14\u5b58\u50a8\u5f00\u9500\u5927\u3002", "method": "IDOD\u4e3b\u8981\u5305\u62ec\u72ec\u7acb\u591a\u6837\u6027\u589e\u5f3a\u6a21\u5757\u3001\u8054\u5408\u65b0\u9896\u6027\u53d1\u73b0\u6a21\u5757\u548c\u6b63\u4ea4\u6027\u589e\u91cf\u6a21\u5757\uff0c\u5206\u522b\u7528\u4e8e\u907f\u514d\u5206\u7c7b\u7279\u5f81\u8fc7\u5ea6\u96c6\u4e2d\u3001\u51cf\u5c11\u9519\u8bef\u79ef\u7d2f\u5f71\u54cd\u4ee5\u53ca\u901a\u8fc7\u6b63\u4ea4\u539f\u578b\u964d\u4f4e\u5b58\u50a8\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cIDOD\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "IDOD\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u901a\u8fc7\u72ec\u7acb\u591a\u6837\u6027\u589e\u5f3a\u3001\u8054\u5408\u65b0\u9896\u6027\u53d1\u73b0\u548c\u6b63\u4ea4\u6027\u589e\u91cf\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u7c7b\u522b\u53d1\u73b0\u4e2d\u7684\u77db\u76fe\u4e0e\u9519\u8bef\u79ef\u7d2f\u95ee\u9898\u3002"}}
{"id": "2508.11176", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11176", "abs": "https://arxiv.org/abs/2508.11176", "authors": ["Yumiao Zhao", "Bo Jiang", "Yuhe Ding", "Xiao Wang", "Jin Tang", "Bin Luo"], "title": "Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning", "comment": null, "summary": "Adapter-based approaches have garnered attention for fine-tuning pre-trained\nVision-Language Models (VLMs) on few-shot classification tasks. These methods\nstrive to develop a lightweight module that better aligns visual and (category)\ntextual representations, thereby enhancing performance on downstream few-shot\nlearning tasks. However, existing adapters generally learn/align (category)\ntextual-visual modalities via explicit spatial proximity in the underlying\nembedding space, which i) fails to capture the inherent one-to-many\nassociations between categories and image samples and ii) struggles to\nestablish accurate associations between the unknown categories and images. To\naddress these issues, inspired by recent works on hyperbolic learning, we\ndevelop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs\non downstream few-shot classification tasks. The core of LatHAdapter is to\nexploit the latent semantic hierarchy of downstream training data and employ it\nto provide richer, fine-grained guidance for the adapter learning process.\nSpecifically, LatHAdapter first introduces some learnable `attribute' prompts\nas the bridge to align categories and images. Then, it projects the categories,\nattribute prompts, and images within each batch in a hyperbolic space, and\nemploys hierarchical regularization to learn the latent semantic hierarchy of\nthem, thereby fully modeling the inherent one-to-many associations among\ncategories, learnable attributes, and image samples. Extensive experiments on\nfour challenging few-shot tasks show that the proposed LatHAdapter consistently\noutperforms many other fine-tuning approaches, particularly in adapting known\nclasses and generalizing to unknown classes.", "AI": {"tldr": "LatHAdapter\u662f\u4e00\u79cd\u65b0\u578b\u9002\u914d\u5668\uff0c\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\u548c\u6f5c\u5728\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\u4f18\u5316\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5c11\u6837\u672c\u5206\u7c7b\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9002\u914d\u5668\u65b9\u6cd5\u901a\u8fc7\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u663e\u5f0f\u7a7a\u95f4\u90bb\u8fd1\u6027\u5bf9\u9f50\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u672a\u80fd\u6355\u6349\u7c7b\u522b\u4e0e\u56fe\u50cf\u6837\u672c\u4e4b\u95f4\u7684\u4e00\u5bf9\u591a\u5173\u8054\uff0c\u4e14\u5728\u672a\u77e5\u7c7b\u522b\u548c\u56fe\u50cf\u4e4b\u95f4\u5efa\u7acb\u51c6\u786e\u5173\u8054\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "LatHAdapter\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u2018\u5c5e\u6027\u2019\u63d0\u793a\u4f5c\u4e3a\u5bf9\u9f50\u7c7b\u522b\u548c\u56fe\u50cf\u7684\u6865\u6881\uff0c\u5e76\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u6295\u5f71\u7c7b\u522b\u3001\u5c5e\u6027\u63d0\u793a\u548c\u56fe\u50cf\uff0c\u5229\u7528\u5c42\u6b21\u6b63\u5219\u5316\u5b66\u4e60\u5b83\u4eec\u7684\u6f5c\u5728\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u5c11\u6837\u672c\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLatHAdapter\u5728\u6027\u80fd\u4e0a\u6301\u7eed\u4f18\u4e8e\u8bb8\u591a\u5176\u4ed6\u5fae\u8c03\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u9002\u5e94\u5df2\u77e5\u7c7b\u522b\u548c\u6cdb\u5316\u5230\u672a\u77e5\u7c7b\u522b\u65b9\u9762\u3002", "conclusion": "LatHAdapter\u901a\u8fc7\u5229\u7528\u4e0b\u6e38\u8bad\u7ec3\u6570\u636e\u7684\u6f5c\u5728\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\uff0c\u4e3a\u9002\u914d\u5668\u5b66\u4e60\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u3001\u66f4\u7ec6\u7c92\u5ea6\u7684\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u5df2\u77e5\u7c7b\u522b\u9002\u5e94\u548c\u672a\u77e5\u7c7b\u522b\u6cdb\u5316\u65b9\u9762\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11183", "abs": "https://arxiv.org/abs/2508.11183", "authors": ["Zhenghao Chen", "Zicong Chen", "Lei Liu", "Yiming Wu", "Dong Xu"], "title": "Versatile Video Tokenization with Generative 2D Gaussian Splatting", "comment": null, "summary": "Video tokenization procedure is critical for a wide range of video processing\ntasks. Most existing approaches directly transform video into fixed-grid and\npatch-wise tokens, which exhibit limited versatility. Spatially, uniformly\nallocating a fixed number of tokens often leads to over-encoding in\nlow-information regions. Temporally, reducing redundancy remains challenging\nwithout explicitly distinguishing between static and dynamic content. In this\nwork, we propose the Gaussian Video Transformer (GVT), a versatile video\ntokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We\nfirst extract latent rigid features from a video clip and represent them with a\nset of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian\nEmbedding (STGE) mechanism in a feed-forward manner. Such generative 2D\nGaussians not only enhance spatial adaptability by assigning higher (resp.,\nlower) rendering weights to regions with higher (resp., lower) information\ncontent during rasterization, but also improve generalization by avoiding\nper-video optimization.To enhance the temporal versatility, we introduce a\nGaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into\nstatic and dynamic sets, which explicitly model static content shared across\ndifferent time-steps and dynamic content specific to each time-step, enabling a\ncompact representation.We primarily evaluate GVT on the video reconstruction,\nwhile also assessing its performance on action recognition and compression\nusing the UCF101, Kinetics, and DAVIS datasets. Extensive experiments\ndemonstrate that GVT achieves a state-of-the-art video reconstruction quality,\noutperforms the baseline MAGVIT-v2 in action recognition, and delivers\ncomparable compression performance.", "AI": {"tldr": "GVT\u901a\u8fc7\u751f\u6210\u5f0f2D\u9ad8\u65af\u548c\u65f6\u7a7a\u5212\u5206\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u6807\u8bb0\u5316\u7684\u65f6\u7a7a\u9002\u5e94\u6027\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6807\u8bb0\u5316\u65b9\u6cd5\u591a\u4e3a\u56fa\u5b9a\u7f51\u683c\u548c\u8865\u4e01\u65b9\u5f0f\uff0c\u7a7a\u95f4\u4e0a\u5bf9\u4f4e\u4fe1\u606f\u533a\u57df\u8fc7\u5ea6\u7f16\u7801\uff0c\u65f6\u95f4\u4e0a\u96be\u4ee5\u6709\u6548\u533a\u5206\u9759\u6001\u548c\u52a8\u6001\u5185\u5bb9\u3002GVT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "GVT\u91c7\u7528\u751f\u6210\u5f0f2D\u9ad8\u65af\u5e73\u94fa\u7b56\u7565\uff082DGS\uff09\u548c\u65f6\u7a7a\u9ad8\u65af\u5d4c\u5165\u673a\u5236\uff08STGE\uff09\uff0c\u901a\u8fc7\u9ad8\u65af\u96c6\u5212\u5206\uff08GSP\uff09\u7b56\u7565\u5c062D\u9ad8\u65af\u5206\u4e3a\u9759\u6001\u548c\u52a8\u6001\u96c6\uff0c\u4ee5\u63d0\u5347\u65f6\u7a7a\u9002\u5e94\u6027\u3002", "result": "GVT\u5728UCF101\u3001Kinetics\u548cDAVIS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u5728\u89c6\u9891\u91cd\u5efa\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u538b\u7f29\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GVT\u5728\u89c6\u9891\u91cd\u5efa\u8d28\u91cf\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u5728\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u57fa\u7ebfMAGVIT-v2\uff0c\u5e76\u5728\u538b\u7f29\u6027\u80fd\u4e0a\u8868\u73b0\u76f8\u5f53\u3002"}}
{"id": "2508.11616", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11616", "abs": "https://arxiv.org/abs/2508.11616", "authors": ["Oscar Ma\u00f1as", "Pierluca D'Oro", "Koustuv Sinha", "Adriana Romero-Soriano", "Michal Drozdzal", "Aishwarya Agrawal"], "title": "Controlling Multimodal LLMs via Reward-guided Decoding", "comment": "Published at ICCV 2025", "summary": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u5f15\u5bfcMLLMs\u89e3\u7801\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u63a5\u5730\u7684\u53ef\u63a7\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u9002\u5e94\u591a\u6837\u5316\u7684\u7528\u6237\u9700\u6c42\uff0c\u9700\u8981\u63d0\u5347MLLMs\u7684\u53ef\u63a7\u6027\u548c\u89c6\u89c9\u63a5\u5730\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u4e24\u4e2a\u72ec\u7acb\u7684\u5956\u52b1\u6a21\u578b\uff0c\u5206\u522b\u63a7\u5236\u5bf9\u8c61\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u5e76\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u6574\u5956\u52b1\u51fd\u6570\u7684\u6743\u91cd\u548c\u641c\u7d22\u8303\u56f4\u3002", "result": "\u5728\u6807\u51c6\u5bf9\u8c61\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u53ef\u63a7\u6027\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u5e7b\u89c9\u7f13\u89e3\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5956\u52b1\u5f15\u5bfc\u89e3\u7801\u663e\u8457\u63d0\u5347\u4e86MLLMs\u7684\u53ef\u63a7\u6027\uff0c\u5e76\u5728\u89c6\u89c9\u63a5\u5730\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.11185", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11185", "abs": "https://arxiv.org/abs/2508.11185", "authors": ["Abhinav Kumar", "Yuliang Guo", "Zhihao Zhang", "Xinyu Huang", "Liu Ren", "Xiaoming Liu"], "title": "CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector", "comment": "ICCV 2025", "summary": "Monocular 3D object detectors, while effective on data from one ego camera\nheight, struggle with unseen or out-of-distribution camera heights. Existing\nmethods often rely on Plucker embeddings, image transformations or data\naugmentation. This paper takes a step towards this understudied problem by\nfirst investigating the impact of camera height variations on state-of-the-art\n(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset\nwith multiple camera heights, we observe that depth estimation is a primary\nfactor influencing performance under height variations. We mathematically prove\nand also empirically observe consistent negative and positive trends in mean\ndepth error of regressed and ground-based depth models, respectively, under\ncamera height changes. To mitigate this, we propose Camera Height Robust\nMonocular 3D Detector (CHARM3R), which averages both depth estimates within the\nmodel. CHARM3R improves generalization to unseen camera heights by more than\n$45\\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at\nhttps://github.com/abhi1kumar/CHARM3R", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCHARM3R\uff0c\u901a\u8fc7\u6574\u5408\u4e24\u79cd\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u68c0\u6d4b\u5668\u5728\u76f8\u673a\u9ad8\u5ea6\u53d8\u5316\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u76ee3D\u68c0\u6d4b\u5668\u5728\u76f8\u673a\u9ad8\u5ea6\u53d8\u5316\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u4e00\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u4e0d\u8db3\uff0c\u63d0\u5347\u6a21\u578b\u5728\u672a\u89c1\u76f8\u673a\u9ad8\u5ea6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u9996\u5148\u7cfb\u7edf\u5206\u6790\u4e86\u76f8\u673a\u9ad8\u5ea6\u53d8\u5316\u5bf9\u73b0\u6709\u5355\u76ee3D\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6df1\u5ea6\u4f30\u8ba1\u662f\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\u3002\u968f\u540e\uff0c\u4ed6\u4eec\u63d0\u51faCHARM3R\u6a21\u578b\uff0c\u901a\u8fc7\u5e73\u5747\u56de\u5f52\u548c\u57fa\u4e8e\u5730\u9762\u7684\u6df1\u5ea6\u4f30\u8ba1\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "result": "CHARM3R\u5728\u672a\u89c1\u76f8\u673a\u9ad8\u5ea6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u5347\u4e8645%\u4ee5\u4e0a\uff0c\u5e76\u5728CARLA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "CHARM3R\u901a\u8fc7\u6574\u5408\u56de\u5f52\u548c\u57fa\u4e8e\u5730\u9762\u7684\u6df1\u5ea6\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u68c0\u6d4b\u5668\u5728\u672a\u89c1\u76f8\u673a\u9ad8\u5ea6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728CARLA\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11628", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11628", "abs": "https://arxiv.org/abs/2508.11628", "authors": ["Qiang Li", "Shansong Wang", "Mingzhe Hu", "Mojtaba Safari", "Zachary Eidex", "Xiaofeng Yang"], "title": "Is ChatGPT-5 Ready for Mammogram VQA?", "comment": null, "summary": "Mammogram visual question answering (VQA) integrates image interpretation\nwith clinical reasoning and has potential to support breast cancer screening.\nWe systematically evaluated the GPT-5 family and GPT-4o model on four public\nmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,\nabnormality detection, and malignancy classification tasks. GPT-5 consistently\nwas the best performing model but lagged behind both human experts and\ndomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores\namong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),\ncalcification (63.5%), and malignancy (52.8%) classification. On InBreast, it\nattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%\nmalignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection\nand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS\naccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared\nwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and\nspecificity (52.3%). While GPT-5 exhibits promising capabilities for screening\ntasks, its performance remains insufficient for high-stakes clinical imaging\napplications without targeted domain adaptation and optimization. However, the\ntremendous improvements in performance from GPT-4o to GPT-5 show a promising\ntrend in the potential for general large language models (LLMs) to assist with\nmammography VQA tasks.", "AI": {"tldr": "GPT-5\u5728\u4e73\u817aX\u5149\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eGPT-4o\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u4ee5\u5339\u914d\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u3002", "motivation": "\u63a2\u7d22\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e73\u817aX\u5149\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u652f\u6301\u4e73\u817a\u764c\u7b5b\u67e5\u3002", "method": "\u5728\u56db\u4e2a\u516c\u5f00\u4e73\u817aX\u5149\u6570\u636e\u96c6\uff08EMBED\u3001InBreast\u3001CMMD\u3001CBIS-DDSM\uff09\u4e0a\u7cfb\u7edf\u8bc4\u4f30GPT-5\u548cGPT-4o\u6a21\u578b\uff0c\u6db5\u76d6BI-RADS\u8bc4\u4f30\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u6076\u6027\u5206\u7c7b\u4efb\u52a1\u3002", "result": "GPT-5\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u843d\u540e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u548c\u9886\u57df\u4e13\u7528\u6a21\u578b\u3002\u4f8b\u5982\uff0c\u5728EMBED\u6570\u636e\u96c6\u4e2d\uff0c\u5176\u5bc6\u5ea6\u5206\u7c7b\u51c6\u786e\u7387\u4e3a56.8%\uff0c\u6076\u6027\u5206\u7c7b\u4e3a52.8%\u3002", "conclusion": "GPT-5\u5728\u4e73\u817aX\u5149\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u6027\u80fd\u4ecd\u4e0d\u8db3\u4ee5\u652f\u6301\u9ad8\u98ce\u9669\u7684\u4e34\u5e8a\u5f71\u50cf\u5e94\u7528\uff0c\u9700\u8fdb\u4e00\u6b65\u9886\u57df\u9002\u5e94\u548c\u4f18\u5316\u3002"}}
{"id": "2508.11192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11192", "abs": "https://arxiv.org/abs/2508.11192", "authors": ["Lavisha Aggarwal", "Vikas Bahirwani", "Lin Li", "Andrea Colaco"], "title": "Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark", "comment": null, "summary": "Many everyday tasks ranging from fixing appliances, cooking recipes to car\nmaintenance require expert knowledge, especially when tasks are complex and\nmulti-step. Despite growing interest in AI agents, there is a scarcity of\ndialogue-video datasets grounded for real world task assistance. In this paper,\nwe propose a simple yet effective approach that transforms single-person\ninstructional videos into task-guidance two-person dialogues, aligned with fine\ngrained steps and video-clips. Our fully automatic approach, powered by large\nlanguage models, offers an efficient alternative to the substantial cost and\neffort required for human-assisted data collection. Using this technique, we\nbuild HowToDIV, a large-scale dataset containing 507 conversations, 6636\nquestion-answer pairs and 24 hours of videoclips across diverse tasks in\ncooking, mechanics, and planting. Each session includes multi-turn conversation\nwhere an expert teaches a novice user how to perform a task step by step, while\nobserving user's surrounding through a camera and microphone equipped wearable\ndevice. We establish the baseline benchmark performance on HowToDIV dataset\nthrough Gemma-3 model for future research on this new task of dialogues for\nprocedural-task assistance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u8f6c\u6362\u5355\u4eba\u6559\u5b66\u89c6\u9891\u4e3a\u4efb\u52a1\u6307\u5bfc\u5bf9\u8bdd\u7684\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86HowToDIV\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u8f85\u52a9\u4e2d\u5bf9\u8bdd-\u89c6\u9891\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u95ee\u9898\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5c06\u5355\u4eba\u6559\u5b66\u89c6\u9891\u8f6c\u6362\u4e3a\u4e24\u4eba\u5bf9\u8bdd\uff0c\u5e76\u4e0e\u89c6\u9891\u7247\u6bb5\u5bf9\u9f50\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b507\u4e2a\u5bf9\u8bdd\u30016636\u4e2a\u95ee\u7b54\u5bf9\u548c24\u5c0f\u65f6\u89c6\u9891\u7247\u6bb5\u7684HowToDIV\u6570\u636e\u96c6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5355\u4eba\u6559\u5b66\u89c6\u9891\u8f6c\u5316\u4e3a\u4efb\u52a1\u6307\u5bfc\u5bf9\u8bdd\u7684\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86HowToDIV\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2508.11196", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11196", "abs": "https://arxiv.org/abs/2508.11196", "authors": ["Jiajin Guan", "Haibo Mei", "Bonan Zhang", "Dan Liu", "Yuanshuang Fu", "Yue Zhang"], "title": "UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning", "comment": null, "summary": "Recent advances in vision-language models (VLMs) have demonstrated strong\ngeneralization in natural image tasks. However, their performance often\ndegrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features\nhigh resolution, complex spatial semantics, and strict real-time constraints.\nThese challenges limit the applicability of general-purpose VLMs to structured\naerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a\nlightweight VLM explicitly designed for aerial visual reasoning. It is trained\nusing a hybrid method that combines supervised fine-tuning (SFT) and\nmulti-stage reinforcement learning (RL). We leverage the group relative policy\noptimization (GRPO) algorithm to promote structured and interpretable reasoning\nthrough rule-guided rewards and intra-group policy alignment. To support model\ntraining and evaluation, we introduce a high-resolution visual question\nanswering dataset named HRVQA-VL, which consists of 50,019 annotated samples\ncovering eight UAV-relevant reasoning tasks, including object counting,\ntransportation recognition, and spatial scene inference. Experimental results\nshow that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the\nQwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which\nis 36x larger, on multiple tasks. Ablation studies reveal that while SFT\nimproves semantic alignment, it may reduce reasoning diversity in mathematical\ntasks. GRPO-based RL compensates for this limitation by enhancing logical\nflexibility and the robustness of inference. Additionally, UAV-VL-R1 requires\nonly 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with\nINT8, supporting real-time deployment on resource-constrained UAV platforms.", "AI": {"tldr": "UAV-VL-R1\u662f\u4e00\u79cd\u4e13\u4e3a\u65e0\u4eba\u673a\u822a\u62cd\u8bbe\u8ba1\u7684\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u652f\u6301\u5b9e\u65f6\u90e8\u7f72\u3002", "motivation": "\u5f53\u524d\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u4eba\u673a\u822a\u62cd\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u9ad8\u5206\u8fa8\u7387\u3001\u590d\u6742\u7a7a\u95f4\u8bed\u4e49\u548c\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "method": "\u91c7\u7528\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u5229\u7528GRPO\u7b97\u6cd5\u63d0\u5347\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u3002", "result": "UAV-VL-R1\u5728\u96f6\u6837\u672c\u51c6\u786e\u7387\u4e0a\u6bd4\u57fa\u7ebf\u6a21\u578b\u9ad848.17%\uff0c\u751a\u81f3\u4f18\u4e8e\u517636\u500d\u5927\u7684\u53d8\u4f53\uff0c\u4e14\u5185\u5b58\u5360\u7528\u4f4e\uff0c\u652f\u6301\u5b9e\u65f6\u90e8\u7f72\u3002", "conclusion": "UAV-VL-R1\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u4e3a\u65e0\u4eba\u673a\u822a\u62cd\u56fe\u50cf\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u822a\u62cd\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u96f6\u6837\u672c\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u652f\u6301\u5728\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\u5e73\u53f0\u4e0a\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2508.11212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11212", "abs": "https://arxiv.org/abs/2508.11212", "authors": ["Zhangjian Ji", "Wenjin Zhang", "Shaotong Qiao", "Kai Feng", "Yuhua Qian"], "title": "A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network", "comment": null, "summary": "Human pose estimation has been widely applied in the human-centric\nunderstanding and generation, but most existing state-of-the-art human pose\nestimation methods require heavy computational resources for accurate\npredictions. In order to obtain an accurate, robust yet lightweight human pose\nestimator, one feasible way is to transfer pose knowledge from a powerful\nteacher model to a less-parameterized student model by knowledge distillation.\nHowever, the traditional knowledge distillation framework does not fully\nexplore the contextual information among human joints. Thus, in this paper, we\npropose a novel coarse-to-fine two-stage knowledge distillation framework for\nhuman pose estimation. In the first-stage distillation, we introduce the human\njoints structure loss to mine the structural information among human joints so\nas to transfer high-level semantic knowledge from the teacher model to the\nstudent model. In the second-stage distillation, we utilize an Image-Guided\nProgressive Graph Convolutional Network (IGP-GCN) to refine the initial human\npose obtained from the first-stage distillation and supervise the training of\nthe IGP-GCN in the progressive way by the final output pose of teacher model.\nThe extensive experiments on the benchmark dataset: COCO keypoint and CrowdPose\ndatasets, show that our proposed method performs favorably against lots of the\nexisting state-of-the-art human pose estimation methods, especially for the\nmore complex CrowdPose dataset, the performance improvement of our model is\nmore significant.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u635f\u5931\u548c\u6e10\u8fdb\u5f0f\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u8f7b\u91cf\u7ea7\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u672a\u80fd\u5145\u5206\u5229\u7528\u4eba\u4f53\u5173\u8282\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u84b8\u998f\u5f15\u5165\u4eba\u4f53\u5173\u8282\u7ed3\u6784\u635f\u5931\u4ee5\u6316\u6398\u5173\u8282\u95f4\u7ed3\u6784\u4fe1\u606f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u84b8\u998f\u5229\u7528\u56fe\u50cf\u5f15\u5bfc\u7684\u6e10\u8fdb\u5f0f\u56fe\u5377\u79ef\u7f51\u7edc\uff08IGP-GCN\uff09\u7ec6\u5316\u521d\u59cb\u59ff\u6001\uff0c\u5e76\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u7684\u6700\u7ec8\u8f93\u51fa\u59ff\u6001\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\u3002", "result": "\u5728COCO\u5173\u952e\u70b9\u548cCrowdPose\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u5927\u591a\u6570\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u66f4\u590d\u6742\u7684CrowdPose\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u5347\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ece\u7c97\u5230\u7ec6\u7684\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8e\u8f7b\u91cf\u7ea7\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u901a\u8fc7\u5728\u4e24\u4e2a\u9636\u6bb5\u4e2d\u5206\u522b\u5229\u7528\u7ed3\u6784\u635f\u5931\u548c\u6e10\u8fdb\u5f0f\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11218", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11218", "abs": "https://arxiv.org/abs/2508.11218", "authors": ["Jialin Li", "Shuqi Wu", "Ning Wang"], "title": "A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving", "comment": null, "summary": "Re-Identification (ReID) is a critical technology in intelligent perception\nsystems, especially within autonomous driving, where onboard cameras must\nidentify pedestrians across views and time in real-time to support safe\nnavigation and trajectory prediction. However, the presence of uncertain or\nmissing input modalities--such as RGB, infrared, sketches, or textual\ndescriptions--poses significant challenges to conventional ReID approaches.\nWhile large-scale pre-trained models offer strong multimodal semantic modeling\ncapabilities, their computational overhead limits practical deployment in\nresource-constrained environments. To address these challenges, we propose a\nlightweight Uncertainty Modal Modeling (UMM) framework, which integrates a\nmultimodal token mapper, synthetic modality augmentation strategy, and\ncross-modal cue interactive learner. Together, these components enable unified\nfeature representation, mitigate the impact of missing modalities, and extract\ncomplementary information across different data types. Additionally, UMM\nleverages CLIP's vision-language alignment ability to fuse multimodal inputs\nefficiently without extensive finetuning. Experimental results demonstrate that\nUMM achieves strong robustness, generalization, and computational efficiency\nunder uncertain modality conditions, offering a scalable and practical solution\nfor pedestrian re-identification in autonomous driving scenarios.", "AI": {"tldr": "UMM\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e0d\u786e\u5b9a\u6027\u6a21\u6001\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4ee4\u724c\u6620\u5c04\u3001\u6a21\u6001\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u884c\u4eba\u91cd\u8bc6\u522b\u9762\u4e34\u8f93\u5165\u6a21\u6001\u4e0d\u786e\u5b9a\u6216\u7f3a\u5931\u7684\u6311\u6218\uff0c\u4e14\u73b0\u6709\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u65e0\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u56e0\u6b64\u63d0\u51faUMM\u6846\u67b6\u3002", "method": "UMM\u6846\u67b6\u6574\u5408\u4e86\u591a\u6a21\u6001\u4ee4\u724c\u6620\u5c04\u5668\u3001\u5408\u6210\u6a21\u6001\u589e\u5f3a\u7b56\u7565\u548c\u8de8\u6a21\u6001\u7ebf\u7d22\u4ea4\u4e92\u5b66\u4e60\u5668\uff0c\u5229\u7528CLIP\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u80fd\u529b\u9ad8\u6548\u878d\u5408\u591a\u6a21\u6001\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUMM\u5728\u4e0d\u786e\u5b9a\u6a21\u6001\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u5f3a\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "UMM\u6846\u67b6\u5728\u4e0d\u786e\u5b9a\u6a21\u6001\u6761\u4ef6\u4e0b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u884c\u4eba\u91cd\u8bc6\u522b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11255", "abs": "https://arxiv.org/abs/2508.11255", "authors": ["MengChao Wang", "Qiang Wang", "Fan Jiang", "Mu Xu"], "title": "FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation", "comment": "https://fantasy-amap.github.io/fantasy-talking2/", "summary": "Recent advances in audio-driven portrait animation have demonstrated\nimpressive capabilities. However, existing methods struggle to align with\nfine-grained human preferences across multiple dimensions, such as motion\nnaturalness, lip-sync accuracy, and visual quality. This is due to the\ndifficulty of optimizing among competing preference objectives, which often\nconflict with one another, and the scarcity of large-scale, high-quality\ndatasets with multidimensional preference annotations. To address these, we\nfirst introduce Talking-Critic, a multimodal reward model that learns\nhuman-aligned reward functions to quantify how well generated videos satisfy\nmultidimensional expectations. Leveraging this model, we curate Talking-NSQ, a\nlarge-scale multidimensional human preference dataset containing 410K\npreference pairs. Finally, we propose Timestep-Layer adaptive multi-expert\nPreference Optimization (TLPO), a novel framework for aligning diffusion-based\nportrait animation models with fine-grained, multidimensional preferences. TLPO\ndecouples preferences into specialized expert modules, which are then fused\nacross timesteps and network layers, enabling comprehensive, fine-grained\nenhancement across all dimensions without mutual interference. Experiments\ndemonstrate that Talking-Critic significantly outperforms existing methods in\naligning with human preference ratings. Meanwhile, TLPO achieves substantial\nimprovements over baseline models in lip-sync accuracy, motion naturalness, and\nvisual quality, exhibiting superior performance in both qualitative and\nquantitative evaluations. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking2/", "AI": {"tldr": "\u4ecb\u7ecd\u4e86Talking-Critic\u548cTLPO\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ef4\u5956\u52b1\u6a21\u578b\u548c\u4e13\u5bb6\u6a21\u5757\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u52a8\u753b\u7684\u591a\u7ef4\u504f\u597d\u5bf9\u9f50\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u7ef4\u4eba\u7c7b\u504f\u597d\uff08\u5982\u8fd0\u52a8\u81ea\u7136\u5ea6\u3001\u5507\u540c\u6b65\u51c6\u786e\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\uff09\u7684\u5bf9\u9f50\u4e0a\u5b58\u5728\u56f0\u96be\uff0c\u4e14\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684\u591a\u7ef4\u504f\u597d\u6807\u6ce8\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86TLPO\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u504f\u597d\u5206\u89e3\u4e3a\u4e13\u5bb6\u6a21\u5757\u5e76\u5728\u65f6\u95f4\u6b65\u548c\u7f51\u7edc\u5c42\u95f4\u878d\u5408\uff0c\u5b9e\u73b0\u591a\u7ef4\u5ea6\u7684\u7cbe\u7ec6\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTalking-Critic\u5728\u4eba\u7c7b\u504f\u597d\u8bc4\u5206\u5bf9\u9f50\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cTLPO\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "TLPO\u6846\u67b6\u5728\u5507\u540c\u6b65\u51c6\u786e\u5ea6\u3001\u8fd0\u52a8\u81ea\u7136\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5728\u591a\u7ef4\u504f\u597d\u5bf9\u9f50\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.11265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11265", "abs": "https://arxiv.org/abs/2508.11265", "authors": ["Pei He", "Lingling Li", "Licheng Jiao", "Ronghua Shang", "Fang Liu", "Shuang Wang", "Xu Liu", "Wenping Ma"], "title": "Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds", "comment": "to be published in International Conference on Computer Vision, ICCV\n  2025", "summary": "Domain generalization in 3D segmentation is a critical challenge in deploying\nmodels to unseen environments. Current methods mitigate the domain shift by\naugmenting the data distribution of point clouds. However, the model learns\nglobal geometric patterns in point clouds while ignoring the category-level\ndistribution and alignment. In this paper, a category-level geometry learning\nframework is proposed to explore the domain-invariant geometric features for\ndomain generalized 3D semantic segmentation. Specifically, Category-level\nGeometry Embedding (CGE) is proposed to perceive the fine-grained geometric\nproperties of point cloud features, which constructs the geometric properties\nof each class and couples geometric embedding to semantic learning. Secondly,\nGeometric Consistent Learning (GCL) is proposed to simulate the latent 3D\ndistribution and align the category-level geometric embeddings, allowing the\nmodel to focus on the geometric invariant information to improve\ngeneralization. Experimental results verify the effectiveness of the proposed\nmethod, which has very competitive segmentation accuracy compared with the\nstate-of-the-art domain generalized point cloud methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7c7b\u522b\u7ea7\u51e0\u4f55\u5b66\u4e60\u6846\u67b6\uff08CGE\u548cGCL\uff09\uff0c\u901a\u8fc7\u611f\u77e5\u7ec6\u7c92\u5ea6\u51e0\u4f55\u7279\u6027\u548c\u5bf9\u9f50\u7c7b\u522b\u7ea7\u51e0\u4f55\u5d4c\u5165\uff0c\u63d0\u5347\u4e863D\u8bed\u4e49\u5206\u5272\u7684\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u589e\u5f3a\u70b9\u4e91\u7684\u6570\u636e\u5206\u5e03\u6765\u7f13\u89e3\u9886\u57df\u504f\u79fb\uff0c\u4f46\u5ffd\u7565\u4e86\u7c7b\u522b\u7ea7\u5206\u5e03\u548c\u5bf9\u9f50\u95ee\u9898\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u9886\u57df\u4e0d\u53d8\u7684\u51e0\u4f55\u7279\u5f81\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u7c7b\u522b\u7ea7\u51e0\u4f55\u5d4c\u5165\uff08CGE\uff09\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u5b66\u4e60\uff08GCL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u611f\u77e5\u70b9\u4e91\u7279\u5f81\u7684\u7ec6\u7c92\u5ea6\u51e0\u4f55\u7279\u6027\u5e76\u6a21\u62df\u6f5c\u57283D\u5206\u5e03\uff0c\u5b9e\u73b0\u7c7b\u522b\u7ea7\u51e0\u4f55\u5d4c\u5165\u7684\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9886\u57df\u6cdb\u5316\u7684\u70b9\u4e91\u5206\u5272\u4efb\u52a1\u4e2d\u5177\u6709\u8f83\u9ad8\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u7c7b\u522b\u7ea7\u51e0\u4f55\u5b66\u4e60\u6846\u67b6\uff08CGE\u548cGCL\uff09\u5728\u9886\u57df\u6cdb\u5316\u76843D\u8bed\u4e49\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u7684\u5206\u5272\u7cbe\u5ea6\u3002"}}
{"id": "2508.11277", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11277", "abs": "https://arxiv.org/abs/2508.11277", "authors": ["Matthew Lyle Olson", "Musashi Hinck", "Neale Ratzlaff", "Changbai Li", "Phillip Howard", "Vasudev Lal", "Shao-Yen Tseng"], "title": "Probing the Representational Power of Sparse Autoencoders in Vision Models", "comment": "ICCV 2025 Findings", "summary": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u5728\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u63a7\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u89c6\u89c9\u5d4c\u5165\u6a21\u578b\u3001\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001LLM\u4e2d\u3002", "motivation": "\u5c3d\u7ba1\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u5e7f\u53d7\u6b22\u8fce\uff0c\u4f46\u5728\u89c6\u89c9\u9886\u57df\u7684\u7814\u7a76\u4ecd\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22SAE\u5728\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u57fa\u4e8e\u56fe\u50cf\u7684\u4efb\u52a1\uff0c\u5bf9\u4e09\u79cd\u89c6\u89c9\u6a21\u578b\u67b6\u6784\uff08\u89c6\u89c9\u5d4c\u5165\u6a21\u578b\u3001\u591a\u6a21\u6001LLM\u548c\u6269\u6563\u6a21\u578b\uff09\u4e2dSAE\u7684\u8868\u5f81\u80fd\u529b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSAE\u7279\u5f81\u5177\u6709\u8bed\u4e49\u610f\u4e49\uff0c\u80fd\u591f\u63d0\u5347\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u4e09\u79cd\u89c6\u89c9\u6a21\u578b\u67b6\u6784\u4e2d\u5b9e\u73b0\u53ef\u63a7\u751f\u6210\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u89c6\u89c9\u5d4c\u5165\u6a21\u578b\u4e2d\uff0cSAE\u7279\u5f81\u53ef\u7528\u4e8eOOD\u68c0\u6d4b\u5e76\u6062\u590d\u5e95\u5c42\u6a21\u578b\u7684\u672c\u4f53\u7ed3\u6784\uff1b\u5728\u6269\u6563\u6a21\u578b\u4e2d\uff0cSAE\u652f\u6301\u901a\u8fc7\u6587\u672c\u7f16\u7801\u5668\u64cd\u4f5c\u5b9e\u73b0\u8bed\u4e49\u5f15\u5bfc\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u6d41\u7a0b\u4ee5\u53d1\u73b0\u4eba\u7c7b\u53ef\u89e3\u91ca\u5c5e\u6027\uff1b\u5728\u591a\u6a21\u6001LLM\u4e2d\uff0cSAE\u7279\u5f81\u63ed\u793a\u4e86\u8de8\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u7684\u5171\u4eab\u8868\u793a\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u89c6\u89c9\u6a21\u578b\u4e2d\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7684\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5176\u5728\u89c6\u89c9\u9886\u57df\u4e2d\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u63a7\u6027\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.11282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11282", "abs": "https://arxiv.org/abs/2508.11282", "authors": ["Muzammil Khan", "Enzo Kerkhof", "Matteo Fusaglia", "Koert Kuhlmann", "Theo Ruers", "Fran\u00e7oise J. Siepel"], "title": "Unifying Scale-Aware Depth Prediction and Perceptual Priors for Monocular Endoscope Pose Estimation and Tissue Reconstruction", "comment": "18 pages, 8 figures, 3 Tables, submitted to IEEE Access for review", "summary": "Accurate endoscope pose estimation and 3D tissue surface reconstruction\nsignificantly enhances monocular minimally invasive surgical procedures by\nenabling accurate navigation and improved spatial awareness. However, monocular\nendoscope pose estimation and tissue reconstruction face persistent challenges,\nincluding depth ambiguity, physiological tissue deformation, inconsistent\nendoscope motion, limited texture fidelity, and a restricted field of view. To\novercome these limitations, a unified framework for monocular endoscopic tissue\nreconstruction that integrates scale-aware depth prediction with\ntemporally-constrained perceptual refinement is presented. This framework\nincorporates a novel MAPIS-Depth module, which leverages Depth Pro for robust\ninitialisation and Depth Anything for efficient per-frame depth prediction, in\nconjunction with L-BFGS-B optimisation, to generate pseudo-metric depth\nestimates. These estimates are temporally refined by computing pixel\ncorrespondences using RAFT and adaptively blending flow-warped frames based on\nLPIPS perceptual similarity, thereby reducing artefacts arising from\nphysiological tissue deformation and motion. To ensure accurate registration of\nthe synthesised pseudo-RGBD frames from MAPIS-Depth, a novel WEMA-RTDL module\nis integrated, optimising both rotation and translation. Finally, truncated\nsigned distance function-based volumetric fusion and marching cubes are applied\nto extract a comprehensive 3D surface mesh. Evaluations on HEVD and SCARED,\nwith ablation and comparative analyses, demonstrate the framework's robustness\nand superiority over state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u5c3a\u5ea6\u611f\u77e5\u6df1\u5ea6\u9884\u6d4b\u548c\u65f6\u95f4\u7ea6\u675f\u611f\u77e5\u7ec6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee\u5185\u7aa5\u955c\u624b\u672f\u4e2d\u7684\u7ec4\u7ec7\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u5355\u76ee\u5185\u7aa5\u955c\u59ff\u6001\u4f30\u8ba1\u548c\u7ec4\u7ec7\u91cd\u5efa\u9762\u4e34\u6df1\u5ea6\u6a21\u7cca\u3001\u7ec4\u7ec7\u53d8\u5f62\u3001\u8fd0\u52a8\u4e0d\u4e00\u81f4\u3001\u7eb9\u7406\u4fdd\u771f\u5ea6\u4f4e\u548c\u89c6\u91ce\u53d7\u9650\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u8be5\u6846\u67b6\u6574\u5408\u4e86\u5c3a\u5ea6\u611f\u77e5\u6df1\u5ea6\u9884\u6d4b\u4e0e\u65f6\u95f4\u7ea6\u675f\u7684\u611f\u77e5\u7ec6\u5316\uff0c\u5305\u62ecMAPIS-Depth\u6a21\u5757\uff08\u5229\u7528Depth Pro\u548cDepth Anything\u8fdb\u884c\u6df1\u5ea6\u9884\u6d4b\uff09\u548cL-BFGS-B\u4f18\u5316\uff0c\u4ee5\u53caWEMA-RTDL\u6a21\u5757\uff08\u4f18\u5316\u65cb\u8f6c\u548c\u5e73\u79fb\uff09\u3002\u6700\u540e\u901a\u8fc7\u622a\u65ad\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\u4f53\u79ef\u878d\u5408\u548c\u884c\u8fdb\u7acb\u65b9\u4f53\u63d0\u53d63D\u8868\u9762\u7f51\u683c\u3002", "result": "\u6846\u67b6\u5728HEVD\u548cSCARED\u6570\u636e\u96c6\u4e0a\u901a\u8fc7\u6d88\u878d\u548c\u5bf9\u6bd4\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728HEVD\u548cSCARED\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5176\u5728\u5355\u76ee\u5185\u7aa5\u955c\u7ec4\u7ec7\u91cd\u5efa\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.11284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11284", "abs": "https://arxiv.org/abs/2508.11284", "authors": ["Yilin Mi", "Qixin Yan", "Zheng-Peng Duan", "Chunle Guo", "Hubery Yin", "Hao Liu", "Chen Li", "Chongyi Li"], "title": "TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation", "comment": null, "summary": "With the advancement of generative models, facial image editing has made\nsignificant progress. However, achieving fine-grained age editing while\npreserving personal identity remains a challenging task.In this paper, we\npropose TimeMachine, a novel diffusion-based framework that achieves accurate\nage editing while keeping identity features unchanged. To enable fine-grained\nage editing, we inject high-precision age information into the multi-cross\nattention module, which explicitly separates age-related and identity-related\nfeatures. This design facilitates more accurate disentanglement of age\nattributes, thereby allowing precise and controllable manipulation of facial\naging.Furthermore, we propose an Age Classifier Guidance (ACG) module that\npredicts age directly in the latent space, instead of performing denoising\nimage reconstruction during training. By employing a lightweight module to\nincorporate age constraints, this design enhances age editing accuracy by\nmodest increasing training cost. Additionally, to address the lack of\nlarge-scale, high-quality facial age datasets, we construct a HFFA dataset\n(High-quality Fine-grained Facial-Age dataset) which contains one million\nhigh-resolution images labeled with identity and facial attributes.\nExperimental results demonstrate that TimeMachine achieves state-of-the-art\nperformance in fine-grained age editing while preserving identity consistency.", "AI": {"tldr": "TimeMachine\u662f\u4e00\u79cd\u65b0\u578b\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u548cACG\u6a21\u5757\u5b9e\u73b0\u7cbe\u786e\u5e74\u9f84\u7f16\u8f91\uff0c\u540c\u65f6\u6784\u5efaHFFA\u6570\u636e\u96c6\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u6a21\u578b\u5728\u9762\u90e8\u56fe\u50cf\u7f16\u8f91\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5982\u4f55\u5728\u4fdd\u6301\u4e2a\u4eba\u8eab\u4efd\u7684\u540c\u65f6\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5e74\u9f84\u7f16\u8f91\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6TimeMachine\uff0c\u901a\u8fc7\u5c06\u9ad8\u7cbe\u5ea6\u5e74\u9f84\u4fe1\u606f\u6ce8\u5165\u591a\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u660e\u786e\u5206\u79bb\u5e74\u9f84\u76f8\u5173\u548c\u8eab\u4efd\u76f8\u5173\u7279\u5f81\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86Age Classifier Guidance (ACG)\u6a21\u5757\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u76f4\u63a5\u9884\u6d4b\u5e74\u9f84\uff0c\u5e76\u6784\u5efa\u4e86HFFA\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTimeMachine\u5728\u7ec6\u7c92\u5ea6\u5e74\u9f84\u7f16\u8f91\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TimeMachine\u5728\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u5e74\u9f84\u7f16\u8f91\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002"}}
{"id": "2508.11301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11301", "abs": "https://arxiv.org/abs/2508.11301", "authors": ["Jiarong Li", "Imad Ali Shah", "Enda Ward", "Martin Glavin", "Edward Jones", "Brian Deegan"], "title": "Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study", "comment": "Submitted to IEEE ICVES, July, 2025", "summary": "Pedestrian segmentation in automotive perception systems faces critical\nsafety challenges due to metamerism in RGB imaging, where pedestrians and\nbackgrounds appear visually indistinguishable.. This study investigates the\npotential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation\nin urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We\ncompared standard RGB against two dimensionality-reduction approaches by\nconverting 128-channel HSI data into three-channel representations: Principal\nComponent Analysis (PCA) and optimal band selection using Contrast\nSignal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM).\nThree semantic segmentation models were evaluated: U-Net, DeepLabV3+, and\nSegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements\nof 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian\nsegmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25%\nF1-score improvements. These improved performance results from enhanced\nspectral discrimination of optimally selected HSI bands effectively reducing\nfalse positives. This study demonstrates robust pedestrian segmentation through\noptimal HSI band selection, showing significant potential for safety-critical\nautomotive applications.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7HSI\u548c\u4f18\u5316\u6ce2\u6bb5\u9009\u62e9\uff08CSNR-JMIM\uff09\u63d0\u5347\u4e86\u884c\u4eba\u5206\u5272\u6027\u80fd\uff0cIoU\u548cF1\u5206\u6570\u5747\u6709\u663e\u8457\u63d0\u9ad8\u3002", "motivation": "\u7531\u4e8eRGB\u6210\u50cf\u4e2d\u7684\u540c\u8272\u5f02\u8c31\u73b0\u8c61\uff0c\u884c\u4eba\u4e0e\u80cc\u666f\u5728\u89c6\u89c9\u4e0a\u96be\u4ee5\u533a\u5206\uff0c\u5bfc\u81f4\u6c7d\u8f66\u611f\u77e5\u7cfb\u7edf\u4e2d\u7684\u884c\u4eba\u5206\u5272\u9762\u4e34\u5173\u952e\u5b89\u5168\u6311\u6218\u3002", "method": "\u6bd4\u8f83\u4e86\u6807\u51c6RGB\u4e0e\u4e24\u79cd\u964d\u7ef4\u65b9\u6cd5\uff08PCA\u548cCSNR-JMIM\uff09\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff08U-Net\u3001DeepLabV3+\u548cSegFormer\uff09\u3002", "result": "CSNR-JMIM\u5728\u884c\u4eba\u5206\u5272\u4e2d\u5e73\u5747IoU\u63d0\u9ad8\u4e861.44%\uff0cF1\u5206\u6570\u63d0\u9ad8\u4e862.18%\uff1b\u5728\u9a91\u884c\u8005\u5206\u5272\u4e2d\u4e5f\u6709\u7c7b\u4f3c\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6700\u4f18HSI\u6ce2\u6bb5\u9009\u62e9\u5c55\u793a\u4e86\u7a33\u5065\u7684\u884c\u4eba\u5206\u5272\uff0c\u663e\u793a\u51fa\u5728\u5b89\u5168\u5173\u952e\u7684\u6c7d\u8f66\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.11313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11313", "abs": "https://arxiv.org/abs/2508.11313", "authors": ["Weijia Liu", "Jiuxin Cao", "Bo Miao", "Zhiheng Fu", "Xuelin Zhu", "Jiawei Ge", "Bo Liu", "Mehwish Nasim", "Ajmal Mian"], "title": "Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval", "comment": "Accepted by IJCAI 2025", "summary": "Current text-driven Video Moment Retrieval (VMR) methods encode all video\nclips, including irrelevant ones, disrupting multimodal alignment and hindering\noptimization. To this end, we propose a denoise-then-retrieve paradigm that\nexplicitly filters text-irrelevant clips from videos and then retrieves the\ntarget moment using purified multimodal representations. Following this\nparadigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising\nText-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF)\nmodules. TCD integrates cross-attention and structured state space blocks to\ndynamically identify noisy clips and produce a noise mask to purify multimodal\nvideo representations. TRF further distills a single query embedding from\npurified video representations and aligns it with the text embedding, serving\nas auxiliary supervision for denoising during training. Finally, we perform\nconditional retrieval using text embeddings on purified video representations\nfor accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that\nour approach surpasses state-of-the-art methods on all metrics. Furthermore,\nour denoise-then-retrieve paradigm is adaptable and can be seamlessly\nintegrated into advanced VMR models to boost performance.", "AI": {"tldr": "DRNet \u901a\u8fc7\u53bb\u566a-\u68c0\u7d22\u8303\u5f0f\uff08TCD + TRF \u6a21\u5757\uff09\u8fc7\u6ee4\u65e0\u5173\u89c6\u9891\u7247\u6bb5\u5e76\u51c0\u5316\u591a\u6a21\u6001\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u6027\u80fd\u4e14\u5177\u6709\u901a\u7528\u6027\u3002", "motivation": "\u73b0\u6709VMR\u65b9\u6cd5\u7f16\u7801\u6240\u6709\u89c6\u9891\u7247\u6bb5\uff08\u5305\u62ec\u65e0\u5173\u7247\u6bb5\uff09\u4f1a\u7834\u574f\u591a\u6a21\u6001\u5bf9\u9f50\u5e76\u963b\u788d\u4f18\u5316\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8fc7\u6ee4\u65e0\u5173\u7247\u6bb5\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86 Denoise-then-Retrieve Network (DRNet)\uff0c\u5305\u542b Text-Conditioned Denoising (TCD) \u548c Text-Reconstruction Feedback (TRF) \u6a21\u5757\uff0c\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u65e0\u5173\u89c6\u9891\u7247\u6bb5\u5e76\u51c0\u5316\u591a\u6a21\u6001\u8868\u793a\u6765\u5b9e\u73b0\u7cbe\u51c6\u68c0\u7d22\u3002", "result": "\u5728 Charades-STA \u548c QVHighlights \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDRNet \u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DRNet \u63d0\u51fa\u7684\u53bb\u566a-\u68c0\u7d22\u8303\u5f0f\u4e0d\u4ec5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u7684\u6027\u80fd\uff0c\u8fd8\u5c55\u793a\u4e86\u5176\u4e0e\u5176\u4ed6\u5148\u8fdbVMR\u6a21\u578b\u7684\u65e0\u7f1d\u96c6\u6210\u80fd\u529b\u3002"}}
{"id": "2508.11317", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.11317", "abs": "https://arxiv.org/abs/2508.11317", "authors": ["Yuchen Zhou", "Jiayu Tang", "Shuo Yang", "Xiaoyan Xiao", "Yuqin Dai", "Wenhao Yang", "Chao Gou", "Xiaobo Xia", "Tat-Seng Chua"], "title": "Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs), exemplified by CLIP, have emerged as\nfoundational for multimodal intelligence. However, their capacity for logical\nunderstanding remains significantly underexplored, resulting in critical\n''logical blindspots'' that limit their reliability in practical applications.\nTo systematically diagnose this, we introduce LogicBench, a comprehensive\nbenchmark with over 50,000 vision-language pairs across 9 logical categories\nand 4 diverse scenarios: images, videos, anomaly detection, and medical\ndiagnostics. Our evaluation reveals that existing VLMs, even the\nstate-of-the-art ones, fall at over 40 accuracy points below human performance,\nparticularly in challenging tasks like Causality and Conditionality,\nhighlighting their reliance on surface semantics over critical logical\nstructures. To bridge this gap, we propose LogicCLIP, a novel training\nframework designed to boost VLMs' logical sensitivity through advancements in\nboth data generation and optimization objectives. LogicCLIP utilizes\nlogic-aware data generation and a contrastive learning strategy that combines\ncoarse-grained alignment, a fine-grained multiple-choice objective, and a novel\nlogical structure-aware objective. Extensive experiments demonstrate\nLogicCLIP's substantial improvements in logical comprehension across all\nLogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP\nretains, and often surpasses, competitive performance on general\nvision-language benchmarks, demonstrating that the enhanced logical\nunderstanding does not come at the expense of general alignment. We believe\nthat LogicBench and LogicCLIP will be important resources for advancing VLM\nlogical capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faLogicBench\u57fa\u51c6\u548cLogicCLIP\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u8bca\u65ad\u5e76\u63d0\u5347VLM\u7684\u903b\u8f91\u7406\u89e3\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u4e0d\u5f71\u54cd\u901a\u7528\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLM\u5728\u903b\u8f91\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51faLogicCLIP\uff0c\u4e00\u79cd\u901a\u8fc7\u903b\u8f91\u611f\u77e5\u6570\u636e\u751f\u6210\u548c\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff08\u5305\u62ec\u7c97\u7c92\u5ea6\u5bf9\u9f50\u3001\u7ec6\u7c92\u5ea6\u591a\u76ee\u6807\u9009\u62e9\u548c\u903b\u8f91\u7ed3\u6784\u611f\u77e5\u76ee\u6807\uff09\u6765\u589e\u5f3aVLM\u903b\u8f91\u654f\u611f\u6027\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "result": "LogicCLIP\u5728\u6240\u6709LogicBench\u9886\u57df\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u903b\u8f91\u7406\u89e3\u80fd\u529b\uff0c\u4e14\u4e0d\u5f71\u54cd\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u7684\u6027\u80fd\u3002", "conclusion": "LogicBench\u548cLogicCLIP\u5c06\u6210\u4e3a\u63a8\u52a8VLM\u903b\u8f91\u80fd\u529b\u53d1\u5c55\u7684\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2508.11323", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11323", "abs": "https://arxiv.org/abs/2508.11323", "authors": ["Haonan Zhang", "Xinyao Wang", "Boxi Wu", "Tu Zheng", "Wang Yunhua", "Zheng Yang"], "title": "Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking", "comment": null, "summary": "3D multi-object tracking is a critical and challenging task in the field of\nautonomous driving. A common paradigm relies on modeling individual object\nmotion, e.g., Kalman filters, to predict trajectories. While effective in\nsimple scenarios, this approach often struggles in crowded environments or with\ninaccurate detections, as it overlooks the rich geometric relationships between\nobjects. This highlights the need to leverage spatial cues. However, existing\ngeometry-aware methods can be susceptible to interference from irrelevant\nobjects, leading to ambiguous features and incorrect associations. To address\nthis, we propose focusing on cue-consistency: identifying and matching stable\nspatial patterns over time. We introduce the Dynamic Scene Cue-Consistency\nTracker (DSC-Track) to implement this principle. Firstly, we design a unified\nspatiotemporal encoder using Point Pair Features (PPF) to learn discriminative\ntrajectory embeddings while suppressing interference. Secondly, our\ncue-consistency transformer module explicitly aligns consistent feature\nrepresentations between historical tracks and current detections. Finally, a\ndynamic update mechanism preserves salient spatiotemporal information for\nstable online tracking. Extensive experiments on the nuScenes and Waymo Open\nDatasets validate the effectiveness and robustness of our approach. On the\nnuScenes benchmark, for instance, our method achieves state-of-the-art\nperformance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,\nrespectively.", "AI": {"tldr": "DSC-Track\u901a\u8fc7\u52a8\u6001\u573a\u666f\u4e2d\u7684\u7a7a\u95f4\u7ebf\u7d22\u4e00\u81f4\u6027\u63d0\u53473D\u591a\u76ee\u6807\u8ddf\u8e2a\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u62e5\u6324\u73af\u5883\u6216\u68c0\u6d4b\u4e0d\u51c6\u786e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5ffd\u7565\u4e86\u7269\u4f53\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\u6216\u53d7\u5230\u65e0\u5173\u7269\u4f53\u7684\u5e72\u6270\u3002\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u8bc6\u522b\u548c\u5339\u914d\u7a33\u5b9a\u7684\u7a7a\u95f4\u6a21\u5f0f\uff08\u7ebf\u7d22\u4e00\u81f4\u6027\uff09\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1. \u4f7f\u7528\u70b9\u5bf9\u7279\u5f81\uff08PPF\uff09\u8bbe\u8ba1\u7edf\u4e00\u7684\u65f6\u7a7a\u7f16\u7801\u5668\uff0c\u5b66\u4e60\u5177\u6709\u533a\u5206\u6027\u7684\u8f68\u8ff9\u5d4c\u5165\u5e76\u6291\u5236\u5e72\u6270\uff1b2. \u5f15\u5165\u7ebf\u7d22\u4e00\u81f4\u6027\u53d8\u6362\u5668\u6a21\u5757\uff0c\u663e\u5f0f\u5bf9\u9f50\u5386\u53f2\u8f68\u8ff9\u4e0e\u5f53\u524d\u68c0\u6d4b\u7684\u7279\u5f81\u8868\u793a\uff1b3. \u52a8\u6001\u66f4\u65b0\u673a\u5236\u4fdd\u7559\u663e\u8457\u7684\u65f6\u7a7a\u4fe1\u606f\u4ee5\u652f\u6301\u7a33\u5b9a\u5728\u7ebf\u8ddf\u8e2a\u3002", "result": "\u5728nuScenes\u548cWaymo Open Datasets\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDSC-Track\u5177\u6709\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002\u4f8b\u5982\uff0c\u5728nuScenes\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u8fbe\u523073.2%\u548c70.3%\u7684AMOTA\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684DSC-Track\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u52a8\u6001\u573a\u666f\u4e2d\u7684\u7a7a\u95f4\u7ebf\u7d22\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u591a\u76ee\u6807\u8ddf\u8e2a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u62e5\u6324\u73af\u5883\u6216\u68c0\u6d4b\u4e0d\u51c6\u786e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.11330", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11330", "abs": "https://arxiv.org/abs/2508.11330", "authors": ["Yanghao Wang", "Long Chen"], "title": "Noise Matters: Optimizing Matching Noise for Diffusion Classifiers", "comment": null, "summary": "Although today's pretrained discriminative vision-language models (e.g.,\nCLIP) have demonstrated strong perception abilities, such as zero-shot image\nclassification, they also suffer from the bag-of-words problem and spurious\nbias. To mitigate these problems, some pioneering studies leverage powerful\ngenerative models (e.g., pretrained diffusion models) to realize generalizable\nimage classification, dubbed Diffusion Classifier (DC). Specifically, by\nrandomly sampling a Gaussian noise, DC utilizes the differences of denoising\neffects with different category conditions to classify categories.\nUnfortunately, an inherent and notorious weakness of existing DCs is noise\ninstability: different random sampled noises lead to significant performance\nchanges. To achieve stable classification performance, existing DCs always\nensemble the results of hundreds of sampled noises, which significantly reduces\nthe classification speed. To this end, we firstly explore the role of noise in\nDC, and conclude that: there are some ``good noises'' that can relieve the\ninstability. Meanwhile, we argue that these good noises should meet two\nprinciples: Frequency Matching and Spatial Matching. Regarding both principles,\nwe propose a novel Noise Optimization method to learn matching (i.e., good)\nnoise for DCs: NoOp. For frequency matching, NoOp first optimizes a\ndataset-specific noise: Given a dataset and a timestep t, optimize one randomly\ninitialized parameterized noise. For Spatial Matching, NoOp trains a\nMeta-Network that adopts an image as input and outputs image-specific noise\noffset. The sum of optimized noise and noise offset will be used in DC to\nreplace random noise. Extensive ablations on various datasets demonstrated the\neffectiveness of NoOp.", "AI": {"tldr": "NoOp\u901a\u8fc7\u5b66\u4e60\u5339\u914d\u566a\u58f0\u4f18\u5316\u6269\u6563\u5206\u7c7b\u5668\u7684\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u4e86\u6570\u767e\u6b21\u91c7\u6837\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u5206\u7c7b\u5668\uff08DC\uff09\u56e0\u566a\u58f0\u4e0d\u7a33\u5b9a\u6027\u800c\u9700\u8981\u96c6\u6210\u6570\u767e\u6b21\u91c7\u6837\u7ed3\u679c\uff0c\u5bfc\u81f4\u5206\u7c7b\u901f\u5ea6\u663e\u8457\u4e0b\u964d\u3002\u4e3a\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u8bba\u6587\u63a2\u7d22\u4e86\u566a\u58f0\u5728DC\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86NoOp\u65b9\u6cd5\u3002", "method": "NoOp\u901a\u8fc7\u9891\u7387\u5339\u914d\u548c\u7a7a\u95f4\u5339\u914d\u4e24\u4e2a\u539f\u5219\u4f18\u5316\u566a\u58f0\uff1a1\uff09\u4f18\u5316\u4e00\u4e2a\u6570\u636e\u96c6\u7279\u5b9a\u7684\u53c2\u6570\u5316\u566a\u58f0\uff1b2\uff09\u8bad\u7ec3\u4e00\u4e2a\u5143\u7f51\u7edc\u751f\u6210\u56fe\u50cf\u7279\u5b9a\u7684\u566a\u58f0\u504f\u79fb\u3002\u4f18\u5316\u7684\u566a\u58f0\u548c\u566a\u58f0\u504f\u79fb\u4e4b\u548c\u7528\u4e8e\u66ff\u4ee3\u968f\u673a\u566a\u58f0\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cNoOp\u80fd\u6709\u6548\u7f13\u89e3\u566a\u58f0\u4e0d\u7a33\u5b9a\u6027\uff0c\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNoOp\u7684\u566a\u58f0\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u5339\u914d\u566a\u58f0\u6765\u89e3\u51b3\u6269\u6563\u5206\u7c7b\u5668\uff08DC\uff09\u4e2d\u7684\u566a\u58f0\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.11334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11334", "abs": "https://arxiv.org/abs/2508.11334", "authors": ["Md Asgor Hossain Reaj", "Rajan Das Gupta", "Md Yeasin Rahat", "Nafiz Fahad", "Md Jawadul Hasan", "Tze Hui Liew"], "title": "GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition", "comment": "Accepted in ICCVDM '25", "summary": "We introduce GANDiff FR, the first synthetic framework that precisely\ncontrols demographic and environmental factors to measure, explain, and reduce\nbias with reproducible rigor. GANDiff FR unifies StyleGAN3-based\nidentity-preserving generation with diffusion-based attribute control, enabling\nfine-grained manipulation of pose around 30 degrees, illumination (four\ndirections), and expression (five levels) under ceteris paribus conditions. We\nsynthesize 10,000 demographically balanced faces across five cohorts validated\nfor realism via automated detection (98.2%) and human review (89%) to isolate\nand quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under\nmatched operating points shows AdaFace reduces inter-group TPR disparity by 60%\n(2.5% vs. 6.3%), with illumination accounting for 42% of residual bias.\nCross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong\nsynthetic-to-real transfer (r 0.85). Despite around 20% computational overhead\nrelative to pure GANs, GANDiff FR yields three times more attribute-conditioned\nvariants, establishing a reproducible, regulation-aligned (EU AI Act) standard\nfor fairness auditing. Code and data are released to support transparent,\nscalable bias evaluation.", "AI": {"tldr": "GANDiff FR \u662f\u4e00\u4e2a\u7ed3\u5408 GAN \u548c\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7cbe\u786e\u63a7\u5236\u9762\u90e8\u5c5e\u6027\u4ee5\u51cf\u5c11\u504f\u89c1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u516c\u5e73\u6027\u3002", "motivation": "\u4e3a\u4e86\u6d4b\u91cf\u3001\u89e3\u91ca\u548c\u51cf\u5c11\u504f\u89c1\uff0c\u5e76\u786e\u4fdd\u53ef\u91cd\u590d\u6027\uff0c\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86 GANDiff FR \u6846\u67b6\u3002", "method": "GANDiff FR \u7ed3\u5408\u4e86\u57fa\u4e8e StyleGAN3 \u7684\u8eab\u4efd\u4fdd\u7559\u751f\u6210\u548c\u57fa\u4e8e\u6269\u6563\u7684\u5c5e\u6027\u63a7\u5236\uff0c\u80fd\u591f\u7cbe\u7ec6\u64cd\u63a7\u59ff\u6001\uff08\u7ea630\u5ea6\uff09\u3001\u5149\u7167\uff08\u56db\u4e2a\u65b9\u5411\uff09\u548c\u8868\u60c5\uff08\u4e94\u4e2a\u7ea7\u522b\uff09\u3002", "result": "\u5728\u5408\u621010,000\u5f20\u4eba\u53e3\u7edf\u8ba1\u5e73\u8861\u7684\u9762\u5b54\u540e\uff0cAdaFace \u5c06\u7ec4\u95f4 TPR \u5dee\u5f02\u51cf\u5c11\u4e8660%\uff082.5% vs. 6.3%\uff09\uff0c\u5149\u7167\u5360\u5269\u4f59\u504f\u89c1\u768442%\u3002\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u663e\u793a\u5408\u6210\u5230\u771f\u5b9e\u7684\u5f3a\u8fc1\u79fb\u6027\uff08r 0.85\uff09\u3002", "conclusion": "GANDiff FR \u901a\u8fc7\u7ed3\u5408 StyleGAN3 \u548c\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4eba\u53e3\u7edf\u8ba1\u548c\u73af\u5883\u56e0\u7d20\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u9762\u90e8\u8bc6\u522b\u4e2d\u7684\u504f\u89c1\uff0c\u5e76\u4e3a\u516c\u5e73\u6027\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u6807\u51c6\u3002"}}
{"id": "2508.11339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11339", "abs": "https://arxiv.org/abs/2508.11339", "authors": ["Mingxiao Ma", "Shunyao Zhu", "Guoliang Kang"], "title": "Index-Aligned Query Distillation for Transformer-based Incremental Object Detection", "comment": "12 pages, 5 figures", "summary": "Incremental object detection (IOD) aims to continuously expand the capability\nof a model to detect novel categories while preserving its performance on\npreviously learned ones. When adopting a transformer-based detection model to\nperform IOD, catastrophic knowledge forgetting may inevitably occur, meaning\nthe detection performance on previously learned categories may severely\ndegenerate. Previous typical methods mainly rely on knowledge distillation (KD)\nto mitigate the catastrophic knowledge forgetting of transformer-based\ndetection models. Specifically, they utilize Hungarian Matching to build a\ncorrespondence between the queries of the last-phase and current-phase\ndetection models and align the classifier and regressor outputs between matched\nqueries to avoid knowledge forgetting. However, we observe that in IOD task,\nHungarian Matching is not a good choice. With Hungarian Matching, the query of\nthe current-phase model may match different queries of the last-phase model at\ndifferent iterations during KD. As a result, the knowledge encoded in each\nquery may be reshaped towards new categories, leading to the forgetting of\npreviously encoded knowledge of old categories. Based on our observations, we\npropose a new distillation approach named Index-Aligned Query Distillation\n(IAQD) for transformer-based IOD. Beyond using Hungarian Matching, IAQD\nestablishes a correspondence between queries of the previous and current phase\nmodels that have the same index. Moreover, we perform index-aligned\ndistillation only on partial queries which are critical for the detection of\nprevious categories. In this way, IAQD largely preserves the previous semantic\nand spatial encoding capabilities without interfering with the learning of new\ncategories. Extensive experiments on representative benchmarks demonstrate that\nIAQD effectively mitigates knowledge forgetting, achieving new state-of-the-art\nperformance.", "AI": {"tldr": "IAQD\u901a\u8fc7\u7d22\u5f15\u5bf9\u9f50\u67e5\u8be2\u84b8\u998f\uff0c\u89e3\u51b3\u4e86transformer\u589e\u91cf\u68c0\u6d4b\u4e2d\u7684\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u57fa\u4e8etransformer\u7684\u589e\u91cf\u76ee\u6807\u68c0\u6d4b\u4e2d\uff0c\u4f20\u7edf\u5308\u7259\u5229\u5339\u914d\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u77e5\u8bc6\u9057\u5fd8\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u84b8\u998f\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u540d\u4e3a\u7d22\u5f15\u5bf9\u9f50\u67e5\u8be2\u84b8\u998f\uff08IAQD\uff09\u7684\u65b0\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u7acb\u76f8\u540c\u7d22\u5f15\u7684\u67e5\u8be2\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u5728\u5173\u952e\u67e5\u8be2\u4e0a\u6267\u884c\u7d22\u5f15\u5bf9\u9f50\u84b8\u998f\u3002", "result": "IAQD\u663e\u8457\u4fdd\u7559\u4e86\u4e4b\u524d\u7684\u8bed\u4e49\u548c\u7a7a\u95f4\u7f16\u7801\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u65b0\u7c7b\u522b\u7684\u5b66\u4e60\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "IAQD\u65b9\u6cd5\u901a\u8fc7\u7d22\u5f15\u5bf9\u9f50\u7684\u67e5\u8be2\u84b8\u998f\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u57fa\u4e8etransformer\u7684\u589e\u91cf\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4e2d\u7684\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u4ee3\u8868\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11340", "categories": ["cs.CV", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2508.11340", "abs": "https://arxiv.org/abs/2508.11340", "authors": ["Yuanlin Liu", "Zhihan Zhou", "Mingqiang Wei", "Youyi Song"], "title": "Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification", "comment": "accepted by CW2025", "summary": "Information on the number and category of cervical cells is crucial for the\ndiagnosis of cervical cancer. However, existing classification methods capable\nof automatically measuring this information require the training dataset to be\nrepresentative, which consumes an expensive or even unaffordable human cost. We\nherein propose active labeling that enables us to construct a representative\ntraining dataset using a much smaller human cost for data-efficient cervical\ncell classification. This cost-effective method efficiently leverages the\nclassifier's uncertainty on the unlabeled cervical cell images to accurately\nselect images that are most beneficial to label. With a fast estimation of the\nuncertainty, this new algorithm exhibits its validity and effectiveness in\nenhancing the representative ability of the constructed training dataset. The\nextensive empirical results confirm its efficacy again in navigating the usage\nof human cost, opening the avenue for data-efficient cervical cell\nclassification.", "AI": {"tldr": "\u63d0\u51fa\u201c\u4e3b\u52a8\u6807\u8bb0\u201d\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u7c7b\u5668\u4e0d\u786e\u5b9a\u6027\u9009\u62e9\u6700\u5177\u6807\u8bb0\u4ef7\u503c\u7684\u5bab\u9888\u7ec6\u80de\u56fe\u50cf\uff0c\u663e\u8457\u964d\u4f4e\u4eba\u529b\u6210\u672c\uff0c\u63d0\u5347\u6570\u636e\u5206\u7c7b\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u5206\u7c7b\u65b9\u6cd5\u9700\u8981\u4ee3\u8868\u6027\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4f46\u6784\u5efa\u8fd9\u6837\u7684\u6570\u636e\u96c6\u901a\u5e38\u9700\u8981\u9ad8\u6602\u751a\u81f3\u96be\u4ee5\u627f\u53d7\u7684\u4eba\u529b\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u4e3b\u52a8\u6807\u8bb0\u201d\u7684\u65b0\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u57fa\u4e8e\u5206\u7c7b\u5668\u5bf9\u672a\u6807\u8bb0\u5bab\u9888\u7ec6\u80de\u56fe\u50cf\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u9ad8\u6548\u9009\u62e9\u6700\u5177\u6807\u8bb0\u4ef7\u503c\u7684\u56fe\u50cf\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u8bad\u7ec3\u6570\u636e\u96c6\u4ee3\u8868\u6027\u65b9\u9762\u6709\u6548\uff0c\u5e76\u663e\u8457\u4f18\u5316\u4e86\u4eba\u529b\u6210\u672c\u7684\u4f7f\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5229\u7528\u672a\u6807\u8bb0\u5bab\u9888\u7ec6\u80de\u56fe\u50cf\u7684\u5206\u7c7b\u5668\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6784\u5efa\u4ee3\u8868\u6027\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u4eba\u529b\u6210\u672c\uff0c\u4e3a\u6570\u636e\u9ad8\u6548\u7684\u5bab\u9888\u7ec6\u80de\u5206\u7c7b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.11341", "categories": ["cs.CV", "cs.CR", "cs.LG", "68T45, 68T01, 68T07, 68T10, 68M25", "I.2.10; I.5.4; I.2.6; I.2.7; K.6.5"], "pdf": "https://arxiv.org/pdf/2508.11341", "abs": "https://arxiv.org/abs/2508.11341", "authors": ["Katarzyna Filus", "Jorge M. Cruz-Duarte"], "title": "Semantically Guided Adversarial Testing of Vision Models Using Language Models", "comment": "12 pages, 4 figures, 3 tables. Submitted for peer review", "summary": "In targeted adversarial attacks on vision models, the selection of the target\nlabel is a critical yet often overlooked determinant of attack success. This\ntarget label corresponds to the class that the attacker aims to force the model\nto predict. Now, existing strategies typically rely on randomness, model\npredictions, or static semantic resources, limiting interpretability,\nreproducibility, or flexibility. This paper then proposes a semantics-guided\nframework for adversarial target selection using the cross-modal knowledge\ntransfer from pretrained language and vision-language models. We evaluate\nseveral state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity\nsources to select the most and least semantically related labels with respect\nto the ground truth, forming best- and worst-case adversarial scenarios. Our\nexperiments on three vision models and five attack methods reveal that these\nmodels consistently render practical adversarial targets and surpass static\nlexical databases, such as WordNet, particularly for distant class\nrelationships. We also observe that static testing of target labels offers a\npreliminary assessment of the effectiveness of similarity sources, \\textit{a\npriori} testing. Our results corroborate the suitability of pretrained models\nfor constructing interpretable, standardized, and scalable adversarial\nbenchmarks across architectures and datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u5f15\u5bfc\u7684\u5bf9\u6297\u76ee\u6807\u9009\u62e9\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u8fdc\u8ddd\u79bb\u7c7b\u522b\u5173\u7cfb\u4e0a\uff0c\u9002\u7528\u4e8e\u6784\u5efa\u6807\u51c6\u5316\u5bf9\u6297\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u7b56\u7565\u901a\u5e38\u4f9d\u8d56\u968f\u673a\u6027\u3001\u6a21\u578b\u9884\u6d4b\u6216\u9759\u6001\u8bed\u4e49\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u91cd\u590d\u6027\u6216\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5f15\u5bfc\u7684\u5bf9\u6297\u76ee\u6807\u9009\u62e9\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u3002\u8bc4\u4f30\u4e86BERT\u3001TinyLLAMA\u548cCLIP\u7b49\u6a21\u578b\u4f5c\u4e3a\u76f8\u4f3c\u6027\u6e90\uff0c\u9009\u62e9\u4e0e\u771f\u5b9e\u6807\u7b7e\u6700\u76f8\u5173\u548c\u6700\u4e0d\u76f8\u5173\u7684\u6807\u7b7e\uff0c\u5f62\u6210\u6700\u4f73\u548c\u6700\u5dee\u5bf9\u6297\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u80fd\u6301\u7eed\u63d0\u4f9b\u5b9e\u7528\u7684\u5bf9\u6297\u76ee\u6807\uff0c\u5e76\u8d85\u8d8a\u9759\u6001\u8bcd\u6c47\u6570\u636e\u5e93\uff08\u5982WordNet\uff09\uff0c\u5c24\u5176\u5728\u8fdc\u8ddd\u79bb\u7c7b\u522b\u5173\u7cfb\u4e0a\u3002\u9759\u6001\u76ee\u6807\u6807\u7b7e\u6d4b\u8bd5\u53ef\u4f5c\u4e3a\u76f8\u4f3c\u6027\u6e90\u6709\u6548\u6027\u7684\u521d\u6b65\u8bc4\u4f30\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u7528\u4e8e\u6784\u5efa\u53ef\u89e3\u91ca\u3001\u6807\u51c6\u5316\u548c\u53ef\u6269\u5c55\u7684\u5bf9\u6297\u6027\u57fa\u51c6\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u67b6\u6784\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2508.11350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11350", "abs": "https://arxiv.org/abs/2508.11350", "authors": ["Zhenhao Zhang", "Hanqing Wang", "Xiangyu Zeng", "Ziyu Cheng", "Jiaxin Liu", "Haoyu Yan", "Zhirui Liu", "Kaiyang Ji", "Tianxiang Gui", "Ke Hu", "Kangyi Chen", "Yahao Fan", "Mokai Pan"], "title": "HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model", "comment": null, "summary": "Understanding and recognizing human-object interaction (HOI) is a pivotal\napplication in AR/VR and robotics. Recent open-vocabulary HOI detection\napproaches depend exclusively on large language models for richer textual\nprompts, neglecting their inherent 3D spatial understanding capabilities. To\naddress this shortcoming, we introduce HOID-R1, the first HOI detection\nframework that integrates chain-of-thought (CoT) guided supervised fine-tuning\n(SFT) with group relative policy optimization (GRPO) within a reinforcement\nlearning (RL) paradigm. Specifically, we initially apply SFT to imbue the model\nwith essential reasoning capabilities, forcing the model to articulate its\nthought process in the output. Subsequently, we integrate GRPO to leverage\nmulti-reward signals for policy optimization, thereby enhancing alignment\nacross diverse modalities. To mitigate hallucinations in the CoT reasoning, we\nintroduce an \"MLLM-as-a-judge\" mechanism that supervises the CoT outputs,\nfurther improving generalization. Extensive experiments show that HOID-R1\nachieves state-of-the-art performance on HOI detection benchmarks and\noutperforms existing methods in open-world generalization to novel scenarios.", "AI": {"tldr": "HOID-R1\u7ed3\u5408CoT-SFT\u548cGRPO\uff0c\u5728RL\u6846\u67b6\u4e2d\u63d0\u5347HOI\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u901a\u8fc7'MLLM-as-a-judge'\u51cf\u5c11\u5e7b\u89c9\uff0c\u5b9e\u73b0\u5f00\u653e\u4e16\u754c\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c47HOI\u68c0\u6d4b\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5ffd\u89c6\u4e86\u51763D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u96c6\u6210\u4e86\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u5f15\u5bfc\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\uff0c\u5e76\u5f15\u5165'MLLM-as-a-judge'\u673a\u5236\u4ee5\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "HOID-R1\u5728HOI\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728\u65b0\u573a\u666f\u7684\u5f00\u653e\u4e16\u754c\u6cdb\u5316\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HOID-R1\u901a\u8fc7\u7ed3\u5408CoT\u5f15\u5bfc\u7684SFT\u548cGRPO\uff0c\u5728RL\u6846\u67b6\u4e2d\u663e\u8457\u63d0\u5347\u4e86HOI\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5f00\u653e\u4e16\u754c\u6cdb\u5316\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.11376", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11376", "abs": "https://arxiv.org/abs/2508.11376", "authors": ["Durgesh Mishra", "Rishabh Uikey"], "title": "Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition", "comment": "The paper spans a total of 14 pages, 10 pages for the main content\n  (including references) and 4 pages for the appendix. The main paper contains\n  3 figures and 1 table, while the appendix includes 1 pseudo-code algorithm\n  and 4 tables. The work was recently accepted for publication at IJCB 2025", "summary": "Knowledge Distillation is crucial for optimizing face recognition models for\ndeployment in computationally limited settings, such as edge devices.\nTraditional KD methods, such as Raw L2 Feature Distillation or Feature\nConsistency loss, often fail to capture both fine-grained instance-level\ndetails and complex relational structures, leading to suboptimal performance.\nWe propose a unified approach that integrates two novel loss functions,\nInstance-Level Embedding Distillation and Relation-Based Pairwise Similarity\nDistillation. Instance-Level Embedding Distillation focuses on aligning\nindividual feature embeddings by leveraging a dynamic hard mining strategy,\nthereby enhancing learning from challenging examples. Relation-Based Pairwise\nSimilarity Distillation captures relational information through pairwise\nsimilarity relationships, employing a memory bank mechanism and a sample mining\nstrategy. This unified framework ensures both effective instance-level\nalignment and preservation of geometric relationships between samples, leading\nto a more comprehensive distillation process. Our unified framework outperforms\nstate-of-the-art distillation methods across multiple benchmark face\nrecognition datasets, as demonstrated by extensive experimental evaluations.\nInterestingly, when using strong teacher networks compared to the student, our\nunified KD enables the student to even surpass the teacher's accuracy.", "AI": {"tldr": "A unified knowledge distillation method combining instance-level and relational losses improves face recognition model performance on edge devices, even surpassing teacher models in accuracy.", "motivation": "Traditional knowledge distillation methods often fail to capture both fine-grained instance-level details and complex relational structures, leading to suboptimal performance in face recognition models for edge devices.", "method": "The approach integrates two novel loss functions: Instance-Level Embedding Distillation and Relation-Based Pairwise Similarity Distillation, utilizing dynamic hard mining and memory bank mechanisms.", "result": "The unified framework outperforms state-of-the-art distillation methods across multiple benchmark face recognition datasets.", "conclusion": "The proposed unified knowledge distillation framework effectively combines instance-level and relational information, outperforming traditional methods and even enabling the student model to surpass the teacher's accuracy in some cases."}}
{"id": "2508.11409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11409", "abs": "https://arxiv.org/abs/2508.11409", "authors": ["Zhiming Liu", "Nantheera Anantrasirichai"], "title": "RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator", "comment": null, "summary": "Atmospheric turbulence severely degrades video quality by introducing\ndistortions such as geometric warping, blur, and temporal flickering, posing\nsignificant challenges to both visual clarity and temporal consistency. Current\nstate-of-the-art methods are based on transformer and 3D architectures and\nrequire multi-frame input, but their large computational cost and memory usage\nlimit real-time deployment, especially in resource-constrained scenarios. In\nthis work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric\nTurbulence Mitigator, designed for efficient and temporally consistent video\nrestoration under AT conditions. RMFAT adopts a lightweight recurrent framework\nthat restores each frame using only two inputs at a time, significantly\nreducing temporal window size and computational burden. It further integrates\nmulti-scale feature encoding and decoding with temporal warping modules at both\nencoder and decoder stages to enhance spatial detail and temporal coherence.\nExtensive experiments on synthetic and real-world atmospheric turbulence\ndatasets demonstrate that RMFAT not only outperforms existing methods in terms\nof clarity restoration (with nearly a 9\\% improvement in SSIM) but also\nachieves significantly improved inference speed (more than a fourfold reduction\nin runtime), making it particularly suitable for real-time atmospheric\nturbulence suppression tasks.", "AI": {"tldr": "RMFAT\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5faa\u73af\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u548c\u65f6\u95f4\u626d\u66f2\u6a21\u5757\u9ad8\u6548\u6062\u590d\u5927\u6c14\u6e4d\u6d41\u89c6\u9891\uff0c\u663e\u8457\u63d0\u5347\u6e05\u6670\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u5927\u6c14\u6e4d\u6d41\u4e25\u91cd\u964d\u4f4e\u89c6\u9891\u8d28\u91cf\uff0c\u73b0\u6709\u57fa\u4e8eTransformer\u548c3D\u67b6\u6784\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u65f6\u90e8\u7f72\u3002", "method": "RMFAT\u91c7\u7528\u8f7b\u91cf\u7ea7\u5faa\u73af\u6846\u67b6\uff0c\u4ec5\u9700\u4e24\u5e27\u8f93\u5165\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u7f16\u7801\u548c\u89e3\u7801\u4ee5\u53ca\u65f6\u95f4\u626d\u66f2\u6a21\u5757\u6765\u6062\u590d\u89c6\u9891\u5e27\u3002", "result": "RMFAT\u5728\u6e05\u6670\u5ea6\u6062\u590d\uff08SSIM\u63d0\u5347\u8fd19%\uff09\u548c\u63a8\u7406\u901f\u5ea6\uff08\u8fd0\u884c\u65f6\u51cf\u5c11\u56db\u500d\u4ee5\u4e0a\uff09\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RMFAT\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5faa\u73af\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u7f16\u7801\u548c\u89e3\u7801\u4ee5\u53ca\u65f6\u95f4\u626d\u66f2\u6a21\u5757\uff0c\u63d0\u5347\u4e86\u7a7a\u95f4\u7ec6\u8282\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u5927\u6c14\u6e4d\u6d41\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRMFAT\u5728\u6e05\u6670\u5ea6\u6062\u590d\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.11411", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11411", "abs": "https://arxiv.org/abs/2508.11411", "authors": ["Fabian H. Reith", "Jannik Franzen", "Dinesh R. Palli", "J. Lorenz Rumberger", "Dagmar Kainmueller"], "title": "SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models", "comment": "8 pages, 3 figures. To appear in the proceedings of the BioImage\n  Computing (BIC) Workshop @ ICCVW 2025. This is the accepted author manuscript\n  (camera-ready version)", "summary": "Deep neural networks have become the go-to method for biomedical instance\nsegmentation. Generalist models like Cellpose demonstrate state-of-the-art\nperformance across diverse cellular data, though their effectiveness often\ndegrades on domains that differ from their training data. While supervised\nfine-tuning can address this limitation, it requires annotated data that may\nnot be readily available. We propose SelfAdapt, a method that enables the\nadaptation of pre-trained cell segmentation models without the need for labels.\nOur approach builds upon student-teacher augmentation consistency training,\nintroducing L2-SP regularization and label-free stopping criteria. We evaluate\nour method on the LiveCell and TissueNet datasets, demonstrating relative\nimprovements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we\nshow that our unsupervised adaptation can further improve models that were\npreviously fine-tuned with supervision. We release SelfAdapt as an easy-to-use\nextension of the Cellpose framework. The code for our method is publicly\navailable at https: //github.com/Kainmueller-Lab/self_adapt.", "AI": {"tldr": "SelfAdapt\u901a\u8fc7\u65e0\u6807\u7b7e\u81ea\u9002\u5e94\u65b9\u6cd5\u63d0\u5347Cellpose\u5728\u7ec6\u80de\u5206\u5272\u4e2d\u7684\u6027\u80fd\uff0c\u6700\u9ad8\u63d0\u534729.64%\uff0c\u5e76\u517c\u5bb9\u76d1\u7763\u5fae\u8c03\u6a21\u578b\u3002", "motivation": "\u901a\u7528\u6a21\u578b\u5982Cellpose\u5728\u8de8\u9886\u57df\u6570\u636e\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u76d1\u7763\u5fae\u8c03\u9700\u8981\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6807\u6ce8\u53ef\u80fd\u4e0d\u6613\u83b7\u53d6\u3002", "method": "\u57fa\u4e8e\u5b66\u751f-\u6559\u5e08\u589e\u5f3a\u4e00\u81f4\u6027\u8bad\u7ec3\uff0c\u5f15\u5165L2-SP\u6b63\u5219\u5316\u548c\u65e0\u6807\u7b7e\u505c\u6b62\u6807\u51c6\u3002", "result": "\u5728LiveCell\u548cTissueNet\u6570\u636e\u96c6\u4e0a\uff0cAP0.5\u76f8\u5bf9\u57fa\u7ebfCellpose\u6700\u9ad8\u63d0\u534729.64%\uff0c\u4e14\u65e0\u76d1\u7763\u81ea\u9002\u5e94\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u76d1\u7763\u5fae\u8c03\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "SelfAdapt\u4f5c\u4e3a\u4e00\u79cd\u65e0\u9700\u6807\u7b7e\u7684\u9884\u8bad\u7ec3\u7ec6\u80de\u5206\u5272\u6a21\u578b\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86Cellpose\u5728LiveCell\u548cTissueNet\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u53ef\u4f5c\u4e3aCellpose\u6846\u67b6\u7684\u6613\u7528\u6269\u5c55\u3002"}}
{"id": "2508.11419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11419", "abs": "https://arxiv.org/abs/2508.11419", "authors": ["Florian Bayer", "Maximilian Russo", "Christian Rathgeb"], "title": "Training-free Dimensionality Reduction via Feature Truncation: Enhancing Efficiency in Privacy-preserving Multi-Biometric Systems", "comment": null, "summary": "Biometric recognition is widely used, making the privacy and security of\nextracted templates a critical concern. Biometric Template Protection schemes,\nespecially those utilizing Homomorphic Encryption, introduce significant\ncomputational challenges due to increased workload. Recent advances in deep\nneural networks have enabled state-of-the-art feature extraction for face,\nfingerprint, and iris modalities. The ubiquity and affordability of biometric\nsensors further facilitate multi-modal fusion, which can enhance security by\ncombining features from different modalities. This work investigates the\nbiometric performance of reduced multi-biometric template sizes. Experiments\nare conducted on an in-house virtual multi-biometric database, derived from\nDNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT,\nand CASIA databases. The evaluated approaches are (i) explainable and\nstraightforward to implement under encryption, (ii) training-free, and (iii)\ncapable of generalization. Dimensionality reduction of feature vectors leads to\nfewer operations in the Homomorphic Encryption (HE) domain, enabling more\nefficient encrypted processing while maintaining biometric accuracy and\nsecurity at a level equivalent to or exceeding single-biometric recognition.\nOur results demonstrate that, by fusing feature vectors from multiple\nmodalities, template size can be reduced by 67 % with no loss in Equal Error\nRate (EER) compared to the best-performing single modality.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u51cf\u5c11\u751f\u7269\u7279\u5f81\u6a21\u677f\u5c3a\u5bf8\uff0c\u5b9e\u9a8c\u8868\u660e\u6a21\u677f\u5c3a\u5bf8\u53ef\u51cf\u5c1167%\u4e14\u4e0d\u5f71\u54cd\u8bc6\u522b\u7cbe\u5ea6\u3002", "motivation": "\u751f\u7269\u7279\u5f81\u8bc6\u522b\u5e7f\u6cdb\u4f7f\u7528\uff0c\u63d0\u53d6\u6a21\u677f\u7684\u9690\u79c1\u548c\u5b89\u5168\u6027\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u751f\u7269\u7279\u5f81\u6a21\u677f\u4fdd\u62a4\u65b9\u6848\uff0c\u5c24\u5176\u662f\u4f7f\u7528\u540c\u6001\u52a0\u5bc6\u7684\u65b9\u6848\uff0c\u7531\u4e8e\u589e\u52a0\u4e86\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5e26\u6765\u4e86\u663e\u8457\u7684\u6311\u6218\u3002", "method": "\u5728\u5185\u90e8\u865a\u62df\u591a\u751f\u7269\u7279\u5f81\u6570\u636e\u5e93\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8be5\u6570\u636e\u5e93\u57fa\u4e8eDNN\u63d0\u53d6\u7684\u9762\u90e8\u3001\u6307\u7eb9\u548c\u8679\u819c\u7279\u5f81\uff0c\u4f7f\u7528FRGC\u3001MCYT\u548cCASIA\u6570\u636e\u5e93\u3002\u8bc4\u4f30\u65b9\u6cd5\u5305\u62ec\uff08i\uff09\u53ef\u89e3\u91ca\u4e14\u6613\u4e8e\u5728\u52a0\u5bc6\u4e0b\u5b9e\u73b0\uff0c\uff08ii\uff09\u65e0\u9700\u8bad\u7ec3\uff0c\uff08iii\uff09\u5177\u6709\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u51cf\u5c11\u7279\u5f81\u5411\u91cf\u7684\u7ef4\u5ea6\uff0c\u53ef\u4ee5\u5728\u540c\u6001\u52a0\u5bc6\uff08HE\uff09\u57df\u4e2d\u8fdb\u884c\u66f4\u9ad8\u6548\u7684\u52a0\u5bc6\u5904\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u7269\u7279\u5f81\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\uff0c\u8fbe\u5230\u6216\u8d85\u8fc7\u5355\u751f\u7269\u7279\u5f81\u8bc6\u522b\u7684\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u7279\u5f81\u5411\u91cf\uff0c\u6a21\u677f\u5c3a\u5bf8\u53ef\u4ee5\u51cf\u5c1167%\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u4f73\u5355\u6a21\u6001\u8bc6\u522b\u76f8\u5f53\u7684\u7b49\u9519\u8bef\u7387\uff08EER\uff09\u3002"}}
{"id": "2508.11428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11428", "abs": "https://arxiv.org/abs/2508.11428", "authors": ["Jingyu Li", "Bozhou Zhang", "Xin Jin", "Jiankang Deng", "Xiatian Zhu", "Li Zhang"], "title": "ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving", "comment": null, "summary": "Autonomous driving requires rich contextual comprehension and precise\npredictive reasoning to navigate dynamic and complex environments safely.\nVision-Language Models (VLMs) and Driving World Models (DWMs) have\nindependently emerged as powerful recipes addressing different aspects of this\nchallenge. VLMs provide interpretability and robust action prediction through\ntheir ability to understand multi-modal context, while DWMs excel in generating\ndetailed and plausible future driving scenarios essential for proactive\nplanning. Integrating VLMs with DWMs is an intuitive, promising, yet\nunderstudied strategy to exploit the complementary strengths of accurate\nbehavioral prediction and realistic scene generation. Nevertheless, this\nintegration presents notable challenges, particularly in effectively connecting\naction-level decisions with high-fidelity pixel-level predictions and\nmaintaining computational efficiency. In this paper, we propose ImagiDrive, a\nnovel end-to-end autonomous driving framework that integrates a VLM-based\ndriving agent with a DWM-based scene imaginer to form a unified\nimagination-and-planning loop. The driving agent predicts initial driving\ntrajectories based on multi-modal inputs, guiding the scene imaginer to\ngenerate corresponding future scenarios. These imagined scenarios are\nsubsequently utilized to iteratively refine the driving agent's planning\ndecisions. To address efficiency and predictive accuracy challenges inherent in\nthis integration, we introduce an early stopping mechanism and a trajectory\nselection strategy. Extensive experimental validation on the nuScenes and\nNAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over\nprevious alternatives under both open-loop and closed-loop conditions.", "AI": {"tldr": "ImagiDrive\u6574\u5408VLM\u548cDWM\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u9a7e\u9a76\u51b3\u7b56\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u591a\u6a21\u6001\u7406\u89e3\u548c\u9884\u6d4b\u80fd\u529b\uff0cVLM\u548cDWM\u5404\u6709\u4f18\u52bf\u4f46\u6574\u5408\u4e0d\u8db3\uff0cImagiDrive\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86ImagiDrive\u6846\u67b6\uff0c\u7ed3\u5408VLM\u9a71\u52a8\u7684\u884c\u4e3a\u548cDWM\u9a71\u52a8\u7684\u573a\u666f\u751f\u6210\uff0c\u901a\u8fc7\u65e9\u671f\u505c\u6b62\u673a\u5236\u548c\u8f68\u8ff9\u9009\u62e9\u7b56\u7565\u4f18\u5316\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "result": "\u5728nuScenes\u548cNAVSIM\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86ImagiDrive\u7684\u9c81\u68d2\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "ImagiDrive\u901a\u8fc7\u6574\u5408VLM\u548cDWM\uff0c\u5c55\u793a\u4e86\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5f00\u73af\u548c\u95ed\u73af\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.11431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11431", "abs": "https://arxiv.org/abs/2508.11431", "authors": ["Simona Kocour", "Assia Benbihi", "Torsten Sattler"], "title": "Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting", "comment": "arXiv admin note: substantial text overlap with arXiv:2503.17574", "summary": "Understanding what semantic information persists after object removal is\ncritical for privacy-preserving 3D reconstruction and editable scene\nrepresentations. In this work, we introduce a novel benchmark and evaluation\nframework to measure semantic residuals, the unintended semantic traces left\nbehind, after object removal in 3D Gaussian Splatting. We conduct experiments\nacross a diverse set of indoor and outdoor scenes, showing that current methods\ncan preserve semantic information despite the absence of visual geometry. We\nalso release Remove360, a dataset of pre/post-removal RGB images and\nobject-level masks captured in real-world environments. While prior datasets\nhave focused on isolated object instances, Remove360 covers a broader and more\ncomplex range of indoor and outdoor scenes, enabling evaluation of object\nremoval in the context of full-scene representations. Given ground truth images\nof a scene before and after object removal, we assess whether we can truly\neliminate semantic presence, and if downstream models can still infer what was\nremoved. Our findings reveal critical limitations in current 3D object removal\ntechniques and underscore the need for more robust solutions capable of\nhandling real-world complexity. The evaluation framework is available at\ngithub.com/spatial-intelligence-ai/Remove360.git. Data are available at\nhuggingface.co/datasets/simkoc/Remove360.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f303D\u7269\u4f53\u79fb\u9664\u540e\u8bed\u4e49\u6b8b\u7559\u7684\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u7406\u89e3\u7269\u4f53\u79fb\u9664\u540e\u4fdd\u7559\u7684\u8bed\u4e49\u4fe1\u606f\u5bf9\u9690\u79c1\u4fdd\u62a4\u76843D\u91cd\u5efa\u548c\u53ef\u7f16\u8f91\u573a\u666f\u8868\u793a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u6d4b\u91cf3D\u9ad8\u65af\u6cfc\u6e85\u4e2d\u7269\u4f53\u79fb\u9664\u540e\u7684\u8bed\u4e49\u6b8b\u7559\u3002\u5b9e\u9a8c\u6db5\u76d6\u4e86\u591a\u6837\u5316\u7684\u5ba4\u5185\u5916\u573a\u666f\uff0c\u5e76\u53d1\u5e03\u4e86Remove360\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u65b9\u6cd5\u5728\u89c6\u89c9\u51e0\u4f55\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u7559\u8bed\u4e49\u4fe1\u606f\uff0c\u4f46\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "\u5f53\u524d3D\u7269\u4f53\u79fb\u9664\u6280\u672f\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u590d\u6742\u573a\u666f\u65f6\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11433", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11433", "abs": "https://arxiv.org/abs/2508.11433", "authors": ["Qian Liang", "Yujia Wu", "Kuncheng Li", "Jiwei Wei", "Shiyuan He", "Jinyu Guo", "Ning Xie"], "title": "MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) with unified architectures excel\nacross a wide range of vision-language tasks, yet aligning them with\npersonalized image generation remains a significant challenge. Existing methods\nfor MLLMs are frequently subject-specific, demanding a data-intensive\nfine-tuning process for every new subject, which limits their scalability. In\nthis paper, we introduce MM-R1, a framework that integrates a cross-modal\nChain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of\nunified MLLMs for personalized image generation. Specifically, we structure\npersonalization as an integrated visual reasoning and generation process: (1)\ngrounding subject concepts by interpreting and understanding user-provided\nimages and contextual cues, and (2) generating personalized images conditioned\non both the extracted subject representations and user prompts. To further\nenhance the reasoning capability, we adopt Grouped Reward Proximal Policy\nOptimization (GRPO) to explicitly align the generation. Experiments demonstrate\nthat MM-R1 unleashes the personalization capability of unified MLLMs to\ngenerate images with high subject fidelity and strong text alignment in a\nzero-shot manner.", "AI": {"tldr": "MM-R1\u901a\u8fc7\u8de8\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\u548cGRPO\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u7edf\u4e00MLLMs\u5728\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u9ad8\u6548\u96f6\u6837\u672c\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684MLLMs\u65b9\u6cd5\u901a\u5e38\u662f\u7279\u5b9a\u4e3b\u9898\u7684\uff0c\u9700\u8981\u4e3a\u6bcf\u4e2a\u65b0\u4e3b\u9898\u8fdb\u884c\u6570\u636e\u5bc6\u96c6\u7684\u5fae\u8c03\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u9501\u7edf\u4e00MLLMs\u5728\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6f5c\u529b\u3002", "method": "MM-R1\u901a\u8fc7\u89c6\u89c9\u63a8\u7406\u548c\u751f\u6210\u8fc7\u7a0b\u7684\u96c6\u6210\uff0c\u5305\u62ec\uff081\uff09\u57fa\u4e8e\u7528\u6237\u63d0\u4f9b\u7684\u56fe\u50cf\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\u7684\u4e3b\u9898\u6982\u5ff5\u5b9a\u4f4d\uff0c\uff082\uff09\u57fa\u4e8e\u63d0\u53d6\u7684\u4e3b\u9898\u8868\u793a\u548c\u7528\u6237\u63d0\u793a\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u3002\u6b64\u5916\uff0c\u91c7\u7528\u5206\u7ec4\u5956\u52b1\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u8fdb\u4e00\u6b65\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cMM-R1\u80fd\u591f\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u751f\u6210\u5177\u6709\u9ad8\u4e3b\u9898\u4fdd\u771f\u5ea6\u548c\u5f3a\u6587\u672c\u5bf9\u9f50\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u3002", "conclusion": "MM-R1\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u601d\u7ef4\u94fe\uff08X-CoT\uff09\u63a8\u7406\u7b56\u7565\uff0c\u6210\u529f\u91ca\u653e\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4e3b\u9898\u4fdd\u771f\u5ea6\u548c\u5f3a\u6587\u672c\u5bf9\u9f50\u7684\u96f6\u6837\u672c\u751f\u6210\u3002"}}
{"id": "2508.11464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11464", "abs": "https://arxiv.org/abs/2508.11464", "authors": ["Xiaoya Zhu", "Yibing Nan", "Shiguo Lian"], "title": "Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge", "comment": null, "summary": "With the rapid development of technology in the field of AI, deepfake\ntechnology has emerged as a double-edged sword. It has not only created a large\namount of AI-generated content but also posed unprecedented challenges to\ndigital security. The task of the competition is to determine whether a face\nimage is a Deepfake image and output its probability score of being a Deepfake\nimage. In the image track competition, our approach is based on the Swin\nTransformer V2-B classification network. And online data augmentation and\noffline sample generation methods are employed to enrich the diversity of\ntraining samples and increase the generalization ability of the model. Finally,\nwe got the award of excellence in Deepfake image detection.", "AI": {"tldr": "\u672c\u6587\u5229\u7528Swin Transformer V2-B\u7f51\u7edc\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u6210\u529f\u68c0\u6d4bDeepfake\u56fe\u50cf\u5e76\u5728\u7ade\u8d5b\u4e2d\u83b7\u5956\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0cDeepfake\u6280\u672f\u5e26\u6765\u4e86\u5927\u91cfAI\u751f\u6210\u5185\u5bb9\uff0c\u540c\u65f6\u4e5f\u5bf9\u6570\u5b57\u5b89\u5168\u63d0\u51fa\u4e86\u524d\u6240\u672a\u6709\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528Swin Transformer V2-B\u5206\u7c7b\u7f51\u7edc\uff0c\u7ed3\u5408\u5728\u7ebf\u6570\u636e\u589e\u5f3a\u548c\u79bb\u7ebf\u6837\u672c\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u5728Deepfake\u56fe\u50cf\u68c0\u6d4b\u7ade\u8d5b\u4e2d\u83b7\u5f97\u4f18\u5f02\u5956\u9879\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7Swin Transformer V2-B\u5206\u7c7b\u7f51\u7edc\u53ca\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u5728Deepfake\u56fe\u50cf\u68c0\u6d4b\u7ade\u8d5b\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u5956\u9879\u3002"}}
{"id": "2508.11469", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11469", "abs": "https://arxiv.org/abs/2508.11469", "authors": ["Hongjin Fang", "Daniel Reisenb\u00fcchler", "Kenji Ikemura", "Mert R. Sabuncu", "Yihe Yang", "Ruining Deng"], "title": "CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement Membrane Segmentation", "comment": null, "summary": "Accurate segmentation of the glomerular basement membrane (GBM) in electron\nmicroscopy (EM) images is fundamental for quantifying membrane thickness and\nsupporting the diagnosis of various kidney diseases. While supervised deep\nlearning approaches achieve high segmentation accuracy, their reliance on\nextensive pixel-level annotation renders them impractical for clinical\nworkflows. Few-shot learning can reduce this annotation burden but often\nstruggles to capture the fine structural details necessary for GBM analysis. In\nthis study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot\nsegmentation pipeline designed for GBM delineation in EM images. CoFi first\ntrains a lightweight neural network using only three annotated images to\nproduce an initial coarse segmentation mask. This mask is then automatically\nprocessed to generate high-quality point prompts with morphology-aware pruning,\nwhich are subsequently used to guide SAM in refining the segmentation. The\nproposed method achieved exceptional GBM segmentation performance, with a Dice\ncoefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that\nCoFi not only alleviates the annotation and computational burdens associated\nwith conventional methods, but also achieves accurate and reliable segmentation\nresults. The pipeline's speed and annotation efficiency make it well-suited for\nresearch and hold strong potential for clinical applications in renal\npathology. The pipeline is publicly available at:\nhttps://github.com/ddrrnn123/CoFi.", "AI": {"tldr": "CoFi\u662f\u4e00\u79cd\u5feb\u901f\u9ad8\u6548\u7684\u7c97\u5230\u7cbe\u7ec6\u5c11\u6837\u672c\u5206\u5272\u6d41\u7a0b\uff0c\u7528\u4e8eEM\u56fe\u50cf\u4e2d\u7684GBM\u5206\u5272\uff0c\u51cf\u8f7b\u6807\u6ce8\u548c\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u901f\u5ea6\u3002", "motivation": "\u51c6\u786e\u5206\u5272\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7684GBM\u5bf9\u4e8e\u91cf\u5316\u819c\u539a\u5ea6\u548c\u652f\u6301\u5404\u79cd\u80be\u810f\u75be\u75c5\u7684\u8bca\u65ad\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u5206\u5272\u7cbe\u5ea6\uff0c\u4f46\u5176\u5bf9\u5927\u91cf\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u4f9d\u8d56\u4f7f\u5176\u5728\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002\u5c11\u6837\u672c\u5b66\u4e60\u53ef\u4ee5\u51cf\u5c11\u6807\u6ce8\u8d1f\u62c5\uff0c\u4f46\u5f80\u5f80\u96be\u4ee5\u6355\u6349GBM\u5206\u6790\u6240\u9700\u7684\u7cbe\u7ec6\u7ed3\u6784\u7ec6\u8282\u3002", "method": "CoFi\u9996\u5148\u4f7f\u7528\u4ec5\u4e09\u5f20\u6807\u6ce8\u56fe\u50cf\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u521d\u59cb\u7c97\u5206\u5272\u63a9\u6a21\uff0c\u7136\u540e\u901a\u8fc7\u5f62\u6001\u611f\u77e5\u4fee\u526a\u81ea\u52a8\u5904\u7406\u751f\u6210\u9ad8\u8d28\u91cf\u70b9\u63d0\u793a\uff0c\u7528\u4e8e\u6307\u5bfcSAM\u7ec6\u5316\u5206\u5272\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728GBM\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0cDice\u7cfb\u6570\u8fbe\u523074.54%\uff0c\u63a8\u7406\u901f\u5ea6\u4e3a1.9 FPS\u3002", "conclusion": "CoFi\u4e0d\u4ec5\u51cf\u8f7b\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u6807\u6ce8\u548c\u8ba1\u7b97\u8d1f\u62c5\uff0c\u8fd8\u5b9e\u73b0\u4e86\u51c6\u786e\u53ef\u9760\u7684GBM\u5206\u5272\u7ed3\u679c\uff0c\u5176\u901f\u5ea6\u548c\u6807\u6ce8\u6548\u7387\u4f7f\u5176\u975e\u5e38\u9002\u5408\u7814\u7a76\uff0c\u5e76\u5177\u6709\u5728\u80be\u810f\u75c5\u7406\u5b66\u4e34\u5e8a\u5e94\u7528\u4e2d\u5f3a\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.11478", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11478", "abs": "https://arxiv.org/abs/2508.11478", "authors": ["Xinyi Yin", "Wenbo Yuan", "Xuecheng Wu", "Liangyu Fu", "Danlei Huang"], "title": "TACR-YOLO: A Real-time Detection Framework for Abnormal Human Behaviors Enhanced with Coordinate and Task-Aware Representations", "comment": "8 pages, 4 figures, accepted by IJCNN 2025", "summary": "Abnormal Human Behavior Detection (AHBD) under special scenarios is becoming\nincreasingly crucial. While YOLO-based detection methods excel in real-time\ntasks, they remain hindered by challenges including small objects, task\nconflicts, and multi-scale fusion in AHBD. To tackle them, we propose\nTACR-YOLO, a new real-time framework for AHBD. We introduce a Coordinate\nAttention Module to enhance small object detection, a Task-Aware Attention\nModule to deal with classification-regression conflicts, and a Strengthen Neck\nNetwork for refined multi-scale fusion, respectively. In addition, we optimize\nAnchor Box sizes using K-means clustering and deploy DIoU-Loss to improve\nbounding box regression. The Personnel Anomalous Behavior Detection (PABD)\ndataset, which includes 8,529 samples across four behavior categories, is also\npresented. Extensive experimental results indicate that TACR-YOLO achieves\n91.92% mAP on PABD, with competitive speed and robustness. Ablation studies\nhighlight the contribution of each improvement. This work provides new insights\nfor abnormal behavior detection under special scenarios, advancing its\nprogress.", "AI": {"tldr": "TACR-YOLO \u662f\u4e00\u79cd\u65b0\u578b\u5b9e\u65f6\u5f02\u5e38\u884c\u4e3a\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e2a\u6a21\u5757\u6539\u8fdb YOLO \u65b9\u6cd5\uff0c\u5728 PABD \u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7279\u6b8a\u573a\u666f\u4e0b\u7684\u5f02\u5e38\u4eba\u7c7b\u884c\u4e3a\u68c0\u6d4b\uff08AHBD\uff09\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709 YOLO \u65b9\u6cd5\u5728\u5c0f\u76ee\u6807\u68c0\u6d4b\u3001\u4efb\u52a1\u51b2\u7a81\u548c\u591a\u5c3a\u5ea6\u878d\u5408\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86 TACR-YOLO \u6846\u67b6\uff0c\u5305\u62ec Coordinate Attention Module\uff08\u589e\u5f3a\u5c0f\u76ee\u6807\u68c0\u6d4b\uff09\u3001Task-Aware Attention Module\uff08\u5904\u7406\u5206\u7c7b-\u56de\u5f52\u51b2\u7a81\uff09\u3001Strengthen Neck Network\uff08\u4f18\u5316\u591a\u5c3a\u5ea6\u878d\u5408\uff09\uff0c\u5e76\u4f18\u5316\u4e86 Anchor Box \u5927\u5c0f\u548c DIoU-Loss\u3002", "result": "\u5728 PABD \u6570\u636e\u96c6\u4e0a\uff0cTACR-YOLO \u8fbe\u5230 91.92% mAP\uff0c\u901f\u5ea6\u548c\u9c81\u68d2\u6027\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TACR-YOLO \u5728\u7279\u6b8a\u573a\u666f\u4e0b\u7684\u5f02\u5e38\u884c\u4e3a\u68c0\u6d4b\u4e2d\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2508.11482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11482", "abs": "https://arxiv.org/abs/2508.11482", "authors": ["Ruoxin Xiong", "Yanyu Wang", "Jiannan Cai", "Kaijian Liu", "Yuansheng Zhu", "Pingbo Tang", "Nora El-Gohary"], "title": "OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring", "comment": null, "summary": "The construction industry increasingly relies on visual data to support\nArtificial Intelligence (AI) and Machine Learning (ML) applications for site\nmonitoring. High-quality, domain-specific datasets, comprising images, videos,\nand point clouds, capture site geometry and spatiotemporal dynamics, including\nthe location and interaction of objects, workers, and materials. However,\ndespite growing interest in leveraging visual datasets, existing resources vary\nwidely in sizes, data modalities, annotation quality, and representativeness of\nreal-world construction conditions. A systematic review to categorize their\ndata characteristics and application contexts is still lacking, limiting the\ncommunity's ability to fully understand the dataset landscape, identify\ncritical gaps, and guide future directions toward more effective, reliable, and\nscalable AI applications in construction. To address this gap, this study\nconducts an extensive search of academic databases and open-data platforms,\nyielding 51 publicly available visual datasets that span the 2005-2024 period.\nThese datasets are categorized using a structured data schema covering (i) data\nfundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and\npoint cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)\ndownstream application domains (e.g., progress tracking). This study\nsynthesizes these findings into an open-source catalog, OpenConstruction,\nsupporting data-driven method development. Furthermore, the study discusses\nseveral critical limitations in the existing construction dataset landscape and\npresents a roadmap for future data infrastructure anchored in the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) principles. By\nreviewing the current landscape and outlining strategic priorities, this study\nsupports the advancement of data-centric solutions in the construction sector.", "AI": {"tldr": "\u7814\u7a76\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5efa\u7b51\u884c\u4e1a\u7684\u89c6\u89c9\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86\u5f00\u6e90\u76ee\u5f55\u548cFAIR\u539f\u5219\u4e0b\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u5efa\u7b51\u884c\u4e1a\u5bf9\u89c6\u89c9\u6570\u636e\u7684\u4f9d\u8d56\u65e5\u76ca\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u5728\u89c4\u6a21\u3001\u6570\u636e\u6a21\u6001\u3001\u6807\u6ce8\u8d28\u91cf\u548c\u4ee3\u8868\u6027\u4e0a\u5dee\u5f02\u8f83\u5927\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u9650\u5236\u4e86AI\u5e94\u7528\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u641c\u7d22\u5b66\u672f\u6570\u636e\u5e93\u548c\u5f00\u653e\u6570\u636e\u5e73\u53f0\uff0c\u6536\u96c6\u4e8651\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u89c6\u89c9\u6570\u636e\u96c6\uff082005-2024\u5e74\uff09\uff0c\u5e76\u4f7f\u7528\u7ed3\u6784\u5316\u6570\u636e\u6a21\u5f0f\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u63d0\u51fa\u4e86OpenConstruction\u5f00\u6e90\u76ee\u5f55\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u57fa\u4e8eFAIR\u539f\u5219\u63d0\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u7684\u6218\u7565\u4f18\u5148\u7ea7\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u548c\u5206\u7c7b\u73b0\u6709\u5efa\u7b51\u89c6\u89c9\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86OpenConstruction\u5f00\u6e90\u76ee\u5f55\uff0c\u5e76\u57fa\u4e8eFAIR\u539f\u5219\u63d0\u51fa\u4e86\u672a\u6765\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u7684\u53d1\u5c55\u8def\u7ebf\u56fe\uff0c\u652f\u6301\u5efa\u7b51\u884c\u4e1a\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.11484", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11484", "abs": "https://arxiv.org/abs/2508.11484", "authors": ["Xiaoxue Wu", "Bingjie Gao", "Yu Qiao", "Yaohui Wang", "Xinyuan Chen"], "title": "CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models", "comment": "27 pages, 20 figures", "summary": "Despite significant advances in video synthesis, research into multi-shot\nvideo generation remains in its infancy. Even with scaled-up models and massive\ndatasets, the shot transition capabilities remain rudimentary and unstable,\nlargely confining generated videos to single-shot sequences. In this work, we\nintroduce CineTrans, a novel framework for generating coherent multi-shot\nvideos with cinematic, film-style transitions. To facilitate insights into the\nfilm editing style, we construct a multi-shot video-text dataset Cine250K with\ndetailed shot annotations. Furthermore, our analysis of existing video\ndiffusion models uncovers a correspondence between attention maps in the\ndiffusion model and shot boundaries, which we leverage to design a mask-based\ncontrol mechanism that enables transitions at arbitrary positions and transfers\neffectively in a training-free setting. After fine-tuning on our dataset with\nthe mask mechanism, CineTrans produces cinematic multi-shot sequences while\nadhering to the film editing style, avoiding unstable transitions or naive\nconcatenations. Finally, we propose specialized evaluation metrics for\ntransition control, temporal consistency and overall quality, and demonstrate\nthrough extensive experiments that CineTrans significantly outperforms existing\nbaselines across all criteria.", "AI": {"tldr": "CineTrans\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u5177\u6709\u7535\u5f71\u98ce\u683c\u8f6c\u6362\u7684\u591a\u955c\u5934\u89c6\u9891\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u63a7\u5236\u673a\u5236\u548c\u4e13\u7528\u6570\u636e\u96c6\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u8fde\u8d2f\u6027\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u9891\u5408\u6210\u6280\u672f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u7684\u7814\u7a76\u4ecd\u5904\u4e8e\u521d\u7ea7\u9636\u6bb5\uff0c\u73b0\u6709\u6a21\u578b\u7684\u955c\u5934\u8f6c\u6362\u80fd\u529b\u6709\u9650\u4e14\u4e0d\u7a33\u5b9a\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u955c\u5934\u89c6\u9891\u6587\u672c\u6570\u636e\u96c6Cine250K\uff0c\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u56fe\u4e0e\u955c\u5934\u8fb9\u754c\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u7684\u63a7\u5236\u673a\u5236\uff0c\u5b9e\u73b0\u4efb\u610f\u4f4d\u7f6e\u7684\u8f6c\u6362\u3002", "result": "CineTrans\u80fd\u591f\u751f\u6210\u5177\u6709\u7535\u5f71\u98ce\u683c\u8f6c\u6362\u7684\u8fde\u8d2f\u591a\u955c\u5934\u89c6\u9891\uff0c\u907f\u514d\u4e86\u4e0d\u7a33\u5b9a\u7684\u8fc7\u6e21\u6216\u7b80\u5355\u62fc\u63a5\uff0c\u5e76\u5728\u6240\u6709\u8bc4\u4f30\u6807\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "CineTrans\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u63a9\u7801\u7684\u63a7\u5236\u673a\u5236\u548c\u4e13\u7528\u8bc4\u4f30\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u7684\u8fde\u8d2f\u6027\u548c\u7535\u5f71\u98ce\u683c\u8f6c\u6362\u7684\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.11486", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11486", "abs": "https://arxiv.org/abs/2508.11486", "authors": ["Kristina Dabrock", "Tim Johansson", "Anna Donarelli", "Mikael Mangold", "Noah Pflugradt", "Jann Michael Weinand", "Jochen Lin\u00dfen"], "title": "Automated Building Heritage Assessment Using Street-Level Imagery", "comment": null, "summary": "Detailed data is required to quantify energy conservation measures in\nbuildings, such as envelop retrofits, without compromising cultural heritage.\nNovel artificial intelligence tools may improve efficiency in identifying\nheritage values in buildings compared to costly and time-consuming traditional\ninventories. In this study, the large language model GPT was used to detect\nvarious aspects of cultural heritage value in fa\\c{c}ade images. Using this\ndata and building register data as features, machine learning models were\ntrained to classify multi-family and non-residential buildings in Stockholm,\nSweden. Validation against an expert-created inventory shows a macro F1-score\nof 0.71 using a combination of register data and features retrieved from GPT,\nand a score of 0.60 using only GPT-derived data. The presented methodology can\ncontribute to a higher-quality database and thus support careful energy\nefficiency measures and integrated consideration of heritage value in\nlarge-scale energetic refurbishment scenarios.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528GPT\u548c\u673a\u5668\u5b66\u4e60\u8bc4\u4f30\u5efa\u7b51\u6587\u5316\u9057\u4ea7\u4ef7\u503c\uff0c\u9a8c\u8bc1\u7ed3\u679c\u663e\u793a\u7ed3\u5408\u767b\u8bb0\u6570\u636e\u6548\u679c\u66f4\u4f73\uff0c\u652f\u6301\u9ad8\u6548\u80fd\u6e90\u6539\u9020\u3002", "motivation": "\u4f20\u7edf\u5efa\u7b51\u6587\u5316\u9057\u4ea7\u4ef7\u503c\u8bc4\u4f30\u8017\u65f6\u4e14\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528GPT\u68c0\u6d4b\u5efa\u7b51\u7acb\u9762\u56fe\u50cf\u7684\u6587\u5316\u9057\u4ea7\u4ef7\u503c\uff0c\u5e76\u7ed3\u5408\u5efa\u7b51\u767b\u8bb0\u6570\u636e\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u7ed3\u5408GPT\u548c\u767b\u8bb0\u6570\u636e\u7684\u6a21\u578b\u5b8fF1\u5f97\u5206\u4e3a0.71\uff0c\u4ec5\u4f7f\u7528GPT\u6570\u636e\u7684\u5f97\u5206\u4e3a0.60\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u6570\u636e\u5e93\u8d28\u91cf\uff0c\u652f\u6301\u5728\u5927\u578b\u80fd\u6e90\u6539\u9020\u4e2d\u7efc\u5408\u8003\u8651\u6587\u5316\u9057\u4ea7\u4ef7\u503c\u3002"}}
{"id": "2508.11488", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11488", "abs": "https://arxiv.org/abs/2508.11488", "authors": ["Bozhou Zhang", "Jingyu Li", "Nan Song", "Li Zhang"], "title": "Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving has achieved remarkable advancements in recent\nyears. Existing methods primarily follow a perception-planning paradigm, where\nperception and planning are executed sequentially within a fully differentiable\nframework for planning-oriented optimization. We further advance this paradigm\nthrough a perception-in-plan framework design, which integrates perception into\nthe planning process. This design facilitates targeted perception guided by\nevolving planning objectives over time, ultimately enhancing planning\nperformance. Building on this insight, we introduce VeteranAD, a coupled\nperception and planning framework for end-to-end autonomous driving. By\nincorporating multi-mode anchored trajectories as planning priors, the\nperception module is specifically designed to gather traffic elements along\nthese trajectories, enabling comprehensive and targeted perception. Planning\ntrajectories are then generated based on both the perception results and the\nplanning priors. To make perception fully serve planning, we adopt an\nautoregressive strategy that progressively predicts future trajectories while\nfocusing on relevant regions for targeted perception at each step. With this\nsimple yet effective design, VeteranAD fully unleashes the potential of\nplanning-oriented end-to-end methods, leading to more accurate and reliable\ndriving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasets\ndemonstrate that our VeteranAD achieves state-of-the-art performance.", "AI": {"tldr": "VeteranAD\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u611f\u77e5-\u89c4\u5212\u4e00\u4f53\u5316\u8bbe\u8ba1\uff0c\u5229\u7528\u89c4\u5212\u76ee\u6807\u5f15\u5bfc\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u9a7e\u9a76\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9075\u5faa\u611f\u77e5-\u89c4\u5212\u8303\u5f0f\uff0c\u4f46\u611f\u77e5\u548c\u89c4\u5212\u662f\u987a\u5e8f\u6267\u884c\u7684\u3002\u672c\u6587\u63d0\u51fa\u611f\u77e5-\u89c4\u5212\u4e00\u4f53\u5316\u8bbe\u8ba1\uff0c\u901a\u8fc7\u89c4\u5212\u76ee\u6807\u5f15\u5bfc\u611f\u77e5\uff0c\u63d0\u5347\u89c4\u5212\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86VeteranAD\u6846\u67b6\uff0c\u91c7\u7528\u611f\u77e5-\u89c4\u5212\u8026\u5408\u8bbe\u8ba1\uff0c\u901a\u8fc7\u591a\u6a21\u5f0f\u951a\u5b9a\u8f68\u8ff9\u4f5c\u4e3a\u89c4\u5212\u5148\u9a8c\uff0c\u611f\u77e5\u6a21\u5757\u4e13\u95e8\u6536\u96c6\u8fd9\u4e9b\u8f68\u8ff9\u4e0a\u7684\u4ea4\u901a\u5143\u7d20\u3002\u91c7\u7528\u81ea\u56de\u5f52\u7b56\u7565\u9010\u6b65\u9884\u6d4b\u672a\u6765\u8f68\u8ff9\uff0c\u5e76\u5728\u6bcf\u4e00\u6b65\u805a\u7126\u76f8\u5173\u533a\u57df\u8fdb\u884c\u9488\u5bf9\u6027\u611f\u77e5\u3002", "result": "\u5728NAVSIM\u548cBench2Drive\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVeteranAD\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "VeteranAD\u901a\u8fc7\u5c06\u611f\u77e5\u6574\u5408\u5230\u89c4\u5212\u8fc7\u7a0b\u4e2d\uff0c\u91ca\u653e\u4e86\u89c4\u5212\u5bfc\u5411\u7aef\u5230\u7aef\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u53ef\u9760\u7684\u9a7e\u9a76\u884c\u4e3a\u3002"}}
{"id": "2508.11497", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11497", "abs": "https://arxiv.org/abs/2508.11497", "authors": ["Feiyue Zhao", "Zhichao Zhang"], "title": "Hierarchical Graph Feature Enhancement with Adaptive Frequency Modulation for Visual Recognition", "comment": null, "summary": "Convolutional neural networks (CNNs) have\n  demonstrated strong performance in visual recognition tasks,\n  but their inherent reliance on regular grid structures limits\n  their capacity to model complex topological relationships and\n  non-local semantics within images. To address this limita tion, we propose\nthe hierarchical graph feature enhancement\n  (HGFE), a novel framework that integrates graph-based rea soning into CNNs to\nenhance both structural awareness and\n  feature representation. HGFE builds two complementary levels\n  of graph structures: intra-window graph convolution to cap ture local spatial\ndependencies and inter-window supernode\n  interactions to model global semantic relationships. Moreover,\n  we introduce an adaptive frequency modulation module that\n  dynamically balances low-frequency and high-frequency signal\n  propagation, preserving critical edge and texture information\n  while mitigating over-smoothing. The proposed HGFE module\n  is lightweight, end-to-end trainable, and can be seamlessly\n  integrated into standard CNN backbone networks. Extensive\n  experiments on CIFAR-100 (classification), PASCAL VOC,\n  and VisDrone (detection), as well as CrackSeg and CarParts\n  (segmentation), validated the effectiveness of the HGFE in\n  improving structural representation and enhancing overall\n  recognition performance.", "AI": {"tldr": "HGFE\u6846\u67b6\u901a\u8fc7\u56fe\u63a8\u7406\u589e\u5f3aCNN\u7684\u7ed3\u6784\u611f\u77e5\u548c\u7279\u5f81\u8868\u793a\uff0c\u5728\u591a\u4e2a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "CNN\u5bf9\u89c4\u5219\u7f51\u683c\u7ed3\u6784\u7684\u4f9d\u8d56\u9650\u5236\u4e86\u5176\u5bf9\u590d\u6742\u62d3\u6251\u5173\u7cfb\u548c\u975e\u5c40\u90e8\u8bed\u4e49\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u9700\u8981\u589e\u5f3a\u7ed3\u6784\u611f\u77e5\u548c\u7279\u5f81\u8868\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u5c42\u56fe\u7279\u5f81\u589e\u5f3a\uff08HGFE\uff09\u6846\u67b6\uff0c\u5305\u542b\u5c40\u90e8\u7a7a\u95f4\u4f9d\u8d56\u7684\u7a97\u53e3\u5185\u56fe\u5377\u79ef\u548c\u5168\u5c40\u8bed\u4e49\u5173\u7cfb\u7684\u7a97\u53e3\u95f4\u8d85\u8282\u70b9\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u9891\u7387\u8c03\u5236\u6a21\u5757\u3002", "result": "\u5728CIFAR-100\u3001PASCAL VOC\u3001VisDrone\u3001CrackSeg\u548cCarParts\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86HGFE\u7684\u6709\u6548\u6027\u3002", "conclusion": "HGFE\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u56fe\u63a8\u7406\u548cCNN\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u7ed3\u6784\u611f\u77e5\u548c\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.11502", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11502", "abs": "https://arxiv.org/abs/2508.11502", "authors": ["Eyad Alshami", "Shashank Agnihotri", "Bernt Schiele", "Margret Keuper"], "title": "AIM: Amending Inherent Interpretability via Self-Supervised Masking", "comment": "Accepted at International Conference on Computer Vision (ICCV) 2025", "summary": "It has been observed that deep neural networks (DNNs) often use both genuine\nas well as spurious features. In this work, we propose \"Amending Inherent\nInterpretability via Self-Supervised Masking\" (AIM), a simple yet interestingly\neffective method that promotes the network's utilization of genuine features\nover spurious alternatives without requiring additional annotations. In\nparticular, AIM uses features at multiple encoding stages to guide a\nself-supervised, sample-specific feature-masking process. As a result, AIM\nenables the training of well-performing and inherently interpretable models\nthat faithfully summarize the decision process. We validate AIM across a\ndiverse range of challenging datasets that test both out-of-distribution\ngeneralization and fine-grained visual understanding. These include\ngeneral-purpose classification benchmarks such as ImageNet100, HardImageNet,\nand ImageWoof, as well as fine-grained classification datasets such as\nWaterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual\nbenefits: interpretability improvements, as measured by the Energy Pointing\nGame (EPG) score, and accuracy gains over strong baselines. These consistent\ngains across domains and architectures provide compelling evidence that AIM\npromotes the use of genuine and meaningful features that directly contribute to\nimproved generalization and human-aligned interpretability.", "AI": {"tldr": "AIM\u901a\u8fc7\u81ea\u76d1\u7763\u63a9\u7801\u63d0\u5347DNNs\u5bf9\u771f\u5b9e\u7279\u5f81\u7684\u5229\u7528\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u5373\u53ef\u63d0\u9ad8\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u5e38\u540c\u65f6\u4f7f\u7528\u771f\u5b9e\u548c\u865a\u5047\u7279\u5f81\uff0cAIM\u65e8\u5728\u4fc3\u8fdb\u7f51\u7edc\u4f18\u5148\u5229\u7528\u771f\u5b9e\u7279\u5f81\uff0c\u63d0\u5347\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "AIM\u5229\u7528\u591a\u9636\u6bb5\u7f16\u7801\u7279\u5f81\u6307\u5bfc\u81ea\u76d1\u7763\u3001\u6837\u672c\u7279\u5b9a\u7684\u7279\u5f81\u63a9\u7801\u8fc7\u7a0b\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u5373\u53ef\u8bad\u7ec3\u51fa\u9ad8\u6027\u80fd\u4e14\u5185\u5728\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u3002", "result": "\u5728\u5305\u62ecImageNet100\u3001HardImageNet\u3001ImageWoof\u4ee5\u53ca\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6570\u636e\u96c6\uff08\u5982Waterbirds\u3001TravelingBirds\u3001CUB-200\uff09\u4e0a\uff0cAIM\u5728\u89e3\u91ca\u6027\uff08EPG\u5f97\u5206\uff09\u548c\u51c6\u786e\u6027\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "AIM\u65b9\u6cd5\u901a\u8fc7\u81ea\u76d1\u7763\u63a9\u7801\u8fc7\u7a0b\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u771f\u5b9e\u7279\u5f81\u800c\u975e\u865a\u5047\u7279\u5f81\uff0c\u4ece\u800c\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.11517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11517", "abs": "https://arxiv.org/abs/2508.11517", "authors": ["Shaoze Huang", "Qi Liu", "Chao Chen", "Yuhang Chen"], "title": "A Real-time Concrete Crack Detection and Segmentation Model Based on YOLOv11", "comment": null, "summary": "Accelerated aging of transportation infrastructure in the rapidly developing\nYangtze River Delta region necessitates efficient concrete crack detection, as\ncrack deterioration critically compromises structural integrity and regional\neconomic growth. To overcome the limitations of inefficient manual inspection\nand the suboptimal performance of existing deep learning models, particularly\nfor small-target crack detection within complex backgrounds, this paper\nproposes YOLOv11-KW-TA-FP, a multi-task concrete crack detection and\nsegmentation model based on the YOLOv11n architecture. The proposed model\nintegrates a three-stage optimization framework: (1) Embedding dynamic\nKernelWarehouse convolution (KWConv) within the backbone network to enhance\nfeature representation through a dynamic kernel sharing mechanism; (2)\nIncorporating a triple attention mechanism (TA) into the feature pyramid to\nstrengthen channel-spatial interaction modeling; and (3) Designing an FP-IoU\nloss function to facilitate adaptive bounding box regression penalization.\nExperimental validation demonstrates that the enhanced model achieves\nsignificant performance improvements over the baseline, attaining 91.3%\nprecision, 76.6% recall, and 86.4% mAP@50. Ablation studies confirm the\nsynergistic efficacy of the proposed modules. Furthermore, robustness tests\nindicate stable performance under conditions of data scarcity and noise\ninterference. This research delivers an efficient computer vision solution for\nautomated infrastructure inspection, exhibiting substantial practical\nengineering value.", "AI": {"tldr": "\u63d0\u51faYOLOv11-KW-TA-FP\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u5377\u79ef\u3001\u6ce8\u610f\u529b\u673a\u5236\u548c\u635f\u5931\u51fd\u6570\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u88c2\u7f1d\u68c0\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u5316\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u3002", "motivation": "\u957f\u6c5f\u4e09\u89d2\u6d32\u5730\u533a\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u7684\u5feb\u901f\u8001\u5316\u4f7f\u5f97\u9ad8\u6548\u6df7\u51dd\u571f\u88c2\u7f1d\u68c0\u6d4b\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\uff0c\u800c\u4f20\u7edf\u4eba\u5de5\u68c0\u6d4b\u6548\u7387\u4f4e\u4e0b\u4e14\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u590d\u6742\u80cc\u666f\u4e0b\u5c0f\u76ee\u6807\u88c2\u7f1d\u68c0\u6d4b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u4f18\u5316\u6846\u67b6\uff1a(1) \u5728\u9aa8\u5e72\u7f51\u7edc\u4e2d\u5d4c\u5165\u52a8\u6001KernelWarehouse\u5377\u79ef\uff08KWConv\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u6838\u5171\u4eab\u673a\u5236\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff1b(2) \u5728\u7279\u5f81\u91d1\u5b57\u5854\u4e2d\u5f15\u5165\u4e09\u91cd\u6ce8\u610f\u529b\u673a\u5236\uff08TA\uff09\uff0c\u4ee5\u52a0\u5f3a\u901a\u9053-\u7a7a\u95f4\u4ea4\u4e92\u5efa\u6a21\uff1b(3) \u8bbe\u8ba1FP-IoU\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u8fb9\u754c\u6846\u56de\u5f52\u60e9\u7f5a\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u589e\u5f3a\u6a21\u578b\u5728\u57fa\u7ebf\u57fa\u7840\u4e0a\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u8fbe\u523091.3%\u7684\u7cbe\u786e\u7387\u300176.6%\u7684\u53ec\u56de\u7387\u548c86.4%\u7684mAP@50\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u6a21\u5757\u7684\u534f\u540c\u6548\u679c\uff0c\u9c81\u68d2\u6027\u6d4b\u8bd5\u8868\u660e\u5728\u6570\u636e\u7a00\u7f3a\u548c\u566a\u58f0\u5e72\u6270\u4e0b\u6027\u80fd\u7a33\u5b9a\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv11n\u67b6\u6784\u7684\u591a\u4efb\u52a1\u6df7\u51dd\u571f\u88c2\u7f1d\u68c0\u6d4b\u4e0e\u5206\u5272\u6a21\u578bYOLOv11-KW-TA-FP\uff0c\u901a\u8fc7\u52a8\u6001KernelWarehouse\u5377\u79ef\u3001\u4e09\u91cd\u6ce8\u610f\u529b\u673a\u5236\u548cFP-IoU\u635f\u5931\u51fd\u6570\u7684\u4e09\u9636\u6bb5\u4f18\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u5316\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5de5\u7a0b\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.11531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11531", "abs": "https://arxiv.org/abs/2508.11531", "authors": ["Shilei Wang", "Gong Cheng", "Pujian Lai", "Dong Gao", "Junwei Han"], "title": "Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction", "comment": null, "summary": "Efficient trackers achieve faster runtime by reducing computational\ncomplexity and model parameters. However, this efficiency often compromises the\nexpense of weakened feature representation capacity, thus limiting their\nability to accurately capture target states using single-layer features. To\novercome this limitation, we propose Multi-State Tracker (MST), which utilizes\nhighly lightweight state-specific enhancement (SSE) to perform specialized\nenhancement on multi-state features produced by multi-state generation (MSG)\nand aggregates them in an interactive and adaptive manner using cross-state\ninteraction (CSI). This design greatly enhances feature representation while\nincurring minimal computational overhead, leading to improved tracking\nrobustness in complex environments. Specifically, the MSG generates multiple\nstate representations at multiple stages during feature extraction, while SSE\nrefines them to highlight target-specific features. The CSI module facilitates\ninformation exchange between these states and ensures the integration of\ncomplementary features. Notably, the introduced SSE and CSI modules adopt a\nhighly lightweight hidden state adaptation-based state space duality (HSA-SSD)\ndesign, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.\nExperimental results demonstrate that MST outperforms all previous efficient\ntrackers across multiple datasets, significantly improving tracking accuracy\nand robustness. In particular, it shows excellent runtime performance, with an\nAO score improvement of 4.5\\% over the previous SOTA efficient tracker HCAT on\nthe GOT-10K dataset. The code is available at https://github.com/wsumel/MST.", "AI": {"tldr": "MST\u901a\u8fc7\u8f7b\u91cf\u7ea7SSE\u548cCSI\u6a21\u5757\u589e\u5f3a\u591a\u72b6\u6001\u7279\u5f81\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u8ddf\u8e2a\u6027\u80fd\u4e14\u4fdd\u6301\u9ad8\u6548\uff0cGOT-10K\u6570\u636e\u96c6AO\u5206\u6570\u63d0\u53474.5%\u3002", "motivation": "\u73b0\u6709\u9ad8\u6548\u8ddf\u8e2a\u5668\u56e0\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u6a21\u578b\u53c2\u6570\u51cf\u5c11\u5bfc\u81f4\u7279\u5f81\u8868\u793a\u80fd\u529b\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8ddf\u8e2a\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faMulti-State Tracker (MST)\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7state-specific enhancement (SSE)\u548ccross-state interaction (CSI)\u6a21\u5757\uff0c\u901a\u8fc7multi-state generation (MSG)\u751f\u6210\u591a\u72b6\u6001\u7279\u5f81\u5e76\u4f18\u5316\u3002", "result": "MST\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u5f00\u9500\u4ec50.1 GFLOPs\u548c0.66 M\u53c2\u6570\uff0c\u8fd0\u884c\u6548\u7387\u9ad8\uff0cGOT-10K\u6570\u636e\u96c6AO\u5206\u6570\u63d0\u53474.5%\u3002", "conclusion": "MST\u901a\u8fc7\u8f7b\u91cf\u7ea7SSE\u548cCSI\u6a21\u5757\u663e\u8457\u63d0\u5347\u4e86\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u4ee5\u5f80\u7684\u9ad8\u6548\u8ddf\u8e2a\u5668\uff0c\u7279\u522b\u662f\u5728GOT-10K\u6570\u636e\u96c6\u4e0aAO\u5206\u6570\u63d0\u5347\u4e864.5%\u3002"}}
{"id": "2508.11532", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11532", "abs": "https://arxiv.org/abs/2508.11532", "authors": ["Jingsong Xia", "Yue Yin", "Xiuhan Li"], "title": "An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture", "comment": null, "summary": "Intelligent analysis of medical imaging plays a crucial role in assisting\nclinical diagnosis. However, achieving efficient and high-accuracy image\nclassification in resource-constrained computational environments remains\nchallenging. This study proposes a medical image classification method based on\nan improved ConvNeXt-Tiny architecture. Through structural optimization and\nloss function design, the proposed method enhances feature extraction\ncapability and classification performance while reducing computational\ncomplexity. Specifically, the method introduces a dual global pooling (Global\nAverage Pooling and Global Max Pooling) feature fusion strategy into the\nConvNeXt-Tiny backbone to simultaneously preserve global statistical features\nand salient response information. A lightweight channel attention module,\ntermed Squeeze-and-Excitation Vector (SEVector), is designed to improve the\nadaptive allocation of channel weights while minimizing parameter overhead.\nAdditionally, a Feature Smoothing Loss is incorporated into the loss function\nto enhance intra-class feature consistency and suppress intra-class variance.\nUnder CPU-only conditions (8 threads), the method achieves a maximum\nclassification accuracy of 89.10% on the test set within 10 training epochs,\nexhibiting a stable convergence trend in loss values. Experimental results\ndemonstrate that the proposed method effectively improves medical image\nclassification performance in resource-limited settings, providing a feasible\nand efficient solution for the deployment and promotion of medical imaging\nanalysis models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684ConvNeXt-Tiny\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5168\u5c40\u6c60\u5316\u548cSEVector\u6a21\u5757\u4f18\u5316\u7279\u5f81\u63d0\u53d6\uff0c\u7ed3\u5408\u7279\u5f81\u5e73\u6ed1\u635f\u5931\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u5728CPU\u73af\u5883\u4e0b\u5b9e\u73b0\u4e8689.10%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8ba1\u7b97\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u7684\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4f18\u5316\u73b0\u6709\u65b9\u6cd5\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u4f18\u5316\u548c\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff0c\u5f15\u5165\u4e86\u53cc\u5168\u5c40\u6c60\u5316\u7279\u5f81\u878d\u5408\u7b56\u7565\u548c\u8f7b\u91cf\u7ea7\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757SEVector\uff0c\u5e76\u7ed3\u5408\u7279\u5f81\u5e73\u6ed1\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u4ec5\u4f7f\u7528CPU\u7684\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u572810\u4e2a\u8bad\u7ec3\u5468\u671f\u5185\u8fbe\u5230\u4e8689.10%\u7684\u6700\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5e76\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u6536\u655b\u8d8b\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u6539\u8fdbConvNeXt-Tiny\u67b6\u6784\u7684\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u6a21\u578b\u7684\u90e8\u7f72\u548c\u63a8\u5e7f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11538", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11538", "abs": "https://arxiv.org/abs/2508.11538", "authors": ["Sitong Gong", "Lu Zhang", "Yunzhi Zhuge", "Xu Jia", "Pingping Zhang", "Huchuan Lu"], "title": "Reinforcing Video Reasoning Segmentation to Think Before It Segments", "comment": "12 pages", "summary": "Video reasoning segmentation (VRS) endeavors to delineate referred objects in\nvideos guided by implicit instructions that encapsulate human intent and\ntemporal logic. Previous approaches leverage large vision language models\n(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.\nHowever, this paradigm suffers from limited interpretability during inference\nand suboptimal performance due to inadequate spatiotemporal reasoning. Drawing\ninspiration from seminal breakthroughs in reinforcement learning, we introduce\nVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in\nsegmentation. Veason-R1 is trained through Group Relative Policy Optimization\n(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we\ncurate high-quality CoT training data to instill structured reasoning\ntrajectories, bridging video-level semantics and frame-level spatial grounding,\nyielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO\nfine-tuning encourages efficient exploration of the reasoning space by\noptimizing reasoning chains. To this end, we incorporate a holistic reward\nmechanism that synergistically enhances spatial alignment and temporal\nconsistency, bolstering keyframe localization and fine-grained grounding.\nComprehensive empirical evaluations demonstrate that Veason-R1 achieves\nstate-of-the-art performance on multiple benchmarks, surpassing prior art by\nsignificant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),\nwhile exhibiting robustness to hallucinations (+8.8 R). Our code and model\nweights will be available at Veason-R1.", "AI": {"tldr": "Veason-R1\u662f\u4e00\u79cd\u4e13\u4e3a\u89c6\u9891\u63a8\u7406\u5206\u5272\u8bbe\u8ba1\u7684LVLM\uff0c\u901a\u8fc7GRPO\u548cCoT\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u89e3\u91ca\u6027\u6709\u9650\uff0c\u4e14\u7531\u4e8e\u65f6\u7a7a\u63a8\u7406\u4e0d\u8db3\u5bfc\u81f4\u6027\u80fd\u6b20\u4f73\u3002\u53d7\u5f3a\u5316\u5b66\u4e60\u7a81\u7834\u542f\u53d1\uff0c\u63d0\u51faVeason-R1\u4ee5\u63d0\u5347\u5206\u5272\u4e2d\u7684\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u3002", "method": "Veason-R1\u662f\u4e00\u79cd\u4e13\u4e3aVRS\u8bbe\u8ba1\u7684LVLM\uff0c\u91c7\u7528Group Relative Policy Optimization (GRPO)\u548cChain-of-Thought (CoT)\u521d\u59cb\u5316\u8fdb\u884c\u8bad\u7ec3\u3002\u9996\u5148\u901a\u8fc7\u9ad8\u8d28\u91cfCoT\u6570\u636e\u8bad\u7ec3Veason-SFT\u6a21\u578b\uff0c\u7136\u540e\u901a\u8fc7GRPO\u4f18\u5316\u63a8\u7406\u94fe\u3002", "result": "Veason-R1\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff08\u5982ReVOS\u4e2dJ&F\u63d0\u53471.3\uff0cReasonVOS\u4e2d\u63d0\u534710.0\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u5e7b\u89c9\u7684\u9c81\u68d2\u6027\uff08R\u63d0\u53478.8\uff09\u3002", "conclusion": "Veason-R1\u901a\u8fc7GRPO\u548cCoT\u521d\u59cb\u5316\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u89c6\u9891\u63a8\u7406\u5206\u5272\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u6297\u5e7b\u89c9\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11550", "abs": "https://arxiv.org/abs/2508.11550", "authors": ["Zuo Zuo", "Jiahao Dong", "Yanyun Qu", "Zongze Wu"], "title": "Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model", "comment": null, "summary": "Industrial anomaly detection (AD) plays a significant role in manufacturing\nwhere a long-standing challenge is data scarcity. A growing body of works have\nemerged to address insufficient anomaly data via anomaly generation. However,\nthese anomaly generation methods suffer from lack of fidelity or need to be\ntrained with extra data. To this end, we propose a training-free anomaly\ngeneration framework dubbed AAG, which is based on Stable Diffusion (SD)'s\nstrong generation ability for effective anomaly image generation. Given a\nnormal image, mask and a simple text prompt, AAG can generate realistic and\nnatural anomalies in the specific regions and simultaneously keep contents in\nother regions unchanged. In particular, we propose Cross-Attention Enhancement\n(CAE) to re-engineer the cross-attention mechanism within Stable Diffusion\nbased on the given mask. CAE increases the similarity between visual tokens in\nspecific regions and text embeddings, which guides these generated visual\ntokens in accordance with the text description. Besides, generated anomalies\nneed to be more natural and plausible with object in given image. We propose\nSelf-Attention Enhancement (SAE) which improves similarity between each normal\nvisual token and anomaly visual tokens. SAE ensures that generated anomalies\nare coherent with original pattern. Extensive experiments on MVTec AD and VisA\ndatasets demonstrate effectiveness of AAG in anomaly generation and its\nutility. Furthermore, anomaly images generated by AAG can bolster performance\nof various downstream anomaly inspection tasks.", "AI": {"tldr": "AAG\u662f\u4e00\u79cd\u57fa\u4e8eStable Diffusion\u7684\u65e0\u8bad\u7ec3\u5f02\u5e38\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7CAE\u548cSAE\u6280\u672f\u751f\u6210\u903c\u771f\u5f02\u5e38\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u6570\u636e\u7a00\u7f3a\u662f\u4e00\u4e2a\u957f\u671f\u6311\u6218\uff0c\u73b0\u6709\u5f02\u5e38\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u4fdd\u771f\u5ea6\u6216\u9700\u8981\u989d\u5916\u8bad\u7ec3\u6570\u636e\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u65e0\u9700\u8bad\u7ec3\u7684AAG\u6846\u67b6\u3002", "method": "AAG\u57fa\u4e8eStable Diffusion\uff0c\u901a\u8fc7Cross-Attention Enhancement (CAE)\u548cSelf-Attention Enhancement (SAE)\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5728\u7279\u5b9a\u533a\u57df\u5185\u751f\u6210\u903c\u771f\u5f02\u5e38\u7684\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u533a\u57df\u5185\u5bb9\u4e0d\u53d8\u3002", "result": "\u5728MVTec AD\u548cVisA\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAAG\u5728\u5f02\u5e38\u751f\u6210\u65b9\u9762\u5177\u6709\u9ad8\u6548\u6027\uff0c\u4e14\u751f\u6210\u7684\u5f02\u5e38\u56fe\u50cf\u80fd\u63d0\u5347\u4e0b\u6e38\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "AAG\u6846\u67b6\u901a\u8fc7\u7ed3\u5408Stable Diffusion\u7684\u5f3a\u5927\u751f\u6210\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5f02\u5e38\u751f\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.11569", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11569", "abs": "https://arxiv.org/abs/2508.11569", "authors": ["Zheng Wang", "Shihao Xu", "Wei Shi"], "title": "TrajSV: A Trajectory-based Model for Sports Video Representations and Applications", "comment": "This paper has been accepted by TCSVT", "summary": "Sports analytics has received significant attention from both academia and\nindustry in recent years. Despite the growing interest and efforts in this\nfield, several issues remain unresolved, including (1) data unavailability, (2)\nlack of an effective trajectory-based framework, and (3) requirement for\nsufficient supervision labels. In this paper, we present TrajSV, a\ntrajectory-based framework that addresses various issues in existing studies.\nTrajSV comprises three components: data preprocessing, Clip Representation\nNetwork (CRNet), and Video Representation Network (VRNet). The data\npreprocessing module extracts player and ball trajectories from sports\nbroadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to\nlearn clip representations based on these trajectories. Additionally, VRNet\nlearns video representations by aggregating clip representations and visual\nfeatures with an encoder-decoder architecture. Finally, a triple contrastive\nloss is introduced to optimize both video and clip representations in an\nunsupervised manner. The experiments are conducted on three broadcast video\ndatasets to verify the effectiveness of TrajSV for three types of sports (i.e.,\nsoccer, basketball, and volleyball) with three downstream applications (i.e.,\nsports video retrieval, action spotting, and video captioning). The results\ndemonstrate that TrajSV achieves state-of-the-art performance in sports video\nretrieval, showcasing a nearly 70% improvement. It outperforms baselines in\naction spotting, achieving state-of-the-art results in 9 out of 17 action\ncategories, and demonstrates a nearly 20% improvement in video captioning.\nAdditionally, we introduce a deployed system along with the three applications\nbased on TrajSV.", "AI": {"tldr": "TrajSV\u662f\u4e00\u4e2a\u57fa\u4e8e\u8f68\u8ff9\u7684\u4f53\u80b2\u89c6\u9891\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u548cTransformer\u6a21\u5757\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u68c0\u7d22\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u5b57\u5e55\u751f\u6210\u7684\u6027\u80fd\u3002", "motivation": "\u4f53\u80b2\u5206\u6790\u9886\u57df\u5b58\u5728\u6570\u636e\u4e0d\u53ef\u7528\u3001\u7f3a\u4e4f\u6709\u6548\u7684\u8f68\u8ff9\u6846\u67b6\u548c\u76d1\u7763\u6807\u7b7e\u4e0d\u8db3\u7b49\u95ee\u9898\uff0cTrajSV\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "TrajSV\u5305\u542b\u6570\u636e\u9884\u5904\u7406\u3001Clip Representation Network (CRNet)\u548cVideo Representation Network (VRNet)\u4e09\u4e2a\u6a21\u5757\uff0c\u91c7\u7528\u8f68\u8ff9\u589e\u5f3a\u7684Transformer\u6a21\u5757\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u4f18\u5316\u89c6\u9891\u548c\u7247\u6bb5\u8868\u793a\u3002", "result": "\u5728\u4e09\u79cd\u4f53\u80b2\u89c6\u9891\u6570\u636e\u96c6\u4e0a\uff0cTrajSV\u5728\u89c6\u9891\u68c0\u7d22\u4e2d\u63d0\u5347\u8fd170%\uff0c\u52a8\u4f5c\u8bc6\u522b\u4e2d17\u4e2a\u7c7b\u522b\u4e2d9\u4e2a\u8fbe\u5230\u6700\u4f73\uff0c\u89c6\u9891\u5b57\u5e55\u751f\u6210\u63d0\u5347\u8fd120%\u3002", "conclusion": "TrajSV\u6846\u67b6\u901a\u8fc7\u8f68\u8ff9\u589e\u5f3a\u7684Transformer\u6a21\u5757\u548c\u4e09\u91cd\u5bf9\u6bd4\u635f\u5931\u4f18\u5316\uff0c\u5728\u4f53\u80b2\u89c6\u9891\u68c0\u7d22\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u89c6\u9891\u5b57\u5e55\u751f\u6210\u7b49\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.11576", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11576", "abs": "https://arxiv.org/abs/2508.11576", "authors": ["Yumeng Shi", "Quanyu Long", "Yin Wu", "Wenya Wang"], "title": "Causality Matters: How Temporal Information Emerges in Video Language Models", "comment": null, "summary": "Video language models (VideoLMs) have made significant progress in multimodal\nunderstanding. However, temporal understanding, which involves identifying\nevent order, duration, and relationships across time, still remains a core\nchallenge. Prior works emphasize positional encodings (PEs) as a key mechanism\nfor encoding temporal structure. Surprisingly, we find that removing or\nmodifying PEs in video inputs yields minimal degradation in the performance of\ntemporal understanding. In contrast, reversing the frame sequence while\npreserving the original PEs causes a substantial drop. To explain this\nbehavior, we conduct substantial analysis experiments to trace how temporal\ninformation is integrated within the model. We uncover a causal information\npathway: temporal cues are progressively synthesized through inter-frame\nattention, aggregated in the final frame, and subsequently integrated into the\nquery tokens. This emergent mechanism shows that temporal reasoning emerges\nfrom inter-visual token interactions under the constraints of causal attention,\nwhich implicitly encodes temporal structure. Based on these insights, we\npropose two efficiency-oriented strategies: staged cross-modal attention and a\ntemporal exit mechanism for early token truncation. Experiments on two\nbenchmarks validate the effectiveness of both approaches. To the best of our\nknowledge, this is the first work to systematically investigate video temporal\nunderstanding in VideoLMs, offering insights for future model improvement.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u63a8\u7406\u901a\u8fc7\u5e27\u95f4\u6ce8\u610f\u529b\u9010\u6b65\u5408\u6210\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u6548\u7387\u7b56\u7565\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u65f6\u95f4\u7406\u89e3\uff08\u5982\u4e8b\u4ef6\u987a\u5e8f\u3001\u6301\u7eed\u65f6\u95f4\u548c\u8de8\u65f6\u95f4\u5173\u7cfb\uff09\u4ecd\u662f\u6838\u5fc3\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u65f6\u95f4\u4fe1\u606f\u5728\u6a21\u578b\u4e2d\u7684\u6574\u5408\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5b9e\u9a8c\u8ffd\u8e2a\u65f6\u95f4\u4fe1\u606f\u5728\u6a21\u578b\u4e2d\u7684\u6574\u5408\u65b9\u5f0f\uff0c\u53d1\u73b0\u65f6\u95f4\u7ebf\u7d22\u901a\u8fc7\u5e27\u95f4\u6ce8\u610f\u529b\u9010\u6b65\u5408\u6210\uff0c\u5e76\u5728\u6700\u540e\u4e00\u5e27\u4e2d\u805a\u5408\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u5206\u9636\u6bb5\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u548c\u65f6\u95f4\u9000\u51fa\u673a\u5236\u3002", "result": "\u53bb\u9664\u6216\u4fee\u6539\u89c6\u9891\u8f93\u5165\u4e2d\u7684\u4f4d\u7f6e\u7f16\u7801\u5bf9\u65f6\u95f4\u7406\u89e3\u7684\u6027\u80fd\u5f71\u54cd\u5f88\u5c0f\uff0c\u800c\u53cd\u8f6c\u5e27\u5e8f\u5217\u4f1a\u5bfc\u81f4\u663e\u8457\u4e0b\u964d\u3002\u63d0\u51fa\u7684\u4e24\u79cd\u7b56\u7565\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u65f6\u95f4\u63a8\u7406\u7684\u56e0\u679c\u4fe1\u606f\u8def\u5f84\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u6548\u7387\u5bfc\u5411\u7684\u7b56\u7565\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11591", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.11591", "abs": "https://arxiv.org/abs/2508.11591", "authors": ["Durga Joshi", "Chandi Witharana", "Robert Fahey", "Thomas Worthley", "Zhe Zhu", "Diego Cerrai"], "title": "DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring", "comment": "35 Pages, 15 figures", "summary": "Our study introduces a novel, low-cost, and reproducible framework for\nreal-time, object-level structural assessment and geolocation of roadside\nvegetation and infrastructure with commonly available but underutilized\ndashboard camera (dashcam) video data. We developed an end-to-end pipeline that\ncombines monocular depth estimation, depth error correction, and geometric\ntriangulation to generate accurate spatial and structural data from\nstreet-level video streams from vehicle-mounted dashcams. Depth maps were first\nestimated using a state-of-the-art monocular depth model, then refined via a\ngradient-boosted regression framework to correct underestimations, particularly\nfor distant objects. The depth correction model achieved strong predictive\nperformance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly\nreducing bias beyond 15 m. Further, object locations were estimated using\nGPS-based triangulation, while object heights were calculated using pin hole\ncamera geometry. Our method was evaluated under varying conditions of camera\nplacement and vehicle speed. Low-speed vehicle with inside camera gave the\nhighest accuracy, with mean geolocation error of 2.83 m, and mean absolute\nerror (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To\nthe best of our knowledge, it is the first framework to combine monocular depth\nmodeling, triangulated GPS-based geolocation, and real-time structural\nassessment for urban vegetation and infrastructure using consumer-grade video\ndata. Our approach complements conventional RS methods, such as LiDAR and image\nby offering a fast, real-time, and cost-effective solution for object-level\nmonitoring of vegetation risks and infrastructure exposure, making it\nespecially valuable for utility companies, and urban planners aiming for\nscalable and frequent assessments in dynamic urban environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u8f66\u8f7d\u6444\u50cf\u5934\u89c6\u9891\u6570\u636e\uff0c\u901a\u8fc7\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u51e0\u4f55\u4e09\u89d2\u6d4b\u91cf\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u5b9e\u65f6\u7684\u5bf9\u8c61\u7ea7\u7ed3\u6784\u8bc4\u4f30\u548c\u5730\u7406\u5b9a\u4f4d\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u57ce\u5e02\u690d\u88ab\u548c\u57fa\u7840\u8bbe\u65bd\u76d1\u6d4b\u3002", "motivation": "\u4f20\u7edf\u9065\u611f\u65b9\u6cd5\u5982LiDAR\u548c\u56fe\u50cf\u6210\u672c\u9ad8\u4e14\u4e0d\u5b9e\u65f6\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u6d88\u8d39\u8005\u7ea7\u89c6\u9891\u6570\u636e\u63d0\u4f9b\u4e00\u79cd\u5feb\u901f\u3001\u5b9e\u65f6\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5bf9\u8c61\u7ea7\u76d1\u6d4b\u690d\u88ab\u98ce\u9669\u548c\u57fa\u7840\u8bbe\u65bd\u66b4\u9732\u3002", "method": "\u7ed3\u5408\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u6df1\u5ea6\u8bef\u5dee\u6821\u6b63\u548c\u51e0\u4f55\u4e09\u89d2\u6d4b\u91cf\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u4ece\u8f66\u8f7d\u6444\u50cf\u5934\u7684\u8857\u7ea7\u89c6\u9891\u6d41\u4e2d\u751f\u6210\u51c6\u786e\u7684\u7a7a\u95f4\u548c\u7ed3\u6784\u6570\u636e\u3002\u6df1\u5ea6\u56fe\u9996\u5148\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u5355\u76ee\u6df1\u5ea6\u6a21\u578b\u4f30\u8ba1\uff0c\u7136\u540e\u901a\u8fc7\u68af\u5ea6\u63d0\u5347\u56de\u5f52\u6846\u67b6\u8fdb\u884c\u7ec6\u5316\u4ee5\u6821\u6b63\u4f4e\u4f30\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8fdc\u5904\u7269\u4f53\u3002", "result": "\u6df1\u5ea6\u6821\u6b63\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9884\u6d4b\u6027\u80fd\uff08R2 = 0.92\uff0cMAE = 0.31\uff09\uff0c\u663e\u8457\u51cf\u5c11\u4e8615\u7c73\u4ee5\u5916\u7684\u504f\u5dee\u3002\u5bf9\u8c61\u4f4d\u7f6e\u901a\u8fc7\u57fa\u4e8eGPS\u7684\u4e09\u89d2\u6d4b\u91cf\u4f30\u8ba1\uff0c\u5bf9\u8c61\u9ad8\u5ea6\u901a\u8fc7\u9488\u5b54\u76f8\u673a\u51e0\u4f55\u8ba1\u7b97\u3002\u5728\u4f4e\u901f\u8f66\u8f86\u548c\u5185\u90e8\u6444\u50cf\u5934\u6761\u4ef6\u4e0b\uff0c\u5730\u7406\u5b9a\u4f4d\u8bef\u5dee\u5e73\u5747\u4e3a2.83\u7c73\uff0c\u9ad8\u5ea6\u4f30\u8ba1\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u6811\u6728\u4e3a2.09\u7c73\uff0c\u6746\u4e3a0.88\u7c73\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u53ef\u590d\u5236\u7684\u6846\u67b6\uff0c\u5229\u7528\u8f66\u8f7d\u6444\u50cf\u5934\u89c6\u9891\u6570\u636e\u8fdb\u884c\u5b9e\u65f6\u3001\u5bf9\u8c61\u7ea7\u7684\u7ed3\u6784\u8bc4\u4f30\u548c\u5730\u7406\u5b9a\u4f4d\uff0c\u4e3a\u57ce\u5e02\u690d\u88ab\u548c\u57fa\u7840\u8bbe\u65bd\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u5b9e\u65f6\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u516c\u7528\u4e8b\u4e1a\u516c\u53f8\u548c\u57ce\u5e02\u89c4\u5212\u8005\u3002"}}
{"id": "2508.11603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11603", "abs": "https://arxiv.org/abs/2508.11603", "authors": ["Zhe Zhu", "Honghua Chen", "Peng Li", "Mingqiang Wei"], "title": "CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion", "comment": null, "summary": "Text-driven 3D editing seeks to modify 3D scenes according to textual\ndescriptions, and most existing approaches tackle this by adapting pre-trained\n2D image editors to multi-view inputs. However, without explicit control over\nmulti-view information exchange, they often fail to maintain cross-view\nconsistency, leading to insufficient edits and blurry details. We introduce\nCoreEditor, a novel framework for consistent text-to-3D editing. The key\ninnovation is a correspondence-constrained attention mechanism that enforces\nprecise interactions between pixels expected to remain consistent throughout\nthe diffusion denoising process. Beyond relying solely on geometric alignment,\nwe further incorporate semantic similarity estimated during denoising, enabling\nmore reliable correspondence modeling and robust multi-view editing. In\naddition, we design a selective editing pipeline that allows users to choose\npreferred results from multiple candidates, offering greater flexibility and\nuser control. Extensive experiments show that CoreEditor produces high-quality,\n3D-consistent edits with sharper details, significantly outperforming prior\nmethods.", "AI": {"tldr": "CoreEditor\u901a\u8fc7\u5bf9\u5e94\u7ea6\u675f\u6ce8\u610f\u529b\u673a\u5236\u548c\u9009\u62e9\u6027\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6587\u672c\u9a71\u52a83D\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u89c6\u56fe\u4fe1\u606f\u4ea4\u6362\u4e0a\u7f3a\u4e4f\u663e\u5f0f\u63a7\u5236\uff0c\u5bfc\u81f4\u7f16\u8f91\u4e0d\u8db3\u548c\u7ec6\u8282\u6a21\u7cca\u3002", "method": "\u63d0\u51fa\u4e86CoreEditor\u6846\u67b6\uff0c\u91c7\u7528\u5bf9\u5e94\u7ea6\u675f\u7684\u6ce8\u610f\u529b\u673a\u5236\u7ed3\u5408\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u589e\u5f3a\u591a\u89c6\u56fe\u4e00\u81f4\u6027\uff0c\u5e76\u8bbe\u8ba1\u9009\u62e9\u6027\u7f16\u8f91\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoreEditor\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u30013D\u4e00\u81f4\u7684\u7f16\u8f91\u7ed3\u679c\uff0c\u7ec6\u8282\u66f4\u6e05\u6670\u3002", "conclusion": "CoreEditor\u901a\u8fc7\u5f15\u5165\u5bf9\u5e94\u7ea6\u675f\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u9009\u62e9\u6027\u7f16\u8f91\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u9a71\u52a83D\u7f16\u8f91\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.11624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11624", "abs": "https://arxiv.org/abs/2508.11624", "authors": ["Niki Foteinopoulou", "Ignas Budvytis", "Stephan Liwicki"], "title": "LoRAtorio: An intrinsic approach to LoRA Skill Composition", "comment": "32 pages, 17 figures", "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted technique in\ntext-to-image diffusion models, enabling the personalisation of visual concepts\nsuch as characters, styles, and objects. However, existing approaches struggle\nto effectively compose multiple LoRA adapters, particularly in open-ended\nsettings where the number and nature of required skills are not known in\nadvance. In this work, we present LoRAtorio, a novel train-free framework for\nmulti-LoRA composition that leverages intrinsic model behaviour. Our method is\nmotivated by two key observations: (1) LoRA adapters trained on narrow domains\nproduce denoised outputs that diverge from the base model, and (2) when\noperating out-of-distribution, LoRA outputs show behaviour closer to the base\nmodel than when conditioned in distribution. The balance between these two\nobservations allows for exceptional performance in the single LoRA scenario,\nwhich nevertheless deteriorates when multiple LoRAs are loaded. Our method\noperates in the latent space by dividing it into spatial patches and computing\ncosine similarity between each patch's predicted noise and that of the base\nmodel. These similarities are used to construct a spatially-aware weight\nmatrix, which guides a weighted aggregation of LoRA outputs. To address domain\ndrift, we further propose a modification to classifier-free guidance that\nincorporates the base model's unconditional score into the composition. We\nextend this formulation to a dynamic module selection setting, enabling\ninference-time selection of relevant LoRA adapters from a large pool. LoRAtorio\nachieves state-of-the-art performance, showing up to a 1.3% improvement in\nClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises\neffectively to multiple latent diffusion models.", "AI": {"tldr": "LoRAtorio\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u591aLoRA\u7ec4\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u6743\u91cd\u77e9\u9635\u548c\u4fee\u6539\u7684\u5206\u7c7b\u5668\u65e0\u5173\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5f00\u653e\u73af\u5883\u4e2d\u96be\u4ee5\u6709\u6548\u7ec4\u5408\u591a\u4e2aLoRA\u9002\u914d\u5668\uff0c\u7279\u522b\u662f\u5728\u6240\u9700\u6280\u80fd\u7684\u6570\u91cf\u548c\u6027\u8d28\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\u3002LoRAtorio\u7684\u63d0\u51fa\u57fa\u4e8e\u4e24\u4e2a\u5173\u952e\u89c2\u5bdf\uff1a\u7a84\u57df\u8bad\u7ec3\u7684LoRA\u9002\u914d\u5668\u4ea7\u751f\u7684\u53bb\u566a\u8f93\u51fa\u4e0e\u57fa\u7840\u6a21\u578b\u5b58\u5728\u5dee\u5f02\uff0c\u4ee5\u53ca\u5728\u5206\u5e03\u5916\u64cd\u4f5c\u65f6LoRA\u8f93\u51fa\u66f4\u63a5\u8fd1\u57fa\u7840\u6a21\u578b\u884c\u4e3a\u3002", "method": "\u8be5\u65b9\u6cd5\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u901a\u8fc7\u5212\u5206\u7a7a\u95f4\u8865\u4e01\u5e76\u8ba1\u7b97\u6bcf\u4e2a\u8865\u4e01\u9884\u6d4b\u566a\u58f0\u4e0e\u57fa\u7840\u6a21\u578b\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u6784\u5efa\u7a7a\u95f4\u611f\u77e5\u6743\u91cd\u77e9\u9635\uff0c\u6307\u5bfcLoRA\u8f93\u51fa\u7684\u52a0\u6743\u805a\u5408\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u5bf9\u5206\u7c7b\u5668\u65e0\u5173\u5f15\u5bfc\u7684\u4fee\u6539\uff0c\u4ee5\u89e3\u51b3\u9886\u57df\u6f02\u79fb\u95ee\u9898\u3002", "result": "LoRAtorio\u5728ClipScore\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe1.3%\u7684\u63d0\u5347\uff0c\u5728GPT-4V\u6210\u5bf9\u8bc4\u4f30\u4e2d\u83b7\u5f97\u4e8672.43%\u7684\u80dc\u7387\uff0c\u5e76\u5728\u591a\u79cd\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LoRAtorio\u901a\u8fc7\u5229\u7528\u5185\u5728\u6a21\u578b\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u591aLoRA\u7ec4\u5408\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u5728\u591a\u79cd\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.11630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11630", "abs": "https://arxiv.org/abs/2508.11630", "authors": ["Yi-Fan Zhang", "Xingyu Lu", "Shukang Yin", "Chaoyou Fu", "Wei Chen", "Xiao Hu", "Bin Wen", "Kaiyu Jiang", "Changyi Liu", "Tianke Zhang", "Haonan Fan", "Kaibing Chen", "Jiankang Chen", "Haojie Ding", "Kaiyu Tang", "Zhang Zhang", "Liang Wang", "Fan Yang", "Tingting Gao", "Guorui Zhou"], "title": "Thyme: Think Beyond Images", "comment": "Project page: https://thyme-vl.github.io/", "summary": "Following OpenAI's introduction of the ``thinking with images'' concept,\nrecent efforts have explored stimulating the use of visual information in the\nreasoning process to enhance model performance in perception and reasoning\ntasks. However, to the best of our knowledge, no open-source work currently\noffers a feature set as rich as proprietary models (O3), which can perform\ndiverse image manipulations and simultaneously enhance logical reasoning\ncapabilities through code. In this paper, we make a preliminary attempt in this\ndirection by introducing Thyme (Think Beyond Images), a novel paradigm for\nenabling MLLMs to transcend existing ``think with images'' approaches by\nautonomously generating and executing diverse image processing and\ncomputational operations via executable code. This approach not only\nfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,\nrotation, contrast enhancement) but also allows for mathematical computations,\nall while maintaining high autonomy in deciding when and how to apply these\noperations. We activate this capability through a two-stage training strategy:\nan initial SFT on a curated dataset of 500K samples to teach code generation,\nfollowed by a RL phase to refine decision-making. For the RL stage, we manually\ncollect and design high-resolution question-answer pairs to increase the\nlearning difficulty, and we propose GRPO-ATS (Group Relative Policy\nOptimization with Adaptive Temperature Sampling), an algorithm that applies\ndistinct temperatures to text and code generation to balance reasoning\nexploration with code execution precision. We conduct extensive experimental\nanalysis and ablation studies. Comprehensive evaluations on nearly 20\nbenchmarks show that Thyme yields significant and consistent performance gains,\nparticularly in challenging high-resolution perception and complex reasoning\ntasks.", "AI": {"tldr": "Thyme \u662f\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u548c\u6267\u884c\u589e\u5f3a MLLMs \u7684\u56fe\u50cf\u5904\u7406\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08SFT+RL\uff09\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90\u6a21\u578b\u5728\u56fe\u50cf\u64cd\u4f5c\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\u4e0a\u4e0d\u53ca\u4e13\u6709\u6a21\u578b\uff08\u5982 O3\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7801\u589e\u5f3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u63a8\u7406\u548c\u64cd\u4f5c\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u521d\u59cb SFT \u9636\u6bb5\u5728 50 \u4e07\u6837\u672c\u7684\u7cbe\u9009\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u968f\u540e\u901a\u8fc7 RL \u9636\u6bb5\uff08\u4f7f\u7528 GRPO-ATS \u7b97\u6cd5\uff09\u4f18\u5316\u51b3\u7b56\u80fd\u529b\u3002", "result": "\u5728\u8fd1 20 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cThyme \u8868\u73b0\u51fa\u663e\u8457\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5206\u8fa8\u7387\u611f\u77e5\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u3002", "conclusion": "Thyme \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u81ea\u4e3b\u751f\u6210\u548c\u6267\u884c\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u4f7f MLLMs \u8d85\u8d8a\u73b0\u6709\u7684\u201c\u56fe\u50cf\u601d\u7ef4\u201d\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u9ad8\u5206\u8fa8\u7387\u611f\u77e5\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
