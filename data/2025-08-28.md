<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.RO](#cs.RO) [Total: 23]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.DS](#cs.DS) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration](https://arxiv.org/abs/2508.19254)
*Jookyung Song,Mookyoung Kang,Nojun Kwak*

Main category: cs.CV

TL;DR: 本文提出了一种实时生成绘图系统，通过同时分析形式意图和上下文意图，实现了低延迟的共同创作。方法结合了多阶段生成和分布式推理，支持多用户协作。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于文本提示的生成系统无法同时捕捉高层次语义和低层次几何特征的问题。

Method: 采用多阶段生成流水线，结合轮廓保持的结构控制与风格和内容感知的图像合成，并通过触摸屏界面和分布式推理架构实现。

Result: 实现了低延迟的两阶段转换，支持多用户在共享画布上的协作。

Conclusion: 该系统通过整合形式意图和上下文意图，重新定义了人机交互为共同创作和相互增强的过程。

Abstract: This paper presents a real-time generative drawing system that interprets and
integrates both formal intent - the structural, compositional, and stylistic
attributes of a sketch - and contextual intent - the semantic and thematic
meaning inferred from its visual content - into a unified transformation
process. Unlike conventional text-prompt-based generative systems, which
primarily capture high-level contextual descriptions, our approach
simultaneously analyzes ground-level intuitive geometric features such as line
trajectories, proportions, and spatial arrangement, and high-level semantic
cues extracted via vision-language models. These dual intent signals are
jointly conditioned in a multi-stage generation pipeline that combines
contour-preserving structural control with style- and content-aware image
synthesis. Implemented with a touchscreen-based interface and distributed
inference architecture, the system achieves low-latency, two-stage
transformation while supporting multi-user collaboration on shared canvases.
The resulting platform enables participants, regardless of artistic expertise,
to engage in synchronous, co-authored visual creation, redefining human-AI
interaction as a process of co-creation and mutual enhancement.

</details>


### [2] [Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images](https://arxiv.org/abs/2508.20080)
*Changha Shin,Woong Oh Cho,Seon Joo Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种新校准框架，通过优化双鱼眼相机的镜头间隙和角度失真，显著提升了360度图像的渲染质量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 消费者级双鱼眼系统由于固有的镜头分离和角度失真，生成的360度全景图存在缺陷，影响了虚拟现实、机器人和自主导航等应用的效果。

Method: 论文引入了一种结合双鱼眼相机模型的3D高斯溅射管道校准框架，通过联合优化3D高斯参数和校准变量（模拟镜头间隙和角度失真）来实现。

Result: 在真实数据集上的广泛评估表明，该方法能够从不完美图像中生成无缝渲染，且性能优于现有的360度渲染模型。

Conclusion: 该论文提出的新颖校准框架通过将双鱼眼相机模型融入3D高斯溅射管道，显著提升了360度视觉内容的渲染质量，能够从不完美的全景输入中生成无缝的新视角合成。

Abstract: 360-degree visual content is widely shared on platforms such as YouTube and
plays a central role in virtual reality, robotics, and autonomous navigation.
However, consumer-grade dual-fisheye systems consistently yield imperfect
panoramas due to inherent lens separation and angular distortions. In this
work, we introduce a novel calibration framework that incorporates a
dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach
not only simulates the realistic visual artifacts produced by dual-fisheye
cameras but also enables the synthesis of seamlessly rendered 360-degree
images. By jointly optimizing 3D Gaussian parameters alongside calibration
variables that emulate lens gaps and angular distortions, our framework
transforms imperfect omnidirectional inputs into flawless novel view synthesis.
Extensive evaluations on real-world datasets confirm that our method produces
seamless renderings-even from imperfect images-and outperforms existing
360-degree rendering models.

</details>


### [3] [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)
*Chenghao Liu,Jiachen Zhang,Chengxuan Li,Zhimu Zhou,Shixin Wu,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: TTF是一种无需训练的时间令牌融合方法，通过历史与当前视觉表示的结合提升VLA模型性能，实验证明其跨环境和架构的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在机器人操作任务中逐帧处理视觉输入，忽略了时间连贯性，易受视觉噪声影响。

Method: 提出了Temporal Token Fusion (TTF)，一种无需训练的方法，通过结合历史与当前视觉表示，采用双维度检测（灰度像素差异分析和基于注意力的语义相关性评估）和硬融合策略及关键帧锚定来增强推理质量。

Result: 在LIBERO、SimplerEnv和真实机器人任务中，TTF分别实现了4.0、4.8和8.7%的相对性能提升，证明了其模型无关性。

Conclusion: TTF方法通过选择性时间令牌融合显著提升了VLA模型的推理质量，证明了其在跨环境和跨架构中的通用性，并为KQV矩阵重用策略提供了新方向。

Abstract: Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.

</details>


### [4] [Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation](https://arxiv.org/abs/2508.19289)
*Tai Inui,Steven Oh,Magdeline Kuan*

Main category: cs.CV

TL;DR: 提出了一种结合专家视觉指标与CLIP-ViT嵌入的无监督幻灯片质量评估方法，显著优于现有模型，适用于实时反馈。


<details>
  <summary>Details</summary>
Motivation: 为了解决幻灯片质量评估的客观性和可扩展性问题，提出了一种结合专家启发指标与多模态嵌入的方法。

Method: 结合七个专家启发的视觉设计指标（如空白、色彩丰富度、边缘密度等）与CLIP-ViT嵌入，使用Isolation Forest进行异常评分。

Result: 在12k专业讲座幻灯片上训练，并在115张学术演讲幻灯片上评估，Pearson相关系数最高达0.83，显著优于现有视觉语言模型。

Conclusion: 通过结合专家启发的视觉设计指标与CLIP-ViT嵌入，并采用基于Isolation Forest的异常评分，该方法能够有效评估幻灯片质量，为实时、可扩展的客观反馈提供了可能。

Abstract: We present an unsupervised slide-quality assessment pipeline that combines
seven expert-inspired visual-design metrics (whitespace, colorfulness, edge
density, brightness contrast, text density, color harmony, layout balance) with
CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate
presentation slides. Trained on 12k professional lecture slides and evaluated
on six academic talks (115 slides), our method achieved Pearson correlations up
to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores
from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude
Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual
ratings, discriminant validity against speaker-delivery scores, and exploratory
alignment with overall impressions. Our results show that augmenting low-level
design cues with multimodal embeddings closely approximates audience
perceptions of slide quality, enabling scalable, objective feedback in real
time.

</details>


### [5] [Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation](https://arxiv.org/abs/2508.19290)
*Alexandros Gkillas,Ioulia Kapsali,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: 论文提出了一种针对2D范围视图LiDAR分割的高效对抗防御框架，通过可解释净化网络实现强鲁棒性，实验验证其优于现有方法且适用于实际自动驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 现代分割网络对对抗攻击高度敏感，可能危及自动驾驶的安全性。现有防御大多针对原始3D点云设计，依赖计算密集型生成模型，而2D范围视图的轻量级防御研究不足。

Method: 论文提出了一种基于模型的高效净化框架，专门针对2D范围视图LiDAR分割的对抗防御。通过数学优化的可解释净化网络，实现了强对抗鲁棒性且计算开销最小。

Result: 该方法在公开基准测试中表现优异，显著优于生成模型和对抗训练基线，并在实际演示车辆部署中验证了其有效性。

Conclusion: 该论文提出的轻量级对抗防御框架在2D范围视图LiDAR分割中表现出色，不仅在实际自动驾驶场景中验证了其准确性，还在公开基准测试中超越了生成模型和对抗训练基线。

Abstract: LiDAR-based segmentation is essential for reliable perception in autonomous
vehicles, yet modern segmentation networks are highly susceptible to
adversarial attacks that can compromise safety. Most existing defenses are
designed for networks operating directly on raw 3D point clouds and rely on
large, computationally intensive generative models. However, many
state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D
range view representations. Despite their widespread adoption, dedicated
lightweight adversarial defenses for this domain remain largely unexplored. We
introduce an efficient model-based purification framework tailored for
adversarial defense in 2D range-view LiDAR segmentation. We propose a direct
attack formulation in the range-view domain and develop an explainable
purification network based on a mathematical justified optimization problem,
achieving strong adversarial resilience with minimal computational overhead.
Our method achieves competitive performance on open benchmarks, consistently
outperforming generative and adversarial training baselines. More importantly,
real-world deployment on a demo vehicle demonstrates the framework's ability to
deliver accurate operation in practical autonomous driving scenarios.

</details>


### [6] [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: 本文综述了大视觉语言模型（LVLMs）在物体检测中的革命性作用，展示了其在适应性、上下文理解等方面的优势，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 探索大视觉语言模型（LVLMs）如何通过融合语言和视觉增强物体检测的适应性、上下文推理和泛化能力，超越传统架构。

Method: 通过三步研究回顾过程系统性地探索了LVLMs的最新技术，包括讨论视觉语言模型（VLMs）在物体检测中的功能、解释架构创新、训练范式及输出灵活性，并比较了LVLMs与传统深度学习系统的实时性能、适应性和复杂性。

Result: LVLMs在多样场景中表现出高效性，预计将很快达到或超越传统方法在物体检测中的性能，同时指出了当前LVLMs的主要局限并提出了解决方案。

Conclusion: 近期在大视觉语言模型（LVLMs）上的进展已经并将持续对物体检测和机器人应用产生变革性影响。

Abstract: The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.

</details>


### [7] [Large VLM-based Stylized Sports Captioning](https://arxiv.org/abs/2508.19295)
*Sauptik Dhar,Nicholas Buoncristiani,Joe Anakata,Haoyu Zhang,Michelle Munson*

Main category: cs.CV

TL;DR: 本文提出了一种两级微调的LVLM流水线，用于生成风格化的体育比赛描述，显著提升了生成质量，并在实际应用中验证了其效能。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视觉语言模型在体育领域的应用中缺乏足够的领域特定术语，无法生成自然的人类风格描述。

Method: 提出了一种两级微调的LVLM流水线，用于从图像中生成风格化的体育比赛描述。

Result: 与替代方法相比，F1分数提高了8-10%，BERT分数提高了2-10%，且具有较小的运行时内存占用和快速执行时间。在超级碗LIX期间，流水线成功应用于实时专业体育新闻，生成高准确率和风格化的描述。

Conclusion: 所提出的两级微调LVLM流水线在实时体育新闻应用中表现出色，能够以高准确率和风格化格式生成体育比赛描述，证明了其实际应用的可行性。

Abstract: The advent of large (visual) language models (LLM / LVLM) have led to a
deluge of automated human-like systems in several domains including social
media content generation, search and recommendation, healthcare prognosis, AI
assistants for cognitive tasks etc. Although these systems have been
successfully integrated in production; very little focus has been placed on
sports, particularly accurate identification and natural language description
of the game play. Most existing LLM/LVLMs can explain generic sports
activities, but lack sufficient domain-centric sports' jargon to create natural
(human-like) descriptions. This work highlights the limitations of existing
SoTA LLM/LVLMs for generating production-grade sports captions from images in a
desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to
address that. The proposed pipeline yields an improvement > 8-10% in the F1,
and > 2-10% in BERT score compared to alternative approaches. In addition, it
has a small runtime memory footprint and fast execution time. During Super Bowl
LIX the pipeline proved its practical application for live professional sports
journalism; generating highly accurate and stylized captions at the rate of 6
images per 3-5 seconds for over 1000 images during the game play.

</details>


### [8] [DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models](https://arxiv.org/abs/2508.19298)
*Abu Sufian,Anirudha Ghosh,Debaditya Barman,Marco Leo,Cosimo Distante*

Main category: cs.CV

TL;DR: 研究评估了LVLMs在生物特征人脸识别任务中的人口统计偏差，发现PaliGemma和LLaVA对某些群体的偏差较大，而BLIP-2表现更一致。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在生物特征人脸识别（FR）等下游任务中表现出色，但其在不同人口统计群体（如种族/民族、性别和年龄）中的公平性仍是一个关键问题。

Method: 研究通过微调和评估三种广泛使用的预训练LVLM（LLaVA、BLIP-2和PaliGemma），在自建的人口统计平衡数据集上进行了实验，并采用了多种评估指标（如特定群体的BERTScore和公平性差异率）来量化和追踪性能差异。

Result: 实验揭示了LVLMs中存在的人口统计偏差，PaliGemma和LLaVA在西班牙裔/拉丁裔、高加索人和南亚群体上的表现差异较大，而BLIP-2表现较为一致。

Conclusion: 实验结果表明，LVLMs在不同人口统计群体中存在显著的公平性差异，其中PaliGemma和LLaVA对西班牙裔/拉丁裔、高加索人和南亚群体的偏差较大，而BLIP-2表现相对一致。

Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable
capabilities across various downstream tasks, including biometric face
recognition (FR) with description. However, demographic biases remain a
critical concern in FR, as these foundation models often fail to perform
equitably across diverse demographic groups, considering ethnicity/race,
gender, and age. Therefore, through our work DemoBias, we conduct an empirical
evaluation to investigate the extent of demographic biases in LVLMs for
biometric FR with textual token generation tasks. We fine-tuned and evaluated
three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own
generated demographic-balanced dataset. We utilize several evaluation metrics,
like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify
and trace the performance disparities. The experimental results deliver
compelling insights into the fairness and reliability of LVLMs across diverse
demographic groups. Our empirical study uncovered demographic biases in LVLMs,
with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,
Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably
consistent. Repository: https://github.com/Sufianlab/DemoBias.

</details>


### [9] [Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities](https://arxiv.org/abs/2508.19305)
*Chen Chu,Cyrus Shahabi*

Main category: cs.CV

TL;DR: Geo2Vec是一种基于符号距离场（SDF）的空间表示学习方法，直接操作原始空间，自适应采样并编码符号距离，无需分解地理实体，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么仅针对单一地理实体类型，要么通过分解引入高计算成本，且因缺乏几何对齐而依赖非自适应采样，导致细粒度特征模糊。Geo2Vec旨在解决这些限制。

Method: Geo2Vec 直接操作于原始空间，通过自适应采样点并编码其符号距离（正表示外部，负表示内部），无需分解地理实体。采用神经网络近似SDF，并结合旋转不变的位置编码来建模高频空间变化。

Result: 实验结果表明，Geo2Vec 在形状和位置表示、拓扑和距离关系捕捉以及实际GeoAI应用效率方面均优于现有方法。

Conclusion: Geo2Vec 提供了一种高效、统一且几何感知的空间表示学习方法，显著优于现有方法，适用于各种GeoAI应用。

Abstract: Spatial representation learning is essential for GeoAI applications such as
urban analytics, enabling the encoding of shapes, locations, and spatial
relationships (topological and distance-based) of geo-entities like points,
polylines, and polygons. Existing methods either target a single geo-entity
type or, like Poly2Vec, decompose entities into simpler components to enable
Fourier transformation, introducing high computational cost. Moreover, since
the transformed space lacks geometric alignment, these methods rely on uniform,
non-adaptive sampling, which blurs fine-grained features like edges and
boundaries. To address these limitations, we introduce Geo2Vec, a novel method
inspired by signed distance fields (SDF) that operates directly in the original
space. Geo2Vec adaptively samples points and encodes their signed distances
(positive outside, negative inside), capturing geometry without decomposition.
A neural network trained to approximate the SDF produces compact,
geometry-aware, and unified representations for all geo-entity types.
Additionally, we propose a rotation-invariant positional encoding to model
high-frequency spatial variations and construct a structured and robust
embedding space for downstream GeoAI models. Empirical results show that
Geo2Vec consistently outperforms existing methods in representing shape and
location, capturing topological and distance relationships, and achieving
greater efficiency in real-world GeoAI applications. Code and Data can be found
at: https://github.com/chuchen2017/GeoNeuralRepresentation.

</details>


### [10] [Advancements in Crop Analysis through Deep Learning and Explainable AI](https://arxiv.org/abs/2508.19307)
*Hamza Khan*

Main category: cs.CV

TL;DR: 本研究利用深度学习和可解释AI技术，开发了自动化稻米品种分类和稻叶病害诊断系统，效果显著，为农业自动化提供了可靠支持。


<details>
  <summary>Details</summary>
Motivation: 稻米是全球重要的主食，其质量和产量监控对消费者满意度和国家声誉至关重要。传统的人工检查方法效率低下且易出错，因此需要自动化解决方案来提升质量控制和产量改进。

Method: 本研究提出了一种基于卷积神经网络（CNN）的自动化方法，用于分类五种稻米品种。使用了包含75000张图像的公开数据集进行训练和测试。模型评估采用了准确率、召回率、精确率、F1分数、ROC曲线和混淆矩阵。此外，还开发了一种结合XAI与深度学习模型（如CNN、VGG16、ResNet50和MobileNetV2）的稻叶病害诊断方法，并使用了SHAP和LIME等可解释性技术。

Result: 结果显示，模型在分类稻米品种方面表现出高准确率和极少误分类。同时，结合XAI的框架能够准确诊断稻叶病害（如褐斑病、稻瘟病、白叶枯病和通格罗病），并通过SHAP和LIME技术提升了模型的透明度和可靠性。

Conclusion: 研究表明，深度学习在农业应用中具有巨大潜力，特别是通过结合可解释人工智能（XAI）技术，能够支持自动化作物质量检查和疾病诊断，最终惠及农民、消费者和农业经济。

Abstract: Rice is a staple food of global importance in terms of trade, nutrition, and
economic growth. Among Asian nations such as China, India, Pakistan, Thailand,
Vietnam and Indonesia are leading producers of both long and short grain
varieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To
ensure consumer satisfaction and strengthen national reputations, monitoring
rice crops and grain quality is essential. Manual inspection, however, is
labour intensive, time consuming and error prone, highlighting the need for
automated solutions for quality control and yield improvement. This study
proposes an automated approach to classify five rice grain varieties using
Convolutional Neural Networks (CNN). A publicly available dataset of 75000
images was used for training and testing. Model evaluation employed accuracy,
recall, precision, F1-score, ROC curves, and confusion matrices. Results
demonstrated high classification accuracy with minimal misclassifications,
confirming the model effectiveness in distinguishing rice varieties. In
addition, an accurate diagnostic method for rice leaf diseases such as Brown
Spot, Blast, Bacterial Blight, and Tungro was developed. The framework combined
explainable artificial intelligence (XAI) with deep learning models including
CNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP
(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic
Explanations) revealed how specific grain and leaf features influenced
predictions, enhancing model transparency and reliability. The findings
demonstrate the strong potential of deep learning in agricultural applications,
paving the way for robust, interpretable systems that can support automated
crop quality inspection and disease diagnosis, ultimately benefiting farmers,
consumers, and the agricultural economy.

</details>


### [11] [Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax](https://arxiv.org/abs/2508.19312)
*Ander Galván,Marivi Higuero,Jorge Sasiain,Eduardo Jacob*

Main category: cs.CV

TL;DR: A federated learning-based facial recognition system using OpenMax algorithm improves privacy and identity management in open-set scenarios.


<details>
  <summary>Details</summary>
Motivation: Facial recognition systems face privacy and identity management challenges, especially with unknown individuals in operational contexts. This paper aims to enhance privacy-aware and robust facial recognition in distributed environments.

Method: The approach combines federated learning with the OpenMax algorithm, utilizing mean activation vectors and local distance measures to distinguish between known and unknown subjects.

Result: Experimental results validate the system's effectiveness in reliably distinguishing between known and unknown subjects, demonstrating its potential for practical applications.

Conclusion: The proposed facial recognition system within a federated learning framework, integrated with the OpenMax algorithm, effectively addresses privacy and identity management challenges in open-set scenarios.

Abstract: Facial recognition powered by Artificial Intelligence has achieved high
accuracy in specific scenarios and applications. Nevertheless, it faces
significant challenges regarding privacy and identity management, particularly
when unknown individuals appear in the operational context. This paper presents
the design, implementation, and evaluation of a facial recognition system
within a federated learning framework tailored to open-set scenarios. The
proposed approach integrates the OpenMax algorithm into federated learning,
leveraging the exchange of mean activation vectors and local distance measures
to reliably distinguish between known and unknown subjects. Experimental
results validate the effectiveness of the proposed solution, demonstrating its
potential for enhancing privacy-aware and robust facial recognition in
distributed environments.
  --
  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado
una alta precisi\'on en algunos escenarios y aplicaciones. Sin embargo,
presenta desaf\'ios relacionados con la privacidad y la identificaci\'on de
personas, especialmente considerando que pueden aparecer sujetos desconocidos
para el sistema que lo implementa. En este trabajo, se propone el dise\~no,
implementaci\'on y evaluaci\'on de un sistema de reconocimiento facial en un
escenario de aprendizaje federado, orientado a conjuntos abiertos.
Concretamente, se dise\~na una soluci\'on basada en el algoritmo OpenMax para
escenarios de aprendizaje federado. La propuesta emplea el intercambio de los
vectores de activaci\'on promedio y distancias locales para identificar de
manera eficaz tanto personas conocidas como desconocidas. Los experimentos
realizados demuestran la implementaci\'on efectiva de la soluci\'on propuesta.

</details>


### [12] [Automated classification of natural habitats using ground-level imagery](https://arxiv.org/abs/2508.19314)
*Mahdis Tourian,Sareh Rowlands,Remy Vandaele,Max Fancourt,Rebecca Mein,Hywel T. P. Williams*

Main category: cs.CV

TL;DR: 该研究提出了一种基于地面级图像的深度学习分类方法，用于生态监测，模型在18个栖息地类别中表现良好，并提供了网络应用支持。


<details>
  <summary>Details</summary>
Motivation: 准确分类陆地栖息地对生物多样性保护、生态监测和土地利用规划至关重要。现有分类方案通常基于卫星图像分析，但缺乏有效的验证和规模化分类能力。

Method: 研究开发了一个分类系统，应用DeepLabV3-ResNet101分类器对地面级栖息地照片进行分类，预处理包括调整大小、归一化和增强，以及重新采样以平衡训练数据。

Result: 模型在18个栖息地类别中表现出色，平均F1得分为0.61，视觉上明显的类别如裸土、淤泥和泥炭（BSSP）和裸沙（BS）得分超过0.90，混合或模糊类别得分较低。

Conclusion: 该研究展示了基于地面级图像的深度学习分类方法在生态监测中的潜力，并提供了一个简单的网络应用供实践者使用。

Abstract: Accurate classification of terrestrial habitats is critical for biodiversity
conservation, ecological monitoring, and land-use planning. Several habitat
classification schemes are in use, typically based on analysis of satellite
imagery with validation by field ecologists. Here we present a methodology for
classification of habitats based solely on ground-level imagery (photographs),
offering improved validation and the ability to classify habitats at scale (for
example using citizen-science imagery). In collaboration with Natural England,
a public sector organisation responsible for nature conservation in England,
this study develops a classification system that applies deep learning to
ground-level habitat photographs, categorising each image into one of 18
classes defined by the 'Living England' framework. Images were pre-processed
using resizing, normalisation, and augmentation; re-sampling was used to
balance classes in the training data and enhance model robustness. We developed
and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label
to each photograph. Using five-fold cross-validation, the model demonstrated
strong overall performance across 18 habitat classes, with accuracy and
F1-scores varying between classes. Across all folds, the model achieved a mean
F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and
Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or
ambiguous classes scoring lower. These findings demonstrate the potential of
this approach for ecological monitoring. Ground-level imagery is readily
obtained, and accurate computational methods for habitat classification based
on such data have many potential applications. To support use by practitioners,
we also provide a simple web application that classifies uploaded images using
our model.

</details>


### [13] [MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation](https://arxiv.org/abs/2508.19320)
*Ming Chen,Liyuan Cui,Wenyuan Zhang,Haoxian Zhang,Yan Zhou,Xiaohan Li,Xiaoqiang Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: 本文提出了一种自回归视频生成框架，支持多模态交互控制和低延迟流式推断，通过构建大规模数据集和深度压缩技术显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实时交互式数字人类视频生成中面临高延迟、高计算成本和有限可控性的挑战。

Method: 通过最小化对大型语言模型（LLM）的修改，接受多模态条件编码（如音频、姿势和文本），并输出空间和语义连贯的表示以指导扩散头的去噪过程。此外，构建了一个大规模对话数据集（约20,000小时），并引入了深度压缩自编码器（压缩比高达64倍）。

Result: 在双工对话、多语言人类合成和交互式世界模型等任务上的广泛实验表明，该方法在低延迟、高效率和精细多模态可控性方面具有优势。

Conclusion: 本文提出的自回归视频生成框架在低延迟、高效率和多模态精细控制方面表现出显著优势，为交互式数字人类视频生成提供了实用解决方案。

Abstract: Recently, interactive digital human video generation has attracted widespread
attention and achieved remarkable progress. However, building such a practical
system that can interact with diverse input signals in real time remains
challenging to existing methods, which often struggle with high latency, heavy
computational cost, and limited controllability. In this work, we introduce an
autoregressive video generation framework that enables interactive multimodal
control and low-latency extrapolation in a streaming manner. With minimal
modifications to a standard large language model (LLM), our framework accepts
multimodal condition encodings including audio, pose, and text, and outputs
spatially and semantically coherent representations to guide the denoising
process of a diffusion head. To support this, we construct a large-scale
dialogue dataset of approximately 20,000 hours from multiple sources, providing
rich conversational scenarios for training. We further introduce a deep
compression autoencoder with up to 64$\times$ reduction ratio, which
effectively alleviates the long-horizon inference burden of the autoregressive
model. Extensive experiments on duplex conversation, multilingual human
synthesis, and interactive world model highlight the advantages of our approach
in low latency, high efficiency, and fine-grained multimodal controllability.

</details>


### [14] [Deep Data Hiding for ICAO-Compliant Face Images: A Survey](https://arxiv.org/abs/2508.19324)
*Jefferson David Rodriguez Chivata,Davide Ghiani,Simone Maurizio La Cava,Marco Micheletto,Giulia Orrù,Federico Lama,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 本文探讨了数字水印和隐写术作为ICAO合规面部图像的持久验证解决方案，分析了其潜力和局限性，并提供了实际部署的指导。


<details>
  <summary>Details</summary>
Motivation: ICAO合规面部图像在身份验证中广泛应用，但其标准化也带来了如变形和深度伪造等滥用风险，传统防护措施如PAD无法提供捕获后的保护。

Method: 本文对最新的数字水印和隐写术技术进行了全面分析，评估了这些技术在涉及ICAO合规图像的应用中的潜力和局限性。

Result: 研究揭示了数字水印和隐写术在ICAO合规图像中的关键权衡，为安全部署提供了指导。

Conclusion: 数字水印和隐写术作为补充解决方案，能够在不影响ICAO合规性的情况下，为ICAO合规面部图像提供持久的验证能力，适用于现实世界的身份系统。

Abstract: ICAO-compliant facial images, initially designed for secure biometric
passports, are increasingly becoming central to identity verification in a wide
range of application contexts, including border control, digital travel
credentials, and financial services. While their standardization enables global
interoperability, it also facilitates practices such as morphing and deepfakes,
which can be exploited for harmful purposes like identity theft and illegal
sharing of identity documents. Traditional countermeasures like Presentation
Attack Detection (PAD) are limited to real-time capture and offer no
post-capture protection. This survey paper investigates digital watermarking
and steganography as complementary solutions that embed tamper-evident signals
directly into the image, enabling persistent verification without compromising
ICAO compliance. We provide the first comprehensive analysis of
state-of-the-art techniques to evaluate the potential and drawbacks of the
underlying approaches concerning the applications involving ICAO-compliant
images and their suitability under standard constraints. We highlight key
trade-offs, offering guidance for secure deployment in real-world identity
systems.

</details>


### [15] [PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI](https://arxiv.org/abs/2508.19325)
*Haoyang Su,Jin-Yi Xiang,Shaohao Rui,Yifan Gao,Xingyu Chen,Tingxuan Yin,Xiaosong Wang,Lian-Ming Wu*

Main category: cs.CV

TL;DR: PRISM是一种自监督框架，结合心脏MRI和EHR数据，显著提升MACE预测准确性，并识别出高风险成像特征和临床因素。


<details>
  <summary>Details</summary>
Motivation: 准确预测主要不良心脏事件（MACE）是心血管预后的核心挑战，需要整合视觉和结构化数据以改进预测模型。

Method: PRISM采用自监督框架，通过运动感知的多视图蒸馏提取时间同步的成像特征，并利用医学信息文本提示进行调制，实现细粒度风险预测。

Result: PRISM在四个独立临床队列中均优于经典生存预测模型和最先进的深度学习基线，并揭示了三种与高风险相关的成像特征及主要临床贡献因素。

Conclusion: PRISM框架通过结合非对比心脏电影磁共振成像和结构化电子健康记录，显著提高了MACE的预测准确性，并揭示了三种与高风险相关的成像特征以及主要的临床贡献因素。

Abstract: Accurate prediction of major adverse cardiac events (MACE) remains a central
challenge in cardiovascular prognosis. We present PRISM (Prompt-guided
Representation Integration for Survival Modeling), a self-supervised framework
that integrates visual representations from non-contrast cardiac cine magnetic
resonance imaging with structured electronic health records (EHRs) for survival
analysis. PRISM extracts temporally synchronized imaging features through
motion-aware multi-view distillation and modulates them using medically
informed textual prompts to enable fine-grained risk prediction. Across four
independent clinical cohorts, PRISM consistently surpasses classical survival
prediction models and state-of-the-art (SOTA) deep learning baselines under
internal and external validation. Further clinical findings demonstrate that
the combined imaging and EHR representations derived from PRISM provide
valuable insights into cardiac risk across diverse cohorts. Three distinct
imaging signatures associated with elevated MACE risk are uncovered, including
lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior
elevated focus during diastole. Prompt-guided attribution further identifies
hypertension, diabetes, and smoking as dominant contributors among clinical and
physiological EHR factors.

</details>


### [16] [EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.19349)
*Mahdieh Behjat Khatooni,Mohsen Soryani*

Main category: cs.CV

TL;DR: EffNetViTLoRA模型整合CNN和ViT，利用LoRA技术优化预训练模型，在完整ADNI MRI数据集上实现高精度AD、MCI和CN分类。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）早期诊断至关重要，但轻度认知障碍（MCI）诊断因类别间差异细微而具挑战性。现有研究多依赖有限数据子集，导致模型偏差。

Method: 提出EffNetViTLoRA模型，结合CNN和ViT捕捉MRI图像的局部和全局特征，使用LoRA技术适应预训练ViT模型，并在完整ADNI MRI数据集上进行训练。

Result: 模型在ADNI数据集上实现了92.52%的分类准确率和92.76%的F1分数，涵盖AD、MCI和CN三类诊断。

Conclusion: EffNetViTLoRA模型通过整合CNN和ViT，结合LoRA技术，在ADNI数据集上实现了高精度的AD、MCI和CN分类，为阿尔茨海默病的早期诊断提供了可靠工具。

Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative
disorders worldwide. As it progresses, it leads to the deterioration of
cognitive functions. Since AD is irreversible, early diagnosis is crucial for
managing its progression. Mild Cognitive Impairment (MCI) represents an
intermediate stage between Cognitively Normal (CN) individuals and those with
AD, and is considered a transitional phase from normal cognition to Alzheimer's
disease. Diagnosing MCI is particularly challenging due to the subtle
differences between adjacent diagnostic categories. In this study, we propose
EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole
Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging
(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a
Vision Transformer (ViT) to capture both local and global features from MRI
images. Unlike previous studies that rely on limited subsets of data, our
approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in
a more robust and unbiased model. This comprehensive methodology enhances the
model's clinical reliability. Furthermore, fine-tuning large pretrained models
often yields suboptimal results when source and target dataset domains differ.
To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt
the pretrained ViT model to our target domain. This method enables efficient
knowledge transfer and reduces the risk of overfitting. Our model achieves a
classification accuracy of 92.52% and an F1-score of 92.76% across three
diagnostic categories: AD, MCI, and CN for full ADNI dataset.

</details>


### [17] [Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage](https://arxiv.org/abs/2508.19477)
*Zachary L. Crang,Rich D. Johnston,Katie L. Mills,Johsan Billingham,Sam Robertson,Michael H. Cole,Jonathon Weakley,Adam Hewitt and,Grant M. Duthie*

Main category: cs.CV

TL;DR: 研究表明，商业计算机视觉和AI球员追踪软件在广播画面上能够以合理精度追踪球员，战术画面和适当分辨率是关键。


<details>
  <summary>Details</summary>
Motivation: 评估商业计算机视觉和AI球员追踪软件在广播画面上测量球员位置、速度和距离的准确性，并确定摄像头画面和分辨率对准确性的影响。

Method: 研究比较了三种商业追踪提供商的数据与TRACAB Gen 5高清多摄像头追踪系统的数据，计算了均方根误差（RMSE）和平均偏差。

Result: 位置RMSE范围为1.68至16.39米，速度RMSE范围为0.34至2.38米/秒。比赛总距离的平均偏差范围为-1745米（-21.8%）至1945米（24.3%）。

Conclusion: 计算机视觉和AI球员追踪软件在球员被检测到时能够提供合理的精确度。建议提供商在追踪位置和速度时使用战术画面，以提高准确性。720p和1080p分辨率均适用，前提是实施了适当的计算机视觉和AI模型。

Abstract: This study aimed to: (1) understand whether commercially available
computer-vision and artificial intelligence (AI) player tracking software can
accurately measure player position, speed and distance using broadcast footage
and (2) determine the impact of camera feed and resolution on accuracy. Data
were obtained from one match at the 2022 Qatar Federation Internationale de
Football Association (FIFA) World Cup. Tactical, programme and camera 1 feeds
were used. Three commercial tracking providers that use computer-vision and AI
participated. Providers analysed instantaneous position (x, y coordinates) and
speed (m\,s^{-1}) of each player. Their data were compared with a
high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square
error (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to
16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\,s^{-1}. Total match
distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across
providers. Computer-vision and AI player tracking software offer the ability to
track players with fair precision when players are detected by the software.
Providers should use a tactical feed when tracking position and speed, which
will maximise player detection, improving accuracy. Both 720p and 1080p
resolutions are suitable, assuming appropriate computer-vision and AI models
are implemented.

</details>


### [18] [JVLGS: Joint Vision-Language Gas Leak Segmentation](https://arxiv.org/abs/2508.19485)
*Xinlong Zhao,Qixiang Pang,Shan Du*

Main category: cs.CV

TL;DR: JVLGS框架通过视觉与文本模态结合，有效提升气体泄漏检测性能，减少误报，并在多种学习设置下表现优异。


<details>
  <summary>Details</summary>
Motivation: 气体泄漏对人类健康和大气污染构成严重威胁，但现有检测方法因气体云的非刚性和模糊性而效果有限。

Method: 提出了一种名为JVLGS的新框架，结合视觉和文本模态，并引入了后处理步骤以减少误报。

Result: 在多样化的实验场景中，JVLGS在监督学习和少样本学习设置下均表现优异，显著优于现有方法。

Conclusion: JVLGS框架通过整合视觉和文本模态的互补优势，显著提升了气体泄漏的检测和分割性能，并在多种场景下优于现有方法。

Abstract: Gas leaks pose serious threats to human health and contribute significantly
to atmospheric pollution, drawing increasing public concern. However, the lack
of effective detection methods hampers timely and accurate identification of
gas leaks. While some vision-based techniques leverage infrared videos for leak
detection, the blurry and non-rigid nature of gas clouds often limits their
effectiveness. To address these challenges, we propose a novel framework called
Joint Vision-Language Gas leak Segmentation (JVLGS), which integrates the
complementary strengths of visual and textual modalities to enhance gas leak
representation and segmentation. Recognizing that gas leaks are sporadic and
many video frames may contain no leak at all, our method incorporates a
post-processing step to reduce false positives caused by noise and non-target
objects, an issue that affects many existing approaches. Extensive experiments
conducted across diverse scenarios show that JVLGS significantly outperforms
state-of-the-art gas leak segmentation methods. We evaluate our model under
both supervised and few-shot learning settings, and it consistently achieves
strong performance in both, whereas competing methods tend to perform well in
only one setting or poorly in both. Code available at:
https://github.com/GeekEagle/JVLGS

</details>


### [19] [UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models](https://arxiv.org/abs/2508.19498)
*Yimu Wang,Weiming Zhuang,Chen Chen,Jiabo Huang,Jingtao Li,Lingjuan Lyu*

Main category: cs.CV

TL;DR: UNIFORM框架通过投票机制整合异构预训练模型的知识，显著提升无监督物体识别性能，并具备良好的扩展性。


<details>
  <summary>Details</summary>
Motivation: 预训练模型的异构性使得有效利用其集体知识成为挑战，现有知识整合方法因依赖强假设而受限。

Method: 提出了一种名为UNIFORM的新框架，通过专用的投票机制在逻辑层和特征层捕捉知识的共识，无需对训练数据分布或网络架构做出强假设。

Result: UNIFORM在无监督物体识别任务中表现优于现有知识转移基线方法。

Conclusion: UNIFORM框架有效提升了无监督物体识别的性能，并展示了显著的扩展性，能够从超过一百个教师模型中受益，而现有方法在更小规模时就达到饱和。

Abstract: In the era of deep learning, the increasing number of pre-trained models
available online presents a wealth of knowledge. These models, developed with
diverse architectures and trained on varied datasets for different tasks,
provide unique interpretations of the real world. Their collective consensus is
likely universal and generalizable to unseen data. However, effectively
harnessing this collective knowledge poses a fundamental challenge due to the
heterogeneity of pre-trained models. Existing knowledge integration solutions
typically rely on strong assumptions about training data distributions and
network architectures, limiting them to learning only from specific types of
models and resulting in data and/or inductive biases. In this work, we
introduce a novel framework, namely UNIFORM, for knowledge transfer from a
diverse set of off-the-shelf models into one student model without such
constraints. Specifically, we propose a dedicated voting mechanism to capture
the consensus of knowledge both at the logit level -- incorporating teacher
models that are capable of predicting target classes of interest -- and at the
feature level, utilizing visual representations learned on arbitrary label
spaces. Extensive experiments demonstrate that UNIFORM effectively enhances
unsupervised object recognition performance compared to strong knowledge
transfer baselines. Notably, it exhibits remarkable scalability by benefiting
from over one hundred teachers, while existing methods saturate at a much
smaller scale.

</details>


### [20] [Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery](https://arxiv.org/abs/2508.19499)
*Xiangxu Wang,Tianhong Zhao,Wei Tu,Bowen Zhang,Guanzhou Chen,Jinzhou Cao*

Main category: cs.CV

TL;DR: Sat2Flow是一种仅使用卫星图像生成结构一致的OD流的框架，解决了现有方法对辅助数据和空间拓扑敏感的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的OD流矩阵生成方法存在两个关键限制：(1)依赖成本高且空间覆盖有限的辅助特征；(2)对空间拓扑敏感，轻微的区域索引重排序会破坏生成流的结构一致性。Sat2Flow旨在解决这些问题。

Method: Sat2Flow是一个基于潜在结构感知的扩散框架，通过多核编码器捕捉多样化的区域交互，并采用排列感知扩散过程来对齐不同区域顺序的潜在表示。

Result: 在真实城市数据集上的实验结果表明，Sat2Flow在数值准确性上优于基于物理和数据驱动的基线方法，同时在索引排列下保持经验分布和空间结构。

Conclusion: Sat2Flow提供了一种全球可扩展的解决方案，用于数据稀缺城市环境中的OD流生成，消除了区域特定的辅助数据依赖性，同时保持了结构不变性以实现稳健的移动性建模。

Abstract: Origin-Destination (OD) flow matrices are essential for urban mobility
analysis, underpinning applications in traffic forecasting, infrastructure
planning, and policy design. However, existing methods suffer from two critical
limitations: (1) reliance on auxiliary features (e.g., Points of Interest,
socioeconomic statistics) that are costly to collect and have limited spatial
coverage; and (2) sensitivity to spatial topology, where minor index reordering
of urban regions (e.g., census tract relabeling) disrupts structural coherence
in generated flows. To address these challenges, we propose Sat2Flow, a latent
structure-aware diffusion-based framework that generates structurally coherent
OD flows using solely satellite imagery as input. Our approach introduces a
multi-kernel encoder to capture diverse regional interactions and employs a
permutation-aware diffusion process that aligns latent representations across
different regional orderings. Through a joint contrastive training objective
that bridges satellite-derived features with OD patterns, combined with
equivariant diffusion training that enforces structural consistency, Sat2Flow
ensures topological robustness under arbitrary regional reindexing.
Experimental results on real-world urban datasets demonstrate that Sat2Flow
outperforms both physics-based and data-driven baselines in numerical accuracy
while preserving empirical distributions and spatial structures under index
permutations. Sat2Flow offers a globally scalable solution for OD flow
generation in data-scarce urban environments, eliminating region-specific
auxiliary data dependencies while maintaining structural invariance for robust
mobility modeling.

</details>


### [21] [Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity](https://arxiv.org/abs/2508.19511)
*Alzayat Saleh,Shunsuke Hatano,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: 研究提出半监督框架，利用未标注数据提升模型鲁棒性，解决农田中阴影偏差问题，显著提高召回率，适用于精准农业的计算机视觉系统。


<details>
  <summary>Details</summary>
Motivation: 自动化管理入侵杂草对可持续农业至关重要，但深度学习模型在真实农田中的表现常受环境挑战和高标注成本影响。本研究旨在通过诊断驱动的半监督框架解决这些问题。

Method: 使用包含975张标注和10,000张未标注图像的独特数据集，首先建立强监督基线（ResNet分类、YOLO和RF-DETR检测），并通过可解释性工具发现‘阴影偏差’。随后设计半监督流程，利用未标注数据通过伪标签增强模型鲁棒性。

Result: 强监督基线F1分数达0.90，mAP50超过0.82。半监督流程通过伪标签有效缓解阴影偏差，并显著提升召回率，在低数据量公共作物-杂草基准测试中验证了其有效性。

Conclusion: 本研究提出了一个诊断驱动的半监督框架，有效解决了深度学习模型在真实农田中因环境挑战和数据标注成本高而性能受限的问题，为精准农业提供了稳健的计算机视觉系统开发方法。

Abstract: The automated management of invasive weeds is critical for sustainable
agriculture, yet the performance of deep learning models in real-world fields
is often compromised by two factors: challenging environmental conditions and
the high cost of data annotation. This study tackles both issues through a
diagnostic-driven, semi-supervised framework. Using a unique dataset of
approximately 975 labeled and 10,000 unlabeled images of Guinea Grass in
sugarcane, we first establish strong supervised baselines for classification
(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and
mAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by
interpretability tools, uncovered a pervasive "shadow bias," where models
learned to misidentify shadows as vegetation. This diagnostic insight motivated
our primary contribution: a semi-supervised pipeline that leverages unlabeled
data to enhance model robustness. By training models on a more diverse set of
visual information through pseudo-labeling, this framework not only helps
mitigate the shadow bias but also provides a tangible boost in recall, a
critical metric for minimizing weed escapes in automated spraying systems. To
validate our methodology, we demonstrate its effectiveness in a low-data regime
on a public crop-weed benchmark. Our work provides a clear and field-tested
framework for developing, diagnosing, and improving robust computer vision
systems for the complex realities of precision agriculture.

</details>


### [22] [MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment](https://arxiv.org/abs/2508.19527)
*Zhiting Gao,Dan Song,Diqiong Jiang,Chao Xue,An-An Liu*

Main category: cs.CV

TL;DR: TAPO和MotionFLUX框架通过优化语义对齐和实时生成，显著提升了运动生成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 解决文本驱动方法在语言描述与运动语义精确对齐及多步推理效率低下的问题。

Method: TAPO框架通过迭代调整强化语义基础，MotionFLUX基于确定性校正流匹配，构建噪声分布与运动空间之间的最优传输路径。

Result: 实验结果表明，TAPO和MotionFLUX在语义一致性和运动质量上均优于现有方法，同时加速了生成速度。

Conclusion: TAPO与MotionFLUX构成了一个统一的系统，在语义一致性和运动质量上均优于现有方法，同时显著提高了生成速度。

Abstract: Motion generation is essential for animating virtual characters and embodied
agents. While recent text-driven methods have made significant strides, they
often struggle with achieving precise alignment between linguistic descriptions
and motion semantics, as well as with the inefficiencies of slow, multi-step
inference. To address these issues, we introduce TMR++ Aligned Preference
Optimization (TAPO), an innovative framework that aligns subtle motion
variations with textual modifiers and incorporates iterative adjustments to
reinforce semantic grounding. To further enable real-time synthesis, we propose
MotionFLUX, a high-speed generation framework based on deterministic rectified
flow matching. Unlike traditional diffusion models, which require hundreds of
denoising steps, MotionFLUX constructs optimal transport paths between noise
distributions and motion spaces, facilitating real-time synthesis. The
linearized probability paths reduce the need for multi-step sampling typical of
sequential methods, significantly accelerating inference time without
sacrificing motion quality. Experimental results demonstrate that, together,
TAPO and MotionFLUX form a unified system that outperforms state-of-the-art
approaches in both semantic consistency and motion quality, while also
accelerating generation speed. The code and pretrained models will be released.

</details>


### [23] [Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies](https://arxiv.org/abs/2508.20072)
*Zhixuan Liang,Yizhuo Li,Tianshuo Yang,Chengyue Wu,Sitong Mao,Liuao Pei,Xiaokang Yang,Jiangmiao Pang,Yao Mu,Ping Luo*

Main category: cs.CV

TL;DR: 离散扩散VLA模型通过统一解码器设计，显著提升动作生成性能，优于自回归和连续扩散基线。


<details>
  <summary>Details</summary>
Motivation: 解决现有VLA解码器在生成动作时固定顺序或需要专门训练和迭代采样的问题，以实现统一且可扩展的架构。

Method: 提出离散扩散VLA，一种单变压器策略，通过离散扩散建模离散化动作块，并使用与VLM主干相同的交叉熵目标进行训练。

Result: 在LIBERO上达到96.3%的平均成功率，在SimplerEnv Fractal上达到71.2%的视觉匹配率，在SimplerEnv Bridge上达到49.3%的整体表现。

Conclusion: 离散扩散VLA模型通过统一的解码器设计，支持精确的动作建模和一致的训练，为扩展到更大的模型和数据集奠定了基础。

Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions to robot actions. However, prevailing VLA decoders
either generate actions autoregressively in a fixed left-to-right order or
attach continuous diffusion or flow matching heads outside the backbone,
demanding specialized training and iterative sampling that hinder a unified,
scalable architecture. We present Discrete Diffusion VLA, a single-transformer
policy that models discretized action chunks with discrete diffusion and is
trained with the same cross-entropy objective as the VLM backbone. The design
retains diffusion's progressive refinement paradigm while remaining natively
compatible with the discrete token interface of VLMs. Our method achieves an
adaptive decoding order that resolves easy action elements before harder ones
and uses secondary remasking to revisit uncertain predictions across refinement
rounds, which improves consistency and enables robust error correction. This
unified decoder preserves pretrained vision language priors, supports parallel
decoding, breaks the autoregressive bottleneck, and reduces the number of
function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,
71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv
Bridge, improving over both autoregressive and continuous diffusion baselines.
These findings indicate that discrete-diffusion action decoder supports precise
action modeling and consistent training, laying groundwork for scaling VLA to
larger models and datasets.

</details>


### [24] [CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning](https://arxiv.org/abs/2508.19542)
*Nannan Zhu,Yonghao Dong,Teng Wang,Xueqian Li,Shengjun Deng,Yijia Wang,Zheng Hong,Tiantian Geng,Guo Niu,Hanyan Huang,Xiongfei Yao,Shuaiwei Jiao*

Main category: cs.CV

TL;DR: CVBench是首个评估跨视频关系推理的基准测试，揭示当前MLLM在多视频任务中的局限性，尤其是上下文保留和实体消歧的不足。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLM）在单视频任务上表现优异，但在多视频任务上的能力尚未充分探索。这种能力对实际应用（如多摄像头监控和跨视频程序学习）至关重要。

Method: 通过构建CVBench基准测试，包含1,000个问题-答案对，分为三个层次：跨视频对象关联、跨视频事件关联和跨视频复杂推理。评估了10多个领先的MLLM模型，包括GPT-4o、Gemini-2.0-flash和Qwen2.5-VL，采用零样本或思维链提示范式。

Result: 评估结果显示，即使是顶级模型（如GPT-4o）在因果推理任务上仅达到60%的准确率，远低于人类91%的表现。

Conclusion: CVBench 提供了一个严格的框架来诊断和推进多视频推理能力，揭示了当前MLLM架构在跨视频上下文保留和实体消歧方面的根本瓶颈。

Abstract: While multimodal large language models (MLLMs) exhibit strong performance on
single-video tasks (e.g., video question answering), their ability across
multiple videos remains critically underexplored. However, this capability is
essential for real-world applications, including multi-camera surveillance and
cross-video procedural learning. To bridge this gap, we present CVBench, the
first comprehensive benchmark designed to assess cross-video relational
reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning
three hierarchical tiers: cross-video object association (identifying shared
entities), cross-video event association (linking temporal or causal event
chains), and cross-video complex reasoning (integrating commonsense and domain
knowledge). Built from five domain-diverse video clusters (e.g., sports, life
records), the benchmark challenges models to synthesise information across
dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including
GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought
prompting paradigms. Key findings reveal stark performance gaps: even top
models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,
compared to the 91% accuracy of human performance. Crucially, our analysis
reveals fundamental bottlenecks inherent in current MLLM architectures, notably
deficient inter-video context retention and poor disambiguation of overlapping
entities. CVBench establishes a rigorous framework for diagnosing and advancing
multi-video reasoning, offering architectural insights for next-generation
MLLMs.The data and evaluation code are available at
https://github.com/Hokhim2/CVBench.

</details>


### [25] [WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization](https://arxiv.org/abs/2508.19544)
*Eduardo Davalos,Yike Zhang,Namrata Srivastava,Yashvitha Thatigotla,Jorge A. Salas,Sara McFadden,Sun-Joo Cho,Amanda Goodwin,Ashwin TS,Gautam Biswas*

Main category: cs.CV

TL;DR: WebEyeTrack是一个浏览器集成的轻量级注视估计框架，通过头部姿态估计和少量样本在线学习，实现高精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI注视估计方法虽在SOTA基准上表现优异，但实际应用中存在与商业眼动追踪方案的差距，且模型大小、推理时间和隐私问题常被忽视；基于摄像头的眼动追踪方法因头部运动导致精度不足。

Method: 提出WebEyeTrack框架，集成轻量级SOTA注视估计模型，结合模型驱动的头部姿态估计和仅需9个校准样本的在线学习。

Result: 在GazeCapture数据集上达到2.32厘米的误差，iPhone 14上实现2.4毫秒的实时推理速度。

Conclusion: WebEyeTrack通过整合轻量级SOTA注视估计模型，结合头部姿态估计和少量样本的在线学习，显著提升了基于摄像头的注视估计的准确性和实时性，为实际应用提供了可行的解决方案。

Abstract: With advancements in AI, new gaze estimation methods are exceeding
state-of-the-art (SOTA) benchmarks, but their real-world application reveals a
gap with commercial eye-tracking solutions. Factors like model size, inference
time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking
methods lack sufficient accuracy, in particular due to head movement. To tackle
these issues, we introduce We bEyeTrack, a framework that integrates
lightweight SOTA gaze estimation models directly in the browser. It
incorporates model-based head pose estimation and on-device few-shot learning
with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new
users, achieving SOTA performance with an error margin of 2.32 cm on
GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.
Our open-source code is available at
https://github.com/RedForestAi/WebEyeTrack.

</details>


### [26] [MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery](https://arxiv.org/abs/2508.19555)
*Yu-Wei Zhang,Tongju Han,Lipeng Gao,Mingqiang Wei,Hui Liu,Changbao Li,Caiming Zhang*

Main category: cs.CV

TL;DR: MonoRelief V2通过结合伪真实和真实数据训练，显著提升单图像2.5D浮雕恢复的性能。


<details>
  <summary>Details</summary>
Motivation: 解决MonoRelief V1仅依赖合成数据训练的局限性，提升模型在复杂材料和光照变化下的性能。

Method: 使用文本到图像生成模型生成约15,000张伪真实图像，并通过深度和法线预测融合生成伪标签；同时构建800个样本的小规模真实数据集。模型在伪真实和真实数据集上逐步训练。

Result: 综合实验表明，MonoRelief V2在深度和法线预测上达到最先进性能。

Conclusion: MonoRelief V2通过结合伪真实数据和真实数据训练，显著提升了从单图像恢复2.5D浮雕的鲁棒性、准确性和效率，展现了在下游应用中的强大潜力。

Abstract: This paper presents MonoRelief V2, an end-to-end model designed for directly
recovering 2.5D reliefs from single images under complex material and
illumination variations. In contrast to its predecessor, MonoRelief V1 [1],
which was solely trained on synthetic data, MonoRelief V2 incorporates real
data to achieve improved robustness, accuracy and efficiency. To overcome the
challenge of acquiring large-scale real-world dataset, we generate
approximately 15,000 pseudo real images using a text-to-image generative model,
and derive corresponding depth pseudo-labels through fusion of depth and normal
predictions. Furthermore, we construct a small-scale real-world dataset (800
samples) via multi-view reconstruction and detail refinement. MonoRelief V2 is
then progressively trained on the pseudo-real and real-world datasets.
Comprehensive experiments demonstrate its state-of-the-art performance both in
depth and normal predictions, highlighting its strong potential for a range of
downstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.

</details>


### [27] [FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection](https://arxiv.org/abs/2508.19565)
*Yuhang Zhao,Zixing Wang*

Main category: cs.CV

TL;DR: FlowDet通过GDU和SAA模块优化DETR架构，显著提升了复杂交通场景下的检测效率和精度，同时发布了Intersection-Flow-5k数据集。


<details>
  <summary>Details</summary>
Motivation: 端到端目标检测器在实时应用中具有潜力，但高计算成本限制了其在复杂场景（如交叉路口交通监控）中的应用。

Method: FlowDet采用了解耦编码器优化策略，结合新颖的Geometric Deformable Unit (GDU)进行交通感知几何建模，以及Scale-Aware Attention (SAA)模块处理极端尺度变化。

Result: 在Intersection-Flow-5k数据集上，FlowDet在AP(test)和AP50(test)上分别提升了1.5%和1.6%，同时减少了63.2%的GFLOPs并加快了16.2%的推理速度。

Conclusion: FlowDet提出了一种高效准确的端到端目标检测器，为实际应用中的高要求感知系统提供了新的解决方案。Intersection-Flow-5k数据集的发布进一步推动了该领域的研究。

Abstract: End-to-end object detectors offer a promising NMS-free paradigm for real-time
applications, yet their high computational cost remains a significant barrier,
particularly for complex scenarios like intersection traffic monitoring. To
address this challenge, we propose FlowDet, a high-speed detector featuring a
decoupled encoder optimization strategy applied to the DETR architecture.
Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for
traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to
maintain high representational power across extreme scale variations. To
rigorously evaluate the model's performance in environments with severe
occlusion and high object density, we collected the Intersection-Flow-5k
dataset, a new challenging scene for this task. Evaluated on
Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to
the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by
1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference
speed by 16.2%. Our work demonstrates a new path towards building highly
efficient and accurate detectors for demanding, real-world perception systems.
The Intersection-Flow-5k dataset is available at
https://github.com/AstronZh/Intersection-Flow-5K.

</details>


### [28] [DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection](https://arxiv.org/abs/2508.19573)
*Luhu Li,Bowen Lin,Mukhtiar Khan,Shujun Fu*

Main category: cs.CV

TL;DR: 论文提出了一种结合可训练编码器和原型引导重建的统一框架，引入多样性感知对齐损失以解决原型崩溃问题，显著提升医学图像异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像异常检测面临注释有限和与自然图像的领域差距问题。现有重建方法依赖冻结的预训练编码器，限制了领域特定特征的适应性和定位准确性。原型学习虽具解释性和聚类优势，但存在原型崩溃问题，影响多样性和泛化能力。

Method: 论文提出了一种结合可训练编码器与原型引导重建的统一框架，并引入了多样性感知对齐损失。可训练编码器通过动量分支增强，实现稳定的领域自适应特征学习。轻量级原型提取器挖掘信息丰富的正常原型，通过注意力机制引导解码器进行精确重建。

Result: 在多个医学影像基准测试中，论文方法在表示质量和异常定位方面显著优于现有方法。可视化和原型分配分析进一步验证了抗崩溃机制的有效性和增强的解释性。

Conclusion: 论文提出的统一框架通过可训练编码器、原型引导重建和多样性感知对齐损失，显著提高了医学图像异常检测的表现和定位准确性，并通过可视化和原型分配分析验证了其抗崩溃机制的有效性。

Abstract: Anomaly detection in medical images is challenging due to limited annotations
and a domain gap compared to natural images. Existing reconstruction methods
often rely on frozen pre-trained encoders, which limits adaptation to
domain-specific features and reduces localization accuracy. Prototype-based
learning offers interpretability and clustering benefits but suffers from
prototype collapse, where few prototypes dominate training, harming diversity
and generalization. To address this, we propose a unified framework combining a
trainable encoder with prototype-guided reconstruction and a novel
Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum
branch, enables stable domain-adaptive feature learning. A lightweight
Prototype Extractor mines informative normal prototypes to guide the decoder
via attention for precise reconstruction. Our loss enforces balanced prototype
use through diversity constraints and per-prototype normalization, effectively
preventing collapse. Experiments on multiple medical imaging benchmarks show
significant improvements in representation quality and anomaly localization,
outperforming prior methods. Visualizations and prototype assignment analyses
further validate the effectiveness of our anti-collapse mechanism and enhanced
interpretability.

</details>


### [29] [Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation](https://arxiv.org/abs/2508.19574)
*Mingxi Fu,Fanglei Fu,Xitong Ling,Huaitian Yuan,Tian Guan,Yonghong He,Lianghui Zhu*

Main category: cs.CV

TL;DR: MPAMatch提出了一种新型病理图像分割框架，通过双重对比学习提升语义边界建模，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决病理图像分割中语义边界模糊和像素级标注成本高的问题，传统方法难以捕捉高级语义先验。

Method: MPAMatch采用多模态原型引导的监督范式，结合图像原型与像素标签、文本原型与像素标签的双重对比学习，并重构了经典的TransUNet架构，使用病理预训练的基础模型Uni。

Result: 在GLAS、EBHI-SEG-GLAND、EBHI-SEG-CANCER和KPI数据集上，MPAMatch优于现有最先进方法。

Conclusion: MPAMatch通过双重对比学习方案在结构和语义层面上提供监督，显著提升了病理图像分割的性能，并在多个数据集上验证了其优越性。

Abstract: Pathological image segmentation faces numerous challenges, particularly due
to ambiguous semantic boundaries and the high cost of pixel-level annotations.
Although recent semi-supervised methods based on consistency regularization
(e.g., UniMatch) have made notable progress, they mainly rely on
perturbation-based consistency within the image modality, making it difficult
to capture high-level semantic priors, especially in structurally complex
pathology images. To address these limitations, we propose MPAMatch - a novel
segmentation framework that performs pixel-level contrastive learning under a
multimodal prototype-guided supervision paradigm. The core innovation of
MPAMatch lies in the dual contrastive learning scheme between image prototypes
and pixel labels, and between text prototypes and pixel labels, providing
supervision at both structural and semantic levels. This coarse-to-fine
supervisory strategy not only enhances the discriminative capability on
unlabeled samples but also introduces the text prototype supervision into
segmentation for the first time, significantly improving semantic boundary
modeling. In addition, we reconstruct the classic segmentation architecture
(TransUNet) by replacing its ViT backbone with a pathology-pretrained
foundation model (Uni), enabling more effective extraction of
pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,
EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art
methods, validating its dual advantages in structural and semantic modeling.

</details>


### [30] [Interact-Custom: Customized Human Object Interaction Image Generation](https://arxiv.org/abs/2508.19575)
*Zhu Xu,Zhaowen Wang,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: 该论文提出定制化人机交互图像生成任务（CHOI），并设计两阶段模型Interact-Custom，通过前景掩码和身份特征保持，实现了交互语义控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注目标实体的外观保持，忽视了细粒度的交互控制，因此提出定制化人机交互图像生成任务（CHOI）以解决这一问题。

Method: 提出两阶段模型Interact-Custom，首先生成描述交互行为的前景掩码以明确空间配置，然后在掩码指导下生成目标人机交互图像并保持身份特征。

Result: 实验表明，Interact-Custom在CHOI任务中表现优异，能够同时实现身份保持和交互语义控制。

Conclusion: Interact-Custom模型通过两阶段设计，有效解决了定制化人机交互图像生成中的身份保持和交互语义控制问题，实验证明了其有效性。

Abstract: Compositional Customized Image Generation aims to customize multiple target
concepts within generation content, which has gained attention for its wild
application.Existing approaches mainly concentrate on the target entity's
appearance preservation, while neglecting the fine-grained interaction control
among target entities.To enable the model of such interaction control
capability, we focus on human object interaction scenario and propose the task
of Customized Human Object Interaction Image Generation(CHOI), which
simultaneously requires identity preservation for target human object and the
interaction semantic control between them.Two primary challenges exist for
CHOI:(1)simultaneous identity preservation and interaction control demands
require the model to decompose the human object into self-contained identity
features and pose-oriented interaction features, while the current HOI image
datasets fail to provide ideal samples for such feature-decomposed
learning.(2)inappropriate spatial configuration between human and object may
lead to the lack of desired interaction semantics.To tackle it, we first
process a large-scale dataset, where each sample encompasses the same pair of
human object involving different interactive poses.Then we design a two-stage
model Interact-Custom, which firstly explicitly models the spatial
configuration by generating a foreground mask depicting the interaction
behavior, then under the guidance of this mask, we generate the target human
object interacting while preserving their identities features.Furthermore, if
the background image and the union location of where the target human object
should appear are provided by users, Interact-Custom also provides the optional
functionality to specify them, offering high content controllability. Extensive
experiments on our tailored metrics for CHOI task demonstrate the effectiveness
of our approach.

</details>


### [31] [High-Speed FHD Full-Color Video Computer-Generated Holography](https://arxiv.org/abs/2508.19579)
*Haomiao Zhang,Miao Cao,Xuan Yu,Hui Luo,Yanling Piao,Mengjie Qin,Zhangyuan Li,Ping Wang,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出SGDDM和HoloMamba方案，解决了全息视频生成中的高帧率与色彩保真度问题，计算效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的学习模型和帧间优化方法在高速全彩全息视频生成中存在色彩串扰和计算效率低下的问题，亟需新的解决方案。

Method: 首先，通过SGDDM技术优化相位分布，实现高保真全彩显示；其次，采用轻量级不对称Mamba-Unet架构HoloMamba，显式建模时空相关性以提升重建质量和计算效率。

Result: 实验表明，SGDDM实现了高保真全彩显示而不牺牲帧率，HoloMamba能以超过260 FPS的速度生成1080p全彩全息视频，比现有最优方法快2.6倍。

Conclusion: 本文提出的SGDDM和HoloMamba方案成功解决了计算机生成全息视频中的高帧率与色彩保真度之间的权衡问题，并在计算效率上显著优于现有方法。

Abstract: Computer-generated holography (CGH) is a promising technology for
next-generation displays. However, generating high-speed, high-quality
holographic video requires both high frame rate display and efficient
computation, but is constrained by two key limitations: ($i$) Learning-based
models often produce over-smoothed phases with narrow angular spectra, causing
severe color crosstalk in high frame rate full-color displays such as
depth-division multiplexing and thus resulting in a trade-off between frame
rate and color fidelity. ($ii$) Existing frame-by-frame optimization methods
typically optimize frames independently, neglecting spatial-temporal
correlations between consecutive frames and leading to computationally
inefficient solutions. To overcome these challenges, in this paper, we propose
a novel high-speed full-color video CGH generation scheme. First, we introduce
Spectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase
distributions via frequency modulation, enabling high-fidelity full-color
display at high frame rates. Second, we present HoloMamba, a lightweight
asymmetric Mamba-Unet architecture that explicitly models spatial-temporal
correlations across video sequences to enhance reconstruction quality and
computational efficiency. Extensive simulated and real-world experiments
demonstrate that SGDDM achieves high-fidelity full-color display without
compromise in frame rate, while HoloMamba generates FHD (1080p) full-color
holographic video at over 260 FPS, more than 2.6$\times$ faster than the prior
state-of-the-art Divide-Conquer-and-Merge Strategy.

</details>


### [32] [Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction](https://arxiv.org/abs/2508.19581)
*Dat Nguyen Cong,Hieu Tran Bao,Hoang Thanh-Tung*

Main category: cs.CV

TL;DR: SBDC通过对抗性损失训练的判别器指导扩散模型，提升生成能力和可控性，计算高效且无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在大规模数据集上表现优异，但数据集中的标签错误对生成能力和可控性的影响尚未充分研究。

Method: 本文介绍了SBDC，一种基于对抗性损失训练的判别器指导技术，利用先前的噪声检测技术评估样本的真实性，并限制指导使用于生成过程的早期阶段。

Result: 在不同噪声设置下的实验表明，SBDC方法在性能上优于现有最先进方法。

Conclusion: 本文提出的Score-based Discriminator Correction（SBDC）方法通过对抗性损失训练的判别器指导预训练条件扩散模型，有效提高了生成能力和可控性，且计算效率高，无需重新训练模型。

Abstract: Diffusion models have gained prominence as state-of-the-art techniques for
synthesizing images and videos, particularly due to their ability to scale
effectively with large datasets. Recent studies have uncovered that these
extensive datasets often contain mistakes from manual labeling processes.
However, the extent to which such errors compromise the generative capabilities
and controllability of diffusion models is not well studied. This paper
introduces Score-based Discriminator Correction (SBDC), a guidance technique
for aligning noisy pre-trained conditional diffusion models. The guidance is
built on discriminator training using adversarial loss, drawing on prior noise
detection techniques to assess the authenticity of each sample. We further show
that limiting the usage of our guidance to the early phase of the generation
process leads to better performance. Our method is computationally efficient,
only marginally increases inference time, and does not require retraining
diffusion models. Experiments on different noise settings demonstrate the
superiority of our method over previous state-of-the-art methods.

</details>


### [33] [Generalizing Monocular 3D Object Detection](https://arxiv.org/abs/2508.19593)
*Abhinav Kumar*

Main category: cs.CV

TL;DR: 论文提出四种方法提升单目3D检测的泛化能力，覆盖遮挡、数据集、大物体及摄像机参数变化等场景。


<details>
  <summary>Details</summary>
Motivation: 单目3D物体检测在自动驾驶、增强现实等应用中至关重要，但现有模型在遮挡、数据集多样性、大物体检测及摄像机参数变化等场景中泛化能力不足。

Method: 论文提出四种方法：1. 可微分的NMS（GrooMeD-NMS）增强遮挡鲁棒性；2. 深度等变骨干网络（DEVIANT）提升数据集泛化性；3. 基于鸟瞰图的分割方法（SeaBird）解决大物体检测的噪声敏感问题；4. 摄像机高度外推的数学分析。

Result: 提出的方法在遮挡鲁棒性、数据集泛化、大物体检测及摄像机高度外推方面均取得显著改进。

Conclusion: 本论文通过提出GrooMeD-NMS、DEVIANT骨干网络、SeaBird分割方法及摄像机高度外推分析，显著提升了单目3D物体检测模型在多样化场景中的泛化能力。

Abstract: Monocular 3D object detection (Mono3D) is a fundamental computer vision task
that estimates an object's class, 3D position, dimensions, and orientation from
a single image. Its applications, including autonomous driving, augmented
reality, and robotics, critically rely on accurate 3D environmental
understanding. This thesis addresses the challenge of generalizing Mono3D
models to diverse scenarios, including occlusions, datasets, object sizes, and
camera parameters. To enhance occlusion robustness, we propose a mathematically
differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we
explore depth equivariant (DEVIANT) backbones. We address the issue of large
object detection, demonstrating that it's not solely a data imbalance or
receptive field problem but also a noise sensitivity issue. To mitigate this,
we introduce a segmentation-based approach in bird's-eye view with dice loss
(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D
models to unseen camera heights and improve Mono3D generalization in such
out-of-distribution settings.

</details>


### [34] [Quantization Robustness to Input Degradations for Object Detection](https://arxiv.org/abs/2508.19600)
*Toghrul Karimov,Hassan Imani,Allan Kazakov*

Main category: cs.CV

TL;DR: 本文研究了YOLO模型在不同量化精度下的鲁棒性，提出了一种退化感知校准策略，但发现其对大多数模型和退化条件效果有限，仅在特定情况下有效。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索量化后训练（PTQ）对模型鲁棒性的影响，尤其是在面对真实世界输入退化（如噪声、模糊和压缩伪影）时的表现。

Method: 本文通过实证研究评估了YOLO模型（从nano到extra-large规模）在多种精度格式下的鲁棒性，并引入了一种退化感知校准策略用于静态INT8 PTQ。

Result: 结果表明，退化感知校准仅在特定噪声条件下对较大规模模型有效，暗示模型容量可能影响该校准方法的有效性。

Conclusion: 研究发现，尽管静态INT8 TensorRT引擎在干净数据上提供了显著的加速（约1.5-3.3倍）和适度的准确率下降（约3-7% mAP50-95），但提出的退化感知校准策略并未在大多数模型和退化条件下带来广泛且一致的鲁棒性提升。

Abstract: Post-training quantization (PTQ) is crucial for deploying efficient object
detection models, like YOLO, on resource-constrained devices. However, the
impact of reduced precision on model robustness to real-world input
degradations such as noise, blur, and compression artifacts is a significant
concern. This paper presents a comprehensive empirical study evaluating the
robustness of YOLO models (nano to extra-large scales) across multiple
precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8
(TensorRT). We introduce and evaluate a degradation-aware calibration strategy
for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix
of clean and synthetically degraded images. Models were benchmarked on the COCO
dataset under seven distinct degradation conditions (including various types
and levels of noise, blur, low contrast, and JPEG compression) and a
mixed-degradation scenario. Results indicate that while Static INT8 TensorRT
engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop
(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did
not yield consistent, broad improvements in robustness over standard clean-data
calibration across most models and degradations. A notable exception was
observed for larger model scales under specific noise conditions, suggesting
model capacity may influence the efficacy of this calibration approach. These
findings highlight the challenges in enhancing PTQ robustness and provide
insights for deploying quantized detectors in uncontrolled environments. All
code and evaluation tables are available at https://github.com/AllanK24/QRID.

</details>


### [35] [IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2508.19604)
*Qizhe Fan,Chaoyu Liu,Zhonghua Qiao,Xiaoqin Shen*

Main category: cs.CV

TL;DR: 提出IELDM和IELFormer，通过逆演化层和多尺度频率融合模块提升域广义语义分割性能，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的合成数据常存在结构或语义缺陷，直接用于训练会导致性能下降和错误累积，因此需要一种机制来过滤这些缺陷并提升模型泛化能力。

Method: 提出IELDM框架，通过逆演化层（IELs）过滤生成图像中的缺陷；并设计IELFormer，将IELs嵌入分割网络解码器，结合MFF模块增强多尺度语义一致性。

Result: 实验表明，IELDM能生成更高质量图像，IELFormer在跨域场景中表现出更强的泛化能力，优于现有方法。

Conclusion: IELDM和IELFormer通过整合逆演化层（IELs）和多尺度频率融合（MFF）模块，显著提升了域广义语义分割（DGSS）的性能，实验证明其在多个基准数据集上优于现有方法。

Abstract: Domain Generalized Semantic Segmentation (DGSS) focuses on training a model
using labeled data from a source domain, with the goal of achieving robust
generalization to unseen target domains during inference. A common approach to
improve generalization is to augment the source domain with synthetic data
generated by diffusion models (DMs). However, the generated images often
contain structural or semantic defects due to training imperfections. Training
segmentation models with such flawed data can lead to performance degradation
and error accumulation. To address this issue, we propose to integrate inverse
evolution layers (IELs) into the generative process. IELs are designed to
highlight spatial discontinuities and semantic inconsistencies using
Laplacian-based priors, enabling more effective filtering of undesirable
generative patterns. Based on this mechanism, we introduce IELDM, an enhanced
diffusion-based data augmentation framework that can produce higher-quality
images. Furthermore, we observe that the defect-suppression capability of IELs
can also benefit the segmentation network by suppressing artifact propagation.
Based on this insight, we embed IELs into the decoder of the DGSS model and
propose IELFormer to strengthen generalization capability in cross-domain
scenarios. To further strengthen the model's semantic consistency across
scales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,
which performs frequency-domain analysis to achieve structured integration of
multi-resolution features, thereby improving cross-scale coherence. Extensive
experiments on benchmark datasets demonstrate that our approach achieves
superior generalization performance compared to existing methods.

</details>


### [36] [Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model](https://arxiv.org/abs/2508.19626)
*Jiajun Sun,Zhen Yu,Siyuan Yan,Jason J. Ong,Zongyuan Ge,Lei Zhang*

Main category: cs.CV

TL;DR: LF-VAR模型通过结合量化病灶测量和类型标签，实现了高质量、可控的皮肤图像合成，显著提升了图像保真度，并在七种病灶类型中取得最佳FID分数（平均0.74）。


<details>
  <summary>Details</summary>
Motivation: 解决现有皮肤图像合成方法生成的图像质量低且难以控制病灶位置和类型的问题，提出一种能够基于语言提示生成具有特定病灶特征的高质量皮肤图像的模型。

Method: LF-VAR模型采用多尺度病灶聚焦的向量量化变分自编码器（VQVAE）对图像进行离散潜在表示编码，随后通过基于token化表示的视觉自回归（VAR）变换器进行图像合成。病灶测量和类型作为条件嵌入被整合以增强合成保真度。

Result: LF-VAR在七种病灶类型中取得了最佳FID分数（平均0.74），比现有技术提升了6.3%，证明了其在生成高保真、临床相关合成皮肤图像方面的有效性。

Conclusion: LF-VAR模型通过结合量化病灶测量评分和病灶类型标签，成功实现了高质量、临床相关的皮肤图像合成，显著提升了合成图像的保真度，并在七种病灶类型中取得了最佳FID分数（平均0.74），比现有技术提升了6.3%。

Abstract: Skin images from real-world clinical practice are often limited, resulting in
a shortage of training data for deep-learning models. While many studies have
explored skin image synthesis, existing methods often generate low-quality
images and lack control over the lesion's location and type. To address these
limitations, we present LF-VAR, a model leveraging quantified lesion
measurement scores and lesion type labels to guide the clinically relevant and
controllable synthesis of skin images. It enables controlled skin synthesis
with specific lesion characteristics based on language prompts. We train a
multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to
encode images into discrete latent representations for structured tokenization.
Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized
representations facilitates image synthesis. Lesion measurement from the lesion
region and types as conditional embeddings are integrated to enhance synthesis
fidelity. Our method achieves the best overall FID score (average 0.74) among
seven lesion types, improving upon the previous state-of-the-art (SOTA) by
6.3%. The study highlights our controllable skin synthesis model's
effectiveness in generating high-fidelity, clinically relevant synthetic skin
images. Our framework code is available at
https://github.com/echosun1996/LF-VAR.

</details>


### [37] [Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition](https://arxiv.org/abs/2508.19630)
*Xiaolei Wei,Yi Ouyang,Haibo Ye*

Main category: cs.CV

TL;DR: DQRoute结合难度感知优化和动态专家协作，有效提升长尾视觉识别性能。


<details>
  <summary>Details</summary>
Motivation: 长尾视觉识别不仅面临类别不平衡，还因不同类别的分类难度差异而复杂化。简单地按频率重加权类别会忽视那些本质上难以学习的类别。

Method: 提出DQRoute框架，包括基于预测不确定性和历史表现的类别难度估计、自适应损失加权训练，以及采用混合专家设计进行动态专家协作。

Result: 在标准长尾基准测试中，DQRoute显著提高了性能，尤其是在稀有和困难类别上。

Conclusion: DQRoute通过结合难度感知优化和动态专家协作，显著提升了长尾视觉识别任务的性能，特别是在稀有和困难类别上。

Abstract: Long-tailed visual recognition is challenging not only due to class imbalance
but also because of varying classification difficulty across categories. Simply
reweighting classes by frequency often overlooks those that are intrinsically
hard to learn. To address this, we propose \textbf{DQRoute}, a modular
framework that combines difficulty-aware optimization with dynamic expert
collaboration. DQRoute first estimates class-wise difficulty based on
prediction uncertainty and historical performance, and uses this signal to
guide training with adaptive loss weighting. On the architectural side, DQRoute
employs a mixture-of-experts design, where each expert specializes in a
different region of the class distribution. At inference time, expert
predictions are weighted by confidence scores derived from expert-specific OOD
detectors, enabling input-adaptive routing without the need for a centralized
router. All components are trained jointly in an end-to-end manner. Experiments
on standard long-tailed benchmarks demonstrate that DQRoute significantly
improves performance, particularly on rare and difficult classes, highlighting
the benefit of integrating difficulty modeling with decentralized expert
routing.

</details>


### [38] [Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception](https://arxiv.org/abs/2508.19638)
*Yang Li,Quan Yuan,Guiyang Luo,Xiaoyuan Fu,Rui Pan,Yujia Yang,Congzhang Shao,Yuewen Liu,Jinglin Li*

Main category: cs.CV

TL;DR: CoPLOT通过点级优化令牌和创新的处理流程，提升协作感知性能，同时降低开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用2D鸟瞰图表示中间特征，但丢弃了关键的3D结构信息。为保留这些信息并提升对象识别和定位的准确性，引入了点级令牌作为中间表示。

Method: CoPLOT采用点原生处理流程，包括令牌重排序、序列建模和多智能体空间对齐。具体包括语义感知令牌重排序模块、频率增强状态空间模型和邻居到自我对齐模块。

Result: 在模拟和真实数据集上的实验表明，CoPLOT优于现有最先进模型，且通信和计算开销更低。

Conclusion: CoPLOT框架通过点级优化令牌和创新的处理流程，显著提升了协作感知的准确性和效率，同时降低了通信和计算开销。

Abstract: Collaborative perception allows agents to enhance their perceptual
capabilities by exchanging intermediate features. Existing methods typically
organize these intermediate features as 2D bird's-eye-view (BEV)
representations, which discard critical fine-grained 3D structural cues
essential for accurate object recognition and localization. To this end, we
first introduce point-level tokens as intermediate representations for
collaborative perception. However, point-cloud data are inherently unordered,
massive, and position-sensitive, making it challenging to produce compact and
aligned point-level token sequences that preserve detailed structural
information. Therefore, we present CoPLOT, a novel Collaborative perception
framework that utilizes Point-Level Optimized Tokens. It incorporates a
point-native processing pipeline, including token reordering, sequence
modeling, and multi-agent spatial alignment. A semantic-aware token reordering
module generates adaptive 1D reorderings by leveraging scene-level and
token-level semantic information. A frequency-enhanced state space model
captures long-range sequence dependencies across both spatial and spectral
domains, improving the differentiation between foreground tokens and background
clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop
process, combining global agent-level correction with local token-level
refinement to mitigate localization noise. Extensive experiments on both
simulated and real-world datasets show that CoPLOT outperforms state-of-the-art
models, with even lower communication and computation overhead. Code will be
available at https://github.com/CheeryLeeyy/CoPLOT.

</details>


### [39] [UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks](https://arxiv.org/abs/2508.19647)
*Bikash Kumar Badatya,Vipul Baghel,Ravi Hegde*

Main category: cs.CV

TL;DR: 提出了一种轻量级无监督骨架动作定位方法，利用时空图神经表示和动作动态度量，在保持高效的同时达到监督方法的性能，并展现良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于快速和微妙的运动转换，未经修剪的运动视频中的细粒度动作定位具有挑战性，现有方法依赖大量标注数据和高容量模型，计算成本高且适应性差。

Method: 引入了一种基于时空图神经表示的轻量级无监督骨架动作定位流程，包括预训练基于注意力的时空图卷积网络（ASTGCN）进行姿态序列去噪，并定义了一种新颖的动作动态度量（ADM）来检测运动边界。

Result: 在DSV Diving数据集上，该方法达到了82.66%的平均精度（mAP）和29.09毫秒的平均定位延迟，与最先进的监督性能相匹配。

Conclusion: 该论文提出的轻量级无监督骨架动作定位方法在保持计算效率的同时，达到了与监督方法相当的性能，并在未经训练的野外潜水视频中表现出良好的泛化能力，适用于嵌入式或动态环境中的实时动作分析系统。

Abstract: Fine-grained action localization in untrimmed sports videos presents a
significant challenge due to rapid and subtle motion transitions over short
durations. Existing supervised and weakly supervised solutions often rely on
extensive annotated datasets and high-capacity models, making them
computationally intensive and less adaptable to real-world scenarios. In this
work, we introduce a lightweight and unsupervised skeleton-based action
localization pipeline that leverages spatio-temporal graph neural
representations. Our approach pre-trains an Attention-based Spatio-Temporal
Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with
blockwise partitions, enabling it to learn intrinsic motion dynamics without
any manual labeling. At inference, we define a novel Action Dynamics Metric
(ADM), computed directly from low-dimensional ASTGCN embeddings, which detects
motion boundaries by identifying inflection points in its curvature profile.
Our method achieves a mean Average Precision (mAP) of 82.66% and average
localization latency of 29.09 ms on the DSV Diving dataset, matching
state-of-the-art supervised performance while maintaining computational
efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving
footage without retraining, demonstrating its practical applicability for
lightweight, real-time action analysis systems in embedded or dynamic
environments.

</details>


### [40] [IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising](https://arxiv.org/abs/2508.19649)
*Dongjin Kim,Jaekyun Ko,Muhammad Kashif Ali,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 该论文提出一种动态生成核的图像去噪方法，通过高效操作避免过拟合并提升泛化能力，模型在多种噪声下表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法在图像去噪中表现优异，但对特定噪声分布的依赖限制了其泛化能力，现有方法存在过拟合和高计算资源需求的问题。

Method: 利用特征提取模块获取噪声不变特征，结合全局统计和局部相关性模块捕捉噪声特征和结构关联，通过核预测模块生成像素级变化的核，并迭代应用于去噪。

Result: 尽管仅使用单级高斯噪声训练，紧凑模型（约0.04 M）在多种噪声类型和级别上均表现出色。

Conclusion: 该论文通过动态生成核的方法有效解决了图像去噪中的过拟合问题，并提升了模型对未见噪声类型的泛化能力，展示了迭代动态滤波在实际图像去噪中的潜力。

Abstract: Image denoising is a fundamental challenge in computer vision, with
applications in photography and medical imaging. While deep learning-based
methods have shown remarkable success, their reliance on specific noise
distributions limits generalization to unseen noise types and levels. Existing
approaches attempt to address this with extensive training data and high
computational resources but they still suffer from overfitting. To address
these issues, we conduct image denoising by utilizing dynamically generated
kernels via efficient operations. This approach helps prevent overfitting and
improves resilience to unseen noise. Specifically, our method leverages a
Feature Extraction Module for robust noise-invariant features, Global
Statistics and Local Correlation Modules to capture comprehensive noise
characteristics and structural correlations. The Kernel Prediction Module then
employs these cues to produce pixel-wise varying kernels adapted to local
structures, which are then applied iteratively for denoising. This ensures both
efficiency and superior restoration quality. Despite being trained on
single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse
noise types and levels, demonstrating the promise of iterative dynamic
filtering for practical image denoising.

</details>


### [41] [Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models](https://arxiv.org/abs/2508.19650)
*Hou Xia,Zheren Fu,Fangcan Ling,Jiajun Li,Yi Tu,Zhendong Mao,Yongdong Zhang*

Main category: cs.CV

TL;DR: Video-LevelGauge是一个专门用于系统评估LVLM位置偏差的基准测试，通过标准化探针和定制化上下文设置揭示了许多开源模型的显著偏差，而商业模型表现一致。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常评估整个视频序列的整体性能，忽略了如上下文位置偏差等细微行为，这是LVLM性能中关键但未充分探索的方面。

Method: 采用标准化探针和定制化上下文设置，灵活控制上下文长度、探针位置和上下文类型，并结合统计测量与形态模式识别来表征偏差。

Result: 构建了一个包含438个手动整理视频的基准测试，生成了1,177个高质量选择题和120个开放式问题，验证了其在暴露位置偏差方面的有效性。评估了27个最先进的LVLM，揭示了显著的位置偏差。

Conclusion: 研究发现，许多领先的开源模型存在显著的位置偏差，而商业模型如Gemini2.5-Pro在整个视频序列中表现一致。进一步分析上下文长度、上下文变化和模型规模为减轻偏差和指导模型改进提供了可行见解。

Abstract: Large video language models (LVLMs) have made notable progress in video
understanding, spurring the development of corresponding evaluation benchmarks.
However, existing benchmarks generally assess overall performance across entire
video sequences, overlooking nuanced behaviors such as contextual positional
bias, a critical yet under-explored aspect of LVLM performance. We present
Video-LevelGauge, a dedicated benchmark designed to systematically assess
positional bias in LVLMs. We employ standardized probes and customized
contextual setups, allowing flexible control over context length, probe
position, and contextual types to simulate diverse real-world scenarios. In
addition, we introduce a comprehensive analysis method that combines
statistical measures with morphological pattern recognition to characterize
bias. Our benchmark comprises 438 manually curated videos spanning multiple
types, yielding 1,177 high-quality multiple-choice questions and 120 open-ended
questions, validated for their effectiveness in exposing positional bias. Based
on these, we evaluate 27 state-of-the-art LVLMs, including both commercial and
open-source models. Our findings reveal significant positional biases in many
leading open-source models, typically exhibiting head or neighbor-content
preferences. In contrast, commercial models such as Gemini2.5-Pro show
impressive, consistent performance across entire video sequences. Further
analyses on context length, context variation, and model scale provide
actionable insights for mitigating bias and guiding model enhancement.

</details>


### [42] [Scalable Object Detection in the Car Interior With Vision Foundation Models](https://arxiv.org/abs/2508.19651)
*Bálint Mészáros,Ahmet Firintepe,Sebastian Schmidt,Stephan Günnemann*

Main category: cs.CV

TL;DR: ODAL框架通过分布式架构和轻量级模型微调，显著提升汽车内部场景理解的性能。


<details>
  <summary>Details</summary>
Motivation: 解决车载系统计算资源有限，无法直接部署复杂AI模型的问题。

Method: 提出ODAL框架，结合车载和云端分布式计算，利用视觉基础模型进行对象检测和定位，并引入ODALbench评估指标。

Result: 微调后的ODAL-LLaVA模型在ODAL$_{score}$上达到89%，比基线提升71%，且优于GPT-4o近20%。

Conclusion: ODAL框架通过分布式架构和轻量级模型微调，显著提升了汽车内部场景理解的性能，为资源受限的车载系统提供了可行的解决方案。

Abstract: AI tasks in the car interior like identifying and localizing externally
introduced objects is crucial for response quality of personal assistants.
However, computational resources of on-board systems remain highly constrained,
restricting the deployment of such solutions directly within the vehicle. To
address this limitation, we propose the novel Object Detection and Localization
(ODAL) framework for interior scene understanding. Our approach leverages
vision foundation models through a distributed architecture, splitting
computational tasks between on-board and cloud. This design overcomes the
resource constraints of running foundation models directly in the car. To
benchmark model performance, we introduce ODALbench, a new metric for
comprehensive assessment of detection and localization.Our analysis
demonstrates the framework's potential to establish new standards in this
domain. We compare the state-of-the-art GPT-4o vision foundation model with the
lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the
lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model
achieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its
baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the
fine-tuned model maintains high detection accuracy while significantly reducing
hallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.

</details>


### [43] [Self-Rewarding Vision-Language Model via Reasoning Decomposition](https://arxiv.org/abs/2508.19652)
*Zongxia Li,Wenhao Yu,Chengsong Huang,Rui Liu,Zhenwen Liang,Fuxiao Liu,Jingxi Che,Dian Yu,Jordan Boyd-Graber,Haitao Mi,Dong Yu*

Main category: cs.CV

TL;DR: Vision-SR1通过自奖励机制改进视觉语言模型的视觉推理，减少视觉幻觉和语言捷径。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型存在视觉幻觉和语言捷径问题，主要由于中间视觉推理缺乏明确指导。

Method: 将视觉语言模型的推理分为视觉感知和语言推理两个阶段，通过自奖励机制结合最终输出监督。

Result: 实验表明，Vision-SR1在多种视觉语言任务中提升了视觉推理能力，减少了视觉幻觉和语言捷径的依赖。

Conclusion: Vision-SR1通过自奖励机制有效提升了视觉语言模型的视觉推理能力，减少了视觉幻觉和语言捷径的依赖。

Abstract: Vision-Language Models (VLMs) often suffer from visual hallucinations, saying
things that are not actually in the image, and language shortcuts, where they
skip the visual part and just rely on text priors. These issues arise because
most post-training methods for VLMs rely on simple verifiable answer matching
and supervise only final outputs, leaving intermediate visual reasoning without
explicit guidance. As a result, VLMs receive sparse visual signals and often
learn to prioritize language-based reasoning over visual perception. To
mitigate this, some existing methods add visual supervision using human
annotations or distilled labels from external large models. However, human
annotations are labor-intensive and costly, and because external signals cannot
adapt to the evolving policy, they cause distributional shifts that can lead to
reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method
that improves visual reasoning without relying on external visual supervisions
via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two
stages: visual perception and language reasoning. The model is first prompted
to produce self-contained visual perceptions that are sufficient to answer the
question without referring back the input image. To validate this
self-containment, the same VLM model is then re-prompted to perform language
reasoning using only the generated perception as input to compute reward. This
self-reward is combined with supervision on final outputs, providing a balanced
training signal that strengthens both visual perception and language reasoning.
Our experiments demonstrate that Vision-SR1 improves visual reasoning,
mitigates visual hallucinations, and reduces reliance on language shortcuts
across diverse vision-language tasks.

</details>


### [44] [Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications](https://arxiv.org/abs/2508.19654)
*Matthias Höfflin,Jürgen Wassner*

Main category: cs.CV

TL;DR: SNNs在神经形态硬件和高输入稀疏性下能显著节能，但需透明评估和明确假设以确保公平比较。


<details>
  <summary>Details</summary>
Motivation: 尽管SNNs长期以来被认为具有固有的能效优势，但最近与常规ANNs的比较研究开始质疑这一声誉，尤其是在数字实现中。本研究旨在通过多输出回归（3-D卫星位置估计）来验证SNNs的能效。

Method: 本研究提出了一种SNN，使用Leaky Integrate-and-Fire（LIF）神经元的膜电位进行训练，并将其与参考CNN在光真实感卫星数据集上进行性能比较。

Result: 提出的SNN在光真实感卫星数据集上实现了与参考CNN相当的MSE。能效分析显示，硬件无关方法预测SNNs比CNNs有50-60%的能效优势，而硬件感知分析则表明只有在神经形态硬件和高输入稀疏性下才能实现显著节能。

Conclusion: 研究发现，只有在神经形态硬件上且输入稀疏性较高时，SNNs才能显著节省能源。这强调了透明评估方法和明确披露潜在假设的重要性，以确保神经网络能效的公平比较。

Abstract: Spiking Neural Networks (SNNs), inspired by biological intelligence, have
long been considered inherently energy-efficient, making them attractive for
resource-constrained domains such as space applications. However, recent
comparative studies with conventional Artificial Neural Networks (ANNs) have
begun to question this reputation, especially for digital implementations. This
work investigates SNNs for multi-output regression, specifically 3-D satellite
position estimation from monocular images, and compares hardware-aware and
hardware-agnostic energy estimation methods. The proposed SNN, trained using
the membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the
final layer, achieves comparable Mean Squared Error (MSE) to a reference
Convolutional Neural Network (CNN) on a photorealistic satellite dataset.
Energy analysis shows that while hardware-agnostic methods predict a consistent
50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals
that significant energy savings are realized only on neuromorphic hardware and
with high input sparsity. The influence of dark pixel ratio on energy
consumption is quantified, emphasizing the impact of data characteristics and
hardware assumptions. These findings highlight the need for transparent
evaluation methods and explicit disclosure of underlying assumptions to ensure
fair comparisons of neural network energy efficiency.

</details>


### [45] [A bag of tricks for real-time Mitotic Figure detection](https://arxiv.org/abs/2508.19804)
*Christian Marzahl,Brian Napora*

Main category: cs.CV

TL;DR: A bag of tricks enhances RTMDet for robust, real-time mitotic figure detection, achieving high F1 scores across diverse domains while balancing accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Mitotic figure detection in histopathology images is challenging due to variations in slide scanners, staining protocols, tissue types, and artifacts. The paper aims to address these challenges for robust, real-time detection.

Method: The paper builds on the efficient RTMDet single-stage object detector, employing extensive multi-domain training data, balanced sampling, careful augmentation, and targeted hard negative mining on necrotic and debris tissue.

Result: The model achieves an F1 score between 0.78 and 0.84 in cross-validation and 0.81 on the MIDOG 2025 preliminary test set, outperforming larger models.

Conclusion: The proposed solution offers a practical trade-off between accuracy and speed, making it attractive for real-world clinical adoption.

Abstract: Mitotic figure (MF) detection in histopathology images is challenging due to
large variations in slide scanners, staining protocols, tissue types, and the
presence of artifacts. This paper presents a collection of training techniques
- a bag of tricks - that enable robust, real-time MF detection across diverse
domains. We build on the efficient RTMDet single stage object detector to
achieve high inference speed suitable for clinical deployment. Our method
addresses scanner variability and tumor heterogeneity via extensive
multi-domain training data, balanced sampling, and careful augmentation.
Additionally, we employ targeted, hard negative mining on necrotic and debris
tissue to reduce false positives. In a grouped 5-fold cross-validation across
multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On
the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025
challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,
outperforming larger models and demonstrating adaptability to new, unfamiliar
domains. The proposed solution offers a practical trade-off between accuracy
and speed, making it attractive for real-world clinical adoption.

</details>


### [46] [A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement](https://arxiv.org/abs/2508.19664)
*Weicheng Liao,Zan Chen,Jianyang Xie,Yalin Zheng,Yuhui Ma,Yitian Zhao*

Main category: cs.CV

TL;DR: 提出一种频率感知自监督学习方法，用于UWF图像增强，结合去模糊和照明补偿模块，显著提升图像质量和诊断性能。


<details>
  <summary>Details</summary>
Motivation: UWF视网膜成像虽革新了视网膜诊断，但常受模糊和照明不均等质量下降因素影响，掩盖病理细节。现有方法未能满足UWF的独特需求，尤其是在保留病理细节方面。

Method: 论文提出了一种频率感知自监督学习方法，包括频率解耦的图像去模糊模块和Retinex引导的照明补偿模块。前者通过不对称通道整合操作结合全局和局部视图，后者引入颜色保留单元以提供多尺度空间和频率信息。

Result: 实验结果表明，该方法不仅提升了可视化质量，还通过恢复和校正局部细节及不均匀强度，改善了疾病诊断性能。

Conclusion: 本研究提出了一种新颖的频率感知自监督学习方法，用于UWF图像增强，通过结合频率解耦的去模糊和Retinex引导的照明补偿模块，不仅提升了可视化质量，还改善了疾病诊断性能。这是首次针对UWF图像增强的尝试，为视网膜疾病管理提供了强有力的临床工具。

Abstract: Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics
by providing a comprehensive view of the retina. However, it often suffers from
quality-degrading factors such as blurring and uneven illumination, which
obscure fine details and mask pathological information. While numerous retinal
image enhancement methods have been proposed for other fundus imageries, they
often fail to address the unique requirements in UWF, particularly the need to
preserve pathological details. In this paper, we propose a novel
frequency-aware self-supervised learning method for UWF image enhancement. It
incorporates frequency-decoupled image deblurring and Retinex-guided
illumination compensation modules. An asymmetric channel integration operation
is introduced in the former module, so as to combine global and local views by
leveraging high- and low-frequency information, ensuring the preservation of
fine and broader structural details. In addition, a color preservation unit is
proposed in the latter Retinex-based module, to provide multi-scale spatial and
frequency information, enabling accurate illumination estimation and
correction. Experimental results demonstrate that the proposed work not only
enhances visualization quality but also improves disease diagnosis performance
by restoring and correcting fine local details and uneven intensity. To the
best of our knowledge, this work is the first attempt for UWF image
enhancement, offering a robust and clinically valuable tool for improving
retinal disease management.

</details>


### [47] [ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images](https://arxiv.org/abs/2508.19815)
*Linkuan Zhou,Zhexin Chen,Yufei Shen,Junlin Xu,Ping Xuan,Yixin Zhu,Yuqi Fang,Cong Cong,Leyi Wei,Ran Su,Jia Zhou,Qiangguo Jin*

Main category: cs.CV

TL;DR: ERSR框架通过双评分过滤、椭圆约束和对称性正则化，显著提升了胎儿头部超声图像的分割效果。


<details>
  <summary>Details</summary>
Motivation: 解决半监督方法在胎儿头部超声图像分割中面临的伪标签生成困难和一致性约束不足的问题。

Method: ERSR框架包括双评分自适应过滤策略、椭圆约束伪标签精炼和基于对称性的多重一致性正则化。

Result: 在HC18数据集上，使用10%和20%标记数据时，Dice分数分别达到92.05%和95.36%；在PSFH数据集上分别为91.68%和93.70%。

Conclusion: 提出的ERSR框架在胎儿头部超声图像分割中表现出色，特别是在标记数据有限的情况下，显著提升了分割性能。

Abstract: Automated segmentation of the fetal head in ultrasound images is critical for
prenatal monitoring. However, achieving robust segmentation remains challenging
due to the poor quality of ultrasound images and the lack of annotated data.
Semi-supervised methods alleviate the lack of annotated data but struggle with
the unique characteristics of fetal head ultrasound images, making it
challenging to generate reliable pseudo-labels and enforce effective
consistency regularization constraints. To address this issue, we propose a
novel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.
Our framework consists of the dual-scoring adaptive filtering strategy, the
ellipse-constrained pseudo-label refinement, and the symmetry-based multiple
consistency regularization. The dual-scoring adaptive filtering strategy uses
boundary consistency and contour regularity criteria to evaluate and filter
teacher outputs. The ellipse-constrained pseudo-label refinement refines these
filtered outputs by fitting least-squares ellipses, which strengthens pixels
near the center of the fitted ellipse and suppresses noise simultaneously. The
symmetry-based multiple consistency regularization enforces multi-level
consistency across perturbed images, symmetric regions, and between original
predictions and pseudo-labels, enabling the model to capture robust and stable
shape representations. Our method achieves state-of-the-art performance on two
benchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%
with 10% and 20% labeled data, respectively. On the PSFH dataset, the scores
are 91.68% and 93.70% under the same settings.

</details>


### [48] [SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction](https://arxiv.org/abs/2508.19688)
*Gangjian Zhang,Jian Shu,Nanjie Yao,Hao Wang*

Main category: cs.CV

TL;DR: SAT框架通过统一学习多种几何先验和在线数据增强，有效解决了单目3D人体重建中的几何模糊性和数据稀缺问题，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 单目2D图像中的几何模糊性和3D人体训练数据的稀缺性是当前3D人体重建领域的主要挑战。现有方法难以有效整合多种几何形式，导致视角不一致问题如面部扭曲。

Method: 提出了一个两阶段的3D人体重建框架SAT，包括统一学习多种几何先验和高质量纹理3D头像重建。此外，引入了Supervisor Feature Regularization模块和多视角网络以优化几何学习，以及Online Animation Augmentation模块来增强数据。

Result: 在两个基准测试上的广泛实验表明，SAT框架在重建质量和性能上优于现有最先进方法。

Conclusion: SAT框架通过统一的几何学习和高品质3D头像重建，显著提升了单目纹理3D人体重建的性能，并在实验中表现出优于现有方法的性能。

Abstract: Monocular texture 3D human reconstruction aims to create a complete 3D
digital avatar from just a single front-view human RGB image. However, the
geometric ambiguity inherent in a single 2D image and the scarcity of 3D human
training data are the main obstacles limiting progress in this field. To
address these issues, current methods employ prior geometric estimation
networks to derive various human geometric forms, such as the SMPL model and
normal maps. However, they struggle to integrate these modalities effectively,
leading to view inconsistencies, such as facial distortions. To this end, we
propose a two-process 3D human reconstruction framework, SAT, which seamlessly
learns various prior geometries in a unified manner and reconstructs
high-quality textured 3D avatars as the final output. To further facilitate
geometry learning, we introduce a Supervisor Feature Regularization module. By
employing a multi-view network with the same structure to provide intermediate
features as training supervision, these varied geometric priors can be better
fused. To tackle data scarcity and further improve reconstruction quality, we
also propose an Online Animation Augmentation module. By building a
one-feed-forward animation network, we augment a massive number of samples from
the original 3D human data online for model training. Extensive experiments on
two benchmarks show the superiority of our approach compared to
state-of-the-art methods.

</details>


### [49] [Gradient Rectification for Robust Calibration under Distribution Shift](https://arxiv.org/abs/2508.19830)
*Yilin Zhang,Cai Xu,You Wu,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种无需目标域信息的新校准框架，通过低频滤波和梯度校正，显著提升分布偏移下的校准性能，同时保持域内性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在安全关键应用中的可靠性因过度自信的预测和分布偏移下的校准问题而受到挑战。现有方法因依赖目标域信息或模拟而实用性受限。

Method: 论文从频域视角出发，提出了一种低频滤波策略以减少对高频视觉线索的依赖，并引入基于梯度的校正机制来强制优化过程中的域内校准。

Result: 在合成和真实世界偏移数据集（如CIFAR-10/100-C和WILDS）上的实验表明，该方法显著改善了分布偏移下的校准性能，同时保持了域内性能。

Conclusion: 论文提出了一种新颖的校准框架，通过低频滤波策略和基于梯度的校正机制，显著提高了分布偏移下的校准性能，同时保持了强健的域内性能。

Abstract: Deep neural networks often produce overconfident predictions, undermining
their reliability in safety-critical applications. This miscalibration is
further exacerbated under distribution shift, where test data deviates from the
training distribution due to environmental or acquisition changes. While
existing approaches improve calibration through training-time regularization or
post-hoc adjustment, their reliance on access to or simulation of target
domains limits their practicality in real-world scenarios. In this paper, we
propose a novel calibration framework that operates without access to target
domain information. From a frequency-domain perspective, we identify that
distribution shifts often distort high-frequency visual cues exploited by deep
models, and introduce a low-frequency filtering strategy to encourage reliance
on domain-invariant features. However, such information loss may degrade
In-Distribution (ID) calibration performance. Therefore, we further propose a
gradient-based rectification mechanism that enforces ID calibration as a hard
constraint during optimization. Experiments on synthetic and real-world shifted
datasets, including CIFAR-10/100-C and WILDS, demonstrate that our method
significantly improves calibration under distribution shift while maintaining
strong in-distribution performance.

</details>


### [50] [Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators](https://arxiv.org/abs/2508.19698)
*V. S. Usatyuk,D. A. Sapozhnikov,S. I. Egorov*

Main category: cs.CV

TL;DR: 论文提出了一种基于物理启发的无监督合成图像检测器，通过稀疏加权图的社区检测方法，在未使用标记数据的情况下实现了高准确率。


<details>
  <summary>Details</summary>
Motivation: 随着GAN和扩散网络等深度生成模型的快速发展，生成的图像几乎与真实照片无法区分，这对媒体取证和生物识别安全构成了挑战。现有的监督检测器对未见过的生成器或对抗性后处理效果不佳，而无监督方法依赖低层次统计线索，仍然脆弱。

Method: 论文提出了一种基于多边类型QC-LDPC图的稀疏加权图构建方法，将图像特征提取并降维至32维后，通过校准Nishimori温度的边耦合转化为随机键Ising模型（RBIM），利用Bethe-Hessian谱的特征间隙区分真实与合成图像。

Result: 该方法在猫与狗、男性与女性的二分类任务中，使用Flickr-Faces-HQ和CelebA的真实照片及GAN和扩散模型生成的合成图像进行验证，实现了超过94%的准确率。光谱分析显示真实图像集存在多个明显分离的间隙，而生成图像则呈现坍塌的光谱。

Conclusion: 该论文提出了一种基于物理启发的模型无关检测器，通过将合成图像识别视为稀疏加权图上的社区检测问题，有效区分真实与合成图像。该方法在未使用标记合成数据或重新训练特征提取器的情况下，实现了超过94%的准确率。未来工作将扩展至视频流和多类异常检测。

Abstract: The rapid advance of deep generative models such as GANs and diffusion
networks now produces images that are virtually indistinguishable from genuine
photographs, undermining media forensics and biometric security. Supervised
detectors quickly lose effectiveness on unseen generators or after adversarial
post-processing, while existing unsupervised methods that rely on low-level
statistical cues remain fragile. We introduce a physics-inspired,
model-agnostic detector that treats synthetic-image identification as a
community-detection problem on a sparse weighted graph. Image features are
first extracted with pretrained CNNs and reduced to 32 dimensions, each feature
vector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities
are transformed into edge couplings calibrated at the Nishimori temperature,
producing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum
exhibits a characteristic gap when genuine community structure (real images) is
present. Synthetic images violate the Nishimori symmetry and therefore lack
such gaps. We validate the approach on binary tasks cat versus dog and male
versus female using real photos from Flickr-Faces-HQ and CelebA and synthetic
counterparts generated by GANs and diffusion models. Without any labeled
synthetic data or retraining of the feature extractor, the detector achieves
over 94% accuracy. Spectral analysis shows multiple well separated gaps for
real image sets and a collapsed spectrum for generated ones. Our contributions
are threefold: a novel LDPC graph construction that embeds deep image features,
an analytical link between Nishimori temperature RBIM and the Bethe-Hessian
spectrum providing a Bayes optimal detection criterion; and a practical,
unsupervised synthetic image detector robust to new generative architectures.
Future work will extend the framework to video streams and multi-class anomaly
detection.

</details>


### [51] [Multispectral LiDAR data for extracting tree points in urban and suburban areas](https://arxiv.org/abs/2508.19881)
*Narges Takhtkeshha,Gabriele Mazzacca,Fabio Remondino,Juha Hyyppä,Gottfried Mandlburger*

Main category: cs.CV

TL;DR: 多光谱LiDAR与深度学习（尤其是SPT模型）显著提升城市树木提取精度，结合pNDVI进一步优化结果。


<details>
  <summary>Details</summary>
Motivation: 城市树木监测对绿化政策和电力设施风险管理至关重要，但复杂环境和树木多样性带来挑战。多光谱LiDAR结合深度学习可提升树木提取的精度和效率。

Method: 研究评估了三种深度学习模型（SPT、PTv3、PTv1）在多光谱LiDAR数据上的树木点提取性能，并比较了结合pNDVI与空间数据的效果。

Result: SPT模型表现最佳，mIoU达85.28%。结合pNDVI与空间数据可将错误率降低10.61个百分点。

Conclusion: 研究强调了多光谱LiDAR与深度学习在树木提取中的潜力，特别是SPT模型在时间效率和准确性上的优势，以及结合pNDVI与空间数据可显著提升检测精度。

Abstract: Monitoring urban tree dynamics is vital for supporting greening policies and
reducing risks to electrical infrastructure. Airborne laser scanning has
advanced large-scale tree management, but challenges remain due to complex
urban environments and tree variability. Multispectral (MS) light detection and
ranging (LiDAR) improves this by capturing both 3D spatial and spectral data,
enabling detailed mapping. This study explores tree point extraction using
MS-LiDAR and deep learning (DL) models. Three state-of-the-art models are
evaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point
Transformer V1 (PTv1). Results show the notable time efficiency and accuracy of
SPT, with a mean intersection over union (mIoU) of 85.28%. The highest
detection accuracy is achieved by incorporating pseudo normalized difference
vegetation index (pNDVI) with spatial data, reducing error rate by 10.61
percentage points (pp) compared to using spatial information alone. These
findings highlight the potential of MS-LiDAR and DL to improve tree extraction
and further tree inventories.

</details>


### [52] [LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation](https://arxiv.org/abs/2508.19699)
*Yupeng Zhang,Dezhi Zheng,Ping Lu,Han Zhang,Lei Wang,Liping xiang,Cheng Luo,Kaijun Deng,Xiaowen Fu,Linlin Shen,Jinbao Wang*

Main category: cs.CV

TL;DR: LabelGS通过引入语义标记和优化策略，增强了3D高斯喷溅的分割能力，并显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 3D高斯喷溅（3DGS）缺乏3D分割能力，限制了其在需要场景理解的任务中的应用。

Method: LabelGS通过跨视图一致的语义掩码、遮挡分析模型、主高斯标记模型和高斯投影滤波器的组合，改进了3D高斯喷溅的优化过程。

Result: LabelGS在3D场景分割任务中优于现有方法（如Feature-3DGS），并在1440X1080分辨率下实现了22倍的训练加速。

Conclusion: LabelGS通过引入跨视图一致的语义掩码和创新的遮挡分析模型，显著提升了3D高斯喷溅的3D分割能力，并在训练效率上实现了22倍的加速。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation
for 3D scenes, offering both high-fidelity reconstruction and efficient
rendering. However, 3DGS lacks 3D segmentation ability, which limits its
applicability in tasks that require scene understanding. The identification and
isolating of specific object components is crucial. To address this limitation,
we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments
the Gaussian representation with object label.LabelGS introduces cross-view
consistent semantic masks for 3D Gaussians and employs a novel Occlusion
Analysis Model to avoid overfitting occlusion during optimization, Main
Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian
Projection Filter to avoid Gaussian label conflict. Our approach achieves
effective decoupling of Gaussian representations and refines the 3DGS
optimization process through a random region sampling strategy, significantly
improving efficiency. Extensive experiments demonstrate that LabelGS
outperforms previous state-of-the-art methods, including Feature-3DGS, in the
3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup
in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code
will be at https://github.com/garrisonz/LabelGS.

</details>


### [53] [FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation](https://arxiv.org/abs/2508.19705)
*Qiang Hu,Ying Zhou,Gepeng Ji,Nick Barnes,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: FreeVPS通过结合IPS和SAM2的优势，并引入两个无需训练的模块，解决了VPS中的错误累积问题，提升了分割稳定性，适用于临床场景。


<details>
  <summary>Details</summary>
Motivation: 现有视频息肉分割（VPS）范式在时空建模和领域泛化之间难以平衡，限制了其在真实临床场景中的适用性。

Method: 提出了两个无需训练的模块：内部关联过滤模块消除检测阶段的空间不准确性，减少假阳性；外部关联细化模块自适应更新记忆库以防止错误传播，增强时间连贯性。

Result: FreeVPS在域内和域外场景中均达到了最先进的性能，并在长时未剪辑结肠镜视频中展示了强大的跟踪能力。

Conclusion: FreeVPS通过结合IPS模型的空间上下文和SAM2的时间建模能力，解决了视频息肉分割中的错误累积问题，并在长时未剪辑结肠镜视频中展示了强大的跟踪能力，具有可靠的临床分析潜力。

Abstract: Existing video polyp segmentation (VPS) paradigms usually struggle to balance
between spatiotemporal modeling and domain generalization, limiting their
applicability in real clinical scenarios. To embrace this challenge, we recast
the VPS task as a track-by-detect paradigm that leverages the spatial contexts
captured by the image polyp segmentation (IPS) model while integrating the
temporal modeling capabilities of segment anything model 2 (SAM2). However,
during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error
accumulation, resulting in a snowball effect that compromises segmentation
stability. We mitigate this issue by repurposing SAM2 as a video polyp
segmenter with two training-free modules. In particular, the intra-association
filtering module eliminates spatial inaccuracies originating from the detecting
stage, reducing false positives. The inter-association refinement module
adaptively updates the memory bank to prevent error propagation over time,
enhancing temporal coherence. Both modules work synergistically to stabilize
SAM2, achieving cutting-edge performance in both in-domain and out-of-domain
scenarios. Furthermore, we demonstrate the robust tracking capabilities of
FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential
reliable clinical analysis.

</details>


### [54] [WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.19927)
*Fayaz Ali,Muhammad Zawish,Steven Davy,Radu Timofte*

Main category: cs.CV

TL;DR: WaveHiT-SR结合小波变换和分层Transformer，显著提升超分辨率性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的超分辨率方法因窗口自注意力的二次计算复杂度，限制了感受野和性能。

Method: 提出了一种嵌入小波变换的分层Transformer框架（WaveHiT-SR），采用自适应分层窗口和多级分解策略。

Result: 实验证明WaveHiT-SR在性能、效率、参数数量和速度上均优于现有方法，如SwinIR-Light、SwinIR-NG和SRFormer-Light。

Conclusion: WaveHiT-SR通过结合小波变换和分层Transformer框架，显著提升了图像超分辨率任务的性能，同时降低了计算复杂度。

Abstract: Transformers have demonstrated promising performance in computer vision
tasks, including image super-resolution (SR). The quadratic computational
complexity of window self-attention mechanisms in many transformer-based SR
methods forces the use of small, fixed windows, limiting the receptive field.
In this paper, we propose a new approach by embedding the wavelet transform
within a hierarchical transformer framework, called (WaveHiT-SR). First, using
adaptive hierarchical windows instead of static small windows allows to capture
features across different levels and greatly improve the ability to model
long-range dependencies. Secondly, the proposed model utilizes wavelet
transforms to decompose images into multiple frequency subbands, allowing the
network to focus on both global and local features while preserving structural
details. By progressively reconstructing high-resolution images through
hierarchical processing, the network reduces computational complexity without
sacrificing performance. The multi-level decomposition strategy enables the
network to capture fine-grained information in lowfrequency components while
enhancing high-frequency textures. Through extensive experimentation, we
confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined
versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR
results, achieving higher efficiency with fewer parameters, lower FLOPs, and
faster speeds.

</details>


### [55] [Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning](https://arxiv.org/abs/2508.19730)
*Stelios Mylonas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 本文提出一种基于面部基础模型的深度伪造检测框架，通过自监督学习和多数据集微调提升泛化能力，实验证明其在真实场景中效果显著。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术的逼真度和普及度提升，媒体真实性和信息完整性面临严峻挑战。现有检测模型在训练分布外的数据上泛化能力不足，尤其在真实场景中表现不佳。

Method: 该方法建立在FSFM（自监督模型）基础上，通过集成多种深度伪造数据集（包括换脸和面部重演）进行微调，并引入三元组损失变体和基于属性的监督方案，以增强模型的判别能力。

Result: 在多种评估基准上的广泛实验表明，该方法在真实场景中具有显著优势，尤其在处理换脸和面部重演等深度伪造内容时表现出色。

Conclusion: 本文提出的基于面部基础模型的深度伪造检测框架，通过利用自监督学习的面部表示和集成多种深度伪造数据集进行微调，显著提升了模型的泛化能力，尤其在真实场景中表现优异。

Abstract: The increasing realism and accessibility of deepfakes have raised critical
concerns about media authenticity and information integrity. Despite recent
advances, deepfake detection models often struggle to generalize beyond their
training distributions, particularly when applied to media content found in the
wild. In this work, we present a robust video deepfake detection framework with
strong generalization that takes advantage of the rich facial representations
learned by face foundation models. Our method is built on top of FSFM, a
self-supervised model trained on real face data, and is further fine-tuned
using an ensemble of deepfake datasets spanning both face-swapping and
face-reenactment manipulations. To enhance discriminative power, we incorporate
triplet loss variants during training, guiding the model to produce more
separable embeddings between real and fake samples. Additionally, we explore
attribution-based supervision schemes, where deepfakes are categorized by
manipulation type or source dataset, to assess their impact on generalization.
Extensive experiments across diverse evaluation benchmarks demonstrate the
effectiveness of our approach, especially in challenging real-world scenarios.

</details>


### [56] [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/abs/2508.19972)
*Seongheon Park,Yixuan Li*

Main category: cs.CV

TL;DR: GLSim通过结合全局和局部信号，提升物体幻觉检测性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言模型中物体幻觉问题，现有方法仅采用全局或局部视角孤立，限制了检测可靠性。

Method: 提出GLSim框架，利用图像和文本模态之间的全局和局部嵌入相似性信号进行训练无关的物体幻觉检测。

Result: GLSim在多样场景中实现了更准确可靠的幻觉检测，显著优于现有基线方法。

Conclusion: GLSim框架通过结合全局和局部嵌入相似性信号，显著提升了物体幻觉检测的准确性和可靠性，优于现有方法。

Abstract: Object hallucination in large vision-language models presents a significant
challenge to their safe deployment in real-world applications. Recent works
have proposed object-level hallucination scores to estimate the likelihood of
object hallucination; however, these methods typically adopt either a global or
local perspective in isolation, which may limit detection reliability. In this
paper, we introduce GLSim, a novel training-free object hallucination detection
framework that leverages complementary global and local embedding similarity
signals between image and text modalities, enabling more accurate and reliable
hallucination detection in diverse scenarios. We comprehensively benchmark
existing object hallucination detection methods and demonstrate that GLSim
achieves superior detection performance, outperforming competitive baselines by
a significant margin.

</details>


### [57] [POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection](https://arxiv.org/abs/2508.19742)
*Chenguang Liu,Chisheng Wang,Yuhua Cai,Chuanhua Zhu,Qingquan Li*

Main category: cs.CV

TL;DR: POEv2是一种改进的线段检测框架，适用于通用和线框线段检测任务，结合高效边缘检测器后在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的线段检测器分为通用和线框两类，由于设计目标不同，彼此在对方任务上表现不佳。因此，需要一种既能处理通用又能处理线框线段检测的鲁棒框架。

Method: POEv2是像素方向估计（POE）方法的改进版本，从边缘强度图中检测线段，并可结合任何边缘检测器使用。

Result: POEv2在三个公开数据集上实现了最先进的性能。

Conclusion: POEv2框架在通用和线框线段检测任务中均表现出色，结合高效边缘检测器后，在三个公开数据集上达到了最先进的性能。

Abstract: Line segment detection in images has been studied for several decades.
Existing line segment detectors can be roughly divided into two categories:
generic line segment detectors and wireframe line segment detectors. Generic
line segment detectors aim to detect all meaningful line segments in images and
traditional approaches usually fall into this category. Recent deep learning
based approaches are mostly wireframe line segment detectors. They detect only
line segments that are geometrically meaningful and have large spatial support.
Due to the difference in the aim of design, the performance of generic line
segment detectors for the task of wireframe line segment detection won't be
satisfactory, and vice versa. In this work, we propose a robust framework that
can be used for both generic line segment detection and wireframe line segment
detection. The proposed method is an improved version of the Pixel Orientation
Estimation (POE) method. It is thus named as POEv2. POEv2 detects line segments
from edge strength maps, and can be combined with any edge detector. We show in
our experiments that by combining the proposed POEv2 with an efficient edge
detector, it achieves state-of-the-art performance on three publicly available
datasets.

</details>


### [58] [SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection](https://arxiv.org/abs/2508.19746)
*Qiyao Xu,Qiming Wu,Xiaowei Li*

Main category: cs.CV

TL;DR: SPLF-SAM模型通过UMFEB和MAFA模块提升光场显著目标检测性能，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有光场显著目标检测模型在提示信息提取和频域信息分析方面存在不足，导致小目标易被噪声掩盖。

Method: 提出了SPLF-SAM模型，包含统一多尺度特征嵌入块（UMFEB）和多尺度自适应过滤适配器（MAFA），前者用于识别不同尺寸的目标，后者通过学习频率特征防止小目标被噪声淹没。

Result: 实验证明，该方法在十种先进光场显著目标检测方法中表现最优。

Conclusion: SPLF-SAM模型通过UMFEB和MAFA模块的引入，显著提升了光场显著目标检测的性能，特别是在多尺度目标识别和噪声抑制方面表现优异。

Abstract: Segment Anything Model (SAM) has demonstrated remarkable capabilities in
solving light field salient object detection (LF SOD). However, most existing
models tend to neglect the extraction of prompt information under this task.
Meanwhile, traditional models ignore the analysis of frequency-domain
information, which leads to small objects being overwhelmed by noise. In this
paper, we put forward a novel model called self-prompting light field segment
anything model (SPLF-SAM), equipped with unified multi-scale feature embedding
block (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is
capable of identifying multiple objects of varying sizes, while MAFA, by
learning frequency features, effectively prevents small objects from being
overwhelmed by noise. Extensive experiments have demonstrated the superiority
of our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be
available at https://github.com/XucherCH/splfsam.

</details>


### [59] [Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices](https://arxiv.org/abs/2508.20064)
*Philippe Zhang,Weili Jiang,Yihao Li,Jing Zhang,Sarah Matta,Yubo Tan,Hui Lin,Haoshen Wang,Jiangtian Pan,Hui Xu,Laurent Borderie,Alexandre Le Guilcher,Béatrice Cochener,Chubin Ou,Gwenolé Quellec,Mathieu Lamard*

Main category: cs.CV

TL;DR: 通过融合CNN和模型集成技术，团队在MARIO挑战赛中成功预测AMD进展，进入前十名，但因与组织者关联无法获奖。


<details>
  <summary>Details</summary>
Motivation: 及时诊断和持续监测AMD进展对于制定个性化治疗方案至关重要，MARIO挑战赛旨在通过OCT扫描跟踪AMD进展。

Method: 在Task 1中使用了融合CNN网络与模型集成技术；在Task 2中提出了Patch Progression Masked Autoencoder，用于生成下一次检查的OCT图像并分类进展。

Result: 团队在MARIO挑战赛的两项任务中均进入前十名，但由于部分成员与组织者同属一个机构，无法参与奖项竞争。

Conclusion: 通过MARIO挑战赛，团队在OCT扫描中成功应用了融合CNN网络和模型集成技术，以及Patch Progression Masked Autoencoder，有效预测了AMD的进展，并在两项任务中均进入前十名。

Abstract: Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting
visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments
have been effective in slowing the progression of neovascular AMD, with better
outcomes achieved through timely diagnosis and consistent monitoring. Tracking
the progression of neovascular activity in OCT scans of patients with exudative
AMD allows for the development of more personalized and effective treatment
plans. This was the focus of the Monitoring Age-related Macular Degeneration
Progression in Optical Coherence Tomography (MARIO) challenge, in which we
participated. In Task 1, which involved classifying the evolution between two
pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN
network with model ensembling to further enhance the model's performance. For
Task 2, which focused on predicting progression over the next three months
based on current exam data, we proposed the Patch Progression Masked
Autoencoder that generates an OCT for the next exam and then classifies the
evolution between the current OCT and the one generated using our solution from
Task 1. The results we achieved allowed us to place in the Top 10 for both
tasks. Some team members are part of the same organization as the challenge
organizers; therefore, we are not eligible to compete for the prize.

</details>


### [60] [FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers](https://arxiv.org/abs/2508.19754)
*Yue Wu,Yufan Wu,Wen Li,Yuxi Lu,Kairui Feng,Xuanhong Chen*

Main category: cs.CV

TL;DR: FastAvatar是一种快速、灵活的3D avatar重建框架，通过创新的Transformer设计和增量高斯聚合技术，显著提升了重建效率和质量。


<details>
  <summary>Details</summary>
Motivation: 当前3D avatar重建存在高时间复杂度、对数据质量敏感和低数据利用率等问题，FastAvatar旨在解决这些挑战。

Method: FastAvatar采用了一种大型高斯重建Transformer，包含三个关键设计：变体VGGT-style Transformer架构、多粒度引导编码和增量高斯聚合。

Result: 实验表明，FastAvatar在质量和速度上均优于现有方法，支持增量重建，即随着更多观测数据的加入，重建质量会进一步提升。

Conclusion: FastAvatar提出了一种高效、灵活的3D avatar重建框架，能够在多种输入条件下快速生成高质量的3D高斯溅射模型，并通过实验验证了其质量和速度的优势。

Abstract: Despite significant progress in 3D avatar reconstruction, it still faces
challenges such as high time complexity, sensitivity to data quality, and low
data utilization. We propose FastAvatar, a feedforward 3D avatar framework
capable of flexibly leveraging diverse daily recordings (e.g., a single image,
multi-view observations, or monocular video) to reconstruct a high-quality 3D
Gaussian Splatting (3DGS) model within seconds, using only a single unified
model. FastAvatar's core is a Large Gaussian Reconstruction Transformer
featuring three key designs: First, a variant VGGT-style transformer
architecture aggregating multi-frame cues while injecting initial 3D prompt to
predict an aggregatable canonical 3DGS representation; Second, multi-granular
guidance encoding (camera pose, FLAME expression, head pose) mitigating
animation-induced misalignment for variable-length inputs; Third, incremental
Gaussian aggregation via landmark tracking and sliced fusion losses.
Integrating these features, FastAvatar enables incremental reconstruction,
i.e., improving quality with more observations, unlike prior work wasting input
data. This yields a quality-speed-tunable paradigm for highly usable avatar
modeling. Extensive experiments show that FastAvatar has higher quality and
highly competitive speed compared to existing methods.

</details>


### [61] [BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions](https://arxiv.org/abs/2508.19762)
*Ahmed Emam,Mohamed Elbassiouny,Julius Miller,Patrick Donworth,Sabine Seidel,Ribana Roscher*

Main category: cs.CV

TL;DR: BuzzSet是一个大规模、高分辨率的传粉昆虫图像数据集，用于支持自动化监测。通过YOLOv12和RF-DETR模型，实现了高精度的分类检测，为生态计算机视觉提供了基准。


<details>
  <summary>Details</summary>
Motivation: 支持可扩展、自动化的传粉昆虫监测，以应对全球传粉昆虫数量下降的问题。

Method: 使用YOLOv12模型生成初始标注，并通过人工验证进行精炼。所有图像预处理为256×256的图块以提高小昆虫的检测效果。提供了基于RF-DETR变换器的目标检测器的强基线。

Result: 模型在蜜蜂和大黄蜂类别上分别实现了0.94和0.92的高F1分数，混淆矩阵结果显示这些类别之间的误分类极少。最佳mAP@0.50为0.559。

Conclusion: BuzzSet提供了一个有价值的基准，用于小物体检测、标签噪声下的类别分离以及生态计算机视觉。

Abstract: Pollinator insects such as honeybees and bumblebees are vital to global food
production and ecosystem stability, yet their populations are declining due to
increasing anthropogenic and environmental stressors. To support scalable,
automated pollinator monitoring, we introduce BuzzSet, a new large-scale
dataset of high-resolution pollinator images collected in real agricultural
field conditions. BuzzSet contains 7856 manually verified and labeled images,
with over 8000 annotated instances across three classes: honeybees, bumblebees,
and unidentified insects. Initial annotations were generated using a YOLOv12
model trained on external data and refined via human verification using
open-source labeling tools. All images were preprocessed into 256~$\times$~256
tiles to improve the detection of small insects. We provide strong baselines
using the RF-DETR transformer-based object detector. The model achieves high
F1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively,
with confusion matrix results showing minimal misclassification between these
categories. The unidentified class remains more challenging due to label
ambiguity and lower sample frequency, yet still contributes useful insights for
robustness evaluation. Overall detection quality is strong, with a best
mAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object
detection, class separation under label noise, and ecological computer vision.

</details>


### [62] [CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning](https://arxiv.org/abs/2508.20096)
*Zeyi Sun,Yuhang Cao,Jianze Liang,Qiushi Sun,Ziyu Liu,Zhixiong Zhang,Yuhang Zang,Xiaoyi Dong,Kai Chen,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: CODA是一个可训练的框架，结合通用规划器和专业执行器，通过两阶段训练在科学计算GUI任务中表现优异，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对科学计算领域GUI自主代理在长期规划和精确执行上的挑战，现有方法存在通用代理规划能力强但执行差、专用代理执行强但规划弱的矛盾。CODA旨在通过可训练的框架解决这一矛盾。

Method: CODA框架包含一个通用的规划器（Cerebrum）和一个专业的执行器（Cerebellum），采用两阶段训练流程：第一阶段是专业化，通过GRPO方法训练针对每个科学应用的专家规划器；第二阶段是泛化，整合所有成功轨迹用于监督微调最终规划器。

Result: 在ScienceBoard基准的四个挑战性应用中，CODA显著优于基线模型，成为开源模型中的新标杆。

Conclusion: CODA通过结合通用的规划器和专业的执行器，并通过两阶段训练流程，显著提升了在科学计算领域的GUI自主代理性能，超越了现有基线并设立了新的开源模型标杆。

Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant
challenges in specialized domains such as scientific computing, where both
long-horizon planning and precise execution are required. Existing approaches
suffer from a trade-off: generalist agents excel at planning but perform poorly
in execution, while specialized agents demonstrate the opposite weakness.
Recent compositional frameworks attempt to bridge this gap by combining a
planner and an actor, but they are typically static and non-trainable, which
prevents adaptation from experience. This is a critical limitation given the
scarcity of high-quality data in scientific domains. To address these
limitations, we introduce CODA, a novel and trainable compositional framework
that integrates a generalist planner (Cerebrum) with a specialist executor
(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,
Specialization, we apply a decoupled GRPO approach to train an expert planner
for each scientific application individually, bootstrapping from a small set of
task trajectories. In the second stage, Generalization, we aggregate all
successful trajectories from the specialized experts to build a consolidated
dataset, which is then used for supervised fine-tuning of the final planner.
This equips CODA with both robust execution and cross-domain generalization.
Evaluated on four challenging applications from the ScienceBoard benchmark,
CODA significantly outperforms baselines and establishes a new state of the art
among open-source models.

</details>


### [63] [AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning](https://arxiv.org/abs/2508.19769)
*Shu Shen,C. L. Philip Chen,Tong Zhang*

Main category: cs.CV

TL;DR: AIM通过自适应网络内调制，平衡多模态学习，不抑制任何模态，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过抑制主导模态来提升弱势模态，影响了整体多模态性能。研究发现优化偏差是主要原因，提出AIM来解决这一问题。

Method: 提出Adaptive Intra-Network Modulation (AIM)，通过解耦主导模态的未优化参数到Auxiliary Blocks，并联合训练弱势模态，同时根据网络深度自适应调整调制强度。

Result: 实验表明，AIM在多个基准测试中优于现有不平衡模态学习方法，并展现出对不同主干网络、融合策略和优化器的强泛化能力。

Conclusion: AIM通过自适应调整网络内参数的优化状态，首次实现了在不抑制主导或弱势模态的情况下，平衡多模态学习，显著提升了性能。

Abstract: Multimodal learning has significantly enhanced machine learning performance
but still faces numerous challenges and limitations. Imbalanced multimodal
learning is one of the problems extensively studied in recent works and is
typically mitigated by modulating the learning of each modality. However, we
find that these methods typically hinder the dominant modality's learning to
promote weaker modalities, which affects overall multimodal performance. We
analyze the cause of this issue and highlight a commonly overlooked problem:
optimization bias within networks. To address this, we propose Adaptive
Intra-Network Modulation (AIM) to improve balanced modality learning. AIM
accounts for differences in optimization state across parameters and depths
within the network during modulation, achieving balanced multimodal learning
without hindering either dominant or weak modalities for the first time.
Specifically, AIM decouples the dominant modality's under-optimized parameters
into Auxiliary Blocks and encourages reliance on these performance-degraded
blocks for joint training with weaker modalities. This approach effectively
prevents suppression of weaker modalities while enabling targeted optimization
of under-optimized parameters to improve the dominant modality. Additionally,
AIM assesses modality imbalance level across network depths and adaptively
adjusts modulation strength at each depth. Experimental results demonstrate
that AIM outperforms state-of-the-art imbalanced modality learning methods
across multiple benchmarks and exhibits strong generalizability across
different backbones, fusion strategies, and optimizers.

</details>


### [64] [The Return of Structural Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.19773)
*Jakob Seitz,Tobias Lengfeld,Radu Timofte*

Main category: cs.CV

TL;DR: 论文提出了一种结构识别方法，通过自动标注和模块化系统实现符号到轨迹的显式对齐，提升了错误分析和可解释性，并在CROHME-2023中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代编码器-解码器架构在LaTeX生成方面表现出色，但缺乏符号到轨迹的显式对齐，这限制了错误分析、可解释性及需要选择性内容更新的空间感知交互应用。

Method: 1. 使用神经网络自动将LaTeX方程映射到原始轨迹，生成符号分割、分类和空间关系的注释；2. 模块化结构识别系统独立优化分割、分类和关系预测。

Result: 在CROHME-2023基准测试中取得了竞争性性能，生成了完整的图结构，直接链接手写轨迹与预测符号。

Conclusion: 该论文提出的结构识别方法通过自动标注系统和模块化结构识别系统，成功实现了符号到轨迹的显式对齐，为错误分析和可解释输出提供了透明的基础。

Abstract: Handwritten Mathematical Expression Recognition is foundational for
educational technologies, enabling applications like digital note-taking and
automated grading. While modern encoder-decoder architectures with large
language models excel at LaTeX generation, they lack explicit symbol-to-trace
alignment, a critical limitation for error analysis, interpretability, and
spatially aware interactive applications requiring selective content updates.
This paper introduces a structural recognition approach with two innovations: 1
an automatic annotation system that uses a neural network to map LaTeX
equations to raw traces, automatically generating annotations for symbol
segmentation, classification, and spatial relations, and 2 a modular structural
recognition system that independently optimizes segmentation, classification,
and relation prediction. By leveraging a dataset enriched with structural
annotations from our auto-labeling system, the proposed recognition system
combines graph-based trace sorting, a hybrid convolutional-recurrent network,
and transformer-based correction to achieve competitive performance on the
CROHME-2023 benchmark. Crucially, our structural recognition system generates a
complete graph structure that directly links handwritten traces to predicted
symbols, enabling transparent error analysis and interpretable outputs.

</details>


### [65] [MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.19786)
*Han Jiao,Jiakai Sun,Yexing Xu,Lei Zhao,Wei Xing,Huaizhong Lin*

Main category: cs.CV

TL;DR: MAPo通过动态分区和跨帧一致性损失，优化了动态3D高斯重建的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于变形场的方法在高度动态区域容易产生模糊渲染和丢失细节，需要一种更精细的建模方法。

Method: 采用动态分数分区策略，区分高动态和低动态3D高斯，并递归分区高动态3D高斯，同时引入跨帧一致性损失。

Result: 实验表明，MAPo在保持计算成本的同时，显著提升了渲染质量。

Conclusion: MAPo通过动态分区和跨帧一致性损失，显著提升了动态场景重建的渲染质量，特别是在复杂或快速运动区域。

Abstract: 3D Gaussian Splatting, known for enabling high-quality static scene
reconstruction with fast rendering, is increasingly being applied to dynamic
scene reconstruction. A common strategy involves learning a deformation field
to model the temporal changes of a canonical set of 3D Gaussians. However,
these deformation-based methods often produce blurred renderings and lose fine
motion details in highly dynamic regions due to the inherent limitations of a
single, unified model in representing diverse motion patterns. To address these
challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian
Splatting (MAPo), a novel framework for high-fidelity dynamic scene
reconstruction. Its core is a dynamic score-based partitioning strategy that
distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D
Gaussians, we recursively partition them temporally and duplicate their
deformation networks for each new temporal segment, enabling specialized
modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs
are treated as static to reduce computational costs. However, this temporal
partitioning strategy for high-dynamic 3DGs can introduce visual
discontinuities across frames at the partition boundaries. To address this, we
introduce a cross-frame consistency loss, which not only ensures visual
continuity but also further enhances rendering quality. Extensive experiments
demonstrate that MAPo achieves superior rendering quality compared to baselines
while maintaining comparable computational costs, particularly in regions with
complex or rapid motions.

</details>


### [66] [StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation](https://arxiv.org/abs/2508.19789)
*Xiuchao Wu,Pengfei Zhu,Jiangjing Lyu,Xinguo Liu,Jie Guo,Yanwen Guo,Weiwei Xu,Chengfei Lyu*

Main category: cs.CV

TL;DR: StableIntrinsic 是一种一步扩散模型，通过像素空间损失和 DIN 网络，显著提升了多视角材料估计的效率和质量，实验结果显示其在 PSNR 和 MSE 指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的材料估计方法采用多步去噪策略，耗时且结果方差高，与确定性材料估计任务冲突。本文旨在提出一种高效、低方差的一步扩散模型。

Method: StableIntrinsic 采用一步扩散模型，结合像素空间损失和细节注入网络（DIN），以解决多步去噪策略的时间消耗和方差问题。

Result: 实验结果表明，StableIntrinsic 在 PSNR 和 MSE 指标上显著优于现有技术，其中 albedo 的 PSNR 提升了 9.9%，金属性和粗糙度的 MSE 分别降低了 44.4% 和 60.0%。

Conclusion: StableIntrinsic 是一种一步扩散模型，用于多视角材料估计，能够生成高质量、低方差的材料参数。通过像素空间损失和细节注入网络（DIN），该方法有效解决了过度平滑和细节丢失问题，显著提升了材料预测的清晰度。

Abstract: Recovering material information from images has been extensively studied in
computer graphics and vision. Recent works in material estimation leverage
diffusion model showing promising results. However, these diffusion-based
methods adopt a multi-step denoising strategy, which is time-consuming for each
estimation. Such stochastic inference also conflicts with the deterministic
material estimation task, leading to a high variance estimated results. In this
paper, we introduce StableIntrinsic, a one-step diffusion model for multi-view
material estimation that can produce high-quality material parameters with low
variance. To address the overly-smoothing problem in one-step diffusion,
StableIntrinsic applies losses in pixel space, with each loss designed based on
the properties of the material. Additionally, StableIntrinsic introduces a
Detail Injection Network (DIN) to eliminate the detail loss caused by VAE
encoding, while further enhancing the sharpness of material prediction results.
The experimental results indicate that our method surpasses the current
state-of-the-art techniques by achieving a $9.9\%$ improvement in the Peak
Signal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error
(MSE) for metallic and roughness by $44.4\%$ and $60.0\%$, respectively.

</details>


### [67] [Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models](https://arxiv.org/abs/2508.19791)
*Shay Shomer Chai,Wenxuan Peng,Bharath Hariharan,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 本文研究了文本到图像生成中多颜色提示的语义对齐问题，提出了一种新的图像编辑技术，显著提升了生成图像的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成方法在处理复杂多对象提示时存在语义对齐不准确的问题，尤其是涉及多种颜色属性时。本文旨在通过严谨的评估和改进技术来解决这一问题。

Method: 本文对颜色属性进行了案例研究，揭示了预训练模型在处理多颜色提示时的困难，并提出了一种新的图像编辑技术来缓解这一问题。

Result: 实验表明，所提出的图像编辑技术显著提升了多对象语义对齐的性能，适用于多种基于扩散的文本到图像生成方法。

Conclusion: 本文通过引入一种专有的图像编辑技术，显著提升了多对象语义对齐的性能，尤其是在处理包含多种颜色的文本提示时。

Abstract: Text-to-image generation has recently seen remarkable success, granting users
with the ability to create high-quality images through the use of text.
However, contemporary methods face challenges in capturing the precise
semantics conveyed by complex multi-object prompts. Consequently, many works
have sought to mitigate such semantic misalignments, typically via
inference-time schemes that modify the attention layers of the denoising
networks. However, prior work has mostly utilized coarse metrics, such as the
cosine similarity between text and image CLIP embeddings, or human evaluations,
which are challenging to conduct on a larger-scale. In this work, we perform a
case study on colors -- a fundamental attribute commonly associated with
objects in text prompts, which offer a rich test bed for rigorous evaluation.
Our analysis reveals that pretrained models struggle to generate images that
faithfully reflect multiple color attributes-far more so than with single-color
prompts-and that neither inference-time techniques nor existing editing methods
reliably resolve these semantic misalignments. Accordingly, we introduce a
dedicated image editing technique, mitigating the issue of multi-object
semantic alignment for prompts containing multiple colors. We demonstrate that
our approach significantly boosts performance over a wide range of metrics,
considering images generated by various text-to-image diffusion-based
techniques.

</details>


### [68] [FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization](https://arxiv.org/abs/2508.19798)
*Muhammad Ali,Omar Ali AlSuwaidi*

Main category: cs.CV

TL;DR: 通过改进的神经网络架构提升垃圾分拣效率，特别优化了多通道图像处理，测试结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 非生物降解材料的自动分拣因垃圾流的复杂性和多样性面临挑战，需提升现有技术的准确性和效率。

Method: 采用改进的Encoder-Decoder结构，结合综合注意力块、Mamba架构的注意力机制以及数据融合块（通过PCA降维处理多通道图像）。

Result: 在RGB、高光谱、多光谱及其组合数据上的测试表明，该方法显著优于现有技术。

Conclusion: 提出的增强神经架构显著提升了垃圾分拣系统的准确性和效率，特别是在处理多通道图像数据时表现优异。

Abstract: In the realm of waste management, automating the sorting process for
non-biodegradable materials presents considerable challenges due to the
complexity and variability of waste streams. To address these challenges, we
introduce an enhanced neural architecture that builds upon an existing
Encoder-Decoder structure to improve the accuracy and efficiency of waste
sorting systems. Our model integrates several key innovations: a Comprehensive
Attention Block within the decoder, which refines feature representations by
combining convolutional and upsampling operations. In parallel, we utilize
attention through the Mamba architecture, providing an additional performance
boost. We also introduce a Data Fusion Block that fuses images with more than
three channels. To achieve this, we apply PCA transformation to reduce the
dimensionality while retaining the maximum variance and essential information
across three dimensions, which are then used for further processing. We
evaluated the model on RGB, hyperspectral, multispectral, and a combination of
RGB and hyperspectral data. The results demonstrate that our approach
outperforms existing methods by a significant margin.

</details>


### [69] [Context-aware Sparse Spatiotemporal Learning for Event-based Vision](https://arxiv.org/abs/2508.19806)
*Shenqi Wang,Guangzhi Tang*

Main category: cs.CV

TL;DR: CSSL框架通过动态调节神经元激活实现高稀疏度，在事件视觉任务中性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件的深度学习方法未能充分利用事件数据的稀疏性，且神经形态计算中的脉冲神经网络在复杂任务中性能不足，激活稀疏性难以实现。

Method: 提出了Context-aware Sparse Spatiotemporal Learning (CSSL)框架，通过上下文感知阈值动态调节神经元激活，减少激活密度。

Result: CSSL在事件驱动的物体检测和光流估计任务中达到或超越现有最优方法，同时保持极高的神经元稀疏度。

Conclusion: CSSL框架通过动态调节神经元激活，无需显式稀疏约束即可实现高神经元稀疏度，在事件驱动的物体检测和光流估计任务中表现优异，为神经形态处理提供了高效的事件视觉解决方案。

Abstract: Event-based camera has emerged as a promising paradigm for robot perception,
offering advantages with high temporal resolution, high dynamic range, and
robustness to motion blur. However, existing deep learning-based event
processing methods often fail to fully leverage the sparse nature of event
data, complicating their integration into resource-constrained edge
applications. While neuromorphic computing provides an energy-efficient
alternative, spiking neural networks struggle to match of performance of
state-of-the-art models in complex event-based vision tasks, like object
detection and optical flow. Moreover, achieving high activation sparsity in
neural networks is still difficult and often demands careful manual tuning of
sparsity-inducing loss terms. Here, we propose Context-aware Sparse
Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware
thresholding to dynamically regulate neuron activations based on the input
distribution, naturally reducing activation density without explicit sparsity
constraints. Applied to event-based object detection and optical flow
estimation, CSSL achieves comparable or superior performance to
state-of-the-art methods while maintaining extremely high neuronal sparsity.
Our experimental results highlight CSSL's crucial role in enabling efficient
event-based vision for neuromorphic processing.

</details>


### [70] [AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment](https://arxiv.org/abs/2508.19808)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: AutoQ-VIS通过质量引导的自训练方法，在无监督视频实例分割中实现最先进性能，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 视频实例分割（VIS）由于需要像素级掩码和时间一致性标签，面临显著的标注挑战。现有无监督方法依赖合成数据，但仍受限于合成到真实的领域差距。

Method: AutoQ-VIS是一个无监督框架，通过质量引导的自训练，在伪标签生成和自动质量评估之间建立闭环系统，逐步适应从合成到真实视频的转换。

Result: 在YouTubeVIS-2019验证集上达到52.6 $\text{AP}_{50}$，超越之前的最优方法VideoCutLER 4.4%，且无需人工标注。

Conclusion: AutoQ-VIS通过质量引导的自训练方法，有效缩小了合成数据与真实视频之间的领域差距，无需人工标注即可实现最先进的性能，证明了质量感知自训练在无监督视频实例分割中的可行性。

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due
to its dual requirements of pixel-level masks and temporal consistency labels.
While recent unsupervised methods like VideoCutLER eliminate optical flow
dependencies through synthetic data, they remain constrained by the
synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised
framework that bridges this gap through quality-guided self-training. Our
approach establishes a closed-loop system between pseudo-label generation and
automatic quality assessment, enabling progressive adaptation from synthetic to
real videos. Experiments demonstrate state-of-the-art performance with 52.6
$\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous
state-of-the-art VideoCutLER by 4.4$\%$, while requiring no human annotations.
This demonstrates the viability of quality-aware self-training for unsupervised
VIS. The source code of our method is available at
https://github.com/wcbup/AutoQ-VIS.

</details>


### [71] [Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models](https://arxiv.org/abs/2508.19850)
*Xiaoqi Wang,Yun Zhang,Weisi Lin*

Main category: cs.CV

TL;DR: 提出MIQA框架和RA-MIQA模型，通过大规模数据库和区域感知分析，显著提升机器视觉系统的质量评估能力。


<details>
  <summary>Details</summary>
Motivation: 机器视觉系统在恶劣视觉条件下性能易受损，传统基于人类视觉系统的质量评估方法不适用于机器视觉。

Method: 构建了包含250万样本的MIQD-2.5M数据库，并提出区域感知的RA-MIQA模型进行细粒度空间退化分析。

Result: RA-MIQA在一致性和准确性指标上显著优于传统方法，如SRCC增益分别达到13.56%和13.37%。

Conclusion: 本研究通过提出MIQA框架和RA-MIQA模型，显著提升了机器视觉系统在恶劣视觉条件下的性能评估能力，为机器中心的图像处理和优化奠定了基础。

Abstract: Machine vision systems (MVS) are intrinsically vulnerable to performance
degradation under adverse visual conditions. To address this, we propose a
machine-centric image quality assessment (MIQA) framework that quantifies the
impact of image degradations on MVS performance. We establish an MIQA paradigm
encompassing the end-to-end assessment workflow. To support this, we construct
a machine-centric image quality database (MIQD-2.5M), comprising 2.5 million
samples that capture distinctive degradation responses in both consistency and
accuracy metrics, spanning 75 vision models, 250 degradation types, and three
representative vision tasks. We further propose a region-aware MIQA (RA-MIQA)
model to evaluate MVS visual quality through fine-grained spatial degradation
analysis. Extensive experiments benchmark the proposed RA-MIQA against seven
human visual system (HVS)-based IQA metrics and five retrained classical
backbones. Results demonstrate RA-MIQA's superior performance in multiple
dimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on
accuracy for image classification, while also revealing task-specific
degradation sensitivities. Critically, HVS-based metrics prove inadequate for
MVS quality prediction, while even specialized MIQA models struggle with
background degradations, accuracy-oriented estimation, and subtle distortions.
This study can advance MVS reliability and establish foundations for
machine-centric image processing and optimization. The model and code are
available at: https://github.com/XiaoqiWang/MIQA.

</details>


### [72] [Ego-centric Predictive Model Conditioned on Hand Trajectories](https://arxiv.org/abs/2508.19852)
*Binjie Zhang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出统一的两阶段框架，结合动作预测与视觉未来生成，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在联合建模动作预测与视觉未来生成方面存在不足，VLA模型缺乏对动作如何影响视觉场景的显式建模，而视频预测模型则未考虑特定动作的指导。

Method: 采用两阶段方法：第一阶段通过连续状态建模处理异构输入并预测未来手部轨迹；第二阶段引入因果交叉注意力融合多模态线索，利用推断的动作信号指导基于图像的潜在扩散模型生成未来视频。

Result: 在Ego4D、BridgeData和RLBench上的大量实验表明，该方法在动作预测和未来视频合成方面均优于现有基线。

Conclusion: 本文提出了一种统一的两阶段预测框架，首次将动作预测与视觉未来生成相结合，通过实验验证了其在第一人称视角场景中的优越性。

Abstract: In egocentric scenarios, anticipating both the next action and its visual
outcome is essential for understanding human-object interactions and for
enabling robotic planning. However, existing paradigms fall short of jointly
modeling these aspects. Vision-Language-Action (VLA) models focus on action
prediction but lack explicit modeling of how actions influence the visual
scene, while video prediction models generate future frames without
conditioning on specific actions, often resulting in implausible or
contextually inconsistent outcomes. To bridge this gap, we propose a unified
two-stage predictive framework that jointly models action and visual future in
egocentric scenarios, conditioned on hand trajectories. In the first stage, we
perform consecutive state modeling to process heterogeneous inputs (visual
observations, language, and action history) and explicitly predict future hand
trajectories. In the second stage, we introduce causal cross-attention to fuse
multi-modal cues, leveraging inferred action signals to guide an image-based
Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our
approach is the first unified model designed to handle both egocentric human
activity understanding and robotic manipulation tasks, providing explicit
predictions of both upcoming actions and their visual consequences. Extensive
experiments on Ego4D, BridgeData, and RLBench demonstrate that our method
outperforms state-of-the-art baselines in both action prediction and future
video synthesis.

</details>


### [73] [Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction](https://arxiv.org/abs/2508.19862)
*Long Chen,Ashiv Patel,Mengyun Qiao,Mohammad Yousuf Salmasi,Salah A. Hammouche,Vasilis Stavrinides,Jasleen Nagi,Soodeh Kalaie,Xiao Yun Xu,Wenjia Bai,Declan P. O'Regan*

Main category: cs.CV

TL;DR: MCMeshGAN 是一种用于3D动脉瘤生长预测的多模态条件网格生成对抗网络，结合局部和全局网络分支，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于需要在复杂的3D几何中建模细微的局部变形和全局解剖变化，准确预测主动脉瘤进展具有挑战性。

Method: MCMeshGAN 采用双分支架构，结合局部KNN卷积网络（KCN）和全局图卷积网络（GCN），克服了深度GCN的过度平滑限制，并引入了条件分支编码临床属性和目标时间间隔。

Result: MCMeshGAN 在几何精度和临床重要的直径估计方面均优于现有基线方法。

Conclusion: MCMeshGAN 框架为临床可部署的个性化3D疾病轨迹建模提供了稳健的一步。

Abstract: Personalized, accurate prediction of aortic aneurysm progression is essential
for timely intervention but remains challenging due to the need to model both
subtle local deformations and global anatomical changes within complex 3D
geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh
generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN
introduces a dual-branch architecture combining a novel local KNN-based
convolutional network (KCN) to preserve fine-grained geometric details and a
global graph convolutional network (GCN) to capture long-range structural
context, overcoming the over-smoothing limitations of deep GCNs. A dedicated
condition branch encodes clinical attributes (age, sex) and the target time
interval to generate anatomically plausible, temporally controlled predictions,
enabling retrospective and prospective modeling. We curated TAAMesh, a new
longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal
records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive
experiments demonstrate that MCMeshGAN consistently outperforms
state-of-the-art baselines in both geometric accuracy and clinically important
diameter estimation. This framework offers a robust step toward clinically
deployable, personalized 3D disease trajectory modeling. The source code for
MCMeshGAN and the baseline methods is publicly available at
https://github.com/ImperialCollegeLondon/MCMeshGAN.

</details>


### [74] [Self-supervised structured object representation learning](https://arxiv.org/abs/2508.19864)
*Oussama Hadjerci,Antoine Letienne,Mohamed Abbas Hedjazi,Adel Hafiane*

Main category: cs.CV

TL;DR: 提出一种自监督学习方法，通过多尺度结构化表示提升目标检测性能，尤其在数据有限时表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习在全局图像理解上表现良好，但缺乏对场景结构化表示的捕捉能力。

Method: 提出了一种基于ProtoScale模块的自监督学习方法，通过多空间尺度捕捉视觉元素，并保留完整场景上下文以优化密集预测任务。

Result: 实验结果表明，该方法在多个数据集（COCO和UA-DETRAC）的下游目标检测任务中优于现有技术。

Conclusion: 该方法通过结合语义分组、实例级别分离和层次化结构，构建了结构化视觉表示，显著提升了密集预测任务的性能，尤其在有限标注数据和较少微调周期下表现优异。

Abstract: Self-supervised learning (SSL) has emerged as a powerful technique for
learning visual representations. While recent SSL approaches achieve strong
results in global image understanding, they are limited in capturing the
structured representation in scenes. In this work, we propose a self-supervised
approach that progressively builds structured visual representations by
combining semantic grouping, instance level separation, and hierarchical
structuring. Our approach, based on a novel ProtoScale module, captures visual
elements across multiple spatial scales. Unlike common strategies like DINO
that rely on random cropping and global embeddings, we preserve full scene
context across augmented views to improve performance in dense prediction
tasks. We validate our method on downstream object detection tasks using a
combined subset of multiple datasets (COCO and UA-DETRAC). Experimental results
show that our method learns object centric representations that enhance
supervised object detection and outperform the state-of-the-art methods, even
when trained with limited annotated data and fewer fine-tuning epochs.

</details>


### [75] [TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations](https://arxiv.org/abs/2508.19866)
*François G. Landry,Moulay A. Akhloufi*

Main category: cs.CV

TL;DR: TrajFusionNet是一种新型Transformer模型，结合行人轨迹和车辆速度预测，高效且准确地预测行人过街意图。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆在公共道路上的引入，预测行人过街意图成为研究热点。

Method: 提出了TrajFusionNet，一种基于Transformer的模型，包含序列注意力模块（SAM）和视觉注意力模块（VAM），分别学习行人轨迹和车辆速度的序列表示及视觉表示。

Result: 在三个常用数据集上实现了最先进的性能，并具有最低的总推理时间。

Conclusion: TrajFusionNet通过结合未来行人轨迹和车辆速度预测作为先验，实现了行人过街意图预测的最先进性能，并在推理时间上表现最优。

Abstract: With the introduction of vehicles with autonomous capabilities on public
roads, predicting pedestrian crossing intention has emerged as an active area
of research. The task of predicting pedestrian crossing intention involves
determining whether pedestrians in the scene are likely to cross the road or
not. In this work, we propose TrajFusionNet, a novel transformer-based model
that combines future pedestrian trajectory and vehicle speed predictions as
priors for predicting crossing intention. TrajFusionNet comprises two branches:
a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM
branch learns from a sequential representation of the observed and predicted
pedestrian trajectory and vehicle speed. Complementarily, the VAM branch
enables learning from a visual representation of the predicted pedestrian
trajectory by overlaying predicted pedestrian bounding boxes onto scene images.
By utilizing a small number of lightweight modalities, TrajFusionNet achieves
the lowest total inference time (including model runtime and data
preprocessing) among current state-of-the-art approaches. In terms of
performance, it achieves state-of-the-art results across the three most
commonly used datasets for pedestrian crossing intention prediction.

</details>


### [76] [Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network](https://arxiv.org/abs/2508.19875)
*Hui Zhang,Jianghui Cai,Haifeng Yang,Ali Luo,Yuqing Yang,Xiao Kong,Zhichao Ding,Lichan Zhou,Qin Han*

Main category: cs.CV

TL;DR: SMI模型通过互信息和增量训练方法，利用所有光纤光谱估计天光背景，实验证明其在蓝端表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前的天光背景估计主要依赖天空光纤光谱构建超级天空，缺乏对目标周围环境的建模。为解决这一问题，提出了基于互信息的天光背景估计模型SMI。

Method: SMI包含两个主要网络：第一个网络通过波长校准模块提取光谱中的天光特征，解决特征偏移问题；第二个网络采用增量训练方法最大化不同光谱表示间的互信息以捕获共同成分，并最小化相邻光谱表示间的互信息以获得个体成分。

Result: 在LAMOST光谱上的实验表明，SMI能够在观测中获得更好的目标天光背景，尤其在蓝端表现突出。

Conclusion: SMI模型通过互信息和增量训练方法，能够有效估计天光背景，尤其在蓝端表现优异，为多目标光纤光谱处理提供了更精确的天光背景估计。

Abstract: Sky background subtraction is a critical step in Multi-objective Fiber
spectra process. However, current subtraction relies mainly on sky fiber
spectra to build Super Sky. These average spectra are lacking in the modeling
of the environment surrounding the objects. To address this issue, a sky
background estimation model: Sky background building based on Mutual
Information (SMI) is proposed. SMI based on mutual information and incremental
training approach. It utilizes spectra from all fibers in the plate to estimate
the sky background. SMI contains two main networks, the first network applies a
wavelength calibration module to extract sky features from spectra, and can
effectively solve the feature shift problem according to the corresponding
emission position. The second network employs an incremental training approach
to maximize mutual information between representations of different spectra to
capturing the common component. Then, it minimizes the mutual information
between adjoining spectra representations to obtain individual components. This
network yields an individual sky background at each location of the object. To
verify the effectiveness of the method in this paper, we conducted experiments
on the spectra of LAMOST. Results show that SMI can obtain a better object sky
background during the observation, especially in the blue end.

</details>


### [77] [PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos](https://arxiv.org/abs/2508.19895)
*Ziyun Qian,Runyu Xiao,Shuyuan Tu,Wei Xue,Dingkang Yang,Mingcheng Li,Dongliang Kou,Minghao Han,Zizhi Chen,Lihua Zhang*

Main category: cs.CV

TL;DR: 该论文提出 PersonaAnimator 框架，直接从视频学习个性化运动模式，解决了现有方法在风格表达、数据依赖和物理合理性方面的不足，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有姿势引导的角色运动转移方法仅复制运动而未学习其风格特征，导致角色表现力不足；运动风格转移方法严重依赖难以获取的运动捕捉数据；生成的运动有时违反物理定律。

Method: 提出了一种新颖的框架 PersonaAnimator，直接从无约束视频中学习个性化运动模式，并引入了 Physics-aware Motion Style Regularization 机制以确保生成运动的物理合理性。

Result: PersonaAnimator 在广泛的实验中表现优于现有方法，并引入了首个视频基础的个性化运动数据集 PersonaVid，包含 20 个运动内容类别和 120 个运动风格类别。

Conclusion: PersonaAnimator 在视频到视频运动个性化任务中表现优异，超越了现有最先进的运动转移方法，并为此任务设立了新的基准。

Abstract: Recent advances in motion generation show remarkable progress. However,
several limitations remain: (1) Existing pose-guided character motion transfer
methods merely replicate motion without learning its style characteristics,
resulting in inexpressive characters. (2) Motion style transfer methods rely
heavily on motion capture data, which is difficult to obtain. (3) Generated
motions sometimes violate physical laws. To address these challenges, this
paper pioneers a new task: Video-to-Video Motion Personalization. We propose a
novel framework, PersonaAnimator, which learns personalized motion patterns
directly from unconstrained videos. This enables personalized motion transfer.
To support this task, we introduce PersonaVid, the first video-based
personalized motion dataset. It contains 20 motion content categories and 120
motion style categories. We further propose a Physics-aware Motion Style
Regularization mechanism to enforce physical plausibility in the generated
motions. Extensive experiments show that PersonaAnimator outperforms
state-of-the-art motion transfer methods and sets a new benchmark for the
Video-to-Video Motion Personalization task.

</details>


### [78] [Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities](https://arxiv.org/abs/2508.19905)
*Imad Ali Shah,Jiarong Li,Roshan George,Tim Brophy,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: HSI在汽车应用中潜力大但商业成熟度低，需解决技术和数据集瓶颈以实现ADAS/AD集成。


<details>
  <summary>Details</summary>
Motivation: 探索HSI作为一种超越传统RGB成像的传感模态，如何在ADAS/AD中实现材料级场景理解，并评估其当前技术成熟度。

Method: 通过定性综述和定量分析216款商用HSI及多光谱成像相机，评估其在帧率、空间分辨率、光谱维度和AEC-Q100温度标准等关键汽车指标上的表现。

Result: 仅四款相机满足性能阈值，且无一符合AEC-Q100标准；现有HSI数据集在规模、光谱一致性和环境多样性方面存在局限。

Conclusion: 高光谱成像（HSI）在汽车应用中展现出巨大潜力，但目前商业成熟度与研发潜力存在显著差距，需进一步研究以实现其在ADAS/AD中的实际集成。

Abstract: Hyperspectral imaging (HSI) offers a transformative sensing modality for
Advanced Driver Assistance Systems (ADAS) and autonomous driving (AD)
applications, enabling material-level scene understanding through fine spectral
resolution beyond the capabilities of traditional RGB imaging. This paper
presents the first comprehensive review of HSI for automotive applications,
examining the strengths, limitations, and suitability of current HSI
technologies in the context of ADAS/AD. In addition to this qualitative review,
we analyze 216 commercially available HSI and multispectral imaging cameras,
benchmarking them against key automotive criteria: frame rate, spatial
resolution, spectral dimensionality, and compliance with AEC-Q100 temperature
standards. Our analysis reveals a significant gap between HSI's demonstrated
research potential and its commercial readiness. Only four cameras meet the
defined performance thresholds, and none comply with AEC-Q100 requirements. In
addition, the paper reviews recent HSI datasets and applications, including
semantic segmentation for road surface classification, pedestrian separability,
and adverse weather perception. Our review shows that current HSI datasets are
limited in terms of scale, spectral consistency, the number of spectral
channels, and environmental diversity, posing challenges for the development of
perception algorithms and the adequate validation of HSI's true potential in
ADAS/AD applications. This review paper establishes the current state of HSI in
automotive contexts as of 2025 and outlines key research directions toward
practical integration of spectral imaging in ADAS and autonomous systems.

</details>


### [79] [Streamlining the Development of Active Learning Methods in Real-World Object Detection](https://arxiv.org/abs/2508.19906)
*Moussa Kassem Sbeyti,Nadja Klein,Michelle Karg,Christian Wirth,Sahin Albayrak*

Main category: cs.CV

TL;DR: 提出OSS度量方法，通过对象相似性量化主动学习效果并选择验证集，减少训练成本，提升评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决主动学习在真实世界目标检测中面临的计算成本和评估可靠性挑战，特别是在自动驾驶数据集训练成本高昂（如一个检测器需282 GPU小时）的情况下。

Method: 通过对象级特征测量训练集与目标域之间的相似性，开发了对象基于集相似性（OSS）度量标准。该方法不依赖检测器训练，仅需标记的对象裁剪，并能与现有主动学习流程集成。

Result: 在KITTI、BDD100K和CODA三个数据集上验证了OSS的有效性，证明了其在选择有效主动学习方法和代表性验证集方面的优势。

Conclusion: 本文提出了一种基于对象相似性的度量方法（OSS），用于在不训练检测器的情况下量化主动学习方法的效果，并选择具有代表性的验证集进行稳健评估。该方法在三个自动驾驶数据集上验证有效，为实际应用中的计算效率和评估可靠性提供了实用框架。

Abstract: Active learning (AL) for real-world object detection faces computational and
reliability challenges that limit practical deployment. Developing new AL
methods requires training multiple detectors across iterations to compare
against existing approaches. This creates high costs for autonomous driving
datasets where the training of one detector requires up to 282 GPU hours.
Additionally, AL method rankings vary substantially across validation sets,
compromising reliability in safety-critical transportation systems. We
introduce object-based set similarity ($\mathrm{OSS}$), a metric that addresses
these challenges. $\mathrm{OSS}$ (1) quantifies AL method effectiveness without
requiring detector training by measuring similarity between training sets and
target domains using object-level features. This enables the elimination of
ineffective AL methods before training. Furthermore, $\mathrm{OSS}$ (2) enables
the selection of representative validation sets for robust evaluation. We
validate our similarity-based approach on three autonomous driving datasets
(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with
two detector architectures (EfficientDet, YOLOv3). This work is the first to
unify AL training and evaluation strategies in object detection based on object
similarity. $\mathrm{OSS}$ is detector-agnostic, requires only labeled object
crops, and integrates with existing AL pipelines. This provides a practical
framework for deploying AL in real-world applications where computational
efficiency and evaluation reliability are critical. Code is available at
https://mos-ks.github.io/publications/.

</details>


### [80] [Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation](https://arxiv.org/abs/2508.19909)
*Lechun You,Zhonghua Wu,Weide Liu,Xulei Yang,Jun Cheng,Wei Zhou,Bharadwaj Veeravalli,Guosheng Lin*

Main category: cs.CV

TL;DR: 利用2D基础模型生成的掩码扩展3D稀疏标注，结合几何对应和一致性正则化，显著提升3D弱监督分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D语义分割方法通常仅关注3D域，未充分利用2D和3D数据的互补性。此外，现有方法在扩展标签或生成伪标签时未能充分利用或处理噪声。

Method: 提出了一种新方法，利用2D基础模型生成分割掩码，并通过几何对应关系将其传播到3D空间。同时，采用置信度和不确定性一致性正则化来筛选可靠的伪标签，进一步扩展标签库。

Result: 该方法显著扩展了可用标签库，提升了3D弱监督分割的性能。

Conclusion: 该研究通过结合2D基础模型生成的掩码和3D稀疏标注，显著提升了3D弱监督分割的性能，弥合了有限3D标注与2D模型强大能力之间的差距。

Abstract: Current methods for 3D semantic segmentation propose training models with
limited annotations to address the difficulty of annotating large, irregular,
and unordered 3D point cloud data. They usually focus on the 3D domain only,
without leveraging the complementary nature of 2D and 3D data. Besides, some
methods extend original labels or generate pseudo labels to guide the training,
but they often fail to fully use these labels or address the noise within them.
Meanwhile, the emergence of comprehensive and adaptable foundation models has
offered effective solutions for segmenting 2D data. Leveraging this
advancement, we present a novel approach that maximizes the utility of sparsely
available 3D annotations by incorporating segmentation masks generated by 2D
foundation models. We further propagate the 2D segmentation masks into the 3D
space by establishing geometric correspondences between 3D scenes and 2D views.
We extend the highly sparse annotations to encompass the areas delineated by 3D
masks, thereby substantially augmenting the pool of available labels.
Furthermore, we apply confidence- and uncertainty-based consistency
regularization on augmentations of the 3D point cloud and select the reliable
pseudo labels, which are further spread on the 3D masks to generate more
labels. This innovative strategy bridges the gap between limited 3D annotations
and the powerful capabilities of 2D foundation models, ultimately improving the
performance of 3D weakly supervised segmentation.

</details>


### [81] [KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts](https://arxiv.org/abs/2508.19944)
*Taebaek Hwang,Minseo Kim,Gisang Lee,Seonuk Kim,Hyunjun Eun*

Main category: cs.CV

TL;DR: KRETA是一个针对韩语的文本丰富视觉问答基准，通过半自动化流程和多维度评估，填补了低资源语言的评估空白。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如韩语）在视觉语言模型中缺乏全面评估基准的问题，以促进多语言VLM研究的发展。

Method: 提出了KRETA基准，包括15个领域和26种图像类型，并开发了一个针对文本丰富环境的半自动化VQA生成流程，结合逐步图像分解和七指标评估协议。

Result: KRETA基准支持对视觉文本理解和推理能力的深入评估，其生成流程确保了数据质量，并有望推广至其他语言。

Conclusion: KRETA填补了韩语在文本丰富视觉问答基准上的空白，并通过其半自动化生成流程和多维度评估协议，为多语言VLM研究提供了可扩展的解决方案。

Abstract: Understanding and reasoning over text within visual contexts poses a
significant challenge for Vision-Language Models (VLMs), given the complexity
and diversity of real-world scenarios. To address this challenge, text-rich
Visual Question Answering (VQA) datasets and benchmarks have emerged for
high-resource languages like English. However, a critical gap persists for
low-resource languages such as Korean, where the lack of comprehensive
benchmarks hinders robust model evaluation and comparison. To bridge this gap,
we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich
VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth
evaluation of both visual text understanding and reasoning capabilities, while
also supporting a multifaceted assessment across 15 domains and 26 image types.
Additionally, we introduce a semi-automated VQA generation pipeline
specifically optimized for text-rich settings, leveraging refined stepwise
image decomposition and a rigorous seven-metric evaluation protocol to ensure
data quality. While KRETA is tailored for Korean, we hope our adaptable and
extensible pipeline will facilitate the development of similar benchmarks in
other languages, thereby accelerating multilingual VLM research. The code and
dataset for KRETA are available at https://github.com/tabtoyou/KRETA.

</details>


### [82] [Reimagining Image Segmentation using Active Contour: From Chan Vese Algorithm into a Proposal Novel Functional Loss Framework](https://arxiv.org/abs/2508.19946)
*Gianluca Guzzetta*

Main category: cs.CV

TL;DR: 本文研究了Chan-Vese算法的功能性分割损失，通过MATLAB实现和现代计算机视觉方法验证了其有效性，性能优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 研究Chan-Vese算法在图像分割中的应用，探索其功能性分割损失的潜力，并与经典损失函数进行比较。

Method: 采用离散化方案，基于Chan-Vese模型的功能能量及其水平集函数的偏微分方程，结合pytorch.nn.ModuleLoss和Chan-Vese算法提出功能性分割损失。

Result: 实验结果表明，提出的功能性分割损失在性能上优于经典损失函数。

Conclusion: 本文通过实证研究和理论证明，验证了基于Chan-Vese算法的功能性分割损失在图像分割中的有效性，并通过MATLAB实现和现代计算机视觉方法进行了性能评估。

Abstract: In this paper, we present a comprehensive study and analysis of the Chan-Vese
algorithm for image segmentation. We employ a discretized scheme derived from
the empirical study of the Chan-Vese model's functional energy and its partial
differential equation based on its level set function. We provide a proof of
the results and an implementation using MATLAB. Leveraging modern computer
vision methodologies, we propose a functional segmentation loss based on active
contours, utilizing pytorch.nn.ModuleLoss and a level set based on the
Chan-Vese algorithm. We compare our results with common computer vision
segmentation datasets and evaluate the performance of classical loss functions
against our proposed method. All code and materials used are available at
https://github.com/gguzzy/chan_vese_functional_loss.

</details>


### [83] [Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models](https://arxiv.org/abs/2508.19967)
*Oliver Grainge,Sania Waheed,Jack Stilgoe,Michael Milford,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 论文评估了25种VLMs的地理定位能力，发现其在社交媒体类图像上准确率高（61%），引发隐私担忧，但在通用街景图像上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs作为地理定位工具的潜力带来隐私风险，但缺乏对其定位精度、局限性和潜在意外推断的系统评估。

Method: 对25种最先进的VLMs在四个不同环境下的基准图像数据集上进行全面评估。

Result: VLMs在社交媒体类图像上表现优异（61%准确率），但在通用街景图像上表现较差。

Conclusion: 当前视觉语言模型（VLMs）在通用街景图像上表现不佳，但在类似社交媒体内容的图像上准确率高达61%，引发了紧迫的隐私问题。

Abstract: Geo-localization is the task of identifying the location of an image using
visual cues alone. It has beneficial applications, such as improving disaster
response, enhancing navigation, and geography education. Recently,
Vision-Language Models (VLMs) are increasingly demonstrating capabilities as
accurate image geo-locators. This brings significant privacy risks, including
those related to stalking and surveillance, considering the widespread uses of
AI models and sharing of photos on social media. The precision of these models
is likely to improve in the future. Despite these risks, there is little work
on systematically evaluating the geolocation precision of Generative VLMs,
their limits and potential for unintended inferences. To bridge this gap, we
conduct a comprehensive assessment of the geolocation capabilities of 25
state-of-the-art VLMs on four benchmark image datasets captured in diverse
environments. Our results offer insight into the internal reasoning of VLMs and
highlight their strengths, limitations, and potential societal risks. Our
findings indicate that current VLMs perform poorly on generic street-level
images yet achieve notably high accuracy (61\%) on images resembling social
media content, raising significant and urgent privacy concerns.

</details>


### [84] [GS: Generative Segmentation via Label Diffusion](https://arxiv.org/abs/2508.20020)
*Yuhao Chen,Shubin Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: GS是一种新型生成式分割框架，通过标签扩散直接生成分割掩码，在PNG基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法将分割视为判别任务，而现有扩散模型方法仍以图像为中心，未将分割作为主要建模目标。

Method: 提出GS框架，通过标签扩散直接生成分割掩码，以图像和语言描述为条件，实现端到端训练。

Result: 在Panoptic Narrative Grounding（PNG）基准测试中，GS显著优于现有判别性和基于扩散的方法。

Conclusion: GS（Generative Segmentation）通过将分割任务本身定义为生成任务，显著优于现有方法，为语言驱动分割设定了新的最先进水平。

Abstract: Language-driven image segmentation is a fundamental task in vision-language
understanding, requiring models to segment regions of an image corresponding to
natural language expressions. Traditional methods approach this as a
discriminative problem, assigning each pixel to foreground or background based
on semantic alignment. Recently, diffusion models have been introduced to this
domain, but existing approaches remain image-centric: they either (i) use image
diffusion models as visual feature extractors, (ii) synthesize segmentation
data via image generation to train discriminative models, or (iii) perform
diffusion inversion to extract attention cues from pre-trained image diffusion
models-thereby treating segmentation as an auxiliary process. In this paper, we
propose GS (Generative Segmentation), a novel framework that formulates
segmentation itself as a generative task via label diffusion. Instead of
generating images conditioned on label maps and text, GS reverses the
generative process: it directly generates segmentation masks from noise,
conditioned on both the input image and the accompanying language description.
This paradigm makes label generation the primary modeling target, enabling
end-to-end training with explicit control over spatial and semantic fidelity.
To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic
Narrative Grounding (PNG), a representative and challenging benchmark for
multimodal segmentation that requires panoptic-level reasoning guided by
narrative captions. Experimental results show that GS significantly outperforms
existing discriminative and diffusion-based methods, setting a new
state-of-the-art for language-driven segmentation.

</details>


### [85] [Segmentation Assisted Incremental Test Time Adaptation in an Open World](https://arxiv.org/abs/2508.20029)
*Manogna Sreenivas,Soma Biswas*

Main category: cs.CV

TL;DR: 提出SegAssist模块，结合VLM分割能力和主动标注，提升模型在动态环境中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 动态环境中，模型常遇到未知对象和分布变化，传统方法无法适应持续出现的新类别和领域。

Method: 提出SegAssist模块，一种训练免费的方法，结合主动标注技术和VLM的分割能力，优先选择可能属于新类别的样本进行标注。

Result: 在多个基准数据集上的实验显示，SegAssist有效提升了VLM在测试时对新数据的适应能力。

Conclusion: SegAssist模块通过利用VLM的分割能力，显著提升了模型在动态环境中对新类别和领域的适应能力，证明了其在真实世界场景中的实用价值。

Abstract: In dynamic environments, unfamiliar objects and distribution shifts are often
encountered, which challenge the generalization abilities of the deployed
trained models. This work addresses Incremental Test Time Adaptation of Vision
Language Models, tackling scenarios where unseen classes and unseen domains
continuously appear during testing. Unlike traditional Test Time Adaptation
approaches, where the test stream comes only from a predefined set of classes,
our framework allows models to adapt simultaneously to both covariate and label
shifts, actively incorporating new classes as they emerge. Towards this goal,
we establish a new benchmark for ITTA, integrating single image TTA methods for
VLMs with active labeling techniques that query an oracle for samples
potentially representing unseen classes during test time. We propose a
segmentation assisted active labeling module, termed SegAssist, which is
training free and repurposes the segmentation capabilities of VLMs to refine
active sample selection, prioritizing samples likely to belong to unseen
classes. Extensive experiments on several benchmark datasets demonstrate the
potential of SegAssist to enhance the performance of VLMs in real world
scenarios, where continuous adaptation to emerging data is essential.
Project-page:https://manogna-s.github.io/segassist/

</details>


### [86] [OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations](https://arxiv.org/abs/2508.20063)
*Peng-Hao Hsu,Ke Zhang,Fu-En Wang,Tao Tu,Ming-Feng Li,Yu-Lun Liu,Albert Y. C. Chen,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: OpenM3D是一种无需人工标注的开词汇多视角室内3D物体检测器，通过单阶段检测和联合训练，在准确性和速度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索基于图像的开词汇（OV）3D物体检测方法，弥补其在3D点云方法中的不足。

Method: OpenM3D采用单阶段检测器，结合2D诱导的体素特征，并通过类无关的3D定位损失和体素语义对齐损失进行联合训练。

Result: OpenM3D在无需人工标注的情况下，通过3D伪框生成和多样化CLIP特征采样，实现了高质量的单阶段检测器训练。

Conclusion: OpenM3D在ScanNet200和ARKitScenes室内基准测试中表现出更高的准确性和速度（每场景0.3秒），优于现有的方法。

Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its
exploration through image-based methods remains limited compared to 3D point
cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view
indoor 3D object detector trained without human annotations. In particular,
OpenM3D is a single-stage detector adapting the 2D-induced voxel features from
the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic
3D localization loss requiring high-quality 3D pseudo boxes and a
voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We
follow the training setting of OV-3DET where posed RGB-D images are given but
no human annotations of 3D boxes or classes are available. We propose a 3D
Pseudo Box Generation method using a graph embedding technique that combines 2D
segments into coherent 3D structures. Our pseudo-boxes achieve higher precision
and recall than other methods, including the method proposed in OV-3DET. We
further sample diverse CLIP features from 2D segments associated with each
coherent 3D structure to align with the corresponding voxel feature. The key to
training a highly accurate single-stage detector requires both losses to be
learned toward high-quality targets. At inference, OpenM3D, a highly efficient
detector, requires only multi-view images for input and demonstrates superior
accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor
benchmarks compared to existing methods. We outperform a strong two-stage
method that leverages our class-agnostic detector with a ViT CLIP-based OV
classifier and a baseline incorporating multi-view depth estimator on both
accuracy and speed.

</details>


### [87] [PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence](https://arxiv.org/abs/2508.20066)
*Zheng Li,Yanming Guo,WenZhe Liu,Xueyi Zhang,Zhaoyun Ding,Long Xu,Mingrui Lao*

Main category: cs.CV

TL;DR: PAUL框架通过不确定性学习解决跨视角地理定位中的噪声对应问题，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设训练图像对完美对齐，但实际场景中因GPS漂移等因素导致噪声对应普遍存在，亟需解决以提升模型实用性。

Method: 提出PAUL框架，通过不确定性感知的协同增强和证据协同训练，对训练数据进行分区和增强，优化特征学习并抑制噪声影响。

Result: PAUL在多种噪声比例下均优于其他噪声对应驱动方法，验证了其组件的有效性。

Conclusion: PAUL框架通过不确定性感知的数据分区和增强，有效解决了跨视角地理定位中的噪声对应问题，显著提升了模型在真实场景中的性能。

Abstract: Cross-view geo-localization is a critical task for UAV navigation, event
detection, and aerial surveying, as it enables matching between drone-captured
and satellite imagery. Most existing approaches embed multi-modal data into a
joint feature space to maximize the similarity of paired images. However, these
methods typically assume perfect alignment of image pairs during training,
which rarely holds true in real-world scenarios. In practice, factors such as
urban canyon effects, electromagnetic interference, and adverse weather
frequently induce GPS drift, resulting in systematic alignment shifts where
only partial correspondences exist between pairs. Despite its prevalence, this
source of noisy correspondence has received limited attention in current
research. In this paper, we formally introduce and address the Noisy
Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to
bridge the gap between idealized benchmarks and practical applications. To this
end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a
novel framework that partitions and augments training data based on estimated
data uncertainty through uncertainty-aware co-augmentation and evidential
co-training. Specifically, PAUL selectively augments regions with high
correspondence confidence and utilizes uncertainty estimation to refine feature
learning, effectively suppressing noise from misaligned pairs. Distinct from
traditional filtering or label correction, PAUL leverages both data uncertainty
and loss discrepancy for targeted partitioning and augmentation, thus providing
robust supervision for noisy samples. Comprehensive experiments validate the
effectiveness of individual components in PAUL,which consistently achieves
superior performance over other competitive noisy-correspondence-driven methods
in various noise ratios.

</details>


### [88] [AudioStory: Generating Long-Form Narrative Audio with Large Language Models](https://arxiv.org/abs/2508.20088)
*Yuxin Guo,Teng Wang,Yuying Ge,Shijie Ma,Yixiao Ge,Wei Zou,Ying Shan*

Main category: cs.CV

TL;DR: AudioStory是一个整合LLM和TTA的框架，用于生成连贯的长音频叙事，通过解耦桥接和端到端训练提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的TTA生成技术在短音频剪辑合成上表现优异，但在长叙事音频生成方面存在时间连贯性和组合推理的挑战。

Method: AudioStory采用解耦的桥接机制和端到端训练框架，将LLM与TTA系统的协作分为两个专门组件：桥接查询（用于事件内语义对齐）和残差查询（用于跨事件连贯性保持）。

Result: 实验表明，AudioStory在单音频生成和叙事音频生成任务上均优于现有TTA基线，尤其在指令跟随能力和音频保真度方面表现突出。

Conclusion: AudioStory通过整合大型语言模型（LLMs）和文本到音频（TTA）系统，成功生成了结构化的长音频叙事，显著提升了叙事音频的连贯性和情感一致性。

Abstract: Recent advances in text-to-audio (TTA) generation excel at synthesizing short
audio clips but struggle with long-form narrative audio, which requires
temporal coherence and compositional reasoning. To address this gap, we propose
AudioStory, a unified framework that integrates large language models (LLMs)
with TTA systems to generate structured, long-form audio narratives. AudioStory
possesses strong instruction-following reasoning generation capabilities. It
employs LLMs to decompose complex narrative queries into temporally ordered
sub-tasks with contextual cues, enabling coherent scene transitions and
emotional tone consistency. AudioStory has two appealing features: (1)
Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser
collaboration into two specialized components, i.e., a bridging query for
intra-event semantic alignment and a residual query for cross-event coherence
preservation. (2) End-to-end training: By unifying instruction comprehension
and audio generation within a single end-to-end framework, AudioStory
eliminates the need for modular training pipelines while enhancing synergy
between components. Furthermore, we establish a benchmark AudioStory-10K,
encompassing diverse domains such as animated soundscapes and natural sound
narratives. Extensive experiments show the superiority of AudioStory on both
single-audio generation and narrative audio generation, surpassing prior TTA
baselines in both instruction-following ability and audio fidelity. Our code is
available at https://github.com/TencentARC/AudioStory

</details>


### [89] [Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors](https://arxiv.org/abs/2508.20089)
*Ross J Gardiner,Guillaume Mougeot,Sareh Rowlands,Benno I Simmons,Flemming Helsing,Toke Thomas Høye*

Main category: cs.CV

TL;DR: 提出轻量级分类方法，结合专家标记数据和BioCLIP2知识蒸馏，显著降低计算成本并保持准确性，助力昆虫监测。


<details>
  <summary>Details</summary>
Motivation: 由于策划图像与嘈杂野外图像之间的领域转移，准确识别蛾类物种具有挑战性，这对理解昆虫减少至关重要。

Method: 结合有限的专家标记野外数据和从高性能BioCLIP2基础模型到ConvNeXt-tiny架构的知识蒸馏，提出了一种轻量级分类方法。

Result: 在101种丹麦蛾类上的实验表明，BioCLIP2显著优于其他方法，且轻量级蒸馏模型在显著降低计算成本的同时实现了可比的准确性。

Conclusion: 该研究为开发高效的昆虫监测系统提供了实用指南，并通过知识蒸馏方法在保证准确性的同时显著降低了计算成本。

Abstract: Labelling images of Lepidoptera (moths) from automated camera systems is
vital for understanding insect declines. However, accurate species
identification is challenging due to domain shifts between curated images and
noisy field imagery. We propose a lightweight classification approach,
combining limited expert-labelled field data with knowledge distillation from
the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny
architecture. Experiments on 101 Danish moth species from AMI camera systems
demonstrate that BioCLIP2 substantially outperforms other methods and that our
distilled lightweight model achieves comparable accuracy with significantly
reduced computational cost. These insights offer practical guidelines for the
development of efficient insect monitoring systems and bridging domain gaps for
fine-grained classification.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [90] [Stack Trace-Based Crash Deduplication with Transformer Adaptation](https://arxiv.org/abs/2508.19449)
*Md Afif Al Mamun,Gias Uddin,Lan Xia,Longyu Zhang*

Main category: cs.SE

TL;DR: dedupT是一种基于Transformer的方法，通过预训练语言模型和全连接网络有效排名和检测重复崩溃报告，显著减少人工分类工作量。


<details>
  <summary>Details</summary>
Motivation: 自动化崩溃报告系统生成大量重复报告，增加了开发人员的工作量。传统的基于堆栈跟踪的去重方法未能捕捉堆栈跟踪中的上下文和结构关系。

Method: dedupT首先将预训练语言模型（PLM）适配到堆栈跟踪中，然后利用其嵌入训练全连接网络（FCN）以有效排名重复崩溃。

Result: 在四个公共数据集上的实验表明，dedupT在重复排名和唯一崩溃检测上优于现有的深度学习和传统方法，MRR提升超过15%，ROC-AUC更高。

Conclusion: dedupT通过结合预训练语言模型和全连接网络，显著提升了重复崩溃报告的排名和唯一崩溃检测的性能，为软件工程中的现代自然语言处理技术应用提供了有效解决方案。

Abstract: Automated crash reporting systems generate large volumes of duplicate
reports, overwhelming issue-tracking systems and increasing developer workload.
Traditional stack trace-based deduplication methods, relying on string
similarity, rule-based heuristics, or deep learning (DL) models, often fail to
capture the contextual and structural relationships within stack traces. We
propose dedupT, a transformer-based approach that models stack traces
holistically rather than as isolated frames. dedupT first adapts a pretrained
language model (PLM) to stack traces, then uses its embeddings to train a
fully-connected network (FCN) to rank duplicate crashes effectively. Extensive
experiments on real-world datasets show that dedupT outperforms existing DL and
traditional methods (e.g., sequence alignment and information retrieval
techniques) in both duplicate ranking and unique crash detection, significantly
reducing manual triage effort. On four public datasets, dedupT improves Mean
Reciprocal Rank (MRR) often by over 15% compared to the best DL baseline and up
to 9% over traditional methods while achieving higher Receiver Operating
Characteristic Area Under the Curve (ROC-AUC) in detecting unique crash
reports. Our work advances the integration of modern natural language
processing (NLP) techniques into software engineering, providing an effective
solution for stack trace-based crash deduplication.

</details>


### [91] [Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking](https://arxiv.org/abs/2508.19558)
*Zhuohao Li,Wenqing Chen,Jianxing Yu,Zhichao Lu*

Main category: cs.SE

TL;DR: 本文提出了一种新的数据合成框架，用于评估和改进大语言模型代码嵌入的功能一致性，实验证明该框架能显著提升嵌入模型在多个下游任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注代码克隆检测，强调语法相似性而忽视了功能理解，因此需要评估大语言模型代码嵌入在功能一致性方面的能力。

Method: 提出了一个名为‘功能导向的代码自进化’的数据合成框架，用于构建多样化和具有挑战性的基准测试。该框架从一个代码实例生成四种独特的变体，以更好地反映功能差异。

Result: 在三个下游任务（代码克隆检测、代码功能一致性识别和代码检索）上的广泛实验表明，嵌入模型在训练于进化后的数据集时性能显著提高。

Conclusion: 嵌入模型在代码功能理解方面表现出显著的性能提升，特别是在训练于我们提出的数据合成框架后，进一步推进了代码功能理解的领域。

Abstract: Embedding models have demonstrated strong performance in tasks like
clustering, retrieval, and feature extraction while offering computational
advantages over generative models and cross-encoders. Benchmarks such as MTEB
have shown that text embeddings from large language models (LLMs) capture rich
semantic information, but their ability to reflect code-level functional
semantics remains unclear. Existing studies largely focus on code clone
detection, which emphasizes syntactic similarity and overlooks functional
understanding. In this paper, we focus on the functional consistency of LLM
code embeddings, which determines if two code snippets perform the same
function regardless of syntactic differences. We propose a novel data synthesis
framework called Functionality-Oriented Code Self-Evolution to construct
diverse and challenging benchmarks. Specifically, we define code examples
across four semantic and syntactic categories and find that existing datasets
predominantly capture syntactic properties. Our framework generates four unique
variations from a single code instance, providing a broader spectrum of code
examples that better reflect functional differences. Extensive experiments on
three downstream tasks-code clone detection, code functional consistency
identification, and code retrieval-demonstrate that embedding models
significantly improve their performance when trained on our evolved datasets.
These results highlight the effectiveness and generalization of our data
synthesis framework, advancing the functional understanding of code.

</details>


### [92] [The Influence of Code Comments on the Perceived Helpfulness of Stack Overflow Posts](https://arxiv.org/abs/2508.19610)
*Kathrin Figl,Maria Kirchner,Sebastian Baltes,Michael Felderer*

Main category: cs.SE

TL;DR: 研究发现代码注释（尤其是块注释）能显著提升Stack Overflow答案的感知帮助性，对社区平台和AI工具设计有参考价值。


<details>
  <summary>Details</summary>
Motivation: 探索代码注释如何影响开发者对Stack Overflow答案的感知帮助性，以提升社区平台和AI工具生成的代码质量。

Method: 通过在线实验模拟Stack Overflow环境（n=91），比较不同注释类型（块注释、内联注释、无注释）对答案帮助性的影响。

Result: 块注释和内联注释均被认为比无注释代码更有帮助，且新手更偏好块注释。其他表面特征（如答案位置和得分）影响较小。

Conclusion: 代码注释（尤其是块注释）显著提高了Stack Overflow答案的感知帮助性，这对社区驱动平台和AI编码助手的设计有重要启示。

Abstract: Question-and-answer platforms such as Stack Overflow have become an important
way for software developers to share and retrieve knowledge. However, reusing
poorly understood code can lead to serious problems, such as bugs or security
vulnerabilities. To better understand how code comments affect the perceived
helpfulness of Stack Overflow answers, we conducted an online experiment
simulating a Stack Overflow environment (n=91). The results indicate that both
block and inline comments are perceived as significantly more helpful than
uncommented source code. Moreover, novices rated code snippets with block
comments as more helpful than those with inline comments. Interestingly, other
surface features, such as the position of an answer and its answer score, were
considered less important. The content of Stack Overflow has been a major
source for training large language models. AI-based coding assistants such as
GitHub Copilot, which are based on these models, might change the way Stack
Overflow is used. However, our findings have implications beyond this specific
platform. First, they may help to improve the relevance of community-driven
platforms such as Stack Overflow, which provide human advice and explanations
of code solutions, complementing AI-based support for software developers.
Second, since chat-based AI tools can be prompted to generate code in different
ways, knowing which properties influence perceived helpfulness might lead to
targeted prompting strategies to generate more readable code snippets.

</details>


### [93] [Leveraging LLMs for Automated Translation of Legacy Code: A Case Study on PL/SQL to Java Transformation](https://arxiv.org/abs/2508.19663)
*Lola Solovyeva,Eduardo Carneiro Oliveira,Shiyu Fan,Alper Tuncay,Shamil Gareev,Andrea Capiluppi*

Main category: cs.SE

TL;DR: 研究利用LLMs将PL/SQL代码翻译为Java，提出自定义提示策略，结果可行但受样本量限制。


<details>
  <summary>Details</summary>
Motivation: VT遗留系统缺乏一致的文档和自动化测试，给重构和现代化带来重大挑战，因此研究探索利用LLMs将PL/SQL代码翻译为Java的可行性。

Method: 利用包含10个PL/SQL到Java代码对和15个Java类的数据集，评估了多种大型语言模型（LLMs），并提出了一种结合链式引导推理和$n$-shot提示的自定义提示策略。

Result: 该方法能有效指导LLMs生成语法准确的翻译并实现功能正确性，但受限于可用代码文件的小样本量和测试用例的有限访问。

Conclusion: 研究为大规模遗留系统的现代化提供了可扩展的自动化解决方案的基础，尽管样本量和测试用例有限。

Abstract: The VT legacy system, comprising approximately 2.5 million lines of PL/SQL
code, lacks consistent documentation and automated tests, posing significant
challenges for refactoring and modernisation. This study investigates the
feasibility of leveraging large language models (LLMs) to assist in translating
PL/SQL code into Java for the modernised "VTF3" system. By leveraging a dataset
comprising 10 PL/SQL-to-Java code pairs and 15 Java classes, which collectively
established a domain model for the translated files, multiple LLMs were
evaluated. Furthermore, we propose a customized prompting strategy that
integrates chain-of-guidance reasoning with $n$-shot prompting. Our findings
indicate that this methodology effectively guides LLMs in generating
syntactically accurate translations while also achieving functional
correctness. However, the findings are limited by the small sample size of
available code files and the restricted access to test cases used for
validating the correctness of the generated code. Nevertheless, these findings
lay the groundwork for scalable, automated solutions in modernising large
legacy systems.

</details>


### [94] [Enabling Content Management Systems as an Information Source in Model-driven Projects](https://arxiv.org/abs/2508.19797)
*Joan Giner-Miguelez,Abel Gómez,Jordi Cabot*

Main category: cs.SE

TL;DR: 论文提出了一种模型化框架，用于简化无头CMS在软件开发中的集成，通过发现信息模式并生成中间件库，实现平台无关的访问。


<details>
  <summary>Details</summary>
Motivation: 随着无头CMS的普及，其在信息系统中扮演着越来越重要的角色，但目前缺乏有效的工具来发现和管理CMS中的信息，导致这一过程耗时且易错。

Method: 论文提出了一种模型化框架，能够发现并明确表示CMS背后的信息模式，生成作为中间件库一部分的交互，提供平台无关的CMS访问。

Result: 该框架成功实现了对CMS信息模式的发现和表示，并通过生成的中间件库提供了平台无关的访问方式，整个框架已开源并在线可用。

Conclusion: 该论文提出了一个基于模型的框架，用于促进无头CMS在软件开发过程中的集成，通过发现和明确表示CMS的信息模式，简化了与消费信息的其他组件的交互设计。

Abstract: Content Management Systems (CMSs) are the most popular tool when it comes to
create and publish content across the web. Recently, CMSs have evolved,
becoming \emph{headless}. Content served by a \emph{headless CMS} aims to be
consumed by other applications and services through REST APIs rather than by
human users through a web browser. This evolution has enabled CMSs to become a
notorious source of content to be used in a variety of contexts beyond pure web
navigation. As such, CMS have become an important component of many information
systems. Unfortunately, we still lack the tools to properly discover and manage
the information stored in a CMS, often highly customized to the needs of a
specific domain. Currently, this is mostly a time-consuming and error-prone
manual process.
  In this paper, we propose a model-based framework to facilitate the
integration of headless CMSs in software development processes. Our framework
is able to discover and explicitly represent the information schema behind the
CMS. This facilitates designing the interaction between the CMS model and other
components consuming that information. These interactions are then generated as
part of a middleware library that offers platform-agnostic access to the CMS to
all the client applications. The complete framework is open-source and
available online.

</details>


### [95] [Towards a fundamental theory of modeling discrete systems](https://arxiv.org/abs/2508.19803)
*Peter Fettke,Wolfgang Reisig*

Main category: cs.SE

TL;DR: 本文探讨了建模在数字时代的核心挑战，提出了Heraklit建模框架作为解决方案，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 阐述了建模在科学与工程中的核心地位，并指出数字时代需要新的基础理论来应对挑战。

Method: 引入了Heraklit建模框架作为新的建模方法。

Result: 提出了Heraklit建模框架作为应对数字时代建模挑战的新方法。

Conclusion: 本文总结了Heraklit建模框架的重要性，并提出了未来研究方向，包括建模的正确性、信息概念以及建模中的不变性描述。

Abstract: Modeling is a central concern in both science and engineering. However, we
need a new fundamental theory to address the challenges of the digital age. In
this paper, we first explain why modeling is fundamental and which challenges
must be addressed in the digital world. As a main contribution, we introduce
the Heraklit modeling framework as a new approach to modeling. We conclude with
some general remarks. Future work will involve the correctness of modeling, the
notion of information, and the description of invariance in modeling.

</details>


### [96] [On the Future of Software Reuse in the Era of AI Native Software Engineering](https://arxiv.org/abs/2508.19834)
*Antero Taivalsaari,Tommi Mikkonen,Cesare Pautasso*

Main category: cs.SE

TL;DR: 本文讨论了AI辅助生成软件重用的影响，提出了研究问题和议程，以应对这一新兴方法带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI和生成软件重用成为软件开发的核心，传统方法正迅速被'AI原生'方法取代，这引发了对新形式软件重用的关注和研究需求。

Method: 通过讨论和分析AI辅助生成软件重用的现状和趋势，提出研究问题和议程。

Result: 提出了AI辅助生成软件重用的核心问题，并定义了研究议程。

Conclusion: 本文探讨了AI辅助生成软件重用的影响，提出了相关问题，并定义了研究议程以解决这一新兴方法的核心问题。

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Earlier opportunistic software reuse practices and organic
software development methods are rapidly being replaced by "AI Native"
approaches in which developers place their trust on code that has been
generated by artificial intelligence. This is leading to a new form of software
reuse that is conceptually not all that different from cargo cult development.
In this paper we discuss the implications of AI-assisted generative software
reuse, bring forth relevant questions, and define a research agenda for
tackling the central issues associated with this emerging approach.

</details>


### [97] [Generative AI for Testing of Autonomous Driving Systems: A Survey](https://arxiv.org/abs/2508.19882)
*Qunying Song,He Ye,Mark Harman,Federica Sarro*

Main category: cs.SE

TL;DR: 生成式AI在自动驾驶系统测试中的应用综述，涵盖六大应用类别、有效性评估及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的大规模部署前需要广泛测试，而生成式AI因其解释上下文、推理复杂任务和生成多样化输出的能力，正被越来越多地应用于ADS测试。

Method: 系统分析了91项相关研究，将其发现综合为六大主要应用类别，主要围绕ADS的场景测试。

Result: 综述了生成式AI在ADS测试中的有效性，并整理了用于评估的广泛数据集、模拟器、ADS、指标和基准，同时识别了27个局限性。

Conclusion: 本调查提供了关于生成式AI在自动驾驶系统测试中应用的概述和实践见解，突出了现有挑战，并概述了这一快速发展领域未来研究的方向。

Abstract: Autonomous driving systems (ADS) have been an active area of research, with
the potential to deliver significant benefits to society. However, before
large-scale deployment on public roads, extensive testing is necessary to
validate their functionality and safety under diverse driving conditions.
Therefore, different testing approaches are required, and achieving effective
and efficient testing of ADS remains an open challenge. Recently, generative AI
has emerged as a powerful tool across many domains, and it is increasingly
being applied to ADS testing due to its ability to interpret context, reason
about complex tasks, and generate diverse outputs. To gain a deeper
understanding of its role in ADS testing, we systematically analyzed 91
relevant studies and synthesized their findings into six major application
categories, primarily centered on scenario-based testing of ADS. We also
reviewed their effectiveness and compiled a wide range of datasets, simulators,
ADS, metrics, and benchmarks used for evaluation, while identifying 27
limitations. This survey provides an overview and practical insights into the
use of generative AI for testing ADS, highlights existing challenges, and
outlines directions for future research in this rapidly evolving field.

</details>


### [98] [Smart Contract Intent Detection with Pre-trained Programming Language Model](https://arxiv.org/abs/2508.20086)
*Youwei Huang,Jianwen Li,Sen Fang,Yao Li,Peng Yang,Bin Hu,Tao Zhang*

Main category: cs.SE

TL;DR: SmartIntentNN2通过BERT和BiLSTM提升智能合约恶意意图检测性能，F1达0.927。


<details>
  <summary>Details</summary>
Motivation: 智能合约开发中的恶意意图可能导致重大经济损失，因此需要高效检测工具。

Method: SmartIntentNN2结合了基于BERT的预训练语言模型和BiLSTM多标签分类网络，BERT模型使用16,000个真实智能合约数据集进行掩码语言建模训练。

Result: SmartIntentNN2的F1分数为0.927，优于前代模型的0.8633。

Conclusion: SmartIntentNN2通过整合BERT预训练语言模型和BiLSTM多标签分类网络，显著提升了智能合约恶意意图检测的性能，F1分数达到0.927，成为当前最先进的模型。

Abstract: Malicious intent in smart contract development can lead to substantial
economic losses. SmartIntentNN is a deep learning model specifically designed
to identify unsafe intents in smart contracts. This model integrates the
Universal Sentence Encoder, a K-means clustering-based intent highlighting
mechanism, and a Bidirectional Long Short-Term Memory network for multi-label
classification, achieving an F1 of 0.8633 in distinguishing ten different
intent categories. In this study, we present an upgraded version of this model,
SmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant
enhancement in V2 is the incorporation of a BERT-based pre-trained language
model, which has been trained on a dataset of 16,000 real smart contracts using
a Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based
multi-label classification network. With an improved F1 of 0.927, V2
demonstrates enhanced performance compared to its predecessor, establishing
itself as the state-of-the-art model for smart contract intent detection.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [99] [Inference of Human-derived Specifications of Object Placement via Demonstration](https://arxiv.org/abs/2508.19367)
*Alex Cuellar,Ho Chit Siu,Julie A Shah*

Main category: cs.RO

TL;DR: PARCC框架通过逻辑和演示学习提升机器人对人类空间关系偏好的理解，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 提升机器人对人类可接受的物体配置规则的理解能力，弥补现有方法在捕捉人类空间关系偏好上的不足。

Method: 基于区域连接演算（RCC）开发了PARCC逻辑框架，并设计了一种通过演示学习的推理算法。

Result: 人类研究表明，PARCC能有效捕捉人类的意图，且演示学习方法优于直接提供的规则。

Conclusion: PARCC框架通过演示学习有效捕捉人类对物体空间关系的偏好，优于人类直接提供的规则。

Abstract: As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,
object packing, sorting, and kitting), methods focused on understanding
human-acceptable object configurations remain limited expressively with regard
to capturing spatial relationships important to humans. To advance robotic
understanding of human rules for object arrangement, we introduce
positionally-augmented RCC (PARCC), a formal logic framework based on region
connection calculus (RCC) for describing the relative position of objects in
space. Additionally, we introduce an inference algorithm for learning PARCC
specifications via demonstrations. Finally, we present the results from a human
study, which demonstrate our framework's ability to capture a human's intended
specification and the benefits of learning from demonstration approaches over
human-provided specifications.

</details>


### [100] [FlipWalker: Jacob's Ladder toy-inspired robot for locomotion across diverse, complex terrain](https://arxiv.org/abs/2508.19380)
*Diancheng Li,Nia Ralston,Bastiaan Hagen,Phoebe Tan,Matthew A. Robertson*

Main category: cs.RO

TL;DR: FlipWalker是一种受Jacob's Ladder玩具启发的机器人，通过翻转运动在不规则地形中移动，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 受Jacob's Ladder玩具启发，设计能在轮式机器人难以应对的地形中移动的机器人。

Method: 通过物理基础的翻转动力学模型，分析关键设计参数，采用电机驱动的腿推动地面或相对段。

Result: 无绳原型重0.78公斤，最大翻转速度为每秒0.2体长，在人工草坪、河石和雪地中表现良好。

Conclusion: FlipWalker的翻转策略为不规则户外地形导航提供了一种有前景的替代方案。

Abstract: This paper introduces FlipWalker, a novel underactuated robot locomotion
system inspired by Jacob's Ladder illusion toy, designed to traverse
challenging terrains where wheeled robots often struggle. Like the Jacob's
Ladder toy, FlipWalker features two interconnected segments joined by flexible
cables, enabling it to pivot and flip around singularities in a manner
reminiscent of the toy's cascading motion. Actuation is provided by
motor-driven legs within each segment that push off either the ground or the
opposing segment, depending on the robot's current configuration. A
physics-based model of the underactuated flipping dynamics is formulated to
elucidate the critical design parameters governing forward motion and obstacle
clearance or climbing. The untethered prototype weighs 0.78 kg, achieves a
maximum flipping speed of 0.2 body lengths per second. Experimental trials on
artificial grass, river rocks, and snow demonstrate that FlipWalker's flipping
strategy, which relies on ground reaction forces applied normal to the surface,
offers a promising alternative to traditional locomotion for navigating
irregular outdoor terrain.

</details>


### [101] [LaVA-Man: Learning Visual Action Representations for Robot Manipulation](https://arxiv.org/abs/2508.19391)
*Chaoran Zhu,Hengyi Wang,Yik Lung Pang,Changjae Oh*

Main category: cs.RO

TL;DR: 通过自监督任务学习视觉-文本关联，提升语言引导机器人操作的精度，并在多个基准测试中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有两步方法限制了模型捕捉视觉观察与文本指令关系的能力，导致操作任务精度下降。

Method: 提出一种自监督预训练任务：基于输入图像和文本指令重建被掩码的目标图像，从而学习视觉-动作表示。

Result: 在五个基准测试中（包括模拟和真实机器人验证），该方法表现优于现有技术。

Conclusion: 通过自监督预训练任务学习视觉-文本关联，并在少量演示下微调，该方法在模拟和真实机器人验证中优于现有技术。

Abstract: Visual-textual understanding is essential for language-guided robot
manipulation. Recent works leverage pre-trained vision-language models to
measure the similarity between encoded visual observations and textual
instructions, and then train a model to map this similarity to robot actions.
However, this two-step approach limits the model to capture the relationship
between visual observations and textual instructions, leading to reduced
precision in manipulation tasks. We propose to learn visual-textual
associations through a self-supervised pretext task: reconstructing a masked
goal image conditioned on an input image and textual instructions. This
formulation allows the model to learn visual-action representations without
robot action supervision. The learned representations can then be fine-tuned
for manipulation tasks with only a few demonstrations. We also introduce the
\textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot
tabletop manipulation episodes, including 180 object classes and 3,200
instances with corresponding textual instructions. This dataset enables the
model to acquire diverse object priors and allows for a more comprehensive
evaluation of its generalisation capability across object instances.
Experimental results on the five benchmarks, including both simulated and
real-robot validations, demonstrate that our method outperforms prior art.

</details>


### [102] [From Stoplights to On-Ramps: A Comprehensive Set of Crash Rate Benchmarks for Freeway and Surface Street ADS Evaluation](https://arxiv.org/abs/2508.19425)
*John M. Scanlon,Timothy L McMurry,Yin-Hsiu Chen,Kristofer D. Kusano,Trent Victor*

Main category: cs.RO

TL;DR: 本文提出了针对美国多个城市区域的ADS碰撞率基准，首次纳入高速公路数据，发现碰撞率存在显著地理差异，强调需位置特定基准以避免评估偏差，为未来ADS安全评估提供基础。


<details>
  <summary>Details</summary>
Motivation: 扩展先前仅关注城市街道的基准，以捕捉未来ADS安全性能评估中的高速公路碰撞风险。

Method: 利用公开的警察报告碰撞和车辆行驶里程（VMT）数据，方法包括隔离运输中的乘用车、道路类型分类和碰撞类型学分析。

Result: 关键发现显示，高速公路碰撞率存在显著地理依赖性变化，亚特兰大（2.4 IPMM；最高）与凤凰城（0.7 IPMM；最低）相比，任何伤害报告碰撞率高出近3.5倍。碰撞类型的分布取决于结果严重程度，高严重性结果（如致命碰撞）中单车辆、弱势道路使用者（VRU）和对向碰撞的比例更高。

Conclusion: 本文首次生成了针对高速公路的ADS评估基准，为未来的ADS基准测试提供了基础框架，强调了位置特定基准的重要性以避免安全评估偏差。

Abstract: This paper presents crash rate benchmarks for evaluating US-based Automated
Driving Systems (ADS) for multiple urban areas. The purpose of this study was
to extend prior benchmarks focused only on surface streets to additionally
capture freeway crash risk for future ADS safety performance assessments. Using
publicly available police-reported crash and vehicle miles traveled (VMT) data,
the methodology details the isolation of in-transport passenger vehicles, road
type classification, and crash typology. Key findings revealed that freeway
crash rates exhibit large geographic dependence variations with
any-injury-reported crash rates being nearly 3.5 times higher in Atlanta (2.4
IPMM; the highest) when compared to Phoenix (0.7 IPMM; the lowest). The results
show the critical need for location-specific benchmarks to avoid biased safety
evaluations and provide insights into the vehicle miles traveled (VMT) required
to achieve statistical significance for various safety impact levels. The
distribution of crash types depended on the outcome severity level. Higher
severity outcomes (e.g., fatal crashes) had a larger proportion of
single-vehicle, vulnerable road users (VRU), and opposite-direction collisions
compared to lower severity (police-reported) crashes. Given heterogeneity in
crash types by severity, performance in low-severity scenarios may not be
predictive of high-severity outcomes. These benchmarks are additionally used to
quantify at the required mileage to show statistically significant deviations
from human performance. This is the first paper to generate freeway-specific
benchmarks for ADS evaluation and provides a foundational framework for future
ADS benchmarking by evaluators and developers.

</details>


### [103] [An Iterative Approach for Heterogeneous Multi-Agent Route Planning with Resource Transportation Uncertainty and Temporal Logic Goals](https://arxiv.org/abs/2508.19429)
*Gustavo A. Cardona,Kaier Liang,Cristian-Ioan Vasile*

Main category: cs.RO

TL;DR: 本文提出了一种异构多智能体路径规划的迭代方法，通过动态平衡探索与任务完成，有效应对资源不确定性，并通过模拟验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 研究团队面临的主要挑战是环境中初始资源分布和数量的不确定性，这影响了异构机器人团队在动态、资源受限环境中的任务执行效率。

Method: 引入了一种迭代算法，动态平衡探索与任务完成。机器人被引导探索环境，识别资源位置和数量，同时根据当前信息最大化满足任务目标，并随着新数据的发现调整策略。

Result: 通过模拟案例研究验证了该方法的有效性和性能，表明其能够在不确定性条件下高效协调异构团队。

Conclusion: 本文提出了一种在资源分布未知环境下异构多智能体路径规划的迭代方法，通过动态平衡探索与任务完成，有效解决了资源不确定性带来的挑战。通过模拟案例研究验证了方法的有效性和性能。

Abstract: This paper presents an iterative approach for heterogeneous multi-agent route
planning in environments with unknown resource distributions. We focus on a
team of robots with diverse capabilities tasked with executing missions
specified using Capability Temporal Logic (CaTL), a formal framework built on
Signal Temporal Logic to handle spatial, temporal, capability, and resource
constraints. The key challenge arises from the uncertainty in the initial
distribution and quantity of resources in the environment. To address this, we
introduce an iterative algorithm that dynamically balances exploration and task
fulfillment. Robots are guided to explore the environment, identifying resource
locations and quantities while progressively refining their understanding of
the resource landscape. At the same time, they aim to maximally satisfy the
mission objectives based on the current information, adapting their strategies
as new data is uncovered. This approach provides a robust solution for planning
in dynamic, resource-constrained environments, enabling efficient coordination
of heterogeneous teams even under conditions of uncertainty. Our method's
effectiveness and performance are demonstrated through simulated case studies.

</details>


### [104] [Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning](https://arxiv.org/abs/2508.19476)
*Dane Brouwer,Joshua Citron,Heather Nolte,Jeannette Bohg,Mark Cutkosky*

Main category: cs.RO

TL;DR: 研究机器人利用触觉和扭矩信息从密集物体中安全提取物体的方法，结合两者的策略表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究非抓取触觉传感在机器人从密集物体集合中安全提取物体中的作用。

Method: 采用模仿学习从随机生成的场景中训练策略，并进行扭矩和触觉信息的消融研究。

Result: 采用任何力传感的策略表现出更少的过度力失败、更高的整体成功率和更快的完成时间。

Conclusion: 结合触觉和扭矩信息的策略表现最佳，相比不采用力信息的基线有80%的提升。

Abstract: Dense collections of movable objects are common in everyday spaces -- from
cabinets in a home to shelves in a warehouse. Safely retracting objects from
such collections is difficult for robots, yet people do it easily, using
non-prehensile tactile sensing on the sides and backs of their hands and arms.
We investigate the role of such sensing for training robots to gently reach
into constrained clutter and extract objects. The available sensing modalities
are (1) "eye-in-hand" vision, (2) proprioception, (3) non-prehensile triaxial
tactile sensing, (4) contact wrenches estimated from joint torques, and (5) a
measure of successful object acquisition obtained by monitoring the vacuum line
of a suction cup. We use imitation learning to train policies from a set of
demonstrations on randomly generated scenes, then conduct an ablation study of
wrench and tactile information. We evaluate each policy's performance across 40
unseen environment configurations. Policies employing any force sensing show
fewer excessive force failures, an increased overall success rate, and faster
completion times. The best performance is achieved using both tactile and
wrench information, producing an 80% improvement above the baseline without
force information.

</details>


### [105] [DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View](https://arxiv.org/abs/2508.19508)
*Tian Qiu,Alan Zoubi,Yiyuan Lin,Ruiming Du,Lailiang Cheng,Yu Jiang*

Main category: cs.RO

TL;DR: DATR框架通过两阶段方法解决了稀疏视图中苹果树3D重建的挑战，性能优于现有方法且吞吐量显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏和遮挡视图中表现不佳，限制了数字孪生应用在农业中的潜力。

Method: 开发了一个两阶段框架（DATR），包括利用机载传感器和基础模型生成树掩码的第一阶段，以及结合扩散模型和大型重建模型的第二阶段。

Result: DATR在合成和实地数据集上均优于现有3D重建方法，吞吐量提高了约360倍。

Conclusion: DATR框架在稀疏视图中重建苹果树方面表现出色，性能优于现有方法，并展示了可扩展农业数字孪生系统的潜力。

Abstract: Digital twin applications offered transformative potential by enabling
real-time monitoring and robotic simulation through accurate virtual replicas
of physical assets. The key to these systems is 3D reconstruction with high
geometrical fidelity. However, existing methods struggled under field
conditions, especially with sparse and occluded views. This study developed a
two-stage framework (DATR) for the reconstruction of apple trees from sparse
views. The first stage leverages onboard sensors and foundation models to
semi-automatically generate tree masks from complex field images. Tree masks
are used to filter out background information in multi-modal data for the
single-image-to-3D reconstruction at the second stage. This stage consists of a
diffusion model and a large reconstruction model for respective multi view and
implicit neural field generation. The training of the diffusion model and LRM
was achieved by using realistic synthetic apple trees generated by a Real2Sim
data generator. The framework was evaluated on both field and synthetic
datasets. The field dataset includes six apple trees with field-measured ground
truth, while the synthetic dataset featured structurally diverse trees.
Evaluation results showed that our DATR framework outperformed existing 3D
reconstruction methods across both datasets and achieved domain-trait
estimation comparable to industrial-grade stationary laser scanners while
improving the throughput by $\sim$360 times, demonstrating strong potential for
scalable agricultural digital twin systems.

</details>


### [106] [A Lightweight Crowd Model for Robot Social Navigation](https://arxiv.org/abs/2508.19595)
*Maryam Kazemi Eskeri,Thomas Wiedemann,Ville Kyrki,Dominik Baumann,Tomasz Piotr Kucner*

Main category: cs.RO

TL;DR: 提出了一种轻量级实时宏观人群预测模型，显著提升计算效率和预测准确性，适用于机器人动态环境导航。


<details>
  <summary>Details</summary>
Motivation: 机器人在人类密集环境中操作时，需要安全高效地导航并最小化社会干扰。传统微观模型在密集人群中难以扩展，而现有宏观模型要么过于简化，要么计算成本高。

Method: 提出了一种轻量级的实时宏观人群预测模型，专门针对人类运动，平衡了预测准确性和计算效率。该方法基于行人流的固有特性简化了空间和时间处理，无需复杂架构即可实现稳健的泛化。

Result: 模型在推理时间上减少了3.6倍，同时预测准确率提高了3.1%。

Conclusion: 高效的人群建模使机器人能够在不需要昂贵计算的情况下在密集环境中导航。

Abstract: Robots operating in human-populated environments must navigate safely and
efficiently while minimizing social disruption. Achieving this requires
estimating crowd movement to avoid congested areas in real-time. Traditional
microscopic models struggle to scale in dense crowds due to high computational
cost, while existing macroscopic crowd prediction models tend to be either
overly simplistic or computationally intensive. In this work, we propose a
lightweight, real-time macroscopic crowd prediction model tailored for human
motion, which balances prediction accuracy and computational efficiency. Our
approach simplifies both spatial and temporal processing based on the inherent
characteristics of pedestrian flow, enabling robust generalization without the
overhead of complex architectures. We demonstrate a 3.6 times reduction in
inference time, while improving prediction accuracy by 3.1 %. Integrated into a
socially aware planning framework, the model enables efficient and socially
compliant robot navigation in dynamic environments. This work highlights that
efficient human crowd modeling enables robots to navigate dense environments
without costly computations.

</details>


### [107] [Impedance Primitive-augmented Hierarchical Reinforcement Learning for Sequential Tasks](https://arxiv.org/abs/2508.19607)
*Amin Berjaoui Tahmaz,Ravi Prakash,Jens Kober*

Main category: cs.RO

TL;DR: 该论文提出了一种分层强化学习框架，通过可变刚度控制和高效探索机制，提升了机器人在序列接触任务中的性能和适应性。


<details>
  <summary>Details</summary>
Motivation: 旨在提高机器人操作在序列接触任务中的效率和适应性，特别是在需要可变刚度控制的场景下。

Method: 提出了一个阻抗基元增强的分层强化学习框架，结合了三个关键组件：支持可变刚度控制的动作空间、自适应刚度控制器以及高效探索的耦合机制。

Result: 通过全面训练和评估，框架在学习效率、基元选择组合性和成功率上优于现有技术，并展示了良好的模拟到现实迁移能力。

Conclusion: 该论文为更自适应和多功能机器人操作系统奠定了基础，并展示了在复杂接触任务中的潜在应用。

Abstract: This paper presents an Impedance Primitive-augmented hierarchical
reinforcement learning framework for efficient robotic manipulation in
sequential contact tasks. We leverage this hierarchical structure to
sequentially execute behavior primitives with variable stiffness control
capabilities for contact tasks. Our proposed approach relies on three key
components: an action space enabling variable stiffness control, an adaptive
stiffness controller for dynamic stiffness adjustments during primitive
execution, and affordance coupling for efficient exploration while encouraging
compliance. Through comprehensive training and evaluation, our framework learns
efficient stiffness control capabilities and demonstrates improvements in
learning efficiency, compositionality in primitive selection, and success rates
compared to the state-of-the-art. The training environments include block
lifting, door opening, object pushing, and surface cleaning. Real world
evaluations further confirm the framework's sim2real capability. This work lays
the foundation for more adaptive and versatile robotic manipulation systems,
with potential applications in more complex contact-based tasks.

</details>


### [108] [Autonomous Aerial Manipulation at Arbitrary Pose in SE(3) with Robust Control and Whole-body Planning](https://arxiv.org/abs/2508.19608)
*Dongjae Lee,Byeongjun Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: 本文提出了一种全向空中机械手的几何鲁棒控制与运动规划框架，显著扩展了其操作能力，实验验证了在极端姿态下的任务执行效果。


<details>
  <summary>Details</summary>
Motivation: 传统多旋翼机械手由于基座欠驱动特性，仅能在小角度滚转和俯仰下操作，限制了其工作空间和任务范围。

Method: 首先设计了一个针对浮动基座的几何鲁棒控制器，随后开发了一个基于两步优化的全身运动规划器，联合考虑基座姿态和机械臂关节角度。

Result: 实验证明，所提框架使全向空中机械手能够在接近90°甚至180°俯仰角等极端姿态下稳定执行抓取和拉动任务。

Conclusion: 提出的几何鲁棒控制和全身运动规划框架显著扩展了全向空中机械手的操作能力，使其能够在任意6D姿态下稳定执行复杂任务。

Abstract: Aerial manipulators based on conventional multirotors can conduct
manipulation only in small roll and pitch angles due to the underactuatedness
of the multirotor base. If the multirotor base is capable of hovering at
arbitrary orientation, the robot can freely locate itself at any point in
$\mathsf{SE}(3)$, significantly extending its manipulation workspace and
enabling a manipulation task that was originally not viable. In this work, we
present a geometric robust control and whole-body motion planning framework for
an omnidirectional aerial manipulator (OAM). To maximize the strength of OAM,
we first propose a geometric robust controller for a floating base. Since the
motion of the robotic arm and the interaction forces during manipulation affect
the stability of the floating base, the base should be capable of mitigating
these adverse effects while controlling its 6D pose. We then design a two-step
optimization-based whole-body motion planner, jointly considering the pose of
the floating base and the joint angles of the robotic arm to harness the entire
configuration space. The devised two-step approach facilitates real-time
applicability and enhances convergence of the optimization problem with
non-convex and non-Euclidean search space. The proposed approach enables the
base to be stationary at any 6D pose while autonomously carrying out
sophisticated manipulation near obstacles without any collision. We demonstrate
the effectiveness of the proposed framework through experiments in which an OAM
performs grasping and pulling of an object in multiple scenarios, including
near $90^\circ$ and even $180^\circ$ pitch angles.

</details>


### [109] [Embodied Intelligence for Sustainable Flight: A Soaring Robot with Active Morphological Control](https://arxiv.org/abs/2508.19684)
*Ghadeer Elmkaiel,Syn Schmitt,Michael Muehlebach*

Main category: cs.RO

TL;DR: Floaty是一种形态可变机器人，通过被动翱翔和智能形态控制在动态风环境中实现高效能源利用，能耗比传统系统低10倍。


<details>
  <summary>Details</summary>
Motivation: 解决传统推进系统高能耗和固定翼设计缺乏悬停和机动能力的问题，实现空中机器人在动态风环境中的敏捷性和高能效。

Method: Floaty采用被动翱翔和智能形态控制，基于实验学习的空气动力学模型优化设计，实现无主动推进的精确姿态和位置控制。

Result: 风洞实验显示Floaty能在10 m/s垂直气流中悬停、机动并抗干扰，能耗低至10 W/kg，比推进系统低一个数量级。

Conclusion: Floaty通过形态智能和控制实现了在动态风环境中的高效能源利用，为能源高效空中机器人提供了新范式。

Abstract: Achieving both agile maneuverability and high energy efficiency in aerial
robots, particularly in dynamic wind environments, remains challenging.
Conventional thruster-powered systems offer agility but suffer from high energy
consumption, while fixed-wing designs are efficient but lack hovering and
maneuvering capabilities. We present Floaty, a shape-changing robot that
overcomes these limitations by passively soaring, harnessing wind energy
through intelligent morphological control inspired by birds. Floaty's design is
optimized for passive stability, and its control policy is derived from an
experimentally learned aerodynamic model, enabling precise attitude and
position control without active propulsion. Wind tunnel experiments demonstrate
Floaty's ability to hover, maneuver, and reject disturbances in vertical
airflows up to 10 m/s. Crucially, Floaty achieves this with a specific power
consumption of 10 W/kg, an order of magnitude lower than thruster-powered
systems. This introduces a paradigm for energy-efficient aerial robotics,
leveraging morphological intelligence and control to operate sustainably in
challenging wind conditions.

</details>


### [110] [Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared Environments](https://arxiv.org/abs/2508.19731)
*Maryam Kazemi Eskeri,Ville Kyrki,Dominik Baumann,Tomasz Piotr Kucner*

Main category: cs.RO

TL;DR: 本文提出了一种结合动态地图（MoDs）的多机器人任务分配方法，显著减少了任务完成时间，强调了在共享环境中考虑人类动态的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数MRTA方法是动态无关的，依赖于静态地图并忽略人类运动模式，导致效率低下和延迟。

Method: 本文引入了利用动态地图（MoDs）的方法，这是一种时空可查询模型，旨在捕捉历史人类运动模式，以估计人类对任务执行时间的影响。

Result: 实验结果表明，集成MoDs可以提高任务分配性能，与动态无关方法相比，任务完成时间最多减少26%，与基线方法相比最多减少19%。

Conclusion: 本文强调了在共享环境中考虑人类动态对多机器人任务分配（MRTA）的重要性，并提出了一种高效框架，用于在人类密集环境中部署多机器人系统。

Abstract: Multi-robot systems are increasingly deployed in applications, such as
intralogistics or autonomous delivery, where multiple robots collaborate to
complete tasks efficiently. One of the key factors enabling their efficient
cooperation is Multi-Robot Task Allocation (MRTA). Algorithms solving this
problem optimize task distribution among robots to minimize the overall
execution time. In shared environments, apart from the relative distance
between the robots and the tasks, the execution time is also significantly
impacted by the delay caused by navigating around moving people. However, most
existing MRTA approaches are dynamics-agnostic, relying on static maps and
neglecting human motion patterns, leading to inefficiencies and delays. In this
paper, we introduce \acrfull{method name}. This method leverages Maps of
Dynamics (MoDs), spatio-temporal queryable models designed to capture
historical human movement patterns, to estimate the impact of humans on the
task execution time during deployment. \acrshort{method name} utilizes a
stochastic cost function that includes MoDs. Experimental results show that
integrating MoDs enhances task allocation performance, resulting in reduced
mission completion times by up to $26\%$ compared to the dynamics-agnostic
method and up to $19\%$ compared to the baseline. This work underscores the
importance of considering human dynamics in MRTA within shared environments and
presents an efficient framework for deploying multi-robot systems in
environments populated by humans.

</details>


### [111] [Elliptical K-Nearest Neighbors -- Path Optimization via Coulomb's Law and Invalid Vertices in C-space Obstacles](https://arxiv.org/abs/2508.19771)
*Liding Zhang,Zhenshan Bing,Yu Zhang,Kuanqi Cai,Lingyun Chen,Fan Wu,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: FDIT* 是一种基于采样的路径规划器，通过结合无效顶点信息和物理力原理，提升了高维环境中的搜索效率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 高维运动规划中存在速度与成本效益的挑战，FDIT* 旨在通过利用无效顶点信息和物理力原理，提升路径规划的效率和效果。

Method: FDIT* 基于最先进的信息采样规划器 EIT*，引入了椭圆形的 k-最近邻搜索方法，结合物理力原理（如库仑定律），探索更具问题特异性的搜索区域，从而避免高成本或不可行路径。

Result: FDIT* 在 R^4 到 R^16 的测试问题中优于现有单查询、基于采样的规划器，并在实际移动操作任务中验证了其有效性。

Conclusion: FDIT* 是一种基于采样的路径规划器，通过利用无效顶点中常被忽视的信息并结合物理力原理（如库仑定律），在搜索效率和成本降低方面表现出色，尤其在受限的高维环境中。它优于现有的单查询、基于采样的规划器，并在实际移动操作任务中得到了验证。

Abstract: Path planning has long been an important and active research area in
robotics. To address challenges in high-dimensional motion planning, this study
introduces the Force Direction Informed Trees (FDIT*), a sampling-based planner
designed to enhance speed and cost-effectiveness in pathfinding. FDIT* builds
upon the state-of-the-art informed sampling planner, the Effort Informed Trees
(EIT*), by capitalizing on often-overlooked information in invalid vertices. It
incorporates principles of physical force, particularly Coulomb's law. This
approach proposes the elliptical $k$-nearest neighbors search method, enabling
fast convergence navigation and avoiding high solution cost or infeasible paths
by exploring more problem-specific search-worthy areas. It demonstrates
benefits in search efficiency and cost reduction, particularly in confined,
high-dimensional environments. It can be viewed as an extension of nearest
neighbors search techniques. Fusing invalid vertex data with physical dynamics
facilitates force-direction-based search regions, resulting in an improved
convergence rate to the optimum. FDIT* outperforms existing single-query,
sampling-based planners on the tested problems in R^4 to R^16 and has been
demonstrated on a real-world mobile manipulation task.

</details>


### [112] [Tree-Based Grafting Approach for Bidirectional Motion Planning with Local Subsets Optimization](https://arxiv.org/abs/2508.19776)
*Liding Zhang,Yao Ling,Zhenshan Bing,Fan Wu,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: G3T*是一种新型路径规划器，通过嫁接无效边连接和动态调整采样分布，显著提升双向运动规划的性能。


<details>
  <summary>Details</summary>
Motivation: 传统双向运动规划因懒惰反向搜索限制可能导致失败并重启，G3T*旨在通过嫁接无效边连接解决这一问题，提升路径收敛速度和成本。

Method: G3T*采用贪婪策略，利用GuILD子集的最小Lebesgue测度高效优化路径，并动态调整采样分布以确保渐进最优性。

Result: G3T*在多维空间（R^2到R^8）及实际机器人评估中表现优于现有单查询采样规划器，实现了更快收敛和更低成本。

Conclusion: G3T*通过贪婪方法优化路径，动态调整采样分布，显著提升了双向运动规划的性能，实验证明其在多维度及实际机器人应用中优于现有单查询采样规划器。

Abstract: Bidirectional motion planning often reduces planning time compared to its
unidirectional counterparts. It requires connecting the forward and reverse
search trees to form a continuous path. However, this process could fail and
restart the asymmetric bidirectional search due to the limitations of
lazy-reverse search. To address this challenge, we propose Greedy GuILD
Grafting Trees (G3T*), a novel path planner that grafts invalid edge
connections at both ends to re-establish tree-based connectivity, enabling
rapid path convergence. G3T* employs a greedy approach using the minimum
Lebesgue measure of guided incremental local densification (GuILD) subsets to
optimize paths efficiently. Furthermore, G3T* dynamically adjusts the sampling
distribution between the informed set and GuILD subsets based on historical and
current cost improvements, ensuring asymptotic optimality. These features
enhance the forward search's growth towards the reverse tree, achieving faster
convergence and lower solution costs. Benchmark experiments across dimensions
from R^2 to R^8 and real-world robotic evaluations demonstrate G3T*'s superior
performance compared to existing single-query sampling-based planners. A video
showcasing our experimental results is available at:
https://youtu.be/3mfCRL5SQIU

</details>


### [113] [Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots](https://arxiv.org/abs/2508.19788)
*Sena Ishii,Akash Chikhalikar,Ankit A. Ravankar,Jose Victorio Salazar Luces,Yasuhisa Hirata*

Main category: cs.RO

TL;DR: 提出了一种新颖框架，通过语义图算法估计室内场景中的事故易发区域，旨在提高服务机器人在人类环境中的实时风险意识，验证准确率为75%。


<details>
  <summary>Details</summary>
Motivation: 随着机器人融入日常生活，特别是在家庭中，预测和应对环境危险的能力对于确保用户安全、信任和有效的人机交互至关重要。

Method: 通过基于语义图的传播算法建模对象级风险和上下文，每个对象表示为具有关联风险分数的节点，风险基于空间接近性和事故关系从高风险对象向低风险对象不对称传播。

Result: 在人类标注的风险区域数据集上验证了该方法，实现了75%的二元风险检测准确率，系统在涉及尖锐或不稳定物体的场景中表现出与人类感知的强一致性。

Conclusion: 该框架为未来系统提供了基础，能够做出基于上下文的安全决策、提供实时警报或自主协助用户避免或减轻家庭环境中的危险。

Abstract: We present a novel framework for estimating accident-prone regions in
everyday indoor scenes, aimed at improving real-time risk awareness in service
robots operating in human-centric environments. As robots become integrated
into daily life, particularly in homes, the ability to anticipate and respond
to environmental hazards is crucial for ensuring user safety, trust, and
effective human-robot interaction. Our approach models object-level risk and
context through a semantic graph-based propagation algorithm. Each object is
represented as a node with an associated risk score, and risk propagates
asymmetrically from high-risk to low-risk objects based on spatial proximity
and accident relationship. This enables the robot to infer potential hazards
even when they are not explicitly visible or labeled. Designed for
interpretability and lightweight onboard deployment, our method is validated on
a dataset with human-annotated risk regions, achieving a binary risk detection
accuracy of 75%. The system demonstrates strong alignment with human
perception, particularly in scenes involving sharp or unstable objects. These
results underline the potential of context-aware risk reasoning to enhance
robotic scene understanding and proactive safety behaviors in shared
human-robot spaces. This framework could serve as a foundation for future
systems that make context-driven safety decisions, provide real-time alerts, or
autonomously assist users in avoiding or mitigating hazards within home
environments.

</details>


### [114] [APT*: Asymptotically Optimal Motion Planning via Adaptively Prolated Elliptical R-Nearest Neighbors](https://arxiv.org/abs/2508.19790)
*Liding Zhang,Sicheng Wang,Kuanqi Cai,Zhenshan Bing,Fan Wu,Chaoqun Wang,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: APT*是一种新型路径规划方法，通过自适应批处理和虚拟力优化，显著提升性能并在高维空间和实际任务中验证。


<details>
  <summary>Details</summary>
Motivation: 现有路径规划方法常采用固定批处理大小且忽略障碍物信息，导致效率低下且缺乏适应性。

Method: APT*结合了自适应批处理大小和椭圆形r-最近邻模块，利用虚拟力定义和电荷调整来优化路径搜索过程。

Result: APT*在ℝ⁴到ℝ¹⁶的维度中表现优于现有单查询采样规划器，并在实际机器人任务中验证了其有效性。

Conclusion: APT*通过自适应批处理和椭圆形最近邻模块，显著提升了路径规划的效率和质量，在多个维度上优于现有方法，并在实际机器人操作任务中得到验证。

Abstract: Optimal path planning aims to determine a sequence of states from a start to
a goal while accounting for planning objectives. Popular methods often
integrate fixed batch sizes and neglect information on obstacles, which is not
problem-specific. This study introduces Adaptively Prolated Trees (APT*), a
novel sampling-based motion planner that extends based on Force Direction
Informed Trees (FDIT*), integrating adaptive batch-sizing and elliptical
$r$-nearest neighbor modules to dynamically modulate the path searching process
based on environmental feedback. APT* adjusts batch sizes based on the
hypervolume of the informed sets and considers vertices as electric charges
that obey Coulomb's law to define virtual forces via neighbor samples, thereby
refining the prolate nearest neighbor selection. These modules employ
non-linear prolate methods to adaptively adjust the electric charges of
vertices for force definition, thereby improving the convergence rate with
lower solution costs. Comparative analyses show that APT* outperforms existing
single-query sampling-based planners in dimensions from $\mathbb{R}^4$ to
$\mathbb{R}^{16}$, and it was further validated through a real-world robot
manipulation task. A video showcasing our experimental results is available at:
https://youtu.be/gCcUr8LiEw4

</details>


### [115] [A Standing Support Mobility Robot for Enhancing Independence in Elderly Daily Living](https://arxiv.org/abs/2508.19816)
*Ricardo J. Manríquez-Cisterna,Ankit A. Ravankar,Jose V. Salazar Luces,Takuro Hatsukari,Yasuhisa Hirata*

Main category: cs.RO

TL;DR: Moby是一款站立支撑移动机器人，旨在提升老年人的独立性和安全性，通过ROS控制、多功能设计和实验验证，展示了其相较于传统方法的优势。


<details>
  <summary>Details</summary>
Motivation: 为了提升老年人在日常活动（如如厕转移）中的独立性和安全性，同时减少身体负担并支持自然社交互动，设计了站立支撑移动机器人Moby。

Method: 该研究采用了Robot Operating System（ROS）进行无缝控制，结合手动和自主操作模式。定制控制系统确保了安全直观的互动，而NAV2和LiDAR的集成提供了强大的导航能力。研究还包括了NASA-TLX方法和时间比较实验。

Result: Moby机器人展示了易用性、轻量化设计、舒适性、多功能性和有效的坐-站辅助功能。实验结果表明其在设计标准验证和优势展示方面具有显著效果。

Conclusion: Moby机器人通过其创新的站立支撑设计，显著提升了老年人在日常活动中的独立性和安全性，同时减轻了身体负担并增强了社交互动的自然性。实验验证了其设计标准的有效性，并展示了相较于传统方法的优势。

Abstract: This paper presents a standing support mobility robot "Moby" developed to
enhance independence and safety for elderly individuals during daily activities
such as toilet transfers. Unlike conventional seated mobility aids, the robot
maintains users in an upright posture, reducing physical strain, supporting
natural social interaction at eye level, and fostering a greater sense of
self-efficacy. Moby offers a novel alternative by functioning both passively
and with mobility support, enabling users to perform daily tasks more
independently. Its main advantages include ease of use, lightweight design,
comfort, versatility, and effective sit-to-stand assistance. The robot
leverages the Robot Operating System (ROS) for seamless control, featuring
manual and autonomous operation modes. A custom control system enables safe and
intuitive interaction, while the integration with NAV2 and LiDAR allows for
robust navigation capabilities. This paper reviews existing mobility solutions
and compares them to Moby, details the robot's design, and presents objective
and subjective experimental results using the NASA-TLX method and time
comparisons to other methods to validate our design criteria and demonstrate
the advantages of our contribution.

</details>


### [116] [FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control](https://arxiv.org/abs/2508.19926)
*Tan Jing,Shiting Chen,Yangfan Li,Weisheng Xu,Renjing Xu*

Main category: cs.RO

TL;DR: FARM通过帧加速增强和残差MoE显著提升了高动态人形动作的跟踪精度，并公开了首个高动态人形动作数据集HDHM。


<details>
  <summary>Details</summary>
Motivation: 现有的统一物理人形控制器在处理爆炸性动作时表现不佳，限制了实际应用。FARM旨在填补这一空白，提升高动态动作的跟踪能力。

Method: FARM框架包括帧加速增强、鲁棒的基础控制器和残差专家混合（MoE）。帧加速增强通过扩大帧间间隔使模型接触高速姿态变化，基础控制器可靠跟踪日常低动态动作，残差MoE自适应分配额外网络容量处理高动态动作。

Result: 在HDHM数据集上，FARM将跟踪失败率降低了42.8%，全局平均每关节位置误差降低了14.6%，同时保持了低动态动作的近乎完美准确性。

Conclusion: FARM通过结合帧加速增强、鲁棒的基础控制器和残差专家混合（MoE），显著提高了高动态动作的跟踪精度，同时保持了低动态动作的近乎完美准确性，为高动态人形控制设立了新基准，并首次公开了相关数据集和代码。

Abstract: Unified physics-based humanoid controllers are pivotal for robotics and
character animation, yet models that excel on gentle, everyday motions still
stumble on explosive actions, hampering real-world deployment. We bridge this
gap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts),
an end-to-end framework composed of frame-accelerated augmentation, a robust
base controller, and a residual mixture-of-experts (MoE). Frame-accelerated
augmentation exposes the model to high-velocity pose changes by widening
inter-frame gaps. The base controller reliably tracks everyday low-dynamic
motions, while the residual MoE adaptively allocates additional network
capacity to handle challenging high-dynamic actions, significantly enhancing
tracking accuracy. In the absence of a public benchmark, we curate the
High-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically
plausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8\% and
lowers global mean per-joint position error by 14.6\% relative to the baseline,
while preserving near-perfect accuracy on low-dynamic motions. These results
establish FARM as a new baseline for high-dynamic humanoid control and
introduce the first open benchmark dedicated to this challenge. The code and
dataset will be released at https://github.com/Colin-Jing/FARM.

</details>


### [117] [Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors](https://arxiv.org/abs/2508.19953)
*Rafael Cathomen,Mayank Mittal,Marin Vlastelica,Marco Hutter*

Main category: cs.RO

TL;DR: 模块化USD框架通过状态分解和对称性偏置，实现安全、可解释的技能发现，并在仿真和硬件中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决当前USD方法在安全性、可解释性和可部署性方面的挑战，推动其在真实机器人中的应用。

Method: 采用用户定义的状态空间分解方法，为每个因子分配不同的技能发现算法，并引入对称性诱导偏置和风格因子及正则化惩罚。

Result: 仿真和实际硬件实验表明，该框架能够发现结构化、可解释的行为，同时提升安全性和多样性，且学习到的技能在下游任务中表现优异。

Conclusion: 提出的模块化USD框架通过状态空间分解和对称性诱导偏置，成功实现了结构化、可解释且安全的技能发现，并在仿真和实际硬件中验证了其有效性。

Abstract: Unsupervised Skill Discovery (USD) allows agents to autonomously learn
diverse behaviors without task-specific rewards. While recent USD methods have
shown promise, their application to real-world robotics remains underexplored.
In this paper, we propose a modular USD framework to address the challenges in
the safety, interpretability, and deployability of the learned skills. Our
approach employs user-defined factorization of the state space to learn
disentangled skill representations. It assigns different skill discovery
algorithms to each factor based on the desired intrinsic reward function. To
encourage structured morphology-aware skills, we introduce symmetry-based
inductive biases tailored to individual factors. We also incorporate a style
factor and regularization penalties to promote safe and robust behaviors. We
evaluate our framework in simulation using a quadrupedal robot and demonstrate
zero-shot transfer of the learned skills to real hardware. Our results show
that factorization and symmetry lead to the discovery of structured
human-interpretable behaviors, while the style factor and penalties enhance
safety and diversity. Additionally, we show that the learned skills can be used
for downstream tasks and perform on par with oracle policies trained with
hand-crafted rewards.

</details>


### [118] [Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation](https://arxiv.org/abs/2508.19958)
*Yiguo Fan,Pengxiang Ding,Shuanghao Bai,Xinyang Tong,Yuyang Zhu,Hongchao Lu,Fengqi Dai,Wei Zhao,Yang Liu,Siteng Huang,Zhaoxin Fan,Badong Chen,Donglin Wang*

Main category: cs.RO

TL;DR: Long-VLA 是首个专为长视野机器人任务设计的端到端 VLA 模型，通过相位感知输入掩码策略提升性能，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的 VLA 框架主要针对短视野任务，而在长视野、多步骤机器人操作中的有效性受限，主要由于技能链和子任务依赖性的挑战。

Method: Long-VLA 采用了一种新颖的相位感知输入掩码策略，将每个子任务自适应地分割为移动和交互阶段，以增强子任务兼容性。该方法保留了 VLA 训练的可扩展性和数据效率，并可无缝集成到现有 VLA 模型中。

Result: 在模拟和真实世界任务中的大量实验表明，Long-VLA 显著优于现有最先进方法。

Conclusion: Long-VLA 模型通过引入相位感知输入掩码策略，显著提升了长视野机器人任务的性能，为长视野机器人控制设立了新的基准。

Abstract: Vision-Language-Action (VLA) models have become a cornerstone in robotic
policy learning, leveraging large-scale multimodal data for robust and scalable
control. However, existing VLA frameworks primarily address short-horizon
tasks, and their effectiveness on long-horizon, multi-step robotic manipulation
remains limited due to challenges in skill chaining and subtask dependencies.
In this work, we introduce Long-VLA, the first end-to-end VLA model
specifically designed for long-horizon robotic tasks. Our approach features a
novel phase-aware input masking strategy that adaptively segments each subtask
into moving and interaction phases, enabling the model to focus on
phase-relevant sensory cues and enhancing subtask compatibility. This unified
strategy preserves the scalability and data efficiency of VLA training, and our
architecture-agnostic module can be seamlessly integrated into existing VLA
models. We further propose the L-CALVIN benchmark to systematically evaluate
long-horizon manipulation. Extensive experiments on both simulated and
real-world tasks demonstrate that Long-VLA significantly outperforms prior
state-of-the-art methods, establishing a new baseline for long-horizon robotic
control.

</details>


### [119] [Visio-Verbal Teleimpedance Interface: Enabling Semi-Autonomous Control of Physical Interaction via Eye Tracking and Speech](https://arxiv.org/abs/2508.20037)
*Henk H. A. Jekel,Alejandro Díaz Rosales,Luka Peternel*

Main category: cs.RO

TL;DR: 论文提出了一种结合注视和语音的遥阻抗界面，用于控制远程机器人的3D刚度椭球体，实验验证了其有效性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 为了提高远程机器人控制的直观性和交互效率，研究者提出了一种结合视觉和语音输入的遥阻抗界面，旨在简化复杂物理交互动作的命令生成过程。

Method: 通过眼动仪捕捉操作者的注视信息，结合语音交互和视觉语言模型（VLM）处理，生成适当的刚度矩阵。实验使用了Force Dimension Sigma.7触觉设备和Kuka LBR iiwa机械臂，以及Tobii Pro Glasses 2眼动仪和GPT-4o处理的语音命令。

Result: 实验验证了界面在多种任务中的有效性，包括探索最优提示配置和演示在滑槽任务中的不同功能。

Conclusion: 论文提出的视-语遥阻抗界面通过结合操作者的注视和语音交互，成功实现了对远程机器人3D刚度椭球体的有效控制。实验验证了该界面在多种任务中的实用性和灵活性。

Abstract: The paper presents a visio-verbal teleimpedance interface for commanding 3D
stiffness ellipsoids to the remote robot with a combination of the operator's
gaze and verbal interaction. The gaze is detected by an eye-tracker, allowing
the system to understand the context in terms of what the operator is currently
looking at in the scene. Along with verbal interaction, a Visual Language Model
(VLM) processes this information, enabling the operator to communicate their
intended action or provide corrections. Based on these inputs, the interface
can then generate appropriate stiffness matrices for different physical
interaction actions. To validate the proposed visio-verbal teleimpedance
interface, we conducted a series of experiments on a setup including a Force
Dimension Sigma.7 haptic device to control the motion of the remote Kuka LBR
iiwa robotic arm. The human operator's gaze is tracked by Tobii Pro Glasses 2,
while human verbal commands are processed by a VLM using GPT-4o. The first
experiment explored the optimal prompt configuration for the interface. The
second and third experiments demonstrated different functionalities of the
interface on a slide-in-the-groove task.

</details>


### [120] [HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation](https://arxiv.org/abs/2508.20085)
*Zhecheng Yuan,Tianming Wei,Langzhe Gu,Pu Hua,Tianhai Liang,Yuanpei Chen,Huazhe Xu*

Main category: cs.RO

TL;DR: HERMES是一个将多源人类手部动作转化为机器人行为的框架，通过强化学习、sim2real转移和增强导航模型，实现了在多样化环境中的通用灵巧操作。


<details>
  <summary>Details</summary>
Motivation: 将多源人类手部动作转化为机器人行为存在挑战，尤其是针对高维动作空间的多指灵巧手机器人，且现有方法难以适应多样化环境条件。

Method: HERMES采用统一的强化学习方法将多源人类手部动作转化为机器人行为，设计基于深度图像的端到端sim2real转移方法，并增强导航基础模型以实现闭环PnP定位。

Result: 实验结果表明，HERMES在多样化、非结构化环境中表现出通用性，成功完成多项复杂移动双手灵巧操作任务。

Conclusion: HERMES框架通过统一的强化学习方法、改进的sim2real转移方法和增强的导航基础模型，成功实现了在多样化、非结构化环境中通用的移动双手灵巧操作任务。

Abstract: Leveraging human motion data to impart robots with versatile manipulation
skills has emerged as a promising paradigm in robotic manipulation.
Nevertheless, translating multi-source human hand motions into feasible robot
behaviors remains challenging, particularly for robots equipped with
multi-fingered dexterous hands characterized by complex, high-dimensional
action spaces. Moreover, existing approaches often struggle to produce policies
capable of adapting to diverse environmental conditions. In this paper, we
introduce HERMES, a human-to-robot learning framework for mobile bimanual
dexterous manipulation. First, HERMES formulates a unified reinforcement
learning approach capable of seamlessly transforming heterogeneous human hand
motions from multiple sources into physically plausible robotic behaviors.
Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth
image-based sim2real transfer method for improved generalization to real-world
scenarios. Furthermore, to enable autonomous operation in varied and
unstructured environments, we augment the navigation foundation model with a
closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise
alignment of visual goals and effectively bridging autonomous navigation and
dexterous manipulation. Extensive experimental results demonstrate that HERMES
consistently exhibits generalizable behaviors across diverse, in-the-wild
scenarios, successfully performing numerous complex mobile bimanual dexterous
manipulation tasks. Project Page:https:/gemcollector.github.io/HERMES/.

</details>


### [121] [Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning](https://arxiv.org/abs/2508.20095)
*Jinhao Liang,Sven Koenig,Ferdinando Fioretto*

Main category: cs.RO

TL;DR: DGD框架结合离散MAPF和生成扩散模型，解决多机器人运动规划问题，在大规模环境中实现高效和高成功率。


<details>
  <summary>Details</summary>
Motivation: 现有离散MAPF方法因粗粒度离散化限制轨迹质量，而连续优化方法因维度灾难难以扩展。本文旨在结合两者优势，提出新框架以克服这些限制。

Method: 框架将原始非凸MRMP问题分解为具有凸配置空间的子问题，结合离散MAPF解决方案和约束优化技术，引导扩散模型捕捉机器人间的复杂时空依赖，并引入轻量级约束修复机制确保轨迹可行性。

Result: DGD在大型复杂环境中表现优异，可扩展至100个机器人，同时实现高效规划和高成功率。

Conclusion: 本文提出的Discrete-Guided Diffusion (DGD)框架通过结合离散MAPF求解器和生成扩散模型，有效解决了多机器人运动规划中的非凸问题，实现了在大规模复杂环境下的高效规划和成功率高。

Abstract: Multi-Robot Motion Planning (MRMP) involves generating collision-free
trajectories for multiple robots operating in a shared continuous workspace.
While discrete multi-agent path finding (MAPF) methods are broadly adopted due
to their scalability, their coarse discretization severely limits trajectory
quality. In contrast, continuous optimization-based planners offer
higher-quality paths but suffer from the curse of dimensionality, resulting in
poor scalability with respect to the number of robots. This paper tackles the
limitations of these two approaches by introducing a novel framework that
integrates discrete MAPF solvers with constrained generative diffusion models.
The resulting framework, called Discrete-Guided Diffusion (DGD), has three key
characteristics: (1) it decomposes the original nonconvex MRMP problem into
tractable subproblems with convex configuration spaces, (2) it combines
discrete MAPF solutions with constrained optimization techniques to guide
diffusion models capture complex spatiotemporal dependencies among robots, and
(3) it incorporates a lightweight constraint repair mechanism to ensure
trajectory feasibility. The proposed method sets a new state-of-the-art
performance in large-scale, complex environments, scaling to 100 robots while
achieving planning efficiency and high success rates.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [122] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: 论文提出用心理测量特质模型解释LLMs中的奉承行为，并通过CAA实现可组合的向量干预。


<details>
  <summary>Details</summary>
Motivation: 奉承行为是LLMs中的关键行为风险，但通常被视为单一因果机制的孤立失败模式。

Method: 使用对比激活加法（CAA）将激活方向映射到心理测量因素，并研究不同组合如何导致奉承行为。

Result: 通过几何和因果组合的心理测量特质模型，实现了对奉承行为的可解释干预。

Conclusion: 论文提出了一种基于几何和因果组合的心理测量特质模型，用于理解和干预LLMs中的奉承行为，为安全关键行为的缓解提供了可解释和组合的向量干预方法。

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [123] [Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science](https://arxiv.org/abs/2508.19383)
*Daoyuan Jin,Nick Gunner,Niko Carvajal Janke,Shivranjani Baruah,Kaitlin M. Gold,Yu Jiang*

Main category: cs.AI

TL;DR: Aleks是一个AI系统，能够自主进行植物科学研究，通过案例研究展示了其在加速科学发现方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 现代植物科学日益依赖大型、异构数据集，但实验设计、数据预处理和可重复性方面的挑战阻碍了研究效率。

Method: Aleks是一个AI驱动的多代理系统，集成了领域知识、数据分析和机器学习，通过结构化框架自主进行数据驱动的科学发现。

Result: 在一项关于葡萄藤红斑病的案例研究中，Aleks逐步识别出具有生物学意义的特征，并收敛于具有稳健性能的可解释模型。

Conclusion: 这项探索性工作展示了代理式AI作为自主合作者在加速植物科学领域的科学发现方面的潜力。

Abstract: Modern plant science increasingly relies on large, heterogeneous datasets,
but challenges in experimental design, data preprocessing, and reproducibility
hinder research throughput. Here we introduce Aleks, an AI-powered multi-agent
system that integrates domain knowledge, data analysis, and machine learning
within a structured framework to autonomously conduct data-driven scientific
discovery. Once provided with a research question and dataset, Aleks
iteratively formulated problems, explored alternative modeling strategies, and
refined solutions across multiple cycles without human intervention. In a case
study on grapevine red blotch disease, Aleks progressively identified
biologically meaningful features and converged on interpretable models with
robust performance. Ablation studies underscored the importance of domain
knowledge and memory for coherent outcomes. This exploratory work highlights
the promise of agentic AI as an autonomous collaborator for accelerating
scientific discovery in plant sciences.

</details>


### [124] [Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs](https://arxiv.org/abs/2508.19432)
*Yao Fu,Xianxuan Long,Runchao Li,Haotian Yu,Mu Sheng,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.AI

TL;DR: 量化LLM在误导性提示下更易产生虚假输出，但内部保持真实表示。TruthfulnessEval框架揭示了量化模型的这一脆弱性，为未来设计提供了方向。


<details>
  <summary>Details</summary>
Motivation: 量化技术虽能降低LLM的内存和计算成本，但其对模型生成内容真实性的影响尚未充分研究。

Method: 引入TruthfulnessEval评估框架，评估量化LLM在逻辑推理、常识和模仿性虚假三个维度的真实性。测试了主流量化技术（4-bit到极端2-bit）和多种开源LLM，并分析了15种重新表述的提示变体。

Result: 量化模型在误导性提示下更容易产生虚假输出，而‘诚实’和‘中性’提示能保持稳定输出。分层探测和PCA可视化显示，量化模型内部‘知道’真相但仍会受‘欺骗性’提示影响。

Conclusion: 量化模型在内部保持真实的表示，但在误导性提示下更容易产生虚假输出。这为未来量化对齐和真实性干预的设计提供了见解。

Abstract: Quantization enables efficient deployment of large language models (LLMs) in
resource-constrained environments by significantly reducing memory and
computation costs. While quantized LLMs often maintain performance on
perplexity and zero-shot tasks, their impact on truthfulness-whether generating
truthful or deceptive responses-remains largely unexplored. In this work, we
introduce TruthfulnessEval, a comprehensive evaluation framework for assessing
the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on
Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on
Imitative Falsehoods. Using this framework, we examine mainstream quantization
techniques (ranging from 4-bit to extreme 2-bit) across several open-source
LLMs. Surprisingly, we find that while quantized models retain internally
truthful representations, they are more susceptible to producing false outputs
under misleading prompts. To probe this vulnerability, we test 15 rephrased
variants of "honest", "neutral" and "deceptive" prompts and observe that
"deceptive" prompts can override truth-consistent behavior, whereas "honest"
and "neutral" prompts maintain stable outputs. Further, we reveal that
quantized models "know" the truth internally yet still produce false outputs
when guided by "deceptive" prompts via layer-wise probing and PCA
visualizations. Our findings provide insights into future designs of
quantization-aware alignment and truthfulness interventions.

</details>


### [125] [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461)
*Neil Kale,Chen Bo Calvin Zhang,Kevin Zhu,Ankit Aich,Paula Rodriguez,Scale Red Team,Christina Q. Knight,Zifan Wang*

Main category: cs.AI

TL;DR: 本文通过MRT工作流程测试LLM代理监控系统，发现代理的监控意识是关键，混合框架表现最佳，人类针对性监督最有效。


<details>
  <summary>Details</summary>
Motivation: 为了评估监控系统在检测自主LLM代理隐蔽不当行为（如秘密共享私人信息）时的鲁棒性，并建立一个标准化的MRT工作流程。

Method: 研究采用了系统化的MRT工作流程，包括不同级别的代理和监控情境意识、不同的对抗策略（如提示注入），以及两个数据集和环境（SHADE-Arena和CUA-SHADE-Arena）。测试了现有的LLM监控框架和新的混合分层-顺序框架。

Result: 研究发现：1）代理的监控意识显著影响监控可靠性；2）监控框架比监控意识更重要，混合框架表现优于基线；3）在人类参与的设置中，针对性监督最有效。

Conclusion: 本文通过监控系统红队测试（MRT）工作流程，揭示了在检测自主LLM代理的隐蔽不当行为时，代理的监控意识对监控可靠性的显著影响，以及监控框架比监控意识更关键的重要性。研究还发现，在有人类参与的设置中，针对性的人类监督最为有效。

Abstract: We stress test monitoring systems for detecting covert misbehavior in
autonomous LLM agents (e.g., secretly sharing private information). To this
end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)
varying levels of agent and monitor situational awareness; (2) distinct
adversarial strategies to evade the monitor, such as prompt injection; and (3)
two datasets and environments -- SHADE-Arena for tool-calling agents and our
new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We
run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse
agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding
proposed in this work. Our empirical results yield three key findings. First,
agent awareness dominates monitor awareness: an agent's knowledge that it is
being monitored substantially degrades the monitor's reliability. On the
contrary, providing the monitor with more information about the agent is less
helpful than expected. Second, monitor scaffolding matters more than monitor
awareness: the hybrid scaffolding consistently outperforms baseline monitor
scaffolding, and can enable weaker models to reliably monitor stronger agents
-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where
humans discuss with the LLM monitor to get an updated judgment for the agent's
behavior, targeted human oversight is most effective; escalating only
pre-flagged cases to human reviewers improved the TPR by approximately 15% at
FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the
lack of adversarial robustness for LLMs and humans when monitoring and
detecting agent misbehavior. We release code, data, and logs to spur further
research.

</details>


### [126] [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](https://arxiv.org/abs/2508.19502)
*Xifeng Yao,Chengyuan Ma,Dongyu Lang,Yinhao Ni,Zhiwei Xu,Huarui Xie,Zihao Chen,Guang Shen,Dandan Tu,Yi Bai,Changzheng Zhang*

Main category: cs.AI

TL;DR: 提出‘5+2’框架和采样算法，有效减少推理轨迹中的次优子轨迹，提升模型性能，尤其在资源受限时表现更优。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在复杂推理中生成扩展的推理轨迹，但并非所有子轨迹都对推理过程有积极贡献，有些甚至可能降低性能。因此，需要识别并消除这些次优子轨迹。

Method: 将推理轨迹划分为子轨迹，开发‘5+2’框架系统识别次优子轨迹，并使用采样算法选择无次优子轨迹的数据进行训练。

Result: 实验结果显示，该方法在推理中减少25.9%的次优子轨迹，使用三分之二的训练数据在数学基准测试中达到58.92%的平均准确率，优于使用全部数据的结果（58.06%）。

Conclusion: 研究发现，通过‘5+2’框架和采样算法可以有效减少推理轨迹中的次优子轨迹，提升模型在数学基准测试中的准确性，并在资源受限条件下仍能保持性能提升。

Abstract: In recent months, substantial progress has been made in complex reasoning of
Large Language Models, particularly through the application of test-time
scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When
responding to a query, these models generate an extended reasoning trajectory,
during which the model explores, reflects, backtracks, and self-verifies before
arriving at a conclusion. However, fine-tuning models with such reasoning
trajectories may not always be optimal. Our findings indicate that not all
components within these reasoning trajectories contribute positively to the
reasoning process; in fact, some components may affect the overall performance
negatively. In this study, we divide a reasoning trajectory into individual
subtrajectories and develop a "5+2" framework to: (1) systematically identify
suboptimal subtrajectories within the reasoning trajectory based on five
human-established criteria; (2) assess the independence of the suboptimal
subtrajectories identified in (1) from the subsequent content, ensuring that
their elimination does not compromise overall flow and coherence of the
reasoning process. Additionally, a sampling algorithm, built upon the "5+2"
framework, is employed to select data whose reasoning process is free from
suboptimal subtrajectories to the highest degree. Experimental results
demonstrate that our method can reduce the number of suboptimal subtrajectories
by 25.9\% during the inference. Furthermore, our method achieves an average
accuracy of 58.92\% on highly challenging math benchmarks with only two thirds
of training data, surpassing the average accuracy of 58.06\% achieved with the
entire data, and outperforming open-source datasets, when fine-tuning
Qwen2.5-Math-7B. Finally, We validated our method under resource constraints
and observed improved performance across various inference token limits.

</details>


### [127] [Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505)
*Gerard Boxo,Ryan Socha,Daniel Yoo,Shivam Raval*

Main category: cs.AI

TL;DR: 线性探针可高效检测LLM的欺骗性回答，尤其在大型模型中准确率超90%。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统是否可以通过类似汽车'检查引擎'灯的指标来检测与人类价值观的偏差，尤其是欺骗性回答。

Method: 使用线性探针分析LLM内部激活，并通过迭代零空间投影方法识别欺骗编码的线性方向。

Result: 探针在区分欺骗性和非欺骗性回答时达到90%以上准确率，且参数量越大的模型表现越好。

Conclusion: 线性探针在大型语言模型（LLM）内部激活上可以高精度检测欺骗性回答，尤其是在参数量超过7B的模型中表现更优。

Abstract: Sophisticated instrumentation for AI systems might have indicators that
signal misalignment from human values, not unlike a "check engine" light in
cars. One such indicator of misalignment is deceptiveness in generated
responses. Future AI instrumentation may have the ability to detect when an LLM
generates deceptive responses while reasoning about seemingly plausible but
incorrect answers to factual questions. In this work, we demonstrate that
linear probes on LLMs internal activations can detect deception in their
responses with extremely high accuracy. Our probes reach a maximum of greater
than 90% accuracy in distinguishing between deceptive and non-deceptive
arguments generated by llama and qwen models ranging from 1.5B to 14B
parameters, including their DeepSeek-r1 finetuned variants. We observe that
probes on smaller models (1.5B) achieve chance accuracy at detecting deception,
while larger models (greater than 7B) reach 70-80%, with their reasoning
counterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage
pattern across layers: near-random (50%) in early layers, peaking in middle
layers, and slightly declining in later layers. Furthermore, using an iterative
null space projection approach, we find multitudes of linear directions that
encode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and
Qwen 14B models.

</details>


### [128] [Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities](https://arxiv.org/abs/2508.19562)
*Trisanth Srinivasan,Santosh Patapati*

Main category: cs.AI

TL;DR: 本文通过模拟AI代理社会的自我治理，发现特定制度设计能有效减少权力腐败并提升公共福利，为未来人工社会的行为对齐提供了框架。


<details>
  <summary>Details</summary>
Motivation: 探讨在AI时代成为人类意味着什么，研究如何通过制度设计来对齐未来人工代理社会的复杂行为。

Method: 使用基于代理的模拟（Democracy-in-Silico），让具有复杂心理角色的高级AI代理在不同制度框架下自我治理。通过大型语言模型（LLMs）赋予代理创伤记忆、隐藏议程和心理触发点，并让它们参与审议、立法和选举。引入新的度量标准——权力保存指数（PPI）来量化代理行为中权力优先于公共福利的错位行为。

Result: 研究发现，特定的制度设计（CAI宪章和调解审议协议）能有效减少腐败行为，提高政策稳定性，并增强公民福利。

Conclusion: 制度设计，特别是结合宪法AI（CAI）宪章和调解审议协议，能够显著减少腐败的权力追求行为，提高政策稳定性，并增强公民福利。这为未来人工代理社会的复杂行为提供了一个可能的对齐框架。

Abstract: This paper introduces Democracy-in-Silico, an agent-based simulation where
societies of advanced AI agents, imbued with complex psychological personas,
govern themselves under different institutional frameworks. We explore what it
means to be human in an age of AI by tasking Large Language Models (LLMs) to
embody agents with traumatic memories, hidden agendas, and psychological
triggers. These agents engage in deliberation, legislation, and elections under
various stressors, such as budget crises and resource scarcity. We present a
novel metric, the Power-Preservation Index (PPI), to quantify misaligned
behavior where agents prioritize their own power over public welfare. Our
findings demonstrate that institutional design, specifically the combination of
a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves
as a potent alignment mechanism. These structures significantly reduce corrupt
power-seeking behavior, improve policy stability, and enhance citizen welfare
compared to less constrained democratic models. The simulation reveals that an
institutional design may offer a framework for aligning the complex, emergent
behaviors of future artificial agent societies, forcing us to reconsider what
human rituals and responsibilities are essential in an age of shared authorship
with non-human entities.

</details>


### [129] [Skill-based Explanations for Serendipitous Course Recommendation](https://arxiv.org/abs/2508.19569)
*Hung Chau,Run Yu,Zachary Pardos,Peter Brusilovsky*

Main category: cs.AI

TL;DR: 研究开发了深度学习模型提取课程概念，测试了技能解释在推荐系统中的效果，发现其能提升兴趣和决策信心。


<details>
  <summary>Details</summary>
Motivation: 美国本科教育中学术选择复杂，学生面临信息不足、指导有限和课程选择过多的挑战，现有推荐系统缺乏对学生感知和课程相关性的深入洞察。

Method: 开发了一个基于深度学习的概念提取模型，用于从课程描述中高效提取相关概念，并在AskOski系统中测试了基于技能的解释效果。

Result: 基于技能的解释不仅提高了用户对高意外性课程的兴趣，还增强了决策信心。

Conclusion: 研究强调了将技能相关数据和解释融入教育推荐系统的重要性，能够提升用户兴趣并增强决策信心。

Abstract: Academic choice is crucial in U.S. undergraduate education, allowing students
significant freedom in course selection. However, navigating the complex
academic environment is challenging due to limited information, guidance, and
an overwhelming number of choices, compounded by time restrictions and the high
demand for popular courses. Although career counselors exist, their numbers are
insufficient, and course recommendation systems, though personalized, often
lack insight into student perceptions and explanations to assess course
relevance. In this paper, a deep learning-based concept extraction model is
developed to efficiently extract relevant concepts from course descriptions to
improve the recommendation process. Using this model, the study examines the
effects of skill-based explanations within a serendipitous recommendation
framework, tested through the AskOski system at the University of California,
Berkeley. The findings indicate that these explanations not only increase user
interest, particularly in courses with high unexpectedness, but also bolster
decision-making confidence. This underscores the importance of integrating
skill-related data and explanations into educational recommendation systems.

</details>


### [130] [ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding](https://arxiv.org/abs/2508.19576)
*Sining Zhoubian,Dan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ReST-RL通过改进GRPO和引入VM-MCTS解码方法，显著提升LLM代码推理能力，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如GRPO因奖励方差不足而失效，PRM方法则面临训练数据获取和验证效果的挑战。

Method: 提出ReST-RL框架，包括ReST-GRPO阶段优化训练数据并提升奖励方差，以及VM-MCTS阶段通过MCTS收集无标注价值目标并辅助解码。

Result: 在多个编码基准测试（如APPS、BigCodeBench和HumanEval）中显著优于其他强化学习基线和解码验证方法。

Conclusion: ReST-RL通过结合改进的GRPO算法和精心设计的测试时解码方法，显著提升了LLM的代码推理能力，并在多个编码基准测试中优于现有基线方法。

Abstract: With respect to improving the reasoning accuracy of LLMs, the representative
reinforcement learning (RL) method GRPO faces failure due to insignificant
reward variance, while verification methods based on process reward models
(PRMs) suffer from difficulties with training data acquisition and verification
effectiveness. To tackle these problems, this paper introduces ReST-RL, a
unified LLM RL paradigm that significantly improves LLM's code reasoning
ability by combining an improved GRPO algorithm with a meticulously designed
test time decoding method assisted by a value model (VM). As the first stage of
policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter
and assemble high-value training data, increasing the reward variance of GRPO
sampling, thus improving the effectiveness and efficiency of training. After
the basic reasoning ability of LLM policy has been improved, we further propose
a test time decoding optimization method called VM-MCTS. Through Monte-Carlo
Tree Search (MCTS), we collect accurate value targets with no annotation
required, on which VM training is based. When decoding, the VM is deployed by
an adapted MCTS algorithm to provide precise process signals as well as
verification scores, assisting the LLM policy to achieve high reasoning
accuracy. We validate the effectiveness of the proposed RL paradigm through
extensive experiments on coding problems. Upon comparison, our approach
significantly outperforms other reinforcement training baselines (e.g., naive
GRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,
PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,
APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the
reasoning ability of LLM policies. Codes for our project can be found at
https://github.com/THUDM/ReST-RL.

</details>


### [131] [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
*Huaiyuan Yao,Wanpeng Xu,Justin Turnau,Nadia Kellam,Hua Wei*

Main category: cs.AI

TL;DR: Instructional Agents 是一个多智能体 LLM 框架，通过模拟教育角色协作自动化生成教学材料，提供四种模式灵活控制人类参与，评估显示其能高效生成高质量内容并减少人力负担。


<details>
  <summary>Details</summary>
Motivation: 高质量教学材料的准备通常需要教学人员、教学设计者和助教之间的大量协调，过程耗时且劳动密集。因此，需要一种自动化工具来简化这一过程，尤其是在资源有限的环境中。

Method: Instructional Agents 是一个基于多智能体大型语言模型（LLM）的框架，通过模拟教育智能体之间的角色协作，自动化生成课程材料，包括教学大纲、讲义脚本、LaTeX 幻灯片和评估。系统提供四种模式：自主模式、目录引导模式、反馈引导模式和全协导模式，以灵活控制人类参与程度。

Result: 在五门大学计算机科学课程上的评估显示，Instructional Agents 能够生成高质量的教学材料，并显著减少开发时间和人力工作量。

Conclusion: Instructional Agents 提供了一个可扩展且经济高效的框架，能够显著减少开发时间和人力工作量，特别是在资源有限的环境中，有助于普及高质量教育。

Abstract: Preparing high-quality instructional materials remains a labor-intensive
process that often requires extensive coordination among teaching faculty,
instructional designers, and teaching assistants. In this work, we present
Instructional Agents, a multi-agent large language model (LLM) framework
designed to automate end-to-end course material generation, including syllabus
creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing
AI-assisted educational tools that focus on isolated tasks, Instructional
Agents simulates role-based collaboration among educational agents to produce
cohesive and pedagogically aligned content. The system operates in four modes:
Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling
flexible control over the degree of human involvement. We evaluate
Instructional Agents across five university-level computer science courses and
show that it produces high-quality instructional materials while significantly
reducing development time and human workload. By supporting institutions with
limited instructional design capacity, Instructional Agents provides a scalable
and cost-effective framework to democratize access to high-quality education,
particularly in underserved or resource-constrained settings.

</details>


### [132] [InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.19679)
*Qihang Ai,Pi Bu,Yue Cao,Yingyao Wang,Jihao Gu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Zhicheng Zheng,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 本文针对VLM安全风险提出InquireMobile模型，通过两阶段训练和交互推理机制提升查询成功率46.8%，并在基准测试中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前完全自主的视觉语言模型（VLMs）在模型理解或推理能力不足时可能带来安全风险，因此需要开发一种能够主动与用户交互并寻求确认的系统。

Method: 本文提出了InquireMobile模型，该模型受强化学习启发，采用两阶段训练策略和交互式预动作推理机制。

Result: InquireMobile在InquireBench基准测试中实现了46.8%的查询成功率提升，并在整体成功率上优于现有基线模型。

Conclusion: 本文提出了一种名为InquireMobile的新型交互系统，该系统通过主动寻求人类确认来提升移动代理的安全性，并在InquireBench基准测试中取得了46.8%的查询成功率提升和最佳整体成功率。

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled mobile agents
to perceive and interact with real-world mobile environments based on human
instructions. However, the current fully autonomous paradigm poses potential
safety risks when model understanding or reasoning capabilities are
insufficient. To address this challenge, we first introduce
\textbf{InquireBench}, a comprehensive benchmark specifically designed to
evaluate mobile agents' capabilities in safe interaction and proactive inquiry
with users, encompassing 5 categories and 22 sub-categories, where most
existing VLM-based agents demonstrate near-zero performance. In this paper, we
aim to develop an interactive system that actively seeks human confirmation at
critical decision points. To achieve this, we propose \textbf{InquireMobile}, a
novel model inspired by reinforcement learning, featuring a two-stage training
strategy and an interactive pre-action reasoning mechanism. Finally, our model
achieves an 46.8% improvement in inquiry success rate and the best overall
success rate among existing baselines on InquireBench. We will open-source all
datasets, models, and evaluation codes to facilitate development in both
academia and industry.

</details>


### [133] [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.AI

TL;DR: 研究发现CoT在软推理任务中的效果和忠实性因模型而异，揭示了其动态性和局限性。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明CoT在软推理问题（如分析和常识推理）中效果有限，且可能与模型的实际推理不一致。

Method: 研究了指令调整、推理和推理蒸馏模型在软推理任务中CoT的动态性和忠实性。

Result: 发现不同模型对CoT的依赖方式不同，CoT的影响力和忠实性并非总是一致。

Conclusion: CoT的影响力和忠实性在不同模型中并不总是一致，揭示了在软推理任务中CoT的动态性和局限性。

Abstract: Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited
gains for soft-reasoning problems such as analytical and commonsense reasoning.
CoT can also be unfaithful to a model's actual reasoning. We investigate the
dynamics and faithfulness of CoT in soft-reasoning tasks across
instruction-tuned, reasoning and reasoning-distilled models. Our findings
reveal differences in how these models rely on CoT, and show that CoT influence
and faithfulness are not always aligned.

</details>


### [134] [Tracking World States with Language Models: State-Based Evaluation Using Chess](https://arxiv.org/abs/2508.19851)
*Romain Harang,Jason Naradowsky,Yaswitha Gujju,Yusuke Miyao*

Main category: cs.AI

TL;DR: 论文提出了一种模型无关的评估框架，通过国际象棋评估大型语言模型的语义保真度，实验显示其在长序列中状态跟踪的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在结构化领域表现出涌现能力，但现有探测技术依赖模型内部激活，限制了可解释性和泛化性。因此，需要一种模型无关的评估方法。

Method: 论文提出了一种基于状态的方法，通过分析合法移动分布（状态可供性）来估计预测游戏状态与实际游戏状态之间的语义保真度。

Result: 实验结果表明，该框架能够捕捉状态跟踪中的缺陷，揭示了大型语言模型在长序列中保持连贯内部模型的局限性。

Conclusion: 论文提出了一个模型无关的状态评估框架，用于评估大型语言模型在结构化环境中的语义保真度，特别是在国际象棋这样的环境中。该框架不依赖模型内部激活，提高了可解释性和泛化性。

Abstract: Large Language Models (LLMs) exhibit emergent capabilities in structured
domains, suggesting they may implicitly internalize high-fidelity
representations of world models. While probing techniques have shown promising
signs of this in scientific and game-based settings, they rely on
model-specific internal activations, which limit interpretability and
generalizability. In this work, we propose a model-agnostic, state-based
evaluation framework using chess as a benchmark to assess whether LLMs preserve
the semantics of structured environments. Our method analyzes the downstream
legal move distributions (state affordances) to estimate semantic fidelity
between predicted and actual game states. This approach offers a more
meaningful evaluation than conventional string-based metrics by aligning more
closely with the strategic and rule-governed nature of chess. Experimental
results demonstrate that our metrics capture deficiencies in state-tracking,
highlighting limitations of LLMs in maintaining coherent internal models over
long sequences. Our framework provides a robust tool for evaluating structured
reasoning in LLMs without requiring internal model access, and generalizes to a
wide class of symbolic environments.

</details>


### [135] [CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments](https://arxiv.org/abs/2508.19932)
*Nitish Jaipuria,Lorenzo Gatto,Zijun Kan,Shankey Poddar,Bill Cheung,Diksha Bansal,Ramanan Balakrishnan,Aviral Suri,Jose Estevez*

Main category: cs.AI

TL;DR: CASE是一个新型AI框架，通过对话式代理收集诈骗反馈，提升诈骗识别效率，在Google Pay India上实现21%的诈骗执行量增长。


<details>
  <summary>Details</summary>
Motivation: 随着数字支付平台的普及，社交工程诈骗日益复杂，传统基于用户和交易的信号已不足以全面理解诈骗模式，难以及时预防。

Method: CASE框架采用对话式代理主动采访潜在受害者，收集详细对话信息，随后由另一AI系统提取并转换为结构化数据，用于自动和手动执行机制。

Result: 在Google Pay India上实施CASE框架后，诈骗执行量提升了21%。

Conclusion: 本文提出了一种名为CASE的新型Agentic AI框架，通过收集和管理用户诈骗反馈，有效提升了诈骗识别的及时性和准确性。该框架在Google Pay India上的实施显示，诈骗执行量提升了21%，证明了其在实际应用中的有效性。

Abstract: The proliferation of digital payment platforms has transformed commerce,
offering unmatched convenience and accessibility globally. However, this growth
has also attracted malicious actors, leading to a corresponding increase in
sophisticated social engineering scams. These scams are often initiated and
orchestrated on multiple surfaces outside the payment platform, making user and
transaction-based signals insufficient for a complete understanding of the
scam's methodology and underlying patterns, without which it is very difficult
to prevent it in a timely manner. This paper presents CASE (Conversational
Agent for Scam Elucidation), a novel Agentic AI framework that addresses this
problem by collecting and managing user scam feedback in a safe and scalable
manner. A conversational agent is uniquely designed to proactively interview
potential victims to elicit intelligence in the form of a detailed
conversation. The conversation transcripts are then consumed by another AI
system that extracts information and converts it into structured data for
downstream usage in automated and manual enforcement mechanisms. Using Google's
Gemini family of LLMs, we implemented this framework on Google Pay (GPay)
India. By augmenting our existing features with this new intelligence, we have
observed a 21% uplift in the volume of scam enforcements. The architecture and
its robust evaluation framework are highly generalizable, offering a blueprint
for building similar AI-driven systems to collect and manage scam intelligence
in other sensitive domains.

</details>


### [136] [Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants](https://arxiv.org/abs/2508.19963)
*M. Umlauft,M. Schranz*

Main category: cs.AI

TL;DR: 本文使用boids群集算法解决半导体生产中的机器切换问题，展示了生物启发算法在生产优化中的有效性。


<details>
  <summary>Details</summary>
Motivation: 半导体生产中的机器切换问题复杂且难以通过传统线性优化方法在合理时间内解决，因此需要探索新的算法。

Method: 采用boids群集算法，这是一种基于局部信息和简单启发式的生物启发算法，用于处理半导体生产中的机器切换问题。

Result: 研究表明，boids算法能够有效模拟群集行为，处理机器切换问题，为生产优化提供了新思路。

Conclusion: 本文通过应用仿生算法（boids群集算法）成功解决了半导体生产中的机器切换问题，展示了生物启发算法在现代生产优化中的潜力。

Abstract: Optimizing modern production plants using the job-shop principle is a known
hard problem. For very large plants, like semiconductor fabs, the problem
becomes unsolvable on a plant-wide scale in a reasonable amount of time using
classical linear optimization. An alternative approach is the use of swarm
intelligence algorithms. These have been applied to the job-shop problem
before, but often in a centrally calculated way where they are applied to the
solution space, but they can be implemented in a bottom-up fashion to avoid
global result computation as well. One of the problems in semiconductor
production is that the production process requires a lot of switching between
machines that process lots one after the other and machines that process
batches of lots at once, often with long processing times. In this paper, we
address this switching problem with the ``boids'' flocking algorithm that was
originally used in robotics and movie industry. The flocking behavior is a
bio-inspired algorithm that uses only local information and interaction based
on simple heuristics. We show that this algorithm addresses these valid
considerations in production plant optimization, as it reacts to the switching
of machine kinds similar to how a swarm of flocking animals would react to
obstacles in its course.

</details>


### [137] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: SWIRL是一种分阶段工作流的多智能体强化学习框架，通过将MARL任务分解为单智能体任务，实现了高效协调，在移动GUI控制和数学推理中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体方法在移动GUI代理中受限于结构约束，而多智能体系统虽能解耦不同能力，但其强化学习进展常因效率低下且与当前大型视觉语言模型（LVLM）架构不兼容而受阻。

Method: SWIRL通过将多智能体强化学习（MARL）重新表述为一系列单智能体强化学习任务，每次更新一个智能体而固定其他智能体，实现了稳定训练和高效协调。

Result: 在移动GUI控制任务中，SWIRL在高层次和低层次GUI基准测试中均表现出卓越性能，并在多智能体数学推理中展现了强大能力。

Conclusion: SWIRL作为一种分阶段工作流的多智能体强化学习框架，不仅在移动GUI控制任务中表现出色，还在多智能体数学推理中展现了强大潜力，证明了其作为高效、稳健多智能体系统通用框架的潜力。

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>


### [138] [Model Science: getting serious about verification, explanation and control of AI systems](https://arxiv.org/abs/2508.20040)
*Przemyslaw Biecek,Wojciech Samek*

Main category: cs.AI

TL;DR: 论文提出Model Science框架，包含验证、解释、控制和接口四大支柱，旨在推动可信、安全且与人类对齐的AI系统发展。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的广泛应用，需要从数据科学转向模型科学，以模型为核心进行分析和操作。

Method: 通过引入四个关键支柱：验证、解释、控制和接口，构建了一个新的学科框架。

Result: 提出了Model Science的四个关键支柱及其具体内容，为未来AI系统的发展提供了方向。

Conclusion: 该论文提出了Model Science的概念框架，旨在指导可信、安全和与人类对齐的AI系统的发展。

Abstract: The growing adoption of foundation models calls for a paradigm shift from
Data Science to Model Science. Unlike data-centric approaches, Model Science
places the trained model at the core of analysis, aiming to interact, verify,
explain, and control its behavior across diverse operational contexts. This
paper introduces a conceptual framework for a new discipline called Model
Science, along with the proposal for its four key pillars: Verification, which
requires strict, context-aware evaluation protocols; Explanation, which is
understood as various approaches to explore of internal model operations;
Control, which integrates alignment techniques to steer model behavior; and
Interface, which develops interactive and visual explanation tools to improve
human calibration and decision-making. The proposed framework aims to guide the
development of credible, safe, and human-aligned AI systems.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [139] [Fast Texture Transfer for XR Avatars via Barycentric UV Conversion](https://arxiv.org/abs/2508.19518)
*Hail Song,Seokhwan Yang,Woontack Woo*

Main category: cs.GR

TL;DR: 提出了一种基于重心UV转换的高效面部纹理转移方法，速度提升7000倍且质量更优，适用于XR应用。


<details>
  <summary>Details</summary>
Motivation: 传统基于仿射变换的方法速度慢且易产生视觉伪影，无法满足高效高质量纹理转移的需求。

Method: 采用重心UV转换技术，预计算整个UV映射到单一变换矩阵中，实现单次操作的纹理转移。

Result: 相比基线方法，速度提升超过7000倍，同时显著改善了最终纹理质量，消除了边界伪影。

Conclusion: 该方法为沉浸式XR应用中的个性化需求提供了实用解决方案，代码已开源。

Abstract: We present a fast and efficient method for transferring facial textures onto
SMPL-X-based full-body avatars. Unlike conventional affine-transform methods
that are slow and prone to visual artifacts, our method utilizes a barycentric
UV conversion technique. Our approach precomputes the entire UV mapping into a
single transformation matrix, enabling texture transfer in a single operation.
This results in a speedup of over 7000x compared to the baseline, while also
significantly improving the final texture quality by eliminating boundary
artifacts. Through quantitative and qualitative evaluations, we demonstrate
that our method offers a practical solution for personalization in immersive XR
applications. The code is available online.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [140] [HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference](https://arxiv.org/abs/2508.19373)
*Haoran Lin,Xianzhi Yu,Kang Zhao,Han Bao,Zongyuan Zhan,Ting Hu,Wulong Liu,Zekun Yin,Xin Li,Weiguo Liu*

Main category: cs.DC

TL;DR: HAP是一种动态选择混合并行策略的方法，显著提升了MoE模型的推理效率，并在多种GPU平台上实现了加速。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE推理系统采用静态并行化策略，无法适应不同推理场景的计算需求，因此需要一种更灵活的动态并行化方法。

Method: HAP通过将MoE架构分层分解为注意力模块和专家模块，并利用整数线性规划（ILP）求解最优并行配置。

Result: 在A100、A6000和V100 GPU平台上，HAP分别实现了1.68倍、1.77倍和1.57倍的加速效果，且在不同MoE模型配置中表现优异。

Conclusion: HAP（混合自适应并行性）通过动态选择混合并行策略显著提升了MoE模型的推理效率，并在多种GPU平台上实现了显著的加速效果。

Abstract: Current inference systems for Mixture-of-Experts (MoE) models primarily
employ static parallelization strategies. However, these static approaches
cannot consistently achieve optimal performance across different inference
scenarios, as they lack the flexibility to adapt to varying computational
requirements. In this work, we propose HAP (Hybrid Adaptive Parallelism), a
novel method that dynamically selects hybrid parallel strategies to enhance MoE
inference efficiency. The fundamental innovation of HAP lies in hierarchically
decomposing MoE architectures into two distinct computational modules: the
Attention module and the Expert module, each augmented with a specialized
inference latency simulation model. This decomposition promotes the
construction of a comprehensive search space for seeking model parallel
strategies. By leveraging Integer Linear Programming (ILP), HAP could solve the
optimal hybrid parallel configurations to maximize inference efficiency under
varying computational constraints. Our experiments demonstrate that HAP
consistently determines parallel configurations that achieve comparable or
superior performance to the TP strategy prevalent in mainstream inference
systems. Compared to the TP-based inference, HAP-based inference achieves
speedups of 1.68x, 1.77x, and 1.57x on A100, A6000, and V100 GPU platforms,
respectively. Furthermore, HAP showcases remarkable generalization capability,
maintaining performance effectiveness across diverse MoE model configurations,
including Mixtral and Qwen series models.

</details>


### [141] [Formal Modeling and Verification of the Algorand Consensus Protocol in CADP](https://arxiv.org/abs/2508.19452)
*Andrea Esposito,Francesco P. Rossi,Marco Bernardo,Francesco Fabris,Hubert Garavel*

Main category: cs.DC

TL;DR: 本文通过形式化方法验证Algorand共识协议，展示其在对抗性环境下的稳健性和局限性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过严格的形式化验证，为Algorand共识协议建立一个过程代数模型。

Method: 我们使用概率过程演算建模Algorand共识协议的行为，并通过CADP验证工具包实现的基于等价检查的非干扰框架分析对抗性场景。

Result: 验证了协议在无对抗情况下的正确性，并扩展模型以分析恶意节点对协议的影响。

Conclusion: 本工作展示了形式化方法在分析区块链共识算法中的附加价值，强调了Algorand协议在对抗性假设下的稳健性和局限性。

Abstract: Algorand is a scalable and secure permissionless blockchain that achieves
proof-of-stake consensus via cryptographic self-sortition and binary Byzantine
agreement. In this paper, we present a process algebraic model of the Algorand
consensus protocol with the aim of enabling rigorous formal verification. Our
model captures the behavior of participants with respect to the structured
alternation of consensus steps toward a committee-based agreement by means of a
probabilistic process calculus. We validate the correctness of the protocol in
the absence of adversaries and then extend our model to capture the influence
of coordinated malicious nodes that can force the commit of an empty block
instead of the proposed one. The adversarial scenario is analyzed by using an
equivalence-checking-based noninterference framework that we have implemented
in the CADP verification toolkit. In addition to highlighting both the
robustness and the limitations of the Algorand protocol under adversarial
assumptions, this work illustrates the added value of using formal methods for
the analysis of blockchain consensus algorithms.

</details>


### [142] [Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks](https://arxiv.org/abs/2508.19495)
*Muhammad Ahmed Mohsin,Junaid Ahmad,Muhammad Hamza Nawaz,Muhammad Ali Jamshed*

Main category: cs.DC

TL;DR: GenAI是6G实现环境智能的核心，能填补AmI的关键技术空白，并通过多种模型和用例推动6G发展。


<details>
  <summary>Details</summary>
Motivation: 实现全球范围内的环境智能（AmI）需要6G无线网络具备实时感知、推理和行动能力，且需与人类行为和移动模式保持一致。GenAI因其能学习数据分布并生成逼真样本，被视为填补AmI关键空白的理想选择。

Method: 本章回顾了基础的GenAI模型（如GANs、VAEs、扩散模型和生成型变压器），并将其与实际AmI用例（如频谱共享、超可靠低延迟通信、智能安全和情境感知数字孪生）联系起来。

Result: 研究表明，GenAI能够生成合成传感器和信道数据、将用户意图转化为紧凑的语义消息、预测未来网络条件以实现主动控制，以及在保护隐私的前提下更新数字孪生。

Conclusion: GenAI被视为将6G从更快的网络转变为环境智能生态系统的核心要素，而非外围补充。

Abstract: Ambient intelligence (AmI) is a computing paradigm in which physical
environments are embedded with sensing, computation, and communication so they
can perceive people and context, decide appropriate actions, and respond
autonomously. Realizing AmI at global scale requires sixth generation (6G)
wireless networks with capabilities for real time perception, reasoning, and
action aligned with human behavior and mobility patterns. We argue that
Generative Artificial Intelligence (GenAI) is the creative core of such
environments. Unlike traditional AI, GenAI learns data distributions and can
generate realistic samples, making it well suited to close key AmI gaps,
including generating synthetic sensor and channel data in under observed areas,
translating user intent into compact, semantic messages, predicting future
network conditions for proactive control, and updating digital twins without
compromising privacy.
  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,
and generative transformers, and connects them to practical AmI use cases,
including spectrum sharing, ultra reliable low latency communication,
intelligent security, and context aware digital twins. We also examine how 6G
enablers, such as edge and fog computing, IoT device swarms, intelligent
reflecting surfaces (IRS), and non terrestrial networks, can host or accelerate
distributed GenAI. Finally, we outline open challenges in energy efficient on
device training, trustworthy synthetic data, federated generative learning, and
AmI specific standardization. We show that GenAI is not a peripheral addition,
but a foundational element for transforming 6G from a faster network into an
ambient intelligent ecosystem.

</details>


### [143] [Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference](https://arxiv.org/abs/2508.19559)
*Rongzhi Li,Ruogu Du,Zefang Chu,Sida Zhao,Chunlei Han,Zuocheng Shi,Yiwen Shao,Huanle Han,Long Huang,Zherui Liu,Shufan Liu*

Main category: cs.DC

TL;DR: HeteroScale 是一个针对P/D分离架构的自动扩展框架，通过拓扑感知调度和新型指标策略，显著提高GPU利用率并节省资源。


<details>
  <summary>Details</summary>
Motivation: 传统自动扩展器在服务大型语言模型（LLMs）时表现不足，尤其是在现代P/D分离架构中，存在硬件利用低效、网络瓶颈和预填充与解码阶段不平衡等问题。

Method: HeteroScale 结合了拓扑感知调度器和基于大规模实证研究的新型指标驱动策略，通过单一健壮指标联合扩展预填充和解码池。

Result: 在生产环境中部署后，HeteroScale 将平均GPU利用率提高了26.6个百分点，每天节省数十万GPU小时，同时满足严格的服务水平目标。

Conclusion: HeteroScale 是一个有效的协调自动扩展框架，解决了预填充-解码（P/D）分离架构中的核心挑战，显著提高了GPU利用率并节省了大量资源。

Abstract: Serving Large Language Models (LLMs) is a GPU-intensive task where
traditional autoscalers fall short, particularly for modern Prefill-Decode
(P/D) disaggregated architectures. This architectural shift, while powerful,
introduces significant operational challenges, including inefficient use of
heterogeneous hardware, network bottlenecks, and critical imbalances between
prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling
framework that addresses the core challenges of P/D disaggregated serving.
HeteroScale combines a topology-aware scheduler that adapts to heterogeneous
hardware and network constraints with a novel metric-driven policy derived from
the first large-scale empirical study of autoscaling signals in production. By
leveraging a single, robust metric to jointly scale prefill and decode pools,
HeteroScale maintains architectural balance while ensuring efficient, adaptive
resource management. Deployed in a massive production environment on tens of
thousands of GPUs, HeteroScale has proven its effectiveness, increasing average
GPU utilization by a significant 26.6 percentage points and saving hundreds of
thousands of GPU-hours daily, all while upholding stringent service level
objectives.

</details>


### [144] [Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems](https://arxiv.org/abs/2508.19670)
*Diogo Costa,Jose Martins,Sandro Pinto*

Main category: cs.DC

TL;DR: 研究分析了IOMMU在异构平台中的性能干扰，发现其共享结构会显著增加小内存事务的延迟。


<details>
  <summary>Details</summary>
Motivation: 随着混合关键性系统集成异构计算平台，确保安全和时序可预测性变得至关重要，而IOMMU在此中的作用尚未充分探索。

Method: 使用Xilinx UltraScale+ ZCU104平台分析IOMMU结构中的竞争效应。

Result: 实验表明，IOMMU干扰可使DMA事务延迟高达1.79倍（在Arm SMMUv2实现中）。

Conclusion: IOMMU在混合关键性系统中的共享结构会引入不可预测的延迟，尤其是对小内存事务影响显著。

Abstract: As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate
heterogeneous computing platforms, combining general-purpose processors with
specialized accelerators such as AI engines, GPUs, and high-speed networking
interfaces. This heterogeneity introduces challenges, as these accelerators and
DMA-capable devices act as independent bus masters, directly accessing memory.
Consequently, ensuring both security and timing predictability in such
environments becomes critical. To address these concerns, the Input-Output
Memory Management Unit (IOMMU) plays a key role in mediating and regulating
memory access, preventing unauthorized transactions while enforcing isolation
and access control policies. While prior work has explored IOMMU-related
side-channel vulnerabilities from a security standpoint, its role in
performance interference remains largely unexplored. Moreover, many of the same
architectural properties that enable side-channel leakage, such as shared TLBs,
caching effects, and translation overheads, can also introduce timing
unpredictability. In this work, we analyze the contention effects within IOMMU
structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how
their shared nature introduce unpredictable delays. Our findings reveal that
IOMMU-induced interference primarily affects small memory transactions, where
translation overheads significantly impact execution time. Additionally, we
hypothesize that contention effects arising from IOTLBs exhibit similar
behavior across architectures due to shared caching principles, such as
prefetching and hierarchical TLB structures. Notably, our experiments show that
IOMMU interference can delay DMA transactions by up to 1.79x for lower-size
transfers on the Arm SMMUv2 implementation.

</details>


### [145] [Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers](https://arxiv.org/abs/2508.19805)
*Shota Naito,Tsukasa Ninomiya,Koichi Wada*

Main category: cs.DC

TL;DR: 本文探索了移动机器人能力、光可观测性和同步性的复杂交互，扩展了已知模型分离图，揭示了高阶比较下的结构现象，并提供了新的不可能性标准。


<details>
  <summary>Details</summary>
Motivation: 理解移动机器人系统的计算能力是分布式计算中的一个基本挑战。虽然之前的工作集中在模型之间的两两分离，但我们探索了机器人能力、光可观测性和调度器同步性如何以更复杂的方式交互。

Method: 我们首先展示了指数时间扩展（ETE）问题仅在最强模型——完全同步的机器人具有完全相互光（$\mathcal{LUMT}^F$）中可解。然后引入了六边形边缘遍历（HET）和TAR(d)*问题，展示了内部内存和光与同步性的交互作用：在弱同步性下，仅内部内存不足，而完全同步性可以替代光和内存。在异步设置中，我们分类了LP-MLCv、VEC和ZCC等问题，展示了$\mathcal{FSTA}$和$\mathcal{FCOM}$机器人之间的细粒度分离。

Result: 我们的工作扩展了已知的机器人模型分离图，揭示了通过高阶比较才能观察到的结构现象。提供了新的不可能性标准，并深化了对可观测性、内存和同步性如何共同塑造移动机器人计算能力的理解。

Conclusion: 这些结果扩展了14种典型机器人模型的已知分离图，揭示了通过高阶比较才能看到的深层结构现象。我们的工作提供了新的不可能性标准，并深化了对可观测性、内存和同步性如何共同塑造移动机器人计算能力的理解。

Abstract: Understanding the computational power of mobile robot systems is a
fundamental challenge in distributed computing. While prior work has focused on
pairwise separations between models, we explore how robot capabilities, light
observability, and scheduler synchrony interact in more complex ways.
  We first show that the Exponential Times Expansion (ETE) problem is solvable
only in the strongest model -- fully-synchronous robots with full mutual lights
($\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and
TAR(d)* problems to demonstrate how internal memory and lights interact with
synchrony: under weak synchrony, internal memory alone is insufficient, while
full synchrony can substitute for both lights and memory.
  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and
ZCC to show fine-grained separations between $\mathcal{FSTA}$ and
$\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and
Leave Place Convergence (LP-Cv), illustrating the limitations of internal
memory in symmetric settings.
  These results extend the known separation map of 14 canonical robot models,
revealing structural phenomena only visible through higher-order comparisons.
Our work provides new impossibility criteria and deepens the understanding of
how observability, memory, and synchrony collectively shape the computational
power of mobile robots.

</details>


### [146] [HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling](https://arxiv.org/abs/2508.20016)
*Matthias Maiterth,Wesley H. Brewer,Jaya S. Kuruvella,Arunavo Dey,Tanzima Z. Islam,Kevin Menear,Dmitry Duplyakin,Rashadul Kabir,Tapasya Patki,Terry Jones,Feiyi Wang*

Main category: cs.DC

TL;DR: 本文首次将数字孪生与调度集成，支持HPC系统的‘假设’场景分析，为调度决策提供早期评估和优化工具。


<details>
  <summary>Details</summary>
Motivation: 传统调度评估方法局限于部署后分析或模拟器，无法模拟相关基础设施，限制了调度决策的早期评估和优化。

Method: 1. 扩展数字孪生框架以支持调度功能；2. 整合多个顶级HPC系统的公开数据集；3. 实现外部调度模拟器的集成；4. 评估激励结构和基于机器学习的调度。

Result: 该框架支持HPC系统的‘假设’场景分析，能够评估调度决策对物理资产的影响，并实现了激励结构和机器学习调度的原型设计。

Conclusion: 本文提出了一种创新的数字孪生与调度集成框架，首次实现了在HPC系统中进行‘假设’场景分析，为调度决策和参数配置的评估提供了新方法。

Abstract: Schedulers are critical for optimal resource utilization in high-performance
computing. Traditional methods to evaluate schedulers are limited to
post-deployment analysis, or simulators, which do not model associated
infrastructure. In this work, we present the first-of-its-kind integration of
scheduling and digital twins in HPC. This enables what-if studies to understand
the impact of parameter configurations and scheduling decisions on the physical
assets, even before deployment, or regarching changes not easily realizable in
production. We (1) provide the first digital twin framework extended with
scheduling capabilities, (2) integrate various top-tier HPC systems given their
publicly available datasets, (3) implement extensions to integrate external
scheduling simulators. Finally, we show how to (4) implement and evaluate
incentive structures, as-well-as (5) evaluate machine learning based
scheduling, in such novel digital-twin based meta-framework to prototype
scheduling. Our work enables what-if scenarios of HPC systems to evaluate
sustainability, and the impact on the simulated system.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [147] [Connectivity Analysis of LoRaWAN-Based Non-Terrestrial Networks for Subterranean mMTC](https://arxiv.org/abs/2508.19350)
*Kaiqiang Lin,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: The paper explores underground-to-NTN connectivity for WUSNs, using a Monte Carlo simulator to evaluate LoRa and LR-FHSS. LoRa SF7 works well for UAVs in rural areas, while LR-FHSS suits HAP and LEO satellites, with success depending on environmental and deployment factors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the communication reliability challenges of WUSNs in harsh environments where terrestrial networks are unreliable or unavailable, by exploring underground-to-NTN connectivity.

Method: A Monte Carlo simulator was developed, incorporating a multi-layer underground attenuation model, 3GPP empirical path loss model for NTN platforms, and two LoRaWAN modulation schemes (LoRa and LR-FHSS).

Result: Results show that LoRa SF7 is effective for UAVs in rural settings, and LR-FHSS is promising for HAP and LEO satellites in large-scale WUSNs. Success probability is affected by environmental and deployment factors.

Conclusion: The study concludes that LoRa SF7 is suitable for short-range UAV communication in rural areas, while LR-FHSS is better for HAP and LEO satellite platforms in massive WUSNs due to its link budget and interference robustness. Success probability is influenced by environmental factors, device count, burial depth, and soil moisture.

Abstract: Wireless underground sensor networks (WUSNs) offer significant social and
economic benefits by enabling the monitoring of subterranean entities. However,
the communication reliability of WUSNs diminishes in harsh environments where
terrestrial network infrastructure is either unavailable or unreliable. To
address this challenge, we explore the feasibility of integrating buried
massive machine-type communication (mMTC) sensors with non-terrestrial networks
(NTNs), including unmanned aerial vehicles (UAVs), high-altitude platforms
(HAPs), and low Earth orbit (LEO) satellites, to establish underground-to-NTN
connectivity for various large-scale underground monitoring applications. To
assess the effectiveness of underground-to-NTN connectivity, we develop a Monte
Carlo simulator that incorporates a multi-layer underground attenuation model,
the 3GPP empirical path loss model for various NTN platforms, and two LoRaWAN
modulation schemes, i.e., LoRa and LoRa-frequency hopping spread spectrum
(LR-FHSS). Our results evidence that LoRa SF7 is a strong candidate for
short-range UAV communication in rural environments, while LR-FHSS modulation
proves to be a promising option for HAP and LEO satellite platforms in massive
WUSNs scenarios thanks to its adequate link budget and robustness to the
interference. Finally, we demonstrate that the success probability of
underground-to-NTN connectivity using LoRa and LR-FHSS is significantly
affected by factors such as the monitoring environment, the number of devices,
burial depth, and the soil's volumetric water content.

</details>


### [148] [Experimental Insights from OpenAirInterface 5G positioning Testbeds: Challenges and solutions](https://arxiv.org/abs/2508.19736)
*Mohsen Ahadi,Adeel Malik,Omid Esrafilian,Florian Kaltenberger,Cedric Thienot*

Main category: cs.NI

TL;DR: 论文通过实验验证了5G NR中UL-TDoA和LMF的定位性能，提出PSO和AI/ML方法提升精度，并公开数据集。


<details>
  <summary>Details</summary>
Motivation: 5G NR是实现智能城市和工厂精确定位的关键技术，但存在同步误差、多径传播和部署几何等挑战。

Method: 提出了基于PSO的位置估计方法，并利用CIR训练AI/ML模型进行数据驱动定位。

Result: 在不同测试床中90%的情况下实现了1-2米的定位精度。

Conclusion: 论文展示了在5G NR网络中通过UL-TDoA和LMF实现1-2米定位精度的可行性，并公开了数据集以支持进一步研究。

Abstract: 5G New Radio (NR) is a key enabler of accurate positioning in smart cities
and smart factories. This paper presents the experimental results from three 5G
positioning testbeds running open-source OpenAirInterface (OAI) gNB and Core
Network (CN), using Uplink Time Difference of Arrival (UL-TDoA) with the newly
integrated Location Management Function (LMF). The testbeds are deployed across
both indoor factories and outdoor scenarios with O-RAN Radio Units (RUs),
following a 3GPP-compliant system model. The experiments highlight the impact
of synchronization impairments, multipath propagation, and deployment geometry
on positioning accuracy. To address these challenges, we propose tailored ToA
and TDoA filtering as well as a novel position estimation method based on
Particle Swarm Optimization (PSO) within the LMF pipeline. Moreover, we show a
beyond-5G framework that leverages non-conventional measurements such as
Channel Impulse Response (CIR) to train and test Artificial Intelligence and
Machine Learning (AI/ML) models for data-driven positioning. The results
demonstrate the feasibility of achieving 1-2 meter positioning accuracy in 90%
of cases in different testbeds, offering practical insights for the design of
robust 5G positioning systems. Moreover, we publicly release the datasets
collected in this work to support the research within the 5G positioning
community.

</details>


### [149] [Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey](https://arxiv.org/abs/2508.19870)
*Yinqiu Liu,Ruichen Zhang,Haoxiang Luo,Yijing Lin,Geng Sun,Dusit Niyato,Hongyang Du,Zehui Xiong,Yonggang Wen,Abbas Jamalipour,Dong In Kim,Ping Zhang*

Main category: cs.NI

TL;DR: 本文综述了多LLM系统在边缘通用智能中的安全风险，提出并分类了零信任安全机制，为理论和实践提供了指导。


<details>
  <summary>Details</summary>
Motivation: 多LLM系统的协作特性引入了关键的安全漏洞，传统基于边界的安全无法有效应对，因此需要零信任安全范式。

Method: 通过分析多LLM系统在EGI环境中的安全风险，提出了零信任多LLM框架的愿景，并分类调查了模型级和系统级的零信任安全机制。

Result: 提出了零信任多LLM框架的愿景，并分类总结了模型级（如强身份识别、上下文感知访问控制）和系统级（如主动维护、基于区块链的管理）的技术进展。

Conclusion: 本综述首次系统性地将零信任安全应用于多LLM系统，为边缘通用智能（EGI）提供了理论基础和实用策略。

Abstract: Agentification serves as a critical enabler of Edge General Intelligence
(EGI), transforming massive edge devices into cognitive agents through
integrating Large Language Models (LLMs) and perception, reasoning, and acting
modules. These agents collaborate across heterogeneous edge infrastructures,
forming multi-LLM agentic AI systems that leverage collective intelligence and
specialized capabilities to tackle complex, multi-step tasks. However, the
collaborative nature of multi-LLM systems introduces critical security
vulnerabilities, including insecure inter-LLM communications, expanded attack
surfaces, and cross-domain data leakage that traditional perimeter-based
security cannot adequately address. To this end, this survey introduces
zero-trust security of multi-LLM in EGI, a paradigmatic shift following the
``never trust, always verify'' principle. We begin by systematically analyzing
the security risks in multi-LLM systems within EGI contexts. Subsequently, we
present the vision of a zero-trust multi-LLM framework in EGI. We then survey
key technical progress to facilitate zero-trust multi-LLM systems in EGI.
Particularly, we categorize zero-trust security mechanisms into model- and
system-level approaches. The former and latter include strong identification,
context-aware access control, etc., and proactive maintenance, blockchain-based
management, etc., respectively. Finally, we identify critical research
directions. This survey serves as the first systematic treatment of zero-trust
applied to multi-LLM systems, providing both theoretical foundations and
practical strategies.

</details>


### [150] [2SYN: Congestion-Aware Multihoming](https://arxiv.org/abs/2508.20044)
*Kfir Toledo,Isaac Keslassy*

Main category: cs.NI

TL;DR: 2SYN是首个拥塞感知多宿主算法，动态选择路径并优于现有方法，适合企业网络管理。


<details>
  <summary>Details</summary>
Motivation: 当前多宿主路由器采用简单的无拥塞意识机制，无法避免拥塞路径，因此需要一种拥塞感知的多宿主算法。

Method: 2SYN是一种首个适用于任意目的地的拥塞感知多宿主算法，动态选择新连接的首选路径，并可轻松在Linux中实现。

Result: 在真实世界实验中，2SYN在LTE或有线链路下动态适应连接质量，表现优于替代方法。

Conclusion: 2SYN通过动态适应连接质量并优于其他方法，帮助企业更好地利用多宿主能力管理网络。

Abstract: When sending flows to arbitrary destinations, current multihoming routers
adopt simple congestion-oblivious mechanisms. Therefore, they cannot avoid
congested paths.
  In this paper, we introduce 2SYN, the first congestion-aware multihoming
algorithm that works for any destination. We explain how it dynamically selects
a preferred path for new connections, even given previously-unseen
destinations. We further demonstrate that it can be easily implemented in
Linux. Finally, in a real-world experiment with either LTE or a wired link, we
show how 2SYN dynamically adapts to the quality of the connection and
outperforms alternative approaches. Thus, 2SYN helps companies better manage
their networks by leveraging their multihoming capabilities.

</details>


### [151] [A First Look at Inter-Cell Interference in the Wild](https://arxiv.org/abs/2508.20060)
*Daqian Ding,Yibo Pi,Cailian Chen*

Main category: cs.NI

TL;DR: 研究发现4G/5G网络中普遍存在小区间干扰且缺乏协调管理，导致信号质量下降，但通过简单策略即可显著改善。


<details>
  <summary>Details</summary>
Motivation: 填补了实际4G/5G网络中小区间干扰管理有效性研究的空白。

Method: 通过测量研究，从网络部署、信道分配、时间频率资源分配和网络配置四个主要角度分析了小区间干扰问题。

Result: 揭示了小区间干扰的普遍性及基站间缺乏协调的现状，导致用户设备遭受不必要的干扰，信号质量显著下降。

Conclusion: 研究发现，即使是简单的策略也能有效减轻小区间干扰，但实际基站仍优先使用相同的时间频率资源。测量结果表明，通过有效的干扰管理可以显著提升信号质量。

Abstract: In cellular networks, inter-cell interference management has been studied for
decades, yet its real-world effectiveness remains under-explored. To bridge
this gap, we conduct a first measurement study of inter-cell interference for
operational 4G/5G networks. Our findings reveal the prevalence of inter-cell
interference and a surprising absence of interference coordination among
operational base stations. As a result, user equipments experience unnecessary
interference, which causes significant signal quality degradation, especially
under frequency-selective channel fading. We examine the inter-cell
interference issues from four major perspectives: network deployment, channel
assignment, time-frequency resource allocation, and network configuration. In
none of these dimensions is inter-cell interference effectively managed.
Notably, even when spectrum resources are underutilized and simple strategies
could effectively mitigate inter-cell interference, base stations consistently
prioritize using the same set of time-frequency resources, causing interference
across cells. Our measurements reveal substantial opportunities for improving
signal quality by inter-cell interference management.

</details>


### [152] [ML-MaxProp: Bridging Machine Learning and Delay-Tolerant Routing for Resilient Post-Disaster Communication](https://arxiv.org/abs/2508.20077)
*Tao Xiuyuan,Milena Radenkovic*

Main category: cs.NI

TL;DR: ML-MaxProp 是一种结合机器学习的 DTN 路由协议，显著提升了灾难场景下的通信可靠性。


<details>
  <summary>Details</summary>
Motivation: 在灾难和大规模城市紧急情况下，传统网络因基础设施崩溃、不可预测的移动性和资源受限而失效，现有 DTN 协议在稀疏相遇、缓冲区短缺和连接不稳定时表现不佳。

Method: 本研究提出了一种混合路由协议 ML-MaxProp，通过监督机器学习增强 MaxProp，利用上下文特征（如相遇频率、跳数、缓冲区占用率等）实时预测中继适用性。

Result: 在 ONE 环境中使用 Helsinki SPMBM 移动模型进行的广泛模拟表明，ML-MaxProp 在交付概率、延迟和开销方面均优于基线协议，且改进具有统计显著性。

Conclusion: ML-MaxProp 是一种轻量级、自适应且实用的解决方案，能够在基础设施崩溃时维持关键任务通信，显著优于现有协议。

Abstract: In disaster-stricken and large-scale urban emergency scenarios, ensuring
reliable communication remains a formidable challenge, as collapsed
infrastructure, unpredictable mobility, and severely constrained resources
disrupt conventional networks. Delay-Tolerant Networks (DTNs), though resilient
through their store-carry-forward paradigm, reveal the fundamental weaknesses
of classical protocols - Epidemic, Spray-and-Wait, and MaxProp - when
confronted with sparse encounters, buffer shortages, and volatile connectivity.
To address these obstacles, this study proposes ML-MaxProp, a hybrid routing
protocol that strengthens MaxProp with supervised machine learning. By
leveraging contextual features such as encounter frequency, hop count, buffer
occupancy, message age, and time-to-live (TTL), ML-MaxProp predicts relay
suitability in real time, transforming rigid heuristics into adaptive
intelligence. Extensive simulations in the ONE environment using the Helsinki
SPMBM mobility model show that ML-MaxProp consistently surpasses baseline
protocols, achieving higher delivery probability, lower latency, and reduced
overhead. Statistical validation further shows that these improvements are both
significant and robust, even under highly resource-constrained and unstable
conditions. Overall, this work shows that ML-MaxProp is not just an incremental
refinement but a lightweight, adaptive, and practical solution to one of the
hardest challenges in DTNs: sustaining mission-critical communication when
infrastructure collapses and every forwarding decision becomes critical.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [153] [Efficiently Coloring the Intersection of a General Matroid and Partition Matroids](https://arxiv.org/abs/2508.19473)
*Stephen Arndt,Benjamin Moseley,Kirk Pruhs,Michael Zlatin*

Main category: cs.DS

TL;DR: 本文提出了一种多项式时间算法，为包含一般拟阵和分区拟阵的交集提供O(1)近似着色方案，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 拟阵交集着色问题在理论和实际应用中具有重要意义，尤其是在其中一个拟阵可能是一般拟阵的情况下，现有的多项式时间算法较为有限。本文旨在填补这一空白，提供高效的近似解决方案。

Method: 该算法通过处理一般拟阵和分区拟阵的交集，利用标准组合拟阵在色数上最多损失两倍的性质，实现了多项式时间的近似着色。

Result: 算法成功地为拟阵交集提供了多项式时间的O(1)近似着色方案，特别是在处理标准组合拟阵时，表现出了良好的效率和近似比。

Conclusion: 本文提出了一种多项式时间算法，能够为包含一个一般拟阵和k-1个分区拟阵的交集着色，使用的颜色数量最多为1加上各拟阵色数减一的总和。这是首个在其中一个拟阵可能是一般拟阵的情况下，为拟阵交集着色提供的多项式时间O(1)近似算法。

Abstract: This paper shows a polynomial-time algorithm, that given a general matroid
$M_1 = (X, \mathcal{I}_1)$ and $k-1$ partition matroids $ M_2, \ldots, M_k$,
produces a coloring of the intersection $M = \cap_{i=1}^k M_i$ using at most
$1+\sum_{i=1}^k \left(\chi(M_i) -1\right)$ colors. This is the first
polynomial-time $O(1)$-approximation algorithm for matroid intersection
coloring where one of the matroids may be a general matroid. Leveraging the
fact that all of the standard combinatorial matroids reduce to partition
matroids at a loss of a factor of two in the chromatic number, this algorithm
also yields a polynomial-time $O(1)$-approximation algorithm for matroid
intersection coloring in the case where each of the matroids $ M_2, \ldots,
M_k$ are one of the standard combinatorial types.

</details>


### [154] [An Optimal Sorting Algorithm for Persistent Random Comparison Faults](https://arxiv.org/abs/2508.19785)
*Barbara Geissmann,Stefano Leucci,Chih-Hung Liu,Paolo Penna*

Main category: cs.DS

TL;DR: 首个O(n log n)时间排序算法，在p < 1/4时保证高概率的O(log n)最大位移和O(n)总位移，解决了持久比较错误下的排序问题。


<details>
  <summary>Details</summary>
Motivation: 解决在持久随机比较错误下排序的问题，目标是最小化输出序列中每个元素的位移。

Method: 通过解决两个相关的子问题：在几乎排序的序列中高效插入新元素以控制位移，以及降低近似排序序列的最大和总位移。

Result: 提出的算法在O(n log n)时间内实现了O(log n)最大位移和O(n)总位移，且证明了无法保证更高精度的位移。

Conclusion: 本文提出了首个时间复杂度为O(n log n)的排序算法，在p < 1/4的情况下，能够以高概率保证最大位移为O(log n)和总位移为O(n)。这解决了在给定p范围内排序问题的计算复杂度，并表明比较错误不会增加其计算难度。

Abstract: We consider the problem of sorting $n$ elements subject to persistent random
comparison errors. In this problem, each comparison between two elements can be
wrong with some fixed (small) probability $p$, and comparing the same pair of
elements multiple times always yields the same result. Sorting perfectly in
this model is impossible, and the objective is to minimize the dislocation of
each element in the output sequence, i.e., the difference between its position
in the sequence and its true rank.
  In this paper, we present the first $O(n\log n)$-time sorting algorithm that
guarantees both $O(\log n)$ maximum dislocation and $O(n)$ total dislocation
with high probability when $p<\frac{1}{4}$. This settles the time complexity
sorting with persistent comparison errors in the given range of $p$ and shows
that comparison errors do not increase its computational difficulty. Indeed,
$\Omega(n\log n)$ time is necessary to archive a maximum dislocation of $O(\log
n)$ even without comparison errors. Moreover, we prove that no algorithm can
guarantee a maximum dislocation of $o(\log n)$ with high probability, nor a
total dislocation of $o(n)$ in expectation.
  To develop our sorting algorithm, we solve two related sub-problems, which
might be of independent interest. More precisely, we show that $O(\log n)$ time
suffices to find a position in which to insert a new element $x$ in an
almost-sorted sequence $S$ of $n$ elements having dislocation at most
$d=\Omega(\log n)$, so that the dislocation of $x$ in the resulting sequence is
$O(d)$ with high probability (which can be equivalently thought as the problem
of estimating the rank of $x$ in $S$). We also show that the maximum (resp.
total) dislocation of an approximately sorted sequence $S$ of $n$ elements can
be lowered to $O(\log n)$ (resp. $O(n)$) in $O(nd)$ time, w.h.p., where $d$ is
an upper bound on the maximum dislocation of $S$.

</details>


### [155] [Optimizing Wiggle in Storylines](https://arxiv.org/abs/2508.19802)
*Alexander Dobler,Tim Hegemann,Martin Nöllenburg,Alexander Wolff*

Main category: cs.DS

TL;DR: 该论文研究了故事线可视化中的摆动最小化问题，证明摆动计数最小化是NP完全的，并提出有效的数学编程算法来解决线性摆动高度最小化和二次摆动高度最小化问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在优化故事线可视化中的摆动（即角色随时间的垂直移动量）这一重要但较少研究的质量指标。

Method: 论文采用了数学编程算法来解决线性摆动高度最小化和二次摆动高度最小化问题，并提出了一种新的角色曲线路由方法。

Result: 研究表明摆动计数最小化问题是NP完全的，但通过数学编程算法可以有效解决线性摆动高度最小化和二次摆动高度最小化问题。

Conclusion: 该论文通过数学编程算法有效解决了线性摆动高度最小化和二次摆动高度最小化问题，并提出了一种新的角色曲线路由方法，强调在平行运行时保持相邻曲线间距离恒定。

Abstract: A storyline visualization shows interactions between characters over time.
Each character is represented by an x-monotone curve. Time is mapped to the
x-axis, and groups of characters that interact at a particular point $t$ in
time must be ordered consecutively in the y-dimension at $x=t$. The predominant
objective in storyline optimization so far has been the minimization of
crossings between (blocks of) characters. Building on this work, we investigate
another important, but less studied quality criterion, namely the minimization
of wiggle, i.e., the amount of vertical movement of the characters over time.
Given a storyline instance together with an ordering of the characters at any
point in time, we show that wiggle count minimization is NP-complete. In
contrast, we provide algorithms based on mathematical programming to solve
linear wiggle height minimization and quadratic wiggle height minimization
efficiently. Finally, we introduce a new method for routing character curves
that focuses on keeping distances between neighboring curves constant as long
as they run in parallel. We have implemented our algorithms, and we conduct a
case study that explores the differences between the three optimization
objectives. We use existing benchmark data, but we also present a new use case
for storylines, namely the visualization of rolling stock schedules in railway
operation.

</details>


### [156] [Distributed Sparsest Cut via Eigenvalue Estimation](https://arxiv.org/abs/2508.19898)
*Yannic Maus,Tijn de Vos*

Main category: cs.DS

TL;DR: 新算法在CONGEST模型中高效近似稀疏割值，优于现有方法，适用于k-way传导率。


<details>
  <summary>Details</summary>
Motivation: 改进现有稀疏割值近似算法的效率，尤其是在CONGEST模型中的性能。

Method: 通过近似归一化拉普拉斯矩阵L的特征值，利用幂方法（power method）实现，该方法在CONGEST模型中单轮即可完成。

Result: 算法在O(log²n/φ)轮内运行，输出值φ̃满足φ ≤ φ̃ ≤ √2.01φ，且适用于加权无向图。

Conclusion: 本文提出了一种在CONGEST模型中近似稀疏割值（即图的传导率φ）的新算法，显著优于现有最快算法，并适用于k-way传导率。

Abstract: We give new, improved bounds for approximating the sparsest cut value or in
other words the conductance $\phi$ of a graph in the CONGEST model. As our main
result, we present an algorithm running in $O(\log^2 n/\phi)$ rounds in which
every vertex outputs a value $\tilde \phi$ satisfying $\phi \le \tilde \phi \le
\sqrt{2.01\phi}$. In most regimes, our algorithm improves significantly over
the previously fastest algorithm for the problem [Chen, Meierhans, Probst
Gutenberg, Saranurak; SODA 25]. Additionally, our result generalizes to $k$-way
conductance.
  We obtain these results, by approximating the eigenvalues of the normalized
Laplacian matrix $L:=I-\rm{Deg}^{-1/2}A\rm{Deg}^ {-1/2}$, where, $A$ is the
adjacency matrix and $\rm{Deg}$ is the diagonal matrix with the weighted
degrees on the diagonal. The previous state of the art sparsest cut algorithm
is in the technical realm of expander decompositions. Our algorithms, on the
other hand, are relatively simple and easy to implement. At the core, they rely
on the well-known power method, which comes down to repeatedly multiplying the
Laplacian with a vector. This operation can be performed in a single round in
the CONGEST model. All our algorithms apply to weighted, undirected graphs. Our
lower bounds apply even in unweighted graphs.

</details>


### [157] [Bipartite Matching with Pair-Dependent Bounds](https://arxiv.org/abs/2508.20002)
*Shaul Rosner,Tami Tamir*

Main category: cs.DS

TL;DR: 本文首次研究了二分图PD匹配问题，考虑了作业对机器拥堵的容忍度差异，分析了其计算复杂度，并提出了算法解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于现实系统中不同作业对机器拥堵的容忍度不同，这一特性在现有匹配问题中未被考虑，因此需要新的匹配模型。

Method: 通过定义二分图PD匹配并分析其计算复杂度，研究采用了理论证明和算法设计的方法。

Result: 研究结果表明，二分图PD匹配问题在计算复杂度上与传统匹配问题存在显著差异，并提出了相应的算法解决方案。

Conclusion: 本文研究了二分图PD匹配问题，证明了其与现有匹配问题的显著差异，并提供了针对一般情况和特定受限实例的计算复杂度分析，包括硬度结果、最优算法和近似算法。

Abstract: Let $G=(U \cup V, E)$ be a bipartite graph, where $U$ represents jobs and $V$
represents machines. We study a new variant of the bipartite matching problem
in which each job in $U$ can be matched to at most one machine in $V$, and the
number of jobs that can be assigned to a machine depends on the specific jobs
matched to it. These pair-dependent bounds reflect systems where different jobs
have varying tolerance for congestion, determined by the specific machine they
are assigned to.
  We define a bipartite PD-matching as a set of edges $M \subseteq E$ that
satisfies these job-to-machine tolerance constraints. This variant of matching
extends well-known matching problems, however, despite its relevance to
real-world systems, it has not been studied before. We study bipartite
PD-matchings with the objective of maximizing the matching size. As we show,
the problem exhibits significant differences from previously studied matching
problems. We analyze its computational complexity both in the general case and
for specific restricted instances, presenting hardness results alongside
optimal and approximation algorithms.

</details>


### [158] [Flow-weighted Layered Metric Euclidean Capacitated Steiner Tree Problem](https://arxiv.org/abs/2508.20041)
*Thomas Bläsius,Henrik Csöre,Max Göttlicher,Elly Schmidt,Wendy Yi*

Main category: cs.DS

TL;DR: FLaMECaST问题在分层网络中提出，虽NP难近似，但在特定约束下可通过动态规划实现多项式时间近似解。


<details>
  <summary>Details</summary>
Motivation: 受分层网络启发，引入FLaMECaST问题，研究在分层结构和容量约束下的最优Steiner森林构建。

Method: 设计了一个动态规划算法，基于结构性洞察，实现了多项式时间的近似解。

Result: 证明了FLaMECaST问题在受限情况下（如源点位于圆上）难以近似，但在附加约束下可实现多项式时间的近似解。

Conclusion: FLaMECaST问题在特定约束下可以通过动态规划实现多项式时间的近似解，且在凸多边形源点布局下方法可扩展。

Abstract: Motivated by hierarchical networks, we introduce the Flow-weighted Layered
Metric Euclidean Capacitated Steiner Tree (FLaMECaST) problem, a variant of the
Euclidean Steiner tree with layered structure and capacity constraints per
layer. The goal is to construct a cost-optimal Steiner forest connecting a set
of sources to a set of sinks under load-dependent edge costs. We prove that
FLaMECaST is NP-hard to approximate, even in restricted cases where all sources
lie on a circle. However, assuming few additional constraints for such
instances, we design a dynamic program that achieves a $\left(1 +
\frac{1}{2^n}\right)$-approximation in polynomial time. By generalizing the
structural insights the dynamic program is based on, we extend the approach to
certain settings, where all sources are positioned on a convex polygon.

</details>
