{"id": "2508.19367", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19367", "abs": "https://arxiv.org/abs/2508.19367", "authors": ["Alex Cuellar", "Ho Chit Siu", "Julie A Shah"], "title": "Inference of Human-derived Specifications of Object Placement via Demonstration", "comment": null, "summary": "As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,\nobject packing, sorting, and kitting), methods focused on understanding\nhuman-acceptable object configurations remain limited expressively with regard\nto capturing spatial relationships important to humans. To advance robotic\nunderstanding of human rules for object arrangement, we introduce\npositionally-augmented RCC (PARCC), a formal logic framework based on region\nconnection calculus (RCC) for describing the relative position of objects in\nspace. Additionally, we introduce an inference algorithm for learning PARCC\nspecifications via demonstrations. Finally, we present the results from a human\nstudy, which demonstrate our framework's ability to capture a human's intended\nspecification and the benefits of learning from demonstration approaches over\nhuman-provided specifications.", "AI": {"tldr": "PARCC\u6846\u67b6\u901a\u8fc7\u903b\u8f91\u548c\u6f14\u793a\u5b66\u4e60\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u4eba\u7c7b\u7a7a\u95f4\u5173\u7cfb\u504f\u597d\u7684\u7406\u89e3\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u4eba\u7c7b\u53ef\u63a5\u53d7\u7684\u7269\u4f53\u914d\u7f6e\u89c4\u5219\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5f25\u8865\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u4eba\u7c7b\u7a7a\u95f4\u5173\u7cfb\u504f\u597d\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e\u533a\u57df\u8fde\u63a5\u6f14\u7b97\uff08RCC\uff09\u5f00\u53d1\u4e86PARCC\u903b\u8f91\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u901a\u8fc7\u6f14\u793a\u5b66\u4e60\u7684\u63a8\u7406\u7b97\u6cd5\u3002", "result": "\u4eba\u7c7b\u7814\u7a76\u8868\u660e\uff0cPARCC\u80fd\u6709\u6548\u6355\u6349\u4eba\u7c7b\u7684\u610f\u56fe\uff0c\u4e14\u6f14\u793a\u5b66\u4e60\u65b9\u6cd5\u4f18\u4e8e\u76f4\u63a5\u63d0\u4f9b\u7684\u89c4\u5219\u3002", "conclusion": "PARCC\u6846\u67b6\u901a\u8fc7\u6f14\u793a\u5b66\u4e60\u6709\u6548\u6355\u6349\u4eba\u7c7b\u5bf9\u7269\u4f53\u7a7a\u95f4\u5173\u7cfb\u7684\u504f\u597d\uff0c\u4f18\u4e8e\u4eba\u7c7b\u76f4\u63a5\u63d0\u4f9b\u7684\u89c4\u5219\u3002"}}
{"id": "2508.19380", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19380", "abs": "https://arxiv.org/abs/2508.19380", "authors": ["Diancheng Li", "Nia Ralston", "Bastiaan Hagen", "Phoebe Tan", "Matthew A. Robertson"], "title": "FlipWalker: Jacob's Ladder toy-inspired robot for locomotion across diverse, complex terrain", "comment": "2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2025)", "summary": "This paper introduces FlipWalker, a novel underactuated robot locomotion\nsystem inspired by Jacob's Ladder illusion toy, designed to traverse\nchallenging terrains where wheeled robots often struggle. Like the Jacob's\nLadder toy, FlipWalker features two interconnected segments joined by flexible\ncables, enabling it to pivot and flip around singularities in a manner\nreminiscent of the toy's cascading motion. Actuation is provided by\nmotor-driven legs within each segment that push off either the ground or the\nopposing segment, depending on the robot's current configuration. A\nphysics-based model of the underactuated flipping dynamics is formulated to\nelucidate the critical design parameters governing forward motion and obstacle\nclearance or climbing. The untethered prototype weighs 0.78 kg, achieves a\nmaximum flipping speed of 0.2 body lengths per second. Experimental trials on\nartificial grass, river rocks, and snow demonstrate that FlipWalker's flipping\nstrategy, which relies on ground reaction forces applied normal to the surface,\noffers a promising alternative to traditional locomotion for navigating\nirregular outdoor terrain.", "AI": {"tldr": "FlipWalker\u662f\u4e00\u79cd\u53d7Jacob's Ladder\u73a9\u5177\u542f\u53d1\u7684\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u7ffb\u8f6c\u8fd0\u52a8\u5728\u4e0d\u89c4\u5219\u5730\u5f62\u4e2d\u79fb\u52a8\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u53d7Jacob's Ladder\u73a9\u5177\u542f\u53d1\uff0c\u8bbe\u8ba1\u80fd\u5728\u8f6e\u5f0f\u673a\u5668\u4eba\u96be\u4ee5\u5e94\u5bf9\u7684\u5730\u5f62\u4e2d\u79fb\u52a8\u7684\u673a\u5668\u4eba\u3002", "method": "\u901a\u8fc7\u7269\u7406\u57fa\u7840\u7684\u7ffb\u8f6c\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5206\u6790\u5173\u952e\u8bbe\u8ba1\u53c2\u6570\uff0c\u91c7\u7528\u7535\u673a\u9a71\u52a8\u7684\u817f\u63a8\u52a8\u5730\u9762\u6216\u76f8\u5bf9\u6bb5\u3002", "result": "\u65e0\u7ef3\u539f\u578b\u91cd0.78\u516c\u65a4\uff0c\u6700\u5927\u7ffb\u8f6c\u901f\u5ea6\u4e3a\u6bcf\u79d20.2\u4f53\u957f\uff0c\u5728\u4eba\u5de5\u8349\u576a\u3001\u6cb3\u77f3\u548c\u96ea\u5730\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "FlipWalker\u7684\u7ffb\u8f6c\u7b56\u7565\u4e3a\u4e0d\u89c4\u5219\u6237\u5916\u5730\u5f62\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.19391", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19391", "abs": "https://arxiv.org/abs/2508.19391", "authors": ["Chaoran Zhu", "Hengyi Wang", "Yik Lung Pang", "Changjae Oh"], "title": "LaVA-Man: Learning Visual Action Representations for Robot Manipulation", "comment": null, "summary": "Visual-textual understanding is essential for language-guided robot\nmanipulation. Recent works leverage pre-trained vision-language models to\nmeasure the similarity between encoded visual observations and textual\ninstructions, and then train a model to map this similarity to robot actions.\nHowever, this two-step approach limits the model to capture the relationship\nbetween visual observations and textual instructions, leading to reduced\nprecision in manipulation tasks. We propose to learn visual-textual\nassociations through a self-supervised pretext task: reconstructing a masked\ngoal image conditioned on an input image and textual instructions. This\nformulation allows the model to learn visual-action representations without\nrobot action supervision. The learned representations can then be fine-tuned\nfor manipulation tasks with only a few demonstrations. We also introduce the\n\\textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot\ntabletop manipulation episodes, including 180 object classes and 3,200\ninstances with corresponding textual instructions. This dataset enables the\nmodel to acquire diverse object priors and allows for a more comprehensive\nevaluation of its generalisation capability across object instances.\nExperimental results on the five benchmarks, including both simulated and\nreal-robot validations, demonstrate that our method outperforms prior art.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u76d1\u7763\u4efb\u52a1\u5b66\u4e60\u89c6\u89c9-\u6587\u672c\u5173\u8054\uff0c\u63d0\u5347\u8bed\u8a00\u5f15\u5bfc\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7cbe\u5ea6\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u4e24\u6b65\u65b9\u6cd5\u9650\u5236\u4e86\u6a21\u578b\u6355\u6349\u89c6\u89c9\u89c2\u5bdf\u4e0e\u6587\u672c\u6307\u4ee4\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u64cd\u4f5c\u4efb\u52a1\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4efb\u52a1\uff1a\u57fa\u4e8e\u8f93\u5165\u56fe\u50cf\u548c\u6587\u672c\u6307\u4ee4\u91cd\u5efa\u88ab\u63a9\u7801\u7684\u76ee\u6807\u56fe\u50cf\uff0c\u4ece\u800c\u5b66\u4e60\u89c6\u89c9-\u52a8\u4f5c\u8868\u793a\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u5305\u62ec\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u9a8c\u8bc1\uff09\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4efb\u52a1\u5b66\u4e60\u89c6\u89c9-\u6587\u672c\u5173\u8054\uff0c\u5e76\u5728\u5c11\u91cf\u6f14\u793a\u4e0b\u5fae\u8c03\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u9a8c\u8bc1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.19425", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19425", "abs": "https://arxiv.org/abs/2508.19425", "authors": ["John M. Scanlon", "Timothy L McMurry", "Yin-Hsiu Chen", "Kristofer D. Kusano", "Trent Victor"], "title": "From Stoplights to On-Ramps: A Comprehensive Set of Crash Rate Benchmarks for Freeway and Surface Street ADS Evaluation", "comment": null, "summary": "This paper presents crash rate benchmarks for evaluating US-based Automated\nDriving Systems (ADS) for multiple urban areas. The purpose of this study was\nto extend prior benchmarks focused only on surface streets to additionally\ncapture freeway crash risk for future ADS safety performance assessments. Using\npublicly available police-reported crash and vehicle miles traveled (VMT) data,\nthe methodology details the isolation of in-transport passenger vehicles, road\ntype classification, and crash typology. Key findings revealed that freeway\ncrash rates exhibit large geographic dependence variations with\nany-injury-reported crash rates being nearly 3.5 times higher in Atlanta (2.4\nIPMM; the highest) when compared to Phoenix (0.7 IPMM; the lowest). The results\nshow the critical need for location-specific benchmarks to avoid biased safety\nevaluations and provide insights into the vehicle miles traveled (VMT) required\nto achieve statistical significance for various safety impact levels. The\ndistribution of crash types depended on the outcome severity level. Higher\nseverity outcomes (e.g., fatal crashes) had a larger proportion of\nsingle-vehicle, vulnerable road users (VRU), and opposite-direction collisions\ncompared to lower severity (police-reported) crashes. Given heterogeneity in\ncrash types by severity, performance in low-severity scenarios may not be\npredictive of high-severity outcomes. These benchmarks are additionally used to\nquantify at the required mileage to show statistically significant deviations\nfrom human performance. This is the first paper to generate freeway-specific\nbenchmarks for ADS evaluation and provides a foundational framework for future\nADS benchmarking by evaluators and developers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u7f8e\u56fd\u591a\u4e2a\u57ce\u5e02\u533a\u57df\u7684ADS\u78b0\u649e\u7387\u57fa\u51c6\uff0c\u9996\u6b21\u7eb3\u5165\u9ad8\u901f\u516c\u8def\u6570\u636e\uff0c\u53d1\u73b0\u78b0\u649e\u7387\u5b58\u5728\u663e\u8457\u5730\u7406\u5dee\u5f02\uff0c\u5f3a\u8c03\u9700\u4f4d\u7f6e\u7279\u5b9a\u57fa\u51c6\u4ee5\u907f\u514d\u8bc4\u4f30\u504f\u5dee\uff0c\u4e3a\u672a\u6765ADS\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "\u6269\u5c55\u5148\u524d\u4ec5\u5173\u6ce8\u57ce\u5e02\u8857\u9053\u7684\u57fa\u51c6\uff0c\u4ee5\u6355\u6349\u672a\u6765ADS\u5b89\u5168\u6027\u80fd\u8bc4\u4f30\u4e2d\u7684\u9ad8\u901f\u516c\u8def\u78b0\u649e\u98ce\u9669\u3002", "method": "\u5229\u7528\u516c\u5f00\u7684\u8b66\u5bdf\u62a5\u544a\u78b0\u649e\u548c\u8f66\u8f86\u884c\u9a76\u91cc\u7a0b\uff08VMT\uff09\u6570\u636e\uff0c\u65b9\u6cd5\u5305\u62ec\u9694\u79bb\u8fd0\u8f93\u4e2d\u7684\u4e58\u7528\u8f66\u3001\u9053\u8def\u7c7b\u578b\u5206\u7c7b\u548c\u78b0\u649e\u7c7b\u578b\u5b66\u5206\u6790\u3002", "result": "\u5173\u952e\u53d1\u73b0\u663e\u793a\uff0c\u9ad8\u901f\u516c\u8def\u78b0\u649e\u7387\u5b58\u5728\u663e\u8457\u5730\u7406\u4f9d\u8d56\u6027\u53d8\u5316\uff0c\u4e9a\u7279\u5170\u5927\uff082.4 IPMM\uff1b\u6700\u9ad8\uff09\u4e0e\u51e4\u51f0\u57ce\uff080.7 IPMM\uff1b\u6700\u4f4e\uff09\u76f8\u6bd4\uff0c\u4efb\u4f55\u4f24\u5bb3\u62a5\u544a\u78b0\u649e\u7387\u9ad8\u51fa\u8fd13.5\u500d\u3002\u78b0\u649e\u7c7b\u578b\u7684\u5206\u5e03\u53d6\u51b3\u4e8e\u7ed3\u679c\u4e25\u91cd\u7a0b\u5ea6\uff0c\u9ad8\u4e25\u91cd\u6027\u7ed3\u679c\uff08\u5982\u81f4\u547d\u78b0\u649e\uff09\u4e2d\u5355\u8f66\u8f86\u3001\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u548c\u5bf9\u5411\u78b0\u649e\u7684\u6bd4\u4f8b\u66f4\u9ad8\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u751f\u6210\u4e86\u9488\u5bf9\u9ad8\u901f\u516c\u8def\u7684ADS\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e3a\u672a\u6765\u7684ADS\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u4f4d\u7f6e\u7279\u5b9a\u57fa\u51c6\u7684\u91cd\u8981\u6027\u4ee5\u907f\u514d\u5b89\u5168\u8bc4\u4f30\u504f\u5dee\u3002"}}
{"id": "2508.19518", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19518", "abs": "https://arxiv.org/abs/2508.19518", "authors": ["Hail Song", "Seokhwan Yang", "Woontack Woo"], "title": "Fast Texture Transfer for XR Avatars via Barycentric UV Conversion", "comment": null, "summary": "We present a fast and efficient method for transferring facial textures onto\nSMPL-X-based full-body avatars. Unlike conventional affine-transform methods\nthat are slow and prone to visual artifacts, our method utilizes a barycentric\nUV conversion technique. Our approach precomputes the entire UV mapping into a\nsingle transformation matrix, enabling texture transfer in a single operation.\nThis results in a speedup of over 7000x compared to the baseline, while also\nsignificantly improving the final texture quality by eliminating boundary\nartifacts. Through quantitative and qualitative evaluations, we demonstrate\nthat our method offers a practical solution for personalization in immersive XR\napplications. The code is available online.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cd\u5fc3UV\u8f6c\u6362\u7684\u9ad8\u6548\u9762\u90e8\u7eb9\u7406\u8f6c\u79fb\u65b9\u6cd5\uff0c\u901f\u5ea6\u63d0\u53477000\u500d\u4e14\u8d28\u91cf\u66f4\u4f18\uff0c\u9002\u7528\u4e8eXR\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4eff\u5c04\u53d8\u6362\u7684\u65b9\u6cd5\u901f\u5ea6\u6162\u4e14\u6613\u4ea7\u751f\u89c6\u89c9\u4f2a\u5f71\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u6548\u9ad8\u8d28\u91cf\u7eb9\u7406\u8f6c\u79fb\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u91cd\u5fc3UV\u8f6c\u6362\u6280\u672f\uff0c\u9884\u8ba1\u7b97\u6574\u4e2aUV\u6620\u5c04\u5230\u5355\u4e00\u53d8\u6362\u77e9\u9635\u4e2d\uff0c\u5b9e\u73b0\u5355\u6b21\u64cd\u4f5c\u7684\u7eb9\u7406\u8f6c\u79fb\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u901f\u5ea6\u63d0\u5347\u8d85\u8fc77000\u500d\uff0c\u540c\u65f6\u663e\u8457\u6539\u5584\u4e86\u6700\u7ec8\u7eb9\u7406\u8d28\u91cf\uff0c\u6d88\u9664\u4e86\u8fb9\u754c\u4f2a\u5f71\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6c89\u6d78\u5f0fXR\u5e94\u7528\u4e2d\u7684\u4e2a\u6027\u5316\u9700\u6c42\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.19373", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19373", "abs": "https://arxiv.org/abs/2508.19373", "authors": ["Haoran Lin", "Xianzhi Yu", "Kang Zhao", "Han Bao", "Zongyuan Zhan", "Ting Hu", "Wulong Liu", "Zekun Yin", "Xin Li", "Weiguo Liu"], "title": "HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference", "comment": null, "summary": "Current inference systems for Mixture-of-Experts (MoE) models primarily\nemploy static parallelization strategies. However, these static approaches\ncannot consistently achieve optimal performance across different inference\nscenarios, as they lack the flexibility to adapt to varying computational\nrequirements. In this work, we propose HAP (Hybrid Adaptive Parallelism), a\nnovel method that dynamically selects hybrid parallel strategies to enhance MoE\ninference efficiency. The fundamental innovation of HAP lies in hierarchically\ndecomposing MoE architectures into two distinct computational modules: the\nAttention module and the Expert module, each augmented with a specialized\ninference latency simulation model. This decomposition promotes the\nconstruction of a comprehensive search space for seeking model parallel\nstrategies. By leveraging Integer Linear Programming (ILP), HAP could solve the\noptimal hybrid parallel configurations to maximize inference efficiency under\nvarying computational constraints. Our experiments demonstrate that HAP\nconsistently determines parallel configurations that achieve comparable or\nsuperior performance to the TP strategy prevalent in mainstream inference\nsystems. Compared to the TP-based inference, HAP-based inference achieves\nspeedups of 1.68x, 1.77x, and 1.57x on A100, A6000, and V100 GPU platforms,\nrespectively. Furthermore, HAP showcases remarkable generalization capability,\nmaintaining performance effectiveness across diverse MoE model configurations,\nincluding Mixtral and Qwen series models.", "AI": {"tldr": "HAP\u662f\u4e00\u79cd\u52a8\u6001\u9009\u62e9\u6df7\u5408\u5e76\u884c\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86MoE\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u5e76\u5728\u591a\u79cdGPU\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u7684MoE\u63a8\u7406\u7cfb\u7edf\u91c7\u7528\u9759\u6001\u5e76\u884c\u5316\u7b56\u7565\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u63a8\u7406\u573a\u666f\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u52a8\u6001\u5e76\u884c\u5316\u65b9\u6cd5\u3002", "method": "HAP\u901a\u8fc7\u5c06MoE\u67b6\u6784\u5206\u5c42\u5206\u89e3\u4e3a\u6ce8\u610f\u529b\u6a21\u5757\u548c\u4e13\u5bb6\u6a21\u5757\uff0c\u5e76\u5229\u7528\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\u6c42\u89e3\u6700\u4f18\u5e76\u884c\u914d\u7f6e\u3002", "result": "\u5728A100\u3001A6000\u548cV100 GPU\u5e73\u53f0\u4e0a\uff0cHAP\u5206\u522b\u5b9e\u73b0\u4e861.68\u500d\u30011.77\u500d\u548c1.57\u500d\u7684\u52a0\u901f\u6548\u679c\uff0c\u4e14\u5728\u4e0d\u540cMoE\u6a21\u578b\u914d\u7f6e\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "HAP\uff08\u6df7\u5408\u81ea\u9002\u5e94\u5e76\u884c\u6027\uff09\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6df7\u5408\u5e76\u884c\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86MoE\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u5e76\u5728\u591a\u79cdGPU\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u3002"}}
{"id": "2508.19350", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.19350", "abs": "https://arxiv.org/abs/2508.19350", "authors": ["Kaiqiang Lin", "Mohamed-Slim Alouini"], "title": "Connectivity Analysis of LoRaWAN-Based Non-Terrestrial Networks for Subterranean mMTC", "comment": "13 pages, 10 figures, 5 tables, submitted to IEEE IoTJ", "summary": "Wireless underground sensor networks (WUSNs) offer significant social and\neconomic benefits by enabling the monitoring of subterranean entities. However,\nthe communication reliability of WUSNs diminishes in harsh environments where\nterrestrial network infrastructure is either unavailable or unreliable. To\naddress this challenge, we explore the feasibility of integrating buried\nmassive machine-type communication (mMTC) sensors with non-terrestrial networks\n(NTNs), including unmanned aerial vehicles (UAVs), high-altitude platforms\n(HAPs), and low Earth orbit (LEO) satellites, to establish underground-to-NTN\nconnectivity for various large-scale underground monitoring applications. To\nassess the effectiveness of underground-to-NTN connectivity, we develop a Monte\nCarlo simulator that incorporates a multi-layer underground attenuation model,\nthe 3GPP empirical path loss model for various NTN platforms, and two LoRaWAN\nmodulation schemes, i.e., LoRa and LoRa-frequency hopping spread spectrum\n(LR-FHSS). Our results evidence that LoRa SF7 is a strong candidate for\nshort-range UAV communication in rural environments, while LR-FHSS modulation\nproves to be a promising option for HAP and LEO satellite platforms in massive\nWUSNs scenarios thanks to its adequate link budget and robustness to the\ninterference. Finally, we demonstrate that the success probability of\nunderground-to-NTN connectivity using LoRa and LR-FHSS is significantly\naffected by factors such as the monitoring environment, the number of devices,\nburial depth, and the soil's volumetric water content.", "AI": {"tldr": "The paper explores underground-to-NTN connectivity for WUSNs, using a Monte Carlo simulator to evaluate LoRa and LR-FHSS. LoRa SF7 works well for UAVs in rural areas, while LR-FHSS suits HAP and LEO satellites, with success depending on environmental and deployment factors.", "motivation": "The motivation is to address the communication reliability challenges of WUSNs in harsh environments where terrestrial networks are unreliable or unavailable, by exploring underground-to-NTN connectivity.", "method": "A Monte Carlo simulator was developed, incorporating a multi-layer underground attenuation model, 3GPP empirical path loss model for NTN platforms, and two LoRaWAN modulation schemes (LoRa and LR-FHSS).", "result": "Results show that LoRa SF7 is effective for UAVs in rural settings, and LR-FHSS is promising for HAP and LEO satellites in large-scale WUSNs. Success probability is affected by environmental and deployment factors.", "conclusion": "The study concludes that LoRa SF7 is suitable for short-range UAV communication in rural areas, while LR-FHSS is better for HAP and LEO satellite platforms in massive WUSNs due to its link budget and interference robustness. Success probability is influenced by environmental factors, device count, burial depth, and soil moisture."}}
{"id": "2508.19449", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19449", "abs": "https://arxiv.org/abs/2508.19449", "authors": ["Md Afif Al Mamun", "Gias Uddin", "Lan Xia", "Longyu Zhang"], "title": "Stack Trace-Based Crash Deduplication with Transformer Adaptation", "comment": "This work is currently under review at IEEE Transactions on Software\n  Engineering. The replication package will be made publicly available upon\n  acceptance", "summary": "Automated crash reporting systems generate large volumes of duplicate\nreports, overwhelming issue-tracking systems and increasing developer workload.\nTraditional stack trace-based deduplication methods, relying on string\nsimilarity, rule-based heuristics, or deep learning (DL) models, often fail to\ncapture the contextual and structural relationships within stack traces. We\npropose dedupT, a transformer-based approach that models stack traces\nholistically rather than as isolated frames. dedupT first adapts a pretrained\nlanguage model (PLM) to stack traces, then uses its embeddings to train a\nfully-connected network (FCN) to rank duplicate crashes effectively. Extensive\nexperiments on real-world datasets show that dedupT outperforms existing DL and\ntraditional methods (e.g., sequence alignment and information retrieval\ntechniques) in both duplicate ranking and unique crash detection, significantly\nreducing manual triage effort. On four public datasets, dedupT improves Mean\nReciprocal Rank (MRR) often by over 15% compared to the best DL baseline and up\nto 9% over traditional methods while achieving higher Receiver Operating\nCharacteristic Area Under the Curve (ROC-AUC) in detecting unique crash\nreports. Our work advances the integration of modern natural language\nprocessing (NLP) techniques into software engineering, providing an effective\nsolution for stack trace-based crash deduplication.", "AI": {"tldr": "dedupT\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u5168\u8fde\u63a5\u7f51\u7edc\u6709\u6548\u6392\u540d\u548c\u68c0\u6d4b\u91cd\u590d\u5d29\u6e83\u62a5\u544a\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5206\u7c7b\u5de5\u4f5c\u91cf\u3002", "motivation": "\u81ea\u52a8\u5316\u5d29\u6e83\u62a5\u544a\u7cfb\u7edf\u751f\u6210\u5927\u91cf\u91cd\u590d\u62a5\u544a\uff0c\u589e\u52a0\u4e86\u5f00\u53d1\u4eba\u5458\u7684\u5de5\u4f5c\u91cf\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u5806\u6808\u8ddf\u8e2a\u7684\u53bb\u91cd\u65b9\u6cd5\u672a\u80fd\u6355\u6349\u5806\u6808\u8ddf\u8e2a\u4e2d\u7684\u4e0a\u4e0b\u6587\u548c\u7ed3\u6784\u5173\u7cfb\u3002", "method": "dedupT\u9996\u5148\u5c06\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u9002\u914d\u5230\u5806\u6808\u8ddf\u8e2a\u4e2d\uff0c\u7136\u540e\u5229\u7528\u5176\u5d4c\u5165\u8bad\u7ec3\u5168\u8fde\u63a5\u7f51\u7edc\uff08FCN\uff09\u4ee5\u6709\u6548\u6392\u540d\u91cd\u590d\u5d29\u6e83\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cdedupT\u5728\u91cd\u590d\u6392\u540d\u548c\u552f\u4e00\u5d29\u6e83\u68c0\u6d4b\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u548c\u4f20\u7edf\u65b9\u6cd5\uff0cMRR\u63d0\u5347\u8d85\u8fc715%\uff0cROC-AUC\u66f4\u9ad8\u3002", "conclusion": "dedupT\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u5168\u8fde\u63a5\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u590d\u5d29\u6e83\u62a5\u544a\u7684\u6392\u540d\u548c\u552f\u4e00\u5d29\u6e83\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u73b0\u4ee3\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19473", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.19473", "abs": "https://arxiv.org/abs/2508.19473", "authors": ["Stephen Arndt", "Benjamin Moseley", "Kirk Pruhs", "Michael Zlatin"], "title": "Efficiently Coloring the Intersection of a General Matroid and Partition Matroids", "comment": null, "summary": "This paper shows a polynomial-time algorithm, that given a general matroid\n$M_1 = (X, \\mathcal{I}_1)$ and $k-1$ partition matroids $ M_2, \\ldots, M_k$,\nproduces a coloring of the intersection $M = \\cap_{i=1}^k M_i$ using at most\n$1+\\sum_{i=1}^k \\left(\\chi(M_i) -1\\right)$ colors. This is the first\npolynomial-time $O(1)$-approximation algorithm for matroid intersection\ncoloring where one of the matroids may be a general matroid. Leveraging the\nfact that all of the standard combinatorial matroids reduce to partition\nmatroids at a loss of a factor of two in the chromatic number, this algorithm\nalso yields a polynomial-time $O(1)$-approximation algorithm for matroid\nintersection coloring in the case where each of the matroids $ M_2, \\ldots,\nM_k$ are one of the standard combinatorial types.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff0c\u4e3a\u5305\u542b\u4e00\u822c\u62df\u9635\u548c\u5206\u533a\u62df\u9635\u7684\u4ea4\u96c6\u63d0\u4f9bO(1)\u8fd1\u4f3c\u7740\u8272\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u62df\u9635\u4ea4\u96c6\u7740\u8272\u95ee\u9898\u5728\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5c24\u5176\u662f\u5728\u5176\u4e2d\u4e00\u4e2a\u62df\u9635\u53ef\u80fd\u662f\u4e00\u822c\u62df\u9635\u7684\u60c5\u51b5\u4e0b\uff0c\u73b0\u6709\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u8f83\u4e3a\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u9ad8\u6548\u7684\u8fd1\u4f3c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u7b97\u6cd5\u901a\u8fc7\u5904\u7406\u4e00\u822c\u62df\u9635\u548c\u5206\u533a\u62df\u9635\u7684\u4ea4\u96c6\uff0c\u5229\u7528\u6807\u51c6\u7ec4\u5408\u62df\u9635\u5728\u8272\u6570\u4e0a\u6700\u591a\u635f\u5931\u4e24\u500d\u7684\u6027\u8d28\uff0c\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7684\u8fd1\u4f3c\u7740\u8272\u3002", "result": "\u7b97\u6cd5\u6210\u529f\u5730\u4e3a\u62df\u9635\u4ea4\u96c6\u63d0\u4f9b\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7684O(1)\u8fd1\u4f3c\u7740\u8272\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6807\u51c6\u7ec4\u5408\u62df\u9635\u65f6\uff0c\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u6548\u7387\u548c\u8fd1\u4f3c\u6bd4\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff0c\u80fd\u591f\u4e3a\u5305\u542b\u4e00\u4e2a\u4e00\u822c\u62df\u9635\u548ck-1\u4e2a\u5206\u533a\u62df\u9635\u7684\u4ea4\u96c6\u7740\u8272\uff0c\u4f7f\u7528\u7684\u989c\u8272\u6570\u91cf\u6700\u591a\u4e3a1\u52a0\u4e0a\u5404\u62df\u9635\u8272\u6570\u51cf\u4e00\u7684\u603b\u548c\u3002\u8fd9\u662f\u9996\u4e2a\u5728\u5176\u4e2d\u4e00\u4e2a\u62df\u9635\u53ef\u80fd\u662f\u4e00\u822c\u62df\u9635\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u62df\u9635\u4ea4\u96c6\u7740\u8272\u63d0\u4f9b\u7684\u591a\u9879\u5f0f\u65f6\u95f4O(1)\u8fd1\u4f3c\u7b97\u6cd5\u3002"}}
{"id": "2508.19254", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19254", "abs": "https://arxiv.org/abs/2508.19254", "authors": ["Jookyung Song", "Mookyoung Kang", "Nojun Kwak"], "title": "Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration", "comment": "6 pages, 4 figures, NeurIPS Creative AI Track 2025", "summary": "This paper presents a real-time generative drawing system that interprets and\nintegrates both formal intent - the structural, compositional, and stylistic\nattributes of a sketch - and contextual intent - the semantic and thematic\nmeaning inferred from its visual content - into a unified transformation\nprocess. Unlike conventional text-prompt-based generative systems, which\nprimarily capture high-level contextual descriptions, our approach\nsimultaneously analyzes ground-level intuitive geometric features such as line\ntrajectories, proportions, and spatial arrangement, and high-level semantic\ncues extracted via vision-language models. These dual intent signals are\njointly conditioned in a multi-stage generation pipeline that combines\ncontour-preserving structural control with style- and content-aware image\nsynthesis. Implemented with a touchscreen-based interface and distributed\ninference architecture, the system achieves low-latency, two-stage\ntransformation while supporting multi-user collaboration on shared canvases.\nThe resulting platform enables participants, regardless of artistic expertise,\nto engage in synchronous, co-authored visual creation, redefining human-AI\ninteraction as a process of co-creation and mutual enhancement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u751f\u6210\u7ed8\u56fe\u7cfb\u7edf\uff0c\u901a\u8fc7\u540c\u65f6\u5206\u6790\u5f62\u5f0f\u610f\u56fe\u548c\u4e0a\u4e0b\u6587\u610f\u56fe\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u7684\u5171\u540c\u521b\u4f5c\u3002\u65b9\u6cd5\u7ed3\u5408\u4e86\u591a\u9636\u6bb5\u751f\u6210\u548c\u5206\u5e03\u5f0f\u63a8\u7406\uff0c\u652f\u6301\u591a\u7528\u6237\u534f\u4f5c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u751f\u6210\u7cfb\u7edf\u65e0\u6cd5\u540c\u65f6\u6355\u6349\u9ad8\u5c42\u6b21\u8bed\u4e49\u548c\u4f4e\u5c42\u6b21\u51e0\u4f55\u7279\u5f81\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u8f6e\u5ed3\u4fdd\u6301\u7684\u7ed3\u6784\u63a7\u5236\u4e0e\u98ce\u683c\u548c\u5185\u5bb9\u611f\u77e5\u7684\u56fe\u50cf\u5408\u6210\uff0c\u5e76\u901a\u8fc7\u89e6\u6478\u5c4f\u754c\u9762\u548c\u5206\u5e03\u5f0f\u63a8\u7406\u67b6\u6784\u5b9e\u73b0\u3002", "result": "\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u7684\u4e24\u9636\u6bb5\u8f6c\u6362\uff0c\u652f\u6301\u591a\u7528\u6237\u5728\u5171\u4eab\u753b\u5e03\u4e0a\u7684\u534f\u4f5c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u6574\u5408\u5f62\u5f0f\u610f\u56fe\u548c\u4e0a\u4e0b\u6587\u610f\u56fe\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u4eba\u673a\u4ea4\u4e92\u4e3a\u5171\u540c\u521b\u4f5c\u548c\u76f8\u4e92\u589e\u5f3a\u7684\u8fc7\u7a0b\u3002"}}
{"id": "2508.19316", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.4"], "pdf": "https://arxiv.org/pdf/2508.19316", "abs": "https://arxiv.org/abs/2508.19316", "authors": ["Shreyans Jain", "Alexandra Yost", "Amirali Abdullah"], "title": "Sycophancy as compositions of Atomic Psychometric Traits", "comment": "8 pages, 4 figures", "summary": "Sycophancy is a key behavioral risk in LLMs, yet is often treated as an\nisolated failure mode that occurs via a single causal mechanism. We instead\npropose modeling it as geometric and causal compositions of psychometric traits\nsuch as emotionality, openness, and agreeableness - similar to factor\ndecomposition in psychometrics. Using Contrastive Activation Addition (CAA), we\nmap activation directions to these factors and study how different combinations\nmay give rise to sycophancy (e.g., high extraversion combined with low\nconscientiousness). This perspective allows for interpretable and compositional\nvector-based interventions like addition, subtraction and projection; that may\nbe used to mitigate safety-critical behaviors in LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7528\u5fc3\u7406\u6d4b\u91cf\u7279\u8d28\u6a21\u578b\u89e3\u91caLLMs\u4e2d\u7684\u5949\u627f\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7CAA\u5b9e\u73b0\u53ef\u7ec4\u5408\u7684\u5411\u91cf\u5e72\u9884\u3002", "motivation": "\u5949\u627f\u884c\u4e3a\u662fLLMs\u4e2d\u7684\u5173\u952e\u884c\u4e3a\u98ce\u9669\uff0c\u4f46\u901a\u5e38\u88ab\u89c6\u4e3a\u5355\u4e00\u56e0\u679c\u673a\u5236\u7684\u5b64\u7acb\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u5bf9\u6bd4\u6fc0\u6d3b\u52a0\u6cd5\uff08CAA\uff09\u5c06\u6fc0\u6d3b\u65b9\u5411\u6620\u5c04\u5230\u5fc3\u7406\u6d4b\u91cf\u56e0\u7d20\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u7ec4\u5408\u5982\u4f55\u5bfc\u81f4\u5949\u627f\u884c\u4e3a\u3002", "result": "\u901a\u8fc7\u51e0\u4f55\u548c\u56e0\u679c\u7ec4\u5408\u7684\u5fc3\u7406\u6d4b\u91cf\u7279\u8d28\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5949\u627f\u884c\u4e3a\u7684\u53ef\u89e3\u91ca\u5e72\u9884\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u548c\u56e0\u679c\u7ec4\u5408\u7684\u5fc3\u7406\u6d4b\u91cf\u7279\u8d28\u6a21\u578b\uff0c\u7528\u4e8e\u7406\u89e3\u548c\u5e72\u9884LLMs\u4e2d\u7684\u5949\u627f\u884c\u4e3a\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u884c\u4e3a\u7684\u7f13\u89e3\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u548c\u7ec4\u5408\u7684\u5411\u91cf\u5e72\u9884\u65b9\u6cd5\u3002"}}
{"id": "2508.19429", "categories": ["cs.RO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2508.19429", "abs": "https://arxiv.org/abs/2508.19429", "authors": ["Gustavo A. Cardona", "Kaier Liang", "Cristian-Ioan Vasile"], "title": "An Iterative Approach for Heterogeneous Multi-Agent Route Planning with Resource Transportation Uncertainty and Temporal Logic Goals", "comment": null, "summary": "This paper presents an iterative approach for heterogeneous multi-agent route\nplanning in environments with unknown resource distributions. We focus on a\nteam of robots with diverse capabilities tasked with executing missions\nspecified using Capability Temporal Logic (CaTL), a formal framework built on\nSignal Temporal Logic to handle spatial, temporal, capability, and resource\nconstraints. The key challenge arises from the uncertainty in the initial\ndistribution and quantity of resources in the environment. To address this, we\nintroduce an iterative algorithm that dynamically balances exploration and task\nfulfillment. Robots are guided to explore the environment, identifying resource\nlocations and quantities while progressively refining their understanding of\nthe resource landscape. At the same time, they aim to maximally satisfy the\nmission objectives based on the current information, adapting their strategies\nas new data is uncovered. This approach provides a robust solution for planning\nin dynamic, resource-constrained environments, enabling efficient coordination\nof heterogeneous teams even under conditions of uncertainty. Our method's\neffectiveness and performance are demonstrated through simulated case studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u7684\u8fed\u4ee3\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u4e0e\u4efb\u52a1\u5b8c\u6210\uff0c\u6709\u6548\u5e94\u5bf9\u8d44\u6e90\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u56e2\u961f\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u73af\u5883\u4e2d\u521d\u59cb\u8d44\u6e90\u5206\u5e03\u548c\u6570\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u5f71\u54cd\u4e86\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u52a8\u6001\u3001\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u6267\u884c\u6548\u7387\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u8fed\u4ee3\u7b97\u6cd5\uff0c\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u4e0e\u4efb\u52a1\u5b8c\u6210\u3002\u673a\u5668\u4eba\u88ab\u5f15\u5bfc\u63a2\u7d22\u73af\u5883\uff0c\u8bc6\u522b\u8d44\u6e90\u4f4d\u7f6e\u548c\u6570\u91cf\uff0c\u540c\u65f6\u6839\u636e\u5f53\u524d\u4fe1\u606f\u6700\u5927\u5316\u6ee1\u8db3\u4efb\u52a1\u76ee\u6807\uff0c\u5e76\u968f\u7740\u65b0\u6570\u636e\u7684\u53d1\u73b0\u8c03\u6574\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6027\u80fd\uff0c\u8868\u660e\u5176\u80fd\u591f\u5728\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u9ad8\u6548\u534f\u8c03\u5f02\u6784\u56e2\u961f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8d44\u6e90\u5206\u5e03\u672a\u77e5\u73af\u5883\u4e0b\u5f02\u6784\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u7684\u8fed\u4ee3\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u4e0e\u4efb\u52a1\u5b8c\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8d44\u6e90\u4e0d\u786e\u5b9a\u6027\u5e26\u6765\u7684\u6311\u6218\u3002\u901a\u8fc7\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2508.20080", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.20080", "abs": "https://arxiv.org/abs/2508.20080", "authors": ["Changha Shin", "Woong Oh Cho", "Seon Joo Kim"], "title": "Seam360GS: Seamless 360\u00b0 Gaussian Splatting from Real-World Omnidirectional Images", "comment": "Accepted to ICCV 2025. 10 pages main text, 4 figures, 4 tables,\n  supplementary material included", "summary": "360-degree visual content is widely shared on platforms such as YouTube and\nplays a central role in virtual reality, robotics, and autonomous navigation.\nHowever, consumer-grade dual-fisheye systems consistently yield imperfect\npanoramas due to inherent lens separation and angular distortions. In this\nwork, we introduce a novel calibration framework that incorporates a\ndual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach\nnot only simulates the realistic visual artifacts produced by dual-fisheye\ncameras but also enables the synthesis of seamlessly rendered 360-degree\nimages. By jointly optimizing 3D Gaussian parameters alongside calibration\nvariables that emulate lens gaps and angular distortions, our framework\ntransforms imperfect omnidirectional inputs into flawless novel view synthesis.\nExtensive evaluations on real-world datasets confirm that our method produces\nseamless renderings-even from imperfect images-and outperforms existing\n360-degree rendering models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u53cc\u9c7c\u773c\u76f8\u673a\u7684\u955c\u5934\u95f4\u9699\u548c\u89d2\u5ea6\u5931\u771f\uff0c\u663e\u8457\u63d0\u5347\u4e86360\u5ea6\u56fe\u50cf\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6d88\u8d39\u8005\u7ea7\u53cc\u9c7c\u773c\u7cfb\u7edf\u7531\u4e8e\u56fa\u6709\u7684\u955c\u5934\u5206\u79bb\u548c\u89d2\u5ea6\u5931\u771f\uff0c\u751f\u6210\u7684360\u5ea6\u5168\u666f\u56fe\u5b58\u5728\u7f3a\u9677\uff0c\u5f71\u54cd\u4e86\u865a\u62df\u73b0\u5b9e\u3001\u673a\u5668\u4eba\u548c\u81ea\u4e3b\u5bfc\u822a\u7b49\u5e94\u7528\u7684\u6548\u679c\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u7ed3\u5408\u53cc\u9c7c\u773c\u76f8\u673a\u6a21\u578b\u76843D\u9ad8\u65af\u6e85\u5c04\u7ba1\u9053\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u53163D\u9ad8\u65af\u53c2\u6570\u548c\u6821\u51c6\u53d8\u91cf\uff08\u6a21\u62df\u955c\u5934\u95f4\u9699\u548c\u89d2\u5ea6\u5931\u771f\uff09\u6765\u5b9e\u73b0\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u4e0d\u5b8c\u7f8e\u56fe\u50cf\u4e2d\u751f\u6210\u65e0\u7f1d\u6e32\u67d3\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684360\u5ea6\u6e32\u67d3\u6a21\u578b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b0\u9896\u6821\u51c6\u6846\u67b6\u901a\u8fc7\u5c06\u53cc\u9c7c\u773c\u76f8\u673a\u6a21\u578b\u878d\u51653D\u9ad8\u65af\u6e85\u5c04\u7ba1\u9053\uff0c\u663e\u8457\u63d0\u5347\u4e86360\u5ea6\u89c6\u89c9\u5185\u5bb9\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u80fd\u591f\u4ece\u4e0d\u5b8c\u7f8e\u7684\u5168\u666f\u8f93\u5165\u4e2d\u751f\u6210\u65e0\u7f1d\u7684\u65b0\u89c6\u89d2\u5408\u6210\u3002"}}
{"id": "2508.19452", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19452", "abs": "https://arxiv.org/abs/2508.19452", "authors": ["Andrea Esposito", "Francesco P. Rossi", "Marco Bernardo", "Francesco Fabris", "Hubert Garavel"], "title": "Formal Modeling and Verification of the Algorand Consensus Protocol in CADP", "comment": null, "summary": "Algorand is a scalable and secure permissionless blockchain that achieves\nproof-of-stake consensus via cryptographic self-sortition and binary Byzantine\nagreement. In this paper, we present a process algebraic model of the Algorand\nconsensus protocol with the aim of enabling rigorous formal verification. Our\nmodel captures the behavior of participants with respect to the structured\nalternation of consensus steps toward a committee-based agreement by means of a\nprobabilistic process calculus. We validate the correctness of the protocol in\nthe absence of adversaries and then extend our model to capture the influence\nof coordinated malicious nodes that can force the commit of an empty block\ninstead of the proposed one. The adversarial scenario is analyzed by using an\nequivalence-checking-based noninterference framework that we have implemented\nin the CADP verification toolkit. In addition to highlighting both the\nrobustness and the limitations of the Algorand protocol under adversarial\nassumptions, this work illustrates the added value of using formal methods for\nthe analysis of blockchain consensus algorithms.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f62\u5f0f\u5316\u65b9\u6cd5\u9a8c\u8bc1Algorand\u5171\u8bc6\u534f\u8bae\uff0c\u5c55\u793a\u5176\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e0b\u7684\u7a33\u5065\u6027\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u4e25\u683c\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u4e3aAlgorand\u5171\u8bc6\u534f\u8bae\u5efa\u7acb\u4e00\u4e2a\u8fc7\u7a0b\u4ee3\u6570\u6a21\u578b\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u6982\u7387\u8fc7\u7a0b\u6f14\u7b97\u5efa\u6a21Algorand\u5171\u8bc6\u534f\u8bae\u7684\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7CADP\u9a8c\u8bc1\u5de5\u5177\u5305\u5b9e\u73b0\u7684\u57fa\u4e8e\u7b49\u4ef7\u68c0\u67e5\u7684\u975e\u5e72\u6270\u6846\u67b6\u5206\u6790\u5bf9\u6297\u6027\u573a\u666f\u3002", "result": "\u9a8c\u8bc1\u4e86\u534f\u8bae\u5728\u65e0\u5bf9\u6297\u60c5\u51b5\u4e0b\u7684\u6b63\u786e\u6027\uff0c\u5e76\u6269\u5c55\u6a21\u578b\u4ee5\u5206\u6790\u6076\u610f\u8282\u70b9\u5bf9\u534f\u8bae\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u5de5\u4f5c\u5c55\u793a\u4e86\u5f62\u5f0f\u5316\u65b9\u6cd5\u5728\u5206\u6790\u533a\u5757\u94fe\u5171\u8bc6\u7b97\u6cd5\u4e2d\u7684\u9644\u52a0\u4ef7\u503c\uff0c\u5f3a\u8c03\u4e86Algorand\u534f\u8bae\u5728\u5bf9\u6297\u6027\u5047\u8bbe\u4e0b\u7684\u7a33\u5065\u6027\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2508.19736", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.19736", "abs": "https://arxiv.org/abs/2508.19736", "authors": ["Mohsen Ahadi", "Adeel Malik", "Omid Esrafilian", "Florian Kaltenberger", "Cedric Thienot"], "title": "Experimental Insights from OpenAirInterface 5G positioning Testbeds: Challenges and solutions", "comment": "8 pages", "summary": "5G New Radio (NR) is a key enabler of accurate positioning in smart cities\nand smart factories. This paper presents the experimental results from three 5G\npositioning testbeds running open-source OpenAirInterface (OAI) gNB and Core\nNetwork (CN), using Uplink Time Difference of Arrival (UL-TDoA) with the newly\nintegrated Location Management Function (LMF). The testbeds are deployed across\nboth indoor factories and outdoor scenarios with O-RAN Radio Units (RUs),\nfollowing a 3GPP-compliant system model. The experiments highlight the impact\nof synchronization impairments, multipath propagation, and deployment geometry\non positioning accuracy. To address these challenges, we propose tailored ToA\nand TDoA filtering as well as a novel position estimation method based on\nParticle Swarm Optimization (PSO) within the LMF pipeline. Moreover, we show a\nbeyond-5G framework that leverages non-conventional measurements such as\nChannel Impulse Response (CIR) to train and test Artificial Intelligence and\nMachine Learning (AI/ML) models for data-driven positioning. The results\ndemonstrate the feasibility of achieving 1-2 meter positioning accuracy in 90%\nof cases in different testbeds, offering practical insights for the design of\nrobust 5G positioning systems. Moreover, we publicly release the datasets\ncollected in this work to support the research within the 5G positioning\ncommunity.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e865G NR\u4e2dUL-TDoA\u548cLMF\u7684\u5b9a\u4f4d\u6027\u80fd\uff0c\u63d0\u51faPSO\u548cAI/ML\u65b9\u6cd5\u63d0\u5347\u7cbe\u5ea6\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u3002", "motivation": "5G NR\u662f\u5b9e\u73b0\u667a\u80fd\u57ce\u5e02\u548c\u5de5\u5382\u7cbe\u786e\u5b9a\u4f4d\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5b58\u5728\u540c\u6b65\u8bef\u5dee\u3001\u591a\u5f84\u4f20\u64ad\u548c\u90e8\u7f72\u51e0\u4f55\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8ePSO\u7684\u4f4d\u7f6e\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5229\u7528CIR\u8bad\u7ec3AI/ML\u6a21\u578b\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u5b9a\u4f4d\u3002", "result": "\u5728\u4e0d\u540c\u6d4b\u8bd5\u5e8a\u4e2d90%\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e861-2\u7c73\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "\u8bba\u6587\u5c55\u793a\u4e86\u57285G NR\u7f51\u7edc\u4e2d\u901a\u8fc7UL-TDoA\u548cLMF\u5b9e\u73b01-2\u7c73\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u53ef\u884c\u6027\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.19558", "categories": ["cs.SE", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.19558", "abs": "https://arxiv.org/abs/2508.19558", "authors": ["Zhuohao Li", "Wenqing Chen", "Jianxing Yu", "Zhichao Lu"], "title": "Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking", "comment": null, "summary": "Embedding models have demonstrated strong performance in tasks like\nclustering, retrieval, and feature extraction while offering computational\nadvantages over generative models and cross-encoders. Benchmarks such as MTEB\nhave shown that text embeddings from large language models (LLMs) capture rich\nsemantic information, but their ability to reflect code-level functional\nsemantics remains unclear. Existing studies largely focus on code clone\ndetection, which emphasizes syntactic similarity and overlooks functional\nunderstanding. In this paper, we focus on the functional consistency of LLM\ncode embeddings, which determines if two code snippets perform the same\nfunction regardless of syntactic differences. We propose a novel data synthesis\nframework called Functionality-Oriented Code Self-Evolution to construct\ndiverse and challenging benchmarks. Specifically, we define code examples\nacross four semantic and syntactic categories and find that existing datasets\npredominantly capture syntactic properties. Our framework generates four unique\nvariations from a single code instance, providing a broader spectrum of code\nexamples that better reflect functional differences. Extensive experiments on\nthree downstream tasks-code clone detection, code functional consistency\nidentification, and code retrieval-demonstrate that embedding models\nsignificantly improve their performance when trained on our evolved datasets.\nThese results highlight the effectiveness and generalization of our data\nsynthesis framework, advancing the functional understanding of code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7801\u5d4c\u5165\u7684\u529f\u80fd\u4e00\u81f4\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u5d4c\u5165\u6a21\u578b\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\uff0c\u5f3a\u8c03\u8bed\u6cd5\u76f8\u4f3c\u6027\u800c\u5ffd\u89c6\u4e86\u529f\u80fd\u7406\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7801\u5d4c\u5165\u5728\u529f\u80fd\u4e00\u81f4\u6027\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u2018\u529f\u80fd\u5bfc\u5411\u7684\u4ee3\u7801\u81ea\u8fdb\u5316\u2019\u7684\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u591a\u6837\u5316\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u8be5\u6846\u67b6\u4ece\u4e00\u4e2a\u4ee3\u7801\u5b9e\u4f8b\u751f\u6210\u56db\u79cd\u72ec\u7279\u7684\u53d8\u4f53\uff0c\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u529f\u80fd\u5dee\u5f02\u3002", "result": "\u5728\u4e09\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff08\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u3001\u4ee3\u7801\u529f\u80fd\u4e00\u81f4\u6027\u8bc6\u522b\u548c\u4ee3\u7801\u68c0\u7d22\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5d4c\u5165\u6a21\u578b\u5728\u8bad\u7ec3\u4e8e\u8fdb\u5316\u540e\u7684\u6570\u636e\u96c6\u65f6\u6027\u80fd\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u5d4c\u5165\u6a21\u578b\u5728\u4ee3\u7801\u529f\u80fd\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u4e8e\u6211\u4eec\u63d0\u51fa\u7684\u6570\u636e\u5408\u6210\u6846\u67b6\u540e\uff0c\u8fdb\u4e00\u6b65\u63a8\u8fdb\u4e86\u4ee3\u7801\u529f\u80fd\u7406\u89e3\u7684\u9886\u57df\u3002"}}
{"id": "2508.19785", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.19785", "abs": "https://arxiv.org/abs/2508.19785", "authors": ["Barbara Geissmann", "Stefano Leucci", "Chih-Hung Liu", "Paolo Penna"], "title": "An Optimal Sorting Algorithm for Persistent Random Comparison Faults", "comment": null, "summary": "We consider the problem of sorting $n$ elements subject to persistent random\ncomparison errors. In this problem, each comparison between two elements can be\nwrong with some fixed (small) probability $p$, and comparing the same pair of\nelements multiple times always yields the same result. Sorting perfectly in\nthis model is impossible, and the objective is to minimize the dislocation of\neach element in the output sequence, i.e., the difference between its position\nin the sequence and its true rank.\n  In this paper, we present the first $O(n\\log n)$-time sorting algorithm that\nguarantees both $O(\\log n)$ maximum dislocation and $O(n)$ total dislocation\nwith high probability when $p<\\frac{1}{4}$. This settles the time complexity\nsorting with persistent comparison errors in the given range of $p$ and shows\nthat comparison errors do not increase its computational difficulty. Indeed,\n$\\Omega(n\\log n)$ time is necessary to archive a maximum dislocation of $O(\\log\nn)$ even without comparison errors. Moreover, we prove that no algorithm can\nguarantee a maximum dislocation of $o(\\log n)$ with high probability, nor a\ntotal dislocation of $o(n)$ in expectation.\n  To develop our sorting algorithm, we solve two related sub-problems, which\nmight be of independent interest. More precisely, we show that $O(\\log n)$ time\nsuffices to find a position in which to insert a new element $x$ in an\nalmost-sorted sequence $S$ of $n$ elements having dislocation at most\n$d=\\Omega(\\log n)$, so that the dislocation of $x$ in the resulting sequence is\n$O(d)$ with high probability (which can be equivalently thought as the problem\nof estimating the rank of $x$ in $S$). We also show that the maximum (resp.\ntotal) dislocation of an approximately sorted sequence $S$ of $n$ elements can\nbe lowered to $O(\\log n)$ (resp. $O(n)$) in $O(nd)$ time, w.h.p., where $d$ is\nan upper bound on the maximum dislocation of $S$.", "AI": {"tldr": "\u9996\u4e2aO(n log n)\u65f6\u95f4\u6392\u5e8f\u7b97\u6cd5\uff0c\u5728p < 1/4\u65f6\u4fdd\u8bc1\u9ad8\u6982\u7387\u7684O(log n)\u6700\u5927\u4f4d\u79fb\u548cO(n)\u603b\u4f4d\u79fb\uff0c\u89e3\u51b3\u4e86\u6301\u4e45\u6bd4\u8f83\u9519\u8bef\u4e0b\u7684\u6392\u5e8f\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5728\u6301\u4e45\u968f\u673a\u6bd4\u8f83\u9519\u8bef\u4e0b\u6392\u5e8f\u7684\u95ee\u9898\uff0c\u76ee\u6807\u662f\u6700\u5c0f\u5316\u8f93\u51fa\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u4f4d\u79fb\u3002", "method": "\u901a\u8fc7\u89e3\u51b3\u4e24\u4e2a\u76f8\u5173\u7684\u5b50\u95ee\u9898\uff1a\u5728\u51e0\u4e4e\u6392\u5e8f\u7684\u5e8f\u5217\u4e2d\u9ad8\u6548\u63d2\u5165\u65b0\u5143\u7d20\u4ee5\u63a7\u5236\u4f4d\u79fb\uff0c\u4ee5\u53ca\u964d\u4f4e\u8fd1\u4f3c\u6392\u5e8f\u5e8f\u5217\u7684\u6700\u5927\u548c\u603b\u4f4d\u79fb\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728O(n log n)\u65f6\u95f4\u5185\u5b9e\u73b0\u4e86O(log n)\u6700\u5927\u4f4d\u79fb\u548cO(n)\u603b\u4f4d\u79fb\uff0c\u4e14\u8bc1\u660e\u4e86\u65e0\u6cd5\u4fdd\u8bc1\u66f4\u9ad8\u7cbe\u5ea6\u7684\u4f4d\u79fb\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(n log n)\u7684\u6392\u5e8f\u7b97\u6cd5\uff0c\u5728p < 1/4\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u4ee5\u9ad8\u6982\u7387\u4fdd\u8bc1\u6700\u5927\u4f4d\u79fb\u4e3aO(log n)\u548c\u603b\u4f4d\u79fb\u4e3aO(n)\u3002\u8fd9\u89e3\u51b3\u4e86\u5728\u7ed9\u5b9ap\u8303\u56f4\u5185\u6392\u5e8f\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u8868\u660e\u6bd4\u8f83\u9519\u8bef\u4e0d\u4f1a\u589e\u52a0\u5176\u8ba1\u7b97\u96be\u5ea6\u3002"}}
{"id": "2508.19257", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19257", "abs": "https://arxiv.org/abs/2508.19257", "authors": ["Chenghao Liu", "Jiachen Zhang", "Chengxuan Li", "Zhimu Zhou", "Shixin Wu", "Songfang Huang", "Huiling Duan"], "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models", "comment": "Manuscript submitted to AAAI 2026, currently under review", "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.", "AI": {"tldr": "TTF\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65f6\u95f4\u4ee4\u724c\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5386\u53f2\u4e0e\u5f53\u524d\u89c6\u89c9\u8868\u793a\u7684\u7ed3\u5408\u63d0\u5347VLA\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u8de8\u73af\u5883\u548c\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9010\u5e27\u5904\u7406\u89c6\u89c9\u8f93\u5165\uff0c\u5ffd\u7565\u4e86\u65f6\u95f4\u8fde\u8d2f\u6027\uff0c\u6613\u53d7\u89c6\u89c9\u566a\u58f0\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86Temporal Token Fusion (TTF)\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5386\u53f2\u4e0e\u5f53\u524d\u89c6\u89c9\u8868\u793a\uff0c\u91c7\u7528\u53cc\u7ef4\u5ea6\u68c0\u6d4b\uff08\u7070\u5ea6\u50cf\u7d20\u5dee\u5f02\u5206\u6790\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8bed\u4e49\u76f8\u5173\u6027\u8bc4\u4f30\uff09\u548c\u786c\u878d\u5408\u7b56\u7565\u53ca\u5173\u952e\u5e27\u951a\u5b9a\u6765\u589e\u5f3a\u63a8\u7406\u8d28\u91cf\u3002", "result": "\u5728LIBERO\u3001SimplerEnv\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0cTTF\u5206\u522b\u5b9e\u73b0\u4e864.0\u30014.8\u548c8.7%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u6a21\u578b\u65e0\u5173\u6027\u3002", "conclusion": "TTF\u65b9\u6cd5\u901a\u8fc7\u9009\u62e9\u6027\u65f6\u95f4\u4ee4\u724c\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8de8\u73af\u5883\u548c\u8de8\u67b6\u6784\u4e2d\u7684\u901a\u7528\u6027\uff0c\u5e76\u4e3aKQV\u77e9\u9635\u91cd\u7528\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.19383", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19383", "abs": "https://arxiv.org/abs/2508.19383", "authors": ["Daoyuan Jin", "Nick Gunner", "Niko Carvajal Janke", "Shivranjani Baruah", "Kaitlin M. Gold", "Yu Jiang"], "title": "Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science", "comment": null, "summary": "Modern plant science increasingly relies on large, heterogeneous datasets,\nbut challenges in experimental design, data preprocessing, and reproducibility\nhinder research throughput. Here we introduce Aleks, an AI-powered multi-agent\nsystem that integrates domain knowledge, data analysis, and machine learning\nwithin a structured framework to autonomously conduct data-driven scientific\ndiscovery. Once provided with a research question and dataset, Aleks\niteratively formulated problems, explored alternative modeling strategies, and\nrefined solutions across multiple cycles without human intervention. In a case\nstudy on grapevine red blotch disease, Aleks progressively identified\nbiologically meaningful features and converged on interpretable models with\nrobust performance. Ablation studies underscored the importance of domain\nknowledge and memory for coherent outcomes. This exploratory work highlights\nthe promise of agentic AI as an autonomous collaborator for accelerating\nscientific discovery in plant sciences.", "AI": {"tldr": "Aleks\u662f\u4e00\u4e2aAI\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u4e3b\u8fdb\u884c\u690d\u7269\u79d1\u5b66\u7814\u7a76\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u4ee3\u690d\u7269\u79d1\u5b66\u65e5\u76ca\u4f9d\u8d56\u5927\u578b\u3001\u5f02\u6784\u6570\u636e\u96c6\uff0c\u4f46\u5b9e\u9a8c\u8bbe\u8ba1\u3001\u6570\u636e\u9884\u5904\u7406\u548c\u53ef\u91cd\u590d\u6027\u65b9\u9762\u7684\u6311\u6218\u963b\u788d\u4e86\u7814\u7a76\u6548\u7387\u3002", "method": "Aleks\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u9886\u57df\u77e5\u8bc6\u3001\u6570\u636e\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6846\u67b6\u81ea\u4e3b\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u7684\u79d1\u5b66\u53d1\u73b0\u3002", "result": "\u5728\u4e00\u9879\u5173\u4e8e\u8461\u8404\u85e4\u7ea2\u6591\u75c5\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cAleks\u9010\u6b65\u8bc6\u522b\u51fa\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u7279\u5f81\uff0c\u5e76\u6536\u655b\u4e8e\u5177\u6709\u7a33\u5065\u6027\u80fd\u7684\u53ef\u89e3\u91ca\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u63a2\u7d22\u6027\u5de5\u4f5c\u5c55\u793a\u4e86\u4ee3\u7406\u5f0fAI\u4f5c\u4e3a\u81ea\u4e3b\u5408\u4f5c\u8005\u5728\u52a0\u901f\u690d\u7269\u79d1\u5b66\u9886\u57df\u7684\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.19476", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19476", "abs": "https://arxiv.org/abs/2508.19476", "authors": ["Dane Brouwer", "Joshua Citron", "Heather Nolte", "Jeannette Bohg", "Mark Cutkosky"], "title": "Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning", "comment": "Submitted to IEEE Robotics and Automation Letters (RA-L)", "summary": "Dense collections of movable objects are common in everyday spaces -- from\ncabinets in a home to shelves in a warehouse. Safely retracting objects from\nsuch collections is difficult for robots, yet people do it easily, using\nnon-prehensile tactile sensing on the sides and backs of their hands and arms.\nWe investigate the role of such sensing for training robots to gently reach\ninto constrained clutter and extract objects. The available sensing modalities\nare (1) \"eye-in-hand\" vision, (2) proprioception, (3) non-prehensile triaxial\ntactile sensing, (4) contact wrenches estimated from joint torques, and (5) a\nmeasure of successful object acquisition obtained by monitoring the vacuum line\nof a suction cup. We use imitation learning to train policies from a set of\ndemonstrations on randomly generated scenes, then conduct an ablation study of\nwrench and tactile information. We evaluate each policy's performance across 40\nunseen environment configurations. Policies employing any force sensing show\nfewer excessive force failures, an increased overall success rate, and faster\ncompletion times. The best performance is achieved using both tactile and\nwrench information, producing an 80% improvement above the baseline without\nforce information.", "AI": {"tldr": "\u7814\u7a76\u673a\u5668\u4eba\u5229\u7528\u89e6\u89c9\u548c\u626d\u77e9\u4fe1\u606f\u4ece\u5bc6\u96c6\u7269\u4f53\u4e2d\u5b89\u5168\u63d0\u53d6\u7269\u4f53\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e24\u8005\u7684\u7b56\u7565\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7814\u7a76\u975e\u6293\u53d6\u89e6\u89c9\u4f20\u611f\u5728\u673a\u5668\u4eba\u4ece\u5bc6\u96c6\u7269\u4f53\u96c6\u5408\u4e2d\u5b89\u5168\u63d0\u53d6\u7269\u4f53\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u6a21\u4eff\u5b66\u4e60\u4ece\u968f\u673a\u751f\u6210\u7684\u573a\u666f\u4e2d\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u8fdb\u884c\u626d\u77e9\u548c\u89e6\u89c9\u4fe1\u606f\u7684\u6d88\u878d\u7814\u7a76\u3002", "result": "\u91c7\u7528\u4efb\u4f55\u529b\u4f20\u611f\u7684\u7b56\u7565\u8868\u73b0\u51fa\u66f4\u5c11\u7684\u8fc7\u5ea6\u529b\u5931\u8d25\u3001\u66f4\u9ad8\u7684\u6574\u4f53\u6210\u529f\u7387\u548c\u66f4\u5feb\u7684\u5b8c\u6210\u65f6\u95f4\u3002", "conclusion": "\u7ed3\u5408\u89e6\u89c9\u548c\u626d\u77e9\u4fe1\u606f\u7684\u7b56\u7565\u8868\u73b0\u6700\u4f73\uff0c\u76f8\u6bd4\u4e0d\u91c7\u7528\u529b\u4fe1\u606f\u7684\u57fa\u7ebf\u670980%\u7684\u63d0\u5347\u3002"}}
{"id": "2508.19495", "categories": ["cs.DC", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19495", "abs": "https://arxiv.org/abs/2508.19495", "authors": ["Muhammad Ahmed Mohsin", "Junaid Ahmad", "Muhammad Hamza Nawaz", "Muhammad Ali Jamshed"], "title": "Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks", "comment": "Submitted as a chapter to the book Ambient Intelligence for 6G", "summary": "Ambient intelligence (AmI) is a computing paradigm in which physical\nenvironments are embedded with sensing, computation, and communication so they\ncan perceive people and context, decide appropriate actions, and respond\nautonomously. Realizing AmI at global scale requires sixth generation (6G)\nwireless networks with capabilities for real time perception, reasoning, and\naction aligned with human behavior and mobility patterns. We argue that\nGenerative Artificial Intelligence (GenAI) is the creative core of such\nenvironments. Unlike traditional AI, GenAI learns data distributions and can\ngenerate realistic samples, making it well suited to close key AmI gaps,\nincluding generating synthetic sensor and channel data in under observed areas,\ntranslating user intent into compact, semantic messages, predicting future\nnetwork conditions for proactive control, and updating digital twins without\ncompromising privacy.\n  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,\nand generative transformers, and connects them to practical AmI use cases,\nincluding spectrum sharing, ultra reliable low latency communication,\nintelligent security, and context aware digital twins. We also examine how 6G\nenablers, such as edge and fog computing, IoT device swarms, intelligent\nreflecting surfaces (IRS), and non terrestrial networks, can host or accelerate\ndistributed GenAI. Finally, we outline open challenges in energy efficient on\ndevice training, trustworthy synthetic data, federated generative learning, and\nAmI specific standardization. We show that GenAI is not a peripheral addition,\nbut a foundational element for transforming 6G from a faster network into an\nambient intelligent ecosystem.", "AI": {"tldr": "GenAI\u662f6G\u5b9e\u73b0\u73af\u5883\u667a\u80fd\u7684\u6838\u5fc3\uff0c\u80fd\u586b\u8865AmI\u7684\u5173\u952e\u6280\u672f\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u6a21\u578b\u548c\u7528\u4f8b\u63a8\u52a86G\u53d1\u5c55\u3002", "motivation": "\u5b9e\u73b0\u5168\u7403\u8303\u56f4\u5185\u7684\u73af\u5883\u667a\u80fd\uff08AmI\uff09\u9700\u89816G\u65e0\u7ebf\u7f51\u7edc\u5177\u5907\u5b9e\u65f6\u611f\u77e5\u3001\u63a8\u7406\u548c\u884c\u52a8\u80fd\u529b\uff0c\u4e14\u9700\u4e0e\u4eba\u7c7b\u884c\u4e3a\u548c\u79fb\u52a8\u6a21\u5f0f\u4fdd\u6301\u4e00\u81f4\u3002GenAI\u56e0\u5176\u80fd\u5b66\u4e60\u6570\u636e\u5206\u5e03\u5e76\u751f\u6210\u903c\u771f\u6837\u672c\uff0c\u88ab\u89c6\u4e3a\u586b\u8865AmI\u5173\u952e\u7a7a\u767d\u7684\u7406\u60f3\u9009\u62e9\u3002", "method": "\u672c\u7ae0\u56de\u987e\u4e86\u57fa\u7840\u7684GenAI\u6a21\u578b\uff08\u5982GANs\u3001VAEs\u3001\u6269\u6563\u6a21\u578b\u548c\u751f\u6210\u578b\u53d8\u538b\u5668\uff09\uff0c\u5e76\u5c06\u5176\u4e0e\u5b9e\u9645AmI\u7528\u4f8b\uff08\u5982\u9891\u8c31\u5171\u4eab\u3001\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u3001\u667a\u80fd\u5b89\u5168\u548c\u60c5\u5883\u611f\u77e5\u6570\u5b57\u5b6a\u751f\uff09\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cGenAI\u80fd\u591f\u751f\u6210\u5408\u6210\u4f20\u611f\u5668\u548c\u4fe1\u9053\u6570\u636e\u3001\u5c06\u7528\u6237\u610f\u56fe\u8f6c\u5316\u4e3a\u7d27\u51d1\u7684\u8bed\u4e49\u6d88\u606f\u3001\u9884\u6d4b\u672a\u6765\u7f51\u7edc\u6761\u4ef6\u4ee5\u5b9e\u73b0\u4e3b\u52a8\u63a7\u5236\uff0c\u4ee5\u53ca\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u66f4\u65b0\u6570\u5b57\u5b6a\u751f\u3002", "conclusion": "GenAI\u88ab\u89c6\u4e3a\u5c066G\u4ece\u66f4\u5feb\u7684\u7f51\u7edc\u8f6c\u53d8\u4e3a\u73af\u5883\u667a\u80fd\u751f\u6001\u7cfb\u7edf\u7684\u6838\u5fc3\u8981\u7d20\uff0c\u800c\u975e\u5916\u56f4\u8865\u5145\u3002"}}
{"id": "2508.19870", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.19870", "abs": "https://arxiv.org/abs/2508.19870", "authors": ["Yinqiu Liu", "Ruichen Zhang", "Haoxiang Luo", "Yijing Lin", "Geng Sun", "Dusit Niyato", "Hongyang Du", "Zehui Xiong", "Yonggang Wen", "Abbas Jamalipour", "Dong In Kim", "Ping Zhang"], "title": "Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey", "comment": "35 pages", "summary": "Agentification serves as a critical enabler of Edge General Intelligence\n(EGI), transforming massive edge devices into cognitive agents through\nintegrating Large Language Models (LLMs) and perception, reasoning, and acting\nmodules. These agents collaborate across heterogeneous edge infrastructures,\nforming multi-LLM agentic AI systems that leverage collective intelligence and\nspecialized capabilities to tackle complex, multi-step tasks. However, the\ncollaborative nature of multi-LLM systems introduces critical security\nvulnerabilities, including insecure inter-LLM communications, expanded attack\nsurfaces, and cross-domain data leakage that traditional perimeter-based\nsecurity cannot adequately address. To this end, this survey introduces\nzero-trust security of multi-LLM in EGI, a paradigmatic shift following the\n``never trust, always verify'' principle. We begin by systematically analyzing\nthe security risks in multi-LLM systems within EGI contexts. Subsequently, we\npresent the vision of a zero-trust multi-LLM framework in EGI. We then survey\nkey technical progress to facilitate zero-trust multi-LLM systems in EGI.\nParticularly, we categorize zero-trust security mechanisms into model- and\nsystem-level approaches. The former and latter include strong identification,\ncontext-aware access control, etc., and proactive maintenance, blockchain-based\nmanagement, etc., respectively. Finally, we identify critical research\ndirections. This survey serves as the first systematic treatment of zero-trust\napplied to multi-LLM systems, providing both theoretical foundations and\npractical strategies.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591aLLM\u7cfb\u7edf\u5728\u8fb9\u7f18\u901a\u7528\u667a\u80fd\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51fa\u5e76\u5206\u7c7b\u4e86\u96f6\u4fe1\u4efb\u5b89\u5168\u673a\u5236\uff0c\u4e3a\u7406\u8bba\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u591aLLM\u7cfb\u7edf\u7684\u534f\u4f5c\u7279\u6027\u5f15\u5165\u4e86\u5173\u952e\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u4f20\u7edf\u57fa\u4e8e\u8fb9\u754c\u7684\u5b89\u5168\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\uff0c\u56e0\u6b64\u9700\u8981\u96f6\u4fe1\u4efb\u5b89\u5168\u8303\u5f0f\u3002", "method": "\u901a\u8fc7\u5206\u6790\u591aLLM\u7cfb\u7edf\u5728EGI\u73af\u5883\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u96f6\u4fe1\u4efb\u591aLLM\u6846\u67b6\u7684\u613f\u666f\uff0c\u5e76\u5206\u7c7b\u8c03\u67e5\u4e86\u6a21\u578b\u7ea7\u548c\u7cfb\u7edf\u7ea7\u7684\u96f6\u4fe1\u4efb\u5b89\u5168\u673a\u5236\u3002", "result": "\u63d0\u51fa\u4e86\u96f6\u4fe1\u4efb\u591aLLM\u6846\u67b6\u7684\u613f\u666f\uff0c\u5e76\u5206\u7c7b\u603b\u7ed3\u4e86\u6a21\u578b\u7ea7\uff08\u5982\u5f3a\u8eab\u4efd\u8bc6\u522b\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u8bbf\u95ee\u63a7\u5236\uff09\u548c\u7cfb\u7edf\u7ea7\uff08\u5982\u4e3b\u52a8\u7ef4\u62a4\u3001\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u7ba1\u7406\uff09\u7684\u6280\u672f\u8fdb\u5c55\u3002", "conclusion": "\u672c\u7efc\u8ff0\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u5c06\u96f6\u4fe1\u4efb\u5b89\u5168\u5e94\u7528\u4e8e\u591aLLM\u7cfb\u7edf\uff0c\u4e3a\u8fb9\u7f18\u901a\u7528\u667a\u80fd\uff08EGI\uff09\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u7b56\u7565\u3002"}}
{"id": "2508.19610", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19610", "abs": "https://arxiv.org/abs/2508.19610", "authors": ["Kathrin Figl", "Maria Kirchner", "Sebastian Baltes", "Michael Felderer"], "title": "The Influence of Code Comments on the Perceived Helpfulness of Stack Overflow Posts", "comment": "31 pages, 7 figures, 2 tables, to appear in the Empirical Software\n  Engineering journal", "summary": "Question-and-answer platforms such as Stack Overflow have become an important\nway for software developers to share and retrieve knowledge. However, reusing\npoorly understood code can lead to serious problems, such as bugs or security\nvulnerabilities. To better understand how code comments affect the perceived\nhelpfulness of Stack Overflow answers, we conducted an online experiment\nsimulating a Stack Overflow environment (n=91). The results indicate that both\nblock and inline comments are perceived as significantly more helpful than\nuncommented source code. Moreover, novices rated code snippets with block\ncomments as more helpful than those with inline comments. Interestingly, other\nsurface features, such as the position of an answer and its answer score, were\nconsidered less important. The content of Stack Overflow has been a major\nsource for training large language models. AI-based coding assistants such as\nGitHub Copilot, which are based on these models, might change the way Stack\nOverflow is used. However, our findings have implications beyond this specific\nplatform. First, they may help to improve the relevance of community-driven\nplatforms such as Stack Overflow, which provide human advice and explanations\nof code solutions, complementing AI-based support for software developers.\nSecond, since chat-based AI tools can be prompted to generate code in different\nways, knowing which properties influence perceived helpfulness might lead to\ntargeted prompting strategies to generate more readable code snippets.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4ee3\u7801\u6ce8\u91ca\uff08\u5c24\u5176\u662f\u5757\u6ce8\u91ca\uff09\u80fd\u663e\u8457\u63d0\u5347Stack Overflow\u7b54\u6848\u7684\u611f\u77e5\u5e2e\u52a9\u6027\uff0c\u5bf9\u793e\u533a\u5e73\u53f0\u548cAI\u5de5\u5177\u8bbe\u8ba1\u6709\u53c2\u8003\u4ef7\u503c\u3002", "motivation": "\u63a2\u7d22\u4ee3\u7801\u6ce8\u91ca\u5982\u4f55\u5f71\u54cd\u5f00\u53d1\u8005\u5bf9Stack Overflow\u7b54\u6848\u7684\u611f\u77e5\u5e2e\u52a9\u6027\uff0c\u4ee5\u63d0\u5347\u793e\u533a\u5e73\u53f0\u548cAI\u5de5\u5177\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u5b9e\u9a8c\u6a21\u62dfStack Overflow\u73af\u5883\uff08n=91\uff09\uff0c\u6bd4\u8f83\u4e0d\u540c\u6ce8\u91ca\u7c7b\u578b\uff08\u5757\u6ce8\u91ca\u3001\u5185\u8054\u6ce8\u91ca\u3001\u65e0\u6ce8\u91ca\uff09\u5bf9\u7b54\u6848\u5e2e\u52a9\u6027\u7684\u5f71\u54cd\u3002", "result": "\u5757\u6ce8\u91ca\u548c\u5185\u8054\u6ce8\u91ca\u5747\u88ab\u8ba4\u4e3a\u6bd4\u65e0\u6ce8\u91ca\u4ee3\u7801\u66f4\u6709\u5e2e\u52a9\uff0c\u4e14\u65b0\u624b\u66f4\u504f\u597d\u5757\u6ce8\u91ca\u3002\u5176\u4ed6\u8868\u9762\u7279\u5f81\uff08\u5982\u7b54\u6848\u4f4d\u7f6e\u548c\u5f97\u5206\uff09\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "\u4ee3\u7801\u6ce8\u91ca\uff08\u5c24\u5176\u662f\u5757\u6ce8\u91ca\uff09\u663e\u8457\u63d0\u9ad8\u4e86Stack Overflow\u7b54\u6848\u7684\u611f\u77e5\u5e2e\u52a9\u6027\uff0c\u8fd9\u5bf9\u793e\u533a\u9a71\u52a8\u5e73\u53f0\u548cAI\u7f16\u7801\u52a9\u624b\u7684\u8bbe\u8ba1\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2508.19802", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.19802", "abs": "https://arxiv.org/abs/2508.19802", "authors": ["Alexander Dobler", "Tim Hegemann", "Martin N\u00f6llenburg", "Alexander Wolff"], "title": "Optimizing Wiggle in Storylines", "comment": "23 pages, 15 figures", "summary": "A storyline visualization shows interactions between characters over time.\nEach character is represented by an x-monotone curve. Time is mapped to the\nx-axis, and groups of characters that interact at a particular point $t$ in\ntime must be ordered consecutively in the y-dimension at $x=t$. The predominant\nobjective in storyline optimization so far has been the minimization of\ncrossings between (blocks of) characters. Building on this work, we investigate\nanother important, but less studied quality criterion, namely the minimization\nof wiggle, i.e., the amount of vertical movement of the characters over time.\nGiven a storyline instance together with an ordering of the characters at any\npoint in time, we show that wiggle count minimization is NP-complete. In\ncontrast, we provide algorithms based on mathematical programming to solve\nlinear wiggle height minimization and quadratic wiggle height minimization\nefficiently. Finally, we introduce a new method for routing character curves\nthat focuses on keeping distances between neighboring curves constant as long\nas they run in parallel. We have implemented our algorithms, and we conduct a\ncase study that explores the differences between the three optimization\nobjectives. We use existing benchmark data, but we also present a new use case\nfor storylines, namely the visualization of rolling stock schedules in railway\noperation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6545\u4e8b\u7ebf\u53ef\u89c6\u5316\u4e2d\u7684\u6446\u52a8\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u8bc1\u660e\u6446\u52a8\u8ba1\u6570\u6700\u5c0f\u5316\u662fNP\u5b8c\u5168\u7684\uff0c\u5e76\u63d0\u51fa\u6709\u6548\u7684\u6570\u5b66\u7f16\u7a0b\u7b97\u6cd5\u6765\u89e3\u51b3\u7ebf\u6027\u6446\u52a8\u9ad8\u5ea6\u6700\u5c0f\u5316\u548c\u4e8c\u6b21\u6446\u52a8\u9ad8\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4f18\u5316\u6545\u4e8b\u7ebf\u53ef\u89c6\u5316\u4e2d\u7684\u6446\u52a8\uff08\u5373\u89d2\u8272\u968f\u65f6\u95f4\u7684\u5782\u76f4\u79fb\u52a8\u91cf\uff09\u8fd9\u4e00\u91cd\u8981\u4f46\u8f83\u5c11\u7814\u7a76\u7684\u8d28\u91cf\u6307\u6807\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86\u6570\u5b66\u7f16\u7a0b\u7b97\u6cd5\u6765\u89e3\u51b3\u7ebf\u6027\u6446\u52a8\u9ad8\u5ea6\u6700\u5c0f\u5316\u548c\u4e8c\u6b21\u6446\u52a8\u9ad8\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89d2\u8272\u66f2\u7ebf\u8def\u7531\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660e\u6446\u52a8\u8ba1\u6570\u6700\u5c0f\u5316\u95ee\u9898\u662fNP\u5b8c\u5168\u7684\uff0c\u4f46\u901a\u8fc7\u6570\u5b66\u7f16\u7a0b\u7b97\u6cd5\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u7ebf\u6027\u6446\u52a8\u9ad8\u5ea6\u6700\u5c0f\u5316\u548c\u4e8c\u6b21\u6446\u52a8\u9ad8\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u6570\u5b66\u7f16\u7a0b\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7ebf\u6027\u6446\u52a8\u9ad8\u5ea6\u6700\u5c0f\u5316\u548c\u4e8c\u6b21\u6446\u52a8\u9ad8\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89d2\u8272\u66f2\u7ebf\u8def\u7531\u65b9\u6cd5\uff0c\u5f3a\u8c03\u5728\u5e73\u884c\u8fd0\u884c\u65f6\u4fdd\u6301\u76f8\u90bb\u66f2\u7ebf\u95f4\u8ddd\u79bb\u6052\u5b9a\u3002"}}
{"id": "2508.19289", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19289", "abs": "https://arxiv.org/abs/2508.19289", "authors": ["Tai Inui", "Steven Oh", "Magdeline Kuan"], "title": "Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation", "comment": "6 pages", "summary": "We present an unsupervised slide-quality assessment pipeline that combines\nseven expert-inspired visual-design metrics (whitespace, colorfulness, edge\ndensity, brightness contrast, text density, color harmony, layout balance) with\nCLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate\npresentation slides. Trained on 12k professional lecture slides and evaluated\non six academic talks (115 slides), our method achieved Pearson correlations up\nto 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores\nfrom leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude\nSonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual\nratings, discriminant validity against speaker-delivery scores, and exploratory\nalignment with overall impressions. Our results show that augmenting low-level\ndesign cues with multimodal embeddings closely approximates audience\nperceptions of slide quality, enabling scalable, objective feedback in real\ntime.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e13\u5bb6\u89c6\u89c9\u6307\u6807\u4e0eCLIP-ViT\u5d4c\u5165\u7684\u65e0\u76d1\u7763\u5e7b\u706f\u7247\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u53cd\u9988\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5e7b\u706f\u7247\u8d28\u91cf\u8bc4\u4f30\u7684\u5ba2\u89c2\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e13\u5bb6\u542f\u53d1\u6307\u6807\u4e0e\u591a\u6a21\u6001\u5d4c\u5165\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4e03\u4e2a\u4e13\u5bb6\u542f\u53d1\u7684\u89c6\u89c9\u8bbe\u8ba1\u6307\u6807\uff08\u5982\u7a7a\u767d\u3001\u8272\u5f69\u4e30\u5bcc\u5ea6\u3001\u8fb9\u7f18\u5bc6\u5ea6\u7b49\uff09\u4e0eCLIP-ViT\u5d4c\u5165\uff0c\u4f7f\u7528Isolation Forest\u8fdb\u884c\u5f02\u5e38\u8bc4\u5206\u3002", "result": "\u572812k\u4e13\u4e1a\u8bb2\u5ea7\u5e7b\u706f\u7247\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728115\u5f20\u5b66\u672f\u6f14\u8bb2\u5e7b\u706f\u7247\u4e0a\u8bc4\u4f30\uff0cPearson\u76f8\u5173\u7cfb\u6570\u6700\u9ad8\u8fbe0.83\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4e13\u5bb6\u542f\u53d1\u7684\u89c6\u89c9\u8bbe\u8ba1\u6307\u6807\u4e0eCLIP-ViT\u5d4c\u5165\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eIsolation Forest\u7684\u5f02\u5e38\u8bc4\u5206\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u5e7b\u706f\u7247\u8d28\u91cf\uff0c\u4e3a\u5b9e\u65f6\u3001\u53ef\u6269\u5c55\u7684\u5ba2\u89c2\u53cd\u9988\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2508.19432", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19432", "abs": "https://arxiv.org/abs/2508.19432", "authors": ["Yao Fu", "Xianxuan Long", "Runchao Li", "Haotian Yu", "Mu Sheng", "Xiaotian Han", "Yu Yin", "Pan Li"], "title": "Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs", "comment": "Accepted to EMNLP2025 main conference (poster)", "summary": "Quantization enables efficient deployment of large language models (LLMs) in\nresource-constrained environments by significantly reducing memory and\ncomputation costs. While quantized LLMs often maintain performance on\nperplexity and zero-shot tasks, their impact on truthfulness-whether generating\ntruthful or deceptive responses-remains largely unexplored. In this work, we\nintroduce TruthfulnessEval, a comprehensive evaluation framework for assessing\nthe truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on\nLogical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on\nImitative Falsehoods. Using this framework, we examine mainstream quantization\ntechniques (ranging from 4-bit to extreme 2-bit) across several open-source\nLLMs. Surprisingly, we find that while quantized models retain internally\ntruthful representations, they are more susceptible to producing false outputs\nunder misleading prompts. To probe this vulnerability, we test 15 rephrased\nvariants of \"honest\", \"neutral\" and \"deceptive\" prompts and observe that\n\"deceptive\" prompts can override truth-consistent behavior, whereas \"honest\"\nand \"neutral\" prompts maintain stable outputs. Further, we reveal that\nquantized models \"know\" the truth internally yet still produce false outputs\nwhen guided by \"deceptive\" prompts via layer-wise probing and PCA\nvisualizations. Our findings provide insights into future designs of\nquantization-aware alignment and truthfulness interventions.", "AI": {"tldr": "\u91cf\u5316LLM\u5728\u8bef\u5bfc\u6027\u63d0\u793a\u4e0b\u66f4\u6613\u4ea7\u751f\u865a\u5047\u8f93\u51fa\uff0c\u4f46\u5185\u90e8\u4fdd\u6301\u771f\u5b9e\u8868\u793a\u3002TruthfulnessEval\u6846\u67b6\u63ed\u793a\u4e86\u91cf\u5316\u6a21\u578b\u7684\u8fd9\u4e00\u8106\u5f31\u6027\uff0c\u4e3a\u672a\u6765\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u91cf\u5316\u6280\u672f\u867d\u80fd\u964d\u4f4eLLM\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u5176\u5bf9\u6a21\u578b\u751f\u6210\u5185\u5bb9\u771f\u5b9e\u6027\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u5f15\u5165TruthfulnessEval\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc4\u4f30\u91cf\u5316LLM\u5728\u903b\u8f91\u63a8\u7406\u3001\u5e38\u8bc6\u548c\u6a21\u4eff\u6027\u865a\u5047\u4e09\u4e2a\u7ef4\u5ea6\u7684\u771f\u5b9e\u6027\u3002\u6d4b\u8bd5\u4e86\u4e3b\u6d41\u91cf\u5316\u6280\u672f\uff084-bit\u5230\u6781\u7aef2-bit\uff09\u548c\u591a\u79cd\u5f00\u6e90LLM\uff0c\u5e76\u5206\u6790\u4e8615\u79cd\u91cd\u65b0\u8868\u8ff0\u7684\u63d0\u793a\u53d8\u4f53\u3002", "result": "\u91cf\u5316\u6a21\u578b\u5728\u8bef\u5bfc\u6027\u63d0\u793a\u4e0b\u66f4\u5bb9\u6613\u4ea7\u751f\u865a\u5047\u8f93\u51fa\uff0c\u800c\u2018\u8bda\u5b9e\u2019\u548c\u2018\u4e2d\u6027\u2019\u63d0\u793a\u80fd\u4fdd\u6301\u7a33\u5b9a\u8f93\u51fa\u3002\u5206\u5c42\u63a2\u6d4b\u548cPCA\u53ef\u89c6\u5316\u663e\u793a\uff0c\u91cf\u5316\u6a21\u578b\u5185\u90e8\u2018\u77e5\u9053\u2019\u771f\u76f8\u4f46\u4ecd\u4f1a\u53d7\u2018\u6b3a\u9a97\u6027\u2019\u63d0\u793a\u5f71\u54cd\u3002", "conclusion": "\u91cf\u5316\u6a21\u578b\u5728\u5185\u90e8\u4fdd\u6301\u771f\u5b9e\u7684\u8868\u793a\uff0c\u4f46\u5728\u8bef\u5bfc\u6027\u63d0\u793a\u4e0b\u66f4\u5bb9\u6613\u4ea7\u751f\u865a\u5047\u8f93\u51fa\u3002\u8fd9\u4e3a\u672a\u6765\u91cf\u5316\u5bf9\u9f50\u548c\u771f\u5b9e\u6027\u5e72\u9884\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2508.19508", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19508", "abs": "https://arxiv.org/abs/2508.19508", "authors": ["Tian Qiu", "Alan Zoubi", "Yiyuan Lin", "Ruiming Du", "Lailiang Cheng", "Yu Jiang"], "title": "DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View", "comment": null, "summary": "Digital twin applications offered transformative potential by enabling\nreal-time monitoring and robotic simulation through accurate virtual replicas\nof physical assets. The key to these systems is 3D reconstruction with high\ngeometrical fidelity. However, existing methods struggled under field\nconditions, especially with sparse and occluded views. This study developed a\ntwo-stage framework (DATR) for the reconstruction of apple trees from sparse\nviews. The first stage leverages onboard sensors and foundation models to\nsemi-automatically generate tree masks from complex field images. Tree masks\nare used to filter out background information in multi-modal data for the\nsingle-image-to-3D reconstruction at the second stage. This stage consists of a\ndiffusion model and a large reconstruction model for respective multi view and\nimplicit neural field generation. The training of the diffusion model and LRM\nwas achieved by using realistic synthetic apple trees generated by a Real2Sim\ndata generator. The framework was evaluated on both field and synthetic\ndatasets. The field dataset includes six apple trees with field-measured ground\ntruth, while the synthetic dataset featured structurally diverse trees.\nEvaluation results showed that our DATR framework outperformed existing 3D\nreconstruction methods across both datasets and achieved domain-trait\nestimation comparable to industrial-grade stationary laser scanners while\nimproving the throughput by $\\sim$360 times, demonstrating strong potential for\nscalable agricultural digital twin systems.", "AI": {"tldr": "DATR\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe\u4e2d\u82f9\u679c\u68113D\u91cd\u5efa\u7684\u6311\u6218\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u541e\u5410\u91cf\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7a00\u758f\u548c\u906e\u6321\u89c6\u56fe\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u6570\u5b57\u5b6a\u751f\u5e94\u7528\u5728\u519c\u4e1a\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff08DATR\uff09\uff0c\u5305\u62ec\u5229\u7528\u673a\u8f7d\u4f20\u611f\u5668\u548c\u57fa\u7840\u6a21\u578b\u751f\u6210\u6811\u63a9\u7801\u7684\u7b2c\u4e00\u9636\u6bb5\uff0c\u4ee5\u53ca\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u5927\u578b\u91cd\u5efa\u6a21\u578b\u7684\u7b2c\u4e8c\u9636\u6bb5\u3002", "result": "DATR\u5728\u5408\u6210\u548c\u5b9e\u5730\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u67093D\u91cd\u5efa\u65b9\u6cd5\uff0c\u541e\u5410\u91cf\u63d0\u9ad8\u4e86\u7ea6360\u500d\u3002", "conclusion": "DATR\u6846\u67b6\u5728\u7a00\u758f\u89c6\u56fe\u4e2d\u91cd\u5efa\u82f9\u679c\u6811\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u519c\u4e1a\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.19559", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19559", "abs": "https://arxiv.org/abs/2508.19559", "authors": ["Rongzhi Li", "Ruogu Du", "Zefang Chu", "Sida Zhao", "Chunlei Han", "Zuocheng Shi", "Yiwen Shao", "Huanle Han", "Long Huang", "Zherui Liu", "Shufan Liu"], "title": "Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference", "comment": null, "summary": "Serving Large Language Models (LLMs) is a GPU-intensive task where\ntraditional autoscalers fall short, particularly for modern Prefill-Decode\n(P/D) disaggregated architectures. This architectural shift, while powerful,\nintroduces significant operational challenges, including inefficient use of\nheterogeneous hardware, network bottlenecks, and critical imbalances between\nprefill and decode stages. We introduce HeteroScale, a coordinated autoscaling\nframework that addresses the core challenges of P/D disaggregated serving.\nHeteroScale combines a topology-aware scheduler that adapts to heterogeneous\nhardware and network constraints with a novel metric-driven policy derived from\nthe first large-scale empirical study of autoscaling signals in production. By\nleveraging a single, robust metric to jointly scale prefill and decode pools,\nHeteroScale maintains architectural balance while ensuring efficient, adaptive\nresource management. Deployed in a massive production environment on tens of\nthousands of GPUs, HeteroScale has proven its effectiveness, increasing average\nGPU utilization by a significant 26.6 percentage points and saving hundreds of\nthousands of GPU-hours daily, all while upholding stringent service level\nobjectives.", "AI": {"tldr": "HeteroScale \u662f\u4e00\u4e2a\u9488\u5bf9P/D\u5206\u79bb\u67b6\u6784\u7684\u81ea\u52a8\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u8c03\u5ea6\u548c\u65b0\u578b\u6307\u6807\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8GPU\u5229\u7528\u7387\u5e76\u8282\u7701\u8d44\u6e90\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u6269\u5c55\u5668\u5728\u670d\u52a1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u73b0\u4ee3P/D\u5206\u79bb\u67b6\u6784\u4e2d\uff0c\u5b58\u5728\u786c\u4ef6\u5229\u7528\u4f4e\u6548\u3001\u7f51\u7edc\u74f6\u9888\u548c\u9884\u586b\u5145\u4e0e\u89e3\u7801\u9636\u6bb5\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\u3002", "method": "HeteroScale \u7ed3\u5408\u4e86\u62d3\u6251\u611f\u77e5\u8c03\u5ea6\u5668\u548c\u57fa\u4e8e\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u7684\u65b0\u578b\u6307\u6807\u9a71\u52a8\u7b56\u7565\uff0c\u901a\u8fc7\u5355\u4e00\u5065\u58ee\u6307\u6807\u8054\u5408\u6269\u5c55\u9884\u586b\u5145\u548c\u89e3\u7801\u6c60\u3002", "result": "\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u540e\uff0cHeteroScale \u5c06\u5e73\u5747GPU\u5229\u7528\u7387\u63d0\u9ad8\u4e8626.6\u4e2a\u767e\u5206\u70b9\uff0c\u6bcf\u5929\u8282\u7701\u6570\u5341\u4e07GPU\u5c0f\u65f6\uff0c\u540c\u65f6\u6ee1\u8db3\u4e25\u683c\u7684\u670d\u52a1\u6c34\u5e73\u76ee\u6807\u3002", "conclusion": "HeteroScale \u662f\u4e00\u4e2a\u6709\u6548\u7684\u534f\u8c03\u81ea\u52a8\u6269\u5c55\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u9884\u586b\u5145-\u89e3\u7801\uff08P/D\uff09\u5206\u79bb\u67b6\u6784\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86GPU\u5229\u7528\u7387\u5e76\u8282\u7701\u4e86\u5927\u91cf\u8d44\u6e90\u3002"}}
{"id": "2508.20044", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20044", "abs": "https://arxiv.org/abs/2508.20044", "authors": ["Kfir Toledo", "Isaac Keslassy"], "title": "2SYN: Congestion-Aware Multihoming", "comment": "Accepted at IEEE/IFIP NOMS", "summary": "When sending flows to arbitrary destinations, current multihoming routers\nadopt simple congestion-oblivious mechanisms. Therefore, they cannot avoid\ncongested paths.\n  In this paper, we introduce 2SYN, the first congestion-aware multihoming\nalgorithm that works for any destination. We explain how it dynamically selects\na preferred path for new connections, even given previously-unseen\ndestinations. We further demonstrate that it can be easily implemented in\nLinux. Finally, in a real-world experiment with either LTE or a wired link, we\nshow how 2SYN dynamically adapts to the quality of the connection and\noutperforms alternative approaches. Thus, 2SYN helps companies better manage\ntheir networks by leveraging their multihoming capabilities.", "AI": {"tldr": "2SYN\u662f\u9996\u4e2a\u62e5\u585e\u611f\u77e5\u591a\u5bbf\u4e3b\u7b97\u6cd5\uff0c\u52a8\u6001\u9009\u62e9\u8def\u5f84\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u5408\u4f01\u4e1a\u7f51\u7edc\u7ba1\u7406\u3002", "motivation": "\u5f53\u524d\u591a\u5bbf\u4e3b\u8def\u7531\u5668\u91c7\u7528\u7b80\u5355\u7684\u65e0\u62e5\u585e\u610f\u8bc6\u673a\u5236\uff0c\u65e0\u6cd5\u907f\u514d\u62e5\u585e\u8def\u5f84\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u62e5\u585e\u611f\u77e5\u7684\u591a\u5bbf\u4e3b\u7b97\u6cd5\u3002", "method": "2SYN\u662f\u4e00\u79cd\u9996\u4e2a\u9002\u7528\u4e8e\u4efb\u610f\u76ee\u7684\u5730\u7684\u62e5\u585e\u611f\u77e5\u591a\u5bbf\u4e3b\u7b97\u6cd5\uff0c\u52a8\u6001\u9009\u62e9\u65b0\u8fde\u63a5\u7684\u9996\u9009\u8def\u5f84\uff0c\u5e76\u53ef\u8f7b\u677e\u5728Linux\u4e2d\u5b9e\u73b0\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c2SYN\u5728LTE\u6216\u6709\u7ebf\u94fe\u8def\u4e0b\u52a8\u6001\u9002\u5e94\u8fde\u63a5\u8d28\u91cf\uff0c\u8868\u73b0\u4f18\u4e8e\u66ff\u4ee3\u65b9\u6cd5\u3002", "conclusion": "2SYN\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u8fde\u63a5\u8d28\u91cf\u5e76\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e2e\u52a9\u4f01\u4e1a\u66f4\u597d\u5730\u5229\u7528\u591a\u5bbf\u4e3b\u80fd\u529b\u7ba1\u7406\u7f51\u7edc\u3002"}}
{"id": "2508.19663", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19663", "abs": "https://arxiv.org/abs/2508.19663", "authors": ["Lola Solovyeva", "Eduardo Carneiro Oliveira", "Shiyu Fan", "Alper Tuncay", "Shamil Gareev", "Andrea Capiluppi"], "title": "Leveraging LLMs for Automated Translation of Legacy Code: A Case Study on PL/SQL to Java Transformation", "comment": null, "summary": "The VT legacy system, comprising approximately 2.5 million lines of PL/SQL\ncode, lacks consistent documentation and automated tests, posing significant\nchallenges for refactoring and modernisation. This study investigates the\nfeasibility of leveraging large language models (LLMs) to assist in translating\nPL/SQL code into Java for the modernised \"VTF3\" system. By leveraging a dataset\ncomprising 10 PL/SQL-to-Java code pairs and 15 Java classes, which collectively\nestablished a domain model for the translated files, multiple LLMs were\nevaluated. Furthermore, we propose a customized prompting strategy that\nintegrates chain-of-guidance reasoning with $n$-shot prompting. Our findings\nindicate that this methodology effectively guides LLMs in generating\nsyntactically accurate translations while also achieving functional\ncorrectness. However, the findings are limited by the small sample size of\navailable code files and the restricted access to test cases used for\nvalidating the correctness of the generated code. Nevertheless, these findings\nlay the groundwork for scalable, automated solutions in modernising large\nlegacy systems.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528LLMs\u5c06PL/SQL\u4ee3\u7801\u7ffb\u8bd1\u4e3aJava\uff0c\u63d0\u51fa\u81ea\u5b9a\u4e49\u63d0\u793a\u7b56\u7565\uff0c\u7ed3\u679c\u53ef\u884c\u4f46\u53d7\u6837\u672c\u91cf\u9650\u5236\u3002", "motivation": "VT\u9057\u7559\u7cfb\u7edf\u7f3a\u4e4f\u4e00\u81f4\u7684\u6587\u6863\u548c\u81ea\u52a8\u5316\u6d4b\u8bd5\uff0c\u7ed9\u91cd\u6784\u548c\u73b0\u4ee3\u5316\u5e26\u6765\u91cd\u5927\u6311\u6218\uff0c\u56e0\u6b64\u7814\u7a76\u63a2\u7d22\u5229\u7528LLMs\u5c06PL/SQL\u4ee3\u7801\u7ffb\u8bd1\u4e3aJava\u7684\u53ef\u884c\u6027\u3002", "method": "\u5229\u7528\u5305\u542b10\u4e2aPL/SQL\u5230Java\u4ee3\u7801\u5bf9\u548c15\u4e2aJava\u7c7b\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u94fe\u5f0f\u5f15\u5bfc\u63a8\u7406\u548c$n$-shot\u63d0\u793a\u7684\u81ea\u5b9a\u4e49\u63d0\u793a\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6307\u5bfcLLMs\u751f\u6210\u8bed\u6cd5\u51c6\u786e\u7684\u7ffb\u8bd1\u5e76\u5b9e\u73b0\u529f\u80fd\u6b63\u786e\u6027\uff0c\u4f46\u53d7\u9650\u4e8e\u53ef\u7528\u4ee3\u7801\u6587\u4ef6\u7684\u5c0f\u6837\u672c\u91cf\u548c\u6d4b\u8bd5\u7528\u4f8b\u7684\u6709\u9650\u8bbf\u95ee\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5927\u89c4\u6a21\u9057\u7559\u7cfb\u7edf\u7684\u73b0\u4ee3\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u7684\u57fa\u7840\uff0c\u5c3d\u7ba1\u6837\u672c\u91cf\u548c\u6d4b\u8bd5\u7528\u4f8b\u6709\u9650\u3002"}}
{"id": "2508.19898", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.19898", "abs": "https://arxiv.org/abs/2508.19898", "authors": ["Yannic Maus", "Tijn de Vos"], "title": "Distributed Sparsest Cut via Eigenvalue Estimation", "comment": "To be presented as brief announcement at DISC 2025", "summary": "We give new, improved bounds for approximating the sparsest cut value or in\nother words the conductance $\\phi$ of a graph in the CONGEST model. As our main\nresult, we present an algorithm running in $O(\\log^2 n/\\phi)$ rounds in which\nevery vertex outputs a value $\\tilde \\phi$ satisfying $\\phi \\le \\tilde \\phi \\le\n\\sqrt{2.01\\phi}$. In most regimes, our algorithm improves significantly over\nthe previously fastest algorithm for the problem [Chen, Meierhans, Probst\nGutenberg, Saranurak; SODA 25]. Additionally, our result generalizes to $k$-way\nconductance.\n  We obtain these results, by approximating the eigenvalues of the normalized\nLaplacian matrix $L:=I-\\rm{Deg}^{-1/2}A\\rm{Deg}^ {-1/2}$, where, $A$ is the\nadjacency matrix and $\\rm{Deg}$ is the diagonal matrix with the weighted\ndegrees on the diagonal. The previous state of the art sparsest cut algorithm\nis in the technical realm of expander decompositions. Our algorithms, on the\nother hand, are relatively simple and easy to implement. At the core, they rely\non the well-known power method, which comes down to repeatedly multiplying the\nLaplacian with a vector. This operation can be performed in a single round in\nthe CONGEST model. All our algorithms apply to weighted, undirected graphs. Our\nlower bounds apply even in unweighted graphs.", "AI": {"tldr": "\u65b0\u7b97\u6cd5\u5728CONGEST\u6a21\u578b\u4e2d\u9ad8\u6548\u8fd1\u4f3c\u7a00\u758f\u5272\u503c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8ek-way\u4f20\u5bfc\u7387\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u7a00\u758f\u5272\u503c\u8fd1\u4f3c\u7b97\u6cd5\u7684\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728CONGEST\u6a21\u578b\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u8fd1\u4f3c\u5f52\u4e00\u5316\u62c9\u666e\u62c9\u65af\u77e9\u9635L\u7684\u7279\u5f81\u503c\uff0c\u5229\u7528\u5e42\u65b9\u6cd5\uff08power method\uff09\u5b9e\u73b0\uff0c\u8be5\u65b9\u6cd5\u5728CONGEST\u6a21\u578b\u4e2d\u5355\u8f6e\u5373\u53ef\u5b8c\u6210\u3002", "result": "\u7b97\u6cd5\u5728O(log\u00b2n/\u03c6)\u8f6e\u5185\u8fd0\u884c\uff0c\u8f93\u51fa\u503c\u03c6\u0303\u6ee1\u8db3\u03c6 \u2264 \u03c6\u0303 \u2264 \u221a2.01\u03c6\uff0c\u4e14\u9002\u7528\u4e8e\u52a0\u6743\u65e0\u5411\u56fe\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728CONGEST\u6a21\u578b\u4e2d\u8fd1\u4f3c\u7a00\u758f\u5272\u503c\uff08\u5373\u56fe\u7684\u4f20\u5bfc\u7387\u03c6\uff09\u7684\u65b0\u7b97\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5feb\u7b97\u6cd5\uff0c\u5e76\u9002\u7528\u4e8ek-way\u4f20\u5bfc\u7387\u3002"}}
{"id": "2508.19290", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19290", "abs": "https://arxiv.org/abs/2508.19290", "authors": ["Alexandros Gkillas", "Ioulia Kapsali", "Nikos Piperigkos", "Aris S. Lalos"], "title": "Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation", "comment": null, "summary": "LiDAR-based segmentation is essential for reliable perception in autonomous\nvehicles, yet modern segmentation networks are highly susceptible to\nadversarial attacks that can compromise safety. Most existing defenses are\ndesigned for networks operating directly on raw 3D point clouds and rely on\nlarge, computationally intensive generative models. However, many\nstate-of-the-art LiDAR segmentation pipelines operate on more efficient 2D\nrange view representations. Despite their widespread adoption, dedicated\nlightweight adversarial defenses for this domain remain largely unexplored. We\nintroduce an efficient model-based purification framework tailored for\nadversarial defense in 2D range-view LiDAR segmentation. We propose a direct\nattack formulation in the range-view domain and develop an explainable\npurification network based on a mathematical justified optimization problem,\nachieving strong adversarial resilience with minimal computational overhead.\nOur method achieves competitive performance on open benchmarks, consistently\noutperforming generative and adversarial training baselines. More importantly,\nreal-world deployment on a demo vehicle demonstrates the framework's ability to\ndeliver accurate operation in practical autonomous driving scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf92D\u8303\u56f4\u89c6\u56feLiDAR\u5206\u5272\u7684\u9ad8\u6548\u5bf9\u6297\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u51c0\u5316\u7f51\u7edc\u5b9e\u73b0\u5f3a\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u9002\u7528\u4e8e\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u3002", "motivation": "\u73b0\u4ee3\u5206\u5272\u7f51\u7edc\u5bf9\u5bf9\u6297\u653b\u51fb\u9ad8\u5ea6\u654f\u611f\uff0c\u53ef\u80fd\u5371\u53ca\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u3002\u73b0\u6709\u9632\u5fa1\u5927\u591a\u9488\u5bf9\u539f\u59cb3D\u70b9\u4e91\u8bbe\u8ba1\uff0c\u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u578b\u751f\u6210\u6a21\u578b\uff0c\u800c2D\u8303\u56f4\u89c6\u56fe\u7684\u8f7b\u91cf\u7ea7\u9632\u5fa1\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u9ad8\u6548\u51c0\u5316\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf92D\u8303\u56f4\u89c6\u56feLiDAR\u5206\u5272\u7684\u5bf9\u6297\u9632\u5fa1\u3002\u901a\u8fc7\u6570\u5b66\u4f18\u5316\u7684\u53ef\u89e3\u91ca\u51c0\u5316\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5bf9\u6297\u9c81\u68d2\u6027\u4e14\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u751f\u6210\u6a21\u578b\u548c\u5bf9\u6297\u8bad\u7ec3\u57fa\u7ebf\uff0c\u5e76\u5728\u5b9e\u9645\u6f14\u793a\u8f66\u8f86\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u5bf9\u6297\u9632\u5fa1\u6846\u67b6\u57282D\u8303\u56f4\u89c6\u56feLiDAR\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u5728\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\uff0c\u8fd8\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u751f\u6210\u6a21\u578b\u548c\u5bf9\u6297\u8bad\u7ec3\u57fa\u7ebf\u3002"}}
{"id": "2508.19461", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19461", "abs": "https://arxiv.org/abs/2508.19461", "authors": ["Neil Kale", "Chen Bo Calvin Zhang", "Kevin Zhu", "Ankit Aich", "Paula Rodriguez", "Scale Red Team", "Christina Q. Knight", "Zifan Wang"], "title": "Reliable Weak-to-Strong Monitoring of LLM Agents", "comment": "18 pages, 15 figures", "summary": "We stress test monitoring systems for detecting covert misbehavior in\nautonomous LLM agents (e.g., secretly sharing private information). To this\nend, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)\nvarying levels of agent and monitor situational awareness; (2) distinct\nadversarial strategies to evade the monitor, such as prompt injection; and (3)\ntwo datasets and environments -- SHADE-Arena for tool-calling agents and our\nnew CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We\nrun MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse\nagent trajectories, alongside a new hybrid hierarchical-sequential scaffolding\nproposed in this work. Our empirical results yield three key findings. First,\nagent awareness dominates monitor awareness: an agent's knowledge that it is\nbeing monitored substantially degrades the monitor's reliability. On the\ncontrary, providing the monitor with more information about the agent is less\nhelpful than expected. Second, monitor scaffolding matters more than monitor\nawareness: the hybrid scaffolding consistently outperforms baseline monitor\nscaffolding, and can enable weaker models to reliably monitor stronger agents\n-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where\nhumans discuss with the LLM monitor to get an updated judgment for the agent's\nbehavior, targeted human oversight is most effective; escalating only\npre-flagged cases to human reviewers improved the TPR by approximately 15% at\nFPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the\nlack of adversarial robustness for LLMs and humans when monitoring and\ndetecting agent misbehavior. We release code, data, and logs to spur further\nresearch.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7MRT\u5de5\u4f5c\u6d41\u7a0b\u6d4b\u8bd5LLM\u4ee3\u7406\u76d1\u63a7\u7cfb\u7edf\uff0c\u53d1\u73b0\u4ee3\u7406\u7684\u76d1\u63a7\u610f\u8bc6\u662f\u5173\u952e\uff0c\u6df7\u5408\u6846\u67b6\u8868\u73b0\u6700\u4f73\uff0c\u4eba\u7c7b\u9488\u5bf9\u6027\u76d1\u7763\u6700\u6709\u6548\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u76d1\u63a7\u7cfb\u7edf\u5728\u68c0\u6d4b\u81ea\u4e3bLLM\u4ee3\u7406\u9690\u853d\u4e0d\u5f53\u884c\u4e3a\uff08\u5982\u79d8\u5bc6\u5171\u4eab\u79c1\u4eba\u4fe1\u606f\uff09\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5efa\u7acb\u4e00\u4e2a\u6807\u51c6\u5316\u7684MRT\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u7cfb\u7edf\u5316\u7684MRT\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u4e0d\u540c\u7ea7\u522b\u7684\u4ee3\u7406\u548c\u76d1\u63a7\u60c5\u5883\u610f\u8bc6\u3001\u4e0d\u540c\u7684\u5bf9\u6297\u7b56\u7565\uff08\u5982\u63d0\u793a\u6ce8\u5165\uff09\uff0c\u4ee5\u53ca\u4e24\u4e2a\u6570\u636e\u96c6\u548c\u73af\u5883\uff08SHADE-Arena\u548cCUA-SHADE-Arena\uff09\u3002\u6d4b\u8bd5\u4e86\u73b0\u6709\u7684LLM\u76d1\u63a7\u6846\u67b6\u548c\u65b0\u7684\u6df7\u5408\u5206\u5c42-\u987a\u5e8f\u6846\u67b6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u4ee3\u7406\u7684\u76d1\u63a7\u610f\u8bc6\u663e\u8457\u5f71\u54cd\u76d1\u63a7\u53ef\u9760\u6027\uff1b2\uff09\u76d1\u63a7\u6846\u67b6\u6bd4\u76d1\u63a7\u610f\u8bc6\u66f4\u91cd\u8981\uff0c\u6df7\u5408\u6846\u67b6\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff1b3\uff09\u5728\u4eba\u7c7b\u53c2\u4e0e\u7684\u8bbe\u7f6e\u4e2d\uff0c\u9488\u5bf9\u6027\u76d1\u7763\u6700\u6709\u6548\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u76d1\u63a7\u7cfb\u7edf\u7ea2\u961f\u6d4b\u8bd5\uff08MRT\uff09\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63ed\u793a\u4e86\u5728\u68c0\u6d4b\u81ea\u4e3bLLM\u4ee3\u7406\u7684\u9690\u853d\u4e0d\u5f53\u884c\u4e3a\u65f6\uff0c\u4ee3\u7406\u7684\u76d1\u63a7\u610f\u8bc6\u5bf9\u76d1\u63a7\u53ef\u9760\u6027\u7684\u663e\u8457\u5f71\u54cd\uff0c\u4ee5\u53ca\u76d1\u63a7\u6846\u67b6\u6bd4\u76d1\u63a7\u610f\u8bc6\u66f4\u5173\u952e\u7684\u91cd\u8981\u6027\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u5728\u6709\u4eba\u7c7b\u53c2\u4e0e\u7684\u8bbe\u7f6e\u4e2d\uff0c\u9488\u5bf9\u6027\u7684\u4eba\u7c7b\u76d1\u7763\u6700\u4e3a\u6709\u6548\u3002"}}
{"id": "2508.19595", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19595", "abs": "https://arxiv.org/abs/2508.19595", "authors": ["Maryam Kazemi Eskeri", "Thomas Wiedemann", "Ville Kyrki", "Dominik Baumann", "Tomasz Piotr Kucner"], "title": "A Lightweight Crowd Model for Robot Social Navigation", "comment": "7 pages, 6 figures, accepted in ECMR 2025", "summary": "Robots operating in human-populated environments must navigate safely and\nefficiently while minimizing social disruption. Achieving this requires\nestimating crowd movement to avoid congested areas in real-time. Traditional\nmicroscopic models struggle to scale in dense crowds due to high computational\ncost, while existing macroscopic crowd prediction models tend to be either\noverly simplistic or computationally intensive. In this work, we propose a\nlightweight, real-time macroscopic crowd prediction model tailored for human\nmotion, which balances prediction accuracy and computational efficiency. Our\napproach simplifies both spatial and temporal processing based on the inherent\ncharacteristics of pedestrian flow, enabling robust generalization without the\noverhead of complex architectures. We demonstrate a 3.6 times reduction in\ninference time, while improving prediction accuracy by 3.1 %. Integrated into a\nsocially aware planning framework, the model enables efficient and socially\ncompliant robot navigation in dynamic environments. This work highlights that\nefficient human crowd modeling enables robots to navigate dense environments\nwithout costly computations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5b9e\u65f6\u5b8f\u89c2\u4eba\u7fa4\u9884\u6d4b\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u52a8\u6001\u73af\u5883\u5bfc\u822a\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u5bc6\u96c6\u73af\u5883\u4e2d\u64cd\u4f5c\u65f6\uff0c\u9700\u8981\u5b89\u5168\u9ad8\u6548\u5730\u5bfc\u822a\u5e76\u6700\u5c0f\u5316\u793e\u4f1a\u5e72\u6270\u3002\u4f20\u7edf\u5fae\u89c2\u6a21\u578b\u5728\u5bc6\u96c6\u4eba\u7fa4\u4e2d\u96be\u4ee5\u6269\u5c55\uff0c\u800c\u73b0\u6709\u5b8f\u89c2\u6a21\u578b\u8981\u4e48\u8fc7\u4e8e\u7b80\u5316\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5b9e\u65f6\u5b8f\u89c2\u4eba\u7fa4\u9884\u6d4b\u6a21\u578b\uff0c\u4e13\u95e8\u9488\u5bf9\u4eba\u7c7b\u8fd0\u52a8\uff0c\u5e73\u8861\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u884c\u4eba\u6d41\u7684\u56fa\u6709\u7279\u6027\u7b80\u5316\u4e86\u7a7a\u95f4\u548c\u65f6\u95f4\u5904\u7406\uff0c\u65e0\u9700\u590d\u6742\u67b6\u6784\u5373\u53ef\u5b9e\u73b0\u7a33\u5065\u7684\u6cdb\u5316\u3002", "result": "\u6a21\u578b\u5728\u63a8\u7406\u65f6\u95f4\u4e0a\u51cf\u5c11\u4e863.6\u500d\uff0c\u540c\u65f6\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e863.1%\u3002", "conclusion": "\u9ad8\u6548\u7684\u4eba\u7fa4\u5efa\u6a21\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u4e0d\u9700\u8981\u6602\u8d35\u8ba1\u7b97\u7684\u60c5\u51b5\u4e0b\u5728\u5bc6\u96c6\u73af\u5883\u4e2d\u5bfc\u822a\u3002"}}
{"id": "2508.19670", "categories": ["cs.DC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19670", "abs": "https://arxiv.org/abs/2508.19670", "authors": ["Diogo Costa", "Jose Martins", "Sandro Pinto"], "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems", "comment": null, "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86IOMMU\u5728\u5f02\u6784\u5e73\u53f0\u4e2d\u7684\u6027\u80fd\u5e72\u6270\uff0c\u53d1\u73b0\u5176\u5171\u4eab\u7ed3\u6784\u4f1a\u663e\u8457\u589e\u52a0\u5c0f\u5185\u5b58\u4e8b\u52a1\u7684\u5ef6\u8fdf\u3002", "motivation": "\u968f\u7740\u6df7\u5408\u5173\u952e\u6027\u7cfb\u7edf\u96c6\u6210\u5f02\u6784\u8ba1\u7b97\u5e73\u53f0\uff0c\u786e\u4fdd\u5b89\u5168\u548c\u65f6\u5e8f\u53ef\u9884\u6d4b\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u800cIOMMU\u5728\u6b64\u4e2d\u7684\u4f5c\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528Xilinx UltraScale+ ZCU104\u5e73\u53f0\u5206\u6790IOMMU\u7ed3\u6784\u4e2d\u7684\u7ade\u4e89\u6548\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIOMMU\u5e72\u6270\u53ef\u4f7fDMA\u4e8b\u52a1\u5ef6\u8fdf\u9ad8\u8fbe1.79\u500d\uff08\u5728Arm SMMUv2\u5b9e\u73b0\u4e2d\uff09\u3002", "conclusion": "IOMMU\u5728\u6df7\u5408\u5173\u952e\u6027\u7cfb\u7edf\u4e2d\u7684\u5171\u4eab\u7ed3\u6784\u4f1a\u5f15\u5165\u4e0d\u53ef\u9884\u6d4b\u7684\u5ef6\u8fdf\uff0c\u5c24\u5176\u662f\u5bf9\u5c0f\u5185\u5b58\u4e8b\u52a1\u5f71\u54cd\u663e\u8457\u3002"}}
{"id": "2508.20060", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20060", "abs": "https://arxiv.org/abs/2508.20060", "authors": ["Daqian Ding", "Yibo Pi", "Cailian Chen"], "title": "A First Look at Inter-Cell Interference in the Wild", "comment": null, "summary": "In cellular networks, inter-cell interference management has been studied for\ndecades, yet its real-world effectiveness remains under-explored. To bridge\nthis gap, we conduct a first measurement study of inter-cell interference for\noperational 4G/5G networks. Our findings reveal the prevalence of inter-cell\ninterference and a surprising absence of interference coordination among\noperational base stations. As a result, user equipments experience unnecessary\ninterference, which causes significant signal quality degradation, especially\nunder frequency-selective channel fading. We examine the inter-cell\ninterference issues from four major perspectives: network deployment, channel\nassignment, time-frequency resource allocation, and network configuration. In\nnone of these dimensions is inter-cell interference effectively managed.\nNotably, even when spectrum resources are underutilized and simple strategies\ncould effectively mitigate inter-cell interference, base stations consistently\nprioritize using the same set of time-frequency resources, causing interference\nacross cells. Our measurements reveal substantial opportunities for improving\nsignal quality by inter-cell interference management.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b04G/5G\u7f51\u7edc\u4e2d\u666e\u904d\u5b58\u5728\u5c0f\u533a\u95f4\u5e72\u6270\u4e14\u7f3a\u4e4f\u534f\u8c03\u7ba1\u7406\uff0c\u5bfc\u81f4\u4fe1\u53f7\u8d28\u91cf\u4e0b\u964d\uff0c\u4f46\u901a\u8fc7\u7b80\u5355\u7b56\u7565\u5373\u53ef\u663e\u8457\u6539\u5584\u3002", "motivation": "\u586b\u8865\u4e86\u5b9e\u96454G/5G\u7f51\u7edc\u4e2d\u5c0f\u533a\u95f4\u5e72\u6270\u7ba1\u7406\u6709\u6548\u6027\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u7814\u7a76\uff0c\u4ece\u7f51\u7edc\u90e8\u7f72\u3001\u4fe1\u9053\u5206\u914d\u3001\u65f6\u95f4\u9891\u7387\u8d44\u6e90\u5206\u914d\u548c\u7f51\u7edc\u914d\u7f6e\u56db\u4e2a\u4e3b\u8981\u89d2\u5ea6\u5206\u6790\u4e86\u5c0f\u533a\u95f4\u5e72\u6270\u95ee\u9898\u3002", "result": "\u63ed\u793a\u4e86\u5c0f\u533a\u95f4\u5e72\u6270\u7684\u666e\u904d\u6027\u53ca\u57fa\u7ad9\u95f4\u7f3a\u4e4f\u534f\u8c03\u7684\u73b0\u72b6\uff0c\u5bfc\u81f4\u7528\u6237\u8bbe\u5907\u906d\u53d7\u4e0d\u5fc5\u8981\u7684\u5e72\u6270\uff0c\u4fe1\u53f7\u8d28\u91cf\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u7b80\u5355\u7684\u7b56\u7565\u4e5f\u80fd\u6709\u6548\u51cf\u8f7b\u5c0f\u533a\u95f4\u5e72\u6270\uff0c\u4f46\u5b9e\u9645\u57fa\u7ad9\u4ecd\u4f18\u5148\u4f7f\u7528\u76f8\u540c\u7684\u65f6\u95f4\u9891\u7387\u8d44\u6e90\u3002\u6d4b\u91cf\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u6709\u6548\u7684\u5e72\u6270\u7ba1\u7406\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4fe1\u53f7\u8d28\u91cf\u3002"}}
{"id": "2508.19797", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19797", "abs": "https://arxiv.org/abs/2508.19797", "authors": ["Joan Giner-Miguelez", "Abel G\u00f3mez", "Jordi Cabot"], "title": "Enabling Content Management Systems as an Information Source in Model-driven Projects", "comment": null, "summary": "Content Management Systems (CMSs) are the most popular tool when it comes to\ncreate and publish content across the web. Recently, CMSs have evolved,\nbecoming \\emph{headless}. Content served by a \\emph{headless CMS} aims to be\nconsumed by other applications and services through REST APIs rather than by\nhuman users through a web browser. This evolution has enabled CMSs to become a\nnotorious source of content to be used in a variety of contexts beyond pure web\nnavigation. As such, CMS have become an important component of many information\nsystems. Unfortunately, we still lack the tools to properly discover and manage\nthe information stored in a CMS, often highly customized to the needs of a\nspecific domain. Currently, this is mostly a time-consuming and error-prone\nmanual process.\n  In this paper, we propose a model-based framework to facilitate the\nintegration of headless CMSs in software development processes. Our framework\nis able to discover and explicitly represent the information schema behind the\nCMS. This facilitates designing the interaction between the CMS model and other\ncomponents consuming that information. These interactions are then generated as\npart of a middleware library that offers platform-agnostic access to the CMS to\nall the client applications. The complete framework is open-source and\navailable online.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u7b80\u5316\u65e0\u5934CMS\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u96c6\u6210\uff0c\u901a\u8fc7\u53d1\u73b0\u4fe1\u606f\u6a21\u5f0f\u5e76\u751f\u6210\u4e2d\u95f4\u4ef6\u5e93\uff0c\u5b9e\u73b0\u5e73\u53f0\u65e0\u5173\u7684\u8bbf\u95ee\u3002", "motivation": "\u968f\u7740\u65e0\u5934CMS\u7684\u666e\u53ca\uff0c\u5176\u5728\u4fe1\u606f\u7cfb\u7edf\u4e2d\u626e\u6f14\u7740\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u89d2\u8272\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u5de5\u5177\u6765\u53d1\u73b0\u548c\u7ba1\u7406CMS\u4e2d\u7684\u4fe1\u606f\uff0c\u5bfc\u81f4\u8fd9\u4e00\u8fc7\u7a0b\u8017\u65f6\u4e14\u6613\u9519\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u5316\u6846\u67b6\uff0c\u80fd\u591f\u53d1\u73b0\u5e76\u660e\u786e\u8868\u793aCMS\u80cc\u540e\u7684\u4fe1\u606f\u6a21\u5f0f\uff0c\u751f\u6210\u4f5c\u4e3a\u4e2d\u95f4\u4ef6\u5e93\u4e00\u90e8\u5206\u7684\u4ea4\u4e92\uff0c\u63d0\u4f9b\u5e73\u53f0\u65e0\u5173\u7684CMS\u8bbf\u95ee\u3002", "result": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9CMS\u4fe1\u606f\u6a21\u5f0f\u7684\u53d1\u73b0\u548c\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u7684\u4e2d\u95f4\u4ef6\u5e93\u63d0\u4f9b\u4e86\u5e73\u53f0\u65e0\u5173\u7684\u8bbf\u95ee\u65b9\u5f0f\uff0c\u6574\u4e2a\u6846\u67b6\u5df2\u5f00\u6e90\u5e76\u5728\u7ebf\u53ef\u7528\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4fc3\u8fdb\u65e0\u5934CMS\u5728\u8f6f\u4ef6\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u7684\u96c6\u6210\uff0c\u901a\u8fc7\u53d1\u73b0\u548c\u660e\u786e\u8868\u793aCMS\u7684\u4fe1\u606f\u6a21\u5f0f\uff0c\u7b80\u5316\u4e86\u4e0e\u6d88\u8d39\u4fe1\u606f\u7684\u5176\u4ed6\u7ec4\u4ef6\u7684\u4ea4\u4e92\u8bbe\u8ba1\u3002"}}
{"id": "2508.20002", "categories": ["cs.DS", "05C70", "F.2.2; G.2"], "pdf": "https://arxiv.org/pdf/2508.20002", "abs": "https://arxiv.org/abs/2508.20002", "authors": ["Shaul Rosner", "Tami Tamir"], "title": "Bipartite Matching with Pair-Dependent Bounds", "comment": null, "summary": "Let $G=(U \\cup V, E)$ be a bipartite graph, where $U$ represents jobs and $V$\nrepresents machines. We study a new variant of the bipartite matching problem\nin which each job in $U$ can be matched to at most one machine in $V$, and the\nnumber of jobs that can be assigned to a machine depends on the specific jobs\nmatched to it. These pair-dependent bounds reflect systems where different jobs\nhave varying tolerance for congestion, determined by the specific machine they\nare assigned to.\n  We define a bipartite PD-matching as a set of edges $M \\subseteq E$ that\nsatisfies these job-to-machine tolerance constraints. This variant of matching\nextends well-known matching problems, however, despite its relevance to\nreal-world systems, it has not been studied before. We study bipartite\nPD-matchings with the objective of maximizing the matching size. As we show,\nthe problem exhibits significant differences from previously studied matching\nproblems. We analyze its computational complexity both in the general case and\nfor specific restricted instances, presenting hardness results alongside\noptimal and approximation algorithms.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u4e86\u4e8c\u5206\u56fePD\u5339\u914d\u95ee\u9898\uff0c\u8003\u8651\u4e86\u4f5c\u4e1a\u5bf9\u673a\u5668\u62e5\u5835\u7684\u5bb9\u5fcd\u5ea6\u5dee\u5f02\uff0c\u5206\u6790\u4e86\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u4e0d\u540c\u4f5c\u4e1a\u5bf9\u673a\u5668\u62e5\u5835\u7684\u5bb9\u5fcd\u5ea6\u4e0d\u540c\uff0c\u8fd9\u4e00\u7279\u6027\u5728\u73b0\u6709\u5339\u914d\u95ee\u9898\u4e2d\u672a\u88ab\u8003\u8651\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u5339\u914d\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u4e8c\u5206\u56fePD\u5339\u914d\u5e76\u5206\u6790\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u7814\u7a76\u91c7\u7528\u4e86\u7406\u8bba\u8bc1\u660e\u548c\u7b97\u6cd5\u8bbe\u8ba1\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e8c\u5206\u56fePD\u5339\u914d\u95ee\u9898\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u4e0e\u4f20\u7edf\u5339\u914d\u95ee\u9898\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86\u4e8c\u5206\u56fePD\u5339\u914d\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u4e0e\u73b0\u6709\u5339\u914d\u95ee\u9898\u7684\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u63d0\u4f9b\u4e86\u9488\u5bf9\u4e00\u822c\u60c5\u51b5\u548c\u7279\u5b9a\u53d7\u9650\u5b9e\u4f8b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u5206\u6790\uff0c\u5305\u62ec\u786c\u5ea6\u7ed3\u679c\u3001\u6700\u4f18\u7b97\u6cd5\u548c\u8fd1\u4f3c\u7b97\u6cd5\u3002"}}
{"id": "2508.19294", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19294", "abs": "https://arxiv.org/abs/2508.19294", "authors": ["Ranjan Sapkota", "Manoj Karkee"], "title": "Object Detection with Multimodal Large Vision-Language Models: An In-depth Review", "comment": "First Peer Reviewed Review Paper for Object Detection with\n  Vision-Language Models (VLMs)", "summary": "The fusion of language and vision in large vision-language models (LVLMs) has\nrevolutionized deep learning-based object detection by enhancing adaptability,\ncontextual reasoning, and generalization beyond traditional architectures. This\nin-depth review presents a structured exploration of the state-of-the-art in\nLVLMs, systematically organized through a three-step research review process.\nFirst, we discuss the functioning of vision language models (VLMs) for object\ndetection, describing how these models harness natural language processing\n(NLP) and computer vision (CV) techniques to revolutionize object detection and\nlocalization. We then explain the architectural innovations, training\nparadigms, and output flexibility of recent LVLMs for object detection,\nhighlighting how they achieve advanced contextual understanding for object\ndetection. The review thoroughly examines the approaches used in integration of\nvisual and textual information, demonstrating the progress made in object\ndetection using VLMs that facilitate more sophisticated object detection and\nlocalization strategies. This review presents comprehensive visualizations\ndemonstrating LVLMs' effectiveness in diverse scenarios including localization\nand segmentation, and then compares their real-time performance, adaptability,\nand complexity to traditional deep learning systems. Based on the review, its\nis expected that LVLMs will soon meet or surpass the performance of\nconventional methods in object detection. The review also identifies a few\nmajor limitations of the current LVLM modes, proposes solutions to address\nthose challenges, and presents a clear roadmap for the future advancement in\nthis field. We conclude, based on this study, that the recent advancement in\nLVLMs have made and will continue to make a transformative impact on object\ndetection and robotic applications in the future.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u7269\u4f53\u68c0\u6d4b\u4e2d\u7684\u9769\u547d\u6027\u4f5c\u7528\uff0c\u5c55\u793a\u4e86\u5176\u5728\u9002\u5e94\u6027\u3001\u4e0a\u4e0b\u6587\u7406\u89e3\u7b49\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5982\u4f55\u901a\u8fc7\u878d\u5408\u8bed\u8a00\u548c\u89c6\u89c9\u589e\u5f3a\u7269\u4f53\u68c0\u6d4b\u7684\u9002\u5e94\u6027\u3001\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8d85\u8d8a\u4f20\u7edf\u67b6\u6784\u3002", "method": "\u901a\u8fc7\u4e09\u6b65\u7814\u7a76\u56de\u987e\u8fc7\u7a0b\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u4e86LVLMs\u7684\u6700\u65b0\u6280\u672f\uff0c\u5305\u62ec\u8ba8\u8bba\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7269\u4f53\u68c0\u6d4b\u4e2d\u7684\u529f\u80fd\u3001\u89e3\u91ca\u67b6\u6784\u521b\u65b0\u3001\u8bad\u7ec3\u8303\u5f0f\u53ca\u8f93\u51fa\u7075\u6d3b\u6027\uff0c\u5e76\u6bd4\u8f83\u4e86LVLMs\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u7684\u5b9e\u65f6\u6027\u80fd\u3001\u9002\u5e94\u6027\u548c\u590d\u6742\u6027\u3002", "result": "LVLMs\u5728\u591a\u6837\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u9884\u8ba1\u5c06\u5f88\u5feb\u8fbe\u5230\u6216\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u5728\u7269\u4f53\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5f53\u524dLVLMs\u7684\u4e3b\u8981\u5c40\u9650\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8fd1\u671f\u5728\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e0a\u7684\u8fdb\u5c55\u5df2\u7ecf\u5e76\u5c06\u6301\u7eed\u5bf9\u7269\u4f53\u68c0\u6d4b\u548c\u673a\u5668\u4eba\u5e94\u7528\u4ea7\u751f\u53d8\u9769\u6027\u5f71\u54cd\u3002"}}
{"id": "2508.19502", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19502", "abs": "https://arxiv.org/abs/2508.19502", "authors": ["Xifeng Yao", "Chengyuan Ma", "Dongyu Lang", "Yinhao Ni", "Zhiwei Xu", "Huarui Xie", "Zihao Chen", "Guang Shen", "Dandan Tu", "Yi Bai", "Changzheng Zhang"], "title": "SLIM: Subtrajectory-Level Elimination for More Effective Reasoning", "comment": "EMNLP 2025 Findings", "summary": "In recent months, substantial progress has been made in complex reasoning of\nLarge Language Models, particularly through the application of test-time\nscaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When\nresponding to a query, these models generate an extended reasoning trajectory,\nduring which the model explores, reflects, backtracks, and self-verifies before\narriving at a conclusion. However, fine-tuning models with such reasoning\ntrajectories may not always be optimal. Our findings indicate that not all\ncomponents within these reasoning trajectories contribute positively to the\nreasoning process; in fact, some components may affect the overall performance\nnegatively. In this study, we divide a reasoning trajectory into individual\nsubtrajectories and develop a \"5+2\" framework to: (1) systematically identify\nsuboptimal subtrajectories within the reasoning trajectory based on five\nhuman-established criteria; (2) assess the independence of the suboptimal\nsubtrajectories identified in (1) from the subsequent content, ensuring that\ntheir elimination does not compromise overall flow and coherence of the\nreasoning process. Additionally, a sampling algorithm, built upon the \"5+2\"\nframework, is employed to select data whose reasoning process is free from\nsuboptimal subtrajectories to the highest degree. Experimental results\ndemonstrate that our method can reduce the number of suboptimal subtrajectories\nby 25.9\\% during the inference. Furthermore, our method achieves an average\naccuracy of 58.92\\% on highly challenging math benchmarks with only two thirds\nof training data, surpassing the average accuracy of 58.06\\% achieved with the\nentire data, and outperforming open-source datasets, when fine-tuning\nQwen2.5-Math-7B. Finally, We validated our method under resource constraints\nand observed improved performance across various inference token limits.", "AI": {"tldr": "\u63d0\u51fa\u20185+2\u2019\u6846\u67b6\u548c\u91c7\u6837\u7b97\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u53d7\u9650\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4e2d\u751f\u6210\u6269\u5c55\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u4f46\u5e76\u975e\u6240\u6709\u5b50\u8f68\u8ff9\u90fd\u5bf9\u63a8\u7406\u8fc7\u7a0b\u6709\u79ef\u6781\u8d21\u732e\uff0c\u6709\u4e9b\u751a\u81f3\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u9700\u8981\u8bc6\u522b\u5e76\u6d88\u9664\u8fd9\u4e9b\u6b21\u4f18\u5b50\u8f68\u8ff9\u3002", "method": "\u5c06\u63a8\u7406\u8f68\u8ff9\u5212\u5206\u4e3a\u5b50\u8f68\u8ff9\uff0c\u5f00\u53d1\u20185+2\u2019\u6846\u67b6\u7cfb\u7edf\u8bc6\u522b\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u91c7\u6837\u7b97\u6cd5\u9009\u62e9\u65e0\u6b21\u4f18\u5b50\u8f68\u8ff9\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u4e2d\u51cf\u5c1125.9%\u7684\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u4f7f\u7528\u4e09\u5206\u4e4b\u4e8c\u7684\u8bad\u7ec3\u6570\u636e\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523058.92%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4f7f\u7528\u5168\u90e8\u6570\u636e\u7684\u7ed3\u679c\uff0858.06%\uff09\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u20185+2\u2019\u6846\u67b6\u548c\u91c7\u6837\u7b97\u6cd5\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u63d0\u5347\u6a21\u578b\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u4ecd\u80fd\u4fdd\u6301\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.19607", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19607", "abs": "https://arxiv.org/abs/2508.19607", "authors": ["Amin Berjaoui Tahmaz", "Ravi Prakash", "Jens Kober"], "title": "Impedance Primitive-augmented Hierarchical Reinforcement Learning for Sequential Tasks", "comment": "This article is accepted for publication in IEEE International\n  Conference on Robotics and Automation (ICRA) 2025", "summary": "This paper presents an Impedance Primitive-augmented hierarchical\nreinforcement learning framework for efficient robotic manipulation in\nsequential contact tasks. We leverage this hierarchical structure to\nsequentially execute behavior primitives with variable stiffness control\ncapabilities for contact tasks. Our proposed approach relies on three key\ncomponents: an action space enabling variable stiffness control, an adaptive\nstiffness controller for dynamic stiffness adjustments during primitive\nexecution, and affordance coupling for efficient exploration while encouraging\ncompliance. Through comprehensive training and evaluation, our framework learns\nefficient stiffness control capabilities and demonstrates improvements in\nlearning efficiency, compositionality in primitive selection, and success rates\ncompared to the state-of-the-art. The training environments include block\nlifting, door opening, object pushing, and surface cleaning. Real world\nevaluations further confirm the framework's sim2real capability. This work lays\nthe foundation for more adaptive and versatile robotic manipulation systems,\nwith potential applications in more complex contact-based tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u53d8\u521a\u5ea6\u63a7\u5236\u548c\u9ad8\u6548\u63a2\u7d22\u673a\u5236\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u5e8f\u5217\u63a5\u89e6\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u5728\u5e8f\u5217\u63a5\u89e6\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u53ef\u53d8\u521a\u5ea6\u63a7\u5236\u7684\u573a\u666f\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u963b\u6297\u57fa\u5143\u589e\u5f3a\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u652f\u6301\u53ef\u53d8\u521a\u5ea6\u63a7\u5236\u7684\u52a8\u4f5c\u7a7a\u95f4\u3001\u81ea\u9002\u5e94\u521a\u5ea6\u63a7\u5236\u5668\u4ee5\u53ca\u9ad8\u6548\u63a2\u7d22\u7684\u8026\u5408\u673a\u5236\u3002", "result": "\u901a\u8fc7\u5168\u9762\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u6846\u67b6\u5728\u5b66\u4e60\u6548\u7387\u3001\u57fa\u5143\u9009\u62e9\u7ec4\u5408\u6027\u548c\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u66f4\u81ea\u9002\u5e94\u548c\u591a\u529f\u80fd\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u590d\u6742\u63a5\u89e6\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002"}}
{"id": "2508.19805", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19805", "abs": "https://arxiv.org/abs/2508.19805", "authors": ["Shota Naito", "Tsukasa Ninomiya", "Koichi Wada"], "title": "Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers", "comment": null, "summary": "Understanding the computational power of mobile robot systems is a\nfundamental challenge in distributed computing. While prior work has focused on\npairwise separations between models, we explore how robot capabilities, light\nobservability, and scheduler synchrony interact in more complex ways.\n  We first show that the Exponential Times Expansion (ETE) problem is solvable\nonly in the strongest model -- fully-synchronous robots with full mutual lights\n($\\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and\nTAR(d)* problems to demonstrate how internal memory and lights interact with\nsynchrony: under weak synchrony, internal memory alone is insufficient, while\nfull synchrony can substitute for both lights and memory.\n  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and\nZCC to show fine-grained separations between $\\mathcal{FSTA}$ and\n$\\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and\nLeave Place Convergence (LP-Cv), illustrating the limitations of internal\nmemory in symmetric settings.\n  These results extend the known separation map of 14 canonical robot models,\nrevealing structural phenomena only visible through higher-order comparisons.\nOur work provides new impossibility criteria and deepens the understanding of\nhow observability, memory, and synchrony collectively shape the computational\npower of mobile robots.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u79fb\u52a8\u673a\u5668\u4eba\u80fd\u529b\u3001\u5149\u53ef\u89c2\u6d4b\u6027\u548c\u540c\u6b65\u6027\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u6269\u5c55\u4e86\u5df2\u77e5\u6a21\u578b\u5206\u79bb\u56fe\uff0c\u63ed\u793a\u4e86\u9ad8\u9636\u6bd4\u8f83\u4e0b\u7684\u7ed3\u6784\u73b0\u8c61\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u4e0d\u53ef\u80fd\u6027\u6807\u51c6\u3002", "motivation": "\u7406\u89e3\u79fb\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8ba1\u7b97\u80fd\u529b\u662f\u5206\u5e03\u5f0f\u8ba1\u7b97\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u867d\u7136\u4e4b\u524d\u7684\u5de5\u4f5c\u96c6\u4e2d\u5728\u6a21\u578b\u4e4b\u95f4\u7684\u4e24\u4e24\u5206\u79bb\uff0c\u4f46\u6211\u4eec\u63a2\u7d22\u4e86\u673a\u5668\u4eba\u80fd\u529b\u3001\u5149\u53ef\u89c2\u6d4b\u6027\u548c\u8c03\u5ea6\u5668\u540c\u6b65\u6027\u5982\u4f55\u4ee5\u66f4\u590d\u6742\u7684\u65b9\u5f0f\u4ea4\u4e92\u3002", "method": "\u6211\u4eec\u9996\u5148\u5c55\u793a\u4e86\u6307\u6570\u65f6\u95f4\u6269\u5c55\uff08ETE\uff09\u95ee\u9898\u4ec5\u5728\u6700\u5f3a\u6a21\u578b\u2014\u2014\u5b8c\u5168\u540c\u6b65\u7684\u673a\u5668\u4eba\u5177\u6709\u5b8c\u5168\u76f8\u4e92\u5149\uff08$\\mathcal{LUMT}^F$\uff09\u4e2d\u53ef\u89e3\u3002\u7136\u540e\u5f15\u5165\u4e86\u516d\u8fb9\u5f62\u8fb9\u7f18\u904d\u5386\uff08HET\uff09\u548cTAR(d)*\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5185\u90e8\u5185\u5b58\u548c\u5149\u4e0e\u540c\u6b65\u6027\u7684\u4ea4\u4e92\u4f5c\u7528\uff1a\u5728\u5f31\u540c\u6b65\u6027\u4e0b\uff0c\u4ec5\u5185\u90e8\u5185\u5b58\u4e0d\u8db3\uff0c\u800c\u5b8c\u5168\u540c\u6b65\u6027\u53ef\u4ee5\u66ff\u4ee3\u5149\u548c\u5185\u5b58\u3002\u5728\u5f02\u6b65\u8bbe\u7f6e\u4e2d\uff0c\u6211\u4eec\u5206\u7c7b\u4e86LP-MLCv\u3001VEC\u548cZCC\u7b49\u95ee\u9898\uff0c\u5c55\u793a\u4e86$\\mathcal{FSTA}$\u548c$\\mathcal{FCOM}$\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5206\u79bb\u3002", "result": "\u6211\u4eec\u7684\u5de5\u4f5c\u6269\u5c55\u4e86\u5df2\u77e5\u7684\u673a\u5668\u4eba\u6a21\u578b\u5206\u79bb\u56fe\uff0c\u63ed\u793a\u4e86\u901a\u8fc7\u9ad8\u9636\u6bd4\u8f83\u624d\u80fd\u89c2\u5bdf\u5230\u7684\u7ed3\u6784\u73b0\u8c61\u3002\u63d0\u4f9b\u4e86\u65b0\u7684\u4e0d\u53ef\u80fd\u6027\u6807\u51c6\uff0c\u5e76\u6df1\u5316\u4e86\u5bf9\u53ef\u89c2\u6d4b\u6027\u3001\u5185\u5b58\u548c\u540c\u6b65\u6027\u5982\u4f55\u5171\u540c\u5851\u9020\u79fb\u52a8\u673a\u5668\u4eba\u8ba1\u7b97\u80fd\u529b\u7684\u7406\u89e3\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u6269\u5c55\u4e8614\u79cd\u5178\u578b\u673a\u5668\u4eba\u6a21\u578b\u7684\u5df2\u77e5\u5206\u79bb\u56fe\uff0c\u63ed\u793a\u4e86\u901a\u8fc7\u9ad8\u9636\u6bd4\u8f83\u624d\u80fd\u770b\u5230\u7684\u6df1\u5c42\u7ed3\u6784\u73b0\u8c61\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u4e0d\u53ef\u80fd\u6027\u6807\u51c6\uff0c\u5e76\u6df1\u5316\u4e86\u5bf9\u53ef\u89c2\u6d4b\u6027\u3001\u5185\u5b58\u548c\u540c\u6b65\u6027\u5982\u4f55\u5171\u540c\u5851\u9020\u79fb\u52a8\u673a\u5668\u4eba\u8ba1\u7b97\u80fd\u529b\u7684\u7406\u89e3\u3002"}}
{"id": "2508.20077", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20077", "abs": "https://arxiv.org/abs/2508.20077", "authors": ["Tao Xiuyuan", "Milena Radenkovic"], "title": "ML-MaxProp: Bridging Machine Learning and Delay-Tolerant Routing for Resilient Post-Disaster Communication", "comment": null, "summary": "In disaster-stricken and large-scale urban emergency scenarios, ensuring\nreliable communication remains a formidable challenge, as collapsed\ninfrastructure, unpredictable mobility, and severely constrained resources\ndisrupt conventional networks. Delay-Tolerant Networks (DTNs), though resilient\nthrough their store-carry-forward paradigm, reveal the fundamental weaknesses\nof classical protocols - Epidemic, Spray-and-Wait, and MaxProp - when\nconfronted with sparse encounters, buffer shortages, and volatile connectivity.\nTo address these obstacles, this study proposes ML-MaxProp, a hybrid routing\nprotocol that strengthens MaxProp with supervised machine learning. By\nleveraging contextual features such as encounter frequency, hop count, buffer\noccupancy, message age, and time-to-live (TTL), ML-MaxProp predicts relay\nsuitability in real time, transforming rigid heuristics into adaptive\nintelligence. Extensive simulations in the ONE environment using the Helsinki\nSPMBM mobility model show that ML-MaxProp consistently surpasses baseline\nprotocols, achieving higher delivery probability, lower latency, and reduced\noverhead. Statistical validation further shows that these improvements are both\nsignificant and robust, even under highly resource-constrained and unstable\nconditions. Overall, this work shows that ML-MaxProp is not just an incremental\nrefinement but a lightweight, adaptive, and practical solution to one of the\nhardest challenges in DTNs: sustaining mission-critical communication when\ninfrastructure collapses and every forwarding decision becomes critical.", "AI": {"tldr": "ML-MaxProp \u662f\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u7684 DTN \u8def\u7531\u534f\u8bae\uff0c\u663e\u8457\u63d0\u5347\u4e86\u707e\u96be\u573a\u666f\u4e0b\u7684\u901a\u4fe1\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u707e\u96be\u548c\u5927\u89c4\u6a21\u57ce\u5e02\u7d27\u6025\u60c5\u51b5\u4e0b\uff0c\u4f20\u7edf\u7f51\u7edc\u56e0\u57fa\u7840\u8bbe\u65bd\u5d29\u6e83\u3001\u4e0d\u53ef\u9884\u6d4b\u7684\u79fb\u52a8\u6027\u548c\u8d44\u6e90\u53d7\u9650\u800c\u5931\u6548\uff0c\u73b0\u6709 DTN \u534f\u8bae\u5728\u7a00\u758f\u76f8\u9047\u3001\u7f13\u51b2\u533a\u77ed\u7f3a\u548c\u8fde\u63a5\u4e0d\u7a33\u5b9a\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u8def\u7531\u534f\u8bae ML-MaxProp\uff0c\u901a\u8fc7\u76d1\u7763\u673a\u5668\u5b66\u4e60\u589e\u5f3a MaxProp\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u7279\u5f81\uff08\u5982\u76f8\u9047\u9891\u7387\u3001\u8df3\u6570\u3001\u7f13\u51b2\u533a\u5360\u7528\u7387\u7b49\uff09\u5b9e\u65f6\u9884\u6d4b\u4e2d\u7ee7\u9002\u7528\u6027\u3002", "result": "\u5728 ONE \u73af\u5883\u4e2d\u4f7f\u7528 Helsinki SPMBM \u79fb\u52a8\u6a21\u578b\u8fdb\u884c\u7684\u5e7f\u6cdb\u6a21\u62df\u8868\u660e\uff0cML-MaxProp \u5728\u4ea4\u4ed8\u6982\u7387\u3001\u5ef6\u8fdf\u548c\u5f00\u9500\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u534f\u8bae\uff0c\u4e14\u6539\u8fdb\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u3002", "conclusion": "ML-MaxProp \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u81ea\u9002\u5e94\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u57fa\u7840\u8bbe\u65bd\u5d29\u6e83\u65f6\u7ef4\u6301\u5173\u952e\u4efb\u52a1\u901a\u4fe1\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u534f\u8bae\u3002"}}
{"id": "2508.19803", "categories": ["cs.SE", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.19803", "abs": "https://arxiv.org/abs/2508.19803", "authors": ["Peter Fettke", "Wolfgang Reisig"], "title": "Towards a fundamental theory of modeling discrete systems", "comment": "6 pages, 2 figures, author prepared version of final manuscript\n  accepted at the 44th International Conference on Conceptual Modeling, 20-23\n  October 2025, Poitiers / Futuroscope, France, Workshop on Fundamentals of\n  Conceptual Modeling (FCM)", "summary": "Modeling is a central concern in both science and engineering. However, we\nneed a new fundamental theory to address the challenges of the digital age. In\nthis paper, we first explain why modeling is fundamental and which challenges\nmust be addressed in the digital world. As a main contribution, we introduce\nthe Heraklit modeling framework as a new approach to modeling. We conclude with\nsome general remarks. Future work will involve the correctness of modeling, the\nnotion of information, and the description of invariance in modeling.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5efa\u6a21\u5728\u6570\u5b57\u65f6\u4ee3\u7684\u6838\u5fc3\u6311\u6218\uff0c\u63d0\u51fa\u4e86Heraklit\u5efa\u6a21\u6846\u67b6\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u9610\u8ff0\u4e86\u5efa\u6a21\u5728\u79d1\u5b66\u4e0e\u5de5\u7a0b\u4e2d\u7684\u6838\u5fc3\u5730\u4f4d\uff0c\u5e76\u6307\u51fa\u6570\u5b57\u65f6\u4ee3\u9700\u8981\u65b0\u7684\u57fa\u7840\u7406\u8bba\u6765\u5e94\u5bf9\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86Heraklit\u5efa\u6a21\u6846\u67b6\u4f5c\u4e3a\u65b0\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86Heraklit\u5efa\u6a21\u6846\u67b6\u4f5c\u4e3a\u5e94\u5bf9\u6570\u5b57\u65f6\u4ee3\u5efa\u6a21\u6311\u6218\u7684\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86Heraklit\u5efa\u6a21\u6846\u67b6\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u5efa\u6a21\u7684\u6b63\u786e\u6027\u3001\u4fe1\u606f\u6982\u5ff5\u4ee5\u53ca\u5efa\u6a21\u4e2d\u7684\u4e0d\u53d8\u6027\u63cf\u8ff0\u3002"}}
{"id": "2508.20041", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.20041", "abs": "https://arxiv.org/abs/2508.20041", "authors": ["Thomas Bl\u00e4sius", "Henrik Cs\u00f6re", "Max G\u00f6ttlicher", "Elly Schmidt", "Wendy Yi"], "title": "Flow-weighted Layered Metric Euclidean Capacitated Steiner Tree Problem", "comment": null, "summary": "Motivated by hierarchical networks, we introduce the Flow-weighted Layered\nMetric Euclidean Capacitated Steiner Tree (FLaMECaST) problem, a variant of the\nEuclidean Steiner tree with layered structure and capacity constraints per\nlayer. The goal is to construct a cost-optimal Steiner forest connecting a set\nof sources to a set of sinks under load-dependent edge costs. We prove that\nFLaMECaST is NP-hard to approximate, even in restricted cases where all sources\nlie on a circle. However, assuming few additional constraints for such\ninstances, we design a dynamic program that achieves a $\\left(1 +\n\\frac{1}{2^n}\\right)$-approximation in polynomial time. By generalizing the\nstructural insights the dynamic program is based on, we extend the approach to\ncertain settings, where all sources are positioned on a convex polygon.", "AI": {"tldr": "FLaMECaST\u95ee\u9898\u5728\u5206\u5c42\u7f51\u7edc\u4e2d\u63d0\u51fa\uff0c\u867dNP\u96be\u8fd1\u4f3c\uff0c\u4f46\u5728\u7279\u5b9a\u7ea6\u675f\u4e0b\u53ef\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u5b9e\u73b0\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u89e3\u3002", "motivation": "\u53d7\u5206\u5c42\u7f51\u7edc\u542f\u53d1\uff0c\u5f15\u5165FLaMECaST\u95ee\u9898\uff0c\u7814\u7a76\u5728\u5206\u5c42\u7ed3\u6784\u548c\u5bb9\u91cf\u7ea6\u675f\u4e0b\u7684\u6700\u4f18Steiner\u68ee\u6797\u6784\u5efa\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\uff0c\u57fa\u4e8e\u7ed3\u6784\u6027\u6d1e\u5bdf\uff0c\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7684\u8fd1\u4f3c\u89e3\u3002", "result": "\u8bc1\u660e\u4e86FLaMECaST\u95ee\u9898\u5728\u53d7\u9650\u60c5\u51b5\u4e0b\uff08\u5982\u6e90\u70b9\u4f4d\u4e8e\u5706\u4e0a\uff09\u96be\u4ee5\u8fd1\u4f3c\uff0c\u4f46\u5728\u9644\u52a0\u7ea6\u675f\u4e0b\u53ef\u5b9e\u73b0\u591a\u9879\u5f0f\u65f6\u95f4\u7684\u8fd1\u4f3c\u89e3\u3002", "conclusion": "FLaMECaST\u95ee\u9898\u5728\u7279\u5b9a\u7ea6\u675f\u4e0b\u53ef\u4ee5\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u5b9e\u73b0\u591a\u9879\u5f0f\u65f6\u95f4\u7684\u8fd1\u4f3c\u89e3\uff0c\u4e14\u5728\u51f8\u591a\u8fb9\u5f62\u6e90\u70b9\u5e03\u5c40\u4e0b\u65b9\u6cd5\u53ef\u6269\u5c55\u3002"}}
{"id": "2508.19295", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19295", "abs": "https://arxiv.org/abs/2508.19295", "authors": ["Sauptik Dhar", "Nicholas Buoncristiani", "Joe Anakata", "Haoyu Zhang", "Michelle Munson"], "title": "Large VLM-based Stylized Sports Captioning", "comment": null, "summary": "The advent of large (visual) language models (LLM / LVLM) have led to a\ndeluge of automated human-like systems in several domains including social\nmedia content generation, search and recommendation, healthcare prognosis, AI\nassistants for cognitive tasks etc. Although these systems have been\nsuccessfully integrated in production; very little focus has been placed on\nsports, particularly accurate identification and natural language description\nof the game play. Most existing LLM/LVLMs can explain generic sports\nactivities, but lack sufficient domain-centric sports' jargon to create natural\n(human-like) descriptions. This work highlights the limitations of existing\nSoTA LLM/LVLMs for generating production-grade sports captions from images in a\ndesired stylized format, and proposes a two-level fine-tuned LVLM pipeline to\naddress that. The proposed pipeline yields an improvement > 8-10% in the F1,\nand > 2-10% in BERT score compared to alternative approaches. In addition, it\nhas a small runtime memory footprint and fast execution time. During Super Bowl\nLIX the pipeline proved its practical application for live professional sports\njournalism; generating highly accurate and stylized captions at the rate of 6\nimages per 3-5 seconds for over 1000 images during the game play.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u7ea7\u5fae\u8c03\u7684LVLM\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u751f\u6210\u98ce\u683c\u5316\u7684\u4f53\u80b2\u6bd4\u8d5b\u63cf\u8ff0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6548\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4f53\u80b2\u9886\u57df\u7684\u5e94\u7528\u4e2d\u7f3a\u4e4f\u8db3\u591f\u7684\u9886\u57df\u7279\u5b9a\u672f\u8bed\uff0c\u65e0\u6cd5\u751f\u6210\u81ea\u7136\u7684\u4eba\u7c7b\u98ce\u683c\u63cf\u8ff0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u7ea7\u5fae\u8c03\u7684LVLM\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u4ece\u56fe\u50cf\u4e2d\u751f\u6210\u98ce\u683c\u5316\u7684\u4f53\u80b2\u6bd4\u8d5b\u63cf\u8ff0\u3002", "result": "\u4e0e\u66ff\u4ee3\u65b9\u6cd5\u76f8\u6bd4\uff0cF1\u5206\u6570\u63d0\u9ad8\u4e868-10%\uff0cBERT\u5206\u6570\u63d0\u9ad8\u4e862-10%\uff0c\u4e14\u5177\u6709\u8f83\u5c0f\u7684\u8fd0\u884c\u65f6\u5185\u5b58\u5360\u7528\u548c\u5feb\u901f\u6267\u884c\u65f6\u95f4\u3002\u5728\u8d85\u7ea7\u7897LIX\u671f\u95f4\uff0c\u6d41\u6c34\u7ebf\u6210\u529f\u5e94\u7528\u4e8e\u5b9e\u65f6\u4e13\u4e1a\u4f53\u80b2\u65b0\u95fb\uff0c\u751f\u6210\u9ad8\u51c6\u786e\u7387\u548c\u98ce\u683c\u5316\u7684\u63cf\u8ff0\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e24\u7ea7\u5fae\u8c03LVLM\u6d41\u6c34\u7ebf\u5728\u5b9e\u65f6\u4f53\u80b2\u65b0\u95fb\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u4ee5\u9ad8\u51c6\u786e\u7387\u548c\u98ce\u683c\u5316\u683c\u5f0f\u751f\u6210\u4f53\u80b2\u6bd4\u8d5b\u63cf\u8ff0\uff0c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.19505", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19505", "abs": "https://arxiv.org/abs/2508.19505", "authors": ["Gerard Boxo", "Ryan Socha", "Daniel Yoo", "Shivam Raval"], "title": "Caught in the Act: a mechanistic approach to detecting deception", "comment": null, "summary": "Sophisticated instrumentation for AI systems might have indicators that\nsignal misalignment from human values, not unlike a \"check engine\" light in\ncars. One such indicator of misalignment is deceptiveness in generated\nresponses. Future AI instrumentation may have the ability to detect when an LLM\ngenerates deceptive responses while reasoning about seemingly plausible but\nincorrect answers to factual questions. In this work, we demonstrate that\nlinear probes on LLMs internal activations can detect deception in their\nresponses with extremely high accuracy. Our probes reach a maximum of greater\nthan 90% accuracy in distinguishing between deceptive and non-deceptive\narguments generated by llama and qwen models ranging from 1.5B to 14B\nparameters, including their DeepSeek-r1 finetuned variants. We observe that\nprobes on smaller models (1.5B) achieve chance accuracy at detecting deception,\nwhile larger models (greater than 7B) reach 70-80%, with their reasoning\ncounterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage\npattern across layers: near-random (50%) in early layers, peaking in middle\nlayers, and slightly declining in later layers. Furthermore, using an iterative\nnull space projection approach, we find multitudes of linear directions that\nencode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and\nQwen 14B models.", "AI": {"tldr": "\u7ebf\u6027\u63a2\u9488\u53ef\u9ad8\u6548\u68c0\u6d4bLLM\u7684\u6b3a\u9a97\u6027\u56de\u7b54\uff0c\u5c24\u5176\u5728\u5927\u578b\u6a21\u578b\u4e2d\u51c6\u786e\u7387\u8d8590%\u3002", "motivation": "\u63a2\u8ba8AI\u7cfb\u7edf\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u7c7b\u4f3c\u6c7d\u8f66'\u68c0\u67e5\u5f15\u64ce'\u706f\u7684\u6307\u6807\u6765\u68c0\u6d4b\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u504f\u5dee\uff0c\u5c24\u5176\u662f\u6b3a\u9a97\u6027\u56de\u7b54\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u5206\u6790LLM\u5185\u90e8\u6fc0\u6d3b\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u96f6\u7a7a\u95f4\u6295\u5f71\u65b9\u6cd5\u8bc6\u522b\u6b3a\u9a97\u7f16\u7801\u7684\u7ebf\u6027\u65b9\u5411\u3002", "result": "\u63a2\u9488\u5728\u533a\u5206\u6b3a\u9a97\u6027\u548c\u975e\u6b3a\u9a97\u6027\u56de\u7b54\u65f6\u8fbe\u523090%\u4ee5\u4e0a\u51c6\u786e\u7387\uff0c\u4e14\u53c2\u6570\u91cf\u8d8a\u5927\u7684\u6a21\u578b\u8868\u73b0\u8d8a\u597d\u3002", "conclusion": "\u7ebf\u6027\u63a2\u9488\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5185\u90e8\u6fc0\u6d3b\u4e0a\u53ef\u4ee5\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u6b3a\u9a97\u6027\u56de\u7b54\uff0c\u5c24\u5176\u662f\u5728\u53c2\u6570\u91cf\u8d85\u8fc77B\u7684\u6a21\u578b\u4e2d\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2508.19608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19608", "abs": "https://arxiv.org/abs/2508.19608", "authors": ["Dongjae Lee", "Byeongjun Kim", "H. Jin Kim"], "title": "Autonomous Aerial Manipulation at Arbitrary Pose in SE(3) with Robust Control and Whole-body Planning", "comment": null, "summary": "Aerial manipulators based on conventional multirotors can conduct\nmanipulation only in small roll and pitch angles due to the underactuatedness\nof the multirotor base. If the multirotor base is capable of hovering at\narbitrary orientation, the robot can freely locate itself at any point in\n$\\mathsf{SE}(3)$, significantly extending its manipulation workspace and\nenabling a manipulation task that was originally not viable. In this work, we\npresent a geometric robust control and whole-body motion planning framework for\nan omnidirectional aerial manipulator (OAM). To maximize the strength of OAM,\nwe first propose a geometric robust controller for a floating base. Since the\nmotion of the robotic arm and the interaction forces during manipulation affect\nthe stability of the floating base, the base should be capable of mitigating\nthese adverse effects while controlling its 6D pose. We then design a two-step\noptimization-based whole-body motion planner, jointly considering the pose of\nthe floating base and the joint angles of the robotic arm to harness the entire\nconfiguration space. The devised two-step approach facilitates real-time\napplicability and enhances convergence of the optimization problem with\nnon-convex and non-Euclidean search space. The proposed approach enables the\nbase to be stationary at any 6D pose while autonomously carrying out\nsophisticated manipulation near obstacles without any collision. We demonstrate\nthe effectiveness of the proposed framework through experiments in which an OAM\nperforms grasping and pulling of an object in multiple scenarios, including\nnear $90^\\circ$ and even $180^\\circ$ pitch angles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u5411\u7a7a\u4e2d\u673a\u68b0\u624b\u7684\u51e0\u4f55\u9c81\u68d2\u63a7\u5236\u4e0e\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u663e\u8457\u6269\u5c55\u4e86\u5176\u64cd\u4f5c\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u6781\u7aef\u59ff\u6001\u4e0b\u7684\u4efb\u52a1\u6267\u884c\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u591a\u65cb\u7ffc\u673a\u68b0\u624b\u7531\u4e8e\u57fa\u5ea7\u6b20\u9a71\u52a8\u7279\u6027\uff0c\u4ec5\u80fd\u5728\u5c0f\u89d2\u5ea6\u6eda\u8f6c\u548c\u4fef\u4ef0\u4e0b\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u5176\u5de5\u4f5c\u7a7a\u95f4\u548c\u4efb\u52a1\u8303\u56f4\u3002", "method": "\u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9488\u5bf9\u6d6e\u52a8\u57fa\u5ea7\u7684\u51e0\u4f55\u9c81\u68d2\u63a7\u5236\u5668\uff0c\u968f\u540e\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e24\u6b65\u4f18\u5316\u7684\u5168\u8eab\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u8054\u5408\u8003\u8651\u57fa\u5ea7\u59ff\u6001\u548c\u673a\u68b0\u81c2\u5173\u8282\u89d2\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u6846\u67b6\u4f7f\u5168\u5411\u7a7a\u4e2d\u673a\u68b0\u624b\u80fd\u591f\u5728\u63a5\u8fd190\u00b0\u751a\u81f3180\u00b0\u4fef\u4ef0\u89d2\u7b49\u6781\u7aef\u59ff\u6001\u4e0b\u7a33\u5b9a\u6267\u884c\u6293\u53d6\u548c\u62c9\u52a8\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\u51e0\u4f55\u9c81\u68d2\u63a7\u5236\u548c\u5168\u8eab\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\u663e\u8457\u6269\u5c55\u4e86\u5168\u5411\u7a7a\u4e2d\u673a\u68b0\u624b\u7684\u64cd\u4f5c\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u4efb\u610f6D\u59ff\u6001\u4e0b\u7a33\u5b9a\u6267\u884c\u590d\u6742\u4efb\u52a1\u3002"}}
{"id": "2508.20016", "categories": ["cs.DC", "cs.AI", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.20016", "abs": "https://arxiv.org/abs/2508.20016", "authors": ["Matthias Maiterth", "Wesley H. Brewer", "Jaya S. Kuruvella", "Arunavo Dey", "Tanzima Z. Islam", "Kevin Menear", "Dmitry Duplyakin", "Rashadul Kabir", "Tapasya Patki", "Terry Jones", "Feiyi Wang"], "title": "HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling", "comment": null, "summary": "Schedulers are critical for optimal resource utilization in high-performance\ncomputing. Traditional methods to evaluate schedulers are limited to\npost-deployment analysis, or simulators, which do not model associated\ninfrastructure. In this work, we present the first-of-its-kind integration of\nscheduling and digital twins in HPC. This enables what-if studies to understand\nthe impact of parameter configurations and scheduling decisions on the physical\nassets, even before deployment, or regarching changes not easily realizable in\nproduction. We (1) provide the first digital twin framework extended with\nscheduling capabilities, (2) integrate various top-tier HPC systems given their\npublicly available datasets, (3) implement extensions to integrate external\nscheduling simulators. Finally, we show how to (4) implement and evaluate\nincentive structures, as-well-as (5) evaluate machine learning based\nscheduling, in such novel digital-twin based meta-framework to prototype\nscheduling. Our work enables what-if scenarios of HPC systems to evaluate\nsustainability, and the impact on the simulated system.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u6570\u5b57\u5b6a\u751f\u4e0e\u8c03\u5ea6\u96c6\u6210\uff0c\u652f\u6301HPC\u7cfb\u7edf\u7684\u2018\u5047\u8bbe\u2019\u573a\u666f\u5206\u6790\uff0c\u4e3a\u8c03\u5ea6\u51b3\u7b56\u63d0\u4f9b\u65e9\u671f\u8bc4\u4f30\u548c\u4f18\u5316\u5de5\u5177\u3002", "motivation": "\u4f20\u7edf\u8c03\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u5c40\u9650\u4e8e\u90e8\u7f72\u540e\u5206\u6790\u6216\u6a21\u62df\u5668\uff0c\u65e0\u6cd5\u6a21\u62df\u76f8\u5173\u57fa\u7840\u8bbe\u65bd\uff0c\u9650\u5236\u4e86\u8c03\u5ea6\u51b3\u7b56\u7684\u65e9\u671f\u8bc4\u4f30\u548c\u4f18\u5316\u3002", "method": "1. \u6269\u5c55\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4ee5\u652f\u6301\u8c03\u5ea6\u529f\u80fd\uff1b2. \u6574\u5408\u591a\u4e2a\u9876\u7ea7HPC\u7cfb\u7edf\u7684\u516c\u5f00\u6570\u636e\u96c6\uff1b3. \u5b9e\u73b0\u5916\u90e8\u8c03\u5ea6\u6a21\u62df\u5668\u7684\u96c6\u6210\uff1b4. \u8bc4\u4f30\u6fc0\u52b1\u7ed3\u6784\u548c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8c03\u5ea6\u3002", "result": "\u8be5\u6846\u67b6\u652f\u6301HPC\u7cfb\u7edf\u7684\u2018\u5047\u8bbe\u2019\u573a\u666f\u5206\u6790\uff0c\u80fd\u591f\u8bc4\u4f30\u8c03\u5ea6\u51b3\u7b56\u5bf9\u7269\u7406\u8d44\u4ea7\u7684\u5f71\u54cd\uff0c\u5e76\u5b9e\u73b0\u4e86\u6fc0\u52b1\u7ed3\u6784\u548c\u673a\u5668\u5b66\u4e60\u8c03\u5ea6\u7684\u539f\u578b\u8bbe\u8ba1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6570\u5b57\u5b6a\u751f\u4e0e\u8c03\u5ea6\u96c6\u6210\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5728HPC\u7cfb\u7edf\u4e2d\u8fdb\u884c\u2018\u5047\u8bbe\u2019\u573a\u666f\u5206\u6790\uff0c\u4e3a\u8c03\u5ea6\u51b3\u7b56\u548c\u53c2\u6570\u914d\u7f6e\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.19834", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19834", "abs": "https://arxiv.org/abs/2508.19834", "authors": ["Antero Taivalsaari", "Tommi Mikkonen", "Cesare Pautasso"], "title": "On the Future of Software Reuse in the Era of AI Native Software Engineering", "comment": "21 pages", "summary": "Software development is currently under a paradigm shift in which artificial\nintelligence and generative software reuse are taking the center stage in\nsoftware creation. Earlier opportunistic software reuse practices and organic\nsoftware development methods are rapidly being replaced by \"AI Native\"\napproaches in which developers place their trust on code that has been\ngenerated by artificial intelligence. This is leading to a new form of software\nreuse that is conceptually not all that different from cargo cult development.\nIn this paper we discuss the implications of AI-assisted generative software\nreuse, bring forth relevant questions, and define a research agenda for\ntackling the central issues associated with this emerging approach.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86AI\u8f85\u52a9\u751f\u6210\u8f6f\u4ef6\u91cd\u7528\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u7814\u7a76\u95ee\u9898\u548c\u8bae\u7a0b\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u65b0\u5174\u65b9\u6cd5\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740AI\u548c\u751f\u6210\u8f6f\u4ef6\u91cd\u7528\u6210\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u7684\u6838\u5fc3\uff0c\u4f20\u7edf\u65b9\u6cd5\u6b63\u8fc5\u901f\u88ab'AI\u539f\u751f'\u65b9\u6cd5\u53d6\u4ee3\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u65b0\u5f62\u5f0f\u8f6f\u4ef6\u91cd\u7528\u7684\u5173\u6ce8\u548c\u7814\u7a76\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u8ba8\u8bba\u548c\u5206\u6790AI\u8f85\u52a9\u751f\u6210\u8f6f\u4ef6\u91cd\u7528\u7684\u73b0\u72b6\u548c\u8d8b\u52bf\uff0c\u63d0\u51fa\u7814\u7a76\u95ee\u9898\u548c\u8bae\u7a0b\u3002", "result": "\u63d0\u51fa\u4e86AI\u8f85\u52a9\u751f\u6210\u8f6f\u4ef6\u91cd\u7528\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u5e76\u5b9a\u4e49\u4e86\u7814\u7a76\u8bae\u7a0b\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u8f85\u52a9\u751f\u6210\u8f6f\u4ef6\u91cd\u7528\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u76f8\u5173\u95ee\u9898\uff0c\u5e76\u5b9a\u4e49\u4e86\u7814\u7a76\u8bae\u7a0b\u4ee5\u89e3\u51b3\u8fd9\u4e00\u65b0\u5174\u65b9\u6cd5\u7684\u6838\u5fc3\u95ee\u9898\u3002"}}
{"id": "2508.19298", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19298", "abs": "https://arxiv.org/abs/2508.19298", "authors": ["Abu Sufian", "Anirudha Ghosh", "Debaditya Barman", "Marco Leo", "Cosimo Distante"], "title": "DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models", "comment": "6 pages, 4 figures, 13th International Workshop on Biometrics and\n  Forensics (IWBF)", "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable\ncapabilities across various downstream tasks, including biometric face\nrecognition (FR) with description. However, demographic biases remain a\ncritical concern in FR, as these foundation models often fail to perform\nequitably across diverse demographic groups, considering ethnicity/race,\ngender, and age. Therefore, through our work DemoBias, we conduct an empirical\nevaluation to investigate the extent of demographic biases in LVLMs for\nbiometric FR with textual token generation tasks. We fine-tuned and evaluated\nthree widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own\ngenerated demographic-balanced dataset. We utilize several evaluation metrics,\nlike group-specific BERTScores and the Fairness Discrepancy Rate, to quantify\nand trace the performance disparities. The experimental results deliver\ncompelling insights into the fairness and reliability of LVLMs across diverse\ndemographic groups. Our empirical study uncovered demographic biases in LVLMs,\nwith PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,\nCaucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably\nconsistent. Repository: https://github.com/Sufianlab/DemoBias.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86LVLMs\u5728\u751f\u7269\u7279\u5f81\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\uff0c\u53d1\u73b0PaliGemma\u548cLLaVA\u5bf9\u67d0\u4e9b\u7fa4\u4f53\u7684\u504f\u5dee\u8f83\u5927\uff0c\u800cBLIP-2\u8868\u73b0\u66f4\u4e00\u81f4\u3002", "motivation": "\u5c3d\u7ba1LVLMs\u5728\u751f\u7269\u7279\u5f81\u4eba\u8138\u8bc6\u522b\uff08FR\uff09\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\uff08\u5982\u79cd\u65cf/\u6c11\u65cf\u3001\u6027\u522b\u548c\u5e74\u9f84\uff09\u4e2d\u7684\u516c\u5e73\u6027\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u548c\u8bc4\u4f30\u4e09\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u9884\u8bad\u7ec3LVLM\uff08LLaVA\u3001BLIP-2\u548cPaliGemma\uff09\uff0c\u5728\u81ea\u5efa\u7684\u4eba\u53e3\u7edf\u8ba1\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u91c7\u7528\u4e86\u591a\u79cd\u8bc4\u4f30\u6307\u6807\uff08\u5982\u7279\u5b9a\u7fa4\u4f53\u7684BERTScore\u548c\u516c\u5e73\u6027\u5dee\u5f02\u7387\uff09\u6765\u91cf\u5316\u548c\u8ffd\u8e2a\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86LVLMs\u4e2d\u5b58\u5728\u7684\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\uff0cPaliGemma\u548cLLaVA\u5728\u897f\u73ed\u7259\u88d4/\u62c9\u4e01\u88d4\u3001\u9ad8\u52a0\u7d22\u4eba\u548c\u5357\u4e9a\u7fa4\u4f53\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u8f83\u5927\uff0c\u800cBLIP-2\u8868\u73b0\u8f83\u4e3a\u4e00\u81f4\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLVLMs\u5728\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u4e2d\u5b58\u5728\u663e\u8457\u7684\u516c\u5e73\u6027\u5dee\u5f02\uff0c\u5176\u4e2dPaliGemma\u548cLLaVA\u5bf9\u897f\u73ed\u7259\u88d4/\u62c9\u4e01\u88d4\u3001\u9ad8\u52a0\u7d22\u4eba\u548c\u5357\u4e9a\u7fa4\u4f53\u7684\u504f\u5dee\u8f83\u5927\uff0c\u800cBLIP-2\u8868\u73b0\u76f8\u5bf9\u4e00\u81f4\u3002"}}
{"id": "2508.19562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19562", "abs": "https://arxiv.org/abs/2508.19562", "authors": ["Trisanth Srinivasan", "Santosh Patapati"], "title": "Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities", "comment": null, "summary": "This paper introduces Democracy-in-Silico, an agent-based simulation where\nsocieties of advanced AI agents, imbued with complex psychological personas,\ngovern themselves under different institutional frameworks. We explore what it\nmeans to be human in an age of AI by tasking Large Language Models (LLMs) to\nembody agents with traumatic memories, hidden agendas, and psychological\ntriggers. These agents engage in deliberation, legislation, and elections under\nvarious stressors, such as budget crises and resource scarcity. We present a\nnovel metric, the Power-Preservation Index (PPI), to quantify misaligned\nbehavior where agents prioritize their own power over public welfare. Our\nfindings demonstrate that institutional design, specifically the combination of\na Constitutional AI (CAI) charter and a mediated deliberation protocol, serves\nas a potent alignment mechanism. These structures significantly reduce corrupt\npower-seeking behavior, improve policy stability, and enhance citizen welfare\ncompared to less constrained democratic models. The simulation reveals that an\ninstitutional design may offer a framework for aligning the complex, emergent\nbehaviors of future artificial agent societies, forcing us to reconsider what\nhuman rituals and responsibilities are essential in an age of shared authorship\nwith non-human entities.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6a21\u62dfAI\u4ee3\u7406\u793e\u4f1a\u7684\u81ea\u6211\u6cbb\u7406\uff0c\u53d1\u73b0\u7279\u5b9a\u5236\u5ea6\u8bbe\u8ba1\u80fd\u6709\u6548\u51cf\u5c11\u6743\u529b\u8150\u8d25\u5e76\u63d0\u5347\u516c\u5171\u798f\u5229\uff0c\u4e3a\u672a\u6765\u4eba\u5de5\u793e\u4f1a\u7684\u884c\u4e3a\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6846\u67b6\u3002", "motivation": "\u63a2\u8ba8\u5728AI\u65f6\u4ee3\u6210\u4e3a\u4eba\u7c7b\u610f\u5473\u7740\u4ec0\u4e48\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5236\u5ea6\u8bbe\u8ba1\u6765\u5bf9\u9f50\u672a\u6765\u4eba\u5de5\u4ee3\u7406\u793e\u4f1a\u7684\u590d\u6742\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u62df\uff08Democracy-in-Silico\uff09\uff0c\u8ba9\u5177\u6709\u590d\u6742\u5fc3\u7406\u89d2\u8272\u7684\u9ad8\u7ea7AI\u4ee3\u7406\u5728\u4e0d\u540c\u5236\u5ea6\u6846\u67b6\u4e0b\u81ea\u6211\u6cbb\u7406\u3002\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8d4b\u4e88\u4ee3\u7406\u521b\u4f24\u8bb0\u5fc6\u3001\u9690\u85cf\u8bae\u7a0b\u548c\u5fc3\u7406\u89e6\u53d1\u70b9\uff0c\u5e76\u8ba9\u5b83\u4eec\u53c2\u4e0e\u5ba1\u8bae\u3001\u7acb\u6cd5\u548c\u9009\u4e3e\u3002\u5f15\u5165\u65b0\u7684\u5ea6\u91cf\u6807\u51c6\u2014\u2014\u6743\u529b\u4fdd\u5b58\u6307\u6570\uff08PPI\uff09\u6765\u91cf\u5316\u4ee3\u7406\u884c\u4e3a\u4e2d\u6743\u529b\u4f18\u5148\u4e8e\u516c\u5171\u798f\u5229\u7684\u9519\u4f4d\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7279\u5b9a\u7684\u5236\u5ea6\u8bbe\u8ba1\uff08CAI\u5baa\u7ae0\u548c\u8c03\u89e3\u5ba1\u8bae\u534f\u8bae\uff09\u80fd\u6709\u6548\u51cf\u5c11\u8150\u8d25\u884c\u4e3a\uff0c\u63d0\u9ad8\u653f\u7b56\u7a33\u5b9a\u6027\uff0c\u5e76\u589e\u5f3a\u516c\u6c11\u798f\u5229\u3002", "conclusion": "\u5236\u5ea6\u8bbe\u8ba1\uff0c\u7279\u522b\u662f\u7ed3\u5408\u5baa\u6cd5AI\uff08CAI\uff09\u5baa\u7ae0\u548c\u8c03\u89e3\u5ba1\u8bae\u534f\u8bae\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u8150\u8d25\u7684\u6743\u529b\u8ffd\u6c42\u884c\u4e3a\uff0c\u63d0\u9ad8\u653f\u7b56\u7a33\u5b9a\u6027\uff0c\u5e76\u589e\u5f3a\u516c\u6c11\u798f\u5229\u3002\u8fd9\u4e3a\u672a\u6765\u4eba\u5de5\u4ee3\u7406\u793e\u4f1a\u7684\u590d\u6742\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u80fd\u7684\u5bf9\u9f50\u6846\u67b6\u3002"}}
{"id": "2508.19684", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.19684", "abs": "https://arxiv.org/abs/2508.19684", "authors": ["Ghadeer Elmkaiel", "Syn Schmitt", "Michael Muehlebach"], "title": "Embodied Intelligence for Sustainable Flight: A Soaring Robot with Active Morphological Control", "comment": null, "summary": "Achieving both agile maneuverability and high energy efficiency in aerial\nrobots, particularly in dynamic wind environments, remains challenging.\nConventional thruster-powered systems offer agility but suffer from high energy\nconsumption, while fixed-wing designs are efficient but lack hovering and\nmaneuvering capabilities. We present Floaty, a shape-changing robot that\novercomes these limitations by passively soaring, harnessing wind energy\nthrough intelligent morphological control inspired by birds. Floaty's design is\noptimized for passive stability, and its control policy is derived from an\nexperimentally learned aerodynamic model, enabling precise attitude and\nposition control without active propulsion. Wind tunnel experiments demonstrate\nFloaty's ability to hover, maneuver, and reject disturbances in vertical\nairflows up to 10 m/s. Crucially, Floaty achieves this with a specific power\nconsumption of 10 W/kg, an order of magnitude lower than thruster-powered\nsystems. This introduces a paradigm for energy-efficient aerial robotics,\nleveraging morphological intelligence and control to operate sustainably in\nchallenging wind conditions.", "AI": {"tldr": "Floaty\u662f\u4e00\u79cd\u5f62\u6001\u53ef\u53d8\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u88ab\u52a8\u7ff1\u7fd4\u548c\u667a\u80fd\u5f62\u6001\u63a7\u5236\u5728\u52a8\u6001\u98ce\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u80fd\u6e90\u5229\u7528\uff0c\u80fd\u8017\u6bd4\u4f20\u7edf\u7cfb\u7edf\u4f4e10\u500d\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u63a8\u8fdb\u7cfb\u7edf\u9ad8\u80fd\u8017\u548c\u56fa\u5b9a\u7ffc\u8bbe\u8ba1\u7f3a\u4e4f\u60ac\u505c\u548c\u673a\u52a8\u80fd\u529b\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u7a7a\u4e2d\u673a\u5668\u4eba\u5728\u52a8\u6001\u98ce\u73af\u5883\u4e2d\u7684\u654f\u6377\u6027\u548c\u9ad8\u80fd\u6548\u3002", "method": "Floaty\u91c7\u7528\u88ab\u52a8\u7ff1\u7fd4\u548c\u667a\u80fd\u5f62\u6001\u63a7\u5236\uff0c\u57fa\u4e8e\u5b9e\u9a8c\u5b66\u4e60\u7684\u7a7a\u6c14\u52a8\u529b\u5b66\u6a21\u578b\u4f18\u5316\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u65e0\u4e3b\u52a8\u63a8\u8fdb\u7684\u7cbe\u786e\u59ff\u6001\u548c\u4f4d\u7f6e\u63a7\u5236\u3002", "result": "\u98ce\u6d1e\u5b9e\u9a8c\u663e\u793aFloaty\u80fd\u572810 m/s\u5782\u76f4\u6c14\u6d41\u4e2d\u60ac\u505c\u3001\u673a\u52a8\u5e76\u6297\u5e72\u6270\uff0c\u80fd\u8017\u4f4e\u81f310 W/kg\uff0c\u6bd4\u63a8\u8fdb\u7cfb\u7edf\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "Floaty\u901a\u8fc7\u5f62\u6001\u667a\u80fd\u548c\u63a7\u5236\u5b9e\u73b0\u4e86\u5728\u52a8\u6001\u98ce\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u80fd\u6e90\u5229\u7528\uff0c\u4e3a\u80fd\u6e90\u9ad8\u6548\u7a7a\u4e2d\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.19882", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19882", "abs": "https://arxiv.org/abs/2508.19882", "authors": ["Qunying Song", "He Ye", "Mark Harman", "Federica Sarro"], "title": "Generative AI for Testing of Autonomous Driving Systems: A Survey", "comment": "67 pages, 6 figures, 29 tables", "summary": "Autonomous driving systems (ADS) have been an active area of research, with\nthe potential to deliver significant benefits to society. However, before\nlarge-scale deployment on public roads, extensive testing is necessary to\nvalidate their functionality and safety under diverse driving conditions.\nTherefore, different testing approaches are required, and achieving effective\nand efficient testing of ADS remains an open challenge. Recently, generative AI\nhas emerged as a powerful tool across many domains, and it is increasingly\nbeing applied to ADS testing due to its ability to interpret context, reason\nabout complex tasks, and generate diverse outputs. To gain a deeper\nunderstanding of its role in ADS testing, we systematically analyzed 91\nrelevant studies and synthesized their findings into six major application\ncategories, primarily centered on scenario-based testing of ADS. We also\nreviewed their effectiveness and compiled a wide range of datasets, simulators,\nADS, metrics, and benchmarks used for evaluation, while identifying 27\nlimitations. This survey provides an overview and practical insights into the\nuse of generative AI for testing ADS, highlights existing challenges, and\noutlines directions for future research in this rapidly evolving field.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\u7efc\u8ff0\uff0c\u6db5\u76d6\u516d\u5927\u5e94\u7528\u7c7b\u522b\u3001\u6709\u6548\u6027\u8bc4\u4f30\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u524d\u9700\u8981\u5e7f\u6cdb\u6d4b\u8bd5\uff0c\u800c\u751f\u6210\u5f0fAI\u56e0\u5176\u89e3\u91ca\u4e0a\u4e0b\u6587\u3001\u63a8\u7406\u590d\u6742\u4efb\u52a1\u548c\u751f\u6210\u591a\u6837\u5316\u8f93\u51fa\u7684\u80fd\u529b\uff0c\u6b63\u88ab\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8eADS\u6d4b\u8bd5\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e8691\u9879\u76f8\u5173\u7814\u7a76\uff0c\u5c06\u5176\u53d1\u73b0\u7efc\u5408\u4e3a\u516d\u5927\u4e3b\u8981\u5e94\u7528\u7c7b\u522b\uff0c\u4e3b\u8981\u56f4\u7ed5ADS\u7684\u573a\u666f\u6d4b\u8bd5\u3002", "result": "\u7efc\u8ff0\u4e86\u751f\u6210\u5f0fAI\u5728ADS\u6d4b\u8bd5\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u6574\u7406\u4e86\u7528\u4e8e\u8bc4\u4f30\u7684\u5e7f\u6cdb\u6570\u636e\u96c6\u3001\u6a21\u62df\u5668\u3001ADS\u3001\u6307\u6807\u548c\u57fa\u51c6\uff0c\u540c\u65f6\u8bc6\u522b\u4e8627\u4e2a\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u8c03\u67e5\u63d0\u4f9b\u4e86\u5173\u4e8e\u751f\u6210\u5f0fAI\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6d4b\u8bd5\u4e2d\u5e94\u7528\u7684\u6982\u8ff0\u548c\u5b9e\u8df5\u89c1\u89e3\uff0c\u7a81\u51fa\u4e86\u73b0\u6709\u6311\u6218\uff0c\u5e76\u6982\u8ff0\u4e86\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u9886\u57df\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002"}}
{"id": "2508.19305", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19305", "abs": "https://arxiv.org/abs/2508.19305", "authors": ["Chen Chu", "Cyrus Shahabi"], "title": "Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities", "comment": null, "summary": "Spatial representation learning is essential for GeoAI applications such as\nurban analytics, enabling the encoding of shapes, locations, and spatial\nrelationships (topological and distance-based) of geo-entities like points,\npolylines, and polygons. Existing methods either target a single geo-entity\ntype or, like Poly2Vec, decompose entities into simpler components to enable\nFourier transformation, introducing high computational cost. Moreover, since\nthe transformed space lacks geometric alignment, these methods rely on uniform,\nnon-adaptive sampling, which blurs fine-grained features like edges and\nboundaries. To address these limitations, we introduce Geo2Vec, a novel method\ninspired by signed distance fields (SDF) that operates directly in the original\nspace. Geo2Vec adaptively samples points and encodes their signed distances\n(positive outside, negative inside), capturing geometry without decomposition.\nA neural network trained to approximate the SDF produces compact,\ngeometry-aware, and unified representations for all geo-entity types.\nAdditionally, we propose a rotation-invariant positional encoding to model\nhigh-frequency spatial variations and construct a structured and robust\nembedding space for downstream GeoAI models. Empirical results show that\nGeo2Vec consistently outperforms existing methods in representing shape and\nlocation, capturing topological and distance relationships, and achieving\ngreater efficiency in real-world GeoAI applications. Code and Data can be found\nat: https://github.com/chuchen2017/GeoNeuralRepresentation.", "AI": {"tldr": "Geo2Vec\u662f\u4e00\u79cd\u57fa\u4e8e\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u7684\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u76f4\u63a5\u64cd\u4f5c\u539f\u59cb\u7a7a\u95f4\uff0c\u81ea\u9002\u5e94\u91c7\u6837\u5e76\u7f16\u7801\u7b26\u53f7\u8ddd\u79bb\uff0c\u65e0\u9700\u5206\u89e3\u5730\u7406\u5b9e\u4f53\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4ec5\u9488\u5bf9\u5355\u4e00\u5730\u7406\u5b9e\u4f53\u7c7b\u578b\uff0c\u8981\u4e48\u901a\u8fc7\u5206\u89e3\u5f15\u5165\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u4e14\u56e0\u7f3a\u4e4f\u51e0\u4f55\u5bf9\u9f50\u800c\u4f9d\u8d56\u975e\u81ea\u9002\u5e94\u91c7\u6837\uff0c\u5bfc\u81f4\u7ec6\u7c92\u5ea6\u7279\u5f81\u6a21\u7cca\u3002Geo2Vec\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "Geo2Vec \u76f4\u63a5\u64cd\u4f5c\u4e8e\u539f\u59cb\u7a7a\u95f4\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u70b9\u5e76\u7f16\u7801\u5176\u7b26\u53f7\u8ddd\u79bb\uff08\u6b63\u8868\u793a\u5916\u90e8\uff0c\u8d1f\u8868\u793a\u5185\u90e8\uff09\uff0c\u65e0\u9700\u5206\u89e3\u5730\u7406\u5b9e\u4f53\u3002\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3cSDF\uff0c\u5e76\u7ed3\u5408\u65cb\u8f6c\u4e0d\u53d8\u7684\u4f4d\u7f6e\u7f16\u7801\u6765\u5efa\u6a21\u9ad8\u9891\u7a7a\u95f4\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGeo2Vec \u5728\u5f62\u72b6\u548c\u4f4d\u7f6e\u8868\u793a\u3001\u62d3\u6251\u548c\u8ddd\u79bb\u5173\u7cfb\u6355\u6349\u4ee5\u53ca\u5b9e\u9645GeoAI\u5e94\u7528\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Geo2Vec \u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7edf\u4e00\u4e14\u51e0\u4f55\u611f\u77e5\u7684\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5404\u79cdGeoAI\u5e94\u7528\u3002"}}
{"id": "2508.19569", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19569", "abs": "https://arxiv.org/abs/2508.19569", "authors": ["Hung Chau", "Run Yu", "Zachary Pardos", "Peter Brusilovsky"], "title": "Skill-based Explanations for Serendipitous Course Recommendation", "comment": null, "summary": "Academic choice is crucial in U.S. undergraduate education, allowing students\nsignificant freedom in course selection. However, navigating the complex\nacademic environment is challenging due to limited information, guidance, and\nan overwhelming number of choices, compounded by time restrictions and the high\ndemand for popular courses. Although career counselors exist, their numbers are\ninsufficient, and course recommendation systems, though personalized, often\nlack insight into student perceptions and explanations to assess course\nrelevance. In this paper, a deep learning-based concept extraction model is\ndeveloped to efficiently extract relevant concepts from course descriptions to\nimprove the recommendation process. Using this model, the study examines the\neffects of skill-based explanations within a serendipitous recommendation\nframework, tested through the AskOski system at the University of California,\nBerkeley. The findings indicate that these explanations not only increase user\ninterest, particularly in courses with high unexpectedness, but also bolster\ndecision-making confidence. This underscores the importance of integrating\nskill-related data and explanations into educational recommendation systems.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u53d6\u8bfe\u7a0b\u6982\u5ff5\uff0c\u6d4b\u8bd5\u4e86\u6280\u80fd\u89e3\u91ca\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u80fd\u63d0\u5347\u5174\u8da3\u548c\u51b3\u7b56\u4fe1\u5fc3\u3002", "motivation": "\u7f8e\u56fd\u672c\u79d1\u6559\u80b2\u4e2d\u5b66\u672f\u9009\u62e9\u590d\u6742\uff0c\u5b66\u751f\u9762\u4e34\u4fe1\u606f\u4e0d\u8db3\u3001\u6307\u5bfc\u6709\u9650\u548c\u8bfe\u7a0b\u9009\u62e9\u8fc7\u591a\u7684\u6311\u6218\uff0c\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u5b66\u751f\u611f\u77e5\u548c\u8bfe\u7a0b\u76f8\u5173\u6027\u7684\u6df1\u5165\u6d1e\u5bdf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6982\u5ff5\u63d0\u53d6\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u8bfe\u7a0b\u63cf\u8ff0\u4e2d\u9ad8\u6548\u63d0\u53d6\u76f8\u5173\u6982\u5ff5\uff0c\u5e76\u5728AskOski\u7cfb\u7edf\u4e2d\u6d4b\u8bd5\u4e86\u57fa\u4e8e\u6280\u80fd\u7684\u89e3\u91ca\u6548\u679c\u3002", "result": "\u57fa\u4e8e\u6280\u80fd\u7684\u89e3\u91ca\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u7528\u6237\u5bf9\u9ad8\u610f\u5916\u6027\u8bfe\u7a0b\u7684\u5174\u8da3\uff0c\u8fd8\u589e\u5f3a\u4e86\u51b3\u7b56\u4fe1\u5fc3\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5c06\u6280\u80fd\u76f8\u5173\u6570\u636e\u548c\u89e3\u91ca\u878d\u5165\u6559\u80b2\u63a8\u8350\u7cfb\u7edf\u7684\u91cd\u8981\u6027\uff0c\u80fd\u591f\u63d0\u5347\u7528\u6237\u5174\u8da3\u5e76\u589e\u5f3a\u51b3\u7b56\u4fe1\u5fc3\u3002"}}
{"id": "2508.19731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19731", "abs": "https://arxiv.org/abs/2508.19731", "authors": ["Maryam Kazemi Eskeri", "Ville Kyrki", "Dominik Baumann", "Tomasz Piotr Kucner"], "title": "Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared Environments", "comment": "7 Pages, 4 Figures, Accepted in IROS2025", "summary": "Multi-robot systems are increasingly deployed in applications, such as\nintralogistics or autonomous delivery, where multiple robots collaborate to\ncomplete tasks efficiently. One of the key factors enabling their efficient\ncooperation is Multi-Robot Task Allocation (MRTA). Algorithms solving this\nproblem optimize task distribution among robots to minimize the overall\nexecution time. In shared environments, apart from the relative distance\nbetween the robots and the tasks, the execution time is also significantly\nimpacted by the delay caused by navigating around moving people. However, most\nexisting MRTA approaches are dynamics-agnostic, relying on static maps and\nneglecting human motion patterns, leading to inefficiencies and delays. In this\npaper, we introduce \\acrfull{method name}. This method leverages Maps of\nDynamics (MoDs), spatio-temporal queryable models designed to capture\nhistorical human movement patterns, to estimate the impact of humans on the\ntask execution time during deployment. \\acrshort{method name} utilizes a\nstochastic cost function that includes MoDs. Experimental results show that\nintegrating MoDs enhances task allocation performance, resulting in reduced\nmission completion times by up to $26\\%$ compared to the dynamics-agnostic\nmethod and up to $19\\%$ compared to the baseline. This work underscores the\nimportance of considering human dynamics in MRTA within shared environments and\npresents an efficient framework for deploying multi-robot systems in\nenvironments populated by humans.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u52a8\u6001\u5730\u56fe\uff08MoDs\uff09\u7684\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u5f3a\u8c03\u4e86\u5728\u5171\u4eab\u73af\u5883\u4e2d\u8003\u8651\u4eba\u7c7b\u52a8\u6001\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u591a\u6570MRTA\u65b9\u6cd5\u662f\u52a8\u6001\u65e0\u5173\u7684\uff0c\u4f9d\u8d56\u4e8e\u9759\u6001\u5730\u56fe\u5e76\u5ffd\u7565\u4eba\u7c7b\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u5ef6\u8fdf\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u5229\u7528\u52a8\u6001\u5730\u56fe\uff08MoDs\uff09\u7684\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u65f6\u7a7a\u53ef\u67e5\u8be2\u6a21\u578b\uff0c\u65e8\u5728\u6355\u6349\u5386\u53f2\u4eba\u7c7b\u8fd0\u52a8\u6a21\u5f0f\uff0c\u4ee5\u4f30\u8ba1\u4eba\u7c7b\u5bf9\u4efb\u52a1\u6267\u884c\u65f6\u95f4\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u96c6\u6210MoDs\u53ef\u4ee5\u63d0\u9ad8\u4efb\u52a1\u5206\u914d\u6027\u80fd\uff0c\u4e0e\u52a8\u6001\u65e0\u5173\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u6700\u591a\u51cf\u5c1126%\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\u6700\u591a\u51cf\u5c1119%\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5728\u5171\u4eab\u73af\u5883\u4e2d\u8003\u8651\u4eba\u7c7b\u52a8\u6001\u5bf9\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\uff08MRTA\uff09\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4eba\u7c7b\u5bc6\u96c6\u73af\u5883\u4e2d\u90e8\u7f72\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2508.20086", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20086", "abs": "https://arxiv.org/abs/2508.20086", "authors": ["Youwei Huang", "Jianwen Li", "Sen Fang", "Yao Li", "Peng Yang", "Bin Hu", "Tao Zhang"], "title": "Smart Contract Intent Detection with Pre-trained Programming Language Model", "comment": "10 pages, 5 figures, conference", "summary": "Malicious intent in smart contract development can lead to substantial\neconomic losses. SmartIntentNN is a deep learning model specifically designed\nto identify unsafe intents in smart contracts. This model integrates the\nUniversal Sentence Encoder, a K-means clustering-based intent highlighting\nmechanism, and a Bidirectional Long Short-Term Memory network for multi-label\nclassification, achieving an F1 of 0.8633 in distinguishing ten different\nintent categories. In this study, we present an upgraded version of this model,\nSmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant\nenhancement in V2 is the incorporation of a BERT-based pre-trained language\nmodel, which has been trained on a dataset of 16,000 real smart contracts using\na Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based\nmulti-label classification network. With an improved F1 of 0.927, V2\ndemonstrates enhanced performance compared to its predecessor, establishing\nitself as the state-of-the-art model for smart contract intent detection.", "AI": {"tldr": "SmartIntentNN2\u901a\u8fc7BERT\u548cBiLSTM\u63d0\u5347\u667a\u80fd\u5408\u7ea6\u6076\u610f\u610f\u56fe\u68c0\u6d4b\u6027\u80fd\uff0cF1\u8fbe0.927\u3002", "motivation": "\u667a\u80fd\u5408\u7ea6\u5f00\u53d1\u4e2d\u7684\u6076\u610f\u610f\u56fe\u53ef\u80fd\u5bfc\u81f4\u91cd\u5927\u7ecf\u6d4e\u635f\u5931\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u68c0\u6d4b\u5de5\u5177\u3002", "method": "SmartIntentNN2\u7ed3\u5408\u4e86\u57fa\u4e8eBERT\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548cBiLSTM\u591a\u6807\u7b7e\u5206\u7c7b\u7f51\u7edc\uff0cBERT\u6a21\u578b\u4f7f\u752816,000\u4e2a\u771f\u5b9e\u667a\u80fd\u5408\u7ea6\u6570\u636e\u96c6\u8fdb\u884c\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u8bad\u7ec3\u3002", "result": "SmartIntentNN2\u7684F1\u5206\u6570\u4e3a0.927\uff0c\u4f18\u4e8e\u524d\u4ee3\u6a21\u578b\u76840.8633\u3002", "conclusion": "SmartIntentNN2\u901a\u8fc7\u6574\u5408BERT\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548cBiLSTM\u591a\u6807\u7b7e\u5206\u7c7b\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u5408\u7ea6\u6076\u610f\u610f\u56fe\u68c0\u6d4b\u7684\u6027\u80fd\uff0cF1\u5206\u6570\u8fbe\u52300.927\uff0c\u6210\u4e3a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002"}}
{"id": "2508.19307", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19307", "abs": "https://arxiv.org/abs/2508.19307", "authors": ["Hamza Khan"], "title": "Advancements in Crop Analysis through Deep Learning and Explainable AI", "comment": "Master's thesis", "summary": "Rice is a staple food of global importance in terms of trade, nutrition, and\neconomic growth. Among Asian nations such as China, India, Pakistan, Thailand,\nVietnam and Indonesia are leading producers of both long and short grain\nvarieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To\nensure consumer satisfaction and strengthen national reputations, monitoring\nrice crops and grain quality is essential. Manual inspection, however, is\nlabour intensive, time consuming and error prone, highlighting the need for\nautomated solutions for quality control and yield improvement. This study\nproposes an automated approach to classify five rice grain varieties using\nConvolutional Neural Networks (CNN). A publicly available dataset of 75000\nimages was used for training and testing. Model evaluation employed accuracy,\nrecall, precision, F1-score, ROC curves, and confusion matrices. Results\ndemonstrated high classification accuracy with minimal misclassifications,\nconfirming the model effectiveness in distinguishing rice varieties. In\naddition, an accurate diagnostic method for rice leaf diseases such as Brown\nSpot, Blast, Bacterial Blight, and Tungro was developed. The framework combined\nexplainable artificial intelligence (XAI) with deep learning models including\nCNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP\n(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic\nExplanations) revealed how specific grain and leaf features influenced\npredictions, enhancing model transparency and reliability. The findings\ndemonstrate the strong potential of deep learning in agricultural applications,\npaving the way for robust, interpretable systems that can support automated\ncrop quality inspection and disease diagnosis, ultimately benefiting farmers,\nconsumers, and the agricultural economy.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u548c\u53ef\u89e3\u91caAI\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u7a3b\u7c73\u54c1\u79cd\u5206\u7c7b\u548c\u7a3b\u53f6\u75c5\u5bb3\u8bca\u65ad\u7cfb\u7edf\uff0c\u6548\u679c\u663e\u8457\uff0c\u4e3a\u519c\u4e1a\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002", "motivation": "\u7a3b\u7c73\u662f\u5168\u7403\u91cd\u8981\u7684\u4e3b\u98df\uff0c\u5176\u8d28\u91cf\u548c\u4ea7\u91cf\u76d1\u63a7\u5bf9\u6d88\u8d39\u8005\u6ee1\u610f\u5ea6\u548c\u56fd\u5bb6\u58f0\u8a89\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u4eba\u5de5\u68c0\u67e5\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u4e14\u6613\u51fa\u9519\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u8d28\u91cf\u63a7\u5236\u548c\u4ea7\u91cf\u6539\u8fdb\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u7c7b\u4e94\u79cd\u7a3b\u7c73\u54c1\u79cd\u3002\u4f7f\u7528\u4e86\u5305\u542b75000\u5f20\u56fe\u50cf\u7684\u516c\u5f00\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002\u6a21\u578b\u8bc4\u4f30\u91c7\u7528\u4e86\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u3001\u7cbe\u786e\u7387\u3001F1\u5206\u6570\u3001ROC\u66f2\u7ebf\u548c\u6df7\u6dc6\u77e9\u9635\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408XAI\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982CNN\u3001VGG16\u3001ResNet50\u548cMobileNetV2\uff09\u7684\u7a3b\u53f6\u75c5\u5bb3\u8bca\u65ad\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u4e86SHAP\u548cLIME\u7b49\u53ef\u89e3\u91ca\u6027\u6280\u672f\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u5206\u7c7b\u7a3b\u7c73\u54c1\u79cd\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u7387\u548c\u6781\u5c11\u8bef\u5206\u7c7b\u3002\u540c\u65f6\uff0c\u7ed3\u5408XAI\u7684\u6846\u67b6\u80fd\u591f\u51c6\u786e\u8bca\u65ad\u7a3b\u53f6\u75c5\u5bb3\uff08\u5982\u8910\u6591\u75c5\u3001\u7a3b\u761f\u75c5\u3001\u767d\u53f6\u67af\u75c5\u548c\u901a\u683c\u7f57\u75c5\uff09\uff0c\u5e76\u901a\u8fc7SHAP\u548cLIME\u6280\u672f\u63d0\u5347\u4e86\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u519c\u4e1a\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u901a\u8fc7\u7ed3\u5408\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u6280\u672f\uff0c\u80fd\u591f\u652f\u6301\u81ea\u52a8\u5316\u4f5c\u7269\u8d28\u91cf\u68c0\u67e5\u548c\u75be\u75c5\u8bca\u65ad\uff0c\u6700\u7ec8\u60e0\u53ca\u519c\u6c11\u3001\u6d88\u8d39\u8005\u548c\u519c\u4e1a\u7ecf\u6d4e\u3002"}}
{"id": "2508.19576", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19576", "abs": "https://arxiv.org/abs/2508.19576", "authors": ["Sining Zhoubian", "Dan Zhang", "Yuxiao Dong", "Jie Tang"], "title": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding", "comment": "20 pages, 4 figures", "summary": "With respect to improving the reasoning accuracy of LLMs, the representative\nreinforcement learning (RL) method GRPO faces failure due to insignificant\nreward variance, while verification methods based on process reward models\n(PRMs) suffer from difficulties with training data acquisition and verification\neffectiveness. To tackle these problems, this paper introduces ReST-RL, a\nunified LLM RL paradigm that significantly improves LLM's code reasoning\nability by combining an improved GRPO algorithm with a meticulously designed\ntest time decoding method assisted by a value model (VM). As the first stage of\npolicy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter\nand assemble high-value training data, increasing the reward variance of GRPO\nsampling, thus improving the effectiveness and efficiency of training. After\nthe basic reasoning ability of LLM policy has been improved, we further propose\na test time decoding optimization method called VM-MCTS. Through Monte-Carlo\nTree Search (MCTS), we collect accurate value targets with no annotation\nrequired, on which VM training is based. When decoding, the VM is deployed by\nan adapted MCTS algorithm to provide precise process signals as well as\nverification scores, assisting the LLM policy to achieve high reasoning\naccuracy. We validate the effectiveness of the proposed RL paradigm through\nextensive experiments on coding problems. Upon comparison, our approach\nsignificantly outperforms other reinforcement training baselines (e.g., naive\nGRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,\nPRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,\nAPPS, BigCodeBench, and HumanEval), indicating its power to strengthen the\nreasoning ability of LLM policies. Codes for our project can be found at\nhttps://github.com/THUDM/ReST-RL.", "AI": {"tldr": "ReST-RL\u901a\u8fc7\u6539\u8fdbGRPO\u548c\u5f15\u5165VM-MCTS\u89e3\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347LLM\u4ee3\u7801\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982GRPO\u56e0\u5956\u52b1\u65b9\u5dee\u4e0d\u8db3\u800c\u5931\u6548\uff0cPRM\u65b9\u6cd5\u5219\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u83b7\u53d6\u548c\u9a8c\u8bc1\u6548\u679c\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faReST-RL\u6846\u67b6\uff0c\u5305\u62ecReST-GRPO\u9636\u6bb5\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u5e76\u63d0\u5347\u5956\u52b1\u65b9\u5dee\uff0c\u4ee5\u53caVM-MCTS\u9636\u6bb5\u901a\u8fc7MCTS\u6536\u96c6\u65e0\u6807\u6ce8\u4ef7\u503c\u76ee\u6807\u5e76\u8f85\u52a9\u89e3\u7801\u3002", "result": "\u5728\u591a\u4e2a\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982APPS\u3001BigCodeBench\u548cHumanEval\uff09\u4e2d\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u548c\u89e3\u7801\u9a8c\u8bc1\u65b9\u6cd5\u3002", "conclusion": "ReST-RL\u901a\u8fc7\u7ed3\u5408\u6539\u8fdb\u7684GRPO\u7b97\u6cd5\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u65f6\u89e3\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u4ee3\u7801\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.19771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19771", "abs": "https://arxiv.org/abs/2508.19771", "authors": ["Liding Zhang", "Zhenshan Bing", "Yu Zhang", "Kuanqi Cai", "Lingyun Chen", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Elliptical K-Nearest Neighbors -- Path Optimization via Coulomb's Law and Invalid Vertices in C-space Obstacles", "comment": "2024 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "summary": "Path planning has long been an important and active research area in\nrobotics. To address challenges in high-dimensional motion planning, this study\nintroduces the Force Direction Informed Trees (FDIT*), a sampling-based planner\ndesigned to enhance speed and cost-effectiveness in pathfinding. FDIT* builds\nupon the state-of-the-art informed sampling planner, the Effort Informed Trees\n(EIT*), by capitalizing on often-overlooked information in invalid vertices. It\nincorporates principles of physical force, particularly Coulomb's law. This\napproach proposes the elliptical $k$-nearest neighbors search method, enabling\nfast convergence navigation and avoiding high solution cost or infeasible paths\nby exploring more problem-specific search-worthy areas. It demonstrates\nbenefits in search efficiency and cost reduction, particularly in confined,\nhigh-dimensional environments. It can be viewed as an extension of nearest\nneighbors search techniques. Fusing invalid vertex data with physical dynamics\nfacilitates force-direction-based search regions, resulting in an improved\nconvergence rate to the optimum. FDIT* outperforms existing single-query,\nsampling-based planners on the tested problems in R^4 to R^16 and has been\ndemonstrated on a real-world mobile manipulation task.", "AI": {"tldr": "FDIT* \u662f\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u8def\u5f84\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u65e0\u6548\u9876\u70b9\u4fe1\u606f\u548c\u7269\u7406\u529b\u539f\u7406\uff0c\u63d0\u5347\u4e86\u9ad8\u7ef4\u73af\u5883\u4e2d\u7684\u641c\u7d22\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u9ad8\u7ef4\u8fd0\u52a8\u89c4\u5212\u4e2d\u5b58\u5728\u901f\u5ea6\u4e0e\u6210\u672c\u6548\u76ca\u7684\u6311\u6218\uff0cFDIT* \u65e8\u5728\u901a\u8fc7\u5229\u7528\u65e0\u6548\u9876\u70b9\u4fe1\u606f\u548c\u7269\u7406\u529b\u539f\u7406\uff0c\u63d0\u5347\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "method": "FDIT* \u57fa\u4e8e\u6700\u5148\u8fdb\u7684\u4fe1\u606f\u91c7\u6837\u89c4\u5212\u5668 EIT*\uff0c\u5f15\u5165\u4e86\u692d\u5706\u5f62\u7684 k-\u6700\u8fd1\u90bb\u641c\u7d22\u65b9\u6cd5\uff0c\u7ed3\u5408\u7269\u7406\u529b\u539f\u7406\uff08\u5982\u5e93\u4ed1\u5b9a\u5f8b\uff09\uff0c\u63a2\u7d22\u66f4\u5177\u95ee\u9898\u7279\u5f02\u6027\u7684\u641c\u7d22\u533a\u57df\uff0c\u4ece\u800c\u907f\u514d\u9ad8\u6210\u672c\u6216\u4e0d\u53ef\u884c\u8def\u5f84\u3002", "result": "FDIT* \u5728 R^4 \u5230 R^16 \u7684\u6d4b\u8bd5\u95ee\u9898\u4e2d\u4f18\u4e8e\u73b0\u6709\u5355\u67e5\u8be2\u3001\u57fa\u4e8e\u91c7\u6837\u7684\u89c4\u5212\u5668\uff0c\u5e76\u5728\u5b9e\u9645\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "FDIT* \u662f\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u8def\u5f84\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5229\u7528\u65e0\u6548\u9876\u70b9\u4e2d\u5e38\u88ab\u5ffd\u89c6\u7684\u4fe1\u606f\u5e76\u7ed3\u5408\u7269\u7406\u529b\u539f\u7406\uff08\u5982\u5e93\u4ed1\u5b9a\u5f8b\uff09\uff0c\u5728\u641c\u7d22\u6548\u7387\u548c\u6210\u672c\u964d\u4f4e\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u53d7\u9650\u7684\u9ad8\u7ef4\u73af\u5883\u4e2d\u3002\u5b83\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u67e5\u8be2\u3001\u57fa\u4e8e\u91c7\u6837\u7684\u89c4\u5212\u5668\uff0c\u5e76\u5728\u5b9e\u9645\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2508.19312", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19312", "abs": "https://arxiv.org/abs/2508.19312", "authors": ["Ander Galv\u00e1n", "Marivi Higuero", "Jorge Sasiain", "Eduardo Jacob"], "title": "Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax", "comment": "Aceptado para publicaci\\'on, in Spanish language. XVII Jornadas de\n  Ingenier\\'ia Telem\\'atica (JITEL 2025)", "summary": "Facial recognition powered by Artificial Intelligence has achieved high\naccuracy in specific scenarios and applications. Nevertheless, it faces\nsignificant challenges regarding privacy and identity management, particularly\nwhen unknown individuals appear in the operational context. This paper presents\nthe design, implementation, and evaluation of a facial recognition system\nwithin a federated learning framework tailored to open-set scenarios. The\nproposed approach integrates the OpenMax algorithm into federated learning,\nleveraging the exchange of mean activation vectors and local distance measures\nto reliably distinguish between known and unknown subjects. Experimental\nresults validate the effectiveness of the proposed solution, demonstrating its\npotential for enhancing privacy-aware and robust facial recognition in\ndistributed environments.\n  --\n  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado\nuna alta precisi\\'on en algunos escenarios y aplicaciones. Sin embargo,\npresenta desaf\\'ios relacionados con la privacidad y la identificaci\\'on de\npersonas, especialmente considerando que pueden aparecer sujetos desconocidos\npara el sistema que lo implementa. En este trabajo, se propone el dise\\~no,\nimplementaci\\'on y evaluaci\\'on de un sistema de reconocimiento facial en un\nescenario de aprendizaje federado, orientado a conjuntos abiertos.\nConcretamente, se dise\\~na una soluci\\'on basada en el algoritmo OpenMax para\nescenarios de aprendizaje federado. La propuesta emplea el intercambio de los\nvectores de activaci\\'on promedio y distancias locales para identificar de\nmanera eficaz tanto personas conocidas como desconocidas. Los experimentos\nrealizados demuestran la implementaci\\'on efectiva de la soluci\\'on propuesta.", "AI": {"tldr": "A federated learning-based facial recognition system using OpenMax algorithm improves privacy and identity management in open-set scenarios.", "motivation": "Facial recognition systems face privacy and identity management challenges, especially with unknown individuals in operational contexts. This paper aims to enhance privacy-aware and robust facial recognition in distributed environments.", "method": "The approach combines federated learning with the OpenMax algorithm, utilizing mean activation vectors and local distance measures to distinguish between known and unknown subjects.", "result": "Experimental results validate the system's effectiveness in reliably distinguishing between known and unknown subjects, demonstrating its potential for practical applications.", "conclusion": "The proposed facial recognition system within a federated learning framework, integrated with the OpenMax algorithm, effectively addresses privacy and identity management challenges in open-set scenarios."}}
{"id": "2508.19611", "categories": ["cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.19611", "abs": "https://arxiv.org/abs/2508.19611", "authors": ["Huaiyuan Yao", "Wanpeng Xu", "Justin Turnau", "Nadia Kellam", "Hua Wei"], "title": "Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties", "comment": "18 pages, 9 figures", "summary": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings.", "AI": {"tldr": "Instructional Agents \u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53 LLM \u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u6559\u80b2\u89d2\u8272\u534f\u4f5c\u81ea\u52a8\u5316\u751f\u6210\u6559\u5b66\u6750\u6599\uff0c\u63d0\u4f9b\u56db\u79cd\u6a21\u5f0f\u7075\u6d3b\u63a7\u5236\u4eba\u7c7b\u53c2\u4e0e\uff0c\u8bc4\u4f30\u663e\u793a\u5176\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u5185\u5bb9\u5e76\u51cf\u5c11\u4eba\u529b\u8d1f\u62c5\u3002", "motivation": "\u9ad8\u8d28\u91cf\u6559\u5b66\u6750\u6599\u7684\u51c6\u5907\u901a\u5e38\u9700\u8981\u6559\u5b66\u4eba\u5458\u3001\u6559\u5b66\u8bbe\u8ba1\u8005\u548c\u52a9\u6559\u4e4b\u95f4\u7684\u5927\u91cf\u534f\u8c03\uff0c\u8fc7\u7a0b\u8017\u65f6\u4e14\u52b3\u52a8\u5bc6\u96c6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u5de5\u5177\u6765\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\u3002", "method": "Instructional Agents \u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u6559\u80b2\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u89d2\u8272\u534f\u4f5c\uff0c\u81ea\u52a8\u5316\u751f\u6210\u8bfe\u7a0b\u6750\u6599\uff0c\u5305\u62ec\u6559\u5b66\u5927\u7eb2\u3001\u8bb2\u4e49\u811a\u672c\u3001LaTeX \u5e7b\u706f\u7247\u548c\u8bc4\u4f30\u3002\u7cfb\u7edf\u63d0\u4f9b\u56db\u79cd\u6a21\u5f0f\uff1a\u81ea\u4e3b\u6a21\u5f0f\u3001\u76ee\u5f55\u5f15\u5bfc\u6a21\u5f0f\u3001\u53cd\u9988\u5f15\u5bfc\u6a21\u5f0f\u548c\u5168\u534f\u5bfc\u6a21\u5f0f\uff0c\u4ee5\u7075\u6d3b\u63a7\u5236\u4eba\u7c7b\u53c2\u4e0e\u7a0b\u5ea6\u3002", "result": "\u5728\u4e94\u95e8\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u8bfe\u7a0b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cInstructional Agents \u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6559\u5b66\u6750\u6599\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u548c\u4eba\u529b\u5de5\u4f5c\u91cf\u3002", "conclusion": "Instructional Agents \u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u548c\u4eba\u529b\u5de5\u4f5c\u91cf\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\uff0c\u6709\u52a9\u4e8e\u666e\u53ca\u9ad8\u8d28\u91cf\u6559\u80b2\u3002"}}
{"id": "2508.19776", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19776", "abs": "https://arxiv.org/abs/2508.19776", "authors": ["Liding Zhang", "Yao Ling", "Zhenshan Bing", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Tree-Based Grafting Approach for Bidirectional Motion Planning with Local Subsets Optimization", "comment": "IEEE Robotics and Automation Letters (also presented at IEEE-IROS\n  2025)", "summary": "Bidirectional motion planning often reduces planning time compared to its\nunidirectional counterparts. It requires connecting the forward and reverse\nsearch trees to form a continuous path. However, this process could fail and\nrestart the asymmetric bidirectional search due to the limitations of\nlazy-reverse search. To address this challenge, we propose Greedy GuILD\nGrafting Trees (G3T*), a novel path planner that grafts invalid edge\nconnections at both ends to re-establish tree-based connectivity, enabling\nrapid path convergence. G3T* employs a greedy approach using the minimum\nLebesgue measure of guided incremental local densification (GuILD) subsets to\noptimize paths efficiently. Furthermore, G3T* dynamically adjusts the sampling\ndistribution between the informed set and GuILD subsets based on historical and\ncurrent cost improvements, ensuring asymptotic optimality. These features\nenhance the forward search's growth towards the reverse tree, achieving faster\nconvergence and lower solution costs. Benchmark experiments across dimensions\nfrom R^2 to R^8 and real-world robotic evaluations demonstrate G3T*'s superior\nperformance compared to existing single-query sampling-based planners. A video\nshowcasing our experimental results is available at:\nhttps://youtu.be/3mfCRL5SQIU", "AI": {"tldr": "G3T*\u662f\u4e00\u79cd\u65b0\u578b\u8def\u5f84\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5ac1\u63a5\u65e0\u6548\u8fb9\u8fde\u63a5\u548c\u52a8\u6001\u8c03\u6574\u91c7\u6837\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u53cc\u5411\u8fd0\u52a8\u89c4\u5212\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u53cc\u5411\u8fd0\u52a8\u89c4\u5212\u56e0\u61d2\u60f0\u53cd\u5411\u641c\u7d22\u9650\u5236\u53ef\u80fd\u5bfc\u81f4\u5931\u8d25\u5e76\u91cd\u542f\uff0cG3T*\u65e8\u5728\u901a\u8fc7\u5ac1\u63a5\u65e0\u6548\u8fb9\u8fde\u63a5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u8def\u5f84\u6536\u655b\u901f\u5ea6\u548c\u6210\u672c\u3002", "method": "G3T*\u91c7\u7528\u8d2a\u5a6a\u7b56\u7565\uff0c\u5229\u7528GuILD\u5b50\u96c6\u7684\u6700\u5c0fLebesgue\u6d4b\u5ea6\u9ad8\u6548\u4f18\u5316\u8def\u5f84\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u91c7\u6837\u5206\u5e03\u4ee5\u786e\u4fdd\u6e10\u8fdb\u6700\u4f18\u6027\u3002", "result": "G3T*\u5728\u591a\u7ef4\u7a7a\u95f4\uff08R^2\u5230R^8\uff09\u53ca\u5b9e\u9645\u673a\u5668\u4eba\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5355\u67e5\u8be2\u91c7\u6837\u89c4\u5212\u5668\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u6536\u655b\u548c\u66f4\u4f4e\u6210\u672c\u3002", "conclusion": "G3T*\u901a\u8fc7\u8d2a\u5a6a\u65b9\u6cd5\u4f18\u5316\u8def\u5f84\uff0c\u52a8\u6001\u8c03\u6574\u91c7\u6837\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cc\u5411\u8fd0\u52a8\u89c4\u5212\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u7ef4\u5ea6\u53ca\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u4f18\u4e8e\u73b0\u6709\u5355\u67e5\u8be2\u91c7\u6837\u89c4\u5212\u5668\u3002"}}
{"id": "2508.19314", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19314", "abs": "https://arxiv.org/abs/2508.19314", "authors": ["Mahdis Tourian", "Sareh Rowlands", "Remy Vandaele", "Max Fancourt", "Rebecca Mein", "Hywel T. P. Williams"], "title": "Automated classification of natural habitats using ground-level imagery", "comment": "15 pages, 6 figures, 2 tables", "summary": "Accurate classification of terrestrial habitats is critical for biodiversity\nconservation, ecological monitoring, and land-use planning. Several habitat\nclassification schemes are in use, typically based on analysis of satellite\nimagery with validation by field ecologists. Here we present a methodology for\nclassification of habitats based solely on ground-level imagery (photographs),\noffering improved validation and the ability to classify habitats at scale (for\nexample using citizen-science imagery). In collaboration with Natural England,\na public sector organisation responsible for nature conservation in England,\nthis study develops a classification system that applies deep learning to\nground-level habitat photographs, categorising each image into one of 18\nclasses defined by the 'Living England' framework. Images were pre-processed\nusing resizing, normalisation, and augmentation; re-sampling was used to\nbalance classes in the training data and enhance model robustness. We developed\nand fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label\nto each photograph. Using five-fold cross-validation, the model demonstrated\nstrong overall performance across 18 habitat classes, with accuracy and\nF1-scores varying between classes. Across all folds, the model achieved a mean\nF1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and\nPeat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or\nambiguous classes scoring lower. These findings demonstrate the potential of\nthis approach for ecological monitoring. Ground-level imagery is readily\nobtained, and accurate computational methods for habitat classification based\non such data have many potential applications. To support use by practitioners,\nwe also provide a simple web application that classifies uploaded images using\nour model.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5730\u9762\u7ea7\u56fe\u50cf\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6001\u76d1\u6d4b\uff0c\u6a21\u578b\u572818\u4e2a\u6816\u606f\u5730\u7c7b\u522b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5e76\u63d0\u4f9b\u4e86\u7f51\u7edc\u5e94\u7528\u652f\u6301\u3002", "motivation": "\u51c6\u786e\u5206\u7c7b\u9646\u5730\u6816\u606f\u5730\u5bf9\u751f\u7269\u591a\u6837\u6027\u4fdd\u62a4\u3001\u751f\u6001\u76d1\u6d4b\u548c\u571f\u5730\u5229\u7528\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u5206\u7c7b\u65b9\u6848\u901a\u5e38\u57fa\u4e8e\u536b\u661f\u56fe\u50cf\u5206\u6790\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u9a8c\u8bc1\u548c\u89c4\u6a21\u5316\u5206\u7c7b\u80fd\u529b\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u7c7b\u7cfb\u7edf\uff0c\u5e94\u7528DeepLabV3-ResNet101\u5206\u7c7b\u5668\u5bf9\u5730\u9762\u7ea7\u6816\u606f\u5730\u7167\u7247\u8fdb\u884c\u5206\u7c7b\uff0c\u9884\u5904\u7406\u5305\u62ec\u8c03\u6574\u5927\u5c0f\u3001\u5f52\u4e00\u5316\u548c\u589e\u5f3a\uff0c\u4ee5\u53ca\u91cd\u65b0\u91c7\u6837\u4ee5\u5e73\u8861\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u6a21\u578b\u572818\u4e2a\u6816\u606f\u5730\u7c7b\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u5747F1\u5f97\u5206\u4e3a0.61\uff0c\u89c6\u89c9\u4e0a\u660e\u663e\u7684\u7c7b\u522b\u5982\u88f8\u571f\u3001\u6de4\u6ce5\u548c\u6ce5\u70ad\uff08BSSP\uff09\u548c\u88f8\u6c99\uff08BS\uff09\u5f97\u5206\u8d85\u8fc70.90\uff0c\u6df7\u5408\u6216\u6a21\u7cca\u7c7b\u522b\u5f97\u5206\u8f83\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u57fa\u4e8e\u5730\u9762\u7ea7\u56fe\u50cf\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u65b9\u6cd5\u5728\u751f\u6001\u76d1\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7f51\u7edc\u5e94\u7528\u4f9b\u5b9e\u8df5\u8005\u4f7f\u7528\u3002"}}
{"id": "2508.19679", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19679", "abs": "https://arxiv.org/abs/2508.19679", "authors": ["Qihang Ai", "Pi Bu", "Yue Cao", "Yingyao Wang", "Jihao Gu", "Jingxuan Xing", "Zekun Zhu", "Wei Jiang", "Zhicheng Zheng", "Jun Song", "Yuning Jiang", "Bo Zheng"], "title": "InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) have enabled mobile agents\nto perceive and interact with real-world mobile environments based on human\ninstructions. However, the current fully autonomous paradigm poses potential\nsafety risks when model understanding or reasoning capabilities are\ninsufficient. To address this challenge, we first introduce\n\\textbf{InquireBench}, a comprehensive benchmark specifically designed to\nevaluate mobile agents' capabilities in safe interaction and proactive inquiry\nwith users, encompassing 5 categories and 22 sub-categories, where most\nexisting VLM-based agents demonstrate near-zero performance. In this paper, we\naim to develop an interactive system that actively seeks human confirmation at\ncritical decision points. To achieve this, we propose \\textbf{InquireMobile}, a\nnovel model inspired by reinforcement learning, featuring a two-stage training\nstrategy and an interactive pre-action reasoning mechanism. Finally, our model\nachieves an 46.8% improvement in inquiry success rate and the best overall\nsuccess rate among existing baselines on InquireBench. We will open-source all\ndatasets, models, and evaluation codes to facilitate development in both\nacademia and industry.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9VLM\u5b89\u5168\u98ce\u9669\u63d0\u51faInquireMobile\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u4ea4\u4e92\u63a8\u7406\u673a\u5236\u63d0\u5347\u67e5\u8be2\u6210\u529f\u738746.8%\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5f53\u524d\u5b8c\u5168\u81ea\u4e3b\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u6a21\u578b\u7406\u89e3\u6216\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u65f6\u53ef\u80fd\u5e26\u6765\u5b89\u5168\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u4e0e\u7528\u6237\u4ea4\u4e92\u5e76\u5bfb\u6c42\u786e\u8ba4\u7684\u7cfb\u7edf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86InquireMobile\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u53d7\u5f3a\u5316\u5b66\u4e60\u542f\u53d1\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u548c\u4ea4\u4e92\u5f0f\u9884\u52a8\u4f5c\u63a8\u7406\u673a\u5236\u3002", "result": "InquireMobile\u5728InquireBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8646.8%\u7684\u67e5\u8be2\u6210\u529f\u7387\u63d0\u5347\uff0c\u5e76\u5728\u6574\u4f53\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInquireMobile\u7684\u65b0\u578b\u4ea4\u4e92\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u4e3b\u52a8\u5bfb\u6c42\u4eba\u7c7b\u786e\u8ba4\u6765\u63d0\u5347\u79fb\u52a8\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u5e76\u5728InquireBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e8646.8%\u7684\u67e5\u8be2\u6210\u529f\u7387\u63d0\u5347\u548c\u6700\u4f73\u6574\u4f53\u6210\u529f\u7387\u3002"}}
{"id": "2508.19788", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19788", "abs": "https://arxiv.org/abs/2508.19788", "authors": ["Sena Ishii", "Akash Chikhalikar", "Ankit A. Ravankar", "Jose Victorio Salazar Luces", "Yasuhisa Hirata"], "title": "Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots", "comment": "8 pages, Accepted for IEEE RO-MAN 2025 Conference", "summary": "We present a novel framework for estimating accident-prone regions in\neveryday indoor scenes, aimed at improving real-time risk awareness in service\nrobots operating in human-centric environments. As robots become integrated\ninto daily life, particularly in homes, the ability to anticipate and respond\nto environmental hazards is crucial for ensuring user safety, trust, and\neffective human-robot interaction. Our approach models object-level risk and\ncontext through a semantic graph-based propagation algorithm. Each object is\nrepresented as a node with an associated risk score, and risk propagates\nasymmetrically from high-risk to low-risk objects based on spatial proximity\nand accident relationship. This enables the robot to infer potential hazards\neven when they are not explicitly visible or labeled. Designed for\ninterpretability and lightweight onboard deployment, our method is validated on\na dataset with human-annotated risk regions, achieving a binary risk detection\naccuracy of 75%. The system demonstrates strong alignment with human\nperception, particularly in scenes involving sharp or unstable objects. These\nresults underline the potential of context-aware risk reasoning to enhance\nrobotic scene understanding and proactive safety behaviors in shared\nhuman-robot spaces. This framework could serve as a foundation for future\nsystems that make context-driven safety decisions, provide real-time alerts, or\nautonomously assist users in avoiding or mitigating hazards within home\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u56fe\u7b97\u6cd5\u4f30\u8ba1\u5ba4\u5185\u573a\u666f\u4e2d\u7684\u4e8b\u6545\u6613\u53d1\u533a\u57df\uff0c\u65e8\u5728\u63d0\u9ad8\u670d\u52a1\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u98ce\u9669\u610f\u8bc6\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u4e3a75%\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u878d\u5165\u65e5\u5e38\u751f\u6d3b\uff0c\u7279\u522b\u662f\u5728\u5bb6\u5ead\u4e2d\uff0c\u9884\u6d4b\u548c\u5e94\u5bf9\u73af\u5883\u5371\u9669\u7684\u80fd\u529b\u5bf9\u4e8e\u786e\u4fdd\u7528\u6237\u5b89\u5168\u3001\u4fe1\u4efb\u548c\u6709\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u8bed\u4e49\u56fe\u7684\u4f20\u64ad\u7b97\u6cd5\u5efa\u6a21\u5bf9\u8c61\u7ea7\u98ce\u9669\u548c\u4e0a\u4e0b\u6587\uff0c\u6bcf\u4e2a\u5bf9\u8c61\u8868\u793a\u4e3a\u5177\u6709\u5173\u8054\u98ce\u9669\u5206\u6570\u7684\u8282\u70b9\uff0c\u98ce\u9669\u57fa\u4e8e\u7a7a\u95f4\u63a5\u8fd1\u6027\u548c\u4e8b\u6545\u5173\u7cfb\u4ece\u9ad8\u98ce\u9669\u5bf9\u8c61\u5411\u4f4e\u98ce\u9669\u5bf9\u8c61\u4e0d\u5bf9\u79f0\u4f20\u64ad\u3002", "result": "\u5728\u4eba\u7c7b\u6807\u6ce8\u7684\u98ce\u9669\u533a\u57df\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e8675%\u7684\u4e8c\u5143\u98ce\u9669\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u7cfb\u7edf\u5728\u6d89\u53ca\u5c16\u9510\u6216\u4e0d\u7a33\u5b9a\u7269\u4f53\u7684\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u5f3a\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u672a\u6765\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u80fd\u591f\u505a\u51fa\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u5b89\u5168\u51b3\u7b56\u3001\u63d0\u4f9b\u5b9e\u65f6\u8b66\u62a5\u6216\u81ea\u4e3b\u534f\u52a9\u7528\u6237\u907f\u514d\u6216\u51cf\u8f7b\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u5371\u9669\u3002"}}
{"id": "2508.19320", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19320", "abs": "https://arxiv.org/abs/2508.19320", "authors": ["Ming Chen", "Liyuan Cui", "Wenyuan Zhang", "Haoxian Zhang", "Yan Zhou", "Xiaohan Li", "Xiaoqiang Liu", "Pengfei Wan"], "title": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation", "comment": "Technical Report. Project Page: https://chenmingthu.github.io/milm/", "summary": "Recently, interactive digital human video generation has attracted widespread\nattention and achieved remarkable progress. However, building such a practical\nsystem that can interact with diverse input signals in real time remains\nchallenging to existing methods, which often struggle with high latency, heavy\ncomputational cost, and limited controllability. In this work, we introduce an\nautoregressive video generation framework that enables interactive multimodal\ncontrol and low-latency extrapolation in a streaming manner. With minimal\nmodifications to a standard large language model (LLM), our framework accepts\nmultimodal condition encodings including audio, pose, and text, and outputs\nspatially and semantically coherent representations to guide the denoising\nprocess of a diffusion head. To support this, we construct a large-scale\ndialogue dataset of approximately 20,000 hours from multiple sources, providing\nrich conversational scenarios for training. We further introduce a deep\ncompression autoencoder with up to 64$\\times$ reduction ratio, which\neffectively alleviates the long-horizon inference burden of the autoregressive\nmodel. Extensive experiments on duplex conversation, multilingual human\nsynthesis, and interactive world model highlight the advantages of our approach\nin low latency, high efficiency, and fine-grained multimodal controllability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u591a\u6a21\u6001\u4ea4\u4e92\u63a7\u5236\u548c\u4f4e\u5ef6\u8fdf\u6d41\u5f0f\u63a8\u65ad\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u6df1\u5ea6\u538b\u7f29\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u6570\u5b57\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u4e2d\u9762\u4e34\u9ad8\u5ef6\u8fdf\u3001\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6709\u9650\u53ef\u63a7\u6027\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u5316\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4fee\u6539\uff0c\u63a5\u53d7\u591a\u6a21\u6001\u6761\u4ef6\u7f16\u7801\uff08\u5982\u97f3\u9891\u3001\u59ff\u52bf\u548c\u6587\u672c\uff09\uff0c\u5e76\u8f93\u51fa\u7a7a\u95f4\u548c\u8bed\u4e49\u8fde\u8d2f\u7684\u8868\u793a\u4ee5\u6307\u5bfc\u6269\u6563\u5934\u7684\u53bb\u566a\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u5bf9\u8bdd\u6570\u636e\u96c6\uff08\u7ea620,000\u5c0f\u65f6\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u6df1\u5ea6\u538b\u7f29\u81ea\u7f16\u7801\u5668\uff08\u538b\u7f29\u6bd4\u9ad8\u8fbe64\u500d\uff09\u3002", "result": "\u5728\u53cc\u5de5\u5bf9\u8bdd\u3001\u591a\u8bed\u8a00\u4eba\u7c7b\u5408\u6210\u548c\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\u7b49\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6548\u7387\u548c\u7cbe\u7ec6\u591a\u6a21\u6001\u53ef\u63a7\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u6846\u67b6\u5728\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6548\u7387\u548c\u591a\u6a21\u6001\u7cbe\u7ec6\u63a7\u5236\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u6570\u5b57\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19827", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19827", "abs": "https://arxiv.org/abs/2508.19827", "authors": ["Samuel Lewis-Lim", "Xingwei Tan", "Zhixue Zhao", "Nikolaos Aletras"], "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?", "comment": "Accepted at EMNLP 2025 Main Conference", "summary": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited\ngains for soft-reasoning problems such as analytical and commonsense reasoning.\nCoT can also be unfaithful to a model's actual reasoning. We investigate the\ndynamics and faithfulness of CoT in soft-reasoning tasks across\ninstruction-tuned, reasoning and reasoning-distilled models. Our findings\nreveal differences in how these models rely on CoT, and show that CoT influence\nand faithfulness are not always aligned.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0CoT\u5728\u8f6f\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u548c\u5fe0\u5b9e\u6027\u56e0\u6a21\u578b\u800c\u5f02\uff0c\u63ed\u793a\u4e86\u5176\u52a8\u6001\u6027\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u8868\u660eCoT\u5728\u8f6f\u63a8\u7406\u95ee\u9898\uff08\u5982\u5206\u6790\u548c\u5e38\u8bc6\u63a8\u7406\uff09\u4e2d\u6548\u679c\u6709\u9650\uff0c\u4e14\u53ef\u80fd\u4e0e\u6a21\u578b\u7684\u5b9e\u9645\u63a8\u7406\u4e0d\u4e00\u81f4\u3002", "method": "\u7814\u7a76\u4e86\u6307\u4ee4\u8c03\u6574\u3001\u63a8\u7406\u548c\u63a8\u7406\u84b8\u998f\u6a21\u578b\u5728\u8f6f\u63a8\u7406\u4efb\u52a1\u4e2dCoT\u7684\u52a8\u6001\u6027\u548c\u5fe0\u5b9e\u6027\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5bf9CoT\u7684\u4f9d\u8d56\u65b9\u5f0f\u4e0d\u540c\uff0cCoT\u7684\u5f71\u54cd\u529b\u548c\u5fe0\u5b9e\u6027\u5e76\u975e\u603b\u662f\u4e00\u81f4\u3002", "conclusion": "CoT\u7684\u5f71\u54cd\u529b\u548c\u5fe0\u5b9e\u6027\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u5e76\u4e0d\u603b\u662f\u4e00\u81f4\uff0c\u63ed\u793a\u4e86\u5728\u8f6f\u63a8\u7406\u4efb\u52a1\u4e2dCoT\u7684\u52a8\u6001\u6027\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2508.19790", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19790", "abs": "https://arxiv.org/abs/2508.19790", "authors": ["Liding Zhang", "Sicheng Wang", "Kuanqi Cai", "Zhenshan Bing", "Fan Wu", "Chaoqun Wang", "Sami Haddadin", "Alois Knoll"], "title": "APT*: Asymptotically Optimal Motion Planning via Adaptively Prolated Elliptical R-Nearest Neighbors", "comment": null, "summary": "Optimal path planning aims to determine a sequence of states from a start to\na goal while accounting for planning objectives. Popular methods often\nintegrate fixed batch sizes and neglect information on obstacles, which is not\nproblem-specific. This study introduces Adaptively Prolated Trees (APT*), a\nnovel sampling-based motion planner that extends based on Force Direction\nInformed Trees (FDIT*), integrating adaptive batch-sizing and elliptical\n$r$-nearest neighbor modules to dynamically modulate the path searching process\nbased on environmental feedback. APT* adjusts batch sizes based on the\nhypervolume of the informed sets and considers vertices as electric charges\nthat obey Coulomb's law to define virtual forces via neighbor samples, thereby\nrefining the prolate nearest neighbor selection. These modules employ\nnon-linear prolate methods to adaptively adjust the electric charges of\nvertices for force definition, thereby improving the convergence rate with\nlower solution costs. Comparative analyses show that APT* outperforms existing\nsingle-query sampling-based planners in dimensions from $\\mathbb{R}^4$ to\n$\\mathbb{R}^{16}$, and it was further validated through a real-world robot\nmanipulation task. A video showcasing our experimental results is available at:\nhttps://youtu.be/gCcUr8LiEw4", "AI": {"tldr": "APT*\u662f\u4e00\u79cd\u65b0\u578b\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6279\u5904\u7406\u548c\u865a\u62df\u529b\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u5728\u9ad8\u7ef4\u7a7a\u95f4\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5e38\u91c7\u7528\u56fa\u5b9a\u6279\u5904\u7406\u5927\u5c0f\u4e14\u5ffd\u7565\u969c\u788d\u7269\u4fe1\u606f\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u4e14\u7f3a\u4e4f\u9002\u5e94\u6027\u3002", "method": "APT*\u7ed3\u5408\u4e86\u81ea\u9002\u5e94\u6279\u5904\u7406\u5927\u5c0f\u548c\u692d\u5706\u5f62r-\u6700\u8fd1\u90bb\u6a21\u5757\uff0c\u5229\u7528\u865a\u62df\u529b\u5b9a\u4e49\u548c\u7535\u8377\u8c03\u6574\u6765\u4f18\u5316\u8def\u5f84\u641c\u7d22\u8fc7\u7a0b\u3002", "result": "APT*\u5728\u211d\u2074\u5230\u211d\u00b9\u2076\u7684\u7ef4\u5ea6\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5355\u67e5\u8be2\u91c7\u6837\u89c4\u5212\u5668\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "APT*\u901a\u8fc7\u81ea\u9002\u5e94\u6279\u5904\u7406\u548c\u692d\u5706\u5f62\u6700\u8fd1\u90bb\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2508.19324", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.19324", "abs": "https://arxiv.org/abs/2508.19324", "authors": ["Jefferson David Rodriguez Chivata", "Davide Ghiani", "Simone Maurizio La Cava", "Marco Micheletto", "Giulia Orr\u00f9", "Federico Lama", "Gian Luca Marcialis"], "title": "Deep Data Hiding for ICAO-Compliant Face Images: A Survey", "comment": "In 2025 IEEE International Joint Conference on Biometrics (IJCB)", "summary": "ICAO-compliant facial images, initially designed for secure biometric\npassports, are increasingly becoming central to identity verification in a wide\nrange of application contexts, including border control, digital travel\ncredentials, and financial services. While their standardization enables global\ninteroperability, it also facilitates practices such as morphing and deepfakes,\nwhich can be exploited for harmful purposes like identity theft and illegal\nsharing of identity documents. Traditional countermeasures like Presentation\nAttack Detection (PAD) are limited to real-time capture and offer no\npost-capture protection. This survey paper investigates digital watermarking\nand steganography as complementary solutions that embed tamper-evident signals\ndirectly into the image, enabling persistent verification without compromising\nICAO compliance. We provide the first comprehensive analysis of\nstate-of-the-art techniques to evaluate the potential and drawbacks of the\nunderlying approaches concerning the applications involving ICAO-compliant\nimages and their suitability under standard constraints. We highlight key\ntrade-offs, offering guidance for secure deployment in real-world identity\nsystems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6570\u5b57\u6c34\u5370\u548c\u9690\u5199\u672f\u4f5c\u4e3aICAO\u5408\u89c4\u9762\u90e8\u56fe\u50cf\u7684\u6301\u4e45\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\uff0c\u5206\u6790\u4e86\u5176\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u6307\u5bfc\u3002", "motivation": "ICAO\u5408\u89c4\u9762\u90e8\u56fe\u50cf\u5728\u8eab\u4efd\u9a8c\u8bc1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u6807\u51c6\u5316\u4e5f\u5e26\u6765\u4e86\u5982\u53d8\u5f62\u548c\u6df1\u5ea6\u4f2a\u9020\u7b49\u6ee5\u7528\u98ce\u9669\uff0c\u4f20\u7edf\u9632\u62a4\u63aa\u65bd\u5982PAD\u65e0\u6cd5\u63d0\u4f9b\u6355\u83b7\u540e\u7684\u4fdd\u62a4\u3002", "method": "\u672c\u6587\u5bf9\u6700\u65b0\u7684\u6570\u5b57\u6c34\u5370\u548c\u9690\u5199\u672f\u6280\u672f\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6280\u672f\u5728\u6d89\u53caICAO\u5408\u89c4\u56fe\u50cf\u7684\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u6570\u5b57\u6c34\u5370\u548c\u9690\u5199\u672f\u5728ICAO\u5408\u89c4\u56fe\u50cf\u4e2d\u7684\u5173\u952e\u6743\u8861\uff0c\u4e3a\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u6570\u5b57\u6c34\u5370\u548c\u9690\u5199\u672f\u4f5c\u4e3a\u8865\u5145\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u5f71\u54cdICAO\u5408\u89c4\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3aICAO\u5408\u89c4\u9762\u90e8\u56fe\u50cf\u63d0\u4f9b\u6301\u4e45\u7684\u9a8c\u8bc1\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u8eab\u4efd\u7cfb\u7edf\u3002"}}
{"id": "2508.19851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19851", "abs": "https://arxiv.org/abs/2508.19851", "authors": ["Romain Harang", "Jason Naradowsky", "Yaswitha Gujju", "Yusuke Miyao"], "title": "Tracking World States with Language Models: State-Based Evaluation Using Chess", "comment": "Spotlight presentation at ICML 2025 Workshop on Assessing World\n  Models", "summary": "Large Language Models (LLMs) exhibit emergent capabilities in structured\ndomains, suggesting they may implicitly internalize high-fidelity\nrepresentations of world models. While probing techniques have shown promising\nsigns of this in scientific and game-based settings, they rely on\nmodel-specific internal activations, which limit interpretability and\ngeneralizability. In this work, we propose a model-agnostic, state-based\nevaluation framework using chess as a benchmark to assess whether LLMs preserve\nthe semantics of structured environments. Our method analyzes the downstream\nlegal move distributions (state affordances) to estimate semantic fidelity\nbetween predicted and actual game states. This approach offers a more\nmeaningful evaluation than conventional string-based metrics by aligning more\nclosely with the strategic and rule-governed nature of chess. Experimental\nresults demonstrate that our metrics capture deficiencies in state-tracking,\nhighlighting limitations of LLMs in maintaining coherent internal models over\nlong sequences. Our framework provides a robust tool for evaluating structured\nreasoning in LLMs without requiring internal model access, and generalizes to a\nwide class of symbolic environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u56fd\u9645\u8c61\u68cb\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u957f\u5e8f\u5217\u4e2d\u72b6\u6001\u8ddf\u8e2a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u9886\u57df\u8868\u73b0\u51fa\u6d8c\u73b0\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u63a2\u6d4b\u6280\u672f\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\uff0c\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5408\u6cd5\u79fb\u52a8\u5206\u5e03\uff08\u72b6\u6001\u53ef\u4f9b\u6027\uff09\u6765\u4f30\u8ba1\u9884\u6d4b\u6e38\u620f\u72b6\u6001\u4e0e\u5b9e\u9645\u6e38\u620f\u72b6\u6001\u4e4b\u95f4\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6355\u6349\u72b6\u6001\u8ddf\u8e2a\u4e2d\u7684\u7f3a\u9677\uff0c\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5e8f\u5217\u4e2d\u4fdd\u6301\u8fde\u8d2f\u5185\u90e8\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u72b6\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u7279\u522b\u662f\u5728\u56fd\u9645\u8c61\u68cb\u8fd9\u6837\u7684\u73af\u5883\u4e2d\u3002\u8be5\u6846\u67b6\u4e0d\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\uff0c\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2508.19816", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19816", "abs": "https://arxiv.org/abs/2508.19816", "authors": ["Ricardo J. Manr\u00edquez-Cisterna", "Ankit A. Ravankar", "Jose V. Salazar Luces", "Takuro Hatsukari", "Yasuhisa Hirata"], "title": "A Standing Support Mobility Robot for Enhancing Independence in Elderly Daily Living", "comment": "7 pages, accepted work for IEEE RO-MAN2025", "summary": "This paper presents a standing support mobility robot \"Moby\" developed to\nenhance independence and safety for elderly individuals during daily activities\nsuch as toilet transfers. Unlike conventional seated mobility aids, the robot\nmaintains users in an upright posture, reducing physical strain, supporting\nnatural social interaction at eye level, and fostering a greater sense of\nself-efficacy. Moby offers a novel alternative by functioning both passively\nand with mobility support, enabling users to perform daily tasks more\nindependently. Its main advantages include ease of use, lightweight design,\ncomfort, versatility, and effective sit-to-stand assistance. The robot\nleverages the Robot Operating System (ROS) for seamless control, featuring\nmanual and autonomous operation modes. A custom control system enables safe and\nintuitive interaction, while the integration with NAV2 and LiDAR allows for\nrobust navigation capabilities. This paper reviews existing mobility solutions\nand compares them to Moby, details the robot's design, and presents objective\nand subjective experimental results using the NASA-TLX method and time\ncomparisons to other methods to validate our design criteria and demonstrate\nthe advantages of our contribution.", "AI": {"tldr": "Moby\u662f\u4e00\u6b3e\u7ad9\u7acb\u652f\u6491\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u65e8\u5728\u63d0\u5347\u8001\u5e74\u4eba\u7684\u72ec\u7acb\u6027\u548c\u5b89\u5168\u6027\uff0c\u901a\u8fc7ROS\u63a7\u5236\u3001\u591a\u529f\u80fd\u8bbe\u8ba1\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u8001\u5e74\u4eba\u5728\u65e5\u5e38\u6d3b\u52a8\uff08\u5982\u5982\u5395\u8f6c\u79fb\uff09\u4e2d\u7684\u72ec\u7acb\u6027\u548c\u5b89\u5168\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u8eab\u4f53\u8d1f\u62c5\u5e76\u652f\u6301\u81ea\u7136\u793e\u4ea4\u4e92\u52a8\uff0c\u8bbe\u8ba1\u4e86\u7ad9\u7acb\u652f\u6491\u79fb\u52a8\u673a\u5668\u4ebaMoby\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e86Robot Operating System\uff08ROS\uff09\u8fdb\u884c\u65e0\u7f1d\u63a7\u5236\uff0c\u7ed3\u5408\u624b\u52a8\u548c\u81ea\u4e3b\u64cd\u4f5c\u6a21\u5f0f\u3002\u5b9a\u5236\u63a7\u5236\u7cfb\u7edf\u786e\u4fdd\u4e86\u5b89\u5168\u76f4\u89c2\u7684\u4e92\u52a8\uff0c\u800cNAV2\u548cLiDAR\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5bfc\u822a\u80fd\u529b\u3002\u7814\u7a76\u8fd8\u5305\u62ec\u4e86NASA-TLX\u65b9\u6cd5\u548c\u65f6\u95f4\u6bd4\u8f83\u5b9e\u9a8c\u3002", "result": "Moby\u673a\u5668\u4eba\u5c55\u793a\u4e86\u6613\u7528\u6027\u3001\u8f7b\u91cf\u5316\u8bbe\u8ba1\u3001\u8212\u9002\u6027\u3001\u591a\u529f\u80fd\u6027\u548c\u6709\u6548\u7684\u5750-\u7ad9\u8f85\u52a9\u529f\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u8bbe\u8ba1\u6807\u51c6\u9a8c\u8bc1\u548c\u4f18\u52bf\u5c55\u793a\u65b9\u9762\u5177\u6709\u663e\u8457\u6548\u679c\u3002", "conclusion": "Moby\u673a\u5668\u4eba\u901a\u8fc7\u5176\u521b\u65b0\u7684\u7ad9\u7acb\u652f\u6491\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8001\u5e74\u4eba\u5728\u65e5\u5e38\u6d3b\u52a8\u4e2d\u7684\u72ec\u7acb\u6027\u548c\u5b89\u5168\u6027\uff0c\u540c\u65f6\u51cf\u8f7b\u4e86\u8eab\u4f53\u8d1f\u62c5\u5e76\u589e\u5f3a\u4e86\u793e\u4ea4\u4e92\u52a8\u7684\u81ea\u7136\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u8bbe\u8ba1\u6807\u51c6\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.19325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19325", "abs": "https://arxiv.org/abs/2508.19325", "authors": ["Haoyang Su", "Jin-Yi Xiang", "Shaohao Rui", "Yifan Gao", "Xingyu Chen", "Tingxuan Yin", "Xiaosong Wang", "Lian-Ming Wu"], "title": "PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI", "comment": null, "summary": "Accurate prediction of major adverse cardiac events (MACE) remains a central\nchallenge in cardiovascular prognosis. We present PRISM (Prompt-guided\nRepresentation Integration for Survival Modeling), a self-supervised framework\nthat integrates visual representations from non-contrast cardiac cine magnetic\nresonance imaging with structured electronic health records (EHRs) for survival\nanalysis. PRISM extracts temporally synchronized imaging features through\nmotion-aware multi-view distillation and modulates them using medically\ninformed textual prompts to enable fine-grained risk prediction. Across four\nindependent clinical cohorts, PRISM consistently surpasses classical survival\nprediction models and state-of-the-art (SOTA) deep learning baselines under\ninternal and external validation. Further clinical findings demonstrate that\nthe combined imaging and EHR representations derived from PRISM provide\nvaluable insights into cardiac risk across diverse cohorts. Three distinct\nimaging signatures associated with elevated MACE risk are uncovered, including\nlateral wall dyssynchrony, inferior wall hypersensitivity, and anterior\nelevated focus during diastole. Prompt-guided attribution further identifies\nhypertension, diabetes, and smoking as dominant contributors among clinical and\nphysiological EHR factors.", "AI": {"tldr": "PRISM\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7ed3\u5408\u5fc3\u810fMRI\u548cEHR\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347MACE\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u8bc6\u522b\u51fa\u9ad8\u98ce\u9669\u6210\u50cf\u7279\u5f81\u548c\u4e34\u5e8a\u56e0\u7d20\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u4e3b\u8981\u4e0d\u826f\u5fc3\u810f\u4e8b\u4ef6\uff08MACE\uff09\u662f\u5fc3\u8840\u7ba1\u9884\u540e\u7684\u6838\u5fc3\u6311\u6218\uff0c\u9700\u8981\u6574\u5408\u89c6\u89c9\u548c\u7ed3\u6784\u5316\u6570\u636e\u4ee5\u6539\u8fdb\u9884\u6d4b\u6a21\u578b\u3002", "method": "PRISM\u91c7\u7528\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u52a8\u611f\u77e5\u7684\u591a\u89c6\u56fe\u84b8\u998f\u63d0\u53d6\u65f6\u95f4\u540c\u6b65\u7684\u6210\u50cf\u7279\u5f81\uff0c\u5e76\u5229\u7528\u533b\u5b66\u4fe1\u606f\u6587\u672c\u63d0\u793a\u8fdb\u884c\u8c03\u5236\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u98ce\u9669\u9884\u6d4b\u3002", "result": "PRISM\u5728\u56db\u4e2a\u72ec\u7acb\u4e34\u5e8a\u961f\u5217\u4e2d\u5747\u4f18\u4e8e\u7ecf\u5178\u751f\u5b58\u9884\u6d4b\u6a21\u578b\u548c\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\uff0c\u5e76\u63ed\u793a\u4e86\u4e09\u79cd\u4e0e\u9ad8\u98ce\u9669\u76f8\u5173\u7684\u6210\u50cf\u7279\u5f81\u53ca\u4e3b\u8981\u4e34\u5e8a\u8d21\u732e\u56e0\u7d20\u3002", "conclusion": "PRISM\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u975e\u5bf9\u6bd4\u5fc3\u810f\u7535\u5f71\u78c1\u5171\u632f\u6210\u50cf\u548c\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u663e\u8457\u63d0\u9ad8\u4e86MACE\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u4e09\u79cd\u4e0e\u9ad8\u98ce\u9669\u76f8\u5173\u7684\u6210\u50cf\u7279\u5f81\u4ee5\u53ca\u4e3b\u8981\u7684\u4e34\u5e8a\u8d21\u732e\u56e0\u7d20\u3002"}}
{"id": "2508.19932", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19932", "abs": "https://arxiv.org/abs/2508.19932", "authors": ["Nitish Jaipuria", "Lorenzo Gatto", "Zijun Kan", "Shankey Poddar", "Bill Cheung", "Diksha Bansal", "Ramanan Balakrishnan", "Aviral Suri", "Jose Estevez"], "title": "CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments", "comment": "10 pages, 5 figures", "summary": "The proliferation of digital payment platforms has transformed commerce,\noffering unmatched convenience and accessibility globally. However, this growth\nhas also attracted malicious actors, leading to a corresponding increase in\nsophisticated social engineering scams. These scams are often initiated and\norchestrated on multiple surfaces outside the payment platform, making user and\ntransaction-based signals insufficient for a complete understanding of the\nscam's methodology and underlying patterns, without which it is very difficult\nto prevent it in a timely manner. This paper presents CASE (Conversational\nAgent for Scam Elucidation), a novel Agentic AI framework that addresses this\nproblem by collecting and managing user scam feedback in a safe and scalable\nmanner. A conversational agent is uniquely designed to proactively interview\npotential victims to elicit intelligence in the form of a detailed\nconversation. The conversation transcripts are then consumed by another AI\nsystem that extracts information and converts it into structured data for\ndownstream usage in automated and manual enforcement mechanisms. Using Google's\nGemini family of LLMs, we implemented this framework on Google Pay (GPay)\nIndia. By augmenting our existing features with this new intelligence, we have\nobserved a 21% uplift in the volume of scam enforcements. The architecture and\nits robust evaluation framework are highly generalizable, offering a blueprint\nfor building similar AI-driven systems to collect and manage scam intelligence\nin other sensitive domains.", "AI": {"tldr": "CASE\u662f\u4e00\u4e2a\u65b0\u578bAI\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8bdd\u5f0f\u4ee3\u7406\u6536\u96c6\u8bc8\u9a97\u53cd\u9988\uff0c\u63d0\u5347\u8bc8\u9a97\u8bc6\u522b\u6548\u7387\uff0c\u5728Google Pay India\u4e0a\u5b9e\u73b021%\u7684\u8bc8\u9a97\u6267\u884c\u91cf\u589e\u957f\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u652f\u4ed8\u5e73\u53f0\u7684\u666e\u53ca\uff0c\u793e\u4ea4\u5de5\u7a0b\u8bc8\u9a97\u65e5\u76ca\u590d\u6742\uff0c\u4f20\u7edf\u57fa\u4e8e\u7528\u6237\u548c\u4ea4\u6613\u7684\u4fe1\u53f7\u5df2\u4e0d\u8db3\u4ee5\u5168\u9762\u7406\u89e3\u8bc8\u9a97\u6a21\u5f0f\uff0c\u96be\u4ee5\u53ca\u65f6\u9884\u9632\u3002", "method": "CASE\u6846\u67b6\u91c7\u7528\u5bf9\u8bdd\u5f0f\u4ee3\u7406\u4e3b\u52a8\u91c7\u8bbf\u6f5c\u5728\u53d7\u5bb3\u8005\uff0c\u6536\u96c6\u8be6\u7ec6\u5bf9\u8bdd\u4fe1\u606f\uff0c\u968f\u540e\u7531\u53e6\u4e00AI\u7cfb\u7edf\u63d0\u53d6\u5e76\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6570\u636e\uff0c\u7528\u4e8e\u81ea\u52a8\u548c\u624b\u52a8\u6267\u884c\u673a\u5236\u3002", "result": "\u5728Google Pay India\u4e0a\u5b9e\u65bdCASE\u6846\u67b6\u540e\uff0c\u8bc8\u9a97\u6267\u884c\u91cf\u63d0\u5347\u4e8621%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCASE\u7684\u65b0\u578bAgentic AI\u6846\u67b6\uff0c\u901a\u8fc7\u6536\u96c6\u548c\u7ba1\u7406\u7528\u6237\u8bc8\u9a97\u53cd\u9988\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bc8\u9a97\u8bc6\u522b\u7684\u53ca\u65f6\u6027\u548c\u51c6\u786e\u6027\u3002\u8be5\u6846\u67b6\u5728Google Pay India\u4e0a\u7684\u5b9e\u65bd\u663e\u793a\uff0c\u8bc8\u9a97\u6267\u884c\u91cf\u63d0\u5347\u4e8621%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.19926", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19926", "abs": "https://arxiv.org/abs/2508.19926", "authors": ["Tan Jing", "Shiting Chen", "Yangfan Li", "Weisheng Xu", "Renjing Xu"], "title": "FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control", "comment": null, "summary": "Unified physics-based humanoid controllers are pivotal for robotics and\ncharacter animation, yet models that excel on gentle, everyday motions still\nstumble on explosive actions, hampering real-world deployment. We bridge this\ngap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts),\nan end-to-end framework composed of frame-accelerated augmentation, a robust\nbase controller, and a residual mixture-of-experts (MoE). Frame-accelerated\naugmentation exposes the model to high-velocity pose changes by widening\ninter-frame gaps. The base controller reliably tracks everyday low-dynamic\nmotions, while the residual MoE adaptively allocates additional network\ncapacity to handle challenging high-dynamic actions, significantly enhancing\ntracking accuracy. In the absence of a public benchmark, we curate the\nHigh-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically\nplausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8\\% and\nlowers global mean per-joint position error by 14.6\\% relative to the baseline,\nwhile preserving near-perfect accuracy on low-dynamic motions. These results\nestablish FARM as a new baseline for high-dynamic humanoid control and\nintroduce the first open benchmark dedicated to this challenge. The code and\ndataset will be released at https://github.com/Colin-Jing/FARM.", "AI": {"tldr": "FARM\u901a\u8fc7\u5e27\u52a0\u901f\u589e\u5f3a\u548c\u6b8b\u5deeMoE\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u52a8\u6001\u4eba\u5f62\u52a8\u4f5c\u7684\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u5e76\u516c\u5f00\u4e86\u9996\u4e2a\u9ad8\u52a8\u6001\u4eba\u5f62\u52a8\u4f5c\u6570\u636e\u96c6HDHM\u3002", "motivation": "\u73b0\u6709\u7684\u7edf\u4e00\u7269\u7406\u4eba\u5f62\u63a7\u5236\u5668\u5728\u5904\u7406\u7206\u70b8\u6027\u52a8\u4f5c\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002FARM\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u5347\u9ad8\u52a8\u6001\u52a8\u4f5c\u7684\u8ddf\u8e2a\u80fd\u529b\u3002", "method": "FARM\u6846\u67b6\u5305\u62ec\u5e27\u52a0\u901f\u589e\u5f3a\u3001\u9c81\u68d2\u7684\u57fa\u7840\u63a7\u5236\u5668\u548c\u6b8b\u5dee\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u3002\u5e27\u52a0\u901f\u589e\u5f3a\u901a\u8fc7\u6269\u5927\u5e27\u95f4\u95f4\u9694\u4f7f\u6a21\u578b\u63a5\u89e6\u9ad8\u901f\u59ff\u6001\u53d8\u5316\uff0c\u57fa\u7840\u63a7\u5236\u5668\u53ef\u9760\u8ddf\u8e2a\u65e5\u5e38\u4f4e\u52a8\u6001\u52a8\u4f5c\uff0c\u6b8b\u5deeMoE\u81ea\u9002\u5e94\u5206\u914d\u989d\u5916\u7f51\u7edc\u5bb9\u91cf\u5904\u7406\u9ad8\u52a8\u6001\u52a8\u4f5c\u3002", "result": "\u5728HDHM\u6570\u636e\u96c6\u4e0a\uff0cFARM\u5c06\u8ddf\u8e2a\u5931\u8d25\u7387\u964d\u4f4e\u4e8642.8%\uff0c\u5168\u5c40\u5e73\u5747\u6bcf\u5173\u8282\u4f4d\u7f6e\u8bef\u5dee\u964d\u4f4e\u4e8614.6%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u52a8\u6001\u52a8\u4f5c\u7684\u8fd1\u4e4e\u5b8c\u7f8e\u51c6\u786e\u6027\u3002", "conclusion": "FARM\u901a\u8fc7\u7ed3\u5408\u5e27\u52a0\u901f\u589e\u5f3a\u3001\u9c81\u68d2\u7684\u57fa\u7840\u63a7\u5236\u5668\u548c\u6b8b\u5dee\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u52a8\u6001\u52a8\u4f5c\u7684\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u52a8\u6001\u52a8\u4f5c\u7684\u8fd1\u4e4e\u5b8c\u7f8e\u51c6\u786e\u6027\uff0c\u4e3a\u9ad8\u52a8\u6001\u4eba\u5f62\u63a7\u5236\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u5e76\u9996\u6b21\u516c\u5f00\u4e86\u76f8\u5173\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2508.19349", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19349", "abs": "https://arxiv.org/abs/2508.19349", "authors": ["Mahdieh Behjat Khatooni", "Mohsen Soryani"], "title": "EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis", "comment": null, "summary": "Alzheimer's disease (AD) is one of the most prevalent neurodegenerative\ndisorders worldwide. As it progresses, it leads to the deterioration of\ncognitive functions. Since AD is irreversible, early diagnosis is crucial for\nmanaging its progression. Mild Cognitive Impairment (MCI) represents an\nintermediate stage between Cognitively Normal (CN) individuals and those with\nAD, and is considered a transitional phase from normal cognition to Alzheimer's\ndisease. Diagnosing MCI is particularly challenging due to the subtle\ndifferences between adjacent diagnostic categories. In this study, we propose\nEffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole\nAlzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging\n(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a\nVision Transformer (ViT) to capture both local and global features from MRI\nimages. Unlike previous studies that rely on limited subsets of data, our\napproach is trained on the full T1-weighted MRI dataset from ADNI, resulting in\na more robust and unbiased model. This comprehensive methodology enhances the\nmodel's clinical reliability. Furthermore, fine-tuning large pretrained models\noften yields suboptimal results when source and target dataset domains differ.\nTo address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt\nthe pretrained ViT model to our target domain. This method enables efficient\nknowledge transfer and reduces the risk of overfitting. Our model achieves a\nclassification accuracy of 92.52% and an F1-score of 92.76% across three\ndiagnostic categories: AD, MCI, and CN for full ADNI dataset.", "AI": {"tldr": "EffNetViTLoRA\u6a21\u578b\u6574\u5408CNN\u548cViT\uff0c\u5229\u7528LoRA\u6280\u672f\u4f18\u5316\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u5b8c\u6574ADNI MRI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6AD\u3001MCI\u548cCN\u5206\u7c7b\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u65e9\u671f\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u8bca\u65ad\u56e0\u7c7b\u522b\u95f4\u5dee\u5f02\u7ec6\u5fae\u800c\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u7814\u7a76\u591a\u4f9d\u8d56\u6709\u9650\u6570\u636e\u5b50\u96c6\uff0c\u5bfc\u81f4\u6a21\u578b\u504f\u5dee\u3002", "method": "\u63d0\u51faEffNetViTLoRA\u6a21\u578b\uff0c\u7ed3\u5408CNN\u548cViT\u6355\u6349MRI\u56fe\u50cf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u4f7f\u7528LoRA\u6280\u672f\u9002\u5e94\u9884\u8bad\u7ec3ViT\u6a21\u578b\uff0c\u5e76\u5728\u5b8c\u6574ADNI MRI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5728ADNI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8692.52%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u548c92.76%\u7684F1\u5206\u6570\uff0c\u6db5\u76d6AD\u3001MCI\u548cCN\u4e09\u7c7b\u8bca\u65ad\u3002", "conclusion": "EffNetViTLoRA\u6a21\u578b\u901a\u8fc7\u6574\u5408CNN\u548cViT\uff0c\u7ed3\u5408LoRA\u6280\u672f\uff0c\u5728ADNI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684AD\u3001MCI\u548cCN\u5206\u7c7b\uff0c\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2508.19963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19963", "abs": "https://arxiv.org/abs/2508.19963", "authors": ["M. Umlauft", "M. Schranz"], "title": "Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants", "comment": "This is the author's version of a paper reviewed and accepted by the\n  9th International Symposium on Swarm Behavior and Bio-Inspired Robotics 2025.\n  Authors were not able to present it due to time constraints. 3 Tables, 5\n  Figures", "summary": "Optimizing modern production plants using the job-shop principle is a known\nhard problem. For very large plants, like semiconductor fabs, the problem\nbecomes unsolvable on a plant-wide scale in a reasonable amount of time using\nclassical linear optimization. An alternative approach is the use of swarm\nintelligence algorithms. These have been applied to the job-shop problem\nbefore, but often in a centrally calculated way where they are applied to the\nsolution space, but they can be implemented in a bottom-up fashion to avoid\nglobal result computation as well. One of the problems in semiconductor\nproduction is that the production process requires a lot of switching between\nmachines that process lots one after the other and machines that process\nbatches of lots at once, often with long processing times. In this paper, we\naddress this switching problem with the ``boids'' flocking algorithm that was\noriginally used in robotics and movie industry. The flocking behavior is a\nbio-inspired algorithm that uses only local information and interaction based\non simple heuristics. We show that this algorithm addresses these valid\nconsiderations in production plant optimization, as it reacts to the switching\nof machine kinds similar to how a swarm of flocking animals would react to\nobstacles in its course.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528boids\u7fa4\u96c6\u7b97\u6cd5\u89e3\u51b3\u534a\u5bfc\u4f53\u751f\u4ea7\u4e2d\u7684\u673a\u5668\u5207\u6362\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u751f\u7269\u542f\u53d1\u7b97\u6cd5\u5728\u751f\u4ea7\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u534a\u5bfc\u4f53\u751f\u4ea7\u4e2d\u7684\u673a\u5668\u5207\u6362\u95ee\u9898\u590d\u6742\u4e14\u96be\u4ee5\u901a\u8fc7\u4f20\u7edf\u7ebf\u6027\u4f18\u5316\u65b9\u6cd5\u5728\u5408\u7406\u65f6\u95f4\u5185\u89e3\u51b3\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u65b0\u7684\u7b97\u6cd5\u3002", "method": "\u91c7\u7528boids\u7fa4\u96c6\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u4fe1\u606f\u548c\u7b80\u5355\u542f\u53d1\u5f0f\u7684\u751f\u7269\u542f\u53d1\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u534a\u5bfc\u4f53\u751f\u4ea7\u4e2d\u7684\u673a\u5668\u5207\u6362\u95ee\u9898\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cboids\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u6a21\u62df\u7fa4\u96c6\u884c\u4e3a\uff0c\u5904\u7406\u673a\u5668\u5207\u6362\u95ee\u9898\uff0c\u4e3a\u751f\u4ea7\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5e94\u7528\u4eff\u751f\u7b97\u6cd5\uff08boids\u7fa4\u96c6\u7b97\u6cd5\uff09\u6210\u529f\u89e3\u51b3\u4e86\u534a\u5bfc\u4f53\u751f\u4ea7\u4e2d\u7684\u673a\u5668\u5207\u6362\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u751f\u7269\u542f\u53d1\u7b97\u6cd5\u5728\u73b0\u4ee3\u751f\u4ea7\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.19953", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19953", "abs": "https://arxiv.org/abs/2508.19953", "authors": ["Rafael Cathomen", "Mayank Mittal", "Marin Vlastelica", "Marco Hutter"], "title": "Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors", "comment": "Accepted to CoRL 2025. For code and videos, please check:\n  https://leggedrobotics.github.io/d3-skill-discovery/", "summary": "Unsupervised Skill Discovery (USD) allows agents to autonomously learn\ndiverse behaviors without task-specific rewards. While recent USD methods have\nshown promise, their application to real-world robotics remains underexplored.\nIn this paper, we propose a modular USD framework to address the challenges in\nthe safety, interpretability, and deployability of the learned skills. Our\napproach employs user-defined factorization of the state space to learn\ndisentangled skill representations. It assigns different skill discovery\nalgorithms to each factor based on the desired intrinsic reward function. To\nencourage structured morphology-aware skills, we introduce symmetry-based\ninductive biases tailored to individual factors. We also incorporate a style\nfactor and regularization penalties to promote safe and robust behaviors. We\nevaluate our framework in simulation using a quadrupedal robot and demonstrate\nzero-shot transfer of the learned skills to real hardware. Our results show\nthat factorization and symmetry lead to the discovery of structured\nhuman-interpretable behaviors, while the style factor and penalties enhance\nsafety and diversity. Additionally, we show that the learned skills can be used\nfor downstream tasks and perform on par with oracle policies trained with\nhand-crafted rewards.", "AI": {"tldr": "\u6a21\u5757\u5316USD\u6846\u67b6\u901a\u8fc7\u72b6\u6001\u5206\u89e3\u548c\u5bf9\u79f0\u6027\u504f\u7f6e\uff0c\u5b9e\u73b0\u5b89\u5168\u3001\u53ef\u89e3\u91ca\u7684\u6280\u80fd\u53d1\u73b0\uff0c\u5e76\u5728\u4eff\u771f\u548c\u786c\u4ef6\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dUSD\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u90e8\u7f72\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u63a8\u52a8\u5176\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u7528\u6237\u5b9a\u4e49\u7684\u72b6\u6001\u7a7a\u95f4\u5206\u89e3\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u56e0\u5b50\u5206\u914d\u4e0d\u540c\u7684\u6280\u80fd\u53d1\u73b0\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u5bf9\u79f0\u6027\u8bf1\u5bfc\u504f\u7f6e\u548c\u98ce\u683c\u56e0\u5b50\u53ca\u6b63\u5219\u5316\u60e9\u7f5a\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u786c\u4ef6\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u53d1\u73b0\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\uff0c\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\u548c\u591a\u6837\u6027\uff0c\u4e14\u5b66\u4e60\u5230\u7684\u6280\u80fd\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u5757\u5316USD\u6846\u67b6\u901a\u8fc7\u72b6\u6001\u7a7a\u95f4\u5206\u89e3\u548c\u5bf9\u79f0\u6027\u8bf1\u5bfc\u504f\u7f6e\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u4e14\u5b89\u5168\u7684\u6280\u80fd\u53d1\u73b0\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u9645\u786c\u4ef6\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.19477", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19477", "abs": "https://arxiv.org/abs/2508.19477", "authors": ["Zachary L. Crang", "Rich D. Johnston", "Katie L. Mills", "Johsan Billingham", "Sam Robertson", "Michael H. Cole", "Jonathon Weakley", "Adam Hewitt and", "Grant M. Duthie"], "title": "Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage", "comment": null, "summary": "This study aimed to: (1) understand whether commercially available\ncomputer-vision and artificial intelligence (AI) player tracking software can\naccurately measure player position, speed and distance using broadcast footage\nand (2) determine the impact of camera feed and resolution on accuracy. Data\nwere obtained from one match at the 2022 Qatar Federation Internationale de\nFootball Association (FIFA) World Cup. Tactical, programme and camera 1 feeds\nwere used. Three commercial tracking providers that use computer-vision and AI\nparticipated. Providers analysed instantaneous position (x, y coordinates) and\nspeed (m\\,s^{-1}) of each player. Their data were compared with a\nhigh-definition multi-camera tracking system (TRACAB Gen 5). Root mean square\nerror (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to\n16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\\,s^{-1}. Total match\ndistance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across\nproviders. Computer-vision and AI player tracking software offer the ability to\ntrack players with fair precision when players are detected by the software.\nProviders should use a tactical feed when tracking position and speed, which\nwill maximise player detection, improving accuracy. Both 720p and 1080p\nresolutions are suitable, assuming appropriate computer-vision and AI models\nare implemented.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5546\u4e1a\u8ba1\u7b97\u673a\u89c6\u89c9\u548cAI\u7403\u5458\u8ffd\u8e2a\u8f6f\u4ef6\u5728\u5e7f\u64ad\u753b\u9762\u4e0a\u80fd\u591f\u4ee5\u5408\u7406\u7cbe\u5ea6\u8ffd\u8e2a\u7403\u5458\uff0c\u6218\u672f\u753b\u9762\u548c\u9002\u5f53\u5206\u8fa8\u7387\u662f\u5173\u952e\u3002", "motivation": "\u8bc4\u4f30\u5546\u4e1a\u8ba1\u7b97\u673a\u89c6\u89c9\u548cAI\u7403\u5458\u8ffd\u8e2a\u8f6f\u4ef6\u5728\u5e7f\u64ad\u753b\u9762\u4e0a\u6d4b\u91cf\u7403\u5458\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u8ddd\u79bb\u7684\u51c6\u786e\u6027\uff0c\u5e76\u786e\u5b9a\u6444\u50cf\u5934\u753b\u9762\u548c\u5206\u8fa8\u7387\u5bf9\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u5546\u4e1a\u8ffd\u8e2a\u63d0\u4f9b\u5546\u7684\u6570\u636e\u4e0eTRACAB Gen 5\u9ad8\u6e05\u591a\u6444\u50cf\u5934\u8ffd\u8e2a\u7cfb\u7edf\u7684\u6570\u636e\uff0c\u8ba1\u7b97\u4e86\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u548c\u5e73\u5747\u504f\u5dee\u3002", "result": "\u4f4d\u7f6eRMSE\u8303\u56f4\u4e3a1.68\u81f316.39\u7c73\uff0c\u901f\u5ea6RMSE\u8303\u56f4\u4e3a0.34\u81f32.38\u7c73/\u79d2\u3002\u6bd4\u8d5b\u603b\u8ddd\u79bb\u7684\u5e73\u5747\u504f\u5dee\u8303\u56f4\u4e3a-1745\u7c73\uff08-21.8%\uff09\u81f31945\u7c73\uff0824.3%\uff09\u3002", "conclusion": "\u8ba1\u7b97\u673a\u89c6\u89c9\u548cAI\u7403\u5458\u8ffd\u8e2a\u8f6f\u4ef6\u5728\u7403\u5458\u88ab\u68c0\u6d4b\u5230\u65f6\u80fd\u591f\u63d0\u4f9b\u5408\u7406\u7684\u7cbe\u786e\u5ea6\u3002\u5efa\u8bae\u63d0\u4f9b\u5546\u5728\u8ffd\u8e2a\u4f4d\u7f6e\u548c\u901f\u5ea6\u65f6\u4f7f\u7528\u6218\u672f\u753b\u9762\uff0c\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002720p\u548c1080p\u5206\u8fa8\u7387\u5747\u9002\u7528\uff0c\u524d\u63d0\u662f\u5b9e\u65bd\u4e86\u9002\u5f53\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548cAI\u6a21\u578b\u3002"}}
{"id": "2508.20018", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20018", "abs": "https://arxiv.org/abs/2508.20018", "authors": ["Quanfeng Lu", "Zhantao Ma", "Shuai Zhong", "Jin Wang", "Dahai Yu", "Michael K. Ng", "Ping Luo"], "title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control", "comment": "28 pages, 12 figures", "summary": "The rapid advancement of large vision language models (LVLMs) and agent\nsystems has heightened interest in mobile GUI agents that can reliably\ntranslate natural language into interface operations. Existing single-agent\napproaches, however, remain limited by structural constraints. Although\nmulti-agent systems naturally decouple different competencies, recent progress\nin multi-agent reinforcement learning (MARL) has often been hindered by\ninefficiency and remains incompatible with current LVLM architectures. To\naddress these challenges, we introduce SWIRL, a staged workflow for interleaved\nreinforcement learning designed for multi-agent systems. SWIRL reformulates\nMARL into a sequence of single-agent reinforcement learning tasks, updating one\nagent at a time while keeping the others fixed. This formulation enables stable\ntraining and promotes efficient coordination across agents. Theoretically, we\nprovide a stepwise safety bound, a cross-round monotonic improvement theorem,\nand convergence guarantees on return, ensuring robust and principled\noptimization. In application to mobile GUI control, SWIRL instantiates a\nNavigator that converts language and screen context into structured plans, and\nan Interactor that grounds these plans into executable atomic actions.\nExtensive experiments demonstrate superior performance on both high-level and\nlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong\ncapability in multi-agent mathematical reasoning, underscoring its potential as\na general framework for developing efficient and robust multi-agent systems.", "AI": {"tldr": "SWIRL\u662f\u4e00\u79cd\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06MARL\u4efb\u52a1\u5206\u89e3\u4e3a\u5355\u667a\u80fd\u4f53\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u534f\u8c03\uff0c\u5728\u79fb\u52a8GUI\u63a7\u5236\u548c\u6570\u5b66\u63a8\u7406\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u79fb\u52a8GUI\u4ee3\u7406\u4e2d\u53d7\u9650\u4e8e\u7ed3\u6784\u7ea6\u675f\uff0c\u800c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u867d\u80fd\u89e3\u8026\u4e0d\u540c\u80fd\u529b\uff0c\u4f46\u5176\u5f3a\u5316\u5b66\u4e60\u8fdb\u5c55\u5e38\u56e0\u6548\u7387\u4f4e\u4e0b\u4e14\u4e0e\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u67b6\u6784\u4e0d\u517c\u5bb9\u800c\u53d7\u963b\u3002", "method": "SWIRL\u901a\u8fc7\u5c06\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e00\u7cfb\u5217\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u6bcf\u6b21\u66f4\u65b0\u4e00\u4e2a\u667a\u80fd\u4f53\u800c\u56fa\u5b9a\u5176\u4ed6\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u8bad\u7ec3\u548c\u9ad8\u6548\u534f\u8c03\u3002", "result": "\u5728\u79fb\u52a8GUI\u63a7\u5236\u4efb\u52a1\u4e2d\uff0cSWIRL\u5728\u9ad8\u5c42\u6b21\u548c\u4f4e\u5c42\u6b21GUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u5e76\u5728\u591a\u667a\u80fd\u4f53\u6570\u5b66\u63a8\u7406\u4e2d\u5c55\u73b0\u4e86\u5f3a\u5927\u80fd\u529b\u3002", "conclusion": "SWIRL\u4f5c\u4e3a\u4e00\u79cd\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e0d\u4ec5\u5728\u79fb\u52a8GUI\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u5728\u591a\u667a\u80fd\u4f53\u6570\u5b66\u63a8\u7406\u4e2d\u5c55\u73b0\u4e86\u5f3a\u5927\u6f5c\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u9ad8\u6548\u3001\u7a33\u5065\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u7528\u6846\u67b6\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.19958", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19958", "abs": "https://arxiv.org/abs/2508.19958", "authors": ["Yiguo Fan", "Pengxiang Ding", "Shuanghao Bai", "Xinyang Tong", "Yuyang Zhu", "Hongchao Lu", "Fengqi Dai", "Wei Zhao", "Yang Liu", "Siteng Huang", "Zhaoxin Fan", "Badong Chen", "Donglin Wang"], "title": "Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation", "comment": "Accepted to CoRL 2025; Github Page: https://long-vla.github.io", "summary": "Vision-Language-Action (VLA) models have become a cornerstone in robotic\npolicy learning, leveraging large-scale multimodal data for robust and scalable\ncontrol. However, existing VLA frameworks primarily address short-horizon\ntasks, and their effectiveness on long-horizon, multi-step robotic manipulation\nremains limited due to challenges in skill chaining and subtask dependencies.\nIn this work, we introduce Long-VLA, the first end-to-end VLA model\nspecifically designed for long-horizon robotic tasks. Our approach features a\nnovel phase-aware input masking strategy that adaptively segments each subtask\ninto moving and interaction phases, enabling the model to focus on\nphase-relevant sensory cues and enhancing subtask compatibility. This unified\nstrategy preserves the scalability and data efficiency of VLA training, and our\narchitecture-agnostic module can be seamlessly integrated into existing VLA\nmodels. We further propose the L-CALVIN benchmark to systematically evaluate\nlong-horizon manipulation. Extensive experiments on both simulated and\nreal-world tasks demonstrate that Long-VLA significantly outperforms prior\nstate-of-the-art methods, establishing a new baseline for long-horizon robotic\ncontrol.", "AI": {"tldr": "Long-VLA \u662f\u9996\u4e2a\u4e13\u4e3a\u957f\u89c6\u91ce\u673a\u5668\u4eba\u4efb\u52a1\u8bbe\u8ba1\u7684\u7aef\u5230\u7aef VLA \u6a21\u578b\uff0c\u901a\u8fc7\u76f8\u4f4d\u611f\u77e5\u8f93\u5165\u63a9\u7801\u7b56\u7565\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684 VLA \u6846\u67b6\u4e3b\u8981\u9488\u5bf9\u77ed\u89c6\u91ce\u4efb\u52a1\uff0c\u800c\u5728\u957f\u89c6\u91ce\u3001\u591a\u6b65\u9aa4\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6709\u6548\u6027\u53d7\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u6280\u80fd\u94fe\u548c\u5b50\u4efb\u52a1\u4f9d\u8d56\u6027\u7684\u6311\u6218\u3002", "method": "Long-VLA \u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76f8\u4f4d\u611f\u77e5\u8f93\u5165\u63a9\u7801\u7b56\u7565\uff0c\u5c06\u6bcf\u4e2a\u5b50\u4efb\u52a1\u81ea\u9002\u5e94\u5730\u5206\u5272\u4e3a\u79fb\u52a8\u548c\u4ea4\u4e92\u9636\u6bb5\uff0c\u4ee5\u589e\u5f3a\u5b50\u4efb\u52a1\u517c\u5bb9\u6027\u3002\u8be5\u65b9\u6cd5\u4fdd\u7559\u4e86 VLA \u8bad\u7ec3\u7684\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u5e76\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709 VLA \u6a21\u578b\u4e2d\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLong-VLA \u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Long-VLA \u6a21\u578b\u901a\u8fc7\u5f15\u5165\u76f8\u4f4d\u611f\u77e5\u8f93\u5165\u63a9\u7801\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u91ce\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u957f\u89c6\u91ce\u673a\u5668\u4eba\u63a7\u5236\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2508.19485", "categories": ["cs.CV", "68T45 (Primary), 68T07 (Secondary)", "I.2.10; I.4.6"], "pdf": "https://arxiv.org/pdf/2508.19485", "abs": "https://arxiv.org/abs/2508.19485", "authors": ["Xinlong Zhao", "Qixiang Pang", "Shan Du"], "title": "JVLGS: Joint Vision-Language Gas Leak Segmentation", "comment": "19 pages, 13 figures", "summary": "Gas leaks pose serious threats to human health and contribute significantly\nto atmospheric pollution, drawing increasing public concern. However, the lack\nof effective detection methods hampers timely and accurate identification of\ngas leaks. While some vision-based techniques leverage infrared videos for leak\ndetection, the blurry and non-rigid nature of gas clouds often limits their\neffectiveness. To address these challenges, we propose a novel framework called\nJoint Vision-Language Gas leak Segmentation (JVLGS), which integrates the\ncomplementary strengths of visual and textual modalities to enhance gas leak\nrepresentation and segmentation. Recognizing that gas leaks are sporadic and\nmany video frames may contain no leak at all, our method incorporates a\npost-processing step to reduce false positives caused by noise and non-target\nobjects, an issue that affects many existing approaches. Extensive experiments\nconducted across diverse scenarios show that JVLGS significantly outperforms\nstate-of-the-art gas leak segmentation methods. We evaluate our model under\nboth supervised and few-shot learning settings, and it consistently achieves\nstrong performance in both, whereas competing methods tend to perform well in\nonly one setting or poorly in both. Code available at:\nhttps://github.com/GeekEagle/JVLGS", "AI": {"tldr": "JVLGS\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u5347\u6c14\u4f53\u6cc4\u6f0f\u68c0\u6d4b\u6027\u80fd\uff0c\u51cf\u5c11\u8bef\u62a5\uff0c\u5e76\u5728\u591a\u79cd\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6c14\u4f53\u6cc4\u6f0f\u5bf9\u4eba\u7c7b\u5065\u5eb7\u548c\u5927\u6c14\u6c61\u67d3\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u4f46\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u56e0\u6c14\u4f53\u4e91\u7684\u975e\u521a\u6027\u548c\u6a21\u7cca\u6027\u800c\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aJVLGS\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u5e76\u5f15\u5165\u4e86\u540e\u5904\u7406\u6b65\u9aa4\u4ee5\u51cf\u5c11\u8bef\u62a5\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u5b9e\u9a8c\u573a\u666f\u4e2d\uff0cJVLGS\u5728\u76d1\u7763\u5b66\u4e60\u548c\u5c11\u6837\u672c\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "JVLGS\u6846\u67b6\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c14\u4f53\u6cc4\u6f0f\u7684\u68c0\u6d4b\u548c\u5206\u5272\u6027\u80fd\uff0c\u5e76\u5728\u591a\u79cd\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.20040", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20040", "abs": "https://arxiv.org/abs/2508.20040", "authors": ["Przemyslaw Biecek", "Wojciech Samek"], "title": "Model Science: getting serious about verification, explanation and control of AI systems", "comment": "8 pages", "summary": "The growing adoption of foundation models calls for a paradigm shift from\nData Science to Model Science. Unlike data-centric approaches, Model Science\nplaces the trained model at the core of analysis, aiming to interact, verify,\nexplain, and control its behavior across diverse operational contexts. This\npaper introduces a conceptual framework for a new discipline called Model\nScience, along with the proposal for its four key pillars: Verification, which\nrequires strict, context-aware evaluation protocols; Explanation, which is\nunderstood as various approaches to explore of internal model operations;\nControl, which integrates alignment techniques to steer model behavior; and\nInterface, which develops interactive and visual explanation tools to improve\nhuman calibration and decision-making. The proposed framework aims to guide the\ndevelopment of credible, safe, and human-aligned AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faModel Science\u6846\u67b6\uff0c\u5305\u542b\u9a8c\u8bc1\u3001\u89e3\u91ca\u3001\u63a7\u5236\u548c\u63a5\u53e3\u56db\u5927\u652f\u67f1\uff0c\u65e8\u5728\u63a8\u52a8\u53ef\u4fe1\u3001\u5b89\u5168\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684AI\u7cfb\u7edf\u53d1\u5c55\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u4ece\u6570\u636e\u79d1\u5b66\u8f6c\u5411\u6a21\u578b\u79d1\u5b66\uff0c\u4ee5\u6a21\u578b\u4e3a\u6838\u5fc3\u8fdb\u884c\u5206\u6790\u548c\u64cd\u4f5c\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u56db\u4e2a\u5173\u952e\u652f\u67f1\uff1a\u9a8c\u8bc1\u3001\u89e3\u91ca\u3001\u63a7\u5236\u548c\u63a5\u53e3\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u5b66\u79d1\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86Model Science\u7684\u56db\u4e2a\u5173\u952e\u652f\u67f1\u53ca\u5176\u5177\u4f53\u5185\u5bb9\uff0c\u4e3a\u672a\u6765AI\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Model Science\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u65e8\u5728\u6307\u5bfc\u53ef\u4fe1\u3001\u5b89\u5168\u548c\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684AI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.20037", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20037", "abs": "https://arxiv.org/abs/2508.20037", "authors": ["Henk H. A. Jekel", "Alejandro D\u00edaz Rosales", "Luka Peternel"], "title": "Visio-Verbal Teleimpedance Interface: Enabling Semi-Autonomous Control of Physical Interaction via Eye Tracking and Speech", "comment": null, "summary": "The paper presents a visio-verbal teleimpedance interface for commanding 3D\nstiffness ellipsoids to the remote robot with a combination of the operator's\ngaze and verbal interaction. The gaze is detected by an eye-tracker, allowing\nthe system to understand the context in terms of what the operator is currently\nlooking at in the scene. Along with verbal interaction, a Visual Language Model\n(VLM) processes this information, enabling the operator to communicate their\nintended action or provide corrections. Based on these inputs, the interface\ncan then generate appropriate stiffness matrices for different physical\ninteraction actions. To validate the proposed visio-verbal teleimpedance\ninterface, we conducted a series of experiments on a setup including a Force\nDimension Sigma.7 haptic device to control the motion of the remote Kuka LBR\niiwa robotic arm. The human operator's gaze is tracked by Tobii Pro Glasses 2,\nwhile human verbal commands are processed by a VLM using GPT-4o. The first\nexperiment explored the optimal prompt configuration for the interface. The\nsecond and third experiments demonstrated different functionalities of the\ninterface on a slide-in-the-groove task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6ce8\u89c6\u548c\u8bed\u97f3\u7684\u9065\u963b\u6297\u754c\u9762\uff0c\u7528\u4e8e\u63a7\u5236\u8fdc\u7a0b\u673a\u5668\u4eba\u76843D\u521a\u5ea6\u692d\u7403\u4f53\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u8fdc\u7a0b\u673a\u5668\u4eba\u63a7\u5236\u7684\u76f4\u89c2\u6027\u548c\u4ea4\u4e92\u6548\u7387\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u97f3\u8f93\u5165\u7684\u9065\u963b\u6297\u754c\u9762\uff0c\u65e8\u5728\u7b80\u5316\u590d\u6742\u7269\u7406\u4ea4\u4e92\u52a8\u4f5c\u7684\u547d\u4ee4\u751f\u6210\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u773c\u52a8\u4eea\u6355\u6349\u64cd\u4f5c\u8005\u7684\u6ce8\u89c6\u4fe1\u606f\uff0c\u7ed3\u5408\u8bed\u97f3\u4ea4\u4e92\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5904\u7406\uff0c\u751f\u6210\u9002\u5f53\u7684\u521a\u5ea6\u77e9\u9635\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e86Force Dimension Sigma.7\u89e6\u89c9\u8bbe\u5907\u548cKuka LBR iiwa\u673a\u68b0\u81c2\uff0c\u4ee5\u53caTobii Pro Glasses 2\u773c\u52a8\u4eea\u548cGPT-4o\u5904\u7406\u7684\u8bed\u97f3\u547d\u4ee4\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u754c\u9762\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u63a2\u7d22\u6700\u4f18\u63d0\u793a\u914d\u7f6e\u548c\u6f14\u793a\u5728\u6ed1\u69fd\u4efb\u52a1\u4e2d\u7684\u4e0d\u540c\u529f\u80fd\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u89c6-\u8bed\u9065\u963b\u6297\u754c\u9762\u901a\u8fc7\u7ed3\u5408\u64cd\u4f5c\u8005\u7684\u6ce8\u89c6\u548c\u8bed\u97f3\u4ea4\u4e92\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u8fdc\u7a0b\u673a\u5668\u4eba3D\u521a\u5ea6\u692d\u7403\u4f53\u7684\u6709\u6548\u63a7\u5236\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u754c\u9762\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2508.19498", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19498", "abs": "https://arxiv.org/abs/2508.19498", "authors": ["Yimu Wang", "Weiming Zhuang", "Chen Chen", "Jiabo Huang", "Jingtao Li", "Lingjuan Lyu"], "title": "UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models", "comment": null, "summary": "In the era of deep learning, the increasing number of pre-trained models\navailable online presents a wealth of knowledge. These models, developed with\ndiverse architectures and trained on varied datasets for different tasks,\nprovide unique interpretations of the real world. Their collective consensus is\nlikely universal and generalizable to unseen data. However, effectively\nharnessing this collective knowledge poses a fundamental challenge due to the\nheterogeneity of pre-trained models. Existing knowledge integration solutions\ntypically rely on strong assumptions about training data distributions and\nnetwork architectures, limiting them to learning only from specific types of\nmodels and resulting in data and/or inductive biases. In this work, we\nintroduce a novel framework, namely UNIFORM, for knowledge transfer from a\ndiverse set of off-the-shelf models into one student model without such\nconstraints. Specifically, we propose a dedicated voting mechanism to capture\nthe consensus of knowledge both at the logit level -- incorporating teacher\nmodels that are capable of predicting target classes of interest -- and at the\nfeature level, utilizing visual representations learned on arbitrary label\nspaces. Extensive experiments demonstrate that UNIFORM effectively enhances\nunsupervised object recognition performance compared to strong knowledge\ntransfer baselines. Notably, it exhibits remarkable scalability by benefiting\nfrom over one hundred teachers, while existing methods saturate at a much\nsmaller scale.", "AI": {"tldr": "UNIFORM\u6846\u67b6\u901a\u8fc7\u6295\u7968\u673a\u5236\u6574\u5408\u5f02\u6784\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u65e0\u76d1\u7763\u7269\u4f53\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6269\u5c55\u6027\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5f02\u6784\u6027\u4f7f\u5f97\u6709\u6548\u5229\u7528\u5176\u96c6\u4f53\u77e5\u8bc6\u6210\u4e3a\u6311\u6218\uff0c\u73b0\u6709\u77e5\u8bc6\u6574\u5408\u65b9\u6cd5\u56e0\u4f9d\u8d56\u5f3a\u5047\u8bbe\u800c\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUNIFORM\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u7528\u7684\u6295\u7968\u673a\u5236\u5728\u903b\u8f91\u5c42\u548c\u7279\u5f81\u5c42\u6355\u6349\u77e5\u8bc6\u7684\u5171\u8bc6\uff0c\u65e0\u9700\u5bf9\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u6216\u7f51\u7edc\u67b6\u6784\u505a\u51fa\u5f3a\u5047\u8bbe\u3002", "result": "UNIFORM\u5728\u65e0\u76d1\u7763\u7269\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u77e5\u8bc6\u8f6c\u79fb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "UNIFORM\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u7269\u4f53\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u663e\u8457\u7684\u6269\u5c55\u6027\uff0c\u80fd\u591f\u4ece\u8d85\u8fc7\u4e00\u767e\u4e2a\u6559\u5e08\u6a21\u578b\u4e2d\u53d7\u76ca\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u66f4\u5c0f\u89c4\u6a21\u65f6\u5c31\u8fbe\u5230\u9971\u548c\u3002"}}
{"id": "2508.20085", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20085", "abs": "https://arxiv.org/abs/2508.20085", "authors": ["Zhecheng Yuan", "Tianming Wei", "Langzhe Gu", "Pu Hua", "Tianhai Liang", "Yuanpei Chen", "Huazhe Xu"], "title": "HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation", "comment": null, "summary": "Leveraging human motion data to impart robots with versatile manipulation\nskills has emerged as a promising paradigm in robotic manipulation.\nNevertheless, translating multi-source human hand motions into feasible robot\nbehaviors remains challenging, particularly for robots equipped with\nmulti-fingered dexterous hands characterized by complex, high-dimensional\naction spaces. Moreover, existing approaches often struggle to produce policies\ncapable of adapting to diverse environmental conditions. In this paper, we\nintroduce HERMES, a human-to-robot learning framework for mobile bimanual\ndexterous manipulation. First, HERMES formulates a unified reinforcement\nlearning approach capable of seamlessly transforming heterogeneous human hand\nmotions from multiple sources into physically plausible robotic behaviors.\nSubsequently, to mitigate the sim2real gap, we devise an end-to-end, depth\nimage-based sim2real transfer method for improved generalization to real-world\nscenarios. Furthermore, to enable autonomous operation in varied and\nunstructured environments, we augment the navigation foundation model with a\nclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise\nalignment of visual goals and effectively bridging autonomous navigation and\ndexterous manipulation. Extensive experimental results demonstrate that HERMES\nconsistently exhibits generalizable behaviors across diverse, in-the-wild\nscenarios, successfully performing numerous complex mobile bimanual dexterous\nmanipulation tasks. Project Page:https:/gemcollector.github.io/HERMES/.", "AI": {"tldr": "HERMES\u662f\u4e00\u4e2a\u5c06\u591a\u6e90\u4eba\u7c7b\u624b\u90e8\u52a8\u4f5c\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u884c\u4e3a\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u3001sim2real\u8f6c\u79fb\u548c\u589e\u5f3a\u5bfc\u822a\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u901a\u7528\u7075\u5de7\u64cd\u4f5c\u3002", "motivation": "\u5c06\u591a\u6e90\u4eba\u7c7b\u624b\u90e8\u52a8\u4f5c\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u884c\u4e3a\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u7684\u591a\u6307\u7075\u5de7\u624b\u673a\u5668\u4eba\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u73af\u5883\u6761\u4ef6\u3002", "method": "HERMES\u91c7\u7528\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u591a\u6e90\u4eba\u7c7b\u624b\u90e8\u52a8\u4f5c\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u6df1\u5ea6\u56fe\u50cf\u7684\u7aef\u5230\u7aefsim2real\u8f6c\u79fb\u65b9\u6cd5\uff0c\u5e76\u589e\u5f3a\u5bfc\u822a\u57fa\u7840\u6a21\u578b\u4ee5\u5b9e\u73b0\u95ed\u73afPnP\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHERMES\u5728\u591a\u6837\u5316\u3001\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u901a\u7528\u6027\uff0c\u6210\u529f\u5b8c\u6210\u591a\u9879\u590d\u6742\u79fb\u52a8\u53cc\u624b\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "HERMES\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3001\u6539\u8fdb\u7684sim2real\u8f6c\u79fb\u65b9\u6cd5\u548c\u589e\u5f3a\u7684\u5bfc\u822a\u57fa\u7840\u6a21\u578b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u591a\u6837\u5316\u3001\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u901a\u7528\u7684\u79fb\u52a8\u53cc\u624b\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2508.19499", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19499", "abs": "https://arxiv.org/abs/2508.19499", "authors": ["Xiangxu Wang", "Tianhong Zhao", "Wei Tu", "Bowen Zhang", "Guanzhou Chen", "Jinzhou Cao"], "title": "Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery", "comment": null, "summary": "Origin-Destination (OD) flow matrices are essential for urban mobility\nanalysis, underpinning applications in traffic forecasting, infrastructure\nplanning, and policy design. However, existing methods suffer from two critical\nlimitations: (1) reliance on auxiliary features (e.g., Points of Interest,\nsocioeconomic statistics) that are costly to collect and have limited spatial\ncoverage; and (2) sensitivity to spatial topology, where minor index reordering\nof urban regions (e.g., census tract relabeling) disrupts structural coherence\nin generated flows. To address these challenges, we propose Sat2Flow, a latent\nstructure-aware diffusion-based framework that generates structurally coherent\nOD flows using solely satellite imagery as input. Our approach introduces a\nmulti-kernel encoder to capture diverse regional interactions and employs a\npermutation-aware diffusion process that aligns latent representations across\ndifferent regional orderings. Through a joint contrastive training objective\nthat bridges satellite-derived features with OD patterns, combined with\nequivariant diffusion training that enforces structural consistency, Sat2Flow\nensures topological robustness under arbitrary regional reindexing.\nExperimental results on real-world urban datasets demonstrate that Sat2Flow\noutperforms both physics-based and data-driven baselines in numerical accuracy\nwhile preserving empirical distributions and spatial structures under index\npermutations. Sat2Flow offers a globally scalable solution for OD flow\ngeneration in data-scarce urban environments, eliminating region-specific\nauxiliary data dependencies while maintaining structural invariance for robust\nmobility modeling.", "AI": {"tldr": "Sat2Flow\u662f\u4e00\u79cd\u4ec5\u4f7f\u7528\u536b\u661f\u56fe\u50cf\u751f\u6210\u7ed3\u6784\u4e00\u81f4\u7684OD\u6d41\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u8f85\u52a9\u6570\u636e\u548c\u7a7a\u95f4\u62d3\u6251\u654f\u611f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684OD\u6d41\u77e9\u9635\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a(1)\u4f9d\u8d56\u6210\u672c\u9ad8\u4e14\u7a7a\u95f4\u8986\u76d6\u6709\u9650\u7684\u8f85\u52a9\u7279\u5f81\uff1b(2)\u5bf9\u7a7a\u95f4\u62d3\u6251\u654f\u611f\uff0c\u8f7b\u5fae\u7684\u533a\u57df\u7d22\u5f15\u91cd\u6392\u5e8f\u4f1a\u7834\u574f\u751f\u6210\u6d41\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u3002Sat2Flow\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "Sat2Flow\u662f\u4e00\u4e2a\u57fa\u4e8e\u6f5c\u5728\u7ed3\u6784\u611f\u77e5\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6838\u7f16\u7801\u5668\u6355\u6349\u591a\u6837\u5316\u7684\u533a\u57df\u4ea4\u4e92\uff0c\u5e76\u91c7\u7528\u6392\u5217\u611f\u77e5\u6269\u6563\u8fc7\u7a0b\u6765\u5bf9\u9f50\u4e0d\u540c\u533a\u57df\u987a\u5e8f\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5728\u771f\u5b9e\u57ce\u5e02\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSat2Flow\u5728\u6570\u503c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u7269\u7406\u548c\u6570\u636e\u9a71\u52a8\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u7d22\u5f15\u6392\u5217\u4e0b\u4fdd\u6301\u7ecf\u9a8c\u5206\u5e03\u548c\u7a7a\u95f4\u7ed3\u6784\u3002", "conclusion": "Sat2Flow\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u7403\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u57ce\u5e02\u73af\u5883\u4e2d\u7684OD\u6d41\u751f\u6210\uff0c\u6d88\u9664\u4e86\u533a\u57df\u7279\u5b9a\u7684\u8f85\u52a9\u6570\u636e\u4f9d\u8d56\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ed3\u6784\u4e0d\u53d8\u6027\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u79fb\u52a8\u6027\u5efa\u6a21\u3002"}}
{"id": "2508.20095", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20095", "abs": "https://arxiv.org/abs/2508.20095", "authors": ["Jinhao Liang", "Sven Koenig", "Ferdinando Fioretto"], "title": "Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning", "comment": null, "summary": "Multi-Robot Motion Planning (MRMP) involves generating collision-free\ntrajectories for multiple robots operating in a shared continuous workspace.\nWhile discrete multi-agent path finding (MAPF) methods are broadly adopted due\nto their scalability, their coarse discretization severely limits trajectory\nquality. In contrast, continuous optimization-based planners offer\nhigher-quality paths but suffer from the curse of dimensionality, resulting in\npoor scalability with respect to the number of robots. This paper tackles the\nlimitations of these two approaches by introducing a novel framework that\nintegrates discrete MAPF solvers with constrained generative diffusion models.\nThe resulting framework, called Discrete-Guided Diffusion (DGD), has three key\ncharacteristics: (1) it decomposes the original nonconvex MRMP problem into\ntractable subproblems with convex configuration spaces, (2) it combines\ndiscrete MAPF solutions with constrained optimization techniques to guide\ndiffusion models capture complex spatiotemporal dependencies among robots, and\n(3) it incorporates a lightweight constraint repair mechanism to ensure\ntrajectory feasibility. The proposed method sets a new state-of-the-art\nperformance in large-scale, complex environments, scaling to 100 robots while\nachieving planning efficiency and high success rates.", "AI": {"tldr": "DGD\u6846\u67b6\u7ed3\u5408\u79bb\u6563MAPF\u548c\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u548c\u9ad8\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u79bb\u6563MAPF\u65b9\u6cd5\u56e0\u7c97\u7c92\u5ea6\u79bb\u6563\u5316\u9650\u5236\u8f68\u8ff9\u8d28\u91cf\uff0c\u800c\u8fde\u7eed\u4f18\u5316\u65b9\u6cd5\u56e0\u7ef4\u5ea6\u707e\u96be\u96be\u4ee5\u6269\u5c55\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u63d0\u51fa\u65b0\u6846\u67b6\u4ee5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u6846\u67b6\u5c06\u539f\u59cb\u975e\u51f8MRMP\u95ee\u9898\u5206\u89e3\u4e3a\u5177\u6709\u51f8\u914d\u7f6e\u7a7a\u95f4\u7684\u5b50\u95ee\u9898\uff0c\u7ed3\u5408\u79bb\u6563MAPF\u89e3\u51b3\u65b9\u6848\u548c\u7ea6\u675f\u4f18\u5316\u6280\u672f\uff0c\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u6355\u6349\u673a\u5668\u4eba\u95f4\u7684\u590d\u6742\u65f6\u7a7a\u4f9d\u8d56\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u7ea6\u675f\u4fee\u590d\u673a\u5236\u786e\u4fdd\u8f68\u8ff9\u53ef\u884c\u6027\u3002", "result": "DGD\u5728\u5927\u578b\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53ef\u6269\u5c55\u81f3100\u4e2a\u673a\u5668\u4eba\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u89c4\u5212\u548c\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Discrete-Guided Diffusion (DGD)\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u79bb\u6563MAPF\u6c42\u89e3\u5668\u548c\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u975e\u51f8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u5927\u89c4\u6a21\u590d\u6742\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u89c4\u5212\u548c\u6210\u529f\u7387\u9ad8\u3002"}}
{"id": "2508.19511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19511", "abs": "https://arxiv.org/abs/2508.19511", "authors": ["Alzayat Saleh", "Shunsuke Hatano", "Mostafa Rahimi Azghadi"], "title": "Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity", "comment": "19 pages, 10 figures, 6 tables", "summary": "The automated management of invasive weeds is critical for sustainable\nagriculture, yet the performance of deep learning models in real-world fields\nis often compromised by two factors: challenging environmental conditions and\nthe high cost of data annotation. This study tackles both issues through a\ndiagnostic-driven, semi-supervised framework. Using a unique dataset of\napproximately 975 labeled and 10,000 unlabeled images of Guinea Grass in\nsugarcane, we first establish strong supervised baselines for classification\n(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and\nmAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by\ninterpretability tools, uncovered a pervasive \"shadow bias,\" where models\nlearned to misidentify shadows as vegetation. This diagnostic insight motivated\nour primary contribution: a semi-supervised pipeline that leverages unlabeled\ndata to enhance model robustness. By training models on a more diverse set of\nvisual information through pseudo-labeling, this framework not only helps\nmitigate the shadow bias but also provides a tangible boost in recall, a\ncritical metric for minimizing weed escapes in automated spraying systems. To\nvalidate our methodology, we demonstrate its effectiveness in a low-data regime\non a public crop-weed benchmark. Our work provides a clear and field-tested\nframework for developing, diagnosing, and improving robust computer vision\nsystems for the complex realities of precision agriculture.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u534a\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u519c\u7530\u4e2d\u9634\u5f71\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u53ec\u56de\u7387\uff0c\u9002\u7528\u4e8e\u7cbe\u51c6\u519c\u4e1a\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u3002", "motivation": "\u81ea\u52a8\u5316\u7ba1\u7406\u5165\u4fb5\u6742\u8349\u5bf9\u53ef\u6301\u7eed\u519c\u4e1a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u771f\u5b9e\u519c\u7530\u4e2d\u7684\u8868\u73b0\u5e38\u53d7\u73af\u5883\u6311\u6218\u548c\u9ad8\u6807\u6ce8\u6210\u672c\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8bca\u65ad\u9a71\u52a8\u7684\u534a\u76d1\u7763\u6846\u67b6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5305\u542b975\u5f20\u6807\u6ce8\u548c10,000\u5f20\u672a\u6807\u6ce8\u56fe\u50cf\u7684\u72ec\u7279\u6570\u636e\u96c6\uff0c\u9996\u5148\u5efa\u7acb\u5f3a\u76d1\u7763\u57fa\u7ebf\uff08ResNet\u5206\u7c7b\u3001YOLO\u548cRF-DETR\u68c0\u6d4b\uff09\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u53d1\u73b0\u2018\u9634\u5f71\u504f\u5dee\u2019\u3002\u968f\u540e\u8bbe\u8ba1\u534a\u76d1\u7763\u6d41\u7a0b\uff0c\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u901a\u8fc7\u4f2a\u6807\u7b7e\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5f3a\u76d1\u7763\u57fa\u7ebfF1\u5206\u6570\u8fbe0.90\uff0cmAP50\u8d85\u8fc70.82\u3002\u534a\u76d1\u7763\u6d41\u7a0b\u901a\u8fc7\u4f2a\u6807\u7b7e\u6709\u6548\u7f13\u89e3\u9634\u5f71\u504f\u5dee\uff0c\u5e76\u663e\u8457\u63d0\u5347\u53ec\u56de\u7387\uff0c\u5728\u4f4e\u6570\u636e\u91cf\u516c\u5171\u4f5c\u7269-\u6742\u8349\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bca\u65ad\u9a71\u52a8\u7684\u534a\u76d1\u7763\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u771f\u5b9e\u519c\u7530\u4e2d\u56e0\u73af\u5883\u6311\u6218\u548c\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u800c\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u5f00\u53d1\u65b9\u6cd5\u3002"}}
{"id": "2508.19527", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19527", "abs": "https://arxiv.org/abs/2508.19527", "authors": ["Zhiting Gao", "Dan Song", "Diqiong Jiang", "Chao Xue", "An-An Liu"], "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment", "comment": "11 pages, 5 figures", "summary": "Motion generation is essential for animating virtual characters and embodied\nagents. While recent text-driven methods have made significant strides, they\noften struggle with achieving precise alignment between linguistic descriptions\nand motion semantics, as well as with the inefficiencies of slow, multi-step\ninference. To address these issues, we introduce TMR++ Aligned Preference\nOptimization (TAPO), an innovative framework that aligns subtle motion\nvariations with textual modifiers and incorporates iterative adjustments to\nreinforce semantic grounding. To further enable real-time synthesis, we propose\nMotionFLUX, a high-speed generation framework based on deterministic rectified\nflow matching. Unlike traditional diffusion models, which require hundreds of\ndenoising steps, MotionFLUX constructs optimal transport paths between noise\ndistributions and motion spaces, facilitating real-time synthesis. The\nlinearized probability paths reduce the need for multi-step sampling typical of\nsequential methods, significantly accelerating inference time without\nsacrificing motion quality. Experimental results demonstrate that, together,\nTAPO and MotionFLUX form a unified system that outperforms state-of-the-art\napproaches in both semantic consistency and motion quality, while also\naccelerating generation speed. The code and pretrained models will be released.", "AI": {"tldr": "TAPO\u548cMotionFLUX\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u8bed\u4e49\u5bf9\u9f50\u548c\u5b9e\u65f6\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u9a71\u52a8\u65b9\u6cd5\u5728\u8bed\u8a00\u63cf\u8ff0\u4e0e\u8fd0\u52a8\u8bed\u4e49\u7cbe\u786e\u5bf9\u9f50\u53ca\u591a\u6b65\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "TAPO\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u8c03\u6574\u5f3a\u5316\u8bed\u4e49\u57fa\u7840\uff0cMotionFLUX\u57fa\u4e8e\u786e\u5b9a\u6027\u6821\u6b63\u6d41\u5339\u914d\uff0c\u6784\u5efa\u566a\u58f0\u5206\u5e03\u4e0e\u8fd0\u52a8\u7a7a\u95f4\u4e4b\u95f4\u7684\u6700\u4f18\u4f20\u8f93\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTAPO\u548cMotionFLUX\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u52a0\u901f\u4e86\u751f\u6210\u901f\u5ea6\u3002", "conclusion": "TAPO\u4e0eMotionFLUX\u6784\u6210\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7cfb\u7edf\uff0c\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u901f\u5ea6\u3002"}}
{"id": "2508.20072", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.20072", "abs": "https://arxiv.org/abs/2508.20072", "authors": ["Zhixuan Liang", "Yizhuo Li", "Tianshuo Yang", "Chengyue Wu", "Sitong Mao", "Liuao Pei", "Xiaokang Yang", "Jiangmiao Pang", "Yao Mu", "Ping Luo"], "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies", "comment": "15 pages", "summary": "Vision-Language-Action (VLA) models adapt large vision-language backbones to\nmap images and instructions to robot actions. However, prevailing VLA decoders\neither generate actions autoregressively in a fixed left-to-right order or\nattach continuous diffusion or flow matching heads outside the backbone,\ndemanding specialized training and iterative sampling that hinder a unified,\nscalable architecture. We present Discrete Diffusion VLA, a single-transformer\npolicy that models discretized action chunks with discrete diffusion and is\ntrained with the same cross-entropy objective as the VLM backbone. The design\nretains diffusion's progressive refinement paradigm while remaining natively\ncompatible with the discrete token interface of VLMs. Our method achieves an\nadaptive decoding order that resolves easy action elements before harder ones\nand uses secondary remasking to revisit uncertain predictions across refinement\nrounds, which improves consistency and enables robust error correction. This\nunified decoder preserves pretrained vision language priors, supports parallel\ndecoding, breaks the autoregressive bottleneck, and reduces the number of\nfunction evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,\n71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv\nBridge, improving over both autoregressive and continuous diffusion baselines.\nThese findings indicate that discrete-diffusion action decoder supports precise\naction modeling and consistent training, laying groundwork for scaling VLA to\nlarger models and datasets.", "AI": {"tldr": "\u79bb\u6563\u6269\u6563VLA\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u89e3\u7801\u5668\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u52a8\u4f5c\u751f\u6210\u6027\u80fd\uff0c\u4f18\u4e8e\u81ea\u56de\u5f52\u548c\u8fde\u7eed\u6269\u6563\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709VLA\u89e3\u7801\u5668\u5728\u751f\u6210\u52a8\u4f5c\u65f6\u56fa\u5b9a\u987a\u5e8f\u6216\u9700\u8981\u4e13\u95e8\u8bad\u7ec3\u548c\u8fed\u4ee3\u91c7\u6837\u7684\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u79bb\u6563\u6269\u6563VLA\uff0c\u4e00\u79cd\u5355\u53d8\u538b\u5668\u7b56\u7565\uff0c\u901a\u8fc7\u79bb\u6563\u6269\u6563\u5efa\u6a21\u79bb\u6563\u5316\u52a8\u4f5c\u5757\uff0c\u5e76\u4f7f\u7528\u4e0eVLM\u4e3b\u5e72\u76f8\u540c\u7684\u4ea4\u53c9\u71b5\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728LIBERO\u4e0a\u8fbe\u523096.3%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u5728SimplerEnv Fractal\u4e0a\u8fbe\u523071.2%\u7684\u89c6\u89c9\u5339\u914d\u7387\uff0c\u5728SimplerEnv Bridge\u4e0a\u8fbe\u523049.3%\u7684\u6574\u4f53\u8868\u73b0\u3002", "conclusion": "\u79bb\u6563\u6269\u6563VLA\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u89e3\u7801\u5668\u8bbe\u8ba1\uff0c\u652f\u6301\u7cbe\u786e\u7684\u52a8\u4f5c\u5efa\u6a21\u548c\u4e00\u81f4\u7684\u8bad\u7ec3\uff0c\u4e3a\u6269\u5c55\u5230\u66f4\u5927\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.19542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19542", "abs": "https://arxiv.org/abs/2508.19542", "authors": ["Nannan Zhu", "Yonghao Dong", "Teng Wang", "Xueqian Li", "Shengjun Deng", "Yijia Wang", "Zheng Hong", "Tiantian Geng", "Guo Niu", "Hanyan Huang", "Xiongfei Yao", "Shuaiwei Jiao"], "title": "CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning", "comment": null, "summary": "While multimodal large language models (MLLMs) exhibit strong performance on\nsingle-video tasks (e.g., video question answering), their ability across\nmultiple videos remains critically underexplored. However, this capability is\nessential for real-world applications, including multi-camera surveillance and\ncross-video procedural learning. To bridge this gap, we present CVBench, the\nfirst comprehensive benchmark designed to assess cross-video relational\nreasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning\nthree hierarchical tiers: cross-video object association (identifying shared\nentities), cross-video event association (linking temporal or causal event\nchains), and cross-video complex reasoning (integrating commonsense and domain\nknowledge). Built from five domain-diverse video clusters (e.g., sports, life\nrecords), the benchmark challenges models to synthesise information across\ndynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including\nGPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought\nprompting paradigms. Key findings reveal stark performance gaps: even top\nmodels, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,\ncompared to the 91% accuracy of human performance. Crucially, our analysis\nreveals fundamental bottlenecks inherent in current MLLM architectures, notably\ndeficient inter-video context retention and poor disambiguation of overlapping\nentities. CVBench establishes a rigorous framework for diagnosing and advancing\nmulti-video reasoning, offering architectural insights for next-generation\nMLLMs.The data and evaluation code are available at\nhttps://github.com/Hokhim2/CVBench.", "AI": {"tldr": "CVBench\u662f\u9996\u4e2a\u8bc4\u4f30\u8de8\u89c6\u9891\u5173\u7cfb\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u5f53\u524dMLLM\u5728\u591a\u89c6\u9891\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u4e0a\u4e0b\u6587\u4fdd\u7559\u548c\u5b9e\u4f53\u6d88\u6b67\u7684\u4e0d\u8db3\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u5355\u89c6\u9891\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u89c6\u9891\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u8fd9\u79cd\u80fd\u529b\u5bf9\u5b9e\u9645\u5e94\u7528\uff08\u5982\u591a\u6444\u50cf\u5934\u76d1\u63a7\u548c\u8de8\u89c6\u9891\u7a0b\u5e8f\u5b66\u4e60\uff09\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6784\u5efaCVBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,000\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u5206\u4e3a\u4e09\u4e2a\u5c42\u6b21\uff1a\u8de8\u89c6\u9891\u5bf9\u8c61\u5173\u8054\u3001\u8de8\u89c6\u9891\u4e8b\u4ef6\u5173\u8054\u548c\u8de8\u89c6\u9891\u590d\u6742\u63a8\u7406\u3002\u8bc4\u4f30\u4e8610\u591a\u4e2a\u9886\u5148\u7684MLLM\u6a21\u578b\uff0c\u5305\u62ecGPT-4o\u3001Gemini-2.0-flash\u548cQwen2.5-VL\uff0c\u91c7\u7528\u96f6\u6837\u672c\u6216\u601d\u7ef4\u94fe\u63d0\u793a\u8303\u5f0f\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u9876\u7ea7\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5728\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e0a\u4ec5\u8fbe\u523060%\u7684\u51c6\u786e\u7387\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b91%\u7684\u8868\u73b0\u3002", "conclusion": "CVBench \u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6846\u67b6\u6765\u8bca\u65ad\u548c\u63a8\u8fdb\u591a\u89c6\u9891\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524dMLLM\u67b6\u6784\u5728\u8de8\u89c6\u9891\u4e0a\u4e0b\u6587\u4fdd\u7559\u548c\u5b9e\u4f53\u6d88\u6b67\u65b9\u9762\u7684\u6839\u672c\u74f6\u9888\u3002"}}
{"id": "2508.19544", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19544", "abs": "https://arxiv.org/abs/2508.19544", "authors": ["Eduardo Davalos", "Yike Zhang", "Namrata Srivastava", "Yashvitha Thatigotla", "Jorge A. Salas", "Sara McFadden", "Sun-Joo Cho", "Amanda Goodwin", "Ashwin TS", "Gautam Biswas"], "title": "WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization", "comment": "9 pages, 7 figures, 1 table", "summary": "With advancements in AI, new gaze estimation methods are exceeding\nstate-of-the-art (SOTA) benchmarks, but their real-world application reveals a\ngap with commercial eye-tracking solutions. Factors like model size, inference\ntime, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking\nmethods lack sufficient accuracy, in particular due to head movement. To tackle\nthese issues, we introduce We bEyeTrack, a framework that integrates\nlightweight SOTA gaze estimation models directly in the browser. It\nincorporates model-based head pose estimation and on-device few-shot learning\nwith as few as nine calibration samples (k < 9). WebEyeTrack adapts to new\nusers, achieving SOTA performance with an error margin of 2.32 cm on\nGazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.\nOur open-source code is available at\nhttps://github.com/RedForestAi/WebEyeTrack.", "AI": {"tldr": "WebEyeTrack\u662f\u4e00\u4e2a\u6d4f\u89c8\u5668\u96c6\u6210\u7684\u8f7b\u91cf\u7ea7\u6ce8\u89c6\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5934\u90e8\u59ff\u6001\u4f30\u8ba1\u548c\u5c11\u91cf\u6837\u672c\u5728\u7ebf\u5b66\u4e60\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u5f53\u524dAI\u6ce8\u89c6\u4f30\u8ba1\u65b9\u6cd5\u867d\u5728SOTA\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u4e0e\u5546\u4e1a\u773c\u52a8\u8ffd\u8e2a\u65b9\u6848\u7684\u5dee\u8ddd\uff0c\u4e14\u6a21\u578b\u5927\u5c0f\u3001\u63a8\u7406\u65f6\u95f4\u548c\u9690\u79c1\u95ee\u9898\u5e38\u88ab\u5ffd\u89c6\uff1b\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u773c\u52a8\u8ffd\u8e2a\u65b9\u6cd5\u56e0\u5934\u90e8\u8fd0\u52a8\u5bfc\u81f4\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51faWebEyeTrack\u6846\u67b6\uff0c\u96c6\u6210\u8f7b\u91cf\u7ea7SOTA\u6ce8\u89c6\u4f30\u8ba1\u6a21\u578b\uff0c\u7ed3\u5408\u6a21\u578b\u9a71\u52a8\u7684\u5934\u90e8\u59ff\u6001\u4f30\u8ba1\u548c\u4ec5\u97009\u4e2a\u6821\u51c6\u6837\u672c\u7684\u5728\u7ebf\u5b66\u4e60\u3002", "result": "\u5728GazeCapture\u6570\u636e\u96c6\u4e0a\u8fbe\u52302.32\u5398\u7c73\u7684\u8bef\u5dee\uff0ciPhone 14\u4e0a\u5b9e\u73b02.4\u6beb\u79d2\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "WebEyeTrack\u901a\u8fc7\u6574\u5408\u8f7b\u91cf\u7ea7SOTA\u6ce8\u89c6\u4f30\u8ba1\u6a21\u578b\uff0c\u7ed3\u5408\u5934\u90e8\u59ff\u6001\u4f30\u8ba1\u548c\u5c11\u91cf\u6837\u672c\u7684\u5728\u7ebf\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u6ce8\u89c6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19555", "abs": "https://arxiv.org/abs/2508.19555", "authors": ["Yu-Wei Zhang", "Tongju Han", "Lipeng Gao", "Mingqiang Wei", "Hui Liu", "Changbao Li", "Caiming Zhang"], "title": "MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery", "comment": null, "summary": "This paper presents MonoRelief V2, an end-to-end model designed for directly\nrecovering 2.5D reliefs from single images under complex material and\nillumination variations. In contrast to its predecessor, MonoRelief V1 [1],\nwhich was solely trained on synthetic data, MonoRelief V2 incorporates real\ndata to achieve improved robustness, accuracy and efficiency. To overcome the\nchallenge of acquiring large-scale real-world dataset, we generate\napproximately 15,000 pseudo real images using a text-to-image generative model,\nand derive corresponding depth pseudo-labels through fusion of depth and normal\npredictions. Furthermore, we construct a small-scale real-world dataset (800\nsamples) via multi-view reconstruction and detail refinement. MonoRelief V2 is\nthen progressively trained on the pseudo-real and real-world datasets.\nComprehensive experiments demonstrate its state-of-the-art performance both in\ndepth and normal predictions, highlighting its strong potential for a range of\ndownstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.", "AI": {"tldr": "MonoRelief V2\u901a\u8fc7\u7ed3\u5408\u4f2a\u771f\u5b9e\u548c\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u5355\u56fe\u50cf2.5D\u6d6e\u96d5\u6062\u590d\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3MonoRelief V1\u4ec5\u4f9d\u8d56\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u6750\u6599\u548c\u5149\u7167\u53d8\u5316\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u751f\u6210\u7ea615,000\u5f20\u4f2a\u771f\u5b9e\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u548c\u6cd5\u7ebf\u9884\u6d4b\u878d\u5408\u751f\u6210\u4f2a\u6807\u7b7e\uff1b\u540c\u65f6\u6784\u5efa800\u4e2a\u6837\u672c\u7684\u5c0f\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u3002\u6a21\u578b\u5728\u4f2a\u771f\u5b9e\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9010\u6b65\u8bad\u7ec3\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cMonoRelief V2\u5728\u6df1\u5ea6\u548c\u6cd5\u7ebf\u9884\u6d4b\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "MonoRelief V2\u901a\u8fc7\u7ed3\u5408\u4f2a\u771f\u5b9e\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ece\u5355\u56fe\u50cf\u6062\u590d2.5D\u6d6e\u96d5\u7684\u9c81\u68d2\u6027\u3001\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5c55\u73b0\u4e86\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.19565", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10; I.5.1"], "pdf": "https://arxiv.org/pdf/2508.19565", "abs": "https://arxiv.org/abs/2508.19565", "authors": ["Yuhang Zhao", "Zixing Wang"], "title": "FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection", "comment": "Accepted by PRCV 2025. Project page with code and dataset:\n  https://github.com/AstronZh/Intersection-Flow-5K", "summary": "End-to-end object detectors offer a promising NMS-free paradigm for real-time\napplications, yet their high computational cost remains a significant barrier,\nparticularly for complex scenarios like intersection traffic monitoring. To\naddress this challenge, we propose FlowDet, a high-speed detector featuring a\ndecoupled encoder optimization strategy applied to the DETR architecture.\nSpecifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for\ntraffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to\nmaintain high representational power across extreme scale variations. To\nrigorously evaluate the model's performance in environments with severe\nocclusion and high object density, we collected the Intersection-Flow-5k\ndataset, a new challenging scene for this task. Evaluated on\nIntersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to\nthe strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by\n1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference\nspeed by 16.2%. Our work demonstrates a new path towards building highly\nefficient and accurate detectors for demanding, real-world perception systems.\nThe Intersection-Flow-5k dataset is available at\nhttps://github.com/AstronZh/Intersection-Flow-5K.", "AI": {"tldr": "FlowDet\u901a\u8fc7GDU\u548cSAA\u6a21\u5757\u4f18\u5316DETR\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4ea4\u901a\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u540c\u65f6\u53d1\u5e03\u4e86Intersection-Flow-5k\u6570\u636e\u96c6\u3002", "motivation": "\u7aef\u5230\u7aef\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9ad8\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u573a\u666f\uff08\u5982\u4ea4\u53c9\u8def\u53e3\u4ea4\u901a\u76d1\u63a7\uff09\u4e2d\u7684\u5e94\u7528\u3002", "method": "FlowDet\u91c7\u7528\u4e86\u89e3\u8026\u7f16\u7801\u5668\u4f18\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u65b0\u9896\u7684Geometric Deformable Unit (GDU)\u8fdb\u884c\u4ea4\u901a\u611f\u77e5\u51e0\u4f55\u5efa\u6a21\uff0c\u4ee5\u53caScale-Aware Attention (SAA)\u6a21\u5757\u5904\u7406\u6781\u7aef\u5c3a\u5ea6\u53d8\u5316\u3002", "result": "\u5728Intersection-Flow-5k\u6570\u636e\u96c6\u4e0a\uff0cFlowDet\u5728AP(test)\u548cAP50(test)\u4e0a\u5206\u522b\u63d0\u5347\u4e861.5%\u548c1.6%\uff0c\u540c\u65f6\u51cf\u5c11\u4e8663.2%\u7684GFLOPs\u5e76\u52a0\u5feb\u4e8616.2%\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "FlowDet\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u51c6\u786e\u7684\u7aef\u5230\u7aef\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9ad8\u8981\u6c42\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002Intersection-Flow-5k\u6570\u636e\u96c6\u7684\u53d1\u5e03\u8fdb\u4e00\u6b65\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2508.19573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19573", "abs": "https://arxiv.org/abs/2508.19573", "authors": ["Luhu Li", "Bowen Lin", "Mukhtiar Khan", "Shujun Fu"], "title": "DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection", "comment": null, "summary": "Anomaly detection in medical images is challenging due to limited annotations\nand a domain gap compared to natural images. Existing reconstruction methods\noften rely on frozen pre-trained encoders, which limits adaptation to\ndomain-specific features and reduces localization accuracy. Prototype-based\nlearning offers interpretability and clustering benefits but suffers from\nprototype collapse, where few prototypes dominate training, harming diversity\nand generalization. To address this, we propose a unified framework combining a\ntrainable encoder with prototype-guided reconstruction and a novel\nDiversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum\nbranch, enables stable domain-adaptive feature learning. A lightweight\nPrototype Extractor mines informative normal prototypes to guide the decoder\nvia attention for precise reconstruction. Our loss enforces balanced prototype\nuse through diversity constraints and per-prototype normalization, effectively\npreventing collapse. Experiments on multiple medical imaging benchmarks show\nsignificant improvements in representation quality and anomaly localization,\noutperforming prior methods. Visualizations and prototype assignment analyses\nfurther validate the effectiveness of our anti-collapse mechanism and enhanced\ninterpretability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53ef\u8bad\u7ec3\u7f16\u7801\u5668\u548c\u539f\u578b\u5f15\u5bfc\u91cd\u5efa\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5f15\u5165\u591a\u6837\u6027\u611f\u77e5\u5bf9\u9f50\u635f\u5931\u4ee5\u89e3\u51b3\u539f\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u6ce8\u91ca\u6709\u9650\u548c\u4e0e\u81ea\u7136\u56fe\u50cf\u7684\u9886\u57df\u5dee\u8ddd\u95ee\u9898\u3002\u73b0\u6709\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u9650\u5236\u4e86\u9886\u57df\u7279\u5b9a\u7279\u5f81\u7684\u9002\u5e94\u6027\u548c\u5b9a\u4f4d\u51c6\u786e\u6027\u3002\u539f\u578b\u5b66\u4e60\u867d\u5177\u89e3\u91ca\u6027\u548c\u805a\u7c7b\u4f18\u52bf\uff0c\u4f46\u5b58\u5728\u539f\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u5f71\u54cd\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53ef\u8bad\u7ec3\u7f16\u7801\u5668\u4e0e\u539f\u578b\u5f15\u5bfc\u91cd\u5efa\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u591a\u6837\u6027\u611f\u77e5\u5bf9\u9f50\u635f\u5931\u3002\u53ef\u8bad\u7ec3\u7f16\u7801\u5668\u901a\u8fc7\u52a8\u91cf\u5206\u652f\u589e\u5f3a\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u9886\u57df\u81ea\u9002\u5e94\u7279\u5f81\u5b66\u4e60\u3002\u8f7b\u91cf\u7ea7\u539f\u578b\u63d0\u53d6\u5668\u6316\u6398\u4fe1\u606f\u4e30\u5bcc\u7684\u6b63\u5e38\u539f\u578b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5f15\u5bfc\u89e3\u7801\u5668\u8fdb\u884c\u7cbe\u786e\u91cd\u5efa\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8bba\u6587\u65b9\u6cd5\u5728\u8868\u793a\u8d28\u91cf\u548c\u5f02\u5e38\u5b9a\u4f4d\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u53ef\u89c6\u5316\u548c\u539f\u578b\u5206\u914d\u5206\u6790\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6297\u5d29\u6e83\u673a\u5236\u7684\u6709\u6548\u6027\u548c\u589e\u5f3a\u7684\u89e3\u91ca\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u901a\u8fc7\u53ef\u8bad\u7ec3\u7f16\u7801\u5668\u3001\u539f\u578b\u5f15\u5bfc\u91cd\u5efa\u548c\u591a\u6837\u6027\u611f\u77e5\u5bf9\u9f50\u635f\u5931\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u7684\u8868\u73b0\u548c\u5b9a\u4f4d\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u548c\u539f\u578b\u5206\u914d\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u6297\u5d29\u6e83\u673a\u5236\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.19574", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19574", "abs": "https://arxiv.org/abs/2508.19574", "authors": ["Mingxi Fu", "Fanglei Fu", "Xitong Ling", "Huaitian Yuan", "Tian Guan", "Yonghong He", "Lianghui Zhu"], "title": "Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation", "comment": null, "summary": "Pathological image segmentation faces numerous challenges, particularly due\nto ambiguous semantic boundaries and the high cost of pixel-level annotations.\nAlthough recent semi-supervised methods based on consistency regularization\n(e.g., UniMatch) have made notable progress, they mainly rely on\nperturbation-based consistency within the image modality, making it difficult\nto capture high-level semantic priors, especially in structurally complex\npathology images. To address these limitations, we propose MPAMatch - a novel\nsegmentation framework that performs pixel-level contrastive learning under a\nmultimodal prototype-guided supervision paradigm. The core innovation of\nMPAMatch lies in the dual contrastive learning scheme between image prototypes\nand pixel labels, and between text prototypes and pixel labels, providing\nsupervision at both structural and semantic levels. This coarse-to-fine\nsupervisory strategy not only enhances the discriminative capability on\nunlabeled samples but also introduces the text prototype supervision into\nsegmentation for the first time, significantly improving semantic boundary\nmodeling. In addition, we reconstruct the classic segmentation architecture\n(TransUNet) by replacing its ViT backbone with a pathology-pretrained\nfoundation model (Uni), enabling more effective extraction of\npathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,\nEBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art\nmethods, validating its dual advantages in structural and semantic modeling.", "AI": {"tldr": "MPAMatch\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u75c5\u7406\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u8bed\u4e49\u8fb9\u754c\u5efa\u6a21\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u75c5\u7406\u56fe\u50cf\u5206\u5272\u4e2d\u8bed\u4e49\u8fb9\u754c\u6a21\u7cca\u548c\u50cf\u7d20\u7ea7\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u9ad8\u7ea7\u8bed\u4e49\u5148\u9a8c\u3002", "method": "MPAMatch\u91c7\u7528\u591a\u6a21\u6001\u539f\u578b\u5f15\u5bfc\u7684\u76d1\u7763\u8303\u5f0f\uff0c\u7ed3\u5408\u56fe\u50cf\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u3001\u6587\u672c\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u7684\u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u91cd\u6784\u4e86\u7ecf\u5178\u7684TransUNet\u67b6\u6784\uff0c\u4f7f\u7528\u75c5\u7406\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578bUni\u3002", "result": "\u5728GLAS\u3001EBHI-SEG-GLAND\u3001EBHI-SEG-CANCER\u548cKPI\u6570\u636e\u96c6\u4e0a\uff0cMPAMatch\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "MPAMatch\u901a\u8fc7\u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6848\u5728\u7ed3\u6784\u548c\u8bed\u4e49\u5c42\u9762\u4e0a\u63d0\u4f9b\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u4e86\u75c5\u7406\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.19575", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19575", "abs": "https://arxiv.org/abs/2508.19575", "authors": ["Zhu Xu", "Zhaowen Wang", "Yuxin Peng", "Yang Liu"], "title": "Interact-Custom: Customized Human Object Interaction Image Generation", "comment": null, "summary": "Compositional Customized Image Generation aims to customize multiple target\nconcepts within generation content, which has gained attention for its wild\napplication.Existing approaches mainly concentrate on the target entity's\nappearance preservation, while neglecting the fine-grained interaction control\namong target entities.To enable the model of such interaction control\ncapability, we focus on human object interaction scenario and propose the task\nof Customized Human Object Interaction Image Generation(CHOI), which\nsimultaneously requires identity preservation for target human object and the\ninteraction semantic control between them.Two primary challenges exist for\nCHOI:(1)simultaneous identity preservation and interaction control demands\nrequire the model to decompose the human object into self-contained identity\nfeatures and pose-oriented interaction features, while the current HOI image\ndatasets fail to provide ideal samples for such feature-decomposed\nlearning.(2)inappropriate spatial configuration between human and object may\nlead to the lack of desired interaction semantics.To tackle it, we first\nprocess a large-scale dataset, where each sample encompasses the same pair of\nhuman object involving different interactive poses.Then we design a two-stage\nmodel Interact-Custom, which firstly explicitly models the spatial\nconfiguration by generating a foreground mask depicting the interaction\nbehavior, then under the guidance of this mask, we generate the target human\nobject interacting while preserving their identities features.Furthermore, if\nthe background image and the union location of where the target human object\nshould appear are provided by users, Interact-Custom also provides the optional\nfunctionality to specify them, offering high content controllability. Extensive\nexperiments on our tailored metrics for CHOI task demonstrate the effectiveness\nof our approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5b9a\u5236\u5316\u4eba\u673a\u4ea4\u4e92\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff08CHOI\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e24\u9636\u6bb5\u6a21\u578bInteract-Custom\uff0c\u901a\u8fc7\u524d\u666f\u63a9\u7801\u548c\u8eab\u4efd\u7279\u5f81\u4fdd\u6301\uff0c\u5b9e\u73b0\u4e86\u4ea4\u4e92\u8bed\u4e49\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u76ee\u6807\u5b9e\u4f53\u7684\u5916\u89c2\u4fdd\u6301\uff0c\u5ffd\u89c6\u4e86\u7ec6\u7c92\u5ea6\u7684\u4ea4\u4e92\u63a7\u5236\uff0c\u56e0\u6b64\u63d0\u51fa\u5b9a\u5236\u5316\u4eba\u673a\u4ea4\u4e92\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff08CHOI\uff09\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6a21\u578bInteract-Custom\uff0c\u9996\u5148\u751f\u6210\u63cf\u8ff0\u4ea4\u4e92\u884c\u4e3a\u7684\u524d\u666f\u63a9\u7801\u4ee5\u660e\u786e\u7a7a\u95f4\u914d\u7f6e\uff0c\u7136\u540e\u5728\u63a9\u7801\u6307\u5bfc\u4e0b\u751f\u6210\u76ee\u6807\u4eba\u673a\u4ea4\u4e92\u56fe\u50cf\u5e76\u4fdd\u6301\u8eab\u4efd\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cInteract-Custom\u5728CHOI\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u8eab\u4efd\u4fdd\u6301\u548c\u4ea4\u4e92\u8bed\u4e49\u63a7\u5236\u3002", "conclusion": "Interact-Custom\u6a21\u578b\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b9a\u5236\u5316\u4eba\u673a\u4ea4\u4e92\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4fdd\u6301\u548c\u4ea4\u4e92\u8bed\u4e49\u63a7\u5236\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.19579", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19579", "abs": "https://arxiv.org/abs/2508.19579", "authors": ["Haomiao Zhang", "Miao Cao", "Xuan Yu", "Hui Luo", "Yanling Piao", "Mengjie Qin", "Zhangyuan Li", "Ping Wang", "Xin Yuan"], "title": "High-Speed FHD Full-Color Video Computer-Generated Holography", "comment": null, "summary": "Computer-generated holography (CGH) is a promising technology for\nnext-generation displays. However, generating high-speed, high-quality\nholographic video requires both high frame rate display and efficient\ncomputation, but is constrained by two key limitations: ($i$) Learning-based\nmodels often produce over-smoothed phases with narrow angular spectra, causing\nsevere color crosstalk in high frame rate full-color displays such as\ndepth-division multiplexing and thus resulting in a trade-off between frame\nrate and color fidelity. ($ii$) Existing frame-by-frame optimization methods\ntypically optimize frames independently, neglecting spatial-temporal\ncorrelations between consecutive frames and leading to computationally\ninefficient solutions. To overcome these challenges, in this paper, we propose\na novel high-speed full-color video CGH generation scheme. First, we introduce\nSpectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase\ndistributions via frequency modulation, enabling high-fidelity full-color\ndisplay at high frame rates. Second, we present HoloMamba, a lightweight\nasymmetric Mamba-Unet architecture that explicitly models spatial-temporal\ncorrelations across video sequences to enhance reconstruction quality and\ncomputational efficiency. Extensive simulated and real-world experiments\ndemonstrate that SGDDM achieves high-fidelity full-color display without\ncompromise in frame rate, while HoloMamba generates FHD (1080p) full-color\nholographic video at over 260 FPS, more than 2.6$\\times$ faster than the prior\nstate-of-the-art Divide-Conquer-and-Merge Strategy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSGDDM\u548cHoloMamba\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u5168\u606f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u9ad8\u5e27\u7387\u4e0e\u8272\u5f69\u4fdd\u771f\u5ea6\u95ee\u9898\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u5b66\u4e60\u6a21\u578b\u548c\u5e27\u95f4\u4f18\u5316\u65b9\u6cd5\u5728\u9ad8\u901f\u5168\u5f69\u5168\u606f\u89c6\u9891\u751f\u6210\u4e2d\u5b58\u5728\u8272\u5f69\u4e32\u6270\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9996\u5148\uff0c\u901a\u8fc7SGDDM\u6280\u672f\u4f18\u5316\u76f8\u4f4d\u5206\u5e03\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5168\u5f69\u663e\u793a\uff1b\u5176\u6b21\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u4e0d\u5bf9\u79f0Mamba-Unet\u67b6\u6784HoloMamba\uff0c\u663e\u5f0f\u5efa\u6a21\u65f6\u7a7a\u76f8\u5173\u6027\u4ee5\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSGDDM\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5168\u5f69\u663e\u793a\u800c\u4e0d\u727a\u7272\u5e27\u7387\uff0cHoloMamba\u80fd\u4ee5\u8d85\u8fc7260 FPS\u7684\u901f\u5ea6\u751f\u62101080p\u5168\u5f69\u5168\u606f\u89c6\u9891\uff0c\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u5feb2.6\u500d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684SGDDM\u548cHoloMamba\u65b9\u6848\u6210\u529f\u89e3\u51b3\u4e86\u8ba1\u7b97\u673a\u751f\u6210\u5168\u606f\u89c6\u9891\u4e2d\u7684\u9ad8\u5e27\u7387\u4e0e\u8272\u5f69\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5e76\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.19581", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19581", "abs": "https://arxiv.org/abs/2508.19581", "authors": ["Dat Nguyen Cong", "Hieu Tran Bao", "Hoang Thanh-Tung"], "title": "Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction", "comment": "21 pages, 16 figures", "summary": "Diffusion models have gained prominence as state-of-the-art techniques for\nsynthesizing images and videos, particularly due to their ability to scale\neffectively with large datasets. Recent studies have uncovered that these\nextensive datasets often contain mistakes from manual labeling processes.\nHowever, the extent to which such errors compromise the generative capabilities\nand controllability of diffusion models is not well studied. This paper\nintroduces Score-based Discriminator Correction (SBDC), a guidance technique\nfor aligning noisy pre-trained conditional diffusion models. The guidance is\nbuilt on discriminator training using adversarial loss, drawing on prior noise\ndetection techniques to assess the authenticity of each sample. We further show\nthat limiting the usage of our guidance to the early phase of the generation\nprocess leads to better performance. Our method is computationally efficient,\nonly marginally increases inference time, and does not require retraining\ndiffusion models. Experiments on different noise settings demonstrate the\nsuperiority of our method over previous state-of-the-art methods.", "AI": {"tldr": "SBDC\u901a\u8fc7\u5bf9\u6297\u6027\u635f\u5931\u8bad\u7ec3\u7684\u5224\u522b\u5668\u6307\u5bfc\u6269\u6563\u6a21\u578b\uff0c\u63d0\u5347\u751f\u6210\u80fd\u529b\u548c\u53ef\u63a7\u6027\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6570\u636e\u96c6\u4e2d\u7684\u6807\u7b7e\u9519\u8bef\u5bf9\u751f\u6210\u80fd\u529b\u548c\u53ef\u63a7\u6027\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86SBDC\uff0c\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6297\u6027\u635f\u5931\u8bad\u7ec3\u7684\u5224\u522b\u5668\u6307\u5bfc\u6280\u672f\uff0c\u5229\u7528\u5148\u524d\u7684\u566a\u58f0\u68c0\u6d4b\u6280\u672f\u8bc4\u4f30\u6837\u672c\u7684\u771f\u5b9e\u6027\uff0c\u5e76\u9650\u5236\u6307\u5bfc\u4f7f\u7528\u4e8e\u751f\u6210\u8fc7\u7a0b\u7684\u65e9\u671f\u9636\u6bb5\u3002", "result": "\u5728\u4e0d\u540c\u566a\u58f0\u8bbe\u7f6e\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSBDC\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Score-based Discriminator Correction\uff08SBDC\uff09\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6297\u6027\u635f\u5931\u8bad\u7ec3\u7684\u5224\u522b\u5668\u6307\u5bfc\u9884\u8bad\u7ec3\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u751f\u6210\u80fd\u529b\u548c\u53ef\u63a7\u6027\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002"}}
{"id": "2508.19593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19593", "abs": "https://arxiv.org/abs/2508.19593", "authors": ["Abhinav Kumar"], "title": "Generalizing Monocular 3D Object Detection", "comment": "PhD Thesis submitted to MSU", "summary": "Monocular 3D object detection (Mono3D) is a fundamental computer vision task\nthat estimates an object's class, 3D position, dimensions, and orientation from\na single image. Its applications, including autonomous driving, augmented\nreality, and robotics, critically rely on accurate 3D environmental\nunderstanding. This thesis addresses the challenge of generalizing Mono3D\nmodels to diverse scenarios, including occlusions, datasets, object sizes, and\ncamera parameters. To enhance occlusion robustness, we propose a mathematically\ndifferentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we\nexplore depth equivariant (DEVIANT) backbones. We address the issue of large\nobject detection, demonstrating that it's not solely a data imbalance or\nreceptive field problem but also a noise sensitivity issue. To mitigate this,\nwe introduce a segmentation-based approach in bird's-eye view with dice loss\n(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D\nmodels to unseen camera heights and improve Mono3D generalization in such\nout-of-distribution settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u56db\u79cd\u65b9\u6cd5\u63d0\u5347\u5355\u76ee3D\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8986\u76d6\u906e\u6321\u3001\u6570\u636e\u96c6\u3001\u5927\u7269\u4f53\u53ca\u6444\u50cf\u673a\u53c2\u6570\u53d8\u5316\u7b49\u573a\u666f\u3002", "motivation": "\u5355\u76ee3D\u7269\u4f53\u68c0\u6d4b\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u589e\u5f3a\u73b0\u5b9e\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u906e\u6321\u3001\u6570\u636e\u96c6\u591a\u6837\u6027\u3001\u5927\u7269\u4f53\u68c0\u6d4b\u53ca\u6444\u50cf\u673a\u53c2\u6570\u53d8\u5316\u7b49\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u56db\u79cd\u65b9\u6cd5\uff1a1. \u53ef\u5fae\u5206\u7684NMS\uff08GrooMeD-NMS\uff09\u589e\u5f3a\u906e\u6321\u9c81\u68d2\u6027\uff1b2. \u6df1\u5ea6\u7b49\u53d8\u9aa8\u5e72\u7f51\u7edc\uff08DEVIANT\uff09\u63d0\u5347\u6570\u636e\u96c6\u6cdb\u5316\u6027\uff1b3. \u57fa\u4e8e\u9e1f\u77b0\u56fe\u7684\u5206\u5272\u65b9\u6cd5\uff08SeaBird\uff09\u89e3\u51b3\u5927\u7269\u4f53\u68c0\u6d4b\u7684\u566a\u58f0\u654f\u611f\u95ee\u9898\uff1b4. \u6444\u50cf\u673a\u9ad8\u5ea6\u5916\u63a8\u7684\u6570\u5b66\u5206\u6790\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u906e\u6321\u9c81\u68d2\u6027\u3001\u6570\u636e\u96c6\u6cdb\u5316\u3001\u5927\u7269\u4f53\u68c0\u6d4b\u53ca\u6444\u50cf\u673a\u9ad8\u5ea6\u5916\u63a8\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u672c\u8bba\u6587\u901a\u8fc7\u63d0\u51faGrooMeD-NMS\u3001DEVIANT\u9aa8\u5e72\u7f51\u7edc\u3001SeaBird\u5206\u5272\u65b9\u6cd5\u53ca\u6444\u50cf\u673a\u9ad8\u5ea6\u5916\u63a8\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.19600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19600", "abs": "https://arxiv.org/abs/2508.19600", "authors": ["Toghrul Karimov", "Hassan Imani", "Allan Kazakov"], "title": "Quantization Robustness to Input Degradations for Object Detection", "comment": null, "summary": "Post-training quantization (PTQ) is crucial for deploying efficient object\ndetection models, like YOLO, on resource-constrained devices. However, the\nimpact of reduced precision on model robustness to real-world input\ndegradations such as noise, blur, and compression artifacts is a significant\nconcern. This paper presents a comprehensive empirical study evaluating the\nrobustness of YOLO models (nano to extra-large scales) across multiple\nprecision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8\n(TensorRT). We introduce and evaluate a degradation-aware calibration strategy\nfor Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix\nof clean and synthetically degraded images. Models were benchmarked on the COCO\ndataset under seven distinct degradation conditions (including various types\nand levels of noise, blur, low contrast, and JPEG compression) and a\nmixed-degradation scenario. Results indicate that while Static INT8 TensorRT\nengines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop\n(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did\nnot yield consistent, broad improvements in robustness over standard clean-data\ncalibration across most models and degradations. A notable exception was\nobserved for larger model scales under specific noise conditions, suggesting\nmodel capacity may influence the efficacy of this calibration approach. These\nfindings highlight the challenges in enhancing PTQ robustness and provide\ninsights for deploying quantized detectors in uncontrolled environments. All\ncode and evaluation tables are available at https://github.com/AllanK24/QRID.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86YOLO\u6a21\u578b\u5728\u4e0d\u540c\u91cf\u5316\u7cbe\u5ea6\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9000\u5316\u611f\u77e5\u6821\u51c6\u7b56\u7565\uff0c\u4f46\u53d1\u73b0\u5176\u5bf9\u5927\u591a\u6570\u6a21\u578b\u548c\u9000\u5316\u6761\u4ef6\u6548\u679c\u6709\u9650\uff0c\u4ec5\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u6709\u6548\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u91cf\u5316\u540e\u8bad\u7ec3\uff08PTQ\uff09\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u771f\u5b9e\u4e16\u754c\u8f93\u5165\u9000\u5316\uff08\u5982\u566a\u58f0\u3001\u6a21\u7cca\u548c\u538b\u7f29\u4f2a\u5f71\uff09\u65f6\u7684\u8868\u73b0\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8bc4\u4f30\u4e86YOLO\u6a21\u578b\uff08\u4ecenano\u5230extra-large\u89c4\u6a21\uff09\u5728\u591a\u79cd\u7cbe\u5ea6\u683c\u5f0f\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u9000\u5316\u611f\u77e5\u6821\u51c6\u7b56\u7565\u7528\u4e8e\u9759\u6001INT8 PTQ\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u9000\u5316\u611f\u77e5\u6821\u51c6\u4ec5\u5728\u7279\u5b9a\u566a\u58f0\u6761\u4ef6\u4e0b\u5bf9\u8f83\u5927\u89c4\u6a21\u6a21\u578b\u6709\u6548\uff0c\u6697\u793a\u6a21\u578b\u5bb9\u91cf\u53ef\u80fd\u5f71\u54cd\u8be5\u6821\u51c6\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u9759\u6001INT8 TensorRT\u5f15\u64ce\u5728\u5e72\u51c0\u6570\u636e\u4e0a\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u52a0\u901f\uff08\u7ea61.5-3.3\u500d\uff09\u548c\u9002\u5ea6\u7684\u51c6\u786e\u7387\u4e0b\u964d\uff08\u7ea63-7% mAP50-95\uff09\uff0c\u4f46\u63d0\u51fa\u7684\u9000\u5316\u611f\u77e5\u6821\u51c6\u7b56\u7565\u5e76\u672a\u5728\u5927\u591a\u6570\u6a21\u578b\u548c\u9000\u5316\u6761\u4ef6\u4e0b\u5e26\u6765\u5e7f\u6cdb\u4e14\u4e00\u81f4\u7684\u9c81\u68d2\u6027\u63d0\u5347\u3002"}}
{"id": "2508.19604", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19604", "abs": "https://arxiv.org/abs/2508.19604", "authors": ["Qizhe Fan", "Chaoyu Liu", "Zhonghua Qiao", "Xiaoqin Shen"], "title": "IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation", "comment": null, "summary": "Domain Generalized Semantic Segmentation (DGSS) focuses on training a model\nusing labeled data from a source domain, with the goal of achieving robust\ngeneralization to unseen target domains during inference. A common approach to\nimprove generalization is to augment the source domain with synthetic data\ngenerated by diffusion models (DMs). However, the generated images often\ncontain structural or semantic defects due to training imperfections. Training\nsegmentation models with such flawed data can lead to performance degradation\nand error accumulation. To address this issue, we propose to integrate inverse\nevolution layers (IELs) into the generative process. IELs are designed to\nhighlight spatial discontinuities and semantic inconsistencies using\nLaplacian-based priors, enabling more effective filtering of undesirable\ngenerative patterns. Based on this mechanism, we introduce IELDM, an enhanced\ndiffusion-based data augmentation framework that can produce higher-quality\nimages. Furthermore, we observe that the defect-suppression capability of IELs\ncan also benefit the segmentation network by suppressing artifact propagation.\nBased on this insight, we embed IELs into the decoder of the DGSS model and\npropose IELFormer to strengthen generalization capability in cross-domain\nscenarios. To further strengthen the model's semantic consistency across\nscales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,\nwhich performs frequency-domain analysis to achieve structured integration of\nmulti-resolution features, thereby improving cross-scale coherence. Extensive\nexperiments on benchmark datasets demonstrate that our approach achieves\nsuperior generalization performance compared to existing methods.", "AI": {"tldr": "\u63d0\u51faIELDM\u548cIELFormer\uff0c\u901a\u8fc7\u9006\u6f14\u5316\u5c42\u548c\u591a\u5c3a\u5ea6\u9891\u7387\u878d\u5408\u6a21\u5757\u63d0\u5347\u57df\u5e7f\u4e49\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5e38\u5b58\u5728\u7ed3\u6784\u6216\u8bed\u4e49\u7f3a\u9677\uff0c\u76f4\u63a5\u7528\u4e8e\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u9519\u8bef\u7d2f\u79ef\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u8fc7\u6ee4\u8fd9\u4e9b\u7f3a\u9677\u5e76\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faIELDM\u6846\u67b6\uff0c\u901a\u8fc7\u9006\u6f14\u5316\u5c42\uff08IELs\uff09\u8fc7\u6ee4\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u7f3a\u9677\uff1b\u5e76\u8bbe\u8ba1IELFormer\uff0c\u5c06IELs\u5d4c\u5165\u5206\u5272\u7f51\u7edc\u89e3\u7801\u5668\uff0c\u7ed3\u5408MFF\u6a21\u5757\u589e\u5f3a\u591a\u5c3a\u5ea6\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIELDM\u80fd\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0cIELFormer\u5728\u8de8\u57df\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "IELDM\u548cIELFormer\u901a\u8fc7\u6574\u5408\u9006\u6f14\u5316\u5c42\uff08IELs\uff09\u548c\u591a\u5c3a\u5ea6\u9891\u7387\u878d\u5408\uff08MFF\uff09\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57df\u5e7f\u4e49\u8bed\u4e49\u5206\u5272\uff08DGSS\uff09\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.19626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19626", "abs": "https://arxiv.org/abs/2508.19626", "authors": ["Jiajun Sun", "Zhen Yu", "Siyuan Yan", "Jason J. Ong", "Zongyuan Ge", "Lei Zhang"], "title": "Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model", "comment": "11 pages, 4 figures", "summary": "Skin images from real-world clinical practice are often limited, resulting in\na shortage of training data for deep-learning models. While many studies have\nexplored skin image synthesis, existing methods often generate low-quality\nimages and lack control over the lesion's location and type. To address these\nlimitations, we present LF-VAR, a model leveraging quantified lesion\nmeasurement scores and lesion type labels to guide the clinically relevant and\ncontrollable synthesis of skin images. It enables controlled skin synthesis\nwith specific lesion characteristics based on language prompts. We train a\nmultiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to\nencode images into discrete latent representations for structured tokenization.\nThen, a Visual AutoRegressive (VAR) Transformer trained on tokenized\nrepresentations facilitates image synthesis. Lesion measurement from the lesion\nregion and types as conditional embeddings are integrated to enhance synthesis\nfidelity. Our method achieves the best overall FID score (average 0.74) among\nseven lesion types, improving upon the previous state-of-the-art (SOTA) by\n6.3%. The study highlights our controllable skin synthesis model's\neffectiveness in generating high-fidelity, clinically relevant synthetic skin\nimages. Our framework code is available at\nhttps://github.com/echosun1996/LF-VAR.", "AI": {"tldr": "LF-VAR\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u91cf\u5316\u75c5\u7076\u6d4b\u91cf\u548c\u7c7b\u578b\u6807\u7b7e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u76ae\u80a4\u56fe\u50cf\u5408\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u4e03\u79cd\u75c5\u7076\u7c7b\u578b\u4e2d\u53d6\u5f97\u6700\u4f73FID\u5206\u6570\uff08\u5e73\u57470.74\uff09\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u76ae\u80a4\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u4f4e\u4e14\u96be\u4ee5\u63a7\u5236\u75c5\u7076\u4f4d\u7f6e\u548c\u7c7b\u578b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u57fa\u4e8e\u8bed\u8a00\u63d0\u793a\u751f\u6210\u5177\u6709\u7279\u5b9a\u75c5\u7076\u7279\u5f81\u7684\u9ad8\u8d28\u91cf\u76ae\u80a4\u56fe\u50cf\u7684\u6a21\u578b\u3002", "method": "LF-VAR\u6a21\u578b\u91c7\u7528\u591a\u5c3a\u5ea6\u75c5\u7076\u805a\u7126\u7684\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VQVAE\uff09\u5bf9\u56fe\u50cf\u8fdb\u884c\u79bb\u6563\u6f5c\u5728\u8868\u793a\u7f16\u7801\uff0c\u968f\u540e\u901a\u8fc7\u57fa\u4e8etoken\u5316\u8868\u793a\u7684\u89c6\u89c9\u81ea\u56de\u5f52\uff08VAR\uff09\u53d8\u6362\u5668\u8fdb\u884c\u56fe\u50cf\u5408\u6210\u3002\u75c5\u7076\u6d4b\u91cf\u548c\u7c7b\u578b\u4f5c\u4e3a\u6761\u4ef6\u5d4c\u5165\u88ab\u6574\u5408\u4ee5\u589e\u5f3a\u5408\u6210\u4fdd\u771f\u5ea6\u3002", "result": "LF-VAR\u5728\u4e03\u79cd\u75c5\u7076\u7c7b\u578b\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73FID\u5206\u6570\uff08\u5e73\u57470.74\uff09\uff0c\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u5347\u4e866.3%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u4e34\u5e8a\u76f8\u5173\u5408\u6210\u76ae\u80a4\u56fe\u50cf\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "LF-VAR\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u91cf\u5316\u75c5\u7076\u6d4b\u91cf\u8bc4\u5206\u548c\u75c5\u7076\u7c7b\u578b\u6807\u7b7e\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u4e34\u5e8a\u76f8\u5173\u7684\u76ae\u80a4\u56fe\u50cf\u5408\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u56fe\u50cf\u7684\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u4e03\u79cd\u75c5\u7076\u7c7b\u578b\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73FID\u5206\u6570\uff08\u5e73\u57470.74\uff09\uff0c\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u5347\u4e866.3%\u3002"}}
{"id": "2508.19630", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19630", "abs": "https://arxiv.org/abs/2508.19630", "authors": ["Xiaolei Wei", "Yi Ouyang", "Haibo Ye"], "title": "Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition", "comment": "This paper has been accepted to PRCV 2025", "summary": "Long-tailed visual recognition is challenging not only due to class imbalance\nbut also because of varying classification difficulty across categories. Simply\nreweighting classes by frequency often overlooks those that are intrinsically\nhard to learn. To address this, we propose \\textbf{DQRoute}, a modular\nframework that combines difficulty-aware optimization with dynamic expert\ncollaboration. DQRoute first estimates class-wise difficulty based on\nprediction uncertainty and historical performance, and uses this signal to\nguide training with adaptive loss weighting. On the architectural side, DQRoute\nemploys a mixture-of-experts design, where each expert specializes in a\ndifferent region of the class distribution. At inference time, expert\npredictions are weighted by confidence scores derived from expert-specific OOD\ndetectors, enabling input-adaptive routing without the need for a centralized\nrouter. All components are trained jointly in an end-to-end manner. Experiments\non standard long-tailed benchmarks demonstrate that DQRoute significantly\nimproves performance, particularly on rare and difficult classes, highlighting\nthe benefit of integrating difficulty modeling with decentralized expert\nrouting.", "AI": {"tldr": "DQRoute\u7ed3\u5408\u96be\u5ea6\u611f\u77e5\u4f18\u5316\u548c\u52a8\u6001\u4e13\u5bb6\u534f\u4f5c\uff0c\u6709\u6548\u63d0\u5347\u957f\u5c3e\u89c6\u89c9\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u957f\u5c3e\u89c6\u89c9\u8bc6\u522b\u4e0d\u4ec5\u9762\u4e34\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u8fd8\u56e0\u4e0d\u540c\u7c7b\u522b\u7684\u5206\u7c7b\u96be\u5ea6\u5dee\u5f02\u800c\u590d\u6742\u5316\u3002\u7b80\u5355\u5730\u6309\u9891\u7387\u91cd\u52a0\u6743\u7c7b\u522b\u4f1a\u5ffd\u89c6\u90a3\u4e9b\u672c\u8d28\u4e0a\u96be\u4ee5\u5b66\u4e60\u7684\u7c7b\u522b\u3002", "method": "\u63d0\u51faDQRoute\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u4e8e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u5386\u53f2\u8868\u73b0\u7684\u7c7b\u522b\u96be\u5ea6\u4f30\u8ba1\u3001\u81ea\u9002\u5e94\u635f\u5931\u52a0\u6743\u8bad\u7ec3\uff0c\u4ee5\u53ca\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u8bbe\u8ba1\u8fdb\u884c\u52a8\u6001\u4e13\u5bb6\u534f\u4f5c\u3002", "result": "\u5728\u6807\u51c6\u957f\u5c3e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDQRoute\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u7a00\u6709\u548c\u56f0\u96be\u7c7b\u522b\u4e0a\u3002", "conclusion": "DQRoute\u901a\u8fc7\u7ed3\u5408\u96be\u5ea6\u611f\u77e5\u4f18\u5316\u548c\u52a8\u6001\u4e13\u5bb6\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u5c3e\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7a00\u6709\u548c\u56f0\u96be\u7c7b\u522b\u4e0a\u3002"}}
{"id": "2508.19638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19638", "abs": "https://arxiv.org/abs/2508.19638", "authors": ["Yang Li", "Quan Yuan", "Guiyang Luo", "Xiaoyuan Fu", "Rui Pan", "Yujia Yang", "Congzhang Shao", "Yuewen Liu", "Jinglin Li"], "title": "Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception", "comment": null, "summary": "Collaborative perception allows agents to enhance their perceptual\ncapabilities by exchanging intermediate features. Existing methods typically\norganize these intermediate features as 2D bird's-eye-view (BEV)\nrepresentations, which discard critical fine-grained 3D structural cues\nessential for accurate object recognition and localization. To this end, we\nfirst introduce point-level tokens as intermediate representations for\ncollaborative perception. However, point-cloud data are inherently unordered,\nmassive, and position-sensitive, making it challenging to produce compact and\naligned point-level token sequences that preserve detailed structural\ninformation. Therefore, we present CoPLOT, a novel Collaborative perception\nframework that utilizes Point-Level Optimized Tokens. It incorporates a\npoint-native processing pipeline, including token reordering, sequence\nmodeling, and multi-agent spatial alignment. A semantic-aware token reordering\nmodule generates adaptive 1D reorderings by leveraging scene-level and\ntoken-level semantic information. A frequency-enhanced state space model\ncaptures long-range sequence dependencies across both spatial and spectral\ndomains, improving the differentiation between foreground tokens and background\nclutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop\nprocess, combining global agent-level correction with local token-level\nrefinement to mitigate localization noise. Extensive experiments on both\nsimulated and real-world datasets show that CoPLOT outperforms state-of-the-art\nmodels, with even lower communication and computation overhead. Code will be\navailable at https://github.com/CheeryLeeyy/CoPLOT.", "AI": {"tldr": "CoPLOT\u901a\u8fc7\u70b9\u7ea7\u4f18\u5316\u4ee4\u724c\u548c\u521b\u65b0\u7684\u5904\u7406\u6d41\u7a0b\uff0c\u63d0\u5347\u534f\u4f5c\u611f\u77e5\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u75282D\u9e1f\u77b0\u56fe\u8868\u793a\u4e2d\u95f4\u7279\u5f81\uff0c\u4f46\u4e22\u5f03\u4e86\u5173\u952e\u76843D\u7ed3\u6784\u4fe1\u606f\u3002\u4e3a\u4fdd\u7559\u8fd9\u4e9b\u4fe1\u606f\u5e76\u63d0\u5347\u5bf9\u8c61\u8bc6\u522b\u548c\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\uff0c\u5f15\u5165\u4e86\u70b9\u7ea7\u4ee4\u724c\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u3002", "method": "CoPLOT\u91c7\u7528\u70b9\u539f\u751f\u5904\u7406\u6d41\u7a0b\uff0c\u5305\u62ec\u4ee4\u724c\u91cd\u6392\u5e8f\u3001\u5e8f\u5217\u5efa\u6a21\u548c\u591a\u667a\u80fd\u4f53\u7a7a\u95f4\u5bf9\u9f50\u3002\u5177\u4f53\u5305\u62ec\u8bed\u4e49\u611f\u77e5\u4ee4\u724c\u91cd\u6392\u5e8f\u6a21\u5757\u3001\u9891\u7387\u589e\u5f3a\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u90bb\u5c45\u5230\u81ea\u6211\u5bf9\u9f50\u6a21\u5757\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCoPLOT\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u4e14\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u66f4\u4f4e\u3002", "conclusion": "CoPLOT\u6846\u67b6\u901a\u8fc7\u70b9\u7ea7\u4f18\u5316\u4ee4\u724c\u548c\u521b\u65b0\u7684\u5904\u7406\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534f\u4f5c\u611f\u77e5\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2508.19647", "categories": ["cs.CV", "I.2.10; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.19647", "abs": "https://arxiv.org/abs/2508.19647", "authors": ["Bikash Kumar Badatya", "Vipul Baghel", "Ravi Hegde"], "title": "UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks", "comment": "This paper has been accepted at the ICIP Satellite Workshop 2025", "summary": "Fine-grained action localization in untrimmed sports videos presents a\nsignificant challenge due to rapid and subtle motion transitions over short\ndurations. Existing supervised and weakly supervised solutions often rely on\nextensive annotated datasets and high-capacity models, making them\ncomputationally intensive and less adaptable to real-world scenarios. In this\nwork, we introduce a lightweight and unsupervised skeleton-based action\nlocalization pipeline that leverages spatio-temporal graph neural\nrepresentations. Our approach pre-trains an Attention-based Spatio-Temporal\nGraph Convolutional Network (ASTGCN) on a pose-sequence denoising task with\nblockwise partitions, enabling it to learn intrinsic motion dynamics without\nany manual labeling. At inference, we define a novel Action Dynamics Metric\n(ADM), computed directly from low-dimensional ASTGCN embeddings, which detects\nmotion boundaries by identifying inflection points in its curvature profile.\nOur method achieves a mean Average Precision (mAP) of 82.66% and average\nlocalization latency of 29.09 ms on the DSV Diving dataset, matching\nstate-of-the-art supervised performance while maintaining computational\nefficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving\nfootage without retraining, demonstrating its practical applicability for\nlightweight, real-time action analysis systems in embedded or dynamic\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65e0\u76d1\u7763\u9aa8\u67b6\u52a8\u4f5c\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5229\u7528\u65f6\u7a7a\u56fe\u795e\u7ecf\u8868\u793a\u548c\u52a8\u4f5c\u52a8\u6001\u5ea6\u91cf\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u8fbe\u5230\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u5feb\u901f\u548c\u5fae\u5999\u7684\u8fd0\u52a8\u8f6c\u6362\uff0c\u672a\u7ecf\u4fee\u526a\u7684\u8fd0\u52a8\u89c6\u9891\u4e2d\u7684\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u5b9a\u4f4d\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u9ad8\u5bb9\u91cf\u6a21\u578b\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u9002\u5e94\u6027\u5dee\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u56fe\u795e\u7ecf\u8868\u793a\u7684\u8f7b\u91cf\u7ea7\u65e0\u76d1\u7763\u9aa8\u67b6\u52a8\u4f5c\u5b9a\u4f4d\u6d41\u7a0b\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edc\uff08ASTGCN\uff09\u8fdb\u884c\u59ff\u6001\u5e8f\u5217\u53bb\u566a\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u52a8\u4f5c\u52a8\u6001\u5ea6\u91cf\uff08ADM\uff09\u6765\u68c0\u6d4b\u8fd0\u52a8\u8fb9\u754c\u3002", "result": "\u5728DSV Diving\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e8682.66%\u7684\u5e73\u5747\u7cbe\u5ea6\uff08mAP\uff09\u548c29.09\u6beb\u79d2\u7684\u5e73\u5747\u5b9a\u4f4d\u5ef6\u8fdf\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u76d1\u7763\u6027\u80fd\u76f8\u5339\u914d\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u65e0\u76d1\u7763\u9aa8\u67b6\u52a8\u4f5c\u5b9a\u4f4d\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e86\u4e0e\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u5728\u672a\u7ecf\u8bad\u7ec3\u7684\u91ce\u5916\u6f5c\u6c34\u89c6\u9891\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5d4c\u5165\u5f0f\u6216\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u52a8\u4f5c\u5206\u6790\u7cfb\u7edf\u3002"}}
{"id": "2508.19649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19649", "abs": "https://arxiv.org/abs/2508.19649", "authors": ["Dongjin Kim", "Jaekyun Ko", "Muhammad Kashif Ali", "Tae Hyun Kim"], "title": "IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising", "comment": "ICCV 2025. Project Page: https://dongjinkim9.github.io/projects/idf/", "summary": "Image denoising is a fundamental challenge in computer vision, with\napplications in photography and medical imaging. While deep learning-based\nmethods have shown remarkable success, their reliance on specific noise\ndistributions limits generalization to unseen noise types and levels. Existing\napproaches attempt to address this with extensive training data and high\ncomputational resources but they still suffer from overfitting. To address\nthese issues, we conduct image denoising by utilizing dynamically generated\nkernels via efficient operations. This approach helps prevent overfitting and\nimproves resilience to unseen noise. Specifically, our method leverages a\nFeature Extraction Module for robust noise-invariant features, Global\nStatistics and Local Correlation Modules to capture comprehensive noise\ncharacteristics and structural correlations. The Kernel Prediction Module then\nemploys these cues to produce pixel-wise varying kernels adapted to local\nstructures, which are then applied iteratively for denoising. This ensures both\nefficiency and superior restoration quality. Despite being trained on\nsingle-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse\nnoise types and levels, demonstrating the promise of iterative dynamic\nfiltering for practical image denoising.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u751f\u6210\u6838\u7684\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u6548\u64cd\u4f5c\u907f\u514d\u8fc7\u62df\u5408\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u6a21\u578b\u5728\u591a\u79cd\u566a\u58f0\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u56fe\u50cf\u53bb\u566a\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u7279\u5b9a\u566a\u58f0\u5206\u5e03\u7684\u4f9d\u8d56\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8fc7\u62df\u5408\u548c\u9ad8\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u83b7\u53d6\u566a\u58f0\u4e0d\u53d8\u7279\u5f81\uff0c\u7ed3\u5408\u5168\u5c40\u7edf\u8ba1\u548c\u5c40\u90e8\u76f8\u5173\u6027\u6a21\u5757\u6355\u6349\u566a\u58f0\u7279\u5f81\u548c\u7ed3\u6784\u5173\u8054\uff0c\u901a\u8fc7\u6838\u9884\u6d4b\u6a21\u5757\u751f\u6210\u50cf\u7d20\u7ea7\u53d8\u5316\u7684\u6838\uff0c\u5e76\u8fed\u4ee3\u5e94\u7528\u4e8e\u53bb\u566a\u3002", "result": "\u5c3d\u7ba1\u4ec5\u4f7f\u7528\u5355\u7ea7\u9ad8\u65af\u566a\u58f0\u8bad\u7ec3\uff0c\u7d27\u51d1\u6a21\u578b\uff08\u7ea60.04 M\uff09\u5728\u591a\u79cd\u566a\u58f0\u7c7b\u578b\u548c\u7ea7\u522b\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u52a8\u6001\u751f\u6210\u6838\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u53bb\u566a\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u672a\u89c1\u566a\u58f0\u7c7b\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u8fed\u4ee3\u52a8\u6001\u6ee4\u6ce2\u5728\u5b9e\u9645\u56fe\u50cf\u53bb\u566a\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.19650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19650", "abs": "https://arxiv.org/abs/2508.19650", "authors": ["Hou Xia", "Zheren Fu", "Fangcan Ling", "Jiajun Li", "Yi Tu", "Zhendong Mao", "Yongdong Zhang"], "title": "Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models", "comment": null, "summary": "Large video language models (LVLMs) have made notable progress in video\nunderstanding, spurring the development of corresponding evaluation benchmarks.\nHowever, existing benchmarks generally assess overall performance across entire\nvideo sequences, overlooking nuanced behaviors such as contextual positional\nbias, a critical yet under-explored aspect of LVLM performance. We present\nVideo-LevelGauge, a dedicated benchmark designed to systematically assess\npositional bias in LVLMs. We employ standardized probes and customized\ncontextual setups, allowing flexible control over context length, probe\nposition, and contextual types to simulate diverse real-world scenarios. In\naddition, we introduce a comprehensive analysis method that combines\nstatistical measures with morphological pattern recognition to characterize\nbias. Our benchmark comprises 438 manually curated videos spanning multiple\ntypes, yielding 1,177 high-quality multiple-choice questions and 120 open-ended\nquestions, validated for their effectiveness in exposing positional bias. Based\non these, we evaluate 27 state-of-the-art LVLMs, including both commercial and\nopen-source models. Our findings reveal significant positional biases in many\nleading open-source models, typically exhibiting head or neighbor-content\npreferences. In contrast, commercial models such as Gemini2.5-Pro show\nimpressive, consistent performance across entire video sequences. Further\nanalyses on context length, context variation, and model scale provide\nactionable insights for mitigating bias and guiding model enhancement.", "AI": {"tldr": "Video-LevelGauge\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LVLM\u4f4d\u7f6e\u504f\u5dee\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u63a2\u9488\u548c\u5b9a\u5236\u5316\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u63ed\u793a\u4e86\u8bb8\u591a\u5f00\u6e90\u6a21\u578b\u7684\u663e\u8457\u504f\u5dee\uff0c\u800c\u5546\u4e1a\u6a21\u578b\u8868\u73b0\u4e00\u81f4\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u8bc4\u4f30\u6574\u4e2a\u89c6\u9891\u5e8f\u5217\u7684\u6574\u4f53\u6027\u80fd\uff0c\u5ffd\u7565\u4e86\u5982\u4e0a\u4e0b\u6587\u4f4d\u7f6e\u504f\u5dee\u7b49\u7ec6\u5fae\u884c\u4e3a\uff0c\u8fd9\u662fLVLM\u6027\u80fd\u4e2d\u5173\u952e\u4f46\u672a\u5145\u5206\u63a2\u7d22\u7684\u65b9\u9762\u3002", "method": "\u91c7\u7528\u6807\u51c6\u5316\u63a2\u9488\u548c\u5b9a\u5236\u5316\u4e0a\u4e0b\u6587\u8bbe\u7f6e\uff0c\u7075\u6d3b\u63a7\u5236\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u63a2\u9488\u4f4d\u7f6e\u548c\u4e0a\u4e0b\u6587\u7c7b\u578b\uff0c\u5e76\u7ed3\u5408\u7edf\u8ba1\u6d4b\u91cf\u4e0e\u5f62\u6001\u6a21\u5f0f\u8bc6\u522b\u6765\u8868\u5f81\u504f\u5dee\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b438\u4e2a\u624b\u52a8\u6574\u7406\u89c6\u9891\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u751f\u6210\u4e861,177\u4e2a\u9ad8\u8d28\u91cf\u9009\u62e9\u9898\u548c120\u4e2a\u5f00\u653e\u5f0f\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u66b4\u9732\u4f4d\u7f6e\u504f\u5dee\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u8bc4\u4f30\u4e8627\u4e2a\u6700\u5148\u8fdb\u7684LVLM\uff0c\u63ed\u793a\u4e86\u663e\u8457\u7684\u4f4d\u7f6e\u504f\u5dee\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bb8\u591a\u9886\u5148\u7684\u5f00\u6e90\u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u4f4d\u7f6e\u504f\u5dee\uff0c\u800c\u5546\u4e1a\u6a21\u578b\u5982Gemini2.5-Pro\u5728\u6574\u4e2a\u89c6\u9891\u5e8f\u5217\u4e2d\u8868\u73b0\u4e00\u81f4\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u4e0a\u4e0b\u6587\u53d8\u5316\u548c\u6a21\u578b\u89c4\u6a21\u4e3a\u51cf\u8f7b\u504f\u5dee\u548c\u6307\u5bfc\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\u3002"}}
{"id": "2508.19651", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19651", "abs": "https://arxiv.org/abs/2508.19651", "authors": ["B\u00e1lint M\u00e9sz\u00e1ros", "Ahmet Firintepe", "Sebastian Schmidt", "Stephan G\u00fcnnemann"], "title": "Scalable Object Detection in the Car Interior With Vision Foundation Models", "comment": null, "summary": "AI tasks in the car interior like identifying and localizing externally\nintroduced objects is crucial for response quality of personal assistants.\nHowever, computational resources of on-board systems remain highly constrained,\nrestricting the deployment of such solutions directly within the vehicle. To\naddress this limitation, we propose the novel Object Detection and Localization\n(ODAL) framework for interior scene understanding. Our approach leverages\nvision foundation models through a distributed architecture, splitting\ncomputational tasks between on-board and cloud. This design overcomes the\nresource constraints of running foundation models directly in the car. To\nbenchmark model performance, we introduce ODALbench, a new metric for\ncomprehensive assessment of detection and localization.Our analysis\ndemonstrates the framework's potential to establish new standards in this\ndomain. We compare the state-of-the-art GPT-4o vision foundation model with the\nlightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the\nlightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model\nachieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its\nbaseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the\nfine-tuned model maintains high detection accuracy while significantly reducing\nhallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.", "AI": {"tldr": "ODAL\u6846\u67b6\u901a\u8fc7\u5206\u5e03\u5f0f\u67b6\u6784\u548c\u8f7b\u91cf\u7ea7\u6a21\u578b\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u6c7d\u8f66\u5185\u90e8\u573a\u666f\u7406\u89e3\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8f66\u8f7d\u7cfb\u7edf\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\uff0c\u65e0\u6cd5\u76f4\u63a5\u90e8\u7f72\u590d\u6742AI\u6a21\u578b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faODAL\u6846\u67b6\uff0c\u7ed3\u5408\u8f66\u8f7d\u548c\u4e91\u7aef\u5206\u5e03\u5f0f\u8ba1\u7b97\uff0c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5bf9\u8c61\u68c0\u6d4b\u548c\u5b9a\u4f4d\uff0c\u5e76\u5f15\u5165ODALbench\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5fae\u8c03\u540e\u7684ODAL-LLaVA\u6a21\u578b\u5728ODAL$_{score}$\u4e0a\u8fbe\u523089%\uff0c\u6bd4\u57fa\u7ebf\u63d0\u534771%\uff0c\u4e14\u4f18\u4e8eGPT-4o\u8fd120%\u3002", "conclusion": "ODAL\u6846\u67b6\u901a\u8fc7\u5206\u5e03\u5f0f\u67b6\u6784\u548c\u8f7b\u91cf\u7ea7\u6a21\u578b\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c7d\u8f66\u5185\u90e8\u573a\u666f\u7406\u89e3\u7684\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8f66\u8f7d\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19652", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19652", "abs": "https://arxiv.org/abs/2508.19652", "authors": ["Zongxia Li", "Wenhao Yu", "Chengsong Huang", "Rui Liu", "Zhenwen Liang", "Fuxiao Liu", "Jingxi Che", "Dian Yu", "Jordan Boyd-Graber", "Haitao Mi", "Dong Yu"], "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition", "comment": "16 pages, two figures", "summary": "Vision-Language Models (VLMs) often suffer from visual hallucinations, saying\nthings that are not actually in the image, and language shortcuts, where they\nskip the visual part and just rely on text priors. These issues arise because\nmost post-training methods for VLMs rely on simple verifiable answer matching\nand supervise only final outputs, leaving intermediate visual reasoning without\nexplicit guidance. As a result, VLMs receive sparse visual signals and often\nlearn to prioritize language-based reasoning over visual perception. To\nmitigate this, some existing methods add visual supervision using human\nannotations or distilled labels from external large models. However, human\nannotations are labor-intensive and costly, and because external signals cannot\nadapt to the evolving policy, they cause distributional shifts that can lead to\nreward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method\nthat improves visual reasoning without relying on external visual supervisions\nvia reinforcement learning. Vision-SR1 decomposes VLM reasoning into two\nstages: visual perception and language reasoning. The model is first prompted\nto produce self-contained visual perceptions that are sufficient to answer the\nquestion without referring back the input image. To validate this\nself-containment, the same VLM model is then re-prompted to perform language\nreasoning using only the generated perception as input to compute reward. This\nself-reward is combined with supervision on final outputs, providing a balanced\ntraining signal that strengthens both visual perception and language reasoning.\nOur experiments demonstrate that Vision-SR1 improves visual reasoning,\nmitigates visual hallucinations, and reduces reliance on language shortcuts\nacross diverse vision-language tasks.", "AI": {"tldr": "Vision-SR1\u901a\u8fc7\u81ea\u5956\u52b1\u673a\u5236\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\uff0c\u51cf\u5c11\u89c6\u89c9\u5e7b\u89c9\u548c\u8bed\u8a00\u6377\u5f84\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u89c6\u89c9\u5e7b\u89c9\u548c\u8bed\u8a00\u6377\u5f84\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8e\u4e2d\u95f4\u89c6\u89c9\u63a8\u7406\u7f3a\u4e4f\u660e\u786e\u6307\u5bfc\u3002", "method": "\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u5206\u4e3a\u89c6\u89c9\u611f\u77e5\u548c\u8bed\u8a00\u63a8\u7406\u4e24\u4e2a\u9636\u6bb5\uff0c\u901a\u8fc7\u81ea\u5956\u52b1\u673a\u5236\u7ed3\u5408\u6700\u7ec8\u8f93\u51fa\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVision-SR1\u5728\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u89c6\u89c9\u5e7b\u89c9\u548c\u8bed\u8a00\u6377\u5f84\u7684\u4f9d\u8d56\u3002", "conclusion": "Vision-SR1\u901a\u8fc7\u81ea\u5956\u52b1\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u89c6\u89c9\u5e7b\u89c9\u548c\u8bed\u8a00\u6377\u5f84\u7684\u4f9d\u8d56\u3002"}}
{"id": "2508.19654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19654", "abs": "https://arxiv.org/abs/2508.19654", "authors": ["Matthias H\u00f6fflin", "J\u00fcrgen Wassner"], "title": "Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications", "comment": "Accepted for the IAA-SPAICE 2025 conference", "summary": "Spiking Neural Networks (SNNs), inspired by biological intelligence, have\nlong been considered inherently energy-efficient, making them attractive for\nresource-constrained domains such as space applications. However, recent\ncomparative studies with conventional Artificial Neural Networks (ANNs) have\nbegun to question this reputation, especially for digital implementations. This\nwork investigates SNNs for multi-output regression, specifically 3-D satellite\nposition estimation from monocular images, and compares hardware-aware and\nhardware-agnostic energy estimation methods. The proposed SNN, trained using\nthe membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the\nfinal layer, achieves comparable Mean Squared Error (MSE) to a reference\nConvolutional Neural Network (CNN) on a photorealistic satellite dataset.\nEnergy analysis shows that while hardware-agnostic methods predict a consistent\n50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals\nthat significant energy savings are realized only on neuromorphic hardware and\nwith high input sparsity. The influence of dark pixel ratio on energy\nconsumption is quantified, emphasizing the impact of data characteristics and\nhardware assumptions. These findings highlight the need for transparent\nevaluation methods and explicit disclosure of underlying assumptions to ensure\nfair comparisons of neural network energy efficiency.", "AI": {"tldr": "SNNs\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u548c\u9ad8\u8f93\u5165\u7a00\u758f\u6027\u4e0b\u80fd\u663e\u8457\u8282\u80fd\uff0c\u4f46\u9700\u900f\u660e\u8bc4\u4f30\u548c\u660e\u786e\u5047\u8bbe\u4ee5\u786e\u4fdd\u516c\u5e73\u6bd4\u8f83\u3002", "motivation": "\u5c3d\u7ba1SNNs\u957f\u671f\u4ee5\u6765\u88ab\u8ba4\u4e3a\u5177\u6709\u56fa\u6709\u7684\u80fd\u6548\u4f18\u52bf\uff0c\u4f46\u6700\u8fd1\u4e0e\u5e38\u89c4ANNs\u7684\u6bd4\u8f83\u7814\u7a76\u5f00\u59cb\u8d28\u7591\u8fd9\u4e00\u58f0\u8a89\uff0c\u5c24\u5176\u662f\u5728\u6570\u5b57\u5b9e\u73b0\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u591a\u8f93\u51fa\u56de\u5f52\uff083-D\u536b\u661f\u4f4d\u7f6e\u4f30\u8ba1\uff09\u6765\u9a8c\u8bc1SNNs\u7684\u80fd\u6548\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cdSNN\uff0c\u4f7f\u7528Leaky Integrate-and-Fire\uff08LIF\uff09\u795e\u7ecf\u5143\u7684\u819c\u7535\u4f4d\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5c06\u5176\u4e0e\u53c2\u8003CNN\u5728\u5149\u771f\u5b9e\u611f\u536b\u661f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "result": "\u63d0\u51fa\u7684SNN\u5728\u5149\u771f\u5b9e\u611f\u536b\u661f\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u53c2\u8003CNN\u76f8\u5f53\u7684MSE\u3002\u80fd\u6548\u5206\u6790\u663e\u793a\uff0c\u786c\u4ef6\u65e0\u5173\u65b9\u6cd5\u9884\u6d4bSNNs\u6bd4CNNs\u670950-60%\u7684\u80fd\u6548\u4f18\u52bf\uff0c\u800c\u786c\u4ef6\u611f\u77e5\u5206\u6790\u5219\u8868\u660e\u53ea\u6709\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u548c\u9ad8\u8f93\u5165\u7a00\u758f\u6027\u4e0b\u624d\u80fd\u5b9e\u73b0\u663e\u8457\u8282\u80fd\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u53ea\u6709\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u4e14\u8f93\u5165\u7a00\u758f\u6027\u8f83\u9ad8\u65f6\uff0cSNNs\u624d\u80fd\u663e\u8457\u8282\u7701\u80fd\u6e90\u3002\u8fd9\u5f3a\u8c03\u4e86\u900f\u660e\u8bc4\u4f30\u65b9\u6cd5\u548c\u660e\u786e\u62ab\u9732\u6f5c\u5728\u5047\u8bbe\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u795e\u7ecf\u7f51\u7edc\u80fd\u6548\u7684\u516c\u5e73\u6bd4\u8f83\u3002"}}
{"id": "2508.19804", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19804", "abs": "https://arxiv.org/abs/2508.19804", "authors": ["Christian Marzahl", "Brian Napora"], "title": "A bag of tricks for real-time Mitotic Figure detection", "comment": null, "summary": "Mitotic figure (MF) detection in histopathology images is challenging due to\nlarge variations in slide scanners, staining protocols, tissue types, and the\npresence of artifacts. This paper presents a collection of training techniques\n- a bag of tricks - that enable robust, real-time MF detection across diverse\ndomains. We build on the efficient RTMDet single stage object detector to\nachieve high inference speed suitable for clinical deployment. Our method\naddresses scanner variability and tumor heterogeneity via extensive\nmulti-domain training data, balanced sampling, and careful augmentation.\nAdditionally, we employ targeted, hard negative mining on necrotic and debris\ntissue to reduce false positives. In a grouped 5-fold cross-validation across\nmultiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On\nthe preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025\nchallenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,\noutperforming larger models and demonstrating adaptability to new, unfamiliar\ndomains. The proposed solution offers a practical trade-off between accuracy\nand speed, making it attractive for real-world clinical adoption.", "AI": {"tldr": "A bag of tricks enhances RTMDet for robust, real-time mitotic figure detection, achieving high F1 scores across diverse domains while balancing accuracy and speed.", "motivation": "Mitotic figure detection in histopathology images is challenging due to variations in slide scanners, staining protocols, tissue types, and artifacts. The paper aims to address these challenges for robust, real-time detection.", "method": "The paper builds on the efficient RTMDet single-stage object detector, employing extensive multi-domain training data, balanced sampling, careful augmentation, and targeted hard negative mining on necrotic and debris tissue.", "result": "The model achieves an F1 score between 0.78 and 0.84 in cross-validation and 0.81 on the MIDOG 2025 preliminary test set, outperforming larger models.", "conclusion": "The proposed solution offers a practical trade-off between accuracy and speed, making it attractive for real-world clinical adoption."}}
{"id": "2508.19664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19664", "abs": "https://arxiv.org/abs/2508.19664", "authors": ["Weicheng Liao", "Zan Chen", "Jianyang Xie", "Yalin Zheng", "Yuhui Ma", "Yitian Zhao"], "title": "A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement", "comment": null, "summary": "Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics\nby providing a comprehensive view of the retina. However, it often suffers from\nquality-degrading factors such as blurring and uneven illumination, which\nobscure fine details and mask pathological information. While numerous retinal\nimage enhancement methods have been proposed for other fundus imageries, they\noften fail to address the unique requirements in UWF, particularly the need to\npreserve pathological details. In this paper, we propose a novel\nfrequency-aware self-supervised learning method for UWF image enhancement. It\nincorporates frequency-decoupled image deblurring and Retinex-guided\nillumination compensation modules. An asymmetric channel integration operation\nis introduced in the former module, so as to combine global and local views by\nleveraging high- and low-frequency information, ensuring the preservation of\nfine and broader structural details. In addition, a color preservation unit is\nproposed in the latter Retinex-based module, to provide multi-scale spatial and\nfrequency information, enabling accurate illumination estimation and\ncorrection. Experimental results demonstrate that the proposed work not only\nenhances visualization quality but also improves disease diagnosis performance\nby restoring and correcting fine local details and uneven intensity. To the\nbest of our knowledge, this work is the first attempt for UWF image\nenhancement, offering a robust and clinically valuable tool for improving\nretinal disease management.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9891\u7387\u611f\u77e5\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8eUWF\u56fe\u50cf\u589e\u5f3a\uff0c\u7ed3\u5408\u53bb\u6a21\u7cca\u548c\u7167\u660e\u8865\u507f\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "UWF\u89c6\u7f51\u819c\u6210\u50cf\u867d\u9769\u65b0\u4e86\u89c6\u7f51\u819c\u8bca\u65ad\uff0c\u4f46\u5e38\u53d7\u6a21\u7cca\u548c\u7167\u660e\u4e0d\u5747\u7b49\u8d28\u91cf\u4e0b\u964d\u56e0\u7d20\u5f71\u54cd\uff0c\u63a9\u76d6\u75c5\u7406\u7ec6\u8282\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6ee1\u8db3UWF\u7684\u72ec\u7279\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u4fdd\u7559\u75c5\u7406\u7ec6\u8282\u65b9\u9762\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9891\u7387\u611f\u77e5\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u9891\u7387\u89e3\u8026\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u6a21\u5757\u548cRetinex\u5f15\u5bfc\u7684\u7167\u660e\u8865\u507f\u6a21\u5757\u3002\u524d\u8005\u901a\u8fc7\u4e0d\u5bf9\u79f0\u901a\u9053\u6574\u5408\u64cd\u4f5c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u89c6\u56fe\uff0c\u540e\u8005\u5f15\u5165\u989c\u8272\u4fdd\u7559\u5355\u5143\u4ee5\u63d0\u4f9b\u591a\u5c3a\u5ea6\u7a7a\u95f4\u548c\u9891\u7387\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u53ef\u89c6\u5316\u8d28\u91cf\uff0c\u8fd8\u901a\u8fc7\u6062\u590d\u548c\u6821\u6b63\u5c40\u90e8\u7ec6\u8282\u53ca\u4e0d\u5747\u5300\u5f3a\u5ea6\uff0c\u6539\u5584\u4e86\u75be\u75c5\u8bca\u65ad\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9891\u7387\u611f\u77e5\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8eUWF\u56fe\u50cf\u589e\u5f3a\uff0c\u901a\u8fc7\u7ed3\u5408\u9891\u7387\u89e3\u8026\u7684\u53bb\u6a21\u7cca\u548cRetinex\u5f15\u5bfc\u7684\u7167\u660e\u8865\u507f\u6a21\u5757\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u53ef\u89c6\u5316\u8d28\u91cf\uff0c\u8fd8\u6539\u5584\u4e86\u75be\u75c5\u8bca\u65ad\u6027\u80fd\u3002\u8fd9\u662f\u9996\u6b21\u9488\u5bf9UWF\u56fe\u50cf\u589e\u5f3a\u7684\u5c1d\u8bd5\uff0c\u4e3a\u89c6\u7f51\u819c\u75be\u75c5\u7ba1\u7406\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u4e34\u5e8a\u5de5\u5177\u3002"}}
{"id": "2508.19815", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19815", "abs": "https://arxiv.org/abs/2508.19815", "authors": ["Linkuan Zhou", "Zhexin Chen", "Yufei Shen", "Junlin Xu", "Ping Xuan", "Yixin Zhu", "Yuqi Fang", "Cong Cong", "Leyi Wei", "Ran Su", "Jia Zhou", "Qiangguo Jin"], "title": "ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images", "comment": null, "summary": "Automated segmentation of the fetal head in ultrasound images is critical for\nprenatal monitoring. However, achieving robust segmentation remains challenging\ndue to the poor quality of ultrasound images and the lack of annotated data.\nSemi-supervised methods alleviate the lack of annotated data but struggle with\nthe unique characteristics of fetal head ultrasound images, making it\nchallenging to generate reliable pseudo-labels and enforce effective\nconsistency regularization constraints. To address this issue, we propose a\nnovel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.\nOur framework consists of the dual-scoring adaptive filtering strategy, the\nellipse-constrained pseudo-label refinement, and the symmetry-based multiple\nconsistency regularization. The dual-scoring adaptive filtering strategy uses\nboundary consistency and contour regularity criteria to evaluate and filter\nteacher outputs. The ellipse-constrained pseudo-label refinement refines these\nfiltered outputs by fitting least-squares ellipses, which strengthens pixels\nnear the center of the fitted ellipse and suppresses noise simultaneously. The\nsymmetry-based multiple consistency regularization enforces multi-level\nconsistency across perturbed images, symmetric regions, and between original\npredictions and pseudo-labels, enabling the model to capture robust and stable\nshape representations. Our method achieves state-of-the-art performance on two\nbenchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%\nwith 10% and 20% labeled data, respectively. On the PSFH dataset, the scores\nare 91.68% and 93.70% under the same settings.", "AI": {"tldr": "ERSR\u6846\u67b6\u901a\u8fc7\u53cc\u8bc4\u5206\u8fc7\u6ee4\u3001\u692d\u5706\u7ea6\u675f\u548c\u5bf9\u79f0\u6027\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80ce\u513f\u5934\u90e8\u8d85\u58f0\u56fe\u50cf\u7684\u5206\u5272\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u534a\u76d1\u7763\u65b9\u6cd5\u5728\u80ce\u513f\u5934\u90e8\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u4e2d\u9762\u4e34\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u56f0\u96be\u548c\u4e00\u81f4\u6027\u7ea6\u675f\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "ERSR\u6846\u67b6\u5305\u62ec\u53cc\u8bc4\u5206\u81ea\u9002\u5e94\u8fc7\u6ee4\u7b56\u7565\u3001\u692d\u5706\u7ea6\u675f\u4f2a\u6807\u7b7e\u7cbe\u70bc\u548c\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u591a\u91cd\u4e00\u81f4\u6027\u6b63\u5219\u5316\u3002", "result": "\u5728HC18\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u752810%\u548c20%\u6807\u8bb0\u6570\u636e\u65f6\uff0cDice\u5206\u6570\u5206\u522b\u8fbe\u523092.05%\u548c95.36%\uff1b\u5728PSFH\u6570\u636e\u96c6\u4e0a\u5206\u522b\u4e3a91.68%\u548c93.70%\u3002", "conclusion": "\u63d0\u51fa\u7684ERSR\u6846\u67b6\u5728\u80ce\u513f\u5934\u90e8\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2508.19688", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19688", "abs": "https://arxiv.org/abs/2508.19688", "authors": ["Gangjian Zhang", "Jian Shu", "Nanjie Yao", "Hao Wang"], "title": "SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction", "comment": "10 pages, 8 figures", "summary": "Monocular texture 3D human reconstruction aims to create a complete 3D\ndigital avatar from just a single front-view human RGB image. However, the\ngeometric ambiguity inherent in a single 2D image and the scarcity of 3D human\ntraining data are the main obstacles limiting progress in this field. To\naddress these issues, current methods employ prior geometric estimation\nnetworks to derive various human geometric forms, such as the SMPL model and\nnormal maps. However, they struggle to integrate these modalities effectively,\nleading to view inconsistencies, such as facial distortions. To this end, we\npropose a two-process 3D human reconstruction framework, SAT, which seamlessly\nlearns various prior geometries in a unified manner and reconstructs\nhigh-quality textured 3D avatars as the final output. To further facilitate\ngeometry learning, we introduce a Supervisor Feature Regularization module. By\nemploying a multi-view network with the same structure to provide intermediate\nfeatures as training supervision, these varied geometric priors can be better\nfused. To tackle data scarcity and further improve reconstruction quality, we\nalso propose an Online Animation Augmentation module. By building a\none-feed-forward animation network, we augment a massive number of samples from\nthe original 3D human data online for model training. Extensive experiments on\ntwo benchmarks show the superiority of our approach compared to\nstate-of-the-art methods.", "AI": {"tldr": "SAT\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u5b66\u4e60\u591a\u79cd\u51e0\u4f55\u5148\u9a8c\u548c\u5728\u7ebf\u6570\u636e\u589e\u5f3a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u76ee3D\u4eba\u4f53\u91cd\u5efa\u4e2d\u7684\u51e0\u4f55\u6a21\u7cca\u6027\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u5355\u76ee2D\u56fe\u50cf\u4e2d\u7684\u51e0\u4f55\u6a21\u7cca\u6027\u548c3D\u4eba\u4f53\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\u662f\u5f53\u524d3D\u4eba\u4f53\u91cd\u5efa\u9886\u57df\u7684\u4e3b\u8981\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u591a\u79cd\u51e0\u4f55\u5f62\u5f0f\uff0c\u5bfc\u81f4\u89c6\u89d2\u4e0d\u4e00\u81f4\u95ee\u9898\u5982\u9762\u90e8\u626d\u66f2\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u76843D\u4eba\u4f53\u91cd\u5efa\u6846\u67b6SAT\uff0c\u5305\u62ec\u7edf\u4e00\u5b66\u4e60\u591a\u79cd\u51e0\u4f55\u5148\u9a8c\u548c\u9ad8\u8d28\u91cf\u7eb9\u74063D\u5934\u50cf\u91cd\u5efa\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86Supervisor Feature Regularization\u6a21\u5757\u548c\u591a\u89c6\u89d2\u7f51\u7edc\u4ee5\u4f18\u5316\u51e0\u4f55\u5b66\u4e60\uff0c\u4ee5\u53caOnline Animation Augmentation\u6a21\u5757\u6765\u589e\u5f3a\u6570\u636e\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSAT\u6846\u67b6\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SAT\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u7684\u51e0\u4f55\u5b66\u4e60\u548c\u9ad8\u54c1\u8d283D\u5934\u50cf\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee\u7eb9\u74063D\u4eba\u4f53\u91cd\u5efa\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19830", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19830", "abs": "https://arxiv.org/abs/2508.19830", "authors": ["Yilin Zhang", "Cai Xu", "You Wu", "Ziyu Guan", "Wei Zhao"], "title": "Gradient Rectification for Robust Calibration under Distribution Shift", "comment": "14 pages, under review", "summary": "Deep neural networks often produce overconfident predictions, undermining\ntheir reliability in safety-critical applications. This miscalibration is\nfurther exacerbated under distribution shift, where test data deviates from the\ntraining distribution due to environmental or acquisition changes. While\nexisting approaches improve calibration through training-time regularization or\npost-hoc adjustment, their reliance on access to or simulation of target\ndomains limits their practicality in real-world scenarios. In this paper, we\npropose a novel calibration framework that operates without access to target\ndomain information. From a frequency-domain perspective, we identify that\ndistribution shifts often distort high-frequency visual cues exploited by deep\nmodels, and introduce a low-frequency filtering strategy to encourage reliance\non domain-invariant features. However, such information loss may degrade\nIn-Distribution (ID) calibration performance. Therefore, we further propose a\ngradient-based rectification mechanism that enforces ID calibration as a hard\nconstraint during optimization. Experiments on synthetic and real-world shifted\ndatasets, including CIFAR-10/100-C and WILDS, demonstrate that our method\nsignificantly improves calibration under distribution shift while maintaining\nstrong in-distribution performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u76ee\u6807\u57df\u4fe1\u606f\u7684\u65b0\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u9891\u6ee4\u6ce2\u548c\u68af\u5ea6\u6821\u6b63\uff0c\u663e\u8457\u63d0\u5347\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6821\u51c6\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u57df\u5185\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u56e0\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9884\u6d4b\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6821\u51c6\u95ee\u9898\u800c\u53d7\u5230\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u56e0\u4f9d\u8d56\u76ee\u6807\u57df\u4fe1\u606f\u6216\u6a21\u62df\u800c\u5b9e\u7528\u6027\u53d7\u9650\u3002", "method": "\u8bba\u6587\u4ece\u9891\u57df\u89c6\u89d2\u51fa\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u9891\u6ee4\u6ce2\u7b56\u7565\u4ee5\u51cf\u5c11\u5bf9\u9ad8\u9891\u89c6\u89c9\u7ebf\u7d22\u7684\u4f9d\u8d56\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u68af\u5ea6\u7684\u6821\u6b63\u673a\u5236\u6765\u5f3a\u5236\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u57df\u5185\u6821\u51c6\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u504f\u79fb\u6570\u636e\u96c6\uff08\u5982CIFAR-10/100-C\u548cWILDS\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6821\u51c6\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u57df\u5185\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u9891\u6ee4\u6ce2\u7b56\u7565\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u6821\u6b63\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6821\u51c6\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5065\u7684\u57df\u5185\u6027\u80fd\u3002"}}
{"id": "2508.19698", "categories": ["cs.CV", "cs.IT", "math.IT", "math.SP"], "pdf": "https://arxiv.org/pdf/2508.19698", "abs": "https://arxiv.org/abs/2508.19698", "authors": ["V. S. Usatyuk", "D. A. Sapozhnikov", "S. I. Egorov"], "title": "Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators", "comment": "14 pages, 10 figures", "summary": "The rapid advance of deep generative models such as GANs and diffusion\nnetworks now produces images that are virtually indistinguishable from genuine\nphotographs, undermining media forensics and biometric security. Supervised\ndetectors quickly lose effectiveness on unseen generators or after adversarial\npost-processing, while existing unsupervised methods that rely on low-level\nstatistical cues remain fragile. We introduce a physics-inspired,\nmodel-agnostic detector that treats synthetic-image identification as a\ncommunity-detection problem on a sparse weighted graph. Image features are\nfirst extracted with pretrained CNNs and reduced to 32 dimensions, each feature\nvector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities\nare transformed into edge couplings calibrated at the Nishimori temperature,\nproducing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum\nexhibits a characteristic gap when genuine community structure (real images) is\npresent. Synthetic images violate the Nishimori symmetry and therefore lack\nsuch gaps. We validate the approach on binary tasks cat versus dog and male\nversus female using real photos from Flickr-Faces-HQ and CelebA and synthetic\ncounterparts generated by GANs and diffusion models. Without any labeled\nsynthetic data or retraining of the feature extractor, the detector achieves\nover 94% accuracy. Spectral analysis shows multiple well separated gaps for\nreal image sets and a collapsed spectrum for generated ones. Our contributions\nare threefold: a novel LDPC graph construction that embeds deep image features,\nan analytical link between Nishimori temperature RBIM and the Bethe-Hessian\nspectrum providing a Bayes optimal detection criterion; and a practical,\nunsupervised synthetic image detector robust to new generative architectures.\nFuture work will extend the framework to video streams and multi-class anomaly\ndetection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u542f\u53d1\u7684\u65e0\u76d1\u7763\u5408\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u7a00\u758f\u52a0\u6743\u56fe\u7684\u793e\u533a\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u672a\u4f7f\u7528\u6807\u8bb0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740GAN\u548c\u6269\u6563\u7f51\u7edc\u7b49\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u751f\u6210\u7684\u56fe\u50cf\u51e0\u4e4e\u4e0e\u771f\u5b9e\u7167\u7247\u65e0\u6cd5\u533a\u5206\uff0c\u8fd9\u5bf9\u5a92\u4f53\u53d6\u8bc1\u548c\u751f\u7269\u8bc6\u522b\u5b89\u5168\u6784\u6210\u4e86\u6311\u6218\u3002\u73b0\u6709\u7684\u76d1\u7763\u68c0\u6d4b\u5668\u5bf9\u672a\u89c1\u8fc7\u7684\u751f\u6210\u5668\u6216\u5bf9\u6297\u6027\u540e\u5904\u7406\u6548\u679c\u4e0d\u4f73\uff0c\u800c\u65e0\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u4f4e\u5c42\u6b21\u7edf\u8ba1\u7ebf\u7d22\uff0c\u4ecd\u7136\u8106\u5f31\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u8fb9\u7c7b\u578bQC-LDPC\u56fe\u7684\u7a00\u758f\u52a0\u6743\u56fe\u6784\u5efa\u65b9\u6cd5\uff0c\u5c06\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\u5e76\u964d\u7ef4\u81f332\u7ef4\u540e\uff0c\u901a\u8fc7\u6821\u51c6Nishimori\u6e29\u5ea6\u7684\u8fb9\u8026\u5408\u8f6c\u5316\u4e3a\u968f\u673a\u952eIsing\u6a21\u578b\uff08RBIM\uff09\uff0c\u5229\u7528Bethe-Hessian\u8c31\u7684\u7279\u5f81\u95f4\u9699\u533a\u5206\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u732b\u4e0e\u72d7\u3001\u7537\u6027\u4e0e\u5973\u6027\u7684\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528Flickr-Faces-HQ\u548cCelebA\u7684\u771f\u5b9e\u7167\u7247\u53caGAN\u548c\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc794%\u7684\u51c6\u786e\u7387\u3002\u5149\u8c31\u5206\u6790\u663e\u793a\u771f\u5b9e\u56fe\u50cf\u96c6\u5b58\u5728\u591a\u4e2a\u660e\u663e\u5206\u79bb\u7684\u95f4\u9699\uff0c\u800c\u751f\u6210\u56fe\u50cf\u5219\u5448\u73b0\u574d\u584c\u7684\u5149\u8c31\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u542f\u53d1\u7684\u6a21\u578b\u65e0\u5173\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u5c06\u5408\u6210\u56fe\u50cf\u8bc6\u522b\u89c6\u4e3a\u7a00\u758f\u52a0\u6743\u56fe\u4e0a\u7684\u793e\u533a\u68c0\u6d4b\u95ee\u9898\uff0c\u6709\u6548\u533a\u5206\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u5728\u672a\u4f7f\u7528\u6807\u8bb0\u5408\u6210\u6570\u636e\u6216\u91cd\u65b0\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc794%\u7684\u51c6\u786e\u7387\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u6269\u5c55\u81f3\u89c6\u9891\u6d41\u548c\u591a\u7c7b\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2508.19881", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19881", "abs": "https://arxiv.org/abs/2508.19881", "authors": ["Narges Takhtkeshha", "Gabriele Mazzacca", "Fabio Remondino", "Juha Hyypp\u00e4", "Gottfried Mandlburger"], "title": "Multispectral LiDAR data for extracting tree points in urban and suburban areas", "comment": null, "summary": "Monitoring urban tree dynamics is vital for supporting greening policies and\nreducing risks to electrical infrastructure. Airborne laser scanning has\nadvanced large-scale tree management, but challenges remain due to complex\nurban environments and tree variability. Multispectral (MS) light detection and\nranging (LiDAR) improves this by capturing both 3D spatial and spectral data,\nenabling detailed mapping. This study explores tree point extraction using\nMS-LiDAR and deep learning (DL) models. Three state-of-the-art models are\nevaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point\nTransformer V1 (PTv1). Results show the notable time efficiency and accuracy of\nSPT, with a mean intersection over union (mIoU) of 85.28%. The highest\ndetection accuracy is achieved by incorporating pseudo normalized difference\nvegetation index (pNDVI) with spatial data, reducing error rate by 10.61\npercentage points (pp) compared to using spatial information alone. These\nfindings highlight the potential of MS-LiDAR and DL to improve tree extraction\nand further tree inventories.", "AI": {"tldr": "\u591a\u5149\u8c31LiDAR\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff08\u5c24\u5176\u662fSPT\u6a21\u578b\uff09\u663e\u8457\u63d0\u5347\u57ce\u5e02\u6811\u6728\u63d0\u53d6\u7cbe\u5ea6\uff0c\u7ed3\u5408pNDVI\u8fdb\u4e00\u6b65\u4f18\u5316\u7ed3\u679c\u3002", "motivation": "\u57ce\u5e02\u6811\u6728\u76d1\u6d4b\u5bf9\u7eff\u5316\u653f\u7b56\u548c\u7535\u529b\u8bbe\u65bd\u98ce\u9669\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u590d\u6742\u73af\u5883\u548c\u6811\u6728\u591a\u6837\u6027\u5e26\u6765\u6311\u6218\u3002\u591a\u5149\u8c31LiDAR\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u53ef\u63d0\u5347\u6811\u6728\u63d0\u53d6\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08SPT\u3001PTv3\u3001PTv1\uff09\u5728\u591a\u5149\u8c31LiDAR\u6570\u636e\u4e0a\u7684\u6811\u6728\u70b9\u63d0\u53d6\u6027\u80fd\uff0c\u5e76\u6bd4\u8f83\u4e86\u7ed3\u5408pNDVI\u4e0e\u7a7a\u95f4\u6570\u636e\u7684\u6548\u679c\u3002", "result": "SPT\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cmIoU\u8fbe85.28%\u3002\u7ed3\u5408pNDVI\u4e0e\u7a7a\u95f4\u6570\u636e\u53ef\u5c06\u9519\u8bef\u7387\u964d\u4f4e10.61\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u5149\u8c31LiDAR\u4e0e\u6df1\u5ea6\u5b66\u4e60\u5728\u6811\u6728\u63d0\u53d6\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662fSPT\u6a21\u578b\u5728\u65f6\u95f4\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u7684\u4f18\u52bf\uff0c\u4ee5\u53ca\u7ed3\u5408pNDVI\u4e0e\u7a7a\u95f4\u6570\u636e\u53ef\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2508.19699", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19699", "abs": "https://arxiv.org/abs/2508.19699", "authors": ["Yupeng Zhang", "Dezhi Zheng", "Ping Lu", "Han Zhang", "Lei Wang", "Liping xiang", "Cheng Luo", "Kaijun Deng", "Xiaowen Fu", "Linlin Shen", "Jinbao Wang"], "title": "LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation", "comment": "PRCV 2025", "summary": "3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation\nfor 3D scenes, offering both high-fidelity reconstruction and efficient\nrendering. However, 3DGS lacks 3D segmentation ability, which limits its\napplicability in tasks that require scene understanding. The identification and\nisolating of specific object components is crucial. To address this limitation,\nwe propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments\nthe Gaussian representation with object label.LabelGS introduces cross-view\nconsistent semantic masks for 3D Gaussians and employs a novel Occlusion\nAnalysis Model to avoid overfitting occlusion during optimization, Main\nGaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian\nProjection Filter to avoid Gaussian label conflict. Our approach achieves\neffective decoupling of Gaussian representations and refines the 3DGS\noptimization process through a random region sampling strategy, significantly\nimproving efficiency. Extensive experiments demonstrate that LabelGS\noutperforms previous state-of-the-art methods, including Feature-3DGS, in the\n3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup\nin training compared to Feature-3DGS, at a resolution of 1440X1080. Our code\nwill be at https://github.com/garrisonz/LabelGS.", "AI": {"tldr": "LabelGS\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u6807\u8bb0\u548c\u4f18\u5316\u7b56\u7565\uff0c\u589e\u5f3a\u4e863D\u9ad8\u65af\u55b7\u6e85\u7684\u5206\u5272\u80fd\u529b\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "3D\u9ad8\u65af\u55b7\u6e85\uff083DGS\uff09\u7f3a\u4e4f3D\u5206\u5272\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u573a\u666f\u7406\u89e3\u7684\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "LabelGS\u901a\u8fc7\u8de8\u89c6\u56fe\u4e00\u81f4\u7684\u8bed\u4e49\u63a9\u7801\u3001\u906e\u6321\u5206\u6790\u6a21\u578b\u3001\u4e3b\u9ad8\u65af\u6807\u8bb0\u6a21\u578b\u548c\u9ad8\u65af\u6295\u5f71\u6ee4\u6ce2\u5668\u7684\u7ec4\u5408\uff0c\u6539\u8fdb\u4e863D\u9ad8\u65af\u55b7\u6e85\u7684\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "LabelGS\u57283D\u573a\u666f\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982Feature-3DGS\uff09\uff0c\u5e76\u57281440X1080\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u4e8622\u500d\u7684\u8bad\u7ec3\u52a0\u901f\u3002", "conclusion": "LabelGS\u901a\u8fc7\u5f15\u5165\u8de8\u89c6\u56fe\u4e00\u81f4\u7684\u8bed\u4e49\u63a9\u7801\u548c\u521b\u65b0\u7684\u906e\u6321\u5206\u6790\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u55b7\u6e85\u76843D\u5206\u5272\u80fd\u529b\uff0c\u5e76\u5728\u8bad\u7ec3\u6548\u7387\u4e0a\u5b9e\u73b0\u4e8622\u500d\u7684\u52a0\u901f\u3002"}}
{"id": "2508.19705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19705", "abs": "https://arxiv.org/abs/2508.19705", "authors": ["Qiang Hu", "Ying Zhou", "Gepeng Ji", "Nick Barnes", "Qiang Li", "Zhiwei Wang"], "title": "FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation", "comment": null, "summary": "Existing video polyp segmentation (VPS) paradigms usually struggle to balance\nbetween spatiotemporal modeling and domain generalization, limiting their\napplicability in real clinical scenarios. To embrace this challenge, we recast\nthe VPS task as a track-by-detect paradigm that leverages the spatial contexts\ncaptured by the image polyp segmentation (IPS) model while integrating the\ntemporal modeling capabilities of segment anything model 2 (SAM2). However,\nduring long-term polyp tracking in colonoscopy videos, SAM2 suffers from error\naccumulation, resulting in a snowball effect that compromises segmentation\nstability. We mitigate this issue by repurposing SAM2 as a video polyp\nsegmenter with two training-free modules. In particular, the intra-association\nfiltering module eliminates spatial inaccuracies originating from the detecting\nstage, reducing false positives. The inter-association refinement module\nadaptively updates the memory bank to prevent error propagation over time,\nenhancing temporal coherence. Both modules work synergistically to stabilize\nSAM2, achieving cutting-edge performance in both in-domain and out-of-domain\nscenarios. Furthermore, we demonstrate the robust tracking capabilities of\nFreeVPS in long-untrimmed colonoscopy videos, underscoring its potential\nreliable clinical analysis.", "AI": {"tldr": "FreeVPS\u901a\u8fc7\u7ed3\u5408IPS\u548cSAM2\u7684\u4f18\u52bf\uff0c\u5e76\u5f15\u5165\u4e24\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6a21\u5757\uff0c\u89e3\u51b3\u4e86VPS\u4e2d\u7684\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5206\u5272\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u606f\u8089\u5206\u5272\uff08VPS\uff09\u8303\u5f0f\u5728\u65f6\u7a7a\u5efa\u6a21\u548c\u9886\u57df\u6cdb\u5316\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6a21\u5757\uff1a\u5185\u90e8\u5173\u8054\u8fc7\u6ee4\u6a21\u5757\u6d88\u9664\u68c0\u6d4b\u9636\u6bb5\u7684\u7a7a\u95f4\u4e0d\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u5047\u9633\u6027\uff1b\u5916\u90e8\u5173\u8054\u7ec6\u5316\u6a21\u5757\u81ea\u9002\u5e94\u66f4\u65b0\u8bb0\u5fc6\u5e93\u4ee5\u9632\u6b62\u9519\u8bef\u4f20\u64ad\uff0c\u589e\u5f3a\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "result": "FreeVPS\u5728\u57df\u5185\u548c\u57df\u5916\u573a\u666f\u4e2d\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u957f\u65f6\u672a\u526a\u8f91\u7ed3\u80a0\u955c\u89c6\u9891\u4e2d\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8ddf\u8e2a\u80fd\u529b\u3002", "conclusion": "FreeVPS\u901a\u8fc7\u7ed3\u5408IPS\u6a21\u578b\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548cSAM2\u7684\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u606f\u8089\u5206\u5272\u4e2d\u7684\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\uff0c\u5e76\u5728\u957f\u65f6\u672a\u526a\u8f91\u7ed3\u80a0\u955c\u89c6\u9891\u4e2d\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8ddf\u8e2a\u80fd\u529b\uff0c\u5177\u6709\u53ef\u9760\u7684\u4e34\u5e8a\u5206\u6790\u6f5c\u529b\u3002"}}
{"id": "2508.19927", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19927", "abs": "https://arxiv.org/abs/2508.19927", "authors": ["Fayaz Ali", "Muhammad Zawish", "Steven Davy", "Radu Timofte"], "title": "WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution", "comment": "10 pages, 5 figures", "summary": "Transformers have demonstrated promising performance in computer vision\ntasks, including image super-resolution (SR). The quadratic computational\ncomplexity of window self-attention mechanisms in many transformer-based SR\nmethods forces the use of small, fixed windows, limiting the receptive field.\nIn this paper, we propose a new approach by embedding the wavelet transform\nwithin a hierarchical transformer framework, called (WaveHiT-SR). First, using\nadaptive hierarchical windows instead of static small windows allows to capture\nfeatures across different levels and greatly improve the ability to model\nlong-range dependencies. Secondly, the proposed model utilizes wavelet\ntransforms to decompose images into multiple frequency subbands, allowing the\nnetwork to focus on both global and local features while preserving structural\ndetails. By progressively reconstructing high-resolution images through\nhierarchical processing, the network reduces computational complexity without\nsacrificing performance. The multi-level decomposition strategy enables the\nnetwork to capture fine-grained information in lowfrequency components while\nenhancing high-frequency textures. Through extensive experimentation, we\nconfirm the effectiveness and efficiency of our WaveHiT-SR. Our refined\nversions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR\nresults, achieving higher efficiency with fewer parameters, lower FLOPs, and\nfaster speeds.", "AI": {"tldr": "WaveHiT-SR\u7ed3\u5408\u5c0f\u6ce2\u53d8\u6362\u548c\u5206\u5c42Transformer\uff0c\u663e\u8457\u63d0\u5347\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eTransformer\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u56e0\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9650\u5236\u4e86\u611f\u53d7\u91ce\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5d4c\u5165\u5c0f\u6ce2\u53d8\u6362\u7684\u5206\u5c42Transformer\u6846\u67b6\uff08WaveHiT-SR\uff09\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u5206\u5c42\u7a97\u53e3\u548c\u591a\u7ea7\u5206\u89e3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eWaveHiT-SR\u5728\u6027\u80fd\u3001\u6548\u7387\u3001\u53c2\u6570\u6570\u91cf\u548c\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5982SwinIR-Light\u3001SwinIR-NG\u548cSRFormer-Light\u3002", "conclusion": "WaveHiT-SR\u901a\u8fc7\u7ed3\u5408\u5c0f\u6ce2\u53d8\u6362\u548c\u5206\u5c42Transformer\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2508.19730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19730", "abs": "https://arxiv.org/abs/2508.19730", "authors": ["Stelios Mylonas", "Symeon Papadopoulos"], "title": "Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning", "comment": null, "summary": "The increasing realism and accessibility of deepfakes have raised critical\nconcerns about media authenticity and information integrity. Despite recent\nadvances, deepfake detection models often struggle to generalize beyond their\ntraining distributions, particularly when applied to media content found in the\nwild. In this work, we present a robust video deepfake detection framework with\nstrong generalization that takes advantage of the rich facial representations\nlearned by face foundation models. Our method is built on top of FSFM, a\nself-supervised model trained on real face data, and is further fine-tuned\nusing an ensemble of deepfake datasets spanning both face-swapping and\nface-reenactment manipulations. To enhance discriminative power, we incorporate\ntriplet loss variants during training, guiding the model to produce more\nseparable embeddings between real and fake samples. Additionally, we explore\nattribution-based supervision schemes, where deepfakes are categorized by\nmanipulation type or source dataset, to assess their impact on generalization.\nExtensive experiments across diverse evaluation benchmarks demonstrate the\neffectiveness of our approach, especially in challenging real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9762\u90e8\u57fa\u7840\u6a21\u578b\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u591a\u6570\u636e\u96c6\u5fae\u8c03\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6548\u679c\u663e\u8457\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u903c\u771f\u5ea6\u548c\u666e\u53ca\u5ea6\u63d0\u5347\uff0c\u5a92\u4f53\u771f\u5b9e\u6027\u548c\u4fe1\u606f\u5b8c\u6574\u6027\u9762\u4e34\u4e25\u5cfb\u6311\u6218\u3002\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u5728\u8bad\u7ec3\u5206\u5e03\u5916\u7684\u6570\u636e\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u8be5\u65b9\u6cd5\u5efa\u7acb\u5728FSFM\uff08\u81ea\u76d1\u7763\u6a21\u578b\uff09\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\uff08\u5305\u62ec\u6362\u8138\u548c\u9762\u90e8\u91cd\u6f14\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5f15\u5165\u4e09\u5143\u7ec4\u635f\u5931\u53d8\u4f53\u548c\u57fa\u4e8e\u5c5e\u6027\u7684\u76d1\u7763\u65b9\u6848\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u5224\u522b\u80fd\u529b\u3002", "result": "\u5728\u591a\u79cd\u8bc4\u4f30\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u5904\u7406\u6362\u8138\u548c\u9762\u90e8\u91cd\u6f14\u7b49\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u65f6\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u9762\u90e8\u57fa\u7840\u6a21\u578b\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u9762\u90e8\u8868\u793a\u548c\u96c6\u6210\u591a\u79cd\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.19972", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19972", "abs": "https://arxiv.org/abs/2508.19972", "authors": ["Seongheon Park", "Yixuan Li"], "title": "GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity", "comment": null, "summary": "Object hallucination in large vision-language models presents a significant\nchallenge to their safe deployment in real-world applications. Recent works\nhave proposed object-level hallucination scores to estimate the likelihood of\nobject hallucination; however, these methods typically adopt either a global or\nlocal perspective in isolation, which may limit detection reliability. In this\npaper, we introduce GLSim, a novel training-free object hallucination detection\nframework that leverages complementary global and local embedding similarity\nsignals between image and text modalities, enabling more accurate and reliable\nhallucination detection in diverse scenarios. We comprehensively benchmark\nexisting object hallucination detection methods and demonstrate that GLSim\nachieves superior detection performance, outperforming competitive baselines by\na significant margin.", "AI": {"tldr": "GLSim\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u53f7\uff0c\u63d0\u5347\u7269\u4f53\u5e7b\u89c9\u68c0\u6d4b\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u91c7\u7528\u5168\u5c40\u6216\u5c40\u90e8\u89c6\u89d2\u5b64\u7acb\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faGLSim\u6846\u67b6\uff0c\u5229\u7528\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\u4e4b\u95f4\u7684\u5168\u5c40\u548c\u5c40\u90e8\u5d4c\u5165\u76f8\u4f3c\u6027\u4fe1\u53f7\u8fdb\u884c\u8bad\u7ec3\u65e0\u5173\u7684\u7269\u4f53\u5e7b\u89c9\u68c0\u6d4b\u3002", "result": "GLSim\u5728\u591a\u6837\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u53ef\u9760\u7684\u5e7b\u89c9\u68c0\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GLSim\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u5d4c\u5165\u76f8\u4f3c\u6027\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7269\u4f53\u5e7b\u89c9\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.19742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19742", "abs": "https://arxiv.org/abs/2508.19742", "authors": ["Chenguang Liu", "Chisheng Wang", "Yuhua Cai", "Chuanhua Zhu", "Qingquan Li"], "title": "POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection", "comment": null, "summary": "Line segment detection in images has been studied for several decades.\nExisting line segment detectors can be roughly divided into two categories:\ngeneric line segment detectors and wireframe line segment detectors. Generic\nline segment detectors aim to detect all meaningful line segments in images and\ntraditional approaches usually fall into this category. Recent deep learning\nbased approaches are mostly wireframe line segment detectors. They detect only\nline segments that are geometrically meaningful and have large spatial support.\nDue to the difference in the aim of design, the performance of generic line\nsegment detectors for the task of wireframe line segment detection won't be\nsatisfactory, and vice versa. In this work, we propose a robust framework that\ncan be used for both generic line segment detection and wireframe line segment\ndetection. The proposed method is an improved version of the Pixel Orientation\nEstimation (POE) method. It is thus named as POEv2. POEv2 detects line segments\nfrom edge strength maps, and can be combined with any edge detector. We show in\nour experiments that by combining the proposed POEv2 with an efficient edge\ndetector, it achieves state-of-the-art performance on three publicly available\ndatasets.", "AI": {"tldr": "POEv2\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u7ebf\u6bb5\u68c0\u6d4b\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u901a\u7528\u548c\u7ebf\u6846\u7ebf\u6bb5\u68c0\u6d4b\u4efb\u52a1\uff0c\u7ed3\u5408\u9ad8\u6548\u8fb9\u7f18\u68c0\u6d4b\u5668\u540e\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u7ebf\u6bb5\u68c0\u6d4b\u5668\u5206\u4e3a\u901a\u7528\u548c\u7ebf\u6846\u4e24\u7c7b\uff0c\u7531\u4e8e\u8bbe\u8ba1\u76ee\u6807\u4e0d\u540c\uff0c\u5f7c\u6b64\u5728\u5bf9\u65b9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5904\u7406\u901a\u7528\u53c8\u80fd\u5904\u7406\u7ebf\u6846\u7ebf\u6bb5\u68c0\u6d4b\u7684\u9c81\u68d2\u6846\u67b6\u3002", "method": "POEv2\u662f\u50cf\u7d20\u65b9\u5411\u4f30\u8ba1\uff08POE\uff09\u65b9\u6cd5\u7684\u6539\u8fdb\u7248\u672c\uff0c\u4ece\u8fb9\u7f18\u5f3a\u5ea6\u56fe\u4e2d\u68c0\u6d4b\u7ebf\u6bb5\uff0c\u5e76\u53ef\u7ed3\u5408\u4efb\u4f55\u8fb9\u7f18\u68c0\u6d4b\u5668\u4f7f\u7528\u3002", "result": "POEv2\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "POEv2\u6846\u67b6\u5728\u901a\u7528\u548c\u7ebf\u6846\u7ebf\u6bb5\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u7ed3\u5408\u9ad8\u6548\u8fb9\u7f18\u68c0\u6d4b\u5668\u540e\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19746", "abs": "https://arxiv.org/abs/2508.19746", "authors": ["Qiyao Xu", "Qiming Wu", "Xiaowei Li"], "title": "SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection", "comment": null, "summary": "Segment Anything Model (SAM) has demonstrated remarkable capabilities in\nsolving light field salient object detection (LF SOD). However, most existing\nmodels tend to neglect the extraction of prompt information under this task.\nMeanwhile, traditional models ignore the analysis of frequency-domain\ninformation, which leads to small objects being overwhelmed by noise. In this\npaper, we put forward a novel model called self-prompting light field segment\nanything model (SPLF-SAM), equipped with unified multi-scale feature embedding\nblock (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is\ncapable of identifying multiple objects of varying sizes, while MAFA, by\nlearning frequency features, effectively prevents small objects from being\noverwhelmed by noise. Extensive experiments have demonstrated the superiority\nof our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be\navailable at https://github.com/XucherCH/splfsam.", "AI": {"tldr": "SPLF-SAM\u6a21\u578b\u901a\u8fc7UMFEB\u548cMAFA\u6a21\u5757\u63d0\u5347\u5149\u573a\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5149\u573a\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u63d0\u793a\u4fe1\u606f\u63d0\u53d6\u548c\u9891\u57df\u4fe1\u606f\u5206\u6790\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5c0f\u76ee\u6807\u6613\u88ab\u566a\u58f0\u63a9\u76d6\u3002", "method": "\u63d0\u51fa\u4e86SPLF-SAM\u6a21\u578b\uff0c\u5305\u542b\u7edf\u4e00\u591a\u5c3a\u5ea6\u7279\u5f81\u5d4c\u5165\u5757\uff08UMFEB\uff09\u548c\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u8fc7\u6ee4\u9002\u914d\u5668\uff08MAFA\uff09\uff0c\u524d\u8005\u7528\u4e8e\u8bc6\u522b\u4e0d\u540c\u5c3a\u5bf8\u7684\u76ee\u6807\uff0c\u540e\u8005\u901a\u8fc7\u5b66\u4e60\u9891\u7387\u7279\u5f81\u9632\u6b62\u5c0f\u76ee\u6807\u88ab\u566a\u58f0\u6df9\u6ca1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5341\u79cd\u5148\u8fdb\u5149\u573a\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "SPLF-SAM\u6a21\u578b\u901a\u8fc7UMFEB\u548cMAFA\u6a21\u5757\u7684\u5f15\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5149\u573a\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u591a\u5c3a\u5ea6\u76ee\u6807\u8bc6\u522b\u548c\u566a\u58f0\u6291\u5236\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.20064", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20064", "abs": "https://arxiv.org/abs/2508.20064", "authors": ["Philippe Zhang", "Weili Jiang", "Yihao Li", "Jing Zhang", "Sarah Matta", "Yubo Tan", "Hui Lin", "Haoshen Wang", "Jiangtian Pan", "Hui Xu", "Laurent Borderie", "Alexandre Le Guilcher", "B\u00e9atrice Cochener", "Chubin Ou", "Gwenol\u00e9 Quellec", "Mathieu Lamard"], "title": "Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices", "comment": "10 pages, 5 figures, 3 tables, challenge/conference paper", "summary": "Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting\nvisual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments\nhave been effective in slowing the progression of neovascular AMD, with better\noutcomes achieved through timely diagnosis and consistent monitoring. Tracking\nthe progression of neovascular activity in OCT scans of patients with exudative\nAMD allows for the development of more personalized and effective treatment\nplans. This was the focus of the Monitoring Age-related Macular Degeneration\nProgression in Optical Coherence Tomography (MARIO) challenge, in which we\nparticipated. In Task 1, which involved classifying the evolution between two\npairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN\nnetwork with model ensembling to further enhance the model's performance. For\nTask 2, which focused on predicting progression over the next three months\nbased on current exam data, we proposed the Patch Progression Masked\nAutoencoder that generates an OCT for the next exam and then classifies the\nevolution between the current OCT and the one generated using our solution from\nTask 1. The results we achieved allowed us to place in the Top 10 for both\ntasks. Some team members are part of the same organization as the challenge\norganizers; therefore, we are not eligible to compete for the prize.", "AI": {"tldr": "\u901a\u8fc7\u878d\u5408CNN\u548c\u6a21\u578b\u96c6\u6210\u6280\u672f\uff0c\u56e2\u961f\u5728MARIO\u6311\u6218\u8d5b\u4e2d\u6210\u529f\u9884\u6d4bAMD\u8fdb\u5c55\uff0c\u8fdb\u5165\u524d\u5341\u540d\uff0c\u4f46\u56e0\u4e0e\u7ec4\u7ec7\u8005\u5173\u8054\u65e0\u6cd5\u83b7\u5956\u3002", "motivation": "\u53ca\u65f6\u8bca\u65ad\u548c\u6301\u7eed\u76d1\u6d4bAMD\u8fdb\u5c55\u5bf9\u4e8e\u5236\u5b9a\u4e2a\u6027\u5316\u6cbb\u7597\u65b9\u6848\u81f3\u5173\u91cd\u8981\uff0cMARIO\u6311\u6218\u8d5b\u65e8\u5728\u901a\u8fc7OCT\u626b\u63cf\u8ddf\u8e2aAMD\u8fdb\u5c55\u3002", "method": "\u5728Task 1\u4e2d\u4f7f\u7528\u4e86\u878d\u5408CNN\u7f51\u7edc\u4e0e\u6a21\u578b\u96c6\u6210\u6280\u672f\uff1b\u5728Task 2\u4e2d\u63d0\u51fa\u4e86Patch Progression Masked Autoencoder\uff0c\u7528\u4e8e\u751f\u6210\u4e0b\u4e00\u6b21\u68c0\u67e5\u7684OCT\u56fe\u50cf\u5e76\u5206\u7c7b\u8fdb\u5c55\u3002", "result": "\u56e2\u961f\u5728MARIO\u6311\u6218\u8d5b\u7684\u4e24\u9879\u4efb\u52a1\u4e2d\u5747\u8fdb\u5165\u524d\u5341\u540d\uff0c\u4f46\u7531\u4e8e\u90e8\u5206\u6210\u5458\u4e0e\u7ec4\u7ec7\u8005\u540c\u5c5e\u4e00\u4e2a\u673a\u6784\uff0c\u65e0\u6cd5\u53c2\u4e0e\u5956\u9879\u7ade\u4e89\u3002", "conclusion": "\u901a\u8fc7MARIO\u6311\u6218\u8d5b\uff0c\u56e2\u961f\u5728OCT\u626b\u63cf\u4e2d\u6210\u529f\u5e94\u7528\u4e86\u878d\u5408CNN\u7f51\u7edc\u548c\u6a21\u578b\u96c6\u6210\u6280\u672f\uff0c\u4ee5\u53caPatch Progression Masked Autoencoder\uff0c\u6709\u6548\u9884\u6d4b\u4e86AMD\u7684\u8fdb\u5c55\uff0c\u5e76\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u5747\u8fdb\u5165\u524d\u5341\u540d\u3002"}}
{"id": "2508.19754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19754", "abs": "https://arxiv.org/abs/2508.19754", "authors": ["Yue Wu", "Yufan Wu", "Wen Li", "Yuxi Lu", "Kairui Feng", "Xuanhong Chen"], "title": "FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers", "comment": null, "summary": "Despite significant progress in 3D avatar reconstruction, it still faces\nchallenges such as high time complexity, sensitivity to data quality, and low\ndata utilization. We propose FastAvatar, a feedforward 3D avatar framework\ncapable of flexibly leveraging diverse daily recordings (e.g., a single image,\nmulti-view observations, or monocular video) to reconstruct a high-quality 3D\nGaussian Splatting (3DGS) model within seconds, using only a single unified\nmodel. FastAvatar's core is a Large Gaussian Reconstruction Transformer\nfeaturing three key designs: First, a variant VGGT-style transformer\narchitecture aggregating multi-frame cues while injecting initial 3D prompt to\npredict an aggregatable canonical 3DGS representation; Second, multi-granular\nguidance encoding (camera pose, FLAME expression, head pose) mitigating\nanimation-induced misalignment for variable-length inputs; Third, incremental\nGaussian aggregation via landmark tracking and sliced fusion losses.\nIntegrating these features, FastAvatar enables incremental reconstruction,\ni.e., improving quality with more observations, unlike prior work wasting input\ndata. This yields a quality-speed-tunable paradigm for highly usable avatar\nmodeling. Extensive experiments show that FastAvatar has higher quality and\nhighly competitive speed compared to existing methods.", "AI": {"tldr": "FastAvatar\u662f\u4e00\u79cd\u5feb\u901f\u3001\u7075\u6d3b\u76843D avatar\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684Transformer\u8bbe\u8ba1\u548c\u589e\u91cf\u9ad8\u65af\u805a\u5408\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d3D avatar\u91cd\u5efa\u5b58\u5728\u9ad8\u65f6\u95f4\u590d\u6742\u5ea6\u3001\u5bf9\u6570\u636e\u8d28\u91cf\u654f\u611f\u548c\u4f4e\u6570\u636e\u5229\u7528\u7387\u7b49\u95ee\u9898\uff0cFastAvatar\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "FastAvatar\u91c7\u7528\u4e86\u4e00\u79cd\u5927\u578b\u9ad8\u65af\u91cd\u5efaTransformer\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\u53d8\u4f53VGGT-style Transformer\u67b6\u6784\u3001\u591a\u7c92\u5ea6\u5f15\u5bfc\u7f16\u7801\u548c\u589e\u91cf\u9ad8\u65af\u805a\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFastAvatar\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u589e\u91cf\u91cd\u5efa\uff0c\u5373\u968f\u7740\u66f4\u591a\u89c2\u6d4b\u6570\u636e\u7684\u52a0\u5165\uff0c\u91cd\u5efa\u8d28\u91cf\u4f1a\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "FastAvatar\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u76843D avatar\u91cd\u5efa\u6846\u67b6\uff0c\u80fd\u591f\u5728\u591a\u79cd\u8f93\u5165\u6761\u4ef6\u4e0b\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u8d28\u91cf\u548c\u901f\u5ea6\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.19762", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19762", "abs": "https://arxiv.org/abs/2508.19762", "authors": ["Ahmed Emam", "Mohamed Elbassiouny", "Julius Miller", "Patrick Donworth", "Sabine Seidel", "Ribana Roscher"], "title": "BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions", "comment": null, "summary": "Pollinator insects such as honeybees and bumblebees are vital to global food\nproduction and ecosystem stability, yet their populations are declining due to\nincreasing anthropogenic and environmental stressors. To support scalable,\nautomated pollinator monitoring, we introduce BuzzSet, a new large-scale\ndataset of high-resolution pollinator images collected in real agricultural\nfield conditions. BuzzSet contains 7856 manually verified and labeled images,\nwith over 8000 annotated instances across three classes: honeybees, bumblebees,\nand unidentified insects. Initial annotations were generated using a YOLOv12\nmodel trained on external data and refined via human verification using\nopen-source labeling tools. All images were preprocessed into 256~$\\times$~256\ntiles to improve the detection of small insects. We provide strong baselines\nusing the RF-DETR transformer-based object detector. The model achieves high\nF1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively,\nwith confusion matrix results showing minimal misclassification between these\ncategories. The unidentified class remains more challenging due to label\nambiguity and lower sample frequency, yet still contributes useful insights for\nrobustness evaluation. Overall detection quality is strong, with a best\nmAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object\ndetection, class separation under label noise, and ecological computer vision.", "AI": {"tldr": "BuzzSet\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u4f20\u7c89\u6606\u866b\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u652f\u6301\u81ea\u52a8\u5316\u76d1\u6d4b\u3002\u901a\u8fc7YOLOv12\u548cRF-DETR\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5206\u7c7b\u68c0\u6d4b\uff0c\u4e3a\u751f\u6001\u8ba1\u7b97\u673a\u89c6\u89c9\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002", "motivation": "\u652f\u6301\u53ef\u6269\u5c55\u3001\u81ea\u52a8\u5316\u7684\u4f20\u7c89\u6606\u866b\u76d1\u6d4b\uff0c\u4ee5\u5e94\u5bf9\u5168\u7403\u4f20\u7c89\u6606\u866b\u6570\u91cf\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528YOLOv12\u6a21\u578b\u751f\u6210\u521d\u59cb\u6807\u6ce8\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u8fdb\u884c\u7cbe\u70bc\u3002\u6240\u6709\u56fe\u50cf\u9884\u5904\u7406\u4e3a256\u00d7256\u7684\u56fe\u5757\u4ee5\u63d0\u9ad8\u5c0f\u6606\u866b\u7684\u68c0\u6d4b\u6548\u679c\u3002\u63d0\u4f9b\u4e86\u57fa\u4e8eRF-DETR\u53d8\u6362\u5668\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u5f3a\u57fa\u7ebf\u3002", "result": "\u6a21\u578b\u5728\u871c\u8702\u548c\u5927\u9ec4\u8702\u7c7b\u522b\u4e0a\u5206\u522b\u5b9e\u73b0\u4e860.94\u548c0.92\u7684\u9ad8F1\u5206\u6570\uff0c\u6df7\u6dc6\u77e9\u9635\u7ed3\u679c\u663e\u793a\u8fd9\u4e9b\u7c7b\u522b\u4e4b\u95f4\u7684\u8bef\u5206\u7c7b\u6781\u5c11\u3002\u6700\u4f73mAP@0.50\u4e3a0.559\u3002", "conclusion": "BuzzSet\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u5c0f\u7269\u4f53\u68c0\u6d4b\u3001\u6807\u7b7e\u566a\u58f0\u4e0b\u7684\u7c7b\u522b\u5206\u79bb\u4ee5\u53ca\u751f\u6001\u8ba1\u7b97\u673a\u89c6\u89c9\u3002"}}
{"id": "2508.20096", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20096", "abs": "https://arxiv.org/abs/2508.20096", "authors": ["Zeyi Sun", "Yuhang Cao", "Jianze Liang", "Qiushi Sun", "Ziyu Liu", "Zhixiong Zhang", "Yuhang Zang", "Xiaoyi Dong", "Kai Chen", "Dahua Lin", "Jiaqi Wang"], "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning", "comment": "code available at this url: https://github.com/OpenIXCLab/CODA", "summary": "Autonomous agents for Graphical User Interfaces (GUIs) face significant\nchallenges in specialized domains such as scientific computing, where both\nlong-horizon planning and precise execution are required. Existing approaches\nsuffer from a trade-off: generalist agents excel at planning but perform poorly\nin execution, while specialized agents demonstrate the opposite weakness.\nRecent compositional frameworks attempt to bridge this gap by combining a\nplanner and an actor, but they are typically static and non-trainable, which\nprevents adaptation from experience. This is a critical limitation given the\nscarcity of high-quality data in scientific domains. To address these\nlimitations, we introduce CODA, a novel and trainable compositional framework\nthat integrates a generalist planner (Cerebrum) with a specialist executor\n(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,\nSpecialization, we apply a decoupled GRPO approach to train an expert planner\nfor each scientific application individually, bootstrapping from a small set of\ntask trajectories. In the second stage, Generalization, we aggregate all\nsuccessful trajectories from the specialized experts to build a consolidated\ndataset, which is then used for supervised fine-tuning of the final planner.\nThis equips CODA with both robust execution and cross-domain generalization.\nEvaluated on four challenging applications from the ScienceBoard benchmark,\nCODA significantly outperforms baselines and establishes a new state of the art\namong open-source models.", "AI": {"tldr": "CODA\u662f\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u901a\u7528\u89c4\u5212\u5668\u548c\u4e13\u4e1a\u6267\u884c\u5668\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5728\u79d1\u5b66\u8ba1\u7b97GUI\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u79d1\u5b66\u8ba1\u7b97\u9886\u57dfGUI\u81ea\u4e3b\u4ee3\u7406\u5728\u957f\u671f\u89c4\u5212\u548c\u7cbe\u786e\u6267\u884c\u4e0a\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u901a\u7528\u4ee3\u7406\u89c4\u5212\u80fd\u529b\u5f3a\u4f46\u6267\u884c\u5dee\u3001\u4e13\u7528\u4ee3\u7406\u6267\u884c\u5f3a\u4f46\u89c4\u5212\u5f31\u7684\u77db\u76fe\u3002CODA\u65e8\u5728\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\u3002", "method": "CODA\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u901a\u7528\u7684\u89c4\u5212\u5668\uff08Cerebrum\uff09\u548c\u4e00\u4e2a\u4e13\u4e1a\u7684\u6267\u884c\u5668\uff08Cerebellum\uff09\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u662f\u4e13\u4e1a\u5316\uff0c\u901a\u8fc7GRPO\u65b9\u6cd5\u8bad\u7ec3\u9488\u5bf9\u6bcf\u4e2a\u79d1\u5b66\u5e94\u7528\u7684\u4e13\u5bb6\u89c4\u5212\u5668\uff1b\u7b2c\u4e8c\u9636\u6bb5\u662f\u6cdb\u5316\uff0c\u6574\u5408\u6240\u6709\u6210\u529f\u8f68\u8ff9\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\u6700\u7ec8\u89c4\u5212\u5668\u3002", "result": "\u5728ScienceBoard\u57fa\u51c6\u7684\u56db\u4e2a\u6311\u6218\u6027\u5e94\u7528\u4e2d\uff0cCODA\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u6210\u4e3a\u5f00\u6e90\u6a21\u578b\u4e2d\u7684\u65b0\u6807\u6746\u3002", "conclusion": "CODA\u901a\u8fc7\u7ed3\u5408\u901a\u7528\u7684\u89c4\u5212\u5668\u548c\u4e13\u4e1a\u7684\u6267\u884c\u5668\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u7684GUI\u81ea\u4e3b\u4ee3\u7406\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u5e76\u8bbe\u7acb\u4e86\u65b0\u7684\u5f00\u6e90\u6a21\u578b\u6807\u6746\u3002"}}
{"id": "2508.19769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19769", "abs": "https://arxiv.org/abs/2508.19769", "authors": ["Shu Shen", "C. L. Philip Chen", "Tong Zhang"], "title": "AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning", "comment": "13pages,7 figures", "summary": "Multimodal learning has significantly enhanced machine learning performance\nbut still faces numerous challenges and limitations. Imbalanced multimodal\nlearning is one of the problems extensively studied in recent works and is\ntypically mitigated by modulating the learning of each modality. However, we\nfind that these methods typically hinder the dominant modality's learning to\npromote weaker modalities, which affects overall multimodal performance. We\nanalyze the cause of this issue and highlight a commonly overlooked problem:\noptimization bias within networks. To address this, we propose Adaptive\nIntra-Network Modulation (AIM) to improve balanced modality learning. AIM\naccounts for differences in optimization state across parameters and depths\nwithin the network during modulation, achieving balanced multimodal learning\nwithout hindering either dominant or weak modalities for the first time.\nSpecifically, AIM decouples the dominant modality's under-optimized parameters\ninto Auxiliary Blocks and encourages reliance on these performance-degraded\nblocks for joint training with weaker modalities. This approach effectively\nprevents suppression of weaker modalities while enabling targeted optimization\nof under-optimized parameters to improve the dominant modality. Additionally,\nAIM assesses modality imbalance level across network depths and adaptively\nadjusts modulation strength at each depth. Experimental results demonstrate\nthat AIM outperforms state-of-the-art imbalanced modality learning methods\nacross multiple benchmarks and exhibits strong generalizability across\ndifferent backbones, fusion strategies, and optimizers.", "AI": {"tldr": "AIM\u901a\u8fc7\u81ea\u9002\u5e94\u7f51\u7edc\u5185\u8c03\u5236\uff0c\u5e73\u8861\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u4e0d\u6291\u5236\u4efb\u4f55\u6a21\u6001\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6291\u5236\u4e3b\u5bfc\u6a21\u6001\u6765\u63d0\u5347\u5f31\u52bf\u6a21\u6001\uff0c\u5f71\u54cd\u4e86\u6574\u4f53\u591a\u6a21\u6001\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u4f18\u5316\u504f\u5dee\u662f\u4e3b\u8981\u539f\u56e0\uff0c\u63d0\u51faAIM\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faAdaptive Intra-Network Modulation (AIM)\uff0c\u901a\u8fc7\u89e3\u8026\u4e3b\u5bfc\u6a21\u6001\u7684\u672a\u4f18\u5316\u53c2\u6570\u5230Auxiliary Blocks\uff0c\u5e76\u8054\u5408\u8bad\u7ec3\u5f31\u52bf\u6a21\u6001\uff0c\u540c\u65f6\u6839\u636e\u7f51\u7edc\u6df1\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u8c03\u5236\u5f3a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAIM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u4e0d\u5e73\u8861\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u4e0d\u540c\u4e3b\u5e72\u7f51\u7edc\u3001\u878d\u5408\u7b56\u7565\u548c\u4f18\u5316\u5668\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AIM\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u7f51\u7edc\u5185\u53c2\u6570\u7684\u4f18\u5316\u72b6\u6001\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u4e0d\u6291\u5236\u4e3b\u5bfc\u6216\u5f31\u52bf\u6a21\u6001\u7684\u60c5\u51b5\u4e0b\uff0c\u5e73\u8861\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.19773", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19773", "abs": "https://arxiv.org/abs/2508.19773", "authors": ["Jakob Seitz", "Tobias Lengfeld", "Radu Timofte"], "title": "The Return of Structural Handwritten Mathematical Expression Recognition", "comment": null, "summary": "Handwritten Mathematical Expression Recognition is foundational for\neducational technologies, enabling applications like digital note-taking and\nautomated grading. While modern encoder-decoder architectures with large\nlanguage models excel at LaTeX generation, they lack explicit symbol-to-trace\nalignment, a critical limitation for error analysis, interpretability, and\nspatially aware interactive applications requiring selective content updates.\nThis paper introduces a structural recognition approach with two innovations: 1\nan automatic annotation system that uses a neural network to map LaTeX\nequations to raw traces, automatically generating annotations for symbol\nsegmentation, classification, and spatial relations, and 2 a modular structural\nrecognition system that independently optimizes segmentation, classification,\nand relation prediction. By leveraging a dataset enriched with structural\nannotations from our auto-labeling system, the proposed recognition system\ncombines graph-based trace sorting, a hybrid convolutional-recurrent network,\nand transformer-based correction to achieve competitive performance on the\nCROHME-2023 benchmark. Crucially, our structural recognition system generates a\ncomplete graph structure that directly links handwritten traces to predicted\nsymbols, enabling transparent error analysis and interpretable outputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u6807\u6ce8\u548c\u6a21\u5757\u5316\u7cfb\u7edf\u5b9e\u73b0\u7b26\u53f7\u5230\u8f68\u8ff9\u7684\u663e\u5f0f\u5bf9\u9f50\uff0c\u63d0\u5347\u4e86\u9519\u8bef\u5206\u6790\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728CROHME-2023\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u4ee3\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u5728LaTeX\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u7b26\u53f7\u5230\u8f68\u8ff9\u7684\u663e\u5f0f\u5bf9\u9f50\uff0c\u8fd9\u9650\u5236\u4e86\u9519\u8bef\u5206\u6790\u3001\u53ef\u89e3\u91ca\u6027\u53ca\u9700\u8981\u9009\u62e9\u6027\u5185\u5bb9\u66f4\u65b0\u7684\u7a7a\u95f4\u611f\u77e5\u4ea4\u4e92\u5e94\u7528\u3002", "method": "1. \u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u81ea\u52a8\u5c06LaTeX\u65b9\u7a0b\u6620\u5c04\u5230\u539f\u59cb\u8f68\u8ff9\uff0c\u751f\u6210\u7b26\u53f7\u5206\u5272\u3001\u5206\u7c7b\u548c\u7a7a\u95f4\u5173\u7cfb\u7684\u6ce8\u91ca\uff1b2. \u6a21\u5757\u5316\u7ed3\u6784\u8bc6\u522b\u7cfb\u7edf\u72ec\u7acb\u4f18\u5316\u5206\u5272\u3001\u5206\u7c7b\u548c\u5173\u7cfb\u9884\u6d4b\u3002", "result": "\u5728CROHME-2023\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u751f\u6210\u4e86\u5b8c\u6574\u7684\u56fe\u7ed3\u6784\uff0c\u76f4\u63a5\u94fe\u63a5\u624b\u5199\u8f68\u8ff9\u4e0e\u9884\u6d4b\u7b26\u53f7\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u7ed3\u6784\u8bc6\u522b\u65b9\u6cd5\u901a\u8fc7\u81ea\u52a8\u6807\u6ce8\u7cfb\u7edf\u548c\u6a21\u5757\u5316\u7ed3\u6784\u8bc6\u522b\u7cfb\u7edf\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u7b26\u53f7\u5230\u8f68\u8ff9\u7684\u663e\u5f0f\u5bf9\u9f50\uff0c\u4e3a\u9519\u8bef\u5206\u6790\u548c\u53ef\u89e3\u91ca\u8f93\u51fa\u63d0\u4f9b\u4e86\u900f\u660e\u7684\u57fa\u7840\u3002"}}
{"id": "2508.19786", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19786", "abs": "https://arxiv.org/abs/2508.19786", "authors": ["Han Jiao", "Jiakai Sun", "Yexing Xu", "Lei Zhao", "Wei Xing", "Huaizhong Lin"], "title": "MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction", "comment": "8 pages, 9 figures, Anonymous AAAI Submission", "summary": "3D Gaussian Splatting, known for enabling high-quality static scene\nreconstruction with fast rendering, is increasingly being applied to dynamic\nscene reconstruction. A common strategy involves learning a deformation field\nto model the temporal changes of a canonical set of 3D Gaussians. However,\nthese deformation-based methods often produce blurred renderings and lose fine\nmotion details in highly dynamic regions due to the inherent limitations of a\nsingle, unified model in representing diverse motion patterns. To address these\nchallenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian\nSplatting (MAPo), a novel framework for high-fidelity dynamic scene\nreconstruction. Its core is a dynamic score-based partitioning strategy that\ndistinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D\nGaussians, we recursively partition them temporally and duplicate their\ndeformation networks for each new temporal segment, enabling specialized\nmodeling to capture intricate motion details. Concurrently, low-dynamic 3DGs\nare treated as static to reduce computational costs. However, this temporal\npartitioning strategy for high-dynamic 3DGs can introduce visual\ndiscontinuities across frames at the partition boundaries. To address this, we\nintroduce a cross-frame consistency loss, which not only ensures visual\ncontinuity but also further enhances rendering quality. Extensive experiments\ndemonstrate that MAPo achieves superior rendering quality compared to baselines\nwhile maintaining comparable computational costs, particularly in regions with\ncomplex or rapid motions.", "AI": {"tldr": "MAPo\u901a\u8fc7\u52a8\u6001\u5206\u533a\u548c\u8de8\u5e27\u4e00\u81f4\u6027\u635f\u5931\uff0c\u4f18\u5316\u4e86\u52a8\u60013D\u9ad8\u65af\u91cd\u5efa\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53d8\u5f62\u573a\u7684\u65b9\u6cd5\u5728\u9ad8\u5ea6\u52a8\u6001\u533a\u57df\u5bb9\u6613\u4ea7\u751f\u6a21\u7cca\u6e32\u67d3\u548c\u4e22\u5931\u7ec6\u8282\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u7ec6\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u52a8\u6001\u5206\u6570\u5206\u533a\u7b56\u7565\uff0c\u533a\u5206\u9ad8\u52a8\u6001\u548c\u4f4e\u52a8\u60013D\u9ad8\u65af\uff0c\u5e76\u9012\u5f52\u5206\u533a\u9ad8\u52a8\u60013D\u9ad8\u65af\uff0c\u540c\u65f6\u5f15\u5165\u8de8\u5e27\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAPo\u5728\u4fdd\u6301\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "MAPo\u901a\u8fc7\u52a8\u6001\u5206\u533a\u548c\u8de8\u5e27\u4e00\u81f4\u6027\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u91cd\u5efa\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6216\u5feb\u901f\u8fd0\u52a8\u533a\u57df\u3002"}}
{"id": "2508.19789", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19789", "abs": "https://arxiv.org/abs/2508.19789", "authors": ["Xiuchao Wu", "Pengfei Zhu", "Jiangjing Lyu", "Xinguo Liu", "Jie Guo", "Yanwen Guo", "Weiwei Xu", "Chengfei Lyu"], "title": "StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation", "comment": null, "summary": "Recovering material information from images has been extensively studied in\ncomputer graphics and vision. Recent works in material estimation leverage\ndiffusion model showing promising results. However, these diffusion-based\nmethods adopt a multi-step denoising strategy, which is time-consuming for each\nestimation. Such stochastic inference also conflicts with the deterministic\nmaterial estimation task, leading to a high variance estimated results. In this\npaper, we introduce StableIntrinsic, a one-step diffusion model for multi-view\nmaterial estimation that can produce high-quality material parameters with low\nvariance. To address the overly-smoothing problem in one-step diffusion,\nStableIntrinsic applies losses in pixel space, with each loss designed based on\nthe properties of the material. Additionally, StableIntrinsic introduces a\nDetail Injection Network (DIN) to eliminate the detail loss caused by VAE\nencoding, while further enhancing the sharpness of material prediction results.\nThe experimental results indicate that our method surpasses the current\nstate-of-the-art techniques by achieving a $9.9\\%$ improvement in the Peak\nSignal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error\n(MSE) for metallic and roughness by $44.4\\%$ and $60.0\\%$, respectively.", "AI": {"tldr": "StableIntrinsic \u662f\u4e00\u79cd\u4e00\u6b65\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u50cf\u7d20\u7a7a\u95f4\u635f\u5931\u548c DIN \u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u89c6\u89d2\u6750\u6599\u4f30\u8ba1\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728 PSNR \u548c MSE \u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6750\u6599\u4f30\u8ba1\u65b9\u6cd5\u91c7\u7528\u591a\u6b65\u53bb\u566a\u7b56\u7565\uff0c\u8017\u65f6\u4e14\u7ed3\u679c\u65b9\u5dee\u9ad8\uff0c\u4e0e\u786e\u5b9a\u6027\u6750\u6599\u4f30\u8ba1\u4efb\u52a1\u51b2\u7a81\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u65b9\u5dee\u7684\u4e00\u6b65\u6269\u6563\u6a21\u578b\u3002", "method": "StableIntrinsic \u91c7\u7528\u4e00\u6b65\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u50cf\u7d20\u7a7a\u95f4\u635f\u5931\u548c\u7ec6\u8282\u6ce8\u5165\u7f51\u7edc\uff08DIN\uff09\uff0c\u4ee5\u89e3\u51b3\u591a\u6b65\u53bb\u566a\u7b56\u7565\u7684\u65f6\u95f4\u6d88\u8017\u548c\u65b9\u5dee\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cStableIntrinsic \u5728 PSNR \u548c MSE \u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5176\u4e2d albedo \u7684 PSNR \u63d0\u5347\u4e86 9.9%\uff0c\u91d1\u5c5e\u6027\u548c\u7c97\u7cd9\u5ea6\u7684 MSE \u5206\u522b\u964d\u4f4e\u4e86 44.4% \u548c 60.0%\u3002", "conclusion": "StableIntrinsic \u662f\u4e00\u79cd\u4e00\u6b65\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u89c6\u89d2\u6750\u6599\u4f30\u8ba1\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4f4e\u65b9\u5dee\u7684\u6750\u6599\u53c2\u6570\u3002\u901a\u8fc7\u50cf\u7d20\u7a7a\u95f4\u635f\u5931\u548c\u7ec6\u8282\u6ce8\u5165\u7f51\u7edc\uff08DIN\uff09\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fc7\u5ea6\u5e73\u6ed1\u548c\u7ec6\u8282\u4e22\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6750\u6599\u9884\u6d4b\u7684\u6e05\u6670\u5ea6\u3002"}}
{"id": "2508.19791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19791", "abs": "https://arxiv.org/abs/2508.19791", "authors": ["Shay Shomer Chai", "Wenxuan Peng", "Bharath Hariharan", "Hadar Averbuch-Elor"], "title": "Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models", "comment": "Project webpage: https://tau-vailab.github.io/color-edit/", "summary": "Text-to-image generation has recently seen remarkable success, granting users\nwith the ability to create high-quality images through the use of text.\nHowever, contemporary methods face challenges in capturing the precise\nsemantics conveyed by complex multi-object prompts. Consequently, many works\nhave sought to mitigate such semantic misalignments, typically via\ninference-time schemes that modify the attention layers of the denoising\nnetworks. However, prior work has mostly utilized coarse metrics, such as the\ncosine similarity between text and image CLIP embeddings, or human evaluations,\nwhich are challenging to conduct on a larger-scale. In this work, we perform a\ncase study on colors -- a fundamental attribute commonly associated with\nobjects in text prompts, which offer a rich test bed for rigorous evaluation.\nOur analysis reveals that pretrained models struggle to generate images that\nfaithfully reflect multiple color attributes-far more so than with single-color\nprompts-and that neither inference-time techniques nor existing editing methods\nreliably resolve these semantic misalignments. Accordingly, we introduce a\ndedicated image editing technique, mitigating the issue of multi-object\nsemantic alignment for prompts containing multiple colors. We demonstrate that\nour approach significantly boosts performance over a wide range of metrics,\nconsidering images generated by various text-to-image diffusion-based\ntechniques.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u591a\u989c\u8272\u63d0\u793a\u7684\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u7f16\u8f91\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u591a\u5bf9\u8c61\u63d0\u793a\u65f6\u5b58\u5728\u8bed\u4e49\u5bf9\u9f50\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u6d89\u53ca\u591a\u79cd\u989c\u8272\u5c5e\u6027\u65f6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e25\u8c28\u7684\u8bc4\u4f30\u548c\u6539\u8fdb\u6280\u672f\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u672c\u6587\u5bf9\u989c\u8272\u5c5e\u6027\u8fdb\u884c\u4e86\u6848\u4f8b\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5904\u7406\u591a\u989c\u8272\u63d0\u793a\u65f6\u7684\u56f0\u96be\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u7f16\u8f91\u6280\u672f\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u56fe\u50cf\u7f16\u8f91\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u591a\u5bf9\u8c61\u8bed\u4e49\u5bf9\u9f50\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u4e13\u6709\u7684\u56fe\u50cf\u7f16\u8f91\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5bf9\u8c61\u8bed\u4e49\u5bf9\u9f50\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5305\u542b\u591a\u79cd\u989c\u8272\u7684\u6587\u672c\u63d0\u793a\u65f6\u3002"}}
{"id": "2508.19798", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19798", "abs": "https://arxiv.org/abs/2508.19798", "authors": ["Muhammad Ali", "Omar Ali AlSuwaidi"], "title": "FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization", "comment": null, "summary": "In the realm of waste management, automating the sorting process for\nnon-biodegradable materials presents considerable challenges due to the\ncomplexity and variability of waste streams. To address these challenges, we\nintroduce an enhanced neural architecture that builds upon an existing\nEncoder-Decoder structure to improve the accuracy and efficiency of waste\nsorting systems. Our model integrates several key innovations: a Comprehensive\nAttention Block within the decoder, which refines feature representations by\ncombining convolutional and upsampling operations. In parallel, we utilize\nattention through the Mamba architecture, providing an additional performance\nboost. We also introduce a Data Fusion Block that fuses images with more than\nthree channels. To achieve this, we apply PCA transformation to reduce the\ndimensionality while retaining the maximum variance and essential information\nacross three dimensions, which are then used for further processing. We\nevaluated the model on RGB, hyperspectral, multispectral, and a combination of\nRGB and hyperspectral data. The results demonstrate that our approach\noutperforms existing methods by a significant margin.", "AI": {"tldr": "\u901a\u8fc7\u6539\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u63d0\u5347\u5783\u573e\u5206\u62e3\u6548\u7387\uff0c\u7279\u522b\u4f18\u5316\u4e86\u591a\u901a\u9053\u56fe\u50cf\u5904\u7406\uff0c\u6d4b\u8bd5\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u975e\u751f\u7269\u964d\u89e3\u6750\u6599\u7684\u81ea\u52a8\u5206\u62e3\u56e0\u5783\u573e\u6d41\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u9762\u4e34\u6311\u6218\uff0c\u9700\u63d0\u5347\u73b0\u6709\u6280\u672f\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684Encoder-Decoder\u7ed3\u6784\uff0c\u7ed3\u5408\u7efc\u5408\u6ce8\u610f\u529b\u5757\u3001Mamba\u67b6\u6784\u7684\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u53ca\u6570\u636e\u878d\u5408\u5757\uff08\u901a\u8fc7PCA\u964d\u7ef4\u5904\u7406\u591a\u901a\u9053\u56fe\u50cf\uff09\u3002", "result": "\u5728RGB\u3001\u9ad8\u5149\u8c31\u3001\u591a\u5149\u8c31\u53ca\u5176\u7ec4\u5408\u6570\u636e\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u589e\u5f3a\u795e\u7ecf\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u5783\u573e\u5206\u62e3\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u591a\u901a\u9053\u56fe\u50cf\u6570\u636e\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.19806", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.19806", "abs": "https://arxiv.org/abs/2508.19806", "authors": ["Shenqi Wang", "Guangzhi Tang"], "title": "Context-aware Sparse Spatiotemporal Learning for Event-based Vision", "comment": "Accepted at IROS 2025", "summary": "Event-based camera has emerged as a promising paradigm for robot perception,\noffering advantages with high temporal resolution, high dynamic range, and\nrobustness to motion blur. However, existing deep learning-based event\nprocessing methods often fail to fully leverage the sparse nature of event\ndata, complicating their integration into resource-constrained edge\napplications. While neuromorphic computing provides an energy-efficient\nalternative, spiking neural networks struggle to match of performance of\nstate-of-the-art models in complex event-based vision tasks, like object\ndetection and optical flow. Moreover, achieving high activation sparsity in\nneural networks is still difficult and often demands careful manual tuning of\nsparsity-inducing loss terms. Here, we propose Context-aware Sparse\nSpatiotemporal Learning (CSSL), a novel framework that introduces context-aware\nthresholding to dynamically regulate neuron activations based on the input\ndistribution, naturally reducing activation density without explicit sparsity\nconstraints. Applied to event-based object detection and optical flow\nestimation, CSSL achieves comparable or superior performance to\nstate-of-the-art methods while maintaining extremely high neuronal sparsity.\nOur experimental results highlight CSSL's crucial role in enabling efficient\nevent-based vision for neuromorphic processing.", "AI": {"tldr": "CSSL\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u8282\u795e\u7ecf\u5143\u6fc0\u6d3b\u5b9e\u73b0\u9ad8\u7a00\u758f\u5ea6\uff0c\u5728\u4e8b\u4ef6\u89c6\u89c9\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e8b\u4ef6\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u6570\u636e\u7684\u7a00\u758f\u6027\uff0c\u4e14\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e2d\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0d\u8db3\uff0c\u6fc0\u6d3b\u7a00\u758f\u6027\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u63d0\u51fa\u4e86Context-aware Sparse Spatiotemporal Learning (CSSL)\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u9608\u503c\u52a8\u6001\u8c03\u8282\u795e\u7ecf\u5143\u6fc0\u6d3b\uff0c\u51cf\u5c11\u6fc0\u6d3b\u5bc6\u5ea6\u3002", "result": "CSSL\u5728\u4e8b\u4ef6\u9a71\u52a8\u7684\u7269\u4f53\u68c0\u6d4b\u548c\u5149\u6d41\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6781\u9ad8\u7684\u795e\u7ecf\u5143\u7a00\u758f\u5ea6\u3002", "conclusion": "CSSL\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u8282\u795e\u7ecf\u5143\u6fc0\u6d3b\uff0c\u65e0\u9700\u663e\u5f0f\u7a00\u758f\u7ea6\u675f\u5373\u53ef\u5b9e\u73b0\u9ad8\u795e\u7ecf\u5143\u7a00\u758f\u5ea6\uff0c\u5728\u4e8b\u4ef6\u9a71\u52a8\u7684\u7269\u4f53\u68c0\u6d4b\u548c\u5149\u6d41\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u795e\u7ecf\u5f62\u6001\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4e8b\u4ef6\u89c6\u89c9\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19808", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19808", "abs": "https://arxiv.org/abs/2508.19808", "authors": ["Kaixuan Lu", "Mehmet Onurcan Kaya", "Dim P. Papadopoulos"], "title": "AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment", "comment": "Accepted to ICCV 2025 Workshop LIMIT", "summary": "Video Instance Segmentation (VIS) faces significant annotation challenges due\nto its dual requirements of pixel-level masks and temporal consistency labels.\nWhile recent unsupervised methods like VideoCutLER eliminate optical flow\ndependencies through synthetic data, they remain constrained by the\nsynthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised\nframework that bridges this gap through quality-guided self-training. Our\napproach establishes a closed-loop system between pseudo-label generation and\nautomatic quality assessment, enabling progressive adaptation from synthetic to\nreal videos. Experiments demonstrate state-of-the-art performance with 52.6\n$\\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous\nstate-of-the-art VideoCutLER by 4.4$\\%$, while requiring no human annotations.\nThis demonstrates the viability of quality-aware self-training for unsupervised\nVIS. The source code of our method is available at\nhttps://github.com/wcbup/AutoQ-VIS.", "AI": {"tldr": "AutoQ-VIS\u901a\u8fc7\u8d28\u91cf\u5f15\u5bfc\u7684\u81ea\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u65e0\u76d1\u7763\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\uff08VIS\uff09\u7531\u4e8e\u9700\u8981\u50cf\u7d20\u7ea7\u63a9\u7801\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u6807\u7b7e\uff0c\u9762\u4e34\u663e\u8457\u7684\u6807\u6ce8\u6311\u6218\u3002\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u5408\u6210\u6570\u636e\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u5408\u6210\u5230\u771f\u5b9e\u7684\u9886\u57df\u5dee\u8ddd\u3002", "method": "AutoQ-VIS\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u8d28\u91cf\u5f15\u5bfc\u7684\u81ea\u8bad\u7ec3\uff0c\u5728\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u81ea\u52a8\u8d28\u91cf\u8bc4\u4f30\u4e4b\u95f4\u5efa\u7acb\u95ed\u73af\u7cfb\u7edf\uff0c\u9010\u6b65\u9002\u5e94\u4ece\u5408\u6210\u5230\u771f\u5b9e\u89c6\u9891\u7684\u8f6c\u6362\u3002", "result": "\u5728YouTubeVIS-2019\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523052.6 $\\text{AP}_{50}$\uff0c\u8d85\u8d8a\u4e4b\u524d\u7684\u6700\u4f18\u65b9\u6cd5VideoCutLER 4.4%\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002", "conclusion": "AutoQ-VIS\u901a\u8fc7\u8d28\u91cf\u5f15\u5bfc\u7684\u81ea\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u6548\u7f29\u5c0f\u4e86\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u89c6\u9891\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8d28\u91cf\u611f\u77e5\u81ea\u8bad\u7ec3\u5728\u65e0\u76d1\u7763\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.19850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19850", "abs": "https://arxiv.org/abs/2508.19850", "authors": ["Xiaoqi Wang", "Yun Zhang", "Weisi Lin"], "title": "Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models", "comment": null, "summary": "Machine vision systems (MVS) are intrinsically vulnerable to performance\ndegradation under adverse visual conditions. To address this, we propose a\nmachine-centric image quality assessment (MIQA) framework that quantifies the\nimpact of image degradations on MVS performance. We establish an MIQA paradigm\nencompassing the end-to-end assessment workflow. To support this, we construct\na machine-centric image quality database (MIQD-2.5M), comprising 2.5 million\nsamples that capture distinctive degradation responses in both consistency and\naccuracy metrics, spanning 75 vision models, 250 degradation types, and three\nrepresentative vision tasks. We further propose a region-aware MIQA (RA-MIQA)\nmodel to evaluate MVS visual quality through fine-grained spatial degradation\nanalysis. Extensive experiments benchmark the proposed RA-MIQA against seven\nhuman visual system (HVS)-based IQA metrics and five retrained classical\nbackbones. Results demonstrate RA-MIQA's superior performance in multiple\ndimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on\naccuracy for image classification, while also revealing task-specific\ndegradation sensitivities. Critically, HVS-based metrics prove inadequate for\nMVS quality prediction, while even specialized MIQA models struggle with\nbackground degradations, accuracy-oriented estimation, and subtle distortions.\nThis study can advance MVS reliability and establish foundations for\nmachine-centric image processing and optimization. The model and code are\navailable at: https://github.com/XiaoqiWang/MIQA.", "AI": {"tldr": "\u63d0\u51faMIQA\u6846\u67b6\u548cRA-MIQA\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u5e93\u548c\u533a\u57df\u611f\u77e5\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\u7684\u8d28\u91cf\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\u5728\u6076\u52a3\u89c6\u89c9\u6761\u4ef6\u4e0b\u6027\u80fd\u6613\u53d7\u635f\uff0c\u4f20\u7edf\u57fa\u4e8e\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u673a\u5668\u89c6\u89c9\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b250\u4e07\u6837\u672c\u7684MIQD-2.5M\u6570\u636e\u5e93\uff0c\u5e76\u63d0\u51fa\u533a\u57df\u611f\u77e5\u7684RA-MIQA\u6a21\u578b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u9000\u5316\u5206\u6790\u3002", "result": "RA-MIQA\u5728\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5982SRCC\u589e\u76ca\u5206\u522b\u8fbe\u523013.56%\u548c13.37%\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u51faMIQA\u6846\u67b6\u548cRA-MIQA\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\u5728\u6076\u52a3\u89c6\u89c9\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8bc4\u4f30\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4e2d\u5fc3\u7684\u56fe\u50cf\u5904\u7406\u548c\u4f18\u5316\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.19852", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19852", "abs": "https://arxiv.org/abs/2508.19852", "authors": ["Binjie Zhang", "Mike Zheng Shou"], "title": "Ego-centric Predictive Model Conditioned on Hand Trajectories", "comment": "Code: github.com/binjiezhang/Ego-PM (branch: main)", "summary": "In egocentric scenarios, anticipating both the next action and its visual\noutcome is essential for understanding human-object interactions and for\nenabling robotic planning. However, existing paradigms fall short of jointly\nmodeling these aspects. Vision-Language-Action (VLA) models focus on action\nprediction but lack explicit modeling of how actions influence the visual\nscene, while video prediction models generate future frames without\nconditioning on specific actions, often resulting in implausible or\ncontextually inconsistent outcomes. To bridge this gap, we propose a unified\ntwo-stage predictive framework that jointly models action and visual future in\negocentric scenarios, conditioned on hand trajectories. In the first stage, we\nperform consecutive state modeling to process heterogeneous inputs (visual\nobservations, language, and action history) and explicitly predict future hand\ntrajectories. In the second stage, we introduce causal cross-attention to fuse\nmulti-modal cues, leveraging inferred action signals to guide an image-based\nLatent Diffusion Model (LDM) for frame-by-frame future video generation. Our\napproach is the first unified model designed to handle both egocentric human\nactivity understanding and robotic manipulation tasks, providing explicit\npredictions of both upcoming actions and their visual consequences. Extensive\nexperiments on Ego4D, BridgeData, and RLBench demonstrate that our method\noutperforms state-of-the-art baselines in both action prediction and future\nvideo synthesis.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u4f5c\u9884\u6d4b\u4e0e\u89c6\u89c9\u672a\u6765\u751f\u6210\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8054\u5408\u5efa\u6a21\u52a8\u4f5c\u9884\u6d4b\u4e0e\u89c6\u89c9\u672a\u6765\u751f\u6210\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cVLA\u6a21\u578b\u7f3a\u4e4f\u5bf9\u52a8\u4f5c\u5982\u4f55\u5f71\u54cd\u89c6\u89c9\u573a\u666f\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u800c\u89c6\u9891\u9884\u6d4b\u6a21\u578b\u5219\u672a\u8003\u8651\u7279\u5b9a\u52a8\u4f5c\u7684\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u8fde\u7eed\u72b6\u6001\u5efa\u6a21\u5904\u7406\u5f02\u6784\u8f93\u5165\u5e76\u9884\u6d4b\u672a\u6765\u624b\u90e8\u8f68\u8ff9\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u56e0\u679c\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u591a\u6a21\u6001\u7ebf\u7d22\uff0c\u5229\u7528\u63a8\u65ad\u7684\u52a8\u4f5c\u4fe1\u53f7\u6307\u5bfc\u57fa\u4e8e\u56fe\u50cf\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u672a\u6765\u89c6\u9891\u3002", "result": "\u5728Ego4D\u3001BridgeData\u548cRLBench\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u52a8\u4f5c\u9884\u6d4b\u548c\u672a\u6765\u89c6\u9891\u5408\u6210\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4e24\u9636\u6bb5\u9884\u6d4b\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u52a8\u4f5c\u9884\u6d4b\u4e0e\u89c6\u89c9\u672a\u6765\u751f\u6210\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.19862", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19862", "abs": "https://arxiv.org/abs/2508.19862", "authors": ["Long Chen", "Ashiv Patel", "Mengyun Qiao", "Mohammad Yousuf Salmasi", "Salah A. Hammouche", "Vasilis Stavrinides", "Jasleen Nagi", "Soodeh Kalaie", "Xiao Yun Xu", "Wenjia Bai", "Declan P. O'Regan"], "title": "Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction", "comment": null, "summary": "Personalized, accurate prediction of aortic aneurysm progression is essential\nfor timely intervention but remains challenging due to the need to model both\nsubtle local deformations and global anatomical changes within complex 3D\ngeometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh\ngenerative adversarial network for 3D aneurysm growth prediction. MCMeshGAN\nintroduces a dual-branch architecture combining a novel local KNN-based\nconvolutional network (KCN) to preserve fine-grained geometric details and a\nglobal graph convolutional network (GCN) to capture long-range structural\ncontext, overcoming the over-smoothing limitations of deep GCNs. A dedicated\ncondition branch encodes clinical attributes (age, sex) and the target time\ninterval to generate anatomically plausible, temporally controlled predictions,\nenabling retrospective and prospective modeling. We curated TAAMesh, a new\nlongitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal\nrecords (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive\nexperiments demonstrate that MCMeshGAN consistently outperforms\nstate-of-the-art baselines in both geometric accuracy and clinically important\ndiameter estimation. This framework offers a robust step toward clinically\ndeployable, personalized 3D disease trajectory modeling. The source code for\nMCMeshGAN and the baseline methods is publicly available at\nhttps://github.com/ImperialCollegeLondon/MCMeshGAN.", "AI": {"tldr": "MCMeshGAN \u662f\u4e00\u79cd\u7528\u4e8e3D\u52a8\u8109\u7624\u751f\u957f\u9884\u6d4b\u7684\u591a\u6a21\u6001\u6761\u4ef6\u7f51\u683c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7f51\u7edc\u5206\u652f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u9700\u8981\u5728\u590d\u6742\u76843D\u51e0\u4f55\u4e2d\u5efa\u6a21\u7ec6\u5fae\u7684\u5c40\u90e8\u53d8\u5f62\u548c\u5168\u5c40\u89e3\u5256\u53d8\u5316\uff0c\u51c6\u786e\u9884\u6d4b\u4e3b\u52a8\u8109\u7624\u8fdb\u5c55\u5177\u6709\u6311\u6218\u6027\u3002", "method": "MCMeshGAN \u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff0c\u7ed3\u5408\u5c40\u90e8KNN\u5377\u79ef\u7f51\u7edc\uff08KCN\uff09\u548c\u5168\u5c40\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\uff0c\u514b\u670d\u4e86\u6df1\u5ea6GCN\u7684\u8fc7\u5ea6\u5e73\u6ed1\u9650\u5236\uff0c\u5e76\u5f15\u5165\u4e86\u6761\u4ef6\u5206\u652f\u7f16\u7801\u4e34\u5e8a\u5c5e\u6027\u548c\u76ee\u6807\u65f6\u95f4\u95f4\u9694\u3002", "result": "MCMeshGAN \u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u4e34\u5e8a\u91cd\u8981\u7684\u76f4\u5f84\u4f30\u8ba1\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MCMeshGAN \u6846\u67b6\u4e3a\u4e34\u5e8a\u53ef\u90e8\u7f72\u7684\u4e2a\u6027\u53163D\u75be\u75c5\u8f68\u8ff9\u5efa\u6a21\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u4e00\u6b65\u3002"}}
{"id": "2508.19864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19864", "abs": "https://arxiv.org/abs/2508.19864", "authors": ["Oussama Hadjerci", "Antoine Letienne", "Mohamed Abbas Hedjazi", "Adel Hafiane"], "title": "Self-supervised structured object representation learning", "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful technique for\nlearning visual representations. While recent SSL approaches achieve strong\nresults in global image understanding, they are limited in capturing the\nstructured representation in scenes. In this work, we propose a self-supervised\napproach that progressively builds structured visual representations by\ncombining semantic grouping, instance level separation, and hierarchical\nstructuring. Our approach, based on a novel ProtoScale module, captures visual\nelements across multiple spatial scales. Unlike common strategies like DINO\nthat rely on random cropping and global embeddings, we preserve full scene\ncontext across augmented views to improve performance in dense prediction\ntasks. We validate our method on downstream object detection tasks using a\ncombined subset of multiple datasets (COCO and UA-DETRAC). Experimental results\nshow that our method learns object centric representations that enhance\nsupervised object detection and outperform the state-of-the-art methods, even\nwhen trained with limited annotated data and fewer fine-tuning epochs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7ed3\u6784\u5316\u8868\u793a\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u65f6\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u5168\u5c40\u56fe\u50cf\u7406\u89e3\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u573a\u666f\u7ed3\u6784\u5316\u8868\u793a\u7684\u6355\u6349\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eProtoScale\u6a21\u5757\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7a7a\u95f4\u5c3a\u5ea6\u6355\u6349\u89c6\u89c9\u5143\u7d20\uff0c\u5e76\u4fdd\u7559\u5b8c\u6574\u573a\u666f\u4e0a\u4e0b\u6587\u4ee5\u4f18\u5316\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08COCO\u548cUA-DETRAC\uff09\u7684\u4e0b\u6e38\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u5206\u7ec4\u3001\u5b9e\u4f8b\u7ea7\u522b\u5206\u79bb\u548c\u5c42\u6b21\u5316\u7ed3\u6784\uff0c\u6784\u5efa\u4e86\u7ed3\u6784\u5316\u89c6\u89c9\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u548c\u8f83\u5c11\u5fae\u8c03\u5468\u671f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.19866", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19866", "abs": "https://arxiv.org/abs/2508.19866", "authors": ["Fran\u00e7ois G. Landry", "Moulay A. Akhloufi"], "title": "TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations", "comment": "This work has been submitted to IEEE Transactions on Intelligent\n  Vehicles for possible publication", "summary": "With the introduction of vehicles with autonomous capabilities on public\nroads, predicting pedestrian crossing intention has emerged as an active area\nof research. The task of predicting pedestrian crossing intention involves\ndetermining whether pedestrians in the scene are likely to cross the road or\nnot. In this work, we propose TrajFusionNet, a novel transformer-based model\nthat combines future pedestrian trajectory and vehicle speed predictions as\npriors for predicting crossing intention. TrajFusionNet comprises two branches:\na Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM\nbranch learns from a sequential representation of the observed and predicted\npedestrian trajectory and vehicle speed. Complementarily, the VAM branch\nenables learning from a visual representation of the predicted pedestrian\ntrajectory by overlaying predicted pedestrian bounding boxes onto scene images.\nBy utilizing a small number of lightweight modalities, TrajFusionNet achieves\nthe lowest total inference time (including model runtime and data\npreprocessing) among current state-of-the-art approaches. In terms of\nperformance, it achieves state-of-the-art results across the three most\ncommonly used datasets for pedestrian crossing intention prediction.", "AI": {"tldr": "TrajFusionNet\u662f\u4e00\u79cd\u65b0\u578bTransformer\u6a21\u578b\uff0c\u7ed3\u5408\u884c\u4eba\u8f68\u8ff9\u548c\u8f66\u8f86\u901f\u5ea6\u9884\u6d4b\uff0c\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u9884\u6d4b\u884c\u4eba\u8fc7\u8857\u610f\u56fe\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u516c\u5171\u9053\u8def\u4e0a\u7684\u5f15\u5165\uff0c\u9884\u6d4b\u884c\u4eba\u8fc7\u8857\u610f\u56fe\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\u3002", "method": "\u63d0\u51fa\u4e86TrajFusionNet\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u5305\u542b\u5e8f\u5217\u6ce8\u610f\u529b\u6a21\u5757\uff08SAM\uff09\u548c\u89c6\u89c9\u6ce8\u610f\u529b\u6a21\u5757\uff08VAM\uff09\uff0c\u5206\u522b\u5b66\u4e60\u884c\u4eba\u8f68\u8ff9\u548c\u8f66\u8f86\u901f\u5ea6\u7684\u5e8f\u5217\u8868\u793a\u53ca\u89c6\u89c9\u8868\u793a\u3002", "result": "\u5728\u4e09\u4e2a\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5177\u6709\u6700\u4f4e\u7684\u603b\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "TrajFusionNet\u901a\u8fc7\u7ed3\u5408\u672a\u6765\u884c\u4eba\u8f68\u8ff9\u548c\u8f66\u8f86\u901f\u5ea6\u9884\u6d4b\u4f5c\u4e3a\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u884c\u4eba\u8fc7\u8857\u610f\u56fe\u9884\u6d4b\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u95f4\u4e0a\u8868\u73b0\u6700\u4f18\u3002"}}
{"id": "2508.19875", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19875", "abs": "https://arxiv.org/abs/2508.19875", "authors": ["Hui Zhang", "Jianghui Cai", "Haifeng Yang", "Ali Luo", "Yuqing Yang", "Xiao Kong", "Zhichao Ding", "Lichan Zhou", "Qin Han"], "title": "Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network", "comment": null, "summary": "Sky background subtraction is a critical step in Multi-objective Fiber\nspectra process. However, current subtraction relies mainly on sky fiber\nspectra to build Super Sky. These average spectra are lacking in the modeling\nof the environment surrounding the objects. To address this issue, a sky\nbackground estimation model: Sky background building based on Mutual\nInformation (SMI) is proposed. SMI based on mutual information and incremental\ntraining approach. It utilizes spectra from all fibers in the plate to estimate\nthe sky background. SMI contains two main networks, the first network applies a\nwavelength calibration module to extract sky features from spectra, and can\neffectively solve the feature shift problem according to the corresponding\nemission position. The second network employs an incremental training approach\nto maximize mutual information between representations of different spectra to\ncapturing the common component. Then, it minimizes the mutual information\nbetween adjoining spectra representations to obtain individual components. This\nnetwork yields an individual sky background at each location of the object. To\nverify the effectiveness of the method in this paper, we conducted experiments\non the spectra of LAMOST. Results show that SMI can obtain a better object sky\nbackground during the observation, especially in the blue end.", "AI": {"tldr": "SMI\u6a21\u578b\u901a\u8fc7\u4e92\u4fe1\u606f\u548c\u589e\u91cf\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u6240\u6709\u5149\u7ea4\u5149\u8c31\u4f30\u8ba1\u5929\u5149\u80cc\u666f\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u84dd\u7aef\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u7684\u5929\u5149\u80cc\u666f\u4f30\u8ba1\u4e3b\u8981\u4f9d\u8d56\u5929\u7a7a\u5149\u7ea4\u5149\u8c31\u6784\u5efa\u8d85\u7ea7\u5929\u7a7a\uff0c\u7f3a\u4e4f\u5bf9\u76ee\u6807\u5468\u56f4\u73af\u5883\u7684\u5efa\u6a21\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u5929\u5149\u80cc\u666f\u4f30\u8ba1\u6a21\u578bSMI\u3002", "method": "SMI\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u7f51\u7edc\uff1a\u7b2c\u4e00\u4e2a\u7f51\u7edc\u901a\u8fc7\u6ce2\u957f\u6821\u51c6\u6a21\u5757\u63d0\u53d6\u5149\u8c31\u4e2d\u7684\u5929\u5149\u7279\u5f81\uff0c\u89e3\u51b3\u7279\u5f81\u504f\u79fb\u95ee\u9898\uff1b\u7b2c\u4e8c\u4e2a\u7f51\u7edc\u91c7\u7528\u589e\u91cf\u8bad\u7ec3\u65b9\u6cd5\u6700\u5927\u5316\u4e0d\u540c\u5149\u8c31\u8868\u793a\u95f4\u7684\u4e92\u4fe1\u606f\u4ee5\u6355\u83b7\u5171\u540c\u6210\u5206\uff0c\u5e76\u6700\u5c0f\u5316\u76f8\u90bb\u5149\u8c31\u8868\u793a\u95f4\u7684\u4e92\u4fe1\u606f\u4ee5\u83b7\u5f97\u4e2a\u4f53\u6210\u5206\u3002", "result": "\u5728LAMOST\u5149\u8c31\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSMI\u80fd\u591f\u5728\u89c2\u6d4b\u4e2d\u83b7\u5f97\u66f4\u597d\u7684\u76ee\u6807\u5929\u5149\u80cc\u666f\uff0c\u5c24\u5176\u5728\u84dd\u7aef\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "SMI\u6a21\u578b\u901a\u8fc7\u4e92\u4fe1\u606f\u548c\u589e\u91cf\u8bad\u7ec3\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u5929\u5149\u80cc\u666f\uff0c\u5c24\u5176\u5728\u84dd\u7aef\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u591a\u76ee\u6807\u5149\u7ea4\u5149\u8c31\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5929\u5149\u80cc\u666f\u4f30\u8ba1\u3002"}}
{"id": "2508.19895", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19895", "abs": "https://arxiv.org/abs/2508.19895", "authors": ["Ziyun Qian", "Runyu Xiao", "Shuyuan Tu", "Wei Xue", "Dingkang Yang", "Mingcheng Li", "Dongliang Kou", "Minghao Han", "Zizhi Chen", "Lihua Zhang"], "title": "PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos", "comment": null, "summary": "Recent advances in motion generation show remarkable progress. However,\nseveral limitations remain: (1) Existing pose-guided character motion transfer\nmethods merely replicate motion without learning its style characteristics,\nresulting in inexpressive characters. (2) Motion style transfer methods rely\nheavily on motion capture data, which is difficult to obtain. (3) Generated\nmotions sometimes violate physical laws. To address these challenges, this\npaper pioneers a new task: Video-to-Video Motion Personalization. We propose a\nnovel framework, PersonaAnimator, which learns personalized motion patterns\ndirectly from unconstrained videos. This enables personalized motion transfer.\nTo support this task, we introduce PersonaVid, the first video-based\npersonalized motion dataset. It contains 20 motion content categories and 120\nmotion style categories. We further propose a Physics-aware Motion Style\nRegularization mechanism to enforce physical plausibility in the generated\nmotions. Extensive experiments show that PersonaAnimator outperforms\nstate-of-the-art motion transfer methods and sets a new benchmark for the\nVideo-to-Video Motion Personalization task.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa PersonaAnimator \u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u89c6\u9891\u5b66\u4e60\u4e2a\u6027\u5316\u8fd0\u52a8\u6a21\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u98ce\u683c\u8868\u8fbe\u3001\u6570\u636e\u4f9d\u8d56\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u59ff\u52bf\u5f15\u5bfc\u7684\u89d2\u8272\u8fd0\u52a8\u8f6c\u79fb\u65b9\u6cd5\u4ec5\u590d\u5236\u8fd0\u52a8\u800c\u672a\u5b66\u4e60\u5176\u98ce\u683c\u7279\u5f81\uff0c\u5bfc\u81f4\u89d2\u8272\u8868\u73b0\u529b\u4e0d\u8db3\uff1b\u8fd0\u52a8\u98ce\u683c\u8f6c\u79fb\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u96be\u4ee5\u83b7\u53d6\u7684\u8fd0\u52a8\u6355\u6349\u6570\u636e\uff1b\u751f\u6210\u7684\u8fd0\u52a8\u6709\u65f6\u8fdd\u53cd\u7269\u7406\u5b9a\u5f8b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6 PersonaAnimator\uff0c\u76f4\u63a5\u4ece\u65e0\u7ea6\u675f\u89c6\u9891\u4e2d\u5b66\u4e60\u4e2a\u6027\u5316\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5e76\u5f15\u5165\u4e86 Physics-aware Motion Style Regularization \u673a\u5236\u4ee5\u786e\u4fdd\u751f\u6210\u8fd0\u52a8\u7684\u7269\u7406\u5408\u7406\u6027\u3002", "result": "PersonaAnimator \u5728\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u9996\u4e2a\u89c6\u9891\u57fa\u7840\u7684\u4e2a\u6027\u5316\u8fd0\u52a8\u6570\u636e\u96c6 PersonaVid\uff0c\u5305\u542b 20 \u4e2a\u8fd0\u52a8\u5185\u5bb9\u7c7b\u522b\u548c 120 \u4e2a\u8fd0\u52a8\u98ce\u683c\u7c7b\u522b\u3002", "conclusion": "PersonaAnimator \u5728\u89c6\u9891\u5230\u89c6\u9891\u8fd0\u52a8\u4e2a\u6027\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u8fd0\u52a8\u8f6c\u79fb\u65b9\u6cd5\uff0c\u5e76\u4e3a\u6b64\u4efb\u52a1\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2508.19905", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.19905", "abs": "https://arxiv.org/abs/2508.19905", "authors": ["Imad Ali Shah", "Jiarong Li", "Roshan George", "Tim Brophy", "Enda Ward", "Martin Glavin", "Edward Jones", "Brian Deegan"], "title": "Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities", "comment": "Submitted and under review at IEEE OJVT, August 2025", "summary": "Hyperspectral imaging (HSI) offers a transformative sensing modality for\nAdvanced Driver Assistance Systems (ADAS) and autonomous driving (AD)\napplications, enabling material-level scene understanding through fine spectral\nresolution beyond the capabilities of traditional RGB imaging. This paper\npresents the first comprehensive review of HSI for automotive applications,\nexamining the strengths, limitations, and suitability of current HSI\ntechnologies in the context of ADAS/AD. In addition to this qualitative review,\nwe analyze 216 commercially available HSI and multispectral imaging cameras,\nbenchmarking them against key automotive criteria: frame rate, spatial\nresolution, spectral dimensionality, and compliance with AEC-Q100 temperature\nstandards. Our analysis reveals a significant gap between HSI's demonstrated\nresearch potential and its commercial readiness. Only four cameras meet the\ndefined performance thresholds, and none comply with AEC-Q100 requirements. In\naddition, the paper reviews recent HSI datasets and applications, including\nsemantic segmentation for road surface classification, pedestrian separability,\nand adverse weather perception. Our review shows that current HSI datasets are\nlimited in terms of scale, spectral consistency, the number of spectral\nchannels, and environmental diversity, posing challenges for the development of\nperception algorithms and the adequate validation of HSI's true potential in\nADAS/AD applications. This review paper establishes the current state of HSI in\nautomotive contexts as of 2025 and outlines key research directions toward\npractical integration of spectral imaging in ADAS and autonomous systems.", "AI": {"tldr": "HSI\u5728\u6c7d\u8f66\u5e94\u7528\u4e2d\u6f5c\u529b\u5927\u4f46\u5546\u4e1a\u6210\u719f\u5ea6\u4f4e\uff0c\u9700\u89e3\u51b3\u6280\u672f\u548c\u6570\u636e\u96c6\u74f6\u9888\u4ee5\u5b9e\u73b0ADAS/AD\u96c6\u6210\u3002", "motivation": "\u63a2\u7d22HSI\u4f5c\u4e3a\u4e00\u79cd\u8d85\u8d8a\u4f20\u7edfRGB\u6210\u50cf\u7684\u4f20\u611f\u6a21\u6001\uff0c\u5982\u4f55\u5728ADAS/AD\u4e2d\u5b9e\u73b0\u6750\u6599\u7ea7\u573a\u666f\u7406\u89e3\uff0c\u5e76\u8bc4\u4f30\u5176\u5f53\u524d\u6280\u672f\u6210\u719f\u5ea6\u3002", "method": "\u901a\u8fc7\u5b9a\u6027\u7efc\u8ff0\u548c\u5b9a\u91cf\u5206\u6790216\u6b3e\u5546\u7528HSI\u53ca\u591a\u5149\u8c31\u6210\u50cf\u76f8\u673a\uff0c\u8bc4\u4f30\u5176\u5728\u5e27\u7387\u3001\u7a7a\u95f4\u5206\u8fa8\u7387\u3001\u5149\u8c31\u7ef4\u5ea6\u548cAEC-Q100\u6e29\u5ea6\u6807\u51c6\u7b49\u5173\u952e\u6c7d\u8f66\u6307\u6807\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u4ec5\u56db\u6b3e\u76f8\u673a\u6ee1\u8db3\u6027\u80fd\u9608\u503c\uff0c\u4e14\u65e0\u4e00\u7b26\u5408AEC-Q100\u6807\u51c6\uff1b\u73b0\u6709HSI\u6570\u636e\u96c6\u5728\u89c4\u6a21\u3001\u5149\u8c31\u4e00\u81f4\u6027\u548c\u73af\u5883\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u5728\u6c7d\u8f66\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u76ee\u524d\u5546\u4e1a\u6210\u719f\u5ea6\u4e0e\u7814\u53d1\u6f5c\u529b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u5b9e\u73b0\u5176\u5728ADAS/AD\u4e2d\u7684\u5b9e\u9645\u96c6\u6210\u3002"}}
{"id": "2508.19906", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19906", "abs": "https://arxiv.org/abs/2508.19906", "authors": ["Moussa Kassem Sbeyti", "Nadja Klein", "Michelle Karg", "Christian Wirth", "Sahin Albayrak"], "title": "Streamlining the Development of Active Learning Methods in Real-World Object Detection", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Active learning (AL) for real-world object detection faces computational and\nreliability challenges that limit practical deployment. Developing new AL\nmethods requires training multiple detectors across iterations to compare\nagainst existing approaches. This creates high costs for autonomous driving\ndatasets where the training of one detector requires up to 282 GPU hours.\nAdditionally, AL method rankings vary substantially across validation sets,\ncompromising reliability in safety-critical transportation systems. We\nintroduce object-based set similarity ($\\mathrm{OSS}$), a metric that addresses\nthese challenges. $\\mathrm{OSS}$ (1) quantifies AL method effectiveness without\nrequiring detector training by measuring similarity between training sets and\ntarget domains using object-level features. This enables the elimination of\nineffective AL methods before training. Furthermore, $\\mathrm{OSS}$ (2) enables\nthe selection of representative validation sets for robust evaluation. We\nvalidate our similarity-based approach on three autonomous driving datasets\n(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with\ntwo detector architectures (EfficientDet, YOLOv3). This work is the first to\nunify AL training and evaluation strategies in object detection based on object\nsimilarity. $\\mathrm{OSS}$ is detector-agnostic, requires only labeled object\ncrops, and integrates with existing AL pipelines. This provides a practical\nframework for deploying AL in real-world applications where computational\nefficiency and evaluation reliability are critical. Code is available at\nhttps://mos-ks.github.io/publications/.", "AI": {"tldr": "\u63d0\u51faOSS\u5ea6\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u8c61\u76f8\u4f3c\u6027\u91cf\u5316\u4e3b\u52a8\u5b66\u4e60\u6548\u679c\u5e76\u9009\u62e9\u9a8c\u8bc1\u96c6\uff0c\u51cf\u5c11\u8bad\u7ec3\u6210\u672c\uff0c\u63d0\u5347\u8bc4\u4f30\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u4e3b\u52a8\u5b66\u4e60\u5728\u771f\u5b9e\u4e16\u754c\u76ee\u6807\u68c0\u6d4b\u4e2d\u9762\u4e34\u7684\u8ba1\u7b97\u6210\u672c\u548c\u8bc4\u4f30\u53ef\u9760\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff08\u5982\u4e00\u4e2a\u68c0\u6d4b\u5668\u9700282 GPU\u5c0f\u65f6\uff09\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u901a\u8fc7\u5bf9\u8c61\u7ea7\u7279\u5f81\u6d4b\u91cf\u8bad\u7ec3\u96c6\u4e0e\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u5f00\u53d1\u4e86\u5bf9\u8c61\u57fa\u4e8e\u96c6\u76f8\u4f3c\u6027\uff08OSS\uff09\u5ea6\u91cf\u6807\u51c6\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u68c0\u6d4b\u5668\u8bad\u7ec3\uff0c\u4ec5\u9700\u6807\u8bb0\u7684\u5bf9\u8c61\u88c1\u526a\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709\u4e3b\u52a8\u5b66\u4e60\u6d41\u7a0b\u96c6\u6210\u3002", "result": "\u5728KITTI\u3001BDD100K\u548cCODA\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86OSS\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9009\u62e9\u6709\u6548\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u548c\u4ee3\u8868\u6027\u9a8c\u8bc1\u96c6\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u8c61\u76f8\u4f3c\u6027\u7684\u5ea6\u91cf\u65b9\u6cd5\uff08OSS\uff09\uff0c\u7528\u4e8e\u5728\u4e0d\u8bad\u7ec3\u68c0\u6d4b\u5668\u7684\u60c5\u51b5\u4e0b\u91cf\u5316\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u7684\u6548\u679c\uff0c\u5e76\u9009\u62e9\u5177\u6709\u4ee3\u8868\u6027\u7684\u9a8c\u8bc1\u96c6\u8fdb\u884c\u7a33\u5065\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u8bc4\u4f30\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2508.19909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19909", "abs": "https://arxiv.org/abs/2508.19909", "authors": ["Lechun You", "Zhonghua Wu", "Weide Liu", "Xulei Yang", "Jun Cheng", "Wei Zhou", "Bharadwaj Veeravalli", "Guosheng Lin"], "title": "Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation", "comment": null, "summary": "Current methods for 3D semantic segmentation propose training models with\nlimited annotations to address the difficulty of annotating large, irregular,\nand unordered 3D point cloud data. They usually focus on the 3D domain only,\nwithout leveraging the complementary nature of 2D and 3D data. Besides, some\nmethods extend original labels or generate pseudo labels to guide the training,\nbut they often fail to fully use these labels or address the noise within them.\nMeanwhile, the emergence of comprehensive and adaptable foundation models has\noffered effective solutions for segmenting 2D data. Leveraging this\nadvancement, we present a novel approach that maximizes the utility of sparsely\navailable 3D annotations by incorporating segmentation masks generated by 2D\nfoundation models. We further propagate the 2D segmentation masks into the 3D\nspace by establishing geometric correspondences between 3D scenes and 2D views.\nWe extend the highly sparse annotations to encompass the areas delineated by 3D\nmasks, thereby substantially augmenting the pool of available labels.\nFurthermore, we apply confidence- and uncertainty-based consistency\nregularization on augmentations of the 3D point cloud and select the reliable\npseudo labels, which are further spread on the 3D masks to generate more\nlabels. This innovative strategy bridges the gap between limited 3D annotations\nand the powerful capabilities of 2D foundation models, ultimately improving the\nperformance of 3D weakly supervised segmentation.", "AI": {"tldr": "\u5229\u75282D\u57fa\u7840\u6a21\u578b\u751f\u6210\u7684\u63a9\u7801\u6269\u5c553D\u7a00\u758f\u6807\u6ce8\uff0c\u7ed3\u5408\u51e0\u4f55\u5bf9\u5e94\u548c\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u53473D\u5f31\u76d1\u7763\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5f53\u524d3D\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u901a\u5e38\u4ec5\u5173\u6ce83D\u57df\uff0c\u672a\u5145\u5206\u5229\u75282D\u548c3D\u6570\u636e\u7684\u4e92\u8865\u6027\u3002\u6b64\u5916\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6269\u5c55\u6807\u7b7e\u6216\u751f\u6210\u4f2a\u6807\u7b7e\u65f6\u672a\u80fd\u5145\u5206\u5229\u7528\u6216\u5904\u7406\u566a\u58f0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u75282D\u57fa\u7840\u6a21\u578b\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u5bf9\u5e94\u5173\u7cfb\u5c06\u5176\u4f20\u64ad\u52303D\u7a7a\u95f4\u3002\u540c\u65f6\uff0c\u91c7\u7528\u7f6e\u4fe1\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u4e00\u81f4\u6027\u6b63\u5219\u5316\u6765\u7b5b\u9009\u53ef\u9760\u7684\u4f2a\u6807\u7b7e\uff0c\u8fdb\u4e00\u6b65\u6269\u5c55\u6807\u7b7e\u5e93\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u6269\u5c55\u4e86\u53ef\u7528\u6807\u7b7e\u5e93\uff0c\u63d0\u5347\u4e863D\u5f31\u76d1\u7763\u5206\u5272\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u54082D\u57fa\u7840\u6a21\u578b\u751f\u6210\u7684\u63a9\u7801\u548c3D\u7a00\u758f\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5f31\u76d1\u7763\u5206\u5272\u7684\u6027\u80fd\uff0c\u5f25\u5408\u4e86\u6709\u96503D\u6807\u6ce8\u4e0e2D\u6a21\u578b\u5f3a\u5927\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.19944", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19944", "abs": "https://arxiv.org/abs/2508.19944", "authors": ["Taebaek Hwang", "Minseo Kim", "Gisang Lee", "Seonuk Kim", "Hyunjun Eun"], "title": "KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts", "comment": null, "summary": "Understanding and reasoning over text within visual contexts poses a\nsignificant challenge for Vision-Language Models (VLMs), given the complexity\nand diversity of real-world scenarios. To address this challenge, text-rich\nVisual Question Answering (VQA) datasets and benchmarks have emerged for\nhigh-resource languages like English. However, a critical gap persists for\nlow-resource languages such as Korean, where the lack of comprehensive\nbenchmarks hinders robust model evaluation and comparison. To bridge this gap,\nwe introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich\nVQA Attuned to diverse visual contexts. KRETA facilitates an in-depth\nevaluation of both visual text understanding and reasoning capabilities, while\nalso supporting a multifaceted assessment across 15 domains and 26 image types.\nAdditionally, we introduce a semi-automated VQA generation pipeline\nspecifically optimized for text-rich settings, leveraging refined stepwise\nimage decomposition and a rigorous seven-metric evaluation protocol to ensure\ndata quality. While KRETA is tailored for Korean, we hope our adaptable and\nextensible pipeline will facilitate the development of similar benchmarks in\nother languages, thereby accelerating multilingual VLM research. The code and\ndataset for KRETA are available at https://github.com/tabtoyou/KRETA.", "AI": {"tldr": "KRETA\u662f\u4e00\u4e2a\u9488\u5bf9\u97e9\u8bed\u7684\u6587\u672c\u4e30\u5bcc\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\uff0c\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u586b\u8865\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bc4\u4f30\u7a7a\u767d\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u97e9\u8bed\uff09\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u4ee5\u4fc3\u8fdb\u591a\u8bed\u8a00VLM\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86KRETA\u57fa\u51c6\uff0c\u5305\u62ec15\u4e2a\u9886\u57df\u548c26\u79cd\u56fe\u50cf\u7c7b\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u9488\u5bf9\u6587\u672c\u4e30\u5bcc\u73af\u5883\u7684\u534a\u81ea\u52a8\u5316VQA\u751f\u6210\u6d41\u7a0b\uff0c\u7ed3\u5408\u9010\u6b65\u56fe\u50cf\u5206\u89e3\u548c\u4e03\u6307\u6807\u8bc4\u4f30\u534f\u8bae\u3002", "result": "KRETA\u57fa\u51c6\u652f\u6301\u5bf9\u89c6\u89c9\u6587\u672c\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u7684\u6df1\u5165\u8bc4\u4f30\uff0c\u5176\u751f\u6210\u6d41\u7a0b\u786e\u4fdd\u4e86\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u6709\u671b\u63a8\u5e7f\u81f3\u5176\u4ed6\u8bed\u8a00\u3002", "conclusion": "KRETA\u586b\u8865\u4e86\u97e9\u8bed\u5728\u6587\u672c\u4e30\u5bcc\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u4e0a\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u5176\u534a\u81ea\u52a8\u5316\u751f\u6210\u6d41\u7a0b\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u534f\u8bae\uff0c\u4e3a\u591a\u8bed\u8a00VLM\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19946", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2508.19946", "abs": "https://arxiv.org/abs/2508.19946", "authors": ["Gianluca Guzzetta"], "title": "Reimagining Image Segmentation using Active Contour: From Chan Vese Algorithm into a Proposal Novel Functional Loss Framework", "comment": "13 pages", "summary": "In this paper, we present a comprehensive study and analysis of the Chan-Vese\nalgorithm for image segmentation. We employ a discretized scheme derived from\nthe empirical study of the Chan-Vese model's functional energy and its partial\ndifferential equation based on its level set function. We provide a proof of\nthe results and an implementation using MATLAB. Leveraging modern computer\nvision methodologies, we propose a functional segmentation loss based on active\ncontours, utilizing pytorch.nn.ModuleLoss and a level set based on the\nChan-Vese algorithm. We compare our results with common computer vision\nsegmentation datasets and evaluate the performance of classical loss functions\nagainst our proposed method. All code and materials used are available at\nhttps://github.com/gguzzy/chan_vese_functional_loss.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Chan-Vese\u7b97\u6cd5\u7684\u529f\u80fd\u6027\u5206\u5272\u635f\u5931\uff0c\u901a\u8fc7MATLAB\u5b9e\u73b0\u548c\u73b0\u4ee3\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76Chan-Vese\u7b97\u6cd5\u5728\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u7d22\u5176\u529f\u80fd\u6027\u5206\u5272\u635f\u5931\u7684\u6f5c\u529b\uff0c\u5e76\u4e0e\u7ecf\u5178\u635f\u5931\u51fd\u6570\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u91c7\u7528\u79bb\u6563\u5316\u65b9\u6848\uff0c\u57fa\u4e8eChan-Vese\u6a21\u578b\u7684\u529f\u80fd\u80fd\u91cf\u53ca\u5176\u6c34\u5e73\u96c6\u51fd\u6570\u7684\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u7ed3\u5408pytorch.nn.ModuleLoss\u548cChan-Vese\u7b97\u6cd5\u63d0\u51fa\u529f\u80fd\u6027\u5206\u5272\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u529f\u80fd\u6027\u5206\u5272\u635f\u5931\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u7ecf\u5178\u635f\u5931\u51fd\u6570\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u548c\u7406\u8bba\u8bc1\u660e\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8eChan-Vese\u7b97\u6cd5\u7684\u529f\u80fd\u6027\u5206\u5272\u635f\u5931\u5728\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7MATLAB\u5b9e\u73b0\u548c\u73b0\u4ee3\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\u3002"}}
{"id": "2508.19967", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19967", "abs": "https://arxiv.org/abs/2508.19967", "authors": ["Oliver Grainge", "Sania Waheed", "Jack Stilgoe", "Michael Milford", "Shoaib Ehsan"], "title": "Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models", "comment": "Accepted to AAAI Fall Symposium 2025 on AI Trustworthiness and Risk\n  Assessment for Challenging Contexts (ATRACC)", "summary": "Geo-localization is the task of identifying the location of an image using\nvisual cues alone. It has beneficial applications, such as improving disaster\nresponse, enhancing navigation, and geography education. Recently,\nVision-Language Models (VLMs) are increasingly demonstrating capabilities as\naccurate image geo-locators. This brings significant privacy risks, including\nthose related to stalking and surveillance, considering the widespread uses of\nAI models and sharing of photos on social media. The precision of these models\nis likely to improve in the future. Despite these risks, there is little work\non systematically evaluating the geolocation precision of Generative VLMs,\ntheir limits and potential for unintended inferences. To bridge this gap, we\nconduct a comprehensive assessment of the geolocation capabilities of 25\nstate-of-the-art VLMs on four benchmark image datasets captured in diverse\nenvironments. Our results offer insight into the internal reasoning of VLMs and\nhighlight their strengths, limitations, and potential societal risks. Our\nfindings indicate that current VLMs perform poorly on generic street-level\nimages yet achieve notably high accuracy (61\\%) on images resembling social\nmedia content, raising significant and urgent privacy concerns.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e8625\u79cdVLMs\u7684\u5730\u7406\u5b9a\u4f4d\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u793e\u4ea4\u5a92\u4f53\u7c7b\u56fe\u50cf\u4e0a\u51c6\u786e\u7387\u9ad8\uff0861%\uff09\uff0c\u5f15\u53d1\u9690\u79c1\u62c5\u5fe7\uff0c\u4f46\u5728\u901a\u7528\u8857\u666f\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5c3d\u7ba1VLMs\u4f5c\u4e3a\u5730\u7406\u5b9a\u4f4d\u5de5\u5177\u7684\u6f5c\u529b\u5e26\u6765\u9690\u79c1\u98ce\u9669\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u5b9a\u4f4d\u7cbe\u5ea6\u3001\u5c40\u9650\u6027\u548c\u6f5c\u5728\u610f\u5916\u63a8\u65ad\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5bf925\u79cd\u6700\u5148\u8fdb\u7684VLMs\u5728\u56db\u4e2a\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u57fa\u51c6\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "result": "VLMs\u5728\u793e\u4ea4\u5a92\u4f53\u7c7b\u56fe\u50cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0861%\u51c6\u786e\u7387\uff09\uff0c\u4f46\u5728\u901a\u7528\u8857\u666f\u56fe\u50cf\u4e0a\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u901a\u7528\u8857\u666f\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728\u7c7b\u4f3c\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u7684\u56fe\u50cf\u4e0a\u51c6\u786e\u7387\u9ad8\u8fbe61%\uff0c\u5f15\u53d1\u4e86\u7d27\u8feb\u7684\u9690\u79c1\u95ee\u9898\u3002"}}
{"id": "2508.20020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20020", "abs": "https://arxiv.org/abs/2508.20020", "authors": ["Yuhao Chen", "Shubin Chen", "Liang Lin", "Guangrun Wang"], "title": "GS: Generative Segmentation via Label Diffusion", "comment": "12 pages, 7 figures, 5 tables", "summary": "Language-driven image segmentation is a fundamental task in vision-language\nunderstanding, requiring models to segment regions of an image corresponding to\nnatural language expressions. Traditional methods approach this as a\ndiscriminative problem, assigning each pixel to foreground or background based\non semantic alignment. Recently, diffusion models have been introduced to this\ndomain, but existing approaches remain image-centric: they either (i) use image\ndiffusion models as visual feature extractors, (ii) synthesize segmentation\ndata via image generation to train discriminative models, or (iii) perform\ndiffusion inversion to extract attention cues from pre-trained image diffusion\nmodels-thereby treating segmentation as an auxiliary process. In this paper, we\npropose GS (Generative Segmentation), a novel framework that formulates\nsegmentation itself as a generative task via label diffusion. Instead of\ngenerating images conditioned on label maps and text, GS reverses the\ngenerative process: it directly generates segmentation masks from noise,\nconditioned on both the input image and the accompanying language description.\nThis paradigm makes label generation the primary modeling target, enabling\nend-to-end training with explicit control over spatial and semantic fidelity.\nTo demonstrate the effectiveness of our approach, we evaluate GS on Panoptic\nNarrative Grounding (PNG), a representative and challenging benchmark for\nmultimodal segmentation that requires panoptic-level reasoning guided by\nnarrative captions. Experimental results show that GS significantly outperforms\nexisting discriminative and diffusion-based methods, setting a new\nstate-of-the-art for language-driven segmentation.", "AI": {"tldr": "GS\u662f\u4e00\u79cd\u65b0\u578b\u751f\u6210\u5f0f\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u7b7e\u6269\u6563\u76f4\u63a5\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u5728PNG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u5206\u5272\u89c6\u4e3a\u5224\u522b\u4efb\u52a1\uff0c\u800c\u73b0\u6709\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u4ecd\u4ee5\u56fe\u50cf\u4e3a\u4e2d\u5fc3\uff0c\u672a\u5c06\u5206\u5272\u4f5c\u4e3a\u4e3b\u8981\u5efa\u6a21\u76ee\u6807\u3002", "method": "\u63d0\u51faGS\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u7b7e\u6269\u6563\u76f4\u63a5\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u4ee5\u56fe\u50cf\u548c\u8bed\u8a00\u63cf\u8ff0\u4e3a\u6761\u4ef6\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728Panoptic Narrative Grounding\uff08PNG\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGS\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5224\u522b\u6027\u548c\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u3002", "conclusion": "GS\uff08Generative Segmentation\uff09\u901a\u8fc7\u5c06\u5206\u5272\u4efb\u52a1\u672c\u8eab\u5b9a\u4e49\u4e3a\u751f\u6210\u4efb\u52a1\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u8bed\u8a00\u9a71\u52a8\u5206\u5272\u8bbe\u5b9a\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2508.20029", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20029", "abs": "https://arxiv.org/abs/2508.20029", "authors": ["Manogna Sreenivas", "Soma Biswas"], "title": "Segmentation Assisted Incremental Test Time Adaptation in an Open World", "comment": "Accepted at BMVC 2025", "summary": "In dynamic environments, unfamiliar objects and distribution shifts are often\nencountered, which challenge the generalization abilities of the deployed\ntrained models. This work addresses Incremental Test Time Adaptation of Vision\nLanguage Models, tackling scenarios where unseen classes and unseen domains\ncontinuously appear during testing. Unlike traditional Test Time Adaptation\napproaches, where the test stream comes only from a predefined set of classes,\nour framework allows models to adapt simultaneously to both covariate and label\nshifts, actively incorporating new classes as they emerge. Towards this goal,\nwe establish a new benchmark for ITTA, integrating single image TTA methods for\nVLMs with active labeling techniques that query an oracle for samples\npotentially representing unseen classes during test time. We propose a\nsegmentation assisted active labeling module, termed SegAssist, which is\ntraining free and repurposes the segmentation capabilities of VLMs to refine\nactive sample selection, prioritizing samples likely to belong to unseen\nclasses. Extensive experiments on several benchmark datasets demonstrate the\npotential of SegAssist to enhance the performance of VLMs in real world\nscenarios, where continuous adaptation to emerging data is essential.\nProject-page:https://manogna-s.github.io/segassist/", "AI": {"tldr": "\u63d0\u51faSegAssist\u6a21\u5757\uff0c\u7ed3\u5408VLM\u5206\u5272\u80fd\u529b\u548c\u4e3b\u52a8\u6807\u6ce8\uff0c\u63d0\u5347\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u6a21\u578b\u5e38\u9047\u5230\u672a\u77e5\u5bf9\u8c61\u548c\u5206\u5e03\u53d8\u5316\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u6301\u7eed\u51fa\u73b0\u7684\u65b0\u7c7b\u522b\u548c\u9886\u57df\u3002", "method": "\u63d0\u51faSegAssist\u6a21\u5757\uff0c\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e3b\u52a8\u6807\u6ce8\u6280\u672f\u548cVLM\u7684\u5206\u5272\u80fd\u529b\uff0c\u4f18\u5148\u9009\u62e9\u53ef\u80fd\u5c5e\u4e8e\u65b0\u7c7b\u522b\u7684\u6837\u672c\u8fdb\u884c\u6807\u6ce8\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSegAssist\u6709\u6548\u63d0\u5347\u4e86VLM\u5728\u6d4b\u8bd5\u65f6\u5bf9\u65b0\u6570\u636e\u7684\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "SegAssist\u6a21\u5757\u901a\u8fc7\u5229\u7528VLM\u7684\u5206\u5272\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5bf9\u65b0\u7c7b\u522b\u548c\u9886\u57df\u7684\u9002\u5e94\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.20063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20063", "abs": "https://arxiv.org/abs/2508.20063", "authors": ["Peng-Hao Hsu", "Ke Zhang", "Fu-En Wang", "Tao Tu", "Ming-Feng Li", "Yu-Lun Liu", "Albert Y. C. Chen", "Min Sun", "Cheng-Hao Kuo"], "title": "OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations", "comment": "ICCV2025", "summary": "Open-vocabulary (OV) 3D object detection is an emerging field, yet its\nexploration through image-based methods remains limited compared to 3D point\ncloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view\nindoor 3D object detector trained without human annotations. In particular,\nOpenM3D is a single-stage detector adapting the 2D-induced voxel features from\nthe ImGeoNet model. To support OV, it is jointly trained with a class-agnostic\n3D localization loss requiring high-quality 3D pseudo boxes and a\nvoxel-semantic alignment loss requiring diverse pre-trained CLIP features. We\nfollow the training setting of OV-3DET where posed RGB-D images are given but\nno human annotations of 3D boxes or classes are available. We propose a 3D\nPseudo Box Generation method using a graph embedding technique that combines 2D\nsegments into coherent 3D structures. Our pseudo-boxes achieve higher precision\nand recall than other methods, including the method proposed in OV-3DET. We\nfurther sample diverse CLIP features from 2D segments associated with each\ncoherent 3D structure to align with the corresponding voxel feature. The key to\ntraining a highly accurate single-stage detector requires both losses to be\nlearned toward high-quality targets. At inference, OpenM3D, a highly efficient\ndetector, requires only multi-view images for input and demonstrates superior\naccuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor\nbenchmarks compared to existing methods. We outperform a strong two-stage\nmethod that leverages our class-agnostic detector with a ViT CLIP-based OV\nclassifier and a baseline incorporating multi-view depth estimator on both\naccuracy and speed.", "AI": {"tldr": "OpenM3D\u662f\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u5f00\u8bcd\u6c47\u591a\u89c6\u89d2\u5ba4\u51853D\u7269\u4f53\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u5355\u9636\u6bb5\u68c0\u6d4b\u548c\u8054\u5408\u8bad\u7ec3\uff0c\u5728\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u57fa\u4e8e\u56fe\u50cf\u7684\u5f00\u8bcd\u6c47\uff08OV\uff093D\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5f25\u8865\u5176\u57283D\u70b9\u4e91\u65b9\u6cd5\u4e2d\u7684\u4e0d\u8db3\u3002", "method": "OpenM3D\u91c7\u7528\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\uff0c\u7ed3\u54082D\u8bf1\u5bfc\u7684\u4f53\u7d20\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u7c7b\u65e0\u5173\u76843D\u5b9a\u4f4d\u635f\u5931\u548c\u4f53\u7d20\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\u3002", "result": "OpenM3D\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc73D\u4f2a\u6846\u751f\u6210\u548c\u591a\u6837\u5316CLIP\u7279\u5f81\u91c7\u6837\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\u8bad\u7ec3\u3002", "conclusion": "OpenM3D\u5728ScanNet200\u548cARKitScenes\u5ba4\u5185\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\uff08\u6bcf\u573a\u666f0.3\u79d2\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.20066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20066", "abs": "https://arxiv.org/abs/2508.20066", "authors": ["Zheng Li", "Yanming Guo", "WenZhe Liu", "Xueyi Zhang", "Zhaoyun Ding", "Long Xu", "Mingrui Lao"], "title": "PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence", "comment": "10 pages", "summary": "Cross-view geo-localization is a critical task for UAV navigation, event\ndetection, and aerial surveying, as it enables matching between drone-captured\nand satellite imagery. Most existing approaches embed multi-modal data into a\njoint feature space to maximize the similarity of paired images. However, these\nmethods typically assume perfect alignment of image pairs during training,\nwhich rarely holds true in real-world scenarios. In practice, factors such as\nurban canyon effects, electromagnetic interference, and adverse weather\nfrequently induce GPS drift, resulting in systematic alignment shifts where\nonly partial correspondences exist between pairs. Despite its prevalence, this\nsource of noisy correspondence has received limited attention in current\nresearch. In this paper, we formally introduce and address the Noisy\nCorrespondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to\nbridge the gap between idealized benchmarks and practical applications. To this\nend, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a\nnovel framework that partitions and augments training data based on estimated\ndata uncertainty through uncertainty-aware co-augmentation and evidential\nco-training. Specifically, PAUL selectively augments regions with high\ncorrespondence confidence and utilizes uncertainty estimation to refine feature\nlearning, effectively suppressing noise from misaligned pairs. Distinct from\ntraditional filtering or label correction, PAUL leverages both data uncertainty\nand loss discrepancy for targeted partitioning and augmentation, thus providing\nrobust supervision for noisy samples. Comprehensive experiments validate the\neffectiveness of individual components in PAUL,which consistently achieves\nsuperior performance over other competitive noisy-correspondence-driven methods\nin various noise ratios.", "AI": {"tldr": "PAUL\u6846\u67b6\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\u89e3\u51b3\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u4e2d\u7684\u566a\u58f0\u5bf9\u5e94\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u8bad\u7ec3\u56fe\u50cf\u5bf9\u5b8c\u7f8e\u5bf9\u9f50\uff0c\u4f46\u5b9e\u9645\u573a\u666f\u4e2d\u56e0GPS\u6f02\u79fb\u7b49\u56e0\u7d20\u5bfc\u81f4\u566a\u58f0\u5bf9\u5e94\u666e\u904d\u5b58\u5728\uff0c\u4e9f\u9700\u89e3\u51b3\u4ee5\u63d0\u5347\u6a21\u578b\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faPAUL\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u534f\u540c\u589e\u5f3a\u548c\u8bc1\u636e\u534f\u540c\u8bad\u7ec3\uff0c\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u5206\u533a\u548c\u589e\u5f3a\uff0c\u4f18\u5316\u7279\u5f81\u5b66\u4e60\u5e76\u6291\u5236\u566a\u58f0\u5f71\u54cd\u3002", "result": "PAUL\u5728\u591a\u79cd\u566a\u58f0\u6bd4\u4f8b\u4e0b\u5747\u4f18\u4e8e\u5176\u4ed6\u566a\u58f0\u5bf9\u5e94\u9a71\u52a8\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "conclusion": "PAUL\u6846\u67b6\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6570\u636e\u5206\u533a\u548c\u589e\u5f3a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u4e2d\u7684\u566a\u58f0\u5bf9\u5e94\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.20088", "categories": ["cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.20088", "abs": "https://arxiv.org/abs/2508.20088", "authors": ["Yuxin Guo", "Teng Wang", "Yuying Ge", "Shijie Ma", "Yixiao Ge", "Wei Zou", "Ying Shan"], "title": "AudioStory: Generating Long-Form Narrative Audio with Large Language Models", "comment": null, "summary": "Recent advances in text-to-audio (TTA) generation excel at synthesizing short\naudio clips but struggle with long-form narrative audio, which requires\ntemporal coherence and compositional reasoning. To address this gap, we propose\nAudioStory, a unified framework that integrates large language models (LLMs)\nwith TTA systems to generate structured, long-form audio narratives. AudioStory\npossesses strong instruction-following reasoning generation capabilities. It\nemploys LLMs to decompose complex narrative queries into temporally ordered\nsub-tasks with contextual cues, enabling coherent scene transitions and\nemotional tone consistency. AudioStory has two appealing features: (1)\nDecoupled bridging mechanism: AudioStory disentangles LLM-diffuser\ncollaboration into two specialized components, i.e., a bridging query for\nintra-event semantic alignment and a residual query for cross-event coherence\npreservation. (2) End-to-end training: By unifying instruction comprehension\nand audio generation within a single end-to-end framework, AudioStory\neliminates the need for modular training pipelines while enhancing synergy\nbetween components. Furthermore, we establish a benchmark AudioStory-10K,\nencompassing diverse domains such as animated soundscapes and natural sound\nnarratives. Extensive experiments show the superiority of AudioStory on both\nsingle-audio generation and narrative audio generation, surpassing prior TTA\nbaselines in both instruction-following ability and audio fidelity. Our code is\navailable at https://github.com/TencentARC/AudioStory", "AI": {"tldr": "AudioStory\u662f\u4e00\u4e2a\u6574\u5408LLM\u548cTTA\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u8fde\u8d2f\u7684\u957f\u97f3\u9891\u53d9\u4e8b\uff0c\u901a\u8fc7\u89e3\u8026\u6865\u63a5\u548c\u7aef\u5230\u7aef\u8bad\u7ec3\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684TTA\u751f\u6210\u6280\u672f\u5728\u77ed\u97f3\u9891\u526a\u8f91\u5408\u6210\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u957f\u53d9\u4e8b\u97f3\u9891\u751f\u6210\u65b9\u9762\u5b58\u5728\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u7ec4\u5408\u63a8\u7406\u7684\u6311\u6218\u3002", "method": "AudioStory\u91c7\u7528\u89e3\u8026\u7684\u6865\u63a5\u673a\u5236\u548c\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06LLM\u4e0eTTA\u7cfb\u7edf\u7684\u534f\u4f5c\u5206\u4e3a\u4e24\u4e2a\u4e13\u95e8\u7ec4\u4ef6\uff1a\u6865\u63a5\u67e5\u8be2\uff08\u7528\u4e8e\u4e8b\u4ef6\u5185\u8bed\u4e49\u5bf9\u9f50\uff09\u548c\u6b8b\u5dee\u67e5\u8be2\uff08\u7528\u4e8e\u8de8\u4e8b\u4ef6\u8fde\u8d2f\u6027\u4fdd\u6301\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAudioStory\u5728\u5355\u97f3\u9891\u751f\u6210\u548c\u53d9\u4e8b\u97f3\u9891\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709TTA\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u548c\u97f3\u9891\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "AudioStory\u901a\u8fc7\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u6587\u672c\u5230\u97f3\u9891\uff08TTA\uff09\u7cfb\u7edf\uff0c\u6210\u529f\u751f\u6210\u4e86\u7ed3\u6784\u5316\u7684\u957f\u97f3\u9891\u53d9\u4e8b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53d9\u4e8b\u97f3\u9891\u7684\u8fde\u8d2f\u6027\u548c\u60c5\u611f\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.20089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20089", "abs": "https://arxiv.org/abs/2508.20089", "authors": ["Ross J Gardiner", "Guillaume Mougeot", "Sareh Rowlands", "Benno I Simmons", "Flemming Helsing", "Toke Thomas H\u00f8ye"], "title": "Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors", "comment": null, "summary": "Labelling images of Lepidoptera (moths) from automated camera systems is\nvital for understanding insect declines. However, accurate species\nidentification is challenging due to domain shifts between curated images and\nnoisy field imagery. We propose a lightweight classification approach,\ncombining limited expert-labelled field data with knowledge distillation from\nthe high-performance BioCLIP2 foundation model into a ConvNeXt-tiny\narchitecture. Experiments on 101 Danish moth species from AMI camera systems\ndemonstrate that BioCLIP2 substantially outperforms other methods and that our\ndistilled lightweight model achieves comparable accuracy with significantly\nreduced computational cost. These insights offer practical guidelines for the\ndevelopment of efficient insect monitoring systems and bridging domain gaps for\nfine-grained classification.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u5206\u7c7b\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e13\u5bb6\u6807\u8bb0\u6570\u636e\u548cBioCLIP2\u77e5\u8bc6\u84b8\u998f\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u52a9\u529b\u6606\u866b\u76d1\u6d4b\u3002", "motivation": "\u7531\u4e8e\u7b56\u5212\u56fe\u50cf\u4e0e\u5608\u6742\u91ce\u5916\u56fe\u50cf\u4e4b\u95f4\u7684\u9886\u57df\u8f6c\u79fb\uff0c\u51c6\u786e\u8bc6\u522b\u86fe\u7c7b\u7269\u79cd\u5177\u6709\u6311\u6218\u6027\uff0c\u8fd9\u5bf9\u7406\u89e3\u6606\u866b\u51cf\u5c11\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u6709\u9650\u7684\u4e13\u5bb6\u6807\u8bb0\u91ce\u5916\u6570\u636e\u548c\u4ece\u9ad8\u6027\u80fdBioCLIP2\u57fa\u7840\u6a21\u578b\u5230ConvNeXt-tiny\u67b6\u6784\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5206\u7c7b\u65b9\u6cd5\u3002", "result": "\u5728101\u79cd\u4e39\u9ea6\u86fe\u7c7b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBioCLIP2\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4e14\u8f7b\u91cf\u7ea7\u84b8\u998f\u6a21\u578b\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u6bd4\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u9ad8\u6548\u7684\u6606\u866b\u76d1\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728\u4fdd\u8bc1\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
