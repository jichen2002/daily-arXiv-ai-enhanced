<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 144]
- [cs.AI](#cs.AI) [Total: 50]
- [cs.SE](#cs.SE) [Total: 27]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.DC](#cs.DC) [Total: 17]
- [cs.RO](#cs.RO) [Total: 54]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.NI](#cs.NI) [Total: 24]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Real-Time Diminished Reality Approach to Privacy in MR Collaboration](https://arxiv.org/abs/2509.10466)
*Christian Fane*

Main category: cs.CV

TL;DR: 该论文提出了一种实时减实系统，用于混合现实会议中的隐私控制，通过语义分割和实时修复技术移除敏感对象，并展示了其可行性。


<details>
  <summary>Details</summary>
Motivation: 设计该系统是为了在共享空间混合现实会议中实现隐私控制，允许主要头戴设备用户选择性移除个人或敏感物品，确保其他参与者无法看到这些对象。

Method: 系统通过语义分割和精确对象选择实现对象移除，使用ZED 2i深度摄像头进行实时修复，采用YOLOv11进行对象检测和修改后的DSTT模型进行高质量视频修复。

Result: 系统在720p分辨率下能够维持超过20 fps的帧率，展示了实时减实技术的可行性。

Conclusion: 该研究展示了一种实时、基于修复的DR系统，能够在共享空间混合现实会议中实现隐私控制，证明了实时减实技术在实际隐私保护MR应用中的可行性。

Abstract: Diminished reality (DR) refers to the digital removal of real-world objects
by compositing background content in their place. This thesis presents a
real-time, inpainting-based DR system designed to enable privacy control in
shared-space mixed reality (MR) meetings. The system allows a primary headset
user to selectively remove personal or sensitive items from their environment,
ensuring that those objects are no longer visible to other participants.
Removal is achieved through semantic segmentation and precise object selection,
followed by real-time inpainting from the viewpoint of a secondary observer,
implemented using a mobile ZED 2i depth camera. The solution is designed to be
portable and robust, requiring neither a fixed secondary viewpoint nor prior 3D
scanning of the environment. The system utilises YOLOv11 for object detection
and a modified Decoupled Spatial-Temporal Transformer (DSTT) model for
high-quality video inpainting. At 720p resolution, the pipeline sustains frame
rates exceeding 20 fps, demonstrating the feasibility of real-time diminished
reality for practical privacy-preserving MR applications.

</details>


### [2] [SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning](https://arxiv.org/abs/2509.10555)
*Alejandra Perez,Chinedu Nwoye,Ramtin Raji Kermani,Omid Mohareri,Muhammad Abdullah Jamal*

Main category: cs.CV

TL;DR: 提出了SurgLaVi，最大且最多样的手术视觉-语言数据集，并开发了SurgCLIP框架，显著提升了手术任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的手术视觉-语言预训练数据集在规模、程序多样性、语义质量和层次结构方面存在局限，限制了手术VLP的发展。

Method: 提出了SurgLaVi数据集，通过全自动流水线生成细粒度的手术视频转录，并采用双模态过滤确保高质量标注。同时，引入了SurgCLIP，一种CLIP风格的视频-文本对比框架。

Result: SurgCLIP在阶段、步骤、动作和工具识别任务上表现优异，显著超越现有最佳方法。

Conclusion: SurgLaVi和SurgCLIP的提出验证了大规模、语义丰富且层次结构化的数据集能够直接转化为更强、更通用的表示，确立了SurgLaVi作为开发手术基础模型的关键资源。

Abstract: Vision-language pre-training (VLP) offers unique advantages for surgery by
aligning language with surgical videos, enabling workflow understanding and
transfer across tasks without relying on expert-labeled datasets. However,
progress in surgical VLP remains constrained by the limited scale, procedural
diversity, semantic quality, and hierarchical structure of existing datasets.
In this work, we present SurgLaVi, the largest and most diverse surgical
vision-language dataset to date, comprising nearly 240k clip-caption pairs from
more than 200 procedures, and comprising hierarchical levels at phase-, step-,
and task-level. At the core of SurgLaVi lies a fully automated pipeline that
systematically generates fine-grained transcriptions of surgical videos and
segments them into coherent procedural units. To ensure high-quality
annotations, it applies dual-modality filtering to remove irrelevant and noisy
samples. Within this framework, the resulting captions are enriched with
contextual detail, producing annotations that are both semantically rich and
easy to interpret. To ensure accessibility, we release SurgLaVi-\b{eta}, an
open-source derivative of 113k clip-caption pairs constructed entirely from
public data, which is over four times larger than existing surgical VLP
datasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP,
a CLIP-style video-text contrastive framework with dual encoders, as a
representative base model. SurgCLIP achieves consistent improvements across
phase, step, action, and tool recognition, surpassing prior state-of-the-art
methods, often by large margins. These results validate that large-scale,
semantically rich, and hierarchically structured datasets directly translate
into stronger and more generalizable representations, establishing SurgLaVi as
a key resource for developing surgical foundation models.

</details>


### [3] [Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses](https://arxiv.org/abs/2509.10620)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 提出了一种基于SimCLR的高分辨率3D脑MRI自监督学习基础模型，在多种任务中表现优异，并开源了代码和模型。


<details>
  <summary>Details</summary>
Motivation: 当前3D MRI分析模型多为特定任务设计，缺乏泛化能力；自监督学习在2D医学影像中成功应用，但在3D脑MRI中仍有限制。

Method: 采用SimCLR-based自监督学习方法，预训练于11个公开数据集的18,759名患者（44,958次扫描），涵盖多种神经系统疾病。

Result: 模型在四种下游任务中均优于其他方法，包括使用仅20%标记数据的阿尔茨海默病预测任务。

Conclusion: 本文提出了一个通用的、高分辨率的SimCLR-based自监督学习基础模型，用于3D脑结构MRI分析，并在多种下游任务中表现出色。

Abstract: 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly
acquired in clinical settings to monitor a wide range of neurological
conditions, including neurodegenerative disorders and stroke. While deep
learning models have shown promising results analyzing 3D MRI across a number
of brain imaging tasks, most are highly tailored for specific tasks with
limited labeled data, and are not able to generalize across tasks and/or
populations. The development of self-supervised learning (SSL) has enabled the
creation of large medical foundation models that leverage diverse, unlabeled
datasets ranging from healthy to diseased data, showing significant success in
2D medical imaging applications. However, even the very few foundation models
for 3D brain MRI that have been developed remain limited in resolution, scope,
or accessibility. In this work, we present a general, high-resolution
SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on
18,759 patients (44,958 scans) from 11 publicly available datasets spanning
diverse neurological diseases. We compare our model to Masked Autoencoders
(MAE), as well as two supervised baselines, on four diverse downstream
prediction tasks in both in-distribution and out-of-distribution settings. Our
fine-tuned SimCLR model outperforms all other models across all tasks. Notably,
our model still achieves superior performance when fine-tuned using only 20% of
labeled training samples for predicting Alzheimer's disease. We use publicly
available code and data, and release our trained model at
https://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly
applicable and accessible foundation model for clinical brain MRI analysis.

</details>


### [4] [USCTNet: A deep unfolding nuclear-norm optimization solver for physically consistent HSI reconstruction](https://arxiv.org/abs/2509.10651)
*Xiaoyang Ma,Yiyang Chai,Xinran Qu,Hong Sun*

Main category: cs.CV

TL;DR: USCTNet通过物理正则化和自适应低秩SVT，从RGB图像重建高光谱图像，准确性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决从单RGB图像重建高光谱图像时的物理不一致性问题，特别是在相机光谱敏感性和场景照明被错误指定时。

Method: 提出了一种基于物理的正则化方法，结合可学习的变换域核范数，并引入了数据自适应的低秩子空间SVT操作符。开发了USCTNet，一个深度展开求解器，结合参数估计模块和可学习的近端更新。

Result: 在标准基准测试中，USCTNet在重建准确性上持续优于现有最先进的RGB-based方法。

Conclusion: USCTNet通过物理基础的正则化和可学习的低秩子空间SVT操作符，显著提高了从RGB图像重建高光谱图像的准确性，超越了现有最先进方法。

Abstract: Reconstructing hyperspectral images (HSIs) from a single RGB image is
ill-posed and can become physically inconsistent when the camera spectral
sensitivity (CSS) and scene illumination are misspecified. We formulate
RGB-to-HSI reconstruction as a physics-grounded inverse problem regularized by
a nuclear norm in a learnable transform domain, and we explicitly estimate CSS
and illumination to define the forward operator embedded in each iteration,
ensuring colorimetric consistency. To avoid the cost and instability of full
singular-value decompositions (SVDs) required by singular-value thresholding
(SVT), we introduce a data-adaptive low-rank subspace SVT operator. Building on
these components, we develop USCTNet, a deep unfolding solver tailored to HSI
that couples a parameter estimation module with learnable proximal updates.
Extensive experiments on standard benchmarks show consistent improvements over
state-of-the-art RGB-based methods in reconstruction accuracy. Code:
https://github.com/psykheXX/USCTNet-Code-Implementation.git

</details>


### [5] [A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI](https://arxiv.org/abs/2509.10683)
*Felicia Liu,Jay J. Yoo,Farzad Khalvati*

Main category: cs.CV

TL;DR: LLMs在医疗影像任务中表现不如CNN，尤其在空间理解和分类特异性上存在不足，需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在医疗影像任务（如胶质瘤分类和分割）中的有效性，并与传统CNN进行比较。

Method: 使用BraTS 2020数据集评估了通用视觉语言LLM（LLaMA 3.2 Instruct）在微调前后的性能，并与定制的3D CNN进行了比较。

Result: CNN在分类任务中达到80%准确率，而通用LLM准确率为76%，但特异性仅为18%。微调后特异性提升至55%，但整体性能下降。在分割任务中，CNN表现优于LLM，后者在空间理解上表现有限。

Conclusion: LLMs在当前的形态下不太适合基于图像的任务，可能需要更严格的微调或替代训练策略以提升性能、稳健性和在医疗领域的实用性。

Abstract: Large Language Models (LLMs) have shown strong performance in text-based
healthcare tasks. However, their utility in image-based applications remains
unexplored. We investigate the effectiveness of LLMs for medical imaging tasks,
specifically glioma classification and segmentation, and compare their
performance to that of traditional convolutional neural networks (CNNs). Using
the BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a
general-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after
fine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma
classification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and
balanced precision and recall. The general LLM reached 76% accuracy but
suffered from a specificity of only 18%, often misclassifying Low-Grade tumors.
Fine-tuning improved specificity to 55%, but overall performance declined
(e.g., accuracy dropped to 72%). For segmentation, three methods - center
point, bounding box, and polygon extraction, were implemented. CNNs accurately
localized gliomas, though small tumors were sometimes missed. In contrast, LLMs
consistently clustered predictions near the image center, with no distinction
of glioma size, location, or placement. Fine-tuning improved output formatting
but failed to meaningfully enhance spatial accuracy. The bounding polygon
method yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in
both tasks. LLMs showed limited spatial understanding and minimal improvement
from fine-tuning, indicating that, in their current form, they are not
well-suited for image-based tasks. More rigorous fine-tuning or alternative
training strategies may be needed for LLMs to achieve better performance,
robustness, and utility in the medical space.

</details>


### [6] [Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation](https://arxiv.org/abs/2509.10687)
*Hao Zhang,Chun-Han Yao,Simon Donné,Narendra Ahuja,Varun Jampani*

Main category: cs.CV

TL;DR: SP4D是一个通过双分支扩散模型生成RGB和运动学部件视频的框架，采用空间颜色编码和双向扩散融合模块，适用于动画和运动任务。


<details>
  <summary>Details</summary>
Motivation: 传统部件分割方法依赖基于外观的语义线索，而SP4D旨在生成与物体关节对齐且跨视角和时间一致的动态部件。

Method: SP4D采用双分支扩散模型，联合合成RGB帧和对应的部件分割图，并通过空间颜色编码方案简化架构。双向扩散融合模块（BiDiFuse）和对比部件一致性损失增强了跨分支一致性。

Result: 实验表明，SP4D能泛化到多样场景，包括真实视频、新生成物体和罕见关节姿态，生成的运动感知输出适合下游任务。

Conclusion: SP4D框架通过双分支扩散模型和创新的空间颜色编码方案，能够从单目输入生成配对的RGB和运动学部件视频，适用于下游动画和运动相关任务。

Abstract: We present Stable Part Diffusion 4D (SP4D), a framework for generating paired
RGB and kinematic part videos from monocular inputs. Unlike conventional part
segmentation methods that rely on appearance-based semantic cues, SP4D learns
to produce kinematic parts - structural components aligned with object
articulation and consistent across views and time. SP4D adopts a dual-branch
diffusion model that jointly synthesizes RGB frames and corresponding part
segmentation maps. To simplify the architecture and flexibly enable different
part counts, we introduce a spatial color encoding scheme that maps part masks
to continuous RGB-like images. This encoding allows the segmentation branch to
share the latent VAE from the RGB branch, while enabling part segmentation to
be recovered via straightforward post-processing. A Bidirectional Diffusion
Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a
contrastive part consistency loss to promote spatial and temporal alignment of
part predictions. We demonstrate that the generated 2D part maps can be lifted
to 3D to derive skeletal structures and harmonic skinning weights with few
manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,
a curated dataset of over 20K rigged objects selected and processed from
Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part
video sequences. Experiments show that SP4D generalizes strongly to diverse
scenarios, including real-world videos, novel generated objects, and rare
articulated poses, producing kinematic-aware outputs suitable for downstream
animation and motion-related tasks.

</details>


### [7] [On the Skinning of Gaussian Avatars](https://arxiv.org/abs/2509.11411)
*Nikolaos Zioulis,Nikolaos Kotarelas,Georgios Albanis,Spyridon Thermos,Anargyros Chatzitofis*

Main category: cs.CV

TL;DR: 提出一种基于四元数平均的加权旋转混合方法，解决了Gaussian splatting在动画中的非线性旋转问题，简化了实现并提升了效率。


<details>
  <summary>Details</summary>
Motivation: 尽管Gaussian splatting解决了神经辐射场方法的渲染慢和反向映射问题，但线性混合蒙皮在处理Gaussians的非线性旋转时存在不足。

Method: 采用加权旋转混合方法，利用四元数平均来处理Gaussians的非线性旋转特性。

Result: 该方法能够简化基于顶点的Gaussians动画，并高效集成到任何引擎中。

Conclusion: 提出的加权旋转混合方法利用四元数平均，简化了基于顶点的Gaussians动画，并能在任何引擎中高效集成，仅需修改线性混合蒙皮技术。

Abstract: Radiance field-based methods have recently been used to reconstruct human
avatars, showing that we can significantly downscale the systems needed for
creating animated human avatars. Although this progress has been initiated by
neural radiance fields, their slow rendering and backward mapping from the
observation space to the canonical space have been the main challenges. With
Gaussian splatting overcoming both challenges, a new family of approaches has
emerged that are faster to train and render, while also straightforward to
implement using forward skinning from the canonical to the observation space.
However, the linear blend skinning required for the deformation of the
Gaussians does not provide valid results for their non-linear rotation
properties. To address such artifacts, recent works use mesh properties to
rotate the non-linear Gaussian properties or train models to predict corrective
offsets. Instead, we propose a weighted rotation blending approach that
leverages quaternion averaging. This leads to simpler vertex-based Gaussians
that can be efficiently animated and integrated in any engine by only modifying
the linear blend skinning technique, and using any Gaussian rasterizer.

</details>


### [8] [SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition](https://arxiv.org/abs/2509.10710)
*Sven Schreiber,Noha Sarhan,Simone Frintrop,Christian Wilms*

Main category: cs.CV

TL;DR: SegSLR通过零样本视频分割结合RGB和姿态信息，提升孤立手语识别性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有ISLR方法依赖RGB数据或姿态信息，但结合这些模态时因不精确表示（如边界框）导致关键细节（如手部形状和方向）丢失。

Method: 提出SegSLR系统，结合RGB和姿态信息，通过可提示的零样本视频分割技术精确分割手部和身体部分，聚焦处理最相关的身体部位信息。

Result: 在复杂的ChaLearn249 IsoGD数据集上，SegSLR优于现有最先进方法，消融研究表明其设计选择（聚焦手部和身体）有效。

Conclusion: SegSLR通过结合RGB和姿态信息，利用可提示的零样本视频分割技术，显著提升了孤立手语识别（ISLR）的性能，验证了其设计选择的有效性。

Abstract: Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB
data or signer pose information. However, combining these modalities often
results in the loss of crucial details, such as hand shape and orientation, due
to imprecise representations like bounding boxes. Therefore, we propose the
ISLR system SegSLR, which combines RGB and pose information through promptable
zero-shot video segmentation. Given the rough localization of the hands and the
signer's body from pose information, we segment the respective parts through
the video to maintain all relevant shape information. Subsequently, the
segmentations focus the processing of the RGB data on the most relevant body
parts for ISLR. This effectively combines RGB and pose information. Our
evaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR
outperforms state-of-the-art methods. Furthermore, ablation studies indicate
that SegSLR strongly benefits from focusing on the signer's body and hands,
justifying our design choices.

</details>


### [9] [HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments](https://arxiv.org/abs/2509.12187)
*Johanna Karras,Yingwei Li,Yasamin Jafarian,Ira Kemelmacher-Shlizerman*

Main category: cs.CV

TL;DR: HoloGarment 通过结合真实和合成数据优化共享嵌入空间，实现了对真实世界服装的360度新视角合成，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖于合成3D训练数据，这些数据主要由未遮挡和静态对象组成，导致在真实世界服装上泛化能力差。为了解决这一问题，HoloGarment 旨在通过结合真实和合成数据，提升新视角合成的性能。

Method: HoloGarment 提出了一种新颖的隐式训练范式，结合大规模真实视频数据和小规模合成3D数据，优化共享服装嵌入空间。在推理阶段，通过微调特定真实世界视频的服装嵌入，构建服装“图集”表示，实现动态视频到360度新视角合成。

Result: 实验表明，HoloGarment 在图像和视频的新视角合成任务中达到了最先进的性能，能够鲁棒地处理真实世界中的复杂情况。

Conclusion: HoloGarment 在真实世界服装的新视角合成（NVS）任务中表现出色，能够处理皱纹、姿势变化和遮挡等复杂情况，同时保持高真实感、视角一致性、精细纹理细节和准确几何形状。

Abstract: Novel view synthesis (NVS) of in-the-wild garments is a challenging task due
significant occlusions, complex human poses, and cloth deformations. Prior
methods rely on synthetic 3D training data consisting of mostly unoccluded and
static objects, leading to poor generalization on real-world clothing. In this
paper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3
images or a continuous video of a person wearing a garment and generates
360{\deg} novel views of the garment in a canonical pose. Our key insight is to
bridge the domain gap between real and synthetic data with a novel implicit
training paradigm leveraging a combination of large-scale real video data and
small-scale synthetic 3D data to optimize a shared garment embedding space.
During inference, the shared embedding space further enables dynamic
video-to-360{\deg} NVS through the construction of a garment "atlas"
representation by finetuning a garment embedding on a specific real-world
video. The atlas captures garment-specific geometry and texture across all
viewpoints, independent of body pose or motion. Extensive experiments show that
HoloGarment achieves state-of-the-art performance on NVS of in-the-wild
garments from images and videos. Notably, our method robustly handles
challenging real-world artifacts -- such as wrinkling, pose variation, and
occlusion -- while maintaining photorealism, view consistency, fine texture
details, and accurate geometry. Visit our project page for additional results:
https://johannakarras.github.io/HoloGarment

</details>


### [10] [SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation](https://arxiv.org/abs/2509.10748)
*Jecia Z. Y. Mao,Francis X Creighton,Russell H Taylor,Manish Sahu*

Main category: cs.CV

TL;DR: SCOPE框架结合LLM和VFM，实现术中视频流的实时分割和跟踪，减少对人工提示的依赖，展示了在动态手术环境中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前解决方案依赖于特定领域的监督模型，需要标记数据和特定领域数据来适应新的手术场景。本文旨在解决这些限制，通过开发一个开放集、零样本分割框架，减少对人工视觉或文本提示的依赖。

Method: SCOPE框架整合了LLM的推理能力和开放集VFM的感知能力，支持术中视频流的实时分割、标记和跟踪。关键组件是一个协作感知代理，它生成VFM分割的候选结果，并结合临床医生的语音反馈来指导分割。

Result: 在Cataract1k数据集和内部ex-vivo颅底数据集上的评估表明，该框架能够生成实时分割和跟踪。此外，通过实时模拟ex-vivo实验展示了其动态能力。

Conclusion: 该论文提出的SCOPE框架通过结合大型语言模型（LLM）和视觉基础模型（VFM），展示了在动态手术室环境中开发适应性强的、免提的、以外科医生为中心的工具的潜力。

Abstract: Accurate segmentation and tracking of relevant elements of the surgical scene
is crucial to enable context-aware intraoperative assistance and decision
making. Current solutions remain tethered to domain-specific, supervised models
that rely on labeled data and required domain-specific data to adapt to new
surgical scenarios and beyond predefined label categories. Recent advances in
prompt-driven vision foundation models (VFM) have enabled open-set, zero-shot
segmentation across heterogeneous medical images. However, dependence of these
models on manual visual or textual cues restricts their deployment in
introperative surgical settings. We introduce a speech-guided collaborative
perception (SCOPE) framework that integrates reasoning capabilities of large
language model (LLM) with perception capabilities of open-set VFMs to support
on-the-fly segmentation, labeling and tracking of surgical instruments and
anatomy in intraoperative video streams. A key component of this framework is a
collaborative perception agent, which generates top candidates of VFM-generated
segmentation and incorporates intuitive speech feedback from clinicians to
guide the segmentation of surgical instruments in a natural human-machine
collaboration paradigm. Afterwards, instruments themselves serve as interactive
pointers to label additional elements of the surgical scene. We evaluated our
proposed framework on a subset of publicly available Cataract1k dataset and an
in-house ex-vivo skull-base dataset to demonstrate its potential to generate
on-the-fly segmentation and tracking of surgical scene. Furthermore, we
demonstrate its dynamic capabilities through a live mock ex-vivo experiment.
This human-AI collaboration paradigm showcase the potential of developing
adaptable, hands-free, surgeon-centric tools for dynamic operating-room
environments.

</details>


### [11] [Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation](https://arxiv.org/abs/2509.10759)
*Yi-Ruei Liu,You-Zhe Xie,Yu-Hsiang Hsu,I-Sheng Fang,Yu-Lun Liu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 4D-GRT提出两阶段流程，结合4D高斯喷洒和光线追踪，高效模拟相机效果，速度快且质量高，构建基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉系统假设理想针孔相机，面对鱼眼畸变和滚动快门等真实相机效果时表现不佳，主要因缺乏相关训练数据。现有数据生成方法成本高或仿真效果不准确。

Method: 提出4D-GRT两阶段流程：首先重建动态场景，然后应用光线追踪生成具有可控、物理精确相机效果的视频。

Result: 4D-GRT在渲染速度上最快，渲染质量优于或媲美现有基线，并构建了八个合成动态场景作为基准。

Conclusion: 4D-GRT通过结合4D高斯喷洒和基于物理的光线追踪，提出了一种高效且高质量的相机效果模拟方法，解决了现有数据生成方法的高成本、仿真与现实差距问题，并构建了基准数据集进行评估。

Abstract: Common computer vision systems typically assume ideal pinhole cameras but
fail when facing real-world camera effects such as fisheye distortion and
rolling shutter, mainly due to the lack of learning from training data with
camera effects. Existing data generation approaches suffer from either high
costs, sim-to-real gaps or fail to accurately model camera effects. To address
this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage
pipeline that combines 4D Gaussian Splatting with physically-based ray tracing
for camera effect simulation. Given multi-view videos, 4D-GRT first
reconstructs dynamic scenes, then applies ray tracing to generate videos with
controllable, physically accurate camera effects. 4D-GRT achieves the fastest
rendering speed while performing better or comparable rendering quality
compared to existing baselines. Additionally, we construct eight synthetic
dynamic scenes in indoor environments across four camera effects as a benchmark
to evaluate generated videos with camera effects.

</details>


### [12] [EditDuet: A Multi-Agent System for Video Non-Linear Editing](https://arxiv.org/abs/2509.10761)
*Marcelo Sandoval-Castaneda,Bryan Russell,Josef Sivic,Gregory Shakhnarovich,Fabian Caba Heilbron*

Main category: cs.CV

TL;DR: 提出了一种多智能体自动化视频编辑系统，通过编辑器和评论家智能体实现语言驱动的视频编辑，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动化视频编辑的核心任务，解决现有工作主要集中于检索或用户界面而将实际编辑留给用户的问题。

Method: 采用多智能体方法，设计了编辑器和评论家智能体，并引入了基于学习的方法来实现专业智能体间的有效通信。

Result: 通过用户研究定性和定量评估输出视频序列，发现系统表现优于现有方法。

Conclusion: 该系统在覆盖率、时间约束满足和人类偏好方面显著优于现有方法。

Abstract: Automated tools for video editing and assembly have applications ranging from
filmmaking and advertisement to content creation for social media. Previous
video editing work has mainly focused on either retrieval or user interfaces,
leaving actual editing to the user. In contrast, we propose to automate the
core task of video editing, formulating it as sequential decision making
process. Ours is a multi-agent approach. We design an Editor agent and a Critic
agent. The Editor takes as input a collection of video clips together with
natural language instructions and uses tools commonly found in video editing
software to produce an edited sequence. On the other hand, the Critic gives
natural language feedback to the editor based on the produced sequence or
renders it if it is satisfactory. We introduce a learning-based approach for
enabling effective communication across specialized agents to address the
language-driven video editing task. Finally, we explore an LLM-as-a-judge
metric for evaluating the quality of video editing system and compare it with
general human preference. We evaluate our system's output video sequences
qualitatively and quantitatively through a user study and find that our system
vastly outperforms existing approaches in terms of coverage, time constraint
satisfaction, and human preference.

</details>


### [13] [Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging](https://arxiv.org/abs/2509.10767)
*Sajad Amiri,Shahram Taeb,Sara Gharibi,Setareh Dehghanfard,Somayeh Sadat Mehrnia,Mehrdad Oveisi,Ilker Hacihaliloglu,Arman Rahmim,Mohammad R. Salmanpour*

Main category: cs.CV

TL;DR: 该研究提出了一种稳定性感知框架，通过机器学习从非对比MRI预测胶质瘤的对比增强，减少对钆基造影剂的依赖，并在多中心数据中表现出高准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 钆基造影剂在胶质瘤成像中至关重要，但存在安全、成本和可及性问题。通过机器学习从非对比MRI预测对比增强提供了一种更安全的替代方案。然而，扫描仪和队列的变异性阻碍了稳健的模型选择。

Method: 研究分析了1,446例胶质瘤病例，使用非对比T1WI作为输入，从配对的对比后T1WI中提取增强信息。通过PyRadiomics提取108个特征，结合48种降维方法和25种分类器，构建了1,200个流程。采用旋转验证方法，在三个数据集上训练并在第四个数据集上测试。

Result: 交叉验证预测准确率范围为0.91至0.96，外部测试平均准确率为0.93。F1、精确度和召回率稳定（0.87至0.96），而ROC-AUC变化较大（0.50至0.82），反映了队列的异质性。MI与ETr流程在准确性和稳定性上表现最佳。

Conclusion: 该研究提出的稳定性感知框架能够可靠地从非对比MRI中预测对比增强，减少对钆基造影剂的依赖，并提高跨中心的泛化能力，为神经肿瘤学及其他领域提供了可扩展的模板。

Abstract: Gadolinium-based contrast agents (GBCAs) are central to glioma imaging but
raise safety, cost, and accessibility concerns. Predicting contrast enhancement
from non-contrast MRI using machine learning (ML) offers a safer alternative,
as enhancement reflects tumor aggressiveness and informs treatment planning.
Yet scanner and cohort variability hinder robust model selection. We propose a
stability-aware framework to identify reproducible ML pipelines for multicenter
prediction of glioma MRI contrast enhancement. We analyzed 1,446 glioma cases
from four TCIA datasets (UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG).
Non-contrast T1WI served as input, with enhancement derived from paired
post-contrast T1WI. Using PyRadiomics under IBSI standards, 108 features were
extracted and combined with 48 dimensionality reduction methods and 25
classifiers, yielding 1,200 pipelines. Rotational validation was trained on
three datasets and tested on the fourth. Cross-validation prediction accuracies
ranged from 0.91 to 0.96, with external testing achieving 0.87 (UCSF-PDGM),
0.98 (UPENN-GB), and 0.95 (BRATS-Africa), with an average of 0.93. F1,
precision, and recall were stable (0.87 to 0.96), while ROC-AUC varied more
widely (0.50 to 0.82), reflecting cohort heterogeneity. The MI linked with ETr
pipeline consistently ranked highest, balancing accuracy and stability. This
framework demonstrates that stability-aware model selection enables reliable
prediction of contrast enhancement from non-contrast glioma MRI, reducing
reliance on GBCAs and improving generalizability across centers. It provides a
scalable template for reproducible ML in neuro-oncology and beyond.

</details>


### [14] [Group Evidence Matters: Tiling-based Semantic Gating for Dense Object Detection](https://arxiv.org/abs/2509.10779)
*Yilun Xiao*

Main category: cs.CV

TL;DR: 提出一种检测器无关的后处理框架，通过重叠分块和双门控验证提升小目标召回率，适用于召回敏感场景。


<details>
  <summary>Details</summary>
Motivation: 解决无人机影像中因远距离视角、遮挡和杂波导致的小目标漏检问题。

Method: 采用重叠分块恢复低置信度候选，通过空间门（基于DBSCAN的框中心聚类）和语义门（基于ResNet-18嵌入的DBSCAN）验证群体证据，再进行置信度重加权与类别感知NMS融合。

Result: 在VisDrone数据集上，召回率从0.685提升至0.778（+0.093），F1分数达0.669，后处理延迟平均0.095秒/图像。

Conclusion: 该框架无需重新训练即可与现代检测器集成，未来工作将降低语义门控成本并扩展时空线索。

Abstract: Dense small objects in UAV imagery are often missed due to long-range
viewpoints, occlusion, and clutter[cite: 5]. This paper presents a
detector-agnostic post-processing framework that converts overlap-induced
redundancy into group evidence[cite: 6]. Overlapping tiling first recovers
low-confidence candidates[cite: 7]. A Spatial Gate (DBSCAN on box centroids)
and a Semantic Gate (DBSCAN on ResNet-18 embeddings) then validates group
evidence[cite: 7]. Validated groups receive controlled confidence reweighting
before class-aware NMS fusion[cite: 8]. Experiments on VisDrone show a recall
increase from 0.685 to 0.778 (+0.093) and a precision adjustment from 0.801 to
0.595, yielding F1=0.669[cite: 9]. Post-processing latency averages 0.095 s per
image[cite: 10]. These results indicate recall-first, precision-trade-off
behavior that benefits recall-sensitive applications such as far-field counting
and monitoring[cite: 10]. Ablation confirms that tiling exposes missed objects,
spatial clustering stabilizes geometry, semantic clustering enforces appearance
coherence, and reweighting provides calibrated integration with the
baseline[cite: 11]. The framework requires no retraining and integrates with
modern detectors[cite: 12]. Future work will reduce semantic gating cost and
extend the approach with temporal cues[cite: 13].

</details>


### [15] [InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts](https://arxiv.org/abs/2509.10813)
*Weipeng Zhong,Peizhou Cao,Yichen Jin,Li Luo,Wenzhe Cai,Jingli Lin,Hanqing Wang,Zhaoyang Lyu,Tai Wang,Bo Dai,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: InternScenes是一个大规模、多样化的可模拟室内场景数据集，解决了现有数据集的不足，并展示了其在Embodied AI任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在规模、多样性和布局真实性方面存在不足，限制了Embodied AI的发展。

Method: 整合了三种不同的场景来源（真实扫描、程序生成和设计师创建），包含约40,000个场景和1.96M个3D对象，覆盖15种常见场景类型和288个对象类别。通过数据处理流程确保可模拟性、增强交互性并解决对象碰撞问题。

Result: InternScenes数据集展示了在场景布局生成和点目标导航等任务中的新挑战，并为模型训练的规模化提供了可能。

Conclusion: InternScenes数据集通过整合多种场景来源，提供了大规模、多样化的可模拟室内场景，解决了现有数据集在规模、多样性和布局真实性方面的不足，为Embodied AI的研究开辟了新方向。

Abstract: The advancement of Embodied AI heavily relies on large-scale, simulatable 3D
scene datasets characterized by scene diversity and realistic layouts. However,
existing datasets typically suffer from limitations in data scale or diversity,
sanitized layouts lacking small items, and severe object collisions. To address
these shortcomings, we introduce \textbf{InternScenes}, a novel large-scale
simulatable indoor scene dataset comprising approximately 40,000 diverse scenes
by integrating three disparate scene sources, real-world scans, procedurally
generated scenes, and designer-created scenes, including 1.96M 3D objects and
covering 15 common scene types and 288 object classes. We particularly preserve
massive small items in the scenes, resulting in realistic and complex layouts
with an average of 41.5 objects per region. Our comprehensive data processing
pipeline ensures simulatability by creating real-to-sim replicas for real-world
scans, enhances interactivity by incorporating interactive objects into these
scenes, and resolves object collisions by physical simulations. We demonstrate
the value of InternScenes with two benchmark applications: scene layout
generation and point-goal navigation. Both show the new challenges posed by the
complex and realistic layouts. More importantly, InternScenes paves the way for
scaling up the model training for both tasks, making the generation and
navigation in such complex scenes possible. We commit to open-sourcing the
data, models, and benchmarks to benefit the whole community.

</details>


### [16] [Well-Conditioned Polynomial Representations for Mathematical Handwriting Recognition](https://arxiv.org/abs/2509.10815)
*Robert M. Corless,Deepak Singh Kalhan,Stephen M. Watt*

Main category: cs.CV

TL;DR: 研究了不同基和多项式阶数对数学手写建模的准确性和计算效率的影响。


<details>
  <summary>Details</summary>
Motivation: 探索如何在保证数学手写建模准确性的同时降低计算成本。

Method: 通过考虑这些基中多项式评估的条件数，并界定各种内积如何为符号间的变化提供范数。

Result: 研究发现不同基和多项式阶数的选择对建模准确性和计算效率有显著影响。

Conclusion: 本文探讨了在不同基（如Legendre、Legendre-Sobolev、Chebyshev和Chebyshev-Sobolev）和多项式阶数之间的权衡，以实现低计算成本下的精确建模。

Abstract: Previous work has made use of a parameterized plane curve polynomial
representation for mathematical handwriting, with the polynomials represented
in a Legendre or Legendre-Sobolev graded basis. This provides a compact
geometric representation for the digital ink. Preliminary results have also
been shown for Chebyshev and Chebyshev-Sobolev bases. This article explores the
trade-offs between basis choice and polynomial degree to achieve accurate
modeling with a low computational cost. To do this, we consider the condition
number for polynomial evaluation in these bases and bound how the various inner
products give norms for the variations between symbols.

</details>


### [17] [Multi-Task Diffusion Approach For Prediction of Glioma Tumor Progression](https://arxiv.org/abs/2509.10824)
*Aghiles Kebaili,Romain Modzelewski,Jérôme Lapuyade-Lahorgue,Maxime Fontanilles,Sébastien Thureau,Su Ruan*

Main category: cs.CV

TL;DR: 本文提出多任务扩散框架预测胶质瘤进展，结合变形模块和增强管道，生成FLAIR序列和概率图，临床评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 胶质瘤进展预测面临临床MRI数据稀疏、不规则采集的挑战，导致数据不平衡和建模困难。本文旨在解决这些问题，提供可靠的预测模型。

Method: 采用多任务扩散框架，结合预训练的变形模块和定向增强管道，生成未来FLAIR序列和空间概率肿瘤演化图。引入放疗加权焦点损失项，利用放射剂量图提高模型训练效果。

Result: 在公共数据集和内部私有数据集上均取得良好效果，生成的灵活时间依赖性概率图可帮助临床评估肿瘤进展风险。

Conclusion: 本文提出的多任务扩散框架能够有效预测胶质瘤的进展，通过生成未来FLAIR序列和空间概率肿瘤演化图，为临床提供了灵活的时间依赖性概率图，帮助医生评估肿瘤进展风险。

Abstract: Glioma, an aggressive brain malignancy characterized by rapid progression and
its poor prognosis, poses significant challenges for accurate evolution
prediction. These challenges are exacerbated by sparse, irregularly acquired
longitudinal MRI data in clinical practice, where incomplete follow-up
sequences create data imbalances and make reliable modeling difficult. In this
paper, we present a multitask diffusion framework for time-agnostic, pixel-wise
prediction of glioma progression. The model simultaneously generates future
FLAIR sequences at any chosen time point and estimates spatial probabilistic
tumor evolution maps derived using signed distance fields (SDFs), allowing
uncertainty quantification. To capture temporal dynamics of tumor evolution
across arbitrary intervals, we integrate a pretrained deformation module that
models inter-scan changes using deformation fields. Regarding the common
clinical limitation of data scarcity, we implement a targeted augmentation
pipeline that synthesizes complete sequences of three follow-up scans and
imputes missing MRI modalities from available patient studies, improving the
stability and accuracy of predictive models. Based on merely two follow-up
scans at earlier timepoints, our framework produces flexible time-depending
probability maps, enabling clinicians to interrogate tumor progression risks at
any future temporal milestone. We further introduce a radiotherapy-weighted
focal loss term that leverages radiation dose maps, as these highlight regions
of greater clinical importance during model training. The proposed method was
trained on a public dataset and evaluated on an internal private dataset,
achieving promising results in both cases

</details>


### [18] [Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios](https://arxiv.org/abs/2509.10841)
*Simone Mosco,Daniel Fusaro,Wanmeng Li,Emanuele Menegatti,Alberto Pretto*

Main category: cs.CV

TL;DR: 通过点-平面投影从2D表示学习特征，提升LiDAR点云语义分割在数据稀缺场景的性能，并在标准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在数据稀缺场景下泛化能力有限，且计算复杂度高，需大量训练数据。

Method: 提出了一种基于点-平面投影的方法，从点云的多个信息性2D表示中提取互补信息，并引入了与LiDAR传感器特性对齐的几何感知数据增强技术。

Result: 在数据稀缺场景下性能显著提升，并在SemanticKITTI和PandaSet等标准数据集上取得了竞争性结果。

Conclusion: 通过点-平面投影从2D表示中有效学习特征，该方法在数据稀缺场景下显著提升了性能，并在标准数据集上取得了竞争性结果。

Abstract: LiDAR point cloud semantic segmentation is essential for interpreting 3D
environments in applications such as autonomous driving and robotics. Recent
methods achieve strong performance by exploiting different point cloud
representations or incorporating data from other sensors, such as cameras or
external datasets. However, these approaches often suffer from high
computational complexity and require large amounts of training data, limiting
their generalization in data-scarce scenarios. In this paper, we improve the
performance of point-based methods by effectively learning features from 2D
representations through point-plane projections, enabling the extraction of
complementary information while relying solely on LiDAR data. Additionally, we
introduce a geometry-aware technique for data augmentation that aligns with
LiDAR sensor properties and mitigates class imbalance. We implemented and
evaluated our method that applies point-plane projections onto multiple
informative 2D representations of the point cloud. Experiments demonstrate that
this approach leads to significant improvements in limited-data scenarios,
while also achieving competitive results on two publicly available standard
datasets, as SemanticKITTI and PandaSet. The code of our method is available at
https://github.com/SiMoM0/3PNet

</details>


### [19] [OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds](https://arxiv.org/abs/2509.10842)
*Chongyu Wang,Kunlei Jing,Jihua Zhu,Di Wang*

Main category: cs.CV

TL;DR: OpenUrban3D是首个无需对齐多视角图像、预训练点云分割网络或人工标注的大规模城市3D开放词汇语义分割框架，通过多视角渲染和视觉语言特征提取实现零样本分割。


<details>
  <summary>Details</summary>
Motivation: 解决大规模城市点云数据集中高质量、对齐多视角图像的缺失问题，以及现有3D分割管道在不同城市环境中几何、尺度和外观变化大的泛化能力差的问题。

Method: 通过多视角、多粒度渲染、掩码级视觉语言特征提取和样本平衡融合，直接从原始点云生成鲁棒的语义特征，并将其蒸馏到3D骨干模型中。

Result: 在SensatUrban和SUM等大规模城市基准测试中，OpenUrban3D在分割准确性和跨场景泛化方面均显著优于现有方法。

Conclusion: OpenUrban3D作为一种灵活且可扩展的解决方案，显著提升了3D城市场景理解的准确性和跨场景泛化能力。

Abstract: Open-vocabulary semantic segmentation enables models to recognize and segment
objects from arbitrary natural language descriptions, offering the flexibility
to handle novel, fine-grained, or functionally defined categories beyond fixed
label sets. While this capability is crucial for large-scale urban point clouds
that support applications such as digital twins, smart city management, and
urban analytics, it remains largely unexplored in this domain. The main
obstacles are the frequent absence of high-quality, well-aligned multi-view
imagery in large-scale urban point cloud datasets and the poor generalization
of existing three-dimensional (3D) segmentation pipelines across diverse urban
environments with substantial variation in geometry, scale, and appearance. To
address these challenges, we present OpenUrban3D, the first 3D open-vocabulary
semantic segmentation framework for large-scale urban scenes that operates
without aligned multi-view images, pre-trained point cloud segmentation
networks, or manual annotations. Our approach generates robust semantic
features directly from raw point clouds through multi-view, multi-granularity
rendering, mask-level vision-language feature extraction, and sample-balanced
fusion, followed by distillation into a 3D backbone model. This design enables
zero-shot segmentation for arbitrary text queries while capturing both semantic
richness and geometric priors. Extensive experiments on large-scale urban
benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves
significant improvements in both segmentation accuracy and cross-scene
generalization over existing methods, demonstrating its potential as a flexible
and scalable solution for 3D urban scene understanding.

</details>


### [20] [AutoOEP -- A Multi-modal Framework for Online Exam Proctoring](https://arxiv.org/abs/2509.10887)
*Aryan Kashyap Naveen,Bhuvanesh Singla,Raajan Wankhade,Shreesha M,Ramu S,Ram Mohana Reddy Guddeti*

Main category: cs.CV

TL;DR: AutoOEP是一种多模态自动化监考系统，通过计算机视觉和机器学习技术有效检测作弊行为，准确率高且资源高效。


<details>
  <summary>Details</summary>
Motivation: 在线教育的快速发展亟需强大且可扩展的系统来确保远程考试的学术诚信。传统人工监考难以大规模实施，而现有自动化解决方案要么侵入性强，要么无法检测多种作弊行为。

Method: AutoOEP采用多模态框架，结合计算机视觉和机器学习技术，包括双摄像头设置、面部模块（身份验证、头部姿态估计、视线跟踪、嘴部运动分析）和手部模块（禁止物品检测和手部接近跟踪），并通过LSTM网络分析时间模式计算实时作弊概率分数。

Result: AutoOEP在模拟多样化考试条件的自定义数据集上表现出色，分类可疑活动的准确率达到90.7%，禁止物品检测的mAP@.5为0.57，整个框架处理视频流的速度约为2.4帧/秒（无需GPU）。

Conclusion: AutoOEP是一种有效且资源高效的自动化监考解决方案，显著减少了对人工干预的需求，并提升了在线考试的完整性。

Abstract: The burgeoning of online education has created an urgent need for robust and
scalable systems to ensure academic integrity during remote examinations.
Traditional human proctoring is often not feasible at scale, while existing
automated solutions can be intrusive or fail to detect a wide range of cheating
behaviors. This paper introduces AutoOEP (Automated Online Exam Proctoring), a
comprehensive, multi-modal framework that leverages computer vision and machine
learning to provide effective, automated proctoring. The system utilizes a
dual-camera setup to capture both a frontal view of the examinee and a side
view of the workspace, minimizing blind spots. Our approach integrates several
parallel analyses: the Face Module performs continuous identity verification
using ArcFace, along with head pose estimation, gaze tracking, and mouth
movement analysis to detect suspicious cues. Concurrently, the Hand Module
employs a fine-tuned YOLOv11 model for detecting prohibited items (e.g., mobile
phones, notes) and tracks hand proximity to these objects. Features from these
modules are aggregated and fed into a Long Short-Term Memory (LSTM) network
that analyzes temporal patterns to calculate a real-time cheating probability
score. We evaluate AutoOEP on a custom-collected dataset simulating diverse
exam conditions. Our system achieves an accuracy of 90.7% in classifying
suspicious activities. The object detection component obtains a mean Average
Precision (mAP@.5) of 0.57 for prohibited items, and the entire framework
processes video streams at approximately 2.4 frames per second without a GPU.
The results demonstrate that AutoOEP is an effective and resource-efficient
solution for automated proctoring, significantly reducing the need for human
intervention and enhancing the integrity of online assessments.

</details>


### [21] [Total Variation Subgradient Guided Image Fusion for Dual-Camera CASSI System](https://arxiv.org/abs/2509.10897)
*Weiqiang Zhao,Tianzhu Liu,Yuzhe Gui,Yanfeng Gu*

Main category: cs.CV

TL;DR: 提出了一种结合TV次梯度理论的双相机CASSI重建框架，通过动态正则化和自适应参考生成机制，解决了高压缩比下的重建问题，并保持了物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统模型依赖手工先验性能有限，而深度学习方法缺乏物理可解释性，因此需要一种既能保持性能又具数学可解释性的方法。

Method: 通过建立端到端的SD-CASSI数学模型，降低了逆问题的计算复杂度，并引入了动态正则化策略和自适应参考生成机制。

Result: 实验结果表明，该方法有效保留了空间-光谱结构一致性，并为计算光谱成像提供了可解释的数学基础。

Conclusion: 该论文提出的双相机CASSI重建框架结合了总变分（TV）次梯度理论，有效解决了高压缩比下的重建问题，并在多种重建场景中表现出鲁棒性能。

Abstract: Spectral imaging technology has long-faced fundamental challenges in
balancing spectral, spatial, and temporal resolutions. While compressive
sensing-based Coded Aperture Snapshot Spectral Imaging (CASSI) mitigates this
trade-off through optical encoding, high compression ratios result in ill-posed
reconstruction problems. Traditional model-based methods exhibit limited
performance due to reliance on handcrafted inherent image priors, while deep
learning approaches are constrained by their black-box nature, which
compromises physical interpretability. To address these limitations, we propose
a dual-camera CASSI reconstruction framework that integrates total variation
(TV) subgradient theory. By establishing an end-to-end SD-CASSI mathematical
model, we reduce the computational complexity of solving the inverse problem
and provide a mathematically well-founded framework for analyzing multi-camera
systems. A dynamic regularization strategy is introduced, incorporating
normalized gradient constraints from RGB/panchromatic-derived reference images,
which constructs a TV subgradient similarity function with strict convex
optimization guarantees. Leveraging spatial priors from auxiliary cameras, an
adaptive reference generation and updating mechanism is designed to provide
subgradient guidance. Experimental results demonstrate that the proposed method
effectively preserves spatial-spectral structural consistency. The theoretical
framework establishes an interpretable mathematical foundation for
computational spectral imaging, demonstrating robust performance across diverse
reconstruction scenarios. The source code is available at
https://github.com/bestwishes43/ADMM-TVDS.

</details>


### [22] [Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation](https://arxiv.org/abs/2509.10919)
*Mohanad Albughdadi*

Main category: cs.CV

TL;DR: 提出小型MoE-MAE模型，结合元数据感知，性能媲美大型模型，为地球观测提供高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型计算成本高，限制了其在下游任务中的可访问性和重用性。

Method: 提出了一种仅含250万参数的Metadata-aware Mixture-of-Experts Masked Autoencoder (MoE-MAE)，结合稀疏专家路由与地理时间条件编码。

Result: 尽管模型规模小，但其性能与更大架构相当，元数据感知预训练提高了迁移和标签效率。

Conclusion: 紧凑型、元数据感知的MoE-MAE模型是未来地球观测基础模型的高效且可扩展的一步。

Abstract: Recent advances in Earth Observation have focused on large-scale foundation
models. However, these models are computationally expensive, limiting their
accessibility and reuse for downstream tasks. In this work, we investigate
compact architectures as a practical pathway toward smaller general-purpose EO
models. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder
(MoE-MAE) with only 2.5M parameters. The model combines sparse expert routing
with geo-temporal conditioning, incorporating imagery alongside
latitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAE
on the BigEarthNet-Landsat dataset and evaluate embeddings from its frozen
encoder using linear probes. Despite its small size, the model competes with
much larger architectures, demonstrating that metadata-aware pretraining
improves transfer and label efficiency. To further assess generalization, we
evaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, and
still observe competitive performance compared to models with hundreds of
millions of parameters. These results suggest that compact, metadata-aware
MoE-MAEs are an efficient and scalable step toward future EO foundation models.

</details>


### [23] [Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging](https://arxiv.org/abs/2509.10961)
*Farhan Sadik,Christopher L. Newman,Stuart J. Warden,Rachel K. Surowiec*

Main category: cs.CV

TL;DR: 论文提出了一种边缘增强的自注意力生成对抗网络（ESWGAN-GP）用于HR-pQCT中的运动伪影校正，在模拟和真实数据集上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 刚性运动伪影（如皮质骨条纹和小梁模糊）阻碍了HR-pQCT中对骨微结构的体内评估，目前缺乏标准化的退化模型和运动校正方法。

Method: 提出了一种边缘增强的自注意力Wasserstein生成对抗网络（ESWGAN-GP），结合了边缘增强跳跃连接和自注意力机制，以及基于VGG的感知损失。

Result: ESWGAN-GP在模拟数据集上实现了平均信噪比（SNR）26.78、结构相似性指数（SSIM）0.81和视觉信息保真度（VIF）0.76，在真实数据集上表现更优（SNR 29.31，SSIM 0.87，VIF 0.81）。

Conclusion: 尽管提出的方法对真实运动的简化表示可能无法完全捕捉体内运动伪影的复杂性，但这些方法代表了在HR-pQCT中实现基于深度学习的运动校正的重要初步步骤。

Abstract: Rigid-motion artifacts, such as cortical bone streaking and trabecular
smearing, hinder in vivo assessment of bone microstructures in high-resolution
peripheral quantitative computed tomography (HR-pQCT). Despite various motion
grading techniques, no motion correction methods exist due to the lack of
standardized degradation models. We optimize a conventional sinogram-based
method to simulate motion artifacts in HR-pQCT images, creating paired datasets
of motion-corrupted images and their corresponding ground truth, which enables
seamless integration into supervised learning frameworks for motion correction.
As such, we propose an Edge-enhanced Self-attention Wasserstein Generative
Adversarial Network with Gradient Penalty (ESWGAN-GP) to address motion
artifacts in both simulated (source) and real-world (target) datasets. The
model incorporates edge-enhancing skip connections to preserve trabecular edges
and self-attention mechanisms to capture long-range dependencies, facilitating
motion correction. A visual geometry group (VGG)-based perceptual loss is used
to reconstruct fine micro-structural features. The ESWGAN-GP achieves a mean
signal-to-noise ratio (SNR) of 26.78, structural similarity index measure
(SSIM) of 0.81, and visual information fidelity (VIF) of 0.76 for the source
dataset, while showing improved performance on the target dataset with an SNR
of 29.31, SSIM of 0.87, and VIF of 0.81. The proposed methods address a
simplified representation of real-world motion that may not fully capture the
complexity of in vivo motion artifacts. Nevertheless, because motion artifacts
present one of the foremost challenges to more widespread adoption of this
modality, these methods represent an important initial step toward implementing
deep learning-based motion correction in HR-pQCT.

</details>


### [24] [Gaze Authentication: Factors Influencing Authentication Performance](https://arxiv.org/abs/2509.10969)
*Dillon Lohr,Michael J Proulx,Mehedi Hasan Raju,Oleg V Komogortsev*

Main category: cs.CV

TL;DR: 研究通过大规模数据集和神经网络分析发现，校准目标深度一致、融合校准与非校准数据、提高信号质量能提升注视认证性能，而简单滤波通常会略微降低性能。


<details>
  <summary>Details</summary>
Motivation: 探讨影响最先进的基于注视的认证性能的关键因素。

Method: 采用最先进的神经网络架构，研究了眼动信号质量、眼动校准的多个方面以及简单滤波对认证性能的影响。

Result: 使用相同深度的校准目标、融合校准和非校准的注视数据，以及提高眼动信号质量能显著提升认证性能。简单的三样本移动平均滤波通常会略微降低性能。

Conclusion: 研究发现，使用相同深度的校准目标、融合校准和非校准的注视数据，以及提高眼动信号质量都能提升认证性能。简单的三样本移动平均滤波通常会略微降低性能，但存在一些例外情况。

Abstract: This paper examines the key factors that influence the performance of
state-of-the-art gaze-based authentication. Experiments were conducted on a
large-scale, in-house dataset comprising 8,849 subjects collected with Meta
Quest Pro equivalent hardware running a video oculography-driven gaze
estimation pipeline at 72Hz. The state-of-the-art neural network architecture
was employed to study the influence of the following factors on authentication
performance: eye tracking signal quality, various aspects of eye tracking
calibration, and simple filtering on estimated raw gaze. We found that using
the same calibration target depth for eye tracking calibration, fusing
calibrated and non-calibrated gaze, and improving eye tracking signal quality
all enhance authentication performance. We also found that a simple
three-sample moving average filter slightly reduces authentication performance
in general. While these findings hold true for the most part, some exceptions
were noted.

</details>


### [25] [TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation](https://arxiv.org/abs/2509.10980)
*Haoming Lu*

Main category: cs.CV

TL;DR: TrueSkin是一个系统性分类的肤色数据集，用于改进肤色识别和生成的公平性和准确性，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 肤色识别和生成在模型公平性、医疗保健和生成式AI中具有重要作用，但由于缺乏全面数据集和鲁棒方法而具有挑战性。

Method: 引入TrueSkin数据集（7299张图像，分为6类），在多样化的光照条件、相机角度和捕捉设置下收集，并基于此对现有识别和生成方法进行基准测试。

Result: 使用TrueSkin训练的识别模型比LMM和传统方法的分类准确率提高超过20%；微调生成模型显著提高了肤色保真度。

Conclusion: TrueSkin数据集不仅作为评估现有模型的基准，还为提高肤色识别和生成任务的公平性和准确性提供了宝贵的训练资源。

Abstract: Skin tone recognition and generation play important roles in model fairness,
healthcare, and generative AI, yet they remain challenging due to the lack of
comprehensive datasets and robust methodologies. Compared to other human image
analysis tasks, state-of-the-art large multimodal models (LMMs) and image
generation models struggle to recognize and synthesize skin tones accurately.
To address this, we introduce TrueSkin, a dataset with 7299 images
systematically categorized into 6 classes, collected under diverse lighting
conditions, camera angles, and capture settings. Using TrueSkin, we benchmark
existing recognition and generation approaches, revealing substantial biases:
LMMs tend to misclassify intermediate skin tones as lighter ones, whereas
generative models struggle to accurately produce specified skin tones when
influenced by inherent biases from unrelated attributes in the prompts, such as
hairstyle or environmental context. We further demonstrate that training a
recognition model on TrueSkin improves classification accuracy by more than
20\% compared to LMMs and conventional approaches, and fine-tuning with
TrueSkin significantly improves skin tone fidelity in image generation models.
Our findings highlight the need for comprehensive datasets like TrueSkin, which
not only serves as a benchmark for evaluating existing models but also provides
a valuable training resource to enhance fairness and accuracy in skin tone
recognition and generation tasks.

</details>


### [26] [Policy-Driven Transfer Learning in Resource-Limited Animal Monitoring](https://arxiv.org/abs/2509.10995)
*Nisha Pillai,Aditi Virupakshaiah,Harrison W. Smith,Amanda J. Ashworth,Prasanna Gowda,Phillip R. Owens,Adam R. Rivers,Bindu Nanduri,Mahalingam Ramkumar*

Main category: cs.CV

TL;DR: A RL-based transfer learning framework using UCB algorithm automates pre-trained model selection for animal detection, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Limited labeled training data and the challenge of selecting suitable pre-trained models for animal detection in UAV-based systems motivated the development of an automated model selection framework.

Method: The paper introduces a reinforcement learning (RL)-based transfer learning framework that uses an upper confidence bound (UCB) algorithm to select optimal pre-trained models for animal detection tasks.

Result: Experimental results show the framework achieves higher detection rates with less computational time compared to traditional methods.

Conclusion: The proposed RL-based transfer learning framework with UCB algorithm effectively automates the selection of pre-trained models for animal detection, improving detection rates and reducing computational time.

Abstract: Animal health monitoring and population management are critical aspects of
wildlife conservation and livestock management that increasingly rely on
automated detection and tracking systems. While Unmanned Aerial Vehicle (UAV)
based systems combined with computer vision offer promising solutions for
non-invasive animal monitoring across challenging terrains, limited
availability of labeled training data remains an obstacle in developing
effective deep learning (DL) models for these applications. Transfer learning
has emerged as a potential solution, allowing models trained on large datasets
to be adapted for resource-limited scenarios such as those with limited data.
However, the vast landscape of pre-trained neural network architectures makes
it challenging to select optimal models, particularly for researchers new to
the field. In this paper, we propose a reinforcement learning (RL)-based
transfer learning framework that employs an upper confidence bound (UCB)
algorithm to automatically select the most suitable pre-trained model for
animal detection tasks. Our approach systematically evaluates and ranks
candidate models based on their performance, streamlining the model selection
process. Experimental results demonstrate that our framework achieves a higher
detection rate while requiring significantly less computational time compared
to traditional methods.

</details>


### [27] [Improving Fungi Prototype Representations for Few-Shot Classification](https://arxiv.org/abs/2509.11020)
*Abdarahmane Traore,Éric Hervet,Andy Couturier*

Main category: cs.CV

TL;DR: FungiCLEF 2025竞赛中提出的原型网络方法显著提升了真菌物种识别性能，特别是在稀有物种识别上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决自动真菌物种识别中的挑战，尤其是处理高度不平衡的类别分布和稀有物种的识别问题，以支持大规模生物多样性监测。

Method: 采用基于原型网络的深度学习方法，优化了少样本真菌分类的原型表示。

Result: 原型网络方法在公共（PB）和私有（PR）排行榜上的Recall@5指标上比竞赛基线提高了30个百分点以上。

Conclusion: 所提出的原型网络方法在FungiCLEF 2025竞赛中表现优异，显著提升了真菌物种识别的准确率，特别是对于稀有物种的识别，支持了大规模生物多样性监测的目标。

Abstract: The FungiCLEF 2025 competition addresses the challenge of automatic fungal
species recognition using realistic, field-collected observational data.
Accurate identification tools support both mycologists and citizen scientists,
greatly enhancing large-scale biodiversity monitoring. Effective recognition
systems in this context must handle highly imbalanced class distributions and
provide reliable performance even when very few training samples are available
for many species, especially rare and under-documented taxa that are often
missing from standard training sets. According to competition organizers, about
20\% of all verified fungi observations, representing nearly 20,000 instances,
are associated with these rarely recorded species. To tackle this challenge, we
propose a robust deep learning method based on prototypical networks, which
enhances prototype representations for few-shot fungal classification. Our
prototypical network approach exceeds the competition baseline by more than 30
percentage points in Recall@5 on both the public (PB) and private (PR)
leaderboards. This demonstrates strong potential for accurately identifying
both common and rare fungal species, supporting the main objectives of
FungiCLEF 2025.

</details>


### [28] [Cluster-Level Sparse Multi-Instance Learning for Whole-Slide Images](https://arxiv.org/abs/2509.11034)
*Yuedi Zhang,Zhixiang Xia,Guosheng Yin,Bin Liu*

Main category: cs.CV

TL;DR: csMIL是一种新颖的MIL框架，通过聚类和稀疏性提升性能，在病理学数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统MIL方法在处理实例冗余和缺乏丢弃非信息实例的机制时存在局限性，影响了其鲁棒性和可解释性。

Method: csMIL框架首先进行全局聚类以确定K个簇中心，随后在每个包内进行局部聚类分配簇标签。计算每个簇内的注意力分数，并对簇权重应用稀疏正则化，以选择性地保留诊断相关簇。

Result: csMIL在CAMELYON16和TCGA-NSCLC两个公开组织病理学基准测试中取得了最先进的性能。理论分析表明，csMIL符合压缩感知原则，能够高效恢复相关簇。

Conclusion: csMIL通过全局-局部实例聚类、簇内注意力和簇级稀疏性诱导，显著提升了MIL在计算病理学中的鲁棒性、可解释性和计算效率，并在公开数据集上取得了最先进的性能。

Abstract: Multi-Instance Learning (MIL) is pivotal for analyzing complex, weakly
labeled datasets, such as whole-slide images (WSIs) in computational pathology,
where bags comprise unordered collections of instances with sparse diagnostic
relevance. Traditional MIL approaches, including early statistical methods and
recent attention-based frameworks, struggle with instance redundancy and lack
explicit mechanisms for discarding non-informative instances, limiting their
robustness and interpretability. We propose Cluster-level Sparse MIL (csMIL), a
novel framework that integrates global-local instance clustering,
within-cluster attention, and cluster-level sparsity induction to address these
challenges. Our csMIL first performs global clustering across all bags to
establish $K$ cluster centers, followed by local clustering within each bag to
assign cluster labels. Attention scores are computed within each cluster, and
sparse regularization is applied to cluster weights, enabling the selective
retention of diagnostically relevant clusters while discarding irrelevant ones.
This approach enhances robustness to noisy instances, improves interpretability
by identifying critical regions, and reduces computational complexity.
Theoretical analysis demonstrates that csMIL requires $O(s log K)$ bags to
recover $s$ relevant clusters, aligning with compressed sensing principles.
Empirically, csMIL achieves state-of-the-art performance on two public
histopathology benchmarks (CAMELYON16, TCGA-NSCLC).

</details>


### [29] [Action Hints: Semantic Typicality and Context Uniqueness for Generalizable Skeleton-based Video Anomaly Detection](https://arxiv.org/abs/2509.11058)
*Canhui Tang,Sanping Zhou,Haoyue Shi,Le Wang*

Main category: cs.CV

TL;DR: 提出一种零样本视频异常检测框架，通过动作典型性和独特性学习提升骨架数据的泛化能力，无需目标域训练数据，在多个数据集上取得最优结果。


<details>
  <summary>Details</summary>
Motivation: 解决零样本视频异常检测中的域泛化问题，利用骨架数据的固有优势消除背景和外观的域差异。

Method: 通过动作典型性和独特性学习解锁骨架数据的潜力，包括语言引导的语义典型性建模模块和测试时上下文独特性分析模块。

Result: 在ShanghaiTech、UBnormal、NWPU和UCF-Crime数据集上表现优异，覆盖100多个未见过的监控场景。

Conclusion: 提出的框架在四个大规模VAD数据集上实现了最先进的性能，无需目标域训练数据。

Abstract: Zero-Shot Video Anomaly Detection (ZS-VAD) requires temporally localizing
anomalies without target domain training data, which is a crucial task due to
various practical concerns, e.g., data privacy or new surveillance deployments.
Skeleton-based approach has inherent generalizable advantages in achieving
ZS-VAD as it eliminates domain disparities both in background and human
appearance. However, existing methods only learn low-level skeleton
representation and rely on the domain-limited normality boundary, which cannot
generalize well to new scenes with different normal and abnormal behavior
patterns. In this paper, we propose a novel zero-shot video anomaly detection
framework, unlocking the potential of skeleton data via action typicality and
uniqueness learning. Firstly, we introduce a language-guided semantic
typicality modeling module that projects skeleton snippets into action semantic
space and distills LLM's knowledge of typical normal and abnormal behaviors
during training. Secondly, we propose a test-time context uniqueness analysis
module to finely analyze the spatio-temporal differences between skeleton
snippets and then derive scene-adaptive boundaries. Without using any training
samples from the target domain, our method achieves state-of-the-art results
against skeleton-based methods on four large-scale VAD datasets: ShanghaiTech,
UBnormal, NWPU, and UCF-Crime, featuring over 100 unseen surveillance scenes.

</details>


### [30] [Organoid Tracker: A SAM2-Powered Platform for Zero-shot Cyst Analysis in Human Kidney Organoid Videos](https://arxiv.org/abs/2509.11063)
*Xiaoyu Huang,Lauren M Maxson,Trang Nguyen,Cheng Jack Song,Yuankai Huo*

Main category: cs.CV

TL;DR: 开源平台 Organoid Tracker 利用 SAM2 实现自动化分析，提升肾脏器官模型研究效率。


<details>
  <summary>Details</summary>
Motivation: 解决当前手动分析方法在像素级和纵向信息上的局限性，提升肾脏疾病机制研究和药物发现的效率。

Method: 基于 Segment Anything Model 2 (SAM2) 构建的图形用户界面（GUI）平台，支持零样本分割和自动化分析，量化囊泡形成率、生长速度等关键指标。

Result: 开发了 Organoid Tracker，能够从时空显微镜视频中提取详细定量指标，并生成全面报告。

Conclusion: Organoid Tracker 提供了一个开源、可扩展的平台，显著提升了肾脏器官模型的研究效率，特别是在多囊肾病（PKD）的建模和治疗发现方面。

Abstract: Recent advances in organoid models have revolutionized the study of human
kidney disease mechanisms and drug discovery by enabling scalable,
cost-effective research without the need for animal sacrifice. Here, we present
a kidney organoid platform optimized for efficient screening in polycystic
kidney disease (PKD). While these systems generate rich spatial-temporal
microscopy video datasets, current manual approaches to analysis remain limited
to coarse classifications (e.g., hit vs. non-hit), often missing valuable
pixel-level and longitudinal information. To help overcome this bottleneck, we
developed Organoid Tracker, a graphical user interface (GUI) platform designed
with a modular plugin architecture, which empowers researchers to extract
detailed, quantitative metrics without programming expertise. Built on the
cutting-edge vision foundation model Segment Anything Model 2 (SAM2), Organoid
Tracker enables zero-shot segmentation and automated analysis of
spatial-temporal microscopy videos. It quantifies key metrics such as cyst
formation rate, growth velocity, and morphological changes, while generating
comprehensive reports. By providing an extensible, open-source framework,
Organoid Tracker offers a powerful solution for improving and accelerating
research in kidney development, PKD modeling, and therapeutic discovery. The
platform is publicly available as open-source software at
https://github.com/hrlblab/OrganoidTracker.

</details>


### [31] [The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge](https://arxiv.org/abs/2509.11071)
*Jinghan Peng,Jingwen Wang,Xing Yu,Dehui Du*

Main category: cs.CV

TL;DR: 该研究通过改进LLaVA模型并整合深度信息，结合Chain-of-Thought推理，在自动驾驶语言任务中取得验证集第一的成绩。


<details>
  <summary>Details</summary>
Motivation: 旨在利用视觉语言模型提升自动驾驶场景中的语言理解与决策能力，参与CVPR 2024自动驾驶挑战赛。

Method: 基于LLaVA模型，采用LoRA和DoRA进行微调，并整合开源深度估计模型的信息，同时在推理阶段应用Chain-of-Thought方法处理选择题和是非题。

Result: 在验证集上取得了0.7799的最高分，排名第一。

Conclusion: 作者团队通过结合视觉语言模型、深度信息以及Chain-of-Thought推理方法，在CVPR 2024自动驾驶挑战赛中取得了验证集排行榜第一的成绩，证明了其方法的有效性。

Abstract: This report outlines our approach using vision language model systems for the
Driving with Language track of the CVPR 2024 Autonomous Grand Challenge. We
have exclusively utilized the DriveLM-nuScenes dataset for training our models.
Our systems are built on the LLaVA models, which we enhanced through
fine-tuning with the LoRA and DoRA methods. Additionally, we have integrated
depth information from open-source depth estimation models to enrich the
training and inference processes. For inference, particularly with
multiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning
approach to improve the accuracy of the results. This comprehensive methodology
enabled us to achieve a top score of 0.7799 on the validation set leaderboard,
ranking 1st on the leaderboard.

</details>


### [32] [Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation](https://arxiv.org/abs/2509.11082)
*Zongwu Xie,Kaijie Yun,Yang Liu,Yiming Ji,Han Li*

Main category: cs.CV

TL;DR: 本文提出了一种多模态框架，通过融合相机和LiDAR数据自监督学习预测行星探测车的可通行性成本图，模型鲁棒性强，未来工作包括领域泛化和数据集扩展。


<details>
  <summary>Details</summary>
Motivation: 为了解决行星探测车在复杂地形中的可通行性预测问题，开发一个能够融合多模态数据并自监督学习的模型。

Method: 模型采用DINOv3-based图像编码器、FiLM-based传感器融合，以及结合Huber和平滑项的优化损失函数。通过实验消融（如移除图像颜色、遮挡输入、添加噪声）验证了模型的鲁棒性。

Result: 实验表明，模型在MAE/MSE上的变化较小（如LiDAR稀疏化时MAE从~0.0775增至0.0915），表明几何信息主导了学习成本，模型具有高度鲁棒性。

Conclusion: 本文提出了一种鲁棒的多模态框架，用于预测行星探测车的可通行性成本图。通过融合相机和LiDAR数据，结合自监督学习，模型表现出高度的鲁棒性。未来工作包括领域泛化和数据集扩展。

Abstract: We present a robust multi-modal framework for predicting traversability
costmaps for planetary rovers. Our model fuses camera and LiDAR data to produce
a bird's-eye-view (BEV) terrain costmap, trained self-supervised using
IMU-derived labels. Key updates include a DINOv3-based image encoder,
FiLM-based sensor fusion, and an optimization loss combining Huber and
smoothness terms. Experimental ablations (removing image color, occluding
inputs, adding noise) show only minor changes in MAE/MSE (e.g. MAE increases
from ~0.0775 to 0.0915 when LiDAR is sparsified), indicating that geometry
dominates the learned cost and the model is highly robust. We attribute the
small performance differences to the IMU labeling primarily reflecting terrain
geometry rather than semantics and to limited data diversity. Unlike prior work
claiming large gains, we emphasize our contributions: (1) a high-fidelity,
reproducible simulation environment; (2) a self-supervised IMU-based labeling
pipeline; and (3) a strong multi-modal BEV costmap prediction model. We discuss
limitations and future work such as domain generalization and dataset
expansion.

</details>


### [33] [End-to-End Visual Autonomous Parking via Control-Aided Attention](https://arxiv.org/abs/2509.11090)
*Chao Chen,Shunyu Yao,Yuanwu He,Tao Feng,Ruojing Song,Yuliang Guo,Xinyu Huang,Chenxu Wu,Ren Liu,Chen Feng*

Main category: cs.CV

TL;DR: CAA-Policy通过控制辅助注意力和自监督训练，解决了端到端停车中感知与控制协同不足的问题，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有端到端学习方法在感知与控制间缺乏有效协同，且基于自注意力的空间注意力不稳定，影响策略可靠性。

Method: 提出CAA-Policy，结合控制辅助注意力机制和自监督训练，利用控制输出的梯度引导注意力学习，并引入短时路径预测作为辅助任务。

Result: 在CARLA模拟器中，CAA-Policy在准确性、鲁棒性和可解释性上均优于基线方法和模块化流程。

Conclusion: CAA-Policy通过创新的控制辅助注意力机制和自监督训练策略，显著提升了端到端停车系统的准确性、鲁棒性和可解释性，超越了现有基线方法。

Abstract: Precise parking requires an end-to-end system where perception adaptively
provides policy-relevant details-especially in critical areas where fine
control decisions are essential. End-to-end learning offers a unified framework
by directly mapping sensor inputs to control actions, but existing approaches
lack effective synergy between perception and control. We find that
transformer-based self-attention, when used alone, tends to produce unstable
and temporally inconsistent spatial attention, which undermines the reliability
of downstream policy decisions over time. Instead, we propose CAA-Policy, an
end-to-end imitation learning system that allows control signal to guide the
learning of visual attention via a novel Control-Aided Attention (CAA)
mechanism. For the first time, we train such an attention module in a
self-supervised manner, using backpropagated gradients from the control outputs
instead of from the training loss. This strategy encourages the attention to
focus on visual features that induce high variance in action outputs, rather
than merely minimizing the training loss-a shift we demonstrate leads to a more
robust and generalizable policy. To further enhance stability, CAA-Policy
integrates short-horizon waypoint prediction as an auxiliary task, and
introduces a separately trained motion prediction module to robustly track the
target spot over time. Extensive experiments in the CARLA simulator show that
\titlevariable~consistently surpasses both the end-to-end learning baseline and
the modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy,
robustness, and interpretability. Code is released at
https://github.com/Joechencc/CAAPolicy.

</details>


### [34] [PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation](https://arxiv.org/abs/2509.11092)
*Zeyu Dong,Yuyang Yin,Yuqi Li,Eric Li,Hao-Xiang Guo,Yikai Wang*

Main category: cs.CV

TL;DR: 通过低秩适应技术，将预训练视频扩散模型微调以生成高质量全景视频，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决全景视频生成中的挑战，传统模型难以适应全景与视角投影的根本差异。

Method: 利用低秩适应（LoRA）技术，从视角视频生成全景视频，仅需约1,000个视频进行微调。

Result: 实验结果表明，该方法在保持正确投影几何的同时，在视觉质量、左右一致性和运动多样性方面优于现有方法。

Conclusion: 该方法通过LoRA有效地将预训练的视频扩散模型适应于全景视频生成，且在视觉质量、左右一致性和运动多样性方面超越了现有技术。

Abstract: Generating high-quality 360{\deg} panoramic videos remains a significant
challenge due to the fundamental differences between panoramic and traditional
perspective-view projections. While perspective videos rely on a single
viewpoint with a limited field of view, panoramic content requires rendering
the full surrounding environment, making it difficult for standard video
generation models to adapt. Existing solutions often introduce complex
architectures or large-scale training, leading to inefficiency and suboptimal
results. Motivated by the success of Low-Rank Adaptation (LoRA) in style
transfer tasks, we propose treating panoramic video generation as an adaptation
problem from perspective views. Through theoretical analysis, we demonstrate
that LoRA can effectively model the transformation between these projections
when its rank exceeds the degrees of freedom in the task. Our approach
efficiently fine-tunes a pretrained video diffusion model using only
approximately 1,000 videos while achieving high-quality panoramic generation.
Experimental results demonstrate that our method maintains proper projection
geometry and surpasses previous state-of-the-art approaches in visual quality,
left-right consistency, and motion diversity.

</details>


### [35] [SMILE: A Super-resolution Guided Multi-task Learning Method for Hyperspectral Unmixing](https://arxiv.org/abs/2509.11093)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: SMILE通过理论分析和多任务学习框架，解决了超分辨率与解混结合的挑战，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决超分辨率与解混直接结合时的任务亲和性未验证和解混收敛性无保障的问题。

Method: 提出了超分辨率引导的多任务学习方法（SMILE），通过理论分析验证任务亲和性，并设计共享和特定表示的学习框架。

Result: 在合成和真实数据集上的实验验证了SMILE框架的有效性。

Conclusion: SMILE通过提供渐进式理论支持和设计新的框架，成功解决了超分辨率与解混任务直接结合时的挑战，实验验证了其有效性。

Abstract: The performance of hyperspectral unmixing may be constrained by low spatial
resolution, which can be enhanced using super-resolution in a multitask
learning way. However, integrating super-resolution and unmixing directly may
suffer two challenges: Task affinity is not verified, and the convergence of
unmixing is not guaranteed. To address the above issues, in this paper, we
provide theoretical analysis and propose super-resolution guided multi-task
learning method for hyperspectral unmixing (SMILE). The provided theoretical
analysis validates feasibility of multitask learning way and verifies task
affinity, which consists of relationship and existence theorems by proving the
positive guidance of super-resolution. The proposed framework generalizes
positive information from super-resolution to unmixing by learning both shared
and specific representations. Moreover, to guarantee the convergence, we
provide the accessibility theorem by proving the optimal solution of unmixing.
The major contributions of SMILE include providing progressive theoretical
support, and designing a new framework for unmixing under the guidance of
super-resolution. Our experiments on both synthetic and real datasets have
substantiate the usefulness of our work.

</details>


### [36] [A Copula-Guided Temporal Dependency Method for Multitemporal Hyperspectral Images Unmixing](https://arxiv.org/abs/2509.11096)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: 本文提出Cog-TD方法，利用copula理论建模多时相高光谱解混中的时间依赖性，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模时间依赖性方面存在局限，无法捕捉动态材料演化，copula理论能明确建模依赖结构。

Method: 提出了一种基于copula理论的时间依赖方法（Cog-TD），包括数学模型、copula引导框架和两个关键模块。

Result: 在合成和真实数据集上的实验结果表明，Cog-TD方法在多时相高光谱解混中具有实用性。

Conclusion: 本文通过提出Cog-TD方法，成功解决了多时相高光谱解混中时间依赖建模的局限性，实验证明了其有效性。

Abstract: Multitemporal hyperspectral unmixing (MTHU) aims to model variable endmembers
and dynamical abundances, which emphasizes the critical temporal information.
However, existing methods have limitations in modeling temporal dependency,
thus fail to capture the dynamical material evolution. Motivated by the ability
of copula theory in modeling dependency structure explicitly, in this paper, we
propose a copula-guided temporal dependency method (Cog-TD) for multitemporal
hyperspectral unmixing. Cog-TD defines new mathematical model, constructs
copula-guided framework and provides two key modules with theoretical support.
The mathematical model provides explicit formulations for MTHU problem
definition, which describes temporal dependency structure by incorporating
copula theory. The copula-guided framework is constructed for utilizing copula
function, which estimates dynamical endmembers and abundances with temporal
dependency. The key modules consist of copula function estimation and temporal
dependency guidance, which computes and employs temporal information to guide
unmixing process. Moreover, the theoretical support demonstrates that estimated
copula function is valid and the represented temporal dependency exists in
hyperspectral images. The major contributions of this paper include redefining
MTHU problem with temporal dependency, proposing a copula-guided framework,
developing two key modules and providing theoretical support. Our experimental
results on both synthetic and real-world datasets demonstrate the utility of
the proposed method.

</details>


### [37] [3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment](https://arxiv.org/abs/2509.11097)
*Nhut Le,Ehsan Karimi,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: 提出了首个灾后评估3D数据集3DAeroRelief，填补了灾害场景3D数据的空白，并验证了无人机和3D分割模型在灾害响应中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有3D基准数据集主要关注城市或室内场景，缺乏对灾害影响区域的关注，限制了灾害评估的深度和空间上下文。

Method: 使用低成本无人机采集数据，通过Structure-from-Motion和Multi-View Stereo技术重建密集3D点云，并通过手动2D标注投影到3D空间生成语义注释。

Result: 3DAeroRelief是首个专门针对灾后评估的3D基准数据集，捕获了真实灾害环境中的大规模室外场景和细粒度结构损坏。

Conclusion: 3DAeroRelief数据集为灾害响应中的3D场景理解提供了宝贵资源，展示了无人机在灾害评估中的潜力。

Abstract: Timely assessment of structural damage is critical for disaster response and
recovery. However, most prior work in natural disaster analysis relies on 2D
imagery, which lacks depth, suffers from occlusions, and provides limited
spatial context. 3D semantic segmentation offers a richer alternative, but
existing 3D benchmarks focus mainly on urban or indoor scenes, with little
attention to disaster-affected areas. To address this gap, we present
3DAeroRelief--the first 3D benchmark dataset specifically designed for
post-disaster assessment. Collected using low-cost unmanned aerial vehicles
(UAVs) over hurricane-damaged regions, the dataset features dense 3D point
clouds reconstructed via Structure-from-Motion and Multi-View Stereo
techniques. Semantic annotations were produced through manual 2D labeling and
projected into 3D space. Unlike existing datasets, 3DAeroRelief captures 3D
large-scale outdoor environments with fine-grained structural damage in
real-world disaster contexts. UAVs enable affordable, flexible, and safe data
collection in hazardous areas, making them particularly well-suited for
emergency scenarios. To demonstrate the utility of 3DAeroRelief, we evaluate
several state-of-the-art 3D segmentation models on the dataset to highlight
both the challenges and opportunities of 3D scene understanding in disaster
response. Our dataset serves as a valuable resource for advancing robust 3D
vision systems in real-world applications for post-disaster scenarios.

</details>


### [38] [Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2509.11102)
*Nhi Kieu,Kien Nguyen,Arnold Wiliem,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: GEMMNet通过混合特征提取、多尺度融合和互补损失方案，解决了多模态遥感数据中的缺失和偏差问题，在语义分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多模态信号在现实场景中容易缺失，现有生成模型（如AE和GAN）在遥感语义分割中的效果尚未充分探索，且存在语义上下文捕捉不足和模态偏差问题。

Method: 提出了GEMMNet，包含HyFEx（混合特征提取器）、HyFMA（多尺度感知的混合融合）和CoLoss（互补损失方案）三个核心组件。

Result: GEMMNet在Vaihingen和Potsdam数据集上优于生成基线（AE、cGAN）和非生成方法（mmformer、shaspec）。

Conclusion: GEMMNet通过结合HyFEx、HyFMA和CoLoss三个关键组件，有效解决了多模态遥感数据中的异质性问题和传统生成模型的局限性，在语义分割任务中表现优于现有生成和非生成方法。

Abstract: Multimodal learning has shown significant performance boost compared to
ordinary unimodal models across various domains. However, in real-world
scenarios, multimodal signals are susceptible to missing because of sensor
failures and adverse weather conditions, which drastically deteriorates models'
operation and performance. Generative models such as AutoEncoder (AE) and
Generative Adversarial Network (GAN) are intuitive solutions aiming to
reconstruct missing modality from available ones. Yet, their efficacy in remote
sensing semantic segmentation remains underexplored. In this paper, we first
examine the limitations of existing generative approaches in handling the
heterogeneity of multimodal remote sensing data. They inadequately capture
semantic context in complex scenes with large intra-class and small inter-class
variation. In addition, traditional generative models are susceptible to heavy
dependence on the dominant modality, introducing bias that affects model
robustness under missing modality conditions. To tackle these limitations, we
propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with
three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn
modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness
(HyFMA) to capture modality-synergistic semantic context across scales and (3)
Complementary Loss (CoLoss) scheme to alleviate the inherent bias by
encouraging consistency across modalities and tasks. Our method, GEMMNet,
outperforms both generative baselines AE, cGAN (conditional GAN), and
state-of-the-art non-generative approaches - mmformer and shaspec - on two
challenging semantic segmentation remote sensing datasets (Vaihingen and
Potsdam). Source code is made available.

</details>


### [39] [WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild](https://arxiv.org/abs/2509.11114)
*Yuqiu Liu,Jialin Song,Manolis Savva,Wuyang Chen*

Main category: cs.CV

TL;DR: 提出了一种从野外视频重建动态3D烟雾的流程，显著提升了重建质量并支持逼真的烟雾编辑。


<details>
  <summary>Details</summary>
Motivation: 尽管3D视觉的最新进展显著改善了流体动力学的重建和渲染，但当前的流体重建严重依赖于精心控制的实验室环境，而野外捕获的真实世界视频在很大程度上未被充分探索。

Method: 研究针对野外视频中重建烟雾的三个关键挑战设计了针对性技术，包括带背景去除的烟雾提取、烟雾粒子和相机姿态的初始化，以及多视角视频的推断。

Result: 该方法在野外视频上的平均PSNR提高了2.22，优于先前的重建和生成方法，并能通过模拟烟雾资产实现多样化和逼真的流体动力学编辑。

Conclusion: 该研究提出了一种从野外视频中提取和重建动态3D烟雾资产的流程，并进一步整合了交互式模拟用于烟雾设计和编辑。该方法不仅显著提升了烟雾重建的质量，还能实现多样化和逼真的流体动力学编辑。

Abstract: We propose a pipeline to extract and reconstruct dynamic 3D smoke assets from
a single in-the-wild video, and further integrate interactive simulation for
smoke design and editing. Recent developments in 3D vision have significantly
improved reconstructing and rendering fluid dynamics, supporting realistic and
temporally consistent view synthesis. However, current fluid reconstructions
rely heavily on carefully controlled clean lab environments, whereas real-world
videos captured in the wild are largely underexplored. We pinpoint three key
challenges of reconstructing smoke in real-world videos and design targeted
techniques, including smoke extraction with background removal, initialization
of smoke particles and camera poses, and inferring multi-view videos. Our
method not only outperforms previous reconstruction and generation methods with
high-quality smoke reconstructions (+2.22 average PSNR on wild videos), but
also enables diverse and realistic editing of fluid dynamics by simulating our
smoke assets. We provide our models, data, and 4D smoke assets at
[https://autumnyq.github.io/WildSmoke](https://autumnyq.github.io/WildSmoke).

</details>


### [40] [SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D Gaussian Splatting](https://arxiv.org/abs/2509.11116)
*Ashkan Taghipour,Vahid Naghshin,Benjamin Southwell,Farid Boussaid,Hamid Laga,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: SVR-GS通过空间变体正则化优化高斯数量，减少内存占用并提升实时性能，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于掩码的剪枝方法（如MaskGS）与局部重建损失不匹配，导致图像质量下降。

Method: 引入了空间变体正则化器SVR-GS，通过渲染每个高斯的空间掩码，探索了三种空间掩码聚合策略，并在CUDA中实现。

Result: 在三个数据集上，SVR-GS平均减少了1.79倍（相比MaskGS）和5.63倍（相比3DGS）的高斯数量，仅导致0.50 dB和0.40 dB的PSNR下降。

Conclusion: SVR-GS通过空间变体正则化显著减少了高斯数量，同时保持了较高的图像质量，适用于实时应用。

Abstract: 3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis
but typically relies on densification followed by pruning to optimize the
number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes
the global mean of the mask, which is misaligned with the local per-pixel
(per-ray) reconstruction loss that determines image quality along individual
camera rays. This paper introduces SVR-GS, a spatially variant regularizer that
renders a per-pixel spatial mask from each Gaussian's effective contribution
along the ray, thereby applying sparsity pressure where it matters: on
low-importance Gaussians. We explore three spatial-mask aggregation strategies,
implement them in CUDA, and conduct a gradient analysis to motivate our final
design. Extensive experiments on Tanks\&Temples, Deep Blending, and Mip-NeRF360
datasets demonstrate that, on average across the three datasets, the proposed
SVR-GS reduces the number of Gaussians by 1.79\(\times\) compared to MaskGS and
5.63\(\times\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR
drops, respectively. These gains translate into significantly smaller, faster,
and more memory-efficient models, making them well-suited for real-time
applications such as robotics, AR/VR, and mobile perception.

</details>


### [41] [No Mesh, No Problem: Estimating Coral Volume and Surface from Sparse Multi-View Images](https://arxiv.org/abs/2509.11164)
*Diego Eustachio Farchione,Ramzi Idoughi,Peter Wonka*

Main category: cs.CV

TL;DR: 提出一种轻量级学习框架，通过2D多视角RGB图像预测珊瑚的3D体积和表面积，结合高斯负对数似然损失函数提升预测稳定性，适用于珊瑚生长分析和礁石监测。


<details>
  <summary>Details</summary>
Motivation: 珊瑚复杂的形态使得通过精确的体积和表面积估计来量化珊瑚生长成为一项具有挑战性的任务。

Method: 利用预训练模块（VGGT）从多视角RGB图像中提取密集点图，合并为统一点云并通过并行DGCNN解码器头联合输出珊瑚的体积和表面积及其置信度估计。

Result: 该方法在准确性和泛化性上表现优异，能够适应未见过的珊瑚形态。

Conclusion: 该框架为直接从稀疏图像集进行高效、可扩展的珊瑚几何估计铺平了道路，在珊瑚生长分析和礁石监测中具有潜在应用。

Abstract: Effective reef monitoring requires the quantification of coral growth via
accurate volumetric and surface area estimates, which is a challenging task due
to the complex morphology of corals. We propose a novel, lightweight, and
scalable learning framework that addresses this challenge by predicting the 3D
volume and surface area of coral-like objects from 2D multi-view RGB images.
Our approach utilizes a pre-trained module (VGGT) to extract dense point maps
from each view; these maps are merged into a unified point cloud and enriched
with per-view confidence scores. The resulting cloud is fed to two parallel
DGCNN decoder heads, which jointly output the volume and the surface area of
the coral, as well as their corresponding confidence estimate. To enhance
prediction stability and provide uncertainty estimates, we introduce a
composite loss function based on Gaussian negative log-likelihood in both real
and log domains. Our method achieves competitive accuracy and generalizes well
to unseen morphologies. This framework paves the way for efficient and scalable
coral geometry estimation directly from a sparse set of images, with potential
applications in coral growth analysis and reef monitoring.

</details>


### [42] [Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic](https://arxiv.org/abs/2509.11165)
*Waikit Xiu,Qiang Lu,Xiying Li,Chen Hu,Shengbo Sun*

Main category: cs.CV

TL;DR: 提出了Traffic-MLLM，一种针对细粒度交通分析的多模态大语言模型，通过LoRA微调和知识提示模块显著提升了时空特征建模和逻辑推理能力，在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模时空因果关系和整合领域知识方面存在显著挑战，限制了其在复杂场景中的有效性。

Method: 基于Qwen2.5-VL主干，利用高质量交通特定多模态数据集和LoRA进行轻量级微调，并引入融合CoT推理与RAG的知识提示模块。

Result: Traffic-MLLM在TrafficQA和DriveQA基准测试中实现了最先进的性能。

Conclusion: Traffic-MLLM在TrafficQA和DriveQA基准测试中表现出色，验证了其在处理多模态交通数据方面的卓越能力，并展示了显著的零样本推理和跨场景泛化能力。

Abstract: As intelligent transportation systems advance, traffic video understanding
plays an increasingly pivotal role in comprehensive scene perception and causal
analysis. Yet, existing approaches face notable challenges in accurately
modeling spatiotemporal causality and integrating domain-specific knowledge,
limiting their effectiveness in complex scenarios. To address these
limitations, we propose Traffic-MLLM, a multimodal large language model
tailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone,
our model leverages high-quality traffic-specific multimodal datasets and uses
Low-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing
its capacity to model continuous spatiotemporal features in video sequences.
Furthermore, we introduce an innovative knowledge prompting module fusing
Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG),
enabling precise injection of detailed traffic regulations and domain knowledge
into the inference process. This design markedly boosts the model's logical
reasoning and knowledge adaptation capabilities. Experimental results on
TrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art
performance, validating its superior ability to process multimodal traffic
data. It also exhibits remarkable zero-shot reasoning and cross-scenario
generalization capabilities.

</details>


### [43] [Multispectral-NeRF:a multispectral modeling approach based on neural radiance fields](https://arxiv.org/abs/2509.11169)
*Hong Zhang,Fei Guo,Zihan Xie,Dizhao Yao*

Main category: cs.CV

TL;DR: Multispectral-NeRF通过改进NeRF架构，成功整合多波段光谱信息，实现了高精度3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有的多光谱3D重建方法存在成本高、精度低和几何特征差的问题，而NeRF及其改进模型仅适用于三波段数据，无法处理多波段信息。

Method: 提出了Multispectral-NeRF，一种基于NeRF的增强神经架构，通过扩展隐藏层维度以容纳6波段光谱输入、重新设计残差函数以优化光谱差异计算，以及调整数据压缩模块以适应多光谱图像的高位深度需求。

Result: 实验结果表明，Multispectral-NeRF能够有效处理多波段光谱特征，并准确保留原始场景的光谱特性。

Conclusion: Multispectral-NeRF通过扩展隐藏层维度、重新设计残差函数和调整数据压缩模块，成功解决了多波段光谱信息在3D重建中的整合问题，实现了高精度和高质量的3D重建结果。

Abstract: 3D reconstruction technology generates three-dimensional representations of
real-world objects, scenes, or environments using sensor data such as 2D
images, with extensive applications in robotics, autonomous vehicles, and
virtual reality systems. Traditional 3D reconstruction techniques based on 2D
images typically relies on RGB spectral information. With advances in sensor
technology, additional spectral bands beyond RGB have been increasingly
incorporated into 3D reconstruction workflows. Existing methods that integrate
these expanded spectral data often suffer from expensive scheme prices, low
accuracy and poor geometric features. Three - dimensional reconstruction based
on NeRF can effectively address the various issues in current multispectral 3D
reconstruction methods, producing high - precision and high - quality
reconstruction results. However, currently, NeRF and some improved models such
as NeRFacto are trained on three - band data and cannot take into account the
multi - band information. To address this problem, we propose
Multispectral-NeRF, an enhanced neural architecture derived from NeRF that can
effectively integrates multispectral information. Our technical contributions
comprise threefold modifications: Expanding hidden layer dimensionality to
accommodate 6-band spectral inputs; Redesigning residual functions to optimize
spectral discrepancy calculations between reconstructed and reference images;
Adapting data compression modules to address the increased bit-depth
requirements of multispectral imagery. Experimental results confirm that
Multispectral-NeRF successfully processes multi-band spectral features while
accurately preserving the original scenes' spectral characteristics.

</details>


### [44] [SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene Completion](https://arxiv.org/abs/2509.11171)
*Zhiwen Yang,Yuxin Peng*

Main category: cs.CV

TL;DR: SPHERE整合体素与高斯表示，通过SGI和PHE模块提升3D语义场景完成的语义与几何一致性，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有体素基和平面基方法在捕捉物理规律性方面的不足，以及神经重建方法在大规模复杂场景中计算成本高和语义准确性低的问题。

Method: SPHERE结合了体素和高斯表示，通过语义引导的高斯初始化（SGI）模块和物理感知的谐波增强（PHE）模块，实现了语义与物理信息的联合利用。

Result: 在SemanticKITTI和SSCBench-KITTI-360基准测试中验证了SPHERE的有效性。

Conclusion: SPHERE方法通过结合体素和高斯表示，有效提升了相机基3D语义场景完成的语义和几何一致性，生成了具有真实细节的结果。

Abstract: Camera-based 3D Semantic Scene Completion (SSC) is a critical task in
autonomous driving systems, assessing voxel-level geometry and semantics for
holistic scene perception. While existing voxel-based and plane-based SSC
methods have achieved considerable progress, they struggle to capture physical
regularities for realistic geometric details. On the other hand, neural
reconstruction methods like NeRF and 3DGS demonstrate superior physical
awareness, but suffer from high computational cost and slow convergence when
handling large-scale, complex autonomous driving scenes, leading to inferior
semantic accuracy. To address these issues, we propose the Semantic-PHysical
Engaged REpresentation (SPHERE) for camera-based SSC, which integrates voxel
and Gaussian representations for joint exploitation of semantic and physical
information. First, the Semantic-guided Gaussian Initialization (SGI) module
leverages dual-branch 3D scene representations to locate focal voxels as
anchors to guide efficient Gaussian initialization. Then, the Physical-aware
Harmonics Enhancement (PHE) module incorporates semantic spherical harmonics to
model physical-aware contextual details and promote semantic-geometry
consistency through focal distribution alignment, generating SSC results with
realistic details. Extensive experiments and analyses on the popular
SemanticKITTI and SSCBench-KITTI-360 benchmarks validate the effectiveness of
SPHERE. The code is available at
https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025.

</details>


### [45] [StegOT: Trade-offs in Steganography via Optimal Transport](https://arxiv.org/abs/2509.11178)
*Chengde Lin,Xuezhu Gong,Shuxue Ding,Mingzhe Yang,Xijun Lu,Chengjun Mo*

Main category: cs.CV

TL;DR: StegOT是一种基于最优传输理论的隐写模型，解决了模式崩溃问题，提升了图像隐写和恢复的质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于GAN和VAE的隐写模型普遍存在模式崩溃问题，导致隐写图像中封面和秘密图像之间信息不平衡，影响后续提取。

Method: 本文提出了一种基于自动编码器的隐写模型StegOT，结合了最优传输理论，设计了多通道最优传输（MCOT）模块，将多峰特征分布转换为单峰分布，以实现信息平衡。

Result: 实验表明，StegOT不仅实现了封面和秘密图像之间的信息平衡，还提升了隐写和恢复图像的质量。

Conclusion: StegOT通过引入最优传输理论，成功解决了现有模型中存在的模式崩溃问题，实现了封面图像和秘密图像之间的信息平衡，并提升了隐写和恢复图像的质量。

Abstract: Image hiding is often referred to as steganography, which aims to hide a
secret image in a cover image of the same resolution. Many steganography models
are based on genera-tive adversarial networks (GANs) and variational
autoencoders (VAEs). However, most existing models suffer from mode collapse.
Mode collapse will lead to an information imbalance between the cover and
secret images in the stego image and further affect the subsequent extraction.
To address these challenges, this paper proposes StegOT, an autoencoder-based
steganography model incorporating optimal transport theory. We designed the
multiple channel optimal transport (MCOT) module to transform the feature
distribution, which exhibits multiple peaks, into a single peak to achieve the
trade-off of information. Experiments demonstrate that we not only achieve a
trade-off between the cover and secret images but also enhance the quality of
both the stego and recovery images. The source code will be released on
https://github.com/Rss1124/StegOT.

</details>


### [46] [The Impact of Skin Tone Label Granularity on the Performance and Fairness of AI Based Dermatology Image Classification Models](https://arxiv.org/abs/2509.11184)
*Partha Shah,Durva Sankhe,Maariyah Rashid,Zakaa Khaled,Esther Puyol-Antón,Tiarna Lee,Maram Alqarni,Sweta Rai,Andrew P. King*

Main category: cs.CV

TL;DR: 研究发现FST量表的分类粒度显著影响AI皮肤病变分类模型的性能和偏见，建议采用更公平的肤色替代量表。


<details>
  <summary>Details</summary>
Motivation: 探讨FST量表中肤色分类粒度对AI分类模型性能和偏见的影响。

Method: 通过训练多个AI模型，使用不同粒度的FST特异性数据对良性 vs. 恶性皮肤病变进行分类。

Result: （i）使用FST特异性数据训练的模型性能优于通用模型；（ii）降低FST信息粒度会对性能产生不利影响。

Conclusion: 该论文建议在公平AI研究中逐步放弃FST量表，转向能更好代表人类肤色多样性的替代量表。

Abstract: Artificial intelligence (AI) models to automatically classify skin lesions
from dermatology images have shown promising performance but also
susceptibility to bias by skin tone. The most common way of representing skin
tone information is the Fitzpatrick Skin Tone (FST) scale. The FST scale has
been criticised for having greater granularity in its skin tone categories for
lighter-skinned subjects. This paper conducts an investigation of the impact
(on performance and bias) on AI classification models of granularity in the FST
scale. By training multiple AI models to classify benign vs. malignant lesions
using FST-specific data of differing granularity, we show that: (i) when
training models using FST-specific data based on three groups (FST 1/2, 3/4 and
5/6), performance is generally better for models trained on FST-specific data
compared to a general model trained on FST-balanced data; (ii) reducing the
granularity of FST scale information (from 1/2 and 3/4 to 1/2/3/4) can have a
detrimental effect on performance. Our results highlight the importance of the
granularity of FST groups when training lesion classification models. Given the
question marks over possible human biases in the choice of categories in the
FST scale, this paper provides evidence for a move away from the FST scale in
fair AI research and a transition to an alternative scale that better
represents the diversity of human skin tones.

</details>


### [47] [Scaling Up Forest Vision with Synthetic Data](https://arxiv.org/abs/2509.11201)
*Yihang She,Andrew Blake,David Coomes,Srinivasan Keshav*

Main category: cs.CV

TL;DR: 该论文提出了一种利用合成数据预训练和少量真实数据微调的方法，显著减少了对真实标注数据的需求，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于现有公共3D森林数据集规模不足，无法构建鲁棒的树木分割系统，受合成数据在其他领域成功的启发，研究探索了合成数据在树木分割中的应用。

Method: 开发了一种新的合成数据生成流程，结合游戏引擎和基于物理的LiDAR模拟，生成了一个大规模、多样化的标注3D森林数据集。

Result: 实验表明，仅需对不到0.1公顷的真实森林地块进行微调，预训练模型的分割效果即可与全量真实数据训练的模型媲美。

Conclusion: 该研究通过合成数据预训练和少量真实数据微调的方法，显著减少了真实标注数据的需求，为未来更稳健的3D森林视觉系统铺平了道路。

Abstract: Accurate tree segmentation is a key step in extracting individual tree
metrics from forest laser scans, and is essential to understanding ecosystem
functions in carbon cycling and beyond. Over the past decade, tree segmentation
algorithms have advanced rapidly due to developments in AI. However existing,
public, 3D forest datasets are not large enough to build robust tree
segmentation systems. Motivated by the success of synthetic data in other
domains such as self-driving, we investigate whether similar approaches can
help with tree segmentation. In place of expensive field data collection and
annotation, we use synthetic data during pretraining, and then require only
minimal, real forest plot annotation for fine-tuning.
  We have developed a new synthetic data generation pipeline to do this for
forest vision tasks, integrating advances in game-engines with physics-based
LiDAR simulation. As a result, we have produced a comprehensive, diverse,
annotated 3D forest dataset on an unprecedented scale. Extensive experiments
with a state-of-the-art tree segmentation algorithm and a popular real dataset
show that our synthetic data can substantially reduce the need for labelled
real data. After fine-tuning on just a single, real, forest plot of less than
0.1 hectare, the pretrained model achieves segmentations that are competitive
with a model trained on the full scale real data. We have also identified
critical factors for successful use of synthetic data: physics, diversity, and
scale, paving the way for more robust 3D forest vision systems in the future.
Our data generation pipeline and the resulting dataset are available at
https://github.com/yihshe/CAMP3D.git.

</details>


### [48] [Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation](https://arxiv.org/abs/2509.11213)
*Yufei Tang,Daiheng Gao,Pingyu Wu,Wenbo Zhou,Bang Zhang,Weiming Zhang*

Main category: cs.CV

TL;DR: Beyond Sliders 结合 GANs 和扩散模型，通过细粒度引导提升图像真实感，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如概念滑块）在处理非 AIGC 图像（尤其是真实世界拍摄的图像）时表现不佳，需要更先进的解决方案。

Method: 该方法改进了概念滑块，通过文本和视觉的细粒度引导以对抗方式优化图像。

Result: 实验验证表明，Beyond Sliders 在各类应用中表现出色，图像质量和真实感显著提升。

Conclusion: Beyond Sliders 框架通过整合 GANs 和扩散模型，显著提升了图像质量和真实感，并在多种应用中验证了其鲁棒性和多功能性。

Abstract: In the realm of image generation, the quest for realism and customization has
never been more pressing. While existing methods like concept sliders have made
strides, they often falter when it comes to no-AIGC images, particularly images
captured in real world settings. To bridge this gap, we introduce Beyond
Sliders, an innovative framework that integrates GANs and diffusion models to
facilitate sophisticated image manipulation across diverse image categories.
Improved upon concept sliders, our method refines the image through fine
grained guidance both textual and visual in an adversarial manner, leading to a
marked enhancement in image quality and realism. Extensive experimental
validation confirms the robustness and versatility of Beyond Sliders across a
spectrum of applications.

</details>


### [49] [Geometrically Constrained and Token-Based Probabilistic Spatial Transformers](https://arxiv.org/abs/2509.11218)
*Johann Schmidt,Sebastian Stober*

Main category: cs.CV

TL;DR: 论文提出了一种概率性组件化的STN扩展方法，通过分解仿射变换并建模不确定性，提升了FGVC任务中对几何变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类（FGVC）对几何变化高度敏感，现有等变架构通常需要大量计算资源并限制假设空间。

Method: 提出了一个概率性的组件化扩展方法，将仿射变换分解为旋转、缩放和剪切，并使用共享定位编码器在几何约束下回归每个组件。通过高斯变分后验建模每个组件的不确定性，并在推理时进行基于采样的规范化。

Result: 在具有挑战性的蛾类分类基准测试中，该方法相比其他空间变换网络（STNs）显著提高了鲁棒性。

Conclusion: 该论文通过概率性、组件化的空间变换网络扩展，显著提升了细粒度视觉分类任务中对几何变化的鲁棒性。

Abstract: Fine-grained visual classification (FGVC) remains highly sensitive to
geometric variability, where objects appear under arbitrary orientations,
scales, and perspective distortions. While equivariant architectures address
this issue, they typically require substantial computational resources and
restrict the hypothesis space. We revisit Spatial Transformer Networks (STNs)
as a canonicalization tool for transformer-based vision pipelines, emphasizing
their flexibility, backbone-agnostic nature, and lack of architectural
constraints. We propose a probabilistic, component-wise extension that improves
robustness. Specifically, we decompose affine transformations into rotation,
scaling, and shearing, and regress each component under geometric constraints
using a shared localization encoder. To capture uncertainty, we model each
component with a Gaussian variational posterior and perform sampling-based
canonicalization during inference.A novel component-wise alignment loss
leverages augmentation parameters to guide spatial alignment. Experiments on
challenging moth classification benchmarks demonstrate that our method
consistently improves robustness compared to other STNs.

</details>


### [50] [CCoMAML: Efficient Cattle Identification Using Cooperative Model-Agnostic Meta-Learning](https://arxiv.org/abs/2509.11219)
*Rabin Dulal,Lihong Zheng,Ashad Kabir*

Main category: cs.CV

TL;DR: 本文提出了一种基于CCoMAML和MHAFF的少样本学习框架，用于牛只识别，显著提高了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统RFID耳标系统存在易丢失、损坏、篡改等缺陷，而基于牛鼻纹的生物识别技术因其独特性成为有前景的替代方案。然而，深度学习模型面临数据不足、采集干扰和动态牛群变化等挑战。

Method: 提出了一种新颖的少样本学习框架CCoMAML，结合多头部注意力特征融合（MHAFF）作为特征提取模型。

Result: 实验结果表明，CCoMAML与MHAFF在牛只识别中表现优异，F1分数分别为98.46%和97.91%。

Conclusion: 本文提出的CCoMAML与MHAFF框架在牛只识别中表现出色，F1分数达到98.46%和97.91%，显著优于现有少样本学习方法。

Abstract: Cattle identification is critical for efficient livestock farming management,
currently reliant on radio-frequency identification (RFID) ear tags. However,
RFID-based systems are prone to failure due to loss, damage, tampering, and
vulnerability to external attacks. As a robust alternative, biometric
identification using cattle muzzle patterns similar to human fingerprints has
emerged as a promising solution. Deep learning techniques have demonstrated
success in leveraging these unique patterns for accurate identification. But
deep learning models face significant challenges, including limited data
availability, disruptions during data collection, and dynamic herd compositions
that require frequent model retraining. To address these limitations, this
paper proposes a novel few-shot learning framework for real-time cattle
identification using Cooperative Model-Agnostic Meta-Learning (CCoMAML) with
Multi-Head Attention Feature Fusion (MHAFF) as a feature extractor model. This
model offers great model adaptability to new data through efficient learning
from few data samples without retraining. The proposed approach has been
rigorously evaluated against current state-of-the-art few-shot learning
techniques applied in cattle identification. Comprehensive experimental results
demonstrate that our proposed CCoMAML with MHAFF has superior cattle
identification performance with 98.46% and 97.91% F1 scores.

</details>


### [51] [ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification](https://arxiv.org/abs/2509.11220)
*Gao Yu Lee,Tanmoy Dam,Md Meftahul Ferdaus,Daniel Puiu Poenar,Vu N. Duong*

Main category: cs.CV

TL;DR: ANROT-HELANet是一种基于Hellinger距离的Few-Shot Learning网络，显著提升鲁棒性和性能，对抗扰动和噪声表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的贝叶斯估计方法（如KL散度）在Few-Shot Learning中表现良好，但对对抗性攻击和自然噪声脆弱。

Method: 采用基于Hellinger距离的特征类聚合方案，结合注意力机制和新型Hellinger相似性对比损失函数。

Result: 在对抗性扰动（ε=0.30）和高斯噪声（σ=0.30）下表现稳健，在miniImageNet的1-shot和5-shot场景中分别提升1.20%和1.40%，图像重建质量（FID=2.75）优于传统方法。

Conclusion: ANROT-HELANet通过结合Hellinger距离特征聚合、注意力机制和新型损失函数，在Few-Shot Learning中实现了最先进的性能，同时保持了对对抗性和自然扰动的鲁棒性。

Abstract: Few-Shot Learning (FSL), which involves learning to generalize using only a
few data samples, has demonstrated promising and superior performances to
ordinary CNN methods. While Bayesian based estimation approaches using
Kullback-Leibler (KL) divergence have shown improvements, they remain
vulnerable to adversarial attacks and natural noises. We introduce
ANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation
Network that significantly advances the state-of-the-art in FSL robustness and
performance. Our approach implements an adversarially and naturally robust
Hellinger distance-based feature class aggregation scheme, demonstrating
resilience to adversarial perturbations up to $\epsilon=0.30$ and Gaussian
noise up to $\sigma=0.30$. The network achieves substantial improvements across
benchmark datasets, including gains of 1.20\% and 1.40\% for 1-shot and 5-shot
scenarios on miniImageNet respectively. We introduce a novel Hellinger
Similarity contrastive loss function that generalizes cosine similarity
contrastive loss for variational few-shot inference scenarios. Our approach
also achieves superior image reconstruction quality with a FID score of 2.75,
outperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive
experiments conducted on four few-shot benchmarked datasets verify that
ANROT-HELANet's combination of Hellinger distance-based feature aggregation,
attention mechanisms, and our novel loss function establishes new
state-of-the-art performance while maintaining robustness against both
adversarial and natural perturbations. Our code repository will be available at
https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main.

</details>


### [52] [MIS-LSTM: Multichannel Image-Sequence LSTM for Sleep Quality and Stress Prediction](https://arxiv.org/abs/2509.11232)
*Seongwan Park,Jieun Woo,Siheon Yang*

Main category: cs.CV

TL;DR: MIS-LSTM框架结合CNN和LSTM，通过UALRE集成提升睡眠质量和压力预测性能，实验显示优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 从多模态生活日志数据中预测睡眠质量和压力，需要处理连续传感器数据和稀疏离散事件，并捕捉长时程依赖关系。

Method: 提出MIS-LSTM混合框架，结合CNN编码器和LSTM序列模型，并引入UALRE不确定性感知集成方法。

Result: 在2025 ETRI Lifelog Challenge数据集上，MIS-LSTM的Macro-F1为0.615，加入UALRE后提升至0.647。

Conclusion: MIS-LSTM结合UALRE集成方法在睡眠质量和压力预测任务上表现出色，优于传统LSTM、1D-CNN和CNN基线模型。

Abstract: This paper presents MIS-LSTM, a hybrid framework that joins CNN encoders with
an LSTM sequence model for sleep quality and stress prediction at the day level
from multimodal lifelog data. Continuous sensor streams are first partitioned
into N-hour blocks and rendered as multi-channel images, while sparse discrete
events are encoded with a dedicated 1D-CNN. A Convolutional Block Attention
Module fuses the two modalities into refined block embeddings, which an LSTM
then aggregates to capture long-range temporal dependencies. To further boost
robustness, we introduce UALRE, an uncertainty-aware ensemble that overrides
lowconfidence majority votes with high-confidence individual predictions.
Experiments on the 2025 ETRI Lifelog Challenge dataset show that Our base
MISLSTM achieves Macro-F1 0.615; with the UALRE ensemble, the score improves to
0.647, outperforming strong LSTM, 1D-CNN, and CNN baselines. Ablations confirm
(i) the superiority of multi-channel over stacked-vertical imaging, (ii) the
benefit of a 4-hour block granularity, and (iii) the efficacy of
modality-specific discrete encoding.

</details>


### [53] [Contextualized Multimodal Lifelong Person Re-Identification in Hybrid Clothing States](https://arxiv.org/abs/2509.11247)
*Robert Long,Rongxin Jiang,Mingrui Yan*

Main category: cs.CV

TL;DR: 提出了CMLReID框架，结合CASP和AKFP，有效解决衣物变化和持续学习问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决衣物变化（CCReID）和持续学习（LReID）在现实监控系统中的挑战，避免现有方法仅针对单一应用的局限性。

Method: 提出了CMLReID框架，包含两个新任务：CASP（生成自适应提示并整合上下文）和AKFP（通过双路径学习器和对齐特征生成稳健的SC/CC原型）。

Result: 在多个数据集上的实验表明，CMLReID优于所有现有方法，具有强大的鲁棒性和泛化能力。

Conclusion: CMLReID框架通过结合CASP和AKFP，有效解决了衣物变化和持续学习的问题，表现出强大的鲁棒性和泛化能力。

Abstract: Person Re-Identification (ReID) has several challenges in real-world
surveillance systems due to clothing changes (CCReID) and the need for
maintaining continual learning (LReID). Previous existing methods either
develop models specifically for one application, which is mostly a same-cloth
(SC) setting or treat CCReID as its own separate sub-problem. In this work, we
will introduce the LReID-Hybrid task with the goal of developing a model to
achieve both SC and CC while learning in a continual setting. Mismatched
representations and forgetting from one task to the next are significant
issues, we address this with CMLReID, a CLIP-based framework composed of two
novel tasks: (1) Context-Aware Semantic Prompt (CASP) that generates adaptive
prompts, and also incorporates context to align richly multi-grained visual
cues with semantic text space; and (2) Adaptive Knowledge Fusion and Projection
(AKFP) which produces robust SC/CC prototypes through the use of a dual-path
learner that aligns features with our Clothing-State-Aware Projection Loss.
Experiments performed on a wide range of datasets and illustrate that CMLReID
outperforms all state-of-the-art methods with strong robustness and
generalization despite clothing variations and a sophisticated process of
sequential learning.

</details>


### [54] [Cross-Domain Attribute Alignment with CLIP: A Rehearsal-Free Approach for Class-Incremental Unsupervised Domain Adaptation](https://arxiv.org/abs/2509.11264)
*Kerun Mi,Guoliang Kang,Guangyu Li,Lin Zhao,Tao Zhou,Chen Gong*

Main category: cs.CV

TL;DR: 提出一种基于CLIP的类无关属性建模方法，通过跨域属性对齐减轻灾难性遗忘，无需存储历史样本，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决类增量无监督域适应（CI-UDA）中的灾难性遗忘问题，避免存储和利用历史目标样本的繁琐操作。

Method: 通过使用CLIP提取类无关属性（称为“属性”），并学习“键值”对来表示属性。维护两个属性字典（分别对应不同域），并通过视觉注意力一致性和预测一致性进行跨域属性对齐。

Result: 在三个CI-UDA基准测试中表现优于现有方法，且无需存储历史样本。

Conclusion: 实验结果证明，该方法在三个CI-UDA基准测试中优于先前的最先进方法，并有效减轻了灾难性遗忘。

Abstract: Class-Incremental Unsupervised Domain Adaptation (CI-UDA) aims to adapt a
model from a labeled source domain to an unlabeled target domain, where the
sets of potential target classes appearing at different time steps are disjoint
and are subsets of the source classes. The key to solving this problem lies in
avoiding catastrophic forgetting of knowledge about previous target classes
during continuously mitigating the domain shift. Most previous works
cumbersomely combine two technical components. On one hand, they need to store
and utilize rehearsal target sample from previous time steps to avoid
catastrophic forgetting; on the other hand, they perform alignment only between
classes shared across domains at each time step. Consequently, the memory will
continuously increase and the asymmetric alignment may inevitably result in
knowledge forgetting. In this paper, we propose to mine and preserve
domain-invariant and class-agnostic knowledge to facilitate the CI-UDA task.
Specifically, via using CLIP, we extract the class-agnostic properties which we
name as "attribute". In our framework, we learn a "key-value" pair to represent
an attribute, where the key corresponds to the visual prototype and the value
is the textual prompt. We maintain two attribute dictionaries, each
corresponding to a different domain. Then we perform attribute alignment across
domains to mitigate the domain shift, via encouraging visual attention
consistency and prediction consistency. Through attribute modeling and
cross-domain alignment, we effectively reduce catastrophic knowledge forgetting
while mitigating the domain shift, in a rehearsal-free way. Experiments on
three CI-UDA benchmarks demonstrate that our method outperforms previous
state-of-the-art methods and effectively alleviates catastrophic forgetting.
Code is available at https://github.com/RyunMi/VisTA.

</details>


### [55] [Synthetic Dataset Evaluation Based on Generalized Cross Validation](https://arxiv.org/abs/2509.11273)
*Zhihang Song,Dingyi Yao,Ruibo Ming,Lihui Peng,Danya Yao,Yi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种整合广义交叉验证和领域迁移学习的评估框架，用于量化合成数据集的质量，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据集生成技术的快速发展，评估合成数据的质量已成为关键研究重点。然而，当前针对合成数据集的评估研究仍有限，缺乏普遍接受的标准框架。

Method: 该框架涉及在合成数据集和多个真实世界基准（如KITTI、BDD100K）上训练任务特定模型（如YOLOv5s），形成交叉性能矩阵。通过归一化后，构建广义交叉验证（GCV）矩阵以量化领域可迁移性。

Result: 在Virtual KITTI上的实验验证证明了所提框架和指标在评估合成数据保真度方面的有效性。

Conclusion: 本文提出了一种新颖的评估框架，通过整合广义交叉验证实验和领域迁移学习原理，为合成数据集的质量评估提供了可推广和可比较的解决方案。实验验证表明，该框架在评估合成数据保真度方面具有有效性。

Abstract: With the rapid advancement of synthetic dataset generation techniques,
evaluating the quality of synthetic data has become a critical research focus.
Robust evaluation not only drives innovations in data generation methods but
also guides researchers in optimizing the utilization of these synthetic
resources. However, current evaluation studies for synthetic datasets remain
limited, lacking a universally accepted standard framework. To address this,
this paper proposes a novel evaluation framework integrating generalized
cross-validation experiments and domain transfer learning principles, enabling
generalizable and comparable assessments of synthetic dataset quality. The
framework involves training task-specific models (e.g., YOLOv5s) on both
synthetic datasets and multiple real-world benchmarks (e.g., KITTI, BDD100K),
forming a cross-performance matrix. Following normalization, a Generalized
Cross-Validation (GCV) Matrix is constructed to quantify domain
transferability. The framework introduces two key metrics. One measures the
simulation quality by quantifying the similarity between synthetic data and
real-world datasets, while another evaluates the transfer quality by assessing
the diversity and coverage of synthetic data across various real-world
scenarios. Experimental validation on Virtual KITTI demonstrates the
effectiveness of our proposed framework and metrics in assessing synthetic data
fidelity. This scalable and quantifiable evaluation solution overcomes
traditional limitations, providing a principled approach to guide synthetic
dataset optimization in artificial intelligence research.

</details>


### [56] [ROSGS: Relightable Outdoor Scenes With Gaussian Splatting](https://arxiv.org/abs/2509.11275)
*Lianjun Liao,Chunhui Zhang,Tong Wu,Henglei Lv,Bailin Deng,Lin Gao*

Main category: cs.CV

TL;DR: ROSGS是一种两阶段流程，通过2D高斯泼溅和混合光照模型高效重建户外场景并实现高精度重新光照，解决了现有方法计算开销大和光照表示不足的问题。


<details>
  <summary>Details</summary>
Motivation: 户外图像数据因场景无边界和光照条件多变，难以有效分解为几何、反射率和光照。现有方法如NeRF和3DGS存在计算开销大和低频光照表示不足的问题。

Method: ROSGS采用两阶段流程：首先利用单目法线先验和2D高斯泼溅表示高效重建场景几何；随后通过混合光照模型分解纹理和光照，结合球面高斯函数和球谐系数全面捕捉户外光照的高频和低频成分。

Result: 定量指标和定性比较表明，ROSGS在户外场景重新光照任务中达到了最先进的性能，显著提升了准确性和效率。

Conclusion: ROSGS通过两阶段流程和混合光照模型，在户外场景的重新光照任务中实现了最先进的性能，显著提升了重新光照的准确性和渲染效率。

Abstract: Image data captured outdoors often exhibit unbounded scenes and
unconstrained, varying lighting conditions, making it challenging to decompose
them into geometry, reflectance, and illumination. Recent works have focused on
achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D
Gaussian Splatting (3DGS) representation but remain hindered by two key
limitations: the high computational overhead associated with neural networks of
NeRF and the use of low-frequency lighting representations, which often result
in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS,
a two-stage pipeline designed to efficiently reconstruct relightable outdoor
scenes using the Gaussian Splatting representation. By leveraging monocular
normal priors, ROSGS first reconstructs the scene's geometry with the compact
2D Gaussian Splatting (2DGS) representation, providing an efficient and
accurate geometric foundation. Building upon this reconstructed geometry, ROSGS
then decomposes the scene's texture and lighting through a hybrid lighting
model. This model effectively represents typical outdoor lighting by employing
a spherical Gaussian function to capture the directional, high-frequency
components of sunlight, while learning a radiance transfer function via
Spherical Harmonic coefficients to model the remaining low-frequency skylight
comprehensively. Both quantitative metrics and qualitative comparisons
demonstrate that ROSGS achieves state-of-the-art performance in relighting
outdoor scenes and highlight its ability to deliver superior relighting
accuracy and rendering efficiency.

</details>


### [57] [Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations](https://arxiv.org/abs/2509.11287)
*Yifan Lu,Ziqi Zhang,Chunfeng Yuan,Jun Gao,Congxuan Zhang,Xiaojuan Qi,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: APASI是一种无需外部依赖的幻觉缓解方法，通过自注入幻觉和迭代训练策略，有效提升LVLM的生成一致性。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉缓解方法依赖外部人类注释或辅助模型，成本高且限制可持续改进。APASI旨在无需外部依赖的情况下解决这一问题。

Method: APASI利用目标LVLM自注入幻觉生成不同偏好水平的响应对，并通过迭代对齐训练和课程学习策略持续更新偏好数据。

Result: 在六个基准测试中，APASI不仅有效缓解了三种基线模型的幻觉问题，性能还媲美甚至优于依赖外部的方法。

Conclusion: APASI方法通过自注入幻觉和迭代对齐训练策略，有效减少了大型视觉语言模型（LVLM）的幻觉问题，且无需外部依赖，展示了其高效性和泛化能力。

Abstract: Large Vision-Language Models (LVLMs) suffer from serious hallucination
problems, where the model-generated responses are inconsistent with the visual
inputs. Existing hallucination mitigation methods are mainly based on
preference alignment and require external human annotations or auxiliary models
for preference data collection, which increase costs and limit sustainable
improvement. To tackle these challenges, we propose Autonomous Preference
Alignment via Self-Injection (APASI), a novel and generalizable method that
mitigates hallucinations without external dependencies. APASI leverages the
target LVLM to self-inject hallucinations into a generated response, creating a
pair of responses with varying preference levels. During the self-injection
process, the dis-preferred response is generated based on three key
observations of hallucinations, ensuring it simulates real hallucination
patterns. This fidelity offers an accurate learning signal for hallucination
mitigation. Moreover, APASI incorporates an iterative alignment training
strategy combined with curriculum learning to periodically update the
preference data with increasing challenge, enabling stable and continuous
enhancement of the LVLM. Extensive experiments across six benchmarks show that
APASI not only effectively mitigates hallucinations for three baseline models
but also achieves comparable or even superior performance to alignment-based
methods with external dependency, thereby demonstrating its effectiveness and
generalization capability. The code is available at
https://github.com/davidluciolu/APASI.

</details>


### [58] [Leveraging Geometric Priors for Unaligned Scene Change Detection](https://arxiv.org/abs/2509.11292)
*Ziling Liu,Ziwei Chen,Mingqi Gao,Jinyu Yang,Feng Zheng*

Main category: cs.CV

TL;DR: 提出了一种利用几何先验和视觉基础模型的训练免费框架，显著提升了未对齐场景变化检测的性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理大视角变化时，仅依赖2D视觉线索会导致匹配漂移或失败，且缺乏显式几何推理限制了模型的泛化能力。

Method: 作者提出了一种结合几何基础模型先验和视觉基础模型表示的训练免费框架，用于解决未对齐场景变化检测中的核心挑战。

Result: 在PSCD、ChangeSim和PASLCD数据集上的广泛评估表明，该方法实现了优越且稳健的性能。

Conclusion: 通过利用几何基础模型的几何先验，作者提出了一种无需训练的框架，结合视觉基础模型的强大表示能力，显著提升了未对齐场景变化检测的性能。

Abstract: Unaligned Scene Change Detection aims to detect scene changes between image
pairs captured at different times without assuming viewpoint alignment. To
handle viewpoint variations, current methods rely solely on 2D visual cues to
establish cross-image correspondence to assist change detection. However, large
viewpoint changes can alter visual observations, causing appearance-based
matching to drift or fail. Additionally, supervision limited to 2D change masks
from small-scale SCD datasets restricts the learning of generalizable
multi-view knowledge, making it difficult to reliably identify visual overlaps
and handle occlusions. This lack of explicit geometric reasoning represents a
critical yet overlooked limitation. In this work, we are the first to leverage
geometric priors from a Geometric Foundation Model to address the core
challenges of unaligned SCD, including reliable identification of visual
overlaps, robust correspondence establishment, and explicit occlusion
detection. Building on these priors, we propose a training-free framework that
integrates them with the powerful representations of a visual foundation model
to enable reliable change detection under viewpoint misalignment. Through
extensive evaluation on the PSCD, ChangeSim, and PASLCD datasets, we
demonstrate that our approach achieves superior and robust performance. Our
code will be released at https://github.com/ZilingLiu/GeoSCD.

</details>


### [59] [UnLoc: Leveraging Depth Uncertainties for Floorplan Localization](https://arxiv.org/abs/2509.11301)
*Matthias Wüest,Francis Engelmann,Ondrej Miksik,Marc Pollefeys,Daniel Barath*

Main category: cs.CV

TL;DR: UnLoc是一种高效的数据驱动相机定位方法，通过概率模型和不确定性估计提升性能，无需定制深度网络，在长/短序列中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在深度预测中缺乏不确定性建模以及需要为每个环境训练定制深度网络的问题。

Method: 提出了一种新颖的概率模型，将深度预测建模为显式概率分布，并利用现成的预训练单目深度模型，避免了对每个环境定制深度网络的依赖。

Result: 在大型合成和真实数据集上评估，UnLoc在准确性和鲁棒性上显著优于现有方法，特别是在LaMAR HGE数据集上，长序列（100帧）和短序列（15帧）的定位召回率分别提高了2.7倍和16.7倍。

Conclusion: UnLoc通过引入概率模型和不确定性估计，显著提升了相机定位的准确性和鲁棒性，尤其在长序列和短序列中表现优异。

Abstract: We propose UnLoc, an efficient data-driven solution for sequential camera
localization within floorplans. Floorplan data is readily available, long-term
persistent, and robust to changes in visual appearance. We address key
limitations of recent methods, such as the lack of uncertainty modeling in
depth predictions and the necessity for custom depth networks trained for each
environment. We introduce a novel probabilistic model that incorporates
uncertainty estimation, modeling depth predictions as explicit probability
distributions. By leveraging off-the-shelf pre-trained monocular depth models,
we eliminate the need to rely on per-environment-trained depth networks,
enhancing generalization to unseen spaces. We evaluate UnLoc on large-scale
synthetic and real-world datasets, demonstrating significant improvements over
existing methods in terms of accuracy and robustness. Notably, we achieve $2.7$
times higher localization recall on long sequences (100 frames) and $16.7$
times higher on short ones (15 frames) than the state of the art on the
challenging LaMAR HGE dataset.

</details>


### [60] [Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding](https://arxiv.org/abs/2509.11323)
*Jian Song,Wei Mei,Yunfeng Xu,Qiang Fu,Renke Kou,Lina Bu,Yucheng Long*

Main category: cs.CV

TL;DR: SIKNet通过语义独立编码改进运动估计，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于线性恒定速度模型的卡尔曼滤波在参数不匹配或物体非平稳运动时效果不佳。

Method: 提出了一种名为语义独立卡尔曼网（SIKNet）的新方法，通过语义独立编码器（SIE）分两步编码状态向量。

Result: 实验结果表明SIKNet在性能上优于传统卡尔曼滤波和现有学习辅助滤波器。

Conclusion: 提出的SIKNet方法在运动估计中优于传统卡尔曼滤波，并展现出比现有学习辅助滤波器更高的鲁棒性和准确性。

Abstract: Motion estimation is a crucial component in multi-object tracking (MOT).
  It predicts the trajectory of objects by analyzing the changes in their
positions in consecutive frames of images, reducing tracking failures and
identity switches.
  The Kalman filter (KF) based on the linear constant-velocity model is one of
the most commonly used methods in MOT.
  However, it may yield unsatisfactory results when KF's parameters are
mismatched and objects move in non-stationary.
  In this work, we utilize the learning-aided filter to handle the motion
estimation of MOT.
  In particular, we propose a novel method named Semantic-Independent KalmanNet
(SIKNet), which encodes the state vector (the input feature) using a
Semantic-Independent Encoder (SIE) by two steps.
  First, the SIE uses a 1D convolution with a kernel size of 1, which convolves
along the dimension of homogeneous-semantic elements across different state
vectors to encode independent semantic information.
  Then it employs a fully-connected layer and a nonlinear activation layer to
encode nonlinear and cross-dependency information between
heterogeneous-semantic elements.
  To independently evaluate the performance of the motion estimation module in
MOT, we constructed a large-scale semi-simulated dataset from several
open-source MOT datasets.
  Experimental results demonstrate that the proposed SIKNet outperforms the
traditional KF and achieves superior robustness and accuracy than existing
learning-aided filters.
  The code is available at (https://github.com/SongJgit/filternet and
https://github.com/SongJgit/TBDTracker).

</details>


### [61] [Toward Next-generation Medical Vision Backbones: Modeling Finer-grained Long-range Visual Dependency](https://arxiv.org/abs/2509.11328)
*Mingyuan Meng*

Main category: cs.CV

TL;DR: 该研究探索了深度学习中长程视觉依赖建模的有效性，创新应用transformers并开创MLP模型，发现MLPs在医学图像任务中优于transformers和CNNs。


<details>
  <summary>Details</summary>
Motivation: 医学图像计算需要模型同时捕获全局长程上下文和局部细微视觉特征，而现有方法如CNNs和transformers在处理高分辨率特征和细粒度依赖关系时存在局限性。

Method: 研究首先创新性地应用transformers处理像素级和图像级医学视觉任务，随后转向MLPs，开创性地开发了基于MLP的视觉模型，以捕捉医学图像中细粒度的长程视觉依赖关系。

Result: 广泛的实验证实了长程依赖建模在医学图像计算中的关键作用，并发现MLPs能够更有效地建模高分辨率医学特征中的细粒度长程依赖关系。

Conclusion: 该研究确立了MLPs在医学图像计算中的优越性，超越了传统的transformers和CNNs，为下一代医学视觉骨干网络铺平了道路。

Abstract: Medical Image Computing (MIC) is a broad research topic covering both
pixel-wise (e.g., segmentation, registration) and image-wise (e.g.,
classification, regression) vision tasks. Effective analysis demands models
that capture both global long-range context and local subtle visual
characteristics, necessitating fine-grained long-range visual dependency
modeling. Compared to Convolutional Neural Networks (CNNs) that are limited by
intrinsic locality, transformers excel at long-range modeling; however, due to
the high computational loads of self-attention, transformers typically cannot
process high-resolution features (e.g., full-scale image features before
downsampling or patch embedding) and thus face difficulties in modeling
fine-grained dependency among subtle medical image details. Concurrently,
Multi-layer Perceptron (MLP)-based visual models are recognized as
computation/memory-efficient alternatives in modeling long-range visual
dependency but have yet to be widely investigated in the MIC community. This
doctoral research advances deep learning-based MIC by investigating effective
long-range visual dependency modeling. It first presents innovative use of
transformers for both pixel- and image-wise medical vision tasks. The focus
then shifts to MLPs, pioneeringly developing MLP-based visual models to capture
fine-grained long-range visual dependency in medical images. Extensive
experiments confirm the critical role of long-range dependency modeling in MIC
and reveal a key finding: MLPs provide feasibility in modeling finer-grained
long-range dependency among higher-resolution medical features containing
enriched anatomical/pathological details. This finding establishes MLPs as a
superior paradigm over transformers/CNNs, consistently enhancing performance
across various medical vision tasks and paving the way for next-generation
medical vision backbones.

</details>


### [62] [Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking](https://arxiv.org/abs/2509.11453)
*BaiChen Fan,Sifan Zhou,Jian Li,Shibo Zhao,Muqing Cao,Qin Wang*

Main category: cs.CV

TL;DR: TrajTrack是一种轻量级3D单目标跟踪框架，通过历史轨迹学习运动连续性，显著提升精度且高效。


<details>
  <summary>Details</summary>
Motivation: 解决现有帧间运动估计方法缺乏长期时间上下文和序列方法计算成本高的问题。

Method: TrajTrack采用基于轨迹的范式，通过显式运动提议和隐式运动建模模块，结合历史边界框轨迹信息，优化初始跟踪结果。

Result: 在NuScenes基准测试中，TrajTrack将跟踪精度提升了4.48%，并以56 FPS的速度运行，表现出色。

Conclusion: TrajTrack通过结合历史轨迹信息和轻量级框架，显著提升了3D单目标跟踪的精度和效率，同时保持了较高的计算效率。

Abstract: LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics
and autonomous systems. Existing methods typically follow frame-wise motion
estimation or a sequence-based paradigm. However, the two-frame methods are
efficient but lack long-term temporal context, making them vulnerable in sparse
or occluded scenes, while sequence-based methods that process multiple point
clouds gain robustness at a significant computational cost. To resolve this
dilemma, we propose a novel trajectory-based paradigm and its instantiation,
TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame
tracker by implicitly learning motion continuity from historical bounding box
trajectories alone-without requiring additional, costly point cloud inputs. It
first generates a fast, explicit motion proposal and then uses an implicit
motion modeling module to predict the future trajectory, which in turn refines
and corrects the initial proposal. Extensive experiments on the large-scale
NuScenes benchmark show that TrajTrack achieves new state-of-the-art
performance, dramatically improving tracking precision by 4.48% over a strong
baseline while running at 56 FPS. Besides, we also demonstrate the strong
generalizability of TrajTrack across different base trackers. Video is
available at https://www.bilibili.com/video/BV1ahYgzmEWP.

</details>


### [63] [Dual Band Video Thermography Near Ambient Conditions](https://arxiv.org/abs/2509.11334)
*Sriram Narayanan,Mani Ramanagopal,Srinivasa G. Narasimhan*

Main category: cs.CV

TL;DR: 该论文首次提出使用双波段热成像相机分离反射和发射光成分的方法，解决了近环境条件下热成像分析的关键问题，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在近环境条件下，热成像中的反射和发射光成分通常具有可比性且随时间变化，传统方法无法有效分离这两种成分，因此需要新的解决方案。

Method: 论文提出了一种基于双波段热成像相机的图像形成模型，并开发了算法来估计表面的发射率及其随时间变化的温度，同时隔离动态背景。

Result: 通过定量评估和定性实验，该方法在多种材料和复杂日常场景中表现出色，如装有热液体的玻璃和移动的人群。

Conclusion: 该论文提出了一种新方法，通过双波段热成像相机分离反射和发射光成分，有效解决了近环境条件下热成像分析中的关键挑战。

Abstract: Long-wave infrared radiation captured by a thermal camera consists of two
components: (a) light from the environment reflected or transmitted by a
surface, and (b) light emitted by the surface after undergoing heat transport
through the object and exchanging heat with the surrounding environment.
Separating these components is essential for understanding object properties
such as emissivity, temperature, reflectance and shape. Previous thermography
studies often assume that only one component is dominant (e.g., in welding) or
that the second component is constant and can be subtracted. However, in
near-ambient conditions, which are most relevant to computer vision
applications, both components are typically comparable in magnitude and vary
over time. We introduce the first method that separates reflected and emitted
components of light in videos captured by two thermal cameras with different
spectral sensitivities. We derive a dual-band thermal image formation model and
develop algorithms to estimate the surface's emissivity and its time-varying
temperature while isolating a dynamic background. We quantitatively evaluate
our approach using carefully calibrated emissivities for a range of materials
and show qualitative results on complex everyday scenes, such as a glass filled
with hot liquid and people moving in the background.

</details>


### [64] [Beyond Instance Consistency: Investigating View Diversity in Self-supervised Learning](https://arxiv.org/abs/2509.11344)
*Huaiyuan Qin,Muli Yang,Siyuan Hu,Peng Hu,Yu Zhang,Chen Gong,Hongyuan Zhu*

Main category: cs.CV

TL;DR: SSL在实例一致性不保证时仍有效，适度视图多样性可提升性能，EMD量化显示中等值与效果正相关。


<details>
  <summary>Details</summary>
Motivation: 传统SSL依赖实例一致性假设，即同一图像的不同视图可视为正对。然而，非标志性数据中这一假设可能不成立（不同视图包含不同对象或语义信息）。本文旨在探索实例一致性不保证时SSL的有效性。

Method: 通过广泛的消融实验，研究SSL在实例一致性不保证时的有效性。采用地球移动距离（EMD）作为视图间互信息的估计指标，分析视图多样性（如零重叠或较小裁剪尺度）对SSL性能的影响。

Result: 实验表明，即使正对缺乏严格实例一致性，SSL仍能学习有效表示。适度的视图多样性（如零重叠或较小裁剪尺度）可提升分类和密集预测任务性能，但过度多样性会降低效果。EMD分析显示中等值与SSL学习效果正相关。

Conclusion: 研究发现，自监督学习（SSL）即使在实例一致性不保证的情况下仍能学习有意义的表示，且适度的视图多样性可以提升下游任务性能。通过地球移动距离（EMD）量化视图间互信息，发现中等EMD值与SSL学习效果正相关，为未来SSL框架设计提供了理论依据。

Abstract: Self-supervised learning (SSL) conventionally relies on the instance
consistency paradigm, assuming that different views of the same image can be
treated as positive pairs. However, this assumption breaks down for non-iconic
data, where different views may contain distinct objects or semantic
information. In this paper, we investigate the effectiveness of SSL when
instance consistency is not guaranteed. Through extensive ablation studies, we
demonstrate that SSL can still learn meaningful representations even when
positive pairs lack strict instance consistency. Furthermore, our analysis
further reveals that increasing view diversity, by enforcing zero overlapping
or using smaller crop scales, can enhance downstream performance on
classification and dense prediction tasks. However, excessive diversity is
found to reduce effectiveness, suggesting an optimal range for view diversity.
To quantify this, we adopt the Earth Mover's Distance (EMD) as an estimator to
measure mutual information between views, finding that moderate EMD values
correlate with improved SSL learning, providing insights for future SSL
framework design. We validate our findings across a range of settings,
highlighting their robustness and applicability on diverse data sources.

</details>


### [65] [Learning to Generate 4D LiDAR Sequences](https://arxiv.org/abs/2509.11959)
*Ao Liang,Youquan Liu,Yu Yang,Dongyue Lu,Linfeng Li,Lingdong Kong,Huaici Zhao,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: LiDARCrafter 是一个将自由形式语言转换为可编辑 LiDAR 序列的统一框架，通过扩散模型和自回归模块实现高保真度和时间一致性，为 LiDAR 仿真和数据增强提供支持。


<details>
  <summary>Details</summary>
Motivation: 尽管生成世界模型在视频和基于占用的数据合成方面取得了进展，但 LiDAR 生成仍未被充分探索，尽管其对精确 3D 感知至关重要。

Method: LiDARCrafter 通过三分支扩散模型将自由形式的语言转换为可编辑的 LiDAR 序列，包括对象布局、轨迹和形状，并使用范围图像扩散模型生成初始扫描，再通过自回归模块扩展为时间一致的序列。

Result: LiDARCrafter 在 nuScenes 数据集上实现了最先进的保真度、可控性和时间一致性。

Conclusion: LiDARCrafter 在 nuScenes 数据集上实现了最先进的保真度、可控性和时间一致性，为基于 LiDAR 的仿真和数据增强奠定了基础。

Abstract: While generative world models have advanced video and occupancy-based data
synthesis, LiDAR generation remains underexplored despite its importance for
accurate 3D perception. Extending generation to 4D LiDAR data introduces
challenges in controllability, temporal stability, and evaluation. We present
LiDARCrafter, a unified framework that converts free-form language into
editable LiDAR sequences. Instructions are parsed into ego-centric scene
graphs, which a tri-branch diffusion model transforms into object layouts,
trajectories, and shapes. A range-image diffusion model generates the initial
scan, and an autoregressive module extends it into a temporally coherent
sequence. The explicit layout design further supports object-level editing,
such as insertion or relocation. To enable fair assessment, we provide
EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On
nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and
temporal consistency, offering a foundation for LiDAR-based simulation and data
augmentation.

</details>


### [66] [Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness](https://arxiv.org/abs/2509.11355)
*Robin Narsingh Ranabhat,Longwei Wang,Amit Kumar Patel,KC santosh*

Main category: cs.CV

TL;DR: 论文提出两种正则化策略，通过减少CNN对纹理的依赖并增强形状感知，提升了模型对干扰的鲁棒性，同时保持原始分类准确率。


<details>
  <summary>Details</summary>
Motivation: CNN在图像分类中表现出色，但对常见干扰的脆弱性较高，主要原因是其过度依赖局部纹理线索而非全局物体形状，这与人类感知形成鲜明对比。

Method: 提出了两种正则化策略：一是通过辅助损失函数强制原始输入与低频滤波输入间的特征一致性，减少对高频纹理的依赖；二是结合监督对比学习，围绕类别一致且形状相关的表征构建特征空间。

Result: 在CIFAR-10-C基准测试中，两种方法均在不降低原始准确率的情况下提升了模型对干扰的鲁棒性。

Conclusion: 通过两种互补的正则化策略，研究成功引导CNN更依赖全局物体形状而非局部纹理线索，从而在不影响原始准确率的情况下提升了模型对常见干扰的鲁棒性。

Abstract: Convolutional Neural Networks (CNNs) excel at image classification but remain
vulnerable to common corruptions that humans handle with ease. A key reason for
this fragility is their reliance on local texture cues rather than global
object shapes -- a stark contrast to human perception. To address this, we
propose two complementary regularization strategies designed to encourage
shape-biased representations and enhance robustness. The first introduces an
auxiliary loss that enforces feature consistency between original and
low-frequency filtered inputs, discouraging dependence on high-frequency
textures. The second incorporates supervised contrastive learning to structure
the feature space around class-consistent, shape-relevant representations.
Evaluated on the CIFAR-10-C benchmark, both methods improve corruption
robustness without degrading clean accuracy. Our results suggest that
loss-level regularization can effectively steer CNNs toward more shape-aware,
resilient representations.

</details>


### [67] [GLaVE-Cap: Global-Local Aligned Video Captioning with Vision Expert Integration](https://arxiv.org/abs/2509.11360)
*Wan Xu,Feng Zhu,Yihan Zeng,Yuanfan Guo,Ming Liu,Hang Xu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: GLaVE-Cap通过全局-局部对齐框架和视觉专家集成，解决了现有视频详细描述范式的不足，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有局部到全局的视频详细描述范式导致描述不够详细且上下文不一致，缺乏细粒度描述机制和局部与全局描述的强交互。

Method: 提出GLaVE-Cap框架，包含TrackFusion模块（利用视觉专家获取跨帧视觉提示，结合双流结构生成全面局部描述）和CaptionBridge模块（通过全局上下文指导局部描述，并自适应汇总为连贯全局描述）。

Result: 在四个基准测试中，GLaVE-Cap达到最先进性能，消融实验和学生模型分析验证了模块有效性和GLaVE-1.2M对视频理解社区的贡献。

Conclusion: GLaVE-Cap框架通过TrackFusion和CaptionBridge模块，以及GLaVE-1.2M数据集，显著提升了视频详细描述的性能，并在多个基准测试中达到最先进水平。

Abstract: Video detailed captioning aims to generate comprehensive video descriptions
to facilitate video understanding. Recently, most efforts in the video detailed
captioning community have been made towards a local-to-global paradigm, which
first generates local captions from video clips and then summarizes them into a
global caption. However, we find this paradigm leads to less detailed and
contextual-inconsistent captions, which can be attributed to (1) no mechanism
to ensure fine-grained captions, and (2) weak interaction between local and
global captions. To remedy the above two issues, we propose GLaVE-Cap, a
Global-Local aligned framework with Vision Expert integration for Captioning,
which consists of two core modules: TrackFusion enables comprehensive local
caption generation, by leveraging vision experts to acquire cross-frame visual
prompts, coupled with a dual-stream structure; while CaptionBridge establishes
a local-global interaction, by using global context to guide local captioning,
and adaptively summarizing local captions into a coherent global caption.
Besides, we construct GLaVE-Bench, a comprehensive video captioning benchmark
featuring 5X more queries per video than existing benchmarks, covering diverse
visual dimensions to facilitate reliable evaluation. We further provide a
training dataset GLaVE-1.2M containing 16K high-quality fine-grained video
captions and 1.2M related question-answer pairs. Extensive experiments on four
benchmarks show that our GLaVE-Cap achieves state-of-the-art performance.
Besides, the ablation studies and student model analyses further validate the
effectiveness of the proposed modules and the contribution of GLaVE-1.2M to the
video understanding community. The source code, model weights, benchmark, and
dataset will be open-sourced.

</details>


### [68] [In-Vivo Skin 3-D Surface Reconstruction and Wrinkle Depth Estimation using Handheld High Resolution Tactile Sensing](https://arxiv.org/abs/2509.11385)
*Akhil Padmanabha,Arpit Agarwal,Catherine Li,Austin Williams,Dinesh K. Patel,Sankalp Chopkar,Achu Wilson,Ahmet Ozkan,Wenzhen Yuan,Sonal Choudhary,Arash Mostaghimi,Zackory Erickson,Carmel Majidi*

Main category: cs.CV

TL;DR: 开发了一种便携式高分辨率3-D皮肤重建探头，通过GelSight技术和学习算法实现微米级皱纹测量，验证了其在不同身体部位的应用和保湿剂效果评估。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏便携式、高分辨率且经过验证的3-D皮肤表面重建设备，用于不同身体部位的深度重建。

Method: 基于GelSight触觉成像的紧凑型3-D皮肤重建探头，结合自定义弹性凝胶和学习型重建算法，用于微米级皱纹高度估计。

Result: 在无皮肤疾病的15名参与者研究中，首次提供了多个身体区域的皱纹深度验证指标，并展示了使用非处方保湿剂后三个部位皱纹高度的显著减少。

Conclusion: 该研究提供了一个经过验证的工具，用于临床和化妆品皮肤分析，具有在诊断、治疗监测和护肤品效果评估中的潜在应用。

Abstract: Three-dimensional (3-D) skin surface reconstruction offers promise for
objective and quantitative dermatological assessment, but no portable,
high-resolution device exists that has been validated and used for depth
reconstruction across various body locations. We present a compact 3-D skin
reconstruction probe based on GelSight tactile imaging with a custom elastic
gel and a learning-based reconstruction algorithm for micron-level wrinkle
height estimation. Our probe, integrated into a handheld probe with force
sensing for consistent contact, achieves a mean absolute error of 12.55 micron
on wrinkle-like test objects. In a study with 15 participants without skin
disorders, we provide the first validated wrinkle depth metrics across multiple
body regions. We further demonstrate statistically significant reductions in
wrinkle height at three locations following over-the-counter moisturizer
application. Our work offers a validated tool for clinical and cosmetic skin
analysis, with potential applications in diagnosis, treatment monitoring, and
skincare efficacy evaluation.

</details>


### [69] [MixANT: Observation-dependent Memory Propagation for Stochastic Dense Action Anticipation](https://arxiv.org/abs/2509.11394)
*Syed Talal Wasim,Hamid Suleman,Olga Zatsarynna,Muzammal Naseer,Juergen Gall*

Main category: cs.CV

TL;DR: MixANT通过动态$	extbf{A}$矩阵选择提升人类活动预测性能，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有状态空间模型（如Mamba）的关键遗忘门（$	extbf{A}$矩阵）是静态的，限制了模型的性能。

Method: 采用混合专家方法动态选择$	extbf{A}$矩阵，基于输入特征增强表示能力，同时保持计算效率。

Result: 在50Salads、Breakfast和Assembly101数据集上，MixANT在所有评估设置中均优于最先进方法。

Conclusion: MixANT通过动态选择上下文相关的$	extbf{A}$矩阵，显著提升了长期密集人类活动预测的性能，证明了输入依赖性遗忘门机制在多样化现实场景中的重要性。

Abstract: We present MixANT, a novel architecture for stochastic long-term dense
anticipation of human activities. While recent State Space Models (SSMs) like
Mamba have shown promise through input-dependent selectivity on three key
parameters, the critical forget-gate ($\textbf{A}$ matrix) controlling temporal
memory remains static. We address this limitation by introducing a mixture of
experts approach that dynamically selects contextually relevant $\textbf{A}$
matrices based on input features, enhancing representational capacity without
sacrificing computational efficiency. Extensive experiments on the 50Salads,
Breakfast, and Assembly101 datasets demonstrate that MixANT consistently
outperforms state-of-the-art methods across all evaluation settings. Our
results highlight the importance of input-dependent forget-gate mechanisms for
reliable prediction of human behavior in diverse real-world scenarios.

</details>


### [70] [No Modality Left Behind: Dynamic Model Generation for Incomplete Medical Data](https://arxiv.org/abs/2509.11406)
*Christoph Fürböck,Paul Weiser,Branko Mitic,Philipp Seeböck,Thomas Helbich,Georg Langs*

Main category: cs.CV

TL;DR: 提出一种超网络方法，动态生成适应可用模态的分类模型，显著提升多模态医学影像分析的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现实临床环境中多模态医学影像数据常存在部分缺失，传统方法如丢弃样本、插补或重新利用dropout学习方案限制了模型的鲁棒性和泛化性。

Method: 使用超网络动态生成任务模型的参数，根据可用模态进行调整，从而实现在不完整数据上的训练和推理。

Result: 在25%数据完整性的数据集上，该方法比现有技术方法准确率绝对提升高达8%。

Conclusion: 本研究提出的基于超网络的方法能够动态生成适应可用模态的任务特定分类模型，显著提高了在多模态医学影像数据中的鲁棒性和泛化能力。

Abstract: In real world clinical environments, training and applying deep learning
models on multi-modal medical imaging data often struggles with partially
incomplete data. Standard approaches either discard missing samples, require
imputation or repurpose dropout learning schemes, limiting robustness and
generalizability. To address this, we propose a hypernetwork-based method that
dynamically generates task-specific classification models conditioned on the
set of available modalities. Instead of training a fixed model, a hypernetwork
learns to predict the parameters of a task model adapted to available
modalities, enabling training and inference on all samples, regardless of
completeness. We compare this approach with (1) models trained only on complete
data, (2) state of the art channel dropout methods, and (3) an imputation-based
method, using artificially incomplete datasets to systematically analyze
robustness to missing modalities. Results demonstrate superior adaptability of
our method, outperforming state of the art approaches with an absolute increase
in accuracy of up to 8% when trained on a dataset with 25% completeness (75% of
training data with missing modalities). By enabling a single model to
generalize across all modality configurations, our approach provides an
efficient solution for real-world multi-modal medical data analysis.

</details>


### [71] [Disentanglement of Biological and Technical Factors via Latent Space Rotation in Clinical Imaging Improves Disease Pattern Discovery](https://arxiv.org/abs/2509.11436)
*Jeanny Pan,Philipp Seeböck,Christoph Fürböck,Svitlana Pochepnia,Jennifer Straub,Lucian Beer,Helmut Prosch,Georg Langs*

Main category: cs.CV

TL;DR: 提出一种通过学习潜在空间旋转解耦生物和技术因素的方法，显著提升医学影像数据聚类一致性，并增强生存预测能力。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据中的领域偏移（由不同厂商或扫描参数引起）阻碍了生物有意义聚类的外观发现，需要一种方法来解耦这些因素。

Method: 通过后验旋转数据潜在空间的方法，主动学习领域偏移，实现生物和技术因素的解耦。

Result: 在真实世界异质临床数据上，解耦表示导致跨不同采集设置的稳定组织类型聚类，聚类一致性显著提升（ARI +19.01%，NMI +16.85%，Dice +12.39%），并优于四种最先进的协调方法。

Conclusion: 提出的无标签框架有助于在多中心常规影像数据中发现生物标志物，通过学习解耦表示提高了聚类一致性，并在生存预测中表现出优越性。

Abstract: Identifying new disease-related patterns in medical imaging data with the
help of machine learning enlarges the vocabulary of recognizable findings. This
supports diagnostic and prognostic assessment. However, image appearance varies
not only due to biological differences, but also due to imaging technology
linked to vendors, scanning- or re- construction parameters. The resulting
domain shifts impedes data representation learning strategies and the discovery
of biologically meaningful cluster appearances. To address these challenges, we
introduce an approach to actively learn the domain shift via post-hoc rotation
of the data latent space, enabling disentanglement of biological and technical
factors. Results on real-world heterogeneous clinical data showcase that the
learned disentangled representation leads to stable clusters representing
tissue-types across different acquisition settings. Cluster consistency is
improved by +19.01% (ARI), +16.85% (NMI), and +12.39% (Dice) compared to the
entangled representation, outperforming four state-of-the-art harmonization
methods. When using the clusters to quantify tissue composition on idiopathic
pulmonary fibrosis patients, the learned profiles enhance Cox survival
prediction. This indicates that the proposed label-free framework facilitates
biomarker discovery in multi-center routine imaging data. Code is available on
GitHub https://github.com/cirmuw/latent-space-rotation-disentanglement.

</details>


### [72] [MultiMAE for Brain MRIs: Robustness to Missing Inputs Using Multi-Modal Masked Autoencoder](https://arxiv.org/abs/2509.11442)
*Ayhan Can Erdur,Christian Beischl,Daniel Scholz,Jiazhen Pan,Benedikt Wiestler,Daniel Rueckert,Jan C Peeken*

Main category: cs.CV

TL;DR: 提出了一种多模态MAE方法处理脑MRI中的缺失序列，通过跨模态推理提升模型鲁棒性，并在下游任务中显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据中缺失输入序列是常见问题，这对依赖完整输入数据的深度学习模型提出了挑战。

Method: 开发了一种基于掩码自编码器（MAE）的多模态、多任务学习范式，采用晚期融合风格的Transformer编码器整合多序列信息，并为每个模态配备单独的解码器流。

Result: 在下游分割和分类任务中，该方法相比MAE-ViT基线表现出色，整体Dice分数绝对提升10.1，MCC提升0.46。

Conclusion: 该研究提出了一种灵活的、可泛化的编码器，能够通过跨序列推理处理缺失输入，并适用于多种下游应用。实验结果表明，该方法在缺失输入序列的情况下优于基线模型。

Abstract: Missing input sequences are common in medical imaging data, posing a
challenge for deep learning models reliant on complete input data. In this
work, inspired by MultiMAE [2], we develop a masked autoencoder (MAE) paradigm
for multi-modal, multi-task learning in 3D medical imaging with brain MRIs. Our
method treats each MRI sequence as a separate input modality, leveraging a
late-fusion-style transformer encoder to integrate multi-sequence information
(multi-modal) and individual decoder streams for each modality for multi-task
reconstruction. This pretraining strategy guides the model to learn rich
representations per modality while also equipping it to handle missing inputs
through cross-sequence reasoning. The result is a flexible and generalizable
encoder for brain MRIs that infers missing sequences from available inputs and
can be adapted to various downstream applications. We demonstrate the
performance and robustness of our method against an MAE-ViT baseline in
downstream segmentation and classification tasks, showing absolute improvement
of $10.1$ overall Dice score and $0.46$ MCC over the baselines with missing
input sequences. Our experiments demonstrate the strength of this pretraining
strategy. The implementation is made available.

</details>


### [73] [Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision](https://arxiv.org/abs/2509.11476)
*Tianyao Sun,Dawei Xiang,Tianqi Ding,Xiang Fang,Yijiashun Qi,Zunduo Zhao*

Main category: cs.CV

TL;DR: FusionNet是一种新型端到端融合框架，通过模态感知注意力机制和像素级alpha混合模块，实现了语义保留和解释性强的红外与可见光图像融合。


<details>
  <summary>Details</summary>
Motivation: 红外和可见光图像融合（IVIF）是多模态感知中的一项基本任务，旨在整合不同光谱域的互补结构和纹理线索。

Method: FusionNet引入了一种模态感知的注意力机制，动态调整红外和可见光特征的贡献，并包含一个像素级的alpha混合模块，以自适应和内容感知的方式学习空间变化的融合权重。此外，还制定了目标感知损失，利用弱ROI监督来保持重要对象区域的语义一致性。

Result: 在公开的M3FD数据集上的实验表明，FusionNet生成的融合图像具有增强的语义保留、高感知质量和清晰的解释性。

Conclusion: FusionNet为语义感知的多模态图像融合提供了一个通用且可扩展的解决方案，对下游任务（如目标检测和场景理解）有益。

Abstract: Infrared and visible image fusion (IVIF) is a fundamental task in multi-modal
perception that aims to integrate complementary structural and textural cues
from different spectral domains. In this paper, we propose FusionNet, a novel
end-to-end fusion framework that explicitly models inter-modality interaction
and enhances task-critical regions. FusionNet introduces a modality-aware
attention mechanism that dynamically adjusts the contribution of infrared and
visible features based on their discriminative capacity. To achieve
fine-grained, interpretable fusion, we further incorporate a pixel-wise alpha
blending module, which learns spatially-varying fusion weights in an adaptive
and content-aware manner. Moreover, we formulate a target-aware loss that
leverages weak ROI supervision to preserve semantic consistency in regions
containing important objects (e.g., pedestrians, vehicles). Experiments on the
public M3FD dataset demonstrate that FusionNet generates fused images with
enhanced semantic preservation, high perceptual quality, and clear
interpretability. Our framework provides a general and extensible solution for
semantic-aware multi-modal image fusion, with benefits for downstream tasks
such as object detection and scene understanding.

</details>


### [74] [Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis](https://arxiv.org/abs/2509.11526)
*Wenhao Tang,Sheng Huang,Heng Fang,Fengtao Zhou,Bo Liu,Qingshan Liu*

Main category: cs.CV

TL;DR: MHIM-MIL通过挖掘硬实例，提升癌症诊断等任务的性能，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MIL方法偏向易分类实例，忽视硬实例，而硬实例对建模判别边界至关重要。

Method: 提出了一种新型MIL框架MHIM-MIL，采用Siamese结构和一致性约束，结合动量教师模型和随机掩码技术，挖掘硬实例。

Result: 在多个任务和基准测试中，MHIM-MIL在性能和效率上均优于最新方法。

Conclusion: MHIM-MIL框架通过挖掘硬实例，显著提升了癌症诊断、分型和生存分析任务的性能，并在12个基准测试中表现优异。

Abstract: Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has
opened new avenues for Computational Pathology (CPath). As positive tissue
comprises only a small fraction of gigapixel WSIs, existing Multiple Instance
Learning (MIL) methods typically focus on identifying salient instances via
attention mechanisms. However, this leads to a bias towards easy-to-classify
instances while neglecting challenging ones. Recent studies have shown that
hard examples are crucial for accurately modeling discriminative boundaries.
Applying such an idea at the instance level, we elaborate a novel MIL framework
with masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure
with a consistency constraint to explore the hard instances. Using a
class-aware instance probability, MHIM-MIL employs a momentum teacher to mask
salient instances and implicitly mine hard instances for training the student
model. To obtain diverse, non-redundant hard instances, we adopt large-scale
random masking while utilizing a global recycle network to mitigate the risk of
losing key features. Furthermore, the student updates the teacher using an
exponential moving average, which identifies new hard instances for subsequent
training iterations and stabilizes optimization. Experimental results on cancer
diagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate
that MHIM-MIL outperforms the latest methods in both performance and
efficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.

</details>


### [75] [SFGNet: Semantic and Frequency Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2509.11539)
*Dezhen Wang,Haixiang Zhao,Xiang Shen,Sheng Miao*

Main category: cs.CV

TL;DR: SFGNet结合语义提示和频域特征，通过MBFM和ISEB模块提升伪装物体检测性能，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了不同目标文本提示的语义差异和细粒度频率特征，导致检测性能不足。

Method: 提出了SFGNet，包含多频带傅里叶模块（MBFM）和交互式结构增强块（ISEB），以处理复杂背景和模糊边界。

Result: 在三个COD基准数据集上的实验表明，SFGNet显著优于现有方法。

Conclusion: SFGNet通过结合语义提示和频域特征，显著提升了伪装物体检测的性能，并在边界感知方面表现出色。

Abstract: Camouflaged object detection (COD) aims to segment objects that blend into
their surroundings. However, most existing studies overlook the semantic
differences among textual prompts of different targets as well as fine-grained
frequency features. In this work, we propose a novel Semantic and Frequency
Guided Network (SFGNet), which incorporates semantic prompts and
frequency-domain features to capture camouflaged objects and improve boundary
perception. We further design Multi-Band Fourier Module(MBFM) to enhance the
ability of the network in handling complex backgrounds and blurred boundaries.
In addition, we design an Interactive Structure Enhancement Block (ISEB) to
ensure structural integrity and boundary details in the predictions. Extensive
experiments conducted on three COD benchmark datasets demonstrate that our
method significantly outperforms state-of-the-art approaches. The core code of
the model is available at the following link:
https://github.com/winter794444/SFGNetICASSP2026.

</details>


### [76] [How Auxiliary Reasoning Unleashes GUI Grounding in VLMs](https://arxiv.org/abs/2509.11548)
*Weiming Li,Yan Shao,Jing Yang,Yujing Lu,Ling Zhong,Yuhan Wang,Manni Duan*

Main category: cs.CV

TL;DR: 通过提供显式空间线索的零样本方法，显著提升了VLM在GUI基础任务中的表现，避免了高成本微调。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLM）在潜在基础能力方面表现出色（如通过Pointing Game测量），但在输出显式坐标时表现不佳。为了解决这一差距，并避免当前微调方法的高数据和标注成本。

Method: 提出了三种零样本辅助推理方法，包括在输入图像中提供显式的空间线索（如轴线、网格和标记的交点），以帮助VLM更好地表达其潜在的空间理解能力。

Result: 在四个GUI基础基准测试和七个开源及专有VLM上的评估结果表明，所提出的方法显著提高了GUI基础的性能。

Conclusion: 通过提供显式的空间线索（如轴线、网格和标记的交点），本研究提出的三种零样本辅助推理方法显著提升了视觉语言模型（VLM）在图形用户界面（GUI）基础任务中的表现。

Abstract: Graphical user interface (GUI) grounding is a fundamental task for building
GUI agents. However, general vision-language models (VLMs) struggle with this
task due to a lack of specific optimization. We identify a key gap in this
paper: while VLMs exhibit significant latent grounding potential, as
demonstrated by their performance measured by Pointing Game, they underperform
when tasked with outputting explicit coordinates. To address this discrepancy,
and bypass the high data and annotation costs of current fine-tuning
approaches, we propose three zero-shot auxiliary reasoning methods. By
providing explicit spatial cues such as axes, grids and labeled intersections
as part of the input image, these methods enable VLMs to articulate their
implicit spatial understanding capabilities. We evaluate these methods on four
GUI grounding benchmarks across seven open-source and proprietary VLMs. The
evaluation results demonstrate that the proposed methods substantially improve
the performance of GUI grounding.

</details>


### [77] [Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps](https://arxiv.org/abs/2509.11574)
*Zhexi Peng,Kun Zhou,Tianjia Shao*

Main category: cs.CV

TL;DR: GPS-SLAM通过Gaussian-SDF混合表示，显著提升SLAM速度至150 fps，同时保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有Gaussian-based SLAM方法计算性能低（<20 fps）的问题，因建模场景需要大量Gaussians和复杂优化，导致速度远低于几何中心方法（如KinectFusion）。

Method: 提出Gaussian-SDF混合表示，结合彩色SDF（平滑几何和外观）和3D Gaussians（捕捉细节），通过RGB-D融合高效构建SDF，并优化Gaussians以减少迭代次数。

Result: GPS-SLAM在真实Azure Kinect序列上实现了超过150 fps的速度，Gaussians数量减少50%，优化迭代减少75%。

Conclusion: GPS-SLAM通过结合Gaussian-SDF混合表示，实现了实时3D重建，速度提升了一个数量级（150 fps），同时保持了与现有技术相当的重建质量。

Abstract: While recent Gaussian-based SLAM methods achieve photorealistic
reconstruction from RGB-D data, their computational performance remains a
critical bottleneck. State-of-the-art techniques operate at less than 20 fps,
significantly lagging behind geometry-centric approaches like KinectFusion
(hundreds of fps). This limitation stems from the heavy computational burden:
modeling scenes requires numerous Gaussians and complex iterative optimization
to fit RGB-D data, where insufficient Gaussian counts or optimization
iterations cause severe quality degradation. To address this, we propose a
Gaussian-SDF hybrid representation, combining a colorized Signed Distance Field
(SDF) for smooth geometry and appearance with 3D Gaussians to capture
underrepresented details. The SDF is efficiently constructed via RGB-D fusion
(as in geometry-centric methods), while Gaussians undergo iterative
optimization. Our representation enables drastic Gaussian reduction (50% fewer)
by avoiding full-scene Gaussian modeling, and efficient Gaussian optimization
(75% fewer iterations) through targeted appearance refinement. Building upon
this representation, we develop GPS-SLAM (Gaussian-Plus-SDF SLAM), a real-time
3D reconstruction system achieving over 150 fps on real-world Azure Kinect
sequences -- delivering an order-of-magnitude speedup over state-of-the-art
techniques while maintaining comparable reconstruction quality. We will release
the source code and data to facilitate future research.

</details>


### [78] [Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2509.11587)
*Haonan Shi,Yubin Wang,De Cheng,Lingfeng He,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 提出HIL框架，通过二次聚类和多中心对比学习减少跨模态差异，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用基于聚类的对比学习，但忽略了图像间的细粒度差异，导致跨模态匹配效果不佳。

Method: 采用分层身份学习（HIL）框架，包括二次聚类生成多个记忆、多中心对比学习（MCCL）和双向反向选择传输（BRST）机制。

Result: 在SYSU-MM01和RegDB数据集上的实验表明，该方法优于现有方法。

Conclusion: 提出的HIL框架通过分层身份学习和多中心对比学习，有效减少了跨模态差异，并在SYSU-MM01和RegDB数据集上取得了优于现有方法的性能。

Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to
learn modality-invariant image features from unlabeled cross-modal person
datasets by reducing the modality gap while minimizing reliance on costly
manual annotations. Existing methods typically address USVI-ReID using
cluster-based contrastive learning, which represents a person by a single
cluster center. However, they primarily focus on the commonality of images
within each cluster while neglecting the finer-grained differences among them.
To address the limitation, we propose a Hierarchical Identity Learning (HIL)
framework. Since each cluster may contain several smaller sub-clusters that
reflect fine-grained variations among images, we generate multiple memories for
each existing coarse-grained cluster via a secondary clustering. Additionally,
we propose Multi-Center Contrastive Learning (MCCL) to refine representations
for enhancing intra-modal clustering and minimizing cross-modal discrepancies.
To further improve cross-modal matching quality, we design a Bidirectional
Reverse Selection Transmission (BRST) mechanism, which establishes reliable
cross-modal correspondences by performing bidirectional matching of
pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB
datasets demonstrate that the proposed method outperforms existing approaches.
The source code is available at: https://github.com/haonanshi0125/HIL.

</details>


### [79] [Optimizing Class Distributions for Bias-Aware Multi-Class Learning](https://arxiv.org/abs/2509.11588)
*Mirco Felske,Stefan Stiene*

Main category: cs.CV

TL;DR: BiCDO是一个优化多类图像分类中类分布的框架，优先考虑特定类别，提升模型性能，已验证有效。


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景中，优先考虑特定类别（如“人类”优于“狗”）的需求促使了BiCDO的开发，以解决均匀分布无法满足的特定性能需求。

Method: BiCDO是一个迭代的、以数据为中心的框架，通过识别帕累托优化的类分布来优化多类图像分类。它支持任何标记的多类数据集，并可轻松集成到现有训练流程中。

Result: 在CIFAR-10和iNaturalist21数据集上使用EfficientNet、ResNet和ConvNeXt验证了BiCDO，结果显示通过优化数据分布，模型性能得到了平衡和提升。

Conclusion: BiCDO框架通过优化类分布，显著提升了多类图像分类模型的性能，特别是在需要优先考虑特定类别的安全关键场景中。

Abstract: We propose BiCDO (Bias-Controlled Class Distribution Optimizer), an
iterative, data-centric framework that identifies Pareto optimized class
distributions for multi-class image classification. BiCDO enables performance
prioritization for specific classes, which is useful in safety-critical
scenarios (e.g. prioritizing 'Human' over 'Dog'). Unlike uniform distributions,
BiCDO determines the optimal number of images per class to enhance reliability
and minimize bias and variance in the objective function. BiCDO can be
incorporated into existing training pipelines with minimal code changes and
supports any labelled multi-class dataset. We have validated BiCDO using
EfficientNet, ResNet and ConvNeXt on CIFAR-10 and iNaturalist21 datasets,
demonstrating improved, balanced model performance through optimized data
distribution.

</details>


### [80] [MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment](https://arxiv.org/abs/2509.11589)
*Yanyun Pu,Kehan Li,Zeyi Huang,Zhijie Zhong,Kaixiang Yang*

Main category: cs.CV

TL;DR: MVQA-68K是一个多维度视频质量评估数据集，通过详细标注和思维链推理提升VQA任务的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统VQA方法通常生成单一数值分数，缺乏全面性和可解释性。

Method: 引入MVQA-68K数据集，包含超过68,000个视频，覆盖七个关键质量维度，每个标注包含详细的思维链推理。

Result: 实验表明，MVQA-68K显著提升了MLLMs在VQA任务中的性能，并大幅提升了零样本泛化能力。

Conclusion: MVQA-68K显著提升了多种多模态大语言模型（MLLMs）在视频质量评估（VQA）任务中的性能，不仅在内部测试集上取得了最先进的结果，还在公共基准测试中表现优异。

Abstract: With the rapid advancement of video generation models such as Sora, video
quality assessment (VQA) is becoming increasingly crucial for selecting
high-quality videos from large-scale datasets used in pre-training. Traditional
VQA methods, typically producing single numerical scores, often lack
comprehensiveness and interpretability. To address these challenges, we
introduce MVQA-68K, a novel multi-dimensional VQA dataset comprising over
68,000 carefully annotated videos, covering seven essential quality dimensions:
overall aesthetics, camera movement, dynamic degree, texture detail,
composition, visual quality, and factual consistency. Each annotation includes
detailed chain-of-thought reasoning to facilitate interpretability and
comprehensive understanding. Extensive experiments demonstrate that MVQA-68K
significantly enhances the performance of various multimodal large language
models (MLLMs) on the VQA task, achieving state-of-the-art results not only on
our internal test set (Fig.1) but also on public benchmarks including
LSVQ-test, LSVQ-1080p, and LIVE-VQC. Meantime, incorporating explicit reasoning
process during VQA training substantially boosts the zero-shot generalization.
Code and dataset will be available at github:
https://github.com/Controller01-ai/MVQA-68K

</details>


### [81] [Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework](https://arxiv.org/abs/2509.11598)
*Siming Fu,Sijun Dong,Xiaoliang Meng*

Main category: cs.CV

TL;DR: HyGDL 框架通过内容-风格解耦解决自监督学习的捷径问题，提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自监督学习（SSL）的泛化能力因捷径学习（模型依赖表面特征而非内在结构）而受限，现有方法未能从根本上改变导致捷径依赖的学习机制。

Method: 提出 HyGDL（混合生成-判别学习框架），通过向量投影从表示中定义风格为与风格不变内容正交的分量，实现显式内容-风格解耦。

Result: HyGDL 通过不变性预训练原则，系统地改变输入中的偏差（如风格）同时保持监督信号不变，使模型学习不变的本质。

Conclusion: HyGDL 框架通过显式内容-风格解耦，有效解决了自监督学习中的捷径学习问题，提高了模型在未见领域上的泛化能力。

Abstract: Despite the remarkable success of Self-Supervised Learning (SSL), its
generalization is fundamentally hindered by Shortcut Learning, where models
exploit superficial features like texture instead of intrinsic structure. We
experimentally verify this flaw within the generative paradigm (e.g., MAE) and
argue it is a systemic issue also affecting discriminative methods, identifying
it as the root cause of their failure on unseen domains. While existing methods
often tackle this at a surface level by aligning or separating domain-specific
features, they fail to alter the underlying learning mechanism that fosters
shortcut dependency. To address this at its core, we propose HyGDL (Hybrid
Generative-Discriminative Learning Framework), a hybrid framework that achieves
explicit content-style disentanglement. Our approach is guided by the
Invariance Pre-training Principle: forcing a model to learn an invariant
essence by systematically varying a bias (e.g., style) at the input while
keeping the supervision signal constant. HyGDL operates on a single encoder and
analytically defines style as the component of a representation that is
orthogonal to its style-invariant content, derived via vector projection.

</details>


### [82] [DUAL-VAD: Dual Benchmarks and Anomaly-Focused Sampling for Video Anomaly Detection](https://arxiv.org/abs/2509.11605)
*Seoik Jung,Taekyung Song,Joshua Jordan Daniel,JinYoung Lee,SungJun Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种新的帧分配策略和两个互补基准，显著提升了视频异常检测的性能，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频异常检测基准仅限于帧级别或视频级别任务，限制了模型泛化的全面评估。

Method: 提出了一种基于softmax的帧分配策略，优先处理异常密集的片段，同时保持全视频覆盖，实现了跨时间尺度的平衡采样。并构建了图像基准和视频基准两个互补的测试基准。

Result: 在UCF-Crime数据集上的实验表明，该方法在帧级别和视频级别均取得了性能提升，消融研究证实了异常聚焦采样相对于均匀和随机基线的明显优势。

Conclusion: 该论文通过引入基于softmax的帧分配策略和构建两个互补的基准测试，显著提升了视频异常检测在帧级别和视频级别的性能。

Abstract: Video Anomaly Detection (VAD) is critical for surveillance and public safety.
However, existing benchmarks are limited to either frame-level or video-level
tasks, restricting a holistic view of model generalization. This work first
introduces a softmax-based frame allocation strategy that prioritizes
anomaly-dense segments while maintaining full-video coverage, enabling balanced
sampling across temporal scales. Building on this process, we construct two
complementary benchmarks. The image-based benchmark evaluates frame-level
reasoning with representative frames, while the video-based benchmark extends
to temporally localized segments and incorporates an abnormality scoring
task.Experiments on UCF-Crime demonstrate improvements at both the frame and
video levels, and ablation studies confirm clear advantages of anomaly-focused
sampling over uniform and random baselines.

</details>


### [83] [A Controllable 3D Deepfake Generation Framework with Gaussian Splatting](https://arxiv.org/abs/2509.11624)
*Wending Liu,Siyun Liang,Huy H. Nguyen,Isao Echizen*

Main category: cs.CV

TL;DR: 该论文提出了一种基于3D高斯泼溅的深度伪造框架，实现了真实且身份保持的3D人脸替换和重演，显著提升了多视角一致性，并揭示了该技术可能被滥用的风险。


<details>
  <summary>Details</summary>
Motivation: 传统2D深度伪造方法存在几何不一致性和对新视角泛化能力有限的问题。该研究旨在通过3D高斯泼溅技术解决这些问题，实现更真实、可控的3D深度伪造。

Method: 该方法结合了参数化头部模型与动态高斯表示，通过显式分离头部和背景高斯并使用预训练的2D引导优化面部区域，支持多视角一致渲染、精确表情控制和无缝背景融合。此外，还引入了修复模块以增强极端姿态和表情下的视觉一致性。

Result: 实验表明，该方法在身份保持、姿态和表情一致性方面与最先进的2D方法相当，同时在多视角渲染质量和3D一致性方面显著优于后者。

Conclusion: 该论文提出的基于3D高斯泼溅的深度伪造生成框架在3D空间实现了真实且身份保持的人脸替换和重演，显著提升了多视角渲染质量和3D一致性，为场景感知、可控且沉浸式的视觉伪造开辟了新方向，同时也揭示了3D高斯泼溅技术可能被用于操纵攻击的潜在威胁。

Abstract: We propose a novel 3D deepfake generation framework based on 3D Gaussian
Splatting that enables realistic, identity-preserving face swapping and
reenactment in a fully controllable 3D space. Compared to conventional 2D
deepfake approaches that suffer from geometric inconsistencies and limited
generalization to novel view, our method combines a parametric head model with
dynamic Gaussian representations to support multi-view consistent rendering,
precise expression control, and seamless background integration. To address
editing challenges in point-based representations, we explicitly separate the
head and background Gaussians and use pre-trained 2D guidance to optimize the
facial region across views. We further introduce a repair module to enhance
visual consistency under extreme poses and expressions. Experiments on
NeRSemble and additional evaluation videos demonstrate that our method achieves
comparable performance to state-of-the-art 2D approaches in identity
preservation, as well as pose and expression consistency, while significantly
outperforming them in multi-view rendering quality and 3D consistency. Our
approach bridges the gap between 3D modeling and deepfake synthesis, enabling
new directions for scene-aware, controllable, and immersive visual forgeries,
revealing the threat that emerging 3D Gaussian Splatting technique could be
used for manipulation attacks.

</details>


### [84] [IS-Diff: Improving Diffusion-Based Inpainting with Better Initial Seed](https://arxiv.org/abs/2509.11638)
*Yongzhe Lyu,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: IS-Diff是一种无需训练的扩散模型修复方法，通过和谐种子和动态细化机制提升修复的一致性和连贯性，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的扩散模型在修复任务中因随机初始化种子可能导致语义信息不匹配，影响修复结果的一致性和连贯性。

Method: IS-Diff采用从非掩码区域采样的初始种子来模拟掩码数据分布，并结合动态选择性细化机制动态调整初始化先验的强度。

Result: 在CelebA-HQ、ImageNet和Places2数据集上的实验表明，IS-Diff在所有指标上均优于现有修复方法。

Conclusion: IS-Diff通过引入分布和谐的种子和动态选择性细化机制，显著提高了扩散模型在修复任务中的一致性和连贯性，无需额外训练即可实现优于现有方法的性能。

Abstract: Diffusion models have shown promising results in free-form inpainting. Recent
studies based on refined diffusion samplers or novel architectural designs led
to realistic results and high data consistency. However, random initialization
seed (noise) adopted in vanilla diffusion process may introduce mismatched
semantic information in masked regions, leading to biased inpainting results,
e.g., low consistency and low coherence with the other unmasked area. To
address this issue, we propose the Initial Seed refined Diffusion Model
(IS-Diff), a completely training-free approach incorporating distributional
harmonious seeds to produce harmonious results. Specifically, IS-Diff employs
initial seeds sampled from unmasked areas to imitate the masked data
distribution, thereby setting a promising direction for the diffusion
procedure. Moreover, a dynamic selective refinement mechanism is proposed to
detect severe unharmonious inpaintings in intermediate latent and adjust the
strength of our initialization prior dynamically. We validate our method on
both standard and large-mask inpainting tasks using the CelebA-HQ, ImageNet,
and Places2 datasets, demonstrating its effectiveness across all metrics
compared to state-of-the-art inpainting methods.

</details>


### [85] [WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration](https://arxiv.org/abs/2509.11642)
*Qiyuan Guan,Qianfeng Yang,Xiang Chen,Tianyu Song,Guiyue Jin,Jiyu Jin*

Main category: cs.CV

TL;DR: 该论文提出了一个真实世界的多天气图像修复基准数据集，解决了现有合成数据集的领域差距问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的一体化图像修复方法主要使用混合单天气合成数据集进行训练和评估，但这些数据集在分辨率、风格和领域特征上存在显著差异，导致领域差距大，阻碍了统一模型的开发和公平评估。此外，缺乏大规模真实世界的一体化天气修复数据集是该领域发展的关键瓶颈。

Method: 通过构建包含多种天气条件（如雨、雪、雾）以及不同户外场景和光照设置的图像对数据集，并确保精确对齐的退化与清洁图像，支持监督学习和严格评估。

Result: 通过在各种任务特定、任务通用和一体化修复方法上进行全面实验，验证了所提出数据集的有效性。

Conclusion: 该论文提出了一个真实世界的多天气图像修复基准数据集，为推进该领域的稳健和实用解决方案提供了宝贵的基础。

Abstract: Existing all-in-one image restoration approaches, which aim to handle
multiple weather degradations within a single framework, are predominantly
trained and evaluated using mixed single-weather synthetic datasets. However,
these datasets often differ significantly in resolution, style, and domain
characteristics, leading to substantial domain gaps that hinder the development
and fair evaluation of unified models. Furthermore, the lack of a large-scale,
real-world all-in-one weather restoration dataset remains a critical bottleneck
in advancing this field. To address these limitations, we present a real-world
all-in-one adverse weather image restoration benchmark dataset, which contains
image pairs captured under various weather conditions, including rain, snow,
and haze, as well as diverse outdoor scenes and illumination settings. The
resulting dataset provides precisely aligned degraded and clean images,
enabling supervised learning and rigorous evaluation. We conduct comprehensive
experiments by benchmarking a variety of task-specific, task-general, and
all-in-one restoration methods on our dataset. Our dataset offers a valuable
foundation for advancing robust and practical all-in-one image restoration in
real-world scenarios. The dataset has been publicly released and is available
at https://github.com/guanqiyuan/WeatherBench.

</details>


### [86] [Joint-octamamba:an octa joint segmentation network based on feature enhanced mamba](https://arxiv.org/abs/2509.11649)
*Chuang Liu,Nan Guo*

Main category: cs.CV

TL;DR: 提出RVMamba和Joint-OCTAMamba框架，显著提升OCTA图像中视网膜血管和FAZ的分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前2D方法在视网膜血管分割中精度不足，且现有联合分割模型在OCTA数据上存在任务间性能不平衡。

Method: 提出RVMamba架构，整合多特征提取模块与Mamba状态空间模型；引入FAZMamba和统一框架Joint-OCTAMamba以解决任务间性能不平衡问题。

Result: 实验结果表明Joint-OCTAMamba在各项评估指标上优于现有模型。

Conclusion: Joint-OCTAMamba框架在OCTA-500数据集上表现优于现有模型，为视网膜疾病诊断提供了更准确的工具。

Abstract: OCTA is a crucial non-invasive imaging technique for diagnosing and
monitoring retinal diseases like diabetic retinopathy, age-related macular
degeneration, and glaucoma. Current 2D-based methods for retinal vessel (RV)
segmentation offer insufficient accuracy. To address this, we propose RVMamba,
a novel architecture integrating multiple feature extraction modules with the
Mamba state-space model. Moreover, existing joint segmentation models for OCTA
data exhibit performance imbalance between different tasks. To simultaneously
improve the segmentation of the foveal avascular zone (FAZ) and mitigate this
imbalance, we introduce FAZMamba and a unified Joint-OCTAMamba framework.
Experimental results on the OCTA-500 dataset demonstrate that Joint-OCTAMamba
outperforms existing models across evaluation metrics.The code is available at
https://github.com/lc-sfis/Joint-OCTAMamba.

</details>


### [87] [DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition](https://arxiv.org/abs/2509.11661)
*Lifei Hao,Yue Cheng,Baoqi Huang,Bing Jia,Xuandong Zhao*

Main category: cs.CV

TL;DR: DTGen是一种基于扩散模型的少样本数据增强方法，专为细粒度脏餐具识别设计，通过生成高质量样本提升分类性能，并支持轻量级部署。


<details>
  <summary>Details</summary>
Motivation: 智能餐具清洗在食品安全和智能家居中至关重要，但现有方法受限于粗粒度分类和少样本数据稀缺，难以满足工业化需求。

Method: 提出DTGen，一种基于生成扩散模型的少样本数据增强方案，通过LoRA实现高效领域专业化，结构化提示生成多样化脏污图像，并基于CLIP的跨模态过滤确保数据质量。

Result: 在极有限的真实少样本条件下，DTGen能合成几乎无限的高质量样本，显著提升分类器性能，支持细粒度脏污餐具识别。

Conclusion: DTGen不仅验证了生成式AI在少样本工业视觉中的价值，还为自动化餐具清洗和食品安全监测提供了可行的部署路径。

Abstract: Intelligent tableware cleaning is a critical application in food safety and
smart homes, but existing methods are limited by coarse-grained classification
and scarcity of few-shot data, making it difficult to meet industrialization
requirements. We propose DTGen, a few-shot data augmentation scheme based on
generative diffusion models, specifically designed for fine-grained dirty
tableware recognition. DTGen achieves efficient domain specialization through
LoRA, generates diverse dirty images via structured prompts, and ensures data
quality through CLIP-based cross-modal filtering. Under extremely limited real
few-shot conditions, DTGen can synthesize virtually unlimited high-quality
samples, significantly improving classifier performance and supporting
fine-grained dirty tableware recognition. We further elaborate on lightweight
deployment strategies, promising to transfer DTGen's benefits to embedded
dishwashers and integrate with cleaning programs to intelligently regulate
energy consumption and detergent usage. Research results demonstrate that DTGen
not only validates the value of generative AI in few-shot industrial vision but
also provides a feasible deployment path for automated tableware cleaning and
food safety monitoring.

</details>


### [88] [MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs](https://arxiv.org/abs/2509.11662)
*Feilong Chen,Yijiang Liu,Yi Huang,Hao Wang,Miren Tian,Ya-Qi Yu,Minghui Liao,Jihao Wu*

Main category: cs.CV

TL;DR: MindVL是一种多模态大语言模型，采用原生分辨率Vision Transformers和分布式训练框架，性能与Qwen2.5-VL相当，训练数据量仅为1/10，OCR评估表现领先。


<details>
  <summary>Details</summary>
Motivation: 开发MindVL旨在处理视觉密集型内容（如复杂图表和图表），同时确保在Ascend NPUs上的高效训练。

Method: MindVL采用原生分辨率Vision Transformers处理图像，避免了固定分辨率分块带来的性能下降。训练过程分为三个阶段：预热阶段、多任务训练阶段和监督指令调优阶段。此外，还采用了多模态数据打包和混合并行技术。

Result: MindVL在通用多模态理解和文档/表格理解评估中表现与Qwen2.5-VL相当，OCR评估中表现领先。

Conclusion: MindVL在评估中表现出与Qwen2.5-VL相当的性能，尽管训练数据量仅为后者的1/10，且在OCR评估中表现领先。

Abstract: We propose MindVL, a multimodal large langauge model trained on Ascend NPUs.
Similar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers,
which enables it to process images at their original variable resolutions. This
design avoids the degradation caused by fixed-resolution tiling while
preserving fine-grained details and global layouts, which is crucial for
visually dense content such as complex charts and diagrams. To ensure the
smooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a
distributed multimodal training framework tailored for Ascend NPUs. To maintain
training accuracy, we implement equivalent replacements for certain operators.
MindVL undergoes a three-phase training process, namely the warm-up phase,
multitask training phase, and supervised instruction tuning phase, to gradually
enhance its capabilities. This process starts with basic visual and multimodal
pre-training, followed by large-scale multiask trainging and instruction
tuning. We also adopt multimodal data packaging and hybrid parallelism
techniques, which significantly improve end-to-end training speed. To further
boost model performance, we specifically introduce test-time resolution search
and model weight averaging. Notably, despite using about 1/10 of the training
data required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL
in evaluations of general multimodal understanding and document/table
comprehension. Beyond overall scores, MindVL also delivers leading performance
in OCR assessments.

</details>


### [89] [RouteExtract: A Modular Pipeline for Extracting Routes from Paper Maps](https://arxiv.org/abs/2509.11674)
*Bjoern Kremser,Yusuke Matsui*

Main category: cs.CV

TL;DR: 提出了一种将扫描地图中的路径数字化为GPS导航路线的流程，结合多种技术手段，效果良好。


<details>
  <summary>Details</summary>
Motivation: 由于纸质地图包含精心策划的路径和本地相关注释，而这些信息在Google Maps等数字导航应用中常常缺失，因此需要一种方法将其数字化以供现代导航使用。

Method: 结合地理参考、基于U-Net的二元分割、图构建以及使用路由引擎的迭代细化程序。

Result: 评估表明，该方法能够从不同风格的地图中稳健地恢复路径网络，并生成适合实际使用的GPS路线。

Conclusion: 该论文提出了一种从扫描地图中提取可导航路径的流程，能够有效地将传统纸质地图的信息转化为适用于GPS导航的数字格式。

Abstract: Paper maps remain widely used for hiking and sightseeing because they contain
curated trails and locally relevant annotations that are often missing from
digital navigation applications such as Google Maps. We propose a pipeline to
extract navigable trails from scanned maps, enabling their use in GPS-based
navigation. Our method combines georeferencing, U-Net-based binary
segmentation, graph construction, and an iterative refinement procedure using a
routing engine. We evaluate the full end-to-end pipeline as well as individual
components, showing that the approach can robustly recover trail networks from
diverse map styles and generate GPS routes suitable for practical use.

</details>


### [90] [IMD: A 6-DoF Pose Estimation Benchmark for Industrial Metallic Objects](https://arxiv.org/abs/2509.11680)
*Ruimin Ma,Sebastian Zudaire,Zhen Li,Chi Zhang*

Main category: cs.CV

TL;DR: 论文提出一个针对工业金属物体的6D位姿估计数据集IMD，填补现有基准的不足，并通过实验证明其在工业场景中的挑战性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有6D位姿估计基准主要针对纹理丰富且低反射率的日常物体，难以推广到工业场景中金属、无纹理且高反射的物体。

Method: 论文通过RGB-D相机在自然室内光照和多样物体排列下捕获45个真实比例的工业组件，构建了一个包含视频对象分割、6D位姿跟踪和一次性6D位姿估计三个任务的基准测试。

Result: 评估表明，IMD数据集比现有家庭物体数据集更具挑战性，为工业机器人场景中的算法开发提供了基准。

Conclusion: 该论文提出了一个专为工业应用设计的IMD数据集和基准测试，填补了现有6D位姿估计基准在工业场景中的不足，并为开发更具泛化能力的算法提供了基础。

Abstract: Object 6DoF (6D) pose estimation is essential for robotic perception,
especially in industrial settings. It enables robots to interact with the
environment and manipulate objects. However, existing benchmarks on object 6D
pose estimation primarily use everyday objects with rich textures and
low-reflectivity, limiting model generalization to industrial scenarios where
objects are often metallic, texture-less, and highly reflective. To address
this gap, we propose a novel dataset and benchmark namely \textit{Industrial
Metallic Dataset (IMD)}, tailored for industrial applications. Our dataset
comprises 45 true-to-scale industrial components, captured with an RGB-D camera
under natural indoor lighting and varied object arrangements to replicate
real-world conditions. The benchmark supports three tasks, including video
object segmentation, 6D pose tracking, and one-shot 6D pose estimation. We
evaluate existing state-of-the-art models, including XMem and SAM2 for
segmentation, and BundleTrack and BundleSDF for pose estimation, to assess
model performance in industrial contexts. Evaluation results show that our
industrial dataset is more challenging than existing household object datasets.
This benchmark provides the baseline for developing and comparing segmentation
and pose estimation algorithms that better generalize to industrial robotics
scenarios.

</details>


### [91] [Uncertainty-Aware Retinal Vessel Segmentation via Ensemble Distillation](https://arxiv.org/abs/2509.11689)
*Jeremiah Fadugba,Petru Manescu,Bolanle Oladejo,Delmiro Fernandez-Reyes,Philipp Berens*

Main category: cs.CV

TL;DR: 提出 Ensemble Distillation 作为传统不确定性估计方法的替代方案，实验证明其性能相当且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 在视网膜血管分析中，准确预测对诊断至关重要，但传统的 Deep Ensembles 方法训练和测试成本高。

Method: 使用 Ensemble Distillation 技术，将多个集成模型的知识蒸馏到单一模型中。

Result: 在 DRIVE 和 FIVES 数据集上的实验表明，Ensemble Distillation 在校准和分割指标上表现相当，同时显著降低了计算复杂度。

Conclusion: Ensemble Distillation 提供了一种高效可靠的视网膜血管分割不确定性估计方法，有望成为医学影像应用的有力工具。

Abstract: Uncertainty estimation is critical for reliable medical image segmentation,
particularly in retinal vessel analysis, where accurate predictions are
essential for diagnostic applications. Deep Ensembles, where multiple networks
are trained individually, are widely used to improve medical image segmentation
performance. However, training and testing costs increase with the number of
ensembles. In this work, we propose Ensemble Distillation as a robust
alternative to commonly used uncertainty estimation techniques by distilling
the knowledge of multiple ensemble models into a single model. Through
extensive experiments on the DRIVE and FIVES datasets, we demonstrate that
Ensemble Distillation achieves comparable performance via calibration and
segmentation metrics, while significantly reducing computational complexity.
These findings suggest that Ensemble distillation provides an efficient and
reliable approach for uncertainty estimation in the segmentation of the retinal
vessels, making it a promising tool for medical imaging applications.

</details>


### [92] [The Quest for Universal Master Key Filters in DS-CNNs](https://arxiv.org/abs/2509.11711)
*Zahra Babaiee,Peyman M. Kiassari,Daniela Rus,Radu Grosu*

Main category: cs.CV

TL;DR: 研究发现DS-CNNs天然收敛到8个通用滤波器，这些滤波器与经典图像处理和生物视觉系统的基础结构高度一致，初始化网络表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩展“主密钥滤波器假说”，研究深度可分离卷积网络是否天然收敛到一组极少数通用滤波器。

Method: 通过系统化的无监督搜索，从不同架构和数据集中提取出8个通用滤波器，并验证其在初始化网络中的表现。

Result: 使用这8个冻结的通用滤波器初始化的网络在ImageNet上达到超过80%的准确率，并在小数据集上优于具有数千可训练参数的模型。

Conclusion: 深度可分离卷积网络（DS-CNNs）天然倾向于收敛到一组8个通用滤波器，这些滤波器在图像处理和生物视觉系统中具有基础性，为理解泛化和迁移学习提供了新视角。

Abstract: A recent study has proposed the "Master Key Filters Hypothesis" for
convolutional neural network filters. This paper extends this hypothesis by
radically constraining its scope to a single set of just 8 universal filters
that depthwise separable convolutional networks inherently converge to. While
conventional DS-CNNs employ thousands of distinct trained filters, our analysis
reveals these filters are predominantly linear shifts (ax+b) of our discovered
universal set. Through systematic unsupervised search, we extracted these
fundamental patterns across different architectures and datasets. Remarkably,
networks initialized with these 8 unique frozen filters achieve over 80%
ImageNet accuracy, and even outperform models with thousands of trainable
parameters when applied to smaller datasets. The identified master key filters
closely match Difference of Gaussians (DoGs), Gaussians, and their derivatives,
structures that are not only fundamental to classical image processing but also
strikingly similar to receptive fields in mammalian visual systems. Our
findings provide compelling evidence that depthwise convolutional layers
naturally gravitate toward this fundamental set of spatial operators regardless
of task or architecture. This work offers new insights for understanding
generalization and transfer learning through the universal language of these
master key filters.

</details>


### [93] [Microsurgical Instrument Segmentation for Robot-Assisted Surgery](https://arxiv.org/abs/2509.11727)
*Tae Kyeong Jeong,Garam Kim,Juyoun Park*

Main category: cs.CV

TL;DR: MISRA框架通过增强输入和迭代反馈，显著提升微手术器械分割性能，尤其在薄结构处理上表现优异。


<details>
  <summary>Details</summary>
Motivation: 微手术中薄结构分割的准确性对场景理解至关重要，但由于分辨率损失、低对比度和类别不平衡等问题，现有方法面临挑战。

Method: 提出MISRA分割框架，通过增强RGB输入的光度通道、集成跳跃注意力以保留细长特征，并采用迭代反馈模块（IFM）在多轮处理中恢复连续性。

Result: 实验表明MISRA在平均类别IoU上比竞争方法提高了5.37%，且在器械接触和重叠区域提供更稳定的预测。

Conclusion: MISRA框架在计算机辅助和机器人微手术中展现出可靠场景解析的潜力，其性能优于现有方法，特别是在器械接触和重叠区域提供更稳定的预测。

Abstract: Accurate segmentation of thin structures is critical for microsurgical scene
understanding but remains challenging due to resolution loss, low contrast, and
class imbalance. We propose Microsurgery Instrument Segmentation for Robotic
Assistance(MISRA), a segmentation framework that augments RGB input with
luminance channels, integrates skip attention to preserve elongated features,
and employs an Iterative Feedback Module(IFM) for continuity restoration across
multiple passes. In addition, we introduce a dedicated microsurgical dataset
with fine-grained annotations of surgical instruments including thin objects,
providing a benchmark for robust evaluation Dataset available at
https://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg. Experiments demonstrate
that MISRA achieves competitive performance, improving the mean class IoU by
5.37% over competing methods, while delivering more stable predictions at
instrument contacts and overlaps. These results position MISRA as a promising
step toward reliable scene parsing for computer-assisted and robotic
microsurgery.

</details>


### [94] [Advanced Layout Analysis Models for Docling](https://arxiv.org/abs/2509.11720)
*Nikolaos Livathinos,Christoph Auer,Ahmed Nassar,Rafael Teixeira de Lima,Maksym Lysak,Brown Ebouky,Cesar Berrospi,Michele Dolfi,Panagiotis Vagenas,Matteo Omenetti,Kasper Dinkla,Yusik Kim,Valery Weber,Lucas Morin,Ingmar Meijer,Viktor Kuropiatnyk,Tim Strohmeyer,A. Said Gurbuz,Peter W. J. Staar*

Main category: cs.CV

TL;DR: 本文开发了五种新的文档布局模型，显著提升了mAP和运行时性能，最佳模型在A100 GPU上达到78% mAP和28ms/图像的推理速度。


<details>
  <summary>Details</summary>
Motivation: 开发新的布局分析模型以改进Docling文档转换流程的性能和效率。

Method: 训练了基于RT-DETR、RT-DETRv2和DFINE架构的多种先进目标检测器，并在一个包含15万份文档的异构语料库上进行训练。对原始检测结果进行了后处理步骤以使其更适用于文档转换任务。

Result: 新模型在多个文档基准测试中表现出色，mAP显著提升，同时在不同环境（CPU、Nvidia和Apple GPU）下的运行时性能也得到了验证。

Conclusion: 本文介绍了五种新的文档布局模型，相比之前的基线模型，mAP提升了20.6%至23.9%，并且在运行时性能上表现相当或更好。最佳模型'heron-101'在单个NVIDIA A100 GPU上实现了78%的mAP和28毫秒/图像的推理时间。

Abstract: This technical report documents the development of novel Layout Analysis
models integrated into the Docling document-conversion pipeline. We trained
several state-of-the-art object detectors based on the RT-DETR, RT-DETRv2 and
DFINE architectures on a heterogeneous corpus of 150,000 documents (both openly
available and proprietary). Post-processing steps were applied to the raw
detections to make them more applicable to the document conversion task. We
evaluated the effectiveness of the layout analysis on various document
benchmarks using different methodologies while also measuring the runtime
performance across different environments (CPU, Nvidia and Apple GPUs). We
introduce five new document layout models achieving 20.6% - 23.9% mAP
improvement over Docling's previous baseline, with comparable or better
runtime. Our best model, "heron-101", attains 78% mAP with 28 ms/image
inference time on a single NVIDIA A100 GPU. Extensive quantitative and
qualitative experiments establish best practices for training, evaluating, and
deploying document-layout detectors, providing actionable guidance for the
document conversion community. All trained checkpoints, code, and documentation
are released under a permissive license on HuggingFace.

</details>


### [95] [Bridging the Gap Between Sparsity and Redundancy: A Dual-Decoding Framework with Global Context for Map Inference](https://arxiv.org/abs/2509.11731)
*Yudong Shen,Wenyu Wu,Jiali Mao,Yixiao Tong,Guoping Liu,Chaoya Wang*

Main category: cs.CV

TL;DR: DGMap是一个具有全局上下文感知的双解码框架，通过多尺度网格编码和关键点提取技术，有效解决了轨迹数据中稀疏和密集区域的问题，提升了地图推断的准确性。


<details>
  <summary>Details</summary>
Motivation: 轨迹数据在自动地图推断中具有低成本、广泛覆盖和持续可用的优势，但轨迹密度不均导致稀疏区域道路碎片化和密集区域冗余段，现有方法难以应对这些挑战。

Method: DGMap采用双解码框架，包括多尺度网格编码、掩码增强关键点提取和全局上下文感知关系预测。

Result: 在三个真实数据集上的实验结果表明，DGMap在APLS指标上优于现有方法5%，尤其是在滴滴出行平台的轨迹数据上表现显著。

Conclusion: DGMap通过整合全局语义上下文和局部几何特征，显著提升了关键点检测的准确性，减少了稀疏轨迹区域的碎片化道路问题，同时通过全局上下文感知的关系预测模块抑制了密集轨迹区域的虚假连接。

Abstract: Trajectory data has become a key resource for automated map in-ference due to
its low cost, broad coverage, and continuous availability. However, uneven
trajectory density often leads to frag-mented roads in sparse areas and
redundant segments in dense regions, posing significant challenges for existing
methods. To address these issues, we propose DGMap, a dual-decoding framework
with global context awareness, featuring Multi-scale Grid Encoding,
Mask-enhanced Keypoint Extraction, and Global Context-aware Relation
Prediction. By integrating global semantic context with local geometric
features, DGMap improves keypoint detection accuracy to reduce road
fragmentation in sparse-trajectory areas. Additionally, the Global
Context-aware Relation Prediction module suppresses false connections in
dense-trajectory regions by modeling long-range trajectory patterns.
Experimental results on three real-world datasets show that DGMap outperforms
state-of-the-art methods by 5% in APLS, with notable performance gains on
trajectory data from the Didi Chuxing platform

</details>


### [96] [SpecVLM: Fast Speculative Decoding in Vision-Language Models](https://arxiv.org/abs/2509.11815)
*Haiduo Huang,Fuwei Yang,Zhenhua Liu,Xuanwu Yin,Dong Li,Pengju Ren,Emad Barsoum*

Main category: cs.CV

TL;DR: SpecVLM通过弹性视觉压缩和在线对数蒸馏，显著加速视觉语言模型推理，最高达2.9倍速度提升，且无损解码。


<details>
  <summary>Details</summary>
Motivation: 直接应用推测解码到视觉语言模型面临系统约束，特别是预填充阶段由视觉令牌主导，增加了计算和内存开销。

Method: 提出了一种弹性视觉压缩器，自适应选择剪枝、池化、卷积和重采样原语，平衡FLOPs/参数和每输入精度；并设计了在线对数蒸馏协议，使用交叉熵和Smooth L1目标训练草稿模型。

Result: SpecVLM在LLaVA和MMMU上实现了2.5--2.9倍的端到端加速，且在5个epoch内保持目标模型输出分布。

Conclusion: SpecVLM通过引入弹性视觉压缩器和在线对数蒸馏协议，显著加速了视觉语言模型的推理速度，同时保持了目标模型的输出分布（无损解码）。

Abstract: Speculative decoding is a powerful way to accelerate autoregressive large
language models (LLMs), but directly porting it to vision-language models
(VLMs) faces unique systems constraints: the prefill stage is dominated by
visual tokens whose count scales with image resolution and video length,
inflating both compute and memory, especially the key-value (KV) cache. We
study speculative decoding for VLMs and introduce SpecVLM, a practical system
that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering
1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)
further accelerates VLM inference with an elastic visual compressor that
adaptively selects among pruning, pooling, convolution, and resampler
primitives to balance FLOPs/parameters and accuracy per input. To avoid costly
offline distillation corpora, we propose an online-logit distillation protocol
that trains the draft model with on-the-fly teacher logits and penultimate
features using a combined cross-entropy and Smooth L1 objective, eliminating
storage and preprocessing while remaining compute-efficient. This protocol
reveals a training-time scaling effect: longer online training monotonically
increases the draft model's average accepted length, improving speculative
efficiency. Empirically, SpecVLM achieves additional acceleration, culminating
in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,
consistently over resolutions and task difficulties, while preserving the
target model's output distribution (lossless decoding). Our code is available
at https://github.com/haiduo/SpecVLM.

</details>


### [97] [Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network](https://arxiv.org/abs/2509.11838)
*Navid Hashemi,Samuel Sasaki,Diego Manzanas Lopez,Ipek Oguz,Meiyi Ma,Taylor T. Johnson*

Main category: cs.CV

TL;DR: 论文提出了一种可扩展的概率验证框架，结合采样可达性分析和共形推理，为高维分割任务提供可靠且非保守的安全保证。


<details>
  <summary>Details</summary>
Motivation: 现有概率验证方法难以应对现代分割任务的复杂性和高维度问题，导致其提供的保证过于保守而不实用。

Method: 论文介绍了一种架构无关且可扩展至高维输出的概率验证框架，结合采样可达性分析和共形推理（CI），并提出了减少保守性的新策略。

Result: 在多个大规模分割模型上的实验表明，该方法不仅提供了可靠的安全保证，还显著缩小了与现有最优方法的差距。

Conclusion: 该论文提出了一种结合采样可达性分析和共形推理的概率验证框架，有效解决了现有方法在复杂性和维度上的局限性，为现代分割任务提供了可靠的安全保证。

Abstract: Semantic segmentation networks (SSNs) play a critical role in domains such as
medical imaging, autonomous driving, and environmental monitoring, where safety
hinges on reliable model behavior under uncertainty. Yet, existing
probabilistic verification approaches struggle to scale with the complexity and
dimensionality of modern segmentation tasks, often yielding guarantees that are
too conservative to be practical. We introduce a probabilistic verification
framework that is both architecture-agnostic and scalable to high-dimensional
outputs. Our approach combines sampling-based reachability analysis with
conformal inference (CI) to deliver provable guarantees while avoiding the
excessive conservatism of prior methods. To counteract CI's limitations in
high-dimensional settings, we propose novel strategies that reduce conservatism
without compromising rigor. Empirical evaluation on large-scale segmentation
models across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates
that our method provides reliable safety guarantees while substantially
tightening bounds compared to SOTA. We also provide a toolbox implementing this
technique, available on Github.

</details>


### [98] [A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications](https://arxiv.org/abs/2509.11752)
*Hongyuan Zhang,Yuheng Wu,Mingyang Zhao,Zhiwei Chen,Rebecca Li,Fei Zhu,Haohan Zhao,Xiaohua Yuan,Meng Yang,Chunli Qiu,Xiang Cong,Haiyan Chen,Lina Luan,Randolph H. L. Wong,Huai Liao,Colin A Graham,Shi Chang,Guowei Tao,Dong Yi,Zhen Lei,Nassir Navab,Sebastien Ourselin,Jiebo Luo,Hongbin Liu,Gaofeng Meng*

Main category: cs.CV

TL;DR: EchoCare是一个通过自监督学习开发的通用超声基础模型，在多个超声应用基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实临床环境中大型标记数据集的稀缺性以及任务特定模型的有限泛化能力阻碍了通用临床AI模型的发展。

Method: 通过自监督学习在大型数据集EchoCareData上开发EchoCare模型，并引入分层分类器以联合学习像素级和表示级特征。

Result: EchoCare在10个具有不同诊断难度的超声基准测试中优于最先进的比较模型。

Conclusion: EchoCare作为一个完全开放、通用的基础模型，能够促进多样化的临床超声AI技术的发展。

Abstract: Artificial intelligence (AI) that can effectively learn ultrasound
representations by integrating multi-source data holds significant promise for
advancing clinical care. However, the scarcity of large labeled datasets in
real-world clinical environments and the limited generalizability of
task-specific models have hindered the development of generalizable clinical AI
models for ultrasound applications. In this study, we present EchoCare, a novel
ultrasound foundation model for generalist clinical use, developed via
self-supervised learning on our curated, publicly available, large-scale
dataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images,
sourced from over 23 countries across 5 continents and acquired via a diverse
range of distinct imaging devices, thus encompassing global cohorts that are
multi-center, multi-device, and multi-ethnic. Unlike prior studies that adopt
off-the-shelf vision foundation model architectures, we introduce a
hierarchical classifier into EchoCare to enable joint learning of pixel-level
and representation-level features, capturing both global anatomical contexts
and local ultrasound characteristics. With minimal training, EchoCare
outperforms state-of-the-art comparison models across 10 representative
ultrasound benchmarks of varying diagnostic difficulties, spanning disease
diagnosis, lesion segmentation, organ detection, landmark prediction,
quantitative regression, imaging enhancement and report generation. The code
and pretrained model are publicly released, rendering EchoCare accessible for
fine-tuning and local adaptation, supporting extensibility to additional
applications. EchoCare provides a fully open and generalizable foundation model
to boost the development of AI technologies for diverse clinical ultrasound
applications.

</details>


### [99] [Bridging Vision Language Models and Symbolic Grounding for Video Question Answering](https://arxiv.org/abs/2509.11862)
*Haodi Ma,Vyom Pathak,Daisy Zhe Wang*

Main category: cs.CV

TL;DR: SG-VLM通过场景图基础提升视频问答的因果和时间推理能力，但相对于强视觉语言模型增益有限，展示了符号基础的潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 解决当前视觉语言模型在视频问答中依赖浅层相关性导致的弱时间基础和有限可解释性问题。

Method: SG-VLM是一个模块化框架，通过提示和视觉定位将冻结的视觉语言模型与场景图基础相结合。

Result: 在三个基准测试（NExT-QA、iVQA、ActivityNet-QA）和多个视觉语言模型（QwenVL、InternVL）上，SG-VLM提高了因果和时间推理能力，并优于现有基线，但相对于强大的视觉语言模型增益有限。

Conclusion: SG-VLM框架展示了符号基础在视频问答中的潜力，但也指出了当前符号基础的局限性，为未来混合视觉语言模型与符号方法的视频理解提供了指导。

Abstract: Video Question Answering (VQA) requires models to reason over spatial,
temporal, and causal cues in videos. Recent vision language models (VLMs)
achieve strong results but often rely on shallow correlations, leading to weak
temporal grounding and limited interpretability. We study symbolic scene graphs
(SGs) as intermediate grounding signals for VQA. SGs provide structured
object-relation representations that complement VLMs holistic reasoning. We
introduce SG-VLM, a modular framework that integrates frozen VLMs with scene
graph grounding via prompting and visual localization. Across three benchmarks
(NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLM
improves causal and temporal reasoning and outperforms prior baselines, though
gains over strong VLMs are limited. These findings highlight both the promise
and current limitations of symbolic grounding, and offer guidance for future
hybrid VLM-symbolic approaches in video understanding.

</details>


### [100] [MSMA: Multi-Scale Feature Fusion For Multi-Attribute 3D Face Reconstruction From Unconstrained Images](https://arxiv.org/abs/2509.11763)
*Danling Cao*

Main category: cs.CV

TL;DR: 提出MSMA框架，通过多尺度特征融合和大核注意力模块提升3D人脸重建精度，实验结果优于或媲美现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于无约束环境中多样化的条件，从单张无约束图像重建3D人脸仍具挑战性。现有方法在捕捉细节和多尺度特征方面存在不足。

Method: 整合了多尺度特征融合与多属性学习，并利用大核注意力模块增强跨尺度特征提取的精度。

Result: 在MICC Florence、Facewarehouse和自定义数据集上的实验表明，该方法与当前最先进方法性能相当，部分情况下更优。

Conclusion: 提出的MSMA框架在多种挑战性条件下表现优异，部分情况下甚至超越了当前最先进方法。

Abstract: Reconstructing 3D face from a single unconstrained image remains a
challenging problem due to diverse conditions in unconstrained environments.
Recently, learning-based methods have achieved notable results by effectively
capturing complex facial structures and details across varying conditions.
Consequently, many existing approaches employ projection-based losses between
generated and input images to constrain model training. However, learning-based
methods for 3D face reconstruction typically require substantial amounts of 3D
facial data, which is difficult and costly to obtain. Consequently, to reduce
reliance on labeled 3D face datasets, many existing approaches employ
projection-based losses between generated and input images to constrain model
training. Nonetheless, despite these advancements, existing approaches
frequently struggle to capture detailed and multi-scale features under diverse
facial attributes and conditions, leading to incomplete or less accurate
reconstructions. In this paper, we propose a Multi-Scale Feature Fusion with
Multi-Attribute (MSMA) framework for 3D face reconstruction from unconstrained
images. Our method integrates multi-scale feature fusion with a focus on
multi-attribute learning and leverages a large-kernel attention module to
enhance the precision of feature extraction across scales, enabling accurate 3D
facial parameter estimation from a single 2D image. Comprehensive experiments
on the MICC Florence, Facewarehouse and custom-collect datasets demonstrate
that our approach achieves results on par with current state-of-the-art
methods, and in some instances, surpasses SOTA performance across challenging
conditions.

</details>


### [101] [Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization](https://arxiv.org/abs/2509.11772)
*Diogo Mendonça,Tiago Barros,Cristiano Premebida,Urbano J. Nunes*

Main category: cs.CV

TL;DR: Seg2Track-SAM2是一个无需微调的框架，结合预训练检测器和SAM2，实现了高效的MOTS，性能优异且内存占用低。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型（如SAM2）在视频分割中表现出强大的零样本泛化能力，但其直接应用于MOTS时仍受限于身份管理和内存效率不足。

Method: 该框架整合了预训练的对象检测器、SAM2和新的Seg2Track模块，无需微调且与检测器无关。

Result: 在KITTI MOT和KITTI MOTS基准测试中，Seg2Track-SAM2实现了最先进的性能，同时在关联准确性（AssA）上设立了新基准，并通过滑动窗口内存策略将内存使用减少了75%。

Conclusion: Seg2Track-SAM2通过结合强大的零样本跟踪、增强的身份保持和高效的内存利用，推动了MOTS的发展。

Abstract: Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to
operate reliably in dynamic environments. MOT ensures consistent object
identity assignment and precise spatial delineation. Recent advances in
foundation models, such as SAM2, have demonstrated strong zero-shot
generalization for video segmentation, but their direct application to MOTS
(MOT+Segmentation) remains limited by insufficient identity management and
memory efficiency. This work introduces Seg2Track-SAM2, a framework that
integrates pre-trained object detectors with SAM2 and a novel Seg2Track module
to address track initialization, track management, and reinforcement. The
proposed approach requires no fine-tuning and remains detector-agnostic.
Experimental results on KITTI MOT and KITTI MOTS benchmarks show that
Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth
overall in both car and pedestrian classes on KITTI MOTS, while establishing a
new benchmark in association accuracy (AssA). Furthermore, a sliding-window
memory strategy reduces memory usage by up to 75% with negligible performance
degradation, supporting deployment under resource constraints. These results
confirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot
tracking, enhanced identity preservation, and efficient memory utilization. The
code is available at https://github.com/hcmr-lab/Seg2Track-SAM2

</details>


### [102] [Integrating Prior Observations for Incremental 3D Scene Graph Prediction](https://arxiv.org/abs/2509.11895)
*Marian Renz,Felix Igelbrink,Martin Atzmueller*

Main category: cs.CV

TL;DR: 提出一种异构图模型，通过多模态信息增强增量式3D语义场景图预测，实验证明其在真实环境中有效。


<details>
  <summary>Details</summary>
Motivation: 现有3D语义场景图方法主要依赖传感器数据且需完整场景重建，限制了其在增量式真实场景中的应用。因此，需开发一种能整合多模态信息并支持增量预测的方法。

Method: 采用异构图模型，结合多模态信息（如CLIP语义嵌入和先验观察），通过多层消息传递机制灵活融合全局和局部场景表示，无需完整场景重建或专用模块。

Result: 在3DSSG数据集上的实验表明，整合多模态信息的图神经网络（GNN）在复杂环境中表现优异，具有可扩展性和泛化性。

Conclusion: 本研究提出了一种新颖的异构图模型，用于增量式3D语义场景图预测，通过整合多模态信息（如先验观察和语义嵌入）提升了模型在复杂真实环境中的可扩展性和泛化能力。

Abstract: 3D semantic scene graphs (3DSSG) provide compact structured representations
of environments by explicitly modeling objects, attributes, and relationships.
While 3DSSGs have shown promise in robotics and embodied AI, many existing
methods rely mainly on sensor data, not integrating further information from
semantically rich environments. Additionally, most methods assume access to
complete scene reconstructions, limiting their applicability in real-world,
incremental settings. This paper introduces a novel heterogeneous graph model
for incremental 3DSSG prediction that integrates additional, multi-modal
information, such as prior observations, directly into the message-passing
process. Utilizing multiple layers, the model flexibly incorporates global and
local scene representations without requiring specialized modules or full scene
reconstructions. We evaluate our approach on the 3DSSG dataset, showing that
GNNs enriched with multi-modal information such as semantic embeddings (e.g.,
CLIP) and prior observations offer a scalable and generalizable solution for
complex, real-world environments. The full source code of the presented
architecture will be made available at
https://github.com/m4renz/incremental-scene-graph-prediction.

</details>


### [103] [SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel Segmentation](https://arxiv.org/abs/2509.11774)
*Changlu Guo,Anders Nymark Christensen,Anders Bjorholm Dahl,Yugen Yi,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: SA-UNetv2是一种轻量级视网膜血管分割模型，通过改进注意力机制和损失函数，在资源受限环境下实现了高效性能。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割对早期诊断多种疾病至关重要，但现有SA-UNet在跳跃连接中未充分利用注意力机制，且未解决严重的前景-背景不平衡问题。

Method: 提出了SA-UNetv2，一种轻量级模型，通过在所有跳跃连接中注入跨尺度空间注意力以增强多尺度特征融合，并采用加权BCE加MCC损失函数来提高对类别不平衡的鲁棒性。

Result: 在公共DRIVE和STARE数据集上，SA-UNetv2以仅1.2MB内存和0.26M参数（少于SA-UNet的50%）实现了最先进的性能，并在592 x 592 x 3图像上实现了1秒CPU推理。

Conclusion: SA-UNetv2在资源受限的CPU-only环境下表现出色，具有高效性和可部署性。

Abstract: Retinal vessel segmentation is essential for early diagnosis of diseases such
as diabetic retinopathy, hypertension, and neurodegenerative disorders.
Although SA-UNet introduces spatial attention in the bottleneck, it underuses
attention in skip connections and does not address the severe
foreground-background imbalance. We propose SA-UNetv2, a lightweight model that
injects cross-scale spatial attention into all skip connections to strengthen
multi-scale feature fusion and adopts a weighted Binary Cross-Entropy (BCE)
plus Matthews Correlation Coefficient (MCC) loss to improve robustness to class
imbalance. On the public DRIVE and STARE datasets, SA-UNetv2 achieves
state-of-the-art performance with only 1.2MB memory and 0.26M parameters (less
than 50% of SA-UNet), and 1 second CPU inference on 592 x 592 x 3 images,
demonstrating strong efficiency and deployability in resource-constrained,
CPU-only settings.

</details>


### [104] [FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via Agent-of-Thoughts Reasoning](https://arxiv.org/abs/2509.11796)
*Haodong Chen,Haojian Huang,XinXiang Yin,Dian Shao*

Main category: cs.CV

TL;DR: FineQuest是一个无需训练的双模式推理框架，结合SSGraph体育知识场景图，显著提升了体育视频问答性能。


<details>
  <summary>Details</summary>
Motivation: 体育视频的复杂性使得基于大型语言模型的视频问答面临挑战，需要一种无需训练且能结合领域知识的方法来提高性能。

Method: 提出了FineQuest框架，采用双模式推理（反应性推理和深思熟虑推理）和SSGraph多模态体育知识场景图来增强体育视频的理解能力。

Result: FineQuest在Gym-QA、Diving-QA和SPORTU数据集上取得了最先进的性能，同时保持了强大的通用视频问答能力。

Conclusion: FineQuest通过结合反应性推理和深思熟虑推理的双模式机制，以及SSGraph多模态体育知识场景图，显著提升了体育视频问答的性能，并在多个基准测试中实现了最先进的结果。

Abstract: Video Question Answering (VideoQA) based on Large Language Models (LLMs) has
shown potential in general video understanding but faces significant challenges
when applied to the inherently complex domain of sports videos. In this work,
we propose FineQuest, the first training-free framework that leverages
dual-mode reasoning inspired by cognitive science: i) Reactive Reasoning for
straightforward sports queries and ii) Deliberative Reasoning for more complex
ones. To bridge the knowledge gap between general-purpose models and
domain-specific sports understanding, FineQuest incorporates SSGraph, a
multimodal sports knowledge scene graph spanning nine sports, which encodes
both visual instances and domain-specific terminology to enhance reasoning
accuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QA
and Diving-QA, derived from the FineGym and FineDiving datasets, enabling
diverse and comprehensive evaluation. FineQuest achieves state-of-the-art
performance on these benchmarks as well as the existing SPORTU dataset, while
maintains strong general VideoQA capabilities.

</details>


### [105] [Pseudo-D: Informing Multi-View Uncertainty Estimation with Calibrated Neural Training Dynamics](https://arxiv.org/abs/2509.11800)
*Ang Nan Gu,Michael Tsang,Hooman Vaseli,Purang Abolmaesumi,Teresa Tsang*

Main category: cs.CV

TL;DR: 提出一种基于神经网络训练动态的不确定性感知伪标签生成方法，显著提升医学图像分类的鲁棒性和校准性能。


<details>
  <summary>Details</summary>
Motivation: 当前计算机辅助诊断系统使用的标签过于简化，忽略了诊断不确定性，导致模型在面对噪声或模糊数据时过度自信。

Method: 利用神经网络训练动态（NNTD）评估每个训练样本的固有难度，通过聚合和校准模型预测生成不确定性感知的伪标签。

Result: 在心脏超声分类基准测试中，该方法在校准、选择性分类和多视图融合方面表现优于专门基线。

Conclusion: 该论文提出的方法通过引入不确定性感知的伪标签，显著提升了计算机辅助诊断系统在噪声和模糊数据上的表现，尤其在心脏超声分类任务中优于现有基线。

Abstract: Computer-aided diagnosis systems must make critical decisions from medical
images that are often noisy, ambiguous, or conflicting, yet today's models are
trained on overly simplistic labels that ignore diagnostic uncertainty. One-hot
labels erase inter-rater variability and force models to make overconfident
predictions, especially when faced with incomplete or artifact-laden inputs. We
address this gap by introducing a novel framework that brings uncertainty back
into the label space. Our method leverages neural network training dynamics
(NNTD) to assess the inherent difficulty of each training sample. By
aggregating and calibrating model predictions during training, we generate
uncertainty-aware pseudo-labels that reflect the ambiguity encountered during
learning. This label augmentation approach is architecture-agnostic and can be
applied to any supervised learning pipeline to enhance uncertainty estimation
and robustness. We validate our approach on a challenging echocardiography
classification benchmark, demonstrating superior performance over specialized
baselines in calibration, selective classification, and multi-view fusion.

</details>


### [106] [LFRA-Net: A Lightweight Focal and Region-Aware Attention Network for Retinal Vessel Segmentatio](https://arxiv.org/abs/2509.11811)
*Mehwish Mehmood,Shahzaib Iqbal,Tariq Mahmood Khan,Ivor Spence,Muhammad Fahim*

Main category: cs.CV

TL;DR: LFRA-Net 是一种轻量级网络，通过改进注意力机制优化视网膜血管分割，在准确性和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割对于早期诊断视力威胁和全身性疾病至关重要，尤其是在计算资源有限的临床环境中。当前模型在提取微小血管和高计算成本方面仍面临挑战。

Method: LFRA-Net 通过结合焦点调制注意力在编码器-解码器瓶颈和选择性跳跃连接中的区域感知注意力，优化了视网膜血管分割。

Result: LFRA-Net 在三个公开数据集上表现优异，Dice 分数和 Jaccard 指数均优于现有方法，同时保持轻量级特性（0.17 百万参数，0.66 MB 内存大小，10.50 GFLOPs）。

Conclusion: LFRA-Net 提供了一种在分割准确性和计算成本之间的理想平衡，适用于资源有限的实时临床应用。

Abstract: Retinal vessel segmentation is critical for the early diagnosis of
vision-threatening and systemic diseases, especially in real-world clinical
settings with limited computational resources. Although significant
improvements have been made in deep learning-based segmentation methods,
current models still face challenges in extracting tiny vessels and suffer from
high computational costs. In this study, we present LFRA-Net by incorporating
focal modulation attention at the encoder-decoder bottleneck and region-aware
attention in the selective skip connections. LFRA-Net is a lightweight network
optimized for precise and effective retinal vascular segmentation. It enhances
feature representation and regional focus by efficiently capturing local and
global dependencies. LFRA-Net outperformed many state-of-the-art models while
maintaining lightweight characteristics with only 0.17 million parameters, 0.66
MB memory size, and 10.50 GFLOPs. We validated it on three publicly available
datasets: DRIVE, STARE, and CHASE\_DB. It performed better in terms of Dice
score (84.28\%, 88.44\%, and 85.50\%) and Jaccard index (72.86\%, 79.31\%, and
74.70\%) on the DRIVE, STARE, and CHASE\_DB datasets, respectively. LFRA-Net
provides an ideal ratio between segmentation accuracy and computational cost
compared to existing deep learning methods, which makes it suitable for
real-time clinical applications in areas with limited resources. The code can
be found at https://github.com/Mehwish4593/LFRA-Net.

</details>


### [107] [Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing](https://arxiv.org/abs/2509.12040)
*Bingyu Li,Haocheng Dong,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: 该论文提出了一种针对遥感图像的开放词汇分割框架RSKT-Seg，通过三个关键模块显著提升了性能与效率。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏统一的评估基准和自然图像与遥感图像之间的领域差距，开放词汇遥感图像分割任务尚未得到充分探索。

Method: 提出了RSKT-Seg框架，包含RS-CMA模块、RS-Fusion变换器和RS-Transfer模块，用于捕捉旋转不变性视觉线索、建模空间和语义依赖以及促进领域适应。

Result: RSKT-Seg在基准测试中比现有基线方法提升了+3.8 mIoU和+5.9 mACC，同时推理速度提高了2倍。

Conclusion: RSKT-Seg框架在开放词汇遥感图像分割任务中表现出色，显著优于现有基线方法，并提高了推理效率。

Abstract: Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task
that adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS)
domain, remains underexplored due to the absence of a unified evaluation
benchmark and the domain gap between natural and RS images. To bridge these
gaps, we first establish a standardized OVRSIS benchmark (\textbf{OVRSISBench})
based on widely-used RS segmentation datasets, enabling consistent evaluation
across methods. Using this benchmark, we comprehensively evaluate several
representative OVS/OVRSIS models and reveal their limitations when directly
applied to remote sensing scenarios. Building on these insights, we propose
\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for
remote sensing. RSKT-Seg integrates three key components: (1) a
Multi-Directional Cost Map Aggregation (RS-CMA) module that captures
rotation-invariant visual cues by computing vision-language cosine similarities
across multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion)
transformer, which jointly models spatial and semantic dependencies with a
lightweight dimensionality reduction strategy; and (3) a Remote Sensing
Knowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and
facilitates domain adaptation via enhanced upsampling. Extensive experiments on
the benchmark show that RSKT-Seg consistently outperforms strong OVS baselines
by +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through
efficient aggregation. Our code is
\href{https://github.com/LiBingyu01/RSKT-Seg}{\textcolor{blue}{here}}.

</details>


### [108] [Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking](https://arxiv.org/abs/2509.12046)
*Zirui Zheng,Takashi Isobe,Tong Shen,Xu Jia,Jianbin Zhao,Xiaomin Li,Mengmeng Ge,Baolu Li,Qinghe Wang,Dong Li,Dong Zhou,Yunzhi Zhuge,Huchuan Lu,Emad Barsoum*

Main category: cs.CV

TL;DR: SMARLI通过结构化掩码和GRPO后训练，实现了布局条件与AR图像生成的高效结合，提升了控制准确性。


<details>
  <summary>Details</summary>
Motivation: 解决AR模型在布局条件生成中因稀疏布局条件和特征纠缠带来的挑战。

Method: 采用结构化掩码策略来管理全局提示、布局和图像标记之间的交互，并结合GRPO后训练方案和布局奖励函数。

Result: 实验证明SMARLI能够在保持生成质量的同时，有效整合布局标记，实现优越的布局感知控制。

Conclusion: SMARLI成功地将布局条件无缝整合到基于AR的图像生成中，既保持了生成质量，又提升了布局控制的准确性。

Abstract: While autoregressive (AR) models have demonstrated remarkable success in
image generation, extending them to layout-conditioned generation remains
challenging due to the sparse nature of layout conditions and the risk of
feature entanglement. We present Structured Masking for AR-based
Layout-to-Image (SMARLI), a novel framework for layoutto-image generation that
effectively integrates spatial layout constraints into AR-based image
generation. To equip AR model with layout control, a specially designed
structured masking strategy is applied to attention computation to govern the
interaction among the global prompt, layout, and image tokens. This design
prevents mis-association between different regions and their descriptions while
enabling sufficient injection of layout constraints into the generation
process. To further enhance generation quality and layout accuracy, we
incorporate Group Relative Policy Optimization (GRPO) based post-training
scheme with specially designed layout reward functions for next-set-based AR
models. Experimental results demonstrate that SMARLI is able to seamlessly
integrate layout tokens with text and image tokens without compromising
generation quality. It achieves superior layoutaware control while maintaining
the structural simplicity and generation efficiency of AR models.

</details>


### [109] [MAFS: Masked Autoencoder for Infrared-Visible Image Fusion and Semantic Segmentation](https://arxiv.org/abs/2509.11817)
*Liying Wang,Xiaoli Zhang,Chuanmin Jia,Siwei Ma*

Main category: cs.CV

TL;DR: MAFS是一个并行结构的统一网络，用于红外-可见光图像融合和语义分割，通过异构特征融合和多任务学习策略取得了竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法未从宏观任务层面探讨像素级图像融合与跨模态特征融合感知任务之间的相互促进作用。

Method: 提出了一个并行结构的统一网络MAFS，包含融合子网络和分割子网络，采用异构特征融合策略和多阶段Transformer解码器。

Result: 实验表明，该方法在图像融合和语义分割任务中达到了与现有最先进方法竞争的结果。

Conclusion: MAFS框架通过并行结构和多任务学习策略，在红外-可见光图像融合和语义分割任务中取得了竞争性结果。

Abstract: Infrared-visible image fusion methods aim at generating fused images with
good visual quality and also facilitate the performance of high-level tasks.
Indeed, existing semantic-driven methods have considered semantic information
injection for downstream applications. However, none of them investigates the
potential for reciprocal promotion between pixel-wise image fusion and
cross-modal feature fusion perception tasks from a macroscopic task-level
perspective. To address this limitation, we propose a unified network for image
fusion and semantic segmentation. MAFS is a parallel structure, containing a
fusion sub-network and a segmentation sub-network. On the one hand, We devise a
heterogeneous feature fusion strategy to enhance semantic-aware capabilities
for image fusion. On the other hand, by cascading the fusion sub-network and a
segmentation backbone, segmentation-related knowledge is transferred to promote
feature-level fusion-based segmentation. Within the framework, we design a
novel multi-stage Transformer decoder to aggregate fine-grained multi-scale
fused features efficiently. Additionally, a dynamic factor based on the max-min
fairness allocation principle is introduced to generate adaptive weights of two
tasks and guarantee smooth training in a multi-task manner. Extensive
experiments demonstrate that our approach achieves competitive results compared
with state-of-the-art methods. The code is available at
https://github.com/Abraham-Einstein/MAFS/.

</details>


### [110] [A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset](https://arxiv.org/abs/2509.12047)
*Haiyu Yang,Enhong Liu,Jennifer Sun,Sumit Sharma,Meike van Leerdam,Sebastien Franceschini,Puchun Niu,Miel Hostens*

Main category: cs.CV

TL;DR: 论文提出了一种基于计算机视觉的模块化管道，用于自动化分析群体饲养环境中的动物行为，显著提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统手动观察方法耗时、主观且难以扩展，需要自动化解决方案。

Method: 结合了零样本目标检测、运动感知跟踪与分割，以及基于视觉Transformer的高级特征提取，构建了一个模块化管道。

Result: 在爱丁堡猪行为视频数据集上验证，时间模型总体准确率达94.2%，身份保持分数为93.3%，目标检测精度为89.3%。

Conclusion: 该论文提出的模块化管道为动物行为分析提供了可扩展的解决方案，尤其适用于精准养猪和福利评估。

Abstract: Animal behavior analysis plays a crucial role in understanding animal
welfare, health status, and productivity in agricultural settings. However,
traditional manual observation methods are time-consuming, subjective, and
limited in scalability. We present a modular pipeline that leverages
open-sourced state-of-the-art computer vision techniques to automate animal
behavior analysis in a group housing environment. Our approach combines
state-of-the-art models for zero-shot object detection, motion-aware tracking
and segmentation, and advanced feature extraction using vision transformers for
robust behavior recognition. The pipeline addresses challenges including animal
occlusions and group housing scenarios as demonstrated in indoor pig
monitoring. We validated our system on the Edinburgh Pig Behavior Video Dataset
for multiple behavioral tasks. Our temporal model achieved 94.2% overall
accuracy, representing a 21.2 percentage point improvement over existing
methods. The pipeline demonstrated robust tracking capabilities with 93.3%
identity preservation score and 89.3% object detection precision. The modular
design suggests potential for adaptation to other contexts, though further
validation across species would be required. The open-source implementation
provides a scalable solution for behavior monitoring, contributing to precision
pig farming and welfare assessment through automated, objective, and continuous
analysis.

</details>


### [111] [U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT](https://arxiv.org/abs/2509.12069)
*Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li*

Main category: cs.CV

TL;DR: U-Mamba2是一种高效CBCT分割网络，结合Mamba2和U-Net，在ToothFairy3挑战中表现优异。


<details>
  <summary>Details</summary>
Motivation: CBCT在牙科中的应用需要精确分割解剖结构，但现有方法耗时且具挑战性。

Method: U-Mamba2结合了Mamba2状态空间模型和U-Net架构，通过自监督学习预训练，并融入了牙科领域知识。

Result: U-Mamba2在ToothFairy3挑战的两个任务中均取得前三名，Task 1的Dice为0.792，HD95为93.19；Task 2的Dice为0.852，HD95为7.39。

Conclusion: U-Mamba2在ToothFairy3挑战中表现出色，证明了其在CBCT多解剖结构分割中的高效性和有效性。

Abstract: Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in
dentistry, providing volumetric information about the anatomical structures of
jaws and teeth. Accurate segmentation of these anatomies is critical for
clinical applications such as diagnosis and surgical planning, but remains
time-consuming and challenging. In this paper, we present U-Mamba2, a new
neural network architecture designed for multi-anatomy CBCT segmentation in the
context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state
space models into the U-Net architecture, enforcing stronger structural
constraints for higher efficiency without compromising performance. In
addition, we integrate interactive click prompts with cross-attention blocks,
pre-train U-Mamba2 using self-supervised learning, and incorporate dental
domain knowledge into the model design to address key challenges of dental
anatomy segmentation in CBCT. Extensive experiments, including independent
tests, demonstrate that U-Mamba2 is both effective and efficient, securing top
3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2
achieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with
an average inference time of XX (TBC during the ODIN workshop). In Task 2,
U-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out
test data. The code is publicly available at
https://github.com/zhiqin1998/UMamba2.

</details>


### [112] [Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation](https://arxiv.org/abs/2509.11840)
*Tim Lebailly,Vijay Veerabadran,Satwik Kottur,Karl Ridgeway,Michael Louis Iuzzolino*

Main category: cs.CV

TL;DR: 本研究通过密集对齐图像与VLM生成的合成描述，提升了零样本开放词汇分割性能，同时提高了数据效率。


<details>
  <summary>Details</summary>
Motivation: 生成式视觉语言模型（VLM）在高层图像理解上表现优异，但缺乏视觉与语言模态之间的空间密集对齐。本研究旨在通过合成描述填补这一空白。

Method: 利用生成式视觉语言模型（VLM）生成的合成描述，密集对齐图像与文本，以提升视觉语言对齐的表示学习。

Result: 在标准零样本开放词汇分割基准测试中，该方法优于先前的工作，并显示出更高的数据效率。

Conclusion: 通过将图像与VLM生成的合成描述密集对齐，本研究在零样本开放词汇分割任务上超越了先前的工作，并展示了更高的数据效率。

Abstract: Generative vision-language models (VLMs) exhibit strong high-level image
understanding but lack spatially dense alignment between vision and language
modalities, as our findings indicate. Orthogonal to advancements in generative
VLMs, another line of research has focused on representation learning for
vision-language alignment, targeting zero-shot inference for dense tasks like
segmentation. In this work, we bridge these two directions by densely aligning
images with synthetic descriptions generated by VLMs. Synthetic captions are
inexpensive, scalable, and easy to generate, making them an excellent source of
high-level semantic understanding for dense alignment methods. Empirically, our
approach outperforms prior work on standard zero-shot open-vocabulary
segmentation benchmarks/datasets, while also being more data-efficient.

</details>


### [113] [3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data](https://arxiv.org/abs/2509.12143)
*Nojod M. Alotaibi,Areej M. Alhothali,Manar S. Ali*

Main category: cs.CV

TL;DR: 本文提出了一种结合ViT和GNN的统一流程用于MDD检测，实验表明基于图谱的方法优于基于立方体的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的MDD自动检测方法多依赖于体素级特征或预定义脑图谱的手工区域表示，限制了捕捉复杂脑模式的能力。

Method: 开发了一个统一流程，结合Vision Transformers（ViTs）提取3D区域嵌入和Graph Neural Network（GNN）进行分类，探索了基于图谱和基于立方体的两种区域定义策略。

Result: 在REST-meta-MDD数据集上，最佳模型通过分层10折交叉验证获得了78.98%的准确率、76.54%的敏感性、81.58%的特异性、81.58%的精确度和78.98%的F1分数。

Conclusion: 使用基于图谱的ViT和GNN模型在MDD检测中表现优于基于立方体的方法，强调了领域特异性解剖先验的重要性。

Abstract: Major depressive disorder (MDD) is a prevalent mental health condition that
negatively impacts both individual well-being and global public health.
Automated detection of MDD using structural magnetic resonance imaging (sMRI)
and deep learning (DL) methods holds increasing promise for improving
diagnostic accuracy and enabling early intervention. Most existing methods
employ either voxel-level features or handcrafted regional representations
built from predefined brain atlases, limiting their ability to capture complex
brain patterns. This paper develops a unified pipeline that utilizes Vision
Transformers (ViTs) for extracting 3D region embeddings from sMRI data and
Graph Neural Network (GNN) for classification. We explore two strategies for
defining regions: (1) an atlas-based approach using predefined structural and
functional brain atlases, and (2) an cube-based method by which ViTs are
trained directly to identify regions from uniformly extracted 3D patches.
Further, cosine similarity graphs are generated to model interregional
relationships, and guide GNN-based classification. Extensive experiments were
conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of
our model. With stratified 10-fold cross-validation, the best model obtained
78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and
78.98% F1-score. Further, atlas-based models consistently outperformed the
cube-based approach, highlighting the importance of using domain-specific
anatomical priors for MDD detection.

</details>


### [114] [Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2509.11853)
*Yi-Hsin Li,Thomas Sikora,Sebastian Knorr,Måarten Sjöström*

Main category: cs.CV

TL;DR: SDI-GS通过分割驱动初始化减少3D高斯数量，提升稀疏视图合成的效率和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏视图合成中现有方法依赖SfM或MVS导致的高内存成本和效率问题。

Method: 利用区域分割技术识别并保留结构重要区域，选择性下采样密集点云。

Result: SDI-GS将3D高斯数量减少高达50%，在PSNR和SSIM指标上保持或超越现有方法，仅LPIPS略有下降。

Conclusion: SDI-GS通过基于区域的分割技术，有效减少了3D高斯数量，同时保持了场景的保真度，显著提升了3DGS在稀疏视图场景中的实用性。

Abstract: Sparse-view synthesis remains a challenging problem due to the difficulty of
recovering accurate geometry and appearance from limited observations. While
recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time
rendering with competitive quality, existing pipelines often rely on
Structure-from-Motion (SfM) for camera pose estimation, an approach that
struggles in genuinely sparse-view settings. Moreover, several SfM-free methods
replace SfM with multi-view stereo (MVS) models, but generate massive numbers
of 3D Gaussians by back-projecting every pixel into 3D space, leading to high
memory costs. We propose Segmentation-Driven Initialization for Gaussian
Splatting (SDI-GS), a method that mitigates inefficiency by leveraging
region-based segmentation to identify and retain only structurally significant
regions. This enables selective downsampling of the dense point cloud,
preserving scene fidelity while substantially reducing Gaussian count.
Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count
by up to 50% and achieves comparable or superior rendering quality in PSNR and
SSIM, with only marginal degradation in LPIPS. It further enables faster
training and lower memory footprint, advancing the practicality of 3DGS for
constrained-view scenarios.

</details>


### [115] [Multi Anatomy X-Ray Foundation Model](https://arxiv.org/abs/2509.12146)
*Nishank Singla,Krisztian Koos,Farzin Haddadpour,Amin Honarmandi Shandiz,Lovish Chum,Xiaojian Xu,Qing Jin,Erhan Bas*

Main category: cs.CV

TL;DR: XR-0是一种多解剖学X射线基础模型，通过自监督学习在大规模数据集上训练，在多种任务中表现出色，证明了解剖学多样性和监督的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的AI基础模型大多局限于胸部解剖学，无法泛化到更广泛的临床任务中，因此需要一种能够处理多解剖学任务的X射线基础模型。

Method: 采用自监督学习在包含1.15百万张图像的私人数据集上训练XR-0，涵盖多种解剖区域，并在12个数据集和20个下游任务上进行评估。

Result: XR-0在大多数多解剖学任务上实现了最先进的性能，并在胸部特定基准测试中保持竞争力。

Conclusion: XR-0展示了多解剖学X射线基础模型的潜力，证明了解剖学多样性和监督对于构建健壮、通用的医学视觉模型至关重要，为放射学中可扩展和适应性强的AI系统铺平了道路。

Abstract: X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation
models are limited to chest anatomy and fail to generalize across broader
clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray
foundation model using self-supervised learning on a large, private dataset of
1.15 million images spanning diverse anatomical regions and evaluated across 12
datasets and 20 downstream tasks, including classification, retrieval,
segmentation, localization, visual grounding, and report generation. XR-0
achieves state-of-the-art performance on most multi-anatomy tasks and remains
competitive on chest-specific benchmarks. Our results demonstrate that
anatomical diversity and supervision are critical for building robust,
general-purpose medical vision models, paving the way for scalable and
adaptable AI systems in radiology.

</details>


### [116] [Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding](https://arxiv.org/abs/2509.11866)
*Meng Luo,Shengqiong Wu,Liqiang Jing,Tianjie Ju,Li Zheng,Jinxiang Lai,Tianlong Wu,Xinya Du,Jian Li,Siyuan Yan,Jiebo Luo,William Yang Wang,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: Dr.V框架通过多层次时空定位诊断视频模型幻觉，实验证明有效提升解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型视频模型（LVMs）在视频理解方面取得进展，但仍存在幻觉问题，生成与输入视频冲突的内容，需要解决。

Method: 提出Dr.V框架，包含Dr.V-Bench数据集和Dr.V-Agent代理，通过感知、时间和认知层次的细粒度时空定位来诊断视频幻觉。

Result: 实验证明Dr.V-Agent能有效诊断幻觉，增强解释性和可靠性。

Conclusion: Dr.V框架通过多层次的细粒度时空定位，有效诊断视频模型中的幻觉问题，提升了视频理解的解释性和可靠性，为实际应用提供了实用方案。

Abstract: Recent advancements in large video models (LVMs) have significantly enhance
video understanding. However, these models continue to suffer from
hallucinations, producing content that conflicts with input videos. To address
this issue, we propose Dr.V, a hierarchical framework covering perceptive,
temporal, and cognitive levels to diagnose video hallucination by fine-grained
spatial-temporal grounding. Dr.V comprises of two key components: a benchmark
dataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes
10k instances drawn from 4,974 videos spanning diverse tasks, each enriched
with detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in
LVMs by systematically applying fine-grained spatial-temporal grounding at the
perceptive and temporal levels, followed by cognitive level reasoning. This
step-by-step pipeline mirrors human-like video comprehension and effectively
identifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is
effective in diagnosing hallucination while enhancing interpretability and
reliability, offering a practical blueprint for robust video understanding in
real-world scenarios. All our data and code are available at
https://github.com/Eurekaleo/Dr.V.

</details>


### [117] [Multi-animal tracking in Transition: Comparative Insights into Established and Emerging Methods](https://arxiv.org/abs/2509.11873)
*Anne Marthe Sophie Ngo Bibinbe,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

TL;DR: 研究比较了多动物跟踪（MAT）和多目标跟踪（MOT）方法在猪长期跟踪中的表现，发现MOT方法整体优于MAT工具，展示了其在提升牲畜跟踪准确性和可靠性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 精准畜牧业需要先进的监控工具以满足行业日益增长的管理需求，而计算机视觉系统在长期多动物跟踪（MAT）方面的能力对于连续行为监测至关重要。现有MAT工具在性能上通常不如最先进的MOT方法，可能导致下游任务（如行为分析和健康状态估计）不准确。

Method: 研究对多种MAT和MOT方法进行了基准测试，包括DeepLabCut、idTracker、ByteTrack、DeepSORT、跨输入一致性以及Track-Anything和PromptTrack等新方法。

Result: 所有方法在一个10分钟的猪跟踪数据集上进行了评估，结果显示MOT方法在长期跟踪场景中整体优于传统MAT工具。

Conclusion: 研究结果表明，多目标跟踪（MOT）方法在长期跟踪场景中优于传统的多动物跟踪（MAT）工具，展示了MOT技术在提升自动化牲畜跟踪准确性和可靠性方面的潜力。

Abstract: Precision livestock farming requires advanced monitoring tools to meet the
increasing management needs of the industry. Computer vision systems capable of
long-term multi-animal tracking (MAT) are essential for continuous behavioral
monitoring in livestock production. MAT, a specialized subset of multi-object
tracking (MOT), shares many challenges with MOT, but also faces domain-specific
issues including frequent animal occlusion, highly similar appearances among
animals, erratic motion patterns, and a wide range of behavior types.
  While some existing MAT tools are user-friendly and widely adopted, they
often underperform compared to state-of-the-art MOT methods, which can result
in inaccurate downstream tasks such as behavior analysis, health state
estimation, and related applications. In this study, we benchmarked both MAT
and MOT approaches for long-term tracking of pigs. We compared tools such as
DeepLabCut and idTracker with MOT-based methods including ByteTrack, DeepSORT,
cross-input consistency, and newer approaches like Track-Anything and
PromptTrack.
  All methods were evaluated on a 10-minute pig tracking dataset. Our results
demonstrate that, overall, MOT approaches outperform traditional MAT tools,
even for long-term tracking scenarios. These findings highlight the potential
of recent MOT techniques to enhance the accuracy and reliability of automated
livestock tracking.

</details>


### [118] [Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation](https://arxiv.org/abs/2509.11878)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,K J Joseph*

Main category: cs.CV

TL;DR: 本研究提出了一种加权提示操作（WPM）技术，用于在零样本设置下为诗歌生成和优化图像，通过动态调整扩散模型中的注意力权重和文本嵌入，实现了语义更丰富的视觉化效果。


<details>
  <summary>Details</summary>
Motivation: 诗歌作为一种表达性艺术形式，其多义性使得读者常基于个人情感、经验和文化背景理解诗歌。因此，本研究旨在为零样本设置下的诗歌生成可修改的图像，以满足不同读者的需求。

Method: 采用加权提示操作（WPM）技术，通过动态调整扩散模型中的注意力权重和文本嵌入，结合GPT等大型语言模型（LLMs）和现有诗歌数据集。

Result: WPM技术能够增强或抑制特定词在最终生成图像中的影响，从而产生语义更丰富、上下文更准确的视觉化效果。

Conclusion: 通过引入加权提示操作（WPM）技术，本研究成功实现了在零样本设置下为诗歌生成并优化图像，为文学领域的图像生成提供了新颖且结构化的方法。

Abstract: Poetry is an expressive form of art that invites multiple interpretations, as
readers often bring their own emotions, experiences, and cultural backgrounds
into their understanding of a poem. Recognizing this, we aim to generate images
for poems and improve these images in a zero-shot setting, enabling audiences
to modify images as per their requirements. To achieve this, we introduce a
novel Weighted Prompt Manipulation (WPM) technique, which systematically
modifies attention weights and text embeddings within diffusion models. By
dynamically adjusting the importance of specific words, WPM enhances or
suppresses their influence in the final generated image, leading to
semantically richer and more contextually accurate visualizations. Our approach
exploits diffusion models and large language models (LLMs) such as GPT in
conjunction with existing poetry datasets, ensuring a comprehensive and
structured methodology for improved image generation in the literary domain. To
the best of our knowledge, this is the first attempt at integrating weighted
prompt manipulation for enhancing imagery in poetic language.

</details>


### [119] [SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection](https://arxiv.org/abs/2509.11884)
*Zhenni Yu,Li Zhao,Guobao Xiao,Xiaoqin Zhang*

Main category: cs.CV

TL;DR: SAM-TTT通过逆向参数配置和测试时训练提升COD任务性能，达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有SAM-based COD模型对不利参数关注不足，影响了语义理解。

Method: 提出了逆向SAM参数配置模块和T-Visioner模块，分别用于抑制不利参数和增强有利参数。

Result: 在多个COD基准测试中取得了最先进的性能。

Conclusion: SAM-TTT通过逆向参数配置和测试时训练显著提升了在伪装物体检测任务中的性能，并实现了最先进的性能。

Abstract: This paper introduces a new Segment Anything Model (SAM) that leverages
reverse parameter configuration and test-time training to enhance its
performance on Camouflaged Object Detection (COD), named SAM-TTT. While most
existing SAM-based COD models primarily focus on enhancing SAM by extracting
favorable features and amplifying its advantageous parameters, a crucial gap is
identified: insufficient attention to adverse parameters that impair SAM's
semantic understanding in downstream tasks. To tackle this issue, the Reverse
SAM Parameter Configuration Module is proposed to effectively mitigate the
influence of adverse parameters in a train-free manner by configuring SAM's
parameters. Building on this foundation, the T-Visioner Module is unveiled to
strengthen advantageous parameters by integrating Test-Time Training layers,
originally developed for language tasks, into vision tasks. Test-Time Training
layers represent a new class of sequence modeling layers characterized by
linear complexity and an expressive hidden state. By integrating two modules,
SAM-TTT simultaneously suppresses adverse parameters while reinforcing
advantageous ones, significantly improving SAM's semantic understanding in COD
task. Our experimental results on various COD benchmarks demonstrate that the
proposed approach achieves state-of-the-art performance, setting a new
benchmark in the field. The code will be available at
https://github.com/guobaoxiao/SAM-TTT.

</details>


### [120] [BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation](https://arxiv.org/abs/2509.11885)
*Francis Xiatian Zhang,Emile Mackute,Mohammadreza Kasaei,Kevin Dhaliwal,Robert Thomson,Mohsen Khadem*

Main category: cs.CV

TL;DR: Brea-Depth通过整合气道几何先验和改进的CycleGAN，提升了支气管镜深度估计的准确性和3D重建的解剖真实性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度基础模型在支气管镜场景中缺乏解剖意识，过度拟合局部纹理而非全局气道结构，尤其是在深度线索模糊和光照条件差的情况下。

Method: 提出了一种新颖的框架Brea-Depth，结合了深度感知CycleGAN和气道结构感知损失，以桥接真实支气管镜图像与解剖数据之间的领域差距。

Result: 在收集的离体人肺数据集和开放的支气管镜数据集上，Brea-Depth在解剖深度保持方面优于现有方法。

Conclusion: Brea-Depth通过整合气道特异性几何先验，显著提升了支气管镜深度估计的准确性和鲁棒性，为复杂气道中的实时导航提供了更可靠的3D重建。

Abstract: Monocular depth estimation in bronchoscopy can significantly improve
real-time navigation accuracy and enhance the safety of interventions in
complex, branching airways. Recent advances in depth foundation models have
shown promise for endoscopic scenarios, yet these models often lack anatomical
awareness in bronchoscopy, overfitting to local textures rather than capturing
the global airway structure, particularly under ambiguous depth cues and poor
lighting. To address this, we propose Brea-Depth, a novel framework that
integrates airway-specific geometric priors into foundation model adaptation
for bronchoscopic depth estimation. Our method introduces a depth-aware
CycleGAN, refining the translation between real bronchoscopic images and airway
geometries from anatomical data, effectively bridging the domain gap. In
addition, we introduce an airway structure awareness loss to enforce depth
consistency within the airway lumen while preserving smooth transitions and
structural integrity. By incorporating anatomical priors, Brea-Depth enhances
model generalization and yields more robust, accurate 3D airway
reconstructions. To assess anatomical realism, we introduce Airway Depth
Structure Evaluation, a new metric for structural consistency. We validate
BREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopic
dataset, where it outperforms existing methods in anatomical depth
preservation.

</details>


### [121] [Logit Mixture Outlier Exposure for Fine-grained Out-of-Distribution Detection](https://arxiv.org/abs/2509.11892)
*Akito Shinohara,Kohei Fukuda,Hiroaki Aizawa*

Main category: cs.CV

TL;DR: 论文提出了一种logit空间线性插值技术，通过混合分布内外数据平滑类间logits，显著提升了分布外检测性能，特别是在处理接近分布内数据的异常数据时效果更佳。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的分布外检测方法（如Outlier Exposure和Mixture Outlier Exposure）在训练过程中暴露异常数据以提高性能，但模型仍难以有效学习类间关系并清晰区分分布内和分布外数据。因此，作者关注logit空间，因其类间分布特性在输入或特征空间中更为明显。

Method: 论文提出了一种在logit空间进行线性插值的技术，并强制保持logit空间混合与输入空间混合生成的logits之间的一致性。

Result: 实验表明，logit空间混合技术减少了模型输出在决策边界附近的突变，实现了更平滑可靠的分布内外数据分离。该方法在细粒度分布外检测任务中表现优异。

Conclusion: 该论文提出了一种在logit空间进行线性插值的技术，通过混合分布内和分布外数据来平滑类间的logits，从而提高了分布外检测的性能，特别是在处理接近分布内数据的分布外数据时表现更优。

Abstract: The ability to detect out-of-distribution data is essential not only for
ensuring robustness against unknown or unexpected input data but also for
improving the generalization performance of the model. Among various
out-of-distribution detection methods, Outlier Exposure and Mixture Outlier
Exposure are promising approaches that enhance out-of-distribution detection
performance by exposing the outlier data during training. However, even with
these sophisticated techniques, it remains challenging for models to learn the
relationships between classes effectively and to distinguish data sampling from
in-distribution and out-of-distribution clearly. Therefore, we focus on the
logit space, where the properties between class-wise distributions are
distinctly separated from those in the input or feature spaces. Specifically,
we propose a linear interpolation technique in the logit space that mixes
in-distribution and out-of-distribution data to facilitate smoothing logits
between classes and improve the out-of-distribution detection performance,
particularly for out-of-distribution data that lie close to the in-distribution
data. Additionally, we enforce consistency between the logits obtained through
mixing in the logit space and those generated via mixing in the input space.
Our experiments demonstrate that our logit-space mixing technique reduces the
abrupt fluctuations in the model outputs near the decision boundaries,
resulting in smoother and more reliable separation between in-distribution and
out-of-distribution data. Furthermore, we evaluate the effectiveness of the
proposed method on a fine-grained out-of-distribution detection task.

</details>


### [122] [NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired Geometric Priors for Robust Facial Emotion Recognition](https://arxiv.org/abs/2509.11916)
*Zilin Li,Weiwei Xu,Xuanqi Zhao,Yiran Zhu*

Main category: cs.CV

TL;DR: NeuroGaze-Distill通过脑电信息蒸馏提升FER模型鲁棒性，无需复杂架构或部署时非视觉信号。


<details>
  <summary>Details</summary>
Motivation: 传统基于像素的FER模型因面部外观是情感的间接且有偏代理，难以跨数据集泛化，需引入脑电信息提升模型鲁棒性。

Method: 采用跨模态蒸馏框架，将脑电信息通过静态V/A原型和D-Geo先验传递到仅依赖图像的FER学生模型中，使用ResNet-18/50作为学生模型，结合Proto-KD和D-Geo正则化进行训练。

Result: 在FERPlus验证和跨数据集（AffectNet-mini、CK+）测试中，方法表现优于基线，且5x5原型网格在稳定性上优于更密集网格。

Conclusion: NeuroGaze-Distill框架通过静态V/A原型和D-Geo先验，提升了FER模型的鲁棒性，且无需复杂架构或部署时的非视觉信号。

Abstract: Facial emotion recognition (FER) models trained only on pixels often fail to
generalize across datasets because facial appearance is an indirect and biased
proxy for underlying affect. We present NeuroGaze-Distill, a cross-modal
distillation framework that transfers brain-informed priors into an image-only
FER student via static Valence/Arousal (V/A) prototypes and a
depression-inspired geometric prior (D-Geo). A teacher trained on EEG
topographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a
consolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face
pairing and no non-visual signals at deployment are required. The student
(ResNet-18/50) is trained on FERPlus with conventional CE/KD and two
lightweight regularizers: (i) Proto-KD (cosine) aligns student features to the
static prototypes; (ii) D-Geo softly shapes the embedding geometry in line with
affective findings often reported in depression research (e.g., anhedonia-like
contraction in high-valence regions). We evaluate both within-domain (FERPlus
validation) and cross-dataset protocols (AffectNet-mini; optional CK+),
reporting standard 8-way scores alongside present-only Macro-F1 and balanced
accuracy to fairly handle label-set mismatch. Ablations attribute consistent
gains to prototypes and D-Geo, and favor 5x5 over denser grids for stability.
The method is simple, deployable, and improves robustness without architectural
complexity.

</details>


### [123] [Enriched text-guided variational multimodal knowledge distillation network (VMD) for automated diagnosis of plaque vulnerability in 3D carotid artery MRI](https://arxiv.org/abs/2509.11924)
*Bo Cao,Fan Yu,Mengmeng Feng,SenHao Zhang,Xin Meng,Yue Zhang,Zhen Qian,Jie Lu*

Main category: cs.CV

TL;DR: 本研究开发了一种结合变分推理和多模态知识蒸馏（VMD）的策略，用于自动化诊断颈动脉斑块易损性，实验证明其能有效提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 由于直接从颈动脉3D MRI图像诊断斑块易损性对放射科医生和传统3D视觉网络均具有挑战性，本研究旨在利用多模态学习和领域知识，开发一种更准确的自动化诊断方法。

Method: 本研究提出了一种名为变分推理和多模态知识蒸馏（VMD）的策略，通过结合放射科医生的领域知识和多模态数据（如各种成像模式和放射学报告），来训练诊断网络。

Result: 在内部收集的数据集上进行的深入实验验证了VMD策略的有效性，表明其能够有效利用有限的图像标注和放射学报告中的跨模态先验知识。

Conclusion: 通过变分推理和多模态知识蒸馏（VMD）策略，本研究成功开发了一种自动化诊断颈动脉斑块易损性的方法，显著提升了未标注3D MRI图像的诊断准确性。

Abstract: Multimodal learning has attracted much attention in recent years due to its
ability to effectively utilize data features from a variety of different
modalities. Diagnosing the vulnerability of atherosclerotic plaques directly
from carotid 3D MRI images is relatively challenging for both radiologists and
conventional 3D vision networks. In clinical practice, radiologists assess
patient conditions using a multimodal approach that incorporates various
imaging modalities and domain-specific expertise, paving the way for the
creation of multimodal diagnostic networks. In this paper, we have developed an
effective strategy to leverage radiologists' domain knowledge to automate the
diagnosis of carotid plaque vulnerability through Variation inference and
Multimodal knowledge Distillation (VMD). This method excels in harnessing
cross-modality prior knowledge from limited image annotations and radiology
reports within training data, thereby enhancing the diagnostic network's
accuracy for unannotated 3D MRI images. We conducted in-depth experiments on
the dataset collected in-house and verified the effectiveness of the VMD
strategy we proposed.

</details>


### [124] [Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image Interpolation with Guaranteed Initialization](https://arxiv.org/abs/2509.11926)
*Xue Zhang,Bingshuo Hu,Gene Cheung*

Main category: cs.CV

TL;DR: 该论文提出了一种基于图滤波器和DR迭代的新型神经网络初始化方法，显著提升了图像插值性能并减少了参数数量。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络（DNNs）通过随机初始化参数并使用随机梯度下降（SGD）优化，容易陷入性能较差的局部最小值。针对图像插值问题，该研究旨在通过图滤波器和正则化方法提升性能并减少参数。

Method: 通过初始化基于已知插值器的有向图邻接矩阵A，并学习扰动矩阵P和P(2)来增强A，再通过Douglas-Rachford迭代实现恢复效果，最终将其展开为一个轻量级可解释的神经网络。

Result: 实验结果表明，该方法在图像插值任务中取得了最先进的性能，同时大幅减少了网络参数。

Conclusion: 该论文提出了一种基于图滤波器和Douglas-Rachford迭代的新型神经网络初始化方法，显著提升了图像插值性能，同时大幅减少了网络参数。

Abstract: Conventional deep neural nets (DNNs) initialize network parameters at random
and then optimize each one via stochastic gradient descent (SGD), resulting in
substantial risk of poor-performing local minima.Focusing on the image
interpolation problem and leveraging a recent theorem that maps a
(pseudo-)linear interpolator {\Theta} to a directed graph filter that is a
solution to a MAP problem regularized with a graph shift variation (GSV) prior,
we first initialize a directed graph adjacency matrix A based on a known
interpolator {\Theta}, establishing a baseline performance.Then, towards
further gain, we learn perturbation matrices P and P(2) from data to augment A,
whose restoration effects are implemented via Douglas-Rachford (DR) iterations,
which we unroll into a lightweight interpretable neural net.Experimental
results demonstrate state-of-the-art image interpolation results, while
drastically reducing network parameters.

</details>


### [125] [Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360° Videos](https://arxiv.org/abs/2509.11948)
*Mahmoud Z. A. Wahba,Sara Baldoni,Federica Battisti*

Main category: cs.CV

TL;DR: Sphere-GAN 是一种基于球形卷积生成对抗网络的360度视频显著性检测模型，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着沉浸式应用的成功，需要新的方法来处理和优化360度图像和视频的传输，显著性估计为此提供了有力工具。

Method: 利用带有球形卷积的生成对抗网络（Sphere-GAN）进行显著性检测。

Result: Sphere-GAN 在公开的360度视频显著性数据集上的实验结果表明其性能优于现有模型。

Conclusion: Sphere-GAN 在预测360度视频的显著性地图方面优于现有最先进模型。

Abstract: The recent success of immersive applications is pushing the research
community to define new approaches to process 360{\deg} images and videos and
optimize their transmission. Among these, saliency estimation provides a
powerful tool that can be used to identify visually relevant areas and,
consequently, adapt processing algorithms. Although saliency estimation has
been widely investigated for 2D content, very few algorithms have been proposed
for 360{\deg} saliency estimation. Towards this goal, we introduce Sphere-GAN,
a saliency detection model for 360{\deg} videos that leverages a Generative
Adversarial Network with spherical convolutions. Extensive experiments were
conducted using a public 360{\deg} video saliency dataset, and the results
demonstrate that Sphere-GAN outperforms state-of-the-art models in accurately
predicting saliency maps.

</details>


### [126] [CLAIRE: A Dual Encoder Network with RIFT Loss and Phi-3 Small Language Model Based Interpretability for Cross-Modality Synthetic Aperture Radar and Optical Land Cover Segmentation](https://arxiv.org/abs/2509.11952)
*Debopom Sutradhar,Arefin Ittesafun Abian,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sheikh Izzal Azid,Sami Azam*

Main category: cs.CV

TL;DR: 论文提出双编码器架构CLAIRE和混合损失函数RIFT，显著提升卫星图像土地覆盖分类性能，尤其在类别不平衡和复杂场景下，并增强了模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 卫星图像土地覆盖分类在环境监测和资源管理中至关重要，但面临自然景观复杂性、类别视觉相似性和数据集类别不平衡等挑战。

Method: 提出了一种双编码器架构，分别从光学和SAR图像中提取特征，并通过跨模态注意力融合模块CLAIRE进行特征融合。采用混合损失函数RIFT（加权焦点损失和Tversky损失）解决类别不平衡问题。

Result: 在多个数据集上取得了竞争性性能：WHU-OPT-SAR数据集的mIoU为56.02%，OA为84.56%；OpenEarthMap-SAR数据集的mIoU为59.89%，OA为73.92%；PIE-RGB-SAR数据集在云遮挡条件下的mIoU为86.86%，OA为94.58%。

Conclusion: 该论文提出的双编码器架构CLAIRE在卫星图像土地覆盖分类中表现出色，通过跨模态注意力融合模块和混合损失函数RIFT，显著提升了分类性能，尤其在类别不平衡和复杂场景下。此外，引入的小型语言模型生成的解释模块增强了模型的可解释性。

Abstract: Accurate land cover classification from satellite imagery is crucial in
environmental monitoring and sustainable resource management. However, it
remains challenging due to the complexity of natural landscapes, the visual
similarity between classes, and the significant class imbalance in the
available datasets. To address these issues, we propose a dual encoder
architecture that independently extracts modality-specific features from
optical and Synthetic Aperture Radar (SAR) imagery, which are then fused using
a cross-modality attention-fusion module named Cross-modality Land cover
segmentation with Attention and Imbalance-aware Reasoning-Enhanced Explanations
(CLAIRE). This fusion mechanism highlights complementary spatial and textural
features, enabling the network to better capture detailed and diverse land
cover patterns. We incorporate a hybrid loss function that utilizes Weighted
Focal Loss and Tversky Loss named RIFT (Rare-Instance Focal-Tversky) to address
class imbalance and improve segmentation performance across underrepresented
categories. Our model achieves competitive performance across multiple
benchmarks: a mean Intersection over Union (mIoU) of 56.02% and Overall
Accuracy (OA) of 84.56% on the WHU-OPT-SAR dataset; strong generalization with
a mIoU of 59.89% and OA of 73.92% on the OpenEarthMap-SAR dataset; and
remarkable robustness under cloud-obstructed conditions, achieving an mIoU of
86.86% and OA of 94.58% on the PIE-RGB-SAR dataset. Additionally, we introduce
a metric-driven reasoning module generated by a Small Language Model (Phi-3),
which generates expert-level, sample-specific justifications for model
predictions, thereby enhancing transparency and interpretability.

</details>


### [127] [Lost in Embeddings: Information Loss in Vision-Language Models](https://arxiv.org/abs/2509.11986)
*Wenyan Li,Raphael Tang,Chengzu Li,Caiqi Zhang,Ivan Vulić,Anders Søgaard*

Main category: cs.CV

TL;DR: 研究发现视觉语言模型中的连接器导致显著信息丢失，影响模型性能，并通过k近邻分析和嵌入重建量化了这一现象。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）中，视觉输入通过预训练的视觉编码器处理后，需通过连接器组件投影到语言模型的嵌入空间。这一步骤可能导致信息丢失，但其具体影响尚未充分研究。

Method: 研究采用两种方法：一是通过分析投影前后图像表示的k近邻关系变化来评估语义信息保存；二是通过从投影表示中重建视觉嵌入来直接测量信息丢失。

Result: 实验表明，连接器显著扭曲了视觉表示的局部几何结构，k近邻关系在投影后差异达40-60%，且与检索性能下降相关。嵌入重建揭示了模型在视觉问答任务中表现不佳的实例与高信息丢失区域相关。

Conclusion: 论文得出结论，连接器在视觉表示中引入了显著的几何失真，导致信息丢失，进而影响模型在检索和视觉问答任务中的表现。

Abstract: Vision--language models (VLMs) often process visual inputs through a
pretrained vision encoder, followed by a projection into the language model's
embedding space via a connector component. While crucial for modality fusion,
the potential information loss induced by this projection step and its direct
impact on model capabilities remain understudied. We introduce two
complementary approaches to examine and quantify this loss by analyzing the
latent representation space. First, we evaluate semantic information
preservation by analyzing changes in k-nearest neighbor relationships between
image representations, before and after projection. Second, we directly measure
information loss by reconstructing visual embeddings from the projected
representation, localizing loss at an image patch level. Experiments reveal
that connectors substantially distort the local geometry of visual
representations, with k-nearest neighbors diverging by 40--60\%
post-projection, correlating with degradation in retrieval performance. The
patch-level embedding reconstruction provides interpretable insights for model
behavior on visually grounded question-answering tasks, finding that areas of
high information loss reliably predict instances where models struggle.

</details>


### [128] [Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness](https://arxiv.org/abs/2509.12024)
*Zixuan Fu,Yan Ren,Finn Carter,Chenyue Wen,Le Ku,Daheng Yu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: SCORE是一种新型扩散模型概念擦除框架，通过对抗独立性保证概念擦除，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面取得了前所未有的成功，但也带来了隐私、公平性和安全性方面的风险，因此需要一种方法来擦除敏感或有害概念。

Method: SCORE将概念擦除建模为一个对抗独立性问題，通过最小化目标概念与生成输出之间的互信息，提供可证明的擦除保证。

Result: SCORE在Stable Diffusion和FLUX上的四个具有挑战性的基准测试中表现优于现有方法，擦除效率提高了12.5%，同时保持或提升了图像质量。

Conclusion: SCORE框架通过对抗优化、轨迹一致性和显著性驱动的微调，为扩散模型中的安全和鲁棒概念擦除设定了新标准。

Abstract: Diffusion models have achieved unprecedented success in image generation but
pose increasing risks in terms of privacy, fairness, and security. A growing
demand exists to \emph{erase} sensitive or harmful concepts (e.g., NSFW
content, private individuals, artistic styles) from these models while
preserving their overall generative capabilities. We introduce \textbf{SCORE}
(Secure and Concept-Oriented Robust Erasure), a novel framework for robust
concept removal in diffusion models. SCORE formulates concept erasure as an
\emph{adversarial independence} problem, theoretically guaranteeing that the
model's outputs become statistically independent of the erased concept. Unlike
prior heuristic methods, SCORE minimizes the mutual information between a
target concept and generated outputs, yielding provable erasure guarantees. We
provide formal proofs establishing convergence properties and derive upper
bounds on residual concept leakage. Empirically, we evaluate SCORE on Stable
Diffusion and FLUX across four challenging benchmarks: object erasure, NSFW
removal, celebrity face suppression, and artistic style unlearning. SCORE
consistently outperforms state-of-the-art methods including EraseAnything, ANT,
MACE, ESD, and UCE, achieving up to \textbf{12.5\%} higher erasure efficacy
while maintaining comparable or superior image quality. By integrating
adversarial optimization, trajectory consistency, and saliency-driven
fine-tuning, SCORE sets a new standard for secure and robust concept erasure in
diffusion models.

</details>


### [129] [RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration](https://arxiv.org/abs/2509.12039)
*Zilong Zhang,Chujie Qin,Chunle Guo,Yong Zhang,Chao Xue,Ming-Ming Cheng,Chongyi Li*

Main category: cs.CV

TL;DR: RAM++是一个两阶段图像恢复框架，通过自适应掩码和特征正则化设计，在极端和混合退化场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决现有退化导向方法在极端场景（如退化与图像结构强耦合）中的局限性，并缓解任务间性能不平衡、对已知退化过拟合及对未知退化泛化能力弱等常见挑战。

Method: RAM++采用自适应语义感知掩码（AdaSAM）、掩码属性传导（MAC）和鲁棒特征正则化（RFR）三大设计，结合高层次的语义理解和低层次的纹理生成。

Result: RAM++在已知、未知、极端和混合退化场景下均实现了鲁棒、平衡且最先进的性能。

Conclusion: RAM++通过其两阶段框架（AdaSAM、MAC和RFR设计）实现了全面且鲁棒的图像恢复，在极端和混合退化场景下均表现出色。

Abstract: This work presents Robust Representation Learning via Adaptive Mask (RAM++),
a two-stage framework for all-in-one image restoration. RAM++ integrates
high-level semantic understanding with low-level texture generation to achieve
content-oriented robust restoration. It addresses the limitations of existing
degradation-oriented methods in extreme scenarios (e.g., degradations strongly
coupled with image structures). RAM++ also mitigates common challenges such as
unbalanced performance across tasks, overfitting to seen degradations, and weak
generalization to unseen ones through three key designs: 1) Adaptive
Semantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level
masks to semantically rich and textured regions. This design enables the
network to learn both generative priors and image content priors from various
degradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning
strategy that adjusts the layers with higher contributions to bridge the
integrity gap between masked pretraining and full-image fine-tuning while
retaining learned priors. 3) Robust Feature Regularization (RFR): a strategy
that leverages DINOv2's semantically consistent and degradation-invariant
representations, together with efficient feature fusion, to achieve faithful
and semantically coherent restoration. With these designs, RAM++ achieves
robust, well-balanced, and state-of-the-art performance across seen, unseen,
extreme, and mixed degradations. Our code and model will be released at
https://github.com/DragonisCV/RAM

</details>


### [130] [AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective](https://arxiv.org/abs/2509.12052)
*Yuchen Deng,Xiuyang Wu,Hai-Tao Zheng,Suiyang Zhang,Yi He,Yuxing Han*

Main category: cs.CV

TL;DR: AvatarSync是一种基于音素表示的自回归框架，通过两阶段生成策略解决现有说话头动画方法的帧间闪烁和速度问题，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN或扩散模型的说话头动画方法存在帧间闪烁、身份漂移和推理速度慢的问题。AvatarSync旨在解决这些问题。

Method: 采用两阶段生成策略：1. 面部关键帧生成（FKG），基于音素级语义表示；2. 帧间插值，强调时间连贯性和视觉平滑性。使用时间戳感知的自适应策略和选择性状态空间模型。

Result: 实验表明，AvatarSync在视觉保真度、时间一致性和计算效率上优于现有方法。

Conclusion: AvatarSync通过两阶段生成策略和优化推理流程，在视觉保真度、时间一致性和计算效率上优于现有方法，提供了一个可扩展且可控的解决方案。

Abstract: Existing talking-head animation approaches based on Generative Adversarial
Networks (GANs) or diffusion models often suffer from inter-frame flicker,
identity drift, and slow inference. These limitations inherent to their video
generation pipelines restrict their suitability for applications. To address
this, we introduce AvatarSync, an autoregressive framework on phoneme
representations that generates realistic and controllable talking-head
animations from a single reference image, driven directly text or audio input.
In addition, AvatarSync adopts a two-stage generation strategy, decoupling
semantic modeling from visual dynamics, which is a deliberate "Divide and
Conquer" design. The first stage, Facial Keyframe Generation (FKG), focuses on
phoneme-level semantic representation by leveraging the many-to-one mapping
from text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to
anchor abstract phonemes to character-level units. Combined with a customized
Text-Frame Causal Attention Mask, the keyframes are generated. The second
stage, inter-frame interpolation, emphasizes temporal coherence and visual
smoothness. We introduce a timestamp-aware adaptive strategy based on a
selective state space model, enabling efficient bidirectional context
reasoning. To support deployment, we optimize the inference pipeline to reduce
latency without compromising visual fidelity. Extensive experiments show that
AvatarSync outperforms existing talking-head animation methods in visual
fidelity, temporal consistency, and computational efficiency, providing a
scalable and controllable solution.

</details>


### [131] [Robust Fetal Pose Estimation across Gestational Ages via Cross-Population Augmentation](https://arxiv.org/abs/2509.12062)
*Sebastian Diaz,Benjamin Billot,Neel Dey,Molin Zhang,Esra Abaci Turk,P. Ellen Grant,Polina Golland,Elfar Adalsteinsson*

Main category: cs.CV

TL;DR: 本研究提出了一种跨群体数据增强框架，显著提高了胎儿姿态估计模型在不同孕龄的泛化能力，特别是在早期孕龄的挑战性病例中表现突出。


<details>
  <summary>Details</summary>
Motivation: 胎儿运动是神经系统发育和宫内健康的关键指标，但现有方法难以泛化到早期孕龄，主要由于母体和胎儿在妊娠期间的显著解剖变化以及早期孕龄标注数据的稀缺。

Method: 本研究开发了一个跨群体数据增强框架，利用仅来自较大孕龄队列的标注图像，通过胎儿特定的增强策略模拟早期孕龄的独特宫内环境和胎儿位置。

Result: 实验结果表明，跨群体数据增强在较大孕龄和早期孕龄的挑战性病例中均表现出减少变异性和显著改善的效果。

Conclusion: 通过跨群体数据增强框架，本研究成功提高了胎儿姿态估计模型在不同孕龄（GA）的临床队列中的泛化能力，特别是在早期孕龄的挑战性病例中表现显著改善。

Abstract: Fetal motion is a critical indicator of neurological development and
intrauterine health, yet its quantification remains challenging, particularly
at earlier gestational ages (GA). Current methods track fetal motion by
predicting the location of annotated landmarks on 3D echo planar imaging (EPI)
time-series, primarily in third-trimester fetuses. The predicted landmarks
enable simplification of the fetal body for downstream analysis. While these
methods perform well within their training age distribution, they consistently
fail to generalize to early GAs due to significant anatomical changes in both
mother and fetus across gestation, as well as the difficulty of obtaining
annotated early GA EPI data. In this work, we develop a cross-population data
augmentation framework that enables pose estimation models to robustly
generalize to younger GA clinical cohorts using only annotated images from
older GA cohorts. Specifically, we introduce a fetal-specific augmentation
strategy that simulates the distinct intrauterine environment and fetal
positioning of early GAs. Our experiments find that cross-population
augmentation yields reduced variability and significant improvements across
both older GA and challenging early GA cases. By enabling more reliable pose
estimation across gestation, our work potentially facilitates early clinical
detection and intervention in challenging 4D fetal imaging settings. Code is
available at https://github.com/sebodiaz/cross-population-pose.

</details>


### [132] [End-to-End Learning of Multi-Organ Implicit Surfaces from 3D Medical Imaging Data](https://arxiv.org/abs/2509.12068)
*Farahdiba Zarin,Nicolas Padoy,Jérémy Dana,Vinkle Srivastav*

Main category: cs.CV

TL;DR: ImplMORe是一种基于隐式表面表示的深度学习方法，用于3D医学图像中的多器官重建，提供高分辨率表面细节。


<details>
  <summary>Details</summary>
Motivation: 现有方法在医学图像中因分辨率限制难以实现精细表面重建，而通用计算机视觉中的隐式表示方法无法直接应用于医学图像。

Method: ImplMORe采用端到端深度学习方法，结合3D CNN编码器和多尺度插值技术，利用占用函数在连续域中学习特征。

Result: ImplMORe在totalsegmentator数据集上实现了单器官和多器官的高分辨率重建，提供了比输入图像更高分辨率的器官表面细节。

Conclusion: ImplMORe通过隐式表面表示在3D医学图像中实现了高分辨率的多器官重建，优于传统的离散显式表示方法。

Abstract: The fine-grained surface reconstruction of different organs from 3D medical
imaging can provide advanced diagnostic support and improved surgical planning.
However, the representation of the organs is often limited by the resolution,
with a detailed higher resolution requiring more memory and computing
footprint. Implicit representations of objects have been proposed to alleviate
this problem in general computer vision by providing compact and differentiable
functions to represent the 3D object shapes. However, architectural and
data-related differences prevent the direct application of these methods to
medical images. This work introduces ImplMORe, an end-to-end deep learning
method using implicit surface representations for multi-organ reconstruction
from 3D medical images. ImplMORe incorporates local features using a 3D CNN
encoder and performs multi-scale interpolation to learn the features in the
continuous domain using occupancy functions. We apply our method for single and
multiple organ reconstructions using the totalsegmentator dataset. By
leveraging the continuous nature of occupancy functions, our approach
outperforms the discrete explicit representation based surface reconstruction
approaches, providing fine-grained surface details of the organ at a resolution
higher than the given input image. The source code will be made publicly
available at: https://github.com/CAMMA-public/ImplMORe

</details>


### [133] [Progressive Flow-inspired Unfolding for Spectral Compressive Imaging](https://arxiv.org/abs/2509.12079)
*Xiaodong Wang,Ping Wang,Zijun He,Mengjie Qin,Xin Yuan*

Main category: cs.CV

TL;DR: 提出了一种轨迹可控的展开框架，用于CASSI重建，通过高效Transformer和频域融合模块，实现了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有展开方法在重建轨迹上存在不可控问题，导致质量跳跃和非渐进式优化。

Method: 设计了一种高效的时空Transformer和频域融合模块，用于保证特征一致性。

Result: 实验证明，该方法在仿真和真实数据上均实现了更好的重建质量和效率。

Conclusion: 本文提出的轨迹可控展开框架在编码孔径快照光谱成像（CASSI）重建任务中表现出色，优于现有最先进方法。

Abstract: Coded aperture snapshot spectral imaging (CASSI) retrieves a 3D hyperspectral
image (HSI) from a single 2D compressed measurement, which is a highly
challenging reconstruction task. Recent deep unfolding networks (DUNs),
empowered by explicit data-fidelity updates and implicit deep denoisers, have
achieved the state of the art in CASSI reconstruction. However, existing
unfolding approaches suffer from uncontrollable reconstruction trajectories,
leading to abrupt quality jumps and non-gradual refinement across stages.
Inspired by diffusion trajectories and flow matching, we propose a novel
trajectory-controllable unfolding framework that enforces smooth, continuous
optimization paths from noisy initial estimates to high-quality
reconstructions. To achieve computational efficiency, we design an efficient
spatial-spectral Transformer tailored for hyperspectral reconstruction, along
with a frequency-domain fusion module to gurantee feature consistency.
Experiments on simulation and real data demonstrate that our method achieves
better reconstruction quality and efficiency than prior state-of-the-art
approaches.

</details>


### [134] [End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI](https://arxiv.org/abs/2509.12090)
*Yihong Chen,Jiancheng Yang,Deniz Sayin Mercadier,Hieu Le,Juerg Schwitter,Pascal Fua*

Main category: cs.CV

TL;DR: TetHeart是首个端到端框架，统一了从离线全堆栈采集和术中稀疏切片观测中恢复完整4D多结构心脏网格的方法，具有高准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于完整的CMR堆栈来推断心脏运动，限制了其在仅有稀疏观测的术中场景中的应用。

Method: TetHeart利用深度可变形四面体的显隐式混合表示，结合注意力机制和两阶段弱监督运动学习方案。

Result: TetHeart在三个大型公共数据集上训练和验证，并在额外的私人介入和公共CMR数据集上进行了零样本外部评估，表现出色。

Conclusion: TetHeart框架在心脏运动重建方面实现了最先进的准确性和强泛化能力，适用于术前和术中场景。

Abstract: Reconstructing cardiac motion from cine CMR sequences is critical for
diagnosis, prediction, and intervention. Existing methods rely on complete CMR
stacks to infer full heart motion, limiting their utility in intra-procedural
scenarios where only sparse observations are available. We present TetHeart,
the first end-to-end framework that unifies full 4D multi-structure heart mesh
recovery from both offline full-stack acquisitions and intra-procedural
sparse-slice observations. Our method leverages deep deformable tetrahedra, an
explicit-implicit hybrid representation, to capture shape and motion in a
coherent space shared across cardiac structures. It is initialized from
high-quality pre-procedural or offline-acquired full stacks to build detailed,
patient-specific heart meshes, which can then be updated using whatever slices
are available, from full stacks down to a single slice. We further incorporate
several key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D
feature assembly that dynamically integrates information from arbitrary numbers
of slices at any position, combined with a distillation strategy from
full-slice to sparse-slice settings to ensure accurate reconstruction under
extreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme
requiring only keyframe (e.g., ED and ES) annotations. Trained and validated on
three large public datasets and externally evaluated zero-shot on additional
private interventional and public CMR datasets, TetHeart achieves
state-of-the-art accuracy and strong generalization in both pre- and
intra-procedural settings.

</details>


### [135] [FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation](https://arxiv.org/abs/2509.12105)
*Bernardo Forni,Gabriele Lombardi,Federico Pozzi,Mirco Planamente*

Main category: cs.CV

TL;DR: FS-SAM2利用SAM2的视频能力结合LoRA技术，实现了小样本下的高效语义分割，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有Few-Shot语义分割方法需要大量训练数据的问题，利用SAM2的零分割能力提升小样本下的分割性能。

Method: 提出基于SAM2的Few-Shot分割方法FS-SAM2，通过LoRA技术适应多样化的图像数据，仅需少量参数即可实现高效适应。

Result: 在PASCAL-5$^i$、COCO-20$^i$和FSS-1000数据集上表现优异，计算效率高。

Conclusion: FS-SAM2通过直接利用SAM2的视频能力并结合LoRA技术，实现了在少量样本下的高效语义分割，显著提升了计算效率并在多个数据集上取得了优异表现。

Abstract: Few-shot semantic segmentation has recently attracted great attention. The
goal is to develop a model capable of segmenting unseen classes using only a
few annotated samples. Most existing approaches adapt a pre-trained model by
training from scratch an additional module. Achieving optimal performance with
these approaches requires extensive training on large-scale datasets. The
Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and
video segmentation with a modular design. In this paper, we propose a Few-Shot
segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities
are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank
Adaptation (LoRA) to the original modules in order to handle the diverse images
typically found in standard datasets, unlike the temporally connected frames
used in SAM2's pre-training. With this approach, only a small number of
parameters is meta-trained, which effectively adapts SAM2 while benefiting from
its impressive segmentation performance. Our method supports any K-shot
configuration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and
FSS-1000 datasets, achieving remarkable results and demonstrating excellent
computational efficiency during inference. Code is available at
https://github.com/fornib/FS-SAM2

</details>


### [136] [RailSafeNet: Visual Scene Understanding for Tram Safety](https://arxiv.org/abs/2509.12125)
*Ing. Ondrej Valach,Ing. Ivan Gruber*

Main category: cs.CV

TL;DR: RailSafeNet利用深度学习和数字图像处理技术，实时检测轨道入侵，提升电车运行安全性。


<details>
  <summary>Details</summary>
Motivation: 电车在人口密集区域运行时的碰撞风险高，需一种高效、实时的解决方案来提升行人、司机、乘客等的安全。

Method: 采用SegFormer B3模型进行语义分割，YOLOv8进行目标检测，并结合Distance Assessor评估风险。

Result: 在RailSem19数据集上，SegFormer B3模型达到65% IoU，YOLOv8达到75.6% mAP，系统能准确识别轨道入侵并预警。

Conclusion: RailSafeNet通过结合语义分割、目标检测和基于规则的Distance Assessor，提供了一种实时、准确的轨道入侵检测方案，有效提升了电车运行的安全性。

Abstract: Tram-human interaction safety is an important challenge, given that trams
frequently operate in densely populated areas, where collisions can range from
minor injuries to fatal outcomes. This paper addresses the issue from the
perspective of designing a solution leveraging digital image processing, deep
learning, and artificial intelligence to improve the safety of pedestrians,
drivers, cyclists, pets, and tram passengers. We present RailSafeNet, a
real-time framework that fuses semantic segmentation, object detection and a
rule-based Distance Assessor to highlight track intrusions. Using only
monocular video, the system identifies rails, localises nearby objects and
classifies their risk by comparing projected distances with the standard 1435mm
rail gauge. Experiments on the diverse RailSem19 dataset show that a
class-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),
while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculated
at an intersection over union (IoU) threshold of 0.50. RailSafeNet therefore
delivers accurate, annotation-light scene understanding that can warn drivers
before dangerous situations escalate. Code available at
https://github.com/oValach/RailSafeNet.

</details>


### [137] [Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models](https://arxiv.org/abs/2509.12132)
*Pu Jian,Junhong Wu,Wei Sun,Chen Wang,Shuo Ren,Jiajun Zhang*

Main category: cs.CV

TL;DR: 提出 Reflection-V 模型，通过视觉中心数据构建和奖励设计增强视觉反思能力，在多个视觉推理基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 将文本专用的“慢思考”推理能力迁移到视觉语言模型（VLMs）中面临视觉反思能力不足的挑战。

Method: 通过构建视觉中心的推理数据和设计基于视觉注意力的奖励模型来增强视觉反思能力。

Result: Reflection-V 在多个视觉推理基准上表现显著改进，且对视觉信息的依赖更强和更一致。

Conclusion: Reflection-V 在多个视觉推理基准上表现出显著改进，并保持了在视觉推理过程中对视觉信息更强且更一致的依赖，表明其视觉反思能力得到了有效增强。

Abstract: Recent advances in text-only "slow-thinking" reasoning have prompted efforts
to transfer this capability to vision-language models (VLMs), for training
visual reasoning models (\textbf{VRMs}). owever, such transfer faces critical
challenges: Effective "slow thinking" in VRMs requires \textbf{visual
reflection}, the ability to check the reasoning process based on visual
information. Through quantitative analysis, we observe that current VRMs
exhibit limited visual reflection, as their attention to visual information
diminishes rapidly with longer generated responses. To address this challenge,
we propose a new VRM \textbf{Reflection-V}, which enhances visual reflection
based on reasoning data construction for cold-start and reward design for
reinforcement learning (RL). Firstly, we construct vision-centered reasoning
data by leveraging an agent that interacts between VLMs and reasoning LLMs,
enabling cold-start learning of visual reflection patterns. Secondly, a visual
attention based reward model is employed during RL to encourage reasoning based
on visual information. Therefore, \textbf{Reflection-V} demonstrates
significant improvements across multiple visual reasoning benchmarks.
Furthermore, \textbf{Reflection-V} maintains a stronger and more consistent
reliance on visual information during visual reasoning, indicating effective
enhancement in visual reflection capabilities.

</details>


### [138] [Open-ended Hierarchical Streaming Video Understanding with Vision Language Models](https://arxiv.org/abs/2509.12145)
*Hyolim Kang,Yunsu Park,Youngbeom Yoo,Yeeun Choi,Seon Joo Kim*

Main category: cs.CV

TL;DR: 该论文提出 OpenHOUSE 系统，通过 LLMs 分组原子动作为高层事件，显著提升流式动作边界检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据集中层次化和细粒度时间标注的稀缺问题，并扩展流式动作感知能力。

Method: 提出 OpenHOUSE 系统，包含专门设计的流式模块，用于精确检测相邻动作边界。

Result: OpenHOUSE 的性能比现有方法直接扩展提升了近一倍。

Conclusion: OpenHOUSE 是流式动作感知领域的重要进展，为未来整合强大生成模型奠定了基础。

Abstract: We introduce Hierarchical Streaming Video Understanding, a task that combines
online temporal action localization with free-form description generation.
Given the scarcity of datasets with hierarchical and fine-grained temporal
annotations, we demonstrate that LLMs can effectively group atomic actions into
higher-level events, enriching existing datasets. We then propose OpenHOUSE
(Open-ended Hierarchical Online Understanding System for Events), which extends
streaming action perception beyond action classification. OpenHOUSE features a
specialized streaming module that accurately detects boundaries between closely
adjacent actions, nearly doubling the performance of direct extensions of
existing methods. We envision the future of streaming action perception in the
integration of powerful generative models, with OpenHOUSE representing a key
step in that direction.

</details>


### [139] [LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury](https://arxiv.org/abs/2509.12155)
*M. Bolhassani,B. Veasey,E. Daugherty,S. Keltner,N. Kumar,N. Dunlap,A. Amini*

Main category: cs.CV

TL;DR: LoRA在微调大型视觉模型诊断RILI时表现优异，计算成本低，训练时间短。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估LoRA在微调大型视觉模型诊断RILI的效率和鲁棒性，尤其是与传统方法相比。

Method: 研究比较了LoRA、传统全微调和仅推理（无微调）方法。使用了两种尺寸（50 mm³和75 mm³）的裁剪图像，以及不同的适应技术来调整2D LVMs以适应3D数据，以确定模型对空间上下文的敏感性。

Result: 实验结果表明，LoRA在性能上与传统微调相当或更优，同时显著减少了可训练参数的数量，从而降低了计算成本和训练时间。

Conclusion: LoRA在微调大型视觉模型（如DinoV2和SwinV2）用于诊断放射诱导的肺损伤（RILI）方面表现出与传统全微调相当或更优的性能，同时显著降低了计算成本和训练时间。

Abstract: This study investigates the efficacy of Low-Rank Adaptation (LoRA) for
fine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose
Radiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic
Body Radiation Therapy (SBRT). To evaluate the robustness and efficiency of
this approach, we compare LoRA with traditional full fine-tuning and
inference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3
and 75 mm3), centered at the treatment isocenter, in addition to different
adaptation techniques for adapting the 2D LVMs for 3D data were used to
determine the sensitivity of the models to spatial context. Experimental
results show that LoRA achieves comparable or superior performance to
traditional fine-tuning while significantly reducing computational costs and
training times by requiring fewer trainable parameters.

</details>


### [140] [Domain-Adaptive Pretraining Improves Primate Behavior Recognition](https://arxiv.org/abs/2509.12193)
*Felix B. Mueller,Timo Lueddecke,Richard Vogg,Alexander S. Ecker*

Main category: cs.CV

TL;DR: 自监督学习和DAP显著提升灵长类动物行为识别性能，无需标注数据。


<details>
  <summary>Details</summary>
Motivation: 解决动物行为研究中大规模数据标注成本高的问题，提出数据高效的学习方法。

Method: 采用预训练的V-JEPA模型，并应用领域自适应预训练（DAP）方法。

Result: 在两个大猩猩行为数据集（PanAf和ChimpACT）上，分别比现有最佳模型提高了6.1%的准确率和6.3%的mAP。

Conclusion: 论文展示了自监督学习和领域自适应预训练在灵长类动物行为识别中的有效性，特别是在不需要标注样本的情况下显著提升了性能。

Abstract: Computer vision for animal behavior offers promising tools to aid research in
ecology, cognition, and to support conservation efforts. Video camera traps
allow for large-scale data collection, but high labeling costs remain a
bottleneck to creating large-scale datasets. We thus need data-efficient
learning approaches. In this work, we show that we can utilize self-supervised
learning to considerably improve action recognition on primate behavior. On two
datasets of great ape behavior (PanAf and ChimpACT), we outperform published
state-of-the-art action recognition models by 6.1 %pt. accuracy and 6.3 %pt.
mAP, respectively. We achieve this by utilizing a pretrained V-JEPA model and
applying domain-adaptive pretraining (DAP), i.e. continuing the pretraining
with in-domain data. We show that most of the performance gain stems from the
DAP. Our method promises great potential for improving the recognition of
animal behavior, as DAP does not require labeled samples. Code is available at
https://github.com/ecker-lab/dap-behavior

</details>


### [141] [3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review](https://arxiv.org/abs/2509.12197)
*Salma Galaaoui,Eduardo Valle,David Picard,Nermin Samet*

Main category: cs.CV

TL;DR: 本文综述了LiDAR点云中的3D人体姿态估计和网格恢复方法，提出了分类法并建立了基准表，以推动该领域的发展。


<details>
  <summary>Details</summary>
Motivation: 为了促进LiDAR点云中3D人体理解的进展，需要系统化现有方法并建立公平的比较基准。

Method: 通过比较现有方法的关键维度，提出结构化分类法，并分析每种方法的优缺点和设计选择。此外，还进行了数据集的定量比较、统一评估指标的定义，并建立了基准表。

Result: 提出了一个分类法，分析了现有方法的优缺点，建立了数据集和评估指标的基准表，并指出了未来研究方向。

Conclusion: 本文总结了3D人体姿态估计和人体网格恢复的现有方法，提出了一个分类法来系统化这些方法，并指出了未来研究的关键挑战和方向。

Abstract: In this paper, we present a comprehensive review of 3D human pose estimation
and human mesh recovery from in-the-wild LiDAR point clouds. We compare
existing approaches across several key dimensions, and propose a structured
taxonomy to classify these methods. Following this taxonomy, we analyze each
method's strengths, limitations, and design choices. In addition, (i) we
perform a quantitative comparison of the three most widely used datasets,
detailing their characteristics; (ii) we compile unified definitions of all
evaluation metrics; and (iii) we establish benchmark tables for both tasks on
these datasets to enable fair comparisons and promote progress in the field. We
also outline open challenges and research directions critical for advancing
LiDAR-based 3D human understanding. Moreover, we maintain an accompanying
webpage that organizes papers according to our taxonomy and continuously update
it with new studies:
https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR

</details>


### [142] [OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling](https://arxiv.org/abs/2509.12201)
*Yang Zhou,Yifan Wang,Jianjun Zhou,Wenzheng Chang,Haoyu Guo,Zizun Li,Kaijing Ma,Xinyue Li,Yating Wang,Haoyi Zhu,Mingyu Liu,Dingning Liu,Jiange Yang,Zhoujie Fu,Junyi Chen,Chunhua Shen,Jiangmiao Pang,Kaipeng Zhang,Tong He*

Main category: cs.CV

TL;DR: OmniWorld是一个大规模、多领域、多模态的数据集，旨在解决4D世界建模中数据不足的问题，显著提升了现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和基准测试往往缺乏动态复杂性、多领域多样性和时空注释，无法支持4D几何重建、未来预测和相机控制视频生成等关键任务。

Method: 引入OmniWorld，一个大规模、多领域、多模态的数据集，专门为4D世界建模设计，包括新收集的OmniWorld-Game数据集和几个精选的公共数据集。基于此数据集，建立了一个挑战性的基准测试。

Result: 在OmniWorld上微调现有的SOTA方法，在4D重建和视频生成任务中带来了显著的性能提升，强烈验证了OmniWorld作为训练和评估的强大资源。

Conclusion: OmniWorld被设想为加速通用4D世界模型开发的催化剂，最终推动机器对物理世界的整体理解。

Abstract: The field of 4D world modeling - aiming to jointly capture spatial geometry
and temporal dynamics - has witnessed remarkable progress in recent years,
driven by advances in large-scale generative models and multimodal learning.
However, the development of truly general 4D world models remains fundamentally
constrained by the availability of high-quality data. Existing datasets and
benchmarks often lack the dynamic complexity, multi-domain diversity, and
spatial-temporal annotations required to support key tasks such as 4D geometric
reconstruction, future prediction, and camera-control video generation. To
address this gap, we introduce OmniWorld, a large-scale, multi-domain,
multi-modal dataset specifically designed for 4D world modeling. OmniWorld
consists of a newly collected OmniWorld-Game dataset and several curated public
datasets spanning diverse domains. Compared with existing synthetic datasets,
OmniWorld-Game provides richer modality coverage, larger scale, and more
realistic dynamic interactions. Based on this dataset, we establish a
challenging benchmark that exposes the limitations of current state-of-the-art
(SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning
existing SOTA methods on OmniWorld leads to significant performance gains
across 4D reconstruction and video generation tasks, strongly validating
OmniWorld as a powerful resource for training and evaluation. We envision
OmniWorld as a catalyst for accelerating the development of general-purpose 4D
world models, ultimately advancing machines' holistic understanding of the
physical world.

</details>


### [143] [LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence](https://arxiv.org/abs/2509.12203)
*Zixin Yin,Xili Dai,Duomin Wang,Xianfang Zeng,Lionel M. Ni,Gang Yu,Heung-Yeung Shum*

Main category: cs.CV

TL;DR: LazyDrag是一种无需隐式点匹配的拖拽图像编辑方法，通过显式对应映射提升性能，支持复杂编辑并在DragBench上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有基于拖拽的编辑方法因依赖隐式点匹配而削弱了反转强度并需要昂贵的测试时优化，严重限制了扩散模型的生成能力。

Method: LazyDrag生成显式的对应映射作为可靠参考，以增强注意力控制，从而实现稳定的全强度反转过程。

Result: LazyDrag在DragBench上的拖拽准确性和感知质量均优于基线方法，支持多轮工作流和复杂编辑任务。

Conclusion: LazyDrag通过消除对隐式点匹配的依赖，显著提升了基于拖拽的图像编辑性能，并在DragBench上实现了最先进的性能，为编辑范式开辟了新途径。

Abstract: The reliance on implicit point matching via attention has become a core
bottleneck in drag-based editing, resulting in a fundamental compromise on
weakened inversion strength and costly test-time optimization (TTO). This
compromise severely limits the generative capabilities of diffusion models,
suppressing high-fidelity inpainting and text-guided creation. In this paper,
we introduce LazyDrag, the first drag-based image editing method for
Multi-Modal Diffusion Transformers, which directly eliminates the reliance on
implicit point matching. In concrete terms, our method generates an explicit
correspondence map from user drag inputs as a reliable reference to boost the
attention control. This reliable reference opens the potential for a stable
full-strength inversion process, which is the first in the drag-based editing
task. It obviates the necessity for TTO and unlocks the generative capability
of models. Therefore, LazyDrag naturally unifies precise geometric control with
text guidance, enabling complex edits that were previously out of reach:
opening the mouth of a dog and inpainting its interior, generating new objects
like a ``tennis ball'', or for ambiguous drags, making context-aware changes
like moving a hand into a pocket. Additionally, LazyDrag supports multi-round
workflows with simultaneous move and scale operations. Evaluated on the
DragBench, our method outperforms baselines in drag accuracy and perceptual
quality, as validated by VIEScore and human evaluation. LazyDrag not only
establishes new state-of-the-art performance, but also paves a new way to
editing paradigms.

</details>


### [144] [Character-Centric Understanding of Animated Movies](https://arxiv.org/abs/2509.12204)
*Zhongrui Gui,Junyu Xie,Tengda Han,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出音频-视觉管道解决动画角色识别难题，构建多模态角色库，应用于音频描述和字幕生成，显著提升动画内容的可访问性。


<details>
  <summary>Details</summary>
Motivation: 动画角色外观、动作和形变的高度多样性对现有识别系统构成挑战，需要更鲁棒的方法来增强角色为中心的动画电影理解。

Method: 提出了一种自动构建音频-视觉角色库的管道，包含视觉样本和声音样本，支持多模态角色识别。

Result: 构建了CMD-AM数据集（75部动画电影），并展示了在音频描述生成和角色感知字幕方面的应用效果。

Conclusion: 该论文提出的音频-视觉管道显著提高了动画内容的可访问性和叙事理解能力，优于传统基于面部检测的方法。

Abstract: Animated movies are captivating for their unique character designs and
imaginative storytelling, yet they pose significant challenges for existing
recognition systems. Unlike the consistent visual patterns detected by
conventional face recognition methods, animated characters exhibit extreme
diversity in their appearance, motion, and deformation. In this work, we
propose an audio-visual pipeline to enable automatic and robust animated
character recognition, and thereby enhance character-centric understanding of
animated movies. Central to our approach is the automatic construction of an
audio-visual character bank from online sources. This bank contains both visual
exemplars and voice (audio) samples for each character, enabling subsequent
multi-modal character recognition despite long-tailed appearance distributions.
Building on accurate character recognition, we explore two downstream
applications: Audio Description (AD) generation for visually impaired
audiences, and character-aware subtitling for the hearing impaired. To support
research in this domain, we introduce CMD-AM, a new dataset of 75 animated
movies with comprehensive annotations. Our character-centric pipeline
demonstrates significant improvements in both accessibility and narrative
comprehension for animated content over prior face-detection-based approaches.
For the code and dataset, visit
https://www.robots.ox.ac.uk/~vgg/research/animated_ad/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [145] [Situation Model of the Transport, Transport Emissions and Meteorological Conditions](https://arxiv.org/abs/2509.10541)
*V. Benes,M. Svitek,A. Michalikova,M. Melicherik*

Main category: cs.AI

TL;DR: 本文通过模糊推理系统分析天气对城市交通排放的影响，开发了一个预测模型，旨在帮助城市规划者更环保地管理交通。


<details>
  <summary>Details</summary>
Motivation: 城市空气污染及其减少的可能性是当今社会需要应对的重要因素，本文旨在通过分析天气对交通排放量和扩散的影响，为城市规划提供科学依据。

Method: 使用交通、气象和排放数据，结合模糊推理系统（FIS）开发预测模型。

Result: 开发了一个基于模糊推理系统的模型，能够预测不同条件下排放的变化。

Conclusion: 该论文通过模糊推理系统（FIS）开发了一个预测模型，帮助城市规划和政策制定者更有效地管理城市交通，同时考虑环境保护。

Abstract: Air pollution in cities and the possibilities of reducing this pollution
represents one of the most important factors that today's society has to deal
with. This paper focuses on a systemic approach to traffic emissions with their
relation to meteorological conditions, analyzing the effect of weather on the
quantity and dispersion of traffic emissions in a city. Using fuzzy inference
systems (FIS) the model for prediction of changes in emissions depending on
various conditions is developed. The proposed model is based on traffic,
meteorology and emission data measured in Prague, Czech Republic. The main
objective of the work is to provide insight into how urban planners and
policymakers can plan and manage urban transport more effectively with
environmental protection in mind.

</details>


### [146] [ZapGPT: Free-form Language Prompting for Simulated Cellular Control](https://arxiv.org/abs/2509.10660)
*Nam H. Le,Patrick Erickson,Yanbo Zhang,Michael Levin,Josh Bongard*

Main category: cs.AI

TL;DR: 研究首次证明自由形式自然语言可引导简单代理集体行为，无需任务特定调整，为AI与生物学合作提供新方向。


<details>
  <summary>Details</summary>
Motivation: 解决人工或生物系统缺乏解释和响应自然语言机制的问题，探索自由形式自然语言能否独立引导集体行为。

Method: 使用两个AI模型：一个将命令提示转化为干预措施应用于模拟细胞，另一个评估提示描述细胞动态的准确性，并通过进化前一个模型来提高评分。

Result: 系统能够在不重新训练的情况下推广到未见的提示，且无需工程化适应度函数或特定领域提示设计。

Conclusion: 本研究展示了通过自然语言提示引导简单代理集体行为的可能性，为AI与生物学合作提供了具体步骤，实现语言作为控制层的愿景。

Abstract: Human language is one of the most expressive tools for conveying intent, yet
most artificial or biological systems lack mechanisms to interpret or respond
meaningfully to it. Bridging this gap could enable more natural forms of
control over complex, decentralized systems. In AI and artificial life, recent
work explores how language can specify high-level goals, but most systems still
depend on engineered rewards, task-specific supervision, or rigid command sets,
limiting generalization to novel instructions. Similar constraints apply in
synthetic biology and bioengineering, where the locus of control is often
genomic rather than environmental perturbation.
  A key open question is whether artificial or biological collectives can be
guided by free-form natural language alone, without task-specific tuning or
carefully designed evaluation metrics. We provide one possible answer here by
showing, for the first time, that simple agents' collective behavior can be
guided by free-form language prompts: one AI model transforms an imperative
prompt into an intervention that is applied to simulated cells; a second AI
model scores how well the prompt describes the resulting cellular dynamics; and
the former AI model is evolved to improve the scores generated by the latter.
  Unlike previous work, our method does not require engineered fitness
functions or domain-specific prompt design. We show that the evolved system
generalizes to unseen prompts without retraining. By treating natural language
as a control layer, the system suggests a future in which spoken or written
prompts could direct computational, robotic, or biological systems to desired
behaviors. This work provides a concrete step toward this vision of AI-biology
partnerships, in which language replaces mathematical objective functions,
fixed rules, and domain-specific programming.

</details>


### [147] [Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration](https://arxiv.org/abs/2509.10704)
*Xingchen Wan,Han Zhou,Ruoxi Sun,Hootan Nakhost,Ke Jiang,Rajarishi Sinha,Sercan Ö. Arık*

Main category: cs.AI

TL;DR: Maestro是一种自改进的T2I系统，通过自批判和自进化机制，显著提升了生成图像的质量，减少了对人工干预的依赖。


<details>
  <summary>Details</summary>
Motivation: T2I模型高度依赖人工干预，存在可用性挑战，需要手动迭代提示工程。Maestro旨在通过自主改进生成图像来解决这一问题。

Method: Maestro 结合了自批判（利用多模态LLM代理作为‘评论家’识别图像弱点并纠正不足）和自进化（通过MLLM-as-a-judge进行迭代图像比较，淘汰问题图像并优化提示）。

Result: 实验表明，Maestro在复杂T2I任务中显著提升了图像质量，且效果随MLLM组件的先进性而增强。

Conclusion: Maestro 提供了一种稳健、可解释且有效的自改进文本到图像生成途径，显著提升了图像质量。

Abstract: Text-to-image (T2I) models, while offering immense creative potential, are
highly reliant on human intervention, posing significant usability challenges
that often necessitate manual, iterative prompt engineering over often
underspecified prompts. This paper introduces Maestro, a novel self-evolving
image generation system that enables T2I models to autonomously self-improve
generated images through iterative evolution of prompts, using only an initial
prompt. Maestro incorporates two key innovations: 1) self-critique, where
specialized multimodal LLM (MLLM) agents act as 'critics' to identify
weaknesses in generated images, correct for under-specification, and provide
interpretable edit signals, which are then integrated by a 'verifier' agent
while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge
for head-to-head comparisons between iteratively generated images, eschewing
problematic images, and evolving creative prompt candidates that align with
user intents. Extensive experiments on complex T2I tasks using black-box models
demonstrate that Maestro significantly improves image quality over initial
prompts and state-of-the-art automated methods, with effectiveness scaling with
more advanced MLLM components. This work presents a robust, interpretable, and
effective pathway towards self-improving T2I generation.

</details>


### [148] [Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions](https://arxiv.org/abs/2509.10707)
*Sajjad Abdoli,Rudi Cilibrasi,Rima Al-Shikh*

Main category: cs.AI

TL;DR: 研究发现不同GPT变体在评估视觉语言描述时表现出独特的评估策略和偏见，评估能力不随通用能力提升而线性增长，需多样化架构以实现稳健评估。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地评估其他AI的输出，理解它们的评估行为对于防止级联偏见变得至关重要。

Method: 本研究分析了由NVIDIA的Describe Anything Model生成的视觉语言描述，并由三种GPT变体（GPT-4o、GPT-4o-mini、GPT-5）进行评估，以揭示各自的评估策略和偏见。通过使用Gemini 2.5 Pro作为独立问题生成器的对照实验验证了这些特性的固有性。

Result: GPT-4o-mini表现出系统性一致性且方差最小，GPT-4o擅长错误检测，而GPT-5则表现出极端保守主义和高变异性。所有GPT模型均表现出一致的2:1负面评估偏见。跨家族分析显示GPT模型具有高相似性，而Gemini则表现出显著不同的评估策略。

Conclusion: 评估能力并不随通用能力的提升而线性增长，稳健的AI评估需要多样化的架构视角。

Abstract: As AI systems increasingly evaluate other AI outputs, understanding their
assessment behavior becomes crucial for preventing cascading biases. This study
analyzes vision-language descriptions generated by NVIDIA's Describe Anything
Model and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to
uncover distinct "evaluation personalities" the underlying assessment
strategies and biases each model demonstrates. GPT-4o-mini exhibits systematic
consistency with minimal variance, GPT-4o excels at error detection, while
GPT-5 shows extreme conservatism with high variability. Controlled experiments
using Gemini 2.5 Pro as an independent question generator validate that these
personalities are inherent model properties rather than artifacts. Cross-family
analysis through semantic similarity of generated questions reveals significant
divergence: GPT models cluster together with high similarity while Gemini
exhibits markedly different evaluation strategies. All GPT models demonstrate a
consistent 2:1 bias favoring negative assessment over positive confirmation,
though this pattern appears family-specific rather than universal across AI
architectures. These findings suggest that evaluation competence does not scale
with general capability and that robust AI assessment requires diverse
architectural perspectives.

</details>


### [149] [AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework](https://arxiv.org/abs/2509.10762)
*Arlen Kumar,Leanid Palkhouski*

Main category: cs.AI

TL;DR: GEO-16框架评估AI引擎引用质量，发现页面质量是引用关键因素，并提出实用建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估AI回答引擎在引用网页来源时的质量差异。

Method: 研究引入了GEO-16框架，通过70个产品意图提示收集了1,702个引用，并审核了1,100个唯一URL。

Result: 结果显示，不同引擎在引用页面的GEO质量上存在差异，且某些支柱（如元数据和新鲜度、语义HTML、结构化数据）与引用关联最强。

Conclusion: 研究结论表明，总体页面质量是引用的强预测指标，并提出了一些实用的出版建议。

Abstract: AI answer engines increasingly mediate access to domain knowledge by
generating responses and citing web sources. We introduce GEO-16, a 16 pillar
auditing framework that converts on page quality signals into banded pillar
scores and a normalized GEO score G that ranges from 0 to 1. Using 70 product
intent prompts, we collected 1,702 citations across three engines (Brave
Summary, Google AI Overviews, and Perplexity) and audited 1,100 unique URLs. In
our corpus, the engines differed in the GEO quality of the pages they cited,
and pillars related to Metadata and Freshness, Semantic HTML, and Structured
Data showed the strongest associations with citation. Logistic models with
domain clustered standard errors indicate that overall page quality is a strong
predictor of citation, and simple operating points (for example, G at least
0.70 combined with at least 12 pillar hits) align with substantially higher
citation rates in our data. We report per engine contrasts, vertical effects,
threshold analysis, and diagnostics, then translate findings into a practical
playbook for publishers. The study is observational and focuses on English
language B2B SaaS pages; we discuss limitations, threats to validity, and
reproducibility considerations.

</details>


### [150] [AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise](https://arxiv.org/abs/2509.10769)
*Tara Bogavelli,Roshnee Sharma,Hari Subramani*

Main category: cs.AI

TL;DR: 研究评估了18种代理配置，发现模型偏好和性能不足，旨在为未来系统设计提供实证支持。


<details>
  <summary>Details</summary>
Motivation: 尽管代理架构的单个组件已被单独研究，但对于复杂多代理系统中不同设计维度如何相互作用的实证理解仍然有限。本研究旨在填补这些空白。

Method: 通过一个全面的企业特定基准评估了18种不同的代理配置，考察了四个关键代理系统维度：编排策略、代理提示实现（ReAct与函数调用）、记忆架构和思维工具集成。

Result: 基准测试揭示了显著的模型特定架构偏好，并显示了在企业任务中代理性能的总体弱点。

Conclusion: 研究结果挑战了目前代理AI系统中普遍存在的一刀切范式，并揭示了在企业任务中代理性能的显著不足，最高得分模型在复杂任务中仅达到35.3%的成功率，在简单任务中为70.8%。这些发现旨在为未来代理系统的设计提供基于实证的决策支持。

Abstract: While individual components of agentic architectures have been studied in
isolation, there remains limited empirical understanding of how different
design dimensions interact within complex multi-agent systems. This study aims
to address these gaps by providing a comprehensive enterprise-specific
benchmark evaluating 18 distinct agentic configurations across state-of-the-art
large language models. We examine four critical agentic system dimensions:
orchestration strategy, agent prompt implementation (ReAct versus function
calling), memory architecture, and thinking tool integration. Our benchmark
reveals significant model-specific architectural preferences that challenge the
prevalent one-size-fits-all paradigm in agentic AI systems. It also reveals
significant weaknesses in overall agentic performance on enterprise tasks with
the highest scoring models achieving a maximum of only 35.3\% success on the
more complex task and 70.8\% on the simpler task. We hope these findings inform
the design of future agentic systems by enabling more empirically backed
decisions regarding architectural components and model selection.

</details>


### [151] [LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering](https://arxiv.org/abs/2509.10818)
*Boris Kovalerchuk,Brent D. Fegley*

Main category: cs.AI

TL;DR: 本文探讨了如何利用优化的人机对话和EMM算法提升LLM在决策支持中的效率，特别是在信息缺失的情况下。


<details>
  <summary>Details</summary>
Motivation: 尽管生成技术（如LLMs）在决策支持中显示出潜力，但它们无法解决训练数据中的缺失问题，导致幻觉。RAG等方法虽能部分缓解问题，但仍无法完全解决信息缺失的挑战。

Method: 提出了一个四步EMM算法：(1) 因素识别，(2) 因素的层次结构构建，(3) 生成广义专家心理模型规范，(4) 从该规范生成详细的广义专家心理模型。

Result: 通过EMM算法，LLM能够更高效地支持决策过程，尤其是在处理复杂或信息缺失的任务时。

Conclusion: 本研究提出了一种基于优化人机对话和单调布尔及k值函数的技术，用于发现计算上可处理的个人专家决策模型（EMM）。通过四步EMM算法，LLM提示工程能够更有效地支持决策过程。

Abstract: Difficult decision-making problems abound in various disciplines and domains.
The proliferation of generative techniques, especially large language models
(LLMs), has excited interest in using them for decision support. However, LLMs
cannot yet resolve missingness in their training data, leading to
hallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by
incorporating external information retrieval, reducing hallucinations and
improving accuracy. Yet, RAG and related methods are only partial solutions, as
they may lack access to all necessary sources or key missing information. Even
everyday issues often challenge LLMs' abilities. Submitting longer prompts with
context and examples is one approach to address knowledge gaps, but designing
effective prompts is non-trivial and may not capture complex mental models of
domain experts. For tasks with missing critical information, LLMs are
insufficient, as are many existing systems poorly represented in available
documents. This paper explores how LLMs can make decision-making more
efficient, using a running example of evaluating whether to respond to a call
for proposals. We propose a technology based on optimized human-machine
dialogue and monotone Boolean and k-valued functions to discover a
computationally tractable personal expert mental model (EMM) of
decision-making. Our EMM algorithm for LLM prompt engineering has four steps:
(1) factor identification, (2) hierarchical structuring of factors, (3)
generating a generalized expert mental model specification, and (4) generating
a detailed generalized expert mental model from that specification.

</details>


### [152] [From Grounding to Skolemization: A Logic-Constrained Vector Symbolic Architecture for Complex Query Answering](https://arxiv.org/abs/2509.10837)
*Yuyin Lu,Hegang Chen,Yanghui Rao*

Main category: cs.AI

TL;DR: LVSA 是一种神经符号框架，通过结合可微 Skolemization 和逻辑约束优化，显著提升了复杂查询回答的效率和逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂查询回答中逻辑一致性和计算效率之间的基本权衡问题。

Method: 提出了 Logic-constrained Vector Symbolic Architecture (LVSA)，一个神经符号框架，包含可微 Skolemization 模块、神经否定器和逻辑约束驱动的优化协议。

Result: 理论上，LVSA 保证了所有 EFO$_1$ 查询的普遍性；实证上，它在性能上优于最先进的 Skolemization 方法，并大幅降低了推理成本。

Conclusion: LVSA 提供了一种神经符号框架，通过结合可微 Skolemization 模块和神经否定器，以及逻辑约束驱动的优化协议，有效解决了复杂查询回答中的逻辑一致性和计算效率问题。

Abstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs),
typically formalized as reasoning with Existential First-Order predicate logic
with one free variable (EFO$_1$), faces a fundamental trade-off between logical
soundness and computational efficiency. This work establishes the
Grounding-Skolemization dichotomy for systematically analyzing CQA methods
through the lens of formal logic. While Grounding-based methods inherently
suffer from combinatorial explosion, most Skolemization-based methods neglect
to explicitly model Skolem functions and compromise logical consistency. To
address these limitations, we propose the Logic-constrained Vector Symbolic
Architecture (LVSA), a neuro-symbolic framework that unifies a differentiable
Skolemization module and a neural negator, as well as a logical
constraint-driven optimization protocol to harmonize geometric and logical
requirements. Theoretically, LVSA guarantees universality for all EFO$_1$
queries. Empirically, it outperforms state-of-the-art Skolemization-based
methods and reduces inference costs by orders of magnitude compared to
Grounding-based baselines.

</details>


### [153] [Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?](https://arxiv.org/abs/2509.10875)
*Jesse Gardner,Vladimir A. Baulin*

Main category: cs.AI

TL;DR: 论文批判性评估AI中的代理范式，指出其局限性，提出转向非代理性和系统性框架，以推动更广泛的智能研究。


<details>
  <summary>Details</summary>
Motivation: 代理概念在AI研究中具有深远影响，但其概念模糊性和拟人化偏见可能成为一种限制性框架，需要重新评估其必要性和最优性。

Method: 通过系统回顾相关文献，解构不同AI框架中的代理范式，分析自主性和目标导向性等属性的定义与测量挑战。

Result: 分析表明，代理框架虽然在启发上有用，但可能误导并掩盖底层计算机制，特别是在大语言模型中。提出转向基于系统级动态、世界建模和物质智能的框架。

Conclusion: 论文提出需要超越代理隐喻，重新思考智能的本质，研究非代理性和系统性框架，以推动向更稳健、可扩展且可能非拟人化的通用智能形式发展。

Abstract: The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)
research, guiding development from foundational theories to contemporary
applications like Large Language Model (LLM)-based systems. This paper
critically re-evaluates the necessity and optimality of this agent-centric
paradigm. We argue that its persistent conceptual ambiguities and inherent
anthropocentric biases may represent a limiting framework. We distinguish
between agentic systems (AI inspired by agency, often semi-autonomous, e.g.,
LLM-based agents), agential systems (fully autonomous, self-producing systems,
currently only biological), and non-agentic systems (tools without the
impression of agency). Our analysis, based on a systematic review of relevant
literature, deconstructs the agent paradigm across various AI frameworks,
highlighting challenges in defining and measuring properties like autonomy and
goal-directedness. We argue that the 'agentic' framing of many AI systems,
while heuristically useful, can be misleading and may obscure the underlying
computational mechanisms, particularly in Large Language Models (LLMs). As an
alternative, we propose a shift in focus towards frameworks grounded in
system-level dynamics, world modeling, and material intelligence. We conclude
that investigating non-agentic and systemic frameworks, inspired by complex
systems, biology, and unconventional computing, is essential for advancing
towards robust, scalable, and potentially non-anthropomorphic forms of general
intelligence. This requires not only new architectures but also a fundamental
reconsideration of our understanding of intelligence itself, moving beyond the
agent metaphor.

</details>


### [154] [Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding](https://arxiv.org/abs/2509.10931)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: HaPLa是一种新型通用越狱技术，通过溯因框架和符号编码策略，成功攻击多种LLMs，揭示了安全调整与实用性之间的权衡难题。


<details>
  <summary>Details</summary>
Motivation: 研究通用越狱攻击以利用LLMs架构和学习范式中的内在弱点，从而加强防御措施。

Method: HaPLa结合了两种主要策略：1）溯因框架，指导LLMs推断有害活动的中间步骤；2）符号编码，一种轻量级且灵活的方法，用于混淆有害内容。

Result: HaPLa在GPT系列模型上实现了超过95%的攻击成功率，在所有目标上达到70%。

Conclusion: HaPLa技术揭示了LLMs在安全性和实用性之间的固有挑战，表明在不显著降低模型对良性查询的有用性的情况下，安全调整LLMs仍然困难。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but their potential misuse for harmful purposes remains a
significant concern. To strengthen defenses against such vulnerabilities, it is
essential to investigate universal jailbreak attacks that exploit intrinsic
weaknesses in the architecture and learning paradigms of LLMs. In response, we
propose \textbf{H}armful \textbf{P}rompt \textbf{La}undering (HaPLa), a novel
and broadly applicable jailbreaking technique that requires only black-box
access to target models. HaPLa incorporates two primary strategies: 1)
\textit{abductive framing}, which instructs LLMs to infer plausible
intermediate steps toward harmful activities, rather than directly responding
to explicit harmful queries; and 2) \textit{symbolic encoding}, a lightweight
and flexible approach designed to obfuscate harmful content, given that current
LLMs remain sensitive primarily to explicit harmful keywords. Experimental
results show that HaPLa achieves over 95% attack success rate on GPT-series
models and 70% across all targets. Further analysis with diverse symbolic
encoding rules also reveals a fundamental challenge: it remains difficult to
safely tune LLMs without significantly diminishing their helpfulness in
responding to benign queries.

</details>


### [155] [Public Data Assisted Differentially Private In-Context Learning](https://arxiv.org/abs/2509.10932)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 论文提出了一种结合公共数据的私有上下文学习算法，优化了隐私与效用的平衡，实验验证了其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型（LLMs）中上下文学习（ICL）可能导致的隐私数据泄露风险，以及差分隐私（DP）对ICL效用的显著降低，论文旨在解决这一挑战。

Method: 论文提出了一种私有上下文学习算法，通过结合任务相关公共数据并保持差分隐私（DP）保证，优化了隐私保护和模型效用的平衡。

Result: 实验表明，该方法在公共数据的辅助下显著提高了私有ICL的效用，并对成员推理攻击表现出鲁棒性。

Conclusion: 该论文提出了一种结合任务相关公共数据的私有上下文学习算法，有效平衡了隐私保护和模型效用，并通过实验验证了其优越性和鲁棒性。

Abstract: In-context learning (ICL) in Large Language Models (LLMs) has shown
remarkable performance across various tasks without requiring fine-tuning.
However, recent studies have highlighted the risk of private data leakage
through the prompt in ICL, especially when LLMs are exposed to malicious
attacks. While differential privacy (DP) provides strong privacy guarantees, it
often significantly reduces the utility of in-context learning (ICL). To
address this challenge, we incorporate task-related public data into the ICL
framework while maintaining the DP guarantee. Based on this approach, we
propose a private in-context learning algorithm that effectively balances
privacy protection and model utility. Through experiments, we demonstrate that
our approach significantly improves the utility of private ICL with the
assistance of public data. Additionally, we show that our method is robust
against membership inference attacks, demonstrating empirical privacy
protection.

</details>


### [156] [Enhancing Computational Cognitive Architectures with LLMs: A Case Study](https://arxiv.org/abs/2509.10972)
*Ron Sun*

Main category: cs.AI

TL;DR: 论文提出将Clarion认知架构与LLMs结合，利用LLMs的计算能力增强认知架构，同时保持心理真实性。


<details>
  <summary>Details</summary>
Motivation: 现有认知架构计算能力有限，而LLMs展现出更强的计算能力，结合两者可同时应对现实复杂性和心理真实性。

Method: 利用Clarion的隐式-显式二分法，实现Clarion与LLMs的无缝集成。

Result: 成功整合Clarion与LLMs，结合了LLMs的计算能力和Clarion的心理精细性。

Conclusion: 通过将Clarion认知架构与LLMs协同结合，成功将LLMs的计算能力与Clarion的心理精细性相结合，为处理现实世界复杂性和心理真实性提供了新途径。

Abstract: Computational cognitive architectures are broadly scoped models of the human
mind that combine different psychological functionalities (as well as often
different computational methods for these different functionalities) into one
unified framework. They structure them in a psychologically plausible and
validated way. However, such models thus far have only limited computational
capabilities, mostly limited by the computational tools and techniques that
were adopted. More recently, LLMs have proved to be more capable
computationally than any other tools. Thus, in order to deal with both
real-world complexity and psychological realism at the same time, incorporating
LLMs into cognitive architectures naturally becomes an important task. In the
present article, a synergistic combination of the Clarion cognitive
architecture and LLMs is discussed as a case study. The implicit-explicit
dichotomy that is fundamental to Clarion is leveraged for a seamless
integration of Clarion and LLMs. As a result, computational power of LLMs is
combined with psychological nicety of Clarion.

</details>


### [157] [Rethinking Human Preference Evaluation of LLM Rationales](https://arxiv.org/abs/2509.11026)
*Ziang Li,Manasi Ganti,Zixian Ma,Helena Vasconcelos,Qijia He,Ranjay Krishna*

Main category: cs.AI

TL;DR: 该论文提出了一种基于属性的评估方法，用于更细致地评估LLM生成的理性，揭示了更深入的模型比较和人类偏好背后的关键属性。


<details>
  <summary>Details</summary>
Motivation: 重新思考LLM生成的理性的偏好评估，以克服二元比较的局限性，并理解什么属性定义了好的理性。

Method: 通过自动指标、LLM判断和人工标注评估关键理性属性，并使用SHAP分析人类偏好数据集MT Bench和Chatbot Arena。

Result: 发现细粒度的属性评估可以提供更细致的模型比较和洞察。

Conclusion: 细粒度的属性评估能够更好地描述理性质量，并指导未来研究朝着更具解释性和可靠的评估实践发展。

Abstract: Large language models (LLMs) often generate natural language rationales --
free-form explanations that help improve performance on complex reasoning tasks
and enhance interpretability for human users. However, evaluating these
rationales remains challenging. While recent work has relied on binary
preference judgments from humans or LLM judges, such evaluations are often
opaque and coarse-grained, offering limited insight into what makes one
rationale better than another. In this work, we rethink preference evaluation
for LLM-generated rationales by asking: (1) What attributes define good
rationales? (2) Can human preferences be explained by these attributes? (3) Can
attribute-based evaluation overcome the limitations of binary comparisons? We
identify a set of key rationale attributes from prior literature and assess
them using automatic metrics, LLM judgments, and human annotations. We then
analyze two standard human preference datasets MT Bench and Chatbot Arena using
SHAP to identify which attributes best explain human preference outcomes.
Finally, we re-evaluate model-generated rationales using attribute-specific ELO
scores, revealing more nuanced model comparisons and insights. Our findings
suggest that fine-grained attribute evaluations can better characterize
rationale quality and guide future research toward more interpretable and
reliable evaluation practices.

</details>


### [158] [Free-MAD: Consensus-Free Multi-Agent Debate](https://arxiv.org/abs/2509.11035)
*Yu Cui,Hang Fu,Haibin Zhang,Licheng Wang,Cong Zuo*

Main category: cs.AI

TL;DR: \textsc{Free-MAD} 是一种无需共识的多智能体辩论框架，通过分数机制和反从众机制提升推理性能并减少开销。


<details>
  <summary>Details</summary>
Motivation: 现有MAD方法依赖多轮交互达成共识，存在token开销高、错误传播和多数投票不公平等问题。

Method: \textsc{Free-MAD} 引入了基于分数的决策机制，评估整个辩论轨迹，而非仅最后一轮，并采用反从众机制减少多数意见的过度影响。

Result: 在八个基准数据集上的实验表明，\textsc{Free-MAD} 显著提高了推理性能，同时仅需单轮辩论，降低了token成本。

Conclusion: \textsc{Free-MAD} 提出了一种无需达成共识的多智能体辩论框架，通过基于分数的决策机制和反从众机制，显著提高了推理性能并降低了token开销。

Abstract: Multi-agent debate (MAD) is an emerging approach to improving the reasoning
capabilities of large language models (LLMs). Existing MAD methods rely on
multiple rounds of interaction among agents to reach consensus, and the final
output is selected by majority voting in the last round. However, this
consensus-based design faces several limitations. First, multiple rounds of
communication increases token overhead and limits scalability. Second, due to
the inherent conformity of LLMs, agents that initially produce correct
responses may be influenced by incorrect ones during the debate process,
causing error propagation. Third, majority voting introduces randomness and
unfairness in the decision-making phase, and can degrade the reasoning
performance.
  To address these issues, we propose \textsc{Free-MAD}, a novel MAD framework
that eliminates the need for consensus among agents. \textsc{Free-MAD}
introduces a novel score-based decision mechanism that evaluates the entire
debate trajectory rather than relying on the last round only. This mechanism
tracks how each agent's reasoning evolves, enabling more accurate and fair
outcomes. In addition, \textsc{Free-MAD} reconstructs the debate phase by
introducing anti-conformity, a mechanism that enables agents to mitigate
excessive influence from the majority. Experiments on eight benchmark datasets
demonstrate that \textsc{Free-MAD} significantly improves reasoning performance
while requiring only a single-round debate and thus reducing token costs. We
also show that compared to existing MAD approaches, \textsc{Free-MAD} exhibits
improved robustness in real-world attack scenarios.

</details>


### [159] [Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and Orchestration](https://arxiv.org/abs/2509.11067)
*Liangxuan Guo,Bin Zhu,Qingqian Tao,Kangning Liu,Xun Zhao,Xianzhe Qin,Jin Gao,Guangfu Hao*

Main category: cs.AI

TL;DR: 	extsc{Agentic Lybic}通过FSM动态协调多代理系统，显著提升复杂桌面自动化任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有自主代理在多步骤复杂任务中因协调和质量控制不足而表现不佳，需要更灵活和可靠的解决方案。

Method: 系统由Controller、Manager、三个Worker（Technician、Operator、Analyst）和Evaluator组成，通过FSM路由实现动态任务分配和错误恢复。

Result: 在OSWorld基准测试中，	extsc{Agentic Lybic}以57.07%的成功率在50步内显著优于现有方法。

Conclusion: 	extsc{Agentic Lybic}通过有限状态机（FSM）的动态协调和持续质量控制，显著提升了复杂桌面自动化任务的可靠性和成功率。

Abstract: Autonomous agents for desktop automation struggle with complex multi-step
tasks due to poor coordination and inadequate quality control. We introduce
\textsc{Agentic Lybic}, a novel multi-agent system where the entire
architecture operates as a finite-state machine (FSM). This core innovation
enables dynamic orchestration. Our system comprises four components: a
Controller, a Manager, three Workers (Technician for code-based operations,
Operator for GUI interactions, and Analyst for decision support), and an
Evaluator. The critical mechanism is the FSM-based routing between these
components, which provides flexibility and generalization by dynamically
selecting the optimal execution strategy for each subtask. This principled
orchestration, combined with robust quality gating, enables adaptive replanning
and error recovery. Evaluated officially on the OSWorld benchmark,
\textsc{Agentic Lybic} achieves a state-of-the-art 57.07\% success rate in 50
steps, substantially outperforming existing methods. Results demonstrate that
principled multi-agent orchestration with continuous quality control provides
superior reliability for generalized desktop automation in complex computing
environments.

</details>


### [160] [Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability](https://arxiv.org/abs/2509.11068)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 本文提出了一种验证框架，通过概率性审计确保LLM输出的真实性，验证效率高且可调，为负责任AI和多智能体系统研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决动态多智能体系统中计算信任的挑战，即如何验证一个智能体的输出确实由声称的LLM生成，而非伪造或由廉价/劣质模型生成。

Method: 基于确定性可复制性原则，构建了一个验证框架，允许多个验证者概率性地审计LLM输出的随机小片段，并有效分配验证工作负载。

Result: 模拟显示，目标验证比完全重新生成快12倍以上，且检测概率可调。

Conclusion: 本研究提出了一个可验证的LLM系统框架，为负责任AI奠定了基础，并为未来复杂异构多智能体系统的研究提供了基石。

Abstract: The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic,
multi-agent systems. This introduces a fundamental challenge in establishing
computational trust, specifically how one agent can verify that another's
output was genuinely produced by a claimed LLM, and not falsified or generated
by a cheaper or inferior model. To address this challenge, this paper proposes
a verification framework that achieves tractable asymmetric effort, where the
cost to verify a computation is substantially lower than the cost to perform
it. Our approach is built upon the principle of deterministic replicability, a
property inherent to autoregressive models that strictly necessitates a
computationally homogeneous environment where all agents operate on identical
hardware and software stacks. Within this defined context, our framework
enables multiple validators to probabilistically audit small, random segments
of an LLM's output and it distributes the verification workload effectively.
The simulations demonstrated that targeted verification can be over 12 times
faster than full regeneration, with tunable parameters to adjust the detection
probability. By establishing a tractable mechanism for auditable LLM systems,
our work offers a foundational layer for responsible AI and serves as a
cornerstone for future research into the more complex, heterogeneous
multi-agent systems.

</details>


### [161] [Patient-Zero: A Unified Framework for Real-Record-Free Patient Agent Generation](https://arxiv.org/abs/2509.11078)
*Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: Patient-Zero框架通过无真实医疗记录的多步生成和动态更新机制，解决了现有方法的局限性，显著提升了虚拟患者的生成质量和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要利用LLMs重写和补充现有医疗记录，但仍存在数据隐私、准确性、多样性不足及缺乏真实患者交互能力的问题。

Method: 提出了一个多步生成架构，通过分层次注入医学知识生成全面的患者记录，并设计了动态更新机制以优化交互能力。

Result: 实验结果表明，模型在准确性、多样性和一致性方面表现良好，生成的虚拟患者显著提升了现有模型在MedQA数据集上的性能。

Conclusion: Patient-Zero框架通过无真实医疗记录的生成方式，解决了数据隐私、准确性和多样性的问题，并提升了虚拟患者的交互能力。实验证明其在MedQA数据集上显著提升了模型性能。

Abstract: Synthetic data generation using large language models (LLMs) has emerged as a
promising solution across various domains, particularly in medical field, to
mitigate data collection challenges. However, existing studies mainly utilize
LLMs to rewrite and complete existing medical records, where the limitations in
data privacy, accuracy, and diversity sill exist, and additionally lack the
ability to interact like real patients. To address these issues, we propose a
realistic patient generation framework, Patient-Zero, which requires no real
medical records. Patient-Zero first introduces a medically-aligned multi-step
generation architecture, which builds comprehensive patient records through
hierarchical medical knowledge injection without real medical records. Then, to
optimize the virtual patient's interaction abilities with humans, Patient-Zero
designs a dynamic updating mechanism to improve the consistency and
conversational performance. Our framework enables the generation of
contextually diverse patient records while maintaining strict medical
coherence, supported by adaptive dialogue strategies and real-time clinical
plausibility verification. Experimental results demonstrate that our model
achieves good performance in accuracy, diversity, and consistency. After
training with our generated virtual patients, existing models show significant
improvements on the MedQA dataset.

</details>


### [162] [Difficulty-Aware Agent Orchestration in LLM-Powered Workflows](https://arxiv.org/abs/2509.11079)
*Jinwei Su,Yinghui Xia,Qizhen Lan,Xinyuan Song,Yang Jingsong,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: DAAO是一个动态框架，通过难度感知的工作流调整和异构LLM的智能分配，优化了多代理系统的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有多代理框架常依赖静态或任务级工作流，无法灵活处理简单与复杂查询，且忽视了异构LLM的效率-性能权衡。

Method: DAAO包含三个模块：变分自编码器（VAE）用于难度估计、模块化操作符分配器和成本性能感知的LLM路由器。

Result: DAAO在六个基准测试中均优于现有系统，实现了更高的准确性和推理效率。

Conclusion: DAAO通过动态调整工作流深度、操作符选择和LLM分配，显著提升了多代理系统的准确性和推理效率。

Abstract: Large Language Model (LLM)-based agentic systems have shown strong
capabilities across various tasks. However, existing multi-agent frameworks
often rely on static or task-level workflows, which either over-process simple
queries or underperform on complex ones, while also neglecting the
efficiency-performance trade-offs across heterogeneous LLMs. To address these
limitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a
dynamic framework that adapts workflow depth, operator selection, and LLM
assignment based on the difficulty of each input query. DAAO comprises three
interdependent modules: a variational autoencoder (VAE) for difficulty
estimation, a modular operator allocator, and a cost- and performance-aware LLM
router. By leveraging heterogeneous LLMs and dynamically tailoring workflows,
DAAO enables fine-grained, query-specific reasoning strategies. DAAO
outperforms prior multi-agent systems in both accuracy and inference efficiency
across six benchmarks. We will release our code and implementation details upon
publication.

</details>


### [163] [Neural cellular automata: applications to biology and beyond classical AI](https://arxiv.org/abs/2509.11131)
*Benedikt Hartl,Michael Levin,Léo Pio-Lopez*

Main category: cs.AI

TL;DR: NCA是一个结合生物自组织和现代AI的框架，通过局部交互实现系统级协调，适用于多尺度生物学和生成AI。


<details>
  <summary>Details</summary>
Motivation: NCA是一个强大的框架，用于建模生物自组织，通过可训练的、可微分（或可进化）的更新规则来捕捉生命物质的适应性自我调节动态。

Method: 通过将人工神经网络（ANNs）作为局部决策中心和局部代理之间的交互规则嵌入，NCA可以模拟从分子到系统级尺度的过程。

Result: NCA不仅能够再现生物启发的目标模式，还能在新条件下泛化，表现出对扰动的鲁棒性以及开放式适应和推理的能力。

Conclusion: NCA提供了一个计算效率高的统一范式，不仅将多尺度生物学的基本见解与现代生成AI联系起来，还有潜力设计出真正具有生物启发性的集体智能，能够进行层次推理和控制。

Abstract: Neural Cellular Automata (NCA) represent a powerful framework for modeling
biological self-organization, extending classical rule-based systems with
trainable, differentiable (or evolvable) update rules that capture the adaptive
self-regulatory dynamics of living matter. By embedding Artificial Neural
Networks (ANNs) as local decision-making centers and interaction rules between
localized agents, NCA can simulate processes across molecular, cellular,
tissue, and system-level scales, offering a multiscale competency architecture
perspective on evolution, development, regeneration, aging, morphogenesis, and
robotic control. These models not only reproduce biologically inspired target
patterns but also generalize to novel conditions, demonstrating robustness to
perturbations and the capacity for open-ended adaptation and reasoning. Given
their immense success in recent developments, we here review current literature
of NCAs that are relevant primarily for biological or bioengineering
applications. Moreover, we emphasize that beyond biology, NCAs display robust
and generalizing goal-directed dynamics without centralized control, e.g., in
controlling or regenerating composite robotic morphologies or even on
cutting-edge reasoning tasks such as ARC-AGI-1. In addition, the same
principles of iterative state-refinement is reminiscent to modern generative
Artificial Intelligence (AI), such as probabilistic diffusion models. Their
governing self-regulatory behavior is constraint to fully localized
interactions, yet their collective behavior scales into coordinated
system-level outcomes. We thus argue that NCAs constitute a unifying
computationally lean paradigm that not only bridges fundamental insights from
multiscale biology with modern generative AI, but have the potential to design
truly bio-inspired collective intelligence capable of hierarchical reasoning
and control.

</details>


### [164] [AlignKT: Explicitly Modeling Knowledge State for Knowledge Tracing with Ideal State Alignment](https://arxiv.org/abs/2509.11135)
*Jing Xiao,Chang You,Zhiyu Chen*

Main category: cs.AI

TL;DR: AlignKT通过显式建模知识状态并引入教育学理论作为对齐标准，提升了知识追踪的解释性和性能，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型过于关注学习者的交互序列拟合，而忽视知识状态本身，导致解释性和教学支持不足。

Method: 采用前端到后端架构，定义理想知识状态为对齐标准，使用五个编码器实现，并引入对比学习模块增强对齐过程的鲁棒性。

Result: 在三个真实数据集上，AlignKT优于七个基线模型，其中两个数据集上达到最先进水平，第三个数据集表现具有竞争力。

Conclusion: AlignKT通过前端到后端架构显式建模稳定的知识状态，结合教育学理论定义的理想知识状态作为对齐标准，显著提升了知识追踪模型的解释性和教学支持能力。

Abstract: Knowledge Tracing (KT) serves as a fundamental component of Intelligent
Tutoring Systems (ITS), enabling these systems to monitor and understand
learners' progress by modeling their knowledge state. However, many existing KT
models primarily focus on fitting the sequences of learners' interactions, and
often overlook the knowledge state itself. This limitation leads to reduced
interpretability and insufficient instructional support from the ITS. To
address this challenge, we propose AlignKT, which employs a frontend-to-backend
architecture to explicitly model a stable knowledge state. In this approach,
the preliminary knowledge state is aligned with an additional criterion.
Specifically, we define an ideal knowledge state based on pedagogical theories
as the alignment criterion, providing a foundation for interpretability. We
utilize five encoders to implement this set-up, and incorporate a contrastive
learning module to enhance the robustness of the alignment process. Through
extensive experiments, AlignKT demonstrates superior performance, outperforming
seven KT baselines on three real-world datasets. It achieves state-of-the-art
results on two of these datasets and exhibits competitive performance on the
third. The code of this work is available at
https://github.com/SCNU203/AlignKT.

</details>


### [165] [AI-Generated Content in Cross-Domain Applications: Research Trends, Challenges and Propositions](https://arxiv.org/abs/2509.11151)
*Jianxin Li,Liang Qu,Taotao Cai,Zhixue Zhao,Nur Al Hasan Haldar,Aneesh Krishna,Xiangjie Kong,Flavio Romero Macau,Tanmoy Chakraborty,Aniket Deroy,Binshan Lin,Karen Blackmore,Nasimul Noman,Jingxian Cheng,Ningning Cui,Jianliang Xu*

Main category: cs.AI

TL;DR: 本文通过跨领域视角总结了AIGC的最新进展、社会影响及技术挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管AIGC在各领域显示出高效的内容创造和信息传递能力，但缺乏对其最新进展和新兴挑战的跨领域研究。

Method: 汇集16位多学科学者，提供跨领域的AIGC趋势与挑战分析，包括生成式AI训练技术、检测方法、AI生成内容的传播与使用，以及社会影响和现有方法回顾。

Result: 本文提供了AIGC的广泛概述、社会影响分析及技术挑战，并提出了未来研究建议。

Conclusion: 本文通过跨领域的视角总结了AIGC的最新进展、社会影响及技术挑战，并提出了未来研究方向，为读者提供了全面的AIGC研究趋势和未来方向。

Abstract: Artificial Intelligence Generated Content (AIGC) has rapidly emerged with the
capability to generate different forms of content, including text, images,
videos, and other modalities, which can achieve a quality similar to content
created by humans. As a result, AIGC is now widely applied across various
domains such as digital marketing, education, and public health, and has shown
promising results by enhancing content creation efficiency and improving
information delivery. However, there are few studies that explore the latest
progress and emerging challenges of AIGC across different domains. To bridge
this gap, this paper brings together 16 scholars from multiple disciplines to
provide a cross-domain perspective on the trends and challenges of AIGC.
Specifically, the contributions of this paper are threefold: (1) It first
provides a broader overview of AIGC, spanning the training techniques of
Generative AI, detection methods, and both the spread and use of AI-generated
content across digital platforms. (2) It then introduces the societal impacts
of AIGC across diverse domains, along with a review of existing methods
employed in these contexts. (3) Finally, it discusses the key technical
challenges and presents research propositions to guide future work. Through
these contributions, this vision paper seeks to offer readers a cross-domain
perspective on AIGC, providing insights into its current research trends,
ongoing challenges, and future directions.

</details>


### [166] [VideoAgent: Personalized Synthesis of Scientific Videos](https://arxiv.org/abs/2509.11253)
*Xiao Liang,Bangxin Li,Zixuan Chen,Hanyue Zheng,Zhi Ma,Di Wang,Cong Tian,Quan Wang*

Main category: cs.AI

TL;DR: VideoAgent是一个多智能体框架，通过解析论文和用户需求生成个性化科学视频，性能优于现有商业服务，接近人类水平。


<details>
  <summary>Details</summary>
Motivation: 自动化生成科学视频对于知识传播至关重要，但现有工作主要关注静态媒体，缺乏个性化动态编排和多模态内容同步机制。

Method: VideoAgent解析源论文为细粒度资产库，并通过用户需求引导，编排叙事流，结合静态幻灯片和动态动画来解析复杂概念。同时，提出SciVidEval评估套件，结合自动化指标和基于视频测验的人类评估。

Result: 实验表明，VideoAgent在科学视频生成任务中显著优于现有商业服务，并在知识传播质量上接近人类水平。

Conclusion: VideoAgent通过其多智能体框架和个性化动态编排能力，显著优于现有商业科学视频生成服务，并在科学传播中接近人类水平的质量。

Abstract: Automating the generation of scientific videos is a crucial yet challenging
task for effective knowledge dissemination. However, existing works on document
automation primarily focus on static media such as posters and slides, lacking
mechanisms for personalized dynamic orchestration and multimodal content
synchronization. To address these challenges, we introduce VideoAgent, a novel
multi-agent framework that synthesizes personalized scientific videos through a
conversational interface. VideoAgent parses a source paper into a fine-grained
asset library and, guided by user requirements, orchestrates a narrative flow
that synthesizes both static slides and dynamic animations to explain complex
concepts. To enable rigorous evaluation, we also propose SciVidEval, the first
comprehensive suite for this task, which combines automated metrics for
multimodal content quality and synchronization with a Video-Quiz-based human
evaluation to measure knowledge transfer. Extensive experiments demonstrate
that our method significantly outperforms existing commercial scientific video
generation services and approaches human-level quality in scientific
communication.

</details>


### [167] [Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble](https://arxiv.org/abs/2509.11311)
*Bingchen Wang,Zi-Yu Khoo,Bryan Kian Hsiang Low*

Main category: cs.AI

TL;DR: 论文提出P2P框架，利用LLM代理模拟人类调查受访者，通过两阶段对齐方法（角色构建和代表性选择）实现高保真的响应复现和多样性，无需依赖人口统计条件。


<details>
  <summary>Details</summary>
Motivation: 解决社会科学中调查部署成本上升和调查响应数据人口统计失衡的问题，提供一种成本效益高且可操控的解决方案。

Method: 论文提出了一种两阶段对齐方法：首先构建多样化的代理角色（称为endowments），模拟可能的受访者档案；然后基于观察数据选择一个代表性子集以近似真实人群。P2P系统通过结构化提示工程、熵采样和回归选择实现这一目标。

Result: 实验表明，对齐的代理群体能够高保真地复现实世界意见调查数据集的聚合响应模式，并展现出显著的响应多样性。

Conclusion: 该论文提出的P2P框架通过结构化的提示工程、基于熵的采样和回归选择，成功实现了LLM代理的多样化行为模式对齐，无需依赖人口统计条件即可高保真地复现真实调查数据的聚合响应模式。

Abstract: Large language models (LLMs) have demonstrated promise in emulating
human-like responses across a wide range of tasks. In this paper, we propose a
novel alignment framework that treats LLMs as agent proxies for human survey
respondents, affording a cost-effective and steerable solution to two pressing
challenges in the social sciences: the rising cost of survey deployment and the
growing demographic imbalance in survey response data. Drawing inspiration from
the theory of revealed preference, we formulate alignment as a two-stage
problem: constructing diverse agent personas called endowments that simulate
plausible respondent profiles, and selecting a representative subset to
approximate a ground-truth population based on observed data. To implement the
paradigm, we introduce P2P, a system that steers LLM agents toward
representative behavioral patterns using structured prompt engineering,
entropy-based sampling, and regression-based selection. Unlike
personalization-heavy approaches, our alignment approach is
demographic-agnostic and relies only on aggregate survey results, offering
better generalizability and parsimony. Beyond improving data efficiency in
social science research, our framework offers a testbed for studying the
operationalization of pluralistic alignment. We demonstrate the efficacy of our
approach on real-world opinion survey datasets, showing that our aligned agent
populations can reproduce aggregate response patterns with high fidelity and
exhibit substantial response diversity, even without demographic conditioning.

</details>


### [168] [Decoding Plastic Toxicity: An Intelligent Framework for Conflict-Aware Relational Metapath Extraction from Scientific Abstracts](https://arxiv.org/abs/2509.11330)
*Sudeshna Jana,Manjira Sinha,Tirthankar Dasgupta*

Main category: cs.AI

TL;DR: 论文提出利用大型语言模型构建关系元路径和毒性轨迹图，有效追踪污染物传播及其健康影响，并通过动态证据协调确保知识可靠性。


<details>
  <summary>Details</summary>
Motivation: 塑料及其衍生物在环境中的广泛存在和持久性导致了微塑料和纳米塑料的积累，对人类健康构成严重威胁，需要一种方法来系统追踪污染物来源与健康影响之间的关系。

Method: 框架结合了大型语言模型来提取关系元路径，构建多跳语义链，并通过动态证据协调模块解决语义冲突。

Result: 该方法在从嘈杂的科学文本中提取可靠、高实用性的关系知识方面表现出色，并提供了挖掘特定领域语料库中复杂因果结构的可扩展解决方案。

Conclusion: 该论文提出了一种利用大型语言模型提取关系元路径的新框架，能够有效构建污染物传播的毒性轨迹图，并通过动态证据协调模块确保知识的可靠性和一致性。

Abstract: The widespread use of plastics and their persistence in the environment have
led to the accumulation of micro- and nano-plastics across air, water, and
soil, posing serious health risks including respiratory, gastrointestinal, and
neurological disorders. We propose a novel framework that leverages large
language models to extract relational metapaths, multi-hop semantic chains
linking pollutant sources to health impacts, from scientific abstracts. Our
system identifies and connects entities across diverse contexts to construct
structured relational metapaths, which are aggregated into a Toxicity
Trajectory Graph that traces pollutant propagation through exposure routes and
biological systems. Moreover, to ensure consistency and reliability, we
incorporate a dynamic evidence reconciliation module that resolves semantic
conflicts arising from evolving or contradictory research findings. Our
approach demonstrates strong performance in extracting reliable, high-utility
relational knowledge from noisy scientific text and offers a scalable solution
for mining complex cause-effect structures in domain-specific corpora.

</details>


### [169] [The power of dynamic causality in observer-based design for soft sensor applications](https://arxiv.org/abs/2509.11336)
*William Farlessyost,Sebastian Oberst,Shweta Singh*

Main category: cs.AI

TL;DR: 该论文提出了一种基于LTC网络的动态因果分析框架，优化软传感器选择，显著提高预测准确性和解释性，适用于多个工程和生态领域。


<details>
  <summary>Details</summary>
Motivation: 传统传感器选择方法依赖于线性化可观测性指数或统计相关性，无法捕捉复杂系统的时序演化，因此需要一种更动态的方法。

Method: 使用液体时间常数（LTC）网络进行动态因果分析，通过迭代训练和扰动分析来识别和修剪对状态估计因果影响最小的传感器输入。

Result: 在三个不同物理领域的测试平台上，该方法一致识别出最小传感器集，提高了预测准确性，并区分了物理测量与噪声。

Conclusion: 该论文提出的基于动态因果分析的框架在优化基于观测器的软传感器方面表现出色，不仅提高了预测准确性，还增强了传感器选择的解释性，适用于多个领域。

Abstract: This paper introduces a novel framework for optimizing observer-based soft
sensors through dynamic causality analysis. Traditional approaches to sensor
selection often rely on linearized observability indices or statistical
correlations that fail to capture the temporal evolution of complex systems. We
address this gap by leveraging liquid-time constant (LTC) networks,
continuous-time neural architectures with input-dependent time constants, to
systematically identify and prune sensor inputs with minimal causal influence
on state estimation. Our methodology implements an iterative workflow: training
an LTC observer on candidate inputs, quantifying each input's causal impact
through controlled perturbation analysis, removing inputs with negligible
effect, and retraining until performance degradation occurs. We demonstrate
this approach on three mechanistic testbeds representing distinct physical
domains: a harmonically forced spring-mass-damper system, a nonlinear
continuous stirred-tank reactor, and a predator-prey model following the
structure of the Lotka-Volterra model, but with seasonal forcing and added
complexity. Results show that our causality-guided pruning consistently
identifies minimal sensor sets that align with underlying physics while
improving prediction accuracy. The framework automatically distinguishes
essential physical measurements from noise and determines when derived
interaction terms provide complementary versus redundant information. Beyond
computational efficiency, this approach enhances interpretability by grounding
sensor selection decisions in dynamic causal relationships rather than static
correlations, offering significant benefits for soft sensing applications
across process engineering, ecological monitoring, and agricultural domains.

</details>


### [170] [MAPGD: Multi-Agent Prompt Gradient Descent for Collaborative Prompt Optimization](https://arxiv.org/abs/2509.11361)
*Yichen Han,Bojun Liu,Zhengpeng zhou,Guanyu Liu,Zeng Zhang,Yang Yang,Wenli Wang,Isaac N Shi,Yunyan,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: MAPGD 是一种多智能体协作的提示优化框架，通过梯度融合和冲突解决提升效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法通常依赖单一优化轨迹，导致适应性差、效率低、视角狭窄、梯度冲突和高计算成本。

Method: MAPGD 结合了多智能体协作与基于梯度的优化，包括任务清晰化、示例选择、格式设计和风格优化的专门智能体；语义梯度协调解决冲突；基于多臂老虎机的候选选择实现高效探索-开发平衡；并提供理论收敛保证。

Result: 实验表明，MAPGD 在分类、生成和推理任务上优于单智能体和随机基线，消融实验证实了梯度融合、智能体专业化和冲突解决的有效性。

Conclusion: MAPGD 提供了一个统一的、基于梯度的多智能体框架，用于稳健且可解释的提示优化，通过实验验证了其在准确性和效率上的优势。

Abstract: Prompt engineering is crucial for leveraging large language models (LLMs),
but existing methods often rely on a single optimization trajectory, limiting
adaptability and efficiency while suffering from narrow perspectives, gradient
conflicts, and high computational cost. We propose MAPGD (Multi-Agent Prompt
Gradient Descent), a framework integrating multi-agent collaboration with
gradient-based optimization. MAPGD features specialized agents for task
clarity, example selection, format design, and stylistic refinement; semantic
gradient coordination to resolve conflicts; bandit-based candidate selection
for efficient exploration-exploitation; and theoretical convergence guarantees.
Experiments on classification, generation, and reasoning tasks show MAPGD
outperforms single-agent and random baselines in accuracy and efficiency.
Ablations confirm the benefits of gradient fusion, agent specialization, and
conflict resolution, providing a unified, gradient-inspired multi-agent
approach to robust and interpretable prompt optimization.

</details>


### [171] [Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications](https://arxiv.org/abs/2509.11431)
*Aadil Gani Ganie*

Main category: cs.AI

TL;DR: 本论文提出将RBAC集成到AI代理中，以解决即时注入攻击等安全威胁，支持AI代理的安全和可扩展部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然在各领域有广泛应用，但其静态训练数据和通用性限制了其性能，而AI代理通过访问外部工具和实时数据缓解了部分限制。然而，AI代理仍面临安全威胁，如即时注入攻击。

Method: 通过将RBAC集成到AI代理中，为AI代理提供安全防护措施，特别是针对即时注入攻击等安全威胁。

Result: 提出的RBAC框架为AI代理提供了安全保护，增强了其在工业设置中的可靠性和完整性。

Conclusion: 本论文提出了一个集成基于角色的访问控制（RBAC）到AI代理的框架，旨在为AI代理提供强大的安全保障，支持其有效且可扩展的部署。

Abstract: The emergence of Large Language Models (LLMs) has significantly advanced
solutions across various domains, from political science to software
development. However, these models are constrained by their training data,
which is static and limited to information available up to a specific date.
Additionally, their generalized nature often necessitates fine-tuning --
whether for classification or instructional purposes -- to effectively perform
specific downstream tasks. AI agents, leveraging LLMs as their core, mitigate
some of these limitations by accessing external tools and real-time data,
enabling applications such as live weather reporting and data analysis. In
industrial settings, AI agents are transforming operations by enhancing
decision-making, predictive maintenance, and process optimization. For example,
in manufacturing, AI agents enable near-autonomous systems that boost
productivity and support real-time decision-making. Despite these advancements,
AI agents remain vulnerable to security threats, including prompt injection
attacks, which pose significant risks to their integrity and reliability. To
address these challenges, this paper proposes a framework for integrating
Role-Based Access Control (RBAC) into AI agents, providing a robust security
guardrail. This framework aims to support the effective and scalable deployment
of AI agents, with a focus on on-premises implementations.

</details>


### [172] [Knowledge-Guided Adaptive Mixture of Experts for Precipitation Prediction](https://arxiv.org/abs/2509.11459)
*Chen Jiang,Kofi Osei,Sai Deepthi Yeddula,Dongji Feng,Wei-Shinn Ku*

Main category: cs.AI

TL;DR: 提出自适应混合专家模型，通过动态路由整合多源异构数据，显著提升降水预测精度，并开发交互式可视化工具支持决策。


<details>
  <summary>Details</summary>
Motivation: 降水预测在农业、灾害管理和可持续策略中至关重要，但多源数据的异构性和复杂性给传统深度学习模型带来挑战。

Method: 采用自适应混合专家模型，每个专家专注于特定模态或时空模式，并引入动态路由器分配输入。

Result: 模型在2022年飓风伊恩的真实多模态气候数据集上表现优异，显著超越基线方法。

Conclusion: 提出的自适应混合专家模型在降水率预测中显著优于基线方法，并通过交互式可视化工具支持决策，为气候敏感领域提供了实用解决方案。

Abstract: Accurate precipitation forecasting is indispensable in agriculture, disaster
management, and sustainable strategies. However, predicting rainfall has been
challenging due to the complexity of climate systems and the heterogeneous
nature of multi-source observational data, including radar, satellite imagery,
and surface-level measurements. The multi-source data vary in spatial and
temporal resolution, and they carry domain-specific features, making it
challenging for effective integration in conventional deep learning models.
Previous research has explored various machine learning techniques for weather
prediction; however, most struggle with the integration of data with
heterogeneous modalities. To address these limitations, we propose an Adaptive
Mixture of Experts (MoE) model tailored for precipitation rate prediction. Each
expert within the model specializes in a specific modality or spatio-temporal
pattern. We also incorporated a dynamic router that learns to assign inputs to
the most relevant experts. Our results show that this modular design enhances
predictive accuracy and interpretability. In addition to the modeling
framework, we introduced an interactive web-based visualization tool that
enables users to intuitively explore historical weather patterns over time and
space. The tool was designed to support decision-making for stakeholders in
climate-sensitive sectors. We evaluated our approach using a curated multimodal
climate dataset capturing real-world conditions during Hurricane Ian in 2022.
The benchmark results show that the Adaptive MoE significantly outperformed all
the baselines.

</details>


### [173] [Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs](https://arxiv.org/abs/2509.11480)
*Amir Taherin,Juyi Lin,Arash Akbari,Arman Akbari,Pu Zhao,Weiwei Chen,David Kaeli,Yanzhi Wang*

Main category: cs.AI

TL;DR: 该研究评估了五种VLA模型在不同硬件平台上的性能，发现架构选择和功耗限制对性能有显著影响，并挑战了数据中心硬件在机器人推理中的优越性假设。


<details>
  <summary>Details</summary>
Motivation: 尽管VLA模型已成为机器人控制的强大通用策略，但其在不同模型架构和硬件平台上的性能扩展以及相关功耗预算仍知之甚少。

Method: 通过使用LIBERO基准测试，评估了五种代表性的VLA模型，包括最先进的基线和两种新提出的架构，针对边缘和数据中心GPU平台。测量了准确性以及系统级指标，如延迟、吞吐量和峰值内存使用情况。

Result: 研究发现了不同的扩展趋势：1）架构选择（如动作标记化和模型主干大小）强烈影响吞吐量和内存占用；2）受功耗限制的边缘设备表现出非线性性能下降，某些配置甚至匹配或超过旧的数据中心GPU；3）可以在不显著损失准确性的情况下实现高吞吐量变体。

Conclusion: 该研究挑战了当前关于数据中心硬件在机器人推理中优越性的假设，并提供了在不同部署约束下选择和优化VLA模型的可操作见解。

Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist
policies for robotic control, yet their performance scaling across model
architectures and hardware platforms, as well as their associated power
budgets, remain poorly understood. This work presents an evaluation of five
representative VLA models -- spanning state-of-the-art baselines and two newly
proposed architectures -- targeting edge and datacenter GPU platforms. Using
the LIBERO benchmark, we measure accuracy alongside system-level metrics,
including latency, throughput, and peak memory usage, under varying edge power
constraints and high-performance datacenter GPU configurations. Our results
identify distinct scaling trends: (1) architectural choices, such as action
tokenization and model backbone size, strongly influence throughput and memory
footprint; (2) power-constrained edge devices exhibit non-linear performance
degradation, with some configurations matching or exceeding older datacenter
GPUs; and (3) high-throughput variants can be achieved without significant
accuracy loss. These findings provide actionable insights when selecting and
optimizing VLAs across a range of deployment constraints. Our work challenges
current assumptions about the superiority of datacenter hardware for robotic
inference.

</details>


### [174] [MedicalOS: An LLM Agent based Operating System for Digital Healthcare](https://arxiv.org/abs/2509.11507)
*Jared Zhu,Junde Wu*

Main category: cs.AI

TL;DR: MedicalOS是一个基于代理的操作系统，旨在通过自然语言指令简化临床工作流程，实证验证显示其高效且可靠。


<details>
  <summary>Details</summary>
Motivation: 当前的数字健康技术系统难以学习和使用，临床医生面临管理多个工具、重复手动操作、导航复杂UI树的负担，花费大量时间在管理而非患者护理上。基于大型语言模型（LLM）的代理展示了在编码和计算机操作方面的卓越能力，揭示了通过自然语言指令代理与操作系统交互的潜力。

Method: 提出了MedicalOS，一个统一的基于代理的操作系统，作为医疗保健领域的特定抽象层。它将人类指令翻译为预定义的数字化医疗命令，如患者查询、历史检索、检查管理等，这些命令通过机器语言（如Python、API、MCP、Linux）封装为现成工具。

Result: 在214个患者案例和22个专科中实证验证了MedicalOS，展示了高诊断准确性和信心、临床合理的检查请求以及一致的结构化报告和药物推荐生成。

Conclusion: MedicalOS被证明是一个可信赖且可扩展的基础，用于推进临床实践中的工作流程自动化。

Abstract: Decades' advances in digital health technologies, such as electronic health
records, have largely streamlined routine clinical processes. Yet, most these
systems are still hard to learn and use: Clinicians often face the burden of
managing multiple tools, repeating manual actions for each patient, navigating
complicated UI trees to locate functions, and spending significant time on
administration instead of caring for patients. The recent rise of large
language model (LLM) based agents demonstrates exceptional capability in coding
and computer operation, revealing the potential for humans to interact with
operating systems and software not by direct manipulation, but by instructing
agents through natural language. This shift highlights the need for an
abstraction layer, an agent-computer interface, that translates human language
into machine-executable commands. In digital healthcare, however, requires a
more domain-specific abstractions that strictly follow trusted clinical
guidelines and procedural standards to ensure safety, transparency, and
compliance. To address this need, we present \textbf{MedicalOS}, a unified
agent-based operational system designed as such a domain-specific abstract
layer for healthcare. It translates human instructions into pre-defined digital
healthcare commands, such as patient inquiry, history retrieval, exam
management, report generation, referrals, treatment planning, that we wrapped
as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP,
Linux). We empirically validate MedicalOS on 214 patient cases across 22
specialties, demonstrating high diagnostic accuracy and confidence, clinically
sound examination requests, and consistent generation of structured reports and
medication recommendations. These results highlight MedicalOS as a trustworthy
and scalable foundation for advancing workflow automation in clinical practice.

</details>


### [175] [Task Decoding based on Eye Movements using Synthetic Data Augmentation](https://arxiv.org/abs/2509.11547)
*Shanmuka Sadhu,Arca Baran,Preeti Pandey,Ayush Kumar*

Main category: cs.AI

TL;DR: 本文通过合成数据增强显著提高了眼动数据的任务解码准确率，验证了Yarbus的假设。


<details>
  <summary>Details</summary>
Motivation: 验证Yarbus的假设，即通过眼动数据可以解码观察者的任务，并探索如何通过合成数据增强提高解码准确率。

Method: 使用CTGAN及其变体（如CopulaGAN和Gretel AI）生成合成数据样本，并结合传统机器学习算法（如随机森林和Inception Time）进行任务解码。

Result: 通过增加合成数据，任务解码准确率从28.1%（随机森林）提升至82%（Inception Time），显著优于现有研究。

Conclusion: 通过结合合成数据生成器和传统机器学习算法，本文显著提高了基于眼动数据的任务解码准确率，验证了Yarbus的假设。

Abstract: Machine learning has been extensively used in various applications related to
eye-tracking research. Understanding eye movement is one of the most
significant subsets of eye-tracking research that reveals the scanning pattern
of an individual. Researchers have thoroughly analyzed eye movement data to
understand various eye-tracking applications, such as attention mechanisms,
navigational behavior, task understanding, etc. The outcome of traditional
machine learning algorithms used for decoding tasks based on eye movement data
has received a mixed reaction to Yarbus' claim that it is possible to decode
the observer's task from their eye movements. In this paper, to support the
hypothesis by Yarbus, we are decoding tasks categories while generating
synthetic data samples using well-known Synthetic Data Generators CTGAN and its
variations such as CopulaGAN and Gretel AI Synthetic Data generators on
available data from an in-person user study. Our results show that augmenting
more eye movement data combined with additional synthetically generated
improves classification accuracy even with traditional machine learning
algorithms. We see a significant improvement in task decoding accuracy from
28.1% using Random Forest to 82% using Inception Time when five times more data
is added in addition to the 320 real eye movement dataset sample. Our proposed
framework outperforms all the available studies on this dataset because of the
use of additional synthetic datasets. We validated our claim with various
algorithms and combinations of real and synthetic data to show how decoding
accuracy increases with the increase in the augmentation of generated data to
real data.

</details>


### [176] [Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain](https://arxiv.org/abs/2509.11572)
*Tuan Bui,An Nguyen,Phat Thai,Minh Hua,Ngan Pham L. N.,Ngan Pham T. B.,Dung Le,Long Nguyen,Thanh-Tung Tran,Thang Bui,Tho Quan*

Main category: cs.AI

TL;DR: MCFR框架整合LLMs与模型检查，提升封闭领域QA的推理忠实性，验证其在EduMC-QA数据集上的有效性，并对比了先进LLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的推理轨迹常缺乏忠实性，且与符号引擎结合的静态逻辑方法难以处理动态、基于状态的推理。

Method: 提出了MCFR（Model Checking for Formal Reasoning）框架，将LLMs与模型检查结合，将自然语言转化为形式化规范并在过渡模型上验证。

Result: MCFR在EduMC-QA基准测试中表现优于ChatGPT、DeepSeek和Claude等先进LLMs，提升了推理的忠实性和可解释性。

Conclusion: MCFR通过整合LLMs与模型检查，显著提升了封闭领域QA系统的推理忠实性和可解释性，为高风险应用中的可验证QA提供了可行路径。

Abstract: Reasoning is essential for closed-domain QA systems in which procedural
correctness and policy compliance are critical. While large language models
(LLMs) have shown strong performance on many reasoning tasks, recent work
reveals that their reasoning traces are often unfaithful - serving more as
plausible justifications than as causally grounded derivations. Efforts to
combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved
reliability but remain limited to static forms of logic, struggling with
dynamic, state-based reasoning such as multi-step progressions and conditional
transitions.
  In this paper, we propose MCFR (Model Checking for Formal Reasoning), a
neuro-symbolic framework that integrates LLMs with model checking to support
property verification. MCFR translates natural language into formal
specifications and verifies them over transition models. To support evaluation,
we introduce EduMC-QA, a benchmark dataset grounded in real academic
procedures. Our results show that MCFR improves reasoning faithfulness and
interpretability, offering a viable path toward verifiable QA in high-stakes
closed-domain applications. In addition to evaluating MCFR, we compare its
performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to
contextualize its effectiveness.

</details>


### [177] [A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models](https://arxiv.org/abs/2509.11575)
*Ching Chang,Yidan Shi,Defu Cao,Wei Yang,Jeehyun Hwang,Haixin Wang,Jiacheng Pang,Wei Wang,Yan Liu,Wen-Chih Peng,Tien-Fu Chen*

Main category: cs.AI

TL;DR: 该调查定义了时间序列推理问题，通过推理拓扑结构和主要目标组织文献，强调从准确性到可靠性的转变，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 定义时间序列推理问题并组织文献，展示不同推理拓扑结构的优势和局限性，推动从准确性到可靠性的转变。

Method: 该调查通过推理拓扑结构（直接推理、线性链推理和分支结构推理）与主要目标（时间序列分析、解释与理解、因果推理与决策、时间序列生成）交叉组织文献，并回顾了各领域的方法和系统。

Result: 提出了匹配拓扑与不确定性、基于可观察工件的接地、规划转移和流式处理等指导原则，强调了未来进展可能依赖于将推理质量与效用挂钩的基准测试。

Conclusion: 时间序列推理领域正从狭窄的准确性转向大规模可靠性，使系统不仅能分析动态世界，还能理解、解释和行动，同时具有可追溯的证据和可信的结果。

Abstract: Time series reasoning treats time as a first-class axis and incorporates
intermediate evidence directly into the answer. This survey defines the problem
and organizes the literature by reasoning topology with three families: direct
reasoning in one step, linear chain reasoning with explicit intermediates, and
branch-structured reasoning that explores, revises, and aggregates. The
topology is crossed with the main objectives of the field, including
traditional time series analysis, explanation and understanding, causal
inference and decision making, and time series generation, while a compact tag
set spans these axes and captures decomposition and verification, ensembling,
tool use, knowledge access, multimodality, agent loops, and LLM alignment
regimes. Methods and systems are reviewed across domains, showing what each
topology enables and where it breaks down in faithfulness or robustness, along
with curated datasets, benchmarks, and resources that support study and
deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).
Evaluation practices that keep evidence visible and temporally aligned are
highlighted, and guidance is distilled on matching topology to uncertainty,
grounding with observable artifacts, planning for shift and streaming, and
treating cost and latency as design budgets. We emphasize that reasoning
structures must balance capacity for grounding and self-correction against
computational cost and reproducibility, while future progress will likely
depend on benchmarks that tie reasoning quality to utility and on closed-loop
testbeds that trade off cost and risk under shift-aware, streaming, and
long-horizon settings. Taken together, these directions mark a shift from
narrow accuracy toward reliability at scale, enabling systems that not only
analyze but also understand, explain, and act on dynamic worlds with traceable
evidence and credible outcomes.

</details>


### [178] [AMLNet: A Knowledge-Based Multi-Agent Framework to Generate and Detect Realistic Money Laundering Transactions](https://arxiv.org/abs/2509.11595)
*Sabin Huda,Ernest Foo,Zahra Jadidi,MA Hakim Newton,Abdul Sattar*

Main category: cs.AI

TL;DR: AMLNet提出了一种合成交易数据集和检测框架，解决了反洗钱研究中数据稀缺的问题，展示了高效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 反洗钱研究因缺乏公开可共享的、符合监管的交易数据集而受限。

Method: AMLNet是一个基于知识的多代理框架，包括一个符合监管的交易生成器和一个集成检测管道。生成器产生包含核心洗钱阶段和高级类型的合成交易。

Result: AMLNet生成的数据集在监管对齐度上达到75%，检测集成框架在内部测试中F1得分为0.90，并展示了跨不同合成生成范式的架构通用性。

Conclusion: AMLNet提供了一个合成的、符合监管要求的交易数据集，并展示了其检测框架的高效性和可推广性，推动了反洗钱研究的可重复性和监管意识。

Abstract: Anti-money laundering (AML) research is constrained by the lack of publicly
shareable, regulation-aligned transaction datasets. We present AMLNet, a
knowledge-based multi-agent framework with two coordinated units: a
regulation-aware transaction generator and an ensemble detection pipeline. The
generator produces 1,090,173 synthetic transactions (approximately 0.16\%
laundering-positive) spanning core laundering phases (placement, layering,
integration) and advanced typologies (e.g., structuring, adaptive threshold
behavior). Regulatory alignment reaches 75\% based on AUSTRAC rule coverage
(Section 4.2), while a composite technical fidelity score of 0.75 summarizes
temporal, structural, and behavioral realism components (Section 4.4). The
detection ensemble achieves F1 0.90 (precision 0.84, recall 0.97) on the
internal test partitions of AMLNet and adapts to the external SynthAML dataset,
indicating architectural generalizability across different synthetic generation
paradigms. We provide multi-dimensional evaluation (regulatory, temporal,
network, behavioral) and release the dataset (Version 1.0,
https://doi.org/10.5281/zenodo.16736515), to advance reproducible and
regulation-conscious AML experimentation.

</details>


### [179] [Adapting and Evaluating Multimodal Large Language Models for Adolescent Idiopathic Scoliosis Self-Management: A Divide and Conquer Framework](https://arxiv.org/abs/2509.11645)
*Zhaolong Wu,Pu Luo,Jason Pui Yin Cheung,Teng Zhang*

Main category: cs.AI

TL;DR: 研究评估了MLLMs在AIS自我管理中的应用，发现其在解读脊柱X光片和理解护理知识上存在局限。通过脊柱关键点提示和RAG知识库改进后，模型表现有所提升，但仍无法满足个性化护理需求。


<details>
  <summary>Details</summary>
Motivation: 评估MLLMs在AIS自我管理中的应用潜力，揭示其在复杂医学影像解读和专业知识理解上的局限性，并提出改进方法。

Method: 研究通过构建包含约3,000张前后位X光片及诊断文本的数据库，并采用‘分而治之’框架（包括视觉问答任务、领域知识评估任务和患者教育咨询评估任务）对五种MLLMs进行了评估。针对MLLMs在解读复杂脊柱X光片和理解AIS护理知识上的局限性，研究引入了脊柱关键点提示和检索增强生成（RAG）的AIS知识库。

Result: 视觉提示在不同架构中的效果各异，而RAG显著提升了模型在知识评估任务中的表现。然而，MLLMs在脊柱畸形位置和方向检测上的准确率仍然较低。

Conclusion: 当前的多模态大语言模型（MLLMs）在实现青少年特发性脊柱侧凸（AIS）个性化护理助手方面能力有限，尤其是在脊柱畸形位置和方向的准确检测上表现不佳（最佳准确率分别为0.55和0.13）。

Abstract: This study presents the first comprehensive evaluation of Multimodal Large
Language Models (MLLMs) for Adolescent Idiopathic Scoliosis (AIS)
self-management. We constructed a database of approximately 3,000
anteroposterior X-rays with diagnostic texts and evaluated five MLLMs through a
`Divide and Conquer' framework consisting of a visual question-answering task,
a domain knowledge assessment task, and a patient education counseling
assessment task. Our investigation revealed limitations of MLLMs' ability in
interpreting complex spinal radiographs and comprehending AIS care knowledge.
To address these, we pioneered enhancing MLLMs with spinal keypoint prompting
and compiled an AIS knowledge base for retrieval augmented generation (RAG),
respectively. Results showed varying effectiveness of visual prompting across
different architectures, while RAG substantially improved models' performances
on the knowledge assessment task. Our findings indicate current MLLMs are far
from capable in realizing personalized assistant in AIS care. The greatest
challenge lies in their abilities to obtain accurate detections of spinal
deformity locations (best accuracy: 0.55) and directions (best accuracy: 0.13).

</details>


### [180] [HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction](https://arxiv.org/abs/2509.11719)
*Bingqing Wei,Lianmin Chen,Zhongyu Xia,Yongtao Wang*

Main category: cs.AI

TL;DR: HeLoFusion通过局部多尺度图和异构交互建模，显著提升了多代理轨迹预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以全面捕捉多尺度交互和异构代理行为的复杂性，HeLoFusion旨在解决这一问题。

Method: 通过构建局部多尺度图、聚合-分解消息传递方案和类型特定特征网络，HeLoFusion有效建模了异构和多尺度代理交互。

Result: 在Waymo Open Motion Dataset上，HeLoFusion实现了最先进的性能，包括Soft mAP和minADE等关键指标。

Conclusion: 论文展示了基于局部多尺度和异构交互建模的架构（HeLoFusion）在运动预测中的高效性，显著提升了性能指标。

Abstract: Multi-agent trajectory prediction in autonomous driving requires a
comprehensive understanding of complex social dynamics. Existing methods,
however, often struggle to capture the full richness of these dynamics,
particularly the co-existence of multi-scale interactions and the diverse
behaviors of heterogeneous agents. To address these challenges, this paper
introduces HeLoFusion, an efficient and scalable encoder for modeling
heterogeneous and multi-scale agent interactions. Instead of relying on global
context, HeLoFusion constructs local, multi-scale graphs centered on each
agent, allowing it to effectively model both direct pairwise dependencies and
complex group-wise interactions (\textit{e.g.}, platooning vehicles or
pedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of
agent heterogeneity through an aggregation-decomposition message-passing scheme
and type-specific feature networks, enabling it to learn nuanced,
type-dependent interaction patterns. This locality-focused approach enables a
principled representation of multi-level social context, yielding powerful and
expressive agent embeddings. On the challenging Waymo Open Motion Dataset,
HeLoFusion achieves state-of-the-art performance, setting new benchmarks for
key metrics including Soft mAP and minADE. Our work demonstrates that a
locality-grounded architecture, which explicitly models multi-scale and
heterogeneous interactions, is a highly effective strategy for advancing motion
forecasting.

</details>


### [181] [Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning](https://arxiv.org/abs/2509.11880)
*Carlos Celemin,Joseph Brennan,Pierluigi Vito Amadori,Tim Bradley*

Main category: cs.AI

TL;DR: 论文提出了一种结合监督对比学习与模仿学习的新方法，显著提升了游戏智能体的表现。


<details>
  <summary>Details</summary>
Motivation: 旨在通过学习更有效的状态表示，捕捉与动作相关的因素，从而更好地建模观察与演示者动作之间的因果关系。

Method: 提出了一种将SupCon损失与连续输出空间结合的方法，使SupCon能够不受环境动作类型的限制。

Result: 在3D游戏Astro Bot和Returnal以及多款2D Atari游戏上的实验表明，该方法在表示质量、学习收敛速度和泛化能力上均优于仅使用监督动作预测损失函数的基线模型。

Conclusion: 该论文通过将监督对比学习（SupCon）应用于模仿学习（IL），显著提升了视频游戏环境中智能体的状态表示效果，实现了更快的收敛速度和更好的泛化能力。

Abstract: This paper introduces a novel application of Supervised Contrastive Learning
(SupCon) to Imitation Learning (IL), with a focus on learning more effective
state representations for agents in video game environments. The goal is to
obtain latent representations of the observations that capture better the
action-relevant factors, thereby modeling better the cause-effect relationship
from the observations that are mapped to the actions performed by the
demonstrator, for example, the player jumps whenever an obstacle appears ahead.
We propose an approach to integrate the SupCon loss with continuous output
spaces, enabling SupCon to operate without constraints regarding the type of
actions of the environment. Experiments on the 3D games Astro Bot and Returnal,
and multiple 2D Atari games show improved representation quality, faster
learning convergence, and better generalization compared to baseline models
trained only with supervised action prediction loss functions.

</details>


### [182] [EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models](https://arxiv.org/abs/2509.11914)
*Yiqun Yao,Naitong Yu,Xiang Li,Xin Jiang,Xuezhi Fang,Wenjia Ma,Xuying Meng,Jing Li,Aixin Sun,Yequan Wang*

Main category: cs.AI

TL;DR: EgoMem是首个为实时全双工模型设计的终身记忆代理，通过原始视听流实现用户识别、个性化响应和长期知识管理，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有记忆代理无法直接从原始视听流中识别用户和管理长期知识的局限性，EgoMem被设计为适用于终身、实时和具身场景的记忆代理。

Method: EgoMem通过三个异步过程实现功能：(i) 动态识别用户并检索相关上下文的检索过程；(ii) 基于上下文生成个性化音频响应的全模态对话过程；(iii) 自动检测对话边界并更新长期记忆的记忆管理过程。

Result: 实验结果显示，EgoMem的检索和记忆管理模块在测试集上准确率超过95%，与RoboEgo全模态聊天机器人集成时，事实一致性得分超过87%。

Conclusion: EgoMem为实时全双工模型提供了一个高效、个性化的记忆代理系统，通过原始视听流实现用户识别和知识管理，为未来研究奠定了坚实基础。

Abstract: We introduce EgoMem, the first lifelong memory agent tailored for full-duplex
models that process real-time omnimodal streams. EgoMem enables real-time
models to recognize multiple users directly from raw audiovisual streams, to
provide personalized response, and to maintain long-term knowledge of users'
facts, preferences, and social relationships extracted from audiovisual
history. EgoMem operates with three asynchronous processes: (i) a retrieval
process that dynamically identifies user via face and voice, and gathers
relevant context from a long-term memory; (ii) an omnimodal dialog process that
generates personalized audio responses based on the retrieved context; and
(iii) a memory management process that automatically detects dialog boundaries
from omnimodal streams, and extracts necessary information to update the
long-term memory. Unlike existing memory agents for LLMs, EgoMem relies
entirely on raw audiovisual streams, making it especially suitable for
lifelong, real-time, and embodied scenarios. Experimental results demonstrate
that EgoMem's retrieval and memory management modules achieve over 95% accuracy
on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,
the system achieves fact-consistency scores above 87% in real-time personalized
dialogs, establishing a strong baseline for future research.

</details>


### [183] [BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning](https://arxiv.org/abs/2509.11922)
*Xilei Dai,Ruotian Chen,Songze Guan,Wen-Tai Li,Chau Yuen*

Main category: cs.AI

TL;DR: BuildingGym是一个开源RL框架，用于建筑能源管理，简化了控制策略的训练和优化，尤其在冷却负载管理中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个灵活的框架来在建筑能源管理中跨多种控制问题实施强化学习，BuildingGym旨在填补这一空白。

Method: 提出了BuildingGym，一个集成EnergyPlus作为核心模拟器的开源工具，支持多种内置RL算法，简化了控制策略的配置和训练过程。

Result: BuildingGym在冷却负载管理任务中表现出色，内置算法在恒定和动态负载管理中均展示了强大性能。

Conclusion: BuildingGym是一个有效的开源工具，为建筑能源管理中的强化学习控制策略提供了灵活且研究友好的框架，成功优化了冷却策略。

Abstract: Reinforcement learning (RL) has proven effective for AI-based building energy
management. However, there is a lack of flexible framework to implement RL
across various control problems in building energy management. To address this
gap, we propose BuildingGym, an open-source tool designed as a
research-friendly and flexible framework for training RL control strategies for
common challenges in building energy management. BuildingGym integrates
EnergyPlus as its core simulator, making it suitable for both system-level and
room-level control. Additionally, BuildingGym is able to accept external
signals as control inputs instead of taking the building as a stand-alone
entity. This feature makes BuildingGym applicable for more flexible
environments, e.g. smart grid and EVs community. The tool provides several
built-in RL algorithms for control strategy training, simplifying the process
for building managers to obtain optimal control strategies. Users can achieve
this by following a few straightforward steps to configure BuildingGym for
optimization control for common problems in the building energy management
field. Moreover, AI specialists can easily implement and test state-of-the-art
control algorithms within the platform. BuildingGym bridges the gap between
building managers and AI specialists by allowing for the easy configuration and
replacement of RL algorithms, simulators, and control environments or problems.
With BuildingGym, we efficiently set up training tasks for cooling load
management, targeting both constant and dynamic cooling load management. The
built-in algorithms demonstrated strong performance across both tasks,
highlighting the effectiveness of BuildingGym in optimizing cooling strategies.

</details>


### [184] [Neuromorphic Intelligence](https://arxiv.org/abs/2509.11940)
*Marcel van Gerven*

Main category: cs.AI

TL;DR: 动态系统理论为神经形态计算提供了统一框架，整合多学科知识，利用噪声和差分遗传编程实现自适应行为，推动AI可持续发展。


<details>
  <summary>Details</summary>
Motivation: 神经形态计算旨在模仿人脑的高效、灵活和适应性，但其跨学科特性需要统一的理论框架来整合人工智能、神经科学、物理、化学和材料科学等领域的知识。

Method: 通过动态系统理论，利用微分计算建模推理、学习和控制，并探索噪声作为学习资源的潜力，以及差分遗传编程用于发现实现自适应行为的动态系统。

Result: 动态系统理论为神经形态计算提供了理论基础，能够利用噪声作为学习资源，并通过差分遗传编程实现自适应行为，从而推动智能系统的可持续发展。

Conclusion: 文章认为，动态系统理论为神经形态计算提供了一个统一的理论框架，能够整合多个学科的知识，推动智能行为的涌现和AI的可持续发展。

Abstract: Neuromorphic computing seeks to replicate the remarkable efficiency,
flexibility, and adaptability of the human brain in artificial systems. Unlike
conventional digital approaches, which depend on massive computational and
energy resources, neuromorphic systems exploit brain-inspired principles of
computation to achieve orders of magnitude greater energy efficiency. By
drawing on insights from artificial intelligence, neuroscience, physics,
chemistry, and materials science, neuromorphic computing promises to deliver
intelligent systems that are sustainable, transparent, and widely accessible. A
central challenge, however, is to identify a unifying theoretical framework
capable of bridging these diverse disciplines. We argue that dynamical systems
theory provides such a foundation. Rooted in differential calculus, it offers a
principled language for modeling inference, learning, and control in both
natural and artificial substrates. Within this framework, noise can be
harnessed as a resource for learning, while differential genetic programming
enables the discovery of dynamical systems that implement adaptive behaviors.
Embracing this perspective paves the way toward emergent neuromorphic
intelligence, where intelligent behavior arises from the dynamics of physical
substrates, advancing both the science and sustainability of AI.

</details>


### [185] [How to Evaluate Medical AI](https://arxiv.org/abs/2509.11941)
*Ilia Kopanichuk,Petr Anokhin,Vladimir Shaposhnikov,Vladimir Makharev,Ekaterina Tsapieva,Iaroslav Bespalov,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.AI

TL;DR: 该论文提出新的医疗AI评估指标RPAD和RRAD，通过比较AI与多位专家意见，证明其优于传统指标，并揭示了专家判断的高变异性。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标如精确度和召回率未能考虑专家判断的变异性，导致AI性能评估不一致，需要更可靠且可解释的评估方法。

Method: 通过比较AI输出与多位专家意见而非单一参考，引入RPAD和RRAD指标，并在360个医疗对话中评估多种大型语言模型与医生小组的表现。

Result: 研究显示，表现最佳的模型（如DeepSeek-V3）达到或超过专家共识的一致性，且专家判断的变异性常大于AI与人类之间的差异。

Conclusion: 该研究提出了一种新的评估指标RPAD和RRAD，用于更稳定和真实地衡量AI在医疗诊断中的表现，并证明了专家判断存在显著变异性，支持在医疗AI中采用相对指标。

Abstract: The integration of artificial intelligence (AI) into medical diagnostic
workflows requires robust and consistent evaluation methods to ensure
reliability, clinical relevance, and the inherent variability in expert
judgments. Traditional metrics like precision and recall often fail to account
for the inherent variability in expert judgments, leading to inconsistent
assessments of AI performance. Inter-rater agreement statistics like Cohen's
Kappa are more reliable but they lack interpretability. We introduce Relative
Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new
evaluation metrics that compare AI outputs against multiple expert opinions
rather than a single reference. By normalizing performance against inter-expert
disagreement, these metrics provide a more stable and realistic measure of the
quality of predicted diagnosis. In addition to the comprehensive analysis of
diagnostic quality measures, our study contains a very important side result.
Our evaluation methodology allows us to avoid selecting diagnoses from a
limited list when evaluating a given case. Instead, both the models being
tested and the examiners verifying them arrive at a free-form diagnosis. In
this automated methodology for establishing the identity of free-form clinical
diagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our
approach using 360 medical dialogues, comparing multiple large language models
(LLMs) against a panel of physicians. Large-scale study shows that
top-performing models, such as DeepSeek-V3, achieve consistency on par with or
exceeding expert consensus. Moreover, we demonstrate that expert judgments
exhibit significant variability - often greater than that between AI and
humans. This finding underscores the limitations of any absolute metrics and
supports the need to adopt relative metrics in medical AI.

</details>


### [186] [Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics](https://arxiv.org/abs/2509.11943)
*Antonin Sulc,Thorsten Hellert*

Main category: cs.AI

TL;DR: 本文提出了一种神经符号多智能体架构，通过Kripke模型和模态逻辑提升智能体推理能力，并在模拟环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管模型和数据规模的扩展已带来显著能力提升，但智能体在环境中推理的结构、保真度和逻辑一致性仍是一个未被充分探索的关键维度。本文旨在填补这一空白。

Method: 采用神经符号多智能体架构，将智能体的信念状态形式化为Kripke模型，并利用模态逻辑进行推理。通过领域特定知识的逻辑约束指导语言模型的假设生成，避免物理或逻辑上不可行的结论。

Result: 在高保真模拟粒子加速器环境中，系统成功诊断了复杂的级联故障，展示了结合语言模型的语义直觉与模态逻辑的严格验证的可行性。

Conclusion: 本研究提出了一种结合神经符号多智能体架构的方法，通过Kripke模型形式化表示智能体的信念状态，利用模态逻辑进行推理，有效提升了智能体在复杂环境中的决策能力和可靠性。

Abstract: The development of intelligent agents, particularly those powered by language
models (LMs), has shown the critical role in various environments that require
intelligent and autonomous decision. Environments are not passive testing
grounds and they represent the data required for agents to learn and exhibit
very challenging conditions that require adaptive, complex and autonomous
capacity to make decisions. While the paradigm of scaling models and datasets
has led to remarkable emergent capabilities, we argue that scaling the
structure, fidelity, and logical consistency of agent reasoning within these
environments is a crucial, yet underexplored, dimension of AI research. This
paper introduces a neuro-symbolic multi-agent architecture where the belief
states of individual agents are formally represented as Kripke models. This
foundational choice enables them to reason about known concepts of
\emph{possibility} and \emph{necessity} using the formal language of modal
logic. In this work, we use of immutable, domain-specific knowledge to make
infere information, which is encoded as logical constraints essential for
proper diagnosis. In the proposed model, we show constraints that actively
guide the hypothesis generation of LMs, effectively preventing them from
reaching physically or logically untenable conclusions. In a high-fidelity
simulated particle accelerator environment, our system successfully diagnoses
complex, cascading failures by combining the powerful semantic intuition of LMs
with the rigorous, verifiable validation of modal logic and a factual world
model and showcasing a viable path toward more robust, reliable, and verifiable
autonomous agents.

</details>


### [187] [Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare](https://arxiv.org/abs/2509.11944)
*Susanta Mitra*

Main category: cs.AI

TL;DR: 该论文提出了一种基于时间图的多模态医疗推理框架，通过动态调整和多智能体协作提升诊断准确性，初步实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域的多模态推理模型应用有限且诊断准确性不足。研究旨在通过动态推理和多模态数据分析提升医疗诊断的正确性，辅助医疗专业人员。

Method: 研究采用了一种基于有向图的时间图推理过程，支持回溯、内容精炼及原因的动态调整。此外，多智能体时间推理框架通过任务分配和交叉验证机制进一步优化推理输出。

Result: 提出的方法能有效跟踪和分析患者健康状况及疾病进展，初步实验证实了其在医疗诊断中的实用性和创新性。

Conclusion: 该研究提出了一种基于时间图的新型推理过程，通过动态调整和多模态数据分析，显著提升了医疗诊断的准确性。初步实验验证了其新颖性和实用性。

Abstract: Healthcare and medicine are multimodal disciplines that deal with multimodal
data for reasoning and diagnosing multiple diseases. Although some multimodal
reasoning models have emerged for reasoning complex tasks in scientific
domains, their applications in the healthcare domain remain limited and fall
short in correct reasoning for diagnosis. To address the challenges of
multimodal medical reasoning for correct diagnosis and assist the healthcare
professionals, a novel temporal graph-based reasoning process modelled through
a directed graph has been proposed in the current work. It helps in
accommodating dynamic changes in reasons through backtracking, refining the
reasoning content, and creating new or deleting existing reasons to reach the
best recommendation or answer. Again, consideration of multimodal data at
different time points can enable tracking and analysis of patient health and
disease progression. Moreover, the proposed multi-agent temporal reasoning
framework provides task distributions and a cross-validation mechanism to
further enhance the accuracy of reasoning outputs. A few basic experiments and
analysis results justify the novelty and practical utility of the proposed
preliminary approach.

</details>


### [188] [MusicSwarm: Biologically Inspired Intelligence for Music Composition](https://arxiv.org/abs/2509.11973)
*Markus J. Buehler*

Main category: cs.AI

TL;DR: MusicSwarm展示了通过分散的、相同的模型群体协调实现长形式音乐创作，优于集中式系统，并在多样性和创造力上表现突出。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过分散的、相同的、冻结的基础模型群体，通过间接的、点对点的信号协调，实现连贯的、长形式的音乐创作。

Method: 比较了具有全局批评者的集中式多智能体系统与完全分散的群体系统，后者通过感知和存储和声、节奏及结构线索，适应短期记忆，并达成共识。

Result: 群体系统在符号、音频和图论分析中表现出更高的质量、多样性和结构变化，并在创造力指标上领先。动态收敛到稳定的互补角色配置，自相似网络显示出小世界架构。

Conclusion: MusicSwarm通过将专业化从参数更新转移到交互规则、共享内存和动态共识，为长时程创造性结构提供了一种计算和数据高效的途径，并可直接应用于协作写作、设计和科学发现等领域。

Abstract: We show that coherent, long-form musical composition can emerge from a
decentralized swarm of identical, frozen foundation models that coordinate via
stigmergic, peer-to-peer signals, without any weight updates. We compare a
centralized multi-agent system with a global critic to a fully decentralized
swarm in which bar-wise agents sense and deposit harmonic, rhythmic, and
structural cues, adapt short-term memory, and reach consensus. Across symbolic,
audio, and graph-theoretic analyses, the swarm yields superior quality while
delivering greater diversity and structural variety and leads across creativity
metrics. The dynamics contract toward a stable configuration of complementary
roles, and self-similarity networks reveal a small-world architecture with
efficient long-range connectivity and specialized bridging motifs, clarifying
how local novelties consolidate into global musical form. By shifting
specialization from parameter updates to interaction rules, shared memory, and
dynamic consensus, MusicSwarm provides a compute- and data-efficient route to
long-horizon creative structure that is immediately transferable beyond music
to collaborative writing, design, and scientific discovery.

</details>


### [189] [Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review](https://arxiv.org/abs/2509.12034)
*Emmanuel Adjei Domfeh,Christopher L. Dancy*

Main category: cs.AI

TL;DR: 本文系统回顾了灾害管理中人类-AI协作的四大模式，指出AI的优势与局限，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 在高风险的灾害场景中，及时且明智的决策至关重要，但常受不确定性、动态环境和资源有限的挑战。

Method: 通过对51篇同行评审研究的系统回顾，识别并分析了四大类人类-AI协作模式及其子模式。

Result: 研究发现AI系统可以增强态势感知、提高响应效率并支持复杂决策，但也存在可扩展性、可解释性和系统互操作性等关键限制。

Conclusion: 本文总结了人类-AI协作在灾害管理中的关键挑战和未来研究方向，强调需要适应性、可信赖且情境感知的系统以提升灾害韧性和公平恢复结果。

Abstract: In high-stakes disaster scenarios, timely and informed decision-making is
critical yet often challenged by uncertainty, dynamic environments, and limited
resources. This paper presents a systematic review of Human-AI collaboration
patterns that support decision-making across all disaster management phases.
Drawing from 51 peer-reviewed studies, we identify four major categories:
Human-AI Decision Support Systems, Task and Resource Coordination, Trust and
Transparency, and Simulation and Training. Within these, we analyze
sub-patterns such as cognitive-augmented intelligence, multi-agent
coordination, explainable AI, and virtual training environments. Our review
highlights how AI systems may enhance situational awareness, improves response
efficiency, and support complex decision-making, while also surfacing critical
limitations in scalability, interpretability, and system interoperability. We
conclude by outlining key challenges and future research directions,
emphasizing the need for adaptive, trustworthy, and context-aware Human-AI
systems to improve disaster resilience and equitable recovery outcomes.

</details>


### [190] [When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models](https://arxiv.org/abs/2509.12060)
*Wei Cai,Shujuan Liu,Jian Zhao,Ziyan Shi,Yusheng Zhao,Yuchen Yuan,Tianle Zhang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 论文提出SSUI数据集和SRPO训练框架，解决多模态大语言模型的隐式推理风险，实验表明SRPO模型在安全基准测试中表现最优。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在隐式推理风险，难以在长链推理中保持安全对齐。

Method: 基于SSUI数据集设计了Safety-aware Reasoning Path Optimization (SRPO)训练框架。

Result: SRPO训练模型在Reasoning Path Benchmark (RSBench)等关键安全基准测试中取得最先进成果。

Conclusion: SRPO训练的多模态大语言模型在关键安全基准测试中表现优异，显著优于开源和顶级商业MLLMs。

Abstract: Multimodal Large Language Models (MLLMs) are susceptible to the implicit
reasoning risk, wherein innocuous unimodal inputs synergistically assemble into
risky multimodal data that produce harmful outputs. We attribute this
vulnerability to the difficulty of MLLMs maintaining safety alignment through
long-chain reasoning. To address this issue, we introduce
Safe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring
interpretable reasoning paths tailored for such a cross-modal challenge. A
novel training framework, Safety-aware Reasoning Path Optimization (SRPO), is
also designed based on the SSUI dataset to align the MLLM's internal reasoning
process with human safety values. Experimental results show that our
SRPO-trained models achieve state-of-the-art results on key safety benchmarks,
including the proposed Reasoning Path Benchmark (RSBench), significantly
outperforming both open-source and top-tier commercial MLLMs.

</details>


### [191] [Bridging Engineering and AI Planning through Model-Based Knowledge Transformation for the Validation of Automated Production System Variants](https://arxiv.org/abs/2509.12091)
*Hamied Nabizada,Lasse Beers,Alain Chahine,Felix Gehlhoff,Oliver Niggemann,Alexander Fay*

Main category: cs.AI

TL;DR: 本文提出了一种模型驱动方法，将符号规划语义集成到SysML工程模型中，并通过算法生成PDDL文件，以支持AI规划验证系统变体。


<details>
  <summary>Details</summary>
Motivation: MBSE环境中的工程模型缺乏符号规划语义（如前提条件、效果和资源约束），限制了评估系统变体能否高效完成特定任务的能力。

Method: 提出了一种基于SysML的模型驱动方法，通过专用SysML配置文件引入可重用的核心规划构造，并利用算法生成有效的PDDL域文件和问题文件。

Result: 该方法实现了工程模型与规划语义的原生集成，并保持了一致性，支持AI规划验证系统变体。

Conclusion: 该方法通过一个飞机组装的案例研究展示了其适用性，证明了如何通过AI规划验证系统变体。

Abstract: Engineering models created in Model-Based Systems Engineering (MBSE)
environments contain detailed information about system structure and behavior.
However, they typically lack symbolic planning semantics such as preconditions,
effects, and constraints related to resource availability and timing. This
limits their ability to evaluate whether a given system variant can fulfill
specific tasks and how efficiently it performs compared to alternatives.
  To address this gap, this paper presents a model-driven method that enables
the specification and automated generation of symbolic planning artifacts
within SysML-based engineering models. A dedicated SysML profile introduces
reusable stereotypes for core planning constructs. These are integrated into
existing model structures and processed by an algorithm that generates a valid
domain file and a corresponding problem file in Planning Domain Definition
Language (PDDL). In contrast to previous approaches that rely on manual
transformations or external capability models, the method supports native
integration and maintains consistency between engineering and planning
artifacts.
  The applicability of the method is demonstrated through a case study from
aircraft assembly. The example illustrates how existing engineering models are
enriched with planning semantics and how the proposed workflow is applied to
generate consistent planning artifacts from these models. The generated
planning artifacts enable the validation of system variants through AI
planning.

</details>


### [192] [JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference](https://arxiv.org/abs/2509.12104)
*Zongyue Xue,Siyuan Zheng,Shaochun Wang,Yiran Hu,Shenran Wang,Yuxin Yao,Haitao Li,Qingyao Ai,Yiqun Liu,Yun Liu,Weixing Shen*

Main category: cs.AI

TL;DR: JustEva 是一个用于评估 LLM 在法律任务中公平性的开源工具包，揭示了当前模型的公平性缺陷，并提供了改进方法。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型（LLMs）整合到法律实践中引发了关于司法公平性的紧迫问题，特别是由于其“黑盒”过程。

Method: JustEva 是一个全面的开源评估工具包，具有结构化标签系统、三个核心公平指标、稳健的统计推断方法和信息可视化功能。

Result: JustEva 的实证应用揭示了当前 LLMs 在公平性方面的显著缺陷，突出了缺乏公平和可信赖的 LLM 法律工具。

Conclusion: JustEva 提供了一个便利的工具和方法论基础，用于评估和改进法律领域中的算法公平性。

Abstract: The integration of Large Language Models (LLMs) into legal practice raises
pressing concerns about judicial fairness, particularly due to the nature of
their "black-box" processes. This study introduces JustEva, a comprehensive,
open-source evaluation toolkit designed to measure LLM fairness in legal tasks.
JustEva features several advantages: (1) a structured label system covering 65
extra-legal factors; (2) three core fairness metrics - inconsistency, bias, and
imbalanced inaccuracy; (3) robust statistical inference methods; and (4)
informative visualizations. The toolkit supports two types of experiments,
enabling a complete evaluation workflow: (1) generating structured outputs from
LLMs using a provided dataset, and (2) conducting statistical analysis and
inference on LLMs' outputs through regression and other statistical methods.
Empirical application of JustEva reveals significant fairness deficiencies in
current LLMs, highlighting the lack of fair and trustworthy LLM legal tools.
JustEva offers a convenient tool and methodological foundation for evaluating
and improving algorithmic fairness in the legal domain.

</details>


### [193] [Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation](https://arxiv.org/abs/2509.12179)
*Yubo Li,Weiyi Song*

Main category: cs.AI

TL;DR: BiCA通过双向认知对齐实现人类与AI的相互适应，显著提升协作效果与安全性。


<details>
  <summary>Details</summary>
Motivation: 当前RLHF的单向对齐范式将人类认知视为固定，而BiCA提出人类与AI相互适应的双向对齐。

Method: 采用可学习协议、表示映射和KL预算约束实现受控共同演化。

Result: 在协作导航任务中，BiCA成功率85.5%（基线70.3%），互适应提升230%，协议收敛提升332%；自学习协议优于人工设计84%，双向适应意外提升安全性（OOD鲁棒性+23%）。

Conclusion: 从单向对齐转向双向认知对齐（BiCA）范式，验证了人类与AI在能力交集而非并集处存在最优协作，显著提升了协同效果（46%）和安全性（23%）。

Abstract: Current AI alignment through RLHF follows a single directional paradigm that
AI conforms to human preferences while treating human cognition as fixed. We
propose a shift to co-alignment through Bidirectional Cognitive Alignment
(BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols,
representation mapping, and KL-budget constraints for controlled co-evolution.
In collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline,
with 230% better mutual adaptation and 332% better protocol convergence.
Emergent protocols outperformed handcrafted ones by 84%, while bidirectional
adaptation unexpectedly improved safety (+23% out-of-distribution robustness).
The 46% synergy improvement demonstrates optimal collaboration exists at the
intersection, not union, of human and AI capabilities, validating the shift
from single-directional to co-alignment paradigms.

</details>


### [194] [Advancing Medical Artificial Intelligence Using a Century of Cases](https://arxiv.org/abs/2509.12194)
*Thomas A. Buckley,Riccardo Conci,Peter G. Brodeur,Jason Gusdorf,Sourik Beltrán,Bita Behrouzi,Byron Crowe,Jacob Dockterman,Muzzammil Muhammad,Sarah Ohnigian,Andrew Sanchez,James A. Diao,Aashna P. Shah,Daniel Restrepo,Eric S. Rosenberg,Andrew S. Lea,Marinka Zitnik,Scott H. Podolsky,Zahir Kanjee,Raja-Elie E. Abdulnour,Jacob M. Koshy,Adam Rodman,Arjun K. Manrai*

Main category: cs.AI

TL;DR: LLMs在医学文本诊断上超越医生，但图像和文献任务表现较弱。CPC-Bench和CaBot为医学AI提供透明评估工具。


<details>
  <summary>Details</summary>
Motivation: 传统AI评估仅关注最终诊断，未涉及专家讨论所需的多维推理和展示能力。

Method: 利用7102例CPCs（1923-2025）和1021例图像挑战（2006-2025），通过医生标注和自动化处理构建CPC-Bench基准，评估领先LLMs，并开发AI讨论者“Dr. CaBot”。

Result: GPT-3在当代CPCs中60%病例诊断排名第一，84%进入前十，优于20名医生基线；图像任务准确率67%。CaBot生成的文本在74%试验中被误认为人类专家，且质量评分更高。

Conclusion: 大型语言模型（LLMs）在复杂文本鉴别诊断上超越医生表现，并能逼真模拟专家医学报告，但图像解读和文献检索仍是短板。CPC-Bench和CaBot有望透明持续追踪医学AI进展。

Abstract: BACKGROUND: For over a century, the New England Journal of Medicine
Clinicopathological Conferences (CPCs) have tested the reasoning of expert
physicians and, recently, artificial intelligence (AI). However, prior AI
evaluations have focused on final diagnoses without addressing the multifaceted
reasoning and presentation skills required of expert discussants.
  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),
we conducted extensive physician annotation and automated processing to create
CPC-Bench, a physician-validated benchmark spanning 10 text-based and
multimodal tasks, against which we evaluated leading large language models
(LLMs). Then, we developed "Dr. CaBot," an AI discussant designed to produce
written and slide-based video presentations using only the case presentation,
modeling the role of the human expert in these cases.
  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the
final diagnosis first in 60% of cases and within the top ten in 84% of cases,
outperforming a 20-physician baseline; next-test selection accuracy reached
98%. Event-level physician annotations quantified AI diagnostic accuracy per
unit of information. Performance was lower on literature search and image
tasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image
challenges. In blinded comparisons of CaBot vs. human expert-generated text,
physicians misclassified the source of the differential in 46 of 62 (74%) of
trials, and scored CaBot more favorably across quality dimensions. To promote
research, we are releasing CaBot and CPC-Bench.
  CONCLUSIONS: LLMs exceed physician performance on complex text-based
differential diagnosis and convincingly emulate expert medical presentations,
but image interpretation and literature retrieval remain weaker. CPC-Bench and
CaBot may enable transparent and continued tracking of progress in medical AI.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [195] [Quality Assessment of Tabular Data using Large Language Models and Code Generation](https://arxiv.org/abs/2509.10572)
*Ashlesha Akella,Akshar Kaul,Krishnasuri Narayanam,Sameep Mehta*

Main category: cs.SE

TL;DR: 论文提出一个三阶段框架，结合统计和LLM技术自动化生成数据质量规则和验证器，显著提升验证效率。


<details>
  <summary>Details</summary>
Motivation: 基于规则的数据质量验证存在效率低、依赖人工干预和计算成本高的问题，亟需自动化且高效的解决方案。

Method: 框架包括三个阶段：传统聚类过滤数据样本，迭代提示LLM生成语义有效的质量规则，以及通过代码生成LLM合成可执行验证器。利用检索增强生成（RAG）结合外部知识源和领域特定示例提升规则可靠性。

Result: 在基准数据集上的大量评估证实了该方法的有效性，能够生成可靠的质量规则和可执行验证器。

Conclusion: 该论文的三阶段框架结合统计异常检测与LLM驱动的规则和代码生成，有效提升了表格数据集的质量验证效率，并通过广泛评估验证了其有效性。

Abstract: Reliable data quality is crucial for downstream analysis of tabular datasets,
yet rule-based validation often struggles with inefficiency, human
intervention, and high computational costs. We present a three-stage framework
that combines statistical inliner detection with LLM-driven rule and code
generation. After filtering data samples through traditional clustering, we
iteratively prompt LLMs to produce semantically valid quality rules and
synthesize their executable validators through code-generating LLMs. To
generate reliable quality rules, we aid LLMs with retrieval-augmented
generation (RAG) by leveraging external knowledge sources and domain-specific
few-shot examples. Robust guardrails ensure the accuracy and consistency of
both rules and code snippets. Extensive evaluations on benchmark datasets
confirm the effectiveness of our approach.

</details>


### [196] [Reasonable Experiments in Model-Based Systems Engineering](https://arxiv.org/abs/2509.10649)
*Johan Cederbladh,Loek Cleophas,Eduard Kamburjan,Lucas Lima,Rakshit Mittal,Hans Vangheluwe*

Main category: cs.SE

TL;DR: 论文提出一个实验管理框架，利用案例推理和领域知识智能重用实验数据，减少重复实验，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着基于模型的系统工程向数字工程和早期验证与验证方向发展，实验越来越多地用于估计系统参数和探索设计决策，因此需要智能管理实验配置和结果以加速设计过程。

Method: 论文介绍了一个通用的实验管理器架构，结合案例推理和领域知识，以决定是否可以重用已有实验数据来回答新问题。

Result: 通过一个工业车辆能源系统设计案例研究验证了该方法的有效性。

Conclusion: 该论文提出了一个基于案例推理和领域知识的实验管理框架，旨在通过智能重用实验数据来减少不必要的实验，从而节省时间和资源。

Abstract: With the current trend in Model-Based Systems Engineering towards Digital
Engineering and early Validation & Verification, experiments are increasingly
used to estimate system parameters and explore design decisions. Managing such
experimental configuration metadata and results is of utmost importance in
accelerating overall design effort. In particular, we observe it is important
to 'intelligent-ly' reuse experiment-related data to save time and effort by
not performing potentially superfluous, time-consuming, and resource-intensive
experiments. In this work, we present a framework for managing experiments on
digital and/or physical assets with a focus on case-based reasoning with domain
knowledge to reuse experimental data efficiently by deciding whether an
already-performed experiment (or associated answer) can be reused to answer a
new (potentially different) question from the engineer/user without having to
set up and perform a new experiment. We provide the general architecture for
such an experiment manager and validate our approach using an industrial
vehicular energy system-design case study.

</details>


### [197] [Arguzz: Testing zkVMs for Soundness and Completeness Bugs](https://arxiv.org/abs/2509.10819)
*Christoph Hochrainer,Valentin Wüstholz,Maria Christakis*

Main category: cs.SE

TL;DR: Arguzz是首个自动化测试zkVMs的工具，通过变形测试和故障注入发现漏洞，已在多个真实系统中验证有效性。


<details>
  <summary>Details</summary>
Motivation: zkVMs在去中心化应用和区块链rollups中的广泛应用，但其复杂性和潜在漏洞可能导致严重的安全问题。

Method: Arguzz结合了变形的测试方法和故障注入技术，生成语义等效的程序对，并在zkVM中运行以检测漏洞。

Result: Arguzz测试了六个真实世界的zkVMs，发现了三个系统中的十一个漏洞，其中一个漏洞获得了高额奖金。

Conclusion: Arguzz证明了对zkVMs进行系统性测试的至关重要，通过自动化工具发现了多个真实世界中的zkVMs的漏洞，包括一个价值50,000美元的漏洞。

Abstract: Zero-knowledge virtual machines (zkVMs) are increasingly deployed in
decentralized applications and blockchain rollups since they enable verifiable
off-chain computation. These VMs execute general-purpose programs, frequently
written in Rust, and produce succinct cryptographic proofs. However, zkVMs are
complex, and bugs in their constraint systems or execution logic can cause
critical soundness (accepting invalid executions) or completeness (rejecting
valid ones) issues.
  We present Arguzz, the first automated tool for testing zkVMs for soundness
and completeness bugs. To detect such bugs, Arguzz combines a novel variant of
metamorphic testing with fault injection. In particular, it generates
semantically equivalent program pairs, merges them into a single Rust program
with a known output, and runs it inside a zkVM. By injecting faults into the
VM, Arguzz mimics malicious or buggy provers to uncover overly weak
constraints.
  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1,
OpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug
resulted in a $50,000 bounty, despite prior audits, demonstrating the critical
need for systematic testing of zkVMs.

</details>


### [198] [TPSQLi: Test Prioritization for SQL Injection Vulnerability Detection in Web Applications](https://arxiv.org/abs/2509.10920)
*Guan-Yan Yang,Farn Wang,You-Zong Gu,Ya-Wen Teng,Kuo-Hui Yeh,Ping-Hsueh Ho,Wei-Ling Wen*

Main category: cs.SE

TL;DR: 针对SQL注入漏洞，提出了一种基于测试结果动态调整防御强度的优先级方法，优化测试流程并提高效率。


<details>
  <summary>Details</summary>
Motivation: 网络攻击的快速增长增加了软件测试的复杂性和工作量，需要先进工具支持敏捷开发周期。

Method: 利用先前测试结果调整防御强度向量，为后续测试定制防御机制，并结合动态调整和时间因素。

Result: 该方法通过灵活框架优化了测试工作流，提高了SQL注入漏洞检测的效率和针对性。

Conclusion: 本文提出了一种新颖的测试优先级方法，通过动态调整防御强度向量，优化了SQL注入漏洞的测试流程，提高了漏洞检测和缓解的效率和效果。

Abstract: The rapid proliferation of network applications has led to a significant
increase in network attacks. According to the OWASP Top 10 Projects report
released in 2021, injection attacks rank among the top three vulnerabilities in
software projects. This growing threat landscape has increased the complexity
and workload of software testing, necessitating advanced tools to support agile
development cycles. This paper introduces a novel test prioritization method
for SQL injection vulnerabilities to enhance testing efficiency. By leveraging
previous test outcomes, our method adjusts defense strength vectors for
subsequent tests, optimizing the testing workflow and tailoring defense
mechanisms to specific software needs. This approach aims to improve the
effectiveness and efficiency of vulnerability detection and mitigation through
a flexible framework that incorporates dynamic adjustments and considers the
temporal aspects of vulnerability exposure.

</details>


### [199] [When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning](https://arxiv.org/abs/2509.10946)
*Roberto Morabito,Guanghan Wu*

Main category: cs.SE

TL;DR: 研究LLM在嵌入式ML中的失败模式，提出分类法并探讨改进方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在嵌入式机器学习工作流中用于自动化软件生成，但其输出常无声失败或行为不可预测。

Method: 基于一个自动导航框架，协调数据预处理、模型转换和板载推理代码生成，进行实证调查。

Result: 分析揭示了多种易出错行为，包括格式引起的误解和编译通过但破坏下游的运行时干扰代码，并提出了故障分类法。

Conclusion: 文章讨论了提高LLM驱动的嵌入式ML系统可靠性和可追溯性的方向。

Abstract: Large Language Models (LLMs) are increasingly used to automate software
generation in embedded machine learning workflows, yet their outputs often fail
silently or behave unpredictably. This article presents an empirical
investigation of failure modes in LLM-powered ML pipelines, based on an
autopilot framework that orchestrates data preprocessing, model conversion, and
on-device inference code generation. We show how prompt format, model behavior,
and structural assumptions influence both success rates and failure
characteristics, often in ways that standard validation pipelines fail to
detect. Our analysis reveals a diverse set of error-prone behaviors, including
format-induced misinterpretations and runtime-disruptive code that compiles but
breaks downstream. We derive a taxonomy of failure categories and analyze
errors across multiple LLMs, highlighting common root causes and systemic
fragilities. Though grounded in specific devices, our study reveals broader
challenges in LLM-based code generation. We conclude by discussing directions
for improving reliability and traceability in LLM-powered embedded ML systems.

</details>


### [200] [Hardness, Structural Knowledge, and Opportunity: An Analytical Framework for Modular Performance Modeling](https://arxiv.org/abs/2509.11000)
*Omid Gheibi,Christian Kästner,Pooyan Jamshidi*

Main category: cs.SE

TL;DR: 本文研究了结构知识和系统特性对性能建模的影响，量化了建模‘硬度’，并通过实验证明其与改进机会的关系，为系统设计提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 解决结构知识、系统特性（称为‘结构方面’）与潜在模型改进之间关系不明确的问题。

Method: 通过合成系统模型的受控实验，引入并量化了建模‘硬度’的概念，建立了一个‘分析矩阵’来衡量这些概念。

Result: 研究发现建模硬度主要由模块数量和每个模块的配置选项驱动，且结构知识水平和建模硬度的提高显著增强了改进机会。不同性能指标下这些因素的影响不同。

Conclusion: 本文的结论为系统设计者提供了实际操作建议，指导其根据系统特性和任务目标战略性地分配时间和选择适当的建模方法。

Abstract: Performance-influence models are beneficial for understanding how
configurations affect system performance, but their creation is challenging due
to the exponential growth of configuration spaces. While gray-box approaches
leverage selective "structural knowledge" (like the module execution graph of
the system) to improve modeling, the relationship between this knowledge, a
system's characteristics (we call them "structural aspects"), and potential
model improvements is not well understood. This paper addresses this gap by
formally investigating how variations in structural aspects (e.g., the number
of modules and options per module) and the level of structural knowledge impact
the creation of "opportunities" for improved "modular performance modeling". We
introduce and quantify the concept of modeling "hardness", defined as the
inherent difficulty of performance modeling. Through controlled experiments
with synthetic system models, we establish an "analytical matrix" to measure
these concepts. Our findings show that modeling hardness is primarily driven by
the number of modules and configuration options per module. More importantly,
we demonstrate that both higher levels of structural knowledge and increased
modeling hardness significantly enhance the opportunity for improvement. The
impact of these factors varies by performance metric; for ranking accuracy
(e.g., in debugging task), structural knowledge is more dominant, while for
prediction accuracy (e.g., in resource management task), hardness plays a
stronger role. These results provide actionable insights for system designers,
guiding them to strategically allocate time and select appropriate modeling
approaches based on a system's characteristics and a given task's objectives.

</details>


### [201] [ViScratch: Using Large Language Models and Gameplay Videos for Automated Feedback in Scratch](https://arxiv.org/abs/2509.11065)
*Yuan Si,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: ViScratch是首个结合代码和游戏视频的多模态Scratch调试系统，通过视觉语言模型和两阶段修复流程显著提升调试效果。


<details>
  <summary>Details</summary>
Motivation: 现有Scratch调试工具依赖预定义规则或手动输入，且忽视了平台的视觉特性。ViScratch旨在利用游戏视频和代码进行多模态反馈，提升调试效果。

Method: ViScratch采用两阶段流程：首先通过视觉语言模型对齐视觉症状与代码结构识别关键问题，然后提出最小化的抽象语法树级别修复并在Scratch虚拟机中验证。

Result: ViScratch在真实项目测试中显著优于现有LLM工具和人类测试者，尤其在bug识别和修复质量上表现突出。

Conclusion: ViScratch通过结合视觉和代码分析，显著提升了Scratch项目的调试效率和质量，展示了视频作为视觉编程环境中一流规范的潜力。

Abstract: Block-based programming environments such as Scratch are increasingly popular
in programming education, in particular for young learners. While the use of
blocks helps prevent syntax errors, semantic bugs remain common and difficult
to debug. Existing tools for Scratch debugging rely heavily on predefined rules
or user manual inputs, and crucially, they ignore the platform's inherently
visual nature.
  We introduce ViScratch, the first multimodal feedback generation system for
Scratch that leverages both the project's block code and its generated gameplay
video to diagnose and repair bugs. ViScratch uses a two-stage pipeline: a
vision-language model first aligns visual symptoms with code structure to
identify a single critical issue, then proposes minimal, abstract syntax tree
level repairs that are verified via execution in the Scratch virtual machine.
  We evaluate ViScratch on a set of real-world Scratch projects against
state-of-the-art LLM-based tools and human testers. Results show that gameplay
video is a crucial debugging signal: ViScratch substantially outperforms prior
tools in both bug identification and repair quality, even without access to
project descriptions or goals. This work demonstrates that video can serve as a
first-class specification in visual programming environments, opening new
directions for LLM-based debugging beyond symbolic code alone.

</details>


### [202] [Rethinking Technology Stack Selection with AI Coding Proficiency](https://arxiv.org/abs/2509.11132)
*Xiaoyu Zhang,Weipeng Jiang,Juan Zhai,Shiqing Ma,Qingshuang Bao,Chenhao Lin,Chao Shen,Tianlin Li,Yang Liu*

Main category: cs.SE

TL;DR: 本文提出AI编码熟练度概念，评估LLM生成代码的质量，发现库和模型间的质量差异会影响技术选择，呼吁社区整合AI熟练度评估以保持技术多样性。


<details>
  <summary>Details</summary>
Motivation: 传统技术栈选择方法忽视了LLM是否能有效利用所选技术，导致团队可能选择LLM无法有效使用的技术，增加调试负担和技术债务。

Method: 本文进行了首次全面的实证研究，考察了170个第三方库和61个任务场景中的AI熟练度，评估了六种广泛使用的LLM。

Result: 研究发现，功能相似的库在LLM生成代码质量得分上差异可达84%，不同模型在使用同一库时的生成结果也存在质量差距。这些差距转化为实际工程成本，可能引导开发者选择AI编码熟练度高的库，威胁技术多样性。

Conclusion: 本文呼吁社区将AI编码熟练度评估纳入技术选择框架，并开发缓解策略，以保持AI驱动开发中的竞争平衡。

Abstract: Large language models (LLMs) are now an integral part of software development
workflows and are reshaping the whole process. Traditional technology stack
selection has not caught up. Most of the existing selection methods focus
solely on the inherent attributes of the technology, overlooking whether the
LLM can effectively leverage the chosen technology. For example, when
generating code snippets using popular libraries like Selenium (one of the most
widely used test automation tools with over 33k GitHub stars), existing LLMs
frequently generate low-quality code snippets (e.g., using deprecated APIs and
methods, or containing syntax errors). As such, teams using LLM assistants risk
choosing technologies that cannot be used effectively by LLMs, yielding high
debugging effort and mounting technical debt. We foresee a practical question
in the LLM era, is a technology ready for AI-assisted development? In this
paper, we first propose the concept, AI coding proficiency, the degree to which
LLMs can utilize a given technology to generate high-quality code snippets. We
conduct the first comprehensive empirical study examining AI proficiency across
170 third-party libraries and 61 task scenarios, evaluating six widely used
LLMs. Our findings reveal that libraries with similar functionalities can
exhibit up to 84% differences in the quality score of LLM-generated code, while
different models also exhibit quality gaps among their generation results using
the same library. These gaps translate into real engineering costs and can
steer developer choices toward a narrow set of libraries with high AI coding
proficiency, threatening technological diversity in the ecosystem. We call on
the community to integrate AI proficiency assessments into technology selection
frameworks and develop mitigation strategies, preserving competitive balance in
AI-driven development.

</details>


### [203] [UserTrace: User-Level Requirements Generation and Traceability Recovery from Software Project Repositories](https://arxiv.org/abs/2509.11238)
*Dongming Jin,Zhi Jin,Yiran Zhang,Zheng Fang,Linyu Li,Yuanpeng He,Xiaohong Chen,Weisong Sun*

Main category: cs.SE

TL;DR: UserTrace是一个多智能体系统，自动生成用户级需求并恢复实时追踪链接，优于现有方法，支持用户验证AI生成软件。


<details>
  <summary>Details</summary>
Motivation: 现有自动化代码摘要（ACS）方法主要生成开发者导向的实现级需求（IRs），而需求追踪（RT）技术常忽略项目演化的影响，导致用户级需求（URs）和实时追踪链接未被充分探索，尽管它们对支持用户理解和验证AI生成软件是否符合用户意图至关重要。

Method: UserTrace协调四个专门化智能体（代码审查员、搜索器、编写器和验证器）通过三个阶段：结构化仓库依赖、为代码单元推导实现级需求（IRs）、以及合成具有领域特定上下文的用户级需求（URs）。

Result: 评估表明，UserTrace生成的URs在完整性、正确性和有用性上优于现有基线，且在追踪链接恢复的精确度上优于五种最先进的RT方法。用户研究进一步证实UserTrace有助于用户验证AI生成仓库是否符合其意图。

Conclusion: UserTrace通过多智能体系统自动生成用户级需求（URs）并恢复实时追踪链接，显著提升了需求描述的完整性和正确性，以及追踪链接的精确度，有效支持用户验证AI生成软件是否符合其意图。

Abstract: Software maintainability critically depends on high-quality requirements
descriptions and explicit traceability between requirements and code. Although
automated code summarization (ACS) and requirements traceability (RT)
techniques have been widely studied, existing ACS methods mainly generate
implementation-level (i.e., developer-oriented) requirements (IRs) for
fine-grained units (e.g., methods), while RT techniques often overlook the
impact of project evolution. As a result, user-level (i.e., end user-oriented)
requirements (URs) and live trace links remain underexplored, despite their
importance for supporting user understanding and for validating whether
AI-generated software aligns with user intent. To address this gap, we propose
UserTrace, a multi-agent system that automatically generates URs and recovers
live trace links (from URs to IRs to code) from software repositories.
UserTrace coordinates four specialized agents (i.e., Code Reviewer, Searcher,
Writer, and Verifier) through a three-phase process: structuring repository
dependencies, deriving IRs for code units, and synthesizing URs with
domain-specific context. Our comparative evaluation shows that UserTrace
produces URs with higher completeness, correctness, and helpfulness than an
established baseline, and achieves superior precision in trace link recovery
compared to five state-of-the-art RT approaches. A user study further
demonstrates that UserTrace helps end users validate whether the AI-generated
repositories align with their intent.

</details>


### [204] [Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation](https://arxiv.org/abs/2509.11252)
*Chengze li,Yitong Zhang,Jia Li,Liyi Cai,Ge Li*

Main category: cs.SE

TL;DR: 扩散LLMs通过多令牌预测和灵活生成顺序解决了自回归LLMs的局限性，在代码生成中表现优异，尤其在长代码任务中。


<details>
  <summary>Details</summary>
Motivation: 现有自回归LLMs在代码生成中存在效率低和生成顺序固定的局限性，扩散LLMs通过多令牌预测和灵活生成顺序提供了潜在解决方案。

Method: 研究涉及9种代表性的扩散LLMs，并在4个广泛使用的基准上进行实验。

Result: 扩散LLMs在代码生成中与自回归LLMs表现相当，具备更强的长度外推能力和长代码理解能力。

Conclusion: 扩散LLMs在代码生成领域展现出与自回归LLMs相当的竞争力，尤其在长代码理解和长度外推能力上表现更优。研究还提供了实用指南，并指出了未来改进的方向。

Abstract: LLMs have become the mainstream approaches to code generation. Existing LLMs
mainly employ autoregressive generation, i.e. generating code token-by-token
from left to right. However, the underlying autoregressive generation has two
limitations in code generation. First, autoregressive LLMs only generate a
token at each step, showing low efficiency in practice. Second, programming is
a non-sequential process involving back-and-forth editing, while autoregressive
LLMs only employ the left-to-right generation order. These two intrinsic
limitations hinder the further development of LLMs in code generation.
Recently, diffusion LLMs have emerged as a promising alternative. Diffusion
LLMs address the above limitations with two advances, including multi-token
prediction (i.e. generating multiple tokens at each step) and flexible
generation order (i.e. flexibly determining which positions to generate
tokens). However, there is no systematic study exploring diffusion LLMs in code
generation. To bridge the knowledge gap, we present the first empirical study
of diffusion LLMs for code generation. Our study involves 9 representative
diffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on
the results, we summarize the following findings. (1) Existing diffusion LLMs
are competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs
have a stronger length extrapolation ability than autoregressive LLMs and
perform better in long code understanding. (3) We explore factors impacting the
effectiveness and efficiency of diffusion LLMs, and provide practical guidance.
(4) We discuss several promising further directions to improve diffusion LLMs
on code generation. We open-source all source code, data, and results to
facilitate the following research. The code is publicly available at
https://github.com/zhangyitonggg/dllm4code.

</details>


### [205] [A Web-Based Environment for the Specification and Generation of Smart Legal Contracts](https://arxiv.org/abs/2509.11258)
*Regan Meloche,Durga Sivakumar,Amal A. Anda,Sofana Alfuhaid,Daniel Amyot,Luigi Logrippo,John Mylopoulos*

Main category: cs.SE

TL;DR: 论文提出了一种Web环境，通过用户辅助细化Symboleo规范并自动生成智能合约，填补了自然语言合同与智能合约实现之间的差距，展示了在法律合规背景下的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 监测合同履行与法律义务的合规性对于及时发现违规行为至关重要。虽然智能合约可以提供防篡改保护和一定程度的违规处理自动化，但自然语言合同与智能合约实现之间存在巨大差距。

Method: 引入了一个支持用户辅助细化Symboleo规范（对应法律合同模板）的Web环境，随后自动生成可部署在Hyperledger Fabric平台上的监控智能合约。

Result: 通过一个来自交互能源领域的示例合同展示了该环境的有效性。

Conclusion: 该论文提出的基于Web的环境在加速法律合规背景下智能合约的开发方面显示出巨大潜力。

Abstract: Monitoring the compliance of contract performance against legal obligations
is important in order to detect violations, ideally, as soon as they occur.
Such monitoring can nowadays be achieved through the use of smart contracts,
which provide protection against tampering as well as some level of automation
in handling violations. However, there exists a large gap between natural
language contracts and smart contract implementations. This paper introduces a
Web-based environment that partly fills that gap by supporting the
user-assisted refinement of Symboleo specifications corresponding to legal
contract templates, followed by the automated generation of monitoring smart
contracts deployable on the Hyperledger Fabric platform. This environment,
illustrated using a sample contract from the transactive energy domain, shows
much potential in accelerating the development of smart contracts in a legal
compliance context.

</details>


### [206] [Weakly Supervised Vulnerability Localization via Multiple Instance Learning](https://arxiv.org/abs/2509.11312)
*Wenchao Gu,Yupan Chen,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: WAVES 是一种基于多实例学习的弱监督漏洞定位方法，无需语句级标注，实现了高效的漏洞检测和定位。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法多为粗粒度（如函数或文件级），开发者仍需手动检查大量代码以定位具体漏洞语句，而语句级标注成本高。

Method: WAVES 采用多实例学习（MIL）方法，将函数级标签转换为语句级伪标签，从而训练分类器。

Result: 在三个流行基准数据集上的实验表明，WAVES 在漏洞检测和语句级漏洞定位上均表现出色。

Conclusion: WAVES 方法在无需额外语句级标注的情况下，实现了与现有方法相当的漏洞检测性能和最先进的语句级漏洞定位性能。

Abstract: Software vulnerability detection has emerged as a significant concern in the
field of software security recently, capturing the attention of numerous
researchers and developers. Most previous approaches focus on coarse-grained
vulnerability detection, such as at the function or file level. However, the
developers would still encounter the challenge of manually inspecting a large
volume of code inside the vulnerable function to identify the specific
vulnerable statements for modification, indicating the importance of
vulnerability localization. Training the model for vulnerability localization
usually requires ground-truth labels at the statement-level, and labeling
vulnerable statements demands expert knowledge, which incurs high costs. Hence,
the demand for an approach that eliminates the need for additional labeling at
the statement-level is on the rise. To tackle this problem, we propose a novel
approach called WAVES for WeAkly supervised Vulnerability Localization via
multiplE inStance learning, which does not need the additional statement-level
labels during the training. WAVES has the capability to determine whether a
function is vulnerable (i.e., vulnerability detection) and pinpoint the
vulnerable statements (i.e., vulnerability localization). Specifically,
inspired by the concept of multiple instance learning, WAVES converts the
ground-truth label at the function-level into pseudo labels for individual
statements, eliminating the need for additional statement-level labeling. These
pseudo labels are utilized to train the classifiers for the function-level
representation vectors. Extensive experimentation on three popular benchmark
datasets demonstrates that, in comparison to previous baselines, our approach
achieves comparable performance in vulnerability detection and state-of-the-art
performance in statement-level vulnerability localization.

</details>


### [207] [Large Language Models (LLMs) for Requirements Engineering (RE): A Systematic Literature Review](https://arxiv.org/abs/2509.11446)
*Mohammad Amin Zadenoori,Jacek Dąbrowski,Waad Alhoshan,Liping Zhao,Alessio Ferrari*

Main category: cs.SE

TL;DR: 本文通过系统性文献综述分析了2023-2024年间LLMs在需求工程中的应用，发现主要集中在需求获取和验证，使用GPT模型和零/少样本提示，未来需探索更多任务和真实环境测试。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在需求工程（RE）中的应用，分析其如何辅助复杂、语言密集型任务，并与传统NLP技术进行比较。

Method: 通过对2023年至2024年间发表的74项主要研究进行系统性文献综述，研究从多个维度对文献进行了分类，包括发表趋势、RE活动、提示策略和评估方法。

Result: 研究发现LLMs在需求工程中的应用主要集中在需求获取和验证上，而非以往的缺陷检测和分类。研究还注意到GPT模型的使用较多，多采用零样本或少样本提示策略，且多在受控环境中评估，工业环境中的应用和复杂工作流中的整合有限。

Conclusion: 该研究为研究人员和实践者提供了利用LLMs在需求工程中更有效的方法，包括已识别的工具和数据集列表，并指出了未来研究方向，如扩展RE在SE中的影响力、探索较少研究的任务、改进提示方法以及在真实环境中测试。

Abstract: Large Language Models (LLMs) are finding applications in numerous domains,
and Requirements Engineering (RE) is increasingly benefiting from their
capabilities to assist with complex, language-intensive tasks. This paper
presents a systematic literature review of 74 primary studies published between
2023 and 2024, examining how LLMs are being applied in RE. The study
categorizes the literature according to several dimensions, including
publication trends, RE activities, prompting strategies, and evaluation
methods. Our findings indicate notable patterns, among which we observe
substantial differences compared to previous works leveraging standard Natural
Language Processing (NLP) techniques. Most of the studies focus on using LLMs
for requirements elicitation and validation, rather than defect detection and
classification, which were dominant in the past. Researchers have also
broadened their focus and addressed novel tasks, e.g., test generation,
exploring the integration of RE with other software engineering (SE)
disciplines. Although requirements specifications remain the primary focus,
other artifacts are increasingly considered, including issues from issue
tracking systems, regulations, and technical manuals. The studies mostly rely
on GPT-based models, and often use Zero-shot or Few-shot prompting. They are
usually evaluated in controlled environments, with limited use in industry
settings and limited integration in complex workflows. Our study outlines
important future directions, such as leveraging the potential to expand the
influence of RE in SE, exploring less-studied tasks, improving prompting
methods, and testing in real-world environments. Our contribution also helps
researchers and practitioners use LLMs more effectively in RE, by providing a
list of identified tools leveraging LLMs for RE, as well as datasets.

</details>


### [208] [VulAgent: Hypothesis-Validation based Multi-Agent Vulnerability Detection](https://arxiv.org/abs/2509.11523)
*Ziliang Wang,Ge Li,Jia Li,Hao Zhu,Zhi Jin*

Main category: cs.SE

TL;DR: VulAgent是一个基于假设验证的多代理漏洞检测框架，通过模仿人类审计流程，显著提升了检测性能和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在项目级漏洞检测中难以准确定位安全敏感代码并关联复杂程序上下文，VulAgent旨在模仿人类审计员的工作流程以解决这些问题。

Method: VulAgent采用多代理框架，每个代理专注于特定分析视角（如内存、授权），通过假设验证范式（构建假设条件和触发路径）引导LLM验证相关程序上下文和防御检查。

Result: 在两个数据集上，VulAgent平均提高整体准确率6.6%，漏洞-修复代码对正确识别率最高提升450%（平均246%），误报率降低约36%。

Conclusion: VulAgent通过假设验证范式和多视角检测流程，显著提高了漏洞检测的准确性和覆盖率，同时降低了误报率。

Abstract: The application of language models to project-level vulnerability detection
remains challenging, owing to the dual requirement of accurately localizing
security-sensitive code and correctly correlating and reasoning over complex
program context. We present VulAgent, a multi-agent vulnerability detection
framework based on hypothesis validation. Our design is inspired by how human
auditors review code: when noticing a sensitive operation, they form a
hypothesis about a possible vulnerability, consider potential trigger paths,
and then verify the hypothesis against the surrounding context. VulAgent
implements a semantics-sensitive, multi-view detection pipeline: specialized
agents, each aligned to a specific analysis perspective (e.g., memory,
authorization), collaboratively surface and precisely localize sensitive code
sites with higher coverage. Building on this, VulAgent adopts a
hypothesis-validation paradigm: for each vulnerability report, it builds
hypothesis conditions and a trigger path, steering the LLM to target the
relevant program context and defensive checks during verification, which
reduces false positives. On average across the two datasets, VulAgent improves
overall accuracy by 6.6%, increases the correct identification rate of
vulnerable--fixed code pairs by up to 450% (246% on average), and reduces the
false positive rate by about 36% compared with state-of-the-art LLM-based
baselines.

</details>


### [209] [Sedeve-Kit, a Specification-Driven Development Framework for Building Distributed Systems](https://arxiv.org/abs/2509.11566)
*Hua Guo,Yunhong Ji,Xuan Zhou*

Main category: cs.SE

TL;DR: 该论文提出了一种规范驱动的分布式系统开发框架，通过三个阶段（规范定义、编码、测试）确保系统质量，并解决了非确定性并发和故障带来的复杂性。


<details>
  <summary>Details</summary>
Motivation: 分布式系统的开发面临非确定性并发和故障带来的复杂性挑战。

Method: 方法包括三个阶段：1) 使用TLA${^+}$定义系统规范和不变式，进行模型检查并生成测试用例；2) 根据规范编写代码，确保实现的一致性和准确性；3) 使用初始阶段生成的测试用例对系统进行严格测试。

Result: 通过规范驱动的方法，确保了算法的正确性，并在实现过程中保持了系统质量。

Conclusion: 提出的规范驱动开发框架通过持续的验证过程，确保了系统质量，并保持了抽象设计与具体实现之间的紧密联系。

Abstract: Developing distributed systems presents significant challenges, primarily due
to the complexity introduced by non-deterministic concurrency and faults. To
address these, we propose a specification-driven development framework. Our
method encompasses three key stages. The first stage defines system
specifications and invariants using TLA${^+}$. It allows us to perform model
checking on the algorithm's correctness and generate test cases for subsequent
development phases. In the second stage, based on the established
specifications, we write code to ensure consistency and accuracy in the
implementation. Finally, after completing the coding process, we rigorously
test the system using the test cases generated in the initial stage. This
process ensures system quality by maintaining a strong connection between the
abstract design and the concrete implementation through continuous
verification.

</details>


### [210] [Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools](https://arxiv.org/abs/2509.11626)
*Prerna Agarwal,Himanshu Gupta,Soujanya Soni,Rohith Vallam,Renuka Sindhgatta,Sameep Mehta*

Main category: cs.SE

TL;DR: ACE框架通过自动化工具创建、丰富和动态选择，解决了企业API在LLM代理中的使用难题，提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 企业API工具在LLM代理中的使用因文档不全、输入/输出模式复杂及操作数量庞大而受限，导致工具选择困难且有效载荷形成的准确性降低。

Method: ACE框架通过生成增强的工具规范（包括参数描述和示例）以及动态筛选机制，优化工具选择和调用准确性。

Result: ACE框架在专有和开源API上验证有效，并与代理框架成功集成，显著提升了工具选择和调用准确性。

Conclusion: ACE是一个端到端框架，首次实现了企业API工具的自动化创建、丰富和动态选择，为LLM代理提供了更高效的工具集成方案。

Abstract: Recent advancements in Large Language Models (LLMs) has lead to the
development of agents capable of complex reasoning and interaction with
external tools. In enterprise contexts, the effective use of such tools that
are often enabled by application programming interfaces (APIs), is hindered by
poor documentation, complex input or output schema, and large number of
operations. These challenges make tool selection difficult and reduce the
accuracy of payload formation by up to 25%. We propose ACE, an automated tool
creation and enrichment framework that transforms enterprise APIs into
LLM-compatible tools. ACE, (i) generates enriched tool specifications with
parameter descriptions and examples to improve selection and invocation
accuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters
relevant tools at runtime, reducing prompt complexity while maintaining
scalability. We validate our framework on both proprietary and open-source APIs
and demonstrate its integration with agentic frameworks. To the best of our
knowledge, ACE is the first end-to-end framework that automates the creation,
enrichment, and dynamic selection of enterprise API tools for LLM agents.

</details>


### [211] [Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models](https://arxiv.org/abs/2509.11686)
*Jian Wang,Xiaofei Xie,Qiang Hu,Shangqing Liu,Yi Li*

Main category: cs.SE

TL;DR: 研究发现语义信息对提升Code LLMs的推理能力作用有限，与之前结论相反。


<details>
  <summary>Details</summary>
Motivation: Code LLMs在理解程序实际功能和推理运行时行为方面存在显著局限性，这对其后训练和实际部署提出了挑战。

Method: 引入了一个通用框架，支持将语义信息（如执行追踪）集成到代码任务相关的提示中，并全面研究了语义信息在增强Code LLMs推理能力中的作用。

Result: 实验结果表明，语义信息对Code LLMs的监督微调和测试阶段扩展的增强作用有限。

Conclusion: 语义信息（如执行追踪）对于代码大语言模型（Code LLMs）的监督微调（SFT）和测试阶段扩展的实用性有限，这与之前的研究结果相矛盾。

Abstract: Code Large Language Models (Code LLMs) have opened a new era in programming
with their impressive capabilities. However, recent research has revealed
critical limitations in their ability to reason about runtime behavior and
understand the actual functionality of programs, which poses significant
challenges for their post-training and practical deployment. Specifically, Code
LLMs encounter two principal issues: (1) a lack of proficiency in reasoning
about program execution behavior, as they struggle to interpret what programs
actually do during runtime, and (2) the inconsistent and fragmented
representation of semantic information, such as execution traces, across
existing methods, which hinders their ability to generalize and reason
effectively. These challenges underscore the necessity for more systematic
approaches to enhance the reasoning capabilities of Code LLMs. To address these
issues, we introduce a generic framework to support integrating semantic
information~(e.g., execution trace) to code task-relevant prompts, and conduct
a comprehensive study to explore the role of semantic information in enhancing
the reasoning ability of Code LLMs accordingly. Specifically, we focus on
investigating the usefulness of trace-based semantic information in boosting
supervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The
experimental results surprisingly disagree with previous works and demonstrate
that semantic information has limited usefulness for SFT and test time scaling
of Code LLM.

</details>


### [212] [AI Asset Management for Manufacturing (AIM4M): Development of a Process Model for Operationalization](https://arxiv.org/abs/2509.11691)
*Lukas Rauh,Mel-Rick Süner,Daniel Schel,Thomas Bauernhansl*

Main category: cs.SE

TL;DR: 本文提出了一种新的AI资产生命周期管理过程模型，基于MLOps原则，针对CPPS领域优化，以解决制造业中AI运营化的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于技术系统复杂性、缺乏实施标准和碎片化的组织流程，将AI在制造业中超越原型阶段进行运营化仍是一个重大挑战。

Method: 基于机器学习操作（MLOps）原则，并针对CPPS领域的特定需求优化了三个方面。

Result: 提出的过程模型作为一个理论贡献，旨在解决制造业中的挑战，并促进AI生命周期的有效运营化。

Conclusion: 本文提出的过程模型旨在帮助组织在实践中系统地开发、部署和管理AI资产，同时符合CPPS特有的约束和监管要求。

Abstract: The benefits of adopting artificial intelligence (AI) in manufacturing are
undeniable. However, operationalizing AI beyond the prototype, especially when
involved with cyber-physical production systems (CPPS), remains a significant
challenge due to the technical system complexity, a lack of implementation
standards and fragmented organizational processes. To this end, this paper
proposes a new process model for the lifecycle management of AI assets designed
to address challenges in manufacturing and facilitate effective
operationalization throughout the entire AI lifecycle. The process model, as a
theoretical contribution, builds on machine learning operations (MLOps)
principles and refines three aspects to address the domain-specific
requirements from the CPPS context. As a result, the proposed process model
aims to support organizations in practice to systematically develop, deploy and
manage AI assets across their full lifecycle while aligning with CPPS-specific
constraints and regulatory demands.

</details>


### [213] [From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation](https://arxiv.org/abs/2509.11708)
*Zhantong Xue,Pingchuan Ma,Zhaoyu Wang,Shuai Wang*

Main category: cs.SE

TL;DR: 本文提出了ZK-Eval和ZK-Coder，用于评估和增强大型语言模型在零知识代码生成中的能力，显著提高了生成程序的正确性和成功率。


<details>
  <summary>Details</summary>
Motivation: 零知识证明（ZKPs）在隐私保护认证、区块链扩展性和安全金融等领域的应用日益广泛，但编写ZK程序仍然具有挑战性，因为它需要对有限域算术、约束系统和组件进行推理，这使得ZK开发知识密集且容易出错。尽管大型语言模型在通用编程语言中表现出强大的代码生成能力，但在ZK编程中的有效性尚未被探索。

Method: 我们提出了ZK-Eval，一个领域特定的评估流程，用于探测大型语言模型在三个层次上的能力：语言知识、组件能力和端到端程序生成。此外，我们还引入了ZK-Coder，一个增强大型语言模型的代理框架，通过约束草图、引导检索和交互式修复来提高其性能。

Result: 实验结果表明，最先进的大型语言模型在表面语法上表现优异，但在组件使用和语义正确性方面存在困难，经常生成错误的程序。通过ZK-Coder框架，Circom和Noir的成功率分别从17.35%提高到83.38%和从32.21%提高到90.05%。

Conclusion: 通过引入ZK-Eval和ZK-Coder，我们为系统化评估和增强大型语言模型在零知识代码生成中的能力奠定了基础，降低了实践者的门槛并推动了可信计算的发展。

Abstract: Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as
privacy-preserving authentication, blockchain scalability, and secure finance.
However, authoring ZK programs remains challenging: unlike mainstream
programming, ZK development requires reasoning about finite field arithmetic,
constraint systems, and gadgets, making it knowledge-intensive and error-prone.
While large language models (LLMs) have demonstrated strong code generation
capabilities in general-purpose languages, their effectiveness for ZK
programming, where correctness hinges on both language mastery and gadget-level
reasoning, remains unexplored. To address this gap, we propose
\textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM
capabilities at three levels: language knowledge, gadget competence, and
end-to-end program generation. Our evaluation of four state-of-the-art LLMs
reveals that models excel at surface-level syntax but struggle with gadget
usage and semantic correctness, often yielding incorrect programs. Based on
these insights, we introduce \textsc{ZK-Coder}, an agentic framework that
augments LLMs with constraint sketching, guided retrieval, and interactive
repair. Experiments on Circom and Noir show substantial gains, with success
rates improving from 17.35\% to 83.38\% and from 32.21\% to 90.05\%,
respectively. With \textsc{ZK-Eval} and \textsc{ZK-Coder}, we establish a
foundation for systematically measuring and augmenting LLMs in ZK code
generation to lower barriers for practitioners and advance trustworthy
computation.

</details>


### [214] [Toward Greener Background Processes -- Measuring Energy Cost of Autosave Feature](https://arxiv.org/abs/2509.11738)
*Maria Küüsvek,Hina Anwar*

Main category: cs.SE

TL;DR: 本文提出了一种评估桌面应用后台功能能源行为的三阶段方法，并通过案例研究识别了影响能源使用的关键设计因素，给出了四条绿色实现建议。


<details>
  <summary>Details</summary>
Motivation: 桌面应用程序中的后台进程在能源消耗研究中常被忽视，但它们代表了具有显著累积影响的连续自动化工作负载。

Method: 本文介绍了一个可重复使用的过程，用于在操作设计层面评估背景功能的能源行为。该过程分为三个阶段：1）将背景功能分解为核心操作，2）操作隔离，3）控制测量以实现比较分析。

Result: 通过对三个基于Python的开源文本编辑器的自动保存实现进行案例研究，利用900个基于软件的能源测量，识别了影响能源使用的关键设计因素，包括保存频率、缓冲策略和变更检测等辅助逻辑。

Conclusion: 本文提出了四条可操作的建议，以支持Python中自动保存功能的绿色实现，促进可持续软件开发实践。

Abstract: Background processes in desktop applications are often overlooked in energy
consumption studies, yet they represent continuous, automated workloads with
significant cumulative impact. This paper introduces a reusable process for
evaluating the energy behavior of such features at the level of operational
design. The process works in three phases: 1) decomposing background
functionality into core operations, 2) operational isolation, and 3) controlled
measurements enabling comparative profiling. We instantiate the process in a
case study of autosave implementations across three open-source Python-based
text editors. Using 900 empirical software-based energy measurements, we
identify key design factors affecting energy use, including save frequency,
buffering strategy, and auxiliary logic such as change detection. We give four
actionable recommendations for greener implementations of autosave features in
Python to support sustainable software practices.

</details>


### [215] [Analysing Python Machine Learning Notebooks with Moose](https://arxiv.org/abs/2509.11748)
*Marius Mignard,Steven Costiou,Nicolas Anquetil,Anne Etien*

Main category: cs.SE

TL;DR: Vespucci Linter是一个多级静态分析工具，能有效识别和改善ML笔记本代码的质量问题。


<details>
  <summary>Details</summary>
Motivation: 现有分析工具通常只关注单一层次，难以捕捉ML特定语义，限制了问题的检测能力。因此，需要一种多级分析工具来全面改善ML代码质量。

Method: 本文基于Moose构建了一个静态分析工具Vespucci Linter，采用元建模方法统一笔记本的结构元素与Python代码实体，实现了对三个层次的上下文分析。

Result: 在Kaggle平台的5000个笔记本上应用了22条linting规则，结果显示所有层次均存在违规，验证了多级方法的有效性。

Conclusion: Vespucci Linter通过多级静态分析能力，能够有效识别和改善笔记本环境中ML代码的质量问题，验证了其在实际应用中的潜力。

Abstract: Machine Learning (ML) code, particularly within notebooks, often exhibits
lower quality compared to traditional software. Bad practices arise at three
distinct levels: general Python coding conventions, the organizational
structure of the notebook itself, and ML-specific aspects such as
reproducibility and correct API usage. However, existing analysis tools
typically focus on only one of these levels and struggle to capture ML-specific
semantics, limiting their ability to detect issues. This paper introduces
Vespucci Linter, a static analysis tool with multi-level capabilities, built on
Moose and designed to address this challenge. Leveraging a metamodeling
approach that unifies the notebook's structural elements with Python code
entities, our linter enables a more contextualized analysis to identify issues
across all three levels. We implemented 22 linting rules derived from the
literature and applied our tool to a corpus of 5,000 notebooks from the Kaggle
platform. The results reveal violations at all levels, validating the relevance
of our multi-level approach and demonstrating Vespucci Linter's potential to
improve the quality and reliability of ML development in notebook environments.

</details>


### [216] [CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings](https://arxiv.org/abs/2509.11787)
*Pascal Joos,Islem Bouzenia,Michael Pradel*

Main category: cs.SE

TL;DR: CodeCureAgent利用LLM代理自动修复静态分析警告，通过三步验证确保补丁质量，显著提升修复率和正确率。


<details>
  <summary>Details</summary>
Motivation: 传统上开发者需手动解决静态分析警告，过程繁琐且易被忽略，导致警告积累和代码质量下降。

Method: 采用基于LLM的代理框架，通过三步启发式方法（构建项目、验证警告消失且不引入新警告、运行测试套件）来批准补丁。

Result: 在1,000个SonarQube警告的数据集上，CodeCureAgent为96.8%的警告提供了合理修复，正确修复率为86.3%，优于现有基线方法。

Conclusion: CodeCureAgent能够可靠地修复静态分析警告，并有望帮助清理现有代码库及集成到CI/CD管道中以防止警告积累。

Abstract: Static analysis tools are widely used to detect bugs, vulnerabilities, and
code smells. Traditionally, developers must resolve these warnings manually.
Because this process is tedious, developers sometimes ignore warnings, leading
to an accumulation of warnings and a degradation of code quality. This paper
presents CodeCureAgent, an approach that harnesses LLM-based agents to
automatically analyze, classify, and repair static analysis warnings. Unlike
previous work, our method does not follow a predetermined algorithm. Instead,
we adopt an agentic framework that iteratively invokes tools to gather
additional information from the codebase (e.g., via code search) and edit the
codebase to resolve the warning. CodeCureAgent detects and suppresses false
positives, while fixing true positives when identified. We equip CodeCureAgent
with a three-step heuristic to approve patches: (1) build the project, (2)
verify that the warning disappears without introducing new warnings, and (3)
run the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube
warnings found in 106 Java projects and covering 291 distinct rules. Our
approach produces plausible fixes for 96.8% of the warnings, outperforming
state-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,
respectively. Manual inspection of 291 cases reveals a correct-fix rate of
86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.
The approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end
processing time of about four minutes per warning. We envision CodeCureAgent
helping to clean existing codebases and being integrated into CI/CD pipelines
to prevent the accumulation of static analysis warnings.

</details>


### [217] [MMORE: Massive Multimodal Open RAG & Extraction](https://arxiv.org/abs/2509.11937)
*Alexandre Sallinen,Stefan Krsteski,Paul Teiletche,Marc-Antoine Allard,Baptiste Lecoeur,Michael Zhang,Fabrice Nemo,David Kalajdzic,Matthias Meyer,Mary-Anne Hartley*

Main category: cs.SE

TL;DR: MMORE是一个开源的多模态开放检索增强生成和提取管道，支持多种文件类型，提供高效的分布式处理和混合检索，显著提升处理速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 设计MMORE的目的是为了大规模处理异构文档格式的知识，支持超过15种文件类型，包括文本、表格、图像、电子邮件、音频和视频，并将它们转换为统一格式以支持下游LLM应用。

Method: MMORE采用模块化、分布式处理架构，支持在CPU和GPU上进行可扩展的并行化处理，并集成了混合密集-稀疏检索技术。

Result: 在处理基准测试中，MMORE比单节点基线快3.8倍，在扫描PDF上的准确率比Docling高40%。在PubMedQA上的评估显示，MMORE增强的医学LLMs随着检索深度的增加提高了生物医学QA的准确性。

Conclusion: MMORE提供了一个强大、可扩展的基础，用于在多样化的现实世界多模态数据上部署任务无关的RAG系统。

Abstract: We introduce MMORE, an open-source pipeline for Massive Multimodal Open
RetrievalAugmented Generation and Extraction, designed to ingest, transform,
and retrieve knowledge from heterogeneous document formats at scale. MMORE
supports more than fifteen file types, including text, tables, images, emails,
audio, and video, and processes them into a unified format to enable downstream
applications for LLMs. The architecture offers modular, distributed processing,
enabling scalable parallelization across CPUs and GPUs. On processing
benchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines
and 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates
hybrid dense-sparse retrieval and supports both interactive APIs and batch RAG
endpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve
biomedical QA accuracy with increasing retrieval depth. MMORE provides a
robust, extensible foundation for deploying task-agnostic RAG systems on
diverse, real-world multimodal data. The codebase is available at
https://github.com/swiss-ai/mmore.

</details>


### [218] [VisDocSketcher: Towards Scalable Visual Documentation with Agentic Systems](https://arxiv.org/abs/2509.11942)
*Luís F. Gomes,Xin Zhou,David Lo,Rui Abreu*

Main category: cs.SE

TL;DR: 论文提出首个基于LLM代理的自动化视觉文档生成方法VisDocSketcher及评估框架AutoSketchEval，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 视觉文档能有效降低开发者理解陌生代码的认知障碍，但手动创建耗时且缺乏自动化生成和标准化评估方法。

Method: 结合静态分析与LLM代理的VisDocSketcher方法，以及评估框架AutoSketchEval。

Result: 实验结果显示，该方法能为74.4%的样本生成有效视觉文档，比简单模板基线提升26.7-39.8%，评估框架AUC超过0.87。

Conclusion: 该论文为自动化视觉文档的未来研究奠定了基础，通过引入实用工具不仅生成有效的视觉表示，还能可靠评估其质量。

Abstract: Visual documentation is an effective tool for reducing the cognitive barrier
developers face when understanding unfamiliar code, enabling more intuitive
comprehension. Compared to textual documentation, it provides a higher-level
understanding of the system structure and data flow. Developers usually prefer
visual representations over lengthy textual descriptions for large software
systems. Visual documentation is both difficult to produce and challenging to
evaluate. Manually creating it is time-consuming, and currently, no existing
approach can automatically generate high-level visual documentation directly
from code. Its evaluation is often subjective, making it difficult to
standardize and automate. To address these challenges, this paper presents the
first exploration of using agentic LLM systems to automatically generate visual
documentation. We introduce VisDocSketcher, the first agent-based approach that
combines static analysis with LLM agents to identify key elements in the code
and produce corresponding visual representations. We propose a novel evaluation
framework, AutoSketchEval, for assessing the quality of generated visual
documentation using code-level metrics. The experimental results show that our
approach can valid visual documentation for 74.4% of the samples. It shows an
improvement of 26.7-39.8% over a simple template-based baseline. Our evaluation
framework can reliably distinguish high-quality (code-aligned) visual
documentation from low-quality (non-aligned) ones, achieving an AUC exceeding
0.87. Our work lays the foundation for future research on automated visual
documentation by introducing practical tools that not only generate valid
visual representations but also reliably assess their quality.

</details>


### [219] [LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code Analysis](https://arxiv.org/abs/2509.12021)
*Benedikt Fein,Florian Obermüller,Gordon Fraser*

Main category: cs.SE

TL;DR: LitterBox+框架将LLMs整合到Scratch，通过文本转换解决图形化编程限制，提供API和界面扩展支持查询和修复生成。


<details>
  <summary>Details</summary>
Motivation: 解决图形化编程环境Scratch中LLMs应用的局限性，帮助开发者查询程序、质量问题和生成代码修复。

Method: 提出LitterBox+框架，扩展了Scratch静态代码分析工具LitterBox，结合LLMs生成能力，将块代码转换为文本表示，并提供了API和Scratch用户界面扩展。

Result: LitterBox+框架成功实现了LLMs在Scratch环境中的应用，提供了API和用户界面扩展，支持程序查询、质量问题和代码修复生成。

Conclusion: LitterBox+框架成功地将LLMs的能力整合到Scratch环境中，通过将基于块的代码转换为适合LLMs的文本表示，解决了图形化编程环境中LLMs应用的局限性。

Abstract: Large language models (LLMs) have become an essential tool to support
developers using traditional text-based programming languages, but the
graphical notation of the block-based Scratch programming environment inhibits
the use of LLMs. To overcome this limitation, we propose the LitterBox+
framework that extends the Scratch static code analysis tool LitterBox with the
generative abilities of LLMs. By converting block-based code to a textual
representation suitable for LLMs, LitterBox+ allows users to query LLMs about
their programs, about quality issues reported by LitterBox, and it allows
generating code fixes. Besides offering a programmatic API for these
functionalities, LitterBox+ also extends the Scratch user interface to make
these functionalities available directly in the environment familiar to
learners. The framework is designed to be easily extensible with other prompts,
LLM providers, and new features combining the program analysis capabilities of
LitterBox with the generative features of LLMs. We provide a screencast
demonstrating the tool at https://youtu.be/RZ6E0xgrIgQ.

</details>


### [220] [A New Benchmark for Evaluating Code Translation with Third-Party Libraries](https://arxiv.org/abs/2509.12087)
*Pengyu Xue,Kunwu Zheng,Zhen Yang,Yifei Pei,Linhao Wu,Jiahui Dong,Xiapu Luo,Yan Xiao,Fei Liu,Yuxuan Zhang,Xiran Lyu,Xianhang Li,Xuanyu Zhu,Chengyi Wang*

Main category: cs.SE

TL;DR: TransLibEval是首个专注于库中心代码翻译的基准，揭示了LLM在TPL相关翻译中的性能下降和独特挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基准在TPL类别和规模上有限，难以暴露TPL相关错误，阻碍针对性解决方案的发展。

Method: 构建了TransLibEval基准，评估了七种LLM在六种翻译策略下的表现，并分析了4,831个失败案例。

Result: 实验结果显示，与无库设置相比，性能显著下降（平均CA下降超过60%），不同策略表现出异质性优势。

Conclusion: 研究发现，库中心代码翻译存在独特挑战，并为改进TPL感知代码智能提供了实用指导。

Abstract: In recent years, Large Language Models (LLMs) have been widely studied in the
code translation field on the method, class, and even repository levels.
However, most of these benchmarks are limited in terms of Third-Party Library
(TPL) categories and scales, making TPL-related errors hard to expose and
hindering the development of targeted solutions. Considering the high
dependence (over 90%) on TPLs in practical programming, demystifying and
analyzing LLMs' code translation performance involving various TPLs becomes
imperative. To address this gap, we construct TransLibEval, the first benchmark
dedicated to library-centric code translation. It consists of 200 real-world
tasks across Python, Java, and C++, each explicitly involving TPLs from diverse
categories such as data processing, machine learning, and web development, with
comprehensive dependency coverage and high-coverage test suites. We evaluate
seven recent LLMs of commercial, general, and code-specialized families under
six translation strategies of three categories: Direct, IR-guided, and
Retrieval-augmented. Experimental results show a dramatic performance drop
compared with library-free settings (average CA decline over 60%), while
diverse strategies demonstrate heterogeneous advantages. Furthermore, we
analyze 4,831 failed cases from GPT-4o, one of the State-of-the-Art (SOTA)
LLMs, revealing numerous third-party reference errors that were obscured
previously. These findings highlight the unique challenges of library-centric
translation and provide practical guidance for improving TPL-aware code
intelligence.

</details>


### [221] [EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and Output Token Compression](https://arxiv.org/abs/2509.12159)
*Jingyu Xiao,Zhongyi Zhang,Yuxuan Wan,Yintong Huo,Yang Liu,Michael R. Lyu*

Main category: cs.SE

TL;DR: EfficientUICoder 通过压缩冗余令牌，显著提升了 UI 代码生成的效率，同时保持质量。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在 UI2Code 任务中表现优异，但计算开销高，且存在冗余令牌导致生成代码冗长且无效。

Method: 1. 元素和布局感知令牌压缩；2. 区域感知令牌细化；3. 自适应重复令牌抑制。

Result: 实验表明，EfficientUICoder 实现了 55%-60% 的压缩率，计算成本降低 44.9%，生成令牌减少 41.4%，预填充时间减少 46.6%，推理时间减少 48.8%。

Conclusion: EfficientUICoder 是一种高效的 UI 代码生成压缩框架，通过减少冗余图像和代码令牌，显著降低了计算成本，同时保持了网页质量。

Abstract: Multimodal Large Language Models have demonstrated exceptional performance in
UI2Code tasks, significantly enhancing website development efficiency. However,
these tasks incur substantially higher computational overhead than traditional
code generation due to the large number of input image tokens and extensive
output code tokens required. Our comprehensive study identifies significant
redundancies in both image and code tokens that exacerbate computational
complexity and hinder focus on key UI elements, resulting in excessively
lengthy and often invalid HTML files. We propose EfficientUICoder, a
compression framework for efficient UI code generation with three key
components. First, Element and Layout-aware Token Compression preserves
essential UI information by detecting element regions and constructing UI
element trees. Second, Region-aware Token Refinement leverages attention scores
to discard low-attention tokens from selected regions while integrating
high-attention tokens from unselected regions. Third, Adaptive Duplicate Token
Suppression dynamically reduces repetitive generation by tracking HTML/CSS
structure frequencies and applying exponential penalties. Extensive experiments
show EfficientUICoderachieves a 55%-60% compression ratio without compromising
webpage quality and delivers superior efficiency improvements: reducing
computational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%,
and inference time by 48.8% on 34B-level MLLMs. Code is available at
https://github.com/WebPAI/EfficientUICoder.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [222] [Can any model be fabricated? Inverse operation based planning for hybrid additive-subtractive manufacturing](https://arxiv.org/abs/2509.10599)
*Yongxue Chen,Tao Liu,Yuming Huang,Weiming Wang,Tianyu Zhang,Kun Qian,Zikang Shi,Charlie C. L. Wang*

Main category: cs.GR

TL;DR: 提出了一种通过逆向操作序列制造任意形状模型的方法，理论和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决任意形状模型的制造问题，确保制造过程中的结构稳定性和精确性。

Method: 通过搜索一系列逆向操作（加法或减法步骤）来解决制造规划问题，确保中间形状的可制造性和结构稳定性。

Result: 理论证明任何模型都可以通过该方法精确制造，实验验证了其在数字模型和物理制造中的有效性。

Conclusion: 本研究提出了一种能够精确制造任意形状模型的方法，并通过理论和实践验证了其有效性。

Abstract: This paper presents a method for computing interleaved additive and
subtractive manufacturing operations to fabricate models of arbitrary shapes.
We solve the manufacturing planning problem by searching a sequence of inverse
operations that progressively transform a target model into a null shape. Each
inverse operation corresponds to either an additive or a subtractive step,
ensuring both manufacturability and structural stability of intermediate shapes
throughout the process. We theoretically prove that any model can be fabricated
exactly using a sequence generated by our approach. To demonstrate the
effectiveness of this method, we adopt a voxel-based implementation and develop
a scalable algorithm that works on models represented by a large number of
voxels. Our approach has been tested across a range of digital models and
further validated through physical fabrication on a hybrid manufacturing system
with automatic tool switching.

</details>


### [223] [T2Bs: Text-to-Character Blendshapes via Video Generation](https://arxiv.org/abs/2509.10678)
*Jiahao Luo,Chaoyang Wang,Michael Vasilkovsky,Vladislav Shakhrai,Di Liu,Peiye Zhuang,Sergey Tulyakov,Peter Wonka,Hsin-Ying Lee,James Davis,Jian Wang*

Main category: cs.GR

TL;DR: T2Bs结合文本到3D和视频扩散，生成高质量可动画角色头部模型，优于现有4D技术。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到3D模型能生成详细静态几何但缺乏运动合成，而视频扩散模型生成的运动存在时间和多视角几何不一致问题。T2Bs旨在填补这一空白。

Method: T2Bs利用可变形3D高斯点云对齐静态3D资产与视频输出，通过静态几何约束运动和视图依赖变形MLP，减少了视频伪影和视图不一致性。

Result: T2Bs在准确性和表现力上优于现有4D生成方法，同时重建了平滑、连贯、完全注册的3D几何，适用于构建多样且真实面部运动的可变形模型。

Conclusion: T2Bs框架通过结合静态文本到3D生成和视频扩散，成功生成了高质量、可动画化的角色头部可变形模型，超越了现有的4D生成技术。

Abstract: We present T2Bs, a framework for generating high-quality, animatable
character head morphable models from text by combining static text-to-3D
generation with video diffusion. Text-to-3D models produce detailed static
geometry but lack motion synthesis, while video diffusion models generate
motion with temporal and multi-view geometric inconsistencies. T2Bs bridges
this gap by leveraging deformable 3D Gaussian splatting to align static 3D
assets with video outputs. By constraining motion with static geometry and
employing a view-dependent deformation MLP, T2Bs (i) outperforms existing 4D
generation methods in accuracy and expressiveness while reducing video
artifacts and view inconsistencies, and (ii) reconstructs smooth, coherent,
fully registered 3D geometries designed to scale for building morphable models
with diverse, realistic facial motions. This enables synthesizing expressive,
animatable character heads that surpass current 4D generation techniques.

</details>


### [224] [AD-GS: Alternating Densification for Sparse-Input 3D Gaussian Splatting](https://arxiv.org/abs/2509.11003)
*Gurutva Patle,Nilay Girgaonkar,Nagabhushan Somraj,Rajiv Soundararajan*

Main category: cs.GR

TL;DR: AD-GS通过交替密集化框架有效解决了3DGS在稀疏视图下的几何不准确和过拟合问题，显著提升了渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS在稀疏视图下因不受控的密集化导致几何不准确和过拟合问题，AD-GS旨在通过交替密集化框架解决这些问题。

Method: 提出AD-GS，一种交替密集化框架，交替进行高密集化和低密集化阶段。高密集化阶段通过光度损失训练捕获细节，低密集化阶段通过伪视图一致性和边缘感知深度平滑性修剪高斯并正则化其几何。

Result: 在挑战性数据集上的实验表明，AD-GS显著提升了渲染质量和几何一致性。

Conclusion: AD-GS显著提升了渲染质量和几何一致性，优于现有方法。

Abstract: 3D Gaussian Splatting (3DGS) has shown impressive results in real-time novel
view synthesis. However, it often struggles under sparse-view settings,
producing undesirable artifacts such as floaters, inaccurate geometry, and
overfitting due to limited observations. We find that a key contributing factor
is uncontrolled densification, where adding Gaussian primitives rapidly without
guidance can harm geometry and cause artifacts. We propose AD-GS, a novel
alternating densification framework that interleaves high and low densification
phases. During high densification, the model densifies aggressively, followed
by photometric loss based training to capture fine-grained scene details. Low
densification then primarily involves aggressive opacity pruning of Gaussians
followed by regularizing their geometry through pseudo-view consistency and
edge-aware depth smoothness. This alternating approach helps reduce overfitting
by carefully controlling model capacity growth while progressively refining the
scene representation. Extensive experiments on challenging datasets demonstrate
that AD-GS significantly improves rendering quality and geometric consistency
compared to existing methods.

</details>


### [225] [SH-SAS: An Implicit Neural Representation for Complex Spherical-Harmonic Scattering Fields for 3D Synthetic Aperture Sonar](https://arxiv.org/abs/2509.11087)
*Omkar Shailendra Vengurlekar,Adithya Pediredla,Suren Jayasuriya*

Main category: cs.GR

TL;DR: SH-SAS通过球形谐波系数建模声散射场，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统时间域反向投影算法在方向性建模和采样限制上的不足。

Method: 引入SH-SAS，一种隐式神经表示，通过球形谐波系数建模复杂声散射场。

Result: SH-SAS在合成和真实SAS基准测试中表现更优。

Conclusion: SH-SAS方法在3D重建质量和几何指标上优于先前的方法，适用于合成和真实SAS数据。

Abstract: Synthetic aperture sonar (SAS) reconstruction requires recovering both the
spatial distribution of acoustic scatterers and their direction-dependent
response. Time-domain backprojection is the most common 3D SAS reconstruction
algorithm, but it does not model directionality and can suffer from sampling
limitations, aliasing, and occlusion. Prior neural volumetric methods applied
to synthetic aperture sonar treat each voxel as an isotropic scattering
density, not modeling anisotropic returns. We introduce SH-SAS, an implicit
neural representation that expresses the complex acoustic scattering field as a
set of spherical harmonic (SH) coefficients. A multi-resolution hash encoder
feeds a lightweight MLP that outputs complex SH coefficients up to a specified
degree L. The zeroth-order coefficient acts as an isotropic scattering field,
which also serves as the density term, while higher orders compactly capture
directional scattering with minimal parameter overhead. Because the model
predicts the complex amplitude for any transmit-receive baseline, training is
performed directly from 1-D time-of-flight signals without the need to beamform
intermediate images for supervision. Across synthetic and real SAS (both in-air
and underwater) benchmarks, results show that SH-SAS performs better in terms
of 3D reconstruction quality and geometric metrics than previous methods.

</details>


### [226] [3D Gaussian Modeling and Ray Marching of OpenVDB datasets for Scientific Visualization](https://arxiv.org/abs/2509.11377)
*Isha Sharma,Dieter Schmalstieg*

Main category: cs.GR

TL;DR: 本文利用OpenVDB和3D高斯粒子优化科学可视化中的稀疏体数据建模与压缩，提出高效渲染算法并验证其广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 科学可视化中常用的密集网格数据结构内存效率低下，而OpenVDB的稀疏表示方法为3D高斯粒子建模提供了压缩起点，适用于多种科学体积类型。

Method: 使用OpenVDB库作为稀疏体数据建模框架，将其转换为3D高斯粒子以进一步压缩；基于OptiX8.1实现光线积分算法，计算3D高斯沿光线的贡献；同时实现基于NanoVDB HDDA的光线行进渲染器进行对比。

Result: 提出的方法在稀疏体数据建模和压缩上表现良好，并通过渲染算法验证了其有效性，同时展示了在非规则网格体积（如AMR体积和点云）上的应用潜力。

Conclusion: 本文提出了一种基于OpenVDB和3D高斯粒子的稀疏体数据建模与压缩方法，适用于科学可视化中的多种体积类型，并展示了其渲染效果和应用潜力。

Abstract: 3D Gaussians are currently being heavily investigated for their scene
modeling and compression abilities. In 3D volumes, their use is being explored
for representing dense volumes as sparsely as possible. However, most of these
methods begin with a memory inefficient data format. Specially in Scientific
Visualization(SciVis), where most popular formats are dense-grid data
structures that store every grid cell, irrespective of its contribution.
OpenVDB library and data format were introduced for representing sparse
volumetric data specifically for visual effects use cases such as clouds, fire,
fluids etc. It avoids storing empty cells by masking them during storage. It
presents an opportunity for use in SciVis, specifically as a modeling framework
for conversion to 3D Gaussian particles for further compression and for a
unified modeling approach for different scientific volume types. This
compression head-start is non-trivial and this paper would like to present this
with a rendering algorithm based on line integration implemented in OptiX8.1
for calculating 3D Gaussians contribution along a ray for optical-depth
accumulation. For comparing the rendering results of our ray marching Gaussians
renderer, we also implement a SciVis style primary-ray only NanoVDB HDDA based
ray marcher for OpenVDB voxel grids. Finally, this paper also explores
application of this Gaussian model to formats of volumes other than regular
grids, such as AMR volumes and point clouds, using internal representation of
OpenVDB grid class types for data hierarchy and subdivision structure.

</details>


### [227] [3De Interactive Lenses for Visualization in Virtual Environments](https://arxiv.org/abs/2509.11410)
*Roberta C. R. Mota,Allan Rocha,Julio Daniel Silva,Usman Alim,Ehud Sharlin*

Main category: cs.GR

TL;DR: 3De lens融合3D和Decal透镜，支持多几何数据的无缝操作，并在虚拟现实中实现直接空间操作，适用于表面和流线的定制可视化。


<details>
  <summary>Details</summary>
Motivation: 解决多几何数据在3D可视化中的共存问题，提供一种更自然、直接的探索性数据分析方法。

Method: 融合了3D和Decal两类透镜，创建了一个多功能的透镜技术，支持在多种几何表示上无缝操作，并将其整合到虚拟现实环境中。

Result: 3De lens技术成功应用于两个领域示例，展示了其对表面和流线的定制化可视化能力。

Conclusion: 3De lens技术通过融合3D和Decal两种透镜，为多几何数据提供了无缝的焦点+上下文可视化解决方案，并在虚拟现实中展示了其直接空间操作的潜力。

Abstract: We present 3De lens, a technique for focus+context visualization of
multi-geometry data. It fuses two categories of lenses (3D and Decal) to become
a versatile lens for seamlessly working on multiple geometric representations
that commonly coexist in 3D visualizations. In addition, we incorporate our
lens into virtual reality as it enables a natural style of direct spatial
manipulation for exploratory 3D data analysis. To demonstrate its potential
use, we discuss two domain examples in which our lens technique creates
customized visualizations of both surfaces and streamlines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [228] [Asynchronous Gathering of Opaque Robots with Mobility Faults](https://arxiv.org/abs/2509.10711)
*Subhajit Pramanick,Saswata Jana,Partha Sarathi Mandal,Gokarna Sharma*

Main category: cs.DC

TL;DR: 本文研究了移动故障模型下的机器人聚集问题，提出了两种确定性算法，实现了时间与颜色数量的权衡，并首次分析了时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 针对移动故障模型下的机器人聚集问题，突破现有模型限制，探索在异步调度和视线受阻情况下的可行解决方案。

Method: 本文研究了LUMI模型下的移动故障系统，提出了两种确定性算法：一种使用7色灯光，时间复杂为O(N)；另一种使用26色灯光，时间复杂为O(max{ℓ,f})。

Result: 证明了在(2,1)-移动故障系统中使用2色灯光无法解决聚集问题，但使用3色灯光可以实现最优解。在(N,f)-移动故障系统中，提出的两种算法在时间与颜色数量上实现了权衡，且在小参数情况下达到最优。

Conclusion: 本文在移动故障模型下，通过使用不同颜色的灯光，提出了两种确定性算法，展示了时间与颜色数量的权衡关系，并首次分析了时间复杂度，能够适应异步调度和视线受阻的情况。

Abstract: We consider the fundamental benchmarking problem of gathering in an
$(N,f)$-fault system consisting of $N$ robots, of which at most $f$ might fail
at any execution, under asynchrony. Two seminal results established
impossibility of a solution in the oblivious robot (OBLOT) model in a
$(2,0)$-fault system under semi-synchrony and in a $(3,1)$-Byzantine fault
system under asynchrony. Recently, a breakthrough result circumvented the first
impossibility result by giving a deterministic algorithm in a $(2,0)$-fault
system under asynchrony in the luminous robot (LUMI) model using 2-colored
lights. However, a breakthrough result established impossibility of gathering
in a $(2,1)$-crash system in the LUMI model under semi-synchrony. In this
paper, we consider a {\em mobility fault} model in which a robot crash only
impacts it mobility but not the operation of the light.
  We establish four results under asynchrony in LUMI with the mobility fault
model. We show that it is impossible to solve gathering in a $(2,1)$-mobility
fault system using 2-colored lights, and then give a solution using 3-colored
lights, which is optimal w.r.t. the number of colors. We then consider an
$(N,f)$-mobility fault system, $f<N$, both $N,f$ not known, and give two
deterministic algorithms that exhibit a nice time-color trade-off: The first
with time $O(N)$ using 7-colored lights and the second with time
$O(\max\{\ell,f\})$ using 26-colored lights, where $\ell< N$ is the number of
distinct convex layers of robot positions in the initial configuration.
Interestingly, for $l, f = O(1)$, our result is optimal. Our algorithms for an
$(N,f)$-mobility fault system are the first to be analysed time complexity, can
withstand obstructed visibility (opaque robot model) and asynchronous
scheduling.

</details>


### [229] [MinatoLoader: Accelerating Machine Learning Training Through Efficient Data Preprocessing](https://arxiv.org/abs/2509.10712)
*Rahma Nouaji,Stella Bitchebe,Ricardo Macedo,Oana Balmau*

Main category: cs.DC

TL;DR: MinatoLoader是一种高效的数据加载器，通过优化预处理流程显著提升训练速度和GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 现有数据加载器在处理数据预处理时效率低下，导致GPU闲置率高，影响训练速度。MinatoLoader旨在解决这一问题。

Method: MinatoLoader在单服务器多GPU环境下，通过后台持续准备数据并优先处理预处理速度快的样本，同时并行处理慢速样本，避免了头部阻塞。

Result: 在配备V100和A100 GPU的服务器上，MinatoLoader将训练时间提高了7.5倍（平均3.6倍），GPU利用率从46.4%提升至90.45%。

Conclusion: MinatoLoader通过优化数据预处理流程，显著提高了GPU利用率和训练速度，同时保持了模型准确性。

Abstract: Data loaders are used by Machine Learning (ML) frameworks like PyTorch and
TensorFlow to apply transformations to data before feeding it into the
accelerator. This operation is called data preprocessing. Data preprocessing
plays an important role in the ML training workflow because if it is
inefficiently pipelined with the training, it can yield high GPU idleness,
resulting in important training delays. Unfortunately, existing data loaders
turn out to waste GPU resources, with $76\%$ GPU idleness when using the
PyTorch data loader, for example. One key source of inefficiency is the
variability in preprocessing time across samples within the same dataset.
Existing data loaders are oblivious to this variability, and they construct
batches without any consideration of slow or fast samples. In this case, the
entire batch is delayed by a single slow sample, stalling the training pipeline
and resulting in head-of-line blocking.
  To address these inefficiencies, we present MinatoLoader, a general-purpose
data loader for PyTorch that accelerates training and improves GPU utilization.
MinatoLoader is designed for a single-server setup, containing multiple GPUs.
It continuously prepares data in the background and actively constructs batches
by prioritizing fast-to-preprocess samples, while slower samples are processed
in parallel.
  We evaluate MinatoLoader on servers with V100 and A100 GPUs. On a machine
with four A100 GPUs, MinatoLoader improves the training time of a wide range of
workloads by up to $7.5\times$ ($3.6\times$ on average) over PyTorch DataLoader
and Pecan, and up to $3\times$ ($2.2\times$ on average) over DALI. It also
increases average GPU utilization from 46.4\% with PyTorch to 90.45\%, while
preserving model accuracy and enabling faster convergence.

</details>


### [230] [Coordinated Reinforcement Learning Prefetching Architecture for Multicore Systems](https://arxiv.org/abs/2509.10719)
*Mohammed Humaid Siddiqui,Fernando Guzman,Yufei Wu,Ruishu Ann*

Main category: cs.DC

TL;DR: CRL-Pythia 是一种针对多核系统的协调强化学习预取器，通过跨核协作减少冗余请求并提升性能，实验显示其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 多核架构下传统预取器存在冗余预取请求和性能损失问题，CRL-Pythia 旨在解决这些问题。

Method: 提出了 CRL-Pythia，一种基于协调强化学习的预取器，通过跨核信息共享和协作预取决策来减少冗余请求并提高学习收敛性。

Result: 实验表明 CRL-Pythia 在所有情况下均优于单 Pythia 配置，带宽受限工作负载的 IPC 提升约 12%，且硬件开销适中。

Conclusion: CRL-Pythia 是一种实用且高效的解决方案，适用于当代多核系统，显著提升了性能并减少了冗余预取请求。

Abstract: Hardware prefetching is critical to fill the performance gap between CPU
speeds and slower memory accesses. With multicore architectures becoming
commonplace, traditional prefetchers are severely challenged. Independent core
operation creates significant redundancy (up to 20% of prefetch requests are
duplicates), causing unnecessary memory bus traffic and wasted bandwidth.
Furthermore, cutting-edge prefetchers such as Pythia suffer from about a 10%
performance loss when scaling from a single-core to a four-core system. To
solve these problems, we propose CRL-Pythia, a coordinated reinforcement
learning based prefetcher specifically designed for multicore systems. In this
work, CRL-Pythia addresses these issues by enabling cross-core sharing of
information and cooperative prefetching decisions, which greatly reduces
redundant prefetch requests and improves learning convergence across cores. Our
experiments demonstrate that CRL-Pythia outperforms single Pythia
configurations in all cases, with approximately 12% IPC (instructions per
cycle) improvement for bandwidth-constrained workloads, while imposing moderate
hardware overhead. Our sensitivity analyses also verify its robustness and
scalability, thereby making CRL-Pythia a practical and efficient solution to
contemporary multicore systems.

</details>


### [231] [Enhancing Type Safety in MPI with Rust: A Statically Verified Approach for RSMPI](https://arxiv.org/abs/2509.10803)
*Nafees Iqbal,Jed Brown*

Main category: cs.DC

TL;DR: 本文提出了一种基于Rust的类型安全MPI通信框架，通过静态类型检查消除错误，提升开发效率，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: MPI作为高性能计算的基础工具，其低层接口和缺乏类型安全导致运行时错误和调试困难，Rust的强类型系统为解决这些问题提供了可能。

Method: 提出了TypedCommunicator抽象，利用Rust的Equivalence特性确保通信操作的静态类型安全，支持单值和切片通信。

Result: 该框架消除了常见MPI错误，提高了开发效率，同时保持了性能，符合Rust的零成本抽象原则。

Conclusion: 本文通过引入基于RSMPI库的类型安全通信框架，成功解决了传统MPI编程中的类型安全问题，为并行计算的稳健性奠定了基础。

Abstract: The Message Passing Interface (MPI) is a fundamental tool for building
high-performance computing (HPC) applications, enabling efficient communication
across distributed systems. Despite its widespread adoption, MPI's low-level
interface and lack of built-in type safety make it prone to runtime errors,
undefined behavior, and debugging challenges, especially in large-scale
applications. Rust, a modern systems programming language, offers a compelling
solution with its strong type system, which enforces memory and type safety at
compile time without compromising performance. This paper introduces a
type-safe communication framework for MPI, built on the RSMPI library, to
address the limitations of traditional MPI programming. At its core is the
TypedCommunicator, an abstraction that enforces static type safety in
point-to-point communication operations. By leveraging Rust's Equivalence
trait, our framework guarantees that only compatible types can participate in
communication, catching mismatches either at compile time or through runtime
validation. The framework supports both single-value and slice-based
communication, providing an intuitive API for diverse data structures. Our
implementation demonstrates that this approach eliminates common MPI errors,
improves developer productivity, and maintains performance, adhering to Rust's
principle of zero-cost abstractions. This work lays the foundation for
extending type safety to collective operations, advancing the robustness of
parallel computing in Rust.

</details>


### [232] [Chameleon: Taming Dynamic Operator Sequences for Memory-Intensive LLM Training](https://arxiv.org/abs/2509.11076)
*Zibo Wang,Yuhang Zhou,Zhibin Wang,Shipeng Li,Xinjing Huang,Chendong Cai,Bingxu Mu,Yuqing Sun,Zhiheng Hu,Bin She,Shu You,Guanghuan Fang,Rong Gu,Wanchun Dou,Guihai Chen,Chen Tian*

Main category: cs.DC

TL;DR: Chameleon是一种新型交换内存优化方法，适用于Eager Mode中操作序列变化，显著降低开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）训练时内存需求激增，现有交换方法假设操作序列一致，不适用于Eager Mode中操作序列变化的情况。

Method: Chameleon引入轻量级在线分析器进行持续监控，生成有效的交换策略，并优化策略执行模块以实现准确应用和性能提升。

Result: 实验表明，Chameleon将分析开销降低84.25%，支持训练模型大小达硬件内存4倍，性能提升达38.94%。

Conclusion: Chameleon通过重新设计基于交换的内存优化端到端流程，成功解决了Eager Mode中操作序列变化的问题，显著降低了分析开销，提升了性能。

Abstract: The increasing size of large language models (LLMs) has led to a surge in
memory requirements during training, often exceeding the capacity of
high-bandwidth memory (HBM). Swap-based memory optimization incurs neither
accuracy loss nor additional end-to-end overhead when effectively overlapped,
thus being an attractive solution. However, existing swap methods assume
consistent operator sequences, which is impractical in Eager Mode, where
operator sequences can vary during change.
  We propose Chameleon, which redesigns the end-to-end process of swap-based
memory optimization and is the first work to consider varying operator
sequences in Eager Mode. Chameleon (i) introduces a lightweight online profiler
to enable continuous profiling for monitoring operator sequences, (ii)
generates effective swap policies with limited operator information, and (iii)
optimizes the policy execution module for accurate policy application and
better performance. Experimental results demonstrate that Chameleon reduces
profiling overhead by 84.25%, enables training models up to 4x larger than
hardware memory while adapting to changes in operator sequences, improves
performance by up to 38.94% compared to recomputation or high-degree
parallelism.

</details>


### [233] [GFS: A Preemption-aware Scheduling Framework for GPU Clusters with Predictive Spot Instance Management](https://arxiv.org/abs/2509.11134)
*Jiaang Duan,Shenglin Xu,Shiyou Qian,Dingyu Yang,Kangjin Wang,Chenzhi Liao,Yinghao Yu,Qin Hua,Hanwen Hu,Qi Wang,Wenchao Wu,Dongqing Bao,Tianyu Lu,Jian Cao,Guangtao Xue,Guodong Yang,Liping Zhang,Gang Chen*

Main category: cs.DC

TL;DR: GFS 是一种新型抢占式调度框架，通过预测需求和动态分配，显著优化 GPU 资源管理，减少低优先级任务的中断并提升高优先级任务的合规性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的兴起改变了 GPU 使用模式，现有调度器在高驱逐率和长排队时间方面表现不佳，亟需更高效的资源管理策略。

Method: GFS 结合了轻量级预测模型、动态分配机制和抢占式调度策略，以优化 GPU 资源分配。

Result: GFS 将低优先级任务的驱逐率降低了 33.0%，排队延迟减少了 44.1%，GPU 分配率提升了 22.8%，并在实际生产集群中实现了每月约 459,715 美元的收益。

Conclusion: GFS 框架显著提升了 GPU 资源的管理效率，减少了低优先级任务的驱逐率和排队延迟，同时为高优先级任务提供了更好的 SLO 合规性。

Abstract: The surge in large language models (LLMs) has fundamentally reshaped the
landscape of GPU usage patterns, creating an urgent need for more efficient
management strategies. While cloud providers employ spot instances to reduce
costs for low-priority (LP) tasks, existing schedulers still grapple with high
eviction rates and lengthy queuing times. To address these limitations, we
present GFS, a novel preemptive scheduling framework that enhances
service-level objective (SLO) compliance for high-priority (HP) tasks while
minimizing preemptions to LP tasks. Firstly, GFS utilizes a lightweight
forecasting model that predicts GPU demand among different tenants, enabling
proactive resource management. Secondly, GFS employs a dynamic allocation
mechanism to adjust the spot quota for LP tasks with guaranteed durations.
Lastly, GFS incorporates a preemptive scheduling policy that prioritizes HP
tasks while minimizing the impact on LP tasks. We demonstrate the effectiveness
of GFS through both real-world implementation and simulations. The results show
that GFS reduces eviction rates by 33.0\%, and cuts queuing delays by 44.1\%
for LP tasks. Furthermore, GFS enhances the GPU allocation rate by up to 22.8\%
in real production clusters. In a production cluster of more than 10,000 GPUs,
GFS yields roughly \$459,715 in monthly benefits.

</details>


### [234] [Linear Complexity $\mathcal{H}^2$ Direct Solver for Fine-Grained Parallel Architectures](https://arxiv.org/abs/2509.11152)
*Wajih Boukaram,David Keyes,Sherry Li,Yang Liu,George Turkiyyah*

Main category: cs.DC

TL;DR: 论文提出了一种适用于分层矩阵的线性复杂度直接求解器，支持并行批量操作，实验验证了其高效性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 针对可分层表示的矩阵，设计一种适用于细粒度并行架构的高效直接求解器，以解决现有方法在并行化和内存管理上的局限性。

Method: 论文基于强可接受性的$\mathcal{H}^2$格式，采用强递归骨架化分解压缩远程交互。算法设计为“黑盒”模式，仅需输入矩阵和右侧向量，无需系统来源的分析或几何信息。

Result: 在多达100万规模的密集矩阵上展示了线性复杂度的时间与内存扩展。通过多级矩阵图着色和前缀和内存管理实现了16线程的并行扩展，并进行了实验性后向误差分析。

Conclusion: 论文提出了一种新的线性复杂度直接求解器，适用于细粒度并行架构上的并发批量操作，适用于可分层表示的矩阵。通过实验验证了其在时间和内存上的线性复杂度扩展，并支持多达16线程的并行扩展。

Abstract: We present factorization and solution phases for a new linear complexity
direct solver designed for concurrent batch operations on fine-grained parallel
architectures, for matrices amenable to hierarchical representation. We focus
on the strong-admissibility-based $\mathcal{H}^2$ format, where strong
recursive skeletonization factorization compresses remote interactions. We
build upon previous implementations of $\mathcal{H}^2$ matrix construction for
efficient factorization and solution algorithm design, which are illustrated
graphically in stepwise detail. The algorithms are ``blackbox'' in the sense
that the only inputs are the matrix and right-hand side, without analytical or
geometrical information about the origin of the system. We demonstrate linear
complexity scaling in both time and memory on four representative families of
dense matrices up to one million in size. Parallel scaling up to 16 threads is
enabled by a multi-level matrix graph coloring and avoidance of dynamic memory
allocations thanks to prefix-sum memory management. An experimental backward
error analysis is included. We break down the timings of different phases,
identify phases that are memory-bandwidth limited, and discuss alternatives for
phases that may be sensitive to the trend to employ lower precisions for
performance.

</details>


### [235] [Adaptive K-PackCache: Cost-Centric Data Caching in Cloud](https://arxiv.org/abs/2509.11156)
*Suvarthi Sarkar,Aadarshraj Sah,Poddutoori Sweeya Reddy,Aryabartta Sahu*

Main category: cs.DC

TL;DR: AKPC算法通过动态K项打包缓存，显著降低成本并在实际数据中表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩展传统的2项打包缓存到通用的K项打包，以提高灵活性和性能。

Method: 提出了一种在线算法Adaptive K PackCache (AKPC)，动态形成、合并和拆分数据包，基于用户访问模式和内容相关性。

Result: 在Netflix和Spotify数据集上，AKPC比在线基线分别降低了63%和55%的总成本，性能接近最优解的15%和13%。

Conclusion: AKPC算法通过动态调整数据包的组合，显著降低了总成本，并在实际数据集上验证了其有效性和可扩展性。

Abstract: Recent advances in data analytics have enabled the accurate prediction of
user access patterns, giving rise to the idea of packed caching delivering
multiple co accessed data items together as a bundle. This improves caching
efficiency, as accessing one item often implies the need for others. Prior work
has explored only 2 item pairwise packing. In this paper, we extend the concept
to general K packing, allowing variable size bundles for improved flexibility
and performance. We formulate the K PackCache problem from a content delivery
network CDN operator perspective, aiming to minimize total cost comprising two
components: transfer cost modeled as a base cost plus a linearly increasing
term with the number of items packed, and memory rental cost for caching, which
depends on how long and how much is stored. Overpacking increases cost due to
low utility, underpacking leads to missed sharing opportunities. We propose an
online algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,
and splits data cliques based on user access patterns and content correlation.
Our approach supports batch requests, enables approximate clique merging, and
offers a formal competitive guarantee. Through extensive evaluation on the
Netflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55
percentage over online baselines, respectively, and achieves performance within
15 and 13 percentage of the optimal. This demonstrates its scalability and
effectiveness for real world caching systems.

</details>


### [236] [Energy-Efficient Joint Offloading and Resource Allocation for Deadline-Constrained Tasks in Multi-Access Edge Computing](https://arxiv.org/abs/2509.11162)
*Chuanchao Gao,Arvind Easwaran*

Main category: cs.DC

TL;DR: 论文提出GMA算法解决边缘计算中的任务卸载和资源分配问题，实验证明其节能效果接近最优。


<details>
  <summary>Details</summary>
Motivation: 为了解决多接入边缘计算中任务卸载和资源分配的挑战，同时满足任务截止时间和系统资源约束，最大化物联网设备的节能效果。

Method: 论文将问题建模为整数非线性规划问题，并证明其为NP难问题。提出的GMA算法采用线性松弛、三方图构建和线性规划舍入技术。

Result: GMA算法被证明是一个(1−α)/(2+ϵ)-近似算法，实验结果显示其节能效果接近最优。

Conclusion: 论文提出了一种基于图匹配的近似算法（GMA），用于解决多接入边缘计算中的任务卸载和资源分配问题。实验证明，GMA在实际应用中的节能效果平均达到最优值的97%。

Abstract: This paper addresses the deadline-constrained task offloading and resource
allocation problem in multi-access edge computing. We aim to determine where
each task is offloaded and processed, as well as corresponding communication
and computation resource allocations, to maximize the total saved energy for
IoT devices, while considering task deadline and system resource constraints.
Especially, our system allows each task to be offloaded to one of its
accessible access points (APs) and processed on a server that is not co-located
with its offloading AP. We formulate this problem as an Integer Nonlinear
Programming problem and show it is NP-Hard. To address this problem, we propose
a Graph-Matching-based Approximation Algorithm ($\mathtt{GMA}$), the first
approximation algorithm of its kind. $\mathtt{GMA}$ leverages linear
relaxation, tripartite graph construction, and a Linear Programming rounding
technique. We prove that $\mathtt{GMA}$ is a
$\frac{1-\alpha}{2+\epsilon}$-approximation algorithm, where $\epsilon$ is a
small positive value, and $\alpha$ ($0$$\le$$\alpha$$<$$1$) is a system
parameter that ensures the resource allocated to any task by an AP or a server
cannot exceed $\alpha$ times its resource capacity. Experiments show that, in
practice, $\mathtt{GMA}$'s energy saving achieves $97\%$ of the optimal value
on average.

</details>


### [237] [Parallel/Distributed Tabu Search for Scheduling Microprocessor Tasks in Hybrid Flowshop](https://arxiv.org/abs/2509.11396)
*Adam Janiak,Damian Kowalczyk,Maciej Lichtenstein*

Main category: cs.DC

TL;DR: 该论文提出了一种基于禁忌搜索的算法，用于解决具有多处理器任务的混合流水车间调度问题，有效最小化makespan并在异构网络中表现良好。


<details>
  <summary>Details</summary>
Motivation: 混合流水车间（HFS）通过用多个相同的并行处理器替换每个处理器（处理阶段）来扩展经典流水车间处理器配置。多处理器任务通过允许任务同时需要多个处理器进行处理，进一步扩展了经典假设。

Method: 提出了一种基于禁忌搜索技术的算法，采用并行和分布式机制进行邻域评估，并很好地平衡了异构网络环境。

Result: 论文提出的算法在解决多处理器任务的混合流水车间调度问题中表现出色，特别是在异构网络环境中。

Conclusion: 该论文通过基于禁忌搜索技术的算法，有效解决了具有多处理器任务的混合流水车间调度问题中的makespan最小化问题。

Abstract: The paper deals with the makespan minimization in the hybrid flow shop
scheduling problem with multiprocessor tasks. The hybrid flow shop (HFS)
generalizes the classical flow shop processor configuration by replacing each
processor (processing stage) by some number of identical parallel processors.
Similarly, the multiprocessor tasks generalize the classical assumption, by
allowing a task to require more than one processor simultaneously for its
processing. In this work we present the algorithm for solving the problem based
on the tabu search technique. The proposed algorithm uses parallel and
distributed mechanisms for neighborhood evaluation and well balances
heterogeneous network environment.

</details>


### [238] [Machine Learning-Driven Predictive Resource Management in Complex Science Workflows](https://arxiv.org/abs/2509.11512)
*Tasnuva Chowdhury,Tadashi Maeno,Fatih Furkan Akman,Joseph Boudreau,Sankha Dutta,Shengyu Feng,Adolfy Hoisie,Kuan-Chieh Hsu,Raees Khan,Jaehyung Kim,Ozgur O. Kilic,Scott Klasky,Alexei Klimentov,Tatiana Korchuganova,Verena Ingrid Martinez Outschoorn,Paul Nilsson,David K. Park,Norbert Podhorszki,Yihui Ren,John Rembrandt Steele,Frédéric Suter,Sairam Sri Vatsavai,Torre Wenaus,Wei Yang,Yiming Yang,Shinjae Yoo*

Main category: cs.DC

TL;DR: 研究提出了一种基于机器学习的新方法，用于精确预测科学实验工作流中的资源需求，从而优化处理效率。


<details>
  <summary>Details</summary>
Motivation: 科学实验中的数据流程日益复杂，资源需求的精确预估面临诸多挑战，如分析场景多样、成员技能水平不一及计算选项不断增加。

Method: 研究在PanDA工作流管理系统中部署了一系列机器学习模型，用于预测每个步骤的关键资源需求。

Result: 机器学习模型能够基于有限的前期知识准确预测资源需求，支持工作流管理中的主动决策。

Conclusion: 该研究通过引入机器学习模型的新颖流水线，显著提高了资源需求预测的准确性，从而优化了科学实验中的工作流管理效率。

Abstract: The collaborative efforts of large communities in science experiments, often
comprising thousands of global members, reflect a monumental commitment to
exploration and discovery. Recently, advanced and complex data processing has
gained increasing importance in science experiments. Data processing workflows
typically consist of multiple intricate steps, and the precise specification of
resource requirements is crucial for each step to allocate optimal resources
for effective processing. Estimating resource requirements in advance is
challenging due to a wide range of analysis scenarios, varying skill levels
among community members, and the continuously increasing spectrum of computing
options. One practical approach to mitigate these challenges involves initially
processing a subset of each step to measure precise resource utilization from
actual processing profiles before completing the entire step. While this
two-staged approach enables processing on optimal resources for most of the
workflow, it has drawbacks such as initial inaccuracies leading to potential
failures and suboptimal resource usage, along with overhead from waiting for
initial processing completion, which is critical for fast-turnaround analyses.
In this context, our study introduces a novel pipeline of machine learning
models within a comprehensive workflow management system, the Production and
Distributed Analysis (PanDA) system. These models employ advanced machine
learning techniques to predict key resource requirements, overcoming challenges
posed by limited upfront knowledge of characteristics at each step. Accurate
forecasts of resource requirements enable informed and proactive
decision-making in workflow management, enhancing the efficiency of handling
diverse, complex workflows across heterogeneous resources.

</details>


### [239] [Towards the Distributed Large-scale k-NN Graph Construction by Graph Merge](https://arxiv.org/abs/2509.11697)
*Cheng Zhang,Wan-Lei Zhao,Shihai Xiao,Jiajie Yao,Xuecang Zhang*

Main category: cs.DC

TL;DR: 本文提出高效图合并算法，解决大规模k-NN/索引图构建问题，单/多节点均适用，实验验证其高效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 支持LLMs的实时交互和社交媒体的即时搜索或推荐，需要处理超出单机处理能力的大规模向量化多媒体数据的k-NN图或索引图构建问题。

Method: 提出了两种通用且高度并行化的算法：Two-way Merge和Multi-way Merge，用于单节点子图合并；并基于Two-way Merge设计了多节点合并流程。

Result: 实验表明，并行图合并能高效构建大规模高质量k-NN图（如3节点17小时构建十亿级图），且合并的索引图在保持相似搜索性能的同时大幅减少构建时间。

Conclusion: 本文通过高效的图合并方法解决了大规模k-NN图和索引图的构建问题，提出了单节点和多节点的合并算法，显著提升了构建效率。

Abstract: In order to support the real-time interaction with LLMs and the instant
search or the instant recommendation on social media, it becomes an imminent
problem to build k-NN graph or indexing graph for the massive number of
vectorized multimedia data. In such scenarios, the scale of the data or the
scale of the graph may exceed the processing capacity of a single machine. This
paper aims to address the graph construction problem of such scale via
efficient graph merge. For the graph construction on a single node, two generic
and highly parallelizable algorithms, namely Two-way Merge and Multi-way Merge
are proposed to merge subgraphs into one. For the graph construction across
multiple nodes, a multi-node procedure based on Two-way Merge is presented. The
procedure makes it feasible to construct a large-scale k-NN graph/indexing
graph on either a single node or multiple nodes when the data size exceeds the
memory capacity of one node. Extensive experiments are conducted on both
large-scale k-NN graph and indexing graph construction. For the k-NN graph
construction, the large-scale and high-quality k-NN graphs are constructed by
graph merge in parallel. Typically, a billion-scale k-NN graph can be built in
approximately 17h when only three nodes are employed. For the indexing graph
construction, similar NN search performance as the original indexing graph is
achieved with the merged indexing graphs while requiring much less time of
construction.

</details>


### [240] [A Uniqueness Theorem for Distributed Computation under Physical Constraint](https://arxiv.org/abs/2509.11754)
*Zhiyuan Ren,Mingxuan Lu,Wenchi Cheng*

Main category: cs.DC

TL;DR: 本文通过公理系统证明了在极端环境下，自描述并行流（SDPF）是唯一最优的分布式计算范式。


<details>
  <summary>Details</summary>
Motivation: 在极端环境（如网络内计算）中，物理硬件的限制成为不可违反的定律，导致通信效率、有限内存和鲁棒可扩展性之间的尖锐三难问题。

Method: 作者建立了一个严格的公理系统，形式化了物理约束，并证明了对于具有幂等合并运算符的计算类别，存在唯一最优的范式。

Result: 证明了自描述并行流（SDPF）是唯一最优的范式，具有收敛性、图灵完备性和最小性。

Conclusion: 本文通过建立严格的公理系统，证明了在具有幂等合并运算符的广泛计算类别中，存在唯一最优的范式：自描述并行流（SDPF），这是一种纯粹以数据为中心的模型。

Abstract: Foundational models of computation often abstract away physical hardware
limitations. However, in extreme environments like In-Network Computing (INC),
these limitations become inviolable laws, creating an acute trilemma among
communication efficiency, bounded memory, and robust scalability. Prevailing
distributed paradigms, while powerful in their intended domains, were not
designed for this stringent regime and thus face fundamental challenges. This
paper demonstrates that resolving this trilemma requires a shift in perspective
- from seeking engineering trade-offs to deriving solutions from logical
necessity. We establish a rigorous axiomatic system that formalizes these
physical constraints and prove that for the broad class of computations
admitting an idempotent merge operator, there exists a unique, optimal
paradigm. Any system satisfying these axioms must converge to a single normal
form: Self-Describing Parallel Flows (SDPF), a purely data-centric model where
stateless executors process flows that carry their own control logic. We
further prove this unique paradigm is convergent, Turing-complete, and minimal.
In the same way that the CAP theorem established a boundary for what is
impossible in distributed state management, our work provides a constructive
dual: a uniqueness theorem that reveals what is \textit{inevitable} for
distributed computation flows under physical law.

</details>


### [241] [LASLiN: A Learning-Augmented Peer-to-Peer Network](https://arxiv.org/abs/2509.11904)
*Julien Dallot,Caio Caldeira,Arash Pourdamghani,Olga Goussevskaia,Stefan Schmid*

Main category: cs.DC

TL;DR: 学习增强的P2P网络设计LASLiN，通过动态预测优化拓扑，平衡性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 优化P2P网络的性能，同时平衡预测准确性和鲁棒性。

Method: 采用动态规划和学习增强技术，结合Uniform P2P协议，构建了LASLiN协议。

Result: LASLiN在预测准确时性能接近最优静态SLN，预测错误时性能仍优于现有P2P协议。

Conclusion: 本文提出了一种学习增强的P2P网络设计LASLiN，通过动态预测流量模式优化网络拓扑，在保持标准P2P指标的同时，显著提升了性能。

Abstract: We introduce a learning-augmented peer-to-peer (P2P) network design that
leverages the predictions of traffic patterns to optimize the network's
topology. While keeping formal guarantees on the standard P2P metrics (routing
path length, maximum degree), we optimize the network in a demand-aware manner
and minimize the path lengths weighted by the peer-to-peer communication
demands. Our protocol is learning-augmented, meaning that each node receives an
individual, possibly inaccurate prediction about the future traffic patterns,
with the goal of improving the network's performances. We strike a trade-off
between significantly improved performances when the predictions are correct
(consistency) and polylogarithmic performances when the predictions are
arbitrary (robustness).
  We have two main contributions. First, we consider the centralized setting
and show that the problem of constructing an optimum static skip list network
(SLN) is solvable in polynomial time and can be computed via dynamic
programming. This problem is the natural demand-aware extension of the optimal
skip list problem.
  Second, we introduce the Uniform P2P protocol which generalizes skip list
networks (SLN) by relaxing the node's heights from discrete to continuous. We
show that Uniform achieves state-of-the-art performances: logarithmic routing
and maximum degree, both with high probability. We then use Uniform to build a
learning-augmented P2P protocol in order to incorporate demand-awareness,
leading to our main contribution, LASLiN. We prove that the performances of
LASLiN are consistent with those of an optimum static SLN with correct
predictions (given via our dynamic programming approach), and are at most a
logarithmic factor off the state-of-the-art P2P protocols if the predictions
are arbitrary wrong. For the special case of highly sparse demands, we show
that LASLiN achieves improved performances.

</details>


### [242] [UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code Translation in HPC](https://arxiv.org/abs/2509.12136)
*Tomer Bitan,Tal Kadosh,Erel Kaplan,Shira Meiri,Le Chen,Peter Morales,Niranjan Hasabnis,Gal Oren*

Main category: cs.DC

TL;DR: UniPar是一个评估LLM并行代码翻译能力的框架，通过微调和编译器修复等方法显著提升性能，为研究者提供了改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有并行编程语言翻译工具范围狭窄且过时，而大型语言模型（LLMs）在代码生成和翻译方面的能力为解决这一问题提供了潜在途径。

Method: 引入UniPar评估框架，针对串行代码、CUDA和OpenMP之间的翻译，评估了四种主要使用模式：解码超参数优化、零样本和少样本提示、监督微调以及基于编译器修复的迭代反馈。

Result: UniPar方法将性能提升至2倍（69%编译率和33%功能正确性），而现成模型在默认设置下表现不佳（如GPT-4o-mini仅46%编译率和15%功能正确性）。

Conclusion: UniPar方法结合微调、超参数调整和编译器引导修复，显著提升了LLM在并行代码翻译中的性能，为研究者提供了有价值的改进方向。

Abstract: Translating programs between various parallel programming languages is an
important problem in the high-performance computing (HPC) community. Existing
tools for this problem are either too narrow in scope and/or outdated. Recent
explosive growth in the popularity of large language models (LLMs) and their
ability to generate and translate code offers a potential alternative approach.
Toward that end, we first need to systematically evaluate the ability of LLMs
to translate between parallel languages.
  In this work, we introduce UniPar, a systematic evaluation framework for
LLM-based parallel code translation. Specifically, in this work, we target
translations between serial code, CUDA, and OpenMP. Our goal is to assess how
well current instruction-tuned LLMs -- specifically GPT-4o-mini and
LLaMA-3.3-70B-Instruct -- can be used out of the box or enhanced through known
strategies. We evaluated four major usage modes: hyperparameter optimization
for decoding, zero- and few-shot prompting, supervised fine-tuning, and
iterative feedback through compiler-based repair. As a part of the evaluation,
we construct a new dataset called PARATRANS, covering both serial-to-parallel
translation and cross-paradigm transformations.
  Our findings reveal that while off-the-shelf models struggle under the
default settings (e.g., GPT-4o-mini achieves only 46% compilation and 15%
functional correctness), our UniPar methodology -- combining fine-tuning,
hyperparameter tuning, and compiler-guided repair -- improves performance by up
to 2X (69% compilation and 33% correctness). We believe that our findings will
provide useful insights for researchers to further improve LLMs for the
parallel language translation problem.
  UniPar source code and PARATRANS dataset are available at our GitHub
repository https://github.com/Scientific-Computing-Lab/UniPar_AI.

</details>


### [243] [Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization](https://arxiv.org/abs/2509.12138)
*Mengjiao Han,Andres Sewell,Joseph Insley,Janet Knowles,Victor A. Mateevitsi,Michael E. Papka,Steve Petruzza,Silvio Rizzi*

Main category: cs.DC

TL;DR: 分布式3D-GS技术通过多节点并行处理，显著提升了大规模科学数据可视化的效率，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D-GS技术在单GPU环境下无法有效处理高性能计算系统中的大规模数据集。

Method: 采用数据分区、多节点多GPU并行训练高斯斑点，并通过添加幽灵单元和背景掩码来消除伪影。

Result: 在Polaris上使用8个节点时，Richtmyer-Meshkov数据集（约1.067亿高斯）实现了高达3倍的加速，同时保持图像质量。

Conclusion: 分布式3D-GS技术为大规模科学数据的可视化提供了可扩展的解决方案，并为未来的原位应用奠定了基础。

Abstract: 3D Gaussian Splatting (3D-GS) has recently emerged as a powerful technique
for real-time, photorealistic rendering by optimizing anisotropic Gaussian
primitives from view-dependent images. While 3D-GS has been extended to
scientific visualization, prior work remains limited to single-GPU settings,
restricting scalability for large datasets on high-performance computing (HPC)
systems. We present a distributed 3D-GS pipeline tailored for HPC. Our approach
partitions data across nodes, trains Gaussian splats in parallel using
multi-nodes and multi-GPUs, and merges splats for global rendering. To
eliminate artifacts, we add ghost cells at partition boundaries and apply
background masks to remove irrelevant pixels. Benchmarks on the
Richtmyer-Meshkov datasets (about 106.7M Gaussians) show up to 3X speedup
across 8 nodes on Polaris while preserving image quality. These results
demonstrate that distributed 3D-GS enables scalable visualization of
large-scale scientific data and provide a foundation for future in situ
applications.

</details>


### [244] [When MoE Meets Blockchain: A Trustworthy Distributed Framework of Large Models](https://arxiv.org/abs/2509.12141)
*Weihao Zhu,Long Shi,Kang Wei,Zhen Mei,Zhe Wang,Jiaheng Wang,Jun Li*

Main category: cs.DC

TL;DR: B-MoE利用区块链技术解决分布式MoE的信任问题，提升安全性。


<details>
  <summary>Details</summary>
Motivation: 传统分布式MoE框架缺乏对分布式专家间数据交互的信任机制，易受数据操纵等攻击。

Method: 提出了一种区块链辅助的可信MoE（B-MoE）框架，包含边缘层、区块链层和存储层。边缘层负责处理学习任务，区块链层作为去中心化信任网络追踪、验证和记录计算结果。

Result: 实验结果表明，B-MoE在训练和推理过程中比传统分布式MoE更能抵御数据操纵攻击。

Conclusion: B-MoE框架通过区块链技术增强了分布式MoE的信任机制，有效抵御数据操纵攻击，提升了训练和推理过程的安全性。

Abstract: As an enabling architecture of Large Models (LMs), Mixture of Experts (MoE)
has become prevalent thanks to its sparsely-gated mechanism, which lowers
computational overhead while maintaining learning performance comparable to
dense LMs. The essence of MoE lies in utilizing a group of neural networks
(called experts) with each specializing in different types of tasks, along with
a trainable gating network that selectively activates a subset of these experts
to handle specific tasks. Traditional cloud-based MoE encounters challenges
such as prolonged response latency, high bandwidth consumption, and data
privacy leakage. To address these issues, researchers have proposed to deploy
MoE over distributed edge networks. However, a key concern of distributed MoE
frameworks is the lack of trust in data interactions among distributed experts
without the surveillance of any trusted authority, and thereby prone to
potential attacks such as data manipulation. In response to the security issues
of traditional distributed MoE, we propose a blockchain-aided trustworthy MoE
(B-MoE) framework that consists of three layers: the edge layer, the blockchain
layer, and the storage layer. In this framework, the edge layer employs the
activated experts downloaded from the storage layer to process the learning
tasks, while the blockchain layer functions as a decentralized trustworthy
network to trace, verify, and record the computational results of the experts
from the edge layer. The experimental results demonstrate that B-MoE is more
robust to data manipulation attacks than traditional distributed MoE during
both the training and inference processes.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [245] [Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2509.10570)
*Wei Dai,Shengen Wu,Wei Wu,Zhenhao Wang,Sisuo Lyu,Haicheng Liao,Limin Yu,Weiping Ding,Runwei Guan,Yutao Yue*

Main category: cs.RO

TL;DR: LFMs（尤其是LLMs和MLLMs）通过整合语言和场景语义，提升了轨迹预测的可解释性和泛化能力，未来研究方向包括低延迟推理和因果感知建模。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在轨迹预测中存在可解释性差、依赖大规模标注数据和泛化能力弱等问题，LFMs的出现为这一领域带来了新的研究范式。

Method: 本文系统综述了LFMs在轨迹预测中的三大核心方法：轨迹-语言映射、多模态融合和基于约束的推理。

Result: LFMs通过整合语言和场景语义，显著提升了轨迹预测的安全性和复杂环境中的泛化能力。

Conclusion: LFMs，尤其是LLMs和MLLMs，通过整合语言和场景语义，显著提升了轨迹预测的可解释性和泛化能力。未来研究方向包括低延迟推理、因果感知建模和运动基础模型。

Abstract: Trajectory prediction serves as a critical functionality in autonomous
driving, enabling the anticipation of future motion paths for traffic
participants such as vehicles and pedestrians, which is essential for driving
safety. Although conventional deep learning methods have improved accuracy,
they remain hindered by inherent limitations, including lack of
interpretability, heavy reliance on large-scale annotated data, and weak
generalization in long-tail scenarios. The rise of Large Foundation Models
(LFMs) is transforming the research paradigm of trajectory prediction. This
survey offers a systematic review of recent advances in LFMs, particularly
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for
trajectory prediction. By integrating linguistic and scene semantics, LFMs
facilitate interpretable contextual reasoning, significantly enhancing
prediction safety and generalization in complex environments. The article
highlights three core methodologies: trajectory-language mapping, multimodal
fusion, and constraint-based reasoning. It covers prediction tasks for both
vehicles and pedestrians, evaluation metrics, and dataset analyses. Key
challenges such as computational latency, data scarcity, and real-world
robustness are discussed, along with future research directions including
low-latency inference, causality-aware modeling, and motion foundation models.

</details>


### [246] [STL-Based Motion Planning and Uncertainty-Aware Risk Analysis for Human-Robot Collaboration with a Multi-Rotor Aerial Vehicle](https://arxiv.org/abs/2509.10692)
*Giuseppe Silano,Amr Afifi,Martin Saska,Antonio Franchi*

Main category: cs.RO

TL;DR: 该论文提出一种基于STL和优化框架的MRAV运动规划方法，结合风险分析和重规划策略，显著提升了人机协作的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 提升人机协作的安全性和效率，特别是在动态和不确定环境中（如电力线维护），同时关注人类舒适度和人体工程学。

Method: 使用信号时序逻辑（STL）编码任务目标，结合优化框架生成动态可行轨迹，并采用平滑近似和梯度技术处理计算复杂性。同时引入不确定性感知风险分析和事件触发重规划策略。

Result: 通过MATLAB和Gazebo仿真验证，方法在对象交接任务中表现优异，实现了安全、高效且具有弹性的协作。

Conclusion: 该论文提出的方法通过STL编码和优化框架，成功实现了安全、高效且具有弹性的人机协作，尤其在电力线维护场景中表现出色。

Abstract: This paper presents a novel approach to motion planning and risk analysis for
enhancing human-robot collaboration using a Multi-Rotor Aerial Vehicle (MRAV).
The proposed method uses Signal Temporal Logic (STL) to encode key mission
objectives, such as safety, timing, and human preferences, with a strong focus
on ergonomics and comfort. An optimization framework generates dynamically
feasible trajectories while considering the MRAV's physical constraints. Given
the nonlinear and non-convex nature of the problem, smooth approximations and
gradient-based techniques assist in handling the problem's computational
complexity. Additionally, an uncertainty-aware risk analysis is incorporated to
assess potential deviations from the mission specifications, providing insights
into the likelihood of mission success under uncertain conditions. Further, an
event-triggered replanning strategy is implemented to respond to unforeseen
events and external disturbances. The approach is validated through MATLAB and
Gazebo simulations, using an object handover task in a mock-up environment
inspired by power line maintenance scenarios. The results highlight the
method's effectiveness in achieving safe, efficient, and resilient human-robot
collaboration.

</details>


### [247] [A Survey on LiDAR-based Autonomous Aerial Vehicles](https://arxiv.org/abs/2509.10730)
*Yunfan Ren,Yixi Cai,Haotian Li,Nan Chen,Fangcheng Zhu,Longji Yin,Fanze Kong,Rundong Li,Fu Zhang*

Main category: cs.RO

TL;DR: 本文综述了LiDAR技术在无人机中的最新进展，包括设计、感知、规划和控制，并探讨了实际应用和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LiDAR技术因其高精度、长距离测量和强适应性成为无人机导航的关键技术，尤其在GPS受限环境中。本文旨在综述其最新进展，为研究和实践提供参考。

Method: 通过文献综述，分析了LiDAR技术在无人机设计、感知、规划和控制中的应用，并探讨了相关软件组件和实际应用案例。

Result: LiDAR技术的集成显著提升了无人机的自主性，使其能够在复杂环境中执行任务，并支持工业应用和多无人机协作。

Conclusion: 该调查总结了LiDAR技术在无人机领域的应用现状，并提出了未来研究方向，以推动多无人机协作和系统性能的提升。

Abstract: This survey offers a comprehensive overview of recent advancements in
LiDAR-based autonomous Unmanned Aerial Vehicles (UAVs), covering their design,
perception, planning, and control strategies. Over the past decade, LiDAR
technology has become a crucial enabler for high-speed, agile, and reliable UAV
navigation, especially in GPS-denied environments. The paper begins by
examining the evolution of LiDAR sensors, emphasizing their unique advantages
such as high accuracy, long-range depth measurements, and robust performance
under various lighting conditions, making them particularly well-suited for UAV
applications. The integration of LiDAR with UAVs has significantly enhanced
their autonomy, enabling complex missions in diverse and challenging
environments. Subsequently, we explore essential software components, including
perception technologies for state estimation and mapping, as well as trajectory
planning and control methodologies, and discuss their adoption in LiDAR-based
UAVs. Additionally, we analyze various practical applications of the
LiDAR-based UAVs, ranging from industrial operations to supporting different
aerial platforms and UAV swarm deployments. The survey concludes by discussing
existing challenges and proposing future research directions to advance
LiDAR-based UAVs and enhance multi-UAV collaboration. By synthesizing recent
developments, this paper aims to provide a valuable resource for researchers
and practitioners working to push the boundaries of LiDAR-based UAV systems.

</details>


### [248] [Analytical Design and Development of a Modular and Intuitive Framework for Robotizing and Enhancing the Existing Endoscopic Procedures](https://arxiv.org/abs/2509.10735)
*Mohammad Rafiee Javazm,Yash Kulkarni,Jiaqi Xue,Naruhiko Ikoma,Farshid Alambeigi*

Main category: cs.RO

TL;DR: 本文提出了一种模块化、易安装的机电一体化框架，通过夹持和进给机制及用户界面优化内窥镜控制，数学建模和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 内窥镜设备的手动控制对临床医生具有挑战性，导致工作负荷增加、疲劳和注意力分散等问题，因此需要一种更直观和自动化的解决方案。

Method: 设计并开发了一个模块化、易安装的机电一体化框架，包括嵌套夹持机制、进给机制和用户界面，并通过数学建模和参数优化进行设计分析。

Result: 仿真和实验研究表明，所提出的数学建模和机器人框架在控制内窥镜设备的自由度方面表现出色。

Conclusion: 本文提出的机电一体化框架通过新颖的夹持机制、进给机制和直观的用户界面，有效解决了内窥镜设备手动控制的挑战，并通过数学建模和实验验证了其性能。

Abstract: Despite the widespread adoption of endoscopic devices for several cancer
screening procedures, manual control of these devices still remains challenging
for clinicians, leading to several critical issues such as increased workload,
fatigue, and distractions. To address these issues, in this paper, we introduce
the design and development of an intuitive, modular, and easily installable
mechatronic framework. This framework includes (i) a novel nested collet-chuck
gripping mechanism that can readily be integrated and assembled with the
existing endoscopic devices and control their bending degrees-of-freedom
(DoFs); (ii) a feeder mechanism that can control the insertion/retraction DoF
of a colonoscope, and (iii) a complementary and intuitive user interface that
enables simultaneous control of all DoFs during the procedure. To analyze the
design of the proposed mechanisms, we also introduce a mathematical modeling
approach and a design space for optimal selection of the parameters involved in
the design of gripping and feeder mechanisms. Our simulation and experimental
studies thoroughly demonstrate the performance of the proposed mathematical
modeling and robotic framework.

</details>


### [249] [FastTrack: GPU-Accelerated Tracking for Visual SLAM](https://arxiv.org/abs/2509.10757)
*Kimia Khabiri,Parsa Hosseininejad,Shishir Gopinath,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: 利用GPU加速视觉-惯性SLAM的跟踪模块，性能提升2.8倍。


<details>
  <summary>Details</summary>
Motivation: 为避免定位不准确或跟踪丢失，需要及时完成每帧的跟踪处理。

Method: 通过CUDA在ORB-SLAM3跟踪过程中实现了立体特征匹配和局部地图跟踪的GPU加速。

Result: 在桌面和Jetson Xavier NX板上，立体惯性模式下跟踪性能提升了高达2.8倍。

Conclusion: 该论文提出了一种利用GPU加速视觉-惯性SLAM系统中跟踪模块的新方法，显著提升了跟踪性能。

Abstract: The tracking module of a visual-inertial SLAM system processes incoming image
frames and IMU data to estimate the position of the frame in relation to the
map. It is important for the tracking to complete in a timely manner for each
frame to avoid poor localization or tracking loss. We therefore present a new
approach which leverages GPU computing power to accelerate time-consuming
components of tracking in order to improve its performance. These components
include stereo feature matching and local map tracking. We implement our design
inside the ORB-SLAM3 tracking process using CUDA. Our evaluation demonstrates
an overall improvement in tracking performance of up to 2.8x on a desktop and
Jetson Xavier NX board in stereo-inertial mode, using the well-known SLAM
datasets EuRoC and TUM-VI.

</details>


### [250] [RSL-RL: A Learning Library for Robotics Research](https://arxiv.org/abs/2509.10771)
*Clemens Schwarke,Mayank Mittal,Nikita Rudin,David Hoeller,Marco Hutter*

Main category: cs.RO

TL;DR: RSL-RL是一个专为机器人设计的开源强化学习库，强调轻量化和可扩展性，支持GPU训练，已验证其实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 为机器人研究提供一个轻量级、可扩展且实用的框架，解决机器人特有的挑战。

Method: 其设计理念强调代码库的紧凑性和易修改性，支持GPU训练，并专注于机器人领域广泛采用的算法及辅助技术。

Result: 在大型仿真环境中实现高吞吐性能，并在仿真基准和实际机器人实验中验证了其有效性。

Conclusion: RSL-RL是一个开源强化学习库，专为机器人社区的需求设计，已在仿真和实际机器人实验中验证其有效性。

Abstract: RSL-RL is an open-source Reinforcement Learning library tailored to the
specific needs of the robotics community. Unlike broad general-purpose
frameworks, its design philosophy prioritizes a compact and easily modifiable
codebase, allowing researchers to adapt and extend algorithms with minimal
overhead. The library focuses on algorithms most widely adopted in robotics,
together with auxiliary techniques that address robotics-specific challenges.
Optimized for GPU-only training, RSL-RL achieves high-throughput performance in
large-scale simulation environments. Its effectiveness has been validated in
both simulation benchmarks and in real-world robotic experiments, demonstrating
its utility as a lightweight, extensible, and practical framework to develop
learning-based robotic controllers. The library is open-sourced at:
https://github.com/leggedrobotics/rsl_rl.

</details>


### [251] [Follow-Bench: A Unified Motion Planning Benchmark for Socially-Aware Robot Person Following](https://arxiv.org/abs/2509.10796)
*Hanjing Ye,Weixi Situ,Jianwei Peng,Yu Zhan,Bingyi Xia,Kuanqi Cai,Hong Zhang*

Main category: cs.RO

TL;DR: 该论文首次端到端研究机器人跟随人，提出统一基准并评估六种规划器，揭示安全与舒适性的权衡及未来方向。


<details>
  <summary>Details</summary>
Motivation: 机器人跟随人（RPF）在个人助理、安全巡逻、老年护理和物流等领域有新兴应用，但需确保目标及周围人的安全与舒适。

Method: 论文提出了第一个端到端的机器人跟随人研究，包括（i）调查代表性场景、运动规划方法和评估指标；（ii）引入Follow-Bench统一基准；（iii）重新实现六种流行的RPF规划器。

Result: 论文通过仿真和真实实验评估了六种规划器，并提供了实际部署的见解。

Conclusion: 该论文通过仿真和真实世界实验，定量分析了现有规划器在安全性与舒适性之间的权衡，揭示了开放挑战和未来研究方向。

Abstract: Robot person following (RPF) -- mobile robots that follow and assist a
specific person -- has emerging applications in personal assistance, security
patrols, eldercare, and logistics. To be effective, such robots must follow the
target while ensuring safety and comfort for both the target and surrounding
people. In this work, we present the first end-to-end study of RPF, which (i)
surveys representative scenarios, motion-planning methods, and evaluation
metrics with a focus on safety and comfort; (ii) introduces Follow-Bench, a
unified benchmark simulating diverse scenarios, including various target
trajectory patterns, dynamic-crowd flows, and environmental layouts; and (iii)
re-implements six popular RPF planners, ensuring that both safety and comfort
are systematically considered. Moreover, we evaluate the two highest-performing
planners from our benchmark on a differential-drive robot to provide insights
into real-world deployment. Extensive simulation and real-world experiments
provide quantitative insights into the safety-comfort trade-offs of existing
planners, while revealing open challenges and future research directions.

</details>


### [252] [A Universal Wire Testing Machine for Enhancing the Performance of Wire-Driven Robots](https://arxiv.org/abs/2509.10862)
*Temma Suzuki,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: 研发出通用线缆测试机，优化线缆特性并成功应用于线驱动机器人，减少力误差。


<details>
  <summary>Details</summary>
Motivation: 线缆作为一种轻量、低摩擦的传输机制，由于材料柔性导致建模误差大，在工业和科研机器人中的应用受限。本研究旨在通过优化线缆特性提升线驱动机器人的性能。

Method: 构建了通用线缆测试机，用于测量和调整线缆特性，包括消除初始线缆拉伸、测量八种不同直径被动滑轮的张力传输效率以及测量变长线缆的动态行为。

Result: 测试机成功测量了多种线缆特性，并将数据应用于实际线驱动机器人的力控制，显著减少了末端执行器的力误差。

Conclusion: 通过应用从通用线缆测试机获得的数据，成功减少了实际线驱动机器人的末端执行器力误差，验证了该方法的有效性。

Abstract: Compared with gears and linkages, wires constitute a lightweight,
low-friction transmission mechanism. However, because wires are flexible
materials, they tend to introduce large modeling errors, and their adoption in
industrial and research robots remains limited.In this study, we built a
Universal Wire Testing Machine that enables measurement and adjustment of wire
characteristics to improve the performance of wire-driven mechanisms. Using
this testing machine, we carried out removal of initial wire stretch,
measurement of tension transmission efficiency for eight different diameters of
passive pulleys, and measurement of the dynamic behavior of variable-length
wires. Finally, we applied the data obtained from this testing machine to the
force control of an actual wire-driven robot, reducing the end-effector force
error.

</details>


### [253] [Nav-R1: Reasoning and Navigation in Embodied Scenes](https://arxiv.org/abs/2509.10884)
*Qingxiang Liu,Ting Huang,Zeyu Zhang,Hao Tang*

Main category: cs.RO

TL;DR: Nav-R1是一种新型的导航推理模型，通过大规模数据集和强化学习框架，显著提升了导航性能和实时控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨环境泛化和实时导航控制方面存在不足，需要一种统一的推理模型来提升性能。

Method: 构建Nav-CoT-110K数据集，采用GRPO强化学习框架，结合格式、理解和导航三种奖励，并引入Fast-in-Slow推理范式。

Result: Nav-R1在多个基准测试中平均提升8%的性能，并在真实机器人部署中验证了其鲁棒性。

Conclusion: Nav-R1模型通过整合感知、推理和行动，解决了现有方法在复杂3D环境中推理不连贯和实时控制困难的问题，显著提升了导航和推理性能。

Abstract: Embodied navigation requires agents to integrate perception, reasoning, and
action for robust interaction in complex 3D environments. Existing approaches
often suffer from incoherent and unstable reasoning traces that hinder
generalization across diverse environments, and difficulty balancing
long-horizon semantic reasoning with low-latency control for real-time
navigation. To address these challenges, we propose Nav-R1, an embodied
foundation model that unifies reasoning in embodied environments. We first
construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought
(CoT) for embodied tasks, which enables cold-start initialization with
structured reasoning. Building on this foundation, we design a GRPO-based
reinforcement learning framework with three complementary rewards: format,
understanding, and navigation, to improve structural adherence, semantic
grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow
reasoning paradigm, decoupling deliberate semantic reasoning from low-latency
reactive control for efficient yet coherent navigation. Extensive evaluations
on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms
strong baselines, with over 8% average improvement in reasoning and navigation
performance. Real-world deployment on a mobile robot further validates its
robustness under limited onboard resources. Code:
https://github.com/AIGeeksGroup/Nav-R1. Website:
https://aigeeksgroup.github.io/Nav-R1.

</details>


### [254] [Design of scalable orthogonal digital encoding architecture for large-area flexible tactile sensing in robotics](https://arxiv.org/abs/2509.10888)
*Weijie Liu,Ziyi Qiu,Shihang Wang,Deqing Mei,Yancheng Wang*

Main category: cs.RO

TL;DR: 本研究提出了一种基于正交数字编码的新架构，显著减少布线需求并提高数据吞吐量，为可扩展的类人触觉感知系统铺平道路。


<details>
  <summary>Details</summary>
Motivation: 现有柔性触觉传感器在编码效率和布线复杂性方面存在瓶颈，限制了实现类人皮肤触觉感知的可扩展性和实时性能。

Method: 采用代码分多址启发的正交数字编码架构，实现分布式传感节点的并行能量正交基码叠加，大幅减少布线需求并提高数据吞吐量。

Result: 通过16节点传感阵列验证，仅用单根传输线实现了12.8毫秒的时间分辨率，且在节点数量大幅变化时仍能保持亚20毫秒延迟。

Conclusion: 通过重新定义软电子中的信号编码范式，本研究为开发具有类人感知能力的可扩展智能系统开辟了新领域。

Abstract: Human-like embodied tactile perception is crucial for the next-generation
intelligent robotics. Achieving large-area, full-body soft coverage with high
sensitivity and rapid response, akin to human skin, remains a formidable
challenge due to critical bottlenecks in encoding efficiency and wiring
complexity in existing flexible tactile sensors, thus significantly hinder the
scalability and real-time performance required for human skin-level tactile
perception. Herein, we present a new architecture employing code division
multiple access-inspired orthogonal digital encoding to overcome these
challenges. Our decentralized encoding strategy transforms conventional serial
signal transmission by enabling parallel superposition of energy-orthogonal
base codes from distributed sensing nodes, drastically reducing wiring
requirements and increasing data throughput. We implemented and validated this
strategy with off-the-shelf 16-node sensing array to reconstruct the pressure
distribution, achieving a temporal resolution of 12.8 ms using only a single
transmission wire. Crucially, the architecture can maintain sub-20ms latency
across orders-of-magnitude variations in node number (to thousands of nodes).
By fundamentally redefining signal encoding paradigms in soft electronics, this
work opens new frontiers in developing scalable embodied intelligent systems
with human-like sensory capabilities.

</details>


### [255] [ViSTR-GP: Online Cyberattack Detection via Vision-to-State Tensor Regression and Gaussian Processes in Automated Robotic Operations](https://arxiv.org/abs/2509.10948)
*Navid Aftabi,Philip Samaha,Jin Ma,Long Cheng,Ramy Harik,Dan Li*

Main category: cs.RO

TL;DR: 该论文提出了一种在线检测框架ViSTR-GP，通过视觉和编码器数据的交叉验证，有效检测工业机器人系统中的数据完整性攻击，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着智能制造的发展，工业机器人系统面临日益增长的网络安全风险，尤其是数据完整性攻击，这些攻击难以通过现有入侵检测或基于模型的检测方法发现。

Method: 开发了一个在线检测框架ViSTR-GP，通过交叉检查编码器报告的测量值与来自外部摄像头（不在控制器权限内）的视觉估计值。该框架使用一次性交互式分割初始化SAM-Track生成每帧掩码，低秩张量回归替代将每个掩码映射到测量值，而矩阵变量高斯过程建模名义残差，捕获时间结构和跨关节相关性。

Result: 在真实机器人测试平台上验证，结果显示该框架能准确恢复关节角度，并比所有基线方法更早检测到数据完整性攻击，且警报更频繁。这些改进在最具隐蔽性的攻击中尤为明显。

Conclusion: 工业机器人系统可以通过增加独立的物理通道来检测数据完整性攻击，绕过控制器的权限，而无需复杂的仪器。

Abstract: Industrial robotic systems are central to automating smart manufacturing
operations. Connected and automated factories face growing cybersecurity risks
that can potentially cause interruptions and damages to physical operations.
Among these attacks, data-integrity attacks often involve sophisticated
exploitation of vulnerabilities that enable an attacker to access and
manipulate the operational data and are hence difficult to detect with only
existing intrusion detection or model-based detection. This paper addresses the
challenges in utilizing existing side-channels to detect data-integrity attacks
in robotic manufacturing processes by developing an online detection framework,
ViSTR-GP, that cross-checks encoder-reported measurements against a
vision-based estimate from an overhead camera outside the controller's
authority. In this framework, a one-time interactive segmentation initializes
SAM-Track to generate per-frame masks. A low-rank tensor-regression surrogate
maps each mask to measurements, while a matrix-variate Gaussian process models
nominal residuals, capturing temporal structure and cross-joint correlations. A
frame-wise test statistic derived from the predictive distribution provides an
online detector with interpretable thresholds. We validate the framework on a
real-world robotic testbed with synchronized video frame and encoder data,
collecting multiple nominal cycles and constructing replay attack scenarios
with graded end-effector deviations. Results on the testbed indicate that the
proposed framework recovers joint angles accurately and detects data-integrity
attacks earlier with more frequent alarms than all baselines. These
improvements are most evident in the most subtle attacks. These results show
that plants can detect data-integrity attacks by adding an independent physical
channel, bypassing the controller's authority, without needing complex
instrumentation.

</details>


### [256] [ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation](https://arxiv.org/abs/2509.10952)
*Yangcen Liu,Woo Chul Shin,Yunhai Han,Zhenyang Chen,Harish Ravichandar,Danfei Xu*

Main category: cs.RO

TL;DR: ImMimic通过DTW和MixUp插值，结合人类视频与少量机器人数据，有效缩小领域差距，提升机器人操作性能。


<details>
  <summary>Details</summary>
Motivation: 通过人类视频学习机器人操作可以降低数据收集成本，但领域差距（如视觉、形态、物理差异）阻碍了直接模仿。

Method: 利用动态时间规整（DTW）和MixUp插值技术，将人类手部动作映射到机器人关节，生成中间域以平滑适应。

Result: 在四个真实世界操作任务和四种机器人实施例上，ImMimic显著提高了任务成功率和执行流畅度。

Conclusion: ImMimic框架通过结合人类视频和少量机器人演示，有效缩小了领域差距，提升了机器人操作的鲁棒性和流畅性。

Abstract: Learning robot manipulation from abundant human videos offers a scalable
alternative to costly robot-specific data collection. However, domain gaps
across visual, morphological, and physical aspects hinder direct imitation. To
effectively bridge the domain gap, we propose ImMimic, an embodiment-agnostic
co-training framework that leverages both human videos and a small amount of
teleoperated robot demonstrations. ImMimic uses Dynamic Time Warping (DTW) with
either action- or visual-based mapping to map retargeted human hand poses to
robot joints, followed by MixUp interpolation between paired human and robot
trajectories. Our key insights are (1) retargeted human hand trajectories
provide informative action labels, and (2) interpolation over the mapped data
creates intermediate domains that facilitate smooth domain adaptation during
co-training. Evaluations on four real-world manipulation tasks (Pick and Place,
Push, Hammer, Flip) across four robotic embodiments (Robotiq, Fin Ray, Allegro,
Ability) show that ImMimic improves task success rates and execution
smoothness, highlighting its efficacy to bridge the domain gap for robust robot
manipulation. The project website can be found at
https://sites.google.com/view/immimic.

</details>


### [257] [Pogosim -- a Simulator for Pogobot robots](https://arxiv.org/abs/2509.10968)
*Leo Cazenille,Loona Macabre,Nicolas Bredeche*

Main category: cs.RO

TL;DR: Pogosim是为Pogobots设计的模拟器，解决了算法开发的高成本问题，支持代码无缝切换于模拟与真实实验，优化了研究流程。


<details>
  <summary>Details</summary>
Motivation: Pogobots虽为群体机器人研究提供了低成本、模块化平台，但直接测试分布式算法耗时耗力，复杂问题或参数校准更会资源紧张。Pogosim旨在解决这些开发瓶颈。

Method: 详细介绍了Pogosim的软件架构、配置文件编写方法、用户程序开发流程，以及模拟与实验的差异与近似处理。同时，支持并行启动大量模拟、结果检索与分析，以及通过优化算法调整用户代码参数。

Result: Pogosim成功实现了在模拟环境中高效开发和测试算法，并确保同一代码可直接应用于真实机器人实验，显著提升了研究效率。

Conclusion: Pogosim作为一种快速且可扩展的模拟器，有效降低了Pogobots算法开发成本，实现了代码在模拟与真实机器人实验中的无缝切换。

Abstract: Pogobots are a new type of open-source/open-hardware robots specifically
designed for swarm robotics research. Their cost-effective and modular design,
complemented by vibration-based and wheel-based locomotion, fast infrared
communication and extensive software architecture facilitate the implementation
of swarm intelligence algorithms. However, testing even simple distributed
algorithms directly on robots is particularly labor-intensive. Scaling to more
complex problems or calibrate user code parameters will have a prohibitively
high strain on available resources. In this article we present Pogosim, a fast
and scalable simulator for Pogobots, designed to reduce as much as possible
algorithm development costs. The exact same code will be used in both
simulation and to experimentally drive real robots. This article details the
software architecture of Pogosim, explain how to write configuration files and
user programs and how simulations approximate or differ from experiments. We
describe how a large set of simulations can be launched in parallel, how to
retrieve and analyze the simulation results, and how to optimize user code
parameters using optimization algorithms.

</details>


### [258] [Autonomous Close-Proximity Photovoltaic Panel Coating Using a Quadcopter](https://arxiv.org/abs/2509.10979)
*Dimitri Jacquemont,Carlo Bosio,Teaya Yang,Ruiqi Zhang,Ozgur Orun,Shuai Li,Reza Alam,Thomas M. Schutzius,Simo A. Makiharju,Mark W. Mueller*

Main category: cs.RO

TL;DR: 无人机系统自动化喷涂光伏板保护涂层，提高效率降低成本。


<details>
  <summary>Details</summary>
Motivation: 光伏板在可再生能源领域广泛应用，但现有保护涂层需要频繁手动重新喷涂，成本高且效率低。无人机提供了一种灵活、自主的解决方案。

Method: 提出了一种基于四旋翼无人机的系统，配备液体分散机制，利用视觉惯性里程计和相对位置检测进行定位，采用考虑地面效应和质量变化的模型控制器。

Result: 通过广泛的室内外实验验证了系统的自主能力。

Conclusion: 无人机系统通过自主定位和控制技术，成功实现了光伏板保护涂层的自动化喷涂，验证了其室内外的自主能力。

Abstract: Photovoltaic (PV) panels are becoming increasingly widespread in the domain
of renewable energy, and thus, small efficiency gains can have massive effects.
Anti-reflective and self-cleaning coatings enhance panel performance but
degrade over time, requiring periodic reapplication. Uncrewed Aerial Vehicles
(UAVs) offer a flexible and autonomous way to apply protective coatings more
often and at lower cost compared to traditional manual coating methods. In this
letter, we propose a quadcopter-based system, equipped with a liquid dispersion
mechanism, designed to automate such tasks. The localization stack only uses
onboard sensors, relying on visual-inertial odometry and the relative position
of the PV panel detected with respect to the quadcopter. The control relies on
a model-based controller that accounts for the ground effect and the mass
decrease of the quadcopter during liquid dispersion. We validate the autonomy
capabilities of our system through extensive indoor and outdoor experiments.

</details>


### [259] [Multi-objective task allocation for electric harvesting robots: a hierarchical route reconstruction approach](https://arxiv.org/abs/2509.11025)
*Peng Chen,Jing Liang,Hui Song,Kang-Jia Qiao,Cai-Tong Yue,Kun-Jie Yu,Ponnuthurai Nagaratnam Suganthan,Witold Pedrycz*

Main category: cs.RO

TL;DR: 本文提出了一种名为HRRA的混合分层路由重构算法，用于解决农业多电机器人任务分配问题，实验证明其优于现有算法，并具有探索新解决方案空间的能力。


<details>
  <summary>Details</summary>
Motivation: 农业劳动力成本的增加加速了多机器人系统在果园采摘中的应用，但如何高效协调这些系统仍是一个挑战，尤其是在考虑实际约束条件（如负载依赖的速度变化和电池限制）时。

Method: 本文提出了一种混合分层路由重构算法（HRRA），该算法集成了多种创新机制，包括分层编码结构、双阶段初始化方法、任务序列优化器和专门的路由重构操作符。

Result: 在45个测试实例上的大量实验表明，HRRA在性能上优于七种最先进的算法。统计分析（包括Wilcoxon符号秩检验和Friedman检验）验证了HRRA的竞争力及其探索解决方案空间中先前无法访问区域的独特能力。

Conclusion: 本研究通过提出一种新颖的问题表述和有效算法，为多机器人协调的理论理解做出了贡献，同时也为农业自动化提供了实际见解。

Abstract: The increasing labor costs in agriculture have accelerated the adoption of
multi-robot systems for orchard harvesting. However, efficiently coordinating
these systems is challenging due to the complex interplay between makespan and
energy consumption, particularly under practical constraints like
load-dependent speed variations and battery limitations. This paper defines the
multi-objective agricultural multi-electrical-robot task allocation (AMERTA)
problem, which systematically incorporates these often-overlooked real-world
constraints. To address this problem, we propose a hybrid hierarchical route
reconstruction algorithm (HRRA) that integrates several innovative mechanisms,
including a hierarchical encoding structure, a dual-phase initialization
method, task sequence optimizers, and specialized route reconstruction
operators. Extensive experiments on 45 test instances demonstrate HRRA's
superior performance against seven state-of-the-art algorithms. Statistical
analysis, including the Wilcoxon signed-rank and Friedman tests, empirically
validates HRRA's competitiveness and its unique ability to explore previously
inaccessible regions of the solution space. In general, this research
contributes to the theoretical understanding of multi-robot coordination by
offering a novel problem formulation and an effective algorithm, thereby also
providing practical insights for agricultural automation.

</details>


### [260] [FEWT: Improving Humanoid Robot Perception with Frequency-Enhanced Wavelet-based Transformers](https://arxiv.org/abs/2509.11109)
*Jiaxin Huang,Hanyu Liu,Yunsheng Ma,Jian Shen,Yilin Zheng,Jiayi Wen,Baishu Wan,Pan Li,Zhigong Song*

Main category: cs.RO

TL;DR: FEWT模仿学习框架通过动态融合时域和频域特征，显著提升了人形机器人的操作成功率。


<details>
  <summary>Details</summary>
Motivation: 开发硬件平台以实现直观的远程操作和高效的人形动作数据收集，同时提升人形机器人的感知表现。

Method: 提出了一个名为FEWT的模仿学习框架，包括两个主要模块：FE-EMA和TS-DWT，通过多尺度小波分解和残差网络动态融合时域和频域特征。

Result: FEWT在仿真中将现有算法的成功率提升高达30%，在现实环境中提升6-12%。

Conclusion: FEWT框架通过结合多尺度小波分解和残差网络，显著提升了人形机器人的感知表现和操作成功率，在仿真和现实环境中均优于现有基准。

Abstract: The embodied intelligence bridges the physical world and information space.
As its typical physical embodiment, humanoid robots have shown great promise
through robot learning algorithms in recent years. In this study, a hardware
platform, including humanoid robot and exoskeleton-style teleoperation cabin,
was developed to realize intuitive remote manipulation and efficient collection
of anthropomorphic action data. To improve the perception representation of
humanoid robot, an imitation learning framework, termed Frequency-Enhanced
Wavelet-based Transformer (FEWT), was proposed, which consists of two primary
modules: Frequency-Enhanced Efficient Multi-Scale Attention (FE-EMA) and
Time-Series Discrete Wavelet Transform (TS-DWT). By combining multi-scale
wavelet decomposition with the residual network, FE-EMA can dynamically fuse
features from both time-domain and frequency-domain. This fusion is able to
capture feature information across various scales effectively, thereby
enhancing model robustness. Experimental performance demonstrates that FEWT
improves the success rate of the state-of-the-art algorithm (Action Chunking
with Transformers, ACT baseline) by up to 30% in simulation and by 6-12% in
real-world.

</details>


### [261] [ManiVID-3D: Generalizable View-Invariant Reinforcement Learning for Robotic Manipulation via Disentangled 3D Representations](https://arxiv.org/abs/2509.11125)
*Zheng Li,Pei Qu,Yufei Jia,Shihui Zhou,Haizhou Ge,Jiahang Cao,Jinni Zhou,Guyue Zhou,Jun Ma*

Main category: cs.RO

TL;DR: ManiVID-3D通过自监督学习和高效渲染，提升视觉RL在视角变化下的性能，成功率高且参数少。


<details>
  <summary>Details</summary>
Motivation: 解决视觉强化学习策略因相机视角变化导致的性能下降问题，克服现有方法依赖精确校准或难以应对大视角变化的局限性。

Method: 提出ManiVID-3D架构，结合ViewNet模块自动对齐点云观测到统一空间坐标系，无需外部校准，并开发高效GPU加速批量渲染模块。

Result: 在10个模拟和5个真实任务中，成功率比现有方法高44.7%，参数减少80%，且对严重视角变化具有鲁棒性。

Conclusion: ManiVID-3D通过自监督解耦特征学习和高效的GPU加速渲染模块，显著提升了视觉强化学习策略在视角变化下的鲁棒性和性能，适用于非结构化环境中的机器人操作。

Abstract: Deploying visual reinforcement learning (RL) policies in real-world
manipulation is often hindered by camera viewpoint changes. A policy trained
from a fixed front-facing camera may fail when the camera is shifted--an
unavoidable situation in real-world settings where sensor placement is hard to
manage appropriately. Existing methods often rely on precise camera calibration
or struggle with large perspective changes. To address these limitations, we
propose ManiVID-3D, a novel 3D RL architecture designed for robotic
manipulation, which learns view-invariant representations through
self-supervised disentangled feature learning. The framework incorporates
ViewNet, a lightweight yet effective module that automatically aligns point
cloud observations from arbitrary viewpoints into a unified spatial coordinate
system without the need for extrinsic calibration. Additionally, we develop an
efficient GPU-accelerated batch rendering module capable of processing over
5000 frames per second, enabling large-scale training for 3D visual RL at
unprecedented speeds. Extensive evaluation across 10 simulated and 5 real-world
tasks demonstrates that our approach achieves a 44.7% higher success rate than
state-of-the-art methods under viewpoint variations while using 80% fewer
parameters. The system's robustness to severe perspective changes and strong
sim-to-real performance highlight the effectiveness of learning geometrically
consistent representations for scalable robotic manipulation in unstructured
environments. Our project website can be found in
https://zheng-joe-lee.github.io/manivid3d/.

</details>


### [262] [RoVerFly: Robust and Versatile Learning-based Control of Quadrotor Across Payload Configurations](https://arxiv.org/abs/2509.11149)
*Mintae Kim,Jiaze Cai,Koushil Sreenath*

Main category: cs.RO

TL;DR: RoVerFly是一个学习型控制框架，通过强化学习策略实现了对四旋翼和电缆悬挂负载系统的稳健跟踪控制，适用于多种配置，无需重新调谐。


<details>
  <summary>Details</summary>
Motivation: 由于非线性动力学和欠驱动特性，设计精确的任意轨迹跟踪控制器具有挑战性，尤其是在存在柔性电缆悬挂负载的情况下。传统方法需要大量调谐且无法适应配置变化。

Method: 使用任务和领域随机化训练的强化学习（RL）策略作为控制器。

Result: 控制器在零样本泛化中表现出色，适用于不同负载设置（包括无负载、不同质量和电缆长度），同时保持了反馈跟踪控制器的可解释性和结构。

Conclusion: RoVerFly是一个统一的学习型控制框架，通过强化学习策略实现了对标准四旋翼和电缆悬挂负载系统的稳健和多功能跟踪控制，无需控制器切换或重新调谐。

Abstract: Designing robust controllers for precise, arbitrary trajectory tracking with
quadrotors is challenging due to nonlinear dynamics and underactuation, and
becomes harder with flexible cable-suspended payloads that introduce extra
degrees of freedom and hybridness. Classical model-based methods offer
stability guarantees but require extensive tuning and often do not adapt when
the configuration changes, such as when a payload is added or removed, or when
the payload mass or cable length varies. We present RoVerFly, a unified
learning-based control framework in which a reinforcement learning (RL) policy
serves as a robust and versatile tracking controller for standard quadrotors
and for cable-suspended payload systems across a range of configurations.
Trained with task and domain randomization, the controller is resilient to
disturbances and varying dynamics. It achieves strong zero-shot generalization
across payload settings, including no payload as well as varying mass and cable
length, without controller switching or re-tuning, while retaining the
interpretability and structure of a feedback tracking controller. Code and
supplementary materials are available at
https://github.com/mintaeshkim/roverfly

</details>


### [263] [SAMP: Spatial Anchor-based Motion Policy for Collision-Aware Robotic Manipulators](https://arxiv.org/abs/2509.11185)
*Kai Chen,Zhihai Bi,Guoyang Zhao,Chunxin Zheng,Yulin Li,Hang Zhao,Jun Ma*

Main category: cs.RO

TL;DR: SAMP是一种联合建模机器人和环境几何的神经运动规划框架，显著提升成功率和降低碰撞率。


<details>
  <summary>Details</summary>
Motivation: 现有神经运动规划方法常依赖简化机器人模型或仅关注障碍物表示，导致碰撞检测不完整或在复杂场景中性能下降。

Method: 提出了基于空间锚点的运动策略（SAMP），利用共享空间网格上的符号距离场（SDF）同时编码环境和机械臂，并训练神经运动策略生成平滑、无碰撞的轨迹。

Result: 实验表明，SAMP在仿真和真实环境中均优于现有方法，成功率提高11%，碰撞率降低7%。

Conclusion: SAMP框架通过联合建模机器人和环境几何，在实际挑战性环境中表现出显著优势，成功率和碰撞率均有显著改善。

Abstract: Neural-based motion planning methods have achieved remarkable progress for
robotic manipulators, yet a fundamental challenge lies in simultaneously
accounting for both the robot's physical shape and the surrounding environment
when generating safe and feasible motions. Moreover, existing approaches often
rely on simplified robot models or focus primarily on obstacle representation,
which can lead to incomplete collision detection and degraded performance in
cluttered scenes. To address these limitations, we propose spatial anchor-based
motion policy (SAMP), a unified framework that simultaneously encodes the
environment and the manipulator using signed distance field (SDF) anchored on a
shared spatial grid. SAMP incorporates a dedicated robot SDF network that
captures the manipulator's precise geometry, enabling collision-aware reasoning
beyond coarse link approximations. These representations are fused on spatial
anchors and used to train a neural motion policy that generates smooth,
collision-free trajectories in the proposed efficient feature alignment
strategy. Experiments conducted in both simulated and real-world environments
consistently show that SAMP outperforms existing methods, delivering an 11%
increase in success rate and a 7% reduction in collision rate. These results
highlight the benefits of jointly modelling robot and environment geometry,
demonstrating its practical value in challenging real-world environments.

</details>


### [264] [DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2509.11197)
*Yunheng Wang,Yuetong Fang,Taowen Wang,Yixiao Feng,Yawen Tan,Shuning Zhang,Peiran Liu,Yiding Ji,Renjing Xu*

Main category: cs.RO

TL;DR: DreamNav通过视角校正、轨迹级规划和主动想象解决了零样本VLN的感知成本高、动作语义不对齐和规划短视问题，实现了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有零样本VLN方法依赖昂贵的感知和被动场景理解，导致控制局限于点级选择，部署成本高、动作语义不对齐、规划短视。

Method: DreamNav通过EgoView Corrector对齐视角、Trajectory Predictor进行全局轨迹级规划、Imagination Predictor赋予主动思考能力来解决现有方法的不足。

Result: DreamNav在SR和SPL指标上分别比最强基线高出7.49%和18.15%。

Conclusion: DreamNav在VLN-CE和现实世界测试中实现了零样本的新SOTA，首次统一了轨迹级规划和主动想象，仅使用自我中心输入。

Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE), which
links language instructions to perception and control in the real world, is a
core capability of embodied robots. Recently, large-scale pretrained foundation
models have been leveraged as shared priors for perception, reasoning, and
action, enabling zero-shot VLN without task-specific training. However,
existing zero-shot VLN methods depend on costly perception and passive scene
understanding, collapsing control to point-level choices. As a result, they are
expensive to deploy, misaligned in action semantics, and short-sighted in
planning. To address these issues, we present DreamNav that focuses on the
following three aspects: (1) for reducing sensory cost, our EgoView Corrector
aligns viewpoints and stabilizes egocentric perception; (2) instead of
point-level actions, our Trajectory Predictor favors global trajectory-level
planning to better align with instruction semantics; and (3) to enable
anticipatory and long-horizon planning, we propose an Imagination Predictor to
endow the agent with proactive thinking capability. On VLN-CE and real-world
tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the
strongest egocentric baseline with extra information by up to 7.49\% and
18.15\% in terms of SR and SPL metrics. To our knowledge, this is the first
zero-shot VLN method to unify trajectory-level planning and active imagination
while using only egocentric inputs.

</details>


### [265] [MEMBOT: Memory-Based Robot in Intermittent POMDP](https://arxiv.org/abs/2509.11225)
*Youzhi Liang,Eyan Noronha*

Main category: cs.RO

TL;DR: MEMBOT 是一种模块化内存架构，通过显式信念建模解决机器人控制中的间歇性部分可观测问题，在观测丢失时仍保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现实机器人系统常面临部分或间歇性可观测性，传统强化学习假设完全可观测，难以应对传感器噪声、遮挡或失效的挑战。

Method: MEMBOT 采用模块化内存架构，分两阶段训练：离线多任务预训练学习任务无关的潜在信念编码器（基于状态空间模型和LSTM），再通过行为克隆微调任务特定策略。

Result: 在10个机器人操纵基准任务中，MEMBOT 在50%观测丢失率下仍保持80%峰值性能，优于基线方法。

Conclusion: MEMBOT 通过显式信念建模，在部分可观测的机器人系统中实现了稳健、可迁移且数据高效的控制策略，显著优于无记忆和简单循环基线。

Abstract: Robotic systems deployed in real-world environments often operate under
conditions of partial and often intermittent observability, where sensor inputs
may be noisy, occluded, or entirely unavailable due to failures or
environmental constraints. Traditional reinforcement learning (RL) approaches
that assume full state observability are ill-equipped for such challenges. In
this work, we introduce MEMBOT, a modular memory-based architecture designed to
address intermittent partial observability in robotic control tasks. MEMBOT
decouples belief inference from policy learning through a two-phase training
process: an offline multi-task learning pretraining stage that learns a robust
task-agnostic latent belief encoder using a reconstruction losses, followed by
fine-tuning of task-specific policies using behavior cloning. The belief
encoder, implemented as a state-space model (SSM) and a LSTM, integrates
temporal sequences of observations and actions to infer latent state
representations that persist even when observations are dropped. We train and
evaluate MEMBOT on 10 robotic manipulation benchmark tasks from MetaWorld and
Robomimic under varying rates of observation dropout. Results show that MEMBOT
consistently outperforms both memoryless and naively recurrent baselines,
maintaining up to 80% of peak performance under 50% observation availability.
These findings highlight the effectiveness of explicit belief modeling in
achieving robust, transferable, and data-efficient policies for real-world
partially observable robotic systems.

</details>


### [266] [CORB-Planner: Corridor as Observations for RL Planning in High-Speed Flight](https://arxiv.org/abs/2509.11240)
*Yechen Zhang,Bin Gao,Gang Wang,Jian Sun,Zhuo Li*

Main category: cs.RO

TL;DR: CORB-Planner 是一个 RL 框架，通过 B-spline 和 SFC 实现无人机跨平台高速飞行，训练快速且兼容多种配置。


<details>
  <summary>Details</summary>
Motivation: 解决 RL 在无人机部署中的挑战，如依赖精确动态模型和平台特定传感，阻碍跨平台迁移。

Method: 结合 B-spline 轨迹生成和 RL 策略，通过启发式搜索获得紧凑的安全飞行走廊（SFC）表示，采用基于值的软分解批评 Q（SDCQ）算法进行快速训练。

Result: 在模拟和实际测试中，CORB-Planner 在轻量级机载硬件上实现实时规划，支持在密集杂乱环境中最高 8.2m/s 的飞行速度，且无需外部定位。

Conclusion: CORB-Planner 展示了在异构无人机平台上实现实时高速自主飞行的潜力，其兼容性和鲁棒性使其适用于实际部署。

Abstract: Reinforcement learning (RL) has shown promise in a large number of robotic
control tasks. Nevertheless, its deployment on unmanned aerial vehicles (UAVs)
remains challenging, mainly because of reliance on accurate dynamic models and
platform-specific sensing, which hinders cross-platform transfer. This paper
presents the CORB-Planner (Corridor-as-Observations for RL B-spline planner), a
real-time, RL-based trajectory planning framework for high-speed autonomous UAV
flight across heterogeneous platforms. The key idea is to combine B-spline
trajectory generation with the RL policy producing successive control points
with a compact safe flight corridor (SFC) representation obtained via heuristic
search. The SFC abstracts obstacle information in a low-dimensional form,
mitigating overfitting to platform-specific details and reducing sensitivity to
model inaccuracies. To narrow the sim-to-real gap, we adopt an easy-to-hard
progressive training pipeline in simulation. A value-based soft
decomposed-critic Q (SDCQ) algorithm is used to learn effective policies within
approximately ten minutes of training. Benchmarks in simulation and real-world
tests demonstrate real-time planning on lightweight onboard hardware and
support maximum flight speeds up to 8.2m/s in dense, cluttered environments
without external positioning. Compatibility with various UAV configurations
(quadrotors, hexarotors) and modest onboard compute underlines the generality
and robustness of CORB-Planner for practical deployment.

</details>


### [267] [Embodied Intelligence in Disassembly: Multimodal Perception Cross-validation and Continual Learning in Neuro-Symbolic TAMP](https://arxiv.org/abs/2509.11270)
*Ziwen He,Zhigang Wang,Yanlong Peng,Pengxu Chang,Hong Yang,Ming Chen*

Main category: cs.RO

TL;DR: 本文提出了一种持续学习框架，通过神经符号任务和运动规划及多模态感知交叉验证，显著提升了动态拆解场景中的任务成功率和感知准确性。


<details>
  <summary>Details</summary>
Motivation: 随着新能源汽车行业的快速发展，动力电池的高效拆解和回收成为循环经济的关键挑战。当前非结构化拆解场景中的动态环境严重限制了机器人感知的鲁棒性，阻碍了工业应用中的自主拆解。

Method: 本文提出了一种基于神经符号任务和运动规划（TAMP）的持续学习框架，集成了多模态感知交叉验证机制到双向推理流程中。

Result: 实验结果表明，所提出的框架将动态拆解场景中的任务成功率从81.68%提高到100%，同时将平均感知误判次数从3.389降低到1.128。

Conclusion: 该研究为在复杂工业环境中增强具身智能的鲁棒性和适应性提供了新范式。

Abstract: With the rapid development of the new energy vehicle industry, the efficient
disassembly and recycling of power batteries have become a critical challenge
for the circular economy. In current unstructured disassembly scenarios, the
dynamic nature of the environment severely limits the robustness of robotic
perception, posing a significant barrier to autonomous disassembly in
industrial applications. This paper proposes a continual learning framework
based on Neuro-Symbolic task and motion planning (TAMP) to enhance the
adaptability of embodied intelligence systems in dynamic environments. Our
approach integrates a multimodal perception cross-validation mechanism into a
bidirectional reasoning flow: the forward working flow dynamically refines and
optimizes action strategies, while the backward learning flow autonomously
collects effective data from historical task executions to facilitate continual
system learning, enabling self-optimization. Experimental results show that the
proposed framework improves the task success rate in dynamic disassembly
scenarios from 81.68% to 100%, while reducing the average number of perception
misjudgments from 3.389 to 1.128. This research provides a new paradigm for
enhancing the robustness and adaptability of embodied intelligence in complex
industrial environments.

</details>


### [268] [Policy Learning for Social Robot-Led Physiotherapy](https://arxiv.org/abs/2509.11297)
*Carl Bettosi,Lynne Ballie,Susan Shenkin,Marta Romeo*

Main category: cs.RO

TL;DR: 研究利用专业医疗从业者作为患者代理，训练强化学习策略，使社会机器人能适应个体化需求，自主指导物理治疗。


<details>
  <summary>Details</summary>
Motivation: 解决物理治疗中患者行为数据稀缺的问题，以实现社会机器人在自主指导患者进行物理治疗时的有效决策。

Method: 通过33名专业医疗从业者作为患者代理，与机器人互动生成患者行为模型，并利用该模型在模拟环境中训练强化学习策略。

Result: 训练的策略能够根据患者的个体化耐力和波动表现调整运动指令，适用于不同康复阶段和运动计划的患者。

Conclusion: 研究表明，基于强化学习的策略能够有效适应患者的个体化需求和不同康复阶段，为物理治疗提供了可行的自主指导解决方案。

Abstract: Social robots offer a promising solution for autonomously guiding patients
through physiotherapy exercise sessions, but effective deployment requires
advanced decision-making to adapt to patient needs. A key challenge is the
scarcity of patient behavior data for developing robust policies. To address
this, we engaged 33 expert healthcare practitioners as patient proxies, using
their interactions with our robot to inform a patient behavior model capable of
generating exercise performance metrics and subjective scores on perceived
exertion. We trained a reinforcement learning-based policy in simulation,
demonstrating that it can adapt exercise instructions to individual exertion
tolerances and fluctuating performance, while also being applicable to patients
at different recovery stages with varying exercise plans.

</details>


### [269] [Brain-Robot Interface for Exercise Mimicry](https://arxiv.org/abs/2509.11306)
*Carl Bettosi,Emilyann Nault,Lynne Baillie,Markus Garschall,Marta Romeo,Beatrix Wais-Zechmann,Nicole Binderlehner,Theodoros Georgio*

Main category: cs.RO

TL;DR: 研究开发了一种BRI系统，使社交机器人能实时模仿患者运动，以建立融洽关系。初步研究表明系统有效，参与者对机器人教练持积极态度。


<details>
  <summary>Details</summary>
Motivation: 为了维持社会机器人作为运动教练的长期参与度，建立融洽关系至关重要。模仿（motor mimicry）被认为是一种促进融洽关系的有效工具。

Method: 开发了一种新型的脑-机器人接口（BRI），使社交机器人教练能够通过患者意图的脑命令实时模仿患者的运动动作。

Result: 在14名参与者（3名物理治疗师和11名偏瘫患者）的探索性研究中，系统在12次会话中成功展示了运动模仿，但准确性有所差异。参与者对机器人教练持积极态度，信任和接受度较高。

Conclusion: 社会机器人作为运动教练，通过实时模仿患者的运动动作，能够有效建立信任和接受度，且BRI技术的引入并未影响这些正面感知。

Abstract: For social robots to maintain long-term engagement as exercise instructors,
rapport-building is essential. Motor mimicry--imitating one's physical
actions--during social interaction has long been recognized as a powerful tool
for fostering rapport, and it is widely used in rehabilitation exercises where
patients mirror a physiotherapist or video demonstration. We developed a novel
Brain-Robot Interface (BRI) that allows a social robot instructor to mimic a
patient's exercise movements in real-time, using mental commands derived from
the patient's intention. The system was evaluated in an exploratory study with
14 participants (3 physiotherapists and 11 hemiparetic patients recovering from
stroke or other injuries). We found our system successfully demonstrated
exercise mimicry in 12 sessions; however, accuracy varied. Participants had
positive perceptions of the robot instructor, with high trust and acceptance
levels, which were not affected by the introduction of BRI technology.

</details>


### [270] [ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic Manipulation](https://arxiv.org/abs/2509.11364)
*Sheng Liu,Zhe Li,Weiheng Wang,Han Sun,Heng Zhang,Hongpeng Chen,Yusen Qin,Arash Ajoudani,Yizhao Wang*

Main category: cs.RO

TL;DR: 提出一种结合VLM与机器人想象的主动姿态估计方法，动态解决歧义，实验显示其优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 解决零样本方法在视角歧义下的失败问题，以及固定摄像头在物体移动或自遮挡时的局限性。

Method: 结合视觉语言模型（VLM）与‘机器人想象’，动态检测并实时解决歧义。离线阶段渲染CAD模型的多视角，计算FoundationPose熵，构建几何感知提示。运行时查询VLM歧义分数，若检测到歧义则渲染虚拟视角生成候选相机姿态，选择最佳视角（NBV）进行姿态估计。此外，引入基于扩散策略的主动姿态跟踪模块。

Result: 实验证明该方法在模拟和现实场景中显著优于传统方法。

Conclusion: 提出的主动姿态估计和跟踪方法在模拟和现实世界中显著优于传统基线。

Abstract: Accurate 6-DoF object pose estimation and tracking are critical for reliable
robotic manipulation. However, zero-shot methods often fail under
viewpoint-induced ambiguities and fixed-camera setups struggle when objects
move or become self-occluded. To address these challenges, we propose an active
pose estimation pipeline that combines a Vision-Language Model (VLM) with
"robotic imagination" to dynamically detect and resolve ambiguities in real
time. In an offline stage, we render a dense set of views of the CAD model,
compute the FoundationPose entropy for each view, and construct a
geometric-aware prompt that includes low-entropy (unambiguous) and high-entropy
(ambiguous) examples. At runtime, the system: (1) queries the VLM on the live
image for an ambiguity score; (2) if ambiguity is detected, imagines a discrete
set of candidate camera poses by rendering virtual views, scores each based on
a weighted combination of VLM ambiguity probability and FoundationPose entropy,
and then moves the camera to the Next-Best-View (NBV) to obtain a disambiguated
pose estimation. Furthermore, since moving objects may leave the camera's field
of view, we introduce an active pose tracking module: a diffusion-policy
trained via imitation learning, which generates camera trajectories that
preserve object visibility and minimize pose ambiguity. Experiments in
simulation and real-world show that our approach significantly outperforms
classical baselines.

</details>


### [271] [Quantum deep reinforcement learning for humanoid robot navigation task](https://arxiv.org/abs/2509.11388)
*Romerik Lokossou,Birhanu Shimelis Girma,Ozan K. Tonguz,Ahmed Biyabani*

Main category: cs.RO

TL;DR: 量子深度强化学习（QDRL）在训练人形机器人时比传统方法更高效，性能提升且训练步骤大幅减少。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在复杂、高维环境中面临参数需求大和随机性挑战，本研究探索量子计算与深度强化学习的结合以提升效率。

Method: 采用参数化量子电路和混合量子-经典设置，直接处理高维状态空间，避免传统映射和规划方法。

Result: 量子SAC在Humanoid-v4和Walker2d-v4环境中比经典SAC平均回报提高8%（246.40 vs 228.36），且训练步骤减少92%。

Conclusion: 量子深度强化学习（QDRL）在复杂高维环境中展现出比传统强化学习更高效的学习能力，尤其在减少训练步骤的同时提升性能。

Abstract: Classical reinforcement learning (RL) methods often struggle in complex,
high-dimensional environments because of their extensive parameter requirements
and challenges posed by stochastic, non-deterministic settings. This study
introduces quantum deep reinforcement learning (QDRL) to train humanoid agents
efficiently. While previous quantum RL models focused on smaller environments,
such as wheeled robots and robotic arms, our work pioneers the application of
QDRL to humanoid robotics, specifically in environments with substantial
observation and action spaces, such as MuJoCo's Humanoid-v4 and Walker2d-v4.
Using parameterized quantum circuits, we explored a hybrid quantum-classical
setup to directly navigate high-dimensional state spaces, bypassing traditional
mapping and planning. By integrating quantum computing with deep RL, we aim to
develop models that can efficiently learn complex navigation tasks in humanoid
robots. We evaluated the performance of the Soft Actor-Critic (SAC) in
classical RL against its quantum implementation. The results show that the
quantum SAC achieves an 8% higher average return (246.40) than the classical
SAC (228.36) after 92% fewer steps, highlighting the accelerated learning
potential of quantum computing in RL tasks.

</details>


### [272] [TRUST 2025: SCRITA and RTSS @ RO-MAN 2025](https://arxiv.org/abs/2509.11402)
*Alessandra Rossi,Patrick Holthaus,Gabriella Lakatos,Sílvia Moros,Ali Fallahi,Murat Kirtay,Marie Postma,Erhan Oztop*

Main category: cs.RO

TL;DR: TRUST研讨会整合了SCRITA和RTSS，推动了信任问题的多视角研究。


<details>
  <summary>Details</summary>
Motivation: 为了促进人类-机器人交互领域中对信任问题的多视角研究。

Method: 通过协作和整合两个已有工作坊（SCRITA和RTSS）的资源与目标。

Result: 成功举办了TRUST研讨会，汇集了人类和机器人视角下的信任研究。

Conclusion: TRUST研讨会通过整合SCRITA和RTSS两个工作坊的互补目标，推动了从人类和机器人双重视角对信任问题的研究。

Abstract: The TRUST workshop is the result of a collaboration between two established
workshops in the field of Human-Robot Interaction: SCRITA (Trust, Acceptance
and Social Cues in Human-Robot Interaction) and RTSS (Robot Trust for Symbiotic
Societies). This joint initiative brings together the complementary goals of
these workshops to advance research on trust from both the human and robot
perspectives.
  Website: https://scrita.herts.ac.uk/2025/

</details>


### [273] [Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations](https://arxiv.org/abs/2509.11417)
*Shresth Grover,Akshay Gopalkrishnan,Bo Ai,Henrik I. Christensen,Hao Su,Xuanlin Li*

Main category: cs.RO

TL;DR: 论文提出了一种保留预训练特征的VLA框架，通过双编码器、动作标记器和协同训练策略，显著提升了机器人操作的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 直接从视觉语言模型（VLM）微调的视觉语言动作（VLA）模型可能会破坏预训练表示并限制泛化能力，因此需要一种更好的框架来保留这些特征。

Method: 方法包括：(i) 双编码器设计，一个冻结的视觉编码器保留预训练特征，另一个可训练以适应任务；(ii) 基于字符串的动作标记器，将连续动作转换为字符序列；(iii) 结合机器人演示和强调空间推理及功能的视觉语言数据集的协同训练策略。

Result: 在仿真和真实机器人上的评估表明，该方法在视觉干扰鲁棒性、对新指令和环境的泛化能力以及任务成功率方面优于基线方法。

Conclusion: 该论文提出的框架通过保留预训练特征并适应机器人操作，显著提高了模型对视觉干扰的鲁棒性、对新指令和环境的泛化能力以及任务成功率。

Abstract: Vision-language-action (VLA) models finetuned from vision-language models
(VLMs) hold the promise of leveraging rich pretrained representations to build
generalist robots across diverse tasks and environments. However, direct
fine-tuning on robot data often disrupts these representations and limits
generalization. We present a framework that better preserves pretrained
features while adapting them for robot manipulation. Our approach introduces
three components: (i) a dual-encoder design with one frozen vision encoder to
retain pretrained features and another trainable for task adaptation, (ii) a
string-based action tokenizer that casts continuous actions into character
sequences aligned with the model's pretraining domain, and (iii) a co-training
strategy that combines robot demonstrations with vision-language datasets
emphasizing spatial reasoning and affordances. Evaluations in simulation and on
real robots show that our method improves robustness to visual perturbations,
generalization to novel instructions and environments, and overall task success
compared to baselines.

</details>


### [274] [A Software-Only Post-Processor for Indexed Rotary Machining on GRBL-Based CNCs](https://arxiv.org/abs/2509.11433)
*Pedro Portugal,Damian D. Venghaus,Diego Lopez*

Main category: cs.RO

TL;DR: 该研究提出了一种纯软件框架，用于在基于GRBL的CNC上进行索引旋转加工，降低了多轴加工的门槛。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案通常需要硬件改装、替代控制器或商业CAM软件，增加了成本和复杂性。

Method: 提出了一种纯软件的框架，通过自定义后处理器将平面刀具路径转换为离散的旋转步骤，并通过基于浏览器的界面执行。

Result: 该方法实现了仅使用标准、现成机械的实用旋转轴加工，无需固件修改。

Conclusion: 该框架通过降低技术和财务门槛，扩展了多轴加工在教育、创客空间和小型工作坊中的应用，支持实践学习和快速原型制作。

Abstract: Affordable desktop CNC routers are common in education, prototyping, and
makerspaces, but most lack a rotary axis, limiting fabrication of rotationally
symmetric or multi-sided parts. Existing solutions often require hardware
retrofits, alternative controllers, or commercial CAM software, raising cost
and complexity. This work presents a software-only framework for indexed rotary
machining on GRBL-based CNCs. A custom post-processor converts planar toolpaths
into discrete rotary steps, executed through a browser-based interface. While
not equivalent to continuous 4-axis machining, the method enables practical
rotary-axis fabrication using only standard, off-the-shelf mechanics, without
firmware modification. By reducing technical and financial barriers, the
framework expands access to multi-axis machining in classrooms, makerspaces,
and small workshops, supporting hands-on learning and rapid prototyping.

</details>


### [275] [RAPTOR: A Foundation Policy for Quadrotor Control](https://arxiv.org/abs/2509.11481)
*Jonas Eschmann,Dario Albani,Giuseppe Loianno*

Main category: cs.RO

TL;DR: RAPTOR通过元模仿学习训练出一个轻量级、高度自适应的四旋翼控制策略，能在毫秒级别内零样本适应多种未见过的飞行器。


<details>
  <summary>Details</summary>
Motivation: 解决现代机器人控制系统（如基于强化学习的神经网络策略）在新环境下因过拟合而失效的问题，特别是在模拟到现实的差距中。

Method: 采用元模仿学习算法，通过训练1000个教师策略并使用强化学习，然后将这些策略蒸馏成一个单一的自适应学生策略。

Result: 训练出的轻量级策略（仅2084参数）能够零样本适应10种不同的真实四旋翼飞行器，涵盖多种硬件配置和环境条件。

Conclusion: RAPTOR训练出的基础策略能够通过上下文学习在毫秒级别内零样本适应多种未见过的四旋翼飞行器，展示了其高度适应性和泛化能力。

Abstract: Humans are remarkably data-efficient when adapting to new unseen conditions,
like driving a new car. In contrast, modern robotic control systems, like
neural network policies trained using Reinforcement Learning (RL), are highly
specialized for single environments. Because of this overfitting, they are
known to break down even under small differences like the Simulation-to-Reality
(Sim2Real) gap and require system identification and retraining for even
minimal changes to the system. In this work, we present RAPTOR, a method for
training a highly adaptive foundation policy for quadrotor control. Our method
enables training a single, end-to-end neural-network policy to control a wide
variety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg
that also differ in motor type (brushed vs. brushless), frame type (soft vs.
rigid), propeller type (2/3/4-blade), and flight controller
(PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy
with only 2084 parameters is sufficient for zero-shot adaptation to a wide
variety of platforms. The adaptation through In-Context Learning is made
possible by using a recurrence in the hidden layer. The policy is trained
through a novel Meta-Imitation Learning algorithm, where we sample 1000
quadrotors and train a teacher policy for each of them using Reinforcement
Learning. Subsequently, the 1000 teachers are distilled into a single, adaptive
student policy. We find that within milliseconds, the resulting foundation
policy adapts zero-shot to unseen quadrotors. We extensively test the
capabilities of the foundation policy under numerous conditions (trajectory
tracking, indoor/outdoor, wind disturbance, poking, different propellers).

</details>


### [276] [FR-Net: Learning Robust Quadrupedal Fall Recovery on Challenging Terrains through Mass-Contact Prediction](https://arxiv.org/abs/2509.11504)
*Yidan Lu,Yinzhao Dong,Jiahui Zhang,Ji Ma,Peng Lu*

Main category: cs.RO

TL;DR: FR-Net是一个学习框架，通过质量-接触预测实现四足机器人在复杂地形上的稳健跌倒恢复，无需显式地形数据。


<details>
  <summary>Details</summary>
Motivation: 解决四足机器人在复杂地形上跌倒恢复的挑战，传统控制器因不完全地形感知和不确定交互而失效。

Method: 我们提出了FR-Net，一个基于学习的框架，包括一个质量-接触预测器网络，从有限的感官输入中估计机器人的质量分布和接触状态。通过精心设计的奖励函数和模拟训练，框架无需部署时的显式地形数据即可指导策略学习。

Result: 在Go2机器人上的10个挑战性场景中，FR-Net展示了强大的泛化能力，实现了安全恢复，避免了现有方法常见的危险滚动动作。

Conclusion: 显式的质量-接触预测是稳健跌倒恢复的关键，为通用四足机器人技能提供了有前景的方向。

Abstract: Fall recovery for legged robots remains challenging, particularly on complex
terrains where traditional controllers fail due to incomplete terrain
perception and uncertain interactions. We present \textbf{FR-Net}, a
learning-based framework that enables quadrupedal robots to recover from
arbitrary fall poses across diverse environments. Central to our approach is a
Mass-Contact Predictor network that estimates the robot's mass distribution and
contact states from limited sensory inputs, facilitating effective recovery
strategies. Our carefully designed reward functions ensure safe recovery even
on steep stairs without dangerous rolling motions common to existing methods.
Trained entirely in simulation using privileged learning, our framework guides
policy learning without requiring explicit terrain data during deployment. We
demonstrate the generalization capabilities of \textbf{FR-Net} across different
quadrupedal platforms in simulation and validate its performance through
extensive real-world experiments on the Go2 robot in 10 challenging scenarios.
Our results indicate that explicit mass-contact prediction is key to robust
fall recovery, offering a promising direction for generalizable quadrupedal
skills.

</details>


### [277] [Design and Development of a Remotely Wire-Driven Walking Robot](https://arxiv.org/abs/2509.11506)
*Takahiro Hattori,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: 提出Remote Wire Drive机制，通过线缆远程驱动移动机器人，实验验证可行。


<details>
  <summary>Details</summary>
Motivation: 在恶劣或人类无法进入的环境中操作是机器人的关键角色之一，但电子元件也面临风险。现有的无电子自主移动机器人无法做出复杂决策，而液压或线驱动机器人手臂在核电站等环境中使用。本研究旨在结合移动机器人的优势与线驱动系统的环境适用性。

Method: 该机制采用解耦关节的串联连接，将线驱动机器人手臂的机制应用于动力传输。

Result: 实验验证了Remote Wire Drive机制的可行性，成功驱动了本研究开发的线驱动四足机器人。

Conclusion: 本研究提出了一种名为Remote Wire Drive的新颖机制，通过线缆实现移动机器人的远程驱动，并通过实验验证了其可行性。

Abstract: Operating in environments too harsh or inaccessible for humans is one of the
critical roles expected of robots. However, such environments often pose risks
to electronic components as well. To overcome this, various approaches have
been developed, including autonomous mobile robots without electronics,
hydraulic remotely actuated mobile robots, and long-reach robot arms driven by
wires. Among these, electronics-free autonomous robots cannot make complex
decisions, while hydraulically actuated mobile robots and wire-driven robot
arms are used in harsh environments such as nuclear power plants. Mobile robots
offer greater reach and obstacle avoidance than robot arms, and wire mechanisms
offer broader environmental applicability than hydraulics. However, wire-driven
systems have not been used for remote actuation of mobile robots. In this
study, we propose a novel mechanism called Remote Wire Drive that enables
remote actuation of mobile robots via wires. This mechanism is a series
connection of decoupled joints, a mechanism used in wire-driven robot arms,
adapted for power transmission. We experimentally validated its feasibility by
actuating a wire-driven quadruped robot, which we also developed in this study,
through Remote Wire Drive.

</details>


### [278] [PaiP: An Operational Aware Interactive Planner for Unknown Cabinet Environments](https://arxiv.org/abs/2509.11516)
*Chengjin Wang,Zheng Yan,Yanmin Zhou,Runjie Shen,Zhipeng Wang,Bin Cheng,Bin He*

Main category: cs.RO

TL;DR: PaiP是一种结合触觉感知的实时闭环运动规划框架，有效解决了狭窄空间中的运动规划问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统无碰撞轨迹规划方法在视觉遮挡和受限自由空间下可能失败或导致灾难性碰撞的问题。

Method: 提出了一种操作感知的交互式运动规划器（PaiP），利用多模态触觉感知在交互界面感知运动效果，生成操作成本地图，并扩展基于采样的规划方法以优化路径成本和操作成本。

Result: 实验结果表明，PaiP在狭窄空间中实现了鲁棒的运动。

Conclusion: PaiP框架通过结合多模态触觉感知和实时闭环规划，有效解决了在狭窄空间中因视觉遮挡和受限自由空间导致的机器人运动挑战，实现了鲁棒的运动规划。

Abstract: Box/cabinet scenarios with stacked objects pose significant challenges for
robotic motion due to visual occlusions and constrained free space. Traditional
collision-free trajectory planning methods often fail when no collision-free
paths exist, and may even lead to catastrophic collisions caused by invisible
objects. To overcome these challenges, we propose an operational aware
interactive motion planner (PaiP) a real-time closed-loop planning framework
utilizing multimodal tactile perception. This framework autonomously infers
object interaction features by perceiving motion effects at interaction
interfaces. These interaction features are incorporated into grid maps to
generate operational cost maps. Building upon this representation, we extend
sampling-based planning methods to interactive planning by optimizing both path
cost and operational cost. Experimental results demonstrate that PaiP achieves
robust motion in narrow spaces.

</details>


### [279] [Shape control of simulated multi-segment continuum robots via Koopman operators with per-segment projection](https://arxiv.org/abs/2509.11567)
*Eron Ristich,Jiahe Wang,Lei Zhang,Sultan Haidar Ali,Wanxin Jin,Yi Ren,Jiefeng Sun*

Main category: cs.RO

TL;DR: 本文提出一种基于Koopman算子的数据驱动方法，实现了软体连续机器人的实时形状控制，解决了高计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 当前连续机器人只能实现任务空间控制（如尖端控制），无法实现全形状控制，主要由于无限自由度带来的高计算成本。

Method: 采用数据驱动的Koopman算子方法，结合分段投影方案和线性模型预测控制（MPC），实现对多段肌腱驱动软体连续机器人的形状控制。

Result: 通过分段投影方案，识别出的控制仿射Koopman模型比未使用投影方案的模型准确度高一个数量级，实现了计算高效的闭环控制。

Conclusion: 本文展示了基于数据驱动的Koopman算子方法在软体连续机器人形状控制中的可行性，为实际应用奠定了基础。

Abstract: Soft continuum robots can allow for biocompatible yet compliant motions, such
as the ability of octopus arms to swim, crawl, and manipulate objects. However,
current state-of-the-art continuum robots can only achieve real-time task-space
control (i.e., tip control) but not whole-shape control, mainly due to the high
computational cost from its infinite degrees of freedom. In this paper, we
present a data-driven Koopman operator-based approach for the shape control of
simulated multi-segment tendon-driven soft continuum robots with the Kirchhoff
rod model. Using data collected from these simulated soft robots, we conduct a
per-segment projection scheme on the state of the robots allowing for the
identification of control-affine Koopman models that are an order of magnitude
more accurate than without the projection scheme. Using these learned Koopman
models, we use a linear model predictive control (MPC) to control the robots to
a collection of target shapes of varying complexity. Our method realizes
computationally efficient closed-loop control, and demonstrates the feasibility
of real-time shape control for soft robots. We envision this work can pave the
way for practical shape control of soft continuum robots.

</details>


### [280] [GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning](https://arxiv.org/abs/2509.11594)
*Jizhuo Chen,Diwen Liu,Jiaming Wang,Harold Soh*

Main category: cs.RO

TL;DR: GBPP是一种基于快速学习的评分器，通过两阶段课程学习（自动标注+模拟细化）从RGB-D快照中选择机器人基座姿势，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决从单次RGB-D快照中选择机器人基座姿势的问题，以实现快速在线选择，而无需完整的任务和运动优化。

Method: GBPP采用两阶段课程学习：(1) 使用简单的距离-可见性规则自动标注大规模数据集；(2) 通过少量高保真模拟试验细化模型以匹配真实抓取结果。使用PointNet++风格的点云编码器和MLP对候选姿势密集网格进行评分。

Result: 在模拟和真实移动机械臂上，GBPP优于仅基于接近度和几何的基线方法，选择了更安全、更可达的姿势，并在错误时表现优雅。

Conclusion: GBPP提供了一种数据高效、几何感知的机器人基座放置方法，即使用低成本启发式方法进行覆盖，然后通过有针对性的模拟进行校准。

Abstract: GBPP is a fast learning based scorer that selects a robot base pose for
grasping from a single RGB-D snapshot. The method uses a two stage curriculum:
(1) a simple distance-visibility rule auto-labels a large dataset at low cost;
and (2) a smaller set of high fidelity simulation trials refines the model to
match true grasp outcomes. A PointNet++ style point cloud encoder with an MLP
scores dense grids of candidate poses, enabling rapid online selection without
full task-and-motion optimization. In simulation and on a real mobile
manipulator, GBPP outperforms proximity and geometry only baselines, choosing
safer and more reachable stances and degrading gracefully when wrong. The
results offer a practical recipe for data efficient, geometry aware base
placement: use inexpensive heuristics for coverage, then calibrate with
targeted simulation.

</details>


### [281] [AssemMate: Graph-Based LLM for Robotic Assembly Assistance](https://arxiv.org/abs/2509.11617)
*Qi Zheng,Chaoran Zhang,Zijian Liang,EnTe Lin,Shubo Cui,Qinghongbing Xie,Zhaobo Xu,Long Zeng*

Main category: cs.RO

TL;DR: AssemMate通过图表示和自监督GCN提升LLM在机器人装配辅助中的实时性和精确性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言文本的知识表示方法因上下文冗长和冗余内容，难以满足机器人实时和精确推理的需求。

Method: 利用图结构作为知识表示输入，结合自监督GCN编码知识图谱实体和关系，并与LLM表示对齐，同时采用视觉增强策略处理堆叠场景。

Result: AssemMate在准确率上提升6.4%，推理速度提高3倍，上下文长度缩短28倍，并在随机图谱上展现出强泛化能力。

Conclusion: AssemMate通过图表示和自监督GCN的结合，显著提升了机器人装配辅助的实时性和精确性，并在模拟和现实环境中展示了优越的抓取性能。

Abstract: Large Language Model (LLM)-based robotic assembly assistance has gained
significant research attention. It requires the injection of domain-specific
knowledge to guide the assembly process through natural language interaction
with humans. Despite some progress, existing methods represent knowledge in the
form of natural language text. Due to the long context and redundant content,
they struggle to meet the robots' requirements for real-time and precise
reasoning. In order to bridge this gap, we present AssemMate, which utilizes
the graph\textemdash a concise and accurate form of knowledge
representation\textemdash as input. This graph-based LLM enables knowledge
graph question answering (KGQA), supporting human-robot interaction and
assembly task planning for specific products. Beyond interactive QA, AssemMate
also supports sensing stacked scenes and executing grasping to assist with
assembly. Specifically, a self-supervised Graph Convolutional Network (GCN)
encodes knowledge graph entities and relations into a latent space and aligns
them with LLM's representation, enabling the LLM to understand graph
information. In addition, a vision-enhanced strategy is employed to address
stacked scenes in grasping. Through training and evaluation, AssemMate
outperforms existing methods, achieving 6.4\% higher accuracy, 3 times faster
inference, and 28 times shorter context length, while demonstrating strong
generalization ability on random graphs. And our approach further demonstrates
superiority through robotic grasping experiments in both simulated and
real-world settings. More details can be found on the project page:
https://github.com/cristina304/AssemMate.git

</details>


### [282] [Inference-stage Adaptation-projection Strategy Adapts Diffusion Policy to Cross-manipulators Scenarios](https://arxiv.org/abs/2509.11621)
*Xiangtong Yao,Yirui Zhou,Yuan Meng,Yanwen Liu,Liangyu Dong,Zitao Zhang,Zhenshan Bing,Kai Huang,Fuchun Sun,Alois Knoll*

Main category: cs.RO

TL;DR: 提出零样本适应扩散策略的适应性投影方法，无需重新训练即可适应新机械臂和任务，实验证明高效。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在适应新机械臂或任务需求时表现不佳，通常需要昂贵的数据重新收集和策略重新训练。

Method: 采用SE(3)空间训练扩散策略，并通过在线部署时的适应性投影满足新硬件和任务约束。

Result: 在多个机械臂和末端执行器上验证了高成功率，包括Franka Panda和Kuka iiwa 14。

Conclusion: 论文提出了一种无需重新训练的策略，通过适应性投影实现扩散策略的零样本适应，适用于不同机械臂和动态任务设置，实验验证了其有效性和实用性。

Abstract: Diffusion policies are powerful visuomotor models for robotic manipulation,
yet they often fail to generalize to manipulators or end-effectors unseen
during training and struggle to accommodate new task requirements at inference
time. Addressing this typically requires costly data recollection and policy
retraining for each new hardware or task configuration. To overcome this, we
introduce an adaptation-projection strategy that enables a diffusion policy to
perform zero-shot adaptation to novel manipulators and dynamic task settings,
entirely at inference time and without any retraining. Our method first trains
a diffusion policy in SE(3) space using demonstrations from a base manipulator.
During online deployment, it projects the policy's generated trajectories to
satisfy the kinematic and task-specific constraints imposed by the new hardware
and objectives. Moreover, this projection dynamically adapts to physical
differences (e.g., tool-center-point offsets, jaw widths) and task requirements
(e.g., obstacle heights), ensuring robust and successful execution. We validate
our approach on real-world pick-and-place, pushing, and pouring tasks across
multiple manipulators, including the Franka Panda and Kuka iiwa 14, equipped
with a diverse array of end-effectors like flexible grippers, Robotiq 2F/3F
grippers, and various 3D-printed designs. Our results demonstrate consistently
high success rates in these cross-manipulator scenarios, proving the
effectiveness and practicality of our adaptation-projection strategy. The code
will be released after peer review.

</details>


### [283] [ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering](https://arxiv.org/abs/2509.11663)
*Haisheng Wang,Weiming Zhi*

Main category: cs.RO

TL;DR: 本文提出了ParaEQsA框架，用于并行、紧急感知的具身问题回答，显著提升了多问题场景下的效率和响应速度。


<details>
  <summary>Details</summary>
Motivation: 现实部署中，具身代理需要处理异步到达且具有不同紧急性的多个问题，而传统的EQA仅处理单一问题。

Method: 提出了ParaEQsA框架，包括共享组内存模块以减少冗余探索，以及动态调度问题的优先级规划模块。

Result: ParaEQsA在PAEQs基准测试中表现优于强序列基线，减少了探索和延迟。

Conclusion: ParaEQsA框架通过并行、紧急感知的调度和回答机制，显著提升了在现实多问题工作负载下具身代理的响应速度和效率。

Abstract: This paper formulates the Embodied Questions Answering (EQsA) problem,
introduces a corresponding benchmark, and proposes a system to tackle the
problem. Classical Embodied Question Answering (EQA) is typically formulated as
answering one single question by actively exploring a 3D environment. Real
deployments, however, often demand handling multiple questions that may arrive
asynchronously and carry different urgencies. We formalize this setting as
Embodied Questions Answering (EQsA) and present ParaEQsA, a framework for
parallel, urgency-aware scheduling and answering. ParaEQsA leverages a group
memory module shared among questions to reduce redundant exploration, and a
priority-planning module to dynamically schedule questions. To evaluate this
setting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs)
benchmark containing 40 indoor scenes and five questions per scene (200 in
total), featuring asynchronous follow-up questions and urgency labels. We
further propose metrics for EQsA performance: Direct Answer Rate (DAR), and
Normalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency
and responsiveness of this system. ParaEQsA consistently outperforms strong
sequential baselines adapted from recent EQA systems, while reducing
exploration and delay. Empirical evaluations investigate the relative
contributions of priority, urgency modeling, spatial scope, reward estimation,
and dependency reasoning within our framework. Together, these results
demonstrate that urgency-aware, parallel scheduling is key to making embodied
agents responsive and efficient under realistic, multi-question workloads.

</details>


### [284] [Tensor Invariant Data-Assisted Control and Dynamic Decomposition of Multibody Systems](https://arxiv.org/abs/2509.11688)
*Mostafa Eslami,Maryam Babazadeh*

Main category: cs.RO

TL;DR: 本文提出了一种坐标无关的机器人控制框架，结合张量力学和数据辅助控制，解决了数据效率低下问题，提升了机器人控制的稳健性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有坐标依赖模型在机器人控制中导致数据效率低下，无法在不同参考框架中泛化物理交互，增加了学习任务的复杂性。

Method: 引入了一种基于张量力学的坐标无关多体动力学和运动学模型，结合数据辅助控制架构，提出了一个完整的端到端张量不变建模、控制和学习流程。

Result: 通过模拟验证了模型和闭环系统的有效性，坐标无关控制律为数据高效、框架不变的学习算法提供了理想输入。

Conclusion: 本文提出的坐标无关框架通过结合张量力学和数据辅助控制架构，有效解决了机器人系统在复杂协作空间中的数据效率低下问题，为稳健和通用的机器人控制提供了新途径。

Abstract: The control of robotic systems in complex, shared collaborative workspaces
presents significant challenges in achieving robust performance and safety when
learning from experienced or simulated data is employed in the pipeline. A
primary bottleneck is the reliance on coordinate-dependent models, which leads
to profound data inefficiency by failing to generalize physical interactions
across different frames of reference. This forces learning algorithms to
rediscover fundamental physical principles in every new orientation,
artificially inflating the complexity of the learning task. This paper
introduces a novel framework that synergizes a coordinate-free, unreduced
multibody dynamics and kinematics model based on tensor mechanics with a
Data-Assisted Control (DAC) architecture. A non-recursive, closed-form
Newton-Euler model in an augmented matrix form is derived that is optimized for
tensor-based control design. This structure enables a principled decomposition
of the system into a structurally certain, physically grounded part and an
uncertain, empirical, and interaction-focused part, mediated by a virtual port
variable. Then, a complete, end-to-end tensor-invariant pipeline for modeling,
control, and learning is proposed. The coordinate-free control laws for the
structurally certain part provide a stable and abstract command interface,
proven via Lyapunov analysis. Eventually, the model and closed-loop system are
validated through simulations. This work provides a naturally ideal input for
data-efficient, frame-invariant learning algorithms, such as equivariant
learning, designed to learn the uncertain interaction. The synergy directly
addresses the data-inefficiency problem, increases explainability and
interpretability, and paves the way for more robust and generalizable robotic
control in interactive environments.

</details>


### [285] [From Pixels to Shelf: End-to-End Algorithmic Control of a Mobile Manipulator for Supermarket Stocking and Fronting](https://arxiv.org/abs/2509.11740)
*Davide Peron,Victor Nan Fernandez-Ayala,Lukas Segelmark*

Main category: cs.RO

TL;DR: 本文提出了一种结合商用硬件和ROS2的自主货架补货系统，通过行为树和模型预测控制实现高效操作，实验室测试成功率达98%，但性能和成本仍需改进以匹敌人类工人。


<details>
  <summary>Details</summary>
Motivation: 零售环境（尤其是超市）中的自主补货面临动态人机交互、空间受限和产品几何多样性的挑战，需要一个高效且可扩展的解决方案。

Method: 结合商用硬件和ROS2的感知、规划与控制，采用行为树（BTs）进行任务规划，精细调校的视觉模型用于物体检测，以及基于ArUco标记的两步模型预测控制（MPC）框架进行精确货架导航。

Result: 在模拟真实超市条件的实验室实验中，系统在700多次补货操作中实现了超过98%的成功率。

Conclusion: 虽然当前自主系统的性能和成本效益仍不及人类工人，但通过实验室测试证明了其可靠性，为未来零售环境的广泛应用提供了改进方向和量化目标。

Abstract: Autonomous stocking in retail environments, particularly supermarkets,
presents challenges due to dynamic human interactions, constrained spaces, and
diverse product geometries. This paper introduces an efficient end-to-end
robotic system for autonomous shelf stocking and fronting, integrating
commercially available hardware with a scalable algorithmic architecture. A
major contribution of this work is the system integration of off-the-shelf
hardware and ROS2-based perception, planning, and control into a single
deployable platform for retail environments. Our solution leverages Behavior
Trees (BTs) for task planning, fine-tuned vision models for object detection,
and a two-step Model Predictive Control (MPC) framework for precise shelf
navigation using ArUco markers. Laboratory experiments replicating realistic
supermarket conditions demonstrate reliable performance, achieving over 98%
success in pick-and-place operations across a total of more than 700 stocking
events. However, our comparative benchmarks indicate that the performance and
cost-effectiveness of current autonomous systems remain inferior to that of
human workers, which we use to highlight key improvement areas and quantify the
progress still required before widespread commercial deployment can
realistically be achieved.

</details>


### [286] [Adaptive Motorized LiDAR Scanning Control for Robust Localization with OpenStreetMap](https://arxiv.org/abs/2509.11742)
*Jianping Li,Kaisong Zhu,Zhongyuan Liu,Rui Jin,Xinhang Xu,Pengfei Wan,Lihua Xie*

Main category: cs.RO

TL;DR: 提出了一种结合OSM全局先验与局部可观测性预测的自适应LiDAR扫描框架，显著提高了定位鲁棒性和效率，实验验证了其在复杂环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: OSM作为轻量级全局先验可增强机器人导航的全局一致性，但其不完整或过时的特性限制了可靠性；同时，传统恒速扫描LiDAR系统在特征稀疏区域浪费资源且定位精度下降。

Method: 提出了一个自适应LiDAR扫描框架，结合全局先验（OSM）与局部可观测性预测，通过不确定性感知的模型预测控制（MPC）和OSM感知项，动态分配扫描资源。

Result: 在校园道路、室内走廊和城市环境中的实验表明，该方法相比恒速基线显著降低了轨迹误差，同时保持了扫描完整性。

Conclusion: 该研究通过结合OSM全局先验与局部可观测性预测，提出了自适应LiDAR扫描框架，显著提高了定位的鲁棒性和效率。

Abstract: LiDAR-to-OpenStreetMap (OSM) localization has gained increasing attention, as
OSM provides lightweight global priors such as building footprints. These
priors enhance global consistency for robot navigation, but OSM is often
incomplete or outdated, limiting its reliability in real-world deployment.
Meanwhile, LiDAR itself suffers from a limited field of view (FoV), where
motorized rotation is commonly used to achieve panoramic coverage. Existing
motorized LiDAR systems, however, typically employ constant-speed scanning that
disregards both scene structure and map priors, leading to wasted effort in
feature-sparse regions and degraded localization accuracy. To address these
challenges, we propose Adaptive LiDAR Scanning with OSM guidance, a framework
that integrates global priors with local observability prediction to improve
localization robustness. Specifically, we augment uncertainty-aware model
predictive control with an OSM-aware term that adaptively allocates scanning
effort according to both scene-dependent observability and the spatial
distribution of OSM features. The method is implemented in ROS with a motorized
LiDAR odometry backend and evaluated in both simulation and real-world
experiments. Results on campus roads, indoor corridors, and urban environments
demonstrate significant reductions in trajectory error compared to
constant-speed baselines, while maintaining scan completeness. These findings
highlight the potential of coupling open-source maps with adaptive LiDAR
scanning to achieve robust and efficient localization in complex environments.

</details>


### [287] [Igniting VLMs toward the Embodied Space](https://arxiv.org/abs/2509.11766)
*Andy Zhai,Brae Liu,Bruno Fang,Chalse Cai,Ellie Ma,Ethan Yin,Hao Wang,Hugo Zhou,James Wang,Lights Shi,Lucy Liang,Make Wang,Qian Wang,Roy Gan,Ryan Yu,Shalfun Li,Starrick Liu,Sylas Chen,Vincent Chen,Zach Xu*

Main category: cs.RO

TL;DR: WALL-OSS 是一种端到端具身基础模型，通过大规模多模态预训练实现具身感知的视觉语言理解、语言-动作关联和鲁棒操作能力，解决了 VLMs 在具身领域的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）在空间和具身理解方面存在局限，导致在具身领域中模态、预训练分布和训练目标之间的不匹配，成为实现通用人工智能（AGI）的核心瓶颈。

Method: 采用紧密耦合的架构和多策略训练课程，实现统一的跨级 CoT（思维链），在一个可微分框架内无缝整合指令推理、子目标分解和细粒度动作合成。

Result: WALL-OSS 在复杂长时程操作中取得高成功率，表现出强大的指令遵循能力、复杂理解和推理能力，并优于强基线模型。

Conclusion: WALL-OSS 提供了一条可靠且可扩展的路径，从视觉语言模型（VLMs）到具身基础模型，展示了在复杂操作和指令遵循方面的强大能力。

Abstract: While foundation models show remarkable progress in language and vision,
existing vision-language models (VLMs) still have limited spatial and
embodiment understanding. Transferring VLMs to embodied domains reveals
fundamental mismatches between modalities, pretraining distributions, and
training objectives, leaving action comprehension and generation as a central
bottleneck on the path to AGI.
  We introduce WALL-OSS, an end-to-end embodied foundation model that leverages
large-scale multimodal pretraining to achieve (1) embodiment-aware
vision-language understanding, (2) strong language-action association, and (3)
robust manipulation capability.
  Our approach employs a tightly coupled architecture and multi-strategies
training curriculum that enables Unified Cross-Level CoT-seamlessly unifying
instruction reasoning, subgoal decomposition, and fine-grained action synthesis
within a single differentiable framework.
  Our results show that WALL-OSS attains high success on complex long-horizon
manipulations, demonstrates strong instruction-following capabilities, complex
understanding and reasoning, and outperforms strong baselines, thereby
providing a reliable and scalable path from VLMs to embodied foundation models.

</details>


### [288] [Augmented Reality-Enhanced Robot Teleoperation for Collecting User Demonstrations](https://arxiv.org/abs/2509.11783)
*Shiqi Gong,Sebastian Zudaire,Chi Zhang,Zhen Li*

Main category: cs.RO

TL;DR: 提出AR增强的机器人遥操作系统，结合点云渲染提升直观性和安全性，验证显示任务性能提升28%，用户体验提升12%。


<details>
  <summary>Details</summary>
Motivation: 传统工业机器人编程复杂耗时，而现有的编程示范方法在直观界面和演示收集中仍存在挑战。

Method: 通过结合AR控制和空间点云渲染，提出了一种直观、非接触式的演示收集方法，并在ABB机器人平台上进行了验证。

Result: 增强感知显著提高了任务性能28%，用户体验提升了12%（SUS评分）。

Conclusion: 本文提出了一种增强现实（AR）辅助的机器人遥操作系统，显著提升了任务完成准确性和用户体验，为工业环境中的直观机器人遥操作、AR界面设计及环境感知提供了重要贡献。

Abstract: Traditional industrial robot programming is often complex and time-consuming,
typically requiring weeks or even months of effort from expert programmers.
Although Programming by Demonstration (PbD) offers a more accessible
alternative, intuitive interfaces for robot control and demonstration
collection remain challenging. To address this, we propose an Augmented Reality
(AR)-enhanced robot teleoperation system that integrates AR-based control with
spatial point cloud rendering, enabling intuitive, contact-free demonstrations.
This approach allows operators to control robots remotely without entering the
workspace or using conventional tools like the teach pendant. The proposed
system is generally applicable and has been demonstrated on ABB robot
platforms, specifically validated with the IRB 1200 industrial robot and the
GoFa 5 collaborative robot. A user study evaluates the impact of real-time
environmental perception, specifically with and without point cloud rendering,
on task completion accuracy, efficiency, and user confidence. Results indicate
that enhanced perception significantly improves task performance by 28% and
enhances user experience, as reflected by a 12% increase in the System
Usability Scale (SUS) score. This work contributes to the advancement of
intuitive robot teleoperation, AR interface design, environmental perception,
and teleoperation safety mechanisms in industrial settings for demonstration
collection. The collected demonstrations may serve as valuable training data
for machine learning applications.

</details>


### [289] [Synthetic vs. Real Training Data for Visual Navigation](https://arxiv.org/abs/2509.11791)
*Lauri Suomela,Sasanka Kuruppu Arachchige,German F. Torres,Harry Edelman,Joni-Kristian Kämäräinen*

Main category: cs.RO

TL;DR: 研究发现模拟训练的导航策略在预训练视觉表示支持下，能超越真实世界训练的策略，并在导航成功率上显著提升。


<details>
  <summary>Details</summary>
Motivation: 探讨模拟训练的视觉导航策略与真实世界数据训练的导航策略在性能上的比较，以解决模拟到真实的性能退化问题。

Method: 采用了一种导航策略架构，该架构通过预训练的视觉表示来弥合模拟与真实世界的外观差距，并在机器人硬件上实时运行。

Result: 在轮式移动机器人上的评估显示，模拟训练的策略在导航成功率上比真实世界训练的版本高出31%，比现有最优方法高出50%。

Conclusion: 研究表明，通过利用预训练的视觉表示和实时运行的导航策略架构，模拟训练的导航策略可以超越真实世界训练的版本，并在导航成功率上显著优于现有方法。

Abstract: This paper investigates how the performance of visual navigation policies
trained in simulation compares to policies trained with real-world data.
Performance degradation of simulator-trained policies is often significant when
they are evaluated in the real world. However, despite this well-known
sim-to-real gap, we demonstrate that simulator-trained policies can match the
performance of their real-world-trained counterparts.
  Central to our approach is a navigation policy architecture that bridges the
sim-to-real appearance gap by leveraging pretrained visual representations and
runs real-time on robot hardware. Evaluations on a wheeled mobile robot show
that the proposed policy, when trained in simulation, outperforms its
real-world-trained version by 31% and the prior state-of-the-art methods by 50%
in navigation success rate. Policy generalization is verified by deploying the
same model onboard a drone.
  Our results highlight the importance of diverse image encoder pretraining for
sim-to-real generalization, and identify on-policy learning as a key advantage
of simulated training over training with real data.

</details>


### [290] [UniPilot: Enabling GPS-Denied Autonomy Across Embodiments](https://arxiv.org/abs/2509.11793)
*Mihir Kulkarni,Mihir Dharmadhikari,Nikhil Khedekar,Morten Nissov,Mohit Singh,Philipp Weiss,Kostas Alexis*

Main category: cs.RO

TL;DR: UniPilot 是一种多模态自主负载，适用于 GPS 拒止环境，已在多种平台上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对单一模态方法在复杂环境中可能失效的问题，开发一种能够在 GPS 拒止环境中实现自主操作的通用解决方案。

Method: 该系统集成了 LiDAR、雷达、视觉和惯性传感的多模态感知套件，并运行包括多模态感知、探索和检查路径规划以及基于学习的导航策略在内的完整自主软件。

Result: 实验表明，UniPilot 在多种环境和机器人平台上提供了稳健的定位、建图、规划和安全控制能力。

Conclusion: UniPilot 是一种紧凑型硬件-软件自主负载，能够在 GPS 拒止环境中实现自主操作，并已在多种机器人平台上验证其有效性。

Abstract: This paper presents UniPilot, a compact hardware-software autonomy payload
that can be integrated across diverse robot embodiments to enable autonomous
operation in GPS-denied environments. The system integrates a multi-modal
sensing suite including LiDAR, radar, vision, and inertial sensing for robust
operation in conditions where uni-modal approaches may fail. UniPilot runs a
complete autonomy software comprising multi-modal perception, exploration and
inspection path planning, and learning-based navigation policies. The payload
provides robust localization, mapping, planning, and safety and control
capabilities in a single unit that can be deployed across a wide range of
platforms. A large number of experiments are conducted across diverse
environments and on a variety of robot platforms to validate the mapping,
planning, and safe navigation capabilities enabled by the payload.

</details>


### [291] [TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning](https://arxiv.org/abs/2509.11839)
*Jiacheng Liu,Pengxiang Ding,Qihang Zhou,Yuxuan Wu,Da Huang,Zimian Peng,Wei Xiao,Weinan Zhang,Lixin Yang,Cewu Lu,Donglin Wang*

Main category: cs.RO

TL;DR: KORR框架通过Koopman算子理论指导残差策略更新，显著提升模仿学习在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在长时程任务和高精度控制中因累积误差而表现不佳，现有残差策略学习主要关注局部修正，缺乏全局状态演化的理解，限制了鲁棒性和泛化能力。

Method: 研究提出KORR框架，利用Koopman算子理论在学习的潜在空间中施加线性时不变结构，指导残差策略的更新，实现全局信息驱动的动作优化。

Result: KORR在受扰动的长时程精细机器人家具组装任务中表现出优于基线的性能、鲁棒性和泛化能力。

Conclusion: 该研究通过引入Koopman算子理论，提出了一种全局动态建模方法（KORR），显著提升了模仿学习在长时程任务和高精度控制中的性能、鲁棒性和泛化能力。

Abstract: Imitation learning (IL) enables efficient skill acquisition from
demonstrations but often struggles with long-horizon tasks and high-precision
control due to compounding errors. Residual policy learning offers a promising,
model-agnostic solution by refining a base policy through closed-loop
corrections. However, existing approaches primarily focus on local corrections
to the base policy, lacking a global understanding of state evolution, which
limits robustness and generalization to unseen scenarios. To address this, we
propose incorporating global dynamics modeling to guide residual policy
updates. Specifically, we leverage Koopman operator theory to impose linear
time-invariant structure in a learned latent space, enabling reliable state
transitions and improved extrapolation for long-horizon prediction and unseen
environments. We introduce KORR (Koopman-guided Online Residual Refinement), a
simple yet effective framework that conditions residual corrections on
Koopman-predicted latent states, enabling globally informed and stable action
refinement. We evaluate KORR on long-horizon, fine-grained robotic furniture
assembly tasks under various perturbations. Results demonstrate consistent
gains in performance, robustness, and generalization over strong baselines. Our
findings further highlight the potential of Koopman-based modeling to bridge
modern learning methods with classical control theory. For more details, please
refer to https://jiachengliu3.github.io/TrajBooster.

</details>


### [292] [Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion Transformer](https://arxiv.org/abs/2509.11865)
*Travis Davies,Yiqi Huang,Yunxin Liu,Xiang Chen,Huxian Liu,Luhui Hu*

Main category: cs.RO

TL;DR: Tenma是一种轻量级扩散-Transformer策略，通过多模态和跨体现学习在机器人操作中表现出色。


<details>
  <summary>Details</summary>
Motivation: 研究在轻量级、跨体现学习设置中结合Transformer策略和扩散模型的设计选择，以解决其在异构、多模态机器人数据中的稳定性和性能问题。

Method: Tenma通过跨体现标准化器将不同的状态/动作空间映射到共享潜在空间，采用联合状态-时间编码器进行时间对齐的观察学习，并优化了扩散动作解码器以提高训练稳定性和学习能力。

Result: Tenma在基准测试中达到了88.95%的平均成功率，并在对象和场景变化下保持强劲性能，显著超过基线策略的18.12%。

Conclusion: Tenma展示了多模态和跨体现学习策略在增强基于Transformer的模仿学习策略能力方面的巨大潜力。

Abstract: Scaling Transformer policies and diffusion models has advanced robotic
manipulation, yet combining these techniques in lightweight, cross-embodiment
learning settings remains challenging. We study design choices that most affect
stability and performance for diffusion-transformer policies trained on
heterogeneous, multimodal robot data, and introduce Tenma, a lightweight
diffusion-transformer for bi-manual arm control. Tenma integrates multiview
RGB, proprioception, and language via a cross-embodiment normalizer that maps
disparate state/action spaces into a shared latent space; a Joint State-Time
encoder for temporally aligned observation learning with inference speed
boosts; and a diffusion action decoder optimized for training stability and
learning capacity. Across benchmarks and under matched compute, Tenma achieves
an average success rate of 88.95% in-distribution and maintains strong
performance under object and scene shifts, substantially exceeding baseline
policies whose best in-distribution average is 18.12%. Despite using moderate
data scale, Tenma delivers robust manipulation and generalization, indicating
the great potential for multimodal and cross-embodiment learning strategies for
further augmenting the capacity of transformer-based imitation learning
policies.

</details>


### [293] [VH-Diffuser: Variable Horizon Diffusion Planner for Time-Aware Goal-Conditioned Trajectory Planning](https://arxiv.org/abs/2509.11930)
*Ruijia Liu,Ancheng Hou,Shaoyuan Li,Xiang Yin*

Main category: cs.RO

TL;DR: VHD框架通过预测实例特定视界并调整扩散规划器，解决了固定视界导致的轨迹长度不匹配问题，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散规划器依赖固定视界，导致轨迹长度不匹配（过短或过长）和在不同几何或动态难度实例中的脆弱性能。

Method: VHD框架包含一个学习长度预测器模型，用于预测实例特定的规划视界，并通过初始噪声整形和随机裁剪子轨迹训练扩散规划器生成所需长度的轨迹。

Result: VHD在迷宫导航和机械臂控制基准测试中提高了成功率和路径效率，展示了对视界不匹配和未见长度的更强鲁棒性，同时保持训练的简单性和离线特性。

Conclusion: VHD框架通过将规划视界作为学习变量而非固定超参数，显著提高了扩散规划器在不同任务实例中的鲁棒性和性能表现。

Abstract: Diffusion-based planners have gained significant recent attention for their
robustness and performance in long-horizon tasks. However, most existing
planners rely on a fixed, pre-specified horizon during both training and
inference. This rigidity often produces length-mismatch (trajectories that are
too short or too long) and brittle performance across instances with varying
geometric or dynamical difficulty. In this paper, we introduce the Variable
Horizon Diffuser (VHD) framework, which treats the horizon as a learned
variable rather than a fixed hyperparameter. Given a start-goal pair, we first
predict an instance-specific horizon using a learned Length Predictor model,
which guides a Diffusion Planner to generate a trajectory of the desired
length. Our design maintains compatibility with existing diffusion planners by
controlling trajectory length through initial noise shaping and training on
randomly cropped sub-trajectories, without requiring architectural changes.
Empirically, VHD improves success rates and path efficiency in maze-navigation
and robot-arm control benchmarks, showing greater robustness to horizon
mismatch and unseen lengths, while keeping training simple and offline-only.

</details>


### [294] [E2-BKI: Evidential Ellipsoidal Bayesian Kernel Inference for Uncertainty-aware Gaussian Semantic Mapping](https://arxiv.org/abs/2509.11964)
*Junyoung Kim,Minsik Jeon,Jihong Min,Kiho Kwak,Junwon Seo*

Main category: cs.RO

TL;DR: 提出了一种不确定性感知语义映射框架，通过深度学习和贝叶斯核推断提升复杂户外环境中的语义映射性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有语义映射方法在复杂户外环境中因多源不确定性导致的性能下降问题。

Method: 使用Evidential Deep Learning估计语义预测的不确定性，并将其整合到BKI中，同时采用几何对齐的核和噪声观测聚合为高斯表示。

Result: 在多样化的越野和城市户外环境中，该方法在映射质量、不确定性校准、表示灵活性和鲁棒性方面均表现出持续改进，同时保持实时效率。

Conclusion: 提出的不确定性感知语义映射框架通过结合深度学习和贝叶斯核推断，显著提升了复杂户外环境中的语义映射质量、不确定性校准和鲁棒性。

Abstract: Semantic mapping aims to construct a 3D semantic representation of the
environment, providing essential knowledge for robots operating in complex
outdoor settings. While Bayesian Kernel Inference (BKI) addresses
discontinuities of map inference from sparse sensor data, existing semantic
mapping methods suffer from various sources of uncertainties in challenging
outdoor environments. To address these issues, we propose an uncertainty-aware
semantic mapping framework that handles multiple sources of uncertainties,
which significantly degrade mapping performance. Our method estimates
uncertainties in semantic predictions using Evidential Deep Learning and
incorporates them into BKI for robust semantic inference. It further aggregates
noisy observations into coherent Gaussian representations to mitigate the
impact of unreliable points, while employing geometry-aligned kernels that
adapt to complex scene structures. These Gaussian primitives effectively fuse
local geometric and semantic information, enabling robust, uncertainty-aware
mapping in complex outdoor scenarios. Comprehensive evaluation across diverse
off-road and urban outdoor environments demonstrates consistent improvements in
mapping quality, uncertainty calibration, representational flexibility, and
robustness, while maintaining real-time efficiency.

</details>


### [295] [Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study](https://arxiv.org/abs/2509.11971)
*James C. Ward,Alex Bott,Connor York,Edmund R. Hunt*

Main category: cs.RO

TL;DR: 本文提出一种机器学习对手模型，用于评估多机器人巡逻系统的对抗表现，优于现有基线，为未来设计提供参考。


<details>
  <summary>Details</summary>
Motivation: 模拟物理自主系统的敌对攻击可作为评估其对攻击的鲁棒性及指导漏洞感知设计的有用工具。

Method: 通过机器学习技术构建对手模型，观察机器人巡逻行为，以在有限时间内尝试获得对安全环境的未检测访问。

Result: 新模型在性能上优于现有基线，为多机器人巡逻策略提供了更严格的测试。

Conclusion: 本文提出了一种基于机器学习的对手模型，用于评估多机器人巡逻系统在面对现实潜在对手时的表现，该模型在性能上优于现有基线，为未来的巡逻策略设计提供了更严格的测试。

Abstract: Simulating hostile attacks of physical autonomous systems can be a useful
tool to examine their robustness to attack and inform vulnerability-aware
design. In this work, we examine this through the lens of multi-robot patrol,
by presenting a machine learning-based adversary model that observes robot
patrol behavior in order to attempt to gain undetected access to a secure
environment within a limited time duration. Such a model allows for evaluation
of a patrol system against a realistic potential adversary, offering insight
into future patrol strategy design. We show that our new model outperforms
existing baselines, thus providing a more stringent test, and examine its
performance against multiple leading decentralized multi-robot patrol
strategies.

</details>


### [296] [Gesture-Based Robot Control Integrating Mm-wave Radar and Behavior Trees](https://arxiv.org/abs/2509.12008)
*Yuqing Song,Cesare Tonola,Stefano Savazzi,Sanaz Kianoush,Nicola Pedrocchi,Stephan Sigg*

Main category: cs.RO

TL;DR: 提出了一种基于毫米波雷达的手势控制机器人手臂系统，实现了非接触式的人机交互，并通过案例研究验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在家庭和工业环境中的普及，对直观高效的人机交互需求增加。传统摄像头系统存在隐私和复杂环境适应性差的问题，而雷达传感在隐私保护、遮挡和光照鲁棒性方面具有优势。

Method: 使用毫米波雷达进行手势识别，将九个手势映射为实时控制命令，并构建了一个统一的手势识别与机器人控制的实时流水线。

Result: 系统能够精确识别九个手势并实时控制机器人手臂，案例研究表明其实用性和可靠性。

Conclusion: 该论文提出了一种基于毫米波雷达的手势控制机器人手臂系统，实现了无缝、非接触式的人机交互，并通过案例研究验证了其实用性和可靠性。

Abstract: As robots become increasingly prevalent in both homes and industrial
settings, the demand for intuitive and efficient human-machine interaction
continues to rise. Gesture recognition offers an intuitive control method that
does not require physical contact with devices and can be implemented using
various sensing technologies. Wireless solutions are particularly flexible and
minimally invasive. While camera-based vision systems are commonly used, they
often raise privacy concerns and can struggle in complex or poorly lit
environments. In contrast, radar sensing preserves privacy, is robust to
occlusions and lighting, and provides rich spatial data such as distance,
relative velocity, and angle. We present a gesture-controlled robotic arm using
mm-wave radar for reliable, contactless motion recognition. Nine gestures are
recognized and mapped to real-time commands with precision. Case studies are
conducted to demonstrate the system practicality, performance and reliability
for gesture-based robotic manipulation. Unlike prior work that treats gesture
recognition and robotic control separately, our system unifies both into a
real-time pipeline for seamless, contactless human-robot interaction.

</details>


### [297] [Embodied Navigation Foundation Model](https://arxiv.org/abs/2509.12129)
*Jiazhao Zhang,Anqi Li,Yunpeng Qi,Minghan Li,Jiahang Liu,Shaoan Wang,Haoran Liu,Gengze Zhou,Yuze Wu,Xingxing Li,Yuxin Fan,Wenjun Li,Zhibo Chen,Fei Gao,Qi Wu,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: NavFoM 是一种跨任务和跨形态的导航基础模型，通过统一架构和动态采样策略，在多种导航任务中表现出色，无需任务特定微调。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型在零样本任务上表现优异，但在具体导航任务中的泛化能力受限，因此需要一种跨任务和跨形态的导航基础模型。

Method: NavFoM 采用统一架构处理多模态导航输入，通过标识符令牌嵌入相机视图信息和任务时间上下文，并使用动态调整的采样策略控制观察令牌。

Result: NavFoM 在公开基准测试中实现了最先进或高度竞争的性能，并在真实世界实验中验证了其泛化能力和实用性。

Conclusion: NavFoM 展示了在多种导航任务和机器人形态下的强大泛化能力和实际应用潜力，无需任务特定微调即可达到最先进或高度竞争的性能。

Abstract: Navigation is a fundamental capability in embodied AI, representing the
intelligence required to perceive and interact within physical environments
following language instructions. Despite significant progress in large
Vision-Language Models (VLMs), which exhibit remarkable zero-shot performance
on general vision-language tasks, their generalization ability in embodied
navigation remains largely confined to narrow task settings and
embodiment-specific architectures. In this work, we introduce a
cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained
on eight million navigation samples that encompass quadrupeds, drones, wheeled
robots, and vehicles, and spanning diverse tasks such as vision-and-language
navigation, object searching, target tracking, and autonomous driving. NavFoM
employs a unified architecture that processes multimodal navigation inputs from
varying camera configurations and navigation horizons. To accommodate diverse
camera setups and temporal horizons, NavFoM incorporates identifier tokens that
embed camera view information of embodiments and the temporal context of tasks.
Furthermore, to meet the demands of real-world deployment, NavFoM controls all
observation tokens using a dynamically adjusted sampling strategy under a
limited token length budget. Extensive evaluations on public benchmarks
demonstrate that our model achieves state-of-the-art or highly competitive
performance across multiple navigation tasks and embodiments without requiring
task-specific fine-tuning. Additional real-world experiments further confirm
the strong generalization capability and practical applicability of our
approach.

</details>


### [298] [Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks](https://arxiv.org/abs/2509.12151)
*Zongyao Yi,Joachim Hertzberg,Martin Atzmueller*

Main category: cs.RO

TL;DR: 论文提出了一种改进的GNN-based物理模拟器，显著提升了机器人末端执行器在接触操作中的运动和力-扭矩预测精度，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人末端执行器在接触丰富操作中的运动和力-扭矩预测问题，提高模拟器的准确性和实用性。

Method: 扩展了FIGNet模拟器，引入了新的节点和边类型，支持动作条件预测，适用于控制和状态估计任务。

Result: 在模拟中，MPC代理使用该模型的表现与使用真实动态模型的控制器相当；在实际实验中，运动预测准确率提高了50%，力-扭矩预测精度提升了3倍。

Conclusion: 论文提出了一种可学习的物理模拟器，通过扩展最先进的GNN-based模拟器（FIGNet）并引入新的节点和边类型，显著提升了机器人末端执行器在接触丰富操作中的运动和力-扭矩预测精度。在模拟和实际实验中，该模型均表现出色，尤其在力-扭矩预测精度上实现了3倍的提升。

Abstract: We present a learnable physics simulator that provides accurate motion and
force-torque prediction of robot end effectors in contact-rich manipulation.
The proposed model extends the state-of-the-art GNN-based simulator (FIGNet)
with novel node and edge types, enabling action-conditional predictions for
control and state estimation tasks. In simulation, the MPC agent using our
model matches the performance of the same controller with the ground truth
dynamics model in a challenging peg-in-hole task, while in the real-world
experiment, our model achieves a 50% improvement in motion prediction accuracy
and 3$\times$ increase in force-torque prediction precision over the baseline
physics simulator. Source code and data are publicly available.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [299] [The Chonkers Algorithm: Content-Defined Chunking with Strict Guarantees on Size and Locality](https://arxiv.org/abs/2509.11121)
*Benjamin Berger*

Main category: cs.DS

TL;DR: Chonkers算法通过分层结构和理论保证，实现了分块大小和编辑局部性的严格控制，并引入了Yarn数据类型。


<details>
  <summary>Details</summary>
Motivation: 现有算法（如Rabin指纹和基于锚点的方法）无法同时保证分块大小和编辑局部性的严格控制，因此需要一种新的方法来满足这些需求。

Method: 论文描述了Chonkers算法的分层结构、理论保证和实现考虑，并引入了Yarn数据类型，这是一种基于合并树的去重字符串表示，利用了Chonkers的严格保证。

Result: Chonkers算法实现了编辑的有界传播和对分块大小的精确控制。

Conclusion: Chonkers算法通过其分层结构和理论保证，提供了一种新颖的内容定义分块方法，同时确保了分块大小和编辑局部性的严格保证。

Abstract: This paper presents the Chonkers algorithm, a novel content-defined chunking
method providing simultaneous strict guarantees on chunk size and edit
locality. Unlike existing algorithms such as Rabin fingerprinting and
anchor-based methods, Chonkers achieves bounded propagation of edits and
precise control over chunk sizes. I describe the algorithm's layered structure,
theoretical guarantees, implementation considerations, and introduce the Yarn
datatype, a deduplicated, merge-tree-based string representation benefiting
from Chonkers' strict guarantees.

</details>


### [300] [Triangle-Covered Graphs: Algorithms, Complexity, and Structure](https://arxiv.org/abs/2509.11448)
*Amirali Madani,Anil Maheshwari,Babak Miraftab,Paweł Żyliński*

Main category: cs.DS

TL;DR: 论文研究了将图转换为三角覆盖图的边修改问题，证明了其NP完全性，并提供了多种图类的最小Δ-补集大小的紧界和算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于图论中广泛研究的边修改问题，旨在探索如何将给定图转换为三角覆盖图，并研究其计算复杂性和算法设计。

Method: 论文首先提出了三角覆盖图的概念，并研究了其最小边数的紧下界。随后，定义了Δ-补集的概念，并证明了其决策问题的NP完全性。通过算法视角，研究了不同图类的最小Δ-补集大小，并设计了近似算法和精确算法。

Result: 论文证明了三角覆盖问题的NP完全性，并提供了多种图类的最小Δ-补集大小的紧界。此外，设计了近似算法和针对特定图类的精确算法。

Conclusion: 论文证明了将图转换为三角覆盖图的决策问题是NP完全的，且在标准复杂性假设下不存在常数因子近似算法。此外，研究还提供了多种图类的最小Δ-补集大小的紧界，并设计了针对树和弦图的算法。

Abstract: The widely studied edge modification problems ask how to minimally alter a
graph to satisfy certain structural properties. In this paper, we introduce and
study a new edge modification problem centered around transforming a given
graph into a triangle-covered graph (one in which every vertex belongs to at
least one triangle). We first present tight lower bounds on the number of edges
in any connected triangle-covered graph of order $n$, and then we characterize
all connected graphs that attain this minimum edge count. For a graph $G$, we
define the notion of a $\Delta$-completion set as a set of non-edges of $G$
whose addition to $G$ results in a triangle-covered graph. We prove that the
decision problem of finding a $\Delta$-completion set of size at most $t\geq0$
is $\mathbb{NP}$-complete and does not admit a constant-factor approximation
algorithm under standard complexity assumptions. Moreover, we show that this
problem remains $\mathbb{NP}$-complete even when the input is restricted to
connected bipartite graphs. We then study the problem from an algorithmic
perspective, providing tight bounds on the minimum $\Delta$-completion set size
for several graph classes, including trees, chordal graphs, and cactus graphs.
Furthermore, we show that the triangle-covered problem admits an $(\ln n
+1)$-approximation algorithm for general graphs. For trees and chordal graphs,
we design algorithms that compute minimum $\Delta$-completion sets. Finally, we
show that the threshold for a random graph $\mathbb{G}(n, p)$ to be
triangle-covered occurs at $n^{-2/3}$.

</details>


### [301] [On the Smallest Size of Internal Collage Systems](https://arxiv.org/abs/2509.11602)
*Soichiro Migita,Kyotaro Uehata,Tomohiro I*

Main category: cs.DS

TL;DR: 本文证明了内部拼贴系统的最优大小$\hat{c}(T)$与一般拼贴系统的最优大小$c(T)$在渐进意义上是相等的，并给出了转换方法。此外，还回答了Navarro等人的开放性问题，并提出了计算$\hat{c}(T)$的MAX-SAT公式。


<details>
  <summary>Details</summary>
Motivation: 拼贴系统作为一种广义的直线程序（SLP），通过添加重复规则和截断规则提高了压缩能力。然而，内部拼贴系统（一种拼贴系统的子类）的最优大小$\hat{c}(T)$与一般拼贴系统的最优大小$c(T)$之间的关系尚不明确。本文旨在解决这一问题，并探讨其理论意义和应用价值。

Method: 本文通过将任何大小为$m$的拼贴系统转换为大小为$O(m)$的内部拼贴系统，并在$O(m^2)$时间内完成这一转换，从而证明了$\hat{c}(T) = \Theta(c(T))$。此外，还提出了一个MAX-SAT公式来计算给定字符串$T$的$\hat{c}(T)$。

Result: 本文证明了$\hat{c}(T) = \Theta(c(T))$，即内部拼贴系统的最优大小与一般拼贴系统的最优大小在渐进意义上是相等的。此外，还直接回答了Navarro等人提出的开放性问题，即$b(T) = O(c(T))$，并给出了计算$\hat{c}(T)$的MAX-SAT公式。

Conclusion: 本文证明了内部拼贴系统的最优大小与一般拼贴系统的最优大小在渐进意义上是相等的，即$\hat{c}(T) = \Theta(c(T))$。这一结果使得研究$c(T)$的渐进行为可以集中在内部拼贴系统上，从而避免过度使用截断规则。此外，本文还直接回答了Navarro等人提出的一个开放性问题，即$b(T) = O(c(T))$，并给出了计算$\hat{c}(T)$的MAX-SAT公式。

Abstract: A Straight-Line Program (SLP) for a stirng $T$ is a context-free grammar in
Chomsky normal form that derives $T$ only, which can be seen as a compressed
form of $T$. Kida et al.\ introduced collage systems [Theor. Comput. Sci.,
2003] to generalize SLPs by adding repetition rules and truncation rules. The
smallest size $c(T)$ of collage systems for $T$ has gained attention to see how
these generalized rules improve the compression ability of SLPs. Navarro et al.
[IEEE Trans. Inf. Theory, 2021] showed that $c(T) \in O(z(T))$ and there is a
string family with $c(T) \in \Omega(b(T) \log |T|)$, where $z(T)$ is the number
of Lempel-Ziv parsing of $T$ and $b(T)$ is the smallest size of bidirectional
schemes for $T$. They also introduced a subclass of collage systems, called
internal collage systems, and proved that its smallest size $\hat{c}(T)$ for
$T$ is at least $b(T)$. While $c(T) \le \hat{c}(T)$ is obvious, it is unknown
how large $\hat{c}(T)$ is compared to $c(T)$. In this paper, we prove that
$\hat{c}(T) = \Theta(c(T))$ by showing that any collage system of size $m$ can
be transformed into an internal collage system of size $O(m)$ in $O(m^2)$ time.
Thanks to this result, we can focus on internal collage systems to study the
asymptotic behavior of $c(T)$, which helps to suppress excess use of truncation
rules. As a direct application, we get $b(T) = O(c(T))$, which answers an open
question posed in [Navarro et al., IEEE Trans. Inf. Theory, 2021]. We also give
a MAX-SAT formulation to compute $\hat{c}(T)$ for a given $T$.

</details>


### [302] [An ETH-Tight FPT Algorithm for Rejection-Proof Set Packing with Applications to Kidney Exchange](https://arxiv.org/abs/2509.11965)
*Bart M. P. Jansen,Jeroen S. K. Lamme,Ruben F. A. Verhaegh*

Main category: cs.DS

TL;DR: 论文研究了多智能体肾脏交换问题的参数化复杂性，提出多项式核和FPT算法，并通过参数$c$扩展了复杂性分析。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体肾脏交换问题的参数化复杂性，旨在解决传统肾脏交换问题在多智能体环境下的扩展问题，特别是当智能体可以拒绝不满足其利益的解时，如何设计高效算法和核化方法。

Method: 论文利用向日葵引理对问题的集合包装公式进行分析，提出了一种多项式核的构造方法，并基于此设计了一个时间复杂度为$2^{\mathcal{O}(k \log k)} + n^{\mathcal{O}(1)}$的FPT算法。此外，通过引入参数$c$，论文进一步分析了问题的复杂性变化。

Result: 论文证明了该问题在$d$为常数时存在多项式核，并设计了一个渐近最优的FPT算法。此外，通过引入参数$c$，论文展示了问题从$\Sigma_2^P$-完全到NP-完全的复杂性变化，并分析了不同$c$值对问题难度的影响。

Conclusion: 该论文通过研究多智能体肾脏交换问题的参数化复杂性，展示了如何利用向日葵引理和集合包装公式为该问题提供多项式核，并设计了渐近最优的FPT算法。此外，论文还通过引入参数$c$扩展了问题的复杂性分析，揭示了问题在不同参数设置下的难度变化。

Abstract: We study the parameterized complexity of a recently introduced multi-agent
variant of the Kidney Exchange problem. Given a directed graph $G$ and integers
$d$ and $k$, the standard problem asks whether $G$ contains a packing of
vertex-disjoint cycles, each of length $\leq d$, covering at least $k$ vertices
in total. In the multi-agent setting we consider, the vertex set is partitioned
over several agents who reject a cycle packing as solution if it can be
modified into an alternative packing that covers more of their own vertices. A
cycle packing is called rejection-proof if no agent rejects it and the problem
asks whether such a packing exists that covers at least $k$ vertices.
  We exploit the sunflower lemma on a set packing formulation of the problem to
give a kernel for this $\Sigma_2^P$-complete problem that is polynomial in $k$
for all constant values of $d$. We also provide a $2^{\mathcal{O}(k \log k)} +
n^{\mathcal{O}(1)}$ algorithm based on it and show that this FPT algorithm is
asymptotically optimal under the ETH. Further, we generalize the problem by
including an additional positive integer $c$ in the input that naturally
captures how much agents can modify a given cycle packing to reject it. For
every constant $c$, the resulting problem simplifies from being
$\Sigma_2^P$-complete to NP-complete. With a single-exponential algorithm for
the setting where $c = 1$, we show this to be strictly easier under the ETH
than when $c = 2$. In turn, we show that any $c \geq 2$ yields a problem that
is essentially as hard as the original problem with $c$ unbounded. This
displays an interesting discrepancy between the classical and parameterized
complexity of the problem and gives a good view of what makes it hard.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [303] [Energy-Aware Data Center Management: A Sustainable Approach to Reducing Carbon Footprint](https://arxiv.org/abs/2509.10462)
*Rabab Khan Rongon,Krishna Das*

Main category: cs.NI

TL;DR: 该研究提出了一种能源感知管理框架，通过动态工作负载分配、可再生能源整合和智能冷却系统，旨在减少数据中心的能源消耗和碳足迹，同时保持高性能。研究结果表明，这些策略不仅环保，还能降低运营成本。


<details>
  <summary>Details</summary>
Motivation: 云计算的快速扩张和数据中心基础设施的能源消耗带来了显著的环境挑战，特别是碳足迹的增加。

Method: 研究提出了一个框架，整合了先进的节能技术和资源优化利用，包括动态工作负载分配、可再生能源整合和智能冷却系统。

Result: 通过模拟和案例研究，研究提供了减少数据中心碳排放的实用见解，展示了可持续实践如何在环境和经济上都有益。

Conclusion: 研究发现，通过可扩展的能源感知数据中心设计，可以显著降低环境影响并确保最佳功能，为全球减缓气候变化的努力做出贡献。

Abstract: The rapid expansion of cloud computing and data center infrastructure has led
to significant energy consumption, posing environmental challenges due to the
growing carbon footprint. This research explores energy-aware management
strategies aimed at creating sustainable data center operations. By integrating
advanced energy-efficient technologies and optimizing resource utilization,
this study proposes a framework to minimize power usage while maintaining high
performance. Key elements include dynamic workload allocation, renewable energy
integration, and intelligent cooling systems, all of which contribute to
reducing overall energy consumption. The study also examines the impact of
these strategies on operational costs and performance efficiency, demonstrating
how sustainable practices can be both environmentally and economically
beneficial. Through simulations and case studies, the research offers practical
insights into reducing carbon emissions in data centers, supporting the
transition towards greener cloud infrastructure. The findings highlight the
potential for scalable, energy-aware data center designs that significantly
lower environmental impact while ensuring optimal functionality, contributing
to the global effort of mitigating climate change.

</details>


### [304] [A Dynamic Service Offloading Algorithm Based on Lyapunov Optimization in Edge Computing](https://arxiv.org/abs/2509.10475)
*Peiyan Yuan,Ming Li,Chenyang Wang,Ledong An,Xiaoyan Zhao,Junna Zhang,Xiangyang Li,Huadong Ma*

Main category: cs.NI

TL;DR: 本研究提出了一种动态卸载算法LDSO，通过多跳传输和Lyapunov优化，在保证系统稳定性的同时降低了卸载成本，实验结果验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽视队列稳定性在系统整体性能中的作用，本研究旨在解决这一问题。

Method: 开发了一个多跳数据传输模型和成本模型，结合时变队列模型，并基于Lyapunov优化提出了LDSO算法。

Result: 理论和实验结果均表明，LDSO在成本效率和系统稳定性方面优于现有技术。

Conclusion: 本研究通过提出动态卸载算法（LDSO），在确保长期系统稳定性的同时，显著降低了卸载成本，验证了其在成本和稳定性方面的优越性。

Abstract: This study investigates the trade-off between system stability and offloading
cost in collaborative edge computing. While collaborative offloading among
multiple edge servers enhances resource utilization, existing methods often
overlook the role of queue stability in overall system performance. To address
this, a multi-hop data transmission model is developed, along with a cost model
that captures both energy consumption and delay. A time-varying queue model is
then introduced to maintain system stability. Based on Lyapunov optimization, a
dynamic offloading algorithm (LDSO) is proposed to minimize offloading cost
while ensuring long-term stability. Theoretical analysis and experimental
results verify that the proposed LDSO achieves significant improvements in both
cost efficiency and system stability compared to the state-of-the-art.

</details>


### [305] [The LLM as a Network Operator: A Vision for Generative AI in the 6G Radio Access Network](https://arxiv.org/abs/2509.10478)
*Oluwaseyi Giwa,Michael Adewole,Tobi Awodumila,Pelumi Aderinto*

Main category: cs.NI

TL;DR: 本文提出LLM-RAN Operator框架，利用LLM将人类意图转化为网络行动，通过数学分析验证其可行性和稳定性，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 管理未来的AI原生NextG RAN（如6G及更高版本）的复杂性超出了传统自动化的能力，需要新的解决方案。

Method: 引入LLM-RAN Operator概念，将大型语言模型嵌入RAN控制循环中，将高级人类意图转化为最优网络行动。框架包括与O-RAN标准对齐的适配器，将战略LLM驱动的指导与非实时RIC中的反应执行分离，并提出政策表达性和收敛定理。

Result: 通过数学框架，本文提供了分析AI原生RAN控制可行性和稳定性的工具，并识别了关键研究挑战。

Conclusion: 本文提出了一种基于LLM的RAN操作框架，通过数学严谨性分析了AI原生RAN控制的可行性和稳定性，并指出了安全、实时性能和物理世界基础等关键研究挑战。

Abstract: The management of future AI-native Next-Generation (NextG) Radio Access
Networks (RANs), including 6G and beyond, presents a challenge of immense
complexity that exceeds the capabilities of traditional automation. In
response, we introduce the concept of the LLM-RAN Operator. In this paradigm, a
Large Language Model (LLM) is embedded into the RAN control loop to translate
high-level human intents into optimal network actions. Unlike prior empirical
studies, we present a formal framework for an LLM-RAN operator that builds on
earlier work by making guarantees checkable through an adapter aligned with the
Open RAN (O-RAN) standard, separating strategic LLM-driven guidance in the
Non-Real-Time (RT) RAN intelligent controller (RIC) from reactive execution in
the Near-RT RIC, including a proposition on policy expressiveness and a theorem
on convergence to stable fixed points. By framing the problem with mathematical
rigor, our work provides the analytical tools to reason about the feasibility
and stability of AI-native RAN control. It identifies critical research
challenges in safety, real-time performance, and physical-world grounding. This
paper aims to bridge the gap between AI theory and wireless systems engineering
in the NextG era, aligning with the AI4NextG vision to develop knowledgeable,
intent-driven wireless networks that integrate generative AI into the heart of
the RAN.

</details>


### [306] [Exploring Busy Period for Worst-Case Deadline Failure Probability Analysis](https://arxiv.org/abs/2509.10479)
*Junyi Liu,Xu Jiang,Yuanzhen Mu,Wang Yi,Nan Guan*

Main category: cs.NI

TL;DR: 本文提出WCDFP方法，通过分析不同忙周期起始点的分析改进概率实时调度，实验显示其优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在概率实时调度分析中，仅考虑关键瞬间并不安全，因此需要系统分析不同忙周期起始点的截止时间错过概率。

Method: 提出了一种名为最坏情况截止失败概率（WCDFP）的新方法，用于概率固定优先级抢占式调度。

Result: 实验结果表明，所提出的WCDFP方法在性能上显著优于现有最先进方法。

Conclusion: 本文通过系统分析不同忙周期起始点的截止时间错过概率，提出了一种新颖的WCDFP方法，显著改进了现有技术。

Abstract: Busy period is a fundamental concept in classical deterministic real-time
scheduling analysis. In this deterministic context, only one busy period -
which starts at the critical instant - needs to be considered, which identifies
the worst-case scenario and thus paves the way for the development of efficient
and safe analysis techniques. However, a recent work has revealed that, in the
context of \textit{probabilistic} real-time scheduling analysis, only
considering critical instant is not safe. In this paper, we address this gap by
systematically analyzing deadline miss probabilities across varying busy period
starting points. We propose a novel method of Worst-Case Deadline Failure
Probability (WCDFP) for probabilistic fixed-priority preemptive scheduling.
Experimental results demonstrate significant improvements over state-of-the-art
methods achieved by our proposed method.

</details>


### [307] [Synergetic Empowerment: Wireless Communications Meets Embodied Intelligence](https://arxiv.org/abs/2509.10481)
*Hongtao Liang,Yihe Diao,YuHang Wu,Fuhui Zhou,Qihui Wu*

Main category: cs.NI

TL;DR: 无线通信与具身智能的协同发展可提升智能体通信，但需解决开放问题。


<details>
  <summary>Details</summary>
Motivation: 探讨无线通信如何从简单工具演变为集体智能的数字神经系统，同时提升孤立智能体为具有涌现能力的超级有机体。

Method: 通过感知-认知-执行（PCE）循环的视角，分析具身智能与无线通信的相互促进关系。

Result: 揭示了PCE循环各阶段如何既挑战网络容量又创造系统优化的新机会。

Conclusion: 无线通信与具身智能的协同发展为智能体通信提供了新的机遇，但仍需解决关键开放问题和未来研究方向。

Abstract: Wireless communication is evolving into an agent era, where large-scale
agents with inherent embodied intelligence are not just users but active
participants. The perfect combination of wireless communication and embodied
intelligence can achieve a synergetic empowerment and greatly facilitate the
development of agent communication. An overview of this synergetic empowerment
is presented, framing it as a co-evolutionary process that transforms wireless
communication from a simple utility into the digital nervous system of a
collective intelligence, while simultaneously elevating isolated agents into a
unified superorganism with emergent capabilities far exceeding individual
contributions. Moreover, we elaborate how embodied intelligence and wireless
communication mutually benefit each other through the lens of the
perception-cognition-execution (PCE) loop, revealing a fundamental duality
where each PCE stage both challenges network capacity and creates unprecedented
opportunities for system-wide optimization. Furthermore, critical open issues
and future research directions are identified.

</details>


### [308] [SABR: A Stable Adaptive Bitrate Framework Using Behavior Cloning Pretraining and Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2509.10486)
*Pengcheng Luo,Yunyang Zhao,Bowen Zhang,Genke Yang,Boon-Hee Soong,Chau Yuen*

Main category: cs.NI

TL;DR: SABR框架通过BC预训练和RL微调，解决了ABR方法在OOD场景中的泛化问题，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有学习型ABR方法依赖有限的网络轨迹训练，忽视真实网络条件的广泛分布特性，导致在分布外（OOD）场景中泛化能力差。

Method: 提出了SABR训练框架，结合行为克隆（BC）预训练和强化学习（RL）微调，并引入了ABRBench-3G和ABRBench-4G+基准测试集。

Result: 实验表明，SABR在ABRBench-3G和ABRBench-4G+基准测试中平均排名最佳，优于Pensieve、Comyco和NetLLM。

Conclusion: SABR框架通过结合行为克隆预训练和强化学习微调，显著提升了在未见网络条件下的泛化能力，成为当前最佳的自适应比特率控制方法。

Abstract: With the advent of 5G, the internet has entered a new video-centric era. From
short-video platforms like TikTok to long-video platforms like Bilibili, online
video services are reshaping user consumption habits. Adaptive Bitrate (ABR)
control is widely recognized as a critical factor influencing Quality of
Experience (QoE). Recent learning-based ABR methods have attracted increasing
attention. However, most of them rely on limited network trace sets during
training and overlook the wide-distribution characteristics of real-world
network conditions, resulting in poor generalization in out-of-distribution
(OOD) scenarios. To address this limitation, we propose SABR, a training
framework that combines behavior cloning (BC) pretraining with reinforcement
learning (RL) fine-tuning. We also introduce benchmarks, ABRBench-3G and
ABRBench-4G+, which provide wide-coverage training traces and dedicated OOD
test sets for assessing robustness to unseen network conditions. Experimental
results demonstrate that SABR achieves the best average rank compared with
Pensieve, Comyco, and NetLLM across the proposed benchmarks. These results
indicate that SABR enables more stable learning across wide distributions and
improves generalization to unseen network conditions.

</details>


### [309] [Online Learning Based Efficient Resource Allocation for LoRaWAN Network](https://arxiv.org/abs/2509.10493)
*Ruiqi Wang,Jing Ren,Tongyu Song,Wenjun Li,Xiong Wang,Sheng Wang,Shizhong Xu*

Main category: cs.NI

TL;DR: 论文提出D-LoRa和CD-LoRa框架，通过在线学习动态优化LoRaWAN网络的PDR和EE，显著提升性能，适用于不同环境条件。


<details>
  <summary>Details</summary>
Motivation: 现有方法常简化挑战，仅关注单一指标或缺乏动态适应性，导致性能不佳。为解决这一问题，需要开发能智能权衡PDR和EE的动态资源分配框架。

Method: 提出了两种在线学习资源分配框架：D-LoRa（完全分布式）和CD-LoRa（混合式）。D-LoRa将问题建模为组合多臂老虎机，通过分解联合参数选择和专用奖励函数降低学习复杂度。CD-LoRa结合轻量级集中初始化阶段，加速分布式学习。

Result: 仿真和实地实验表明，D-LoRa在非平稳环境中表现优异，CD-LoRa在平稳条件下收敛最快。实际部署中，PDR提升10.8%，EE提升26.1%。

Conclusion: 所提出的D-LoRa和CD-LoRa框架在实际部署中表现出色，显著提升了PDR和EE，验证了它们在可扩展和高效LoRaWAN网络中的实用性。

Abstract: The deployment of large-scale LoRaWAN networks requires jointly optimizing
conflicting metrics like Packet Delivery Ratio (PDR) and Energy Efficiency (EE)
by dynamically allocating transmission parameters, including Carrier Frequency,
Spreading Factor, and Transmission Power. Existing methods often oversimplify
this challenge, focusing on a single metric or lacking the adaptability needed
for dynamic channel environments, leading to suboptimal performance. To address
this, we propose two online learning-based resource allocation frameworks that
intelligently navigate the PDR-EE trade-off. Our foundational proposal, D-LoRa,
is a fully distributed framework that models the problem as a Combinatorial
Multi-Armed Bandit. By decomposing the joint parameter selection and employing
specialized, disaggregated reward functions, D-LoRa dramatically reduces
learning complexity and enables nodes to autonomously adapt to network
dynamics. To further enhance performance in LoRaWAN networks, we introduce
CD-LoRa, a hybrid framework that integrates a lightweight, centralized
initialization phase to perform a one-time, quasi-optimal channel assignment
and action space pruning, thereby accelerating subsequent distributed learning.
Extensive simulations and real-world field experiments demonstrate the
superiority of our frameworks, showing that D-LoRa excels in non-stationary
environments while CD-LoRa achieves the fastest convergence in stationary
conditions. In physical deployments, our methods outperform state-of-the-art
baselines, improving PDR by up to 10.8% and EE by 26.1%, confirming their
practical effectiveness for scalable and efficient LoRaWAN networks.

</details>


### [310] [Towards Scalable O-RAN Resource Management: Graph-Augmented Proximal Policy Optimization](https://arxiv.org/abs/2509.10499)
*Duc-Thinh Ngo,Kandaraj Piamrat,Ons Aouedi,Thomas Hassan,Philippe Raipin-Parvédy*

Main category: cs.NI

TL;DR: GPPO框架通过GNNs和动作掩码联合优化O-RAN资源分配，显著提升部署成本和奖励，适用于实际场景。


<details>
  <summary>Details</summary>
Motivation: O-RAN架构的灵活性带来了资源管理的重大挑战，需要联合优化功能分割选择和虚拟化单元放置，现有解决方案往往单独处理这些方面或缺乏在大规模实际场景中的可扩展性。

Method: 提出了一种新颖的Graph-Augmented Proximal Policy Optimization (GPPO)框架，利用图神经网络(GNNs)进行拓扑感知特征提取，并集成动作掩码以高效导航组合决策空间。

Result: 在小型和大型O-RAN场景中的广泛实验表明，GPPO始终优于现有基线，实现了高达18%的更低部署成本和25%的更高奖励，同时保持完美可靠性。

Conclusion: GPPO框架在O-RAN部署中表现出高效性和可扩展性，实现了更低的部署成本和更高的奖励，同时保持完美的可靠性。

Abstract: Open Radio Access Network (O-RAN) architectures enable flexible, scalable,
and cost-efficient mobile networks by disaggregating and virtualizing baseband
functions. However, this flexibility introduces significant challenges for
resource management, requiring joint optimization of functional split selection
and virtualized unit placement under dynamic demands and complex topologies.
Existing solutions often address these aspects separately or lack scalability
in large and real-world scenarios. In this work, we propose a novel
Graph-Augmented Proximal Policy Optimization (GPPO) framework that leverages
Graph Neural Networks (GNNs) for topology-aware feature extraction and
integrates action masking to efficiently navigate the combinatorial decision
space. Our approach jointly optimizes functional split and placement decisions,
capturing the full complexity of O-RAN resource allocation. Extensive
experiments on both small-and large-scale O-RAN scenarios demonstrate that GPPO
consistently outperforms state-of-the-art baselines, achieving up to 18% lower
deployment cost and 25% higher reward in generalization tests, while
maintaining perfect reliability. These results highlight the effectiveness and
scalability of GPPO for practical O-RAN deployments.

</details>


### [311] [An Internet of Intelligent Things Framework for Decentralized Heterogeneous Platforms](https://arxiv.org/abs/2509.10507)
*Vadim Allayev,Mahbubur Rahman*

Main category: cs.NI

TL;DR: 论文提出了一种去中心化的IoIT网络模型，结合联邦学习、元启发式和多目标优化，解决了集中式系统的瓶颈和能源效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前IoIT平台多采用集中式系统，存在瓶颈和安全隐患。去中心化系统能解决这些问题，但面临能源效率、计算资源和存储限制等挑战。

Method: 系统采用了联邦学习进行分布式节点训练，利用元启发式优化任务分配和路由路径，并通过多目标优化平衡性能目标。

Result: 提出的异构去中心化系统模型能够有效协调节点，优化可靠性、能源效率和延迟。

Conclusion: 论文提出了一种异构、去中心化的IoIT点对点网状网络系统模型，通过联邦学习、元启发式和多目标优化技术，实现了可靠性、能源效率和延迟的优化目标。

Abstract: Internet of Intelligent Things (IoIT), an emerging field, combines the
utility of Internet of Things (IoT) devices with the innovation of embedded AI
algorithms. However, it does not come without challenges, and struggles
regarding available computing resources, energy supply, and storage
limitations. In particular, many impediments to IoIT are linked to the
energy-efficient deployment of machine learning (ML)/deep learning (DL) models
in embedded devices. Research has been conducted to design energy-efficient
IoIT platforms, but these papers often focus on centralized systems, in which
some central entity processes all the data and coordinates actions. This can be
problematic, e.g., serve as bottleneck or lead to security concerns. In a
decentralized system, nodes/devices would self-organize and make their own
decisions. Therefore, to address such issues, we propose a heterogeneous,
decentralized sensing and monitoring IoIT peer-to-peer mesh network system
model. Nodes in the network will coordinate towards several optimization goals:
reliability, energy efficiency, and latency. The system employs federated
learning to train nodes in a distributed manner, metaheuristics to optimize
task allocation and routing paths, and multi-objective optimization to balance
conflicting performance goals.

</details>


### [312] [CAR-BRAINet: Sub-6GHz Aided Spatial Adaptive Beam Prediction with Multi Head Attention for Heterogeneous Vehicular Networks](https://arxiv.org/abs/2509.10508)
*Aathira G Menon,Prabu Krishnan,Shyam Lal*

Main category: cs.NI

TL;DR: 本文提出了一种轻量级深度学习模型CAR-BRAINet，用于异构车联网中的波束预测，在复杂场景下表现优异，显著提升了频谱效率。


<details>
  <summary>Details</summary>
Motivation: 异构车联网（HetVNets）在5G/B5G车联网中扮演关键角色，但现有研究对HetVNets的专用波束预测解决方案探索不足。因此，开发一种可靠的波束预测解决方案成为当务之急。

Method: 本文提出了一种轻量级的深度学习解决方案"CAR-BRAINet"，结合了卷积神经网络和强大的多头注意力机制。

Result: CAR-BRAINet在所有车联网场景中表现优异，实现了精确的波束预测，最小化波束开销，并在频谱效率上比现有方法提升了17.9422%。

Conclusion: 本研究证明了CAR-BRAINet在复杂异构车联网中的有效性，提供了不依赖移动用户位置角度和天线尺寸的优异性能，从而减少了冗余传感器延迟。

Abstract: Heterogeneous Vehicular Networks (HetVNets) play a key role by stacking
different communication technologies such as sub-6GHz, mm-wave and DSRC to meet
diverse connectivity needs of 5G/B5G vehicular networks. HetVNet helps address
the humongous user demands-but maintaining a steady connection in a highly
mobile, real-world conditions remain a challenge. Though there has been ample
of studies on beam prediction models a dedicated solution for HetVNets is
sparsely explored. Hence, it is the need of the hour to develop a reliable beam
prediction solution, specifically for HetVNets. This paper introduces a
lightweight deep learning-based solution termed-"CAR-BRAINet" which consists of
convolutional neural networks with a powerful multi-head attention (MHA)
mechanism. Existing literature on beam prediction is largely studied under a
limited, idealised vehicular scenario, often overlooking the real-time
complexities and intricacies of vehicular networks. Therefore, this study aims
to mimic the complexities of a real-time driving scenario by incorporating key
factors such as prominent MAC protocols-3GPP-C-V2X and IEEE 802.11BD, the
effect of Doppler shifts under high velocity and varying distance and SNR
levels into three high-quality dynamic datasets pertaining to urban, rural and
highway vehicular networks. CAR-BRAINet performs effectively across all the
vehicular scenarios, demonstrating precise beam prediction with minimal beam
overhead and a steady improvement of 17.9422% on the spectral efficiency over
the existing methods. Thus, this study justifies the effectiveness of
CAR-BRAINet in complex HetVNets, offering promising performance without relying
on the location angle and antenna dimensions of the mobile users, and thereby
reducing the redundant sensor-latency.

</details>


### [313] [Pair-Bid Auction Model for Optimized Network Slicing in 5G RAN](https://arxiv.org/abs/2509.10533)
*Mengyao Li,Sebastian Troia,Yingqian Zhang,Guido Maier*

Main category: cs.NI

TL;DR: 5G网络切片的分层拍卖模型，通过pair-bid和VCG定价优化资源分配和收入，收入提升12.5%。


<details>
  <summary>Details</summary>
Motivation: 解决5G网络切片中公平共享和财务相关的功率效率问题，优化资源分配和收入生成。

Method: 采用分层组合拍卖模型，上层由MNO向MVNO拍卖切片，下层由MVNO向终端用户分配资源，结合pair-bid机制和VCG定价。

Result: 模拟验证了模型在资源分配和财务性能上的有效性，收入比基线提高了12.5%。

Conclusion: 该论文提出的分层组合拍卖模型在资源分配和收入生成方面表现出色，通过pair-bid机制和VCG定价实现了12.5%的收入提升。

Abstract: Network slicing is a key 5G technology that enables multiple virtual networks
to share physical infrastructure, optimizing flexibility and resource
allocation. This involves Mobile Network Operators (MNO), Mobile Virtual
Network Operators (MVNOs), and end users, where MNO leases network slices to
MVNOs, and then provides customized services. This work considers end-to-end
network slicing with a focus on fair sharing and financial-related power
efficiency, modeled as a two level hierarchical combinatorial auction. At the
upper level, an MNO auctions slices to competing MVNOs, while at the lower
level, MVNOs allocate resources to end users through their own auctions.
Dynamic user requests add complexity to the process. Our model optimizes
resource allocation and revenue generation using a pair-bid mechanism and
Vickrey-Clarke-Groves (VCG) pricing. The pair-bid approach enhances competition
and efficiency, while VCG ensures truthful bidding based on marginal system
impact. Simulations validate the model's effectiveness in resource distribution
and financial performance, showing a 12.5% revenue improvement over the
baseline.

</details>


### [314] [ASL360: AI-Enabled Adaptive Streaming of Layered 360° Video over UAV-assisted Wireless Networks](https://arxiv.org/abs/2509.10544)
*Alireza Mohammadhosseini,Jacob Chakareski,Nicholas Mastronarde*

Main category: cs.NI

TL;DR: ASL360是一种基于深度强化学习的自适应调度器，用于无人机辅助5G网络中的360度视频流，显著提升了用户QoE。


<details>
  <summary>Details</summary>
Motivation: 旨在最大化无人机辅助5G无线网络中移动VR用户的整体体验质量（QoE）。

Method: 采用基于约束马尔可夫决策过程（CMDP）的调度决策，并使用策略梯度方法（PPO）来寻找最优策略，同时实现动态调整机制以平衡视频质量、缓冲区占用和质量变化。

Result: ASL360显著提升了QoE，实现了约2 dB更高的平均视频质量、80%更低的平均重新缓冲时间和57%更低的视频质量变化。

Conclusion: ASL360通过其分层和自适应方法显著提升了沉浸式视频流应用中的QoE，特别是在动态和挑战性的网络环境中。

Abstract: We propose ASL360, an adaptive deep reinforcement learning-based scheduler
for on-demand 360{\deg} video streaming to mobile VR users in next generation
wireless networks. We aim to maximize the overall Quality of Experience (QoE)
of the users served over a UAV-assisted 5G wireless network. Our system model
comprises a macro base station (MBS) and a UAV-mounted base station which both
deploy mm-Wave transmission to the users. The 360{\deg} video is encoded into
dependent layers and segmented tiles, allowing a user to schedule downloads of
each layer's segments. Furthermore, each user utilizes multiple buffers to
store the corresponding video layer's segments. We model the scheduling
decision as a Constrained Markov Decision Process (CMDP), where the agent
selects Base or Enhancement layers to maximize the QoE and use a policy
gradient-based method (PPO) to find the optimal policy. Additionally, we
implement a dynamic adjustment mechanism for cost components, allowing the
system to adaptively balance and prioritize the video quality, buffer
occupancy, and quality change based on real-time network and streaming session
conditions. We demonstrate that ASL360 significantly improves the QoE,
achieving approximately 2 dB higher average video quality, 80% lower average
rebuffering time, and 57% lower video quality variation, relative to
competitive baseline methods. Our results show the effectiveness of our layered
and adaptive approach in enhancing the QoE in immersive videostreaming
applications, particularly in dynamic and challenging network environments.

</details>


### [315] [Empowering AI-Native 6G Wireless Networks with Quantum Federated Learning](https://arxiv.org/abs/2509.10559)
*Shaba Shaon,Md Raihan Uddin,Dinh C. Nguyen,Seyyedali Hosseinalipour,Dusit Niyato,Octavia A. Dobre*

Main category: cs.NI

TL;DR: 本文探讨了量子联邦学习（QFL）在AI原生6G网络中的集成，展示了其在边缘智能、网络优化及安全与隐私方面的优势，并通过案例研究验证了其优于传统方法的性能，同时指出了实际部署中的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索量子联邦学习（QFL）在AI原生6G网络中的集成，以解决传统FL方法在异构动态无线网络中面临的设备计算能力有限、连接不可靠、通信间歇性及模型安全与数据隐私漏洞等瓶颈问题。

Method: 通过将量子技术整合到联邦学习（FL）工作流程中的计算、通信和密码学领域，研究了QFL在边缘智能、网络优化及安全与隐私三个关键维度上的新能力。

Result: 案例研究表明，采用量子近似优化算法的QFL框架在模型收敛性上优于传统方法。

Conclusion: 本文指出了量子联邦学习（QFL）在实际部署中面临的挑战，如量子态脆弱性、与传统协议的不兼容性以及硬件限制，并提出了可扩展的实际应用关键研究方向。

Abstract: AI-native 6G networks are envisioned to tightly embed artificial intelligence
(AI) into the wireless ecosystem, enabling real-time, personalized, and
privacy-preserving intelligence at the edge. A foundational pillar of this
vision is federated learning (FL), which allows distributed model training
across devices without sharing raw data. However, implementing classical FL
methods faces several bottlenecks in heterogeneous dynamic wireless networks,
including limited device compute capacity, unreliable connectivity,
intermittent communications, and vulnerability to model security and data
privacy breaches. This article investigates the integration of quantum
federated learning (QFL) into AI-native 6G networks, forming a transformative
paradigm capable of overcoming these challenges. By leveraging quantum
techniques across computing, communication, and cryptography within FL
workflows, QFL offers new capabilities along three key dimensions: (i) edge
intelligence, (ii) network optimization, and (iii) security and privacy, which
are studied in this work. We further present a case study demonstrating that a
QFL framework employing the quantum approximate optimization algorithm
outperforms classical methods in model convergence. We conclude the paper by
identifying practical challenges facing QFL deployment, such as quantum state
fragility, incompatibility with classical protocols, and hardware constraints,
and then outline key research directions toward its scalable real-world
adoption.

</details>


### [316] [gNB-based Local Breakout for URLLC in industrial 5G](https://arxiv.org/abs/2509.10617)
*Rajendra Paudyal,Rajendra Upadhyay,Al Nahian Bin Emran,Duminda Wijesekera*

Main category: cs.NI

TL;DR: 论文提出了一种gNB-local multicast breakout方案，显著降低工业URLLC应用的延迟，并保持3GPP合规性。


<details>
  <summary>Details</summary>
Motivation: 工业URLLC工作负载（如协调机器人、自动导引车等）需要亚5毫秒延迟和五个九的可靠性。现有5G多播/广播服务的核心网络路径和分组延迟在数据发送者和接收者共享同一小区时可以避免。

Method: 设计了一种基于gNB-local multicast breakout的方案，包括资格策略和配置授权上行链路，同时保持3GPP安全性和合规性。

Result: 模拟结果表明，移除回程/UPF/AF段可将端到端延迟从6.5-11.5毫秒降低至1.5-4.0毫秒，实现亚2毫秒平均值和稳定的组大小差距约10毫秒。

Conclusion: 该论文提出了一种gNB-local multicast breakout方案，为私有5G网络中的确定性组内传播提供了实用且符合标准的路径，并计划未来进行多小区和原型验证。

Abstract: Industrial URLLC workloads-coordinated robotics, automated guided vehicles,
machine-vision collaboration require sub-5 ms latency and five-nines
reliability. In standardized 5G Multicast/Broadcast Services, intra-cell group
traffic remains anchored in the core using MB-SMF/MB-UPF, and the Application
Function. This incurs a core network path and packet delay that is avoidable
when data transmitters and receivers share a cell. We propose a gNB-local
multicast breakout that pivots eligible uplink flows to a downlink
point-to-multipoint bearer within the gNB, while maintaining authorization,
membership, and policy in the 5G core. The design specifies an eligibility
policy, configured-grant uplink. 3GPP security and compliance are preserved via
unchanged control-plane anchors. A latency budget and simulation indicate that
removing the backhaul/UPF/AF segment reduces end-to-end latency from
approximate 6.5-11.5 ms (anchored to the core) to 1.5-4.0 ms (local breakout),
producing sub-2 ms averages and a stable gap approximate 10 ms between group
sizes. The approach offers a practical, standards-aligned path to deterministic
intra-cell group dissemination in private 5G. We outline multi-cell and
prototype validation as future work.

</details>


### [317] [Deep Learning based Moving Target Defence for Federated Learning against Poisoning Attack in MEC Systems with a 6G Wireless Model](https://arxiv.org/abs/2509.10914)
*Somayeh Kianpisheh,Tarik Taleb,Jari Iinatti,JaeSeung Song*

Main category: cs.NI

TL;DR: 该论文提出了一种结合流量分析和拓扑变异的联邦学习防御策略，有效对抗模型投毒攻击，提升了DDoS检测性能。


<details>
  <summary>Details</summary>
Motivation: 针对异构模型和复杂攻击场景下传统基于模型的异常检测机制效率不足的问题，旨在通过设备级流量分析和动态参与者调整来增强联邦学习的防御能力。

Method: 采用循环神经网络进行时间序列流量分析，结合6G无线模型，提出了一种基于深度强化学习的拓扑变异优化框架。

Result: 在Botnet攻击场景下，该方法实现了恶意模型的有效排除，并显著提高了识别速度和准确性。

Conclusion: 该论文通过结合设备级流量分析和拓扑变异策略，有效提升了联邦学习对抗模型投毒攻击的防御能力，并在DDoS攻击检测应用中验证了方法的有效性。

Abstract: Collaboration opportunities for devices are facilitated with Federated
Learning (FL). Edge computing facilitates aggregation at edge and reduces
latency. To deal with model poisoning attacks, model-based outlier detection
mechanisms may not operate efficiently with hetereogenous models or in
recognition of complex attacks. This paper fosters the defense line against
model poisoning attack by exploiting device-level traffic analysis to
anticipate the reliability of participants. FL is empowered with a topology
mutation strategy, as a Moving Target Defence (MTD) strategy to dynamically
change the participants in learning. Based on the adoption of recurrent neural
networks for time-series analysis of traffic and a 6G wireless model,
optimization framework for MTD strategy is given. A deep reinforcement
mechanism is provided to optimize topology mutation in adaption with the
anticipated Byzantine status of devices and the communication channel
capabilities at devices. For a DDoS attack detection application and under
Botnet attack at devices level, results illustrate acceptable malicious models
exclusion and improvement in recognition time and accuracy.

</details>


### [318] [RU Energy Modeling for O-RAN in ns3-oran](https://arxiv.org/abs/2509.10978)
*Abdul Wadud,Nima Afraz*

Main category: cs.NI

TL;DR: 论文提出了一种基于ns3-oran的RU功耗模型，支持xApp控制，验证了非线性功率缩放并确定最佳工作点，填补了现有框架的空白。


<details>
  <summary>Details</summary>
Motivation: 现有框架（如EARTH或VBS-DRX）缺乏对RU-centric功耗建模的支持，尤其是在xApp控制下的灵活性。本文旨在填补这一空白，为O-RAN部署中的能量管理策略提供仿真驱动评估工具。

Method: 使用ns3-oran模拟器构建了一个RU-centric的功耗模型，参数化硬件级特性（如收发器数量、功率放大器效率、mmWave开销和待机行为），并通过xApp控制进行模拟。

Result: 模型成功验证了非线性功率缩放的真实性，并确定了RU的理想工作点，为xApp驱动的能量管理策略提供了支持。

Conclusion: 该论文提出了一个基于ns3-oran模拟器的灵活且详细的无线电单元（RU）功耗模型，支持xApp控制，能够模拟不同发射功率下的能效，并与ns-3的能量跟踪系统无缝集成。数值研究验证了模型的有效性，并确定了RU的最佳工作点。

Abstract: This paper presents a detailed and flexible power consumption model for Radio
Units (RUs) in O-RAN using the ns3-oran simulator. This is the first ns3-oran
model supporting xApp control to perform the RU power modeling. In contrast to
existing frameworks like EARTH or VBS-DRX, the proposed framework is RU-centric
and is parameterized by hardware-level features, such as the number of
transceivers, the efficiency of the power amplifier, mmWave overheads, and
standby behavior. It enables simulation-driven assessment of energy efficiency
at various transmit power levels and seamlessly integrates with ns-3's energy
tracking system. To help upcoming xApp-driven energy management strategies in
O-RAN installations, numerical research validates the model's capacity to
represent realistic nonlinear power scaling. It identifies ideal operating
points for effective RU behavior.

</details>


### [319] [Multi-Modal Sensing Aided mmWave Beamforming for V2V Communications with Transformers](https://arxiv.org/abs/2509.11112)
*Muhammad Baqer Mollah,Honggang Wang,Hua Fang*

Main category: cs.NI

TL;DR: 多模态感知与融合学习框架有效减少毫米波通信中的波束训练开销，提升通信效率。


<details>
  <summary>Details</summary>
Motivation: 现有的标准波束成形方法在高度动态的车载环境中产生高开销，减少通信可用时间。因此，需要一种替代方案来降低这些开销。

Method: 通过视觉和GPS坐标感知模态提取特征，使用模态特定编码器，融合多模态特征以预测top-k波束，从而主动建立最佳视距链路。

Result: 在四种真实车辆间通信场景中，该框架在预测top-15波束时达到77.58%的准确率，平均功率损失仅为2.32 dB，并减少了76.56%的波束搜索空间开销。

Conclusion: 该研究提出的多模态感知与融合学习框架在毫米波通信中有效减少了波束训练的开销，提高了通信效率，并在实际车辆间通信场景中展示了良好的泛化能力。

Abstract: Beamforming techniques are utilized in millimeter wave (mmWave) communication
to address the inherent path loss limitation, thereby establishing and
maintaining reliable connections. However, adopting standard defined
beamforming approach in highly dynamic vehicular environments often incurs high
beam training overheads and reduces the available airtime for communications,
which is mainly due to exchanging pilot signals and exhaustive beam
measurements. To this end, we present a multi-modal sensing and fusion learning
framework as a potential alternative solution to reduce such overheads. In this
framework, we first extract the features individually from the visual and GPS
coordinates sensing modalities by modality specific encoders, and subsequently
fuse the multimodal features to obtain predicted top-k beams so that the best
line-of-sight links can be proactively established. To show the
generalizability of the proposed framework, we perform a comprehensive
experiment in four different vehicle-to-vehicle (V2V) scenarios from real-world
multi-modal sensing and communication dataset. From the experiment, we observe
that the proposed framework achieves up to 77.58% accuracy on predicting top-15
beams correctly, outperforms single modalities, incurs roughly as low as 2.32
dB average power loss, and considerably reduces the beam searching space
overheads by 76.56% for top-15 beams with respect to standard defined approach.

</details>


### [320] [On the Feasibility of Inter-Flow Service Degradation Detection](https://arxiv.org/abs/2509.11140)
*Balint Bicski,Adrian Pekar*

Main category: cs.NI

TL;DR: 研究提出流间关联框架解决硬件加速导致的监控盲点，发现流间信号可预测服务降级，但简单模型难以利用复杂上下文信息，需开发结构感知模型。


<details>
  <summary>Details</summary>
Motivation: 现代网络中的硬件加速通过将流量卸载到不可观测状态而创建监控盲点，阻碍了实时服务降级（SD）检测，因此需要解决这一问题。

Method: 提出并形式化了一种新颖的流间关联框架，并进行了全面的统计分析，随后使用标准机器学习模型评估该框架。

Result: 分析显示流间信号通常在目标流降级之前出现，验证了及时检测的潜力，但机器学习模型主要依赖简单的流内特征，表明需要更复杂的模型来利用上下文信息。

Conclusion: 该研究不仅为流间问题提供了基础分析，还明确了未来研究需要开发结构感知模型以解决此问题。

Abstract: Hardware acceleration in modern networks creates monitoring blind spots by
offloading flows to a non-observable state, hindering real-time service
degradation (SD) detection. To address this, we propose and formalize a novel
inter-flow correlation framework, built on the hypothesis that observable flows
can act as environmental sensors for concurrent, non-observable flows. We
conduct a comprehensive statistical analysis of this inter-flow landscape,
revealing a fundamental trade-off: while the potential for correlation is vast,
the most explicit signals (i.e., co-occurring SD events) are sparse and rarely
perfectly align. Critically, however, our analysis shows these signals
frequently precede degradation in the target flow, validating the potential for
timely detection. We then evaluate the framework using a standard machine
learning model. While the model achieves high classification accuracy, a
feature-importance analysis reveals it relies primarily on simpler intra-flow
features. This key finding demonstrates that harnessing the complex contextual
information requires more than simple models. Our work thus provides not only a
foundational analysis of the inter-flow problem but also a clear outline for
future research into the structure-aware models needed to solve it.

</details>


### [321] [UDFS: Lightweight Representation-Driven Robust Network Traffic Classification](https://arxiv.org/abs/2509.11157)
*Youquan Xian,Xueying Zeng,Mei Huang,Aoxiang Zhou,Xiaoyu Cui,Peng Liu,Lei Cui*

Main category: cs.NI

TL;DR: 提出UDFS表示和自适应阈值机制，有效解决加密流量分析中序列建模的复杂度和区分性问题，实验验证其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有序列建模方法存在特征冗余或计算存储开销大的问题，需要一种既能减少复杂度又能保持高区分性的解决方案。

Method: 提出UDFS表示将整个轨迹压缩为二维序列，并通过上下行流量聚合特征化每个流；引入自适应阈值机制动态调整训练权重和拒绝边界。

Result: 实验表明，该方法在粗粒度和细粒度数据集、概念漂移和开放世界场景下均表现出优越的分类性能和鲁棒性。

Conclusion: 提出的UDFS表示和自适应阈值机制在加密流量分析中表现出色，提升了分类性能和鲁棒性。

Abstract: In recent years, sequence features such as packet length have received
considerable attention due to their central role in encrypted traffic analysis.
Existing sequence modeling approaches can be broadly categorized into
flow-level and trace-level methods: the former suffer from high feature
redundancy, limiting their discriminative power, whereas the latter preserve
complete information but incur substantial computational and storage overhead.
To address these limitations, we propose the \textbf{U}p-\textbf{D}own
\textbf{F}low \textbf{S}equence (\textbf{UDFS}) representation, which
compresses an entire trace into a two-dimensional sequence and characterizes
each flow by the aggregate of its upstream and downstream traffic, reducing
complexity while maintaining high discriminability. Furthermore, to address the
challenge of class-specific discriminability differences, we propose an
adaptive threshold mechanism that dynamically adjusts training weights and
rejection boundaries, enhancing the model's classification performance.
Experimental results demonstrate that the proposed method achieves superior
classification performance and robustness on both coarse-grained and
fine-grained datasets, as well as under concept drift and open-world scenarios.
Code and Dataset are available at https://github.com/kid1999/UDFS.

</details>


### [322] [Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation Intelligent Delay-Tolerant Networks](https://arxiv.org/abs/2509.11239)
*Zhekun Huang,Milena Radenkovic*

Main category: cs.NI

TL;DR: 论文提出了一种基于机器学习的DTN智能路由增强方法，通过MLP、SVM和RF分类器预测中继节点质量，显著提高了传递率和降低了延迟，MLP表现最佳。


<details>
  <summary>Details</summary>
Motivation: DTN在高度动态和挑战性场景中至关重要，但现有协议的静态复制策略无法自适应区分高质量中继节点，导致消息传播效率低下。

Method: 提出了一种新颖的智能路由增强方法，将基于机器学习的节点评估集成到Spray and Wait框架中。通过从模拟日志中提取动态核心特征，训练了多层感知器（MLP）、支持向量机（SVM）和随机森林（RF）分类器，预测节点是否适合作为动态条件下的中继。训练模型通过轻量级Flask-based RESTful API部署，实现实时自适应预测。

Result: 实验表明，提出的框架显著提高了传递率并降低了平均延迟。在所有评估的分类器中，MLP表现最为稳健，在准确性、适应性和推理速度上均优于SVM和RF。

Conclusion: 研究证实了将机器学习集成到DTN路由中的新颖性和实用性，为智能城市、灾难恢复等动态环境中的弹性和智能通信系统铺平了道路。

Abstract: Delay Tolerant Networks (DTNs) are critical for emergency communication in
highly dynamic and challenging scenarios characterized by intermittent
connectivity, frequent disruptions, and unpredictable node mobility. While some
protocols are widely adopted for simplicity and low overhead, their static
replication strategy lacks the ability to adaptively distinguish high-quality
relay nodes, often leading to inefficient and suboptimal message dissemination.
To address this challenge, we propose a novel intelligent routing enhancement
that integrates machine learning-based node evaluation into the Spray and Wait
framework. Several dynamic, core features are extracted from simulation logs
and are used to train multiple classifiers - Multi-Layer Perceptron (MLP),
Support Vector Machine (SVM), and Random Forest (RF) - to predict whether a
node is suitable as a relay under dynamic conditions. The trained models are
deployed via a lightweight Flask-based RESTful API, enabling real-time,
adaptive predictions. We implement the enhanced router MLPBasedSprayRouter,
which selectively forwards messages based on the predicted relay quality. A
caching mechanism is incorporated to reduce computational overhead and ensure
stable, low-latency inference. Extensive experiments under realistic emergency
mobility scenarios demonstrate that the proposed framework significantly
improves delivery ratio while reducing average latency compared to the baseline
protocols. Among all evaluated classifiers, MLP achieved the most robust
performance, consistently outperforming both SVM and RF in terms of accuracy,
adaptability, and inference speed. These results confirm the novelty and
practicality of integrating machine learning into DTN routing, paving the way
for resilient and intelligent communication systems in smart cities, disaster
recovery, and other dynamic environments.

</details>


### [323] [Energy-Aware 6G Network Design: A Survey](https://arxiv.org/abs/2509.11289)
*Rashmi Kamran,Mahesh Ganesh Bhat,Pranav Jha,Shana Moothedath,Manjesh Hanawal,Prasanna Chaporkar*

Main category: cs.NI

TL;DR: 本文全面调查了6G网络能源效率设计的方法，包括AI/ML解决方案和标准化进展，总结了开放研究问题和挑战。


<details>
  <summary>Details</summary>
Motivation: 6G移动网络预计将支持大量新能力和数据为中心的应用，可能引发显著的能源效率和可持续性问题，因此需要将可持续性作为设计的关键目标。

Method: 提供了设计能源高效6G网络的全面调查方法，包括能源收集、能源模型和参数、能源感知服务分类以及基于AI/ML的解决方案。

Result: 调查涵盖了多种能源效率设计方法，并展示了将能源意识纳入网络决策的益处。同时，介绍了3GPP、ITU和IEEE的标准化工作，突出了新贡献的机会。

Conclusion: 本调查总结了6G网络能源效率设计的开放研究问题和挑战，旨在实现能源感知设计的可行性，并确保性能和能源目标的最优化。

Abstract: 6th Generation (6G) mobile networks are envisioned to support several new
capabilities and data-centric applications for unprecedented number of users,
potentially raising significant energy efficiency and sustainability concerns.
This brings focus on sustainability as one of the key objectives in the their
design. To move towards sustainable solution, research and standardization
community is focusing on several key issues like energy information monitoring
and exposure, use of renewable energy, and use of Artificial
Intelligence/Machine Learning (AI/ML) for improving the energy efficiency in 6G
networks. The goal is to build energy-aware solutions that takes into account
the energy information resulting in energy efficient networks. Design of
energy-aware 6G networks brings in new challenges like increased overheads in
gathering and exposing of energy related information, and the associated user
consent management. The aim of this paper is to provide a comprehensive survey
of methods used for design of energy efficient 6G networks, like energy
harvesting, energy models and parameters, classification of energy-aware
services, and AI/ML-based solutions. The survey also includes few use cases
that demonstrate the benefits of incorporating energy awareness into network
decisions. Several ongoing standardization efforts in 3GPP, ITU, and IEEE are
included to provide insights into the ongoing work and highlight the
opportunities for new contributions. We conclude this survey with open research
problems and challenges that can be explored to make energy-aware design
feasible and ensure optimality regarding performance and energy goals for 6G
networks.

</details>


### [324] [Federated Edge Learning for Predictive Maintenance in 6G Small Cell Networks](https://arxiv.org/abs/2509.11421)
*Yusuf Emir Sezgin,Mehmet Özdem,Tuğçe Bilen*

Main category: cs.NI

TL;DR: 提出了一种基于联邦边缘学习的6G小基站网络预测维护框架，通过本地训练和模型聚合实现高性能，同时保护隐私并减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 6G网络的推广对自主性、可靠性和可扩展性提出了前所未有的要求，但敏感遥测数据传输到中央服务器引发了隐私和带宽问题。

Method: 采用知识定义网络（KDN）架构，结合联邦平均（FedAvg）算法进行模型聚合，通过Flower框架协调学习流程。

Result: 实验结果表明，联邦模型在准确性和每标签精度方面与集中式训练相当，同时保护了隐私并减少了通信开销。

Conclusion: 联邦边缘学习框架在6G小基站网络中实现了与集中式训练相当的预测维护性能，同时保护了隐私并减少了通信开销。

Abstract: The rollout of 6G networks introduces unprecedented demands for autonomy,
reliability, and scalability. However, the transmission of sensitive telemetry
data to central servers raises concerns about privacy and bandwidth. To address
this, we propose a federated edge learning framework for predictive maintenance
in 6G small cell networks. The system adopts a Knowledge Defined Networking
(KDN) architecture in Data, Knowledge, and Control Planes to support
decentralized intelligence, telemetry-driven training, and coordinated policy
enforcement. In the proposed model, each base station independently trains a
failure prediction model using local telemetry metrics, including SINR, jitter,
delay, and transport block size, without sharing raw data. A threshold-based
multi-label encoding scheme enables the detection of concurrent fault
conditions. We then conduct a comparative analysis of centralized and federated
training strategies to evaluate their performance in this context. A realistic
simulation environment is implemented using the ns-3 mmWave module,
incorporating hybrid user placement and base station fault injection across
various deployment scenarios. The learning pipeline is orchestrated via the
Flower framework, and model aggregation is performed using the Federated
Averaging (FedAvg) algorithm. Experimental results demonstrate that the
federated model achieves performance comparable to centralized training in
terms of accuracy and per-label precision, while preserving privacy and
reducing communication overhead.

</details>


### [325] [Towards Dynamic Urban Scene Synthesis: The Digital Twin Descriptor Service](https://arxiv.org/abs/2509.11810)
*Ioannis Tsampras,Georgios Stergiopoulos,Tanya Politi,Spyros Denazis*

Main category: cs.NI

TL;DR: 本文提出DTDS，通过NGSI-LD技术融合几何资产与上下文信息，解决了数字孪生平台的集成与互操作性问题。


<details>
  <summary>Details</summary>
Motivation: 现有数字孪生平台缺乏城市环境所需的集成、联合和适应性连接能力，导致上下文与表示隔离，互操作性受限。

Method: 提出了DTDS架构组件和描述本体，实现了上下文数据、表示和运行时同步的动态联合集成。

Result: DTDS能够动态联合异构引擎和模拟器中的上下文数据与表示，支持现代智慧城市中的数字孪生流程。

Conclusion: 本文提出了'数字孪生描述符服务（DTDS）'，通过NGSI-LD技术将几何资产和上下文信息的抽象引用融合在一个可扩展的描述符服务中，解决了现有数字孪生平台在集成、联合和适应性连接方面的不足。

Abstract: Digital twins have been introduced as supporters to city operations, yet
existing scene-descriptor formats and digital twin platforms often lack the
integration, federation, and adaptable connectivity that urban environments
demand. Modern digital twin platforms decouple data streams and representations
into separate architectural planes, fusing them only at the visualization layer
and limiting potential for simulation or further processing of the combined
assets. At the same time, geometry-centric file standards for digital twin
description, and services built on top of them, focus primarily on explicitly
declaring geometry and additional structural or photorealistic parameters,
making integration with evolving context information a complicated process
while limiting compatibility with newer representation methods. Additionally,
multi-provider federation, critical in smart city services where multiple
stakeholders may control distinct infrastructure or representation assets, is
sparsely supported. Consequently, most pilots isolate context and
representation, fusing them per use case with ad hoc components and custom
description files or glue code, which hinders interoperability. To address
these gaps, this paper proposes a novel concept, the 'Digital Twin Descriptor
Service (DTDS)' that fuses abstracted references to geometry assets and context
information within a single, extensible descriptor service through NGSI-LD. The
proposed DTDS provides dynamic and federated integration of context data,
representations, and runtime synchronization across heterogeneous engines and
simulators. This concept paper outlines the DTDS architectural components and
description ontology that enable digital-twin processes in the modern smart
city.

</details>


### [326] [Optimization for Massive 3D-RIS Deployment: A Generative Diffusion Model-Based Approach](https://arxiv.org/abs/2509.11969)
*Kaining Wang,Bo Yang,Zhiwen Yu,Xuelin Cao,Mérouane Debbah,Chau Yuen*

Main category: cs.NI

TL;DR: 论文利用扩散模型优化3D RIS部署，解决了现有方法的不足，并在性能上超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的RIS部署优化方法存在高计算复杂度、适应性差和易陷入局部最优等问题，亟需一种更高效的解决方案。

Method: 论文提出了一种基于扩散模型的方法，将目标区域划分为固定网格，将RIS部署视为条件生成任务，通过从训练好的扩散模型中采样获得最优部署策略。

Result: 仿真结果表明，所提出的扩散模型方法在超越率和泛化性方面优于传统基准方法。

Conclusion: 该论文通过基于概率生成学习的扩散模型优化大规模3D RIS的部署，显著提高了覆盖性能，并超越了传统基准方法。

Abstract: Reconfigurable Intelligent Surfaces (RISs) transform the wireless environment
by modifying the amplitude, phase, and polarization of incoming waves,
significantly improving coverage performance. Notably, optimizing the
deployment of RISs becomes vital, but existing optimization methods face
challenges such as high computational complexity, limited adaptability to
changing environments, and a tendency to converge on local optima. In this
paper, we propose to optimize the deployment of large-scale 3D RISs using a
diffusion model based on probabilistic generative learning. We begin by
dividing the target area into fixed grids, with each grid corresponding to a
potential deployment location. Then, a multi-RIS deployment optimization
problem is formulated, which is difficult to solve directly. By treating RIS
deployment as a conditional generation task, the well-trained diffusion model
can generate the distribution of deployment strategies, and thus, the optimal
deployment strategy can be obtained by sampling from this distribution.
Simulation results demonstrate that the proposed diffusion-based method
outperforms traditional benchmark approaches in terms of exceed ratio and
generalization.

</details>
