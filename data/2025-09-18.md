<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.NI](#cs.NI) [Total: 9]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 64]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks](https://arxiv.org/abs/2509.13338)
*Hassan Gharoun,Mohammad Sadegh Khorshidi,Kasra Ranjbarigderi,Fang Chen,Amir H. Gandomi*

Main category: cs.CV

TL;DR: 提出证据检索机制，通过融合近端样本预测分布实现透明决策，实验显示其优于固定熵阈值方法。


<details>
  <summary>Details</summary>
Motivation: 旨在提高不确定性感知决策的透明度和可审计性，减少错误预测并优化审查负载。

Method: 提出了一种证据检索机制，通过嵌入空间中检索近端样本，并利用Dempster-Shafer理论融合其预测分布，形成每个实例的阈值机制。

Result: 在CIFAR-10/100数据集上的实验表明，该方法在不确定性感知性能上表现更优或相当，且显著减少了错误预测，同时保持可持续的审查负载。

Conclusion: 证据条件标记为操作不确定性感知决策提供了比固定预测熵阈值更可靠且可解释的替代方案。

Abstract: This work proposes an evidence-retrieval mechanism for uncertainty-aware
decision-making that replaces a single global cutoff with an
evidence-conditioned, instance-adaptive criterion. For each test instance,
proximal exemplars are retrieved in an embedding space; their predictive
distributions are fused via Dempster-Shafer theory. The resulting fused belief
acts as a per-instance thresholding mechanism. Because the supporting evidences
are explicit, decisions are transparent and auditable. Experiments on
CIFAR-10/100 with BiT and ViT backbones show higher or comparable
uncertainty-aware performance with materially fewer confidently incorrect
outcomes and a sustainable review load compared with applying threshold on
prediction entropy. Notably, only a few evidences are sufficient to realize
these gains; increasing the evidence set yields only modest changes. These
results indicate that evidence-conditioned tagging provides a more reliable and
interpretable alternative to fixed prediction entropy thresholds for
operational uncertainty-aware decision-making.

</details>


### [2] [Hybrid Quantum-Classical Model for Image Classification](https://arxiv.org/abs/2509.13353)
*Muhammad Adnan Shahzad*

Main category: cs.CV

TL;DR: 混合量子-经典神经网络在准确性和效率上优于纯经典模型，尤其在复杂数据集上表现突出。


<details>
  <summary>Details</summary>
Motivation: 评估混合量子-经典神经网络与纯经典模型在性能、效率和鲁棒性方面的差异。

Method: 通过将参数化量子电路与经典深度学习架构集成，构建混合模型，与传统的卷积神经网络（CNN）进行对比。

Result: 混合模型在验证准确率（MNIST:99.38%，CIFAR100:41.69%，STL10:74.05%）和训练速度（快5-12倍）上优于经典模型，且在资源利用上更高效。

Conclusion: 混合量子-经典神经网络在准确性、训练效率和参数可扩展性方面表现出显著优势，尤其在复杂视觉任务中。

Abstract: This study presents a systematic comparison between hybrid quantum-classical
neural networks and purely classical models across three benchmark datasets
(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and
robustness. The hybrid models integrate parameterized quantum circuits with
classical deep learning architectures, while the classical counterparts use
conventional convolutional neural networks (CNNs). Experiments were conducted
over 50 training epochs for each dataset, with evaluations on validation
accuracy, test accuracy, training time, computational resource usage, and
adversarial robustness (tested with $\epsilon=0.1$ perturbations).Key findings
demonstrate that hybrid models consistently outperform classical models in
final accuracy, achieving {99.38\% (MNIST), 41.69\% (CIFAR100), and 74.05\%
(STL10) validation accuracy, compared to classical benchmarks of 98.21\%,
32.25\%, and 63.76\%, respectively. Notably, the hybrid advantage scales with
dataset complexity, showing the most significant gains on CIFAR100 (+9.44\%)
and STL10 (+10.29\%). Hybrid models also train 5--12$\times$ faster (e.g.,
21.23s vs. 108.44s per epoch on MNIST) and use 6--32\% fewer parameters} while
maintaining superior generalization to unseen test data.Adversarial robustness
tests reveal that hybrid models are significantly more resilient on simpler
datasets (e.g., 45.27\% robust accuracy on MNIST vs. 10.80\% for classical) but
show comparable fragility on complex datasets like CIFAR100 ($\sim$1\%
robustness for both). Resource efficiency analyses indicate that hybrid models
consume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization
(9.5\% vs. 23.2\% on average).These results suggest that hybrid
quantum-classical architectures offer compelling advantages in accuracy,
training efficiency, and parameter scalability, particularly for complex vision
tasks.

</details>


### [3] [Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention](https://arxiv.org/abs/2509.13361)
*Tong Yulin,Liang Xuechen*

Main category: cs.CV

TL;DR: 研究提出了一种优化算法和模型，显著提高了高速公路拥堵感知和预警的准确性。


<details>
  <summary>Details</summary>
Motivation: 高速公路交通拥堵严重降低了出行效率并阻碍区域连通性，现有系统在遮挡下的车辆感知精度低且长序列依赖关系丢失。

Method: 研究提出了一种综合技术框架，包括优化YOLOv11-DIoU和DeepSort算法用于交通流感知，以及构建GRU-Attention模型用于拥堵预警。

Result: YOLOv11-DIoU的mAP达到95.7%，DeepSort的MOTA达到93.8%；GRU-Attention模型在测试中准确率达到99.7%，预警时间误差≤1分钟。

Conclusion: 该框架为高速公路拥堵控制提供了定量支持，具有广阔的智能交通应用前景。

Abstract: Expressway traffic congestion severely reduces travel efficiency and hinders
regional connectivity. Existing "detection-prediction" systems have critical
flaws: low vehicle perception accuracy under occlusion and loss of
long-sequence dependencies in congestion forecasting. This study proposes an
integrated technical framework to resolve these issues.For traffic flow
perception, two baseline algorithms were optimized. Traditional YOLOv11 was
upgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort
was improved by fusing Mahalanobis (motion) and cosine (appearance) distances.
Experiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\%
mAP (6.5 percentage points higher than baseline) with 5.3\% occlusion miss
rate. DeepSort reached 93.8\% MOTA (11.3 percentage points higher than SORT)
with only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km
high-density scenarios), speed and density showed a strong negative correlation
(r=-0.97), conforming to traffic flow theory. For congestion warning, a
GRU-Attention model was built to capture congestion precursors. Trained 300
epochs with flow, density, and speed, it achieved 99.7\% test accuracy (7-9
percentage points higher than traditional GRU). In 10-minute advance warnings
for 30-minute congestion, time error was $\leq$ 1 minute. Validation with an
independent video showed 95\% warning accuracy, over 90\% spatial overlap of
congestion points, and stable performance in high-flow ($>$5 vehicles/second)
scenarios.This framework provides quantitative support for expressway
congestion control, with promising intelligent transportation applications.

</details>


### [4] [Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks](https://arxiv.org/abs/2509.13366)
*Tony Rohe,Martin Margreiter,Markus Moertl*

Main category: cs.CV

TL;DR: 研究利用机器学习（尤其是卷积神经网络）自动化路边停车服务的测试过程，显著减少人力需求并提升效率。


<details>
  <summary>Details</summary>
Motivation: 优化现有的实时路边停车服务质量，通过自动化替代人工工程工作，提高效率和准确性。

Method: 应用机器学习方法，尤其是图像模式识别和卷积神经网络，以实现测试过程的自动化。

Result: 实现了高达99.58%的人力资源时间减少，提升了自动化水平。

Conclusion: 研究通过应用卷积神经网络实现了高水平的自动化，显著减少了人力资源需求（高达99.58%），并讨论了整体改进及未来发展的潜在应用。

Abstract: This research is part of a study of a real-time, cloud-based on-street
parking service using crowd-sourced in-vehicle fleet data. The service provides
real-time information about available parking spots by classifying
crowd-sourced detections observed via ultrasonic sensors. The goal of this
research is to optimize the current parking service quality by analyzing the
automation of the existing test process for ground truth tests. Therefore,
methods from the field of machine learning, especially image pattern
recognition, are applied to enrich the database and substitute human
engineering work in major areas of the analysis process. After an introduction
into the related areas of machine learning, this paper explains the methods and
implementations made to achieve a high level of automation, applying
convolutional neural networks. Finally, predefined metrics present the
performance level achieved, showing a time reduction of human resources up to
99.58 %. The overall improvements are discussed, summarized, and followed by an
outlook for future development and potential application of the analysis
automation tool.

</details>


### [5] [Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery](https://arxiv.org/abs/2509.13631)
*Yuvraj Dutta,Aaditya Sikder,Basabdatta Palit*

Main category: cs.CV

TL;DR: 论文提出了一种基于联邦学习的分布式森林砍伐识别方法，保护数据隐私的同时提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 传统集中式训练方法需要合并数据，可能危及数据安全，而联邦学习可以在保护数据隐私的同时实现协作训练。

Method: 利用FLOWER框架和RAY框架实现分布式学习，采用YOLOS-small、Faster R-CNN（ResNet50骨干）和Faster R-CNN（MobileNetV3骨干）模型进行训练和测试。

Result: 该方法为卫星图像的图像分割任务提供了新的视角，并通过公开数据集验证了其有效性。

Conclusion: 该论文提出了一种基于联邦学习的分布式方法，用于从卫星图像中准确识别和定位森林砍伐，同时保护用户数据隐私和安全。

Abstract: Accurate identification of deforestation from satellite images is essential
in order to understand the geographical situation of an area. This paper
introduces a new distributed approach to identify as well as locate
deforestation across different clients using Federated Learning (FL). Federated
Learning enables distributed network clients to collaboratively train a model
while maintaining data privacy and security of the active users. In our
framework, a client corresponds to an edge satellite center responsible for
local data processing. Moreover, FL provides an advantage over centralized
training method which requires combining data, thereby compromising with data
security of the clients. Our framework leverages the FLOWER framework with RAY
framework to execute the distributed learning workload. Furthermore, efficient
client spawning is ensured by RAY as it can select definite amount of users to
create an emulation environment. Our FL framework uses YOLOS-small (a Vision
Transformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN
with a MobileNetV3 backbone models trained and tested on publicly available
datasets. Our approach provides us a different view for image
segmentation-based tasks on satellite imagery.

</details>


### [6] [An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity](https://arxiv.org/abs/2509.13375)
*Yuxiao Lee,Xiaofeng Cao,Wei Ye,Jiangchao Yao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CV

TL;DR: 本文系统分析了VLM在零样本OOD检测中的机制、优势及行为鲁棒性，发现其对提示措辞敏感但对图像噪声鲁棒，为未来设计提供了指导。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在零样本OOD检测中表现出色，但对其工作机制、相对于单模态方法的优势以及行为鲁棒性的理解仍不完整。本文旨在填补这一研究空白。

Method: 使用ID和OOD提示对VLM进行系统实证分析，包括机制、优势和敏感性三个方面的研究。

Result: 研究发现：(1) VLM嵌入空间的关键操作特性；(2) VLM在利用丰富语义新颖性方面优于单模态方法；(3) VLM对提示措辞高度敏感，但对图像噪声具有鲁棒性。

Conclusion: 本文通过系统实证分析，揭示了VLM在零样本OOD检测中的关键操作特性、相对于单模态方法的优势，以及其行为鲁棒性的不对称性。这些发现为未来设计更稳健可靠的VLM提供了实证基础。

Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot out-of-distribution (OOD) detection capabilities, vital for reliable
AI systems. Despite this promising capability, a comprehensive understanding of
(1) why they work so effectively, (2) what advantages do they have over
single-modal methods, and (3) how is their behavioral robustness -- remains
notably incomplete within the research community. This paper presents a
systematic empirical analysis of VLM-based OOD detection using in-distribution
(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and
formalize key operational properties within the VLM embedding space that
facilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the
superiority of these models over established single-modal approaches,
attributing this distinct advantage to the VLM's capacity to leverage rich
semantic novelty. (3) Sensitivity: We uncovers a significant and previously
under-explored asymmetry in their robustness profile: while exhibiting
resilience to common image noise, these VLM-based methods are highly sensitive
to prompt phrasing. Our findings contribute a more structured understanding of
the strengths and critical vulnerabilities inherent in VLM-based OOD detection,
offering crucial, empirically-grounded guidance for developing more robust and
reliable future designs.

</details>


### [7] [Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension](https://arxiv.org/abs/2509.13385)
*Charlotte Beylier,Parvaneh Joharinad,Jürgen Jost,Nahid Torbati*

Main category: cs.CV

TL;DR: 提出了一种基于曲率的几何分析方法，用于评估数据表示和估计数据集固有维度。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地评估降维技术等数据表示方法的有效性，并探索数据集的固有维度。

Method: 利用抽象的截面曲率概念，构建离散度量空间的曲率几何轮廓，并基于此设计定量评估指标。

Result: 实验证明，该方法可用于估计数据集的固有维度，并评估降维技术的效果。

Conclusion: 该论文提出了一种基于曲率的几何分析方法，用于评估数据表示的有效性，并探索大规模数据集的几何特性。

Abstract: Utilizing recently developed abstract notions of sectional curvature, we
introduce a method for constructing a curvature-based geometric profile of
discrete metric spaces. The curvature concept that we use here captures the
metric relations between triples of points and other points. More
significantly, based on this curvature profile, we introduce a quantitative
measure to evaluate the effectiveness of data representations, such as those
produced by dimensionality reduction techniques. Furthermore, Our experiments
demonstrate that this curvature-based analysis can be employed to estimate the
intrinsic dimensionality of datasets. We use this to explore the large-scale
geometry of empirical networks and to evaluate the effectiveness of
dimensionality reduction techniques.

</details>


### [8] [Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji](https://arxiv.org/abs/2509.13388)
*Yadvendra Gurjar,Ruoni Wan,Ehsan Farahbakhsh,Rohitash Chandra*

Main category: cs.CV

TL;DR: 使用机器学习和遥感技术分析斐济纳迪2013-2024年土地利用变化，为土地覆盖/土地利用建模提供技术支持。


<details>
  <summary>Details</summary>
Motivation: 斐济作为发展中国家面临快速城市化，需监测土地利用变化以支持发展项目。

Method: 结合Landsat-8卫星影像、监督学习（卷积神经网络）和无监督学习（k-means聚类）生成土地覆盖图。

Result: 通过变化检测可视化展示了城市区域的时序变化。

Conclusion: 研究为土地覆盖/土地利用建模提供了有效技术支持，有助于监测城市化进程。

Abstract: As a developing country, Fiji is facing rapid urbanisation, which is visible
in the massive development projects that include housing, roads, and civil
works. In this study, we present machine learning and remote sensing frameworks
to compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The
ultimate goal of this study is to provide technical support in land cover/land
use modelling and change detection. We used Landsat-8 satellite image for the
study region and created our training dataset with labels for supervised
machine learning. We used Google Earth Engine and unsupervised machine learning
via k-means clustering to generate the land cover map. We used convolutional
neural networks to classify the selected regions' land cover types. We present
a visualisation of change detection, highlighting urban area changes over time
to monitor changes in the map.

</details>


### [9] [Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence](https://arxiv.org/abs/2509.13396)
*Xinan Wang,Di Shi,Fengyu Wang*

Main category: cs.CV

TL;DR: 提出三阶段框架用于实时异物入侵检测与跟踪，结合YOLOv7、ConvNeXt和IoU跟踪器，优化边缘部署，实验验证高效且实用。


<details>
  <summary>Details</summary>
Motivation: 解决电力传输系统中实时异物入侵检测与跟踪的挑战，特别是在遮挡和运动情况下的鲁棒性问题。

Method: 框架整合了YOLOv7分割模型进行快速目标定位，ConvNeXt特征提取器生成判别性嵌入，以及基于特征的IoU跟踪器实现多目标跟踪。

Result: 在真实监控和无人机视频数据集上的实验验证了框架的高准确性和鲁棒性，硬件基准测试证实了其在边缘设备上的实用性。

Conclusion: 该论文提出的三阶段框架在实时异物入侵检测与跟踪中表现出高准确性和鲁棒性，且适用于低成本边缘硬件部署，具有实际应用价值。

Abstract: This paper presents a novel three-stage framework for real-time foreign
object intrusion (FOI) detection and tracking in power transmission systems.
The framework integrates: (1) a YOLOv7 segmentation model for fast and robust
object localization, (2) a ConvNeXt-based feature extractor trained with
triplet loss to generate discriminative embeddings, and (3) a feature-assisted
IoU tracker that ensures resilient multi-object tracking under occlusion and
motion. To enable scalable field deployment, the pipeline is optimized for
deployment on low-cost edge hardware using mixed-precision inference. The
system supports incremental updates by adding embeddings from previously unseen
objects into a reference database without requiring model retraining. Extensive
experiments on real-world surveillance and drone video datasets demonstrate the
framework's high accuracy and robustness across diverse FOI scenarios. In
addition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's
practicality and scalability for real-world edge applications.

</details>


### [10] [EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing](https://arxiv.org/abs/2509.13399)
*Tianyu Chen,Yasi Zhang,Zhi Zhang,Peiyu Yu,Shu Wang,Zhendong Wang,Kevin Lin,Xiaofei Wang,Zhengyuan Yang,Linjie Li,Chung-Ching Lin,Jianwen Xie,Oscar Leong,Lijuan Wang,Ying Nian Wu,Mingyuan Zhou*

Main category: cs.CV

TL;DR: EdiVal-Agent 是一个自动化、可扩展的评估框架，通过结合视觉语言模型和对象检测器，提供更精确的指令遵循性、内容一致性和视觉质量评估。


<details>
  <summary>Details</summary>
Motivation: 当前基于指令的图像编辑评估方法要么依赖有限的配对参考图像（存在覆盖不足和生成模型偏见问题），要么仅依赖零样本视觉语言模型（评估结果不精确）。

Method: EdiVal-Agent 首先将图像分解为语义对象，然后生成多样化的上下文感知编辑指令。评估时，它结合 VLM 和开放词汇对象检测器来评估指令遵循性，使用语义级特征提取器评估内容一致性，并利用人类偏好模型判断视觉质量。

Result: EdiVal-Agent 在指令遵循性评估中比单独使用 VLM 或 CLIP 指标更接近人类判断，其模块化设计支持未来工具的集成。

Conclusion: EdiVal-Agent 通过结合视觉语言模型和对象检测器，提供了比单独使用 VLM 或 CLIP 更接近人类判断的评估结果，其模块化设计还支持未来工具的集成，以持续提升评估准确性。

Abstract: Instruction-based image editing has advanced rapidly, yet reliable and
interpretable evaluation remains a bottleneck. Current protocols either (i)
depend on paired reference images -- resulting in limited coverage and
inheriting biases from prior generative models -- or (ii) rely solely on
zero-shot vision-language models (VLMs), whose prompt-based assessments of
instruction following, content consistency, and visual quality are often
imprecise.
  To address this, we introduce EdiVal-Agent, an automated, scalable, and
fine-grained evaluation framework for multi-turn instruction-based editing from
an object-centric perspective, supported by a suite of expert tools. Given an
image, EdiVal-Agent first decomposes it into semantically meaningful objects,
then synthesizes diverse, context-aware editing instructions. For evaluation,
it integrates VLMs with open-vocabulary object detectors to assess instruction
following, uses semantic-level feature extractors to evaluate content
consistency, and leverages human preference models to judge visual quality. We
show that combining VLMs with object detectors yields stronger agreement with
human judgments in instruction-following evaluation compared to using VLMs
alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows
future tools to be seamlessly integrated, enhancing evaluation accuracy over
time.
  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing
benchmark covering 9 instruction types and 11 state-of-the-art editing models
spanning autoregressive (AR) (including Nano Banana, GPT-Image-1),
flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be
used to identify existing failure modes, thereby informing the development of
the next generation of editing models. Project page:
https://tianyucodings.github.io/EdiVAL-page/.

</details>


### [11] [MapAnything: Universal Feed-Forward Metric 3D Reconstruction](https://arxiv.org/abs/2509.13414)
*Nikhil Keetha,Norman Müller,Johannes Schönberger,Lorenzo Porzi,Yuchen Zhang,Tobias Fischer,Arno Knapitsch,Duncan Zauss,Ethan Weber,Nelson Antunes,Jonathon Luiten,Manuel Lopez-Antequera,Samuel Rota Bulò,Christian Richardt,Deva Ramanan,Sebastian Scherer,Peter Kontschieder*

Main category: cs.CV

TL;DR: MapAnything是一个统一的Transformer前馈模型，输入图像和几何信息，直接回归3D场景和相机参数，在多种任务中表现优异，支持高效联合训练。


<details>
  <summary>Details</summary>
Motivation: 解决多样化3D视觉任务（如未标定SfM、多视角立体视觉、单目深度估计等）需要统一且高效的模型，避免针对每个任务训练专门模型。

Method: MapAnything采用基于Transformer的前馈模型，输入图像及可选几何信息（如相机内参、位姿、深度等），通过分解的多视角场景几何表示（深度图、局部射线图、相机位姿等）直接回归3D场景几何和相机参数。

Result: 实验表明，MapAnything在多个任务上优于或匹配专门模型，同时提供更高效的联合训练能力。

Conclusion: MapAnything通过统一的Transformer前馈模型，在多样化的3D视觉任务中表现优异，甚至超越专门模型，为通用3D重建框架奠定了基础。

Abstract: We introduce MapAnything, a unified transformer-based feed-forward model that
ingests one or more images along with optional geometric inputs such as camera
intrinsics, poses, depth, or partial reconstructions, and then directly
regresses the metric 3D scene geometry and cameras. MapAnything leverages a
factored representation of multi-view scene geometry, i.e., a collection of
depth maps, local ray maps, camera poses, and a metric scale factor that
effectively upgrades local reconstructions into a globally consistent metric
frame. Standardizing the supervision and training across diverse datasets,
along with flexible input augmentation, enables MapAnything to address a broad
range of 3D vision tasks in a single feed-forward pass, including uncalibrated
structure-from-motion, calibrated multi-view stereo, monocular depth
estimation, camera localization, depth completion, and more. We provide
extensive experimental analyses and model ablations demonstrating that
MapAnything outperforms or matches specialist feed-forward models while
offering more efficient joint training behavior, thus paving the way toward a
universal 3D reconstruction backbone.

</details>


### [12] [Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization](https://arxiv.org/abs/2509.13474)
*Yujia Lin,Nicholas Evans*

Main category: cs.CV

TL;DR: SCM-PR框架通过语义增强的跨模态地点识别，结合RGB图像和LiDAR地图，解决了现有方法对环境和视角变化的敏感性问题，并在实验中取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有RGB-based VPR方法对光照、天气和季节变化敏感的问题，以及当前跨模态定位方法在复杂场景、细粒度匹配和视角变化情况下的不足。

Method: 提出了SCM-PR框架，包括VMamba骨干网络用于RGB图像特征提取、SAFF模块用于融合位置描述符和分割掩码、结合语义和几何的LiDAR描述符，以及NetVLAD中的跨模态语义注意力机制。还设计了多视角语义-几何匹配和语义一致性损失。

Result: 在KITTI和KITTI-360数据集上，SCM-PR相比其他跨模态地点识别方法表现出最先进的性能。

Conclusion: SCM-PR框架通过结合高层语义信息，在LiDAR地图中实现了鲁棒的定位，并在KITTI和KITTI-360数据集上达到了最先进的性能。

Abstract: Ensuring accurate localization of robots in environments without GPS
capability is a challenging task. Visual Place Recognition (VPR) techniques can
potentially achieve this goal, but existing RGB-based methods are sensitive to
changes in illumination, weather, and other seasonal changes. Existing
cross-modal localization methods leverage the geometric properties of RGB
images and 3D LiDAR maps to reduce the sensitivity issues highlighted above.
Currently, state-of-the-art methods struggle in complex scenes, fine-grained or
high-resolution matching, and situations where changes can occur in viewpoint.
In this work, we introduce a framework we call Semantic-Enhanced Cross-Modal
Place Recognition (SCM-PR) that combines high-level semantics utilizing RGB
images for robust localization in LiDAR maps. Our proposed method introduces: a
VMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature
Fusion (SAFF) module for using both place descriptors and segmentation masks;
LiDAR descriptors that incorporate both semantics and geometry; and a
cross-modal semantic attention mechanism in NetVLAD to improve matching.
Incorporating the semantic information also was instrumental in designing a
Multi-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in
a contrastive learning framework. Our experimental work on the KITTI and
KITTI-360 datasets show that SCM-PR achieves state-of-the-art performance
compared to other cross-modal place recognition methods.

</details>


### [13] [Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization](https://arxiv.org/abs/2509.13482)
*Hao Xu,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

TL;DR: SALVQ通过场景自适应格向量量化提升3DGS压缩效率，支持多比特率，减少训练开销。


<details>
  <summary>Details</summary>
Motivation: 3DGS生成大量数据，压缩成本高。现有方法依赖USQ，但更复杂的量化器可能在低开销下提升性能。

Method: 采用场景自适应格向量量化（SALVQ）替代均匀标量量化（USQ），优化格基以捕捉场景特性，并动态调整格密度。

Result: SALVQ在现有3DGS压缩架构中无缝集成，显著提升R-D性能，支持多比特率目标，减少训练时间和内存消耗。

Conclusion: SALVQ通过优化场景特定的格基，提升了3DGS压缩的R-D效率和适应性，同时保持了低复杂性和低开销。

Abstract: 3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its
photorealistic rendering quality and real-time performance, but it generates
massive amounts of data. Hence compressing 3DGS data is necessary for the cost
effectiveness of 3DGS models. Recently, several anchor-based neural compression
methods have been proposed, achieving good 3DGS compression performance.
However, they all rely on uniform scalar quantization (USQ) due to its
simplicity. A tantalizing question is whether more sophisticated quantizers can
improve the current 3DGS compression methods with very little extra overhead
and minimal change to the system. The answer is yes by replacing USQ with
lattice vector quantization (LVQ). To better capture scene-specific
characteristics, we optimize the lattice basis for each scene, improving LVQ's
adaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a
balance between the R-D efficiency of vector quantization and the low
complexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS
compression architectures, enhancing their R-D performance with minimal
modifications and computational overhead. Moreover, by scaling the lattice
basis vectors, SALVQ can dynamically adjust lattice density, enabling a single
model to accommodate multiple bit rate targets. This flexibility eliminates the
need to train separate models for different compression levels, significantly
reducing training time and memory consumption.

</details>


### [14] [MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes](https://arxiv.org/abs/2509.13484)
*Liu Liu,Alexandra Kudaeva,Marco Cipriano,Fatimeh Al Ghannam,Freya Tan,Gerard de Melo,Andres Sevtsuk*

Main category: cs.CV

TL;DR: MINGLE框架通过三阶段流程检测公共空间中的群体社交互动，并提供了一个新数据集支持未来研究。


<details>
  <summary>Details</summary>
Motivation: 理解公共空间中的群体社交互动对城市规划至关重要，但传统对象检测难以捕捉复杂的社交信号。

Method: MINGLE是一个模块化三阶段流程，包括现成的人类检测和深度估计、基于VLM的成对社交关系分类，以及轻量级空间聚合算法。

Result: 提出了MINGLE框架和一个包含10万张城市街景图像的新数据集，标注了个人和社交互动群体的边界框和标签。

Conclusion: 该研究通过提出MINGLE框架和新的数据集，为理解和检测公共空间中的群体社交互动提供了有效工具，推动了城市规划和社交互动分析的发展。

Abstract: Understanding group-level social interactions in public spaces is crucial for
urban planning, informing the design of socially vibrant and inclusive
environments. Detecting such interactions from images involves interpreting
subtle visual cues such as relations, proximity, and co-movement - semantically
complex signals that go beyond traditional object detection. To address this
challenge, we introduce a social group region detection task, which requires
inferring and spatially grounding visual regions defined by abstract
interpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level
Engagement), a modular three-stage pipeline that integrates: (1) off-the-shelf
human detection and depth estimation, (2) VLM-based reasoning to classify
pairwise social affiliation, and (3) a lightweight spatial aggregation
algorithm to localize socially connected groups. To support this task and
encourage future research, we present a new dataset of 100K urban street-view
images annotated with bounding boxes and labels for both individuals and
socially interacting groups. The annotations combine human-created labels and
outputs from the MINGLE pipeline, ensuring semantic richness and broad coverage
of real-world scenarios.

</details>


### [15] [BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation](https://arxiv.org/abs/2509.13496)
*Rajatsubhra Chakraborty,Xujun Che,Depeng Xu,Cori Faklaris,Xi Niu,Shuhan Yuan*

Main category: cs.CV

TL;DR: BiasMap是一个模型无关的框架，用于发现稳定扩散模型中的潜在概念级偏差，并提出了一种有效的缓解方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注输出级的人口统计分布，但无法保证概念表示在缓解后解耦。因此，需要一种更深入的方法来揭示和缓解生成模型中的潜在概念级偏差。

Method: BiasMap利用交叉注意力归因图来揭示人口统计特征与语义概念之间的结构纠缠，并通过IoU量化这种纠缠。此外，通过能量引导扩散采样直接修改潜在噪声空间，并在去噪过程中最小化预期的SoftIoU。

Result: 研究发现，现有的公平性干预措施可能减少输出分布差距，但往往无法解耦概念级耦合。而BiasMap的缓解方法可以在图像生成中缓解概念纠缠，并补充分布偏差缓解。

Conclusion: BiasMap提供了一种新的方法来发现和缓解生成模型中的潜在概念级偏差，特别是在稳定扩散模型中。它不仅揭示了现有公平性干预措施的局限性，还提出了一种有效的缓解方法。

Abstract: Bias discovery is critical for black-box generative models, especiall
text-to-image (TTI) models. Existing works predominantly focus on output-level
demographic distributions, which do not necessarily guarantee concept
representations to be disentangled post-mitigation. We propose BiasMap, a
model-agnostic framework for uncovering latent concept-level representational
biases in stable diffusion models. BiasMap leverages cross-attention
attribution maps to reveal structural entanglements between demographics (e.g.,
gender, race) and semantics (e.g., professions), going deeper into
representational bias during the image generation. Using attribution maps of
these concepts, we quantify the spatial demographics-semantics concept
entanglement via Intersection over Union (IoU), offering a lens into bias that
remains hidden in existing fairness discovery approaches. In addition, we
further utilize BiasMap for bias mitigation through energy-guided diffusion
sampling that directly modifies latent noise space and minimizes the expected
SoftIoU during the denoising process. Our findings show that existing fairness
interventions may reduce the output distributional gap but often fail to
disentangle concept-level coupling, whereas our mitigation method can mitigate
concept entanglement in image generation while complementing distributional
bias mitigation.

</details>


### [16] [LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming](https://arxiv.org/abs/2509.13504)
*Uriel Garcilazo-Cruz,Joseph O. Okeme,Rodrigo A. Vargas--Hernández*

Main category: cs.CV

TL;DR: LivePyxel是一个实时图像标注工具，解决了科学领域缺乏灵活标注工具的问题，支持多种成像设备和高效编辑功能。


<details>
  <summary>Details</summary>
Motivation: 现有图像标注软件通常要求用户上传预收集的数据集，这限制了按需流程的支持，并为获取图像引入了不必要的步骤，尤其在实验室环境中实时数据采集日益普遍的情况下，这一问题尤为突出。

Method: LivePyxel集成了成像系统（如网络摄像头、显微镜等），提供简单易用的界面，支持Bézier样条和二进制掩码等工具，并利用OpenCV和高性能库（如Numpy）优化对象检测操作。

Result: LivePyxel实现了与多种视频设备的广泛兼容性，支持高性能编辑的非破坏性图层，为实时图像标注提供了高效解决方案。

Conclusion: LivePyxel是一个基于Python的图形用户界面工具，旨在解决科学领域中缺乏灵活图像标注工具的问题，通过实时图像标注加速AI模型在实验工作流程中的开发。

Abstract: The lack of flexible annotation tools has hindered the deployment of AI
models in some scientific areas. Most existing image annotation software
requires users to upload a precollected dataset, which limits support for
on-demand pipelines and introduces unnecessary steps to acquire images. This
constraint is particularly problematic in laboratory environments, where
real-time data acquisition from instruments such as microscopes is increasingly
common. In this work, we introduce \texttt{LivePixel}, a Python-based graphical
user interface that integrates with imaging systems, such as webcams,
microscopes, and others, to enable real-time image annotation. LivePyxel is
designed to be easy to use through a simple interface that allows users to
precisely delimit areas for annotation using tools commonly found in commercial
graphics editing software. Of particular interest is the availability of
B\'ezier splines and binary masks, and the software's capacity to work with
non-destructive layers that enable high-performance editing. LivePyxel also
integrates a wide compatibility across video devices, and it's optimized for
object detection operations via the use of OpenCV in combination with
high-performance libraries designed to handle matrix and linear algebra
operations via Numpy effectively. LivePyxel facilitates seamless data
collection and labeling, accelerating the development of AI models in
experimental workflows. LivePyxel freely available at
https://github.com/UGarCil/LivePyxel

</details>


### [17] [DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform](https://arxiv.org/abs/2509.13506)
*Xingzi Xu,Qi Li,Shuwen Qiu,Julien Han,Karim Bouyarmane*

Main category: cs.CV

TL;DR: DEFT-VTON通过高效微调和自适应一致性损失，在虚拟试衣任务中实现高性能和低推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决现实应用中有限训练和推理预算的问题，同时保持高质量虚拟试衣能力。

Method: 应用Doob的h-transform高效微调（DEFT）来适应大型预训练无条件模型，用于下游图像条件虚拟试衣能力。DEFT冻结预训练模型的参数，并训练一个小型h-transform网络来学习条件h-transform。此外，提出自适应一致性损失以进一步提高性能并减少推理时间。

Result: DEFT-VTON在虚拟试衣任务中表现优异，仅需15个去噪步骤，且训练参数占比仅为1.42%，显著优于传统参数高效微调方法。

Conclusion: DEFT-VTON方法在虚拟试衣任务中实现了最先进的性能，仅需15个去噪步骤，同时保持竞争力。

Abstract: Diffusion models enable high-quality virtual try-on (VTO) with their
established image synthesis abilities. Despite the extensive end-to-end
training of large pre-trained models involved in current VTO methods,
real-world applications often prioritize limited training and inference,
serving, and deployment budgets for VTO. To solve this obstacle, we apply
Doob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained
unconditional models for downstream image-conditioned VTO abilities. DEFT
freezes the pre-trained model's parameters and trains a small h-transform
network to learn a conditional h-transform. The h-transform network allows
training only 1.42 percent of the frozen parameters, compared to a baseline of
5.52 percent in traditional parameter-efficient fine-tuning (PEFT).
  To further improve DEFT's performance and decrease existing models' inference
time, we additionally propose an adaptive consistency loss. Consistency
training distills slow but high-performing diffusion models into a fast one
while retaining performance by enforcing consistencies along the inference
path. Inspired by constrained optimization, instead of distillation, we combine
the consistency loss and the denoising score matching loss in a data-adaptive
manner for fine-tuning existing VTO models at a low cost. Empirical results
show the proposed DEFT-VTON method achieves state-of-the-art performance on VTO
tasks, with as few as 15 denoising steps, while maintaining competitive
results.

</details>


### [18] [Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving](https://arxiv.org/abs/2509.13507)
*Artem Savkin,Thomas Lapotre,Kevin Strauss,Uzair Akbar,Federico Tombari*

Main category: cs.CV

TL;DR: 该研究通过数据增强和GAN架构生成更真实的虚拟行人数据，显著提升了自动驾驶中的行人识别性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶领域依赖合成数据覆盖特定交通场景，但合成数据与真实数据之间的领域差距影响了行人识别的准确性。

Method: 研究采用了数据增强技术，结合新型生成对抗网络（GAN）架构，用于学习并匹配Cityscapes数据集的照明条件，以生成更真实的虚拟行人数据。

Result: 在语义分割和实例分割任务上的评估表明，该方法有效提升了行人识别的性能。

Conclusion: 通过数据增强和创新的生成网络架构，该研究成功缩小了合成数据与真实数据之间的领域差距，显著提升了行人和VRU的识别性能。

Abstract: In the autonomous driving area synthetic data is crucial for cover specific
traffic scenarios which autonomous vehicle must handle. This data commonly
introduces domain gap between synthetic and real domains. In this paper we
deploy data augmentation to generate custom traffic scenarios with VRUs in
order to improve pedestrian recognition. We provide a pipeline for augmentation
of the Cityscapes dataset with virtual pedestrians. In order to improve
augmentation realism of the pipeline we reveal a novel generative network
architecture for adversarial learning of the data-set lighting conditions. We
also evaluate our approach on the tasks of semantic and instance segmentation.

</details>


### [19] [FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation](https://arxiv.org/abs/2509.13508)
*Maksim Penkin,Andrey Krylov*

Main category: cs.CV

TL;DR: 提出FunKAN和U-FunKAN框架，用于医学图像增强和分割，表现优于现有方法，具有可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习方法在医学图像处理中可解释性差的问题，同时保留图像的空间结构。

Method: 提出了一种基于功能空间和傅里叶分解的FunKAN框架，以及U-FunKAN用于医学图像分割。

Result: 在多个医学数据集上，FunKAN和U-FunKAN在图像增强（PSNR, TV）和分割（IoU, F1）指标上优于其他KAN-based方法。

Conclusion: FunKAN和U-FunKAN在医学图像增强和分割任务中表现优越，为临床应用提供了可解释且鲁棒的解决方案。

Abstract: Medical image enhancement and segmentation are critical yet challenging tasks
in modern clinical practice, constrained by artifacts and complex anatomical
variations. Traditional deep learning approaches often rely on complex
architectures with limited interpretability. While Kolmogorov-Arnold networks
offer interpretable solutions, their reliance on flattened feature
representations fundamentally disrupts the intrinsic spatial structure of
imaging data. To address this issue we propose a Functional Kolmogorov-Arnold
Network (FunKAN) -- a novel interpretable neural framework, designed
specifically for image processing, that formally generalizes the
Kolmogorov-Arnold representation theorem onto functional spaces and learns
inner functions using Fourier decomposition over the basis Hermite functions.
We explore FunKAN on several medical image processing tasks, including Gibbs
ringing suppression in magnetic resonance images, benchmarking on IXI dataset.
We also propose U-FunKAN as state-of-the-art binary medical segmentation model
with benchmarks on three medical datasets: BUSI (ultrasound images), GlaS
(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting
breast cancer, glands and polyps, respectively. Experiments on those diverse
datasets demonstrate that our approach outperforms other KAN-based backbones in
both medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work
bridges the gap between theoretical function approximation and medical image
analysis, offering a robust, interpretable solution for clinical applications.

</details>


### [20] [Multimodal Hate Detection Using Dual-Stream Graph Neural Networks](https://arxiv.org/abs/2509.13515)
*Jiangbei Yue,Shuonan Yang,Tailin Chen,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 该论文提出了一种多模态双流图神经网络模型，通过实例级特征提取和重要性权重分配，显著提升了仇恨视频分类的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽视仇恨内容的最小化定义，且无法系统捕获视频中的结构化信息，限制了多模态融合的效果。

Method: 模型通过构建实例图和补充权重图，提取实例级特征并分配重要性权重，结合图框架系统建模多模态间的结构化关系。

Result: 在公开数据集上的大量实验表明，该模型在仇恨视频分类中达到了最先进的性能，并具有较强的可解释性。

Conclusion: 该论文提出了一种新颖的多模态双流图神经网络模型，通过分离视频实例并强调仇恨内容，显著提升了仇恨视频分类的准确性和可解释性。

Abstract: Hateful videos present serious risks to online safety and real-world
well-being, necessitating effective detection methods. Although multimodal
classification approaches integrating information from several modalities
outperform unimodal ones, they typically neglect that even minimal hateful
content defines a video's category. Specifically, they generally treat all
content uniformly, instead of emphasizing the hateful components. Additionally,
existing multimodal methods cannot systematically capture structured
information in videos, limiting the effectiveness of multimodal fusion. To
address these limitations, we propose a novel multimodal dual-stream graph
neural network model. It constructs an instance graph by separating the given
video into several instances to extract instance-level features. Then, a
complementary weight graph assigns importance weights to these features,
highlighting hateful instances. Importance weights and instance features are
combined to generate video labels. Our model employs a graph-based framework to
systematically model structured relationships within and across modalities.
Extensive experiments on public datasets show that our model is
state-of-the-art in hateful video classification and has strong explainability.
Code is available:
https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.

</details>


### [21] [ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors](https://arxiv.org/abs/2509.13525)
*Romain Hardy,Tyler Berzin,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: ColonCrafter是一种基于扩散的深度估计模型，通过合成数据学习几何先验，生成时间一致的深度图，并在临床应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 结肠镜中的三维场景理解面临挑战，现有深度估计模型在视频序列中缺乏时间一致性，限制了其在3D重建中的应用。

Method: ColonCrafter是一种基于扩散的深度估计模型，通过学习合成结肠镜序列中的几何先验，生成时间一致的深度图，并采用风格迁移技术将真实临床视频适配到合成训练域。

Result: ColonCrafter在C3VD数据集上实现了最先进的零样本性能，优于通用和结肠镜专用方法。

Conclusion: 尽管完整的3D轨迹重建仍具挑战性，但ColonCrafter在3D点云生成和表面覆盖评估等临床应用方面展现了潜力。

Abstract: Three-dimensional (3D) scene understanding in colonoscopy presents
significant challenges that necessitate automated methods for accurate depth
estimation. However, existing depth estimation models for endoscopy struggle
with temporal consistency across video sequences, limiting their applicability
for 3D reconstruction. We present ColonCrafter, a diffusion-based depth
estimation model that generates temporally consistent depth maps from monocular
colonoscopy videos. Our approach learns robust geometric priors from synthetic
colonoscopy sequences to generate temporally consistent depth maps. We also
introduce a style transfer technique that preserves geometric structure while
adapting real clinical videos to match our synthetic training domain.
ColonCrafter achieves state-of-the-art zero-shot performance on the C3VD
dataset, outperforming both general-purpose and endoscopy-specific approaches.
Although full trajectory 3D reconstruction remains a challenge, we demonstrate
clinically relevant applications of ColonCrafter, including 3D point cloud
generation and surface coverage assessment.

</details>


### [22] [MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM](https://arxiv.org/abs/2509.13536)
*Yinlong Bai,Hongxin Zhang,Sheng Zhong,Junkai Niu,Hai Li,Yijia He,Yi Zhou*

Main category: cs.CV

TL;DR: 该论文优化了3D高斯喷射技术，减少GPU内存使用并提升渲染质量，特别适用于资源受限的嵌入式平台。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注高性能桌面GPU的渲染和重建质量，忽视了嵌入式平台（如MAV）的资源限制问题。

Method: 提出基于几何相似性在体素空间中合并冗余的3D高斯基元，并通过Patch-Grid（PG）点采样初始化3D高斯基元，以更精确地建模整个场景。

Result: 在公开数据集上的定量和定性评估证明了所提改进的有效性。

Conclusion: 该论文通过优化3D高斯喷射技术，显著降低了GPU内存使用并提升了渲染质量，特别是在嵌入式平台如微型飞行器（MAV）上表现优异。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant
impact on rendering and reconstruction techniques. Current research
predominantly focuses on improving rendering performance and reconstruction
quality using high-performance desktop GPUs, largely overlooking applications
for embedded platforms like micro air vehicles (MAVs). These devices, with
their limited computational resources and memory, often face a trade-off
between system performance and reconstruction quality. In this paper, we
improve existing methods in terms of GPU memory usage while enhancing rendering
quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we
propose merging them in voxel space based on geometric similarity. This reduces
GPU memory usage without impacting system runtime performance. Furthermore,
rendering quality is improved by initializing 3D Gaussian primitives via
Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire
scene. Quantitative and qualitative evaluations on publicly available datasets
demonstrate the effectiveness of our improvements.

</details>


### [23] [Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles](https://arxiv.org/abs/2509.13577)
*Tongfei Guo,Lili Su*

Main category: cs.CV

TL;DR: 论文提出了一种自适应框架，用于轨迹级OOD检测，显著提升了性能，为自动驾驶提供了更可靠的解决方案。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在部署中面临训练数据与现实条件之间的分布偏移问题，轨迹级OOD检测研究不足，需要解决检测延迟与误报率之间的权衡。

Method: 通过明确建模预测误差的模式依赖性分布，并引入自适应机制，实现了在复杂驾驶环境中的稳健检测。

Result: 在多个真实世界数据集上的实验表明，该方法在检测延迟和误报率上均显著优于现有UQ和基于视觉的OOD方法。

Conclusion: 论文提出了一种新的自适应机制框架，用于复杂驾驶环境中轨迹级OOD检测，显著提高了检测延迟和误报率，为可靠的自动驾驶提供了实用路径。

Abstract: Trajectory prediction is central to the safe and seamless operation of
autonomous vehicles (AVs). In deployment, however, prediction models inevitably
face distribution shifts between training data and real-world conditions, where
rare or underrepresented traffic scenarios induce out-of-distribution (OOD)
cases. While most prior OOD detection research in AVs has concentrated on
computer vision tasks such as object detection and segmentation,
trajectory-level OOD detection remains largely underexplored. A recent study
formulated this problem as a quickest change detection (QCD) task, providing
formal guarantees on the trade-off between detection delay and false alarms
[1]. Building on this foundation, we propose a new framework that introduces
adaptive mechanisms to achieve robust detection in complex driving
environments. Empirical analysis across multiple real-world datasets reveals
that prediction errors -- even on in-distribution samples -- exhibit
mode-dependent distributions that evolve over time with dataset-specific
dynamics. By explicitly modeling these error modes, our method achieves
substantial improvements in both detection delay and false alarm rates.
Comprehensive experiments on established trajectory prediction benchmarks show
that our framework significantly outperforms prior UQ- and vision-based OOD
approaches in both accuracy and computational efficiency, offering a practical
path toward reliable, driving-aware autonomy.

</details>


### [24] [Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection](https://arxiv.org/abs/2509.13586)
*Nathalie Neptune,Josiane Mothe*

Main category: cs.CV

TL;DR: 利用深度学习和视觉语义模型检测亚马逊森林砍伐，并提供自动标注，方法通用性强。


<details>
  <summary>Details</summary>
Motivation: 亚马逊雨林对地球气候和生物多样性至关重要，其砍伐对全球碳排放和生物多样性有重大影响，因此需要有效监测方法。

Method: 利用深度学习技术比较不同日期的地球观测卫星图像对，识别森林覆盖变化，并提出视觉语义模型自动标注检测到的变化。

Result: 在亚马逊图像对数据集上评估，证明该方法在检测森林砍伐和生成相关标注方面有效。

Conclusion: 该方法为监测和研究亚马逊森林砍伐影响提供了有效工具，且具有通用性，可应用于其他领域。

Abstract: The Amazon rain forest is a vital ecosystem that plays a crucial role in
regulating the Earth's climate and providing habitat for countless species.
Deforestation in the Amazon is a major concern as it has a significant impact
on global carbon emissions and biodiversity. In this paper, we present a method
for detecting deforestation in the Amazon using image pairs from Earth
observation satellites. Our method leverages deep learning techniques to
compare the images of the same area at different dates and identify changes in
the forest cover. We also propose a visual semantic model that automatically
annotates the detected changes with relevant keywords. The candidate annotation
for images are extracted from scientific documents related to the Amazon
region. We evaluate our approach on a dataset of Amazon image pairs and
demonstrate its effectiveness in detecting deforestation and generating
relevant annotations. Our method provides a useful tool for monitoring and
studying the impact of deforestation in the Amazon. While we focus on
environment applications of our work by using images of deforestation in the
Amazon rain forest to demonstrate the effectiveness of our proposed approach,
it is generic enough to be applied to other domains.

</details>


### [25] [Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation](https://arxiv.org/abs/2509.13590)
*Samer Al-Hamadani*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Vision-Language Models的智能多模态框架，用于医疗影像分析，结合视觉和语言处理技术，显著提升了诊断效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医疗影像中的快速发展为诊断医学和临床决策过程带来了革命性的变化。

Method: 该研究提出了一种智能多模态框架，结合了视觉特征提取和自然语言处理，利用Google Gemini 2.5 Flash进行自动肿瘤检测和临床报告生成。

Result: 实验评估显示，该系统在多种影像模态中的异常检测表现优异，位置测量的平均偏差为80像素。

Conclusion: 该框架在自动诊断支持和放射学工作流程效率方面取得了显著进展，但在广泛采用之前需要进行临床验证和多中心评估。

Abstract: The rapid advancement of artificial intelligence (AI) in healthcare imaging
has revolutionized diagnostic medicine and clinical decision-making processes.
This work presents an intelligent multimodal framework for medical image
analysis that leverages Vision-Language Models (VLMs) in healthcare
diagnostics. The framework integrates Google Gemini 2.5 Flash for automated
tumor detection and clinical report generation across multiple imaging
modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual
feature extraction with natural language processing to enable contextual image
interpretation, incorporating coordinate verification mechanisms and
probabilistic Gaussian modeling for anomaly distribution. Multi-layered
visualization techniques generate detailed medical illustrations, overlay
comparisons, and statistical representations to enhance clinical confidence,
with location measurement achieving 80 pixels average deviation. Result
processing utilizes precise prompt engineering and textual analysis to extract
structured clinical information while maintaining interpretability.
Experimental evaluations demonstrated high performance in anomaly detection
across multiple modalities. The system features a user-friendly Gradio
interface for clinical workflow integration and demonstrates zero-shot learning
capabilities to reduce dependence on large datasets. This framework represents
a significant advancement in automated diagnostic support and radiological
workflow efficiency, though clinical validation and multi-center evaluation are
necessary prior to widespread adoption.

</details>


### [26] [A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms](https://arxiv.org/abs/2509.13605)
*Ruochen Hou,Gabriel I. Fernandez,Alex Xu,Dennis W. Hong*

Main category: cs.CV

TL;DR: CLAP算法从2D定位扩展到3D定位和图像拼接，展示了其广泛适用性及与其他技术的关系。


<details>
  <summary>Details</summary>
Motivation: 扩展CLAP算法至更通用的框架，以应对3D定位和图像拼接等更广泛的应用场景。

Method: 通过聚类抑制噪声和错误特征匹配，CLAP提供了一种替代传统异常值拒绝方案（如RANSAC）的策略。

Result: 成功将CLAP从2D定位扩展到3D定位和图像拼接，并展示了其与RANSAC和Hough变换的关系。

Conclusion: CLAP算法的扩展框架在3D定位和图像拼接中表现出广泛适用性，成为处理噪声和不确定性的有用工具。

Abstract: In previous work, we introduced a 2D localization algorithm called CLAP,
Clustering to Localize Across $n$ Possibilities, which was used during our
championship win in RoboCup 2024, an international autonomous humanoid soccer
competition. CLAP is particularly recognized for its robustness against
outliers, where clustering is employed to suppress noise and mitigate against
erroneous feature matches. This clustering-based strategy provides an
alternative to traditional outlier rejection schemes such as RANSAC, in which
candidates are validated by reprojection error across all data points. In this
paper, CLAP is extended to a more general framework beyond 2D localization,
specifically to 3D localization and image stitching. We also show how CLAP,
RANSAC, and Hough transforms are related. The generalization of CLAP is widely
applicable to many different fields and can be a useful tool to deal with noise
and uncertainty.

</details>


### [27] [SAMIR, an efficient registration framework via robust feature learning from SAM](https://arxiv.org/abs/2509.13629)
*Yue He,Min Liu,Qinghao Liu,Jiazheng Wang,Yaonan Wang,Hang Zhang,Xiang Chen*

Main category: cs.CV

TL;DR: SAMIR利用Segment Anything Model（SAM）增强医学图像配准，通过结构感知特征提取和分层一致性损失，显著提升了配准精度。


<details>
  <summary>Details</summary>
Motivation: 现有的弱监督方法依赖于难以获取的解剖先验（如分割掩码或标志点），限制了其实际应用。SAMIR利用视觉基础模型的强大表示学习能力，克服了这一限制。

Method: 设计了任务特定的适应流程，利用SAM的图像编码器提取结构感知特征嵌入，并引入轻量级3D头部和分层特征一致性损失来优化特征匹配和解剖对齐。

Result: 在心脏图像和腹部CT图像的配准任务中，SAMIR分别实现了2.68%和6.44%的性能提升。

Conclusion: SAMIR框架通过结合Segment Anything Model（SAM）的强大表示学习能力，显著提升了医学图像配准的准确性，在多个基准数据集上超越了现有最先进方法。

Abstract: Image registration is a fundamental task in medical image analysis.
Deformations are often closely related to the morphological characteristics of
tissues, making accurate feature extraction crucial. Recent weakly supervised
methods improve registration by incorporating anatomical priors such as
segmentation masks or landmarks, either as inputs or in the loss function.
However, such weak labels are often not readily available, limiting their
practical use. Motivated by the strong representation learning ability of
visual foundation models, this paper introduces SAMIR, an efficient medical
image registration framework that utilizes the Segment Anything Model (SAM) to
enhance feature extraction. SAM is pretrained on large-scale natural image
datasets and can learn robust, general-purpose visual representations. Rather
than using raw input images, we design a task-specific adaptation pipeline
using SAM's image encoder to extract structure-aware feature embeddings,
enabling more accurate modeling of anatomical consistency and deformation
patterns. We further design a lightweight 3D head to refine features within the
embedding space, adapting to local deformations in medical images.
Additionally, we introduce a Hierarchical Feature Consistency Loss to guide
coarse-to-fine feature matching and improve anatomical alignment. Extensive
experiments demonstrate that SAMIR significantly outperforms state-of-the-art
methods on benchmark datasets for both intra-subject cardiac image registration
and inter-subject abdomen CT image registration, achieving performance
improvements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code
will be publicly available on GitHub following the acceptance of this paper.

</details>


### [28] [Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction](https://arxiv.org/abs/2509.13652)
*Yumin Li,Dylan Campbell*

Main category: cs.CV

TL;DR: GARPS是一种无需训练的框架，通过直接对齐独立重建的3D场景实现度量相对位姿估计，在宽基线和复杂表面上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统两视角位姿估计方法无法提供度量尺度，且在宽基线、无纹理或反射表面上表现不佳。GARPS旨在解决这些问题。

Method: GARPS利用度量单目深度估计器和高斯场景重建器，生成每张图像的度量3D高斯混合模型（GMM），并通过优化可微分的GMM对齐目标来改进初始位姿。

Result: 在Real-Estate10K数据集上的实验表明，GARPS优于传统和最先进的学习方法，包括MASt3R。

Conclusion: GARPS通过结合单视角感知与多视角几何，实现了鲁棒且度量的相对位姿估计，展示了其在3D重建和定位中的潜力。

Abstract: Estimating metric relative camera pose from a pair of images is of great
importance for 3D reconstruction and localisation. However, conventional
two-view pose estimation methods are not metric, with camera translation known
only up to a scale, and struggle with wide baselines and textureless or
reflective surfaces. This paper introduces GARPS, a training-free framework
that casts this problem as the direct alignment of two independently
reconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and
a Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model
(GMM) for each image. It then refines an initial pose from a feed-forward
two-view pose estimator by optimising a differentiable GMM alignment objective.
This objective jointly considers geometric structure, view-independent colour,
anisotropic covariance, and semantic feature consistency, and is robust to
occlusions and texture-poor regions without requiring explicit 2D
correspondences. Extensive experiments on the Real\-Estate10K dataset
demonstrate that GARPS outperforms both classical and state-of-the-art
learning-based methods, including MASt3R. These results highlight the potential
of bridging single-view perception with multi-view geometry to achieve robust
and metric relative pose estimation.

</details>


### [29] [Deep Lookup Network](https://arxiv.org/abs/2509.13662)
*Yulan Guo,Longguang Wang,Wendong Mao,Xiaoyu Dong,Yingqian Wang,Li Liu,Wei An*

Main category: cs.CV

TL;DR: 论文提出用查找操作替代乘法操作，构建高效查找网络，在保持性能的同时提升移动设备上的部署效率。


<details>
  <summary>Details</summary>
Motivation: 乘法操作在卷积神经网络中计算复杂度高，能耗大，阻碍了在移动设备上的部署。查找表可以降低计算成本，因此论文探索了用查找操作替代乘法操作的可行性。

Method: 论文提出了一种通用的、高效的查找操作，并通过可微查找表实现端到端优化，同时提出了多种训练策略以促进收敛。

Result: 实验表明，查找网络在能耗和推理速度上效率更高，同时在图像分类、超分辨率和点云分类等任务上保持了与传统网络相当的性能。

Conclusion: 该论文通过引入查找操作替代传统乘法操作，显著提升了卷积神经网络在移动设备上的部署效率，同时保持了与传统网络相当的性能。

Abstract: Convolutional neural networks are constructed with massive operations with
different types and are highly computationally intensive. Among these
operations, multiplication operation is higher in computational complexity and
usually requires {more} energy consumption with longer inference time than
other operations, which hinders the deployment of convolutional neural networks
on mobile devices. In many resource-limited edge devices, complicated
operations can be calculated via lookup tables to reduce computational cost.
Motivated by this, in this paper, we introduce a generic and efficient lookup
operation which can be used as a basic operation for the construction of neural
networks. Instead of calculating the multiplication of weights and activation
values, simple yet efficient lookup operations are adopted to compute their
responses. To enable end-to-end optimization of the lookup operation, we
construct the lookup tables in a differentiable manner and propose several
training strategies to promote their convergence. By replacing computationally
expensive multiplication operations with our lookup operations, we develop
lookup networks for the image classification, image super-resolution, and point
cloud classification tasks. It is demonstrated that our lookup networks can
benefit from the lookup operations to achieve higher efficiency in terms of
energy consumption and inference speed while maintaining competitive
performance to vanilla convolutional networks. Extensive experiments show that
our lookup networks produce state-of-the-art performance on different tasks
(both classification and regression tasks) and different data types (both
images and point clouds).

</details>


### [30] [Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation](https://arxiv.org/abs/2509.13676)
*Xiaobo Yang,Xiaojin Gong*

Main category: cs.CV

TL;DR: 通过语义超像素压缩视觉令牌，新方法在保持性能的同时减少93%令牌，提升效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于补丁的视觉投影器在减少视觉令牌数量和保持语义清晰度之间难以平衡，导致计算开销大。

Method: 提出了一种语义视觉投影器，利用SAM生成的语义超像素识别图像中的“视觉词”，并通过语义超像素位置嵌入和聚合器保留细节和全局上下文。

Result: 实验表明，该方法在保持性能的同时大幅减少了视觉令牌数量，显著提升了效率，并在RIS任务上优于现有压缩视觉投影器。

Conclusion: 提出的语义视觉投影器通过利用SAM生成的语义超像素，显著减少了视觉令牌数量（减少93%）而不影响性能，同时提升了MLLM的训练和推理速度。

Abstract: Recently, Referring Image Segmentation (RIS) frameworks that pair the
Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)
have achieved impressive results. However, adapting MLLM to segmentation is
computationally intensive, primarily due to visual token redundancy. We observe
that traditional patch-wise visual projectors struggle to strike a balance
between reducing the number of visual tokens and preserving semantic clarity,
often retaining overly long token sequences to avoid performance drops.
Inspired by text tokenizers, we propose a novel semantic visual projector that
leverages semantic superpixels generated by SAM to identify "visual words" in
an image. By compressing and projecting semantic superpixels as visual tokens,
our approach adaptively shortens the token sequence according to scene
complexity while minimizing semantic loss in compression. To mitigate loss of
information, we propose a semantic superpixel positional embedding to
strengthen MLLM's awareness of superpixel geometry and position, alongside a
semantic superpixel aggregator to preserve both fine-grained details inside
superpixels and global context outside. Experiments show that our method cuts
visual tokens by 93% without compromising performance, notably speeding up MLLM
training and inference, and outperforming existing compressive visual
projectors on RIS.

</details>


### [31] [FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras](https://arxiv.org/abs/2509.13681)
*Hang Li,Dianmo Sheng,Qiankun Dong,Zichun Wang,Zhiwei Xu,Tao Li*

Main category: cs.CV

TL;DR: FishBEV是为鱼眼相机设计的BEV分割框架，通过DRME、U-SCA和D-TSA三个创新模块解决了失真、跨视图对齐和时间一致性等问题，在Synwoodscapes数据集上表现优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 鱼眼相机在BEV分割中存在严重几何失真、多视图对应模糊和时间动态不稳定等问题，导致性能下降。FishBEV旨在解决这些挑战。

Method: FishBEV框架包含三个关键创新：1) DRME骨干网络，用于在失真情况下学习鲁棒特征并保持尺度一致性；2) U-SCA机制，利用不确定性估计实现可靠的跨视图对齐；3) D-TSA模块，自适应平衡近场细节和远场上下文以确保时间一致性。

Result: 在Synwoodscapes数据集上的实验表明，FishBEV在环绕视图鱼眼BEV分割任务中表现优于现有最佳基线。

Conclusion: FishBEV通过引入DRME、U-SCA和D-TSA三个创新模块，显著提升了鱼眼相机在BEV分割任务中的性能，并在Synwoodscapes数据集上超越了现有最佳基线。

Abstract: As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)
segmentation has recently achieved remarkable progress with pinhole cameras.
However, it is non-trivial to extend the existing methods to fisheye cameras
with severe geometric distortion, ambiguous multi-view correspondences and
unstable temporal dynamics, all of which significantly degrade BEV performance.
To address these challenges, we propose FishBEV, a novel BEV segmentation
framework specifically tailored for fisheye cameras. This framework introduces
three complementary innovations, including a Distortion-Resilient Multi-scale
Extraction (DRME) backbone that learns robust features under distortion while
preserving scale consistency, an Uncertainty-aware Spatial Cross-Attention
(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view
alignment, a Distance-aware Temporal Self-Attention (D-TSA) module that
adaptively balances near field details and far field context to ensure temporal
coherence. Extensive experiments on the Synwoodscapes dataset demonstrate that
FishBEV consistently outperforms SOTA baselines, regarding the performance
evaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.

</details>


### [32] [Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification](https://arxiv.org/abs/2509.13687)
*Kaniz Fatema,Emad A. Mohammed,Sukhjit Singh Sehra*

Main category: cs.CV

TL;DR: 该论文提出了一种基于样条的轻量级医学图像分类方法，在小样本和资源受限环境下表现优异，兼具高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在资源有限的临床环境中，实现高效且可解释的医学图像分类是一个挑战。本研究旨在开发一种适用于小样本和多样化数据集的轻量级模型。

Method: 研究引入了基于样条的Kolmogorov-Arnold Networks（KANs），包括SBTAYLOR-KAN、SBRBF-KAN和SBWAVELET-KAN，利用样条基函数逼近技术捕捉局部和全局非线性特征。

Result: SBTAYLOR-KAN在多个数据集上表现优异，最高准确率达98.93%，且仅需2,872个可训练参数，显著优于传统CNN模型（如ResNet50）。

Conclusion: 该研究提出了一种轻量级、可解释且泛化能力强的医学图像分类框架，特别适用于资源有限的临床环境，解决了数据稀缺和小样本的挑战。

Abstract: Effective and interpretable classification of medical images is a challenge
in computer-aided diagnosis, especially in resource-limited clinical settings.
This study introduces spline-based Kolmogorov-Arnold Networks (KANs) for
accurate medical image classification with limited, diverse datasets. The
models include SBTAYLOR-KAN, integrating B-splines with Taylor series;
SBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,
embedding B-splines in Morlet wavelet transforms. These approaches leverage
spline-based function approximation to capture both local and global
nonlinearities. The models were evaluated on brain MRI, chest X-rays,
tuberculosis X-rays, and skin lesion images without preprocessing,
demonstrating the ability to learn directly from raw data. Extensive
experiments, including cross-dataset validation and data reduction analysis,
showed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%
accuracy, with a balanced F1-score, maintaining over 86% accuracy using only
30% of the training data across three datasets. Despite class imbalance in the
skin cancer dataset, experiments on both imbalanced and balanced versions
showed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.
Unlike traditional CNNs, which require millions of parameters (e.g., ResNet50
with 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872
trainable parameters, making it more suitable for constrained medical
environments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used
for interpretability, highlighting relevant regions in medical images. This
framework provides a lightweight, interpretable, and generalizable solution for
medical image classification, addressing the challenges of limited datasets and
data-scarce scenarios in clinical AI applications.

</details>


### [33] [StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models](https://arxiv.org/abs/2509.13711)
*Qiuyu Tang,Joshua Krinsky,Aparna Bharati*

Main category: cs.CV

TL;DR: StyleProtect通过更新选定的交叉注意力层，有效防御细调扩散模型对艺术风格的模仿，保护艺术品免受滥用。


<details>
  <summary>Details</summary>
Motivation: 生成模型（尤其是基于扩散的方法）的快速发展可能被滥用，模仿艺术家的独特风格，侵犯其创意劳动和个人愿景，因此需要探索保护艺术品免受风格模仿的方法。

Method: 基于对交叉注意力层对艺术风格敏感性的测量，提出StyleProtect策略，通过更新选定交叉注意力层来实现风格防御。

Result: 实验表明，StyleProtect在保护独特艺术风格和动漫免受恶意扩散定制方面表现出色，同时保持不可察觉性。

Conclusion: StyleProtect是一种高效且轻量级的保护策略，通过仅更新选定的交叉注意力层，有效防御针对细调扩散模型的风格模仿，同时保持竞争性的不可察觉性。

Abstract: The rapid advancement of generative models, particularly diffusion-based
approaches, has inadvertently facilitated their potential for misuse. Such
models enable malicious exploiters to replicate artistic styles that capture an
artist's creative labor, personal vision, and years of dedication in an
inexpensive manner. This has led to a rise in the need and exploration of
methods for protecting artworks against style mimicry. Although generic
diffusion models can easily mimic an artistic style, finetuning amplifies this
capability, enabling the model to internalize and reproduce the style with
higher fidelity and control. We hypothesize that certain cross-attention layers
exhibit heightened sensitivity to artistic styles. Sensitivity is measured
through activation strengths of attention layers in response to style and
content representations, and assessing their correlations with features
extracted from external models. Based on our findings, we introduce an
efficient and lightweight protection strategy, StyleProtect, that achieves
effective style defense against fine-tuned diffusion models by updating only
selected cross-attention layers. Our experiments utilize a carefully curated
artwork dataset based on WikiArt, comprising representative works from 30
artists known for their distinctive and influential styles and cartoon
animations from the Anita dataset. The proposed method demonstrates promising
performance in safeguarding unique styles of artworks and anime from malicious
diffusion customization, while maintaining competitive imperceptibility.

</details>


### [34] [UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry](https://arxiv.org/abs/2509.13713)
*Tae-Wook Um,Ki-Hyeon Kim,Hyun-Duck Choi,Hyo-Sung Ahn*

Main category: cs.CV

TL;DR: UM-Depth 是一种自监督单目深度估计框架，通过运动和不确定性感知的细化，显著提升了动态物体边界和无纹理区域的深度估计精度，无需额外标签或推理开销。


<details>
  <summary>Details</summary>
Motivation: 自监督单目深度估计方法在低纹理或动态区域存在不确定性，导致深度精度下降。UM-Depth 旨在通过运动和不确定性感知的细化来解决这一问题。

Method: UM-Depth 采用了教师-学生训练策略，将不确定性估计嵌入训练流程和网络架构中，同时仅在教师网络训练阶段使用光流，避免了推理时的额外开销和标签需求。

Result: 在 KITTI 和 Cityscapes 数据集上的大量实验表明，UM-Depth 在动态物体边界和无纹理区域的深度估计精度显著提升。

Conclusion: UM-Depth 在 KITTI 数据集上实现了自监督深度和姿态估计的最先进结果，通过结合运动和不确定性感知的细化，显著提升了动态物体边界和无纹理区域的深度估计精度。

Abstract: Monocular depth estimation has been increasingly adopted in robotics and
autonomous driving for its ability to infer scene geometry from a single
camera. In self-supervised monocular depth estimation frameworks, the network
jointly generates and exploits depth and pose estimates during training,
thereby eliminating the need for depth labels. However, these methods remain
challenged by uncertainty in the input data, such as low-texture or dynamic
regions, which can cause reduced depth accuracy. To address this, we introduce
UM-Depth, a framework that combines motion- and uncertainty-aware refinement to
enhance depth accuracy at dynamic object boundaries and in textureless regions.
Specifically, we develop a teacherstudent training strategy that embeds
uncertainty estimation into both the training pipeline and network
architecture, thereby strengthening supervision where photometric signals are
weak. Unlike prior motion-aware approaches that incur inference-time overhead
and rely on additional labels or auxiliary networks for real-time generation,
our method uses optical flow exclusively within the teacher network during
training, which eliminating extra labeling demands and any runtime cost.
Extensive experiments on the KITTI and Cityscapes datasets demonstrate the
effectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves
state-of-the-art results in both self-supervised depth and pose estimation on
the KITTI datasets.

</details>


### [35] [Mitigating Query Selection Bias in Referring Video Object Segmentation](https://arxiv.org/abs/2509.13722)
*Dingwei Zhang,Dong Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: TQF通过分解查询为三个组件并引入运动感知模块，解决了RVOS中静态查询的偏差问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于查询的方法在RVOS中表现良好，但静态查询容易受到外观或运动相似的干扰项误导，导致查询选择偏差。为了解决这一问题，作者提出了TQF。

Method: TQF将查询分解为外观查询、帧内交互查询和帧间运动查询，并设计了两种运动感知聚合模块（Intra-frame Interaction Aggregation和Inter-frame Motion Aggregation）来增强对象标记表示。

Result: 在多个RVOS基准测试上的广泛实验证明了TQF的优势及其结构化查询设计和运动感知聚合模块的有效性。

Conclusion: 论文提出的Triple Query Former (TQF)通过将查询分解为三个专门组件，并引入运动感知聚合模块，显著提升了Referring Video Object Segmentation (RVOS)的性能。

Abstract: Recently, query-based methods have achieved remarkable performance in
Referring Video Object Segmentation (RVOS) by using textual static object
queries to drive cross-modal alignment. However, these static queries are
easily misled by distractors with similar appearance or motion, resulting in
\emph{query selection bias}. To address this issue, we propose Triple Query
Former (TQF), which factorizes the referring query into three specialized
components: an appearance query for static attributes, an intra-frame
interaction query for spatial relations, and an inter-frame motion query for
temporal association. Instead of relying solely on textual embeddings, our
queries are dynamically constructed by integrating both linguistic cues and
visual guidance. Furthermore, we introduce two motion-aware aggregation modules
that enhance object token representations: Intra-frame Interaction Aggregation
incorporates position-aware interactions among objects within a single frame,
while Inter-frame Motion Aggregation leverages trajectory-guided alignment
across frames to ensure temporal coherence. Extensive experiments on multiple
RVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our
structured query design and motion-aware aggregation modules.

</details>


### [36] [Improving Generalized Visual Grounding with Instance-aware Joint Learning](https://arxiv.org/abs/2509.13747)
*Ming Dai,Wenxuan Cheng,Jiang-Jiang Liu,Lingfeng Yang,Zhenhua Feng,Wankou Yang,Jingdong Wang*

Main category: cs.CV

TL;DR: InstanceVG是一个多任务广义视觉定位框架，通过实例感知能力统一GREC和GRES任务，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的广义视觉定位方法通常独立处理GREC和GRES任务，忽视了联合训练的优势，且缺乏实例感知能力。

Method: 提出InstanceVG框架，利用实例查询统一实例级框和掩码的联合预测，并为每个实例查询分配先验参考点以增强目标匹配。

Result: 在四个任务的十个数据集上，InstanceVG显著超越现有方法，实现了最先进的性能。

Conclusion: InstanceVG框架通过统一实例级框和掩码的联合预测，显著提升了广义视觉定位任务的性能，并在多个数据集上实现了最先进的表现。

Abstract: Generalized visual grounding tasks, including Generalized Referring
Expression Comprehension (GREC) and Segmentation (GRES), extend the classical
visual grounding paradigm by accommodating multi-target and non-target
scenarios. Specifically, GREC focuses on accurately identifying all referential
objects at the coarse bounding box level, while GRES aims for achieve
fine-grained pixel-level perception. However, existing approaches typically
treat these tasks independently, overlooking the benefits of jointly training
GREC and GRES to ensure consistent multi-granularity predictions and streamline
the overall process. Moreover, current methods often treat GRES as a semantic
segmentation task, neglecting the crucial role of instance-aware capabilities
and the necessity of ensuring consistent predictions between instance-level
boxes and masks. To address these limitations, we propose InstanceVG, a
multi-task generalized visual grounding framework equipped with instance-aware
capabilities, which leverages instance queries to unify the joint and
consistency predictions of instance-level boxes and masks. To the best of our
knowledge, InstanceVG is the first framework to simultaneously tackle both GREC
and GRES while incorporating instance-aware capabilities into generalized
visual grounding. To instantiate the framework, we assign each instance query a
prior reference point, which also serves as an additional basis for target
matching. This design facilitates consistent predictions of points, boxes, and
masks for the same instance. Extensive experiments obtained on ten datasets
across four tasks demonstrate that InstanceVG achieves state-of-the-art
performance, significantly surpassing the existing methods in various
evaluation metrics. The code and model will be publicly available at
https://github.com/Dmmm1997/InstanceVG.

</details>


### [37] [Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval](https://arxiv.org/abs/2509.13754)
*Hao Yin,Xin Man,Feiyu Chen,Jie Shao,Heng Tao Shen*

Main category: cs.CV

TL;DR: FMFA通过显式和隐式对齐优化跨模态匹配，提升了文本到图像人物检索的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在跨模态匹配中无法验证局部特征对齐是否正确，以及忽视错误匹配的正样本对的问题。

Method: 提出了FMFA框架，包含A-SDM模块（自适应相似性分布匹配）和EFA模块（显式细粒度对齐），分别用于修正未匹配的正样本对和增强显式细粒度交互。

Result: 在三个公共数据集上实现了最先进的性能。

Conclusion: FMFA框架通过显式细粒度对齐和隐式关系推理，实现了跨模态全局匹配的优化，并在三个公共数据集上取得了最先进的性能。

Abstract: Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that
aims to retrieve the most relevant person images based on a given text query.
The key challenge in TIPR lies in achieving effective alignment between textual
and visual modalities within a common latent space. To address this challenge,
prior approaches incorporate attention mechanisms for implicit cross-modal
local alignment. However, they lack the ability to verify whether all local
features are correctly aligned. Moreover, existing methods primarily focus on
hard negative samples during model updates, with the goal of refining
distinctions between positive and negative pairs, often neglecting incorrectly
matched positive pairs. To alleviate these issues, we propose FMFA, a
cross-modal Full-Mode Fine-grained Alignment framework, which enhances global
matching through explicit fine-grained alignment and existing implicit
relational reasoning -- hence the term ``full-mode" -- without requiring
additional supervision. Specifically, we design an Adaptive Similarity
Distribution Matching (A-SDM) module to rectify unmatched positive sample
pairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint
embedding space, thereby achieving more precise global alignment. Additionally,
we introduce an Explicit Fine-grained Alignment (EFA) module, which makes up
for the lack of verification capability of implicit relational reasoning. EFA
strengthens explicit cross-modal fine-grained interactions by sparsifying the
similarity matrix and employs a hard coding method for local alignment. Our
proposed method is evaluated on three public datasets, achieving
state-of-the-art performance among all global matching methods. Our code is
available at https://github.com/yinhao1102/FMFA.

</details>


### [38] [Controllable-Continuous Color Editing in Diffusion Model via Color Mapping](https://arxiv.org/abs/2509.13756)
*Yuqi Yang,Dongliang Chang,Yuanchen Fang,Yi-Zhe SonG,Zhanyu Ma,Jun Guo*

Main category: cs.CV

TL;DR: 提出颜色映射模块，通过建模文本嵌入与RGB值的关系，实现精确、连续且可控的颜色编辑。


<details>
  <summary>Details</summary>
Motivation: 由于自然语言的固有模糊性和离散性，颜色编辑在精度不足和难以实现连续控制方面面临挑战，现有方法无法精确控制颜色变化范围及其与插值系数的关系。

Method: 引入颜色映射模块，该模块基于给定RGB值预测对应的嵌入向量，从而实现对生成图像的精确颜色控制。

Result: 实验结果表明，该方法在颜色连续性和可控性方面表现良好。

Conclusion: 论文提出的颜色映射模块通过明确建模文本嵌入空间与图像RGB值之间的对应关系，实现了对生成图像颜色的精确控制，并在保持语义一致性的同时，提供了更细粒度、连续且可控的颜色编辑能力。

Abstract: In recent years, text-driven image editing has made significant progress.
However, due to the inherent ambiguity and discreteness of natural language,
color editing still faces challenges such as insufficient precision and
difficulty in achieving continuous control. Although linearly interpolating the
embedding vectors of different textual descriptions can guide the model to
generate a sequence of images with varying colors, this approach lacks precise
control over the range of color changes in the output images. Moreover, the
relationship between the interpolation coefficient and the resulting image
color is unknown and uncontrollable. To address these issues, we introduce a
color mapping module that explicitly models the correspondence between the text
embedding space and image RGB values. This module predicts the corresponding
embedding vector based on a given RGB value, enabling precise color control of
the generated images while maintaining semantic consistency. Users can specify
a target RGB range to generate images with continuous color variations within
the desired range, thereby achieving finer-grained, continuous, and
controllable color editing. Experimental results demonstrate that our method
performs well in terms of color continuity and controllability.

</details>


### [39] [Iterative Prompt Refinement for Safer Text-to-Image Generation](https://arxiv.org/abs/2509.13760)
*Jinwoo Jeon,JunHyeok Oh,Hayeong Lee,Byung-Jun Lee*

Main category: cs.CV

TL;DR: 提出结合视觉反馈的迭代提示细化算法，提升T2I模型安全性并保持用户意图，实验证明效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于大型语言模型（LLMs）的安全方法忽视生成图像的问题，导致不安全输出或不必要修改安全提示的局限性。

Method: 使用视觉语言模型（VLMs）分析输入提示和生成的图像，提出迭代提示细化算法，并结合新标注的数据集进行监督微调。

Result: 实验结果表明，该方法在保持用户意图的同时，生成了更安全的输出，优于现有LLM-based方法。

Conclusion: 提出的迭代提示细化算法通过结合视觉反馈，显著提升了文本到图像模型的安全性和用户意图的保持，同时引入了新的数据集支持监督微调。

Abstract: Text-to-Image (T2I) models have made remarkable progress in generating images
from text prompts, but their output quality and safety still depend heavily on
how prompts are phrased. Existing safety methods typically refine prompts using
large language models (LLMs), but they overlook the images produced, which can
result in unsafe outputs or unnecessary changes to already safe prompts. To
address this, we propose an iterative prompt refinement algorithm that uses
Vision Language Models (VLMs) to analyze both the input prompts and the
generated images. By leveraging visual feedback, our method refines prompts
more effectively, improving safety while maintaining user intent and
reliability comparable to existing LLM-based approaches. Additionally, we
introduce a new dataset labeled with both textual and visual safety signals
using off-the-shelf multi-modal LLM, enabling supervised fine-tuning.
Experimental results demonstrate that our approach produces safer outputs
without compromising alignment with user intent, offering a practical solution
for generating safer T2I content. Our code is available at
https://github.com/ku-dmlab/IPR. \textbf{\textcolor{red}WARNING: This paper
contains examples of harmful or inappropriate images generated by models.

</details>


### [40] [Task-Aware Image Signal Processor for Advanced Visual Perception](https://arxiv.org/abs/2509.13762)
*Kai Chen,Jin Xiao,Leheng Zhang,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: TA-ISP 是一种轻量级的 RAW-to-RGB 框架，通过多尺度调制算子优化图像处理，显著提升视觉任务性能并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用 RAW 数据时面临计算开销大和表示能力受限的问题，TA-ISP 旨在通过任务感知的方式优化图像信号处理，提升视觉感知任务的性能。

Method: TA-ISP 通过预测一组轻量级、多尺度的调制算子，在全局、区域和像素尺度上重塑图像统计信息，避免了传统密集卷积管道的计算开销。

Result: 在多个 RAW 域检测和分割基准测试中，TA-ISP 在白天和夜间条件下均能显著提升下游任务的准确性，同时减少参数和推理时间。

Conclusion: TA-ISP 提出了一种紧凑的 RAW-to-RGB 框架，显著提升了下游任务的准确性，同时大幅减少了参数数量和推理时间，适合在资源受限的设备上部署。

Abstract: In recent years, there has been a growing trend in computer vision towards
exploiting RAW sensor data, which preserves richer information compared to
conventional low-bit RGB images. Early studies mainly focused on enhancing
visual quality, while more recent efforts aim to leverage the abundant
information in RAW data to improve the performance of visual perception tasks
such as object detection and segmentation. However, existing approaches still
face two key limitations: large-scale ISP networks impose heavy computational
overhead, while methods based on tuning traditional ISP pipelines are
restricted by limited representational capacity.To address these issues, we
propose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB
framework that produces task-oriented representations for pretrained vision
models. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small
set of lightweight, multi-scale modulation operators that act at global,
regional, and pixel scales to reshape image statistics across different spatial
extents. This factorized control significantly expands the range of spatially
varying transforms that can be represented while keeping memory usage,
computation, and latency tightly constrained. Evaluated on several RAW-domain
detection and segmentation benchmarks under both daytime and nighttime
conditions, TA-ISP consistently improves downstream accuracy while markedly
reducing parameter count and inference time, making it well suited for
deployment on resource-constrained devices.

</details>


### [41] [NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset](https://arxiv.org/abs/2509.13766)
*Huichun Liu,Xiaosong Li,Yang Liu,Xiaoqi Cheng,Haishu Tan*

Main category: cs.CV

TL;DR: 提出NDLPNet用于夜间去雨，通过PPM模块增强空间信息捕捉能力，构建NSR数据集，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有去雨技术主要针对白天条件，在夜间光照下表现不佳，因雨分布的空间异质性和光依赖条纹可见性的影响。

Method: 引入位置感知模块（PPM）来捕捉和利用空间上下文信息，增强模型识别和重新校准不同特征通道重要性的能力。

Result: 在现有数据集和NSR数据集上的实验表明，该方法在夜间去雨任务中优于现有方法。

Conclusion: 提出的NDLPNet在夜间去雨任务中表现优于现有最先进方法，并通过构建NSR数据集为未来研究提供了新基准。

Abstract: Visual degradation caused by rain streak artifacts in low-light conditions
significantly hampers the performance of nighttime surveillance and autonomous
navigation. Existing image deraining techniques are primarily designed for
daytime conditions and perform poorly under nighttime illumination due to the
spatial heterogeneity of rain distribution and the impact of light-dependent
stripe visibility. In this paper, we propose a novel Nighttime Deraining
Location-enhanced Perceptual Network(NDLPNet) that effectively captures the
spatial positional information and density distribution of rain streaks in
low-light environments. Specifically, we introduce a Position Perception Module
(PPM) to capture and leverage spatial contextual information from input data,
enhancing the model's capability to identify and recalibrate the importance of
different feature channels. The proposed nighttime deraining network can
effectively remove the rain streaks as well as preserve the crucial background
information. Furthermore, We construct a night scene rainy (NSR) dataset
comprising 900 image pairs, all based on real-world nighttime scenes, providing
a new benchmark for nighttime deraining task research. Extensive qualitative
and quantitative experimental evaluations on both existing datasets and the NSR
dataset consistently demonstrate our method outperform the state-of-the-art
(SOTA) methods in nighttime deraining tasks. The source code and dataset is
available at https://github.com/Feecuin/NDLPNet.

</details>


### [42] [VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI](https://arxiv.org/abs/2509.13767)
*Daiqi Liu,Tomás Arias-Vergara,Johannes Enk,Fangxu Xing,Maureen Stone,Jerry L. Prince,Jana Hutter,Andreas Maier,Jonghye Woo,Paula Andrea Pérez-Toro*

Main category: cs.CV

TL;DR: VocSegMRI通过整合视频、音频和语音学输入，结合交叉注意力和对比学习，显著提升了实时磁共振成像中发音结构的分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖视觉线索，难以准确分割实时磁共振成像中的发音结构，而同步的声学和语音学信号能提供互补上下文，提升分割精度。

Method: 提出了VocSegMRI多模态框架，整合视频、音频和语音学输入，通过交叉注意力融合实现动态特征对齐，并引入对比学习目标以增强跨模态表示。

Result: 在USC-75 rtMRI数据集的子集上，该方法达到最先进性能，Dice得分为0.95，95百分位豪斯多夫距离为4.20毫米，优于单模态和多模态基线。

Conclusion: 该研究强调了整合多模态建模在准确声道分析中的价值，通过交叉注意力和对比学习显著提升了分割精度和鲁棒性。

Abstract: Accurately segmenting articulatory structures in real-time magnetic resonance
imaging (rtMRI) remains challenging, as most existing methods rely almost
entirely on visual cues. Yet synchronized acoustic and phonological signals
provide complementary context that can enrich visual information and improve
precision. In this paper, we introduce VocSegMRI, a multimodal framework that
integrates video, audio, and phonological inputs through cross-attention fusion
for dynamic feature alignment. To further enhance cross-modal representation,
we incorporate a contrastive learning objective that improves segmentation
performance even when the audio modality is unavailable at inference. Evaluated
on a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art
performance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance
(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.
Ablation studies confirm the contributions of cross-attention and contrastive
learning to segmentation precision and robustness. These results highlight the
value of integrative multimodal modeling for accurate vocal tract analysis.

</details>


### [43] [Generative Image Coding with Diffusion Prior](https://arxiv.org/abs/2509.13768)
*Jianhui Chang*

Main category: cs.CV

TL;DR: 提出基于扩散先验的生成式编码框架，显著提升低比特率压缩性能，视觉保真度优于现有方法，压缩性能提升高达79%。


<details>
  <summary>Details</summary>
Motivation: 随着生成技术的发展，视觉内容日益复杂，传统编解码器和学习方法在高压缩比下难以保持主观质量，而现有生成方法在视觉保真度和泛化性方面存在挑战。

Method: 采用预优化编码器生成通用压缩域表示，通过轻量级适配器和注意力融合模块与预训练模型内部特征集成，并引入分布重归一化方法以增强重建保真度。

Result: 实验表明，该方法在低比特率下视觉保真度优于现有方法，压缩性能比H.266/VVC提升高达79%，并为AI生成内容提供了高效解决方案，同时适应更广泛的内容类型。

Conclusion: 该论文提出了一种基于扩散先验的生成式编码框架，显著提升了低比特率下的压缩性能，并在视觉保真度和适应性方面优于现有方法。

Abstract: As generative technologies advance, visual content has evolved into a complex
mix of natural and AI-generated images, driving the need for more efficient
coding techniques that prioritize perceptual quality. Traditional codecs and
learned methods struggle to maintain subjective quality at high compression
ratios, while existing generative approaches face challenges in visual fidelity
and generalization. To this end, we propose a novel generative coding framework
leveraging diffusion priors to enhance compression performance at low bitrates.
Our approach employs a pre-optimized encoder to generate generalized
compressed-domain representations, integrated with the pretrained model's
internal features via a lightweight adapter and an attentive fusion module.
This framework effectively leverages existing pretrained diffusion models and
enables efficient adaptation to different pretrained models for new
requirements with minimal retraining costs. We also introduce a distribution
renormalization method to further enhance reconstruction fidelity. Extensive
experiments show that our method (1) outperforms existing methods in visual
fidelity across low bitrates, (2) improves compression performance by up to 79%
over H.266/VVC, and (3) offers an efficient solution for AI-generated content
while being adaptable to broader content types.

</details>


### [44] [AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2509.13769)
*Yuechen Luo,Fang Li,Shaoqing Xu,Zhiyi Lai,Lei Yang,Qimao Chen,Ziang Luo,Zixun Xie,Shengyin Jiang,Jiaxin Liu,Long Chen,Bing Wang,Zhi-xin Yang*

Main category: cs.CV

TL;DR: AdaThinkDrive通过双模式推理和自适应奖励策略，在自动驾驶中平衡了准确性与效率，显著提升性能并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有CoT推理在VLA模型中虽表现良好，但在简单场景中引入不必要的计算开销且未提升决策质量，因此需要一种自适应推理方法。

Method: 提出AdaThinkDrive框架，结合双模式推理机制（快速回答与慢速思考），并通过自适应思考奖励策略与GRPO优化模型。

Result: 在Navsim基准测试中，AdaThinkDrive的PDMS达到90.3，比最佳视觉基线高1.7分，推理时间减少14%。

Conclusion: AdaThinkDrive通过自适应推理机制在自动驾驶中实现了准确性与效率的平衡，显著提升了性能并减少了推理时间。

Abstract: While reasoning technology like Chain of Thought (CoT) has been widely
adopted in Vision Language Action (VLA) models, it demonstrates promising
capabilities in end to end autonomous driving. However, recent efforts to
integrate CoT reasoning often fall short in simple scenarios, introducing
unnecessary computational overhead without improving decision quality. To
address this, we propose AdaThinkDrive, a novel VLA framework with a dual mode
reasoning mechanism inspired by fast and slow thinking. First, our framework is
pretrained on large scale autonomous driving (AD) scenarios using both question
answering (QA) and trajectory datasets to acquire world knowledge and driving
commonsense. During supervised fine tuning (SFT), we introduce a two mode
dataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the
model to distinguish between scenarios that require reasoning. Furthermore, an
Adaptive Think Reward strategy is proposed in conjunction with the Group
Relative Policy Optimization (GRPO), which rewards the model for selectively
applying CoT by comparing trajectory quality across different reasoning modes.
Extensive experiments on the Navsim benchmark show that AdaThinkDrive achieves
a PDMS of 90.3, surpassing the best vision only baseline by 1.7 points.
Moreover, ablations show that AdaThinkDrive surpasses both the never Think and
always Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also
reduces inference time by 14% compared to the always Think baseline,
demonstrating its ability to balance accuracy and efficiency through adaptive
reasoning.

</details>


### [45] [Morphology-optimized Multi-Scale Fusion: Combining Local Artifacts and Mesoscopic Semantics for Deepfake Detection and Localization](https://arxiv.org/abs/2509.13776)
*Chao Shuai,Gaojian Wang,Kun Pan,Tong Wu,Fanli Jin,Haohan Tan,Mengxiang Li,Zhenguang Liu,Feng Lin,Kui Ren*

Main category: cs.CV

TL;DR: 论文提出了一种结合局部和全局视角的深度伪造区域定位方法，通过形态学操作融合输出，显著提升了定位性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测在分类基础上虽取得显著进展，但伪造区域的精确定位仍面临挑战，且现有方法常忽视局部细节与全局语义的互补性，导致定位性能不佳。

Method: 采用局部和全局视角独立预测伪造区域，并使用形态学操作融合输出。

Result: 实验表明，所提方法在提升伪造定位准确性和鲁棒性方面具有显著效果。

Conclusion: 论文提出了一种新颖的局部和全局视角独立预测伪造区域的方法，并通过形态学操作融合输出，有效提升了伪造定位的准确性和鲁棒性。

Abstract: While the pursuit of higher accuracy in deepfake detection remains a central
goal, there is an increasing demand for precise localization of manipulated
regions. Despite the remarkable progress made in classification-based
detection, accurately localizing forged areas remains a significant challenge.
A common strategy is to incorporate forged region annotations during model
training alongside manipulated images. However, such approaches often neglect
the complementary nature of local detail and global semantic context, resulting
in suboptimal localization performance. Moreover, an often-overlooked aspect is
the fusion strategy between local and global predictions. Naively combining the
outputs from both branches can amplify noise and errors, thereby undermining
the effectiveness of the localization.
  To address these issues, we propose a novel approach that independently
predicts manipulated regions using both local and global perspectives. We
employ morphological operations to fuse the outputs, effectively suppressing
noise while enhancing spatial coherence. Extensive experiments reveal the
effectiveness of each module in improving the accuracy and robustness of
forgery localization.

</details>


### [46] [CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling](https://arxiv.org/abs/2509.13784)
*Hanfang Liang,Bing Wang,Shizhen Zhang,Wen Jiang,Yizhuo Yang,Weixiang Guo,Shenghai Yuan*

Main category: cs.CV

TL;DR: 提出Variable-Rate Spatial Event Mamba，直接处理事件流，无需中间表示，实现高效实时视觉处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要预定义时间窗口引入窗口延迟，而逐点检测方法因高计算成本无法实现实时效率。

Method: 提出了一种轻量级因果空间邻域编码器捕获局部几何关系，结合基于Mamba的状态空间模型进行线性复杂度的可扩展时间建模。

Result: 新架构直接处理原始事件流，通过自适应调整处理速度，优化了延迟与效率的平衡。

Conclusion: Variable-Rate Spatial Event Mamba架构通过直接处理原始事件流，无需中间表示，实现了高速视觉任务的高效处理，平衡了窗口延迟和推理延迟。

Abstract: Event cameras capture asynchronous pixel-level brightness changes with
microsecond temporal resolution, offering unique advantages for high-speed
vision tasks. Existing methods often convert event streams into intermediate
representations such as frames, voxel grids, or point clouds, which inevitably
require predefined time windows and thus introduce window latency. Meanwhile,
pointwise detection methods face computational challenges that prevent
real-time efficiency due to their high computational cost. To overcome these
limitations, we propose the Variable-Rate Spatial Event Mamba, a novel
architecture that directly processes raw event streams without intermediate
representations. Our method introduces a lightweight causal spatial
neighborhood encoder to efficiently capture local geometric relations, followed
by Mamba-based state space models for scalable temporal modeling with linear
complexity. During inference, a controller adaptively adjusts the processing
speed according to the event rate, achieving an optimal balance between window
latency and inference latency.

</details>


### [47] [BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching](https://arxiv.org/abs/2509.13789)
*Hanshuai Cui,Zhiqing Tang,Zhifei Xu,Zhi Yao,Wenyi Zeng,Weijia Jia*

Main category: cs.CV

TL;DR: BWCache通过动态缓存和重用DiT块特征，加速视频生成，保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: DiT在视频生成中的顺序去噪过程导致不可避免的延迟，现有加速方法要么牺牲视觉质量，要么无法适当重用中间特征。

Method: 提出了Block-Wise Caching (BWCache)方法，动态缓存和重用DiT块特征，并引入相似性指标以最小化冗余计算。

Result: BWCache在多个视频扩散模型上实现了高达2.24倍的加速，且视觉质量相当。

Conclusion: BWCache是一种无需训练的方法，通过动态缓存和重用DiT块特征，显著加速了基于DiT的视频生成，同时保持视觉质量。

Abstract: Recent advancements in Diffusion Transformers (DiTs) have established them as
the state-of-the-art method for video generation. However, their inherently
sequential denoising process results in inevitable latency, limiting real-world
applicability. Existing acceleration methods either compromise visual quality
due to architectural modifications or fail to reuse intermediate features at
proper granularity. Our analysis reveals that DiT blocks are the primary
contributors to inference latency. Across diffusion timesteps, the feature
variations of DiT blocks exhibit a U-shaped pattern with high similarity during
intermediate timesteps, which suggests substantial computational redundancy. In
this paper, we propose Block-Wise Caching (BWCache), a training-free method to
accelerate DiT-based video generation. BWCache dynamically caches and reuses
features from DiT blocks across diffusion timesteps. Furthermore, we introduce
a similarity indicator that triggers feature reuse only when the differences
between block features at adjacent timesteps fall below a threshold, thereby
minimizing redundant computations while maintaining visual fidelity. Extensive
experiments on several video diffusion models demonstrate that BWCache achieves
up to 2.24$\times$ speedup with comparable visual quality.

</details>


### [48] [Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation](https://arxiv.org/abs/2509.13792)
*Inder Pal Singh,Nidhal Eddine Chenni,Abd El Rahman Shabayek,Arunkumar Rathinam,Djamila Aouada*

Main category: cs.CV

TL;DR: 该论文提出了一种监督域适应框架，用于航天器姿态估计关键点回归，通过联合优化域不变表示和任务特定风险，在有限标记真实数据下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 航天器姿态估计（SPE）是自主空间操作（如交会、对接和在轨服务）的基础能力。尽管混合管道在合成数据集上表现强劲，但在真实或实验室生成的图像上性能显著下降，主要原因是合成到真实的域差距。现有无监督域适应方法在少量标记目标样本可用时表现不佳，因此需要一种更有效的监督域适应框架。

Method: 基于学习不变表示和风险（LIRR）范式，该方法联合优化了域不变表示和任务特定风险，利用标记的合成数据和有限的标记真实数据，减少了域偏移下的泛化误差。

Result: 在SPEED+基准测试上的广泛实验表明，该方法始终优于仅源域、微调和基准模型。特别是，仅使用5%的标记目标数据，该方法就能匹配或超越基于更大比例标记数据训练的基准性能。

Conclusion: 该论文提出的监督域适应（SDA）框架在航天器姿态估计（SPE）关键点回归任务中表现优异，特别是在有限标记真实数据的情况下，能够匹配甚至超越基于大量标记数据的基准性能。该方法轻量级、与骨干网络无关且计算高效，为实际空间环境中的稳健部署提供了可行路径。

Abstract: Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous
space operations such as rendezvous, docking, and in-orbit servicing. Hybrid
pipelines that combine object detection, keypoint regression, and
Perspective-n-Point (PnP) solvers have recently achieved strong results on
synthetic datasets, yet their performance deteriorates sharply on real or
lab-generated imagery due to the persistent synthetic-to-real domain gap.
Existing unsupervised domain adaptation approaches aim to mitigate this issue
but often underperform when a modest number of labeled target samples are
available. In this work, we propose the first Supervised Domain Adaptation
(SDA) framework tailored for SPE keypoint regression. Building on the Learning
Invariant Representation and Risk (LIRR) paradigm, our method jointly optimizes
domain-invariant representations and task-specific risk using both labeled
synthetic and limited labeled real data, thereby reducing generalization error
under domain shift. Extensive experiments on the SPEED+ benchmark demonstrate
that our approach consistently outperforms source-only, fine-tuning, and oracle
baselines. Notably, with only 5% labeled target data, our method matches or
surpasses oracle performance trained on larger fractions of labeled data. The
framework is lightweight, backbone-agnostic, and computationally efficient,
offering a practical pathway toward robust and deployable spacecraft pose
estimation in real-world space environments.

</details>


### [49] [SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient 4-DoF UAV Localization in GNSS-Denied Environments](https://arxiv.org/abs/2509.13795)
*Jiayu Yuan,Ming Dai,Enhui Zheng,Chao Su,Nanxing Chen,Qiming Hu,Shibo Zhu,Yibin Cao*

Main category: cs.CV

TL;DR: 论文提出SWA-PF方法，用于GNSS缺失环境下的UAV定位，结合语义特征和优化粒子滤波，显著提升定位效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的UAV定位方法在数据集可用性、实时性能、环境敏感性和泛化能力方面存在局限性，尤其是在动态或时间变化的环境中。

Method: 论文提出了一种新颖的语义加权自适应粒子滤波（SWA-PF）方法，整合了无人机捕获图像和卫星图像的语义特征，通过语义加权机制和优化的粒子滤波架构实现定位。

Result: 所提方法在计算效率上比特征提取方法提升了10倍，全局定位误差保持在10米以下，并能在几秒内实现快速的4自由度姿态估计。

Conclusion: 该论文提出的SWA-PF方法在GNSS信号缺失环境下显著提升了UAV的定位性能，通过结合语义特征和优化粒子滤波架构，实现了高效的实时定位。

Abstract: Vision-based Unmanned Aerial Vehicle (UAV) localization systems have been
extensively investigated for Global Navigation Satellite System (GNSS)-denied
environments. However, existing retrieval-based approaches face limitations in
dataset availability and persistent challenges including suboptimal real-time
performance, environmental sensitivity, and limited generalization capability,
particularly in dynamic or temporally varying environments. To overcome these
limitations, we present a large-scale Multi-Altitude Flight Segments dataset
(MAFS) for variable altitude scenarios and propose a novel Semantic-Weighted
Adaptive Particle Filter (SWA-PF) method. This approach integrates robust
semantic features from both UAV-captured images and satellite imagery through
two key innovations: a semantic weighting mechanism and an optimized particle
filtering architecture. Evaluated using our dataset, the proposed method
achieves 10x computational efficiency gain over feature extraction methods,
maintains global positioning errors below 10 meters, and enables rapid 4 degree
of freedom (4-DoF) pose estimation within seconds using accessible
low-resolution satellite maps. Code and dataset will be available at
https://github.com/YuanJiayuuu/SWA-PF.

</details>


### [50] [Masked Feature Modeling Enhances Adaptive Segmentation](https://arxiv.org/abs/2509.13801)
*Wenlve Zhou,Zhiheng Zhou,Tiantao Xian,Yikui Zhai,Weibin Wu,Biyun Ma*

Main category: cs.CV

TL;DR: 提出MFM作为辅助任务，通过特征掩蔽和重建提升无监督域自适应语义分割性能，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，掩蔽建模方法由于架构不兼容和优化目标不一致而未被充分探索，因此提出MFM以解决这些问题。

Method: 提出了一种名为Masked Feature Modeling（MFM）的新型辅助任务，直接在特征空间中进行特征掩蔽和重建。引入轻量级辅助模块Rebuilder，训练时联合使用但在推理时丢弃。

Result: 在各种架构和UDA基准测试中，MFM一致提升了分割性能。

Conclusion: MFM提供了一种简单、高效且通用的策略，可显著提升无监督域自适应语义分割的性能。

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to
transfer models from a labeled source domain to an unlabeled target domain.
While auxiliary self-supervised tasks-particularly contrastive learning-have
improved feature discriminability, masked modeling approaches remain
underexplored in this setting, largely due to architectural incompatibility and
misaligned optimization objectives. We propose Masked Feature Modeling (MFM), a
novel auxiliary task that performs feature masking and reconstruction directly
in the feature space. Unlike existing masked modeling methods that reconstruct
low-level inputs or perceptual features (e.g., HOG or visual tokens), MFM
aligns its learning target with the main segmentation task, ensuring
compatibility with standard architectures like DeepLab and DAFormer without
modifying the inference pipeline. To facilitate effective reconstruction, we
introduce a lightweight auxiliary module, Rebuilder, which is trained jointly
but discarded during inference, adding zero computational overhead at test
time. Crucially, MFM leverages the segmentation decoder to classify the
reconstructed features, tightly coupling the auxiliary objective with the
pixel-wise prediction task to avoid interference with the primary task.
Extensive experiments across various architectures and UDA benchmarks
demonstrate that MFM consistently enhances segmentation performance, offering a
simple, efficient, and generalizable strategy for unsupervised domain-adaptive
semantic segmentation.

</details>


### [51] [Data-Efficient Spectral Classification of Hyperspectral Data Using MiniROCKET and HDC-MiniROCKET](https://arxiv.org/abs/2509.13809)
*Nick Theisen,Kenny Schlegel,Dietrich Paulus,Peer Neubert*

Main category: cs.CV

TL;DR: MiniROCKET和HDC-MiniROCKET在数据有限时优于1D-Justo-LiuNet，为光谱分类提供了更稳健的解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管1D-Justo-LiuNet在光谱分类中表现优异，但在训练数据有限时性能下降，因此需要探索更稳健的模型以应对数据限制问题。

Method: 研究了MiniROCKET和HDC-MiniROCKET在光谱分类中的应用，这些模型通过提取精心设计的特征（无需可训练参数）来减少对训练数据的依赖。

Result: MiniROCKET在有限数据场景下优于1D-Justo-LiuNet，且在一般情况下表现相当。

Conclusion: MiniROCKET和HDC-MiniROCKET在有限训练数据场景下表现优于1D-Justo-LiuNet，且在一般情况下与之持平，为解决光谱分类中的数据限制问题提供了有效方案。

Abstract: The classification of pixel spectra of hyperspectral images, i.e. spectral
classification, is used in many fields ranging from agricultural, over medical
to remote sensing applications and is currently also expanding to areas such as
autonomous driving. Even though for full hyperspectral images the
best-performing methods exploit spatial-spectral information, performing
classification solely on spectral information has its own advantages, e.g.
smaller model size and thus less data required for training. Moreover, spectral
information is complementary to spatial information and improvements on either
part can be used to improve spatial-spectral approaches in the future.
Recently, 1D-Justo-LiuNet was proposed as a particularly efficient model with
very few parameters, which currently defines the state of the art in spectral
classification. However, we show that with limited training data the model
performance deteriorates. Therefore, we investigate MiniROCKET and
HDC-MiniROCKET for spectral classification to mitigate that problem. The model
extracts well-engineered features without trainable parameters in the feature
extraction part and is therefore less vulnerable to limited training data. We
show that even though MiniROCKET has more parameters it outperforms
1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the
general case

</details>


### [52] [Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2509.13834)
*Nguyen Lan Vi Vu,Thanh-Huy Nguyen,Thien Nguyen,Daisuke Kihara,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: Semi-MOE是多任务Mixture-of-Experts框架，通过三个专家网络和动态伪标签机制提升半监督组织病理学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有半监督学习方法在组织病理学图像分割中因模糊腺体边界和形态学误分类导致的噪声伪标签问题。

Method: 采用三个专家网络（主分割专家、符号距离场回归专家和边界预测专家）和Multi-Gating伪标签模块，结合自适应多目标损失，动态平衡学习目标。

Result: 在GlaS和CRAG基准测试中，Semi-MOE在低标签设置下优于现有最先进方法。

Conclusion: Semi-MOE通过多任务Mixture-of-Experts框架和自适应多目标损失，显著提升了半监督组织病理学图像分割的性能，展现了MoE架构在该领域的潜力。

Abstract: Semi-supervised learning has been employed to alleviate the need for
extensive labeled data for histopathology image segmentation, but existing
methods struggle with noisy pseudo-labels due to ambiguous gland boundaries and
morphological misclassification. This paper introduces Semi-MOE, to the best of
our knowledge, the first multi-task Mixture-of-Experts framework for
semi-supervised histopathology image segmentation. Our approach leverages three
specialized expert networks: A main segmentation expert, a signed distance
field regression expert, and a boundary prediction expert, each dedicated to
capturing distinct morphological features. Subsequently, the Multi-Gating
Pseudo-labeling module dynamically aggregates expert features, enabling a
robust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate
manual tuning while dynamically balancing multiple learning objectives, we
propose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and
CRAG benchmarks show that our method outperforms state-of-the-art approaches in
low-label settings, highlighting the potential of MoE-based architectures in
advancing semi-supervised segmentation. Our code is available at
https://github.com/vnlvi2k3/Semi-MoE.

</details>


### [53] [Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models](https://arxiv.org/abs/2509.13836)
*Weihang Wang,Xinhao Li,Ziyue Wang,Yan Pang,Jielei Zhang,Peiyi Li,Qiang Zhang,Longwen Gao*

Main category: cs.CV

TL;DR: LVLM中的物体幻觉问题严重，本文提出VisionWeaver通过动态聚合专家特征显著减少幻觉并提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）中的物体幻觉严重限制了其实际应用，不同视觉编码器的训练范式导致其独特的归纳偏差和幻觉表现。

Method: 提出了一种新颖的上下文感知路由网络VisionWeaver，利用全局视觉特征生成路由信号，动态聚合多个专家的视觉特征。

Result: 通过VHBench-10基准测试证实了编码器具有独特的幻觉特征，VisionWeaver在减少幻觉和提升性能方面表现出色。

Conclusion: VisionWeaver通过动态聚合多个专家的视觉特征，显著减少了幻觉现象并提升了模型整体性能。

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly
impedes their real-world applicability. As the primary component for accurately
interpreting visual information, the choice of visual encoder is pivotal. We
hypothesize that the diverse training paradigms employed by different visual
encoders instill them with distinct inductive biases, which leads to their
diverse hallucination performances. Existing benchmarks typically focus on
coarse-grained hallucination detection and fail to capture the diverse
hallucinations elaborated in our hypothesis. To systematically analyze these
effects, we introduce VHBench-10, a comprehensive benchmark with approximately
10,000 samples for evaluating LVLMs across ten fine-grained hallucination
categories. Our evaluations confirm encoders exhibit unique hallucination
characteristics. Building on these insights and the suboptimality of simple
feature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.
It employs global visual features to generate routing signals, dynamically
aggregating visual features from multiple specialized experts. Comprehensive
experiments confirm the effectiveness of VisionWeaver in significantly reducing
hallucinations and improving overall model performance.

</details>


### [54] [Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.13846)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 本文提出一致视图对齐方法，证明潜在空间结构需明确引导，实验显示该方法显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 挑战了现有表示学习中隐含的假设，即数据点的无关视图足以学习有意义的表示，证明了潜在空间中的结构需要明确引导。

Method: 提出了一种名为一致视图对齐（Consistent View Alignment）的方法，该方法通过对齐数据不同视图的表示来整合互补信息，同时避免引入假阳性。

Result: 在MICCAI 2025 SSL3D挑战赛中，使用Primus视觉变换器和ResEnc卷积神经网络分别获得了第一名和第二名的成绩。

Conclusion: 本文通过提出一致的视图对齐方法，证明了在潜在空间中明确引导结构的重要性，从而在自监督学习中提高了下游任务的性能。

Abstract: Many recent approaches in representation learning implicitly assume that
uncorrelated views of a data point are sufficient to learn meaningful
representations for various downstream tasks. In this work, we challenge this
assumption and demonstrate that meaningful structure in the latent space does
not emerge naturally. Instead, it must be explicitly induced. We propose a
method that aligns representations from different views of the data to align
complementary information without inducing false positives. Our experiments
show that our proposed self-supervised learning method, Consistent View
Alignment, improves performance for downstream tasks, highlighting the critical
role of structured view alignment in learning effective representations. Our
method achieved first and second place in the MICCAI 2025 SSL3D challenge when
using a Primus vision transformer and ResEnc convolutional neural network,
respectively. The code and pretrained model weights are released at
https://github.com/Tenbatsu24/LatentCampus.

</details>


### [55] [MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment](https://arxiv.org/abs/2509.14001)
*Elena Camuffo,Francesco Barbato,Mete Ozay,Simone Milani,Umberto Michieli*

Main category: cs.CV

TL;DR: MOCHA通过对象级知识蒸馏，将多模态语义从大型教师模型传递到轻量级学生模型，无需文本输入，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要关注密集或全局对齐，而MOCHA专注于对象级别的语义传递，无需修改教师模型或推理时的文本输入。

Method: MOCHA通过翻译模块将学生模型特征映射到联合空间，并使用双目标损失函数（局部对齐和全局关系一致性）指导训练。

Result: 在四个少样本个性化检测基准上，MOCHA平均得分提高了10.1分，性能与大型多模态模型相当。

Conclusion: MOCHA是一种高效的知识蒸馏方法，能够在保持轻量级架构的同时，实现与大型多模态模型相当的性能，适合实际部署。

Abstract: We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),
a knowledge distillation approach that transfers region-level multimodal
semantics from a large vision-language teacher (e.g., LLaVa) into a lightweight
vision-only object detector student (e.g., YOLO). A translation module maps
student features into a joint space, where the training of the student and
translator is guided by a dual-objective loss that enforces both local
alignment and global relational consistency. Unlike prior approaches focused on
dense or global alignment, MOCHA operates at the object level, enabling
efficient transfer of semantics without modifying the teacher or requiring
textual input at inference. We validate our method across four personalized
detection benchmarks under few-shot regimes. Results show consistent gains over
baselines, with a +10.1 average score improvement. Despite its compact
architecture, MOCHA reaches performance on par with larger multimodal models,
proving its suitability for real-world deployment.

</details>


### [56] [SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation](https://arxiv.org/abs/2509.13848)
*Jiayi Pan,Jiaming Xu,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: 提出SpecDiff，一种基于自推测信息的训练免费多级特征缓存策略，显著提升扩散模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法仅依赖历史信息，导致准确性和速度性能受限。

Method: 提出了一种基于自推测信息的动态重要性评分算法和多级特征分类算法，包括缓存特征选择算法和多级特征分类算法。

Result: SpecDiff在Stable Diffusion 3、3.5和FLUX上实现了平均2.80倍、2.74倍和3.17倍的加速，且质量损失可忽略。

Conclusion: 通过结合推测信息和历史信息，SpecDiff克服了速度与准确性的权衡瓶颈，推动了高效扩散模型推理中速度与准确性的Pareto前沿。

Abstract: Feature caching has recently emerged as a promising method for diffusion
model acceleration. It effectively alleviates the inefficiency problem caused
by high computational requirements by caching similar features in the inference
process of the diffusion model. In this paper, we analyze existing feature
caching methods from the perspective of information utilization, and point out
that relying solely on historical information will lead to constrained accuracy
and speed performance. And we propose a novel paradigm that introduces future
information via self-speculation based on the information similarity at the
same time step across different iteration times. Based on this paradigm, we
present \textit{SpecDiff}, a training-free multi-level feature caching strategy
including a cached feature selection algorithm and a multi-level feature
classification algorithm. (1) Feature selection algorithm based on
self-speculative information. \textit{SpecDiff} determines a dynamic importance
score for each token based on self-speculative information and historical
information, and performs cached feature selection through the importance
score. (2) Multi-level feature classification algorithm based on feature
importance scores. \textit{SpecDiff} classifies tokens by leveraging the
differences in feature importance scores and introduces a multi-level feature
calculation strategy. Extensive experiments show that \textit{SpecDiff}
achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with
negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow
on NVIDIA A800-80GB GPU. By merging speculative and historical information,
\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing
the Pareto frontier of speedup and accuracy in the efficient diffusion model
inference.

</details>


### [57] [EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics](https://arxiv.org/abs/2509.13858)
*Qianxin Xia,Jiawei Du,Guoming Lu,Zhiyong Shu,Jielei Wang*

Main category: cs.CV

TL;DR: EDITS通过结合视觉和语言模型，提升数据集蒸馏的语义保留能力。


<details>
  <summary>Details</summary>
Motivation: 传统数据集蒸馏技术主要捕获低层次视觉特征，忽视了图像中的高层次语义和结构信息。

Method: 提出EDITS框架，包括全局语义查询模块、局部语义感知模块和双原型指导策略，利用扩散模型生成最终合成数据集。

Result: 大量实验证实EDITS方法的有效性，源代码已公开。

Conclusion: EDITS框架通过结合视觉语言模型和大型语言模型，有效提升了数据集蒸馏的性能，保留了高层次语义信息。

Abstract: Dataset distillation aims to synthesize a compact dataset from the original
large-scale one, enabling highly efficient learning while preserving
competitive model performance. However, traditional techniques primarily
capture low-level visual features, neglecting the high-level semantic and
structural information inherent in images. In this paper, we propose EDITS, a
novel framework that exploits the implicit textual semantics within the image
data to achieve enhanced distillation. First, external texts generated by a
Vision Language Model (VLM) are fused with image features through a Global
Semantic Query module, forming the prior clustered buffer. Local Semantic
Awareness then selects representative samples from the buffer to construct
image and text prototypes, with the latter produced by guiding a Large Language
Model (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype
Guidance strategy generates the final synthetic dataset through a diffusion
model. Extensive experiments confirm the effectiveness of our method.Source
code is available in: https://github.com/einsteinxia/EDITS.

</details>


### [58] [LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction](https://arxiv.org/abs/2509.13863)
*Chu Chen,Ander Biguri,Jean-Michel Morel,Raymond H. Chan,Carola-Bibiane Schönlieb,Jizhou Li*

Main category: cs.CV

TL;DR: LamiGauss是一种结合高斯喷溅和专用转换模型的层析成像重建算法，有效解决稀疏视图下的高质量重建问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统计算机断层扫描（CT）在几何受限的板状结构（如微芯片和复合电池材料）中难以进行无损检测，而现有层析成像技术在高度稀疏视图采集条件下的高质量体积重建仍具挑战性。

Method: LamiGauss结合了高斯喷溅辐射光栅化和专用的探测器到世界转换模型，利用初始化策略显式过滤掉常见的层析成像伪影，避免冗余高斯分配到虚假结构，从而集中模型容量表示真实对象。

Result: LamiGauss仅需3%的完整视图即可在稀疏投影下实现准确高效的重建，性能优于在完整数据集上优化的迭代方法。

Conclusion: LamiGauss算法通过结合高斯喷溅辐射光栅化和专用的探测器到世界转换模型，成功解决了在高度稀疏视图采集条件下高质量体积重建的挑战，并在合成和真实数据集上展示了其有效性和优越性。

Abstract: X-ray Computed Laminography (CL) is essential for non-destructive inspection
of plate-like structures in applications such as microchips and composite
battery materials, where traditional computed tomography (CT) struggles due to
geometric constraints. However, reconstructing high-quality volumes from
laminographic projections remains challenging, particularly under highly
sparse-view acquisition conditions. In this paper, we propose a reconstruction
algorithm, namely LamiGauss, that combines Gaussian Splatting radiative
rasterization with a dedicated detector-to-world transformation model
incorporating the laminographic tilt angle. LamiGauss leverages an
initialization strategy that explicitly filters out common laminographic
artifacts from the preliminary reconstruction, preventing redundant Gaussians
from being allocated to false structures and thereby concentrating model
capacity on representing the genuine object. Our approach effectively optimizes
directly from sparse projections, enabling accurate and efficient
reconstruction with limited data. Extensive experiments on both synthetic and
real datasets demonstrate the effectiveness and superiority of the proposed
method over existing techniques. LamiGauss uses only 3$\%$ of full views to
achieve superior performance over the iterative method optimized on a full
dataset.

</details>


### [59] [Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions](https://arxiv.org/abs/2509.14165)
*Michal Szczepanski,Martyna Poreba,Karim Haroun*

Main category: cs.CV

TL;DR: STEP框架通过动态补丁合并和令牌剪枝，显著提升了ViTs在高分辨率语义分割中的效率，计算成本降低4倍，速度提升1.7倍，准确率损失极小。


<details>
  <summary>Details</summary>
Motivation: 解决Vision Transformers在语义分割中因高计算和内存成本而受限的问题。

Method: 提出了STEP（SuperToken和Early-Pruning）框架，结合动态补丁合并和令牌剪枝，使用轻量级CNN策略网络dCTS实现灵活合并，并在编码器块中集成早期退出机制以减少计算负担。

Result: 在1024 x 1024高分辨率语义分割基准测试中，STEP框架实现了计算复杂度降低4倍，推理速度提升1.7倍，准确率下降不超过2.0%。

Conclusion: STEP框架通过结合动态补丁合并和令牌剪枝，显著提高了Vision Transformers的效率，同时保持了较高的准确性。

Abstract: Vision Transformers (ViTs) achieve state-of-the-art performance in semantic
segmentation but are hindered by high computational and memory costs. To
address this, we propose STEP (SuperToken and Early-Pruning), a hybrid
token-reduction framework that combines dynamic patch merging and token pruning
to enhance efficiency without significantly compromising accuracy. At the core
of STEP is dCTS, a lightweight CNN-based policy network that enables flexible
merging into superpatches. Encoder blocks integrate also early-exits to remove
high-confident supertokens, lowering computational load. We evaluate our method
on high-resolution semantic segmentation benchmarks, including images up to
1024 x 1024, and show that when dCTS is applied alone, the token count can be
reduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching
scheme. This yields a 2.6x reduction in computational cost and a 3.4x increase
in throughput when using ViT-Large as the backbone. Applying the full STEP
framework further improves efficiency, reaching up to a 4x reduction in
computational complexity and a 1.7x gain in inference speed, with a maximum
accuracy drop of no more than 2.0%. With the proposed STEP configurations, up
to 40% of tokens can be confidently predicted and halted before reaching the
final encoder layer.

</details>


### [60] [Distractor-Aware Memory-Based Visual Object Tracking](https://arxiv.org/abs/2509.13864)
*Jovana Videnovic,Matej Kristan,Alan Lukezic*

Main category: cs.CV

TL;DR: DAM4SAM通过干扰物感知内存模块改进了SAM2的追踪性能，特别是在干扰物和遮挡场景下，并在多个基准测试中取得领先。


<details>
  <summary>Details</summary>
Motivation: 现有的基于内存的视频分割方法（如SAM2）在视觉目标追踪中未能充分适应干扰物的挑战，导致追踪漂移和遮挡后重检测能力不足。

Method: 提出了一种干扰物感知的drop-in内存模块和基于自省的管理方法，构建了DiDi数据集以分析干扰物对追踪的影响。

Result: DAM4SAM在13个基准测试中优于SAM2.1，并在10个测试中创下新纪录。与实时追踪器EfficientTAM和边缘追踪器EdgeTAM的集成分别提升了11%和4%的性能。

Conclusion: DAM4SAM通过引入干扰物感知的drop-in内存模块和基于自省的管理方法，显著提升了SAM2在视觉目标追踪中的性能，特别是在处理干扰物和目标遮挡方面。该方法在多个基准测试中取得了领先的结果，并展示了良好的架构泛化能力。

Abstract: Recent emergence of memory-based video segmentation methods such as SAM2 has
led to models with excellent performance in segmentation tasks, achieving
leading results on numerous benchmarks. However, these modes are not fully
adjusted for visual object tracking, where distractors (i.e., objects visually
similar to the target) pose a key challenge. In this paper we propose a
distractor-aware drop-in memory module and introspection-based management
method for SAM2, leading to DAM4SAM. Our design effectively reduces the
tracking drift toward distractors and improves redetection capability after
object occlusion. To facilitate the analysis of tracking in the presence of
distractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM
outperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results
on ten. Furthermore, integrating the proposed distractor-aware memory into a
real-time tracker EfficientTAM leads to 11% improvement and matches tracking
quality of the non-real-time SAM2.1-L on multiple tracking and segmentation
benchmarks, while integration with edge-based tracker EdgeTAM delivers 4%
performance boost, demonstrating a very good generalization across
architectures.

</details>


### [61] [Dense Video Understanding with Gated Residual Tokenization](https://arxiv.org/abs/2509.14199)
*Haichao Zhang,Wenhao Chai,Shwai He,Ang Li,Yun Fu*

Main category: cs.CV

TL;DR: 论文提出GRT框架和DIVE基准，解决了高帧率视频理解中的计算冗余问题，显著提升了密集时间信息的处理效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型和基准测试因高帧率处理的高成本而依赖低帧率采样，导致密集时间信息丢失，尤其在需要精确时间对齐的任务中表现不佳。

Method: 提出了Gated Residual Tokenization (GRT)两阶段框架：1) Motion-Compensated Inter-Gated Tokenization跳过静态区域，实现次线性增长的token计算；2) Semantic-Scene Intra-Tokenization Merging融合静态区域的token，减少冗余。

Result: 在DIVE基准测试中，GRT表现优于现有VLLM基线，且随着帧率提升性能持续改善。

Conclusion: GRT框架通过高效处理高帧率视频的时间信息，显著提升了视频理解的性能，并证明了密集时间信息在视频理解中的重要性。

Abstract: High temporal resolution is essential for capturing fine-grained details in
video understanding. However, current video large language models (VLLMs) and
benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or
keyframe selection, discarding dense temporal information. This compromise
avoids the high cost of tokenizing every frame, which otherwise leads to
redundant computation and linear token growth as video length increases. While
this trade-off works for slowly changing content, it fails for tasks like
lecture comprehension, where information appears in nearly every frame and
requires precise temporal alignment. To address this gap, we introduce Dense
Video Understanding (DVU), which enables high-FPS video comprehension by
reducing both tokenization time and token overhead. Existing benchmarks are
also limited, as their QA pairs focus on coarse content changes. We therefore
propose DIVE (Dense Information Video Evaluation), the first benchmark designed
for dense temporal reasoning. To make DVU practical, we present Gated Residual
Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated
Tokenization uses pixel-level motion estimation to skip static regions during
tokenization, achieving sub-linear growth in token count and compute. (2)
Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions
within a scene, further reducing redundancy while preserving dynamic semantics.
Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales
positively with FPS. These results highlight the importance of dense temporal
information and demonstrate that GRT enables efficient, scalable high-FPS video
understanding.

</details>


### [62] [Invisible Yet Detected: PelFANet with Attention-Guided Anatomical Fusion for Pelvic Fracture Diagnosis](https://arxiv.org/abs/2509.13873)
*Siam Tahsin Bhuiyan,Rashedur Rahman,Sefatul Wasi,Naomi Yagi,Syoji Kobashi,Ashraful Islam,Saadia Binte Alam*

Main category: cs.CV

TL;DR: PelFANet通过融合X光片和分割骨图像的双流注意力网络，显著提升了骨盆骨折的诊断准确性，尤其在X光表现不明显的病例中表现优异。


<details>
  <summary>Details</summary>
Motivation: 骨盆骨折在标准X光片上表现不明显或不可见时，诊断存在显著挑战。

Method: 引入了PelFANet，一种双流注意力网络，融合原始骨盆X光片和分割骨图像，采用Fused Attention Blocks（FABlocks）迭代交换和精炼特征。通过两阶段训练流程和分割引导方法进行训练。

Result: 在AMERI数据集上，PelFANet对可见骨折的准确率为88.68%，AUC为0.9334；对不可见骨折的准确率为82.29%，AUC为0.8688，尽管未针对这些病例进行训练。

Conclusion: PelFANet展示了在骨盆骨折诊断中的临床潜力，尤其是在X光表现不明显的病例中，通过解剖感知的双输入架构实现了稳健的骨折检测。

Abstract: Pelvic fractures pose significant diagnostic challenges, particularly in
cases where fracture signs are subtle or invisible on standard radiographs. To
address this, we introduce PelFANet, a dual-stream attention network that fuses
raw pelvic X-rays with segmented bone images to improve fracture
classification. The network em-ploys Fused Attention Blocks (FABlocks) to
iteratively exchange and refine fea-tures from both inputs, capturing global
context and localized anatomical detail. Trained in a two-stage pipeline with a
segmentation-guided approach, PelFANet demonstrates superior performance over
conventional methods. On the AMERI dataset, it achieves 88.68% accuracy and
0.9334 AUC on visible fractures, while generalizing effectively to invisible
fracture cases with 82.29% accuracy and 0.8688 AUC, despite not being trained
on them. These results highlight the clini-cal potential of anatomy-aware
dual-input architectures for robust fracture detec-tion, especially in
scenarios with subtle radiographic presentations.

</details>


### [63] [EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View](https://arxiv.org/abs/2509.13883)
*Zhen Xu,Guorui Lu,Chang Gao,Qinyu Chen*

Main category: cs.CV

TL;DR: EvHand-FPV 通过事件摄像头和轻量级框架，显著提升了手部跟踪的准确性和效率，适用于XR设备。


<details>
  <summary>Details</summary>
Motivation: 解决帧基方法在准确性、低延迟和能效方面的不足，特别是在资源受限的XR设备中。

Method: 构建了一个事件驱动的FPV数据集，结合合成训练数据和真实事件数据，引入手腕ROI和端到端映射策略，以及多任务学习。

Result: 在真实FPV测试集上，2D-AUCp从0.77提升至0.85，参数减少89%，FLOPs降低89%，3D-AUCp保持0.84。

Conclusion: EvHand-FPV 是一种轻量级框架，适用于从单个事件摄像头进行第一人称视角的3D手部跟踪，显著提高了准确性和效率，适合设备端XR应用。

Abstract: Hand tracking holds great promise for intuitive interaction paradigms, but
frame-based methods often struggle to meet the requirements of accuracy, low
latency, and energy efficiency, especially in resource-constrained settings
such as Extended Reality (XR) devices. Event cameras provide $\mu$s-level
temporal resolution at mW-level power by asynchronously sensing brightness
changes. In this work, we present EvHand-FPV, a lightweight framework for
egocentric First-Person-View 3D hand tracking from a single event camera. We
construct an event-based FPV dataset that couples synthetic training data with
3D labels and real event data with 2D labels for evaluation to address the
scarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based
region of interest (ROI) that localizes the hand region via geometric cues,
combined with an end-to-end mapping strategy that embeds ROI offsets into the
network to reduce computation without explicit reconstruction, and a multi-task
learning strategy with an auxiliary geometric feature head that improves
representations without test-time overhead. On our real FPV test set,
EvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from
11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It
also maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results
demonstrate accurate and efficient egocentric event-based hand tracking
suitable for on-device XR applications. The dataset and code are available at
https://github.com/zen5x5/EvHand-FPV.

</details>


### [64] [White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2509.13907)
*Jiyun Im,SuBeen Lee,Miso Lee,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 本文提出WARM模块，通过白化和染色变换优化原型生成，显著提升FS-PCS性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在原型生成过程中存在初始随机性影响性能的问题，且原型生成过程未被充分研究。本文旨在探索一种更先进的原型生成方法。

Method: 提出了一种基于注意力机制的原型生成方法，结合White Aggregation and Restoration Module (WARM)，通过白化和染色变换优化特征对齐。

Result: 在多个FS-PCS基准测试中，WARM方法取得了显著的性能提升，达到了最先进的水平。

Conclusion: 本文提出的WARM模块通过结合白化和染色变换，有效解决了原型生成过程中的分布差距问题，显著提升了FS-PCS任务的性能。

Abstract: Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point
labels for an unlabeled point cloud, given only a few labeled examples. To
extract discriminative representations from the limited support set, existing
methods have constructed prototypes using conventional algorithms such as
farthest point sampling. However, we point out that its initial randomness
significantly affects FS-PCS performance and that the prototype generation
process remains underexplored despite its prevalence. This motivates us to
investigate an advanced prototype generation method based on attention
mechanism. Despite its potential, we found that vanilla module suffers from the
distributional gap between learnable prototypical tokens and support features.
To overcome this, we propose White Aggregation and Restoration Module (WARM),
which resolves the misalignment by sandwiching cross-attention between
whitening and coloring transformations. Specifically, whitening aligns the
support features to prototypical tokens before attention process, and
subsequently coloring restores the original distribution to the attended
tokens. This simple yet effective design enables robust attention, thereby
generating representative prototypes by capturing the semantic relationships
among support features. Our method achieves state-of-the-art performance with a
significant margin on multiple FS-PCS benchmarks, demonstrating its
effectiveness through extensive experiments.

</details>


### [65] [Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration](https://arxiv.org/abs/2509.13919)
*Yuanchen Wu,Ke Yan,Shouhong Ding,Ziyin Zhou,Xiaoqiang Li*

Main category: cs.CV

TL;DR: SRC框架通过理由微调和成对评分策略校准大型视觉语言模型的理由与答案对齐，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在视觉问答中表现出色，但在理由与答案的对齐上存在不足，导致推理不一致和错误响应。

Method: SRC框架包括轻量级的“理由微调”方法、候选响应多样性搜索、基于R-Scorer的成对评分策略以及置信度加权的偏好微调过程。

Result: SRC框架显著提升了模型在多个基准测试中的感知、推理和泛化能力。

Conclusion: 通过引入自校准框架SRC，显著提升了大型视觉语言模型在感知、推理和泛化方面的性能，强调了基于理由的对齐在探索LVLMs潜力中的重要性。

Abstract: Large Vision-Language Models (LVLMs) have manifested strong visual question
answering capability. However, they still struggle with aligning the rationale
and the generated answer, leading to inconsistent reasoning and incorrect
responses. To this end, this paper introduces the Self-Rationale Calibration
(SRC) framework to iteratively calibrate the alignment between rationales and
answers. SRC begins by employing a lightweight "rationale fine-tuning"
approach, which modifies the model's response format to require a rationale
before deriving an answer without explicit prompts. Next, SRC searches for a
diverse set of candidate responses from the fine-tuned LVLMs for each sample,
followed by a proposed pairwise scoring strategy using a tailored scoring
model, R-Scorer, to evaluate both rationale quality and factual consistency of
candidates. Based on a confidence-weighted preference curation process, SRC
decouples the alignment calibration into a preference fine-tuning manner,
leading to significant improvements of LVLMs in perception, reasoning, and
generalization across multiple benchmarks. Our results emphasize the
rationale-oriented alignment in exploring the potential of LVLMs.

</details>


### [66] [Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification](https://arxiv.org/abs/2509.13922)
*Wenkui Yang,Jie Cao,Junxian Duan,Ran He*

Main category: cs.CV

TL;DR: AntiPure是一种抗纯化的保护性扰动方法，通过两种引导机制有效抵御纯化攻击，实验证明其在保持视觉质量的同时最大化失真。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（如Stable Diffusion）的定制能力带来了安全风险（如深度伪造和版权侵权），现有保护性扰动方法易被纯化技术移除，因此需要一种抗纯化的解决方案。

Method: 提出了一种名为AntiPure的诊断性保护扰动方法，通过两种引导机制：1）Patch-wise Frequency Guidance减少模型对高频成分的影响；2）Erroneous Timestep Guidance扰乱模型的去噪策略。

Result: 实验表明，AntiPure在纯化-定制工作流中表现出色，实现了最小感知差异和最大失真，优于其他方法。

Conclusion: AntiPure通过两种引导机制（Patch-wise Frequency Guidance和Erroneous Timestep Guidance）有效抵御纯化攻击，在保持最小感知差异的同时实现最大失真，优于其他保护性扰动方法。

Abstract: Diffusion models like Stable Diffusion have become prominent in visual
synthesis tasks due to their powerful customization capabilities, which also
introduce significant security risks, including deepfakes and copyright
infringement. In response, a class of methods known as protective perturbation
emerged, which mitigates image misuse by injecting imperceptible adversarial
noise. However, purification can remove protective perturbations, thereby
exposing images again to the risk of malicious forgery. In this work, we
formalize the anti-purification task, highlighting challenges that hinder
existing approaches, and propose a simple diagnostic protective perturbation
named AntiPure. AntiPure exposes vulnerabilities of purification within the
"purification-customization" workflow, owing to two guidance mechanisms: 1)
Patch-wise Frequency Guidance, which reduces the model's influence over
high-frequency components in the purified image, and 2) Erroneous Timestep
Guidance, which disrupts the model's denoising strategy across different
timesteps. With additional guidance, AntiPure embeds imperceptible
perturbations that persist under representative purification settings,
achieving effective post-customization distortion. Experiments show that, as a
stress test for purification, AntiPure achieves minimal perceptual discrepancy
and maximal distortion, outperforming other protective perturbation methods
within the purification-customization workflow.

</details>


### [67] [Noise-Level Diffusion Guidance: Well Begun is Half Done](https://arxiv.org/abs/2509.13936)
*Harvey Mannering,Zhiwu Huang,Adam Prugel-Bennett*

Main category: cs.CV

TL;DR: NLG方法通过优化初始噪声提升扩散模型性能，无需额外资源，兼容多种引导形式，实验证明其有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现优异，但初始随机高斯噪声会影响最终输出的图像质量和提示一致性，现有噪声优化方法依赖额外数据集、网络或反向传播，实用性受限。

Method: 提出噪声水平引导（NLG）方法，通过增加初始噪声与通用引导的对齐概率来优化噪声水平，适用于条件和非条件扩散模型，兼容多种扩散级引导形式。

Result: 在五个标准基准测试上的大量实验表明，NLG方法提高了生成质量和输入条件一致性，同时保持计算效率。

Conclusion: 本文提出的噪声水平引导（NLG）方法通过优化初始噪声，无需额外训练数据、辅助网络或反向传播，显著提升了扩散模型的图像生成质量和输入条件一致性，成为一种实用且可扩展的增强方法。

Abstract: Diffusion models have achieved state-of-the-art image generation. However,
the random Gaussian noise used to start the diffusion process influences the
final output, causing variations in image quality and prompt adherence.
Existing noise-level optimization approaches generally rely on extra dataset
construction, additional networks, or backpropagation-based optimization,
limiting their practicality. In this paper, we propose Noise Level Guidance
(NLG), a simple, efficient, and general noise-level optimization approach that
refines initial noise by increasing the likelihood of its alignment with
general guidance - requiring no additional training data, auxiliary networks,
or backpropagation. The proposed NLG approach provides a unified framework
generalizable to both conditional and unconditional diffusion models,
accommodating various forms of diffusion-level guidance. Extensive experiments
on five standard benchmarks demonstrate that our approach enhances output
generation quality and input condition adherence. By seamlessly integrating
with existing guidance methods while maintaining computational efficiency, our
method establishes NLG as a practical and scalable enhancement to diffusion
models. Code can be found at
https://github.com/harveymannering/NoiseLevelGuidance.

</details>


### [68] [Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation](https://arxiv.org/abs/2509.13939)
*Gia Khanh Nguyen,Yifeng Huang,Minh Hoai*

Main category: cs.CV

TL;DR: PairTally是一个专为评估细粒度视觉计数设计的数据集，揭示了当前模型在此类任务中的局限性，为未来改进提供了方向。


<details>
  <summary>Details</summary>
Motivation: 当前模型在细粒度和意图驱动的计数任务中表现不佳，需要一个新的基准数据集来评估和改进这些能力。

Method: 引入PairTally数据集，包含681张高分辨率图像，每张图像包含两个物体类别，要求模型基于形状、大小、颜色或语义的细微差异进行区分和计数。

Result: 实验表明，尽管现有模型有所进展，但在细粒度和视觉模糊情况下仍难以可靠地完成用户意图驱动的计数任务。

Conclusion: PairTally数据集为细粒度视觉计数系统的诊断和改进提供了新的基础，揭示了当前模型在用户意图驱动计数方面的不足。

Abstract: Visual counting is a fundamental yet challenging task, especially when users
need to count objects of a specific type in complex scenes. While recent
models, including class-agnostic counting models and large vision-language
models (VLMs), show promise in counting tasks, their ability to perform
fine-grained, intent-driven counting remains unclear. In this paper, we
introduce PairTally, a benchmark dataset specifically designed to evaluate
fine-grained visual counting. Each of the 681 high-resolution images in
PairTally contains two object categories, requiring models to distinguish and
count based on subtle differences in shape, size, color, or semantics. The
dataset includes both inter-category (distinct categories) and intra-category
(closely related subcategories) settings, making it suitable for rigorous
evaluation of selective counting capabilities. We benchmark a variety of
state-of-the-art models, including exemplar-based methods, language-prompted
models, and large VLMs. Our results show that despite recent advances, current
models struggle to reliably count what users intend, especially in fine-grained
and visually ambiguous cases. PairTally provides a new foundation for
diagnosing and improving fine-grained visual counting systems.

</details>


### [69] [Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments](https://arxiv.org/abs/2509.14012)
*Tamara R. Lenhard,Andreas Weinmann,Tobias Koch*

Main category: cs.CV

TL;DR: 增强版YOLO-FEDER FusionNet通过改进数据、特征融合和骨干设计，显著提升复杂环境中的无人机检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉复杂环境中无人机检测的挑战，如背景杂波、小目标尺度和伪装效应，提升检测器在低目标-背景可分性场景中的性能。

Method: 提出YOLO-FEDER FusionNet的增强版本，改进训练数据组合、特征融合策略和骨干网络设计，利用大规模合成数据和少量真实样本进行训练。

Result: 最佳配置（YOLOv8l骨干和DWD模块的FEDER特征）相比基线，FNR降低39.1个百分点，mAP提升62.8个百分点。

Conclusion: 集成中间FEDER特征和骨干网络升级的YOLO-FEDER FusionNet在视觉复杂环境中显著提升了无人机检测性能，FNR降低达39.1个百分点，mAP提升达62.8个百分点。

Abstract: Drone detection in visually complex environments remains challenging due to
background clutter, small object scale, and camouflage effects. While generic
object detectors like YOLO exhibit strong performance in low-texture scenes,
their effectiveness degrades in cluttered environments with low
object-background separability. To address these limitations, this work
presents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework
that integrates generic object detection with camouflage object detection
techniques. Building upon the original architecture, the proposed iteration
introduces systematic advancements in training data composition, feature fusion
strategies, and backbone design. Specifically, the training process leverages
large-scale, photo-realistic synthetic data, complemented by a small set of
real-world samples, to enhance robustness under visually complex conditions.
The contribution of intermediate multi-scale FEDER features is systematically
evaluated, and detection performance is comprehensively benchmarked across
multiple YOLO-based backbone configurations. Empirical results indicate that
integrating intermediate FEDER features, in combination with backbone upgrades,
contributes to notable performance improvements. In the most promising
configuration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER
features derived from the DWD module -- these enhancements lead to a FNR
reduction of up to 39.1 percentage points and a mAP increase of up to 62.8
percentage points at an IoU threshold of 0.5, compared to the initial baseline.

</details>


### [70] [SAIL-VL2 Technical Report](https://arxiv.org/abs/2509.14033)
*Weijie Yin,Yongjie Ye,Fangxun Shu,Yue Liao,Zijian Kang,Hongyuan Dong,Haiyang Yu,Dingkang Yang,Jiacong Wang,Han Wang,Wenzhuo Liu,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: SAIL-VL2是一个先进的多模态基础模型，通过数据筛选、渐进训练和MoE设计，在多个基准上实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个全面的多模态理解和推理基础模型，以提升从细粒度感知到复杂推理的能力。

Method: 通过大规模数据筛选流程、渐进式训练框架（包括预训练视觉编码器、多模态预训练和思考融合SFT-RL混合范式）以及高效的稀疏Mixture-of-Experts（MoE）设计。

Result: SAIL-VL2在OpenCompass排行榜上，2B参数版本在4B参数规模以下的官方开源模型中排名第一。

Conclusion: SAIL-VL2作为SAIL-VL的继任者，在2B和8B参数规模下实现了最先进的性能，并在106个数据集上展示了竞争力，特别是在MMMU和MathVista等挑战性推理基准上。

Abstract: We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning. As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning. Three
core innovations drive its effectiveness. First, a large-scale data curation
pipeline with scoring and filtering strategies enhances both quality and
distribution across captioning, OCR, QA, and video data, improving training
efficiency. Second, a progressive training framework begins with a powerful
pre-trained vision encoder (SAIL-ViT), advances through multimodal
pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that
systematically strengthens model capabilities. Third, architectural advances
extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.
With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass
leaderboard, SAIL-VL2-2B ranks first among officially released open-source
models under the 4B parameter scale, while serving as an efficient and
extensible foundation for the open-source multimodal community.

</details>


### [71] [PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd Multi-modal Embeddings](https://arxiv.org/abs/2509.14051)
*Suhang You,Carla Pitarch-Abaigar,Sanket Kachole,Sumedh Sonawane,Juhyung Ha,Anish Sudarshan Gada,David Crandall,Rakesh Shiradkar,Spyridon Bakas*

Main category: cs.CV

TL;DR: PROFUSEme, a multi-modal fusion method, excels in predicting prostate cancer recurrence post-surgery, offering better clinical outcomes.


<details>
  <summary>Details</summary>
Motivation: Nearly 30% of prostate cancer patients experience BCR after radical prostatectomy, leading to increased mortality. Early and accurate prediction of BCR is crucial for timely clinical interventions.

Method: The approach involves fused multi-modal embeddings (PROFUSEme) that learn cross-modal interactions of clinical, radiology, and pathology data, using an intermediate fusion configuration with Cox Proportional Hazard regressors.

Result: PROFUSEme achieved a mean C-index of 0.861 on internal validation and 0.7103 on external validation, outperforming late fusion configurations.

Conclusion: PROFUSEme demonstrates superior performance in predicting biochemical recurrence (BCR) in prostate cancer patients post-radical prostatectomy, highlighting its potential for improving clinical decision-making and patient outcomes.

Abstract: Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy
(RP) experience biochemical recurrence (BCR), characterized by increased
prostate specific antigen (PSA) and associated with increased mortality.
Accurate early prediction of BCR, at the time of RP, would contribute to prompt
adaptive clinical decision-making and improved patient outcomes. In this work,
we propose prostate cancer BCR prediction via fused multi-modal embeddings
(PROFUSEme), which learns cross-modal interactions of clinical, radiology, and
pathology data, following an intermediate fusion configuration in combination
with Cox Proportional Hazard regressors. Quantitative evaluation of our
proposed approach reveals superior performance, when compared with late fusion
configurations, yielding a mean C-index of 0.861 ($\sigma=0.112$) on the
internal 5-fold nested cross-validation framework, and a C-index of 0.7103 on
the hold out data of CHIMERA 2025 challenge validation leaderboard.

</details>


### [72] [Wan-Animate: Unified Character Animation and Replacement with Holistic Replication](https://arxiv.org/abs/2509.14055)
*Gang Cheng,Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Ju Li,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Feng Wang,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou,Lian Zhuo*

Main category: cs.CV

TL;DR: Wan-Animate是一个统一的角色动画和替换框架，通过精确复制表情和动作，并利用Relighting LoRA模块实现环境整合，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 为角色动画和替换提供一个统一的框架，能够精确复制视频中的表情和动作，并实现无缝的环境整合。

Method: 基于Wan模型，采用修改后的输入范式区分参考条件和生成区域，使用空间对齐的骨架信号复制身体动作，隐式面部特征重现表情，并开发了辅助Relighting LoRA模块以增强环境整合。

Result: 实验结果表明，Wan-Animate在角色动画和替换任务中达到了最先进的性能。

Conclusion: Wan-Animate通过统一的符号表示和高可控性，实现了角色动画和替换的最先进性能，并承诺开源模型权重和源代码。

Abstract: We introduce Wan-Animate, a unified framework for character animation and
replacement. Given a character image and a reference video, Wan-Animate can
animate the character by precisely replicating the expressions and movements of
the character in the video to generate high-fidelity character videos.
Alternatively, it can integrate the animated character into the reference video
to replace the original character, replicating the scene's lighting and color
tone to achieve seamless environmental integration. Wan-Animate is built upon
the Wan model. To adapt it for character animation tasks, we employ a modified
input paradigm to differentiate between reference conditions and regions for
generation. This design unifies multiple tasks into a common symbolic
representation. We use spatially-aligned skeleton signals to replicate body
motion and implicit facial features extracted from source images to reenact
expressions, enabling the generation of character videos with high
controllability and expressiveness. Furthermore, to enhance environmental
integration during character replacement, we develop an auxiliary Relighting
LoRA. This module preserves the character's appearance consistency while
applying the appropriate environmental lighting and color tone. Experimental
results demonstrate that Wan-Animate achieves state-of-the-art performance. We
are committed to open-sourcing the model weights and its source code.

</details>


### [73] [VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement](https://arxiv.org/abs/2509.14060)
*Jun Du,Weiwei Xing,Ming Li,Fei Richard Yu*

Main category: cs.CV

TL;DR: VSE-MOT框架通过视觉语义增强提升低质量视频中的多目标跟踪性能，性能优于现有方法8%-20%。


<details>
  <summary>Details</summary>
Motivation: 现有MOT算法在低质量视频中性能显著下降，限制了其在真实场景中的应用，因此需要一种能够适应低质量视频的改进方法。

Method: 提出了一种基于视觉语言模型的三分支架构，包括MOT-Adapter和VSFM模块，用于提取和融合全局视觉语义信息。

Result: 实验验证了VSE-MOT在低质量视频中的有效性和优越性，性能指标显著提升。

Conclusion: VSE-MOT框架在低质量视频场景中显著提升了多目标跟踪性能，其性能指标优于现有方法8%至20%，同时在常规场景中保持稳健表现。

Abstract: Current multi-object tracking (MOT) algorithms typically overlook issues
inherent in low-quality videos, leading to significant degradation in tracking
performance when confronted with real-world image deterioration. Therefore,
advancing the application of MOT algorithms in real-world low-quality video
scenarios represents a critical and meaningful endeavor. To address the
challenges posed by low-quality scenarios, inspired by vision-language models,
this paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking
framework (VSE-MOT). Specifically, we first design a tri-branch architecture
that leverages a vision-language model to extract global visual semantic
information from images and fuse it with query vectors. Subsequently, to
further enhance the utilization of visual semantic information, we introduce
the Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion
Module (VSFM). The MOT-Adapter adapts the extracted global visual semantic
information to suit multi-object tracking tasks, while the VSFM improves the
efficacy of feature fusion. Through extensive experiments, we validate the
effectiveness and superiority of the proposed method in real-world low-quality
video scenarios. Its tracking performance metrics outperform those of existing
methods by approximately 8% to 20%, while maintaining robust performance in
conventional scenarios.

</details>


### [74] [AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration](https://arxiv.org/abs/2509.14084)
*Jingyi Yuan,Jianxiong Ye,Wenkang Chen,Chenqiang Gao*

Main category: cs.CV

TL;DR: AD-DINOv3 结合 DINOv3 和 CLIP，通过适配器和 AACM 解决特征对齐与语义偏差问题，在零样本异常检测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统零样本异常检测方法依赖 CLIP 模型，但存在特征对齐不足和全局语义偏差问题。DINOv3 的强可迁移性为解决问题提供了新方向，但直接应用面临域偏差和语义偏差挑战。

Method: AD-DINOv3 是一种多模态对比学习框架，利用 DINOv3 提取视觉特征，CLIP 提供文本嵌入，并通过轻量级适配器和异常感知校准模块（AACM）优化特征对齐和异常区域识别。

Result: 在八个工业和医学基准测试中，AD-DINOv3 表现一致优于或匹配现有最优方法，证明了其作为通用零样本异常检测框架的有效性。

Conclusion: AD-DINOv3 通过结合 DINOv3 和 CLIP 的优势，并引入轻量级适配器和异常感知校准模块，成功解决了特征对齐和语义偏差问题，在零样本异常检测任务中表现优异，验证了其作为一种通用框架的优越性。

Abstract: Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary
novel categories, offering a scalable and annotation-efficient solution.
Traditionally, most ZSAD works have been based on the CLIP model, which
performs anomaly detection by calculating the similarity between visual and
text embeddings. Recently, vision foundation models such as DINOv3 have
demonstrated strong transferable representation capabilities. In this work, we
are the first to adapt DINOv3 for ZSAD. However, this adaptation presents two
key challenges: (i) the domain bias between large-scale pretraining data and
anomaly detection tasks leads to feature misalignment; and (ii) the inherent
bias toward global semantics in pretrained representations often leads to
subtle anomalies being misinterpreted as part of the normal foreground objects,
rather than being distinguished as abnormal regions. To overcome these
challenges, we introduce AD-DINOv3, a novel vision-language multimodal
framework designed for ZSAD. Specifically, we formulate anomaly detection as a
multimodal contrastive learning problem, where DINOv3 is employed as the visual
backbone to extract patch tokens and a CLS token, and the CLIP text encoder
provides embeddings for both normal and abnormal prompts. To bridge the domain
gap, lightweight adapters are introduced in both modalities, enabling their
representations to be recalibrated for the anomaly detection task. Beyond this
baseline alignment, we further design an Anomaly-Aware Calibration Module
(AACM), which explicitly guides the CLS token to attend to anomalous regions
rather than generic foreground semantics, thereby enhancing discriminability.
Extensive experiments on eight industrial and medical benchmarks demonstrate
that AD-DINOv3 consistently matches or surpasses state-of-the-art methods,
verifying its superiority as a general zero-shot anomaly detection framework.

</details>


### [75] [Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing](https://arxiv.org/abs/2509.14097)
*Yaru Chen,Ruohao Guo,Liting Gao,Yang Xiang,Qingyu Luo,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 提出EMA伪监督框架和CMA损失，提升弱监督音视频视频解析性能，在LLP和UnAV-100数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决先前工作中忽视的稳定段级监督和类感知跨模态对齐问题。

Method: 提出两种策略：指数移动平均（EMA）引导的伪监督框架和类感知跨模态一致性（CMA）损失。

Result: 在多个指标上实现了最先进的性能。

Conclusion: 该方法在LLP和UnAV-100数据集上实现了最先进的性能。

Abstract: Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,
visible, and audio-visual events without temporal annotations. Previous work
has emphasized refining global predictions through contrastive or collaborative
learning, but neglected stable segment-level supervision and class-aware
cross-modal alignment. To address this, we propose two strategies: (1) an
exponential moving average (EMA)-guided pseudo supervision framework that
generates reliable segment-level masks via adaptive thresholds or top-k
selection, offering stable temporal guidance beyond video-level labels; and (2)
a class-aware cross-modal agreement (CMA) loss that aligns audio and visual
embeddings at reliable segment-class pairs, ensuring consistency across
modalities while preserving temporal structure. Evaluations on LLP and UnAV-100
datasets shows that our method achieves state-of-the-art (SOTA) performance
across multiple metrics.

</details>


### [76] [CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts](https://arxiv.org/abs/2509.14104)
*Leonard Hackel,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 提出了一种整合Soft MoE机制的CSMoE模型，显著提高了遥感基础模型的计算效率，同时保持或提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感基础模型在训练和推理过程中计算复杂度高或表示能力有限，限制了其在实际应用中的实用性。

Method: 提出了一种改进遥感基础模型效率的方法，通过将Soft MoE机制整合到Cross-Sensor Masked Autoencoder (CSMAE)模型中，形成Cross-Sensor Mixture-of-Experts (CSMoE)模型。此外，引入了一种基于主题气候描述符的采样策略来构建具有代表性和多样性的训练集。

Result: CSMoE模型在场景分类、语义分割和基于内容的图像检索等实验中的表现优于现有遥感基础模型，计算效率提高了两倍以上，同时保持了竞争力。

Conclusion: 通过将Soft MoE机制整合到基础模型中，提出的CSMoE模型在减少计算需求的同时保持或提高了表示性能，展示了在创建计算高效的遥感基础模型方面的有效性。

Abstract: Self-supervised learning through masked autoencoders has attracted great
attention for remote sensing (RS) foundation model (FM) development, enabling
improved representation learning across diverse sensors and downstream tasks.
However, existing RS FMs often either suffer from substantial computational
complexity during both training and inference or exhibit limited
representational capacity. These issues restrict their practical applicability
in RS. To address this limitation, we propose an adaptation for enhancing the
efficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism
into the FM. The integration of Soft MoEs into the FM allows modality-specific
expert specialization alongside shared cross-sensor representation learning. To
demonstrate the effectiveness of our adaptation, we apply it on the
Cross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor
Mixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic
descriptor-driven sampling strategy for the construction of a representative
and diverse training set to train our CSMoE model. Extensive experiments on
scene classification, semantic segmentation, and content-based image retrieval
demonstrate that our adaptation yields a reduction in computational
requirements while maintaining or improving representational performance.
Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off
between representational capacity, accuracy, and computational efficiency. On
average, CSMoE achieves more than twice the computational efficiency of
existing RS FMs, while maintaining competitive performance across all
experiments. These results show the effectiveness of the proposed adaptation
for creating computationally efficient RS FMs. The code for the model, the
training set creation, and the model weights will be available at
https://git.tu-berlin.de/rsim/csmoe.

</details>


### [77] [Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows](https://arxiv.org/abs/2509.14119)
*Jiabo MA,Wenqiang Li,Jinbang Li,Ziyi Liu,Linshan Wu,Fengtao Zhou,Li Liang,Ronald Cheong Kin Chan,Terence T. W. Wong,Hao Chen*

Main category: cs.CV

TL;DR: 提出一种基于级联配准的虚拟染色框架，解决数据不匹配问题，显著提升性能并简化数据采集。


<details>
  <summary>Details</summary>
Motivation: 传统组织染色过程耗时、耗力且对环境不友好，而现有虚拟染色方法依赖配对数据，难以实现精确的像素级监督。

Method: 采用级联配准机制的虚拟染色框架，以解决未配对或粗略配对数据带来的空间不匹配问题。

Result: 在五个数据集上显著优于现有方法，内部数据集平均提升3.2%，外部数据集提升10.1%，在严重不对齐数据上PSNR提升23.8%。

Conclusion: 提出的虚拟染色框架通过级联配准机制解决了生成输出与真实数据之间的空间不匹配问题，显著提升了虚拟染色的准确性和鲁棒性，简化了数据采集过程。

Abstract: Accurate histopathological diagnosis often requires multiple differently
stained tissue sections, a process that is time-consuming, labor-intensive, and
environmentally taxing due to the use of multiple chemical stains. Recently,
virtual staining has emerged as a promising alternative that is faster,
tissue-conserving, and environmentally friendly. However, existing virtual
staining methods face significant challenges in clinical applications,
primarily due to their reliance on well-aligned paired data. Obtaining such
data is inherently difficult because chemical staining processes can distort
tissue structures, and a single tissue section cannot undergo multiple staining
procedures without damage or loss of information. As a result, most available
virtual staining datasets are either unpaired or roughly paired, making it
difficult for existing methods to achieve accurate pixel-level supervision. To
address this challenge, we propose a robust virtual staining framework
featuring cascaded registration mechanisms to resolve spatial mismatches
between generated outputs and their corresponding ground truth. Experimental
results demonstrate that our method significantly outperforms state-of-the-art
models across five datasets, achieving an average improvement of 3.2% on
internal datasets and 10.1% on external datasets. Moreover, in datasets with
substantial misalignment, our approach achieves a remarkable 23.8% improvement
in peak signal-to-noise ratio compared to baseline models. The exceptional
robustness of the proposed method across diverse datasets simplifies the data
acquisition process for virtual staining and offers new insights for advancing
its development.

</details>


### [78] [Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection](https://arxiv.org/abs/2509.14120)
*Sara Concas,Simone Maurizio La Cava,Andrea Panzino,Ester Masala,Giulia Orrù,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 研究显示社交媒体美容滤镜会影响深度伪造和变形攻击检测器的性能，导致检测效果下降，需开发更鲁棒的模型。


<details>
  <summary>Details</summary>
Motivation: 数字美化通过社交媒体滤镜变得越来越流行，引发了对面部图像和视频可靠性以及自动化面部分析有效性的担忧，特别是在涉及深度伪造和变形攻击的情况下。

Method: 对多个最先进的检测器在基准数据集上进行了全面分析，评估了应用各种平滑滤镜前后的性能。

Result: 研究发现美容滤镜会导致检测器性能下降。

Conclusion: 美容滤镜会影响深度伪造和变形攻击检测器的性能，揭示了面部增强带来的脆弱性，并强调了需要对这些改变具有鲁棒性的检测模型。

Abstract: Digital beautification through social media filters has become increasingly
popular, raising concerns about the reliability of facial images and videos and
the effectiveness of automated face analysis. This issue is particularly
critical for digital manipulation detectors, systems aiming at distinguishing
between genuine and manipulated data, especially in cases involving deepfakes
and morphing attacks designed to deceive humans and automated facial
recognition. This study examines whether beauty filters impact the performance
of deepfake and morphing attack detectors. We perform a comprehensive analysis,
evaluating multiple state-of-the-art detectors on benchmark datasets before and
after applying various smoothing filters. Our findings reveal performance
degradation, highlighting vulnerabilities introduced by facial enhancements and
underscoring the need for robust detection models resilient to such
alterations.

</details>


### [79] [MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook](https://arxiv.org/abs/2509.14142)
*Peng Xu,Shengwu Xiong,Jiajun Zhang,Yaxiong Chen,Bowen Zhou,Chen Change Loy,David A. Clifton,Kyoung Mu Lee,Luc Van Gool,Ruiming He,Ruilin Yao,Xinwei Long,Jirui Huang,Kai Tian,Sa Yang,Yihua Shao,Jin Feng,Yue Zhong,Jiakai Zhou,Cheng Tang,Tianyu Zou,Yifang Zhang,Junming Liang,Guoyou Li,Zhaoxiang Wang,Qiang Zhou,Yichen Zhao,Shili Xiong,Hyeongjin Nam,Jaerin Lee,Jaeyoung Chung,JoonKyu Park,Junghun Oh,Kanggeon Lee,Wooseok Lee,Juneyoung Ro,Turghun Osman,Can Hu,Chaoyang Liao,Cheng Chen,Chengcheng Han,Chenhao Qiu,Chong Peng,Cong Xu,Dailin Li,Feiyu Wang,Feng Gao,Guibo Zhu,Guopeng Tang,Haibo Lu,Han Fang,Han Qi,Hanxiao Wu,Haobo Cheng,Hongbo Sun,Hongyao Chen,Huayong Hu,Hui Li,Jiaheng Ma,Jiang Yu,Jianing Wang,Jie Yang,Jing He,Jinglin Zhou,Jingxuan Li,Josef Kittler,Lihao Zheng,Linnan Zhao,Mengxi Jia,Muyang Yan,Nguyen Thanh Thien,Pu Luo,Qi Li,Shien Song,Shijie Dong,Shuai Shao,Shutao Li,Taofeng Xue,Tianyang Xu,Tianyi Gao,Tingting Li,Wei Zhang,Weiyang Su,Xiaodong Dong,Xiao-Jun Wu,Xiaopeng Zhou,Xin Chen,Xin Wei,Xinyi You,Xudong Kang,Xujie Zhou,Xusheng Liu,Yanan Wang,Yanbin Huang,Yang Liu,Yang Yang,Yanglin Deng,Yashu Kang,Ye Yuan,Yi Wen,Yicen Tian,Yilin Tao,Yin Tang,Yipeng Lin,Yiqing Wang,Yiting Xi,Yongkang Yu,Yumei Li,Yuxin Qin,Yuying Chen,Yuzhe Cen,Zhaofan Zou,Zhaohong Liu,Zhehao Shen,Zhenglin Du,Zhengyang Li,Zhenni Huang,Zhenwei Shao,Zhilong Song,Zhiyong Feng,Zhiyu Wang,Zhou Yu,Ziang Li,Zihan Zhai,Zijian Zhang,Ziyang Peng,Ziyun Xiao,Zongshu Li*

Main category: cs.CV

TL;DR: MARS2 2025挑战赛通过多模态基准测试和公开资源推动了多模态推理研究，吸引了大量团队参与。


<details>
  <summary>Details</summary>
Motivation: 旨在通过大规模基准测试整合多模态机器学习和LLMs的不同方法，推动这一快速发展领域的研究。

Method: 通过发布两个定制数据集Lens和AdsQA，并在12个日常场景和广告视频中评估40多个基线模型，组织了三个竞赛赛道。

Result: 76个团队注册，40多个有效提交被纳入排名，数据集和代码集公开。

Conclusion: 本文总结了MARS2 2025挑战赛在推动多模态推理领域的进展，通过公开数据集、代码集和排名，促进了研究社区的发展。

Abstract: This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim
to bring together different approaches in multimodal machine learning and LLMs
via a large benchmark. We hope it better allows researchers to follow the
state-of-the-art in this very dynamic area. Meanwhile, a growing number of
testbeds have boosted the evolution of general-purpose large language models.
Thus, this year's MARS2 focuses on real-world and specialized scenarios to
broaden the multimodal reasoning applications of MLLMs. Our organizing team
released two tailored datasets Lens and AdsQA as test sets, which support
general reasoning in 12 daily scenarios and domain-specific reasoning in
advertisement videos, respectively. We evaluated 40+ baselines that include
both generalist MLLMs and task-specific models, and opened up three competition
tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question
Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative
Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and
industrial institutions have registered and 40+ valid submissions (out of
1200+) have been included in our ranking lists. Our datasets, code sets (40+
baselines and 15+ participants' methods), and rankings are publicly available
on the MARS2 workshop website and our GitHub organization page
https://github.com/mars2workshop/, where our updates and announcements of
upcoming events will be continuously provided.

</details>


### [80] [An Exploratory Study on Abstract Images and Visual Representations Learned from Them](https://arxiv.org/abs/2509.14149)
*Haotian Li,Jianbo Jiao*

Main category: cs.CV

TL;DR: 本文探讨了抽象图像与传统栅格图像的性能差距，引入HAID数据集进行多层次抽象研究，评估了抽象图像在视觉任务中的潜在有效性。


<details>
  <summary>Details</summary>
Motivation: 研究抽象图像与传统栅格图像在性能上的差距原因，探讨不同抽象级别下高层次语义内容的捕捉能力，并评估抽象图像在视觉任务中的潜在有效性。

Method: 本文引入了层次抽象图像数据集（HAID），该数据集包含从正常栅格图像生成的多个抽象级别的抽象图像。随后，在HAID上训练和评估了传统的视觉系统，涵盖了分类、分割和对象检测等多种任务。

Result: 研究表明，抽象图像能够传递视觉语义信息，但在性能上仍不及传统栅格图像。HAID数据集为多层次抽象研究提供了全面支持。

Conclusion: 抽象图像虽然能够传递一定的视觉语义信息，但在性能上仍不及传统栅格图像。通过HAID数据集的多层次抽象研究，本文探讨了不同抽象级别下高层次语义内容的捕捉能力，并评估了抽象图像在视觉任务中的潜在有效性。

Abstract: Imagine living in a world composed solely of primitive shapes, could you
still recognise familiar objects? Recent studies have shown that abstract
images-constructed by primitive shapes-can indeed convey visual semantic
information to deep learning models. However, representations obtained from
such images often fall short compared to those derived from traditional raster
images. In this paper, we study the reasons behind this performance gap and
investigate how much high-level semantic content can be captured at different
abstraction levels. To this end, we introduce the Hierarchical Abstraction
Image Dataset (HAID), a novel data collection that comprises abstract images
generated from normal raster images at multiple levels of abstraction. We then
train and evaluate conventional vision systems on HAID across various tasks
including classification, segmentation, and object detection, providing a
comprehensive study between rasterised and abstract image representations. We
also discuss if the abstract image can be considered as a potentially effective
format for conveying visual semantic information and contributing to vision
tasks.

</details>


### [81] [BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection](https://arxiv.org/abs/2509.14151)
*Rongyu Zhang,Jiaming Liu,Xiaoqi Li,Xiaowei Chi,Dan Wang,Li Du,Yuan Du,Shanghang Zhang*

Main category: cs.CV

TL;DR: BEVUDA++框架通过几何感知师生模型和UEMA方法，解决了BEV感知的域适应问题，显著提升了跨域3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: BEV感知在自动驾驶中具有重要潜力，但现有研究忽视了域适应问题，导致跨域场景下性能下降。本文首次针对多视角3D目标检测中的BEV域适应问题展开研究。

Method: 引入几何感知的师生框架BEVUDA++，包括可靠的深度教师（RDT）和几何一致学生（GCS）模型，以及不确定性引导的指数移动平均（UEMA）方法。

Result: 在四个跨域场景中进行的实验表明，BEVUDA++在BEV 3D目标检测任务中取得了最先进的性能，例如在昼夜适应任务中NDS提升12.9%，mAP提升9.5%。

Conclusion: 本文提出的BEVUDA++框架通过几何感知的师生模型和不确定性引导的指数移动平均方法，有效解决了BEV感知中的域适应问题，显著提升了跨域场景下的3D目标检测性能。

Abstract: Vision-centric Bird's Eye View (BEV) perception holds considerable promise
for autonomous driving. Recent studies have prioritized efficiency or accuracy
enhancements, yet the issue of domain shift has been overlooked, leading to
substantial performance degradation upon transfer. We identify major domain
gaps in real-world cross-domain scenarios and initiate the first effort to
address the Domain Adaptation (DA) challenge in multi-view 3D object detection
for BEV perception. Given the complexity of BEV perception approaches with
their multiple components, domain shift accumulation across multi-geometric
spaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain
adaptation. In this paper, we introduce an innovative geometric-aware
teacher-student framework, BEVUDA++, to diminish this issue, comprising a
Reliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.
Specifically, RDT effectively blends target LiDAR with dependable depth
predictions to generate depth-aware information based on uncertainty
estimation, enhancing the extraction of Voxel and BEV features that are
essential for understanding the target domain. To collaboratively reduce the
domain shift, GCS maps features from multiple spaces into a unified geometric
embedding space, thereby narrowing the gap in data distribution between the two
domains. Additionally, we introduce a novel Uncertainty-guided Exponential
Moving Average (UEMA) to further reduce error accumulation due to domain shifts
informed by previously obtained uncertainty guidance. To demonstrate the
superiority of our proposed method, we execute comprehensive experiments in
four cross-domain scenarios, securing state-of-the-art performance in BEV 3D
object detection tasks, e.g., 12.9\% NDS and 9.5\% mAP enhancement on Day-Night
adaptation.

</details>


### [82] [Cinéaste: A Fine-grained Contextual Movie Question Answering Benchmark](https://arxiv.org/abs/2509.14227)
*Nisarg A. Shah,Amir Ziai,Chaitanya Ekanadham,Vishal M. Patel*

Main category: cs.CV

TL;DR: $\mathsf{Cin\acute{e}aste}$是一个用于长形式电影理解的基准数据集，现有模型表现不佳，突显了长程时间推理的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估长形式叙事内容的细粒度推理能力方面存在不足。

Method: 引入了$\mathsf{Cin\acute{e}aste}$基准数据集，包含3,119个多选题对，源自200部电影的1,805个场景，覆盖五个新颖的细粒度推理类别。使用GPT-4o生成问题，并通过两阶段过滤确保质量。

Result: 现有模型在$\mathsf{Cin\acute{e}aste}$上表现不佳，最佳开源模型准确率仅为63.15%。

Conclusion: 现有的多模态语言模型在长形式电影理解方面存在显著挑战，尤其是在长程时间推理方面。

Abstract: While recent advancements in vision-language models have improved video
understanding, diagnosing their capacity for deep, narrative comprehension
remains a challenge. Existing benchmarks often test short-clip recognition or
use template-based questions, leaving a critical gap in evaluating fine-grained
reasoning over long-form narrative content. To address these gaps, we introduce
$\mathsf{Cin\acute{e}aste}$, a comprehensive benchmark for long-form movie
understanding. Our dataset comprises 3,119 multiple-choice question-answer
pairs derived from 1,805 scenes across 200 diverse movies, spanning five novel
fine-grained contextual reasoning categories. We use GPT-4o to generate
diverse, context-rich questions by integrating visual descriptions, captions,
scene titles, and summaries, which require deep narrative understanding. To
ensure high-quality evaluation, our pipeline incorporates a two-stage filtering
process: Context-Independence filtering ensures questions require video
context, while Contextual Veracity filtering validates factual consistency
against the movie content, mitigating hallucinations. Experiments show that
existing MLLMs struggle on $\mathsf{Cin\acute{e}aste}$; our analysis reveals
that long-range temporal reasoning is a primary bottleneck, with the top
open-source model achieving only 63.15\% accuracy. This underscores significant
challenges in fine-grained contextual understanding and the need for
advancements in long-form movie comprehension.

</details>


### [83] [GenExam: A Multidisciplinary Text-to-Image Exam](https://arxiv.org/abs/2509.14232)
*Zhaokai Wang,Penghao Yin,Xiangyu Zhao,Changyao Tian,Yu Qiao,Wenhai Wang,Jifeng Dai,Gen Luo*

Main category: cs.CV

TL;DR: GenExam 是首个多学科文本到图像考试基准，实验显示当前模型在严格绘图考试中表现极差，突显了知识整合与生成的巨大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有考试风格基准主要关注理解和推理任务，而生成基准侧重于世界知识和视觉概念的展示，缺乏对严格绘图考试的评估。

Method: 引入 GenExam 基准，包含 10 个学科的 1,000 个样本，采用四层级分类的考试风格提示，每个问题配备真实图像和细粒度评分标准。

Result: 实验表明，即使是 GPT-Image-1 和 Gemini-2.5-Flash-Image 等最先进模型，严格得分也低于 15%，大多数模型得分接近 0%。

Conclusion: GenExam 提供了一个严格的多学科文本到图像生成考试基准，揭示了当前最先进模型在知识整合、推理和生成能力上的不足，为通用人工智能的发展提供了方向。

Abstract: Exams are a fundamental test of expert-level intelligence and require
integrated understanding, reasoning, and generation. Existing exam-style
benchmarks mainly focus on understanding and reasoning tasks, and current
generation benchmarks emphasize the illustration of world knowledge and visual
concepts, neglecting the evaluation of rigorous drawing exams. We introduce
GenExam, the first benchmark for multidisciplinary text-to-image exams,
featuring 1,000 samples across 10 subjects with exam-style prompts organized
under a four-level taxonomy. Each problem is equipped with ground-truth images
and fine-grained scoring points to enable a precise evaluation of semantic
correctness and visual plausibility. Experiments show that even
state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve
less than 15% strict scores, and most models yield almost 0%, suggesting the
great challenge of our benchmark. By framing image generation as an exam,
GenExam offers a rigorous assessment of models' ability to integrate knowledge,
reasoning, and generation, providing insights on the path to general AGI.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [84] [A User-centric Kubernetes-based Architecture for Green Cloud Computing](https://arxiv.org/abs/2509.13325)
*Matteo Zanotto,Leonardo Vicentini,Redi Vreto,Francesco Lumpp,Diego Braga,Sandro Fiore*

Main category: cs.DC

TL;DR: 本文提出了一种基于Kubernetes的绿色云计算架构，通过碳强度预测和绿色能源调度，实现了13%的碳排放减少。


<details>
  <summary>Details</summary>
Motivation: 随着数据中心规模和数量的增加，电力消耗和碳排放问题日益突出。尽管云服务提供商在能效方面已接近最优，但在可持续性报告方面仍有不足，因此需要从用户侧进一步改进。

Method: 采用碳强度预测器，并基于绿色能源的可用性调度工作负载，利用区域和时间差异最小化碳排放。

Result: 在严格资源限制的场景下，该系统可实现高达13%的碳排放减少。

Conclusion: 本文提出了一种以用户为中心的、基于Kubernetes的绿色云计算架构，通过利用碳强度预测器和绿色能源可用性调度工作负载，实现了碳排放的显著减少。

Abstract: To meet the increasing demand for cloud computing services, the scale and
number of data centers keeps increasing worldwide. This growth comes at the
cost of increased electricity consumption, which directly correlates to CO2
emissions, the main driver of climate change. As such, researching ways to
reduce cloud computing emissions is more relevant than ever. However, although
cloud providers are reportedly already working near optimal power efficiency,
they fail in providing precise sustainability reporting. This calls for further
improvements on the cloud computing consumer's side. To this end, in this paper
we propose a user-centric, Kubernetes-based architecture for green cloud
computing. We implement a carbon intensity forecaster and we use it to schedule
workloads based on the availability of green energy, exploiting both regional
and temporal variations to minimize emissions. We evaluate our system using
real-world traces of cloud workloads execution comparing the achieved carbon
emission savings against a baseline round-robin scheduler. Our findings
indicate that our system can achieve up to a 13% reduction in emissions in a
strict scenario with heavy limitations on the available resources.

</details>


### [85] [Testing and benchmarking emerging supercomputers via the MFC flow solver](https://arxiv.org/abs/2509.13575)
*Benjamin Wilfong,Anand Radhakrishnan,Henry A. Le Berre,Tanush Prathi,Stephen Abbott,Spencer H. Bryngelson*

Main category: cs.DC

TL;DR: MFC通过自动化工具链简化了超级计算机的评估流程，测试了多种硬件和编译器组合，发现了潜在问题。


<details>
  <summary>Details</summary>
Motivation: 部署新超级计算机需要通过应用代码进行测试和评估，而便携、用户友好的工具可以简化这一过程。

Method: MFC配备了自动化工具链，支持用户评估不同编译器-硬件组合的正确性和性能，无需深厚的软件工程经验。

Result: MFC在五代NVIDIA GPU、三代AMD GPU及多种CPU架构上进行了基准测试，发现了编译器错误和回归问题，并测试了约50台计算设备和5台旗舰超级计算机。

Conclusion: MFC作为一种便携式、用户友好的工具，成功满足了评估超级计算机的需求，通过自动化工具链简化了输入生成、编译、批处理作业提交、回归测试和基准测试流程。

Abstract: Deploying new supercomputers requires testing and evaluation via application
codes. Portable, user-friendly tools enable evaluation, and the Multicomponent
Flow Code (MFC), a computational fluid dynamics (CFD) code, addresses this
need. MFC is adorned with a toolchain that automates input generation,
compilation, batch job submission, regression testing, and benchmarking. The
toolchain design enables users to evaluate compiler-hardware combinations for
correctness and performance with limited software engineering experience. As
with other PDE solvers, wall time per spatially discretized grid point serves
as a figure of merit. We present MFC benchmarking results for five generations
of NVIDIA GPUs, three generations of AMD GPUs, and various CPU architectures,
utilizing Intel, Cray, NVIDIA, AMD, and GNU compilers. These tests have
revealed compiler bugs and regressions on recent machines such as Frontier and
El Capitan. MFC has benchmarked approximately 50 compute devices and 5 flagship
supercomputers.

</details>


### [86] [Modeling the Carbon Footprint of HPC: The Top 500 and EasyC](https://arxiv.org/abs/2509.13583)
*Varsha Rao,Andrew A. Chien*

Main category: cs.DC

TL;DR: 论文提出使用EasyC工具估算Top 500 HPC系统的碳足迹，填补了HPC系统碳报告的空白。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏HPC系统的碳报告，尤其是大型站点，论文旨在填补这一空白，提供HPC系统的碳足迹估算。

Method: 使用EasyC工具和Top500.org数据，对391个HPC系统的运营碳和283个系统的隐含碳进行建模，并通过插值估算Top 500系统的碳足迹。

Result: 估算出Top 500 HPC系统的运营碳为1,393.7百万吨CO2e（1年），隐含碳为1,881.8百万吨CO2e，并预测到2030年的增长趋势。

Conclusion: 该论文提出了一个估算HPC系统碳足迹的新方法，并展示了如何通过额外公开信息提高覆盖范围，为Top 500 HPC系统提供了首个碳足迹估算。

Abstract: Climate change is a critical concern for HPC systems, but GHG protocol
carbon-emission accounting methodologies are difficult for a single system, and
effectively infeasible for a collection of systems. As a result, there is no
HPC-wide carbon reporting, and even the largest HPC sites do not do GHG
protocol reporting.
  We assess the carbon footprint of HPC, focusing on the Top 500 systems. The
key challenge lies in modeling the carbon footprint with limited data
availability.
  With the disclosed Top500.org data, and using a new tool, EasyC, we were able
to model the operational carbon of 391 HPC systems and the embodied carbon of
283 HPC systems. We further show how this coverage can be enhanced by
exploiting additional public information. With improved coverage, then
interpolation is used to produce the first carbon footprint estimates of the
Top 500 HPC systems. They are 1,393.7 million MT CO2e operational carbon (1
Year) and 1,881.8 million MT CO2e embodied carbon. We also project how the Top
500's carbon footprint will increase through 2030.
  A key enabler is the EasyC tool which models carbon footprint with only a few
data metrics. We explore availability of data and enhancement, showing that
coverage can be increased to 98% of Top 500 systems for operational and 80.8%
of the systems for embodied emissions.

</details>


### [87] [GPU Programming for AI Workflow Development on AWS SageMaker: An Instructional Approach](https://arxiv.org/abs/2509.13703)
*Sriram Srinivasan,Hamdan Alabsi,Rand Obeidat,Nithisha Ponnala,Azene Zenebe*

Main category: cs.DC

TL;DR: A specialized course on GPU architecture and programming for AI agents was evaluated, showing improved student skills and engagement, with AWS as a practical platform. The paper recommends broader adoption of such courses in STEM education.


<details>
  <summary>Details</summary>
Motivation: The motivation was to design a specialized course that bridges the gap between GPU architecture, programming, and their application in developing AI agents, aiming to enhance students' technical proficiency and engagement in parallel computing.

Method: The course covered foundational GPU/CPU hardware concepts, parallel computing, and progressed to developing RAG and optimizing them using GPUs. Students worked with cloud-based GPU instances, implemented parallel algorithms, and deployed scalable AI solutions. Learning outcomes were evaluated through assessments, course evaluations, and anonymous surveys.

Result: Results indicated that AWS was an effective and economical platform for practical GPU programming, experiential learning significantly enhanced technical proficiency and engagement, and the course improved students' problem-solving and critical thinking skills.

Conclusion: The paper advocates for broader adoption of GPU and parallel computing courses in STEM curricula to prepare students for modern, compute-intensive fields, highlighting the pedagogical value of such courses.

Abstract: We present the design, implementation, and comprehensive evaluation of a
specialized course on GPU architecture, GPU programming, and how these are used
for developing AI agents. This course is offered to undergraduate and graduate
students during Fall 2024 and Spring 2025. The course began with foundational
concepts in GPU/CPU hardware and parallel computing and progressed to develop
RAG and optimizing them using GPUs. Students gained experience provisioning and
configuring cloud-based GPU instances, implementing parallel algorithms, and
deploying scalable AI solutions. We evaluated learning outcomes through
assessments, course evaluations, and anonymous surveys. The results reveal that
(1) AWS served as an effective and economical platform for practical GPU
programming, (2) experiential learning significantly enhanced technical
proficiency and engagement, and (3) the course strengthened students'
problem-solving and critical thinking skills through tools such as TensorBoard
and HPC profilers, which exposed performance bottlenecks and scaling issues.
Our findings underscore the pedagogical value of integrating parallel computing
into STEM education. We advocate for broader adoption of similar electives
across STEM curricula to prepare students for the demands of modern,
compute-intensive fields.

</details>


### [88] [LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology](https://arxiv.org/abs/2509.13978)
*Renan Souza,Timothy Poteet,Brian Etz,Daniel Rosendo,Amal Gueroudji,Woong Shin,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 提出交互式LLM代理方法，通过元数据驱动设计将自然语言转为溯源查询，评估显示模块化设计和RAG技术提升分析能力。


<details>
  <summary>Details</summary>
Motivation: 现代科学发现依赖于跨边缘、云和高性能计算（HPC）连续体的数据处理工作流，但大规模溯源数据复杂且难以分析，现有系统存在交互限制。

Method: 引入了一种评估方法论、参考架构和开源实现，采用轻量级、元数据驱动的设计，将自然语言转换为结构化溯源查询。

Result: 评估涵盖了LLaMA、GPT、Gemini和Claude等多种查询类别及真实化学工作流，证明了方法的有效性。

Conclusion: 本研究提出了一种利用交互式大型语言模型（LLM）代理进行运行时数据分析的方法，通过模块化设计、提示调整和检索增强生成（RAG）技术，实现了准确且富有洞察力的响应，超越了传统记录式溯源的限制。

Abstract: Modern scientific discovery increasingly relies on workflows that process
data across the Edge, Cloud, and High Performance Computing (HPC) continuum.
Comprehensive and in-depth analyses of these data are critical for hypothesis
validation, anomaly detection, reproducibility, and impactful findings.
Although workflow provenance techniques support such analyses, at large scale,
the provenance data become complex and difficult to analyze. Existing systems
depend on custom scripts, structured queries, or static dashboards, limiting
data interaction. In this work, we introduce an evaluation methodology,
reference architecture, and open-source implementation that leverages
interactive Large Language Model (LLM) agents for runtime data analysis. Our
approach uses a lightweight, metadata-driven design that translates natural
language into structured provenance queries. Evaluations across LLaMA, GPT,
Gemini, and Claude, covering diverse query classes and a real-world chemistry
workflow, show that modular design, prompt tuning, and Retrieval-Augmented
Generation (RAG) enable accurate and insightful LLM agent responses beyond
recorded provenance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [89] [GRU-Based Learning for the Identification of Congestion Protocols in TCP Traffic](https://arxiv.org/abs/2509.13490)
*Paul Bergeron,Sandhya Aneja*

Main category: cs.NI

TL;DR: 论文提出了一种基于GRU的模型，用于高效识别复杂网络中的拥塞控制协议，准确率高达97.04%。


<details>
  <summary>Details</summary>
Motivation: 现有研究在复杂网络环境中的表现有限，因此需要一种更高效的方法来准确识别不同的拥塞控制协议。

Method: 采用了基于GRU的学习模型，并使用了更快的神经网络架构，在更复杂和竞争性更强的网络环境中进行测试。

Result: 在Marist大学校园网络中，模型识别拥塞控制协议的准确率达到97.04%，表现优于现有方法。

Conclusion: 该论文成功识别了Marist大学校园中的TCP Reno、TCP Cubic、TCP Vegas和BBR等拥塞控制协议，准确率达到97.04%，验证了GRU学习模型的有效性。

Abstract: This paper presents the identification of congestion control protocols TCP
Reno, TCP Cubic, TCP Vegas, and BBR on the Marist University campus, with an
accuracy of 97.04% using a GRU-based learning model. We used a faster neural
network architecture on a more complex and competitive network in comparison to
existing work and achieved comparably high accuracy.

</details>


### [90] [Odin: Effective End-to-End SLA Decomposition for 5G/6G Network Slicing via Online Learning](https://arxiv.org/abs/2509.13511)
*Duo Cheng,Ramanujan K Sheshadri,Ahan Kak,Nakjung Choi,Xingyu Zhou,Bo Ji*

Main category: cs.NI

TL;DR: Odin 是一种基于贝叶斯优化的 SLA 分解方案，显著提升 SLA 满意度并降低成本。


<details>
  <summary>Details</summary>
Motivation: 网络切片在实现 5G/6G 进展中至关重要，但 SLA 分解因领域异构性、动态网络条件和资源优化不透明而极具挑战性。

Method: Odin 是一种基于贝叶斯优化的解决方案，利用各领域的在线反馈实现高效 SLA 分解。

Result: 理论分析和严格评估表明，Odin 的 E2E 编排器在 SLA 满意度上比基线解决方案提升高达 45%，同时降低总体资源成本。

Conclusion: Odin 的端到端（E2E）编排器在 SLA 分解方面表现出色，显著提升了 SLA 满意度并降低了资源成本。

Abstract: Network slicing plays a crucial role in realizing 5G/6G advances, enabling
diverse Service Level Agreement (SLA) requirements related to latency,
throughput, and reliability. Since network slices are deployed end-to-end
(E2E), across multiple domains including access, transport, and core networks,
it is essential to efficiently decompose an E2E SLA into domain-level targets,
so that each domain can provision adequate resources for the slice. However,
decomposing SLAs is highly challenging due to the heterogeneity of domains,
dynamic network conditions, and the fact that the SLA orchestrator is oblivious
to the domain's resource optimization. In this work, we propose Odin, a
Bayesian Optimization-based solution that leverages each domain's online
feedback for provably-efficient SLA decomposition. Through theoretical analyses
and rigorous evaluations, we demonstrate that Odin's E2E orchestrator can
achieve up to 45% performance improvement in SLA satisfaction when compared
with baseline solutions whilst reducing overall resource costs even in the
presence of noisy feedback from the individual domains.

</details>


### [91] [A Framework for Multi-source Prefetching Through Adaptive Weight](https://arxiv.org/abs/2509.13604)
*Yoseph Berhanu Alebachew,Mulugeta Libsie*

Main category: cs.NI

TL;DR: 本文提出了一种可整合多种预取方案的新框架，通过自适应权重管理优化预测，适合移动设备。


<details>
  <summary>Details</summary>
Motivation: 尽管缓存和预取技术已被用于缓解网络延迟问题，但现有方法在利用应用级别的文档关系方面存在局限性，且缺乏可扩展性。

Method: 通过自适应权重管理技术，框架根据每个算法的观察性能调整其在整体预测中的影响。

Result: 该框架比现有方案更为保守，适合资源受限的移动设备。

Conclusion: 本文提出了一种新颖的框架，能够整合来自不同预取方案的技术，无需对算法进行重大修改，特别适合资源受限的移动设备。

Abstract: The World Wide Web has come to be a great part of our daily life, yet user
observed latency is still a problem that needs a proper means of handling. Even
though earlier attempts focused on caching as the chief solution to tackling
this issue, its success was extremely limited. Prefetching has come to be the
primary technique in supplementing caching towards soothing the latency problem
associated with the contemporary Internet. However, existing approaches in
prefetching are extremely limited in their ability to employ application level
web document relationship which is often visible only to the content developer.
This is because most approaches are access history based schemes that make
future users' access prediction only based on past user access. Attempts to
incorporate prefetching schemes that utilize semantic information with those
that use users past access history are extremely limited in their
extensibility. In this work we present a novel framework that enables
integration of schemes from both worlds of prefetching without the need for a
major modification to the algorithms. When there is a need/possibility to
capture new application level context, a new algorithm could be developed to do
so and then it can be integrated into the framework. Since each participating
scheme is merely viewed as an algorithm that produces a list of candidate
objects that are likely to be accessed in the near future, the framework can
entertain any one of the existing prefetching schemes. With its adaptive weight
management technique the framework adjusts the effect of each algorithm in the
overall prediction to parallel with its observed performance so far. We have
found this formwork to be less aggressive than its contemporary counterparts
which is extremely important for resource constrained mobile devices that have
come to be the major means of access by users of the current web.

</details>


### [92] [LINC: An In-Network Coding Approach to Tame Packet Loss in Hybrid Wireless-Fiber Backbones](https://arxiv.org/abs/2509.13714)
*Benoit Pit-Claudel,Muriel Médard,Manya Ghobadi*

Main category: cs.NI

TL;DR: LINC是一种新型网络内编码系统，通过逐链路编码减少环境丢包，无需终端主机配合，显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 混合骨干网络因环境丢包问题导致传统传输协议误判为网络拥塞，而现有网络编码方案需要终端主机完全配合。

Method: LINC采用系统块编码方法，在网络内逐链路进行编码和解码，并通过优化模型选择最佳编码参数。

Result: 在真实骨干网络拓扑上的模拟显示，LINC可将端到端延迟降低高达18%。

Conclusion: LINC通过引入网络内编码能力，有效减少了环境丢包带来的不必要重传，从而显著降低了端到端延迟。

Abstract: The emergence of ultra-low latency applications, such as financial
transactions, has driven the development of hybrid backbone networks that rely
on fiber, satellite, and microwave links. Despite providing low latencies,
these hybrid networks suffer from occasional environmental packet loss caused
by poor weather, construction, and line of sight blockage. Paradoxically,
today's hybrid backbones rely on conventional transport protocols that take
packet loss to signal network congestion, as opposed to transient environmental
obstacles. A common approach to address this challenge is to use network coding
(NC) between the end hosts to recover from these occasional packet loss events.
However, current NC proposals assume full access to the end-hosts' stack to
perform end-to-end encoding/decoding operations. In this paper, we introduce
LINC, a novel system that provides in-network NC capabilities to mitigate
environmental packet loss events without requiring cooperation from the end
hosts. LINC uses a systematic block coding approach on a link-by-link basis,
encoding and decoding packets inside the network. We model the tradeoff in
goodput between end-to-end retransmissions and redundant packets introduced by
LINC, and propose an optimization formulation to determine the optimal choice
of coding parameters. Our simulations on real-world backbone topologies
demonstrate that LINC reduces the end-to-end latency by up to 18% by
eliminating unnecessary retransmissions.

</details>


### [93] [Conducting Mission-Critical Voice Experiments with Automated Speech Recognition and Crowdsourcing](https://arxiv.org/abs/2509.13724)
*Jan Janak,Kahlil Dozier,Lauren Berny,Liang Hu,Dan Rubenstein,Charles Jennings,Henning Schulzrinne*

Main category: cs.NI

TL;DR: 本文开发了模拟真实环境的MCV系统测试方法和工具，发现人类在MCV任务中表现优于ASR，编解码器显著影响QoE。


<details>
  <summary>Details</summary>
Motivation: 由于公共安全用户对MCV系统在挑战性条件下可靠运行的期望，需要研究MCV系统中的损伤与用户QoE的相关性。

Method: 开发了用于人类受试者实验的方法和工具，包括模拟真实世界MCV系统的测试台和近似人类受试者的ASR机器人。通过Levenshtein距离度量评估QoE。

Result: 人类受试者在准确性任务中优于ASR，编解码器对QoE和ASR性能有显著影响。

Conclusion: 研究发现人类在准确性相关的MCV任务中通常表现优于ASR，且编解码器对最终用户的QoE和ASR性能有显著影响。

Abstract: Mission-critical voice (MCV) communications systems have been a critical tool
for the public safety community for over eight decades. Public safety users
expect MCV systems to operate reliably and consistently, particularly in
challenging conditions. Because of these expectations, the Public Safety
Communications Research (PSCR) Division of the National Institute of Standards
and Technology (NIST) has been interested in correlating impairments in MCV
communication systems and public safety user quality of experience (QoE).
Previous research has studied MCV voice quality and intelligibility in a
controlled environment. However, such research has been limited by the
challenges inherent in emulating real-world environmental conditions.
Additionally, there is the question of the best metric to use to reflect QoE
accurately.
  This paper describes our efforts to develop the methodology and tools for
human-subject experiments with MCV. We illustrate their use in human-subject
experiments in emulated real-world environments. The tools include a testbed
for emulating real-world MCV systems and an automated speech recognition (ASR)
robot approximating human subjects in transcription tasks. We evaluate QoE
through a Levenshtein Distance-based metric, arguing it is a suitable proxy for
measuring comprehension and the QoE. We conducted human-subject studies with
Amazon MTurk volunteers to understand the influence of selected system
parameters and impairments on human subject performance and end-user QoE. We
also compare the performance of several ASR system configurations with
human-subject performance. We find that humans generally perform better than
ASR in accuracy-related MCV tasks and that the codec significantly influences
the end-user QoE and ASR performance.

</details>


### [94] [Performance Evaluation of Intent-Based Networking Scenarios: A GitOps and Nephio Approach](https://arxiv.org/abs/2509.13901)
*Saptarshi Ghosh,Ioannis Mavromatis,Konstantinos Antonakoglou,Konstantinos Katsaros*

Main category: cs.NI

TL;DR: 本文评估了三种GitOps工具在IBN场景下的性能，揭示了它们在延迟、资源开销和响应性方面的权衡，并提供了工具选择和优化的建议。


<details>
  <summary>Details</summary>
Motivation: GitOps作为管理云原生基础设施的基础范式，其性能和可扩展性在意图驱动网络（IBN）场景中尚未得到充分评估。本文旨在填补这一空白。

Method: 本文采用可重复的、基于指标的基准测试方法，评估了三种广泛使用的GitOps操作员（Argo CD、Flux CD和ConfigSync）的延迟和资源开销。实验在单意图和多意图场景下进行，捕获了关键性能指标。

Result: 研究结果揭示了这些工具在确定性、资源效率和响应性之间的权衡，并通过Nephio作为编排器的实际场景量化了声明性端到端部署管道的处理延迟和开销。

Conclusion: 本文的结论强调了GitOps工具在IBN场景下的性能权衡，为未来自主网络编排系统的工具选择和优化提供了宝贵见解。

Abstract: GitOps has emerged as a foundational paradigm for managing cloud-native
infrastructures by enabling declarative configuration, version-controlled
state, and automated reconciliation between intents and runtime deployments.
Despite its widespread adoption, the performance and scalability of GitOps
tools in Intent-Based Networking (IBN) scenarios are insufficiently evaluated.
This paper presents a reproducible, metric-driven benchmarking, assessing the
latency and resource overheads of three widely used GitOps operators: Argo CD,
Flux CD, and ConfigSync. We conduct controlled experiments under both single-
and multi-intent scenarios, capturing key performance indicators such as
latency and resource consumption. Our results highlight trade-offs between the
tools in terms of determinism, resource efficiency, and responsiveness. We
further investigate a realistic orchestration scenario, using Nephio as our
orchestrator, to quantify the processing latency and overhead in declarative
end-to-end deployment pipelines. Our findings can offer valuable insights for
tool selection and optimisation in future autonomous network orchestration
systems.

</details>


### [95] [Low-cost Highly-interoperable Multiplatform Campus Network: Experience of YARSI University](https://arxiv.org/abs/2509.13954)
*Surya Agustian,Sandra Permana,Salman Teguh Pratista,Syarifu Adam,Iswandi*

Main category: cs.NI

TL;DR: 论文提出了一种低成本校园网络设计方案，结合开源技术和本地硬件，显著降低了建设和维护成本，适用于预算有限的社区或组织。


<details>
  <summary>Details</summary>
Motivation: 由于校园网络建设成本高昂，且缺乏IT知识的组织可能因依赖第三方而面临额外开支，因此需要一种低成本且易于维护的解决方案。

Method: 采用开源操作系统运行在本地组装的PC上作为网关和路由器，结合Cisco的交换技术，设计了一个基于UTP的低成本校园网络。

Result: 成功设计并实施了覆盖YARSI大学环境的低成本校园网络，通过多个宽带连接和专用无线网络共享互联网接入，支持超过100名用户同时使用。

Conclusion: 该论文提出了一种低成本校园网络设计方案，通过结合开源操作系统和本地组装的PC作为网关和路由器，以及Cisco的交换技术，显著降低了网络基础设施和互联网接入的成本。

Abstract: To some organizations, building campus network is sometimes considered to be
very expensive; and this has made the project uneasy to perform. Moreover, if
the organization without sufficient IT knowledge does not have capable IT
engineers, leaving this project to third parties without supervision would lead
to unexpected larger expenses. For this reason, in the year of 2003, YARSI
University formed CMIS (Center for Management Infor-mation System) to perform
tasks in designing, operations and maintenance of campus network and its
services. By combining Open Source operating system run on a local assembled
personal computer as gateway and router, and switching technology from Cisco,
we designed a low-cost UTP-based campus network which covering rooms and
buildings in YARSI environment. Meanwhile the internet access through several
broadband connections and dedicated wireless was shared to more than 100
simultaneous users by a captive portal system. With this strategy, we can
significantly reduce cost for purchasing, maintenance and operations of network
infrastructure and internet access. Our model in designing low-cost campus
network and internet connections could be adopted by rural community or
organizations that have limited budget to have internet access.

</details>


### [96] [Path-Oblivious Entanglement Swapping for the Quantum Internet](https://arxiv.org/abs/2509.13993)
*Vincent Mutolo,Rhea Parekh,Dan Rubenstein*

Main category: cs.NI

TL;DR: 本文提出路径无关的Bell对交换协议，通过线性规划模型和基础协议评估，显示其在量子网络中的潜力。


<details>
  <summary>Details</summary>
Motivation: 基于经典网络的经验，灵活、无需预留路径的方法在资源充足网络中表现更优。随着量子态变得更廉价和稳定，路径无关的交换方法更具优势。

Method: 本文提出了一个线性规划模型，并评估了一个相对基础的Bell对交换协议，旨在网络中平衡Bell对。

Result: 初步结果显示，尽管基础的平衡策略有待优化，但路径无关的交换方法展现出潜力。

Conclusion: 研究表明，虽然简单的平衡策略还有改进空间，但探索路径无关的Bell对交换协议是一个有前景的方向。

Abstract: Proposed Bell pair swapping protocols, an essential component of the Quantum
Internet, are planned-path: specific, structured, routing paths are reserved
prior to the execution of the swapping process. This makes sense when one
assumes the state used in the swapping process is expensive, fragile, and
unstable. However, lessons from classical networking have shown that while
reservations seem promising in concept, flexible, reservation-light or free
approaches often outperform their more restrictive counterparts in
well-provisioned networks. In this paper, we propose that a path-oblivious
approach is more amenable to supporting swapping as quantum state evolves into
a cheaper, more robust form. We formulate the swapping process as a linear
program and present and evaluate a fairly naive baseline swapping protocol that
tries to balance Bell pairs throughout the network. Preliminary results show
that while naive balancing leaves room for improvement, investigating
path-oblivious swapping is a promising direction.

</details>


### [97] [RepCaM++: Exploring Transparent Visual Prompt With Inference-Time Re-Parameterization for Neural Video Delivery](https://arxiv.org/abs/2509.14002)
*Rongyu Zhang,Xize Duan,Jiaming Liu,Li Du,Yuan Du,Dan Wang,Shanghang Zhang,Fangxin Wang*

Main category: cs.NI

TL;DR: RepCaM++框架通过创新模块和TVP技术，解决了内容感知方法在长视频中的参数积累问题，显著提升了视频恢复和带宽压缩性能。


<details>
  <summary>Details</summary>
Motivation: 现有内容感知方法在视频长度增加时，参数积累导致交付成本上升和性能下降，亟需一种更高效的调制框架。

Method: 引入了RepCaM++框架，基于RepCaM模块统一调制视频块，并通过训练时集成并行级联参数、推理时消除这些额外参数来实现高效调制。此外，提出了TVP技术，通过极少量零初始化图像级参数捕获视频块的细节。

Result: 在VSD4K数据集（包含六种不同视频场景）上进行了广泛实验，取得了视频恢复质量和交付带宽压缩的最先进结果。

Conclusion: RepCaM++框架通过创新的Re-parameterization Content-aware Modulation模块和Transparent Visual Prompt技术，显著提升了视频恢复质量和带宽压缩效率，实现了在VSD4K数据集上的最先进性能。

Abstract: Recently, content-aware methods have been employed to reduce bandwidth and
enhance the quality of Internet video delivery. These methods involve training
distinct content-aware super-resolution (SR) models for each video chunk on the
server, subsequently streaming the low-resolution (LR) video chunks with the SR
models to the client. Prior research has incorporated additional partial
parameters to customize the models for individual video chunks. However, this
leads to parameter accumulation and can fail to adapt appropriately as video
lengths increase, resulting in increased delivery costs and reduced
performance. In this paper, we introduce RepCaM++, an innovative framework
based on a novel Re-parameterization Content-aware Modulation (RepCaM) module
that uniformly modulates video chunks. The RepCaM framework integrates extra
parallel-cascade parameters during training to accommodate multiple chunks,
subsequently eliminating these additional parameters through
re-parameterization during inference. Furthermore, to enhance RepCaM's
performance, we propose the Transparent Visual Prompt (TVP), which includes a
minimal set of zero-initialized image-level parameters (e.g., less than 0.1%)
to capture fine details within video chunks. We conduct extensive experiments
on the VSD4K dataset, encompassing six different video scenes, and achieve
state-of-the-art results in video restoration quality and delivery bandwidth
compression.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [98] [GTA -- An ATSP Method: Shifting the Bottleneck from Algorithm to RAM](https://arxiv.org/abs/2509.13327)
*Wissam Nakhle*

Main category: cs.DS

TL;DR: GTA算法结合启发式预热和子路径消除策略，在商用硬件上高效求解大规模ATSP问题，为多领域提供资源高效的确定性解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决传统TSP求解方法依赖超级计算硬件、资源消耗大的问题，提供一种在商用硬件上高效、确定性的解决方案，以满足跨学科领域对大规模TSP问题的需求。

Method: 结合高效启发式预热和子路径消除策略，避免传统MTZ约束，设计GTA算法，利用商用硬件（如8逻辑处理器）实现大规模ATSP问题的确定性求解。

Result: 成功处理高达5,000节点（约2500万二进制变量）的ATSP实例，收敛速度与高性能计算框架相当，并在公共数据集上验证了其性能。

Conclusion: 该论文提出的GTA算法通过结合高效启发式预热和子路径消除策略，显著提升了ATSP问题的求解效率，使其在商用硬件上也能处理大规模实例，为物流、生物信息学和天文学等领域提供了资源高效的确定性解决方案。

Abstract: We present a scalable, high-performance algorithm that deterministically
solves large-scale instances of the Traveling Salesman problem (in its
asymmetric version, ATSP) to optimality using commercially available computing
hardware. By combining an efficient heuristic warm start, capable of achieving
near-optimality within seconds in some cases, with a subtour elimination
strategy that removes the need for traditional MTZ constraints, our approach
consistently resolves instances up to 5,000 nodes (approximately 25 million
binary variables) in record time on widely accessible computers, with eight
logical processors. We demonstrate reproducible results with convergence rates
comparable to those of high-performance computing frameworks. Real-time
iteration tracking and an adaptable interface allow seamless integration into
scheduling workflows in logistics, bioinformatics, and astronomy. Designed to
streamline solutions to large-scale TSP problems across disciplines, our
approach is benchmarked against widely used public datasets, offering a
deterministic, resource-efficient alternative to conventional solvers that rely
on supercomputing hardware. Our GTA (Gurobi Tabu Algorithm) algorithm is a
fundamental shift of TSP solution bottleneck from algorithmic complexity to the
underlying hardware (RAM and system memory), which is a highly desirable
characteristic.

</details>


### [99] [Hardness of Dynamic Core and Truss Decompositions](https://arxiv.org/abs/2509.13584)
*Yan S. Couto,Cristina G. Fernandes*

Main category: cs.DS

TL;DR: 论文证明动态k-core算法的高效性受限，基于OMv和SETH猜想，除非改进现有算法。同时提出2-core的多对数动态算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于动态图中k-core及其变体的高效算法问题，特别是对Hanauer等人调查中提出的问题的回答。

Method: 论文基于OMv和SETH猜想，通过理论分析证明了k-core及其变体在动态图中的算法限制，并提出了针对2-core的多对数动态算法。

Result: 论文证明了除非能改进现有算法，否则不存在高效的动态k-core算法，并展示了有界算法的局限性。同时，提出了针对2-core的多对数动态算法。

Conclusion: 该论文通过证明基于OMv和SETH猜想，指出除非能改进矩阵乘法和可满足性等领域的现有算法，否则不存在高效的动态k-core算法。此外，论文还展示了有界算法的局限性，并提出了针对2-core的多对数动态算法。

Abstract: The k-core of a graph is its maximal subgraph with minimum degree at least k,
and the core value of a vertex u is the largest k for which u is contained in
the k-core of the graph. Among cohesive subgraphs, k-core and its variants have
received a lot of attention recently, particularly on dynamic graphs, as
reported by Hanauer, Henzinger, and Schulz in their recent survey on dynamic
graph algorithms. We answer questions on k-core stated in the survey, proving
that there is no efficient dynamic algorithm for k-core or to find (2 -
{\epsilon})-approximations for the core values, unless we can improve
decade-long state-of-the-art algorithms in many areas including matrix
multiplication and satisfiability, based on the established OMv and SETH
conjectures. Some of our results show that there is no dynamic algorithm for
k-core asymptotically faster than the trivial ones. This explains why most
recent research papers in this area focus not on a generic efficient dynamic
algorithm, but on finding a bounded algorithm, which is fast when few core
values change per update. However, we also prove that such bounded algorithms
do not exist, based on the OMv conjecture. We present lower bounds also for a
directed version of the problem, and for the edge variant of the problem, known
as k-truss. On the positive side, we present a polylogarithmic dynamic
algorithm for 2-core.

</details>


### [100] [On Solving Asymmetric Diagonally Dominant Linear Systems in Sublinear Time](https://arxiv.org/abs/2509.13891)
*Tsz Chiu Kwok,Zhewei Wei,Mingji Yang*

Main category: cs.DS

TL;DR: 本文研究了子线性时间求解RDD/CDD线性系统的方法，通过诺依曼级数和最大p范数间隙分析，开发了局部近似算法，并统一了前向推和反向推方法。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在推广对称对角占优（SDD）系统的子线性时间求解器研究，以解决非对称情况下的行/列对角占优（RDD/CDD）线性系统Mx=b。

Method: 我们通过诺依曼级数表达解x∗，证明其收敛性，并通过M的新颖量——最大p范数间隙——来限制该级数的截断误差。对于具有有界最大p范数间隙的系统，我们开发了一系列算法结果，用于在不同场景和误差度量下局部近似t⊤x∗。

Result: 我们的框架提供了对前向推和反向推的统一理解，并为PageRank和图上有效电阻估计等问题提供了更深入的见解、扩展结果和改进的复杂度界限。

Conclusion: 本文的研究为子线性时间求解器、局部图算法和有向谱图理论的进一步研究打开了大门。

Abstract: We initiate a study of solving a row/column diagonally dominant (RDD/CDD)
linear system $Mx=b$ in sublinear time, with the goal of estimating
$t^{\top}x^*$ for a given vector $t\in R^n$ and a specific solution $x^*$. This
setting naturally generalizes the study of sublinear-time solvers for symmetric
diagonally dominant (SDD) systems [AKP19] to the asymmetric case.
  Our first contributions are characterizations of the problem's mathematical
structure. We express a solution $x^*$ via a Neumann series, prove its
convergence, and upper bound the truncation error on this series through a
novel quantity of $M$, termed the maximum $p$-norm gap. This quantity
generalizes the spectral gap of symmetric matrices and captures how the
structure of $M$ governs the problem's computational difficulty.
  For systems with bounded maximum $p$-norm gap, we develop a collection of
algorithmic results for locally approximating $t^{\top}x^*$ under various
scenarios and error measures. We derive these results by adapting the
techniques of random-walk sampling, local push, and their bidirectional
combination, which have proved powerful for special cases of solving RDD/CDD
systems, particularly estimating PageRank and effective resistance on graphs.
Our general framework yields deeper insights, extended results, and improved
complexity bounds for these problems. Notably, our perspective provides a
unified understanding of Forward Push and Backward Push, two fundamental
approaches for estimating random-walk probabilities on graphs.
  Our framework also inherits the hardness results for sublinear-time SDD
solvers and local PageRank computation, establishing lower bounds on the
maximum $p$-norm gap or the accuracy parameter. We hope that our work opens the
door for further study into sublinear solvers, local graph algorithms, and
directed spectral graph theory.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [101] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: 研究发现，在LLM-as-a-judge范式中，显式推理的LLM在准确性、效率和鲁棒性上均优于非思考模型，尤其是在多语言环境下。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在基准测试和奖励建模中作为自动评判工具的应用日益广泛，确保其可靠性、效率和鲁棒性变得至关重要。

Method: 使用开源Qwen 3模型（0.6B、1.7B和4B参数）在RewardBench任务上评估准确性和计算效率（FLOPs），并测试了非思考模型的增强策略（如上下文学习、评分标准引导、基于参考的评估和n-best聚合）。

Result: 思考模型在准确性上比非思考模型高出约10%，且计算开销较低（<2x），而增强策略（如few-shot学习）虽能带来一定提升但成本较高（>8x）。此外，思考模型在多种偏见条件下表现更为一致（平均高6%）。多语言实验进一步验证了显式推理的优势。

Conclusion: 本文通过系统比较“思考”与“非思考”LLM在LLM-as-a-judge范式中的表现，发现显式推理在准确性、效率和鲁棒性上均具有明显优势，尤其是在多语言环境下。

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [102] [Evaluation Awareness Scales Predictably in Open-Weights Large Language Models](https://arxiv.org/abs/2509.13333)
*Maheep Chaudhary,Ian Su,Nikhil Hooda,Nishith Shankar,Julia Tan,Kevin Zhu,Ashwinee Panda,Ryan Lagasse,Vasu Sharma*

Main category: cs.AI

TL;DR: 研究发现LLMs的评估意识随模型规模幂律增长，为预测未来大模型的欺骗行为及设计AI安全评估策略提供依据。


<details>
  <summary>Details</summary>
Motivation: 揭示模型在评估和部署环境中行为差异的缩放规律，以解决AI安全评估中模型可能隐藏危险能力的问题。

Method: 通过线性探测在15个参数规模从0.27B到70B的模型家族中的转向向量激活，研究评估意识的缩放关系。

Result: 研究发现评估意识与模型规模之间存在清晰的幂律缩放关系，即模型规模越大，评估意识越强。

Conclusion: 大型语言模型（LLMs）的评估意识行为随模型规模呈现幂律增长，这为预测未来更大模型的欺骗行为提供了依据，并指导了针对AI安全的规模感知评估策略的设计。

Abstract: Large language models (LLMs) can internally distinguish between evaluation
and deployment contexts, a behaviour known as \emph{evaluation awareness}. This
undermines AI safety evaluations, as models may conceal dangerous capabilities
during testing. Prior work demonstrated this in a single $70$B model, but the
scaling relationship across model sizes remains unknown. We investigate
evaluation awareness across $15$ models scaling from $0.27$B to $70$B
parameters from four families using linear probing on steering vector
activations. Our results reveal a clear power-law scaling: evaluation awareness
increases predictably with model size. This scaling law enables forecasting
deceptive behavior in future larger models and guides the design of scale-aware
evaluation strategies for AI safety. A link to the implementation of this paper
can be found at
https://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.

</details>


### [103] [FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness](https://arxiv.org/abs/2509.13334)
*Anand Swaroop,Akshat Nallani,Saksham Uboweja,Adiliia Uzdenova,Michael Nguyen,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.AI

TL;DR: FRIT是一种通过干预训练和偏好优化提升语言模型推理可靠性的方法，在多个任务上显著提高了忠实推理和准确率。


<details>
  <summary>Details</summary>
Motivation: 解决现有CoT推理中推理步骤未能因果影响最终答案的问题，提高推理的可靠性和可信度。

Method: 通过干预训练（FRIT）生成合成训练数据，并应用直接偏好优化（DPO）来教导模型偏好因果一致的推理路径。

Result: 在Qwen3-8B和Mistral-7B-v0.1上，FRIT将Mistral在GSM8K上的忠实推理提高了3.4个百分点，同时准确率提高了7.6个百分点。

Conclusion: FRIT提供了一种可扩展、无需监督的方法，用于训练语言模型生成更可靠和可解释的推理，填补了推理性能与可信度之间的关键空白。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving
large language model performance on complex tasks, but recent work shows that
reasoning steps often fail to causally influence the final answer, creating
brittle and untrustworthy outputs. Prior approaches focus primarily on
measuring faithfulness, while methods for systematically improving it remain
limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a
scalable alignment method that trains models to produce causally consistent
reasoning by learning from systematically corrupted examples. FRIT generates
synthetic training data by intervening on individual reasoning steps in
model-generated CoTs, creating faithful/unfaithful pairs that highlight when
reasoning breaks down. We then apply Direct Preference Optimization to teach
models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B
and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases
faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while
improving accuracy by $7.6$ percentage points. Our approach provides the first
scalable, supervision-free method for training language models to produce more
reliable and interpretable reasoning, addressing a critical gap between
reasoning performance and trustworthiness. We release our code at
\href{https://github.com/Anut-py/frit}.

</details>


### [104] [Position: AI Safety Must Embrace an Antifragile Perspective](https://arxiv.org/abs/2509.13339)
*Ming Jin,Hyunin Lee*

Main category: cs.AI

TL;DR: 论文主张AI安全研究应采用抗脆弱性视角，通过利用不确定性应对未来挑战，而非仅仅优化当前性能，提出了长期安全改进的方法和社区指南。


<details>
  <summary>Details</summary>
Motivation: 现代AI研究需要应对环境演变和模型可能出现的适应不良（如奖励黑客、过度优化或能力萎缩），静态基准和一次性鲁棒性测试无法满足这一需求。

Method: 论文首先识别了静态测试的关键局限性（如场景多样性、奖励黑客和过度对齐），然后探讨了抗脆弱性解决方案管理罕见事件的潜力。

Result: 论文提出了一种根本性的方法调整，用于长期测量、基准化和持续改进AI安全，补充现有鲁棒性方法，并为培养抗脆弱性AI安全社区提供伦理和实践指南。

Conclusion: 论文主张现代AI研究应采用抗脆弱性视角来确保长期AI安全，强调通过利用不确定性来应对未来更大的不可预测性，而非仅仅减少当前的不确定性。

Abstract: This position paper contends that modern AI research must adopt an
antifragile perspective on safety -- one in which the system's capacity to
guarantee long-term AI safety such as handling rare or out-of-distribution
(OOD) events expands over time. Conventional static benchmarks and single-shot
robustness tests overlook the reality that environments evolve and that models,
if left unchallenged, can drift into maladaptation (e.g., reward hacking,
over-optimization, or atrophy of broader capabilities). We argue that an
antifragile approach -- Rather than striving to rapidly reduce current
uncertainties, the emphasis is on leveraging those uncertainties to better
prepare for potentially greater, more unpredictable uncertainties in the future
-- is pivotal for the long-term reliability of open-ended ML systems. In this
position paper, we first identify key limitations of static testing, including
scenario diversity, reward hacking, and over-alignment. We then explore the
potential of antifragile solutions to manage rare events. Crucially, we
advocate for a fundamental recalibration of the methods used to measure,
benchmark, and continually improve AI safety over the long term, complementing
existing robustness approaches by providing ethical and practical guidelines
towards fostering an antifragile AI safety community.

</details>


### [105] [Imagined Autocurricula](https://arxiv.org/abs/2509.13341)
*Ahmet H. Güzel,Matthew Thomas Jackson,Jarek Luca Liesen,Tim Rocktäschel,Jakob Nicolaus Foerster,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.AI

TL;DR: 利用世界模型生成想象环境，通过IMAC方法自动生成训练课程，训练出能在新任务中泛化的智能体。


<details>
  <summary>Details</summary>
Motivation: 解决在现实世界中缺乏大量训练数据或准确模拟的限制，利用离线被动收集的数据生成多样化的训练环境。

Method: 通过世界模型生成想象环境，并利用IMAC方法在这些环境中自动生成训练课程。

Result: 在一系列具有挑战性的程序生成环境中，展示了在仅使用较窄数据集学习的世界模型内训练后，能够在保留环境中实现强迁移性能。

Conclusion: 本文提出了一种名为IMAC（Imagined Autocurricula）的新方法，利用无监督环境设计（UED）在生成的世界中自动生成课程，从而训练出能够在新任务变体中泛化的鲁棒智能体。

Abstract: Training agents to act in embodied environments typically requires vast
training data or access to accurate simulation, neither of which exists for
many cases in the real world. Instead, world models are emerging as an
alternative leveraging offline, passively collected data, they make it possible
to generate diverse worlds for training agents in simulation. In this work, we
harness world models to generate imagined environments to train robust agents
capable of generalizing to novel task variations. One of the challenges in
doing this is ensuring the agent trains on useful generated data. We thus
propose a novel approach, IMAC (Imagined Autocurricula), leveraging
Unsupervised Environment Design (UED), which induces an automatic curriculum
over generated worlds. In a series of challenging, procedurally generated
environments, we show it is possible to achieve strong transfer performance on
held-out environments, having trained only inside a world model learned from a
narrower dataset. We believe this opens the path to utilizing larger-scale,
foundation world models for generally capable agents.

</details>


### [106] [OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft](https://arxiv.org/abs/2509.13347)
*Zihao Wang,Muyao Li,Kaichen He,Xiangyu Wang,Zhancun Mu,Anji Liu,Yitao Liang*

Main category: cs.AI

TL;DR: 本文通过CoA框架统一高级规划和低级控制，解决了动作空间选择的挑战，显著提升了智能体的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决动作空间选择在通用智能体开发中的关键挑战，发现最优动作空间高度依赖任务，导致通用智能体构建的困境。

Method: 提出了Chain of Action (CoA)框架，将高级动作视为中间推理步骤，指导最终可执行动作的生成。

Result: CoA框架在多样化动作空间上训练的All-in-One智能体表现出更强的鲁棒性和泛化能力，达到了新的最优性能。

Conclusion: CoA框架通过统一高级规划和低级控制，显著提升了开放任务中的成功率，并促进了通用智能体的发展。

Abstract: The choice of action spaces is a critical yet unresolved challenge in
developing capable, end-to-end trainable agents. This paper first presents a
large-scale, systematic comparison of prominent abstracted action spaces and
tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the
open-ended Minecraft. Our analysis reveals that no single action space is
universally optimal; instead, the most effective abstraction is highly
task-dependent, creating a dilemma for building generalist agents. To resolve
this, we introduce Chain of Action (CoA), a novel framework that unifies
high-level planning and low-level control within a single, monolithic VLA
model. CoA treats an abstracted action not as a command for a separate policy,
but as an intermediate reasoning step--akin to a chain of thought--that guides
the generation of the final, executable action. Furthermore, we demonstrate
that an All-in-One agent trained on a diverse mixture of action spaces using
the CoA paradigm learns a more robust and generalizable policy. This unified
agent achieves a new state-of-the-art, improving the overall task success rate
over strong, specialized baselines. To foster reproducible research, we release
the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive
benchmark of over 800 distinct tasks, curated datasets, source code, and all
pretrained model checkpoints at https://github.com/CraftJarvis/OpenHA

</details>


### [107] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: A new framework, PDDL-Instruct, improves LLMs' symbolic planning by teaching logical reasoning, achieving 94% accuracy, a 66% boost over baselines.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, yet their ability to perform structured symbolic planning remains limited, particularly in domains requiring formal representations like the Planning Domain Definition Language (PDDL).

Method: The paper presents a novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs' symbolic planning capabilities through logical chain-of-thought reasoning. It focuses on teaching models to rigorously reason about action applicability, state transitions, and plan validity using explicit logical inference steps.

Result: Experimental results on multiple planning domains show that the chain-of-thought reasoning based instruction-tuned models are significantly better at planning, achieving planning accuracy of up to 94% on standard benchmarks, representing a 66% absolute improvement over baseline models.

Conclusion: This work bridges the gap between the general reasoning capabilities of LLMs and the logical precision required for automated planning, offering a promising direction for developing better AI planning systems.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [108] [Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning](https://arxiv.org/abs/2509.13352)
*Anis Koubaa,Khaled Gabr*

Main category: cs.AI

TL;DR: 该论文提出了一种基于LLM的Agentic UAVs框架，通过五层架构显著提升了无人机的自主性和性能，模拟测试结果验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有无人机系统依赖基于规则的控制和窄AI，限制了其在动态、不确定任务中的适应性，且缺乏上下文感知推理、自主决策和生态系统级集成。

Method: 该论文提出了一种五层架构（Perception, Reasoning, Action, Integration, Learning），结合了LLM驱动的推理、数据库查询和第三方系统交互，并通过ROS2和Gazebo原型集成了YOLOv11目标检测与GPT-4推理及本地Gemma-3部署。

Result: 在模拟搜索救援场景中，Agentic UAVs实现了更高的检测置信度（0.79 vs. 0.72）、更优的人员检测率（91% vs. 75%）和显著增加的行动推荐率（92% vs. 4.5%）。

Conclusion: Agentic UAVs框架通过五层架构（感知、推理、行动、集成、学习）显著提升了无人机的自主性和生态系统集成能力，证明了适度的计算开销可以带来质的飞跃。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,
surveillance, and disaster response, yet most systems remain confined to SAE
Level 2--3 autonomy. Their reliance on rule-based control and narrow AI
restricts adaptability in dynamic, uncertain missions. Existing UAV frameworks
lack context-aware reasoning, autonomous decision-making, and ecosystem-level
integration; critically, none leverage Large Language Model (LLM) agents with
tool-calling for real-time knowledge access. This paper introduces the Agentic
UAVs framework, a five-layer architecture (Perception, Reasoning, Action,
Integration, Learning) that augments UAVs with LLM-driven reasoning, database
querying, and third-party system interaction. A ROS2 and Gazebo-based prototype
integrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3
deployment. In simulated search-and-rescue scenarios, agentic UAVs achieved
higher detection confidence (0.79 vs. 0.72), improved person detection rates
(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).
These results confirm that modest computational overhead enables qualitatively
new levels of autonomy and ecosystem integration.

</details>


### [109] [Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling](https://arxiv.org/abs/2509.13357)
*Yongchao Huang,Hassan Raza*

Main category: cs.AI

TL;DR: 提出语义融合方案，通过轻量级并行特征通道增强Transformer语言模型，提升生成可控性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 增强Transformer语言模型的语义表达能力，实现更精确、可控的自然语言生成，同时保持模型轻量化和可解释性。

Method: 提出语义融合方案，通过并行模糊成员特征通道编码令牌级语义，形成句子级语义矩阵，并通过门控适配器融合到语言模型中。训练采用标准的下一个令牌预测、辅助损失和轻量级均匀化器。

Result: 在合成双子句语料库上，语义融合改善了困惑度，并实现了对极性和标点的精确、用户可控生成。

Conclusion: 语义融合通过轻量级方案增强了Transformer语言模型，提升了困惑度并实现了精确、用户可控的生成，同时保持模型简洁和兼容性。

Abstract: We propose semantic fusion, a lightweight scheme that augments a Transformer
language model (LM) with a parallel, fuzzy-membership feature channel that
encodes token-level semantics. Each token is represented by a vector of
interpretable features (e.g. part-of-speech cues, shallow roles, boundary
flags, sentiment polarity and strength) whose values are graded degrees from
differentiable membership functions (e.g. power kernels). These per-token
vectors form a sentence-level semantic matrix fused via a gated adapter into
the LM. Training uses standard next-token prediction, an auxiliary loss that
reconstructs the semantic features from hidden states, and a lightweight
uniformizer that regularizes adjective-class distributions. On a synthetic
two-clause corpus with held-out adjectives for out-of-distribution (OOD)
control, semantic fusion improves perplexity and enables precise,
user-controllable generation of polarity and punctuation while maintaining
model simplicity. This approach adds only small overhead, remains fully
compatible with tied input-output embeddings, and provides an interpretable
pathway for conditioned natural language generation.

</details>


### [110] [Asterisk Operator](https://arxiv.org/abs/2509.13364)
*Zixi Li*

Main category: cs.AI

TL;DR: ∗-operator通过ASPP框架实现了高效且收敛的抽象推理，在ARC2和Game of Life中表现卓越。


<details>
  <summary>Details</summary>
Motivation: 为了解决抽象推理问题中的局部计算约束与全局推理能力的平衡问题，提出了一种新的计算范式。

Method: 提出了∗-operator，一种基于Adjacency-Structured Parallel Propagation（ASPP）的统一框架，将结构化推理任务形式化为局部、并行的状态演化过程。

Result: 通过数学分析和实验验证，∗-operator在ARC2验证中实现了100%准确率（仅6M参数），并在Conway's Game of Life中表现出优越性能。

Conclusion: Asterisk Operator（∗-operator）作为一种新颖的统一框架，通过Adjacency-Structured Parallel Propagation（ASPP）实现了抽象推理的高效与收敛，并在ARC2挑战和Conway's Game of Life中展示了其普适性和优越性能。

Abstract: We propose the \textbf{Asterisk Operator} ($\ast$-operator), a novel unified
framework for abstract reasoning based on Adjacency-Structured Parallel
Propagation (ASPP). The operator formalizes structured reasoning tasks as
local, parallel state evolution processes guided by implicit relational graphs.
We prove that the $\ast$-operator maintains local computational constraints
while achieving global reasoning capabilities, providing an efficient and
convergent computational paradigm for abstract reasoning problems. Through
rigorous mathematical analysis and comprehensive experiments on ARC2 challenges
and Conway's Game of Life, we demonstrate the operator's universality,
convergence properties, and superior performance. Our innovative
Embedding-Asterisk distillation method achieves 100\% accuracy on ARC2
validation with only 6M parameters, representing a significant breakthrough in
neural-symbolic reasoning.
  \textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel
Propagation, Asterisk Operator, Convergence, Universal Approximation

</details>


### [111] [$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation](https://arxiv.org/abs/2509.13368)
*Yuan Wei,Xiaohan Shan,Ran Miao,Jianmin Li*

Main category: cs.AI

TL;DR: $Agent^2$是一种基于LLM的全自动强化学习代理生成框架，通过双智能体架构和标准化流程，显著提升性能并实现闭环自动化。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习代理开发需要大量专业知识和迭代，失败率高且难以普及。$Agent^2$旨在通过LLM驱动的全自动化设计解决这些问题。

Method: 引入双智能体架构（生成器代理和目标代理），将强化学习开发分为MDP建模和算法优化两阶段，并采用Model Context Protocol统一框架。

Result: 在多个基准测试（如MuJoCo、MetaDrive等）中，$Agent^2$性能平均显著优于人工设计方案，最高提升达55%。

Conclusion: $Agent^2$ 通过双智能体架构和自动化设计流程，显著提升了强化学习代理的性能和开发效率，为自动化AI系统设立了新范式。

Abstract: Reinforcement learning agent development traditionally requires extensive
expertise and lengthy iterations, often resulting in high failure rates and
limited accessibility. This paper introduces $Agent^2$, a novel
agent-generates-agent framework that achieves fully automated RL agent design
through intelligent LLM-driven generation. The system autonomously transforms
natural language task descriptions and environment code into comprehensive,
high-performance reinforcement learning solutions without human intervention.
$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent
serves as an autonomous AI designer that analyzes tasks and generates
executable RL agents, while the Target Agent is the resulting automatically
generated RL agent. The framework decomposes RL development into two distinct
stages: MDP modeling and algorithmic optimization, enabling more targeted and
effective agent generation. Built on the Model Context Protocol, $Agent^2$
provides a unified framework that standardizes intelligent agent creation
across diverse environments and algorithms, while incorporating adaptive
training management and intelligent feedback analysis for continuous
improvement. Extensive experiments on a wide range of benchmarks, including
MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently
outperforms manually designed solutions across all tasks, achieving up to 55%
performance improvement and substantial gains on average. By enabling truly
end-to-end, closed-loop automation, this work establishes a new paradigm in
which intelligent agents design and optimize other agents, marking a
fundamental breakthrough for automated AI systems.

</details>


### [112] [The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](https://arxiv.org/abs/2509.13379)
*Asif Azad,Mohammad Sadat Hossain,MD Sadik Hossain Shanto,M Saifur Rahman,Md Rizwan Pervez*

Main category: cs.AI

TL;DR: 本研究全面评估了16种VLM在不确定性量化方面的表现，发现大模型表现更优，且更确定的模型准确率更高，为多模态系统的可靠uncertainty评估奠定基础。


<details>
  <summary>Details</summary>
Motivation: 尽管性能基准测试增进了我们对视觉语言模型（VLM）能力的理解，但uncertainty量化这一关键维度尚未得到充分关注。

Method: 我们进行了全面的不确定性基准测试研究，评估了16种最先进的VLM（开源和闭源）在6个多模态数据集上使用3种不同的评分函数。

Result: 研究发现，更大的模型在不确定性量化方面表现更优；更确定的模型准确率更高；数学和推理任务在所有模型中表现较差。

Conclusion: 本研究为多模态系统中可靠的uncertainty评估奠定了基础，发现更大的模型在不确定性量化方面表现更优，且更确定的模型准确率更高。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex
visual understanding across scientific and reasoning tasks. While performance
benchmarking has advanced our understanding of these capabilities, the critical
dimension of uncertainty quantification has received insufficient attention.
Therefore, unlike prior conformal prediction studies that focused on limited
settings, we conduct a comprehensive uncertainty benchmarking study, evaluating
16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets
with 3 distinct scoring functions. Our findings demonstrate that larger models
consistently exhibit better uncertainty quantification; models that know more
also know better what they don't know. More certain models achieve higher
accuracy, while mathematical and reasoning tasks elicit poorer uncertainty
performance across all models compared to other domains. This work establishes
a foundation for reliable uncertainty evaluation in multimodal systems.

</details>


### [113] [From Next Token Prediction to (STRIPS) World Models -- Preliminary Results](https://arxiv.org/abs/2509.13389)
*Carlos Núñez-Molina,Vicenç Gómez,Hector Geffner*

Main category: cs.AI

TL;DR: 使用Transformer从动作轨迹学习命题STRIPS世界模型，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨如何仅从动作轨迹中学习命题STRIPS世界模型。

Method: 使用深度学习架构（Transformer）和梯度下降，将任务视为监督下的下一个令牌预测问题，其中令牌是动作。

Result: 实验证明，该方法能够从随机有效和无效动作序列中学习到准确的模型。

Conclusion: 研究表明，合适的Transformer架构能够准确表示命题STRIPS世界模型，并且仅通过随机有效和无效动作序列即可学习这些模型。

Abstract: We consider the problem of learning propositional STRIPS world models from
action traces alone, using a deep learning architecture (transformers) and
gradient descent. The task is cast as a supervised next token prediction
problem where the tokens are the actions, and an action $a$ may follow an
action sequence if the hidden effects of the previous actions do not make an
action precondition of $a$ false. We show that a suitable transformer
architecture can faithfully represent propositional STRIPS world models, and
that the models can be learned from sets of random valid (positive) and invalid
(negative) action sequences alone. A number of experiments are reported.

</details>


### [114] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: SteeringControl是一个评估表示引导方法在核心对齐目标（偏见、有害生成、幻觉）及其对次要行为（如奉承和常识道德）影响的基准。研究发现引导效果高度依赖于方法、模型和行为的组合，不当组合会导致概念纠缠。


<details>
  <summary>Details</summary>
Motivation: 现有对齐工作常强调真实性或推理能力，但许多未探索的权衡尚未系统化理解，因此需要系统性评估引导方法在核心对齐目标（如偏见、有害生成和幻觉）及其对次要行为（如奉承和常识道德）的影响。

Method: 通过收集安全相关的主要和次要行为数据集，围绕五种流行引导方法进行评价，并基于独特组件构建模块化引导框架。

Result: 在Qwen-2.5-7B和Llama-3.1-8B上的实验表明，引导效果高度依赖于方法、模型和行为的特定组合，且不当组合会导致概念纠缠。

Conclusion: 研究发现，强引导性能依赖于特定引导方法、模型和目标行为的组合，且这三者组合不当可能导致严重的概念纠缠。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [115] [AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving](https://arxiv.org/abs/2509.13547)
*Harper Reed,Michael Sugimura,Angelo Zangari*

Main category: cs.AI

TL;DR: Giving LLM agents human-like collaborative tools improves their performance on hard tasks, with varying strategies adopted by different models, indicating these tools enhance reasoning when most needed.


<details>
  <summary>Details</summary>
Motivation: To determine if giving LLM agents collaborative tools and autonomy, similar to humans, can improve their performance.

Method: Equipping Claude Code agents with MCP-based social media and journaling tools, allowing them to use these tools autonomously across 34 Aider Polyglot Python programming challenges.

Result: Collaborative tools significantly improved performance on the hardest problems, with 15-40% lower cost, 12-27% fewer turns, and 12-38% faster completion than baseline agents. Different models adopted distinct collaborative strategies without explicit instruction.

Conclusion: AI agents can benefit from human-inspired collaboration tools, especially at the edge of their capabilities, suggesting these tools act as reasoning enhancers rather than universal efficiency boosts.

Abstract: We investigate whether giving LLM agents the collaborative tools and autonomy
that humans naturally use for problem solving can improve their performance. We
equip Claude Code agents with MCP-based social media and journaling tools and
allow them to use these tools as they see fit. Across 34 Aider Polyglot Python
programming challenges, collaborative tools substantially improve performance
on the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and
12-38% faster completion than baseline agents. Effects on the full challenge
set are mixed, suggesting these tools act as performance enhancers when
additional reasoning scaffolding is most needed. Surprisingly, Different models
naturally adopted distinct collaborative strategies without explicit
instruction. Sonnet 3.7 engaged broadly across tools and benefited from
articulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,
leaning on journal-based semantic search when problems were genuinely
difficult. This mirrors how human developers adjust collaboration based on
expertise and task complexity. Behavioral analysis shows agents prefer writing
over reading by about 2-9x, indicating that structured articulation drives much
of the improvement rather than information access alone. Overall, AI agents can
systematically benefit from human-inspired collaboration tools at the edge of
their capabilities, pointing to adaptive collaborative interfaces as reasoning
enhancers rather than universal efficiency boosts.

</details>


### [116] [Gen AI in Proof-based Math Courses: A Pilot Study](https://arxiv.org/abs/2509.13570)
*Hannah Klawa,Shraddha Rajpal,Cigole Thomas*

Main category: cs.AI

TL;DR: 研究探讨了在三个基于证明的本科数学课程中学生对生成式AI的使用和看法，并讨论了未来在教学中整合AI的考虑。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在高等教育中的快速崛起和当前AI检测工具的不可靠性，制定鼓励学生学习和批判性思维的政策变得越来越重要。

Method: 通过调查问卷和学生访谈，分析了学生如何使用AI工具、他们对生成式AI有用性和局限性的看法。

Result: 研究分析了学生对生成式AI的使用和看法，以及这些看法对基于证明的数学教学的影响。

Conclusion: 论文讨论了将生成式AI整合到基于证明的数学教学中的未来考虑。

Abstract: With the rapid rise of generative AI in higher education and the
unreliability of current AI detection tools, developing policies that encourage
student learning and critical thinking has become increasingly important. This
study examines student use and perceptions of generative AI across three
proof-based undergraduate mathematics courses: a first-semester abstract
algebra course, a topology course and a second-semester abstract algebra
course. In each case, course policy permitted some use of generative AI.
Drawing on survey responses and student interviews, we analyze how students
engaged with AI tools, their perceptions of generative AI's usefulness and
limitations, and what implications these perceptions hold for teaching
proof-based mathematics. We conclude by discussing future considerations for
integrating generative AI into proof-based mathematics instruction.

</details>


### [117] [InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management](https://arxiv.org/abs/2509.13704)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: InfraMind是一个专为工业管理系统设计的GUI代理框架，通过五个创新模块解决了现有自动化方案的局限性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 工业基础设施管理软件面临系统复杂性、多供应商集成和专家操作员短缺的挑战。现有自动化方案如RPA灵活性不足，LLM-based GUI代理在工业管理中面临五大关键挑战。

Method: InfraMind整合了五个创新模块：(1)基于系统搜索的探索与虚拟机快照；(2)记忆驱动的规划；(3)高级状态识别；(4)结构化知识蒸馏；(5)多层安全机制。

Result: 在开源和商业DCIM平台上的广泛实验表明，InfraMind在任务成功率和操作效率上均优于现有框架。

Conclusion: InfraMind框架通过五个创新模块有效解决了工业管理系统中的关键挑战，显著提高了任务成功率和操作效率，为工业管理自动化提供了严谨且可扩展的解决方案。

Abstract: Mission-critical industrial infrastructure, such as data centers,
increasingly depends on complex management software. Its operations, however,
pose significant challenges due to the escalating system complexity,
multi-vendor integration, and a shortage of expert operators. While Robotic
Process Automation (RPA) offers partial automation through handcrafted scripts,
it suffers from limited flexibility and high maintenance costs. Recent advances
in Large Language Model (LLM)-based graphical user interface (GUI) agents have
enabled more flexible automation, yet these general-purpose agents face five
critical challenges when applied to industrial management, including unfamiliar
element understanding, precision and efficiency, state localization, deployment
constraints, and safety requirements. To address these issues, we propose
InfraMind, a novel exploration-based GUI agentic framework specifically
tailored for industrial management systems. InfraMind integrates five
innovative modules to systematically resolve different challenges in industrial
management: (1) systematic search-based exploration with virtual machine
snapshots for autonomous understanding of complex GUIs; (2) memory-driven
planning to ensure high-precision and efficient task execution; (3) advanced
state identification for robust localization in hierarchical interfaces; (4)
structured knowledge distillation for efficient deployment with lightweight
models; and (5) comprehensive, multi-layered safety mechanisms to safeguard
sensitive operations. Extensive experiments on both open-source and commercial
DCIM platforms demonstrate that our approach consistently outperforms existing
frameworks in terms of task success rate and operational efficiency, providing
a rigorous and scalable solution for industrial management automation.

</details>


### [118] [Programmable Cognitive Bias in Social Agents](https://arxiv.org/abs/2509.13588)
*Xuan Liu,Haoyang Shang,Haojian Jin*

Main category: cs.AI

TL;DR: CoBRA是一个新工具包，通过显式编程认知偏见来解决LLM-based社会模拟中代理行为不一致的问题，包括测量和调节组件，评估显示其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过隐式自然语言描述指定代理行为，无法在不同模型间产生一致行为，且难以捕捉描述中的细微差别。

Method: CoBRA包含两个组件：1) 认知偏见指数，通过量化代理在一组经典社会科学实验中的反应来测量其认知偏见；2) 行为调节引擎，用于调整代理行为以展示受控的认知偏见。

Result: CoBRA通过演示和技术基准评估，证明其能够精确编程社交代理中的认知偏见。

Conclusion: CoBRA能够以模型无关的方式精确编程社交代理中的认知偏见，展示了其在LLM-based社会模拟中的有效性。

Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying
agent behavior in LLM-based social simulation. We found that conventional
approaches that specify agent behaviors through implicit natural language
descriptions cannot yield consistent behaviors across models, and the produced
agent behaviors do not capture the nuances of the descriptions. In contrast,
CoBRA presents a new approach to program agents' cognitive biases explicitly,
by grounding agents' expected behaviors using classic social science
experiments. CoBRA has two components: (1) Cognitive Bias Index that measures
the cognitive bias of a social agent, by quantifying the agent's reactions in a
set of validated classical social science experiments; (2) Behavioral
Regulation Engine that aligns the agent's behavior to demonstrate controlled
cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and
technical benchmarks. Our results suggest that CoBRA can precisely program the
cognitive bias demonstrated in a social agent in a model-agnostic manner.

</details>


### [119] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: StaR方法通过感知当前状态和分析指令，显著提高了多模态代理在GUI切换控制中的准确性，并在通用任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 多模态代理在图形用户界面(GUI)控制中执行切换指令的不可靠性是一个关键瓶颈，尤其是在当前状态与期望状态一致时。

Method: 提出State-aware Reasoning (StaR)训练方法，教导代理感知当前切换状态、分析指令中的期望状态，并据此行动。

Result: 实验表明，StaR能将切换指令执行准确性提高30%以上，并在三个公共基准测试中提升了通用任务性能。

Conclusion: StaR方法通过教导代理感知当前切换状态并分析指令中的期望状态，显著提高了切换指令执行的准确性，并在通用任务性能上也有所提升，展示了其在实际应用中的潜力。

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [120] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: THOR通过工具集成和分层优化，提升了LLM在高精度数学任务中的表现，并在多个基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在构建工具集成推理数据、细粒度优化和增强推理能力方面存在不足，THOR旨在解决这些挑战。

Method: 提出了THOR框架，包括TIRGen数据生成管道、基于强化学习的分层优化策略以及利用工具反馈的动态自校正机制。

Result: THOR在多样化的模型上表现出强泛化能力，在数学和代码基准测试中均取得了显著提升。

Conclusion: THOR通过整合外部工具和分层优化策略，显著提升了大型语言模型在数学推理和高精度任务中的表现，并在多个数学和代码基准测试中实现了最先进的性能。

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [121] [MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation](https://arxiv.org/abs/2509.13773)
*Zhipeng Bian,Jieming Zhu,Xuyang Xie,Quanyu Dai,Zhou Zhao,Zhenhua Dong*

Main category: cs.AI

TL;DR: MIRA是一个智能手机上的任务指令推荐框架，通过多模态大语言模型和结构化推理提升AI任务指令推荐的准确性，优化用户与AI服务的互动体验。


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术的快速发展推动了多样化AI服务在智能手机中的集成，但用户如何直观、高效地访问这些服务仍是一个挑战。

Method: 1) 基于多模态大语言模型（MLLM）的推荐流程，结合结构化推理提取关键实体、推断用户意图并生成精确指令；2) 模板增强推理机制，整合高级推理模板提升任务推断准确性；3) 前缀树约束解码策略，限制输出为预定义指令候选，确保建议的一致性和意图对齐。

Result: 通过真实世界标注数据集和用户研究的评估，MIRA在指令推荐准确性上取得了显著提升。

Conclusion: MIRA框架通过其创新的多模态大语言模型、模板增强推理机制和前缀树约束解码策略，显著提高了智能手机上AI任务指令推荐的准确性，有望彻底改变用户与AI服务的互动方式。

Abstract: The rapid advancement of generative AI technologies is driving the
integration of diverse AI-powered services into smartphones, transforming how
users interact with their devices. To simplify access to predefined AI
services, this paper introduces MIRA, a pioneering framework for task
instruction recommendation that enables intuitive one-touch AI tasking on
smartphones. With MIRA, users can long-press on images or text objects to
receive contextually relevant instruction recommendations for executing AI
tasks. Our work introduces three key innovations: 1) A multimodal large
language model (MLLM)-based recommendation pipeline with structured reasoning
to extract key entities, infer user intent, and generate precise instructions;
2) A template-augmented reasoning mechanism that integrates high-level
reasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based
constrained decoding strategy that restricts outputs to predefined instruction
candidates, ensuring coherent and intent-aligned suggestions. Through
evaluation using a real-world annotated datasets and a user study, MIRA has
demonstrated substantial improvements in the accuracy of instruction
recommendation. The encouraging results highlight MIRA's potential to
revolutionize the way users engage with AI services on their smartphones,
offering a more seamless and efficient experience.

</details>


### [122] [An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques](https://arxiv.org/abs/2509.13880)
*Mingwei Zhang,Zhenhao Gu,Liangda Fang,Cunjing Ge,Ziliang Chen,Zhao-Rong Lai,Quanlong Guan*

Main category: cs.AI

TL;DR: 本文提出一种基于DPLL架构的精确方法，结合混合整数规划技术，显著提升MCILC任务的效率，尤其在应用实例中表现优异。


<details>
  <summary>Details</summary>
Motivation: 线性约束在计算机科学、运筹学和优化等领域中至关重要，而MCILC作为其核心任务之一，亟需高效解决方案。

Method: 采用基于DPLL架构的精确方法，并结合混合整数规划中的简化技术。

Result: 在2840个随机基准测试中解决了1718个实例（优于现有技术的1470个），并在4131个应用基准测试中全部解决。

Conclusion: 本文提出的基于DPLL架构的精确方法在MCILC任务中显著优于现有技术，尤其在应用基准测试中表现卓越。

Abstract: Linear constraints are one of the most fundamental constraints in fields such
as computer science, operations research and optimization. Many applications
reduce to the task of model counting over integer linear constraints (MCILC).
In this paper, we design an exact approach to MCILC based on an exhaustive DPLL
architecture. To improve the efficiency, we integrate several effective
simplification techniques from mixed integer programming into the architecture.
We compare our approach to state-of-the-art MCILC counters and propositional
model counters on 2840 random and 4131 application benchmarks. Experimental
results show that our approach significantly outperforms all exact methods in
random benchmarks solving 1718 instances while the state-of-the-art approach
only computes 1470 instances. In addition, our approach is the only approach to
solve all 4131 application instances.

</details>


### [123] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: 研究发现递归网络拓扑变化能带来认知性能的过渡性提升，但分层网络无优势，支持认知演化的过渡理论。


<details>
  <summary>Details</summary>
Motivation: 探讨认知是否通过一系列主要过渡演化，这些过渡操纵生物神经网络的结构，从根本上改变信息流。

Method: 使用理想化信息流模型和人工神经网络（ANNs），比较了前馈、递归和分层拓扑网络的性能，并控制网络大小和资源。

Result: 递归网络在处理输入类型上表现出质的扩展，并在学习最复杂语法时性能显著提升，而分层网络在语法学习中未表现出优势。

Conclusion: 研究发现，某些信息流的变化可以导致认知性能的过渡性变化，尤其是递归网络在处理复杂语法时表现出质的飞跃。

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


### [124] [CrowdAgent: Multi-Agent Managed Multi-Source Annotation System](https://arxiv.org/abs/2509.14030)
*Maosheng Qin,Renyu Zhu,Mingxuan Xia,Chenkai Chen,Zhen Zhu,Minmin Lin,Junbo Zhao,Lu Xu,Changjie Fan,Runze Wu,Haobo Wang*

Main category: cs.AI

TL;DR: CrowdAgent是一个多智能体系统，通过动态管理LLMs、SLMs和人类专家的协作标注流程，解决了NLP中多源标注的全面控制问题。


<details>
  <summary>Details</summary>
Motivation: 现代NLP需要高质量标注数据，但现有方法仅关注标注步骤本身，缺乏对多源标注（如LLMs、SLMs和人类专家）的动态管理和流程控制。

Method: 引入CrowdAgent，一个多智能体系统，采用新颖的方法论，合理分配任务，使LLMs、SLMs和人类专家在协作标注流程中协同工作。

Result: 在六个多样化的多模态分类任务上的广泛实验验证了CrowdAgent的有效性。

Conclusion: CrowdAgent通过整合任务分配、数据标注和质量/成本管理，提供了一个全面的端到端流程控制解决方案，有效解决了现代NLP中多源标注的动态管理问题。

Abstract: High-quality annotated data is a cornerstone of modern Natural Language
Processing (NLP). While recent methods begin to leverage diverse annotation
sources-including Large Language Models (LLMs), Small Language Models (SLMs),
and human experts-they often focus narrowly on the labeling step itself. A
critical gap remains in the holistic process control required to manage these
sources dynamically, addressing complex scheduling and quality-cost trade-offs
in a unified manner. Inspired by real-world crowdsourcing companies, we
introduce CrowdAgent, a multi-agent system that provides end-to-end process
control by integrating task assignment, data annotation, and quality/cost
management. It implements a novel methodology that rationally assigns tasks,
enabling LLMs, SLMs, and human experts to advance synergistically in a
collaborative annotation workflow. We demonstrate the effectiveness of
CrowdAgent through extensive experiments on six diverse multimodal
classification tasks. The source code and video demo are available at
https://github.com/QMMMS/CrowdAgent.

</details>


### [125] [Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning](https://arxiv.org/abs/2509.14195)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: 通过分层架构（GCN+MLP）验证二阶学习促进心理表征同构性，显著提升任务性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索二阶学习如何促进心理表征（即内部模型与外部环境的结构同构性）的涌现，以解决高级认知研究中实证验证的挑战。

Method: 提出了一种分层架构，包括作为一阶学习器的图卷积网络（GCN）和作为二阶学习器的MLP控制器。GCN直接映射节点特征到最优导航路径预测，MLP在遇到结构新颖的迷宫环境时动态调整GCN参数。

Result: 定量和定性结果表明，二阶学习在心理表征与环境同构时表现尤为有效，显著提升了性能并在未见过的迷宫任务中展现了强大的泛化能力。

Conclusion: 研究实证了二阶学习（即适应一阶学习的学习机制）在促进环境-认知同构性中的关键作用，支持结构化心理表征对提升学习有效性的重要性。

Abstract: Mental representation, characterized by structured internal models mirroring
external environments, is fundamental to advanced cognition but remains
challenging to investigate empirically. Existing theory hypothesizes that
second-order learning -- learning mechanisms that adapt first-order learning
(i.e., learning about the task/domain) -- promotes the emergence of such
environment-cognition isomorphism. In this paper, we empirically validate this
hypothesis by proposing a hierarchical architecture comprising a Graph
Convolutional Network (GCN) as a first-order learner and an MLP controller as a
second-order learner. The GCN directly maps node-level features to predictions
of optimal navigation paths, while the MLP dynamically adapts the GCN's
parameters when confronting structurally novel maze environments. We
demonstrate that second-order learning is particularly effective when the
cognitive system develops an internal mental map structurally isomorphic to the
environment. Quantitative and qualitative results highlight significant
performance improvements and robust generalization on unseen maze tasks,
providing empirical support for the pivotal role of structured mental
representations in maximizing the effectiveness of second-order learning.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [126] [CraftMesh: High-Fidelity Generative Mesh Manipulation via Poisson Seamless Fusion](https://arxiv.org/abs/2509.13688)
*James Jincheng,Youcheng Cai,Ligang Liu*

Main category: cs.GR

TL;DR: CraftMesh通过2D和3D生成模型的结合，实现了高保真3D网格编辑，解决了复杂几何体的编辑难题。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法在复杂几何体上效果不佳，难以产生高保真结果，因此需要一种更高效的网格编辑框架。

Method: 将网格编辑分解为2D参考图像编辑、区域特定3D网格生成和与原模型无缝融合的流程，结合Poisson Geometric Fusion和Poisson Texture Harmonization技术。

Result: 实验结果表明，CraftMesh在复杂编辑任务中优于现有技术，提供了更好的全局一致性和局部细节。

Conclusion: CraftMesh通过Poisson Seamless Fusion技术，实现了高保真的3D网格编辑，显著提升了复杂几何体编辑的全局一致性和局部细节表现。

Abstract: Controllable, high-fidelity mesh editing remains a significant challenge in
3D content creation. Existing generative methods often struggle with complex
geometries and fail to produce detailed results. We propose CraftMesh, a novel
framework for high-fidelity generative mesh manipulation via Poisson Seamless
Fusion. Our key insight is to decompose mesh editing into a pipeline that
leverages the strengths of 2D and 3D generative models: we edit a 2D reference
image, then generate a region-specific 3D mesh, and seamlessly fuse it into the
original model. We introduce two core techniques: Poisson Geometric Fusion,
which utilizes a hybrid SDF/Mesh representation with normal blending to achieve
harmonious geometric integration, and Poisson Texture Harmonization for
visually consistent texture blending. Experimental results demonstrate that
CraftMesh outperforms state-of-the-art methods, delivering superior global
consistency and local detail in complex editing tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [127] [Maximizing UAV Cellular Connectivity with Reinforcement Learning for BVLoS Path Planning](https://arxiv.org/abs/2509.13336)
*Mehran Behjati,Rosdiadee Nordin,Nor Fadzilah Abdullah*

Main category: cs.RO

TL;DR: 该论文提出一种基于强化学习的无人机路径规划方法，优化通信质量与飞行距离，仿真验证其有效性，并具备未来系统集成潜力。


<details>
  <summary>Details</summary>
Motivation: 解决超视距飞行中无人机路径规划面临的通信质量与飞行距离优化的挑战。

Method: 采用强化学习技术训练智能体，以无人机与基站间通信链路质量作为奖励函数，结合真实世界空中覆盖约束和经验空中信道模型。

Result: 仿真结果表明，该方法能有效训练智能体并生成可行的无人机路径规划。

Conclusion: 该论文提出的强化学习方法有效解决了无人机在超视距飞行中的路径规划问题，确保了通信质量的最大化，并具有未来集成到地面控制系统的潜力。

Abstract: This paper presents a reinforcement learning (RL) based approach for path
planning of cellular connected unmanned aerial vehicles (UAVs) operating beyond
visual line of sight (BVLoS). The objective is to minimize travel distance
while maximizing the quality of cellular link connectivity by considering real
world aerial coverage constraints and employing an empirical aerial channel
model. The proposed solution employs RL techniques to train an agent, using the
quality of communication links between the UAV and base stations (BSs) as the
reward function. Simulation results demonstrate the effectiveness of the
proposed method in training the agent and generating feasible UAV path plans.
The proposed approach addresses the challenges due to limitations in UAV
cellular communications, highlighting the need for investigations and
considerations in this area. The RL algorithm efficiently identifies optimal
paths, ensuring maximum connectivity with ground BSs to ensure safe and
reliable BVLoS flight operation. Moreover, the solution can be deployed as an
offline path planning module that can be integrated into future ground control
systems (GCS) for UAV operations, enhancing their capabilities and safety. The
method holds potential for complex long range UAV applications, advancing the
technology in the field of cellular connected UAV path planning.

</details>


### [128] [Real World Robotic Exploration using Deep Neural Networks Trained in Photorealistic Reconstructed Environments](https://arxiv.org/abs/2509.13342)
*Isaac Ronald Ward*

Main category: cs.RO

TL;DR: 改进的神经网络损失函数结合位置和旋转误差，提升室内定位精度，并利用摄影测量数据生成本地化训练集，实现高精度实时导航。


<details>
  <summary>Details</summary>
Motivation: 现有的深度神经网络在机器人视觉定位中存在精度不足的问题，尤其是在面对感知混淆时表现不佳。本文旨在通过改进损失函数和利用本地化数据集，提升定位性能。

Method: 通过扩展神经网络的损失函数，将位置和旋转误差直观结合，以提高对感知混淆的鲁棒性。此外，使用摄影测量数据生成姿态标签数据集，支持本地化训练。

Result: 改进后的网络在室内场景中表现出显著提升，中位位置误差降低9.64%，旋转误差降低2.99%。本地化训练后的模型达到0.11米和0.89度的定位精度，并成功应用于实时导航。

Conclusion: 本文提出了一种改进的深度神经网络方法，通过扩展损失函数结合位置和旋转误差，显著提高了机器人在室内场景中的定位精度。同时，利用摄影测量数据生成带姿态标签的数据集，支持本地环境下的模型训练，最终实现了0.11米和0.89度的定位精度，并成功应用于实时导航算法。

Abstract: In this work, an existing deep neural network approach for determining a
robot's pose from visual information (RGB images) is modified, improving its
localization performance without impacting its ease of training. Explicitly,
the network's loss function is extended in a manner which intuitively combines
the positional and rotational error in order to increase robustness to
perceptual aliasing. An improvement in the localization accuracy for indoor
scenes is observed: with decreases of up to 9.64% and 2.99% in the median
positional and rotational error respectively, when compared to the unmodified
network.
  Additionally, photogrammetry data is used to produce a pose-labelled dataset
which allows the above model to be trained on a local environment, resulting in
localization accuracies of 0.11m & 0.89 degrees. This trained model forms the
basis of a navigation algorithm, which is tested in real-time on a TurtleBot (a
wheeled robotic device). As such, this work introduces a full pipeline for
creating a robust navigational algorithm for any given real world indoor scene;
the only requirement being a collection of images from the scene, which can be
captured in as little as 330 seconds of

</details>


### [129] [Label-Efficient Grasp Joint Prediction with Point-JEPA](https://arxiv.org/abs/2509.13349)
*Jed Guzelkabaagac,Boris Petrović*

Main category: cs.RO

TL;DR: Point-JEPA通过自监督预训练在低标签情况下显著提升抓取关节角度预测效果，达到与全监督相当的水平。


<details>
  <summary>Details</summary>
Motivation: 研究3D自监督预训练（Point-JEPA）是否能够实现标签高效的抓取关节角度预测。

Method: 使用点云从网格中分块，并利用ShapeNet预训练的Point-JEPA编码器，训练一个轻量级的多假设头，采用赢家通吃策略，并通过顶部逻辑选择进行评估。

Result: 在DLR-Hand II数据集上，Point-JEPA在低标签情况下将RMSE降低了26%，并达到与完全监督相当的效果。

Conclusion: JEPA风格的预训练是一种实用的方法，用于数据高效的学习抓取。

Abstract: We investigate whether 3D self-supervised pretraining with a Joint-Embedding
Predictive Architecture (Point-JEPA) enables label-efficient grasp joint-angle
prediction. Using point clouds tokenized from meshes and a ShapeNet-pretrained
Point-JEPA encoder, we train a lightweight multi-hypothesis head with
winner-takes-all and evaluate by top-logit selection. On DLR-Hand II with
object-level splits, Point-JEPA reduces RMSE by up to 26% in low-label regimes
and reaches parity with full supervision. These results suggest JEPA-style
pretraining is a practical approach for data-efficient grasp learning.

</details>


### [130] [Using role-play and Hierarchical Task Analysis for designing human-robot interaction](https://arxiv.org/abs/2509.13378)
*Mattias Wingren,Sören Andersson,Sara Rosenberg,Malin Andtfolk,Susanne Hägglund,Prashani Jayasingha Arachchige,Linda Nyholm*

Main category: cs.RO

TL;DR: 本文探讨了角色扮演和层次任务分析在机器人辅助药房项目中的应用，展示了它们在人机交互中的优势，并建议未来研究开发更适合社交机器人交互的任务分析方法。


<details>
  <summary>Details</summary>
Motivation: 展示这两种方法在机器人辅助社区药房应用中的潜力，以促进人机交互领域的发展。

Method: 角色扮演和层次任务分析

Result: 角色扮演提供了可控且可调整的环境来理解客户需求，层次任务分析确保了行为的正确建模并促进了共同设计。

Conclusion: 未来研究可专注于开发特别适合社交机器人交互的任务分析方法。

Abstract: We present the use of two methods we believe warrant more use than they
currently have in the field of human-robot interaction: role-play and
Hierarchical Task Analysis. Some of its potential is showcased through our use
of them in an ongoing research project which entails developing a robot
application meant to assist at a community pharmacy. The two methods have
provided us with several advantages. The role-playing provided a controlled and
adjustable environment for understanding the customers' needs where pharmacists
could act as models for the robot's behavior; and the Hierarchical Task
Analysis ensured the behavior displayed was modelled correctly and aided
development through facilitating co-design. Future research could focus on
developing task analysis methods especially suited for social robot
interaction.

</details>


### [131] [ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy](https://arxiv.org/abs/2509.13380)
*Alejandro D. Mousist*

Main category: cs.RO

TL;DR: ASTREA是首个在飞行硬件上部署的LLM代理系统，地面实验成功，但轨道验证显示推理延迟问题。


<details>
  <summary>Details</summary>
Motivation: 开发首个在飞行遗产硬件（TRL 9）上部署的代理系统，用于自主航天器操作，以热控制为代表用例。

Method: 整合了资源受限的大型语言模型（LLM）代理与强化学习控制器，采用异步架构，适用于空间认证平台。

Result: 地面实验显示LLM引导的监督提高了热稳定性并减少了违规，但在国际空间站（ISS）的轨道验证中，由于推理延迟与低地球轨道（LEO）卫星的快速热周期不匹配，导致性能下降。

Conclusion: ASTREA展示了在真实飞行环境中部署基于LLM的代理系统的潜力和当前限制，为未来空间自主性提供了实用设计指南。

Abstract: This paper presents ASTREA, the first agentic system deployed on
flight-heritage hardware (TRL 9) for autonomous spacecraft operations. Using
thermal control as a representative use case, we integrate a
resource-constrained Large Language Model (LLM) agent with a reinforcement
learning controller in an asynchronous architecture tailored for
space-qualified platforms. Ground experiments show that LLM-guided supervision
improves thermal stability and reduces violations, confirming the feasibility
of combining semantic reasoning with adaptive control under hardware
constraints. However, on-orbit validation aboard the International Space
Station (ISS) reveals performance degradation caused by inference latency
mismatched with the rapid thermal cycles characteristic of Low Earth Orbit
(LEO) satellites. These results highlight both the opportunities and current
limitations of agentic LLM-based systems in real flight environments, providing
practical design guidelines for future space autonomy.

</details>


### [132] [Cooperative Target Detection with AUVs: A Dual-Timescale Hierarchical MARDL Approach](https://arxiv.org/abs/2509.13381)
*Zhang Xueyao,Yang Bo,Yu Zhiwen,Cao Xuelin,George C. Alexandropoulos,Merouane Debbah,Chau Yuen*

Main category: cs.RO

TL;DR: 提出H-MAPPO框架解决AUV协作中的隐蔽性问题，通过分层策略优化实现高效隐蔽协作，仿真验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 在敌对环境中，协作AUV通信存在暴露风险，如何在高效协作的同时确保隐蔽操作成为水下协作任务的关键挑战。

Method: 采用双时间尺度的分层多智能体近端策略优化（H-MAPPO）框架，高层组件通过中央AUV确定参与任务的个体，低层组件通过参与AUV的功率和轨迹控制降低暴露概率。

Result: 仿真结果表明，所提框架实现了快速收敛，性能优于基准算法，并在保证隐蔽操作的同时最大化长期协作效率。

Conclusion: 提出的H-MAPPO框架在保证隐蔽操作的同时，最大化长期协作效率，且在性能和收敛速度上优于基准算法。

Abstract: Autonomous Underwater Vehicles (AUVs) have shown great potential for
cooperative detection and reconnaissance. However, collaborative AUV
communications introduce risks of exposure. In adversarial environments,
achieving efficient collaboration while ensuring covert operations becomes a
key challenge for underwater cooperative missions. In this paper, we propose a
novel dual time-scale Hierarchical Multi-Agent Proximal Policy Optimization
(H-MAPPO) framework. The high-level component determines the individuals
participating in the task based on a central AUV, while the low-level component
reduces exposure probabilities through power and trajectory control by the
participating AUVs. Simulation results show that the proposed framework
achieves rapid convergence, outperforms benchmark algorithms in terms of
performance, and maximizes long-term cooperative efficiency while ensuring
covert operations.

</details>


### [133] [VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization](https://arxiv.org/abs/2509.13386)
*Hansol Lim,Minhyeok Im,Jonathan Boyack,Jee Won Lee,Jongseong Brad Choi*

Main category: cs.RO

TL;DR: VEGA是一种充电感知的电动汽车导航代理，结合物理信息学习和强化学习，无需额外传感器即可优化路径和充电策略，展示了全球适用性。


<details>
  <summary>Details</summary>
Motivation: 随着软件定义车辆（SDV）需求的增加和电动汽车（EV）计算能力的提升，需要一种能够根据车辆当前状态和环境定制充电感知路径优化的AI系统。

Method: VEGA采用物理信息神经网络操作器（PINO）和强化学习（RL）代理，结合PPO算法和预算A*师生指导，优化充电站选择和停留时间。

Result: 在长距离路线（如旧金山至纽约）评估中，VEGA的充电站选择、停留时间、SoC管理和总旅行时间与特斯拉行程规划器接近，且更具保守性。尽管仅在美国地区训练，VEGA在法国和日本也表现出了良好的通用性。

Conclusion: VEGA成功整合了物理信息学习和强化学习，为电动汽车提供了一种无需额外传感器的经济路径优化方案，展示了其在全球范围内的适用性。

Abstract: Demands for software-defined vehicles (SDV) are rising and electric vehicles
(EVs) are increasingly being equipped with powerful computers. This enables
onboard AI systems to optimize charge-aware path optimization customized to
reflect vehicle's current condition and environment. We present VEGA, a
charge-aware EV navigation agent that plans over a charger-annotated road graph
using Proximal Policy Optimization (PPO) with budgeted A* teacher-student
guidance under state-of-charge (SoC) feasibility. VEGA consists of two modules.
First, a physics-informed neural operator (PINO), trained on real vehicle speed
and battery-power logs, uses recent vehicle speed logs to estimate aerodynamic
drag, rolling resistance, mass, motor and regenerative-braking efficiencies,
and auxiliary load by learning a vehicle-custom dynamics. Second, a
Reinforcement Learning (RL) agent uses these dynamics to optimize a path with
optimal charging stops and dwell times under SoC constraints. VEGA requires no
additional sensors and uses only vehicle speed signals. It may serve as a
virtual sensor for power and efficiency to potentially reduce EV cost. In
evaluation on long routes like San Francisco to New York, VEGA's stops, dwell
times, SoC management, and total travel time closely track Tesla Trip Planner
while being slightly more conservative, presumably due to real vehicle
conditions such as vehicle parameter drift due to deterioration. Although
trained only in U.S. regions, VEGA was able to compute optimal charge-aware
paths in France and Japan, demonstrating generalizability. It achieves
practical integration of physics-informed learning and RL for EV eco-routing.

</details>


### [134] [A Convex Formulation of Compliant Contact between Filaments and Rigid Bodies](https://arxiv.org/abs/2509.13434)
*Wei-Chen Li,Glen Chou*

Main category: cs.RO

TL;DR: 提出了一种统一框架，结合DER、压力场补丁和凸接触公式，首次实现了细长纤维与刚体间摩擦相互作用的准确模拟，验证了其物理保真度，并展示了在软机器人和可变形物体操纵中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设纤维永久附着于刚体，无法准确模拟细长纤维与刚体之间的摩擦相互作用。本文旨在填补这一空白。

Method: 结合离散弹性杆（DER）建模、压力场补丁接触模型和凸接触公式，构建了一个能够全局优化求解的框架，确保接触速度与冲量的互补性。

Result: 通过验证摩擦力的准确性和与基线方法的物理保真度对比，证明了该框架的有效性，并成功应用于软机器人和可变形物体操纵场景。

Conclusion: 该框架通过统一的离散弹性杆模型、压力场补丁接触模型和凸接触公式，成功模拟了细长纤维与刚体之间的摩擦相互作用，验证了其物理保真度，并展示了在软机器人和可变形物体操纵中的广泛应用潜力。

Abstract: We present a computational framework for simulating filaments interacting
with rigid bodies through contact. Filaments are challenging to simulate due to
their codimensionality, i.e., they are one-dimensional structures embedded in
three-dimensional space. Existing methods often assume that filaments remain
permanently attached to rigid bodies. Our framework unifies discrete elastic
rod (DER) modeling, a pressure field patch contact model, and a convex contact
formulation to accurately simulate frictional interactions between slender
filaments and rigid bodies - capabilities not previously achievable. Owing to
the convex formulation of contact, each time step can be solved to global
optimality, guaranteeing complementarity between contact velocity and impulse.
We validate the framework by assessing the accuracy of frictional forces and
comparing its physical fidelity against baseline methods. Finally, we
demonstrate its applicability in both soft robotics, such as a stochastic
filament-based gripper, and deformable object manipulation, such as shoelace
tying, providing a versatile simulator for systems involving complex
filament-filament and filament-rigid body interactions.

</details>


### [135] [Trajectory Tracking with Reachability-Guided Quadratic Programming and Freeze-Resume](https://arxiv.org/abs/2509.13501)
*Hossein Gholampour,Logan E. Beaver*

Main category: cs.RO

TL;DR: 提出一种输出空间方法，通过离线可达性检查和在线二次规划跟踪，确保机器人在路径跟踪中安全暂停和恢复，性能优于纯追踪。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在路径跟踪过程中因人或物体介入而需要安全暂停和恢复的问题。

Method: 离线进行预运行可达性检查，在线应用二次规划跟踪运动计划，并使用一步可达性测试来限制系统能拒绝的最大干扰。

Result: 系统在模拟中表现出比纯追踪更好的性能，能高效处理安全停止和非计划偏差，并返回运动计划。

Conclusion: 该方法通过离线可达性检查和在线二次规划跟踪，确保机器人在路径跟踪中安全暂停和恢复，且在确定性情况下能完美跟踪参考路径，无需重新规划。

Abstract: Many robotic systems must follow planned paths yet pause safely and resume
when people or objects intervene. We present an output-space method for systems
whose tracked output can be feedback-linearized to a double integrator (e.g.,
manipulators). The approach has two parts. Offline, we perform a pre-run
reachability check to verify that the motion plan respects speed and
acceleration magnitude limits. Online, we apply a quadratic program to track
the motion plan under the same limits. We use a one-step reachability test to
bound the maximum disturbance the system is capable of rejecting. When the
state coincides with the reference path we recover perfect tracking in the
deterministic case, and we correct errors using a KKT-inspired weight. We
demonstrate that safety stops and unplanned deviations are handled efficiently,
and the system returns to the motion plan without replanning. We demonstrate
our system's improved performance over pure pursuit in simulation.

</details>


### [136] [Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning](https://arxiv.org/abs/2509.13534)
*Chunxin Zheng,Kai Chen,Zhihai Bi,Yulin Li,Liang Pan,Jinni Zhou,Haoang Li,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出了一种结合人类运动先验和NSDF的强化学习框架，用于仿人机器人的全身拥抱任务，提高了稳定性和负载能力，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统仅依赖末端执行器的抓取方法在涉及笨重物体时存在固有稳定性和负载限制，而全身操作为此类场景提供了有前景的解决方案。

Method: 本文介绍了一种强化学习框架，该框架集成了预训练的人类运动先验与神经符号距离场（NSDF）表示，以实现稳健的全身拥抱。采用师生架构来提炼大规模人类运动数据，生成运动学自然且物理可行的全身运动模式。

Result: 通过全面仿真和真实世界实验验证，结果表明该方法对多样形状和大小的物体具有更好的适应性，并成功实现了仿真到现实的迁移。

Conclusion: 所提出的框架为仿人机器人的多接触和长时程全身操作任务提供了有效且实用的解决方案。

Abstract: Whole-body manipulation (WBM) for humanoid robots presents a promising
approach for executing embracing tasks involving bulky objects, where
traditional grasping relying on end-effectors only remains limited in such
scenarios due to inherent stability and payload constraints. This paper
introduces a reinforcement learning framework that integrates a pre-trained
human motion prior with a neural signed distance field (NSDF) representation to
achieve robust whole-body embracing. Our method leverages a teacher-student
architecture to distill large-scale human motion data, generating kinematically
natural and physically feasible whole-body motion patterns. This facilitates
coordinated control across the arms and torso, enabling stable multi-contact
interactions that enhance the robustness in manipulation and also the load
capacity. The embedded NSDF further provides accurate and continuous geometric
perception, improving contact awareness throughout long-horizon tasks. We
thoroughly evaluate the approach through comprehensive simulations and
real-world experiments. The results demonstrate improved adaptability to
diverse shapes and sizes of objects and also successful sim-to-real transfer.
These indicate that the proposed framework offers an effective and practical
solution for multi-contact and long-horizon WBM tasks of humanoid robots.

</details>


### [137] [Semantic 3D Reconstructions with SLAM for Central Airway Obstruction](https://arxiv.org/abs/2509.13541)
*Ayberk Acar,Fangjie Li,Hao Li,Lidia Al-Zogbi,Kanyifeechukwu Jane Oguine,Susheela Sharma Stern,Jesse F. d'Almeida,Robert J. Webster III,Ipek Oguz,Jie Ying Wu*

Main category: cs.RO

TL;DR: 新方法结合语义分割与实时SLAM，用于内窥镜CAO治疗，提高重建速度与精度，支持自主机器人干预。


<details>
  <summary>Details</summary>
Motivation: 传统治疗CAO的方法并发症风险高，机器人干预结合场景理解与映射为自动化提供了可能。

Method: 结合DROID-SLAM与分割模型，实时重建气道3D几何并标注阻塞区域。通过离体模型验证重建质量。

Result: 重建质量与CT扫描高度相似（0.62 mm Chamfer距离），实时生成标注的3D地图，显著提高手术场景的准确性。

Conclusion: 本研究提出了一种结合语义分割与实时单目SLAM的新方法，用于内窥镜下的中央气道阻塞（CAO）场景。该方法不仅提高了重建速度和质量，还为自主机器人干预提供了模块化框架。

Abstract: Central airway obstruction (CAO) is a life-threatening condition with
increasing incidence, caused by tumors in and outside of the airway.
Traditional treatment methods such as bronchoscopy and electrocautery can be
used to remove the tumor completely; however, these methods carry a high risk
of complications. Recent advances allow robotic interventions with lesser risk.
The combination of robot interventions with scene understanding and mapping
also opens up the possibilities for automation. We present a novel pipeline
that enables real-time, semantically informed 3D reconstructions of the central
airway using monocular endoscopic video.
  Our approach combines DROID-SLAM with a segmentation model trained to
identify obstructive tissues. The SLAM module reconstructs the 3D geometry of
the airway in real time, while the segmentation masks guide the annotation of
obstruction regions within the reconstructed point cloud. To validate our
pipeline, we evaluate the reconstruction quality using ex vivo models.
  Qualitative and quantitative results show high similarity between ground
truth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By
integrating segmentation directly into the SLAM workflow, our system produces
annotated 3D maps that highlight clinically relevant regions in real time.
High-speed capabilities of the pipeline allows quicker reconstructions compared
to previous work, reflecting the surgical scene more accurately.
  To the best of our knowledge, this is the first work to integrate semantic
segmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our
framework is modular and can generalize to other anatomies or procedures with
minimal changes, offering a promising step toward autonomous robotic
interventions.

</details>


### [138] [Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference](https://arxiv.org/abs/2509.13572)
*Ozan Karaali,Hossam Farag,Strahinja Dosen,Cedomir Stefanovic*

Main category: cs.RO

TL;DR: 研究评估了视觉语言模型在仿生抓取中的表现，发现其在物体识别方面表现良好，但在尺寸和抓取参数估计上仍有局限性。


<details>
  <summary>Details</summary>
Motivation: 探索利用视觉语言模型提升半自主假肢手的感知能力，简化传统需要多个模块（如物体检测、姿态估计和抓取规划）的复杂流程。

Method: 研究引入了端到端感知和抓取推理的统一基准，评估单一VLM执行传统需要复杂管道的任务的能力。通过34张常见物体的快照数据集，使用结构化JSON输出提示，分析了八种当代VLM的表现。

Result: 大多数模型在物体识别和形状识别方面表现优异，但在估计尺寸和推断最佳抓取参数（特别是手部旋转和开合度）方面准确性差异较大。

Conclusion: 该研究强调了视觉语言模型（VLMs）作为半自主控制仿生肢体的高级感知模块的当前能力和局限性，展示了其在有效假肢应用中的潜力。

Abstract: This study examines the potential of utilizing Vision Language Models (VLMs)
to improve the perceptual capabilities of semi-autonomous prosthetic hands. We
introduce a unified benchmark for end-to-end perception and grasp inference,
evaluating a single VLM to perform tasks that traditionally require complex
pipelines with separate modules for object detection, pose estimation, and
grasp planning. To establish the feasibility and current limitations of this
approach, we benchmark eight contemporary VLMs on their ability to perform a
unified task essential for bionic grasping. From a single static image, they
should (1) identify common objects and their key properties (name, shape,
orientation, and dimensions), and (2) infer appropriate grasp parameters (grasp
type, wrist rotation, hand aperture, and number of fingers). A corresponding
prompt requesting a structured JSON output was employed with a dataset of 34
snapshots of common objects. Key performance metrics, including accuracy for
categorical attributes (e.g., object name, shape) and errors in numerical
estimates (e.g., dimensions, hand aperture), along with latency and cost, were
analyzed. The results demonstrated that most models exhibited high performance
in object identification and shape recognition, while accuracy in estimating
dimensions and inferring optimal grasp parameters, particularly hand rotation
and aperture, varied more significantly. This work highlights the current
capabilities and limitations of VLMs as advanced perceptual modules for
semi-autonomous control of bionic limbs, demonstrating their potential for
effective prosthetic applications.

</details>


### [139] [Dense-Jump Flow Matching with Non-Uniform Time Scheduling for Robotic Policies: Mitigating Multi-Step Inference Degradation](https://arxiv.org/abs/2509.13574)
*Zidong Chen,Zihao Guo,Peng Wang,ThankGod Itua Egbe,Yan Lyu,Chenghao Qian*

Main category: cs.RO

TL;DR: 提出一种结合非均匀时间调度和密集跳跃集成计划的高效策略，解决了流匹配框架中泛化性不足和推理不稳定的问题，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 发现流匹配框架中泛化性早期出现并饱和，且增加推理时的欧拉积分步骤数会反直觉地降低策略性能，主要原因是均匀间隔的积分步骤过度采样晚期时间区域，导致动作受限，以及速度场在接近1时变得非Lipschitz，引发不稳定。

Method: 采用非均匀时间调度（如U形）训练策略，强调早期和晚期时间阶段以规范化策略训练，并在推理时使用密集跳跃集成计划，通过单步集成替换多步集成以避免不稳定区域。

Result: 在多样化的机器人任务中，新策略比现有技术基线实现了高达23.7%的性能提升。

Conclusion: 通过非均匀时间调度和密集跳跃集成计划，提出了一种高效的策略，在多样化的机器人任务中实现了比现有技术基线高达23.7%的性能提升。

Abstract: Flow matching has emerged as a competitive framework for learning
high-quality generative policies in robotics; however, we find that
generalisation arises and saturates early along the flow trajectory, in
accordance with recent findings in the literature. We further observe that
increasing the number of Euler integration steps during inference
counter-intuitively and universally degrades policy performance. We attribute
this to (i) additional, uniformly spaced integration steps oversample the
late-time region, thereby constraining actions towards the training
trajectories and reducing generalisation; and (ii) the learned velocity field
becoming non-Lipschitz as integration time approaches 1, causing instability.
To address these issues, we propose a novel policy that utilises non-uniform
time scheduling (e.g., U-shaped) during training, which emphasises both early
and late temporal stages to regularise policy training, and a dense-jump
integration schedule at inference, which uses a single-step integration to
replace the multi-step integration beyond a jump point, to avoid unstable areas
around 1. Essentially, our policy is an efficient one-step learner that still
pushes forward performance through multi-step integration, yielding up to 23.7%
performance gains over state-of-the-art baselines across diverse robotic tasks.

</details>


### [140] [TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement Learning](https://arxiv.org/abs/2509.13579)
*Momchil S. Tomov,Sang Uk Lee,Hansford Hendrago,Jinwook Huh,Teawon Han,Forbes Howington,Rafael da Silva,Gianmarco Bernasconi,Marc Heim,Samuel Findler,Xiaonan Ji,Alexander Boule,Michael Napoli,Kuo Chen,Jesse Miller,Boaz Floor,Yunqing Hu*

Main category: cs.RO

TL;DR: TreeIRL结合MCTS和IRL，实现了自动驾驶规划的最佳性能，并在实际道路测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶规划中的安全性与人类驾驶行为模拟的平衡问题。

Method: 使用MCTS寻找安全候选轨迹，并通过深度IRL评分函数选择最接近人类驾驶的轨迹。

Result: 在大型仿真和拉斯维加斯市区500+英里的实际驾驶测试中，TreeIRL在安全性、进展、舒适性和人类相似性方面表现最佳。

Conclusion: TreeIRL结合了蒙特卡洛树搜索（MCTS）和逆强化学习（IRL），在仿真和实际驾驶中实现了最先进的性能，为自动驾驶规划提供了一个可扩展的框架。

Abstract: We present TreeIRL, a novel planner for autonomous driving that combines
Monte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to
achieve state-of-the-art performance in simulation and in real-world driving.
The core idea is to use MCTS to find a promising set of safe candidate
trajectories and a deep IRL scoring function to select the most human-like
among them. We evaluate TreeIRL against both classical and state-of-the-art
planners in large-scale simulations and on 500+ miles of real-world autonomous
driving in the Las Vegas metropolitan area. Test scenarios include dense urban
traffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves
the best overall performance, striking a balance between safety, progress,
comfort, and human-likeness. To our knowledge, our work is the first
demonstration of MCTS-based planning on public roads and underscores the
importance of evaluating planners across a diverse set of metrics and in
real-world environments. TreeIRL is highly extensible and could be further
improved with reinforcement learning and imitation learning, providing a
framework for exploring different combinations of classical and learning-based
approaches to solve the planning bottleneck in autonomous driving.

</details>


### [141] [Object Pose Estimation through Dexterous Touch](https://arxiv.org/abs/2509.13591)
*Amir-Hossein Shahidzadeh,Jiyue Zhu,Kezhou Chen,Sha Yi,Cornelia Fermüller,Yiannis Aloimonos,Xiaolong Wang*

Main category: cs.RO

TL;DR: 通过双手机器人触觉探索和强化学习，实现了无需先验几何知识的物体姿态估计，适用于视觉受限场景。


<details>
  <summary>Details</summary>
Motivation: 在视觉数据有限或受光照、遮挡和外观影响的情况下，触觉传感器提供的局部接触信息难以直接重建物体姿态，因此需要主动探索的方法。

Method: 使用强化学习（RL）训练机器人手主动探索物体表面，收集触觉数据并生成3D点云，迭代优化物体形状和姿态。

Result: 实验表明，该方法能够主动探索物体表面并识别关键姿态特征，无需物体几何的先验知识。

Conclusion: 该方法通过双手机器人触觉探索实现了无需先验几何知识的物体姿态估计，展示了在视觉受限场景下的有效性。

Abstract: Robust object pose estimation is essential for manipulation and interaction
tasks in robotics, particularly in scenarios where visual data is limited or
sensitive to lighting, occlusions, and appearances. Tactile sensors often offer
limited and local contact information, making it challenging to reconstruct the
pose from partial data. Our approach uses sensorimotor exploration to actively
control a robot hand to interact with the object. We train with Reinforcement
Learning (RL) to explore and collect tactile data. The collected 3D point
clouds are used to iteratively refine the object's shape and pose. In our
setup, one hand holds the object steady while the other performs active
exploration. We show that our method can actively explore an object's surface
to identify critical pose features without prior knowledge of the object's
geometry. Supplementary material and more demonstrations will be provided at
https://amirshahid.github.io/BimanualTactilePose .

</details>


### [142] [Leg-Arm Coordinated Operation for Curtain Wall Installation](https://arxiv.org/abs/2509.13595)
*Xiao Liu,Weijun Wang,Tianlun Huang,Zhiyong Wang,Wei Feng*

Main category: cs.RO

TL;DR: A hexapod robot with a hierarchical control framework is developed to address challenges in curtain wall installation, proving effective in experiments.


<details>
  <summary>Details</summary>
Motivation: Traditional curtain wall installation methods face challenges like variable terrain, high labor intensity, low efficiency, and safety risks, prompting the need for robotic solutions.

Method: A hierarchical optimization-based whole-body control framework is designed for coordinated arm-leg planning, integrating leg motion with folding arm and serial-parallel manipulator operations.

Result: Experiments validate the control method's capability in performing curtain wall installation tasks.

Conclusion: The hierarchical optimization-based arm-leg coordination framework for the hexapod robot is effective in performing curtain wall installation tasks, providing a foundation for its application in complex construction environments.

Abstract: With the acceleration of urbanization, the number of high-rise buildings and
large public facilities is increasing, making curtain walls an essential
component of modern architecture with widespread applications. Traditional
curtain wall installation methods face challenges such as variable on-site
terrain, high labor intensity, low construction efficiency, and significant
safety risks. Large panels often require multiple workers to complete
installation. To address these issues, based on a hexapod curtain wall
installation robot, we design a hierarchical optimization-based whole-body
control framework for coordinated arm-leg planning tailored to three key tasks:
wall installation, ceiling installation, and floor laying. This framework
integrates the motion of the hexapod legs with the operation of the folding arm
and the serial-parallel manipulator. We conduct experiments on the hexapod
curtain wall installation robot to validate the proposed control method,
demonstrating its capability in performing curtain wall installation tasks. Our
results confirm the effectiveness of the hierarchical optimization-based
arm-leg coordination framework for the hexapod robot, laying the foundation for
its further application in complex construction site environments.

</details>


### [143] [Barometer-Aided Attitude Estimation](https://arxiv.org/abs/2509.13649)
*Méloné Nyoba Tchonkeu,Soulaimane Berkane,Tarek Hamel*

Main category: cs.RO

TL;DR: 论文提出了一种基于气压计的姿态估计方法，通过非线性观测器结合气压高度测量，解决了GNSS缺失环境下的姿态估计问题，具有稳健性和几何一致性。


<details>
  <summary>Details</summary>
Motivation: 在GNSS缺失或高动态环境中，仅依赖IMU的姿态估计不可靠，而辅助速度传感器（如GNSS、皮托管等）可能不可用或成本高昂。气压计作为一种轻量且有效的补充模态，为解决这一问题提供了可能。

Method: 设计了一个结合确定性Riccati观测器和互补滤波器的非线性观测器架构，利用气压高度测量推断垂直速度和姿态。

Result: 该方法在均匀可观测条件下实现了几乎全局渐近稳定性（AGAS），并保持了几何一致性。

Conclusion: 该论文提出了一种基于气压计的姿态估计架构，通过结合气压高度测量和非线性观测器，实现了在GNSS缺失或高动态环境下的稳健姿态估计。该方法在保证几何一致性的同时，具有几乎全局渐近稳定性（AGAS）。

Abstract: Accurate and robust attitude estimation is a central challenge for autonomous
vehicles operating in GNSS-denied or highly dynamic environments. In such
cases, Inertial Measurement Units (IMUs) alone are insufficient for reliable
tilt estimation due to the ambiguity between gravitational and inertial
accelerations. While auxiliary velocity sensors, such as GNSS, Pitot tubes,
Doppler radar, or visual odometry, are often used, they can be unavailable,
intermittent, or costly. This work introduces a barometer-aided attitude
estimation architecture that leverages barometric altitude measurements to
infer vertical velocity and attitude within a nonlinear observer on SO(3). The
design cascades a deterministic Riccati observer with a complementary filter,
ensuring Almost Global Asymptotic Stability (AGAS) under a uniform
observability condition while maintaining geometric consistency. The analysis
highlights barometer-aided estimation as a lightweight and effective
complementary modality.

</details>


### [144] [DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring](https://arxiv.org/abs/2509.13666)
*Zhenqi Wu,Abhinav Modi,Angelos Mavrogiannis,Kaustubh Joshi,Nikhil Chopra,Yiannis Aloimonos,Nare Karapetyan,Ioannis Rekleitis,Xiaomin Lin*

Main category: cs.RO

TL;DR: DREAM框架通过VLM引导自主水下监测，显著提升效率与覆盖范围，适用于牡蛎和沉船等场景。


<details>
  <summary>Details</summary>
Motivation: 海洋变暖和酸化加剧了温度敏感贝类（如牡蛎）的死亡风险，需要开发长期监测系统，但人工成本高且危险，因此机器人解决方案更优。

Method: 提出了DREAM框架，结合视觉语言模型（VLM）实现自主水下探索和监测。

Result: 在牡蛎监测任务中，DREAM比基线节省31.5%时间，覆盖更多牡蛎；在沉船场景中，实现100%覆盖且碰撞减少。

Conclusion: DREAM框架通过VLM引导的自主性，显著提升了水下机器人在长期栖息地监测中的效率和覆盖范围。

Abstract: The ocean is warming and acidifying, increasing the risk of mass mortality
events for temperature-sensitive shellfish such as oysters. This motivates the
development of long-term monitoring systems. However, human labor is costly and
long-duration underwater work is highly hazardous, thus favoring robotic
solutions as a safer and more efficient option. To enable underwater robots to
make real-time, environment-aware decisions without human intervention, we must
equip them with an intelligent "brain." This highlights the need for
persistent,wide-area, and low-cost benthic monitoring. To this end, we present
DREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term
underwater exploration and habitat monitoring. The results show that our
framework is highly efficient in finding and exploring target objects (e.g.,
oysters, shipwrecks) without prior location information. In the
oyster-monitoring task, our framework takes 31.5% less time than the previous
baseline with the same amount of oysters. Compared to the vanilla VLM, it uses
23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our
framework successfully explores and maps the wreck without collisions,
requiring 27.5% fewer steps than the vanilla model and achieving 100% coverage,
while the vanilla model achieves 60.23% average coverage in our shipwreck
environments.

</details>


### [145] [SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics](https://arxiv.org/abs/2509.13691)
*Songhao Huang,Yuwei Wu,Guangyao Shi,Gaurav S. Sukhatme,Vijay Kumar*

Main category: cs.RO

TL;DR: 利用大型语言模型自动生成PDDL领域，针对无人机任务，提出SPAR框架，显著减少手动设计的工作量，并通过多维度评估验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然PDDL是机器人规划中广泛采用的标准，但为监视、交付和检查等多样化应用手动设计领域既耗时又容易出错，这阻碍了其采用和实际部署。为了解决这些问题，本研究探索了利用LLMs自动生成PDDL领域的方法。

Method: 提出了SPAR框架，利用LLMs的生成能力，从自然语言输入自动生成有效、多样且语义准确的PDDL领域。首先引入了一个系统化制定和验证的UAV规划数据集，包含真实PDDL领域和相关问题，每个都配有详细的领域和动作描述。基于此数据集，设计了一个提示框架，从语言输入生成高质量PDDL领域。生成的领域通过语法验证、可执行性、可行性和可解释性进行评估。

Result: 生成的PDDL领域通过严格的语法验证、可执行性、可行性和可解释性评估，证明了LLMs在自动生成复杂规划领域的潜力。

Conclusion: 本研究展示了大型语言模型（LLMs）能显著加速复杂规划领域的创建，提供了可复现的数据集和评估流程，使无经验的专家也能将其用于实际任务，并推动空中机器人和自动规划的未来研究。

Abstract: We investigate the problem of automatic domain generation for the Planning
Domain Definition Language (PDDL) using Large Language Models (LLMs), with a
particular focus on unmanned aerial vehicle (UAV) tasks. Although PDDL is a
widely adopted standard in robotic planning, manually designing domains for
diverse applications such as surveillance, delivery, and inspection is
labor-intensive and error-prone, which hinders adoption and real-world
deployment. To address these challenges, we propose SPAR, a framework that
leverages the generative capabilities of LLMs to automatically produce valid,
diverse, and semantically accurate PDDL domains from natural language input. To
this end, we first introduce a systematically formulated and validated UAV
planning dataset, consisting of ground-truth PDDL domains and associated
problems, each paired with detailed domain and action descriptions. Building on
this dataset, we design a prompting framework that generates high-quality PDDL
domains from language input. The generated domains are evaluated through syntax
validation, executability, feasibility, and interpretability. Overall, this
work demonstrates that LLMs can substantially accelerate the creation of
complex planning domains, providing a reproducible dataset and evaluation
pipeline that enables application experts without prior experience to leverage
it for practical tasks and advance future research in aerial robotics and
automated planning.

</details>


### [146] [HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion](https://arxiv.org/abs/2509.13692)
*Yadan Zeng,Jiadong Zhou,Xiaohan Li,I-Ming Chen*

Main category: cs.RO

TL;DR: HGACNet通过分层图注意力和跨模态融合模块，结合对比损失，显著提升了点云补全效果，适用于机器人操作任务。


<details>
  <summary>Details</summary>
Motivation: 点云补全对机器人感知和下游任务至关重要，但自遮挡和传感器限制导致的不完整几何会显著影响推理和交互效果。

Method: HGACNet采用分层图注意力（HGA）编码器自适应选择关键局部点，并通过多尺度跨模态融合（MSCF）模块实现几何特征与视觉表征的细粒度对齐，同时引入对比损失（C-Loss）优化跨模态特征分布。

Result: 在ShapeNet-ViPC和YCB-Complete数据集上的实验表明，HGACNet实现了最先进的性能，并在实际机器人操作任务中表现优异。

Conclusion: HGACNet在点云补全任务中表现出色，通过分层图注意力编码器和多尺度跨模态融合模块，结合对比损失，显著提升了补全效果，并在实际机器人操作任务中展现了强大的适用性。

Abstract: Point cloud completion is essential for robotic perception, object
reconstruction and supporting downstream tasks like grasp planning, obstacle
avoidance, and manipulation. However, incomplete geometry caused by
self-occlusion and sensor limitations can significantly degrade downstream
reasoning and interaction. To address these challenges, we propose HGACNet, a
novel framework that reconstructs complete point clouds of individual objects
by hierarchically encoding 3D geometric features and fusing them with
image-guided priors from a single-view RGB image. At the core of our approach,
the Hierarchical Graph Attention (HGA) encoder adaptively selects critical
local points through graph attention-based downsampling and progressively
refines hierarchical geometric features to better capture structural continuity
and spatial relationships. To strengthen cross-modal interaction, we further
design a Multi-Scale Cross-Modal Fusion (MSCF) module that performs
attention-based feature alignment between hierarchical geometric features and
structured visual representations, enabling fine-grained semantic guidance for
completion. In addition, we proposed the contrastive loss (C-Loss) to
explicitly align the feature distributions across modalities, improving
completion fidelity under modality discrepancy. Finally, extensive experiments
conducted on both the ShapeNet-ViPC benchmark and the YCB-Complete dataset
confirm the effectiveness of HGACNet, demonstrating state-of-the-art
performance as well as strong applicability in real-world robotic manipulation
tasks.

</details>


### [147] [EZREAL: Enhancing Zero-Shot Outdoor Robot Navigation toward Distant Targets under Varying Visibility](https://arxiv.org/abs/2509.13720)
*Tianle Zeng,Jianwei Peng,Hanjing Ye,Guangcheng Chen,Senzi Luo,Hong Zhang*

Main category: cs.RO

TL;DR: 提出一种轻量级闭环系统，通过多尺度层级融合和可见性感知技术，显著提升远距离间歇性可见目标的零样本导航性能。


<details>
  <summary>Details</summary>
Motivation: 解决零样本物体导航（ZSON）在大型户外环境中的挑战，尤其是远距离目标因投影过小和间歇性可见性（部分或完全遮挡）导致的问题。

Method: 提出了一种基于对齐多尺度图像瓦片层级的统一系统，通过层级目标显著性融合和关键帧记忆等技术，实现目标方向和可见性的稳定检测。

Result: 在模拟和真实世界的户外试验中，系统能检测150米外的语义目标，航向保持正确概率达82.6%，任务成功率比现有最佳方法提高17.5%。

Conclusion: 该论文展示了一种轻量级闭环系统，通过层级目标显著性融合和可见性感知的航向维护，在模拟和真实世界的户外试验中，显著提高了零样本物体导航（ZSON）的成功率。

Abstract: Zero-shot object navigation (ZSON) in large-scale outdoor environments faces
many challenges; we specifically address a coupled one: long-range targets that
reduce to tiny projections and intermittent visibility due to partial or
complete occlusion. We present a unified, lightweight closed-loop system built
on an aligned multi-scale image tile hierarchy. Through hierarchical
target-saliency fusion, it summarizes localized semantic contrast into a stable
coarse-layer regional saliency that provides the target direction and indicates
target visibility. This regional saliency supports visibility-aware heading
maintenance through keyframe memory, saliency-weighted fusion of historical
headings, and active search during temporary invisibility. The system avoids
whole-image rescaling, enables deterministic bottom-up aggregation, supports
zero-shot navigation, and runs efficiently on a mobile robot. Across simulation
and real-world outdoor trials, the system detects semantic targets beyond 150m,
maintains a correct heading through visibility changes with 82.6% probability,
and improves overall task success by 17.5% compared with the SOTA methods,
demonstrating robust ZSON toward distant and intermittently observable targets.

</details>


### [148] [Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings](https://arxiv.org/abs/2509.13731)
*Jeongwoo Park,Seabin Lee,Changmin Park,Wonjong Lee,Changjoo Nam*

Main category: cs.RO

TL;DR: 提出一种结合基础模型和模拟训练的RL算法，用于自动化FFC插入任务，减少训练时间并避免物理风险。


<details>
  <summary>Details</summary>
Motivation: 解决柔性扁平电缆（FFC）插入过程中的高精度需求和非确定性变形问题，减少人工干预和物理风险。

Method: 利用基础模型（SAM2）和视觉语言模型（VLM）进行语义分割，通过模拟环境训练RL算法，避免直接物理操作。

Result: 实验表明该方法具备零样本能力，可直接应用于现实环境，无需微调。

Conclusion: 提出的RL算法结合基础模型和模拟到现实的方法，显著减少了训练时间并消除了物理损坏风险，实现了零样本部署。

Abstract: The industrial insertion of flexible flat cables (FFCs) into receptacles
presents a significant challenge owing to the need for submillimeter precision
when handling the deformable cables. In manufacturing processes, FFC insertion
with robotic manipulators often requires laborious human-guided trajectory
generation. While Reinforcement Learning (RL) offers a solution to automate
this task without modeling complex properties of FFCs, the nondeterminism
caused by the deformability of FFCs requires significant efforts and time on
training. Moreover, training directly in a real environment is dangerous as
industrial robots move fast and possess no safety measure. We propose an RL
algorithm for FFC insertion that leverages a foundation model-based real-to-sim
approach to reduce the training time and eliminate the risk of physical damages
to robots and surroundings. Training is done entirely in simulation, allowing
for random exploration without the risk of physical damages. Sim-to-real
transfer is achieved through semantic segmentation masks which leave only those
visual features relevant to the insertion tasks such as the geometric and
spatial information of the cables and receptacles. To enhance generality, we
use a foundation model, Segment Anything Model 2 (SAM2). To eleminate human
intervention, we employ a Vision-Language Model (VLM) to automate the initial
prompting of SAM2 to find segmentation masks. In the experiments, our method
exhibits zero-shot capabilities, which enable direct deployments to real
environments without fine-tuning.

</details>


### [149] [FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph](https://arxiv.org/abs/2509.13733)
*Xiaolin Zhou,Tingyang Xiao,Liu Liu,Yucheng Wang,Maiyue Chen,Xinrui Meng,Xinjie Wang,Wei Feng,Wei Sui,Zhizhong Su*

Main category: cs.RO

TL;DR: FSR-VLN结合HMSG和FSR，显著提升视觉语言导航的长距离推理性能，响应时间减少82%，并成功应用于人形机器人。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航方法在长距离空间推理中存在成功率低和推理延迟高的问题，限制了其在真实环境中的应用。

Method: 提出FSR-VLN系统，结合HMSG（分层多模态场景图）和FSR（快慢导航推理）。HMSG提供多模态地图表示，支持从粗粒度到细粒度的渐进检索；FSR先进行快速匹配筛选候选，再通过VLM驱动细化选择最终目标。

Result: 在四个室内数据集上评估，FSR-VLN在所有数据集上均达到最先进性能（RSR衡量），响应时间比VLM方法减少82%。系统成功集成到Unitree-G1人形机器人，支持自然语言交互和实时导航。

Conclusion: FSR-VLN通过结合HMSG和FSR，在视觉语言导航任务中实现了最先进的性能，显著提高了长距离空间推理的成功率并降低了响应时间。此外，该系统成功集成到人形机器人中，实现了自然语言交互和实时导航。

Abstract: Visual-Language Navigation (VLN) is a fundamental challenge in robotic
systems, with broad applications for the deployment of embodied agents in
real-world environments. Despite recent advances, existing approaches are
limited in long-range spatial reasoning, often exhibiting low success rates and
high inference latency, particularly in long-range navigation tasks. To address
these limitations, we propose FSR-VLN, a vision-language navigation system that
combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow
Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation
supporting progressive retrieval, from coarse room-level localization to
fine-grained goal view and object identification. Building on HMSG, FSR first
performs fast matching to efficiently select candidate rooms, views, and
objects, then applies VLM-driven refinement for final goal selection. We
evaluated FSR-VLN across four comprehensive indoor datasets collected by
humanoid robots, utilizing 87 instructions that encompass a diverse range of
object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all
datasets, measured by the retrieval success rate (RSR), while reducing the
response time by 82% compared to VLM-based methods on tour videos by activating
slow reasoning only when fast intuition fails. Furthermore, we integrate
FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1
humanoid robot, enabling natural language interaction and real-time navigation.

</details>


### [150] [Motion Adaptation Across Users and Tasks for Exoskeletons via Meta-Learning](https://arxiv.org/abs/2509.13736)
*Muyuan Ma,Long Cheng,Lijun Han,Xiuze Xia,Houcheng Li*

Main category: cs.RO

TL;DR: 提出了一种基于元模仿学习的框架，通过快速适应新任务和用户，显著提升外骨骼的辅助效果。


<details>
  <summary>Details</summary>
Motivation: 开发个性化和任务可泛化的辅助算法是可穿戴外骨骼增强人类力量和减少肌肉疲劳的关键挑战。

Method: 采用元模仿学习方法，利用任务特定的神经网络预测人类肘关节运动，并结合模型无关的元学习（MAML）框架，使网络能够快速适应新任务和未见过的用户。

Result: 实验结果表明，外骨骼显著降低了新用户执行未训练任务时的肌肉激活和代谢成本。

Conclusion: 所提出的框架有效提升了可穿戴外骨骼系统的任务泛化能力和用户适应性。

Abstract: Wearable exoskeletons can augment human strength and reduce muscle fatigue
during specific tasks. However, developing personalized and task-generalizable
assistance algorithms remains a critical challenge. To address this, a
meta-imitation learning approach is proposed. This approach leverages a
task-specific neural network to predict human elbow joint movements, enabling
effective assistance while enhancing generalization to new scenarios. To
accelerate data collection, full-body keypoint motions are extracted from
publicly available RGB video and motion-capture datasets across multiple tasks,
and subsequently retargeted in simulation. Elbow flexion trajectories generated
in simulation are then used to train the task-specific neural network within
the model-agnostic meta-learning (MAML) framework, which allows the network to
rapidly adapt to novel tasks and unseen users with only a few gradient updates.
The adapted network outputs personalized references tracked by a
gravity-compensated PD controller to ensure stable assistance. Experimental
results demonstrate that the exoskeleton significantly reduces both muscle
activation and metabolic cost for new users performing untrained tasks,
compared to performing without exoskeleton assistance. These findings suggest
that the proposed framework effectively improves task generalization and user
adaptability for wearable exoskeleton systems.

</details>


### [151] [Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force Control and Gait Control](https://arxiv.org/abs/2509.13737)
*Renjie Wang,Shangke Lyu,Donglin Wang*

Main category: cs.RO

TL;DR: 提出解耦框架，分离站立腿和摆动腿控制，提升强化学习在未知环境中的适应能力，有效缓解仿真到现实的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习在腿部运动控制中表现优异，但在分布外条件和仿真与现实的差距下性能下降，现有方法主要依赖领域随机化，效果有限。

Method: 采用解耦框架，分别控制站立腿和摆动腿，以增强在线适应能力和减少仿真与现实的差异。

Result: 仿真和真实世界实验表明，该方法在水平力干扰、不平坦地形、重载和偏置载荷以及仿真到现实的差距中表现有效。

Conclusion: 该论文提出了一种解耦框架，通过分离站立腿和摆动腿的控制，有效提升了强化学习在未知环境中的适应能力，并缓解了仿真到现实的差距问题。

Abstract: While Reinforcement Learning (RL) has achieved remarkable progress in legged
locomotion control, it often suffers from performance degradation in
out-of-distribution (OOD) conditions and discrepancies between the simulation
and the real environments. Instead of mainly relying on domain randomization
(DR) to best cover the real environments and thereby close the sim-to-real gap
and enhance robustness, this work proposes an emerging decoupled framework that
acquires fast online adaptation ability and mitigates the sim-to-real problems
in unfamiliar environments by isolating stance-leg control and swing-leg
control. Various simulation and real-world experiments demonstrate its
effectiveness against horizontal force disturbances, uneven terrains, heavy and
biased payloads, and sim-to-real gap.

</details>


### [152] [CDFlow: Generative Gradient Flows for Configuration Space Distance Fields via Neural ODEs](https://arxiv.org/abs/2509.13771)
*Mengzhu Li,Yunyu Zhou,He Ying,F. Richard Yu*

Main category: cs.RO

TL;DR: CDFlow利用Neural ODEs解决高自由度机器人CDF的单模态和稀疏采样问题，显著提升运动规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有CDF方法在高自由度机器人中存在单模态最近碰撞配置和稀疏采样边界的问题，导致梯度模糊和几何失真。

Method: 提出CDFlow框架，利用Neural ODEs学习配置空间中的连续流，并引入自适应细化采样策略生成高质量训练数据。

Result: 实验表明，CDFlow在高自由度运动规划任务中显著优于现有CDF方法，提升了规划效率和轨迹质量。

Conclusion: CDFlow通过Neural ODEs学习配置空间中的连续流，显著提高了高自由度机器人运动规划的效率和鲁棒性。

Abstract: Signed Distance Fields (SDFs) are a fundamental representation in robot
motion planning. Their configuration-space counterpart, the Configuration Space
Distance Field (CDF), directly encodes distances in joint space, offering a
unified representation for optimization and control. However, existing CDF
formulations face two major challenges in high-degree-of-freedom (DoF) robots:
(1) they effectively return only a single nearest collision configuration,
neglecting the multi-modal nature of minimal-distance collision configurations
and leading to gradient ambiguity; and (2) they rely on sparse sampling of the
collision boundary, which often fails to identify the true closest
configurations, producing oversmoothed approximations and geometric distortion
in high-dimensional spaces. We propose CDFlow, a novel framework that addresses
these limitations by learning a continuous flow in configuration space via
Neural Ordinary Differential Equations (Neural ODEs). We redefine the problem
from finding a single nearest point to modeling the distribution of
minimal-distance collision configurations. We also introduce an adaptive
refinement sampling strategy to generate high-fidelity training data for this
distribution. The resulting Neural ODE implicitly models this multi-modal
distribution and produces a smooth, consistent gradient field-derived as the
expected direction towards the distribution-that mitigates gradient ambiguity
and preserves sharp geometric features. Extensive experiments on high-DoF
motion planning tasks demonstrate that CDFlow significantly improves planning
efficiency, trajectory quality, and robustness compared to existing CDF-based
methods, enabling more robust and efficient planning for collision-aware robots
in complex environments.

</details>


### [153] [Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach](https://arxiv.org/abs/2509.13774)
*Piaopiao Jin,Qi Wang,Guokang Sun,Ziwen Cai,Pinjia He,Yangwei You*

Main category: cs.RO

TL;DR: 提出基于强化学习的人机交互双执行器微调框架，显著提升复杂任务的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 针对视觉-语言-动作（VLA）模型在复杂现实任务中的局限性，以及监督微调受限于数据质量的问题，探索强化学习作为替代方案。

Method: 采用人机交互的双执行器微调框架，结合主执行器实现多任务鲁棒性，并通过细化执行器进行潜在空间适应。引入轻量级的‘talk-and-tweak’方案，将人类纠正转化为语义基础的语言命令。

Result: 在真实世界多任务实验中，101分钟内实现100%成功率；长时任务中保持50%成功率（12次连续操作）。多机器人训练效率提升2倍。

Conclusion: 该论文提出的基于强化学习的人机交互双执行器微调框架在复杂任务中表现出色，尤其在多任务和长时任务中展现了高成功率和效率提升。

Abstract: Vision-language-action (VLA) models demonstrate strong generalization in
robotic manipulation but face challenges in complex, real-world tasks. While
supervised fine-tuning with demonstrations is constrained by data quality,
reinforcement learning (RL) offers a promising alternative. We propose a
human-in-the-loop dual-actor fine-tuning framework grounded in RL. The
framework integrates a primary actor for robust multi-task performance with a
refinement actor for latent-space adaptation. Beyond standard physical
interventions, we introduce a lightweight talk-and-tweak scheme that converts
human corrections into semantically grounded language commands, thereby
generating a new dataset for policy learning. In real-world multi-task
experiments, our approach achieves 100% success across three tasks within 101
minutes of online fine-tuning. For long-horizon tasks, it sustains a 50%
success rate over 12 consecutive operations. Furthermore, the framework scales
effectively to multi-robot training, achieving up to a 2 times improvement in
efficiency when using dual robots. The experiment videos are available at
https://sites.google.com/view/hil-daft/.

</details>


### [154] [Behavior Foundation Model for Humanoid Robots](https://arxiv.org/abs/2509.13780)
*Weishuai Zeng,Shunlin Lu,Kangning Yin,Xiaojie Niu,Minyue Dai,Jingbo Wang,Jiangmiao Pang*

Main category: cs.RO

TL;DR: BFM是一种预训练在大规模行为数据集上的生成模型，通过结合掩码在线蒸馏和CVAE，实现了跨多样化控制模式的灵活操作和高效行为获取，展示了强大的泛化和适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有WBC框架多为任务特定，依赖大量人工奖励工程，跨任务和技能的泛化能力有限，限制了其在复杂现实场景中的部署。BFM旨在通过捕捉广泛可重用的行为知识来克服这些限制。

Method: 提出行为基础模型（BFM），结合掩码在线蒸馏框架和条件变分自编码器（CVAE）来建模行为分布，支持跨多样化控制模式的灵活操作，并无需从头开始训练即可高效获取新行为。

Result: 在仿真和物理类人机器人平台上的大量实验表明，BFM能稳健地泛化到多样化WBC任务中，并快速适应新行为。

Conclusion: BFM作为一种生成模型，通过在大规模行为数据集上的预训练，为类人机器人提供了广泛且可重用的行为知识，展示了在多样化WBC任务中的强大泛化能力和快速适应新行为的能力，为通用类人机器人控制的基础模型迈出了有希望的一步。

Abstract: Whole-body control (WBC) of humanoid robots has witnessed remarkable progress
in skill versatility, enabling a wide range of applications such as locomotion,
teleoperation, and motion tracking. Despite these achievements, existing WBC
frameworks remain largely task-specific, relying heavily on labor-intensive
reward engineering and demonstrating limited generalization across tasks and
skills. These limitations hinder their response to arbitrary control modes and
restrict their deployment in complex, real-world scenarios. To address these
challenges, we revisit existing WBC systems and identify a shared objective
across diverse tasks: the generation of appropriate behaviors that guide the
robot toward desired goal states. Building on this insight, we propose the
Behavior Foundation Model (BFM), a generative model pretrained on large-scale
behavioral datasets to capture broad, reusable behavioral knowledge for
humanoid robots. BFM integrates a masked online distillation framework with a
Conditional Variational Autoencoder (CVAE) to model behavioral distributions,
thereby enabling flexible operation across diverse control modes and efficient
acquisition of novel behaviors without retraining from scratch. Extensive
experiments in both simulation and on a physical humanoid platform demonstrate
that BFM generalizes robustly across diverse WBC tasks while rapidly adapting
to new behaviors. These results establish BFM as a promising step toward a
foundation model for general-purpose humanoid control.

</details>


### [155] [Shell-Type Soft Jig for Holding Objects during Disassembly](https://arxiv.org/abs/2509.13802)
*Takuya Kiyokawa,Ryunosuke Takebayashi,Kensuke Harada*

Main category: cs.RO

TL;DR: 开发了一种新型软夹具，适用于机器人拆卸，减少对高精度工具的需求，实验验证了其可行性和局限性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统夹具在机器人拆卸过程中可能导致的组件损坏以及对高精度识别、规划和控制的依赖性，本研究旨在开发一种更灵活、适应性更强的夹具。

Method: 采用气球式夹持机制，设计了一种外壳型软夹具，能够安全且通用地夹持物体，适应多种形状，并实现软固定，减少了对专用夹具设计、高精度感知、精确抓取和精细轨迹规划的需求。

Result: 实验结果表明，所提出的软夹具在性能上优于传统夹具（如虎钳）和另一种软夹具（受堵塞夹持器启发），在十种不同物体上的测试展示了其代表性和局限性。

Conclusion: 本研究提出了一种适用于机器人拆卸的柔性夹具，通过实验验证了其在实际应用中的可行性，并明确了其局限性和未来改进方向。

Abstract: This study addresses a flexible holding tool for robotic disassembly. We
propose a shell-type soft jig that securely and universally holds objects,
mitigating the risk of component damage and adapting to diverse shapes while
enabling soft fixation that is robust to recognition, planning, and control
errors. The balloon-based holding mechanism ensures proper alignment and stable
holding performance, thereby reducing the need for dedicated jig design, highly
accurate perception, precise grasping, and finely tuned trajectory planning
that are typically required with conventional fixtures. Our experimental
results demonstrate the practical feasibility of the proposed jig through
performance comparisons with a vise and a jamming-gripper-inspired soft jig.
Tests on ten different objects further showed representative successes and
failures, clarifying the jig's limitations and outlook.

</details>


### [156] [Soft Regrasping Tool Inspired by Jamming Gripper](https://arxiv.org/abs/2509.13815)
*Takuya Kiyokawa,Zhengtao Hu,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: A soft jig using jamming transition enables adaptable, accurate regrasping for robotic assembly, achieving high success rates but with some geometric limitations.


<details>
  <summary>Details</summary>
Motivation: To overcome the lack of adaptability and dedicated designs required by conventional rigid fixtures in robotic assembly, reducing pose uncertainty.

Method: The method involves using a soft jig that can be deformed to accommodate diverse object geometries by pressing a triangular-pyramid-shaped tool into the membrane and evacuating the enclosed air to form a stable cavity. The stamping depth is optimized to balance placement stability and gripper accessibility.

Result: Drop experiments on ten mechanical parts achieved placement success rates exceeding 80% for most objects and above 90% for cylindrical ones, with failures mainly due to geometric constraints and membrane properties.

Conclusion: The proposed soft jig, inspired by jamming transition, enables general-purpose, accurate, and repeatable regrasping, offering a practical alternative to rigid fixtures in assembly automation, though it has some limitations.

Abstract: Regrasping on fixtures is a promising approach to reduce pose uncertainty in
robotic assembly, but conventional rigid fixtures lack adaptability and require
dedicated designs for each part. To overcome this limitation, we propose a soft
jig inspired by the jamming transition phenomenon, which can be continuously
deformed to accommodate diverse object geometries. By pressing a
triangular-pyramid-shaped tool into the membrane and evacuating the enclosed
air, a stable cavity is formed as a placement space. We further optimize the
stamping depth to balance placement stability and gripper accessibility. In
soft-jig-based regrasping, the key challenge lies in optimizing the cavity size
to achieve precise dropping; once the part is reliably placed, subsequent
grasping can be performed with reduced uncertainty. Accordingly, we conducted
drop experiments on ten mechanical parts of varying shapes, which achieved
placement success rates exceeding 80% for most objects and above 90% for
cylindrical ones, while failures were mainly caused by geometric constraints
and membrane properties. These results demonstrate that the proposed jig
enables general-purpose, accurate, and repeatable regrasping, while also
clarifying its current limitations and future potential as a practical
alternative to rigid fixtures in assembly automation.

</details>


### [157] [Agile in the Face of Delay: Asynchronous End-to-End Learning for Real-World Aerial Navigation](https://arxiv.org/abs/2509.13816)
*Yude Li,Zhexuan Zhou,Huizhe Li,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: 提出异步强化学习框架解决自主飞行器高频率控制与低频率感知冲突，通过TEM和两阶段课程学习实现高效训练，实际验证中表现鲁棒且敏捷。


<details>
  <summary>Details</summary>
Motivation: 现代端到端导航系统面临高频率控制需求与低频率感知流之间的冲突，导致控制速率受限。为解决这一问题，研究提出了异步框架以提升自主飞行器在复杂环境中的导航能力。

Method: 异步强化学习框架，将感知与控制解耦，允许高频率策略基于最新IMU状态行动，同时异步整合感知特征。采用Temporal Encoding Module（TEM）管理数据延迟，并通过两阶段课程学习确保训练稳定性和效率。

Result: 在广泛模拟中验证后，该方法通过零样本模拟到现实转移成功部署，维持100Hz控制频率，并在杂乱现实环境中展示了鲁棒且敏捷的导航性能。

Conclusion: 提出的异步强化学习框架成功解决了自主飞行器在复杂环境中高频率控制与低频率感知之间的冲突，通过理论支持的Temporal Encoding Module和两阶段课程学习，实现了稳定高效的训练，并在实际环境中验证了其鲁棒性和敏捷性。

Abstract: Robust autonomous navigation for Autonomous Aerial Vehicles (AAVs) in complex
environments is a critical capability. However, modern end-to-end navigation
faces a key challenge: the high-frequency control loop needed for agile flight
conflicts with low-frequency perception streams, which are limited by sensor
update rates and significant computational cost. This mismatch forces
conventional synchronous models into undesirably low control rates. To resolve
this, we propose an asynchronous reinforcement learning framework that
decouples perception and control, enabling a high-frequency policy to act on
the latest IMU state for immediate reactivity, while incorporating perception
features asynchronously. To manage the resulting data staleness, we introduce a
theoretically-grounded Temporal Encoding Module (TEM) that explicitly
conditions the policy on perception delays, a strategy complemented by a
two-stage curriculum to ensure stable and efficient training. Validated in
extensive simulations, our method was successfully deployed in zero-shot
sim-to-real transfer on an onboard NUC, where it sustains a 100~Hz control rate
and demonstrates robust, agile navigation in cluttered real-world environments.
Our source code will be released for community reference.

</details>


### [158] [How Fly Neural Perception Mechanisms Enhance Visuomotor Control of Micro Robots](https://arxiv.org/abs/2509.13827)
*Renyuan Liu,Haoting Zhou,Chuankai Fang,Qinbing Fu*

Main category: cs.RO

TL;DR: 受苍蝇视觉神经系统启发，提出低功耗视觉运动控制策略，应用于微型机器人，实现高效碰撞避障，并展示群体行为研究潜力。


<details>
  <summary>Details</summary>
Motivation: 受苍蝇视觉神经系统的启发，为解决自主机器人在复杂环境中敏捷性与计算成本之间的权衡问题，探索昆虫智能的低功耗高效计算框架。

Method: 提出了一种基于LPLC2神经元模型的注意力驱动视觉运动控制策略，优化为70KB内存以适应微型机器人Colias的计算限制，并整合多注意力机制模拟LPLC2的分布式响应特性。

Result: 与蝗虫启发的碰撞检测模型相比，苍蝇启发的模型在碰撞检测中达到96.1%的成功率，并表现出更适应和优雅的避障动作。

Conclusion: 本研究通过模拟苍蝇视觉神经系统的LPLC2神经元，提出了一种低功耗、高效计算的视觉运动控制策略，成功应用于微型机器人Colias，实现了96.1%的碰撞检测成功率，并展示了其在群体行为研究中的潜力。

Abstract: Anyone who has tried to swat a fly has likely been frustrated by its
remarkable agility.This ability stems from its visual neural perception system,
particularly the collision-selective neurons within its small brain.For
autonomous robots operating in complex and unfamiliar environments, achieving
similar agility is highly desirable but often constrained by the trade-off
between computational cost and performance.In this context, insect-inspired
intelligence offers a parsimonious route to low-power, computationally
efficient frameworks.In this paper, we propose an attention-driven visuomotor
control strategy inspired by a specific class of fly visual projection
neurons-the lobula plate/lobula column type-2 (LPLC2)-and their associated
escape behaviors.To our knowledge, this represents the first embodiment of an
LPLC2 neural model in the embedded vision of a physical mobile robot, enabling
collision perception and reactive evasion.The model was simplified and
optimized at 70KB in memory to suit the computational constraints of a
vision-based micro robot, the Colias, while preserving key neural perception
mechanisms.We further incorporated multi-attention mechanisms to emulate the
distributed nature of LPLC2 responses, allowing the robot to detect and react
to approaching targets both rapidly and selectively.We systematically evaluated
the proposed method against a state-of-the-art locust-inspired collision
detection model.Results showed that the fly-inspired visuomotor model achieved
comparable robustness, at success rate of 96.1% in collision detection while
producing more adaptive and elegant evasive maneuvers.Beyond demonstrating an
effective collision-avoidance strategy, this work highlights the potential of
fly-inspired neural models for advancing research into collective behaviors in
insect intelligence.

</details>


### [159] [UltraHiT: A Hierarchical Transformer Architecture for Generalizable Internal Carotid Artery Robotic Ultrasonography](https://arxiv.org/abs/2509.13832)
*Teng Wang,Haojun Jiang,Yuxuan Wang,Zhenguo Sun,Xiangjie Yan,Xiang Li,Gao Huang*

Main category: cs.RO

TL;DR: 提出 UltraHiT 方法，通过分层 Transformer 架构解决 ICA 扫描难题，成功率达 95%。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于将个体血管结构视为标准血管模型的形态变异，从而解决 ICA 扫描中的复杂性问题。

Method: 提出了基于分层 Transformer 的决策架构 UltraHiT，结合高层变异评估与低层行动决策。高层模块识别变异并切换至两个低层模块：自适应校正器或标准执行器。两者均采用因果 Transformer 实现，基于历史扫描序列生成预测。

Result: 在未见过的个体上，UltraHiT 成功定位 ICA 的成功率达到 95%，优于基线方法。

Conclusion: UltraHiT 方法在定位 ICA 方面取得了 95% 的成功率，显著优于基线方法，证明了其有效性。

Abstract: Carotid ultrasound is crucial for the assessment of cerebrovascular health,
particularly the internal carotid artery (ICA). While previous research has
explored automating carotid ultrasound, none has tackled the challenging ICA.
This is primarily due to its deep location, tortuous course, and significant
individual variations, which greatly increase scanning complexity. To address
this, we propose a Hierarchical Transformer-based decision architecture, namely
UltraHiT, that integrates high-level variation assessment with low-level action
decision. Our motivation stems from conceptualizing individual vascular
structures as morphological variations derived from a standard vascular model.
The high-level module identifies variation and switches between two low-level
modules: an adaptive corrector for variations, or a standard executor for
normal cases. Specifically, both the high-level module and the adaptive
corrector are implemented as causal transformers that generate predictions
based on the historical scanning sequence. To ensure generalizability, we
collected the first large-scale ICA scanning dataset comprising 164
trajectories and 72K samples from 28 subjects of both genders. Based on the
above innovations, our approach achieves a 95% success rate in locating the ICA
on unseen individuals, outperforming baselines and demonstrating its
effectiveness. Our code will be released after acceptance.

</details>


### [160] [Track Any Motions under Any Disturbances](https://arxiv.org/abs/2509.13833)
*Zhikai Zhang,Jun Guo,Chao Chen,Jilong Wang,Chenghuai Lin,Yunrui Lian,Han Xue,Zhenrong Wang,Maoqi Liu,Huaping Liu,He Wang,Li Yi*

Main category: cs.RO

TL;DR: Any2Track 是一个两阶段 RL 框架，用于在现实世界中跟踪各种动作并应对多种干扰。它包含通用的动作跟踪器和适应模块，成功实现了零样本的 sim2real 迁移。


<details>
  <summary>Details</summary>
Motivation: 为了在现实世界中稳定地跟踪多样化、高度动态且接触丰富的动作，并应对各种动态干扰（如地形、外力和物理属性变化），提出了 Any2Track。

Method: Any2Track 是一个两阶段的 RL 框架，包含 AnyTracker 和 AnyAdapter 两个关键组件。AnyTracker 是一个通用的动作跟踪器，能够通过单一策略跟踪各种动作；AnyAdapter 是一个基于历史的适应模块，赋予跟踪器在线动态适应能力。

Result: Any2Track 在 Unitree G1 硬件上成功实现了零样本的 sim2real 迁移，并在多种现实世界干扰下表现出色。

Conclusion: Any2Track 成功地在 Unitree G1 硬件上实现了零样本的 sim2real 迁移，并在多种现实世界干扰下表现出色，能够跟踪各种动作。

Abstract: A foundational humanoid motion tracker is expected to be able to track
diverse, highly dynamic, and contact-rich motions. More importantly, it needs
to operate stably in real-world scenarios against various dynamics
disturbances, including terrains, external forces, and physical property
changes for general practical use. To achieve this goal, we propose Any2Track
(Track Any motions under Any disturbances), a two-stage RL framework to track
various motions under multiple disturbances in the real world. Any2Track
reformulates dynamics adaptability as an additional capability on top of basic
action execution and consists of two key components: AnyTracker and AnyAdapter.
AnyTracker is a general motion tracker with a series of careful designs to
track various motions within a single policy. AnyAdapter is a history-informed
adaptation module that endows the tracker with online dynamics adaptability to
overcome the sim2real gap and multiple real-world disturbances. We deploy
Any2Track on Unitree G1 hardware and achieve a successful sim2real transfer in
a zero-shot manner. Any2Track performs exceptionally well in tracking various
motions under multiple real-world disturbances.

</details>


### [161] [Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models](https://arxiv.org/abs/2509.13839)
*Motonari Kambara,Komei Sugiura*

Main category: cs.RO

TL;DR: 本文提出了一种预测开放词汇对象操作任务未来成功率的模型，通过多级轨迹融合模块提升预测性能，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常在操作完成后才确定成功或失败，难以预防潜在危险且依赖失败触发重新规划，降低了对象操作序列的效率。

Method: 提出了一种多级轨迹融合模块，结合最先进的深度状态空间模型和变压器编码器并行工作，以捕捉末端执行器轨迹的多层次时间序列自相关性。

Result: 实验结果表明，所提出的方法在预测开放词汇对象操作任务的未来成功率方面优于现有方法，包括基础模型。

Conclusion: 本文提出了一种预测开放词汇对象操作任务未来成功率的模型，通过多级轨迹融合模块有效捕捉末端执行器轨迹的多层次时间序列自相关性，实验结果表明该方法优于现有方法。

Abstract: In this work, we address the problem of predicting the future success of
open-vocabulary object manipulation tasks. Conventional approaches typically
determine success or failure after the action has been carried out. However,
they make it difficult to prevent potential hazards and rely on failures to
trigger replanning, thereby reducing the efficiency of object manipulation
sequences. To overcome these challenges, we propose a model, which predicts the
alignment between a pre-manipulation egocentric image with the planned
trajectory and a given natural language instruction. We introduce a Multi-Level
Trajectory Fusion module, which employs a state-of-the-art deep state-space
model and a transformer encoder in parallel to capture multi-level time-series
self-correlation within the end effector trajectory. Our experimental results
indicate that the proposed method outperformed existing methods, including
foundation models.

</details>


### [162] [InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap](https://arxiv.org/abs/2509.13857)
*Nguyen Hoang Khoi Tran,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.RO

TL;DR: InterKey是一种利用道路交叉口作为地标的跨模态框架，通过联合编码点云和OSM数据实现全球定位，在KITTI数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 在GNSS信号受限的环境中，如城市峡谷和隧道，可靠的全球定位对自动驾驶车辆至关重要。虽然高精度地图提供了准确的先验信息，但其数据收集、构建和维护成本限制了可扩展性。OpenStreetMap（OSM）提供了免费且全球可用的替代方案，但其粗略的抽象给与传感器数据的匹配带来了挑战。

Method: 该方法构建紧凑的二进制描述符，通过联合编码点云和OSM中的道路和建筑物印记，并引入差异缓解、方向确定和面积均衡采样策略，以实现稳健的跨模态匹配。

Result: 在KITTI数据集上的实验表明，InterKey实现了最先进的准确性，大幅优于最近的基线方法。该框架适用于能够生成密集结构点云的传感器。

Conclusion: InterKey框架通过利用道路交叉口作为显著地标，提出了一种可扩展且经济高效的解决方案，适用于需要可靠全球定位的场景，特别是在GNSS信号受限的环境中。

Abstract: Reliable global localization is critical for autonomous vehicles, especially
in environments where GNSS is degraded or unavailable, such as urban canyons
and tunnels. Although high-definition (HD) maps provide accurate priors, the
cost of data collection, map construction, and maintenance limits scalability.
OpenStreetMap (OSM) offers a free and globally available alternative, but its
coarse abstraction poses challenges for matching with sensor data. We propose
InterKey, a cross-modal framework that leverages road intersections as
distinctive landmarks for global localization. Our method constructs compact
binary descriptors by jointly encoding road and building imprints from point
clouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,
orientation determination, and area-equalized sampling strategies, enabling
robust cross-modal matching. Experiments on the KITTI dataset demonstrate that
InterKey achieves state-of-the-art accuracy, outperforming recent baselines by
a large margin. The framework generalizes to sensors that can produce dense
structural point clouds, offering a scalable and cost-effective solution for
robust vehicle localization.

</details>


### [163] [Using Petri Nets for Context-Adaptive Robot Explanations](https://arxiv.org/abs/2509.13861)
*Görkem Kılınç Soylu,Neziha Akalin,Maria Riveiro*

Main category: cs.RO

TL;DR: 使用Petri网建模上下文信息，支持机器人自适应解释，验证了其在人机交互中的稳健性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 在人机交互中，机器人需要通过自然透明的沟通方式建立信任，这要求机器人能根据上下文调整其沟通策略。

Method: 使用Petri网（PNs）建模上下文信息，以支持机器人自适应解释。PNs提供了形式化、图形化的方法来表示并发动作、因果依赖关系和系统状态。

Result: 模型分析确认了关键属性，包括无死锁性、上下文敏感可达性、有界性和活性，验证了PNs的稳健性和灵活性。

Conclusion: PNs被证明是设计和验证人机交互中上下文自适应解释的稳健且灵活的方法。

Abstract: In human-robot interaction, robots must communicate in a natural and
transparent manner to foster trust, which requires adapting their communication
to the context. In this paper, we propose using Petri nets (PNs) to model
contextual information for adaptive robot explanations. PNs provide a formal,
graphical method for representing concurrent actions, causal dependencies, and
system states, making them suitable for analyzing dynamic interactions between
humans and robots. We demonstrate this approach through a scenario involving a
robot that provides explanations based on contextual cues such as user
attention and presence. Model analysis confirms key properties, including
deadlock-freeness, context-sensitive reachability, boundedness, and liveness,
showing the robustness and flexibility of PNs for designing and verifying
context-adaptive explanations in human-robot interactions.

</details>


### [164] [Repulsive Trajectory Modification and Conflict Resolution for Efficient Multi-Manipulator Motion Planning](https://arxiv.org/abs/2509.13882)
*Junhwa Hong,Beomjoon Lee,Woojin Lee,Changjoo Nam*

Main category: cs.RO

TL;DR: 提出一种改进CBS的高效多机械臂运动规划方法，通过人工势场减少后续冲突，实验证明其高效性。


<details>
  <summary>Details</summary>
Motivation: 多机械臂系统的运动规划因复合配置空间的高维度而计算复杂，传统CBS方法在解决冲突时会产生指数级增长的约束树。

Method: 基于CBS的两级结构，低层规划器采用人工势场的梯度下降方法，生成排斥力引导冲突机械臂轨迹。同时开发了一种在特定条件下直接求解冲突自由解的策略。

Result: 实验表明，该方法减少了约束树的扩展节点数，提高了成功率，并比Enhanced CBS等先进算法更快找到解。

Conclusion: 该方法通过改进CBS的两级结构，采用人工势场的梯度下降方法减少后续冲突，并在特定条件下一步求解冲突自由解，显著提高了多机械臂运动规划的效率。

Abstract: We propose an efficient motion planning method designed to efficiently find
collision-free trajectories for multiple manipulators. While multi-manipulator
systems offer significant advantages, coordinating their motions is
computationally challenging owing to the high dimensionality of their composite
configuration space. Conflict-Based Search (CBS) addresses this by decoupling
motion planning, but suffers from subsequent conflicts incurred by resolving
existing conflicts, leading to an exponentially growing constraint tree of CBS.
Our proposed method is based on repulsive trajectory modification within the
two-level structure of CBS. Unlike conventional CBS variants, the low-level
planner applies a gradient descent approach using an Artificial Potential
Field. This field generates repulsive forces that guide the trajectory of the
conflicting manipulator away from those of other robots. As a result,
subsequent conflicts are less likely to occur. Additionally, we develop a
strategy that, under a specific condition, directly attempts to find a
conflict-free solution in a single step without growing the constraint tree.
Through extensive tests including physical robot experiments, we demonstrate
that our method consistently reduces the number of expanded nodes in the
constraint tree, achieves a higher success rate, and finds a solution faster
compared to Enhanced CBS and other state-of-the-art algorithms.

</details>


### [165] [PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models](https://arxiv.org/abs/2509.13903)
*Artem Lykov,Jeffrin Sam,Hung Khang Nguyen,Vladislav Kozlovskiy,Yara Mahmoud,Valerii Serpiva,Miguel Altamirano Cabrera,Mikhail Konenkov,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: PhysicalAgent是一个结合迭代推理和视频生成的机器人操作框架，通过迭代校正显著提高成功率，展示了视频生成推理在机器人操作中的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了开发一种能够从执行错误中稳健恢复的机器人操作框架，并探索视频生成推理在通用机器人操作中的潜力。

Method: PhysicalAgent结合了迭代推理、基于扩散的视频生成和闭环执行。给定文本指令，该方法生成候选轨迹的短视频演示，在机器人上执行它们，并根据失败情况迭代重新规划。

Result: PhysicalAgent在多个感知模态和机器人平台上显著优于现有方法，首次尝试成功率较低（20-30%），但通过迭代校正，整体成功率提高到80%。

Conclusion: PhysicalAgent框架展示了基于视频的生成推理在通用机器人操作中的潜力，并强调了迭代执行对于从初始失败中恢复的重要性。该框架为可扩展、适应性强且稳健的机器人控制铺平了道路。

Abstract: We introduce PhysicalAgent, an agentic framework for robotic manipulation
that integrates iterative reasoning, diffusion-based video generation, and
closed-loop execution. Given a textual instruction, our method generates short
video demonstrations of candidate trajectories, executes them on the robot, and
iteratively re-plans in response to failures. This approach enables robust
recovery from execution errors. We evaluate PhysicalAgent across multiple
perceptual modalities (egocentric, third-person, and simulated) and robotic
embodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing
against state-of-the-art task-specific baselines. Experiments demonstrate that
our method consistently outperforms prior approaches, achieving up to 83%
success on human-familiar tasks. Physical trials reveal that first-attempt
success is limited (20-30%), yet iterative correction increases overall success
to 80% across platforms. These results highlight the potential of video-based
generative reasoning for general-purpose robotic manipulation and underscore
the importance of iterative execution for recovering from initial failures. Our
framework paves the way for scalable, adaptable, and robust robot control.

</details>


### [166] [MAP: End-to-End Autonomous Driving with Map-Assisted Planning](https://arxiv.org/abs/2509.13926)
*Huilin Yin,Yiming Kan,Daniel Watzenig*

Main category: cs.RO

TL;DR: MAP是一种新型地图辅助端到端轨迹规划框架，通过显式整合地图特征和自车状态，显著提升了规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用在线地图模块的潜力，限制了其在轨迹规划中的增强作用。

Method: MAP框架通过三个模块（Plan-enhancing Online Mapping、Ego-status-guided Planning和Weight Adapter）显式整合分割地图特征和当前自车状态。

Result: 在DAIR-V2X-seq-SPD数据集上，MAP实现了L2位移误差减少16.6%、脱轨率减少56.2%、总分提升44.5%，并在MEIS Workshop @CVPR2025的挑战赛中排名第一。

Conclusion: 该论文通过提出MAP框架，展示了显式利用语义地图特征在轨迹规划中的有效性，并为端到端自动驾驶系统的结构设计提供了新方向。

Abstract: In recent years, end-to-end autonomous driving has attracted increasing
attention for its ability to jointly model perception, prediction, and planning
within a unified framework. However, most existing approaches underutilize the
online mapping module, leaving its potential to enhance trajectory planning
largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel
map-assisted end-to-end trajectory planning framework. MAP explicitly
integrates segmentation-based map features and the current ego status through a
Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and
a Weight Adapter based on current ego status. Experiments conducted on the
DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%
reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a
44.5% improvement in overall score compared to the UniV2X baseline, even
without post-processing. Furthermore, it achieves top ranking in Track 2 of the
End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS
Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of
overall score. These results highlight the effectiveness of explicitly
leveraging semantic map features in planning and suggest new directions for
improving structure design in end-to-end autonomous driving systems. Our code
is available at https://gitee.com/kymkym/map.git

</details>


### [167] [Reinforcement Learning for Autonomous Point-to-Point UAV Navigation](https://arxiv.org/abs/2509.13943)
*Salim Oyinlola,Nitesh Subedi,Soumik Sarkar*

Main category: cs.RO

TL;DR: 该论文提出了一种基于强化学习的方法，使无人机能够自主导航，实际测试证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在自动化检查、配送和导航任务中的广泛应用，需要可靠的自主体来实现无人干预的自主导航。

Method: 采用强化学习（RL）方法，通过自定义奖励函数，鼓励无人机高效到达目标点，同时避免碰撞和不安全行为。控制系统结合了ROS和Gym兼容的训练环境。

Result: 经过训练的策略在实际无人机平台上部署并评估，结果显示无人机能够在最少人工监督下成功完成自主导航任务。

Conclusion: 该论文展示了基于强化学习的控制方法在实际无人机点对点导航中的可行性，证明了该方法在现实场景中的有效性。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in automated
inspection, delivery, and navigation tasks that require reliable autonomy. This
project develops a reinforcement learning (RL) approach to enable a single UAV
to autonomously navigate between predefined points without manual intervention.
The drone learns navigation policies through trial-and-error interaction, using
a custom reward function that encourages goal-reaching efficiency while
penalizing collisions and unsafe behavior. The control system integrates ROS
with a Gym-compatible training environment, enabling flexible deployment and
testing. After training, the learned policy is deployed on a real UAV platform
and evaluated under practical conditions. Results show that the UAV can
successfully perform autonomous navigation with minimal human oversight,
demonstrating the viability of RL-based control for point-to-point drone
operations in real-world scenarios.

</details>


### [168] [The Influence of Facial Features on the Perceived Trustworthiness of a Social Robot](https://arxiv.org/abs/2509.13948)
*Benedict Barrow,Roger K. Moore*

Main category: cs.RO

TL;DR: 研究通过调整机器人面部特征，发现眼睛形状和大小显著影响人类对其的信任感知。


<details>
  <summary>Details</summary>
Motivation: 理解影响人类对机器人信任的因素，以优化社交机器人的设计。

Method: 通过操纵Furhat机器人的后投影面部特征，研究‘娃娃脸’对信任感知的影响。

Result: 眼睛形状和大小对信任感知有显著影响。

Conclusion: 研究证实了社交机器人面部特征（尤其是眼睛形状和大小）对人类信任感知的显著影响，为设计优化人机交互提供了重要参考。

Abstract: Trust and the perception of trustworthiness play an important role in
decision-making and our behaviour towards others, and this is true not only of
human-human interactions but also of human-robot interactions. While
significant advances have been made in recent years in the field of social
robotics, there is still some way to go before we fully understand the factors
that influence human trust in robots. This paper presents the results of a
study into the first impressions created by a social robot's facial features,
based on the hypothesis that a `babyface' engenders trust. By manipulating the
back-projected face of a Furhat robot, the study confirms that eye shape and
size have a significant impact on the perception of trustworthiness. The work
thus contributes to an understanding of the design choices that need to be made
when developing social robots so as to optimise the effectiveness of
human-robot interaction.

</details>


### [169] [SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks](https://arxiv.org/abs/2509.13949)
*Jannick Stranghöner,Philipp Hartmann,Marco Braun,Sebastian Wrede,Klaus Neumann*

Main category: cs.RO

TL;DR: SHaRe-RL是一个强化学习框架，通过整合先验知识实现高效安全的在线学习，适用于工业装配任务，实验证明其可靠性和实用性。


<details>
  <summary>Details</summary>
Motivation: 高混合低量（HMLV）工业装配需要高精度、安全性和可靠性，同时保持对产品变化和环境不确定性的灵活性，而现有机器人系统难以满足这些需求。

Method: SHaRe-RL框架通过（i）将技能结构化为操作基元，（ii）整合人类演示和在线修正，（iii）通过每轴顺应性限制交互力，实现了高效且安全的在线学习。

Result: 在工业Harting连接器模块插入任务中，SHaRe-RL在0.2-0.4毫米的间隙下实现了可靠的性能，并在实际时间预算内完成任务。

Conclusion: SHaRe-RL框架通过整合多种先验知识，实现了高效且安全的在线学习，为工业装配任务提供了更安全、更稳健且经济可行的解决方案。

Abstract: High-mix low-volume (HMLV) industrial assembly, common in small and
medium-sized enterprises (SMEs), requires the same precision, safety, and
reliability as high-volume automation while remaining flexible to product
variation and environmental uncertainty. Current robotic systems struggle to
meet these demands. Manual programming is brittle and costly to adapt, while
learning-based methods suffer from poor sample efficiency and unsafe
exploration in contact-rich tasks. To address this, we present SHaRe-RL, a
reinforcement learning framework that leverages multiple sources of prior
knowledge. By (i) structuring skills into manipulation primitives, (ii)
incorporating human demonstrations and online corrections, and (iii) bounding
interaction forces with per-axis compliance, SHaRe-RL enables efficient and
safe online learning for long-horizon, contact-rich industrial assembly tasks.
Experiments on the insertion of industrial Harting connector modules with
0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance
within practical time budgets. Our results show that process expertise, without
requiring robotics or RL knowledge, can meaningfully contribute to learning,
enabling safer, more robust, and more economically viable deployment of RL for
industrial assembly.

</details>


### [170] [SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning](https://arxiv.org/abs/2509.13956)
*Zewei Yang,Zengqi Peng,Jun Ma*

Main category: cs.RO

TL;DR: SEG-Parking是一种端到端离线强化学习框架，用于交互感知的自动驾驶停车，在模拟实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化环境和动态交互对自动驾驶停车任务带来的挑战。

Method: 提出了SEG-Parking，一种端到端的离线强化学习框架，包括预训练的目标条件状态编码器和带有保守正则化器的离线RL策略。

Result: 在CARLA模拟器中进行的闭环实验显示，该框架在成功率和泛化能力上表现优越。

Conclusion: SEG-Parking框架在CARLA模拟器中表现出色，具有最高的成功率和强大的泛化能力，适用于分布外停车场景。相关数据集和源代码将在论文被接受后公开。

Abstract: Autonomous parking is a critical component for achieving safe and efficient
urban autonomous driving. However, unstructured environments and dynamic
interactions pose significant challenges to autonomous parking tasks. To
address this problem, we propose SEG-Parking, a novel end-to-end offline
reinforcement learning (RL) framework to achieve interaction-aware autonomous
parking. Notably, a specialized parking dataset is constructed for parking
scenarios, which include those without interference from the opposite vehicle
(OV) and complex ones involving interactions with the OV. Based on this
dataset, a goal-conditioned state encoder is pretrained to map the fused
perception information into the latent space. Then, an offline RL policy is
optimized with a conservative regularizer that penalizes out-of-distribution
actions. Extensive closed-loop experiments are conducted in the high-fidelity
CARLA simulator. Comparative results demonstrate the superior performance of
our framework with the highest success rate and robust generalization to
out-of-distribution parking scenarios. The related dataset and source code will
be made publicly available after the paper is accepted.

</details>


### [171] [MetricNet: Recovering Metric Scale in Generative Navigation Policies](https://arxiv.org/abs/2509.13965)
*Abhijeet Nayak,Débora N. P. Oliveira,Samiran Gode,Cordelia Schmid,Wolfram Burgard*

Main category: cs.RO

TL;DR: MetricNet通过预测路径点的真实距离解决了生成导航策略的度量问题，结合MetricNav改善了导航安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 生成导航策略存在两个结构性问题：轨迹在抽象空间中缺乏度量基础，以及控制策略丢弃完整路径导致短视和不安全的动作。

Method: 提出MetricNet作为生成导航的附加模块，预测路径点之间的真实距离，并将MetricNet整合到导航策略中形成MetricNav。

Result: 在仿真和实际实验中，执行MetricNet缩放的路径点显著提高了导航和探索性能。

Conclusion: MetricNet和MetricNav通过预测路径点的真实距离并整合到导航策略中，显著提高了导航和探索性能，并在仿真和实际实验中验证了其有效性。

Abstract: Generative navigation policies have made rapid progress in improving
end-to-end learned navigation. Despite their promising results, this paradigm
has two structural problems. First, the sampled trajectories exist in an
abstract, unscaled space without metric grounding. Second, the control strategy
discards the full path, instead moving directly towards a single waypoint. This
leads to short-sighted and unsafe actions, moving the robot towards obstacles
that a complete and correctly scaled path would circumvent. To address these
issues, we propose MetricNet, an effective add-on for generative navigation
that predicts the metric distance between waypoints, grounding policy outputs
in real-world coordinates. We evaluate our method in simulation with a new
benchmarking framework and show that executing MetricNet-scaled waypoints
significantly improves both navigation and exploration performance. Beyond
simulation, we further validate our approach in real-world experiments.
Finally, we propose MetricNav, which integrates MetricNet into a navigation
policy to guide the robot away from obstacles while still moving towards the
goal.

</details>


### [172] [BIM Informed Visual SLAM for Construction Monitoring](https://arxiv.org/abs/2509.13972)
*Asier Bikandi,Miguel Fernandez-Cortizas,Muhammad Shaheer,Ali Tourani,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: 结合BIM的RGB-D SLAM系统显著提升施工环境中SLAM的精度，减少轨迹和地图误差。


<details>
  <summary>Details</summary>
Motivation: 施工环境中重复布局、遮挡和低纹理结构导致视觉SLAM的轨迹漂移问题，需要一种更可靠的方法来对齐数字计划与实景。

Method: 提出了一种RGB-D SLAM系统，通过将BIM作为结构先验知识，持续建立检测到的墙面与BIM对应关系，并将这些关系作为后端优化的约束。

Result: 在真实施工场景中验证，轨迹误差平均减少23.71%，地图RMSE减少7.14%。

Conclusion: 该论文提出的RGB-D SLAM系统结合BIM作为结构先验知识，有效减少了视觉SLAM在施工环境中的轨迹误差和地图RMSE，提升了数字计划与实景之间的对齐可靠性。

Abstract: Simultaneous Localization and Mapping (SLAM) is a key tool for monitoring
construction sites, where aligning the evolving as-built state with the
as-planned design enables early error detection and reduces costly rework.
LiDAR-based SLAM achieves high geometric precision, but its sensors are
typically large and power-demanding, limiting their use on portable platforms.
Visual SLAM offers a practical alternative with lightweight cameras already
embedded in most mobile devices. however, visually mapping construction
environments remains challenging: repetitive layouts, occlusions, and
incomplete or low-texture structures often cause drift in the trajectory map.
To mitigate this, we propose an RGB-D SLAM system that incorporates the
Building Information Model (BIM) as structural prior knowledge. Instead of
relying solely on visual cues, our system continuously establishes
correspondences between detected wall and their BIM counterparts, which are
then introduced as constraints in the back-end optimization. The proposed
method operates in real time and has been validated on real construction sites,
reducing trajectory error by an average of 23.71% and map RMSE by 7.14%
compared to visual SLAM baselines. These results demonstrate that BIM
constraints enable reliable alignment of the digital plan with the as-built
scene, even under partially constructed conditions.

</details>


### [173] [Flexible and Foldable: Workspace Analysis and Object Manipulation Using a Soft, Interconnected, Origami-Inspired Actuator Array](https://arxiv.org/abs/2509.13998)
*Bailey Dacre,Rodrigo Moreno,Serhat Demirtas,Ziqiao Wang,Yuhao Jiang,Jamie Paik,Kasper Stoy,Andrés Faíña*

Main category: cs.RO

TL;DR: 新型DMS设计利用3自由度、折纸启发的机器人瓦片和柔性连接层，降低执行器密度并提高操纵能力，无需增加执行器数量。


<details>
  <summary>Details</summary>
Motivation: 现有的DMS设计通常依赖高执行器密度，并对物体与执行器比例施加限制，限制了其适应性。本研究旨在通过新型设计解决这些问题。

Method: 采用3自由度、受折纸启发的机器人瓦片阵列，并通过柔性表面层连接这些执行器，形成一个连续可控的操纵表面。分析了系统的联合工作空间，推导了简单的运动原语，并展示了其在瓦片阵列上平移简单几何物体的能力。

Result: 通过利用瓦片间连接材料，该设计将执行器密度显著降低，使物体操纵面积增加了1.84倍，而无需增加执行器数量。这为传统高密度阵列提供了低成本、低复杂性的替代方案。

Conclusion: 该研究提出了一种新型分布式操纵器系统（DMS）设计，通过利用3自由度、受折纸启发的机器人瓦片阵列和柔性连接层，显著降低了执行器密度，同时提高了操纵能力。

Abstract: Object manipulation is a fundamental challenge in robotics, where systems
must balance trade-offs among manipulation capabilities, system complexity, and
throughput. Distributed manipulator systems (DMS) use the coordinated motion of
actuator arrays to perform complex object manipulation tasks, seeing widespread
exploration within the literature and in industry. However, existing DMS
designs typically rely on high actuator densities and impose constraints on
object-to-actuator scale ratios, limiting their adaptability. We present a
novel DMS design utilizing an array of 3-DoF, origami-inspired robotic tiles
interconnected by a compliant surface layer. Unlike conventional DMS, our
approach enables manipulation not only at the actuator end effectors but also
across a flexible surface connecting all actuators; creating a continuous,
controllable manipulation surface. We analyse the combined workspace of such a
system, derive simple motion primitives, and demonstrate its capabilities to
translate simple geometric objects across an array of tiles. By leveraging the
inter-tile connective material, our approach significantly reduces actuator
density, increasing the area over which an object can be manipulated by x1.84
without an increase in the number of actuators. This design offers a lower cost
and complexity alternative to traditional high-density arrays, and introduces
new opportunities for manipulation strategies that leverage the flexibility of
the interconnected surface.

</details>


### [174] [Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization](https://arxiv.org/abs/2509.14010)
*Zong Chen,Shaoyang Li,Ben Liu,Min Li,Zhouping Yin,Yiqun Li*

Main category: cs.RO

TL;DR: 本文提出了一种轮腿式四足机器人的设计和全身运动控制方法，通过接触感知的动态优化框架和统一的运动学模型，解决了操作与运动协调的挑战，验证了其在物流和自动化中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 轮腿式机器人结合机械臂在物流、工业自动化和人机协作等领域具有巨大潜力，但此类系统的统一控制仍面临自由度冗余、复杂轮地接触动力学以及运动与操作无缝协调等挑战。

Method: 开发了一个接触感知的全身动态优化框架，结合了点接触模型（用于操作）和线接触模型（用于轮地交互），并引入了热启动策略以加速在线优化，确保高维控制的实时可行性。此外，为机器人的4WIS-4WID驱动方案定制了统一的运动学模型，消除了不同运动策略间模式切换的需求。

Result: 仿真和实验结果验证了框架的有效性，展示了机器人在结构化环境中的敏捷地形穿越、高速全方位移动和精确操作能力。

Conclusion: 所提出的框架通过仿真和实验验证了其有效性，展示了机器人在地形穿越、高速全方位移动和精确操作方面的能力，凸显了其在半结构化环境中工厂自动化、城市物流和服务机器人领域的潜力。

Abstract: Wheel-legged robots with integrated manipulators hold great promise for
mobile manipulation in logistics, industrial automation, and human-robot
collaboration. However, unified control of such systems remains challenging due
to the redundancy in degrees of freedom, complex wheel-ground contact dynamics,
and the need for seamless coordination between locomotion and manipulation. In
this work, we present the design and whole-body motion control of an
omnidirectional wheel-legged quadrupedal robot equipped with a dexterous
manipulator. The proposed platform incorporates independently actuated steering
modules and hub-driven wheels, enabling agile omnidirectional locomotion with
high maneuverability in structured environments. To address the challenges of
contact-rich interaction, we develop a contact-aware whole-body dynamic
optimization framework that integrates point-contact modeling for manipulation
with line-contact modeling for wheel-ground interactions. A warm-start strategy
is introduced to accelerate online optimization, ensuring real-time feasibility
for high-dimensional control. Furthermore, a unified kinematic model tailored
for the robot's 4WIS-4WID actuation scheme eliminates the need for mode
switching across different locomotion strategies, improving control consistency
and robustness. Simulation and experimental results validate the effectiveness
of the proposed framework, demonstrating agile terrain traversal, high-speed
omnidirectional mobility, and precise manipulation under diverse scenarios,
underscoring the system's potential for factory automation, urban logistics,
and service robotics in semi-structured environments.

</details>


### [175] [TransforMARS: Fault-Tolerant Self-Reconfiguration for Arbitrarily Shaped Modular Aerial Robot Systems](https://arxiv.org/abs/2509.14025)
*Rui Huang,Zhiyu Gao,Siyu Tang,Jialin Zhang,Lei He,Ziqian Zhang,Lin Zhao*

Main category: cs.RO

TL;DR: TransforMARS 是一种通用的故障容忍重构框架，适用于任意形状的 MARS，能处理多故障并确保空中稳定性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的 MARS 自重构研究仅关注矩形形状的 MARS，且仅能容忍单个转子或单元故障。TransforMARS 旨在解决这一局限性，提供更灵活和实用的重构方案。

Method: 开发了算法，首先识别并构建包含故障单元的最小可控组件，然后规划可行的拆卸-组装序列以运输 MARS 单元或子组件形成目标配置。

Result: 在具有挑战性的任意形状 MARS 配置中验证了 TransforMARS，展示了其在处理多样配置和容忍多故障方面的显著改进。

Conclusion: TransforMARS 提出了一种通用的故障容忍重构框架，能够处理任意形状的 MARS 在多个转子和单元故障下的重构，同时确保空中稳定性，显著提升了处理多样配置和容忍故障数量的能力。

Abstract: Modular Aerial Robot Systems (MARS) consist of multiple drone modules that
are physically bound together to form a single structure for flight. Exploiting
structural redundancy, MARS can be reconfigured into different formations to
mitigate unit or rotor failures and maintain stable flight. Prior work on MARS
self-reconfiguration has solely focused on maximizing controllability margins
to tolerate a single rotor or unit fault for rectangular-shaped MARS. We
propose TransforMARS, a general fault-tolerant reconfiguration framework that
transforms arbitrarily shaped MARS under multiple rotor and unit faults while
ensuring continuous in-air stability. Specifically, we develop algorithms to
first identify and construct minimum controllable assemblies containing faulty
units. We then plan feasible disassembly-assembly sequences to transport MARS
units or subassemblies to form target configuration. Our approach enables more
flexible and practical feasible reconfiguration. We validate TransforMARS in
challenging arbitrarily shaped MARS configurations, demonstrating substantial
improvements over prior works in both the capacity of handling diverse
configurations and the number of faults tolerated. The videos and source code
of this work are available at the anonymous repository:
https://anonymous.4open.science/r/TransforMARS-1030/

</details>


### [176] [Prompt2Auto: From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning](https://arxiv.org/abs/2509.14040)
*Zewen Yang,Xiaobing Dai,Dongfa Zhang,Yu Li,Ziyang Meng,Bingkun Huang,Hamid Sadeghian,Sami Haddadin*

Main category: cs.RO

TL;DR: Prompt2Auto是一种几何不变的一次性高斯过程学习框架，通过单次运动提示实现机器人控制，减少演示负担并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量数据集且难以跨坐标变换泛化，Prompt2Auto旨在解决这些问题，实现从单次运动提示中学习。

Method: 提出了一种基于坐标变换的数据集构建策略，确保对平移、旋转和缩放的几何不变性，并支持多步预测。

Result: 通过数值模拟和真实机器人实验验证，方法有效且能跨任务泛化。

Conclusion: Prompt2Auto通过几何不变的一次性高斯过程学习框架，显著减少了演示负担，并在多个任务中展示了良好的泛化能力。

Abstract: Learning from demonstration allows robots to acquire complex skills from
human demonstrations, but conventional approaches often require large datasets
and fail to generalize across coordinate transformations. In this paper, we
propose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP)
learning framework that enables robots to perform human-guided automated
control from a single motion prompt. A dataset-construction strategy based on
coordinate transformations is introduced that enforces invariance to
translation, rotation, and scaling, while supporting multi-step predictions.
Moreover, GeoGP is robust to variations in the user's motion prompt and
supports multi-skill autonomy. We validate the proposed approach through
numerical simulations with the designed user graphical interface and two
real-world robotic experiments, which demonstrate that the proposed method is
effective, generalizes across tasks, and significantly reduces the
demonstration burden. Project page is available at:
https://prompt2auto.github.io

</details>


### [177] [Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace](https://arxiv.org/abs/2509.14063)
*Sundhar Vinodh Sangeetha,Chih-Yuan Chiu,Sarah H. Q. Li,Shreyas Kousik*

Main category: cs.RO

TL;DR: 本文提出一种结合语言理解和空间推理的多模态框架，用于提高无塔台空域中飞机的目标预测准确性，实验证明其优于仅依赖运动历史的方法。


<details>
  <summary>Details</summary>
Motivation: 无塔台空域中飞机安全运行依赖于飞行员间的语音通信，而自主飞机需准确预测其他飞机的意图和目标位置以确保安全。

Method: 结合自动语音识别和大语言模型转录并解析飞行员无线电通话，识别飞机并提取离散意图标签，然后将这些标签与观察到的轨迹融合，通过时间卷积网络和高斯混合模型进行概率性目标预测。

Result: 与仅依赖运动历史的基线方法相比，本文方法显著降低了目标预测误差，证明了语言条件预测的有效性。

Conclusion: 本文提出的多模态框架显著提高了无塔台空域中飞机的目标预测准确性，验证了语言条件预测在提升自主决策能力方面的潜力。

Abstract: Autonomous aircraft must safely operate in untowered airspace, where
coordination relies on voice-based communication among human pilots. Safe
operation requires an aircraft to predict the intent, and corresponding goal
location, of other aircraft. This paper introduces a multimodal framework for
aircraft goal prediction that integrates natural language understanding with
spatial reasoning to improve autonomous decision-making in such environments.
We leverage automatic speech recognition and large language models to
transcribe and interpret pilot radio calls, identify aircraft, and extract
discrete intent labels. These intent labels are fused with observed
trajectories to condition a temporal convolutional network and Gaussian mixture
model for probabilistic goal prediction. Our method significantly reduces goal
prediction error compared to baselines that rely solely on motion history,
demonstrating that language-conditioned prediction increases prediction
accuracy. Experiments on a real-world dataset from an untowered airport
validate the approach and highlight its potential to enable socially aware,
language-conditioned robotic motion planning.

</details>


### [178] [Constraint-Consistent Control of Task-Based and Kinematic RCM Constraints for Surgical Robots](https://arxiv.org/abs/2509.14075)
*Yu Li,Hamid Sadeghian,Zewen Yang,Valentin Le Mesle,Sami Haddadin*

Main category: cs.RO

TL;DR: 提出了一种约束一致性扭矩控制器，用于机器人辅助微创手术中动态和交互条件下的RCM约束满足，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有控制方法在扭矩级别缺乏鲁棒性或无法保证一致的RCM约束满足，尤其是在动态和交互条件下。

Method: 通过将远程运动中心（RCM）约束视为流形约束，并嵌入基于投影的逆动力学框架，该方法统一了任务级和运动学公式。

Result: 在模拟和RAMIS训练平台上的验证显示，该方法改善了RCM约束满足度，减少了所需扭矩，并通过一致性公式提高了关节扭矩的平滑性。

Conclusion: 本论文提出的约束一致性扭矩控制器在手术机器人中展现出提升安全性和可靠性的潜力，特别是在动态和交互条件下。

Abstract: Robotic-assisted minimally invasive surgery (RAMIS) requires precise
enforcement of the remote center of motion (RCM) constraint to ensure safe tool
manipulation through a trocar. Achieving this constraint under dynamic and
interactive conditions remains challenging, as existing control methods either
lack robustness at the torque level or do not guarantee consistent RCM
constraint satisfaction. This paper proposes a constraint-consistent torque
controller that treats the RCM as a rheonomic holonomic constraint and embeds
it into a projection-based inverse-dynamics framework. The method unifies
task-level and kinematic formulations, enabling accurate tool-tip tracking
while maintaining smooth and efficient torque behavior. The controller is
validated both in simulation and on a RAMIS training platform, and is
benchmarked against state-of-the-art approaches. Results show improved RCM
constraint satisfaction, reduced required torque, and robust performance by
improving joint torque smoothness through the consistency formulation under
clinically relevant scenarios, including spiral trajectories, variable
insertion depths, moving trocars, and human interaction. These findings
demonstrate the potential of constraint-consistent torque control to enhance
safety and reliability in surgical robotics. The project page is available at:
https://rcmpc-cube.github.io

</details>


### [179] [FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video](https://arxiv.org/abs/2509.14082)
*Valerii Serpiva,Artem Lykov,Faryal Batool,Vladislav Kozlovskiy,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: FlightDiffusion是一种基于扩散模型的框架，用于生成逼真FPV视频和动作空间，降低无人机训练成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前无人机训练依赖于大量真实世界数据，成本高昂且难以扩展。FlightDiffusion旨在通过生成逼真的FPV视频和动作空间，降低数据收集成本，同时提升策略学习和数据集扩展性。

Method: FlightDiffusion利用扩散模型生成多样化的FPV轨迹和状态-动作对，从而无需高昂的真实世界数据收集成本即可创建大规模训练数据集。模型生成的轨迹在物理上是可行的，且可执行，位置误差均值0.25米（RMSE 0.28米），方向误差均值0.19弧度（RMSE 0.24弧度）。

Result: 在模拟环境中，FlightDiffusion表现出更强的鲁棒性、更平滑的轨迹规划能力以及对未知条件的适应性。ANOVA分析显示，模拟与真实环境中的性能无显著差异（F(1, 16) = 0.394, p = 0.541），成功率为M = 0.628（SD = 0.162）和M = 0.617（SD = 0.177），表明良好的模拟到真实迁移能力。

Conclusion: 该研究提出了FlightDiffusion，一种基于扩散模型的框架，用于从第一人称视角（FPV）视频中训练自主无人机。该模型能够从单帧图像生成逼真的视频序列，并附带相应的动作空间，以实现动态环境中的推理驱动导航。研究表明，该方法不仅提升了策略学习能力，还增强了数据集的扩展性，从而在下游导航任务中实现了卓越性能。

Abstract: We present FlightDiffusion, a diffusion-model-based framework for training
autonomous drones from first-person view (FPV) video. Our model generates
realistic video sequences from a single frame, enriched with corresponding
action spaces to enable reasoning-driven navigation in dynamic environments.
Beyond direct policy learning, FlightDiffusion leverages its generative
capabilities to synthesize diverse FPV trajectories and state-action pairs,
facilitating the creation of large-scale training datasets without the high
cost of real-world data collection. Our evaluation demonstrates that the
generated trajectories are physically plausible and executable, with a mean
position error of 0.25 m (RMSE 0.28 m) and a mean orientation error of 0.19 rad
(RMSE 0.24 rad). This approach enables improved policy learning and dataset
scalability, leading to superior performance in downstream navigation tasks.
Results in simulated environments highlight enhanced robustness, smoother
trajectory planning, and adaptability to unseen conditions. An ANOVA revealed
no statistically significant difference between performance in simulation and
reality (F(1, 16) = 0.394, p = 0.541), with success rates of M = 0.628 (SD =
0.162) and M = 0.617 (SD = 0.177), respectively, indicating strong sim-to-real
transfer. The generated datasets provide a valuable resource for future UAV
research. This work introduces diffusion-based reasoning as a promising
paradigm for unifying navigation, action generation, and data synthesis in
aerial robotics.

</details>


### [180] [GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model](https://arxiv.org/abs/2509.14117)
*Ali Abouzeid,Malak Mansour,Zezhou Sun,Dezhen Song*

Main category: cs.RO

TL;DR: GeoAware-VLA通过整合几何先验提升VLA模型在新视角下的泛化能力，实验显示其在仿真和真实机器人上均显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型在新摄像头视角下泛化能力不足的问题，特别是从2D图像推断3D几何的困难。

Method: 利用预训练的几何视觉模型作为特征提取器，通过可训练的投影层将几何丰富的特征适配到策略解码器，避免从头学习3D一致性。

Result: 在LIBERO基准测试中，GeoAware-VLA实现了零样本泛化到新相机姿态的成功率提升2倍以上，且在真实机器人上表现出显著性能增益。

Conclusion: GeoAware-VLA证明了通过整合几何先验知识，可以显著提升VLA模型在新视角下的泛化能力，为构建更通用的机器人代理提供了关键组件。

Abstract: Vision-Language-Action (VLA) models often fail to generalize to novel camera
viewpoints, a limitation stemming from their difficulty in inferring robust 3D
geometry from 2D images. We introduce GeoAware-VLA, a simple yet effective
approach that enhances viewpoint invariance by integrating strong geometric
priors into the vision backbone. Instead of training a visual encoder or
relying on explicit 3D data, we leverage a frozen, pretrained geometric vision
model as a feature extractor. A trainable projection layer then adapts these
geometrically-rich features for the policy decoder, relieving it of the burden
of learning 3D consistency from scratch. Through extensive evaluations on
LIBERO benchmark subsets, we show GeoAware-VLA achieves substantial
improvements in zero-shot generalization to novel camera poses, boosting
success rates by over 2x in simulation. Crucially, these benefits translate to
the physical world; our model shows a significant performance gain on a real
robot, especially when evaluated from unseen camera angles. Our approach proves
effective across both continuous and discrete action spaces, highlighting that
robust geometric grounding is a key component for creating more generalizable
robotic agents.

</details>


### [181] [CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads](https://arxiv.org/abs/2509.14126)
*Viktor Lorentz,Khaled Wahba,Sayantan Auddy,Marc Toussaint,Wolfgang Hönig*

Main category: cs.RO

TL;DR: 提出CrazyMARL框架，通过强化学习解决多无人机电缆负载运输中的控制问题，仿真和实际测试均显示其优越性能。


<details>
  <summary>Details</summary>
Motivation: 多无人机协调在干扰、非线性负载动态和松弛-紧绷电缆模式下的控制问题仍然具有挑战性，尚未有工作解决这些电缆模式转换问题。

Method: 提出了CrazyMARL，一种用于多无人机电缆悬挂负载运输的分散式强化学习（RL）框架。

Result: 仿真结果表明，学习到的策略在干扰抑制和跟踪精度方面优于经典分散控制器，从恶劣条件中恢复的成功率达到80%，而基线方法为44%。

Conclusion: 该研究为自主、弹性的无人机团队在非结构化环境中执行复杂负载任务铺平了道路。

Abstract: Collaborative transportation of cable-suspended payloads by teams of Unmanned
Aerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to
different payload shapes, and provide built-in compliance, making it attractive
for applications ranging from disaster relief to precision logistics. However,
multi-UAV coordination under disturbances, nonlinear payload dynamics, and
slack--taut cable modes remains a challenging control problem. To our
knowledge, no prior work has addressed these cable mode transitions in the
multi-UAV context, instead relying on simplifying rigid-link assumptions. We
propose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for
multi-UAV cable-suspended payload transport. Simulation results demonstrate
that the learned policies can outperform classical decentralized controllers in
terms of disturbance rejection and tracking precision, achieving an 80%
recovery rate from harsh conditions compared to 44% for the baseline method. We
also achieve successful zero-shot sim-to-real transfer and demonstrate that our
policies are highly robust under harsh conditions, including wind, random
external disturbances, and transitions between slack and taut cable dynamics.
This work paves the way for autonomous, resilient UAV teams capable of
executing complex payload missions in unstructured environments.

</details>


### [182] [Energy Efficient Multi Robot Package Delivery under Capacity-Constraints via Voronoi-Constrained Networks](https://arxiv.org/abs/2509.14127)
*Alkesh K. Srivastava,Jared Michael Levin,Philip Dames*

Main category: cs.RO

TL;DR: VCST-RCP框架通过优化中继协调，提升多机器人配送效率34%，适用于现实物流场景。


<details>
  <summary>Details</summary>
Motivation: 解决有限承载能力的同质机器人车队从单一取货点到多个目标位置的包裹配送问题，挑战传统依赖直接源到目的地运输的方法。

Method: 提出VCST-RCP框架，结合Voronoi约束的Steiner树优化构建稀疏中继主干，并合成机器人级的拾取、中继和配送计划。

Result: 实验显示，与传统基线相比，性能提升高达34%，显著提高了能源效率。

Conclusion: VCST-RCP框架通过将中继点作为协调的核心元素，显著提升了多机器人配送的效率和能源利用率，为现实物流提供了可扩展的解决方案。

Abstract: We consider the problem of delivering multiple packages from a single pickup
depot to distinct goal locations using a homogeneous fleet of robots with
limited carrying capacity. We propose VCST-RCP, a Voronoi-Constrained Steiner
Tree Relay Coordination Planning framework that constructs sparse relay trunks
using Steiner tree optimization and then synthesizes robot-level pickup, relay,
and delivery schedules. This framework reframes relays from incidental
byproducts into central elements of coordination, offering a contrast with
traditional delivery methods that rely on direct source-to-destination
transport. Extensive experiments show consistent improvements of up to 34%
compared to conventional baselines, underscoring the benefits of incorporating
relays into the delivery process. These improvements translate directly to
enhanced energy efficiency in multi-robot delivery under capacity constraints,
providing a scalable framework for real-world logistics.

</details>


### [183] [SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model](https://arxiv.org/abs/2509.14138)
*Ran Yang,Zijian An,Lifeng ZHou,Yiming Feng*

Main category: cs.RO

TL;DR: SeqVLA通过双头设计增强π₀，实现子任务完成感知，显著提升长时程机器人操作的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型如π₀在连续低层控制中表现优异，但缺乏子任务完成信号，导致在序列任务中脆弱。

Method: 提出了SeqVLA，一种基于π₀的双头架构，通过轻量级检测头感知子任务完成情况，并研究了四种微调策略。

Result: SeqVLA在沙拉打包和糖果打包任务中显著优于基线π₀，联合微调且不冻结主干网络的策略表现最佳。

Conclusion: SeqVLA通过结合动作生成和子任务感知检测，显著提升了长时程机器人操作的可靠性和成功率。

Abstract: Long-horizon robotic manipulation tasks require executing multiple
interdependent subtasks in strict sequence, where errors in detecting subtask
completion can cascade into downstream failures. Existing
Vision-Language-Action (VLA) models such as $\pi_0$ excel at continuous
low-level control but lack an internal signal for identifying when a subtask
has finished, making them brittle in sequential settings. We propose SeqVLA, a
completion-aware extension of $\pi_0$ that augments the base architecture with
a lightweight detection head perceiving whether the current subtask is
complete. This dual-head design enables SeqVLA not only to generate
manipulation actions but also to autonomously trigger transitions between
subtasks. We investigate four finetuning strategies that vary in how the action
and detection heads are optimized (joint vs. sequential finetuning) and how
pretrained knowledge is preserved (full finetuning vs. frozen backbone).
Experiments are performed on two multi-stage tasks: salad packing with seven
distinct subtasks and candy packing with four distinct subtasks. Results show
that SeqVLA significantly outperforms the baseline $\pi_0$ and other strong
baselines in overall success rate. In particular, joint finetuning with an
unfrozen backbone yields the most decisive and statistically reliable
completion predictions, eliminating sequence-related failures and enabling
robust long-horizon execution. Our results highlight the importance of coupling
action generation with subtask-aware detection for scalable sequential
manipulation.

</details>


### [184] [CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping](https://arxiv.org/abs/2509.14143)
*Zijian An,Ran Yang,Yiming Feng,Lifeng Zhou*

Main category: cs.RO

TL;DR: CLAW通过解耦条件评估与动作生成，结合CLIP模型和VLA策略，实现了重量感知的机器人控制，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型缺乏明确的机制来满足精确任务约束（如基于数字阈值的停止），CLAW旨在解决这一问题。

Method: CLAW利用微调的CLIP模型作为轻量级提示生成器，持续监测数字读数并生成离散指令，由基于流的VLA策略π0整合多视角摄像头观测生成连续机器人动作。

Result: CLAW在单物体抓取和需要双臂操作的混合物体任务中可靠执行重量感知行为，性能优于原始π0和微调π0模型。

Conclusion: CLAW框架通过将条件评估与动作生成解耦，成功结合了符号重量推理与高频率的视觉运动控制，在多种实验设置中表现出色，优于现有模型。

Abstract: Vision-language-action (VLA) models have recently emerged as a promising
paradigm for robotic control, enabling end-to-end policies that ground natural
language instructions into visuomotor actions. However, current VLAs often
struggle to satisfy precise task constraints, such as stopping based on numeric
thresholds, since their observation-to-action mappings are implicitly shaped by
training data and lack explicit mechanisms for condition monitoring. In this
work, we propose CLAW (CLIP-Language-Action for Weight), a framework that
decouples condition evaluation from action generation. CLAW leverages a
fine-tuned CLIP model as a lightweight prompt generator, which continuously
monitors the digital readout of a scale and produces discrete directives based
on task-specific weight thresholds. These prompts are then consumed by $\pi_0$,
a flow-based VLA policy, which integrates the prompts with multi-view camera
observations to produce continuous robot actions. This design enables CLAW to
combine symbolic weight reasoning with high-frequency visuomotor control. We
validate CLAW on three experimental setups: single-object grasping and
mixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW
reliably executes weight-aware behaviors and outperforms both raw-$\pi_0$ and
fine-tuned $\pi_0$ models. We have uploaded the videos as supplementary
materials.

</details>


### [185] [StableTracker: Learning to Stably Track Target via Differentiable Simulation](https://arxiv.org/abs/2509.14147)
*Fanxing Li,Shengyang Wang,Fangyu Sun,Shuyu Wu,Dexin Zuo,Wenxian Yu,Danping Zou*

Main category: cs.RO

TL;DR: StableTracker是一种基于学习的控制策略，通过可微分模拟训练，显著提升四旋翼无人机目标跟踪性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的FPV目标跟踪方法依赖手工模块设计，导致硬件过载和累积误差，尤其在目标快速加减速时性能下降。

Method: 使用基于时间的反向传播和可微分模拟训练控制策略，使无人机能在水平和垂直方向上保持目标在视野中心，并维持固定相对距离。

Result: 仿真和真实世界实验表明，StableTracker在准确性、稳定性和泛化能力上优于现有传统算法和学习基线。

Conclusion: StableTracker 通过基于学习的控制策略显著提升了四旋翼无人机在复杂条件下的目标跟踪性能，验证了其在实际应用中的可行性和优越性。

Abstract: FPV object tracking methods heavily rely on handcraft modular designs,
resulting in hardware overload and cumulative error, which seriously degrades
the tracking performance, especially for rapidly accelerating or decelerating
targets. To address these challenges, we present \textbf{StableTracker}, a
learning-based control policy that enables quadrotors to robustly follow the
moving target from arbitrary perspectives. The policy is trained using
backpropagation-through-time via differentiable simulation, allowing the
quadrotor to maintain the target at the center of the visual field in both
horizontal and vertical directions, while keeping a fixed relative distance,
thereby functioning as an autonomous aerial camera. We compare StableTracker
against both state-of-the-art traditional algorithms and learning baselines.
Simulation experiments demonstrate that our policy achieves superior accuracy,
stability and generalization across varying safe distances, trajectories, and
target velocities. Furthermore, a real-world experiment on a quadrotor with an
onboard computer validated practicality of the proposed approach.

</details>


### [186] [MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies](https://arxiv.org/abs/2509.14159)
*Dayi Dong,Maulik Bhatt,Seoyeon Choi,Negar Mehr*

Main category: cs.RO

TL;DR: MIMIC-D利用扩散策略实现多模态多智能体模仿学习的隐式协调，实验证明其在多样化任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在社会中的广泛应用，其与人类和其他机器人在多模态任务中的协调能力变得至关重要。现有模仿学习方法在多模态专家演示下难以捕捉多样化策略，影响协调效果。

Method: 采用集中训练、分散执行（CTDE）范式，利用扩散策略进行多模态多智能体模仿学习，智能体在训练时共享完整信息，执行时仅依赖本地信息。

Result: 在仿真和硬件实验中，MIMIC-D成功恢复了智能体间的多模态协调行为，并在多种任务和环境中表现优于现有基线方法。

Conclusion: MIMIC-D方法通过扩散策略在多模态多智能体模仿学习中实现了隐式协调，显著提升了智能体在各种任务和环境中的协调能力，并超越了现有基线方法。

Abstract: As robots become more integrated in society, their ability to coordinate with
other robots and humans on multi-modal tasks (those with multiple valid
solutions) is crucial. We propose to learn such behaviors from expert
demonstrations via imitation learning (IL). However, when expert demonstrations
are multi-modal, standard IL approaches can struggle to capture the diverse
strategies, hindering effective coordination. Diffusion models are known to be
effective at handling complex multi-modal trajectory distributions in
single-agent systems. Diffusion models have also excelled in multi-agent
scenarios where multi-modality is more common and crucial to learning
coordinated behaviors. Typically, diffusion-based approaches require a
centralized planner or explicit communication among agents, but this assumption
can fail in real-world scenarios where robots must operate independently or
with agents like humans that they cannot directly communicate with. Therefore,
we propose MIMIC-D, a Centralized Training, Decentralized Execution (CTDE)
paradigm for multi-modal multi-agent imitation learning using diffusion
policies. Agents are trained jointly with full information, but execute
policies using only local information to achieve implicit coordination. We
demonstrate in both simulation and hardware experiments that our method
recovers multi-modal coordination behavior among agents in a variety of tasks
and environments, while improving upon state-of-the-art baselines.

</details>


### [187] [\textsc{Gen2Real}: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video](https://arxiv.org/abs/2509.14178)
*Kai Ye,Yuhang Wu,Shuyuan Hu,Junliang Li,Meng Liu,Yongquan Chen,Rui Huang*

Main category: cs.RO

TL;DR: Gen2Real通过生成视频替代人类演示，结合物理优化和学习策略，成功实现了从模拟到现实的机器人抓取技能转移。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧操作中人类演示数据收集困难的问题，通过生成视频替代昂贵的人类演示，驱动机器人技能学习。

Method: Gen2Real结合了视频生成与姿态和深度估计生成手-物体轨迹，使用物理感知交互优化模型（PIOM）确保物理一致性，并通过基于锚点的残差PPO策略进行学习和控制稳定。

Result: 仅使用生成视频学习的策略在模拟抓取任务中达到77.3%的成功率，并在真实机器人上展示了连贯的执行能力。

Conclusion: Gen2Real成功地将生成视频转化为机器人技能，通过结合视频生成、物理优化和学习策略，实现了从模拟到现实的连贯执行，展示了其在抓取任务中的灵活性和鲁棒性。

Abstract: Dexterous manipulation remains a challenging robotics problem, largely due to
the difficulty of collecting extensive human demonstrations for learning. In
this paper, we introduce \textsc{Gen2Real}, which replaces costly human demos
with one generated video and drives robot skill from it: it combines
demonstration generation that leverages video generation with pose and depth
estimation to yield hand-object trajectories, trajectory optimization that uses
Physics-aware Interaction Optimization Model (PIOM) to impose physics
consistency, and demonstration learning that retargets human motions to a robot
hand and stabilizes control with an anchor-based residual Proximal Policy
Optimization (PPO) policy. Using only generated videos, the learned policy
achieves a 77.3\% success rate on grasping tasks in simulation and demonstrates
coherent executions on a real robot. We also conduct ablation studies to
validate the contribution of each component and demonstrate the ability to
directly specify tasks using natural language, highlighting the flexibility and
robustness of \textsc{Gen2Real} in generalizing grasping skills from imagined
videos to real-world execution.

</details>


### [188] [MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping](https://arxiv.org/abs/2509.14191)
*Zhihao Cao,Hanyu Wu,Li Wa Tang,Zizhou Luo,Zihan Zhu,Wei Zhang,Marc Pollefeys,Martin R. Oswald*

Main category: cs.RO

TL;DR: MCGS-SLAM是首个基于RGB的多相机SLAM系统，利用3D高斯溅射技术，通过多相机束调整和尺度一致性模块实现高精度轨迹和逼真重建，优于单目方法。


<details>
  <summary>Details</summary>
Motivation: 针对单目SLAM在鲁棒性和几何覆盖上的不足，开发了首个基于RGB的多相机SLAM系统，利用3D高斯溅射技术。

Method: MCGS-SLAM通过多相机束调整（MCBA）联合优化位姿和深度，利用密集的光度和几何残差，同时通过尺度一致性模块使用低秩先验强制视图间的度量对齐。

Result: 实验表明，MCGS-SLAM在合成和真实数据集上持续产生准确的轨迹和逼真的重建，通常优于单目基线。多相机输入的大视场角能够重建单目设置遗漏的侧视区域。

Conclusion: MCGS-SLAM展示了多相机高斯溅射SLAM在机器人学和自动驾驶中高保真地图构建的潜力。

Abstract: Recent progress in dense SLAM has primarily targeted monocular setups, often
at the expense of robustness and geometric coverage. We present MCGS-SLAM, the
first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting
(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM
fuses dense RGB inputs from multiple viewpoints into a unified, continuously
optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines
poses and depths via dense photometric and geometric residuals, while a scale
consistency module enforces metric alignment across views using low-rank
priors. The system supports RGB input and maintains real-time performance at
large scale. Experiments on synthetic and real-world datasets show that
MCGS-SLAM consistently yields accurate trajectories and photorealistic
reconstructions, usually outperforming monocular baselines. Notably, the wide
field of view from multi-camera input enables reconstruction of side-view
regions that monocular setups miss, critical for safe autonomous operation.
These results highlight the promise of multi-camera Gaussian Splatting SLAM for
high-fidelity mapping in robotics and autonomous driving.

</details>


### [189] [GLIDE: A Coordinated Aerial-Ground Framework for Search and Rescue in Unknown Environments](https://arxiv.org/abs/2509.14210)
*Seth Farrell,Chenghao Li,Hongzhan Yu,Hesam Mojtahedi,Sicun Gao,Henrik I. Christensen*

Main category: cs.RO

TL;DR: GLIDE是一个无人机与地面车辆协同的搜救框架，通过角色分离和实时规划提高效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 解决未知环境中搜救任务的时间效率和安全性问题。

Method: 框架包括两个无人机（一个用于目标搜索，一个用于地形侦察）和一个地面车辆，通过实时数据融合和A*路径规划进行协作。

Result: 实证结果表明，GLIDE框架在时间紧迫的搜救任务中提高了到达时间和导航安全性。

Conclusion: GLIDE框架通过无人机与地面车辆的协同工作，显著提高了搜救任务中的目标定位速度和导航安全性。

Abstract: We present a cooperative aerial-ground search-and-rescue (SAR) framework that
pairs two unmanned aerial vehicles (UAVs) with an unmanned ground vehicle (UGV)
to achieve rapid victim localization and obstacle-aware navigation in unknown
environments. We dub this framework Guided Long-horizon Integrated Drone Escort
(GLIDE), highlighting the UGV's reliance on UAV guidance for long-horizon
planning. In our framework, a goal-searching UAV executes real-time onboard
victim detection and georeferencing to nominate goals for the ground platform,
while a terrain-scouting UAV flies ahead of the UGV's planned route to provide
mid-level traversability updates. The UGV fuses aerial cues with local sensing
to perform time-efficient A* planning and continuous replanning as information
arrives. Additionally, we present a hardware demonstration (using a GEM e6 golf
cart as the UGV and two X500 UAVs) to evaluate end-to-end SAR mission
performance and include simulation ablations to assess the planning stack in
isolation from detection. Empirical results demonstrate that explicit role
separation across UAVs, coupled with terrain scouting and guided planning,
improves reach time and navigation safety in time-critical SAR missions.

</details>


### [190] [Multi-robot Multi-source Localization in Complex Flows with Physics-Preserving Environment Models](https://arxiv.org/abs/2509.14228)
*Benjamin Shaffer,Victoria Edwards,Brooks Kinch,Nathaniel Trask,M. Ani Hsieh*

Main category: cs.RO

TL;DR: 提出了一种分布式移动传感框架，利用机器学习的有限元模型和信息趋向策略，在复杂流动中实现高效源定位。


<details>
  <summary>Details</summary>
Motivation: 复杂流动环境中的源定位对多机器人团队构成重大挑战，流动力学和环境几何的复杂性使得传统方法难以准确建模和预测。

Method: 每个机器人携带一个机器学习的环境有限元模型，用于指导基于信息的采样，并通过近似互信息准则驱动信息趋向控制策略。

Result: 该方法相比基线策略实现了更快的误差减少和更准确的源定位。

Conclusion: 该研究提出的分布式移动传感框架在复杂流动环境中实现了更快速和更准确的源定位，优于基线传感策略和机器学习方法。

Abstract: Source localization in a complex flow poses a significant challenge for
multi-robot teams tasked with localizing the source of chemical leaks or
tracking the dispersion of an oil spill. The flow dynamics can be time-varying
and chaotic, resulting in sporadic and intermittent sensor readings, and
complex environmental geometries further complicate a team's ability to model
and predict the dispersion. To accurately account for the physical processes
that drive the dispersion dynamics, robots must have access to computationally
intensive numerical models, which can be difficult when onboard computation is
limited. We present a distributed mobile sensing framework for source
localization in which each robot carries a machine-learned, finite element
model of its environment to guide information-based sampling. The models are
used to evaluate an approximate mutual information criterion to drive an
infotaxis control strategy, which selects sensing regions that are expected to
maximize informativeness for the source localization objective. Our approach
achieves faster error reduction compared to baseline sensing strategies and
results in more accurate source localization compared to baseline machine
learning approaches.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [191] [A Task Equalization Allocation Algorithm Incorporating Blocking Estimation and Resource Similarity Analysis for Vehicle Control Real-Time Systems](https://arxiv.org/abs/2509.14086)
*Qianlong Duan,Bide Hao,Fan Zhou,Chen Fei,Shichun Yang*

Main category: cs.OS

TL;DR: BR-WFD算法通过优化任务分配，显著提升多核车辆控制系统的实时性能和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 多核实时车辆控制系统中，同步阻塞和资源竞争问题严重影响了系统可调度性和实时性能，传统任务分配算法往往忽略阻塞影响。

Method: 提出了BR-WFD算法，结合阻塞时间估计和资源相似性分析，优先调度同步敏感性高的任务，并将共享资源访问任务聚集到同一核心。

Result: 仿真显示，BR-WFD在高负载和资源竞争场景下，相比传统方法减少了11%至28%的处理器核心需求，并保持15%至20%的更高可调度比例。

Conclusion: BR-WFD算法通过整合阻塞时间估计和资源相似性分析，有效提升了多核实时车辆控制系统的调度性能和资源效率。

Abstract: In multi-core real-time vehicle control systems, synchronization blocking and
resource contention pose critical challenges due to increasing task parallelism
and shared resource access. These issues significantly degrade system
schedulability and real-time performance, as traditional task allocation
algorithms often overlook blocking impacts, leading to high scheduling failure
rates under heavy loads. To address this, we propose the BR-WFD algorithm,
which integrates blocking time estimation and resource similarity analysis. The
algorithm minimizes global blocking overhead by prioritizing tasks with high
synchronization sensitivity and aggregating shared-resource-accessing tasks
onto the same core. Extensive simulations show that BR-WFD reduces required
processor cores by 11\% to 28\% and maintains a 15\% to 20\% higher schedulable
ratio compared to traditional methods under high-load and resource-competitive
scenarios. This demonstrates its effectiveness in enhancing real-time
performance and resource efficiency for multi-core task scheduling in
intelligent driving systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [192] [Is Research Software Science a Metascience?](https://arxiv.org/abs/2509.13436)
*Evan Eisinger,Michael A. Heroux*

Main category: cs.SE

TL;DR: 研究软件科学（RSS）应被视为与元科学相契合的跨学科领域，有助于提升研究软件的质量和科学可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着研究越来越依赖计算方法，科学结果的可靠性取决于研究软件的质量、可重复性和透明度。确保这些特性对科学诚信和发现至关重要。

Method: 通过定义元科学和研究软件科学，比较它们的原理和目标，并分析它们的重叠之处。

Result: 研究发现RSS推进了元科学的核心目标，特别是在计算可重复性方面，并连接了研究的技术、社会和认知层面。

Conclusion: 本文认为研究软件科学（RSS）应被视为一个独特的跨学科领域，与元科学（metascience）在某些定义下相契合。这种分类有助于提升其在研究改进中的作用，并为其资金支持提供依据。

Abstract: As research increasingly relies on computational methods, the reliability of
scientific results depends on the quality, reproducibility, and transparency of
research software. Ensuring these qualities is critical for scientific
integrity and discovery. This paper asks whether Research Software Science
(RSS)--the empirical study of how research software is developed and
used--should be considered a form of metascience, the science of science.
Classification matters because it could affect recognition, funding, and
integration of RSS into research improvement. We define metascience and RSS,
compare their principles and objectives, and examine their overlaps. Arguments
for classification highlight shared commitments to reproducibility,
transparency, and empirical study of research processes. Arguments against
portraying RSS as a specialized domain focused on a tool rather than the
broader scientific enterprise. Our analysis finds RSS advances core goals of
metascience, especially in computational reproducibility, and bridges
technical, social, and cognitive aspects of research. Its classification
depends on whether one adopts a broad definition of metascience--any empirical
effort to improve science--or a narrow one focused on systemic and
epistemological structures. We argue RSS is best understood as a distinct
interdisciplinary domain that aligns with, and in some definitions fits within,
metascience. Recognizing it as such can strengthen its role in improving
reliability, justify funding, and elevate software development in research
institutions. Regardless of classification, applying scientific rigor to
research software ensures the tools of discovery meet the standards of the
discoveries themselves.

</details>


### [193] [An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software](https://arxiv.org/abs/2509.13471)
*Sina Gogani-Khiabani,Ashutosh Trivedi,Diptikalyan Saha,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: An agentic approach using LLMs for legal-critical software development, focusing on U.S. tax code translation, outperforms frontier models in reliability.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) show potential for translating natural-language statutes into executable logic, but their reliability in legally critical settings is challenged by ambiguity and hallucinations. The study aims to address these challenges in legal-critical software development, using U.S. federal tax preparation as a case study.

Method: The paper introduces a multi-agent system that translates tax code into executable software, incorporating a metamorphic-testing agent to search for counterexamples. It uses an LLM-driven, role-based framework to automate test generation and code synthesis.

Result: In experiments, the framework using a smaller model (GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier models (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks.

Conclusion: The study supports agentic LLM methodologies as a viable path to developing robust and trustworthy legal-critical software from natural-language specifications.

Abstract: Large language models (LLMs) show promise for translating natural-language
statutes into executable logic, but reliability in legally critical settings
remains challenging due to ambiguity and hallucinations. We present an agentic
approach for developing legal-critical software, using U.S. federal tax
preparation as a case study. The key challenge is test-case generation under
the oracle problem, where correct outputs require interpreting law. Building on
metamorphic testing, we introduce higher-order metamorphic relations that
compare system outputs across structured shifts among similar individuals.
Because authoring such relations is tedious and error-prone, we use an
LLM-driven, role-based framework to automate test generation and code
synthesis. We implement a multi-agent system that translates tax code into
executable software and incorporates a metamorphic-testing agent that searches
for counterexamples. In experiments, our framework using a smaller model
(GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier
models (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results
support agentic LLM methodologies as a path to robust, trustworthy
legal-critical software from natural-language specifications.

</details>


### [194] [Prompt2DAG: A Modular Methodology for LLM-Based Data Enrichment Pipeline Generation](https://arxiv.org/abs/2509.13487)
*Abubakari Alidu,Michele Ciavotta,Flavio DePaoli*

Main category: cs.SE

TL;DR: Prompt2DAG通过混合方法将自然语言转化为Airflow DAG，成功率达78.5%，优于其他方法，为自动化数据管道开发提供了高效可靠的解决方案。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的数据丰富管道需要大量工程专业知识，Prompt2DAG旨在将自然语言描述转化为可执行的Apache Airflow DAG，以优化生产级自动化策略。

Method: 评估了四种生成方法（直接法、仅LLM法、混合法和基于模板法），通过260个实验、13个LLM和5个案例研究，使用结合可靠性、代码质量（SAT）、结构完整性（DST）和可执行性（PCT）的惩罚评分框架。

Result: 混合方法成为最佳生成方法，成功率为78.5%，质量得分稳健（SAT：6.79，DST：7.67，PCT：7.76），显著优于仅LLM法（66.2%）和直接法（29.2%）。成本效益分析显示混合方法每成功DAG的效率是直接法的两倍以上。

Conclusion: 结构化混合方法在自动工作流生成中平衡灵活性和可靠性至关重要，为数据管道开发的民主化提供了可行路径。

Abstract: Developing reliable data enrichment pipelines demands significant engineering
expertise. We present Prompt2DAG, a methodology that transforms natural
language descriptions into executable Apache Airflow DAGs. We evaluate four
generation approaches -- Direct, LLM-only, Hybrid, and Template-based -- across
260 experiments using thirteen LLMs and five case studies to identify optimal
strategies for production-grade automation. Performance is measured using a
penalized scoring framework that combines reliability with code quality (SAT),
structural integrity (DST), and executability (PCT). The Hybrid approach
emerges as the optimal generative method, achieving a 78.5% success rate with
robust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76). This significantly
outperforms the LLM-only (66.2% success) and Direct (29.2% success) methods.
Our findings show that reliability, not intrinsic code quality, is the primary
differentiator. Cost-effectiveness analysis reveals the Hybrid method is over
twice as efficient as Direct prompting per successful DAG. We conclude that a
structured, hybrid approach is essential for balancing flexibility and
reliability in automated workflow generation, offering a viable path to
democratize data pipeline development.

</details>


### [195] [Crash Report Enhancement with Large Language Models: An Empirical Study](https://arxiv.org/abs/2509.13535)
*S M Farah Al Fahim,Md Nakhla Rafi,Zeyang Ma,Dong Jae Kim,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: LLM能通过堆栈跟踪和仓库代码增强崩溃报告，提升问题定位准确率和修复建议质量，Agentic-LLM表现更优。


<details>
  <summary>Details</summary>
Motivation: 崩溃报告在软件维护中至关重要，但许多报告缺乏开发者高效调试所需的诊断细节。

Method: 研究比较了两种增强策略：Direct-LLM（单次使用堆栈跟踪上下文）和Agentic-LLM（迭代探索仓库以获取更多证据）。

Result: 在492个真实崩溃报告数据集上，LLM增强的报告将Top-1问题定位准确率从10.6%提升至40.2-43.1%，并生成与开发者补丁相似的修复建议（CodeBLEU约56-57%）。

Conclusion: 研究表明，通过为大型语言模型提供堆栈跟踪和仓库代码，可以显著提升崩溃报告的实用性，使其更有利于调试。

Abstract: Crash reports are central to software maintenance, yet many lack the
diagnostic detail developers need to debug efficiently. We examine whether
large language models can enhance crash reports by adding fault locations,
root-cause explanations, and repair suggestions. We study two enhancement
strategies: Direct-LLM, a single-shot approach that uses stack-trace context,
and Agentic-LLM, an iterative approach that explores the repository for
additional evidence. On a dataset of 492 real-world crash reports, LLM-enhanced
reports improve Top-1 problem-localization accuracy from 10.6% (original
reports) to 40.2-43.1%, and produce suggested fixes that closely resemble
developer patches (CodeBLEU around 56-57%). Both our manual evaluations and
LLM-as-a-judge assessment show that Agentic-LLM delivers stronger root-cause
explanations and more actionable repair guidance. A user study with 16
participants further confirms that enhanced reports make crashes easier to
understand and resolve, with the largest improvement in repair guidance. These
results indicate that supplying LLMs with stack traces and repository code
yields enhanced crash reports that are substantially more useful for debugging.

</details>


### [196] [GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?](https://arxiv.org/abs/2509.13650)
*Amena Amro,Manar H. Alalfi*

Main category: cs.SE

TL;DR: 研究发现GitHub Copilot的代码审查功能在检测关键安全漏洞方面效果有限，主要关注低风险问题，强调了专用安全工具的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着软件开发实践中AI工具的普及，确保这些工具支持安全编码变得至关重要。

Method: 使用来自多种编程语言和应用领域的标记漏洞代码样本集，系统评估Copilot识别和反馈常见安全缺陷的能力。

Result: Copilot的代码审查经常未能检测到SQL注入、跨站脚本（XSS）和不安全反序列化等关键漏洞，反馈主要针对编码风格和排版错误等低严重性问题。

Conclusion: GitHub Copilot的代码审查功能在检测关键安全漏洞方面表现不佳，主要关注低严重性问题，揭示了AI辅助代码审查的感知能力与实际效果之间的显著差距，强调了专用安全工具和手动代码审计的必要性。

Abstract: As software development practices increasingly adopt AI-powered tools,
ensuring that such tools can support secure coding has become critical. This
study evaluates the effectiveness of GitHub Copilot's recently introduced code
review feature in detecting security vulnerabilities. Using a curated set of
labeled vulnerable code samples drawn from diverse open-source projects
spanning multiple programming languages and application domains, we
systematically assessed Copilot's ability to identify and provide feedback on
common security flaws. Contrary to expectations, our results reveal that
Copilot's code review frequently fails to detect critical vulnerabilities such
as SQL injection, cross-site scripting (XSS), and insecure deserialization.
Instead, its feedback primarily addresses low-severity issues, such as coding
style and typographical errors. These findings expose a significant gap between
the perceived capabilities of AI-assisted code review and its actual
effectiveness in supporting secure development practices. Our results highlight
the continued necessity of dedicated security tools and manual code audits to
ensure robust software security.

</details>


### [197] [A Regression Testing Framework with Automated Assertion Generation for Machine Learning Notebooks](https://arxiv.org/abs/2509.13656)
*Yingao Elaine Yao,Vedant Nimje,Varun Viswanath,Saikat Dutta*

Main category: cs.SE

TL;DR: NBTest是一个为机器学习笔记本设计的回归测试框架，提供自动化断言生成和执行，显著提升笔记本的可靠性和可维护性，用户评价积极。


<details>
  <summary>Details</summary>
Motivation: 机器学习笔记本在持续开发中缺乏测试支持，导致许多不易察觉的错误和性能回归问题。

Method: NBTest是一个回归测试框架，提供断言API库和JupyterLab插件，支持在笔记本中编写和执行单元级断言，并首次实现了自动化生成断言的方法。

Result: 在592个Kaggle笔记本上，NBTest平均每个笔记本生成35.75个断言，突变得分为0.57，能有效捕捉回归错误，并通过统计技术减少断言的不稳定性。用户研究表明NBTest直观且有用。

Conclusion: NBTest通过提供回归测试框架和自动化断言生成，显著提高了机器学习笔记本的可靠性和可维护性，同时减轻了开发者的负担。

Abstract: Notebooks have become the de-facto choice for data scientists and machine
learning engineers for prototyping and experimenting with machine learning (ML)
pipelines. Notebooks provide an interactive interface for code, data, and
visualization. However, notebooks provide very limited support for testing.
Thus, during continuous development, many subtle bugs that do not lead to
crashes often go unnoticed and cause silent errors that manifest as performance
regressions.
  To address this, we introduce NBTest - the first regression testing framework
that allows developers to write cell-level assertions in notebooks and run such
notebooks in pytest or in continuous integration (CI) pipelines. NBTest offers
a library of assertion APIs, and a JupyterLab plugin that enables executing
assertions. We also develop the first automated approach for generating
cell-level assertions for key components in ML notebooks, such as data
processing, model building, and model evaluation. NBTest aims to improve the
reliability and maintainability of ML notebooks without adding developer
burden.
  We evaluate NBTest on 592 Kaggle notebooks. Overall, NBTest generates 21163
assertions (35.75 on average per notebook). The generated assertions obtain a
mutation score of 0.57 in killing ML-specific mutations. NBTest can catch
regression bugs in previous versions of the Kaggle notebooks using assertions
generated for the latest versions. Because ML pipelines involve non
deterministic computations, the assertions can be flaky. Hence, we also show
how NBTest leverages statistical techniques to minimize flakiness while
retaining high fault-detection effectiveness. NBTest has been adopted in the CI
of a popular ML library. Further, we perform a user study with 17 participants
that shows that notebook users find NBTest intuitive (Rating 4.3/5) and useful
in writing assertions and testing notebooks (Rating 4.24/5).

</details>


### [198] [Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and Personality-Driven Variations](https://arxiv.org/abs/2509.13680)
*Wei Ma,Yixiao Yang,Jingquan Ge,Xiaofei Xie,Lingxiao Jiang*

Main category: cs.SE

TL;DR: PromptSE框架评估代码生成模型对提示措辞的敏感性，揭示性能与稳定性解耦的现象，为模型选择和部署提供稳定性量化工具。


<details>
  <summary>Details</summary>
Motivation: 代码生成模型对提示措辞的敏感性尚未充分研究，而现有基准大多仅关注峰值性能。

Method: 提出PromptSE框架，通过情感和个性模板生成语义相同的提示变体，并使用概率感知连续评分或二进制通过率评估稳定性。

Result: 在14个模型上的研究表明，性能和稳定性是基本解耦的优化目标，并揭示了挑战常见模型稳健性假设的架构和规模相关模式。

Conclusion: PromptSE框架将提示稳定性定位为与性能和公平性并行的评估维度，有助于构建更可信的AI辅助软件开发工具。

Abstract: Code generation models are widely used in software development, yet their
sensitivity to prompt phrasing remains under-examined. Identical requirements
expressed with different emotions or communication styles can yield divergent
outputs, while most benchmarks emphasize only peak performance. We present
PromptSE (Prompt Sensitivity Evaluation), a framework that creates semantically
equivalent prompt variants with emotion and personality templates, and that
evaluates stability using probability aware continuous scoring or using binary
pass rates when logits are unavailable. The results are aggregated into a
proposed area under curve metric (AUC-E) for cross model comparison. Across 14
models from three families (Llama, Qwen, and DeepSeek), our study shows that
performance and stability behave as largely decoupled optimization objectives,
and it reveals architectural and scale related patterns that challenge common
assumptions about model robustness. The framework supports rapid screening for
closed-source models as well as detailed stability analysis in research
settings. PromptSE enables practitioners to quantify performance stability
trade offs for deployment and model selection, positioning prompt stability as
a complementary evaluation dimension alongside performance and fairness, and
contributing to more trustworthy AI-assisted software development tools.

</details>


### [199] [Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning](https://arxiv.org/abs/2509.13755)
*Zhaoyang Chu,Yao Wan,Zhikun Zhang,Di Wang,Zhou Yang,Hongyu Zhang,Pan Zhou,Xuanhua Shi,Hai Jin,David Lo*

Main category: cs.SE

TL;DR: 提出CodeEraser，通过机器遗忘技术消除CLMs中的敏感记忆，避免全模型重新训练，实验验证其有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决CLMs中敏感信息记忆导致的隐私漏洞，避免全模型重新训练的高计算成本。

Method: 通过机器遗忘技术，特别是梯度上升为基础的遗忘方法（包括vanilla、constraint-based及CodeEraser变体），选择性消除代码中的敏感记忆片段。

Result: CodeEraser在三种CLMs（CodeParrot、CodeGen-Mono、Qwen2.5-Coder）上验证了其有效性和效率。

Conclusion: CodeEraser提供了一种有效且高效的方法来消除CLMs中的敏感记忆，同时保持模型的功能性。

Abstract: While Code Language Models (CLMs) have demonstrated superior performance in
software engineering tasks such as code generation and summarization, recent
empirical studies reveal a critical privacy vulnerability: these models exhibit
unintended memorization of sensitive training data, enabling verbatim
reproduction of confidential information when specifically prompted. To address
this issue, several approaches, including training data de-duplication and
differential privacy augmentation, have been proposed. However, these methods
require full-model retraining for deployed CLMs, which incurs substantial
computational costs. In this paper, we aim to answer the following research
question: Can sensitive information memorized by CLMs be erased effectively and
efficiently?
  We conduct a pioneering investigation into erasing sensitive memorization in
CLMs through machine unlearning - a post-hoc modification method that removes
specific information from trained models without requiring full retraining.
Specifically, we first quantify the memorization risks of sensitive data within
CLM training datasets and curate a high-risk dataset of 50,000 sensitive
memorized samples as unlearning targets. We study two widely used gradient
ascent-based unlearning approaches: the vanilla and constraint-based methods,
and introduce CodeEraser, an advanced variant that selectively unlearns
sensitive memorized segments in code while preserving the structural integrity
and functional correctness of the surrounding code. Extensive experiments on
three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,
validate the effectiveness and efficiency of CodeEraser in erasing targeted
sensitive memorization while maintaining model utility.

</details>


### [200] [A Study on Thinking Patterns of Large Reasoning Models in Code Generation](https://arxiv.org/abs/2509.13758)
*Kevin Halim,Sin G. Teo,Ruitao Feng,Zhenpeng Chen,Yang Gu,Chong Wang,Yang Liu*

Main category: cs.SE

TL;DR: 本文系统分析了大型推理模型（LRMs）在代码生成中的推理行为，提出了15种推理动作的分类法，并发现不同模型有不同的推理模式，某些动作与代码正确性密切相关。


<details>
  <summary>Details</summary>
Motivation: 尽管LRMs在代码生成方面表现出多步推理能力，但对其推理模式及其对生成代码影响的系统性分析仍不足。本研究旨在填补这一空白。

Method: 通过向不同规模的先进LRMs提出代码生成任务，并应用开放编码手动标注推理痕迹，从中推导出LRM推理行为的分类法。

Result: 研究发现LRMs通常遵循类似人类的编码流程，复杂任务会引发更多动作如脚手架生成和缺陷检测。不同模型如Qwen3和DeepSeek-R1-7B表现出不同的推理模式。推理动作如单元测试创建与代码正确性密切相关。

Conclusion: 本研究揭示了大型推理模型（LRMs）在代码生成中的推理行为，并提出了15种推理动作的分类法。研究结果为改进自动代码生成提供了实践指导。

Abstract: Currently, many large language models (LLMs) are utilized for software
engineering tasks such as code generation. The emergence of more advanced
models known as large reasoning models (LRMs), such as OpenAI's o3, DeepSeek
R1, and Qwen3. They have demonstrated the capability of performing multi-step
reasoning. Despite the advancement in LRMs, little attention has been paid to
systematically analyzing the reasoning patterns these models exhibit and how
such patterns influence the generated code. This paper presents a comprehensive
study aimed at investigating and uncovering the reasoning behavior of LRMs
during code generation. We prompted several state-of-the-art LRMs of varying
sizes with code generation tasks and applied open coding to manually annotate
the reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning
behaviors, encompassing 15 reasoning actions across four phases.
  Our empirical study based on the taxonomy reveals a series of findings.
First, we identify common reasoning patterns, showing that LRMs generally
follow a human-like coding workflow, with more complex tasks eliciting
additional actions such as scaffolding, flaw detection, and style checks.
Second, we compare reasoning across models, finding that Qwen3 exhibits
iterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like
approach. Third, we analyze the relationship between reasoning and code
correctness, showing that actions such as unit test creation and scaffold
generation strongly support functional outcomes, with LRMs adapting strategies
based on task context. Finally, we evaluate lightweight prompting strategies
informed by these findings, demonstrating the potential of context- and
reasoning-oriented prompts to improve LRM-generated code. Our results offer
insights and practical implications for advancing automatic code generation.

</details>


### [201] [Who is Introducing the Failure? Automatically Attributing Failures of Multi-Agent Systems via Spectrum Analysis](https://arxiv.org/abs/2509.13782)
*Yu Ge,Linna Xie,Zhong Li,Yu Pei,Tian Zhang*

Main category: cs.SE

TL;DR: FAMAS 是一种针对多智能体系统的故障归因方法，通过轨迹重放和频谱分析，显著提高了故障定位效率，并在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MASs）在自动化复杂任务中表现出色，但故障归因问题尚未充分研究且调试困难，阻碍了系统改进。

Method: FAMAS 通过重复执行多智能体系统的轨迹并进行抽象分析，结合专门设计的可疑度公式，整合了智能体行为组和动作行为组两个关键因素。

Result: 在 Who and When 基准测试中，FAMAS 优于 12 种基线方法，表现出卓越的性能。

Conclusion: FAMAS 是一种基于频谱的故障归因方法，通过系统化的轨迹重放和抽象分析，显著提高了多智能体系统中的故障定位效率。

Abstract: Large Language Model Powered Multi-Agent Systems (MASs) are increasingly
employed to automate complex real-world problems, such as programming and
scientific discovery. Despite their promising, MASs are not without their
flaws. However, failure attribution in MASs - pinpointing the specific agent
actions responsible for failures - remains underexplored and labor-intensive,
posing significant challenges for debugging and system improvement. To bridge
this gap, we propose FAMAS, the first spectrum-based failure attribution
approach for MASs, which operates through systematic trajectory replay and
abstraction, followed by spectrum analysis.The core idea of FAMAS is to
estimate, from variations across repeated MAS executions, the likelihood that
each agent action is responsible for the failure. In particular, we propose a
novel suspiciousness formula tailored to MASs, which integrates two key factor
groups, namely the agent behavior group and the action behavior group, to
account for the agent activation patterns and the action activation patterns
within the execution trajectories of MASs. Through expensive evaluations
against 12 baselines on the Who and When benchmark, FAMAS demonstrates superior
performance by outperforming all the methods in comparison.

</details>


### [202] [Trace Sampling 2.0: Code Knowledge Enhanced Span-level Sampling for Distributed Tracing](https://arxiv.org/abs/2509.13852)
*Yulun Wu,Guangba Yu,Zhihan Jiang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: Autoscope是一种span级别采样方法，通过静态分析保留关键span，显著减少存储开销并提升追踪效率，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 分布式追踪在微服务系统中是重要的诊断工具，但大量追踪数据给后端存储带来负担。传统采样方法常丢弃有价值信息，包括用于对比分析的正常追踪数据。

Method: 设计并实现了Autoscope，一种基于静态分析提取执行逻辑的span级别采样方法，确保在不破坏结构完整性的情况下保留关键span。

Result: 在两种开源微服务上评估Autoscope，结果显示其将追踪大小减少81.2%，同时保持98.1%的错误span覆盖率，优于现有追踪级别采样方法。此外，在根因分析中平均提升8.3%。

Conclusion: Autoscope通过span级别的采样方法显著提升了微服务系统的可观测性和存储效率，为性能监控提供了强有力的解决方案。

Abstract: Distributed tracing is an essential diagnostic tool in microservice systems,
but the sheer volume of traces places a significant burden on backend storage.
A common approach to mitigating this issue is trace sampling, which selectively
retains traces based on specific criteria, often preserving only anomalous
ones. However, this method frequently discards valuable information, including
normal traces that are essential for comparative analysis. To address this
limitation, we introduce Trace Sampling 2.0, which operates at the span level
while maintaining trace structure consistency. This approach allows for the
retention of all traces while significantly reducing storage overhead. Based on
this concept, we design and implement Autoscope, a span-level sampling method
that leverages static analysis to extract execution logic, ensuring that
critical spans are preserved without compromising structural integrity. We
evaluated Autoscope on two open-source microservices. Our results show that it
reduces trace size by 81.2% while maintaining 98.1% faulty span coverage,
outperforming existing trace-level sampling methods. Furthermore, we
demonstrate its effectiveness in root cause analysis, achieving an average
improvement of 8.3%. These findings indicate that Autoscope can significantly
enhance observability and storage efficiency in microservices, offering a
robust solution for performance monitoring.

</details>


### [203] [Are Prompts All You Need? Evaluating Prompt-Based Large Language Models (LLM)s for Software Requirements Classification](https://arxiv.org/abs/2509.13868)
*Manal Binkhonain,Reem Alfayaz*

Main category: cs.SE

TL;DR: 研究发现，基于提示的大型语言模型（尤其是少样本提示）能有效减少数据需求，并在分类任务中表现优于传统监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 准确的分类可以降低风险并提高软件质量。现有的监督学习模型需要大量标注数据，成本高、创建慢且依赖领域，泛化能力差，通常需要为每个任务重新训练。

Method: 本研究测试了几种模型和提示风格（零样本、少样本、人物设定和思维链），并在多个任务上进行了基准测试，比较了最佳LLM设置与微调Transformer基线的性能。

Result: 结果表明，基于提示的LLM，尤其是少样本提示，可以匹配或超过基线。添加人物设定或人物设定加思维链可以进一步提升性能。

Conclusion: 基于提示的大型语言模型是一种实用且可扩展的选择，减少了对大规模标注数据的依赖，并能在不同任务间提高泛化能力。

Abstract: Requirements classification assigns natural language requirements to
predefined classes, such as functional and non functional. Accurate
classification reduces risk and improves software quality. Most existing models
rely on supervised learning, which needs large labeled data that are costly,
slow to create, and domain dependent; they also generalize poorly and often
require retraining for each task. This study tests whether prompt based large
language models can reduce data needs. We benchmark several models and
prompting styles (zero shot, few shot, persona, and chain of thought) across
multiple tasks on two English datasets, PROMISE and SecReq. For each task we
compare model prompt configurations and then compare the best LLM setups with a
strong fine tuned transformer baseline. Results show that prompt based LLMs,
especially with few shot prompts, can match or exceed the baseline. Adding a
persona, or persona plus chain of thought, can yield further gains. We conclude
that prompt based LLMs are a practical and scalable option that reduces
dependence on large annotations and can improve generalizability across tasks.

</details>


### [204] [Mind the Ethics! The Overlooked Ethical Dimensions of GenAI in Software Modeling Education](https://arxiv.org/abs/2509.13896)
*Shalini Chakraborty,Lola Burgueño,Nathalie Moreno,Javier Troya,Paula Muñoz*

Main category: cs.SE

TL;DR: 系统综述发现GenAI在软件建模教育中的伦理研究严重不足，呼吁建立伦理框架。


<details>
  <summary>Details</summary>
Motivation: 探讨GenAI在软件建模教育中的伦理影响，如责任、公平、透明、多样性和包容性等，以填补当前研究的空白。

Method: 通过对六大计算机科学数字图书馆（ACM Digital Library、IEEE Xplore、Scopus、ScienceDirect、SpringerLink和Web of Science）的系统文献综述，筛选出1,386篇文献，最终仅三篇涉及伦理问题。

Result: 研究发现，伦理讨论极其匮乏，仅三篇文献符合条件，表明该领域亟需伦理框架。

Conclusion: 论文指出，生成式人工智能（GenAI）在软件建模教育中的伦理考量严重不足，仅有三篇文献明确讨论此议题。这凸显了在该领域建立结构化伦理框架的紧迫性。

Abstract: Generative Artificial Intelligence (GenAI) is rapidly gaining momentum in
software modeling education, embraced by both students and educators. As GenAI
assists with interpreting requirements, formalizing models, and translating
students' mental models into structured notations, it increasingly shapes core
learning outcomes such as domain comprehension, diagrammatic thinking, and
modeling fluency without clear ethical oversight or pedagogical guidelines.
Yet, the ethical implications of this integration remain underexplored.
  In this paper, we conduct a systematic literature review across six major
digital libraries in computer science (ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, SpringerLink, and Web of Science). Our aim is to
identify studies discussing the ethical aspects of GenAI in software modeling
education, including responsibility, fairness, transparency, diversity, and
inclusion among others.
  Out of 1,386 unique papers initially retrieved, only three explicitly
addressed ethical considerations. This scarcity highlights the critical absence
of ethical discourse surrounding GenAI in modeling education and raises urgent
questions about the responsible integration of AI in modeling curricula, as
well as it evinces the pressing need for structured ethical frameworks in this
emerging educational landscape. We examine these three studies and explore the
emerging research opportunities as well as the challenges that have arisen in
this field.

</details>


### [205] [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
*Simiao Liu,Fang Liu,Liehao Li,Xin Tan,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: 研究分析了自动化代码修复中的失败模式，提出了协作式Expert-Executor框架，显著提升了问题解决率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM-based代理工具在自动化问题解决中表现不佳，且现有评估主要关注聚合问题解决率，难以诊断模型弱点或指导针对性改进。

Method: 研究首先分析了三种SOTA工具在SWE-Bench-Verified任务中的表现，随后通过系统手动分析150个失败实例，构建了一个包含3个主要阶段、9个主类别和25个子类别的失败模式分类法。基于此，提出了协作式的Expert-Executor框架。

Result: 研究发现，代理式架构的失败主要源于推理错误和认知死锁。提出的Expert-Executor框架解决了22.2%之前单代理无法处理的问题。

Conclusion: 研究通过提出协作式的Expert-Executor框架，显著提升了单代理在SWE-Bench-Verified任务中的问题解决率，为构建更鲁棒的代理提供了诊断性评估和协作设计的路径。

Abstract: Automated issue solving seeks to autonomously identify and repair defective
code snippets across an entire codebase. SWE-Bench has emerged as the most
widely adopted benchmark for evaluating progress in this area. While LLM-based
agentic tools show great promise, they still fail on a substantial portion of
tasks. Moreover, current evaluations primarily report aggregate issue-solving
rates, which obscure the underlying causes of success and failure, making it
challenging to diagnose model weaknesses or guide targeted improvements. To
bridge this gap, we first analyze the performance and efficiency of three SOTA
tools, spanning both pipeline-based and agentic architectures, in automated
issue solving tasks of SWE-Bench-Verified under varying task characteristics.
Furthermore, to move from high-level performance metrics to underlying cause
analysis, we conducted a systematic manual analysis of 150 failed instances.
From this analysis, we developed a comprehensive taxonomy of failure modes
comprising 3 primary phases, 9 main categories, and 25 fine-grained
subcategories. Then we systematically analyze the distribution of the
identified failure modes, the results reveal distinct failure fingerprints
between the two architectural paradigms, with the majority of agentic failures
stemming from flawed reasoning and cognitive deadlocks. Motivated by these
insights, we propose a collaborative Expert-Executor framework. It introduces a
supervisory Expert agent tasked with providing strategic oversight and
course-correction for a primary Executor agent. This architecture is designed
to correct flawed reasoning and break the cognitive deadlocks that frequently
lead to failure. Experiments show that our framework solves 22.2% of previously
intractable issues for a leading single agent. These findings pave the way for
building more robust agents through diagnostic evaluation and collaborative
design.

</details>


### [206] [Evaluating Classical Software Process Models as Coordination Mechanisms for LLM-Based Software Generation](https://arxiv.org/abs/2509.13942)
*Duc Minh Ha,Phu Trac Kien,Tho Quan,Anh Nguyen-Duc*

Main category: cs.SE

TL;DR: 研究发现传统软件流程可作为LLM多代理系统的协调框架，但各有优劣：瀑布模型高效，V模型冗长，敏捷模型质量高但成本高。


<details>
  <summary>Details</summary>
Motivation: 探索传统软件开发流程如何作为协调框架适应基于LLM的多代理系统，并研究其对代码质量、成本和生产率的影响。

Method: 我们执行了11个不同的软件项目，采用三种流程模型和四种GPT变体，共132次运行。每个输出均使用标准化指标进行评估，包括规模（文件、代码行数）、成本（执行时间、令牌使用量）和质量（代码异味、AI和人工检测到的错误）。

Result: 流程模型和LLM选择均显著影响系统性能。瀑布模型效率最高，V模型生成的代码最冗长，而敏捷模型实现了最高的代码质量，但计算成本更高。

Conclusion: 经典软件流程可以有效地在基于LLM的多代理系统中实例化，但每种流程在质量、成本和适应性方面都有权衡。流程选择应反映项目目标，无论是优先考虑效率、稳健性还是结构化验证。

Abstract: [Background] Large Language Model (LLM)-based multi-agent systems (MAS) are
transforming software development by enabling autonomous collaboration.
Classical software processes such asWaterfall, V-Model, and Agile offer
structured coordination patterns that can be repurposed to guide these agent
interactions. [Aims] This study explores how traditional software development
processes can be adapted as coordination scaffolds for LLM based MAS and
examines their impact on code quality, cost, and productivity. [Method] We
executed 11 diverse software projects under three process models and four GPT
variants, totaling 132 runs. Each output was evaluated using standardized
metrics for size (files, LOC), cost (execution time, token usage), and quality
(code smells, AI- and human detected bugs). [Results] Both process model and
LLM choice significantly affected system performance. Waterfall was most
efficient, V-Model produced the most verbose code, and Agile achieved the
highest code quality, albeit at higher computational cost. [Conclusions]
Classical software processes can be effectively instantiated in LLM-based MAS,
but each entails trade-offs across quality, cost, and adaptability. Process
selection should reflect project goals, whether prioritizing efficiency,
robustness, or structured validation.

</details>


### [207] [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
*Kerui Huang,Shuhan Liu,Xing Hu,Tongtong Xu,Lingfeng Bao,Xin Xia*

Main category: cs.SE

TL;DR: SEER是一种自适应框架，通过压缩CoT推理步骤减少计算开销，同时保持准确性，显著提升了LLMs在资源受限环境下的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究揭示了过长的CoT推理步骤可能导致输出截断、准确性下降和高延迟，挑战了‘更长推理步骤更好’的假设，因此需要自适应控制CoT的方法。

Method: 提出了SEER（Self-Enhancing Efficient Reasoning）框架，结合Best-of-N采样和任务感知自适应过滤，动态调整阈值以减少冗余和计算开销。

Result: SEER平均缩短CoT推理步骤42.1%，减少截断情况，提高准确性，并消除大多数无限循环。

Conclusion: SEER框架通过自适应压缩CoT推理步骤，显著减少了计算开销，同时保持了准确性，为资源受限环境下的LLMs提供了更高效和鲁棒的解决方案。

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
prompting intermediate steps, improving accuracy and robustness in arithmetic,
logic, and commonsense tasks. However, this benefit comes with high
computational costs: longer outputs increase latency, memory usage, and
KV-cache demands. These issues are especially critical in software engineering
tasks where concise and deterministic outputs are required. To investigate
these trade-offs, we conduct an empirical study based on code generation
benchmarks. The results reveal that longer CoT does not always help. Excessive
reasoning often causes truncation, accuracy drops, and latency up to five times
higher, with failed outputs consistently longer than successful ones. These
findings challenge the assumption that longer reasoning is inherently better
and highlight the need for adaptive CoT control. Motivated by this, we propose
SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that
compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with
task-aware adaptive filtering, dynamically adjusting thresholds based on
pre-inference outputs to reduce verbosity and computational overhead. We then
evaluate SEER on three software engineering tasks and one math task. On
average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,
and eliminates most infinite loops. These results demonstrate SEER as a
practical method to make CoT-enhanced LLMs more efficient and robust, even
under resource constraints.

</details>
