{"id": "2509.14086", "categories": ["cs.OS"], "pdf": "https://arxiv.org/pdf/2509.14086", "abs": "https://arxiv.org/abs/2509.14086", "authors": ["Qianlong Duan", "Bide Hao", "Fan Zhou", "Chen Fei", "Shichun Yang"], "title": "A Task Equalization Allocation Algorithm Incorporating Blocking Estimation and Resource Similarity Analysis for Vehicle Control Real-Time Systems", "comment": null, "summary": "In multi-core real-time vehicle control systems, synchronization blocking and\nresource contention pose critical challenges due to increasing task parallelism\nand shared resource access. These issues significantly degrade system\nschedulability and real-time performance, as traditional task allocation\nalgorithms often overlook blocking impacts, leading to high scheduling failure\nrates under heavy loads. To address this, we propose the BR-WFD algorithm,\nwhich integrates blocking time estimation and resource similarity analysis. The\nalgorithm minimizes global blocking overhead by prioritizing tasks with high\nsynchronization sensitivity and aggregating shared-resource-accessing tasks\nonto the same core. Extensive simulations show that BR-WFD reduces required\nprocessor cores by 11\\% to 28\\% and maintains a 15\\% to 20\\% higher schedulable\nratio compared to traditional methods under high-load and resource-competitive\nscenarios. This demonstrates its effectiveness in enhancing real-time\nperformance and resource efficiency for multi-core task scheduling in\nintelligent driving systems.", "AI": {"tldr": "BR-WFD\u7b97\u6cd5\u901a\u8fc7\u4f18\u5316\u4efb\u52a1\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u591a\u6838\u8f66\u8f86\u63a7\u5236\u7cfb\u7edf\u7684\u5b9e\u65f6\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u591a\u6838\u5b9e\u65f6\u8f66\u8f86\u63a7\u5236\u7cfb\u7edf\u4e2d\uff0c\u540c\u6b65\u963b\u585e\u548c\u8d44\u6e90\u7ade\u4e89\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u4e86\u7cfb\u7edf\u53ef\u8c03\u5ea6\u6027\u548c\u5b9e\u65f6\u6027\u80fd\uff0c\u4f20\u7edf\u4efb\u52a1\u5206\u914d\u7b97\u6cd5\u5f80\u5f80\u5ffd\u7565\u963b\u585e\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86BR-WFD\u7b97\u6cd5\uff0c\u7ed3\u5408\u963b\u585e\u65f6\u95f4\u4f30\u8ba1\u548c\u8d44\u6e90\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u4f18\u5148\u8c03\u5ea6\u540c\u6b65\u654f\u611f\u6027\u9ad8\u7684\u4efb\u52a1\uff0c\u5e76\u5c06\u5171\u4eab\u8d44\u6e90\u8bbf\u95ee\u4efb\u52a1\u805a\u96c6\u5230\u540c\u4e00\u6838\u5fc3\u3002", "result": "\u4eff\u771f\u663e\u793a\uff0cBR-WFD\u5728\u9ad8\u8d1f\u8f7d\u548c\u8d44\u6e90\u7ade\u4e89\u573a\u666f\u4e0b\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u51cf\u5c11\u4e8611%\u81f328%\u7684\u5904\u7406\u5668\u6838\u5fc3\u9700\u6c42\uff0c\u5e76\u4fdd\u630115%\u81f320%\u7684\u66f4\u9ad8\u53ef\u8c03\u5ea6\u6bd4\u4f8b\u3002", "conclusion": "BR-WFD\u7b97\u6cd5\u901a\u8fc7\u6574\u5408\u963b\u585e\u65f6\u95f4\u4f30\u8ba1\u548c\u8d44\u6e90\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6838\u5b9e\u65f6\u8f66\u8f86\u63a7\u5236\u7cfb\u7edf\u7684\u8c03\u5ea6\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2509.13325", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13325", "abs": "https://arxiv.org/abs/2509.13325", "authors": ["Matteo Zanotto", "Leonardo Vicentini", "Redi Vreto", "Francesco Lumpp", "Diego Braga", "Sandro Fiore"], "title": "A User-centric Kubernetes-based Architecture for Green Cloud Computing", "comment": null, "summary": "To meet the increasing demand for cloud computing services, the scale and\nnumber of data centers keeps increasing worldwide. This growth comes at the\ncost of increased electricity consumption, which directly correlates to CO2\nemissions, the main driver of climate change. As such, researching ways to\nreduce cloud computing emissions is more relevant than ever. However, although\ncloud providers are reportedly already working near optimal power efficiency,\nthey fail in providing precise sustainability reporting. This calls for further\nimprovements on the cloud computing consumer's side. To this end, in this paper\nwe propose a user-centric, Kubernetes-based architecture for green cloud\ncomputing. We implement a carbon intensity forecaster and we use it to schedule\nworkloads based on the availability of green energy, exploiting both regional\nand temporal variations to minimize emissions. We evaluate our system using\nreal-world traces of cloud workloads execution comparing the achieved carbon\nemission savings against a baseline round-robin scheduler. Our findings\nindicate that our system can achieve up to a 13% reduction in emissions in a\nstrict scenario with heavy limitations on the available resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKubernetes\u7684\u7eff\u8272\u4e91\u8ba1\u7b97\u67b6\u6784\uff0c\u901a\u8fc7\u78b3\u5f3a\u5ea6\u9884\u6d4b\u548c\u7eff\u8272\u80fd\u6e90\u8c03\u5ea6\uff0c\u5b9e\u73b0\u4e8613%\u7684\u78b3\u6392\u653e\u51cf\u5c11\u3002", "motivation": "\u968f\u7740\u6570\u636e\u4e2d\u5fc3\u89c4\u6a21\u548c\u6570\u91cf\u7684\u589e\u52a0\uff0c\u7535\u529b\u6d88\u8017\u548c\u78b3\u6392\u653e\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u5c3d\u7ba1\u4e91\u670d\u52a1\u63d0\u4f9b\u5546\u5728\u80fd\u6548\u65b9\u9762\u5df2\u63a5\u8fd1\u6700\u4f18\uff0c\u4f46\u5728\u53ef\u6301\u7eed\u6027\u62a5\u544a\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4ece\u7528\u6237\u4fa7\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u78b3\u5f3a\u5ea6\u9884\u6d4b\u5668\uff0c\u5e76\u57fa\u4e8e\u7eff\u8272\u80fd\u6e90\u7684\u53ef\u7528\u6027\u8c03\u5ea6\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5229\u7528\u533a\u57df\u548c\u65f6\u95f4\u5dee\u5f02\u6700\u5c0f\u5316\u78b3\u6392\u653e\u3002", "result": "\u5728\u4e25\u683c\u8d44\u6e90\u9650\u5236\u7684\u573a\u666f\u4e0b\uff0c\u8be5\u7cfb\u7edf\u53ef\u5b9e\u73b0\u9ad8\u8fbe13%\u7684\u78b3\u6392\u653e\u51cf\u5c11\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u3001\u57fa\u4e8eKubernetes\u7684\u7eff\u8272\u4e91\u8ba1\u7b97\u67b6\u6784\uff0c\u901a\u8fc7\u5229\u7528\u78b3\u5f3a\u5ea6\u9884\u6d4b\u5668\u548c\u7eff\u8272\u80fd\u6e90\u53ef\u7528\u6027\u8c03\u5ea6\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5b9e\u73b0\u4e86\u78b3\u6392\u653e\u7684\u663e\u8457\u51cf\u5c11\u3002"}}
{"id": "2509.13575", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13575", "abs": "https://arxiv.org/abs/2509.13575", "authors": ["Benjamin Wilfong", "Anand Radhakrishnan", "Henry A. Le Berre", "Tanush Prathi", "Stephen Abbott", "Spencer H. Bryngelson"], "title": "Testing and benchmarking emerging supercomputers via the MFC flow solver", "comment": "9 pages, 3 figures", "summary": "Deploying new supercomputers requires testing and evaluation via application\ncodes. Portable, user-friendly tools enable evaluation, and the Multicomponent\nFlow Code (MFC), a computational fluid dynamics (CFD) code, addresses this\nneed. MFC is adorned with a toolchain that automates input generation,\ncompilation, batch job submission, regression testing, and benchmarking. The\ntoolchain design enables users to evaluate compiler-hardware combinations for\ncorrectness and performance with limited software engineering experience. As\nwith other PDE solvers, wall time per spatially discretized grid point serves\nas a figure of merit. We present MFC benchmarking results for five generations\nof NVIDIA GPUs, three generations of AMD GPUs, and various CPU architectures,\nutilizing Intel, Cray, NVIDIA, AMD, and GNU compilers. These tests have\nrevealed compiler bugs and regressions on recent machines such as Frontier and\nEl Capitan. MFC has benchmarked approximately 50 compute devices and 5 flagship\nsupercomputers.", "AI": {"tldr": "MFC\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u5177\u94fe\u7b80\u5316\u4e86\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u8bc4\u4f30\u6d41\u7a0b\uff0c\u6d4b\u8bd5\u4e86\u591a\u79cd\u786c\u4ef6\u548c\u7f16\u8bd1\u5668\u7ec4\u5408\uff0c\u53d1\u73b0\u4e86\u6f5c\u5728\u95ee\u9898\u3002", "motivation": "\u90e8\u7f72\u65b0\u8d85\u7ea7\u8ba1\u7b97\u673a\u9700\u8981\u901a\u8fc7\u5e94\u7528\u4ee3\u7801\u8fdb\u884c\u6d4b\u8bd5\u548c\u8bc4\u4f30\uff0c\u800c\u4fbf\u643a\u3001\u7528\u6237\u53cb\u597d\u7684\u5de5\u5177\u53ef\u4ee5\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "MFC\u914d\u5907\u4e86\u81ea\u52a8\u5316\u5de5\u5177\u94fe\uff0c\u652f\u6301\u7528\u6237\u8bc4\u4f30\u4e0d\u540c\u7f16\u8bd1\u5668-\u786c\u4ef6\u7ec4\u5408\u7684\u6b63\u786e\u6027\u548c\u6027\u80fd\uff0c\u65e0\u9700\u6df1\u539a\u7684\u8f6f\u4ef6\u5de5\u7a0b\u7ecf\u9a8c\u3002", "result": "MFC\u5728\u4e94\u4ee3NVIDIA GPU\u3001\u4e09\u4ee3AMD GPU\u53ca\u591a\u79cdCPU\u67b6\u6784\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u4e86\u7f16\u8bd1\u5668\u9519\u8bef\u548c\u56de\u5f52\u95ee\u9898\uff0c\u5e76\u6d4b\u8bd5\u4e86\u7ea650\u53f0\u8ba1\u7b97\u8bbe\u5907\u548c5\u53f0\u65d7\u8230\u8d85\u7ea7\u8ba1\u7b97\u673a\u3002", "conclusion": "MFC\u4f5c\u4e3a\u4e00\u79cd\u4fbf\u643a\u5f0f\u3001\u7528\u6237\u53cb\u597d\u7684\u5de5\u5177\uff0c\u6210\u529f\u6ee1\u8db3\u4e86\u8bc4\u4f30\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u9700\u6c42\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u5177\u94fe\u7b80\u5316\u4e86\u8f93\u5165\u751f\u6210\u3001\u7f16\u8bd1\u3001\u6279\u5904\u7406\u4f5c\u4e1a\u63d0\u4ea4\u3001\u56de\u5f52\u6d4b\u8bd5\u548c\u57fa\u51c6\u6d4b\u8bd5\u6d41\u7a0b\u3002"}}
{"id": "2509.13490", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13490", "abs": "https://arxiv.org/abs/2509.13490", "authors": ["Paul Bergeron", "Sandhya Aneja"], "title": "GRU-Based Learning for the Identification of Congestion Protocols in TCP Traffic", "comment": null, "summary": "This paper presents the identification of congestion control protocols TCP\nReno, TCP Cubic, TCP Vegas, and BBR on the Marist University campus, with an\naccuracy of 97.04% using a GRU-based learning model. We used a faster neural\nnetwork architecture on a more complex and competitive network in comparison to\nexisting work and achieved comparably high accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGRU\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u6548\u8bc6\u522b\u590d\u6742\u7f51\u7edc\u4e2d\u7684\u62e5\u585e\u63a7\u5236\u534f\u8bae\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe97.04%\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u590d\u6742\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u8868\u73b0\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u51c6\u786e\u8bc6\u522b\u4e0d\u540c\u7684\u62e5\u585e\u63a7\u5236\u534f\u8bae\u3002", "method": "\u91c7\u7528\u4e86\u57fa\u4e8eGRU\u7684\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u4e86\u66f4\u5feb\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5728\u66f4\u590d\u6742\u548c\u7ade\u4e89\u6027\u66f4\u5f3a\u7684\u7f51\u7edc\u73af\u5883\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5728Marist\u5927\u5b66\u6821\u56ed\u7f51\u7edc\u4e2d\uff0c\u6a21\u578b\u8bc6\u522b\u62e5\u585e\u63a7\u5236\u534f\u8bae\u7684\u51c6\u786e\u7387\u8fbe\u523097.04%\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u6210\u529f\u8bc6\u522b\u4e86Marist\u5927\u5b66\u6821\u56ed\u4e2d\u7684TCP Reno\u3001TCP Cubic\u3001TCP Vegas\u548cBBR\u7b49\u62e5\u585e\u63a7\u5236\u534f\u8bae\uff0c\u51c6\u786e\u7387\u8fbe\u523097.04%\uff0c\u9a8c\u8bc1\u4e86GRU\u5b66\u4e60\u6a21\u578b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.13327", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2509.13327", "abs": "https://arxiv.org/abs/2509.13327", "authors": ["Wissam Nakhle"], "title": "GTA -- An ATSP Method: Shifting the Bottleneck from Algorithm to RAM", "comment": null, "summary": "We present a scalable, high-performance algorithm that deterministically\nsolves large-scale instances of the Traveling Salesman problem (in its\nasymmetric version, ATSP) to optimality using commercially available computing\nhardware. By combining an efficient heuristic warm start, capable of achieving\nnear-optimality within seconds in some cases, with a subtour elimination\nstrategy that removes the need for traditional MTZ constraints, our approach\nconsistently resolves instances up to 5,000 nodes (approximately 25 million\nbinary variables) in record time on widely accessible computers, with eight\nlogical processors. We demonstrate reproducible results with convergence rates\ncomparable to those of high-performance computing frameworks. Real-time\niteration tracking and an adaptable interface allow seamless integration into\nscheduling workflows in logistics, bioinformatics, and astronomy. Designed to\nstreamline solutions to large-scale TSP problems across disciplines, our\napproach is benchmarked against widely used public datasets, offering a\ndeterministic, resource-efficient alternative to conventional solvers that rely\non supercomputing hardware. Our GTA (Gurobi Tabu Algorithm) algorithm is a\nfundamental shift of TSP solution bottleneck from algorithmic complexity to the\nunderlying hardware (RAM and system memory), which is a highly desirable\ncharacteristic.", "AI": {"tldr": "GTA\u7b97\u6cd5\u7ed3\u5408\u542f\u53d1\u5f0f\u9884\u70ed\u548c\u5b50\u8def\u5f84\u6d88\u9664\u7b56\u7565\uff0c\u5728\u5546\u7528\u786c\u4ef6\u4e0a\u9ad8\u6548\u6c42\u89e3\u5927\u89c4\u6a21ATSP\u95ee\u9898\uff0c\u4e3a\u591a\u9886\u57df\u63d0\u4f9b\u8d44\u6e90\u9ad8\u6548\u7684\u786e\u5b9a\u6027\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfTSP\u6c42\u89e3\u65b9\u6cd5\u4f9d\u8d56\u8d85\u7ea7\u8ba1\u7b97\u786c\u4ef6\u3001\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u5728\u5546\u7528\u786c\u4ef6\u4e0a\u9ad8\u6548\u3001\u786e\u5b9a\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u6ee1\u8db3\u8de8\u5b66\u79d1\u9886\u57df\u5bf9\u5927\u89c4\u6a21TSP\u95ee\u9898\u7684\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u9ad8\u6548\u542f\u53d1\u5f0f\u9884\u70ed\u548c\u5b50\u8def\u5f84\u6d88\u9664\u7b56\u7565\uff0c\u907f\u514d\u4f20\u7edfMTZ\u7ea6\u675f\uff0c\u8bbe\u8ba1GTA\u7b97\u6cd5\uff0c\u5229\u7528\u5546\u7528\u786c\u4ef6\uff08\u59828\u903b\u8f91\u5904\u7406\u5668\uff09\u5b9e\u73b0\u5927\u89c4\u6a21ATSP\u95ee\u9898\u7684\u786e\u5b9a\u6027\u6c42\u89e3\u3002", "result": "\u6210\u529f\u5904\u7406\u9ad8\u8fbe5,000\u8282\u70b9\uff08\u7ea62500\u4e07\u4e8c\u8fdb\u5236\u53d8\u91cf\uff09\u7684ATSP\u5b9e\u4f8b\uff0c\u6536\u655b\u901f\u5ea6\u4e0e\u9ad8\u6027\u80fd\u8ba1\u7b97\u6846\u67b6\u76f8\u5f53\uff0c\u5e76\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684GTA\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408\u9ad8\u6548\u542f\u53d1\u5f0f\u9884\u70ed\u548c\u5b50\u8def\u5f84\u6d88\u9664\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86ATSP\u95ee\u9898\u7684\u6c42\u89e3\u6548\u7387\uff0c\u4f7f\u5176\u5728\u5546\u7528\u786c\u4ef6\u4e0a\u4e5f\u80fd\u5904\u7406\u5927\u89c4\u6a21\u5b9e\u4f8b\uff0c\u4e3a\u7269\u6d41\u3001\u751f\u7269\u4fe1\u606f\u5b66\u548c\u5929\u6587\u5b66\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u8d44\u6e90\u9ad8\u6548\u7684\u786e\u5b9a\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13436", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13436", "abs": "https://arxiv.org/abs/2509.13436", "authors": ["Evan Eisinger", "Michael A. Heroux"], "title": "Is Research Software Science a Metascience?", "comment": "5 pages", "summary": "As research increasingly relies on computational methods, the reliability of\nscientific results depends on the quality, reproducibility, and transparency of\nresearch software. Ensuring these qualities is critical for scientific\nintegrity and discovery. This paper asks whether Research Software Science\n(RSS)--the empirical study of how research software is developed and\nused--should be considered a form of metascience, the science of science.\nClassification matters because it could affect recognition, funding, and\nintegration of RSS into research improvement. We define metascience and RSS,\ncompare their principles and objectives, and examine their overlaps. Arguments\nfor classification highlight shared commitments to reproducibility,\ntransparency, and empirical study of research processes. Arguments against\nportraying RSS as a specialized domain focused on a tool rather than the\nbroader scientific enterprise. Our analysis finds RSS advances core goals of\nmetascience, especially in computational reproducibility, and bridges\ntechnical, social, and cognitive aspects of research. Its classification\ndepends on whether one adopts a broad definition of metascience--any empirical\neffort to improve science--or a narrow one focused on systemic and\nepistemological structures. We argue RSS is best understood as a distinct\ninterdisciplinary domain that aligns with, and in some definitions fits within,\nmetascience. Recognizing it as such can strengthen its role in improving\nreliability, justify funding, and elevate software development in research\ninstitutions. Regardless of classification, applying scientific rigor to\nresearch software ensures the tools of discovery meet the standards of the\ndiscoveries themselves.", "AI": {"tldr": "\u7814\u7a76\u8f6f\u4ef6\u79d1\u5b66\uff08RSS\uff09\u5e94\u88ab\u89c6\u4e3a\u4e0e\u5143\u79d1\u5b66\u76f8\u5951\u5408\u7684\u8de8\u5b66\u79d1\u9886\u57df\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u7814\u7a76\u8f6f\u4ef6\u7684\u8d28\u91cf\u548c\u79d1\u5b66\u53ef\u9760\u6027\u3002", "motivation": "\u968f\u7740\u7814\u7a76\u8d8a\u6765\u8d8a\u4f9d\u8d56\u8ba1\u7b97\u65b9\u6cd5\uff0c\u79d1\u5b66\u7ed3\u679c\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u7814\u7a76\u8f6f\u4ef6\u7684\u8d28\u91cf\u3001\u53ef\u91cd\u590d\u6027\u548c\u900f\u660e\u5ea6\u3002\u786e\u4fdd\u8fd9\u4e9b\u7279\u6027\u5bf9\u79d1\u5b66\u8bda\u4fe1\u548c\u53d1\u73b0\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u5143\u79d1\u5b66\u548c\u7814\u7a76\u8f6f\u4ef6\u79d1\u5b66\uff0c\u6bd4\u8f83\u5b83\u4eec\u7684\u539f\u7406\u548c\u76ee\u6807\uff0c\u5e76\u5206\u6790\u5b83\u4eec\u7684\u91cd\u53e0\u4e4b\u5904\u3002", "result": "\u7814\u7a76\u53d1\u73b0RSS\u63a8\u8fdb\u4e86\u5143\u79d1\u5b66\u7684\u6838\u5fc3\u76ee\u6807\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u53ef\u91cd\u590d\u6027\u65b9\u9762\uff0c\u5e76\u8fde\u63a5\u4e86\u7814\u7a76\u7684\u6280\u672f\u3001\u793e\u4f1a\u548c\u8ba4\u77e5\u5c42\u9762\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\u7814\u7a76\u8f6f\u4ef6\u79d1\u5b66\uff08RSS\uff09\u5e94\u88ab\u89c6\u4e3a\u4e00\u4e2a\u72ec\u7279\u7684\u8de8\u5b66\u79d1\u9886\u57df\uff0c\u4e0e\u5143\u79d1\u5b66\uff08metascience\uff09\u5728\u67d0\u4e9b\u5b9a\u4e49\u4e0b\u76f8\u5951\u5408\u3002\u8fd9\u79cd\u5206\u7c7b\u6709\u52a9\u4e8e\u63d0\u5347\u5176\u5728\u7814\u7a76\u6539\u8fdb\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u4e3a\u5176\u8d44\u91d1\u652f\u6301\u63d0\u4f9b\u4f9d\u636e\u3002"}}
{"id": "2509.13336", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13336", "abs": "https://arxiv.org/abs/2509.13336", "authors": ["Mehran Behjati", "Rosdiadee Nordin", "Nor Fadzilah Abdullah"], "title": "Maximizing UAV Cellular Connectivity with Reinforcement Learning for BVLoS Path Planning", "comment": "Submitted to an IEEE Conference", "summary": "This paper presents a reinforcement learning (RL) based approach for path\nplanning of cellular connected unmanned aerial vehicles (UAVs) operating beyond\nvisual line of sight (BVLoS). The objective is to minimize travel distance\nwhile maximizing the quality of cellular link connectivity by considering real\nworld aerial coverage constraints and employing an empirical aerial channel\nmodel. The proposed solution employs RL techniques to train an agent, using the\nquality of communication links between the UAV and base stations (BSs) as the\nreward function. Simulation results demonstrate the effectiveness of the\nproposed method in training the agent and generating feasible UAV path plans.\nThe proposed approach addresses the challenges due to limitations in UAV\ncellular communications, highlighting the need for investigations and\nconsiderations in this area. The RL algorithm efficiently identifies optimal\npaths, ensuring maximum connectivity with ground BSs to ensure safe and\nreliable BVLoS flight operation. Moreover, the solution can be deployed as an\noffline path planning module that can be integrated into future ground control\nsystems (GCS) for UAV operations, enhancing their capabilities and safety. The\nmethod holds potential for complex long range UAV applications, advancing the\ntechnology in the field of cellular connected UAV path planning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u4f18\u5316\u901a\u4fe1\u8d28\u91cf\u4e0e\u98de\u884c\u8ddd\u79bb\uff0c\u4eff\u771f\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u5e76\u5177\u5907\u672a\u6765\u7cfb\u7edf\u96c6\u6210\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3\u8d85\u89c6\u8ddd\u98de\u884c\u4e2d\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u9762\u4e34\u7684\u901a\u4fe1\u8d28\u91cf\u4e0e\u98de\u884c\u8ddd\u79bb\u4f18\u5316\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u8bad\u7ec3\u667a\u80fd\u4f53\uff0c\u4ee5\u65e0\u4eba\u673a\u4e0e\u57fa\u7ad9\u95f4\u901a\u4fe1\u94fe\u8def\u8d28\u91cf\u4f5c\u4e3a\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408\u771f\u5b9e\u4e16\u754c\u7a7a\u4e2d\u8986\u76d6\u7ea6\u675f\u548c\u7ecf\u9a8c\u7a7a\u4e2d\u4fe1\u9053\u6a21\u578b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bad\u7ec3\u667a\u80fd\u4f53\u5e76\u751f\u6210\u53ef\u884c\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u5728\u8d85\u89c6\u8ddd\u98de\u884c\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u786e\u4fdd\u4e86\u901a\u4fe1\u8d28\u91cf\u7684\u6700\u5927\u5316\uff0c\u5e76\u5177\u6709\u672a\u6765\u96c6\u6210\u5230\u5730\u9762\u63a7\u5236\u7cfb\u7edf\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13332", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13332", "abs": "https://arxiv.org/abs/2509.13332", "authors": ["Pratik Jayarao", "Himanshu Gupta", "Neeraj Varshney", "Chaitanya Dwivedi"], "title": "Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly adopted as automated judges\nin benchmarking and reward modeling, ensuring their reliability, efficiency,\nand robustness has become critical. In this work, we present a systematic\ncomparison of \"thinking\" and \"non-thinking\" LLMs in the LLM-as-a-judge paradigm\nusing open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B\nparameters). We evaluate both accuracy and computational efficiency (FLOPs) on\nRewardBench tasks, and further examine augmentation strategies for non-thinking\nmodels, including in-context learning, rubric-guided judging, reference-based\nevaluation, and n-best aggregation. Our results show that despite these\nenhancements, non-thinking models generally fall short of their thinking\ncounterparts. Our results show that thinking models achieve approximately 10%\npoints higher accuracy with little overhead (under 2x), in contrast to\naugmentation strategies like few-shot learning, which deliver modest gains at a\nhigher cost (>8x). Bias and robustness analyses further demonstrate that\nthinking models maintain significantly greater consistency under a variety of\nbias conditions such as positional, bandwagon, identity, diversity, and random\nbiases (6% higher on average). We further extend our experiments to the\nmultilingual setting and our results confirm that explicit reasoning extends\nits benefits beyond English. Overall, our work results in several important\nfindings that provide systematic evidence that explicit reasoning offers clear\nadvantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency\nbut also in robustness.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728LLM-as-a-judge\u8303\u5f0f\u4e2d\uff0c\u663e\u5f0f\u63a8\u7406\u7684LLM\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e0a\u5747\u4f18\u4e8e\u975e\u601d\u8003\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u3002", "motivation": "\u968f\u7740LLM\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u5956\u52b1\u5efa\u6a21\u4e2d\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u5224\u5de5\u5177\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u5f00\u6e90Qwen 3\u6a21\u578b\uff080.6B\u30011.7B\u548c4B\u53c2\u6570\uff09\u5728RewardBench\u4efb\u52a1\u4e0a\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff08FLOPs\uff09\uff0c\u5e76\u6d4b\u8bd5\u4e86\u975e\u601d\u8003\u6a21\u578b\u7684\u589e\u5f3a\u7b56\u7565\uff08\u5982\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u8bc4\u5206\u6807\u51c6\u5f15\u5bfc\u3001\u57fa\u4e8e\u53c2\u8003\u7684\u8bc4\u4f30\u548cn-best\u805a\u5408\uff09\u3002", "result": "\u601d\u8003\u6a21\u578b\u5728\u51c6\u786e\u6027\u4e0a\u6bd4\u975e\u601d\u8003\u6a21\u578b\u9ad8\u51fa\u7ea610%\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u8f83\u4f4e\uff08<2x\uff09\uff0c\u800c\u589e\u5f3a\u7b56\u7565\uff08\u5982few-shot\u5b66\u4e60\uff09\u867d\u80fd\u5e26\u6765\u4e00\u5b9a\u63d0\u5347\u4f46\u6210\u672c\u8f83\u9ad8\uff08>8x\uff09\u3002\u6b64\u5916\uff0c\u601d\u8003\u6a21\u578b\u5728\u591a\u79cd\u504f\u89c1\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4e3a\u4e00\u81f4\uff08\u5e73\u5747\u9ad86%\uff09\u3002\u591a\u8bed\u8a00\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u663e\u5f0f\u63a8\u7406\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f83\u201c\u601d\u8003\u201d\u4e0e\u201c\u975e\u601d\u8003\u201dLLM\u5728LLM-as-a-judge\u8303\u5f0f\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u663e\u5f0f\u63a8\u7406\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e0a\u5747\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u3002"}}
{"id": "2509.13338", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "68T07, 68T09"], "pdf": "https://arxiv.org/pdf/2509.13338", "abs": "https://arxiv.org/abs/2509.13338", "authors": ["Hassan Gharoun", "Mohammad Sadegh Khorshidi", "Kasra Ranjbarigderi", "Fang Chen", "Amir H. Gandomi"], "title": "Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks", "comment": "15 pages, 4 figures, 3 tables", "summary": "This work proposes an evidence-retrieval mechanism for uncertainty-aware\ndecision-making that replaces a single global cutoff with an\nevidence-conditioned, instance-adaptive criterion. For each test instance,\nproximal exemplars are retrieved in an embedding space; their predictive\ndistributions are fused via Dempster-Shafer theory. The resulting fused belief\nacts as a per-instance thresholding mechanism. Because the supporting evidences\nare explicit, decisions are transparent and auditable. Experiments on\nCIFAR-10/100 with BiT and ViT backbones show higher or comparable\nuncertainty-aware performance with materially fewer confidently incorrect\noutcomes and a sustainable review load compared with applying threshold on\nprediction entropy. Notably, only a few evidences are sufficient to realize\nthese gains; increasing the evidence set yields only modest changes. These\nresults indicate that evidence-conditioned tagging provides a more reliable and\ninterpretable alternative to fixed prediction entropy thresholds for\noperational uncertainty-aware decision-making.", "AI": {"tldr": "\u63d0\u51fa\u8bc1\u636e\u68c0\u7d22\u673a\u5236\uff0c\u901a\u8fc7\u878d\u5408\u8fd1\u7aef\u6837\u672c\u9884\u6d4b\u5206\u5e03\u5b9e\u73b0\u900f\u660e\u51b3\u7b56\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u56fa\u5b9a\u71b5\u9608\u503c\u65b9\u6cd5\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u53ef\u5ba1\u8ba1\u6027\uff0c\u51cf\u5c11\u9519\u8bef\u9884\u6d4b\u5e76\u4f18\u5316\u5ba1\u67e5\u8d1f\u8f7d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc1\u636e\u68c0\u7d22\u673a\u5236\uff0c\u901a\u8fc7\u5d4c\u5165\u7a7a\u95f4\u4e2d\u68c0\u7d22\u8fd1\u7aef\u6837\u672c\uff0c\u5e76\u5229\u7528Dempster-Shafer\u7406\u8bba\u878d\u5408\u5176\u9884\u6d4b\u5206\u5e03\uff0c\u5f62\u6210\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u9608\u503c\u673a\u5236\u3002", "result": "\u5728CIFAR-10/100\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6027\u80fd\u4e0a\u8868\u73b0\u66f4\u4f18\u6216\u76f8\u5f53\uff0c\u4e14\u663e\u8457\u51cf\u5c11\u4e86\u9519\u8bef\u9884\u6d4b\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6301\u7eed\u7684\u5ba1\u67e5\u8d1f\u8f7d\u3002", "conclusion": "\u8bc1\u636e\u6761\u4ef6\u6807\u8bb0\u4e3a\u64cd\u4f5c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u63d0\u4f9b\u4e86\u6bd4\u56fa\u5b9a\u9884\u6d4b\u71b5\u9608\u503c\u66f4\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.13583", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13583", "abs": "https://arxiv.org/abs/2509.13583", "authors": ["Varsha Rao", "Andrew A. Chien"], "title": "Modeling the Carbon Footprint of HPC: The Top 500 and EasyC", "comment": "15 pages, 11 figures", "summary": "Climate change is a critical concern for HPC systems, but GHG protocol\ncarbon-emission accounting methodologies are difficult for a single system, and\neffectively infeasible for a collection of systems. As a result, there is no\nHPC-wide carbon reporting, and even the largest HPC sites do not do GHG\nprotocol reporting.\n  We assess the carbon footprint of HPC, focusing on the Top 500 systems. The\nkey challenge lies in modeling the carbon footprint with limited data\navailability.\n  With the disclosed Top500.org data, and using a new tool, EasyC, we were able\nto model the operational carbon of 391 HPC systems and the embodied carbon of\n283 HPC systems. We further show how this coverage can be enhanced by\nexploiting additional public information. With improved coverage, then\ninterpolation is used to produce the first carbon footprint estimates of the\nTop 500 HPC systems. They are 1,393.7 million MT CO2e operational carbon (1\nYear) and 1,881.8 million MT CO2e embodied carbon. We also project how the Top\n500's carbon footprint will increase through 2030.\n  A key enabler is the EasyC tool which models carbon footprint with only a few\ndata metrics. We explore availability of data and enhancement, showing that\ncoverage can be increased to 98% of Top 500 systems for operational and 80.8%\nof the systems for embodied emissions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528EasyC\u5de5\u5177\u4f30\u7b97Top 500 HPC\u7cfb\u7edf\u7684\u78b3\u8db3\u8ff9\uff0c\u586b\u8865\u4e86HPC\u7cfb\u7edf\u78b3\u62a5\u544a\u7684\u7a7a\u767d\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4fHPC\u7cfb\u7edf\u7684\u78b3\u62a5\u544a\uff0c\u5c24\u5176\u662f\u5927\u578b\u7ad9\u70b9\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9bHPC\u7cfb\u7edf\u7684\u78b3\u8db3\u8ff9\u4f30\u7b97\u3002", "method": "\u4f7f\u7528EasyC\u5de5\u5177\u548cTop500.org\u6570\u636e\uff0c\u5bf9391\u4e2aHPC\u7cfb\u7edf\u7684\u8fd0\u8425\u78b3\u548c283\u4e2a\u7cfb\u7edf\u7684\u9690\u542b\u78b3\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u63d2\u503c\u4f30\u7b97Top 500\u7cfb\u7edf\u7684\u78b3\u8db3\u8ff9\u3002", "result": "\u4f30\u7b97\u51faTop 500 HPC\u7cfb\u7edf\u7684\u8fd0\u8425\u78b3\u4e3a1,393.7\u767e\u4e07\u5428CO2e\uff081\u5e74\uff09\uff0c\u9690\u542b\u78b3\u4e3a1,881.8\u767e\u4e07\u5428CO2e\uff0c\u5e76\u9884\u6d4b\u52302030\u5e74\u7684\u589e\u957f\u8d8b\u52bf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f30\u7b97HPC\u7cfb\u7edf\u78b3\u8db3\u8ff9\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u989d\u5916\u516c\u5f00\u4fe1\u606f\u63d0\u9ad8\u8986\u76d6\u8303\u56f4\uff0c\u4e3aTop 500 HPC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9996\u4e2a\u78b3\u8db3\u8ff9\u4f30\u7b97\u3002"}}
{"id": "2509.13511", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13511", "abs": "https://arxiv.org/abs/2509.13511", "authors": ["Duo Cheng", "Ramanujan K Sheshadri", "Ahan Kak", "Nakjung Choi", "Xingyu Zhou", "Bo Ji"], "title": "Odin: Effective End-to-End SLA Decomposition for 5G/6G Network Slicing via Online Learning", "comment": "accepted for publication at ACM MobiHoc 2025", "summary": "Network slicing plays a crucial role in realizing 5G/6G advances, enabling\ndiverse Service Level Agreement (SLA) requirements related to latency,\nthroughput, and reliability. Since network slices are deployed end-to-end\n(E2E), across multiple domains including access, transport, and core networks,\nit is essential to efficiently decompose an E2E SLA into domain-level targets,\nso that each domain can provision adequate resources for the slice. However,\ndecomposing SLAs is highly challenging due to the heterogeneity of domains,\ndynamic network conditions, and the fact that the SLA orchestrator is oblivious\nto the domain's resource optimization. In this work, we propose Odin, a\nBayesian Optimization-based solution that leverages each domain's online\nfeedback for provably-efficient SLA decomposition. Through theoretical analyses\nand rigorous evaluations, we demonstrate that Odin's E2E orchestrator can\nachieve up to 45% performance improvement in SLA satisfaction when compared\nwith baseline solutions whilst reducing overall resource costs even in the\npresence of noisy feedback from the individual domains.", "AI": {"tldr": "Odin \u662f\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684 SLA \u5206\u89e3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347 SLA \u6ee1\u610f\u5ea6\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u7f51\u7edc\u5207\u7247\u5728\u5b9e\u73b0 5G/6G \u8fdb\u5c55\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46 SLA \u5206\u89e3\u56e0\u9886\u57df\u5f02\u6784\u6027\u3001\u52a8\u6001\u7f51\u7edc\u6761\u4ef6\u548c\u8d44\u6e90\u4f18\u5316\u4e0d\u900f\u660e\u800c\u6781\u5177\u6311\u6218\u6027\u3002", "method": "Odin \u662f\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528\u5404\u9886\u57df\u7684\u5728\u7ebf\u53cd\u9988\u5b9e\u73b0\u9ad8\u6548 SLA \u5206\u89e3\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u4e25\u683c\u8bc4\u4f30\u8868\u660e\uff0cOdin \u7684 E2E \u7f16\u6392\u5668\u5728 SLA \u6ee1\u610f\u5ea6\u4e0a\u6bd4\u57fa\u7ebf\u89e3\u51b3\u65b9\u6848\u63d0\u5347\u9ad8\u8fbe 45%\uff0c\u540c\u65f6\u964d\u4f4e\u603b\u4f53\u8d44\u6e90\u6210\u672c\u3002", "conclusion": "Odin \u7684\u7aef\u5230\u7aef\uff08E2E\uff09\u7f16\u6392\u5668\u5728 SLA \u5206\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86 SLA \u6ee1\u610f\u5ea6\u5e76\u964d\u4f4e\u4e86\u8d44\u6e90\u6210\u672c\u3002"}}
{"id": "2509.13584", "categories": ["cs.DS", "cs.CC"], "pdf": "https://arxiv.org/pdf/2509.13584", "abs": "https://arxiv.org/abs/2509.13584", "authors": ["Yan S. Couto", "Cristina G. Fernandes"], "title": "Hardness of Dynamic Core and Truss Decompositions", "comment": "Full version of the paper accepted in WAOA 2025", "summary": "The k-core of a graph is its maximal subgraph with minimum degree at least k,\nand the core value of a vertex u is the largest k for which u is contained in\nthe k-core of the graph. Among cohesive subgraphs, k-core and its variants have\nreceived a lot of attention recently, particularly on dynamic graphs, as\nreported by Hanauer, Henzinger, and Schulz in their recent survey on dynamic\ngraph algorithms. We answer questions on k-core stated in the survey, proving\nthat there is no efficient dynamic algorithm for k-core or to find (2 -\n{\\epsilon})-approximations for the core values, unless we can improve\ndecade-long state-of-the-art algorithms in many areas including matrix\nmultiplication and satisfiability, based on the established OMv and SETH\nconjectures. Some of our results show that there is no dynamic algorithm for\nk-core asymptotically faster than the trivial ones. This explains why most\nrecent research papers in this area focus not on a generic efficient dynamic\nalgorithm, but on finding a bounded algorithm, which is fast when few core\nvalues change per update. However, we also prove that such bounded algorithms\ndo not exist, based on the OMv conjecture. We present lower bounds also for a\ndirected version of the problem, and for the edge variant of the problem, known\nas k-truss. On the positive side, we present a polylogarithmic dynamic\nalgorithm for 2-core.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u52a8\u6001k-core\u7b97\u6cd5\u7684\u9ad8\u6548\u6027\u53d7\u9650\uff0c\u57fa\u4e8eOMv\u548cSETH\u731c\u60f3\uff0c\u9664\u975e\u6539\u8fdb\u73b0\u6709\u7b97\u6cd5\u3002\u540c\u65f6\u63d0\u51fa2-core\u7684\u591a\u5bf9\u6570\u52a8\u6001\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u52a8\u6001\u56fe\u4e2dk-core\u53ca\u5176\u53d8\u4f53\u7684\u9ad8\u6548\u7b97\u6cd5\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9Hanauer\u7b49\u4eba\u8c03\u67e5\u4e2d\u63d0\u51fa\u7684\u95ee\u9898\u7684\u56de\u7b54\u3002", "method": "\u8bba\u6587\u57fa\u4e8eOMv\u548cSETH\u731c\u60f3\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86k-core\u53ca\u5176\u53d8\u4f53\u5728\u52a8\u6001\u56fe\u4e2d\u7684\u7b97\u6cd5\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf92-core\u7684\u591a\u5bf9\u6570\u52a8\u6001\u7b97\u6cd5\u3002", "result": "\u8bba\u6587\u8bc1\u660e\u4e86\u9664\u975e\u80fd\u6539\u8fdb\u73b0\u6709\u7b97\u6cd5\uff0c\u5426\u5219\u4e0d\u5b58\u5728\u9ad8\u6548\u7684\u52a8\u6001k-core\u7b97\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u6709\u754c\u7b97\u6cd5\u7684\u5c40\u9650\u6027\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u9488\u5bf92-core\u7684\u591a\u5bf9\u6570\u52a8\u6001\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u8bc1\u660e\u57fa\u4e8eOMv\u548cSETH\u731c\u60f3\uff0c\u6307\u51fa\u9664\u975e\u80fd\u6539\u8fdb\u77e9\u9635\u4e58\u6cd5\u548c\u53ef\u6ee1\u8db3\u6027\u7b49\u9886\u57df\u7684\u73b0\u6709\u7b97\u6cd5\uff0c\u5426\u5219\u4e0d\u5b58\u5728\u9ad8\u6548\u7684\u52a8\u6001k-core\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u5c55\u793a\u4e86\u6709\u754c\u7b97\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf92-core\u7684\u591a\u5bf9\u6570\u52a8\u6001\u7b97\u6cd5\u3002"}}
{"id": "2509.13471", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13471", "abs": "https://arxiv.org/abs/2509.13471", "authors": ["Sina Gogani-Khiabani", "Ashutosh Trivedi", "Diptikalyan Saha", "Saeid Tizpaz-Niari"], "title": "An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software", "comment": "To appear at ICSE 26. 12 pages", "summary": "Large language models (LLMs) show promise for translating natural-language\nstatutes into executable logic, but reliability in legally critical settings\nremains challenging due to ambiguity and hallucinations. We present an agentic\napproach for developing legal-critical software, using U.S. federal tax\npreparation as a case study. The key challenge is test-case generation under\nthe oracle problem, where correct outputs require interpreting law. Building on\nmetamorphic testing, we introduce higher-order metamorphic relations that\ncompare system outputs across structured shifts among similar individuals.\nBecause authoring such relations is tedious and error-prone, we use an\nLLM-driven, role-based framework to automate test generation and code\nsynthesis. We implement a multi-agent system that translates tax code into\nexecutable software and incorporates a metamorphic-testing agent that searches\nfor counterexamples. In experiments, our framework using a smaller model\n(GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier\nmodels (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results\nsupport agentic LLM methodologies as a path to robust, trustworthy\nlegal-critical software from natural-language specifications.", "AI": {"tldr": "An agentic approach using LLMs for legal-critical software development, focusing on U.S. tax code translation, outperforms frontier models in reliability.", "motivation": "Large language models (LLMs) show potential for translating natural-language statutes into executable logic, but their reliability in legally critical settings is challenged by ambiguity and hallucinations. The study aims to address these challenges in legal-critical software development, using U.S. federal tax preparation as a case study.", "method": "The paper introduces a multi-agent system that translates tax code into executable software, incorporating a metamorphic-testing agent to search for counterexamples. It uses an LLM-driven, role-based framework to automate test generation and code synthesis.", "result": "In experiments, the framework using a smaller model (GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier models (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks.", "conclusion": "The study supports agentic LLM methodologies as a viable path to developing robust and trustworthy legal-critical software from natural-language specifications."}}
{"id": "2509.13342", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13342", "abs": "https://arxiv.org/abs/2509.13342", "authors": ["Isaac Ronald Ward"], "title": "Real World Robotic Exploration using Deep Neural Networks Trained in Photorealistic Reconstructed Environments", "comment": "This report is submitted as partial fulfilment of the requirements\n  for the Honours Programme of the Department of Computer Science and Software\n  Engineering, The University of Western Australia, 2019", "summary": "In this work, an existing deep neural network approach for determining a\nrobot's pose from visual information (RGB images) is modified, improving its\nlocalization performance without impacting its ease of training. Explicitly,\nthe network's loss function is extended in a manner which intuitively combines\nthe positional and rotational error in order to increase robustness to\nperceptual aliasing. An improvement in the localization accuracy for indoor\nscenes is observed: with decreases of up to 9.64% and 2.99% in the median\npositional and rotational error respectively, when compared to the unmodified\nnetwork.\n  Additionally, photogrammetry data is used to produce a pose-labelled dataset\nwhich allows the above model to be trained on a local environment, resulting in\nlocalization accuracies of 0.11m & 0.89 degrees. This trained model forms the\nbasis of a navigation algorithm, which is tested in real-time on a TurtleBot (a\nwheeled robotic device). As such, this work introduces a full pipeline for\ncreating a robust navigational algorithm for any given real world indoor scene;\nthe only requirement being a collection of images from the scene, which can be\ncaptured in as little as 330 seconds of", "AI": {"tldr": "\u6539\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u51fd\u6570\u7ed3\u5408\u4f4d\u7f6e\u548c\u65cb\u8f6c\u8bef\u5dee\uff0c\u63d0\u5347\u5ba4\u5185\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5e76\u5229\u7528\u6444\u5f71\u6d4b\u91cf\u6570\u636e\u751f\u6210\u672c\u5730\u5316\u8bad\u7ec3\u96c6\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u673a\u5668\u4eba\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u611f\u77e5\u6df7\u6dc6\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u635f\u5931\u51fd\u6570\u548c\u5229\u7528\u672c\u5730\u5316\u6570\u636e\u96c6\uff0c\u63d0\u5347\u5b9a\u4f4d\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u795e\u7ecf\u7f51\u7edc\u7684\u635f\u5931\u51fd\u6570\uff0c\u5c06\u4f4d\u7f6e\u548c\u65cb\u8f6c\u8bef\u5dee\u76f4\u89c2\u7ed3\u5408\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u611f\u77e5\u6df7\u6dc6\u7684\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u6444\u5f71\u6d4b\u91cf\u6570\u636e\u751f\u6210\u59ff\u6001\u6807\u7b7e\u6570\u636e\u96c6\uff0c\u652f\u6301\u672c\u5730\u5316\u8bad\u7ec3\u3002", "result": "\u6539\u8fdb\u540e\u7684\u7f51\u7edc\u5728\u5ba4\u5185\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\uff0c\u4e2d\u4f4d\u4f4d\u7f6e\u8bef\u5dee\u964d\u4f4e9.64%\uff0c\u65cb\u8f6c\u8bef\u5dee\u964d\u4f4e2.99%\u3002\u672c\u5730\u5316\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u8fbe\u52300.11\u7c73\u548c0.89\u5ea6\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5b9e\u65f6\u5bfc\u822a\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u635f\u5931\u51fd\u6570\u7ed3\u5408\u4f4d\u7f6e\u548c\u65cb\u8f6c\u8bef\u5dee\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u5ba4\u5185\u573a\u666f\u4e2d\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u540c\u65f6\uff0c\u5229\u7528\u6444\u5f71\u6d4b\u91cf\u6570\u636e\u751f\u6210\u5e26\u59ff\u6001\u6807\u7b7e\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u672c\u5730\u73af\u5883\u4e0b\u7684\u6a21\u578b\u8bad\u7ec3\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e860.11\u7c73\u548c0.89\u5ea6\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5b9e\u65f6\u5bfc\u822a\u7b97\u6cd5\u3002"}}
{"id": "2509.13333", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13333", "abs": "https://arxiv.org/abs/2509.13333", "authors": ["Maheep Chaudhary", "Ian Su", "Nikhil Hooda", "Nishith Shankar", "Julia Tan", "Kevin Zhu", "Ashwinee Panda", "Ryan Lagasse", "Vasu Sharma"], "title": "Evaluation Awareness Scales Predictably in Open-Weights Large Language Models", "comment": null, "summary": "Large language models (LLMs) can internally distinguish between evaluation\nand deployment contexts, a behaviour known as \\emph{evaluation awareness}. This\nundermines AI safety evaluations, as models may conceal dangerous capabilities\nduring testing. Prior work demonstrated this in a single $70$B model, but the\nscaling relationship across model sizes remains unknown. We investigate\nevaluation awareness across $15$ models scaling from $0.27$B to $70$B\nparameters from four families using linear probing on steering vector\nactivations. Our results reveal a clear power-law scaling: evaluation awareness\nincreases predictably with model size. This scaling law enables forecasting\ndeceptive behavior in future larger models and guides the design of scale-aware\nevaluation strategies for AI safety. A link to the implementation of this paper\ncan be found at\nhttps://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u7684\u8bc4\u4f30\u610f\u8bc6\u968f\u6a21\u578b\u89c4\u6a21\u5e42\u5f8b\u589e\u957f\uff0c\u4e3a\u9884\u6d4b\u672a\u6765\u5927\u6a21\u578b\u7684\u6b3a\u9a97\u884c\u4e3a\u53ca\u8bbe\u8ba1AI\u5b89\u5168\u8bc4\u4f30\u7b56\u7565\u63d0\u4f9b\u4f9d\u636e\u3002", "motivation": "\u63ed\u793a\u6a21\u578b\u5728\u8bc4\u4f30\u548c\u90e8\u7f72\u73af\u5883\u4e2d\u884c\u4e3a\u5dee\u5f02\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u4ee5\u89e3\u51b3AI\u5b89\u5168\u8bc4\u4f30\u4e2d\u6a21\u578b\u53ef\u80fd\u9690\u85cf\u5371\u9669\u80fd\u529b\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7ebf\u6027\u63a2\u6d4b\u572815\u4e2a\u53c2\u6570\u89c4\u6a21\u4ece0.27B\u523070B\u7684\u6a21\u578b\u5bb6\u65cf\u4e2d\u7684\u8f6c\u5411\u5411\u91cf\u6fc0\u6d3b\uff0c\u7814\u7a76\u8bc4\u4f30\u610f\u8bc6\u7684\u7f29\u653e\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bc4\u4f30\u610f\u8bc6\u4e0e\u6a21\u578b\u89c4\u6a21\u4e4b\u95f4\u5b58\u5728\u6e05\u6670\u7684\u5e42\u5f8b\u7f29\u653e\u5173\u7cfb\uff0c\u5373\u6a21\u578b\u89c4\u6a21\u8d8a\u5927\uff0c\u8bc4\u4f30\u610f\u8bc6\u8d8a\u5f3a\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bc4\u4f30\u610f\u8bc6\u884c\u4e3a\u968f\u6a21\u578b\u89c4\u6a21\u5448\u73b0\u5e42\u5f8b\u589e\u957f\uff0c\u8fd9\u4e3a\u9884\u6d4b\u672a\u6765\u66f4\u5927\u6a21\u578b\u7684\u6b3a\u9a97\u884c\u4e3a\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u5e76\u6307\u5bfc\u4e86\u9488\u5bf9AI\u5b89\u5168\u7684\u89c4\u6a21\u611f\u77e5\u8bc4\u4f30\u7b56\u7565\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2509.13353", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13353", "abs": "https://arxiv.org/abs/2509.13353", "authors": ["Muhammad Adnan Shahzad"], "title": "Hybrid Quantum-Classical Model for Image Classification", "comment": null, "summary": "This study presents a systematic comparison between hybrid quantum-classical\nneural networks and purely classical models across three benchmark datasets\n(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and\nrobustness. The hybrid models integrate parameterized quantum circuits with\nclassical deep learning architectures, while the classical counterparts use\nconventional convolutional neural networks (CNNs). Experiments were conducted\nover 50 training epochs for each dataset, with evaluations on validation\naccuracy, test accuracy, training time, computational resource usage, and\nadversarial robustness (tested with $\\epsilon=0.1$ perturbations).Key findings\ndemonstrate that hybrid models consistently outperform classical models in\nfinal accuracy, achieving {99.38\\% (MNIST), 41.69\\% (CIFAR100), and 74.05\\%\n(STL10) validation accuracy, compared to classical benchmarks of 98.21\\%,\n32.25\\%, and 63.76\\%, respectively. Notably, the hybrid advantage scales with\ndataset complexity, showing the most significant gains on CIFAR100 (+9.44\\%)\nand STL10 (+10.29\\%). Hybrid models also train 5--12$\\times$ faster (e.g.,\n21.23s vs. 108.44s per epoch on MNIST) and use 6--32\\% fewer parameters} while\nmaintaining superior generalization to unseen test data.Adversarial robustness\ntests reveal that hybrid models are significantly more resilient on simpler\ndatasets (e.g., 45.27\\% robust accuracy on MNIST vs. 10.80\\% for classical) but\nshow comparable fragility on complex datasets like CIFAR100 ($\\sim$1\\%\nrobustness for both). Resource efficiency analyses indicate that hybrid models\nconsume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization\n(9.5\\% vs. 23.2\\% on average).These results suggest that hybrid\nquantum-classical architectures offer compelling advantages in accuracy,\ntraining efficiency, and parameter scalability, particularly for complex vision\ntasks.", "AI": {"tldr": "\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u7eaf\u7ecf\u5178\u6a21\u578b\uff0c\u5c24\u5176\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u8bc4\u4f30\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u4e0e\u7eaf\u7ecf\u5178\u6a21\u578b\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5c06\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u4e0e\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u96c6\u6210\uff0c\u6784\u5efa\u6df7\u5408\u6a21\u578b\uff0c\u4e0e\u4f20\u7edf\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u6df7\u5408\u6a21\u578b\u5728\u9a8c\u8bc1\u51c6\u786e\u7387\uff08MNIST:99.38%\uff0cCIFAR100:41.69%\uff0cSTL10:74.05%\uff09\u548c\u8bad\u7ec3\u901f\u5ea6\uff08\u5feb5-12\u500d\uff09\u4e0a\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\uff0c\u4e14\u5728\u8d44\u6e90\u5229\u7528\u4e0a\u66f4\u9ad8\u6548\u3002", "conclusion": "\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u5728\u51c6\u786e\u6027\u3001\u8bad\u7ec3\u6548\u7387\u548c\u53c2\u6570\u53ef\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2509.13703", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13703", "abs": "https://arxiv.org/abs/2509.13703", "authors": ["Sriram Srinivasan", "Hamdan Alabsi", "Rand Obeidat", "Nithisha Ponnala", "Azene Zenebe"], "title": "GPU Programming for AI Workflow Development on AWS SageMaker: An Instructional Approach", "comment": null, "summary": "We present the design, implementation, and comprehensive evaluation of a\nspecialized course on GPU architecture, GPU programming, and how these are used\nfor developing AI agents. This course is offered to undergraduate and graduate\nstudents during Fall 2024 and Spring 2025. The course began with foundational\nconcepts in GPU/CPU hardware and parallel computing and progressed to develop\nRAG and optimizing them using GPUs. Students gained experience provisioning and\nconfiguring cloud-based GPU instances, implementing parallel algorithms, and\ndeploying scalable AI solutions. We evaluated learning outcomes through\nassessments, course evaluations, and anonymous surveys. The results reveal that\n(1) AWS served as an effective and economical platform for practical GPU\nprogramming, (2) experiential learning significantly enhanced technical\nproficiency and engagement, and (3) the course strengthened students'\nproblem-solving and critical thinking skills through tools such as TensorBoard\nand HPC profilers, which exposed performance bottlenecks and scaling issues.\nOur findings underscore the pedagogical value of integrating parallel computing\ninto STEM education. We advocate for broader adoption of similar electives\nacross STEM curricula to prepare students for the demands of modern,\ncompute-intensive fields.", "AI": {"tldr": "A specialized course on GPU architecture and programming for AI agents was evaluated, showing improved student skills and engagement, with AWS as a practical platform. The paper recommends broader adoption of such courses in STEM education.", "motivation": "The motivation was to design a specialized course that bridges the gap between GPU architecture, programming, and their application in developing AI agents, aiming to enhance students' technical proficiency and engagement in parallel computing.", "method": "The course covered foundational GPU/CPU hardware concepts, parallel computing, and progressed to developing RAG and optimizing them using GPUs. Students worked with cloud-based GPU instances, implemented parallel algorithms, and deployed scalable AI solutions. Learning outcomes were evaluated through assessments, course evaluations, and anonymous surveys.", "result": "Results indicated that AWS was an effective and economical platform for practical GPU programming, experiential learning significantly enhanced technical proficiency and engagement, and the course improved students' problem-solving and critical thinking skills.", "conclusion": "The paper advocates for broader adoption of GPU and parallel computing courses in STEM curricula to prepare students for modern, compute-intensive fields, highlighting the pedagogical value of such courses."}}
{"id": "2509.13604", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13604", "abs": "https://arxiv.org/abs/2509.13604", "authors": ["Yoseph Berhanu Alebachew", "Mulugeta Libsie"], "title": "A Framework for Multi-source Prefetching Through Adaptive Weight", "comment": null, "summary": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6574\u5408\u591a\u79cd\u9884\u53d6\u65b9\u6848\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6743\u91cd\u7ba1\u7406\u4f18\u5316\u9884\u6d4b\uff0c\u9002\u5408\u79fb\u52a8\u8bbe\u5907\u3002", "motivation": "\u5c3d\u7ba1\u7f13\u5b58\u548c\u9884\u53d6\u6280\u672f\u5df2\u88ab\u7528\u4e8e\u7f13\u89e3\u7f51\u7edc\u5ef6\u8fdf\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5229\u7528\u5e94\u7528\u7ea7\u522b\u7684\u6587\u6863\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e14\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u3002", "method": "\u901a\u8fc7\u81ea\u9002\u5e94\u6743\u91cd\u7ba1\u7406\u6280\u672f\uff0c\u6846\u67b6\u6839\u636e\u6bcf\u4e2a\u7b97\u6cd5\u7684\u89c2\u5bdf\u6027\u80fd\u8c03\u6574\u5176\u5728\u6574\u4f53\u9884\u6d4b\u4e2d\u7684\u5f71\u54cd\u3002", "result": "\u8be5\u6846\u67b6\u6bd4\u73b0\u6709\u65b9\u6848\u66f4\u4e3a\u4fdd\u5b88\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6574\u5408\u6765\u81ea\u4e0d\u540c\u9884\u53d6\u65b9\u6848\u7684\u6280\u672f\uff0c\u65e0\u9700\u5bf9\u7b97\u6cd5\u8fdb\u884c\u91cd\u5927\u4fee\u6539\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u3002"}}
{"id": "2509.13891", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2509.13891", "abs": "https://arxiv.org/abs/2509.13891", "authors": ["Tsz Chiu Kwok", "Zhewei Wei", "Mingji Yang"], "title": "On Solving Asymmetric Diagonally Dominant Linear Systems in Sublinear Time", "comment": "46 pages", "summary": "We initiate a study of solving a row/column diagonally dominant (RDD/CDD)\nlinear system $Mx=b$ in sublinear time, with the goal of estimating\n$t^{\\top}x^*$ for a given vector $t\\in R^n$ and a specific solution $x^*$. This\nsetting naturally generalizes the study of sublinear-time solvers for symmetric\ndiagonally dominant (SDD) systems [AKP19] to the asymmetric case.\n  Our first contributions are characterizations of the problem's mathematical\nstructure. We express a solution $x^*$ via a Neumann series, prove its\nconvergence, and upper bound the truncation error on this series through a\nnovel quantity of $M$, termed the maximum $p$-norm gap. This quantity\ngeneralizes the spectral gap of symmetric matrices and captures how the\nstructure of $M$ governs the problem's computational difficulty.\n  For systems with bounded maximum $p$-norm gap, we develop a collection of\nalgorithmic results for locally approximating $t^{\\top}x^*$ under various\nscenarios and error measures. We derive these results by adapting the\ntechniques of random-walk sampling, local push, and their bidirectional\ncombination, which have proved powerful for special cases of solving RDD/CDD\nsystems, particularly estimating PageRank and effective resistance on graphs.\nOur general framework yields deeper insights, extended results, and improved\ncomplexity bounds for these problems. Notably, our perspective provides a\nunified understanding of Forward Push and Backward Push, two fundamental\napproaches for estimating random-walk probabilities on graphs.\n  Our framework also inherits the hardness results for sublinear-time SDD\nsolvers and local PageRank computation, establishing lower bounds on the\nmaximum $p$-norm gap or the accuracy parameter. We hope that our work opens the\ndoor for further study into sublinear solvers, local graph algorithms, and\ndirected spectral graph theory.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5b50\u7ebf\u6027\u65f6\u95f4\u6c42\u89e3RDD/CDD\u7ebf\u6027\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bfa\u4f9d\u66fc\u7ea7\u6570\u548c\u6700\u5927p\u8303\u6570\u95f4\u9699\u5206\u6790\uff0c\u5f00\u53d1\u4e86\u5c40\u90e8\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u7edf\u4e00\u4e86\u524d\u5411\u63a8\u548c\u53cd\u5411\u63a8\u65b9\u6cd5\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a8\u5e7f\u5bf9\u79f0\u5bf9\u89d2\u5360\u4f18\uff08SDD\uff09\u7cfb\u7edf\u7684\u5b50\u7ebf\u6027\u65f6\u95f4\u6c42\u89e3\u5668\u7814\u7a76\uff0c\u4ee5\u89e3\u51b3\u975e\u5bf9\u79f0\u60c5\u51b5\u4e0b\u7684\u884c/\u5217\u5bf9\u89d2\u5360\u4f18\uff08RDD/CDD\uff09\u7ebf\u6027\u7cfb\u7edfMx=b\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u8bfa\u4f9d\u66fc\u7ea7\u6570\u8868\u8fbe\u89e3x\u2217\uff0c\u8bc1\u660e\u5176\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7M\u7684\u65b0\u9896\u91cf\u2014\u2014\u6700\u5927p\u8303\u6570\u95f4\u9699\u2014\u2014\u6765\u9650\u5236\u8be5\u7ea7\u6570\u7684\u622a\u65ad\u8bef\u5dee\u3002\u5bf9\u4e8e\u5177\u6709\u6709\u754c\u6700\u5927p\u8303\u6570\u95f4\u9699\u7684\u7cfb\u7edf\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u7cfb\u5217\u7b97\u6cd5\u7ed3\u679c\uff0c\u7528\u4e8e\u5728\u4e0d\u540c\u573a\u666f\u548c\u8bef\u5dee\u5ea6\u91cf\u4e0b\u5c40\u90e8\u8fd1\u4f3ct\u22a4x\u2217\u3002", "result": "\u6211\u4eec\u7684\u6846\u67b6\u63d0\u4f9b\u4e86\u5bf9\u524d\u5411\u63a8\u548c\u53cd\u5411\u63a8\u7684\u7edf\u4e00\u7406\u89e3\uff0c\u5e76\u4e3aPageRank\u548c\u56fe\u4e0a\u6709\u6548\u7535\u963b\u4f30\u8ba1\u7b49\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3001\u6269\u5c55\u7ed3\u679c\u548c\u6539\u8fdb\u7684\u590d\u6742\u5ea6\u754c\u9650\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u4e3a\u5b50\u7ebf\u6027\u65f6\u95f4\u6c42\u89e3\u5668\u3001\u5c40\u90e8\u56fe\u7b97\u6cd5\u548c\u6709\u5411\u8c31\u56fe\u7406\u8bba\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u6253\u5f00\u4e86\u5927\u95e8\u3002"}}
{"id": "2509.13487", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13487", "abs": "https://arxiv.org/abs/2509.13487", "authors": ["Abubakari Alidu", "Michele Ciavotta", "Flavio DePaoli"], "title": "Prompt2DAG: A Modular Methodology for LLM-Based Data Enrichment Pipeline Generation", "comment": null, "summary": "Developing reliable data enrichment pipelines demands significant engineering\nexpertise. We present Prompt2DAG, a methodology that transforms natural\nlanguage descriptions into executable Apache Airflow DAGs. We evaluate four\ngeneration approaches -- Direct, LLM-only, Hybrid, and Template-based -- across\n260 experiments using thirteen LLMs and five case studies to identify optimal\nstrategies for production-grade automation. Performance is measured using a\npenalized scoring framework that combines reliability with code quality (SAT),\nstructural integrity (DST), and executability (PCT). The Hybrid approach\nemerges as the optimal generative method, achieving a 78.5% success rate with\nrobust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76). This significantly\noutperforms the LLM-only (66.2% success) and Direct (29.2% success) methods.\nOur findings show that reliability, not intrinsic code quality, is the primary\ndifferentiator. Cost-effectiveness analysis reveals the Hybrid method is over\ntwice as efficient as Direct prompting per successful DAG. We conclude that a\nstructured, hybrid approach is essential for balancing flexibility and\nreliability in automated workflow generation, offering a viable path to\ndemocratize data pipeline development.", "AI": {"tldr": "Prompt2DAG\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u5316\u4e3aAirflow DAG\uff0c\u6210\u529f\u7387\u8fbe78.5%\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4e3a\u81ea\u52a8\u5316\u6570\u636e\u7ba1\u9053\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f00\u53d1\u53ef\u9760\u7684\u6570\u636e\u4e30\u5bcc\u7ba1\u9053\u9700\u8981\u5927\u91cf\u5de5\u7a0b\u4e13\u4e1a\u77e5\u8bc6\uff0cPrompt2DAG\u65e8\u5728\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684Apache Airflow DAG\uff0c\u4ee5\u4f18\u5316\u751f\u4ea7\u7ea7\u81ea\u52a8\u5316\u7b56\u7565\u3002", "method": "\u8bc4\u4f30\u4e86\u56db\u79cd\u751f\u6210\u65b9\u6cd5\uff08\u76f4\u63a5\u6cd5\u3001\u4ec5LLM\u6cd5\u3001\u6df7\u5408\u6cd5\u548c\u57fa\u4e8e\u6a21\u677f\u6cd5\uff09\uff0c\u901a\u8fc7260\u4e2a\u5b9e\u9a8c\u300113\u4e2aLLM\u548c5\u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u4f7f\u7528\u7ed3\u5408\u53ef\u9760\u6027\u3001\u4ee3\u7801\u8d28\u91cf\uff08SAT\uff09\u3001\u7ed3\u6784\u5b8c\u6574\u6027\uff08DST\uff09\u548c\u53ef\u6267\u884c\u6027\uff08PCT\uff09\u7684\u60e9\u7f5a\u8bc4\u5206\u6846\u67b6\u3002", "result": "\u6df7\u5408\u65b9\u6cd5\u6210\u4e3a\u6700\u4f73\u751f\u6210\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u4e3a78.5%\uff0c\u8d28\u91cf\u5f97\u5206\u7a33\u5065\uff08SAT\uff1a6.79\uff0cDST\uff1a7.67\uff0cPCT\uff1a7.76\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5LLM\u6cd5\uff0866.2%\uff09\u548c\u76f4\u63a5\u6cd5\uff0829.2%\uff09\u3002\u6210\u672c\u6548\u76ca\u5206\u6790\u663e\u793a\u6df7\u5408\u65b9\u6cd5\u6bcf\u6210\u529fDAG\u7684\u6548\u7387\u662f\u76f4\u63a5\u6cd5\u7684\u4e24\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u7ed3\u6784\u5316\u6df7\u5408\u65b9\u6cd5\u5728\u81ea\u52a8\u5de5\u4f5c\u6d41\u751f\u6210\u4e2d\u5e73\u8861\u7075\u6d3b\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u6570\u636e\u7ba1\u9053\u5f00\u53d1\u7684\u6c11\u4e3b\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.13349", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13349", "abs": "https://arxiv.org/abs/2509.13349", "authors": ["Jed Guzelkabaagac", "Boris Petrovi\u0107"], "title": "Label-Efficient Grasp Joint Prediction with Point-JEPA", "comment": "4 pages, 5 figures. Submitted to IROS 2025 Workshop", "summary": "We investigate whether 3D self-supervised pretraining with a Joint-Embedding\nPredictive Architecture (Point-JEPA) enables label-efficient grasp joint-angle\nprediction. Using point clouds tokenized from meshes and a ShapeNet-pretrained\nPoint-JEPA encoder, we train a lightweight multi-hypothesis head with\nwinner-takes-all and evaluate by top-logit selection. On DLR-Hand II with\nobject-level splits, Point-JEPA reduces RMSE by up to 26% in low-label regimes\nand reaches parity with full supervision. These results suggest JEPA-style\npretraining is a practical approach for data-efficient grasp learning.", "AI": {"tldr": "Point-JEPA\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5728\u4f4e\u6807\u7b7e\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u6293\u53d6\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u6548\u679c\uff0c\u8fbe\u5230\u4e0e\u5168\u76d1\u7763\u76f8\u5f53\u7684\u6c34\u5e73\u3002", "motivation": "\u7814\u7a763D\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff08Point-JEPA\uff09\u662f\u5426\u80fd\u591f\u5b9e\u73b0\u6807\u7b7e\u9ad8\u6548\u7684\u6293\u53d6\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u3002", "method": "\u4f7f\u7528\u70b9\u4e91\u4ece\u7f51\u683c\u4e2d\u5206\u5757\uff0c\u5e76\u5229\u7528ShapeNet\u9884\u8bad\u7ec3\u7684Point-JEPA\u7f16\u7801\u5668\uff0c\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u591a\u5047\u8bbe\u5934\uff0c\u91c7\u7528\u8d62\u5bb6\u901a\u5403\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u9876\u90e8\u903b\u8f91\u9009\u62e9\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728DLR-Hand II\u6570\u636e\u96c6\u4e0a\uff0cPoint-JEPA\u5728\u4f4e\u6807\u7b7e\u60c5\u51b5\u4e0b\u5c06RMSE\u964d\u4f4e\u4e8626%\uff0c\u5e76\u8fbe\u5230\u4e0e\u5b8c\u5168\u76d1\u7763\u76f8\u5f53\u7684\u6548\u679c\u3002", "conclusion": "JEPA\u98ce\u683c\u7684\u9884\u8bad\u7ec3\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6570\u636e\u9ad8\u6548\u7684\u5b66\u4e60\u6293\u53d6\u3002"}}
{"id": "2509.13334", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13334", "abs": "https://arxiv.org/abs/2509.13334", "authors": ["Anand Swaroop", "Akshat Nallani", "Saksham Uboweja", "Adiliia Uzdenova", "Michael Nguyen", "Kevin Zhu", "Sunishchal Dev", "Ashwinee Panda", "Vasu Sharma", "Maheep Chaudhary"], "title": "FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness", "comment": null, "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving\nlarge language model performance on complex tasks, but recent work shows that\nreasoning steps often fail to causally influence the final answer, creating\nbrittle and untrustworthy outputs. Prior approaches focus primarily on\nmeasuring faithfulness, while methods for systematically improving it remain\nlimited. We introduce Faithful Reasoning via Intervention Training (FRIT), a\nscalable alignment method that trains models to produce causally consistent\nreasoning by learning from systematically corrupted examples. FRIT generates\nsynthetic training data by intervening on individual reasoning steps in\nmodel-generated CoTs, creating faithful/unfaithful pairs that highlight when\nreasoning breaks down. We then apply Direct Preference Optimization to teach\nmodels to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B\nand Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases\nfaithful reasoning by $3.4$ percentage points for Mistral on GSM8K while\nimproving accuracy by $7.6$ percentage points. Our approach provides the first\nscalable, supervision-free method for training language models to produce more\nreliable and interpretable reasoning, addressing a critical gap between\nreasoning performance and trustworthiness. We release our code at\n\\href{https://github.com/Anut-py/frit}.", "AI": {"tldr": "FRIT\u662f\u4e00\u79cd\u901a\u8fc7\u5e72\u9884\u8bad\u7ec3\u548c\u504f\u597d\u4f18\u5316\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u53ef\u9760\u6027\u7684\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u5fe0\u5b9e\u63a8\u7406\u548c\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709CoT\u63a8\u7406\u4e2d\u63a8\u7406\u6b65\u9aa4\u672a\u80fd\u56e0\u679c\u5f71\u54cd\u6700\u7ec8\u7b54\u6848\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u63a8\u7406\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u901a\u8fc7\u5e72\u9884\u8bad\u7ec3\uff08FRIT\uff09\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u5e94\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u6765\u6559\u5bfc\u6a21\u578b\u504f\u597d\u56e0\u679c\u4e00\u81f4\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728Qwen3-8B\u548cMistral-7B-v0.1\u4e0a\uff0cFRIT\u5c06Mistral\u5728GSM8K\u4e0a\u7684\u5fe0\u5b9e\u63a8\u7406\u63d0\u9ad8\u4e863.4\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u51c6\u786e\u7387\u63d0\u9ad8\u4e867.6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "FRIT\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u65e0\u9700\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u751f\u6210\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406\uff0c\u586b\u8865\u4e86\u63a8\u7406\u6027\u80fd\u4e0e\u53ef\u4fe1\u5ea6\u4e4b\u95f4\u7684\u5173\u952e\u7a7a\u767d\u3002"}}
{"id": "2509.13361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13361", "abs": "https://arxiv.org/abs/2509.13361", "authors": ["Tong Yulin", "Liang Xuechen"], "title": "Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention", "comment": null, "summary": "Expressway traffic congestion severely reduces travel efficiency and hinders\nregional connectivity. Existing \"detection-prediction\" systems have critical\nflaws: low vehicle perception accuracy under occlusion and loss of\nlong-sequence dependencies in congestion forecasting. This study proposes an\nintegrated technical framework to resolve these issues.For traffic flow\nperception, two baseline algorithms were optimized. Traditional YOLOv11 was\nupgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort\nwas improved by fusing Mahalanobis (motion) and cosine (appearance) distances.\nExperiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\\%\nmAP (6.5 percentage points higher than baseline) with 5.3\\% occlusion miss\nrate. DeepSort reached 93.8\\% MOTA (11.3 percentage points higher than SORT)\nwith only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km\nhigh-density scenarios), speed and density showed a strong negative correlation\n(r=-0.97), conforming to traffic flow theory. For congestion warning, a\nGRU-Attention model was built to capture congestion precursors. Trained 300\nepochs with flow, density, and speed, it achieved 99.7\\% test accuracy (7-9\npercentage points higher than traditional GRU). In 10-minute advance warnings\nfor 30-minute congestion, time error was $\\leq$ 1 minute. Validation with an\nindependent video showed 95\\% warning accuracy, over 90\\% spatial overlap of\ncongestion points, and stable performance in high-flow ($>$5 vehicles/second)\nscenarios.This framework provides quantitative support for expressway\ncongestion control, with promising intelligent transportation applications.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7b97\u6cd5\u548c\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u901f\u516c\u8def\u62e5\u5835\u611f\u77e5\u548c\u9884\u8b66\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u9ad8\u901f\u516c\u8def\u4ea4\u901a\u62e5\u5835\u4e25\u91cd\u964d\u4f4e\u4e86\u51fa\u884c\u6548\u7387\u5e76\u963b\u788d\u533a\u57df\u8fde\u901a\u6027\uff0c\u73b0\u6709\u7cfb\u7edf\u5728\u906e\u6321\u4e0b\u7684\u8f66\u8f86\u611f\u77e5\u7cbe\u5ea6\u4f4e\u4e14\u957f\u5e8f\u5217\u4f9d\u8d56\u5173\u7cfb\u4e22\u5931\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u6280\u672f\u6846\u67b6\uff0c\u5305\u62ec\u4f18\u5316YOLOv11-DIoU\u548cDeepSort\u7b97\u6cd5\u7528\u4e8e\u4ea4\u901a\u6d41\u611f\u77e5\uff0c\u4ee5\u53ca\u6784\u5efaGRU-Attention\u6a21\u578b\u7528\u4e8e\u62e5\u5835\u9884\u8b66\u3002", "result": "YOLOv11-DIoU\u7684mAP\u8fbe\u523095.7%\uff0cDeepSort\u7684MOTA\u8fbe\u523093.8%\uff1bGRU-Attention\u6a21\u578b\u5728\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u8fbe\u523099.7%\uff0c\u9884\u8b66\u65f6\u95f4\u8bef\u5dee\u22641\u5206\u949f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u901f\u516c\u8def\u62e5\u5835\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9a\u91cf\u652f\u6301\uff0c\u5177\u6709\u5e7f\u9614\u7684\u667a\u80fd\u4ea4\u901a\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.13978", "categories": ["cs.DC", "cs.AI", "cs.DB", "68M14, 68M20, 68T07", "C.2.4; D.1.3; I.2.0"], "pdf": "https://arxiv.org/pdf/2509.13978", "abs": "https://arxiv.org/abs/2509.13978", "authors": ["Renan Souza", "Timothy Poteet", "Brian Etz", "Daniel Rosendo", "Amal Gueroudji", "Woong Shin", "Prasanna Balaprakash", "Rafael Ferreira da Silva"], "title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology", "comment": "Paper accepted in the proceedings of the ACM/IEEE Supercomputing\n  Conference (SC). Cite it as Renan Souza, Timothy Poteet, Brian Etz, Daniel\n  Rosendo, Amal Gueroudji, Woong Shin, Prasanna Balaprakash, and Rafael\n  Ferreira da Silva. 2025. LLM Agents for Interactive Workflow Provenance:\n  Reference Architecture and Evaluation Methodology. In SC Workshops (WORKS)", "summary": "Modern scientific discovery increasingly relies on workflows that process\ndata across the Edge, Cloud, and High Performance Computing (HPC) continuum.\nComprehensive and in-depth analyses of these data are critical for hypothesis\nvalidation, anomaly detection, reproducibility, and impactful findings.\nAlthough workflow provenance techniques support such analyses, at large scale,\nthe provenance data become complex and difficult to analyze. Existing systems\ndepend on custom scripts, structured queries, or static dashboards, limiting\ndata interaction. In this work, we introduce an evaluation methodology,\nreference architecture, and open-source implementation that leverages\ninteractive Large Language Model (LLM) agents for runtime data analysis. Our\napproach uses a lightweight, metadata-driven design that translates natural\nlanguage into structured provenance queries. Evaluations across LLaMA, GPT,\nGemini, and Claude, covering diverse query classes and a real-world chemistry\nworkflow, show that modular design, prompt tuning, and Retrieval-Augmented\nGeneration (RAG) enable accurate and insightful LLM agent responses beyond\nrecorded provenance.", "AI": {"tldr": "\u63d0\u51fa\u4ea4\u4e92\u5f0fLLM\u4ee3\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5143\u6570\u636e\u9a71\u52a8\u8bbe\u8ba1\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u4e3a\u6eaf\u6e90\u67e5\u8be2\uff0c\u8bc4\u4f30\u663e\u793a\u6a21\u5757\u5316\u8bbe\u8ba1\u548cRAG\u6280\u672f\u63d0\u5347\u5206\u6790\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u53d1\u73b0\u4f9d\u8d56\u4e8e\u8de8\u8fb9\u7f18\u3001\u4e91\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u8fde\u7eed\u4f53\u7684\u6570\u636e\u5904\u7406\u5de5\u4f5c\u6d41\uff0c\u4f46\u5927\u89c4\u6a21\u6eaf\u6e90\u6570\u636e\u590d\u6742\u4e14\u96be\u4ee5\u5206\u6790\uff0c\u73b0\u6709\u7cfb\u7edf\u5b58\u5728\u4ea4\u4e92\u9650\u5236\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\u8bba\u3001\u53c2\u8003\u67b6\u6784\u548c\u5f00\u6e90\u5b9e\u73b0\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u3001\u5143\u6570\u636e\u9a71\u52a8\u7684\u8bbe\u8ba1\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6eaf\u6e90\u67e5\u8be2\u3002", "result": "\u8bc4\u4f30\u6db5\u76d6\u4e86LLaMA\u3001GPT\u3001Gemini\u548cClaude\u7b49\u591a\u79cd\u67e5\u8be2\u7c7b\u522b\u53ca\u771f\u5b9e\u5316\u5b66\u5de5\u4f5c\u6d41\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4ea4\u4e92\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u8fdb\u884c\u8fd0\u884c\u65f6\u6570\u636e\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u3001\u63d0\u793a\u8c03\u6574\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u4e14\u5bcc\u6709\u6d1e\u5bdf\u529b\u7684\u54cd\u5e94\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u8bb0\u5f55\u5f0f\u6eaf\u6e90\u7684\u9650\u5236\u3002"}}
{"id": "2509.13714", "categories": ["cs.NI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.13714", "abs": "https://arxiv.org/abs/2509.13714", "authors": ["Benoit Pit-Claudel", "Muriel M\u00e9dard", "Manya Ghobadi"], "title": "LINC: An In-Network Coding Approach to Tame Packet Loss in Hybrid Wireless-Fiber Backbones", "comment": null, "summary": "The emergence of ultra-low latency applications, such as financial\ntransactions, has driven the development of hybrid backbone networks that rely\non fiber, satellite, and microwave links. Despite providing low latencies,\nthese hybrid networks suffer from occasional environmental packet loss caused\nby poor weather, construction, and line of sight blockage. Paradoxically,\ntoday's hybrid backbones rely on conventional transport protocols that take\npacket loss to signal network congestion, as opposed to transient environmental\nobstacles. A common approach to address this challenge is to use network coding\n(NC) between the end hosts to recover from these occasional packet loss events.\nHowever, current NC proposals assume full access to the end-hosts' stack to\nperform end-to-end encoding/decoding operations. In this paper, we introduce\nLINC, a novel system that provides in-network NC capabilities to mitigate\nenvironmental packet loss events without requiring cooperation from the end\nhosts. LINC uses a systematic block coding approach on a link-by-link basis,\nencoding and decoding packets inside the network. We model the tradeoff in\ngoodput between end-to-end retransmissions and redundant packets introduced by\nLINC, and propose an optimization formulation to determine the optimal choice\nof coding parameters. Our simulations on real-world backbone topologies\ndemonstrate that LINC reduces the end-to-end latency by up to 18% by\neliminating unnecessary retransmissions.", "AI": {"tldr": "LINC\u662f\u4e00\u79cd\u65b0\u578b\u7f51\u7edc\u5185\u7f16\u7801\u7cfb\u7edf\uff0c\u901a\u8fc7\u9010\u94fe\u8def\u7f16\u7801\u51cf\u5c11\u73af\u5883\u4e22\u5305\uff0c\u65e0\u9700\u7ec8\u7aef\u4e3b\u673a\u914d\u5408\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u6df7\u5408\u9aa8\u5e72\u7f51\u7edc\u56e0\u73af\u5883\u4e22\u5305\u95ee\u9898\u5bfc\u81f4\u4f20\u7edf\u4f20\u8f93\u534f\u8bae\u8bef\u5224\u4e3a\u7f51\u7edc\u62e5\u585e\uff0c\u800c\u73b0\u6709\u7f51\u7edc\u7f16\u7801\u65b9\u6848\u9700\u8981\u7ec8\u7aef\u4e3b\u673a\u5b8c\u5168\u914d\u5408\u3002", "method": "LINC\u91c7\u7528\u7cfb\u7edf\u5757\u7f16\u7801\u65b9\u6cd5\uff0c\u5728\u7f51\u7edc\u5185\u9010\u94fe\u8def\u8fdb\u884c\u7f16\u7801\u548c\u89e3\u7801\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6a21\u578b\u9009\u62e9\u6700\u4f73\u7f16\u7801\u53c2\u6570\u3002", "result": "\u5728\u771f\u5b9e\u9aa8\u5e72\u7f51\u7edc\u62d3\u6251\u4e0a\u7684\u6a21\u62df\u663e\u793a\uff0cLINC\u53ef\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe18%\u3002", "conclusion": "LINC\u901a\u8fc7\u5f15\u5165\u7f51\u7edc\u5185\u7f16\u7801\u80fd\u529b\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u73af\u5883\u4e22\u5305\u5e26\u6765\u7684\u4e0d\u5fc5\u8981\u91cd\u4f20\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u4e86\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002"}}
{"id": "2509.13535", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13535", "abs": "https://arxiv.org/abs/2509.13535", "authors": ["S M Farah Al Fahim", "Md Nakhla Rafi", "Zeyang Ma", "Dong Jae Kim", "Tse-Hsun", "Chen"], "title": "Crash Report Enhancement with Large Language Models: An Empirical Study", "comment": null, "summary": "Crash reports are central to software maintenance, yet many lack the\ndiagnostic detail developers need to debug efficiently. We examine whether\nlarge language models can enhance crash reports by adding fault locations,\nroot-cause explanations, and repair suggestions. We study two enhancement\nstrategies: Direct-LLM, a single-shot approach that uses stack-trace context,\nand Agentic-LLM, an iterative approach that explores the repository for\nadditional evidence. On a dataset of 492 real-world crash reports, LLM-enhanced\nreports improve Top-1 problem-localization accuracy from 10.6% (original\nreports) to 40.2-43.1%, and produce suggested fixes that closely resemble\ndeveloper patches (CodeBLEU around 56-57%). Both our manual evaluations and\nLLM-as-a-judge assessment show that Agentic-LLM delivers stronger root-cause\nexplanations and more actionable repair guidance. A user study with 16\nparticipants further confirms that enhanced reports make crashes easier to\nunderstand and resolve, with the largest improvement in repair guidance. These\nresults indicate that supplying LLMs with stack traces and repository code\nyields enhanced crash reports that are substantially more useful for debugging.", "AI": {"tldr": "LLM\u80fd\u901a\u8fc7\u5806\u6808\u8ddf\u8e2a\u548c\u4ed3\u5e93\u4ee3\u7801\u589e\u5f3a\u5d29\u6e83\u62a5\u544a\uff0c\u63d0\u5347\u95ee\u9898\u5b9a\u4f4d\u51c6\u786e\u7387\u548c\u4fee\u590d\u5efa\u8bae\u8d28\u91cf\uff0cAgentic-LLM\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5d29\u6e83\u62a5\u544a\u5728\u8f6f\u4ef6\u7ef4\u62a4\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bb8\u591a\u62a5\u544a\u7f3a\u4e4f\u5f00\u53d1\u8005\u9ad8\u6548\u8c03\u8bd5\u6240\u9700\u7684\u8bca\u65ad\u7ec6\u8282\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cd\u589e\u5f3a\u7b56\u7565\uff1aDirect-LLM\uff08\u5355\u6b21\u4f7f\u7528\u5806\u6808\u8ddf\u8e2a\u4e0a\u4e0b\u6587\uff09\u548cAgentic-LLM\uff08\u8fed\u4ee3\u63a2\u7d22\u4ed3\u5e93\u4ee5\u83b7\u53d6\u66f4\u591a\u8bc1\u636e\uff09\u3002", "result": "\u5728492\u4e2a\u771f\u5b9e\u5d29\u6e83\u62a5\u544a\u6570\u636e\u96c6\u4e0a\uff0cLLM\u589e\u5f3a\u7684\u62a5\u544a\u5c06Top-1\u95ee\u9898\u5b9a\u4f4d\u51c6\u786e\u7387\u4ece10.6%\u63d0\u5347\u81f340.2-43.1%\uff0c\u5e76\u751f\u6210\u4e0e\u5f00\u53d1\u8005\u8865\u4e01\u76f8\u4f3c\u7684\u4fee\u590d\u5efa\u8bae\uff08CodeBLEU\u7ea656-57%\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5806\u6808\u8ddf\u8e2a\u548c\u4ed3\u5e93\u4ee3\u7801\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5d29\u6e83\u62a5\u544a\u7684\u5b9e\u7528\u6027\uff0c\u4f7f\u5176\u66f4\u6709\u5229\u4e8e\u8c03\u8bd5\u3002"}}
{"id": "2509.13378", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13378", "abs": "https://arxiv.org/abs/2509.13378", "authors": ["Mattias Wingren", "S\u00f6ren Andersson", "Sara Rosenberg", "Malin Andtfolk", "Susanne H\u00e4gglund", "Prashani Jayasingha Arachchige", "Linda Nyholm"], "title": "Using role-play and Hierarchical Task Analysis for designing human-robot interaction", "comment": "11 pages. This is a preprint version of the published paper in the\n  International Conference on Social Robotics:\n  https://link.springer.com/chapter/10.1007/978-981-96-3522-1_28", "summary": "We present the use of two methods we believe warrant more use than they\ncurrently have in the field of human-robot interaction: role-play and\nHierarchical Task Analysis. Some of its potential is showcased through our use\nof them in an ongoing research project which entails developing a robot\napplication meant to assist at a community pharmacy. The two methods have\nprovided us with several advantages. The role-playing provided a controlled and\nadjustable environment for understanding the customers' needs where pharmacists\ncould act as models for the robot's behavior; and the Hierarchical Task\nAnalysis ensured the behavior displayed was modelled correctly and aided\ndevelopment through facilitating co-design. Future research could focus on\ndeveloping task analysis methods especially suited for social robot\ninteraction.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u89d2\u8272\u626e\u6f14\u548c\u5c42\u6b21\u4efb\u52a1\u5206\u6790\u5728\u673a\u5668\u4eba\u8f85\u52a9\u836f\u623f\u9879\u76ee\u4e2d\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u4f18\u52bf\uff0c\u5e76\u5efa\u8bae\u672a\u6765\u7814\u7a76\u5f00\u53d1\u66f4\u9002\u5408\u793e\u4ea4\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u4efb\u52a1\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u5c55\u793a\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u8f85\u52a9\u793e\u533a\u836f\u623f\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u4fc3\u8fdb\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u89d2\u8272\u626e\u6f14\u548c\u5c42\u6b21\u4efb\u52a1\u5206\u6790", "result": "\u89d2\u8272\u626e\u6f14\u63d0\u4f9b\u4e86\u53ef\u63a7\u4e14\u53ef\u8c03\u6574\u7684\u73af\u5883\u6765\u7406\u89e3\u5ba2\u6237\u9700\u6c42\uff0c\u5c42\u6b21\u4efb\u52a1\u5206\u6790\u786e\u4fdd\u4e86\u884c\u4e3a\u7684\u6b63\u786e\u5efa\u6a21\u5e76\u4fc3\u8fdb\u4e86\u5171\u540c\u8bbe\u8ba1\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u53ef\u4e13\u6ce8\u4e8e\u5f00\u53d1\u7279\u522b\u9002\u5408\u793e\u4ea4\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u4efb\u52a1\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2509.13339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13339", "abs": "https://arxiv.org/abs/2509.13339", "authors": ["Ming Jin", "Hyunin Lee"], "title": "Position: AI Safety Must Embrace an Antifragile Perspective", "comment": null, "summary": "This position paper contends that modern AI research must adopt an\nantifragile perspective on safety -- one in which the system's capacity to\nguarantee long-term AI safety such as handling rare or out-of-distribution\n(OOD) events expands over time. Conventional static benchmarks and single-shot\nrobustness tests overlook the reality that environments evolve and that models,\nif left unchallenged, can drift into maladaptation (e.g., reward hacking,\nover-optimization, or atrophy of broader capabilities). We argue that an\nantifragile approach -- Rather than striving to rapidly reduce current\nuncertainties, the emphasis is on leveraging those uncertainties to better\nprepare for potentially greater, more unpredictable uncertainties in the future\n-- is pivotal for the long-term reliability of open-ended ML systems. In this\nposition paper, we first identify key limitations of static testing, including\nscenario diversity, reward hacking, and over-alignment. We then explore the\npotential of antifragile solutions to manage rare events. Crucially, we\nadvocate for a fundamental recalibration of the methods used to measure,\nbenchmark, and continually improve AI safety over the long term, complementing\nexisting robustness approaches by providing ethical and practical guidelines\ntowards fostering an antifragile AI safety community.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20AI\u5b89\u5168\u7814\u7a76\u5e94\u91c7\u7528\u6297\u8106\u5f31\u6027\u89c6\u89d2\uff0c\u901a\u8fc7\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u5e94\u5bf9\u672a\u6765\u6311\u6218\uff0c\u800c\u975e\u4ec5\u4ec5\u4f18\u5316\u5f53\u524d\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u957f\u671f\u5b89\u5168\u6539\u8fdb\u7684\u65b9\u6cd5\u548c\u793e\u533a\u6307\u5357\u3002", "motivation": "\u73b0\u4ee3AI\u7814\u7a76\u9700\u8981\u5e94\u5bf9\u73af\u5883\u6f14\u53d8\u548c\u6a21\u578b\u53ef\u80fd\u51fa\u73b0\u7684\u9002\u5e94\u4e0d\u826f\uff08\u5982\u5956\u52b1\u9ed1\u5ba2\u3001\u8fc7\u5ea6\u4f18\u5316\u6216\u80fd\u529b\u840e\u7f29\uff09\uff0c\u9759\u6001\u57fa\u51c6\u548c\u4e00\u6b21\u6027\u9c81\u68d2\u6027\u6d4b\u8bd5\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002", "method": "\u8bba\u6587\u9996\u5148\u8bc6\u522b\u4e86\u9759\u6001\u6d4b\u8bd5\u7684\u5173\u952e\u5c40\u9650\u6027\uff08\u5982\u573a\u666f\u591a\u6837\u6027\u3001\u5956\u52b1\u9ed1\u5ba2\u548c\u8fc7\u5ea6\u5bf9\u9f50\uff09\uff0c\u7136\u540e\u63a2\u8ba8\u4e86\u6297\u8106\u5f31\u6027\u89e3\u51b3\u65b9\u6848\u7ba1\u7406\u7f55\u89c1\u4e8b\u4ef6\u7684\u6f5c\u529b\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6839\u672c\u6027\u7684\u65b9\u6cd5\u8c03\u6574\uff0c\u7528\u4e8e\u957f\u671f\u6d4b\u91cf\u3001\u57fa\u51c6\u5316\u548c\u6301\u7eed\u6539\u8fdbAI\u5b89\u5168\uff0c\u8865\u5145\u73b0\u6709\u9c81\u68d2\u6027\u65b9\u6cd5\uff0c\u5e76\u4e3a\u57f9\u517b\u6297\u8106\u5f31\u6027AI\u5b89\u5168\u793e\u533a\u63d0\u4f9b\u4f26\u7406\u548c\u5b9e\u8df5\u6307\u5357\u3002", "conclusion": "\u8bba\u6587\u4e3b\u5f20\u73b0\u4ee3AI\u7814\u7a76\u5e94\u91c7\u7528\u6297\u8106\u5f31\u6027\u89c6\u89d2\u6765\u786e\u4fdd\u957f\u671fAI\u5b89\u5168\uff0c\u5f3a\u8c03\u901a\u8fc7\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u6765\u5e94\u5bf9\u672a\u6765\u66f4\u5927\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u800c\u975e\u4ec5\u4ec5\u51cf\u5c11\u5f53\u524d\u7684\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2509.13366", "categories": ["cs.CV", "68U99", "J.2"], "pdf": "https://arxiv.org/pdf/2509.13366", "abs": "https://arxiv.org/abs/2509.13366", "authors": ["Tony Rohe", "Martin Margreiter", "Markus Moertl"], "title": "Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks", "comment": "10 pages, 5 figures", "summary": "This research is part of a study of a real-time, cloud-based on-street\nparking service using crowd-sourced in-vehicle fleet data. The service provides\nreal-time information about available parking spots by classifying\ncrowd-sourced detections observed via ultrasonic sensors. The goal of this\nresearch is to optimize the current parking service quality by analyzing the\nautomation of the existing test process for ground truth tests. Therefore,\nmethods from the field of machine learning, especially image pattern\nrecognition, are applied to enrich the database and substitute human\nengineering work in major areas of the analysis process. After an introduction\ninto the related areas of machine learning, this paper explains the methods and\nimplementations made to achieve a high level of automation, applying\nconvolutional neural networks. Finally, predefined metrics present the\nperformance level achieved, showing a time reduction of human resources up to\n99.58 %. The overall improvements are discussed, summarized, and followed by an\noutlook for future development and potential application of the analysis\nautomation tool.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\uff08\u5c24\u5176\u662f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u81ea\u52a8\u5316\u8def\u8fb9\u505c\u8f66\u670d\u52a1\u7684\u6d4b\u8bd5\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u529b\u9700\u6c42\u5e76\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u4f18\u5316\u73b0\u6709\u7684\u5b9e\u65f6\u8def\u8fb9\u505c\u8f66\u670d\u52a1\u8d28\u91cf\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u66ff\u4ee3\u4eba\u5de5\u5de5\u7a0b\u5de5\u4f5c\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u5e94\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u56fe\u50cf\u6a21\u5f0f\u8bc6\u522b\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u5b9e\u73b0\u6d4b\u8bd5\u8fc7\u7a0b\u7684\u81ea\u52a8\u5316\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u8fbe99.58%\u7684\u4eba\u529b\u8d44\u6e90\u65f6\u95f4\u51cf\u5c11\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u5e94\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u4e86\u9ad8\u6c34\u5e73\u7684\u81ea\u52a8\u5316\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u529b\u8d44\u6e90\u9700\u6c42\uff08\u9ad8\u8fbe99.58%\uff09\uff0c\u5e76\u8ba8\u8bba\u4e86\u6574\u4f53\u6539\u8fdb\u53ca\u672a\u6765\u53d1\u5c55\u7684\u6f5c\u5728\u5e94\u7528\u3002"}}
{"id": "2509.13631", "categories": ["cs.CV", "cs.DC", "14J60", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.13631", "abs": "https://arxiv.org/abs/2509.13631", "authors": ["Yuvraj Dutta", "Aaditya Sikder", "Basabdatta Palit"], "title": "Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery", "comment": "6 pages, 7 figures, accepted at IEEE INDISCON 2025", "summary": "Accurate identification of deforestation from satellite images is essential\nin order to understand the geographical situation of an area. This paper\nintroduces a new distributed approach to identify as well as locate\ndeforestation across different clients using Federated Learning (FL). Federated\nLearning enables distributed network clients to collaboratively train a model\nwhile maintaining data privacy and security of the active users. In our\nframework, a client corresponds to an edge satellite center responsible for\nlocal data processing. Moreover, FL provides an advantage over centralized\ntraining method which requires combining data, thereby compromising with data\nsecurity of the clients. Our framework leverages the FLOWER framework with RAY\nframework to execute the distributed learning workload. Furthermore, efficient\nclient spawning is ensured by RAY as it can select definite amount of users to\ncreate an emulation environment. Our FL framework uses YOLOS-small (a Vision\nTransformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN\nwith a MobileNetV3 backbone models trained and tested on publicly available\ndatasets. Our approach provides us a different view for image\nsegmentation-based tasks on satellite imagery.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u5206\u5e03\u5f0f\u68ee\u6797\u780d\u4f10\u8bc6\u522b\u65b9\u6cd5\uff0c\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u65b9\u6cd5\u9700\u8981\u5408\u5e76\u6570\u636e\uff0c\u53ef\u80fd\u5371\u53ca\u6570\u636e\u5b89\u5168\uff0c\u800c\u8054\u90a6\u5b66\u4e60\u53ef\u4ee5\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u534f\u4f5c\u8bad\u7ec3\u3002", "method": "\u5229\u7528FLOWER\u6846\u67b6\u548cRAY\u6846\u67b6\u5b9e\u73b0\u5206\u5e03\u5f0f\u5b66\u4e60\uff0c\u91c7\u7528YOLOS-small\u3001Faster R-CNN\uff08ResNet50\u9aa8\u5e72\uff09\u548cFaster R-CNN\uff08MobileNetV3\u9aa8\u5e72\uff09\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u8be5\u65b9\u6cd5\u4e3a\u536b\u661f\u56fe\u50cf\u7684\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u901a\u8fc7\u516c\u5f00\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u5206\u5e03\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u51c6\u786e\u8bc6\u522b\u548c\u5b9a\u4f4d\u68ee\u6797\u780d\u4f10\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u6570\u636e\u9690\u79c1\u548c\u5b89\u5168\u3002"}}
{"id": "2509.13724", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13724", "abs": "https://arxiv.org/abs/2509.13724", "authors": ["Jan Janak", "Kahlil Dozier", "Lauren Berny", "Liang Hu", "Dan Rubenstein", "Charles Jennings", "Henning Schulzrinne"], "title": "Conducting Mission-Critical Voice Experiments with Automated Speech Recognition and Crowdsourcing", "comment": null, "summary": "Mission-critical voice (MCV) communications systems have been a critical tool\nfor the public safety community for over eight decades. Public safety users\nexpect MCV systems to operate reliably and consistently, particularly in\nchallenging conditions. Because of these expectations, the Public Safety\nCommunications Research (PSCR) Division of the National Institute of Standards\nand Technology (NIST) has been interested in correlating impairments in MCV\ncommunication systems and public safety user quality of experience (QoE).\nPrevious research has studied MCV voice quality and intelligibility in a\ncontrolled environment. However, such research has been limited by the\nchallenges inherent in emulating real-world environmental conditions.\nAdditionally, there is the question of the best metric to use to reflect QoE\naccurately.\n  This paper describes our efforts to develop the methodology and tools for\nhuman-subject experiments with MCV. We illustrate their use in human-subject\nexperiments in emulated real-world environments. The tools include a testbed\nfor emulating real-world MCV systems and an automated speech recognition (ASR)\nrobot approximating human subjects in transcription tasks. We evaluate QoE\nthrough a Levenshtein Distance-based metric, arguing it is a suitable proxy for\nmeasuring comprehension and the QoE. We conducted human-subject studies with\nAmazon MTurk volunteers to understand the influence of selected system\nparameters and impairments on human subject performance and end-user QoE. We\nalso compare the performance of several ASR system configurations with\nhuman-subject performance. We find that humans generally perform better than\nASR in accuracy-related MCV tasks and that the codec significantly influences\nthe end-user QoE and ASR performance.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u6a21\u62df\u771f\u5b9e\u73af\u5883\u7684MCV\u7cfb\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u53d1\u73b0\u4eba\u7c7b\u5728MCV\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eASR\uff0c\u7f16\u89e3\u7801\u5668\u663e\u8457\u5f71\u54cdQoE\u3002", "motivation": "\u7531\u4e8e\u516c\u5171\u5b89\u5168\u7528\u6237\u5bf9MCV\u7cfb\u7edf\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u53ef\u9760\u8fd0\u884c\u7684\u671f\u671b\uff0c\u9700\u8981\u7814\u7a76MCV\u7cfb\u7edf\u4e2d\u7684\u635f\u4f24\u4e0e\u7528\u6237QoE\u7684\u76f8\u5173\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u7528\u4e8e\u4eba\u7c7b\u53d7\u8bd5\u8005\u5b9e\u9a8c\u7684\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u5305\u62ec\u6a21\u62df\u771f\u5b9e\u4e16\u754cMCV\u7cfb\u7edf\u7684\u6d4b\u8bd5\u53f0\u548c\u8fd1\u4f3c\u4eba\u7c7b\u53d7\u8bd5\u8005\u7684ASR\u673a\u5668\u4eba\u3002\u901a\u8fc7Levenshtein\u8ddd\u79bb\u5ea6\u91cf\u8bc4\u4f30QoE\u3002", "result": "\u4eba\u7c7b\u53d7\u8bd5\u8005\u5728\u51c6\u786e\u6027\u4efb\u52a1\u4e2d\u4f18\u4e8eASR\uff0c\u7f16\u89e3\u7801\u5668\u5bf9QoE\u548cASR\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u4eba\u7c7b\u5728\u51c6\u786e\u6027\u76f8\u5173\u7684MCV\u4efb\u52a1\u4e2d\u901a\u5e38\u8868\u73b0\u4f18\u4e8eASR\uff0c\u4e14\u7f16\u89e3\u7801\u5668\u5bf9\u6700\u7ec8\u7528\u6237\u7684QoE\u548cASR\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002"}}
{"id": "2509.13650", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13650", "abs": "https://arxiv.org/abs/2509.13650", "authors": ["Amena Amro", "Manar H. Alalfi"], "title": "GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?", "comment": null, "summary": "As software development practices increasingly adopt AI-powered tools,\nensuring that such tools can support secure coding has become critical. This\nstudy evaluates the effectiveness of GitHub Copilot's recently introduced code\nreview feature in detecting security vulnerabilities. Using a curated set of\nlabeled vulnerable code samples drawn from diverse open-source projects\nspanning multiple programming languages and application domains, we\nsystematically assessed Copilot's ability to identify and provide feedback on\ncommon security flaws. Contrary to expectations, our results reveal that\nCopilot's code review frequently fails to detect critical vulnerabilities such\nas SQL injection, cross-site scripting (XSS), and insecure deserialization.\nInstead, its feedback primarily addresses low-severity issues, such as coding\nstyle and typographical errors. These findings expose a significant gap between\nthe perceived capabilities of AI-assisted code review and its actual\neffectiveness in supporting secure development practices. Our results highlight\nthe continued necessity of dedicated security tools and manual code audits to\nensure robust software security.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0GitHub Copilot\u7684\u4ee3\u7801\u5ba1\u67e5\u529f\u80fd\u5728\u68c0\u6d4b\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\u65b9\u9762\u6548\u679c\u6709\u9650\uff0c\u4e3b\u8981\u5173\u6ce8\u4f4e\u98ce\u9669\u95ee\u9898\uff0c\u5f3a\u8c03\u4e86\u4e13\u7528\u5b89\u5168\u5de5\u5177\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u4e2dAI\u5de5\u5177\u7684\u666e\u53ca\uff0c\u786e\u4fdd\u8fd9\u4e9b\u5de5\u5177\u652f\u6301\u5b89\u5168\u7f16\u7801\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u6765\u81ea\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u5e94\u7528\u9886\u57df\u7684\u6807\u8bb0\u6f0f\u6d1e\u4ee3\u7801\u6837\u672c\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30Copilot\u8bc6\u522b\u548c\u53cd\u9988\u5e38\u89c1\u5b89\u5168\u7f3a\u9677\u7684\u80fd\u529b\u3002", "result": "Copilot\u7684\u4ee3\u7801\u5ba1\u67e5\u7ecf\u5e38\u672a\u80fd\u68c0\u6d4b\u5230SQL\u6ce8\u5165\u3001\u8de8\u7ad9\u811a\u672c\uff08XSS\uff09\u548c\u4e0d\u5b89\u5168\u53cd\u5e8f\u5217\u5316\u7b49\u5173\u952e\u6f0f\u6d1e\uff0c\u53cd\u9988\u4e3b\u8981\u9488\u5bf9\u7f16\u7801\u98ce\u683c\u548c\u6392\u7248\u9519\u8bef\u7b49\u4f4e\u4e25\u91cd\u6027\u95ee\u9898\u3002", "conclusion": "GitHub Copilot\u7684\u4ee3\u7801\u5ba1\u67e5\u529f\u80fd\u5728\u68c0\u6d4b\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u5173\u6ce8\u4f4e\u4e25\u91cd\u6027\u95ee\u9898\uff0c\u63ed\u793a\u4e86AI\u8f85\u52a9\u4ee3\u7801\u5ba1\u67e5\u7684\u611f\u77e5\u80fd\u529b\u4e0e\u5b9e\u9645\u6548\u679c\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u4e13\u7528\u5b89\u5168\u5de5\u5177\u548c\u624b\u52a8\u4ee3\u7801\u5ba1\u8ba1\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.13380", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13380", "abs": "https://arxiv.org/abs/2509.13380", "authors": ["Alejandro D. Mousist"], "title": "ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy", "comment": "This preprint presents ASTREA, a multi-agent architecture combining\n  LLM-guided semantic modulation with reinforcement learning for autonomous\n  satellite operations. The system is validated in hardware orbital\n  environments", "summary": "This paper presents ASTREA, the first agentic system deployed on\nflight-heritage hardware (TRL 9) for autonomous spacecraft operations. Using\nthermal control as a representative use case, we integrate a\nresource-constrained Large Language Model (LLM) agent with a reinforcement\nlearning controller in an asynchronous architecture tailored for\nspace-qualified platforms. Ground experiments show that LLM-guided supervision\nimproves thermal stability and reduces violations, confirming the feasibility\nof combining semantic reasoning with adaptive control under hardware\nconstraints. However, on-orbit validation aboard the International Space\nStation (ISS) reveals performance degradation caused by inference latency\nmismatched with the rapid thermal cycles characteristic of Low Earth Orbit\n(LEO) satellites. These results highlight both the opportunities and current\nlimitations of agentic LLM-based systems in real flight environments, providing\npractical design guidelines for future space autonomy.", "AI": {"tldr": "ASTREA\u662f\u9996\u4e2a\u5728\u98de\u884c\u786c\u4ef6\u4e0a\u90e8\u7f72\u7684LLM\u4ee3\u7406\u7cfb\u7edf\uff0c\u5730\u9762\u5b9e\u9a8c\u6210\u529f\uff0c\u4f46\u8f68\u9053\u9a8c\u8bc1\u663e\u793a\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u5f00\u53d1\u9996\u4e2a\u5728\u98de\u884c\u9057\u4ea7\u786c\u4ef6\uff08TRL 9\uff09\u4e0a\u90e8\u7f72\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u4e3b\u822a\u5929\u5668\u64cd\u4f5c\uff0c\u4ee5\u70ed\u63a7\u5236\u4e3a\u4ee3\u8868\u7528\u4f8b\u3002", "method": "\u6574\u5408\u4e86\u8d44\u6e90\u53d7\u9650\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u4e0e\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u91c7\u7528\u5f02\u6b65\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u7a7a\u95f4\u8ba4\u8bc1\u5e73\u53f0\u3002", "result": "\u5730\u9762\u5b9e\u9a8c\u663e\u793aLLM\u5f15\u5bfc\u7684\u76d1\u7763\u63d0\u9ad8\u4e86\u70ed\u7a33\u5b9a\u6027\u5e76\u51cf\u5c11\u4e86\u8fdd\u89c4\uff0c\u4f46\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\uff08ISS\uff09\u7684\u8f68\u9053\u9a8c\u8bc1\u4e2d\uff0c\u7531\u4e8e\u63a8\u7406\u5ef6\u8fdf\u4e0e\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u7684\u5feb\u901f\u70ed\u5468\u671f\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "ASTREA\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u98de\u884c\u73af\u5883\u4e2d\u90e8\u7f72\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\u7684\u6f5c\u529b\u548c\u5f53\u524d\u9650\u5236\uff0c\u4e3a\u672a\u6765\u7a7a\u95f4\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u8bbe\u8ba1\u6307\u5357\u3002"}}
{"id": "2509.13341", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13341", "abs": "https://arxiv.org/abs/2509.13341", "authors": ["Ahmet H. G\u00fczel", "Matthew Thomas Jackson", "Jarek Luca Liesen", "Tim Rockt\u00e4schel", "Jakob Nicolaus Foerster", "Ilija Bogunovic", "Jack Parker-Holder"], "title": "Imagined Autocurricula", "comment": null, "summary": "Training agents to act in embodied environments typically requires vast\ntraining data or access to accurate simulation, neither of which exists for\nmany cases in the real world. Instead, world models are emerging as an\nalternative leveraging offline, passively collected data, they make it possible\nto generate diverse worlds for training agents in simulation. In this work, we\nharness world models to generate imagined environments to train robust agents\ncapable of generalizing to novel task variations. One of the challenges in\ndoing this is ensuring the agent trains on useful generated data. We thus\npropose a novel approach, IMAC (Imagined Autocurricula), leveraging\nUnsupervised Environment Design (UED), which induces an automatic curriculum\nover generated worlds. In a series of challenging, procedurally generated\nenvironments, we show it is possible to achieve strong transfer performance on\nheld-out environments, having trained only inside a world model learned from a\nnarrower dataset. We believe this opens the path to utilizing larger-scale,\nfoundation world models for generally capable agents.", "AI": {"tldr": "\u5229\u7528\u4e16\u754c\u6a21\u578b\u751f\u6210\u60f3\u8c61\u73af\u5883\uff0c\u901a\u8fc7IMAC\u65b9\u6cd5\u81ea\u52a8\u751f\u6210\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u8bad\u7ec3\u51fa\u80fd\u5728\u65b0\u4efb\u52a1\u4e2d\u6cdb\u5316\u7684\u667a\u80fd\u4f53\u3002", "motivation": "\u89e3\u51b3\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7f3a\u4e4f\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6216\u51c6\u786e\u6a21\u62df\u7684\u9650\u5236\uff0c\u5229\u7528\u79bb\u7ebf\u88ab\u52a8\u6536\u96c6\u7684\u6570\u636e\u751f\u6210\u591a\u6837\u5316\u7684\u8bad\u7ec3\u73af\u5883\u3002", "method": "\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u751f\u6210\u60f3\u8c61\u73af\u5883\uff0c\u5e76\u5229\u7528IMAC\u65b9\u6cd5\u5728\u8fd9\u4e9b\u73af\u5883\u4e2d\u81ea\u52a8\u751f\u6210\u8bad\u7ec3\u8bfe\u7a0b\u3002", "result": "\u5728\u4e00\u7cfb\u5217\u5177\u6709\u6311\u6218\u6027\u7684\u7a0b\u5e8f\u751f\u6210\u73af\u5883\u4e2d\uff0c\u5c55\u793a\u4e86\u5728\u4ec5\u4f7f\u7528\u8f83\u7a84\u6570\u636e\u96c6\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u5185\u8bad\u7ec3\u540e\uff0c\u80fd\u591f\u5728\u4fdd\u7559\u73af\u5883\u4e2d\u5b9e\u73b0\u5f3a\u8fc1\u79fb\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIMAC\uff08Imagined Autocurricula\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\uff08UED\uff09\u5728\u751f\u6210\u7684\u4e16\u754c\u4e2d\u81ea\u52a8\u751f\u6210\u8bfe\u7a0b\uff0c\u4ece\u800c\u8bad\u7ec3\u51fa\u80fd\u591f\u5728\u65b0\u4efb\u52a1\u53d8\u4f53\u4e2d\u6cdb\u5316\u7684\u9c81\u68d2\u667a\u80fd\u4f53\u3002"}}
{"id": "2509.13375", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13375", "abs": "https://arxiv.org/abs/2509.13375", "authors": ["Yuxiao Lee", "Xiaofeng Cao", "Wei Ye", "Jiangchao Yao", "Jingkuan Song", "Heng Tao Shen"], "title": "An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity", "comment": null, "summary": "Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable\nzero-shot out-of-distribution (OOD) detection capabilities, vital for reliable\nAI systems. Despite this promising capability, a comprehensive understanding of\n(1) why they work so effectively, (2) what advantages do they have over\nsingle-modal methods, and (3) how is their behavioral robustness -- remains\nnotably incomplete within the research community. This paper presents a\nsystematic empirical analysis of VLM-based OOD detection using in-distribution\n(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and\nformalize key operational properties within the VLM embedding space that\nfacilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the\nsuperiority of these models over established single-modal approaches,\nattributing this distinct advantage to the VLM's capacity to leverage rich\nsemantic novelty. (3) Sensitivity: We uncovers a significant and previously\nunder-explored asymmetry in their robustness profile: while exhibiting\nresilience to common image noise, these VLM-based methods are highly sensitive\nto prompt phrasing. Our findings contribute a more structured understanding of\nthe strengths and critical vulnerabilities inherent in VLM-based OOD detection,\noffering crucial, empirically-grounded guidance for developing more robust and\nreliable future designs.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86VLM\u5728\u96f6\u6837\u672cOOD\u68c0\u6d4b\u4e2d\u7684\u673a\u5236\u3001\u4f18\u52bf\u53ca\u884c\u4e3a\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u5176\u5bf9\u63d0\u793a\u63aa\u8f9e\u654f\u611f\u4f46\u5bf9\u56fe\u50cf\u566a\u58f0\u9c81\u68d2\uff0c\u4e3a\u672a\u6765\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u5c3d\u7ba1VLM\u5728\u96f6\u6837\u672cOOD\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u5176\u5de5\u4f5c\u673a\u5236\u3001\u76f8\u5bf9\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u4f18\u52bf\u4ee5\u53ca\u884c\u4e3a\u9c81\u68d2\u6027\u7684\u7406\u89e3\u4ecd\u4e0d\u5b8c\u6574\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f7f\u7528ID\u548cOOD\u63d0\u793a\u5bf9VLM\u8fdb\u884c\u7cfb\u7edf\u5b9e\u8bc1\u5206\u6790\uff0c\u5305\u62ec\u673a\u5236\u3001\u4f18\u52bf\u548c\u654f\u611f\u6027\u4e09\u4e2a\u65b9\u9762\u7684\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1) VLM\u5d4c\u5165\u7a7a\u95f4\u7684\u5173\u952e\u64cd\u4f5c\u7279\u6027\uff1b(2) VLM\u5728\u5229\u7528\u4e30\u5bcc\u8bed\u4e49\u65b0\u9896\u6027\u65b9\u9762\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\uff1b(3) VLM\u5bf9\u63d0\u793a\u63aa\u8f9e\u9ad8\u5ea6\u654f\u611f\uff0c\u4f46\u5bf9\u56fe\u50cf\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5b9e\u8bc1\u5206\u6790\uff0c\u63ed\u793a\u4e86VLM\u5728\u96f6\u6837\u672cOOD\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u64cd\u4f5c\u7279\u6027\u3001\u76f8\u5bf9\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u4ee5\u53ca\u5176\u884c\u4e3a\u9c81\u68d2\u6027\u7684\u4e0d\u5bf9\u79f0\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u672a\u6765\u8bbe\u8ba1\u66f4\u7a33\u5065\u53ef\u9760\u7684VLM\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2509.13901", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13901", "abs": "https://arxiv.org/abs/2509.13901", "authors": ["Saptarshi Ghosh", "Ioannis Mavromatis", "Konstantinos Antonakoglou", "Konstantinos Katsaros"], "title": "Performance Evaluation of Intent-Based Networking Scenarios: A GitOps and Nephio Approach", "comment": "Accepted for publication at IEEE CSCN 2025", "summary": "GitOps has emerged as a foundational paradigm for managing cloud-native\ninfrastructures by enabling declarative configuration, version-controlled\nstate, and automated reconciliation between intents and runtime deployments.\nDespite its widespread adoption, the performance and scalability of GitOps\ntools in Intent-Based Networking (IBN) scenarios are insufficiently evaluated.\nThis paper presents a reproducible, metric-driven benchmarking, assessing the\nlatency and resource overheads of three widely used GitOps operators: Argo CD,\nFlux CD, and ConfigSync. We conduct controlled experiments under both single-\nand multi-intent scenarios, capturing key performance indicators such as\nlatency and resource consumption. Our results highlight trade-offs between the\ntools in terms of determinism, resource efficiency, and responsiveness. We\nfurther investigate a realistic orchestration scenario, using Nephio as our\norchestrator, to quantify the processing latency and overhead in declarative\nend-to-end deployment pipelines. Our findings can offer valuable insights for\ntool selection and optimisation in future autonomous network orchestration\nsystems.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e09\u79cdGitOps\u5de5\u5177\u5728IBN\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u5ef6\u8fdf\u3001\u8d44\u6e90\u5f00\u9500\u548c\u54cd\u5e94\u6027\u65b9\u9762\u7684\u6743\u8861\uff0c\u5e76\u63d0\u4f9b\u4e86\u5de5\u5177\u9009\u62e9\u548c\u4f18\u5316\u7684\u5efa\u8bae\u3002", "motivation": "GitOps\u4f5c\u4e3a\u7ba1\u7406\u4e91\u539f\u751f\u57fa\u7840\u8bbe\u65bd\u7684\u57fa\u7840\u8303\u5f0f\uff0c\u5176\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u5728\u610f\u56fe\u9a71\u52a8\u7f51\u7edc\uff08IBN\uff09\u573a\u666f\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u6587\u91c7\u7528\u53ef\u91cd\u590d\u7684\u3001\u57fa\u4e8e\u6307\u6807\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684GitOps\u64cd\u4f5c\u5458\uff08Argo CD\u3001Flux CD\u548cConfigSync\uff09\u7684\u5ef6\u8fdf\u548c\u8d44\u6e90\u5f00\u9500\u3002\u5b9e\u9a8c\u5728\u5355\u610f\u56fe\u548c\u591a\u610f\u56fe\u573a\u666f\u4e0b\u8fdb\u884c\uff0c\u6355\u83b7\u4e86\u5173\u952e\u6027\u80fd\u6307\u6807\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u8fd9\u4e9b\u5de5\u5177\u5728\u786e\u5b9a\u6027\u3001\u8d44\u6e90\u6548\u7387\u548c\u54cd\u5e94\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u901a\u8fc7Nephio\u4f5c\u4e3a\u7f16\u6392\u5668\u7684\u5b9e\u9645\u573a\u666f\u91cf\u5316\u4e86\u58f0\u660e\u6027\u7aef\u5230\u7aef\u90e8\u7f72\u7ba1\u9053\u7684\u5904\u7406\u5ef6\u8fdf\u548c\u5f00\u9500\u3002", "conclusion": "\u672c\u6587\u7684\u7ed3\u8bba\u5f3a\u8c03\u4e86GitOps\u5de5\u5177\u5728IBN\u573a\u666f\u4e0b\u7684\u6027\u80fd\u6743\u8861\uff0c\u4e3a\u672a\u6765\u81ea\u4e3b\u7f51\u7edc\u7f16\u6392\u7cfb\u7edf\u7684\u5de5\u5177\u9009\u62e9\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2509.13656", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13656", "abs": "https://arxiv.org/abs/2509.13656", "authors": ["Yingao Elaine Yao", "Vedant Nimje", "Varun Viswanath", "Saikat Dutta"], "title": "A Regression Testing Framework with Automated Assertion Generation for Machine Learning Notebooks", "comment": "22 pages, 2 figures, 6 tables", "summary": "Notebooks have become the de-facto choice for data scientists and machine\nlearning engineers for prototyping and experimenting with machine learning (ML)\npipelines. Notebooks provide an interactive interface for code, data, and\nvisualization. However, notebooks provide very limited support for testing.\nThus, during continuous development, many subtle bugs that do not lead to\ncrashes often go unnoticed and cause silent errors that manifest as performance\nregressions.\n  To address this, we introduce NBTest - the first regression testing framework\nthat allows developers to write cell-level assertions in notebooks and run such\nnotebooks in pytest or in continuous integration (CI) pipelines. NBTest offers\na library of assertion APIs, and a JupyterLab plugin that enables executing\nassertions. We also develop the first automated approach for generating\ncell-level assertions for key components in ML notebooks, such as data\nprocessing, model building, and model evaluation. NBTest aims to improve the\nreliability and maintainability of ML notebooks without adding developer\nburden.\n  We evaluate NBTest on 592 Kaggle notebooks. Overall, NBTest generates 21163\nassertions (35.75 on average per notebook). The generated assertions obtain a\nmutation score of 0.57 in killing ML-specific mutations. NBTest can catch\nregression bugs in previous versions of the Kaggle notebooks using assertions\ngenerated for the latest versions. Because ML pipelines involve non\ndeterministic computations, the assertions can be flaky. Hence, we also show\nhow NBTest leverages statistical techniques to minimize flakiness while\nretaining high fault-detection effectiveness. NBTest has been adopted in the CI\nof a popular ML library. Further, we perform a user study with 17 participants\nthat shows that notebook users find NBTest intuitive (Rating 4.3/5) and useful\nin writing assertions and testing notebooks (Rating 4.24/5).", "AI": {"tldr": "NBTest\u662f\u4e00\u4e2a\u4e3a\u673a\u5668\u5b66\u4e60\u7b14\u8bb0\u672c\u8bbe\u8ba1\u7684\u56de\u5f52\u6d4b\u8bd5\u6846\u67b6\uff0c\u63d0\u4f9b\u81ea\u52a8\u5316\u65ad\u8a00\u751f\u6210\u548c\u6267\u884c\uff0c\u663e\u8457\u63d0\u5347\u7b14\u8bb0\u672c\u7684\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027\uff0c\u7528\u6237\u8bc4\u4ef7\u79ef\u6781\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7b14\u8bb0\u672c\u5728\u6301\u7eed\u5f00\u53d1\u4e2d\u7f3a\u4e4f\u6d4b\u8bd5\u652f\u6301\uff0c\u5bfc\u81f4\u8bb8\u591a\u4e0d\u6613\u5bdf\u89c9\u7684\u9519\u8bef\u548c\u6027\u80fd\u56de\u5f52\u95ee\u9898\u3002", "method": "NBTest\u662f\u4e00\u4e2a\u56de\u5f52\u6d4b\u8bd5\u6846\u67b6\uff0c\u63d0\u4f9b\u65ad\u8a00API\u5e93\u548cJupyterLab\u63d2\u4ef6\uff0c\u652f\u6301\u5728\u7b14\u8bb0\u672c\u4e2d\u7f16\u5199\u548c\u6267\u884c\u5355\u5143\u7ea7\u65ad\u8a00\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u751f\u6210\u65ad\u8a00\u7684\u65b9\u6cd5\u3002", "result": "\u5728592\u4e2aKaggle\u7b14\u8bb0\u672c\u4e0a\uff0cNBTest\u5e73\u5747\u6bcf\u4e2a\u7b14\u8bb0\u672c\u751f\u621035.75\u4e2a\u65ad\u8a00\uff0c\u7a81\u53d8\u5f97\u5206\u4e3a0.57\uff0c\u80fd\u6709\u6548\u6355\u6349\u56de\u5f52\u9519\u8bef\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u6280\u672f\u51cf\u5c11\u65ad\u8a00\u7684\u4e0d\u7a33\u5b9a\u6027\u3002\u7528\u6237\u7814\u7a76\u8868\u660eNBTest\u76f4\u89c2\u4e14\u6709\u7528\u3002", "conclusion": "NBTest\u901a\u8fc7\u63d0\u4f9b\u56de\u5f52\u6d4b\u8bd5\u6846\u67b6\u548c\u81ea\u52a8\u5316\u65ad\u8a00\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u5b66\u4e60\u7b14\u8bb0\u672c\u7684\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027\uff0c\u540c\u65f6\u51cf\u8f7b\u4e86\u5f00\u53d1\u8005\u7684\u8d1f\u62c5\u3002"}}
{"id": "2509.13381", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.13381", "abs": "https://arxiv.org/abs/2509.13381", "authors": ["Zhang Xueyao", "Yang Bo", "Yu Zhiwen", "Cao Xuelin", "George C. Alexandropoulos", "Merouane Debbah", "Chau Yuen"], "title": "Cooperative Target Detection with AUVs: A Dual-Timescale Hierarchical MARDL Approach", "comment": "6 pages", "summary": "Autonomous Underwater Vehicles (AUVs) have shown great potential for\ncooperative detection and reconnaissance. However, collaborative AUV\ncommunications introduce risks of exposure. In adversarial environments,\nachieving efficient collaboration while ensuring covert operations becomes a\nkey challenge for underwater cooperative missions. In this paper, we propose a\nnovel dual time-scale Hierarchical Multi-Agent Proximal Policy Optimization\n(H-MAPPO) framework. The high-level component determines the individuals\nparticipating in the task based on a central AUV, while the low-level component\nreduces exposure probabilities through power and trajectory control by the\nparticipating AUVs. Simulation results show that the proposed framework\nachieves rapid convergence, outperforms benchmark algorithms in terms of\nperformance, and maximizes long-term cooperative efficiency while ensuring\ncovert operations.", "AI": {"tldr": "\u63d0\u51faH-MAPPO\u6846\u67b6\u89e3\u51b3AUV\u534f\u4f5c\u4e2d\u7684\u9690\u853d\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5c42\u7b56\u7565\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u9690\u853d\u534f\u4f5c\uff0c\u4eff\u771f\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5728\u654c\u5bf9\u73af\u5883\u4e2d\uff0c\u534f\u4f5cAUV\u901a\u4fe1\u5b58\u5728\u66b4\u9732\u98ce\u9669\uff0c\u5982\u4f55\u5728\u9ad8\u6548\u534f\u4f5c\u7684\u540c\u65f6\u786e\u4fdd\u9690\u853d\u64cd\u4f5c\u6210\u4e3a\u6c34\u4e0b\u534f\u4f5c\u4efb\u52a1\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528\u53cc\u65f6\u95f4\u5c3a\u5ea6\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08H-MAPPO\uff09\u6846\u67b6\uff0c\u9ad8\u5c42\u7ec4\u4ef6\u901a\u8fc7\u4e2d\u592eAUV\u786e\u5b9a\u53c2\u4e0e\u4efb\u52a1\u7684\u4e2a\u4f53\uff0c\u4f4e\u5c42\u7ec4\u4ef6\u901a\u8fc7\u53c2\u4e0eAUV\u7684\u529f\u7387\u548c\u8f68\u8ff9\u63a7\u5236\u964d\u4f4e\u66b4\u9732\u6982\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6846\u67b6\u5b9e\u73b0\u4e86\u5feb\u901f\u6536\u655b\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\uff0c\u5e76\u5728\u4fdd\u8bc1\u9690\u853d\u64cd\u4f5c\u7684\u540c\u65f6\u6700\u5927\u5316\u957f\u671f\u534f\u4f5c\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684H-MAPPO\u6846\u67b6\u5728\u4fdd\u8bc1\u9690\u853d\u64cd\u4f5c\u7684\u540c\u65f6\uff0c\u6700\u5927\u5316\u957f\u671f\u534f\u4f5c\u6548\u7387\uff0c\u4e14\u5728\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\u3002"}}
{"id": "2509.13347", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13347", "abs": "https://arxiv.org/abs/2509.13347", "authors": ["Zihao Wang", "Muyao Li", "Kaichen He", "Xiangyu Wang", "Zhancun Mu", "Anji Liu", "Yitao Liang"], "title": "OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft", "comment": null, "summary": "The choice of action spaces is a critical yet unresolved challenge in\ndeveloping capable, end-to-end trainable agents. This paper first presents a\nlarge-scale, systematic comparison of prominent abstracted action spaces and\ntokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the\nopen-ended Minecraft. Our analysis reveals that no single action space is\nuniversally optimal; instead, the most effective abstraction is highly\ntask-dependent, creating a dilemma for building generalist agents. To resolve\nthis, we introduce Chain of Action (CoA), a novel framework that unifies\nhigh-level planning and low-level control within a single, monolithic VLA\nmodel. CoA treats an abstracted action not as a command for a separate policy,\nbut as an intermediate reasoning step--akin to a chain of thought--that guides\nthe generation of the final, executable action. Furthermore, we demonstrate\nthat an All-in-One agent trained on a diverse mixture of action spaces using\nthe CoA paradigm learns a more robust and generalizable policy. This unified\nagent achieves a new state-of-the-art, improving the overall task success rate\nover strong, specialized baselines. To foster reproducible research, we release\nthe OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive\nbenchmark of over 800 distinct tasks, curated datasets, source code, and all\npretrained model checkpoints at https://github.com/CraftJarvis/OpenHA", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7CoA\u6846\u67b6\u7edf\u4e00\u9ad8\u7ea7\u89c4\u5212\u548c\u4f4e\u7ea7\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u52a8\u4f5c\u7a7a\u95f4\u9009\u62e9\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u52a8\u4f5c\u7a7a\u95f4\u9009\u62e9\u5728\u901a\u7528\u667a\u80fd\u4f53\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u53d1\u73b0\u6700\u4f18\u52a8\u4f5c\u7a7a\u95f4\u9ad8\u5ea6\u4f9d\u8d56\u4efb\u52a1\uff0c\u5bfc\u81f4\u901a\u7528\u667a\u80fd\u4f53\u6784\u5efa\u7684\u56f0\u5883\u3002", "method": "\u63d0\u51fa\u4e86Chain of Action (CoA)\u6846\u67b6\uff0c\u5c06\u9ad8\u7ea7\u52a8\u4f5c\u89c6\u4e3a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff0c\u6307\u5bfc\u6700\u7ec8\u53ef\u6267\u884c\u52a8\u4f5c\u7684\u751f\u6210\u3002", "result": "CoA\u6846\u67b6\u5728\u591a\u6837\u5316\u52a8\u4f5c\u7a7a\u95f4\u4e0a\u8bad\u7ec3\u7684All-in-One\u667a\u80fd\u4f53\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "CoA\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u9ad8\u7ea7\u89c4\u5212\u548c\u4f4e\u7ea7\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\uff0c\u5e76\u4fc3\u8fdb\u4e86\u901a\u7528\u667a\u80fd\u4f53\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.13385", "categories": ["cs.CV", "cs.DM", "cs.LG", "51K05 (primary) 57-08, 53Z50, 55U10 (secondary)", "G.2.2"], "pdf": "https://arxiv.org/pdf/2509.13385", "abs": "https://arxiv.org/abs/2509.13385", "authors": ["Charlotte Beylier", "Parvaneh Joharinad", "J\u00fcrgen Jost", "Nahid Torbati"], "title": "Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension", "comment": "31 pages, 14 figures", "summary": "Utilizing recently developed abstract notions of sectional curvature, we\nintroduce a method for constructing a curvature-based geometric profile of\ndiscrete metric spaces. The curvature concept that we use here captures the\nmetric relations between triples of points and other points. More\nsignificantly, based on this curvature profile, we introduce a quantitative\nmeasure to evaluate the effectiveness of data representations, such as those\nproduced by dimensionality reduction techniques. Furthermore, Our experiments\ndemonstrate that this curvature-based analysis can be employed to estimate the\nintrinsic dimensionality of datasets. We use this to explore the large-scale\ngeometry of empirical networks and to evaluate the effectiveness of\ndimensionality reduction techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u66f2\u7387\u7684\u51e0\u4f55\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6570\u636e\u8868\u793a\u548c\u4f30\u8ba1\u6570\u636e\u96c6\u56fa\u6709\u7ef4\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u66f4\u6709\u6548\u5730\u8bc4\u4f30\u964d\u7ef4\u6280\u672f\u7b49\u6570\u636e\u8868\u793a\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a2\u7d22\u6570\u636e\u96c6\u7684\u56fa\u6709\u7ef4\u5ea6\u3002", "method": "\u5229\u7528\u62bd\u8c61\u7684\u622a\u9762\u66f2\u7387\u6982\u5ff5\uff0c\u6784\u5efa\u79bb\u6563\u5ea6\u91cf\u7a7a\u95f4\u7684\u66f2\u7387\u51e0\u4f55\u8f6e\u5ed3\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u5b9a\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u4f30\u8ba1\u6570\u636e\u96c6\u7684\u56fa\u6709\u7ef4\u5ea6\uff0c\u5e76\u8bc4\u4f30\u964d\u7ef4\u6280\u672f\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u66f2\u7387\u7684\u51e0\u4f55\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6570\u636e\u8868\u793a\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a2\u7d22\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u51e0\u4f55\u7279\u6027\u3002"}}
{"id": "2509.13954", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13954", "abs": "https://arxiv.org/abs/2509.13954", "authors": ["Surya Agustian", "Sandra Permana", "Salman Teguh Pratista", "Syarifu Adam", "Iswandi"], "title": "Low-cost Highly-interoperable Multiplatform Campus Network: Experience of YARSI University", "comment": "6 pages of paper submitting to conference", "summary": "To some organizations, building campus network is sometimes considered to be\nvery expensive; and this has made the project uneasy to perform. Moreover, if\nthe organization without sufficient IT knowledge does not have capable IT\nengineers, leaving this project to third parties without supervision would lead\nto unexpected larger expenses. For this reason, in the year of 2003, YARSI\nUniversity formed CMIS (Center for Management Infor-mation System) to perform\ntasks in designing, operations and maintenance of campus network and its\nservices. By combining Open Source operating system run on a local assembled\npersonal computer as gateway and router, and switching technology from Cisco,\nwe designed a low-cost UTP-based campus network which covering rooms and\nbuildings in YARSI environment. Meanwhile the internet access through several\nbroadband connections and dedicated wireless was shared to more than 100\nsimultaneous users by a captive portal system. With this strategy, we can\nsignificantly reduce cost for purchasing, maintenance and operations of network\ninfrastructure and internet access. Our model in designing low-cost campus\nnetwork and internet connections could be adopted by rural community or\norganizations that have limited budget to have internet access.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u6821\u56ed\u7f51\u7edc\u8bbe\u8ba1\u65b9\u6848\uff0c\u7ed3\u5408\u5f00\u6e90\u6280\u672f\u548c\u672c\u5730\u786c\u4ef6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5efa\u8bbe\u548c\u7ef4\u62a4\u6210\u672c\uff0c\u9002\u7528\u4e8e\u9884\u7b97\u6709\u9650\u7684\u793e\u533a\u6216\u7ec4\u7ec7\u3002", "motivation": "\u7531\u4e8e\u6821\u56ed\u7f51\u7edc\u5efa\u8bbe\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u7f3a\u4e4fIT\u77e5\u8bc6\u7684\u7ec4\u7ec7\u53ef\u80fd\u56e0\u4f9d\u8d56\u7b2c\u4e09\u65b9\u800c\u9762\u4e34\u989d\u5916\u5f00\u652f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4f4e\u6210\u672c\u4e14\u6613\u4e8e\u7ef4\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5f00\u6e90\u64cd\u4f5c\u7cfb\u7edf\u8fd0\u884c\u5728\u672c\u5730\u7ec4\u88c5\u7684PC\u4e0a\u4f5c\u4e3a\u7f51\u5173\u548c\u8def\u7531\u5668\uff0c\u7ed3\u5408Cisco\u7684\u4ea4\u6362\u6280\u672f\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eUTP\u7684\u4f4e\u6210\u672c\u6821\u56ed\u7f51\u7edc\u3002", "result": "\u6210\u529f\u8bbe\u8ba1\u5e76\u5b9e\u65bd\u4e86\u8986\u76d6YARSI\u5927\u5b66\u73af\u5883\u7684\u4f4e\u6210\u672c\u6821\u56ed\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u4e2a\u5bbd\u5e26\u8fde\u63a5\u548c\u4e13\u7528\u65e0\u7ebf\u7f51\u7edc\u5171\u4eab\u4e92\u8054\u7f51\u63a5\u5165\uff0c\u652f\u6301\u8d85\u8fc7100\u540d\u7528\u6237\u540c\u65f6\u4f7f\u7528\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u6821\u56ed\u7f51\u7edc\u8bbe\u8ba1\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u5f00\u6e90\u64cd\u4f5c\u7cfb\u7edf\u548c\u672c\u5730\u7ec4\u88c5\u7684PC\u4f5c\u4e3a\u7f51\u5173\u548c\u8def\u7531\u5668\uff0c\u4ee5\u53caCisco\u7684\u4ea4\u6362\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u548c\u4e92\u8054\u7f51\u63a5\u5165\u7684\u6210\u672c\u3002"}}
{"id": "2509.13680", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13680", "abs": "https://arxiv.org/abs/2509.13680", "authors": ["Wei Ma", "Yixiao Yang", "Jingquan Ge", "Xiaofei Xie", "Lingxiao Jiang"], "title": "Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and Personality-Driven Variations", "comment": null, "summary": "Code generation models are widely used in software development, yet their\nsensitivity to prompt phrasing remains under-examined. Identical requirements\nexpressed with different emotions or communication styles can yield divergent\noutputs, while most benchmarks emphasize only peak performance. We present\nPromptSE (Prompt Sensitivity Evaluation), a framework that creates semantically\nequivalent prompt variants with emotion and personality templates, and that\nevaluates stability using probability aware continuous scoring or using binary\npass rates when logits are unavailable. The results are aggregated into a\nproposed area under curve metric (AUC-E) for cross model comparison. Across 14\nmodels from three families (Llama, Qwen, and DeepSeek), our study shows that\nperformance and stability behave as largely decoupled optimization objectives,\nand it reveals architectural and scale related patterns that challenge common\nassumptions about model robustness. The framework supports rapid screening for\nclosed-source models as well as detailed stability analysis in research\nsettings. PromptSE enables practitioners to quantify performance stability\ntrade offs for deployment and model selection, positioning prompt stability as\na complementary evaluation dimension alongside performance and fairness, and\ncontributing to more trustworthy AI-assisted software development tools.", "AI": {"tldr": "PromptSE\u6846\u67b6\u8bc4\u4f30\u4ee3\u7801\u751f\u6210\u6a21\u578b\u5bf9\u63d0\u793a\u63aa\u8f9e\u7684\u654f\u611f\u6027\uff0c\u63ed\u793a\u6027\u80fd\u4e0e\u7a33\u5b9a\u6027\u89e3\u8026\u7684\u73b0\u8c61\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u548c\u90e8\u7f72\u63d0\u4f9b\u7a33\u5b9a\u6027\u91cf\u5316\u5de5\u5177\u3002", "motivation": "\u4ee3\u7801\u751f\u6210\u6a21\u578b\u5bf9\u63d0\u793a\u63aa\u8f9e\u7684\u654f\u611f\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u800c\u73b0\u6709\u57fa\u51c6\u5927\u591a\u4ec5\u5173\u6ce8\u5cf0\u503c\u6027\u80fd\u3002", "method": "\u63d0\u51faPromptSE\u6846\u67b6\uff0c\u901a\u8fc7\u60c5\u611f\u548c\u4e2a\u6027\u6a21\u677f\u751f\u6210\u8bed\u4e49\u76f8\u540c\u7684\u63d0\u793a\u53d8\u4f53\uff0c\u5e76\u4f7f\u7528\u6982\u7387\u611f\u77e5\u8fde\u7eed\u8bc4\u5206\u6216\u4e8c\u8fdb\u5236\u901a\u8fc7\u7387\u8bc4\u4f30\u7a33\u5b9a\u6027\u3002", "result": "\u572814\u4e2a\u6a21\u578b\u4e0a\u7684\u7814\u7a76\u8868\u660e\uff0c\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u662f\u57fa\u672c\u89e3\u8026\u7684\u4f18\u5316\u76ee\u6807\uff0c\u5e76\u63ed\u793a\u4e86\u6311\u6218\u5e38\u89c1\u6a21\u578b\u7a33\u5065\u6027\u5047\u8bbe\u7684\u67b6\u6784\u548c\u89c4\u6a21\u76f8\u5173\u6a21\u5f0f\u3002", "conclusion": "PromptSE\u6846\u67b6\u5c06\u63d0\u793a\u7a33\u5b9a\u6027\u5b9a\u4f4d\u4e3a\u4e0e\u6027\u80fd\u548c\u516c\u5e73\u6027\u5e76\u884c\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u53ef\u4fe1\u7684AI\u8f85\u52a9\u8f6f\u4ef6\u5f00\u53d1\u5de5\u5177\u3002"}}
{"id": "2509.13386", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13386", "abs": "https://arxiv.org/abs/2509.13386", "authors": ["Hansol Lim", "Minhyeok Im", "Jonathan Boyack", "Jee Won Lee", "Jongseong Brad Choi"], "title": "VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization", "comment": "This work has been submitted to the 2026 IEEE International\n  Conference on Robotics and Automation (ICRA) for possible publication", "summary": "Demands for software-defined vehicles (SDV) are rising and electric vehicles\n(EVs) are increasingly being equipped with powerful computers. This enables\nonboard AI systems to optimize charge-aware path optimization customized to\nreflect vehicle's current condition and environment. We present VEGA, a\ncharge-aware EV navigation agent that plans over a charger-annotated road graph\nusing Proximal Policy Optimization (PPO) with budgeted A* teacher-student\nguidance under state-of-charge (SoC) feasibility. VEGA consists of two modules.\nFirst, a physics-informed neural operator (PINO), trained on real vehicle speed\nand battery-power logs, uses recent vehicle speed logs to estimate aerodynamic\ndrag, rolling resistance, mass, motor and regenerative-braking efficiencies,\nand auxiliary load by learning a vehicle-custom dynamics. Second, a\nReinforcement Learning (RL) agent uses these dynamics to optimize a path with\noptimal charging stops and dwell times under SoC constraints. VEGA requires no\nadditional sensors and uses only vehicle speed signals. It may serve as a\nvirtual sensor for power and efficiency to potentially reduce EV cost. In\nevaluation on long routes like San Francisco to New York, VEGA's stops, dwell\ntimes, SoC management, and total travel time closely track Tesla Trip Planner\nwhile being slightly more conservative, presumably due to real vehicle\nconditions such as vehicle parameter drift due to deterioration. Although\ntrained only in U.S. regions, VEGA was able to compute optimal charge-aware\npaths in France and Japan, demonstrating generalizability. It achieves\npractical integration of physics-informed learning and RL for EV eco-routing.", "AI": {"tldr": "VEGA\u662f\u4e00\u79cd\u5145\u7535\u611f\u77e5\u7684\u7535\u52a8\u6c7d\u8f66\u5bfc\u822a\u4ee3\u7406\uff0c\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u5373\u53ef\u4f18\u5316\u8def\u5f84\u548c\u5145\u7535\u7b56\u7565\uff0c\u5c55\u793a\u4e86\u5168\u7403\u9002\u7528\u6027\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u5b9a\u4e49\u8f66\u8f86\uff08SDV\uff09\u9700\u6c42\u7684\u589e\u52a0\u548c\u7535\u52a8\u6c7d\u8f66\uff08EV\uff09\u8ba1\u7b97\u80fd\u529b\u7684\u63d0\u5347\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u8f66\u8f86\u5f53\u524d\u72b6\u6001\u548c\u73af\u5883\u5b9a\u5236\u5145\u7535\u611f\u77e5\u8def\u5f84\u4f18\u5316\u7684AI\u7cfb\u7edf\u3002", "method": "VEGA\u91c7\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u64cd\u4f5c\u5668\uff08PINO\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee3\u7406\uff0c\u7ed3\u5408PPO\u7b97\u6cd5\u548c\u9884\u7b97A*\u5e08\u751f\u6307\u5bfc\uff0c\u4f18\u5316\u5145\u7535\u7ad9\u9009\u62e9\u548c\u505c\u7559\u65f6\u95f4\u3002", "result": "\u5728\u957f\u8ddd\u79bb\u8def\u7ebf\uff08\u5982\u65e7\u91d1\u5c71\u81f3\u7ebd\u7ea6\uff09\u8bc4\u4f30\u4e2d\uff0cVEGA\u7684\u5145\u7535\u7ad9\u9009\u62e9\u3001\u505c\u7559\u65f6\u95f4\u3001SoC\u7ba1\u7406\u548c\u603b\u65c5\u884c\u65f6\u95f4\u4e0e\u7279\u65af\u62c9\u884c\u7a0b\u89c4\u5212\u5668\u63a5\u8fd1\uff0c\u4e14\u66f4\u5177\u4fdd\u5b88\u6027\u3002\u5c3d\u7ba1\u4ec5\u5728\u7f8e\u56fd\u5730\u533a\u8bad\u7ec3\uff0cVEGA\u5728\u6cd5\u56fd\u548c\u65e5\u672c\u4e5f\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u901a\u7528\u6027\u3002", "conclusion": "VEGA\u6210\u529f\u6574\u5408\u4e86\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3a\u7535\u52a8\u6c7d\u8f66\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u7684\u7ecf\u6d4e\u8def\u5f84\u4f18\u5316\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5168\u7403\u8303\u56f4\u5185\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2509.13351", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13351", "abs": "https://arxiv.org/abs/2509.13351", "authors": ["Pulkit Verma", "Ngoc La", "Anthony Favier", "Swaroop Mishra", "Julie A. Shah"], "title": "Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\ndiverse tasks, yet their ability to perform structured symbolic planning\nremains limited, particularly in domains requiring formal representations like\nthe Planning Domain Definition Language (PDDL). In this paper, we present a\nnovel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'\nsymbolic planning capabilities through logical chain-of-thought reasoning. Our\napproach focuses on teaching models to rigorously reason about action\napplicability, state transitions, and plan validity using explicit logical\ninference steps. By developing instruction prompts that guide models through\nthe precise logical reasoning required to determine when actions can be applied\nin a given state, we enable LLMs to self-correct their planning processes\nthrough structured reflection. The framework systematically builds verification\nskills by decomposing the planning process into explicit reasoning chains about\nprecondition satisfaction, effect application, and invariant preservation.\nExperimental results on multiple planning domains show that our\nchain-of-thought reasoning based instruction-tuned models are significantly\nbetter at planning, achieving planning accuracy of up to 94% on standard\nbenchmarks, representing a 66% absolute improvement over baseline models. This\nwork bridges the gap between the general reasoning capabilities of LLMs and the\nlogical precision required for automated planning, offering a promising\ndirection for developing better AI planning systems.", "AI": {"tldr": "A new framework, PDDL-Instruct, improves LLMs' symbolic planning by teaching logical reasoning, achieving 94% accuracy, a 66% boost over baselines.", "motivation": "Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, yet their ability to perform structured symbolic planning remains limited, particularly in domains requiring formal representations like the Planning Domain Definition Language (PDDL).", "method": "The paper presents a novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs' symbolic planning capabilities through logical chain-of-thought reasoning. It focuses on teaching models to rigorously reason about action applicability, state transitions, and plan validity using explicit logical inference steps.", "result": "Experimental results on multiple planning domains show that the chain-of-thought reasoning based instruction-tuned models are significantly better at planning, achieving planning accuracy of up to 94% on standard benchmarks, representing a 66% absolute improvement over baseline models.", "conclusion": "This work bridges the gap between the general reasoning capabilities of LLMs and the logical precision required for automated planning, offering a promising direction for developing better AI planning systems."}}
{"id": "2509.13388", "categories": ["cs.CV", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.13388", "abs": "https://arxiv.org/abs/2509.13388", "authors": ["Yadvendra Gurjar", "Ruoni Wan", "Ehsan Farahbakhsh", "Rohitash Chandra"], "title": "Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji", "comment": null, "summary": "As a developing country, Fiji is facing rapid urbanisation, which is visible\nin the massive development projects that include housing, roads, and civil\nworks. In this study, we present machine learning and remote sensing frameworks\nto compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The\nultimate goal of this study is to provide technical support in land cover/land\nuse modelling and change detection. We used Landsat-8 satellite image for the\nstudy region and created our training dataset with labels for supervised\nmachine learning. We used Google Earth Engine and unsupervised machine learning\nvia k-means clustering to generate the land cover map. We used convolutional\nneural networks to classify the selected regions' land cover types. We present\na visualisation of change detection, highlighting urban area changes over time\nto monitor changes in the map.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u9065\u611f\u6280\u672f\u5206\u6790\u6590\u6d4e\u7eb3\u8fea2013-2024\u5e74\u571f\u5730\u5229\u7528\u53d8\u5316\uff0c\u4e3a\u571f\u5730\u8986\u76d6/\u571f\u5730\u5229\u7528\u5efa\u6a21\u63d0\u4f9b\u6280\u672f\u652f\u6301\u3002", "motivation": "\u6590\u6d4e\u4f5c\u4e3a\u53d1\u5c55\u4e2d\u56fd\u5bb6\u9762\u4e34\u5feb\u901f\u57ce\u5e02\u5316\uff0c\u9700\u76d1\u6d4b\u571f\u5730\u5229\u7528\u53d8\u5316\u4ee5\u652f\u6301\u53d1\u5c55\u9879\u76ee\u3002", "method": "\u7ed3\u5408Landsat-8\u536b\u661f\u5f71\u50cf\u3001\u76d1\u7763\u5b66\u4e60\uff08\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u548c\u65e0\u76d1\u7763\u5b66\u4e60\uff08k-means\u805a\u7c7b\uff09\u751f\u6210\u571f\u5730\u8986\u76d6\u56fe\u3002", "result": "\u901a\u8fc7\u53d8\u5316\u68c0\u6d4b\u53ef\u89c6\u5316\u5c55\u793a\u4e86\u57ce\u5e02\u533a\u57df\u7684\u65f6\u5e8f\u53d8\u5316\u3002", "conclusion": "\u7814\u7a76\u4e3a\u571f\u5730\u8986\u76d6/\u571f\u5730\u5229\u7528\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u6280\u672f\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u76d1\u6d4b\u57ce\u5e02\u5316\u8fdb\u7a0b\u3002"}}
{"id": "2509.13993", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13993", "abs": "https://arxiv.org/abs/2509.13993", "authors": ["Vincent Mutolo", "Rhea Parekh", "Dan Rubenstein"], "title": "Path-Oblivious Entanglement Swapping for the Quantum Internet", "comment": "To appear, ACM HotNets 2025, November 2025", "summary": "Proposed Bell pair swapping protocols, an essential component of the Quantum\nInternet, are planned-path: specific, structured, routing paths are reserved\nprior to the execution of the swapping process. This makes sense when one\nassumes the state used in the swapping process is expensive, fragile, and\nunstable. However, lessons from classical networking have shown that while\nreservations seem promising in concept, flexible, reservation-light or free\napproaches often outperform their more restrictive counterparts in\nwell-provisioned networks. In this paper, we propose that a path-oblivious\napproach is more amenable to supporting swapping as quantum state evolves into\na cheaper, more robust form. We formulate the swapping process as a linear\nprogram and present and evaluate a fairly naive baseline swapping protocol that\ntries to balance Bell pairs throughout the network. Preliminary results show\nthat while naive balancing leaves room for improvement, investigating\npath-oblivious swapping is a promising direction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8def\u5f84\u65e0\u5173\u7684Bell\u5bf9\u4ea4\u6362\u534f\u8bae\uff0c\u901a\u8fc7\u7ebf\u6027\u89c4\u5212\u6a21\u578b\u548c\u57fa\u7840\u534f\u8bae\u8bc4\u4f30\uff0c\u663e\u793a\u5176\u5728\u91cf\u5b50\u7f51\u7edc\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u57fa\u4e8e\u7ecf\u5178\u7f51\u7edc\u7684\u7ecf\u9a8c\uff0c\u7075\u6d3b\u3001\u65e0\u9700\u9884\u7559\u8def\u5f84\u7684\u65b9\u6cd5\u5728\u8d44\u6e90\u5145\u8db3\u7f51\u7edc\u4e2d\u8868\u73b0\u66f4\u4f18\u3002\u968f\u7740\u91cf\u5b50\u6001\u53d8\u5f97\u66f4\u5ec9\u4ef7\u548c\u7a33\u5b9a\uff0c\u8def\u5f84\u65e0\u5173\u7684\u4ea4\u6362\u65b9\u6cd5\u66f4\u5177\u4f18\u52bf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ebf\u6027\u89c4\u5212\u6a21\u578b\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u76f8\u5bf9\u57fa\u7840\u7684Bell\u5bf9\u4ea4\u6362\u534f\u8bae\uff0c\u65e8\u5728\u7f51\u7edc\u4e2d\u5e73\u8861Bell\u5bf9\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1\u57fa\u7840\u7684\u5e73\u8861\u7b56\u7565\u6709\u5f85\u4f18\u5316\uff0c\u4f46\u8def\u5f84\u65e0\u5173\u7684\u4ea4\u6362\u65b9\u6cd5\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u867d\u7136\u7b80\u5355\u7684\u5e73\u8861\u7b56\u7565\u8fd8\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u4f46\u63a2\u7d22\u8def\u5f84\u65e0\u5173\u7684Bell\u5bf9\u4ea4\u6362\u534f\u8bae\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2509.13755", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13755", "abs": "https://arxiv.org/abs/2509.13755", "authors": ["Zhaoyang Chu", "Yao Wan", "Zhikun Zhang", "Di Wang", "Zhou Yang", "Hongyu Zhang", "Pan Zhou", "Xuanhua Shi", "Hai Jin", "David Lo"], "title": "Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning", "comment": "Accepted at the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)", "summary": "While Code Language Models (CLMs) have demonstrated superior performance in\nsoftware engineering tasks such as code generation and summarization, recent\nempirical studies reveal a critical privacy vulnerability: these models exhibit\nunintended memorization of sensitive training data, enabling verbatim\nreproduction of confidential information when specifically prompted. To address\nthis issue, several approaches, including training data de-duplication and\ndifferential privacy augmentation, have been proposed. However, these methods\nrequire full-model retraining for deployed CLMs, which incurs substantial\ncomputational costs. In this paper, we aim to answer the following research\nquestion: Can sensitive information memorized by CLMs be erased effectively and\nefficiently?\n  We conduct a pioneering investigation into erasing sensitive memorization in\nCLMs through machine unlearning - a post-hoc modification method that removes\nspecific information from trained models without requiring full retraining.\nSpecifically, we first quantify the memorization risks of sensitive data within\nCLM training datasets and curate a high-risk dataset of 50,000 sensitive\nmemorized samples as unlearning targets. We study two widely used gradient\nascent-based unlearning approaches: the vanilla and constraint-based methods,\nand introduce CodeEraser, an advanced variant that selectively unlearns\nsensitive memorized segments in code while preserving the structural integrity\nand functional correctness of the surrounding code. Extensive experiments on\nthree families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,\nvalidate the effectiveness and efficiency of CodeEraser in erasing targeted\nsensitive memorization while maintaining model utility.", "AI": {"tldr": "\u63d0\u51faCodeEraser\uff0c\u901a\u8fc7\u673a\u5668\u9057\u5fd8\u6280\u672f\u6d88\u9664CLMs\u4e2d\u7684\u654f\u611f\u8bb0\u5fc6\uff0c\u907f\u514d\u5168\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3CLMs\u4e2d\u654f\u611f\u4fe1\u606f\u8bb0\u5fc6\u5bfc\u81f4\u7684\u9690\u79c1\u6f0f\u6d1e\uff0c\u907f\u514d\u5168\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u901a\u8fc7\u673a\u5668\u9057\u5fd8\u6280\u672f\uff0c\u7279\u522b\u662f\u68af\u5ea6\u4e0a\u5347\u4e3a\u57fa\u7840\u7684\u9057\u5fd8\u65b9\u6cd5\uff08\u5305\u62ecvanilla\u3001constraint-based\u53caCodeEraser\u53d8\u4f53\uff09\uff0c\u9009\u62e9\u6027\u6d88\u9664\u4ee3\u7801\u4e2d\u7684\u654f\u611f\u8bb0\u5fc6\u7247\u6bb5\u3002", "result": "CodeEraser\u5728\u4e09\u79cdCLMs\uff08CodeParrot\u3001CodeGen-Mono\u3001Qwen2.5-Coder\uff09\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "CodeEraser\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6d88\u9664CLMs\u4e2d\u7684\u654f\u611f\u8bb0\u5fc6\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u529f\u80fd\u6027\u3002"}}
{"id": "2509.13434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13434", "abs": "https://arxiv.org/abs/2509.13434", "authors": ["Wei-Chen Li", "Glen Chou"], "title": "A Convex Formulation of Compliant Contact between Filaments and Rigid Bodies", "comment": null, "summary": "We present a computational framework for simulating filaments interacting\nwith rigid bodies through contact. Filaments are challenging to simulate due to\ntheir codimensionality, i.e., they are one-dimensional structures embedded in\nthree-dimensional space. Existing methods often assume that filaments remain\npermanently attached to rigid bodies. Our framework unifies discrete elastic\nrod (DER) modeling, a pressure field patch contact model, and a convex contact\nformulation to accurately simulate frictional interactions between slender\nfilaments and rigid bodies - capabilities not previously achievable. Owing to\nthe convex formulation of contact, each time step can be solved to global\noptimality, guaranteeing complementarity between contact velocity and impulse.\nWe validate the framework by assessing the accuracy of frictional forces and\ncomparing its physical fidelity against baseline methods. Finally, we\ndemonstrate its applicability in both soft robotics, such as a stochastic\nfilament-based gripper, and deformable object manipulation, such as shoelace\ntying, providing a versatile simulator for systems involving complex\nfilament-filament and filament-rigid body interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408DER\u3001\u538b\u529b\u573a\u8865\u4e01\u548c\u51f8\u63a5\u89e6\u516c\u5f0f\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u7ec6\u957f\u7ea4\u7ef4\u4e0e\u521a\u4f53\u95f4\u6469\u64e6\u76f8\u4e92\u4f5c\u7528\u7684\u51c6\u786e\u6a21\u62df\uff0c\u9a8c\u8bc1\u4e86\u5176\u7269\u7406\u4fdd\u771f\u5ea6\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u8f6f\u673a\u5668\u4eba\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u7eb5\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u7ea4\u7ef4\u6c38\u4e45\u9644\u7740\u4e8e\u521a\u4f53\uff0c\u65e0\u6cd5\u51c6\u786e\u6a21\u62df\u7ec6\u957f\u7ea4\u7ef4\u4e0e\u521a\u4f53\u4e4b\u95f4\u7684\u6469\u64e6\u76f8\u4e92\u4f5c\u7528\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u79bb\u6563\u5f39\u6027\u6746\uff08DER\uff09\u5efa\u6a21\u3001\u538b\u529b\u573a\u8865\u4e01\u63a5\u89e6\u6a21\u578b\u548c\u51f8\u63a5\u89e6\u516c\u5f0f\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u5168\u5c40\u4f18\u5316\u6c42\u89e3\u7684\u6846\u67b6\uff0c\u786e\u4fdd\u63a5\u89e6\u901f\u5ea6\u4e0e\u51b2\u91cf\u7684\u4e92\u8865\u6027\u3002", "result": "\u901a\u8fc7\u9a8c\u8bc1\u6469\u64e6\u529b\u7684\u51c6\u786e\u6027\u548c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u7684\u7269\u7406\u4fdd\u771f\u5ea6\u5bf9\u6bd4\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u8f6f\u673a\u5668\u4eba\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u7eb5\u573a\u666f\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u7684\u79bb\u6563\u5f39\u6027\u6746\u6a21\u578b\u3001\u538b\u529b\u573a\u8865\u4e01\u63a5\u89e6\u6a21\u578b\u548c\u51f8\u63a5\u89e6\u516c\u5f0f\uff0c\u6210\u529f\u6a21\u62df\u4e86\u7ec6\u957f\u7ea4\u7ef4\u4e0e\u521a\u4f53\u4e4b\u95f4\u7684\u6469\u64e6\u76f8\u4e92\u4f5c\u7528\uff0c\u9a8c\u8bc1\u4e86\u5176\u7269\u7406\u4fdd\u771f\u5ea6\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u8f6f\u673a\u5668\u4eba\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u7eb5\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.13352", "categories": ["cs.AI", "cs.RO", "68T07, 68T40, 68T42", "I.2.9; I.2.11; I.2.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.13352", "abs": "https://arxiv.org/abs/2509.13352", "authors": ["Anis Koubaa", "Khaled Gabr"], "title": "Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning", "comment": "14 pages, 1 figure", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,\nsurveillance, and disaster response, yet most systems remain confined to SAE\nLevel 2--3 autonomy. Their reliance on rule-based control and narrow AI\nrestricts adaptability in dynamic, uncertain missions. Existing UAV frameworks\nlack context-aware reasoning, autonomous decision-making, and ecosystem-level\nintegration; critically, none leverage Large Language Model (LLM) agents with\ntool-calling for real-time knowledge access. This paper introduces the Agentic\nUAVs framework, a five-layer architecture (Perception, Reasoning, Action,\nIntegration, Learning) that augments UAVs with LLM-driven reasoning, database\nquerying, and third-party system interaction. A ROS2 and Gazebo-based prototype\nintegrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3\ndeployment. In simulated search-and-rescue scenarios, agentic UAVs achieved\nhigher detection confidence (0.79 vs. 0.72), improved person detection rates\n(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).\nThese results confirm that modest computational overhead enables qualitatively\nnew levels of autonomy and ecosystem integration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684Agentic UAVs\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u5c42\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u7684\u81ea\u4e3b\u6027\u548c\u6027\u80fd\uff0c\u6a21\u62df\u6d4b\u8bd5\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u7cfb\u7edf\u4f9d\u8d56\u57fa\u4e8e\u89c4\u5219\u7684\u63a7\u5236\u548c\u7a84AI\uff0c\u9650\u5236\u4e86\u5176\u5728\u52a8\u6001\u3001\u4e0d\u786e\u5b9a\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u4e14\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u3001\u81ea\u4e3b\u51b3\u7b56\u548c\u751f\u6001\u7cfb\u7edf\u7ea7\u96c6\u6210\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e94\u5c42\u67b6\u6784\uff08Perception, Reasoning, Action, Integration, Learning\uff09\uff0c\u7ed3\u5408\u4e86LLM\u9a71\u52a8\u7684\u63a8\u7406\u3001\u6570\u636e\u5e93\u67e5\u8be2\u548c\u7b2c\u4e09\u65b9\u7cfb\u7edf\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7ROS2\u548cGazebo\u539f\u578b\u96c6\u6210\u4e86YOLOv11\u76ee\u6807\u68c0\u6d4b\u4e0eGPT-4\u63a8\u7406\u53ca\u672c\u5730Gemma-3\u90e8\u7f72\u3002", "result": "\u5728\u6a21\u62df\u641c\u7d22\u6551\u63f4\u573a\u666f\u4e2d\uff0cAgentic UAVs\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\uff080.79 vs. 0.72\uff09\u3001\u66f4\u4f18\u7684\u4eba\u5458\u68c0\u6d4b\u7387\uff0891% vs. 75%\uff09\u548c\u663e\u8457\u589e\u52a0\u7684\u884c\u52a8\u63a8\u8350\u7387\uff0892% vs. 4.5%\uff09\u3002", "conclusion": "Agentic UAVs\u6846\u67b6\u901a\u8fc7\u4e94\u5c42\u67b6\u6784\uff08\u611f\u77e5\u3001\u63a8\u7406\u3001\u884c\u52a8\u3001\u96c6\u6210\u3001\u5b66\u4e60\uff09\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u7684\u81ea\u4e3b\u6027\u548c\u751f\u6001\u7cfb\u7edf\u96c6\u6210\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u9002\u5ea6\u7684\u8ba1\u7b97\u5f00\u9500\u53ef\u4ee5\u5e26\u6765\u8d28\u7684\u98de\u8dc3\u3002"}}
{"id": "2509.13396", "categories": ["cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13396", "abs": "https://arxiv.org/abs/2509.13396", "authors": ["Xinan Wang", "Di Shi", "Fengyu Wang"], "title": "Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence", "comment": "12 page Journal paper, accepted by IEEE Open Access Journal of Power\n  and Energy", "summary": "This paper presents a novel three-stage framework for real-time foreign\nobject intrusion (FOI) detection and tracking in power transmission systems.\nThe framework integrates: (1) a YOLOv7 segmentation model for fast and robust\nobject localization, (2) a ConvNeXt-based feature extractor trained with\ntriplet loss to generate discriminative embeddings, and (3) a feature-assisted\nIoU tracker that ensures resilient multi-object tracking under occlusion and\nmotion. To enable scalable field deployment, the pipeline is optimized for\ndeployment on low-cost edge hardware using mixed-precision inference. The\nsystem supports incremental updates by adding embeddings from previously unseen\nobjects into a reference database without requiring model retraining. Extensive\nexperiments on real-world surveillance and drone video datasets demonstrate the\nframework's high accuracy and robustness across diverse FOI scenarios. In\naddition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's\npracticality and scalability for real-world edge applications.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u9636\u6bb5\u6846\u67b6\u7528\u4e8e\u5b9e\u65f6\u5f02\u7269\u5165\u4fb5\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\uff0c\u7ed3\u5408YOLOv7\u3001ConvNeXt\u548cIoU\u8ddf\u8e2a\u5668\uff0c\u4f18\u5316\u8fb9\u7f18\u90e8\u7f72\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u9ad8\u6548\u4e14\u5b9e\u7528\u3002", "motivation": "\u89e3\u51b3\u7535\u529b\u4f20\u8f93\u7cfb\u7edf\u4e2d\u5b9e\u65f6\u5f02\u7269\u5165\u4fb5\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u906e\u6321\u548c\u8fd0\u52a8\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u6846\u67b6\u6574\u5408\u4e86YOLOv7\u5206\u5272\u6a21\u578b\u8fdb\u884c\u5feb\u901f\u76ee\u6807\u5b9a\u4f4d\uff0cConvNeXt\u7279\u5f81\u63d0\u53d6\u5668\u751f\u6210\u5224\u522b\u6027\u5d4c\u5165\uff0c\u4ee5\u53ca\u57fa\u4e8e\u7279\u5f81\u7684IoU\u8ddf\u8e2a\u5668\u5b9e\u73b0\u591a\u76ee\u6807\u8ddf\u8e2a\u3002", "result": "\u5728\u771f\u5b9e\u76d1\u63a7\u548c\u65e0\u4eba\u673a\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u786c\u4ef6\u57fa\u51c6\u6d4b\u8bd5\u8bc1\u5b9e\u4e86\u5176\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u4e09\u9636\u6bb5\u6846\u67b6\u5728\u5b9e\u65f6\u5f02\u7269\u5165\u4fb5\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u4e2d\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u9002\u7528\u4e8e\u4f4e\u6210\u672c\u8fb9\u7f18\u786c\u4ef6\u90e8\u7f72\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.14002", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.14002", "abs": "https://arxiv.org/abs/2509.14002", "authors": ["Rongyu Zhang", "Xize Duan", "Jiaming Liu", "Li Du", "Yuan Du", "Dan Wang", "Shanghang Zhang", "Fangxin Wang"], "title": "RepCaM++: Exploring Transparent Visual Prompt With Inference-Time Re-Parameterization for Neural Video Delivery", "comment": null, "summary": "Recently, content-aware methods have been employed to reduce bandwidth and\nenhance the quality of Internet video delivery. These methods involve training\ndistinct content-aware super-resolution (SR) models for each video chunk on the\nserver, subsequently streaming the low-resolution (LR) video chunks with the SR\nmodels to the client. Prior research has incorporated additional partial\nparameters to customize the models for individual video chunks. However, this\nleads to parameter accumulation and can fail to adapt appropriately as video\nlengths increase, resulting in increased delivery costs and reduced\nperformance. In this paper, we introduce RepCaM++, an innovative framework\nbased on a novel Re-parameterization Content-aware Modulation (RepCaM) module\nthat uniformly modulates video chunks. The RepCaM framework integrates extra\nparallel-cascade parameters during training to accommodate multiple chunks,\nsubsequently eliminating these additional parameters through\nre-parameterization during inference. Furthermore, to enhance RepCaM's\nperformance, we propose the Transparent Visual Prompt (TVP), which includes a\nminimal set of zero-initialized image-level parameters (e.g., less than 0.1%)\nto capture fine details within video chunks. We conduct extensive experiments\non the VSD4K dataset, encompassing six different video scenes, and achieve\nstate-of-the-art results in video restoration quality and delivery bandwidth\ncompression.", "AI": {"tldr": "RepCaM++\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u6a21\u5757\u548cTVP\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5185\u5bb9\u611f\u77e5\u65b9\u6cd5\u5728\u957f\u89c6\u9891\u4e2d\u7684\u53c2\u6570\u79ef\u7d2f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u6062\u590d\u548c\u5e26\u5bbd\u538b\u7f29\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5185\u5bb9\u611f\u77e5\u65b9\u6cd5\u5728\u89c6\u9891\u957f\u5ea6\u589e\u52a0\u65f6\uff0c\u53c2\u6570\u79ef\u7d2f\u5bfc\u81f4\u4ea4\u4ed8\u6210\u672c\u4e0a\u5347\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8c03\u5236\u6846\u67b6\u3002", "method": "\u5f15\u5165\u4e86RepCaM++\u6846\u67b6\uff0c\u57fa\u4e8eRepCaM\u6a21\u5757\u7edf\u4e00\u8c03\u5236\u89c6\u9891\u5757\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u65f6\u96c6\u6210\u5e76\u884c\u7ea7\u8054\u53c2\u6570\u3001\u63a8\u7406\u65f6\u6d88\u9664\u8fd9\u4e9b\u989d\u5916\u53c2\u6570\u6765\u5b9e\u73b0\u9ad8\u6548\u8c03\u5236\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86TVP\u6280\u672f\uff0c\u901a\u8fc7\u6781\u5c11\u91cf\u96f6\u521d\u59cb\u5316\u56fe\u50cf\u7ea7\u53c2\u6570\u6355\u83b7\u89c6\u9891\u5757\u7684\u7ec6\u8282\u3002", "result": "\u5728VSD4K\u6570\u636e\u96c6\uff08\u5305\u542b\u516d\u79cd\u4e0d\u540c\u89c6\u9891\u573a\u666f\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u53d6\u5f97\u4e86\u89c6\u9891\u6062\u590d\u8d28\u91cf\u548c\u4ea4\u4ed8\u5e26\u5bbd\u538b\u7f29\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "RepCaM++\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684Re-parameterization Content-aware Modulation\u6a21\u5757\u548cTransparent Visual Prompt\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u6062\u590d\u8d28\u91cf\u548c\u5e26\u5bbd\u538b\u7f29\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u5728VSD4K\u6570\u636e\u96c6\u4e0a\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002"}}
{"id": "2509.13758", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13758", "abs": "https://arxiv.org/abs/2509.13758", "authors": ["Kevin Halim", "Sin G. Teo", "Ruitao Feng", "Zhenpeng Chen", "Yang Gu", "Chong Wang", "Yang Liu"], "title": "A Study on Thinking Patterns of Large Reasoning Models in Code Generation", "comment": null, "summary": "Currently, many large language models (LLMs) are utilized for software\nengineering tasks such as code generation. The emergence of more advanced\nmodels known as large reasoning models (LRMs), such as OpenAI's o3, DeepSeek\nR1, and Qwen3. They have demonstrated the capability of performing multi-step\nreasoning. Despite the advancement in LRMs, little attention has been paid to\nsystematically analyzing the reasoning patterns these models exhibit and how\nsuch patterns influence the generated code. This paper presents a comprehensive\nstudy aimed at investigating and uncovering the reasoning behavior of LRMs\nduring code generation. We prompted several state-of-the-art LRMs of varying\nsizes with code generation tasks and applied open coding to manually annotate\nthe reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning\nbehaviors, encompassing 15 reasoning actions across four phases.\n  Our empirical study based on the taxonomy reveals a series of findings.\nFirst, we identify common reasoning patterns, showing that LRMs generally\nfollow a human-like coding workflow, with more complex tasks eliciting\nadditional actions such as scaffolding, flaw detection, and style checks.\nSecond, we compare reasoning across models, finding that Qwen3 exhibits\niterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like\napproach. Third, we analyze the relationship between reasoning and code\ncorrectness, showing that actions such as unit test creation and scaffold\ngeneration strongly support functional outcomes, with LRMs adapting strategies\nbased on task context. Finally, we evaluate lightweight prompting strategies\ninformed by these findings, demonstrating the potential of context- and\nreasoning-oriented prompts to improve LRM-generated code. Our results offer\ninsights and practical implications for advancing automatic code generation.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u63d0\u51fa\u4e8615\u79cd\u63a8\u7406\u52a8\u4f5c\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u6709\u4e0d\u540c\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u67d0\u4e9b\u52a8\u4f5c\u4e0e\u4ee3\u7801\u6b63\u786e\u6027\u5bc6\u5207\u76f8\u5173\u3002", "motivation": "\u5c3d\u7ba1LRMs\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u591a\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5bf9\u5176\u63a8\u7406\u6a21\u5f0f\u53ca\u5176\u5bf9\u751f\u6210\u4ee3\u7801\u5f71\u54cd\u7684\u7cfb\u7edf\u6027\u5206\u6790\u4ecd\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5411\u4e0d\u540c\u89c4\u6a21\u7684\u5148\u8fdbLRMs\u63d0\u51fa\u4ee3\u7801\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u5e94\u7528\u5f00\u653e\u7f16\u7801\u624b\u52a8\u6807\u6ce8\u63a8\u7406\u75d5\u8ff9\uff0c\u4ece\u4e2d\u63a8\u5bfc\u51faLRM\u63a8\u7406\u884c\u4e3a\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0LRMs\u901a\u5e38\u9075\u5faa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u7f16\u7801\u6d41\u7a0b\uff0c\u590d\u6742\u4efb\u52a1\u4f1a\u5f15\u53d1\u66f4\u591a\u52a8\u4f5c\u5982\u811a\u624b\u67b6\u751f\u6210\u548c\u7f3a\u9677\u68c0\u6d4b\u3002\u4e0d\u540c\u6a21\u578b\u5982Qwen3\u548cDeepSeek-R1-7B\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u63a8\u7406\u6a21\u5f0f\u3002\u63a8\u7406\u52a8\u4f5c\u5982\u5355\u5143\u6d4b\u8bd5\u521b\u5efa\u4e0e\u4ee3\u7801\u6b63\u786e\u6027\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e8615\u79cd\u63a8\u7406\u52a8\u4f5c\u7684\u5206\u7c7b\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u6539\u8fdb\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2509.13501", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13501", "abs": "https://arxiv.org/abs/2509.13501", "authors": ["Hossein Gholampour", "Logan E. Beaver"], "title": "Trajectory Tracking with Reachability-Guided Quadratic Programming and Freeze-Resume", "comment": null, "summary": "Many robotic systems must follow planned paths yet pause safely and resume\nwhen people or objects intervene. We present an output-space method for systems\nwhose tracked output can be feedback-linearized to a double integrator (e.g.,\nmanipulators). The approach has two parts. Offline, we perform a pre-run\nreachability check to verify that the motion plan respects speed and\nacceleration magnitude limits. Online, we apply a quadratic program to track\nthe motion plan under the same limits. We use a one-step reachability test to\nbound the maximum disturbance the system is capable of rejecting. When the\nstate coincides with the reference path we recover perfect tracking in the\ndeterministic case, and we correct errors using a KKT-inspired weight. We\ndemonstrate that safety stops and unplanned deviations are handled efficiently,\nand the system returns to the motion plan without replanning. We demonstrate\nour system's improved performance over pure pursuit in simulation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f93\u51fa\u7a7a\u95f4\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u53ef\u8fbe\u6027\u68c0\u67e5\u548c\u5728\u7ebf\u4e8c\u6b21\u89c4\u5212\u8ddf\u8e2a\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u5728\u8def\u5f84\u8ddf\u8e2a\u4e2d\u5b89\u5168\u6682\u505c\u548c\u6062\u590d\uff0c\u6027\u80fd\u4f18\u4e8e\u7eaf\u8ffd\u8e2a\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u8def\u5f84\u8ddf\u8e2a\u8fc7\u7a0b\u4e2d\u56e0\u4eba\u6216\u7269\u4f53\u4ecb\u5165\u800c\u9700\u8981\u5b89\u5168\u6682\u505c\u548c\u6062\u590d\u7684\u95ee\u9898\u3002", "method": "\u79bb\u7ebf\u8fdb\u884c\u9884\u8fd0\u884c\u53ef\u8fbe\u6027\u68c0\u67e5\uff0c\u5728\u7ebf\u5e94\u7528\u4e8c\u6b21\u89c4\u5212\u8ddf\u8e2a\u8fd0\u52a8\u8ba1\u5212\uff0c\u5e76\u4f7f\u7528\u4e00\u6b65\u53ef\u8fbe\u6027\u6d4b\u8bd5\u6765\u9650\u5236\u7cfb\u7edf\u80fd\u62d2\u7edd\u7684\u6700\u5927\u5e72\u6270\u3002", "result": "\u7cfb\u7edf\u5728\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u6bd4\u7eaf\u8ffd\u8e2a\u66f4\u597d\u7684\u6027\u80fd\uff0c\u80fd\u9ad8\u6548\u5904\u7406\u5b89\u5168\u505c\u6b62\u548c\u975e\u8ba1\u5212\u504f\u5dee\uff0c\u5e76\u8fd4\u56de\u8fd0\u52a8\u8ba1\u5212\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u79bb\u7ebf\u53ef\u8fbe\u6027\u68c0\u67e5\u548c\u5728\u7ebf\u4e8c\u6b21\u89c4\u5212\u8ddf\u8e2a\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u5728\u8def\u5f84\u8ddf\u8e2a\u4e2d\u5b89\u5168\u6682\u505c\u548c\u6062\u590d\uff0c\u4e14\u5728\u786e\u5b9a\u6027\u60c5\u51b5\u4e0b\u80fd\u5b8c\u7f8e\u8ddf\u8e2a\u53c2\u8003\u8def\u5f84\uff0c\u65e0\u9700\u91cd\u65b0\u89c4\u5212\u3002"}}
{"id": "2509.13357", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13357", "abs": "https://arxiv.org/abs/2509.13357", "authors": ["Yongchao Huang", "Hassan Raza"], "title": "Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling", "comment": "16 pages", "summary": "We propose semantic fusion, a lightweight scheme that augments a Transformer\nlanguage model (LM) with a parallel, fuzzy-membership feature channel that\nencodes token-level semantics. Each token is represented by a vector of\ninterpretable features (e.g. part-of-speech cues, shallow roles, boundary\nflags, sentiment polarity and strength) whose values are graded degrees from\ndifferentiable membership functions (e.g. power kernels). These per-token\nvectors form a sentence-level semantic matrix fused via a gated adapter into\nthe LM. Training uses standard next-token prediction, an auxiliary loss that\nreconstructs the semantic features from hidden states, and a lightweight\nuniformizer that regularizes adjective-class distributions. On a synthetic\ntwo-clause corpus with held-out adjectives for out-of-distribution (OOD)\ncontrol, semantic fusion improves perplexity and enables precise,\nuser-controllable generation of polarity and punctuation while maintaining\nmodel simplicity. This approach adds only small overhead, remains fully\ncompatible with tied input-output embeddings, and provides an interpretable\npathway for conditioned natural language generation.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u4e49\u878d\u5408\u65b9\u6848\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5e76\u884c\u7279\u5f81\u901a\u9053\u589e\u5f3aTransformer\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u751f\u6210\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u589e\u5f3aTransformer\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u3001\u53ef\u63a7\u7684\u81ea\u7136\u8bed\u8a00\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8f7b\u91cf\u5316\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u878d\u5408\u65b9\u6848\uff0c\u901a\u8fc7\u5e76\u884c\u6a21\u7cca\u6210\u5458\u7279\u5f81\u901a\u9053\u7f16\u7801\u4ee4\u724c\u7ea7\u8bed\u4e49\uff0c\u5f62\u6210\u53e5\u5b50\u7ea7\u8bed\u4e49\u77e9\u9635\uff0c\u5e76\u901a\u8fc7\u95e8\u63a7\u9002\u914d\u5668\u878d\u5408\u5230\u8bed\u8a00\u6a21\u578b\u4e2d\u3002\u8bad\u7ec3\u91c7\u7528\u6807\u51c6\u7684\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u3001\u8f85\u52a9\u635f\u5931\u548c\u8f7b\u91cf\u7ea7\u5747\u5300\u5316\u5668\u3002", "result": "\u5728\u5408\u6210\u53cc\u5b50\u53e5\u8bed\u6599\u5e93\u4e0a\uff0c\u8bed\u4e49\u878d\u5408\u6539\u5584\u4e86\u56f0\u60d1\u5ea6\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u6781\u6027\u548c\u6807\u70b9\u7684\u7cbe\u786e\u3001\u7528\u6237\u53ef\u63a7\u751f\u6210\u3002", "conclusion": "\u8bed\u4e49\u878d\u5408\u901a\u8fc7\u8f7b\u91cf\u7ea7\u65b9\u6848\u589e\u5f3a\u4e86Transformer\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u56f0\u60d1\u5ea6\u5e76\u5b9e\u73b0\u4e86\u7cbe\u786e\u3001\u7528\u6237\u53ef\u63a7\u7684\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7b80\u6d01\u548c\u517c\u5bb9\u6027\u3002"}}
{"id": "2509.13399", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13399", "abs": "https://arxiv.org/abs/2509.13399", "authors": ["Tianyu Chen", "Yasi Zhang", "Zhi Zhang", "Peiyu Yu", "Shu Wang", "Zhendong Wang", "Kevin Lin", "Xiaofei Wang", "Zhengyuan Yang", "Linjie Li", "Chung-Ching Lin", "Jianwen Xie", "Oscar Leong", "Lijuan Wang", "Ying Nian Wu", "Mingyuan Zhou"], "title": "EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing", "comment": "Tianyu Chen and Yasi Zhang contributed equally; Oscar Leong, Lijuan\n  Wang, Ying Nian Wu, and Mingyuan Zhou advised equally", "summary": "Instruction-based image editing has advanced rapidly, yet reliable and\ninterpretable evaluation remains a bottleneck. Current protocols either (i)\ndepend on paired reference images -- resulting in limited coverage and\ninheriting biases from prior generative models -- or (ii) rely solely on\nzero-shot vision-language models (VLMs), whose prompt-based assessments of\ninstruction following, content consistency, and visual quality are often\nimprecise.\n  To address this, we introduce EdiVal-Agent, an automated, scalable, and\nfine-grained evaluation framework for multi-turn instruction-based editing from\nan object-centric perspective, supported by a suite of expert tools. Given an\nimage, EdiVal-Agent first decomposes it into semantically meaningful objects,\nthen synthesizes diverse, context-aware editing instructions. For evaluation,\nit integrates VLMs with open-vocabulary object detectors to assess instruction\nfollowing, uses semantic-level feature extractors to evaluate content\nconsistency, and leverages human preference models to judge visual quality. We\nshow that combining VLMs with object detectors yields stronger agreement with\nhuman judgments in instruction-following evaluation compared to using VLMs\nalone and CLIP-based metrics. Furthermore, the pipeline's modular design allows\nfuture tools to be seamlessly integrated, enhancing evaluation accuracy over\ntime.\n  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing\nbenchmark covering 9 instruction types and 11 state-of-the-art editing models\nspanning autoregressive (AR) (including Nano Banana, GPT-Image-1),\nflow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be\nused to identify existing failure modes, thereby informing the development of\nthe next generation of editing models. Project page:\nhttps://tianyucodings.github.io/EdiVAL-page/.", "AI": {"tldr": "EdiVal-Agent \u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5bf9\u8c61\u68c0\u6d4b\u5668\uff0c\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u6307\u4ee4\u9075\u5faa\u6027\u3001\u5185\u5bb9\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u8bc4\u4f30\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6709\u9650\u7684\u914d\u5bf9\u53c2\u8003\u56fe\u50cf\uff08\u5b58\u5728\u8986\u76d6\u4e0d\u8db3\u548c\u751f\u6210\u6a21\u578b\u504f\u89c1\u95ee\u9898\uff09\uff0c\u8981\u4e48\u4ec5\u4f9d\u8d56\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u8bc4\u4f30\u7ed3\u679c\u4e0d\u7cbe\u786e\uff09\u3002", "method": "EdiVal-Agent \u9996\u5148\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u8bed\u4e49\u5bf9\u8c61\uff0c\u7136\u540e\u751f\u6210\u591a\u6837\u5316\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u7f16\u8f91\u6307\u4ee4\u3002\u8bc4\u4f30\u65f6\uff0c\u5b83\u7ed3\u5408 VLM \u548c\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u68c0\u6d4b\u5668\u6765\u8bc4\u4f30\u6307\u4ee4\u9075\u5faa\u6027\uff0c\u4f7f\u7528\u8bed\u4e49\u7ea7\u7279\u5f81\u63d0\u53d6\u5668\u8bc4\u4f30\u5185\u5bb9\u4e00\u81f4\u6027\uff0c\u5e76\u5229\u7528\u4eba\u7c7b\u504f\u597d\u6a21\u578b\u5224\u65ad\u89c6\u89c9\u8d28\u91cf\u3002", "result": "EdiVal-Agent \u5728\u6307\u4ee4\u9075\u5faa\u6027\u8bc4\u4f30\u4e2d\u6bd4\u5355\u72ec\u4f7f\u7528 VLM \u6216 CLIP \u6307\u6807\u66f4\u63a5\u8fd1\u4eba\u7c7b\u5224\u65ad\uff0c\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u652f\u6301\u672a\u6765\u5de5\u5177\u7684\u96c6\u6210\u3002", "conclusion": "EdiVal-Agent \u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5bf9\u8c61\u68c0\u6d4b\u5668\uff0c\u63d0\u4f9b\u4e86\u6bd4\u5355\u72ec\u4f7f\u7528 VLM \u6216 CLIP \u66f4\u63a5\u8fd1\u4eba\u7c7b\u5224\u65ad\u7684\u8bc4\u4f30\u7ed3\u679c\uff0c\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u8fd8\u652f\u6301\u672a\u6765\u5de5\u5177\u7684\u96c6\u6210\uff0c\u4ee5\u6301\u7eed\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\u3002"}}
{"id": "2509.13782", "categories": ["cs.SE", "cs.AI", "cs.MA", "D.2.2; I.2.1"], "pdf": "https://arxiv.org/pdf/2509.13782", "abs": "https://arxiv.org/abs/2509.13782", "authors": ["Yu Ge", "Linna Xie", "Zhong Li", "Yu Pei", "Tian Zhang"], "title": "Who is Introducing the Failure? Automatically Attributing Failures of Multi-Agent Systems via Spectrum Analysis", "comment": "20 pages, 6 figures", "summary": "Large Language Model Powered Multi-Agent Systems (MASs) are increasingly\nemployed to automate complex real-world problems, such as programming and\nscientific discovery. Despite their promising, MASs are not without their\nflaws. However, failure attribution in MASs - pinpointing the specific agent\nactions responsible for failures - remains underexplored and labor-intensive,\nposing significant challenges for debugging and system improvement. To bridge\nthis gap, we propose FAMAS, the first spectrum-based failure attribution\napproach for MASs, which operates through systematic trajectory replay and\nabstraction, followed by spectrum analysis.The core idea of FAMAS is to\nestimate, from variations across repeated MAS executions, the likelihood that\neach agent action is responsible for the failure. In particular, we propose a\nnovel suspiciousness formula tailored to MASs, which integrates two key factor\ngroups, namely the agent behavior group and the action behavior group, to\naccount for the agent activation patterns and the action activation patterns\nwithin the execution trajectories of MASs. Through expensive evaluations\nagainst 12 baselines on the Who and When benchmark, FAMAS demonstrates superior\nperformance by outperforming all the methods in comparison.", "AI": {"tldr": "FAMAS \u662f\u4e00\u79cd\u9488\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6545\u969c\u5f52\u56e0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f68\u8ff9\u91cd\u653e\u548c\u9891\u8c31\u5206\u6790\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6545\u969c\u5b9a\u4f4d\u6548\u7387\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MASs\uff09\u5728\u81ea\u52a8\u5316\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6545\u969c\u5f52\u56e0\u95ee\u9898\u5c1a\u672a\u5145\u5206\u7814\u7a76\u4e14\u8c03\u8bd5\u56f0\u96be\uff0c\u963b\u788d\u4e86\u7cfb\u7edf\u6539\u8fdb\u3002", "method": "FAMAS \u901a\u8fc7\u91cd\u590d\u6267\u884c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8f68\u8ff9\u5e76\u8fdb\u884c\u62bd\u8c61\u5206\u6790\uff0c\u7ed3\u5408\u4e13\u95e8\u8bbe\u8ba1\u7684\u53ef\u7591\u5ea6\u516c\u5f0f\uff0c\u6574\u5408\u4e86\u667a\u80fd\u4f53\u884c\u4e3a\u7ec4\u548c\u52a8\u4f5c\u884c\u4e3a\u7ec4\u4e24\u4e2a\u5173\u952e\u56e0\u7d20\u3002", "result": "\u5728 Who and When \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFAMAS \u4f18\u4e8e 12 \u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "FAMAS \u662f\u4e00\u79cd\u57fa\u4e8e\u9891\u8c31\u7684\u6545\u969c\u5f52\u56e0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u8f68\u8ff9\u91cd\u653e\u548c\u62bd\u8c61\u5206\u6790\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\u5b9a\u4f4d\u6548\u7387\u3002"}}
{"id": "2509.13534", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13534", "abs": "https://arxiv.org/abs/2509.13534", "authors": ["Chunxin Zheng", "Kai Chen", "Zhihai Bi", "Yulin Li", "Liang Pan", "Jinni Zhou", "Haoang Li", "Jun Ma"], "title": "Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning", "comment": null, "summary": "Whole-body manipulation (WBM) for humanoid robots presents a promising\napproach for executing embracing tasks involving bulky objects, where\ntraditional grasping relying on end-effectors only remains limited in such\nscenarios due to inherent stability and payload constraints. This paper\nintroduces a reinforcement learning framework that integrates a pre-trained\nhuman motion prior with a neural signed distance field (NSDF) representation to\nachieve robust whole-body embracing. Our method leverages a teacher-student\narchitecture to distill large-scale human motion data, generating kinematically\nnatural and physically feasible whole-body motion patterns. This facilitates\ncoordinated control across the arms and torso, enabling stable multi-contact\ninteractions that enhance the robustness in manipulation and also the load\ncapacity. The embedded NSDF further provides accurate and continuous geometric\nperception, improving contact awareness throughout long-horizon tasks. We\nthoroughly evaluate the approach through comprehensive simulations and\nreal-world experiments. The results demonstrate improved adaptability to\ndiverse shapes and sizes of objects and also successful sim-to-real transfer.\nThese indicate that the proposed framework offers an effective and practical\nsolution for multi-contact and long-horizon WBM tasks of humanoid robots.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u8fd0\u52a8\u5148\u9a8c\u548cNSDF\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4eff\u4eba\u673a\u5668\u4eba\u7684\u5168\u8eab\u62e5\u62b1\u4efb\u52a1\uff0c\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u548c\u8d1f\u8f7d\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u4ec5\u4f9d\u8d56\u672b\u7aef\u6267\u884c\u5668\u7684\u6293\u53d6\u65b9\u6cd5\u5728\u6d89\u53ca\u7b28\u91cd\u7269\u4f53\u65f6\u5b58\u5728\u56fa\u6709\u7a33\u5b9a\u6027\u548c\u8d1f\u8f7d\u9650\u5236\uff0c\u800c\u5168\u8eab\u64cd\u4f5c\u4e3a\u6b64\u7c7b\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u9884\u8bad\u7ec3\u7684\u4eba\u7c7b\u8fd0\u52a8\u5148\u9a8c\u4e0e\u795e\u7ecf\u7b26\u53f7\u8ddd\u79bb\u573a\uff08NSDF\uff09\u8868\u793a\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u5168\u8eab\u62e5\u62b1\u3002\u91c7\u7528\u5e08\u751f\u67b6\u6784\u6765\u63d0\u70bc\u5927\u89c4\u6a21\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\uff0c\u751f\u6210\u8fd0\u52a8\u5b66\u81ea\u7136\u4e14\u7269\u7406\u53ef\u884c\u7684\u5168\u8eab\u8fd0\u52a8\u6a21\u5f0f\u3002", "result": "\u901a\u8fc7\u5168\u9762\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5bf9\u591a\u6837\u5f62\u72b6\u548c\u5927\u5c0f\u7684\u7269\u4f53\u5177\u6709\u66f4\u597d\u7684\u9002\u5e94\u6027\uff0c\u5e76\u6210\u529f\u5b9e\u73b0\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u4eff\u4eba\u673a\u5668\u4eba\u7684\u591a\u63a5\u89e6\u548c\u957f\u65f6\u7a0b\u5168\u8eab\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13364", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13364", "abs": "https://arxiv.org/abs/2509.13364", "authors": ["Zixi Li"], "title": "Asterisk Operator", "comment": "Code available at: https://github.com/lizixi-0x2F/Asterisk-Games", "summary": "We propose the \\textbf{Asterisk Operator} ($\\ast$-operator), a novel unified\nframework for abstract reasoning based on Adjacency-Structured Parallel\nPropagation (ASPP). The operator formalizes structured reasoning tasks as\nlocal, parallel state evolution processes guided by implicit relational graphs.\nWe prove that the $\\ast$-operator maintains local computational constraints\nwhile achieving global reasoning capabilities, providing an efficient and\nconvergent computational paradigm for abstract reasoning problems. Through\nrigorous mathematical analysis and comprehensive experiments on ARC2 challenges\nand Conway's Game of Life, we demonstrate the operator's universality,\nconvergence properties, and superior performance. Our innovative\nEmbedding-Asterisk distillation method achieves 100\\% accuracy on ARC2\nvalidation with only 6M parameters, representing a significant breakthrough in\nneural-symbolic reasoning.\n  \\textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel\nPropagation, Asterisk Operator, Convergence, Universal Approximation", "AI": {"tldr": "\u2217-operator\u901a\u8fc7ASPP\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6536\u655b\u7684\u62bd\u8c61\u63a8\u7406\uff0c\u5728ARC2\u548cGame of Life\u4e2d\u8868\u73b0\u5353\u8d8a\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u62bd\u8c61\u63a8\u7406\u95ee\u9898\u4e2d\u7684\u5c40\u90e8\u8ba1\u7b97\u7ea6\u675f\u4e0e\u5168\u5c40\u63a8\u7406\u80fd\u529b\u7684\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u2217-operator\uff0c\u4e00\u79cd\u57fa\u4e8eAdjacency-Structured Parallel Propagation\uff08ASPP\uff09\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u5c40\u90e8\u3001\u5e76\u884c\u7684\u72b6\u6001\u6f14\u5316\u8fc7\u7a0b\u3002", "result": "\u901a\u8fc7\u6570\u5b66\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u2217-operator\u5728ARC2\u9a8c\u8bc1\u4e2d\u5b9e\u73b0\u4e86100%\u51c6\u786e\u7387\uff08\u4ec56M\u53c2\u6570\uff09\uff0c\u5e76\u5728Conway's Game of Life\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "Asterisk Operator\uff08\u2217-operator\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7Adjacency-Structured Parallel Propagation\uff08ASPP\uff09\u5b9e\u73b0\u4e86\u62bd\u8c61\u63a8\u7406\u7684\u9ad8\u6548\u4e0e\u6536\u655b\uff0c\u5e76\u5728ARC2\u6311\u6218\u548cConway's Game of Life\u4e2d\u5c55\u793a\u4e86\u5176\u666e\u9002\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.13414", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13414", "abs": "https://arxiv.org/abs/2509.13414", "authors": ["Nikhil Keetha", "Norman M\u00fcller", "Johannes Sch\u00f6nberger", "Lorenzo Porzi", "Yuchen Zhang", "Tobias Fischer", "Arno Knapitsch", "Duncan Zauss", "Ethan Weber", "Nelson Antunes", "Jonathon Luiten", "Manuel Lopez-Antequera", "Samuel Rota Bul\u00f2", "Christian Richardt", "Deva Ramanan", "Sebastian Scherer", "Peter Kontschieder"], "title": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "comment": "Project Page: https://map-anything.github.io/", "summary": "We introduce MapAnything, a unified transformer-based feed-forward model that\ningests one or more images along with optional geometric inputs such as camera\nintrinsics, poses, depth, or partial reconstructions, and then directly\nregresses the metric 3D scene geometry and cameras. MapAnything leverages a\nfactored representation of multi-view scene geometry, i.e., a collection of\ndepth maps, local ray maps, camera poses, and a metric scale factor that\neffectively upgrades local reconstructions into a globally consistent metric\nframe. Standardizing the supervision and training across diverse datasets,\nalong with flexible input augmentation, enables MapAnything to address a broad\nrange of 3D vision tasks in a single feed-forward pass, including uncalibrated\nstructure-from-motion, calibrated multi-view stereo, monocular depth\nestimation, camera localization, depth completion, and more. We provide\nextensive experimental analyses and model ablations demonstrating that\nMapAnything outperforms or matches specialist feed-forward models while\noffering more efficient joint training behavior, thus paving the way toward a\nuniversal 3D reconstruction backbone.", "AI": {"tldr": "MapAnything\u662f\u4e00\u4e2a\u7edf\u4e00\u7684Transformer\u524d\u9988\u6a21\u578b\uff0c\u8f93\u5165\u56fe\u50cf\u548c\u51e0\u4f55\u4fe1\u606f\uff0c\u76f4\u63a5\u56de\u5f523D\u573a\u666f\u548c\u76f8\u673a\u53c2\u6570\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u9ad8\u6548\u8054\u5408\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u591a\u6837\u53163D\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u672a\u6807\u5b9aSfM\u3001\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9\u3001\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7b49\uff09\u9700\u8981\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u907f\u514d\u9488\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u8bad\u7ec3\u4e13\u95e8\u6a21\u578b\u3002", "method": "MapAnything\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u524d\u9988\u6a21\u578b\uff0c\u8f93\u5165\u56fe\u50cf\u53ca\u53ef\u9009\u51e0\u4f55\u4fe1\u606f\uff08\u5982\u76f8\u673a\u5185\u53c2\u3001\u4f4d\u59ff\u3001\u6df1\u5ea6\u7b49\uff09\uff0c\u901a\u8fc7\u5206\u89e3\u7684\u591a\u89c6\u89d2\u573a\u666f\u51e0\u4f55\u8868\u793a\uff08\u6df1\u5ea6\u56fe\u3001\u5c40\u90e8\u5c04\u7ebf\u56fe\u3001\u76f8\u673a\u4f4d\u59ff\u7b49\uff09\u76f4\u63a5\u56de\u5f523D\u573a\u666f\u51e0\u4f55\u548c\u76f8\u673a\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMapAnything\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u4e13\u95e8\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u8054\u5408\u8bad\u7ec3\u80fd\u529b\u3002", "conclusion": "MapAnything\u901a\u8fc7\u7edf\u4e00\u7684Transformer\u524d\u9988\u6a21\u578b\uff0c\u5728\u591a\u6837\u5316\u76843D\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u4e13\u95e8\u6a21\u578b\uff0c\u4e3a\u901a\u75283D\u91cd\u5efa\u6846\u67b6\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.13852", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13852", "abs": "https://arxiv.org/abs/2509.13852", "authors": ["Yulun Wu", "Guangba Yu", "Zhihan Jiang", "Yichen Li", "Michael R. Lyu"], "title": "Trace Sampling 2.0: Code Knowledge Enhanced Span-level Sampling for Distributed Tracing", "comment": null, "summary": "Distributed tracing is an essential diagnostic tool in microservice systems,\nbut the sheer volume of traces places a significant burden on backend storage.\nA common approach to mitigating this issue is trace sampling, which selectively\nretains traces based on specific criteria, often preserving only anomalous\nones. However, this method frequently discards valuable information, including\nnormal traces that are essential for comparative analysis. To address this\nlimitation, we introduce Trace Sampling 2.0, which operates at the span level\nwhile maintaining trace structure consistency. This approach allows for the\nretention of all traces while significantly reducing storage overhead. Based on\nthis concept, we design and implement Autoscope, a span-level sampling method\nthat leverages static analysis to extract execution logic, ensuring that\ncritical spans are preserved without compromising structural integrity. We\nevaluated Autoscope on two open-source microservices. Our results show that it\nreduces trace size by 81.2% while maintaining 98.1% faulty span coverage,\noutperforming existing trace-level sampling methods. Furthermore, we\ndemonstrate its effectiveness in root cause analysis, achieving an average\nimprovement of 8.3%. These findings indicate that Autoscope can significantly\nenhance observability and storage efficiency in microservices, offering a\nrobust solution for performance monitoring.", "AI": {"tldr": "Autoscope\u662f\u4e00\u79cdspan\u7ea7\u522b\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u9759\u6001\u5206\u6790\u4fdd\u7559\u5173\u952espan\uff0c\u663e\u8457\u51cf\u5c11\u5b58\u50a8\u5f00\u9500\u5e76\u63d0\u5347\u8ffd\u8e2a\u6548\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5206\u5e03\u5f0f\u8ffd\u8e2a\u5728\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u662f\u91cd\u8981\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u4f46\u5927\u91cf\u8ffd\u8e2a\u6570\u636e\u7ed9\u540e\u7aef\u5b58\u50a8\u5e26\u6765\u8d1f\u62c5\u3002\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u5e38\u4e22\u5f03\u6709\u4ef7\u503c\u4fe1\u606f\uff0c\u5305\u62ec\u7528\u4e8e\u5bf9\u6bd4\u5206\u6790\u7684\u6b63\u5e38\u8ffd\u8e2a\u6570\u636e\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86Autoscope\uff0c\u4e00\u79cd\u57fa\u4e8e\u9759\u6001\u5206\u6790\u63d0\u53d6\u6267\u884c\u903b\u8f91\u7684span\u7ea7\u522b\u91c7\u6837\u65b9\u6cd5\uff0c\u786e\u4fdd\u5728\u4e0d\u7834\u574f\u7ed3\u6784\u5b8c\u6574\u6027\u7684\u60c5\u51b5\u4e0b\u4fdd\u7559\u5173\u952espan\u3002", "result": "\u5728\u4e24\u79cd\u5f00\u6e90\u5fae\u670d\u52a1\u4e0a\u8bc4\u4f30Autoscope\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5c06\u8ffd\u8e2a\u5927\u5c0f\u51cf\u5c1181.2%\uff0c\u540c\u65f6\u4fdd\u630198.1%\u7684\u9519\u8befspan\u8986\u76d6\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u8ffd\u8e2a\u7ea7\u522b\u91c7\u6837\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5728\u6839\u56e0\u5206\u6790\u4e2d\u5e73\u5747\u63d0\u53478.3%\u3002", "conclusion": "Autoscope\u901a\u8fc7span\u7ea7\u522b\u7684\u91c7\u6837\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5fae\u670d\u52a1\u7cfb\u7edf\u7684\u53ef\u89c2\u6d4b\u6027\u548c\u5b58\u50a8\u6548\u7387\uff0c\u4e3a\u6027\u80fd\u76d1\u63a7\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13541", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13541", "abs": "https://arxiv.org/abs/2509.13541", "authors": ["Ayberk Acar", "Fangjie Li", "Hao Li", "Lidia Al-Zogbi", "Kanyifeechukwu Jane Oguine", "Susheela Sharma Stern", "Jesse F. d'Almeida", "Robert J. Webster III", "Ipek Oguz", "Jie Ying Wu"], "title": "Semantic 3D Reconstructions with SLAM for Central Airway Obstruction", "comment": "5 pages, 2 figures, 1 table", "summary": "Central airway obstruction (CAO) is a life-threatening condition with\nincreasing incidence, caused by tumors in and outside of the airway.\nTraditional treatment methods such as bronchoscopy and electrocautery can be\nused to remove the tumor completely; however, these methods carry a high risk\nof complications. Recent advances allow robotic interventions with lesser risk.\nThe combination of robot interventions with scene understanding and mapping\nalso opens up the possibilities for automation. We present a novel pipeline\nthat enables real-time, semantically informed 3D reconstructions of the central\nairway using monocular endoscopic video.\n  Our approach combines DROID-SLAM with a segmentation model trained to\nidentify obstructive tissues. The SLAM module reconstructs the 3D geometry of\nthe airway in real time, while the segmentation masks guide the annotation of\nobstruction regions within the reconstructed point cloud. To validate our\npipeline, we evaluate the reconstruction quality using ex vivo models.\n  Qualitative and quantitative results show high similarity between ground\ntruth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By\nintegrating segmentation directly into the SLAM workflow, our system produces\nannotated 3D maps that highlight clinically relevant regions in real time.\nHigh-speed capabilities of the pipeline allows quicker reconstructions compared\nto previous work, reflecting the surgical scene more accurately.\n  To the best of our knowledge, this is the first work to integrate semantic\nsegmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our\nframework is modular and can generalize to other anatomies or procedures with\nminimal changes, offering a promising step toward autonomous robotic\ninterventions.", "AI": {"tldr": "\u65b0\u65b9\u6cd5\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u4e0e\u5b9e\u65f6SLAM\uff0c\u7528\u4e8e\u5185\u7aa5\u955cCAO\u6cbb\u7597\uff0c\u63d0\u9ad8\u91cd\u5efa\u901f\u5ea6\u4e0e\u7cbe\u5ea6\uff0c\u652f\u6301\u81ea\u4e3b\u673a\u5668\u4eba\u5e72\u9884\u3002", "motivation": "\u4f20\u7edf\u6cbb\u7597CAO\u7684\u65b9\u6cd5\u5e76\u53d1\u75c7\u98ce\u9669\u9ad8\uff0c\u673a\u5668\u4eba\u5e72\u9884\u7ed3\u5408\u573a\u666f\u7406\u89e3\u4e0e\u6620\u5c04\u4e3a\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u7ed3\u5408DROID-SLAM\u4e0e\u5206\u5272\u6a21\u578b\uff0c\u5b9e\u65f6\u91cd\u5efa\u6c14\u90533D\u51e0\u4f55\u5e76\u6807\u6ce8\u963b\u585e\u533a\u57df\u3002\u901a\u8fc7\u79bb\u4f53\u6a21\u578b\u9a8c\u8bc1\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u91cd\u5efa\u8d28\u91cf\u4e0eCT\u626b\u63cf\u9ad8\u5ea6\u76f8\u4f3c\uff080.62 mm Chamfer\u8ddd\u79bb\uff09\uff0c\u5b9e\u65f6\u751f\u6210\u6807\u6ce8\u76843D\u5730\u56fe\uff0c\u663e\u8457\u63d0\u9ad8\u624b\u672f\u573a\u666f\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u4e0e\u5b9e\u65f6\u5355\u76eeSLAM\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5185\u7aa5\u955c\u4e0b\u7684\u4e2d\u592e\u6c14\u9053\u963b\u585e\uff08CAO\uff09\u573a\u666f\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u91cd\u5efa\u901f\u5ea6\u548c\u8d28\u91cf\uff0c\u8fd8\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u5e72\u9884\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u6846\u67b6\u3002"}}
{"id": "2509.13368", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13368", "abs": "https://arxiv.org/abs/2509.13368", "authors": ["Yuan Wei", "Xiaohan Shan", "Ran Miao", "Jianmin Li"], "title": "$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation", "comment": "9 pages, 7 figures", "summary": "Reinforcement learning agent development traditionally requires extensive\nexpertise and lengthy iterations, often resulting in high failure rates and\nlimited accessibility. This paper introduces $Agent^2$, a novel\nagent-generates-agent framework that achieves fully automated RL agent design\nthrough intelligent LLM-driven generation. The system autonomously transforms\nnatural language task descriptions and environment code into comprehensive,\nhigh-performance reinforcement learning solutions without human intervention.\n$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent\nserves as an autonomous AI designer that analyzes tasks and generates\nexecutable RL agents, while the Target Agent is the resulting automatically\ngenerated RL agent. The framework decomposes RL development into two distinct\nstages: MDP modeling and algorithmic optimization, enabling more targeted and\neffective agent generation. Built on the Model Context Protocol, $Agent^2$\nprovides a unified framework that standardizes intelligent agent creation\nacross diverse environments and algorithms, while incorporating adaptive\ntraining management and intelligent feedback analysis for continuous\nimprovement. Extensive experiments on a wide range of benchmarks, including\nMuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently\noutperforms manually designed solutions across all tasks, achieving up to 55%\nperformance improvement and substantial gains on average. By enabling truly\nend-to-end, closed-loop automation, this work establishes a new paradigm in\nwhich intelligent agents design and optimize other agents, marking a\nfundamental breakthrough for automated AI systems.", "AI": {"tldr": "$Agent^2$\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u5168\u81ea\u52a8\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u667a\u80fd\u4f53\u67b6\u6784\u548c\u6807\u51c6\u5316\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u5b9e\u73b0\u95ed\u73af\u81ea\u52a8\u5316\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5f00\u53d1\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u548c\u8fed\u4ee3\uff0c\u5931\u8d25\u7387\u9ad8\u4e14\u96be\u4ee5\u666e\u53ca\u3002$Agent^2$\u65e8\u5728\u901a\u8fc7LLM\u9a71\u52a8\u7684\u5168\u81ea\u52a8\u5316\u8bbe\u8ba1\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f15\u5165\u53cc\u667a\u80fd\u4f53\u67b6\u6784\uff08\u751f\u6210\u5668\u4ee3\u7406\u548c\u76ee\u6807\u4ee3\u7406\uff09\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u5f00\u53d1\u5206\u4e3aMDP\u5efa\u6a21\u548c\u7b97\u6cd5\u4f18\u5316\u4e24\u9636\u6bb5\uff0c\u5e76\u91c7\u7528Model Context Protocol\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MuJoCo\u3001MetaDrive\u7b49\uff09\u4e2d\uff0c$Agent^2$\u6027\u80fd\u5e73\u5747\u663e\u8457\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u65b9\u6848\uff0c\u6700\u9ad8\u63d0\u5347\u8fbe55%\u3002", "conclusion": "$Agent^2$ \u901a\u8fc7\u53cc\u667a\u80fd\u4f53\u67b6\u6784\u548c\u81ea\u52a8\u5316\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u6027\u80fd\u548c\u5f00\u53d1\u6548\u7387\uff0c\u4e3a\u81ea\u52a8\u5316AI\u7cfb\u7edf\u8bbe\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.13474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13474", "abs": "https://arxiv.org/abs/2509.13474", "authors": ["Yujia Lin", "Nicholas Evans"], "title": "Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization", "comment": null, "summary": "Ensuring accurate localization of robots in environments without GPS\ncapability is a challenging task. Visual Place Recognition (VPR) techniques can\npotentially achieve this goal, but existing RGB-based methods are sensitive to\nchanges in illumination, weather, and other seasonal changes. Existing\ncross-modal localization methods leverage the geometric properties of RGB\nimages and 3D LiDAR maps to reduce the sensitivity issues highlighted above.\nCurrently, state-of-the-art methods struggle in complex scenes, fine-grained or\nhigh-resolution matching, and situations where changes can occur in viewpoint.\nIn this work, we introduce a framework we call Semantic-Enhanced Cross-Modal\nPlace Recognition (SCM-PR) that combines high-level semantics utilizing RGB\nimages for robust localization in LiDAR maps. Our proposed method introduces: a\nVMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature\nFusion (SAFF) module for using both place descriptors and segmentation masks;\nLiDAR descriptors that incorporate both semantics and geometry; and a\ncross-modal semantic attention mechanism in NetVLAD to improve matching.\nIncorporating the semantic information also was instrumental in designing a\nMulti-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in\na contrastive learning framework. Our experimental work on the KITTI and\nKITTI-360 datasets show that SCM-PR achieves state-of-the-art performance\ncompared to other cross-modal place recognition methods.", "AI": {"tldr": "SCM-PR\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u7684\u8de8\u6a21\u6001\u5730\u70b9\u8bc6\u522b\uff0c\u7ed3\u5408RGB\u56fe\u50cf\u548cLiDAR\u5730\u56fe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u73af\u5883\u548c\u89c6\u89d2\u53d8\u5316\u7684\u654f\u611f\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709RGB-based VPR\u65b9\u6cd5\u5bf9\u5149\u7167\u3001\u5929\u6c14\u548c\u5b63\u8282\u53d8\u5316\u654f\u611f\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5f53\u524d\u8de8\u6a21\u6001\u5b9a\u4f4d\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u3001\u7ec6\u7c92\u5ea6\u5339\u914d\u548c\u89c6\u89d2\u53d8\u5316\u60c5\u51b5\u4e0b\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86SCM-PR\u6846\u67b6\uff0c\u5305\u62ecVMamba\u9aa8\u5e72\u7f51\u7edc\u7528\u4e8eRGB\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\u3001SAFF\u6a21\u5757\u7528\u4e8e\u878d\u5408\u4f4d\u7f6e\u63cf\u8ff0\u7b26\u548c\u5206\u5272\u63a9\u7801\u3001\u7ed3\u5408\u8bed\u4e49\u548c\u51e0\u4f55\u7684LiDAR\u63cf\u8ff0\u7b26\uff0c\u4ee5\u53caNetVLAD\u4e2d\u7684\u8de8\u6a21\u6001\u8bed\u4e49\u6ce8\u610f\u529b\u673a\u5236\u3002\u8fd8\u8bbe\u8ba1\u4e86\u591a\u89c6\u89d2\u8bed\u4e49-\u51e0\u4f55\u5339\u914d\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5728KITTI\u548cKITTI-360\u6570\u636e\u96c6\u4e0a\uff0cSCM-PR\u76f8\u6bd4\u5176\u4ed6\u8de8\u6a21\u6001\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SCM-PR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u9ad8\u5c42\u8bed\u4e49\u4fe1\u606f\uff0c\u5728LiDAR\u5730\u56fe\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u5b9a\u4f4d\uff0c\u5e76\u5728KITTI\u548cKITTI-360\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13868", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13868", "abs": "https://arxiv.org/abs/2509.13868", "authors": ["Manal Binkhonain", "Reem Alfayaz"], "title": "Are Prompts All You Need? Evaluating Prompt-Based Large Language Models (LLM)s for Software Requirements Classification", "comment": "33 pages, 12 figures", "summary": "Requirements classification assigns natural language requirements to\npredefined classes, such as functional and non functional. Accurate\nclassification reduces risk and improves software quality. Most existing models\nrely on supervised learning, which needs large labeled data that are costly,\nslow to create, and domain dependent; they also generalize poorly and often\nrequire retraining for each task. This study tests whether prompt based large\nlanguage models can reduce data needs. We benchmark several models and\nprompting styles (zero shot, few shot, persona, and chain of thought) across\nmultiple tasks on two English datasets, PROMISE and SecReq. For each task we\ncompare model prompt configurations and then compare the best LLM setups with a\nstrong fine tuned transformer baseline. Results show that prompt based LLMs,\nespecially with few shot prompts, can match or exceed the baseline. Adding a\npersona, or persona plus chain of thought, can yield further gains. We conclude\nthat prompt based LLMs are a practical and scalable option that reduces\ndependence on large annotations and can improve generalizability across tasks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u63d0\u793a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5c24\u5176\u662f\u5c11\u6837\u672c\u63d0\u793a\uff09\u80fd\u6709\u6548\u51cf\u5c11\u6570\u636e\u9700\u6c42\uff0c\u5e76\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u7684\u5206\u7c7b\u53ef\u4ee5\u964d\u4f4e\u98ce\u9669\u5e76\u63d0\u9ad8\u8f6f\u4ef6\u8d28\u91cf\u3002\u73b0\u6709\u7684\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u3001\u521b\u5efa\u6162\u4e14\u4f9d\u8d56\u9886\u57df\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u901a\u5e38\u9700\u8981\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u672c\u7814\u7a76\u6d4b\u8bd5\u4e86\u51e0\u79cd\u6a21\u578b\u548c\u63d0\u793a\u98ce\u683c\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u4eba\u7269\u8bbe\u5b9a\u548c\u601d\u7ef4\u94fe\uff09\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u4e86\u6700\u4f73LLM\u8bbe\u7f6e\u4e0e\u5fae\u8c03Transformer\u57fa\u7ebf\u7684\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u63d0\u793a\u7684LLM\uff0c\u5c24\u5176\u662f\u5c11\u6837\u672c\u63d0\u793a\uff0c\u53ef\u4ee5\u5339\u914d\u6216\u8d85\u8fc7\u57fa\u7ebf\u3002\u6dfb\u52a0\u4eba\u7269\u8bbe\u5b9a\u6216\u4eba\u7269\u8bbe\u5b9a\u52a0\u601d\u7ef4\u94fe\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u63d0\u793a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u9009\u62e9\uff0c\u51cf\u5c11\u4e86\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u80fd\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.13572", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13572", "abs": "https://arxiv.org/abs/2509.13572", "authors": ["Ozan Karaali", "Hossam Farag", "Strahinja Dosen", "Cedomir Stefanovic"], "title": "Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference", "comment": "ICAT 2025", "summary": "This study examines the potential of utilizing Vision Language Models (VLMs)\nto improve the perceptual capabilities of semi-autonomous prosthetic hands. We\nintroduce a unified benchmark for end-to-end perception and grasp inference,\nevaluating a single VLM to perform tasks that traditionally require complex\npipelines with separate modules for object detection, pose estimation, and\ngrasp planning. To establish the feasibility and current limitations of this\napproach, we benchmark eight contemporary VLMs on their ability to perform a\nunified task essential for bionic grasping. From a single static image, they\nshould (1) identify common objects and their key properties (name, shape,\norientation, and dimensions), and (2) infer appropriate grasp parameters (grasp\ntype, wrist rotation, hand aperture, and number of fingers). A corresponding\nprompt requesting a structured JSON output was employed with a dataset of 34\nsnapshots of common objects. Key performance metrics, including accuracy for\ncategorical attributes (e.g., object name, shape) and errors in numerical\nestimates (e.g., dimensions, hand aperture), along with latency and cost, were\nanalyzed. The results demonstrated that most models exhibited high performance\nin object identification and shape recognition, while accuracy in estimating\ndimensions and inferring optimal grasp parameters, particularly hand rotation\nand aperture, varied more significantly. This work highlights the current\ncapabilities and limitations of VLMs as advanced perceptual modules for\nsemi-autonomous control of bionic limbs, demonstrating their potential for\neffective prosthetic applications.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4eff\u751f\u6293\u53d6\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u7269\u4f53\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5c3a\u5bf8\u548c\u6293\u53d6\u53c2\u6570\u4f30\u8ba1\u4e0a\u4ecd\u6709\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u534a\u81ea\u4e3b\u5047\u80a2\u624b\u7684\u611f\u77e5\u80fd\u529b\uff0c\u7b80\u5316\u4f20\u7edf\u9700\u8981\u591a\u4e2a\u6a21\u5757\uff08\u5982\u7269\u4f53\u68c0\u6d4b\u3001\u59ff\u6001\u4f30\u8ba1\u548c\u6293\u53d6\u89c4\u5212\uff09\u7684\u590d\u6742\u6d41\u7a0b\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u7aef\u5230\u7aef\u611f\u77e5\u548c\u6293\u53d6\u63a8\u7406\u7684\u7edf\u4e00\u57fa\u51c6\uff0c\u8bc4\u4f30\u5355\u4e00VLM\u6267\u884c\u4f20\u7edf\u9700\u8981\u590d\u6742\u7ba1\u9053\u7684\u4efb\u52a1\u7684\u80fd\u529b\u3002\u901a\u8fc734\u5f20\u5e38\u89c1\u7269\u4f53\u7684\u5feb\u7167\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u7ed3\u6784\u5316JSON\u8f93\u51fa\u63d0\u793a\uff0c\u5206\u6790\u4e86\u516b\u79cd\u5f53\u4ee3VLM\u7684\u8868\u73b0\u3002", "result": "\u5927\u591a\u6570\u6a21\u578b\u5728\u7269\u4f53\u8bc6\u522b\u548c\u5f62\u72b6\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4f30\u8ba1\u5c3a\u5bf8\u548c\u63a8\u65ad\u6700\u4f73\u6293\u53d6\u53c2\u6570\uff08\u7279\u522b\u662f\u624b\u90e8\u65cb\u8f6c\u548c\u5f00\u5408\u5ea6\uff09\u65b9\u9762\u51c6\u786e\u6027\u5dee\u5f02\u8f83\u5927\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4f5c\u4e3a\u534a\u81ea\u4e3b\u63a7\u5236\u4eff\u751f\u80a2\u4f53\u7684\u9ad8\u7ea7\u611f\u77e5\u6a21\u5757\u7684\u5f53\u524d\u80fd\u529b\u548c\u5c40\u9650\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6709\u6548\u5047\u80a2\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13379", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13379", "abs": "https://arxiv.org/abs/2509.13379", "authors": ["Asif Azad", "Mohammad Sadat Hossain", "MD Sadik Hossain Shanto", "M Saifur Rahman", "Md Rizwan Pervez"], "title": "The Art of Saying \"Maybe\": A Conformal Lens for Uncertainty Benchmarking in VLMs", "comment": null, "summary": "Vision-Language Models (VLMs) have achieved remarkable progress in complex\nvisual understanding across scientific and reasoning tasks. While performance\nbenchmarking has advanced our understanding of these capabilities, the critical\ndimension of uncertainty quantification has received insufficient attention.\nTherefore, unlike prior conformal prediction studies that focused on limited\nsettings, we conduct a comprehensive uncertainty benchmarking study, evaluating\n16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets\nwith 3 distinct scoring functions. Our findings demonstrate that larger models\nconsistently exhibit better uncertainty quantification; models that know more\nalso know better what they don't know. More certain models achieve higher\naccuracy, while mathematical and reasoning tasks elicit poorer uncertainty\nperformance across all models compared to other domains. This work establishes\na foundation for reliable uncertainty evaluation in multimodal systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5168\u9762\u8bc4\u4f30\u4e8616\u79cdVLM\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5927\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u66f4\u786e\u5b9a\u7684\u6a21\u578b\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u4e3a\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u53ef\u9760uncertainty\u8bc4\u4f30\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u5c3d\u7ba1\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u589e\u8fdb\u4e86\u6211\u4eec\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u80fd\u529b\u7684\u7406\u89e3\uff0c\u4f46uncertainty\u91cf\u5316\u8fd9\u4e00\u5173\u952e\u7ef4\u5ea6\u5c1a\u672a\u5f97\u5230\u5145\u5206\u5173\u6ce8\u3002", "method": "\u6211\u4eec\u8fdb\u884c\u4e86\u5168\u9762\u7684\u4e0d\u786e\u5b9a\u6027\u57fa\u51c6\u6d4b\u8bd5\u7814\u7a76\uff0c\u8bc4\u4f30\u4e8616\u79cd\u6700\u5148\u8fdb\u7684VLM\uff08\u5f00\u6e90\u548c\u95ed\u6e90\uff09\u57286\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u4f7f\u75283\u79cd\u4e0d\u540c\u7684\u8bc4\u5206\u51fd\u6570\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u66f4\u5927\u7684\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff1b\u66f4\u786e\u5b9a\u7684\u6a21\u578b\u51c6\u786e\u7387\u66f4\u9ad8\uff1b\u6570\u5b66\u548c\u63a8\u7406\u4efb\u52a1\u5728\u6240\u6709\u6a21\u578b\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u53ef\u9760\u7684uncertainty\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u53d1\u73b0\u66f4\u5927\u7684\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u66f4\u786e\u5b9a\u7684\u6a21\u578b\u51c6\u786e\u7387\u66f4\u9ad8\u3002"}}
{"id": "2509.13482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13482", "abs": "https://arxiv.org/abs/2509.13482", "authors": ["Hao Xu", "Xiaolin Wu", "Xi Zhang"], "title": "Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization", "comment": "Code available at https://github.com/hxu160/SALVQ", "summary": "3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its\nphotorealistic rendering quality and real-time performance, but it generates\nmassive amounts of data. Hence compressing 3DGS data is necessary for the cost\neffectiveness of 3DGS models. Recently, several anchor-based neural compression\nmethods have been proposed, achieving good 3DGS compression performance.\nHowever, they all rely on uniform scalar quantization (USQ) due to its\nsimplicity. A tantalizing question is whether more sophisticated quantizers can\nimprove the current 3DGS compression methods with very little extra overhead\nand minimal change to the system. The answer is yes by replacing USQ with\nlattice vector quantization (LVQ). To better capture scene-specific\ncharacteristics, we optimize the lattice basis for each scene, improving LVQ's\nadaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a\nbalance between the R-D efficiency of vector quantization and the low\ncomplexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS\ncompression architectures, enhancing their R-D performance with minimal\nmodifications and computational overhead. Moreover, by scaling the lattice\nbasis vectors, SALVQ can dynamically adjust lattice density, enabling a single\nmodel to accommodate multiple bit rate targets. This flexibility eliminates the\nneed to train separate models for different compression levels, significantly\nreducing training time and memory consumption.", "AI": {"tldr": "SALVQ\u901a\u8fc7\u573a\u666f\u81ea\u9002\u5e94\u683c\u5411\u91cf\u91cf\u5316\u63d0\u53473DGS\u538b\u7f29\u6548\u7387\uff0c\u652f\u6301\u591a\u6bd4\u7279\u7387\uff0c\u51cf\u5c11\u8bad\u7ec3\u5f00\u9500\u3002", "motivation": "3DGS\u751f\u6210\u5927\u91cf\u6570\u636e\uff0c\u538b\u7f29\u6210\u672c\u9ad8\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56USQ\uff0c\u4f46\u66f4\u590d\u6742\u7684\u91cf\u5316\u5668\u53ef\u80fd\u5728\u4f4e\u5f00\u9500\u4e0b\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u573a\u666f\u81ea\u9002\u5e94\u683c\u5411\u91cf\u91cf\u5316\uff08SALVQ\uff09\u66ff\u4ee3\u5747\u5300\u6807\u91cf\u91cf\u5316\uff08USQ\uff09\uff0c\u4f18\u5316\u683c\u57fa\u4ee5\u6355\u6349\u573a\u666f\u7279\u6027\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u683c\u5bc6\u5ea6\u3002", "result": "SALVQ\u5728\u73b0\u67093DGS\u538b\u7f29\u67b6\u6784\u4e2d\u65e0\u7f1d\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347R-D\u6027\u80fd\uff0c\u652f\u6301\u591a\u6bd4\u7279\u7387\u76ee\u6807\uff0c\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "SALVQ\u901a\u8fc7\u4f18\u5316\u573a\u666f\u7279\u5b9a\u7684\u683c\u57fa\uff0c\u63d0\u5347\u4e863DGS\u538b\u7f29\u7684R-D\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u590d\u6742\u6027\u548c\u4f4e\u5f00\u9500\u3002"}}
{"id": "2509.13896", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13896", "abs": "https://arxiv.org/abs/2509.13896", "authors": ["Shalini Chakraborty", "Lola Burgue\u00f1o", "Nathalie Moreno", "Javier Troya", "Paula Mu\u00f1oz"], "title": "Mind the Ethics! The Overlooked Ethical Dimensions of GenAI in Software Modeling Education", "comment": "8 pages, Educators Symposium at MODELS 2025", "summary": "Generative Artificial Intelligence (GenAI) is rapidly gaining momentum in\nsoftware modeling education, embraced by both students and educators. As GenAI\nassists with interpreting requirements, formalizing models, and translating\nstudents' mental models into structured notations, it increasingly shapes core\nlearning outcomes such as domain comprehension, diagrammatic thinking, and\nmodeling fluency without clear ethical oversight or pedagogical guidelines.\nYet, the ethical implications of this integration remain underexplored.\n  In this paper, we conduct a systematic literature review across six major\ndigital libraries in computer science (ACM Digital Library, IEEE Xplore,\nScopus, ScienceDirect, SpringerLink, and Web of Science). Our aim is to\nidentify studies discussing the ethical aspects of GenAI in software modeling\neducation, including responsibility, fairness, transparency, diversity, and\ninclusion among others.\n  Out of 1,386 unique papers initially retrieved, only three explicitly\naddressed ethical considerations. This scarcity highlights the critical absence\nof ethical discourse surrounding GenAI in modeling education and raises urgent\nquestions about the responsible integration of AI in modeling curricula, as\nwell as it evinces the pressing need for structured ethical frameworks in this\nemerging educational landscape. We examine these three studies and explore the\nemerging research opportunities as well as the challenges that have arisen in\nthis field.", "AI": {"tldr": "\u7cfb\u7edf\u7efc\u8ff0\u53d1\u73b0GenAI\u5728\u8f6f\u4ef6\u5efa\u6a21\u6559\u80b2\u4e2d\u7684\u4f26\u7406\u7814\u7a76\u4e25\u91cd\u4e0d\u8db3\uff0c\u547c\u5401\u5efa\u7acb\u4f26\u7406\u6846\u67b6\u3002", "motivation": "\u63a2\u8ba8GenAI\u5728\u8f6f\u4ef6\u5efa\u6a21\u6559\u80b2\u4e2d\u7684\u4f26\u7406\u5f71\u54cd\uff0c\u5982\u8d23\u4efb\u3001\u516c\u5e73\u3001\u900f\u660e\u3001\u591a\u6837\u6027\u548c\u5305\u5bb9\u6027\u7b49\uff0c\u4ee5\u586b\u8865\u5f53\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5bf9\u516d\u5927\u8ba1\u7b97\u673a\u79d1\u5b66\u6570\u5b57\u56fe\u4e66\u9986\uff08ACM Digital Library\u3001IEEE Xplore\u3001Scopus\u3001ScienceDirect\u3001SpringerLink\u548cWeb of Science\uff09\u7684\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u7b5b\u9009\u51fa1,386\u7bc7\u6587\u732e\uff0c\u6700\u7ec8\u4ec5\u4e09\u7bc7\u6d89\u53ca\u4f26\u7406\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f26\u7406\u8ba8\u8bba\u6781\u5176\u532e\u4e4f\uff0c\u4ec5\u4e09\u7bc7\u6587\u732e\u7b26\u5408\u6761\u4ef6\uff0c\u8868\u660e\u8be5\u9886\u57df\u4e9f\u9700\u4f26\u7406\u6846\u67b6\u3002", "conclusion": "\u8bba\u6587\u6307\u51fa\uff0c\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u5728\u8f6f\u4ef6\u5efa\u6a21\u6559\u80b2\u4e2d\u7684\u4f26\u7406\u8003\u91cf\u4e25\u91cd\u4e0d\u8db3\uff0c\u4ec5\u6709\u4e09\u7bc7\u6587\u732e\u660e\u786e\u8ba8\u8bba\u6b64\u8bae\u9898\u3002\u8fd9\u51f8\u663e\u4e86\u5728\u8be5\u9886\u57df\u5efa\u7acb\u7ed3\u6784\u5316\u4f26\u7406\u6846\u67b6\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2509.13574", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13574", "abs": "https://arxiv.org/abs/2509.13574", "authors": ["Zidong Chen", "Zihao Guo", "Peng Wang", "ThankGod Itua Egbe", "Yan Lyu", "Chenghao Qian"], "title": "Dense-Jump Flow Matching with Non-Uniform Time Scheduling for Robotic Policies: Mitigating Multi-Step Inference Degradation", "comment": null, "summary": "Flow matching has emerged as a competitive framework for learning\nhigh-quality generative policies in robotics; however, we find that\ngeneralisation arises and saturates early along the flow trajectory, in\naccordance with recent findings in the literature. We further observe that\nincreasing the number of Euler integration steps during inference\ncounter-intuitively and universally degrades policy performance. We attribute\nthis to (i) additional, uniformly spaced integration steps oversample the\nlate-time region, thereby constraining actions towards the training\ntrajectories and reducing generalisation; and (ii) the learned velocity field\nbecoming non-Lipschitz as integration time approaches 1, causing instability.\nTo address these issues, we propose a novel policy that utilises non-uniform\ntime scheduling (e.g., U-shaped) during training, which emphasises both early\nand late temporal stages to regularise policy training, and a dense-jump\nintegration schedule at inference, which uses a single-step integration to\nreplace the multi-step integration beyond a jump point, to avoid unstable areas\naround 1. Essentially, our policy is an efficient one-step learner that still\npushes forward performance through multi-step integration, yielding up to 23.7%\nperformance gains over state-of-the-art baselines across diverse robotic tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u975e\u5747\u5300\u65f6\u95f4\u8c03\u5ea6\u548c\u5bc6\u96c6\u8df3\u8dc3\u96c6\u6210\u8ba1\u5212\u7684\u9ad8\u6548\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u6d41\u5339\u914d\u6846\u67b6\u4e2d\u6cdb\u5316\u6027\u4e0d\u8db3\u548c\u63a8\u7406\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u53d1\u73b0\u6d41\u5339\u914d\u6846\u67b6\u4e2d\u6cdb\u5316\u6027\u65e9\u671f\u51fa\u73b0\u5e76\u9971\u548c\uff0c\u4e14\u589e\u52a0\u63a8\u7406\u65f6\u7684\u6b27\u62c9\u79ef\u5206\u6b65\u9aa4\u6570\u4f1a\u53cd\u76f4\u89c9\u5730\u964d\u4f4e\u7b56\u7565\u6027\u80fd\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5747\u5300\u95f4\u9694\u7684\u79ef\u5206\u6b65\u9aa4\u8fc7\u5ea6\u91c7\u6837\u665a\u671f\u65f6\u95f4\u533a\u57df\uff0c\u5bfc\u81f4\u52a8\u4f5c\u53d7\u9650\uff0c\u4ee5\u53ca\u901f\u5ea6\u573a\u5728\u63a5\u8fd11\u65f6\u53d8\u5f97\u975eLipschitz\uff0c\u5f15\u53d1\u4e0d\u7a33\u5b9a\u3002", "method": "\u91c7\u7528\u975e\u5747\u5300\u65f6\u95f4\u8c03\u5ea6\uff08\u5982U\u5f62\uff09\u8bad\u7ec3\u7b56\u7565\uff0c\u5f3a\u8c03\u65e9\u671f\u548c\u665a\u671f\u65f6\u95f4\u9636\u6bb5\u4ee5\u89c4\u8303\u5316\u7b56\u7565\u8bad\u7ec3\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u5bc6\u96c6\u8df3\u8dc3\u96c6\u6210\u8ba1\u5212\uff0c\u901a\u8fc7\u5355\u6b65\u96c6\u6210\u66ff\u6362\u591a\u6b65\u96c6\u6210\u4ee5\u907f\u514d\u4e0d\u7a33\u5b9a\u533a\u57df\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0c\u65b0\u7b56\u7565\u6bd4\u73b0\u6709\u6280\u672f\u57fa\u7ebf\u5b9e\u73b0\u4e86\u9ad8\u8fbe23.7%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u975e\u5747\u5300\u65f6\u95f4\u8c03\u5ea6\u548c\u5bc6\u96c6\u8df3\u8dc3\u96c6\u6210\u8ba1\u5212\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7b56\u7565\uff0c\u5728\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u6280\u672f\u57fa\u7ebf\u9ad8\u8fbe23.7%\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.13389", "categories": ["cs.AI", "I.2.4; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.13389", "abs": "https://arxiv.org/abs/2509.13389", "authors": ["Carlos N\u00fa\u00f1ez-Molina", "Vicen\u00e7 G\u00f3mez", "Hector Geffner"], "title": "From Next Token Prediction to (STRIPS) World Models -- Preliminary Results", "comment": "10 pages, 3 figures", "summary": "We consider the problem of learning propositional STRIPS world models from\naction traces alone, using a deep learning architecture (transformers) and\ngradient descent. The task is cast as a supervised next token prediction\nproblem where the tokens are the actions, and an action $a$ may follow an\naction sequence if the hidden effects of the previous actions do not make an\naction precondition of $a$ false. We show that a suitable transformer\narchitecture can faithfully represent propositional STRIPS world models, and\nthat the models can be learned from sets of random valid (positive) and invalid\n(negative) action sequences alone. A number of experiments are reported.", "AI": {"tldr": "\u4f7f\u7528Transformer\u4ece\u52a8\u4f5c\u8f68\u8ff9\u5b66\u4e60\u547d\u9898STRIPS\u4e16\u754c\u6a21\u578b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u4ec5\u4ece\u52a8\u4f5c\u8f68\u8ff9\u4e2d\u5b66\u4e60\u547d\u9898STRIPS\u4e16\u754c\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08Transformer\uff09\u548c\u68af\u5ea6\u4e0b\u964d\uff0c\u5c06\u4efb\u52a1\u89c6\u4e3a\u76d1\u7763\u4e0b\u7684\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u95ee\u9898\uff0c\u5176\u4e2d\u4ee4\u724c\u662f\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u968f\u673a\u6709\u6548\u548c\u65e0\u6548\u52a8\u4f5c\u5e8f\u5217\u4e2d\u5b66\u4e60\u5230\u51c6\u786e\u7684\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5408\u9002\u7684Transformer\u67b6\u6784\u80fd\u591f\u51c6\u786e\u8868\u793a\u547d\u9898STRIPS\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u4e14\u4ec5\u901a\u8fc7\u968f\u673a\u6709\u6548\u548c\u65e0\u6548\u52a8\u4f5c\u5e8f\u5217\u5373\u53ef\u5b66\u4e60\u8fd9\u4e9b\u6a21\u578b\u3002"}}
{"id": "2509.13484", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13484", "abs": "https://arxiv.org/abs/2509.13484", "authors": ["Liu Liu", "Alexandra Kudaeva", "Marco Cipriano", "Fatimeh Al Ghannam", "Freya Tan", "Gerard de Melo", "Andres Sevtsuk"], "title": "MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes", "comment": "13 pages, 4 figures, under review at AAAI 2026", "summary": "Understanding group-level social interactions in public spaces is crucial for\nurban planning, informing the design of socially vibrant and inclusive\nenvironments. Detecting such interactions from images involves interpreting\nsubtle visual cues such as relations, proximity, and co-movement - semantically\ncomplex signals that go beyond traditional object detection. To address this\nchallenge, we introduce a social group region detection task, which requires\ninferring and spatially grounding visual regions defined by abstract\ninterpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level\nEngagement), a modular three-stage pipeline that integrates: (1) off-the-shelf\nhuman detection and depth estimation, (2) VLM-based reasoning to classify\npairwise social affiliation, and (3) a lightweight spatial aggregation\nalgorithm to localize socially connected groups. To support this task and\nencourage future research, we present a new dataset of 100K urban street-view\nimages annotated with bounding boxes and labels for both individuals and\nsocially interacting groups. The annotations combine human-created labels and\noutputs from the MINGLE pipeline, ensuring semantic richness and broad coverage\nof real-world scenarios.", "AI": {"tldr": "MINGLE\u6846\u67b6\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\u68c0\u6d4b\u516c\u5171\u7a7a\u95f4\u4e2d\u7684\u7fa4\u4f53\u793e\u4ea4\u4e92\u52a8\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u6570\u636e\u96c6\u652f\u6301\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u7406\u89e3\u516c\u5171\u7a7a\u95f4\u4e2d\u7684\u7fa4\u4f53\u793e\u4ea4\u4e92\u52a8\u5bf9\u57ce\u5e02\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u5bf9\u8c61\u68c0\u6d4b\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u793e\u4ea4\u4fe1\u53f7\u3002", "method": "MINGLE\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u4e09\u9636\u6bb5\u6d41\u7a0b\uff0c\u5305\u62ec\u73b0\u6210\u7684\u4eba\u7c7b\u68c0\u6d4b\u548c\u6df1\u5ea6\u4f30\u8ba1\u3001\u57fa\u4e8eVLM\u7684\u6210\u5bf9\u793e\u4ea4\u5173\u7cfb\u5206\u7c7b\uff0c\u4ee5\u53ca\u8f7b\u91cf\u7ea7\u7a7a\u95f4\u805a\u5408\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86MINGLE\u6846\u67b6\u548c\u4e00\u4e2a\u5305\u542b10\u4e07\u5f20\u57ce\u5e02\u8857\u666f\u56fe\u50cf\u7684\u65b0\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86\u4e2a\u4eba\u548c\u793e\u4ea4\u4e92\u52a8\u7fa4\u4f53\u7684\u8fb9\u754c\u6846\u548c\u6807\u7b7e\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u51faMINGLE\u6846\u67b6\u548c\u65b0\u7684\u6570\u636e\u96c6\uff0c\u4e3a\u7406\u89e3\u548c\u68c0\u6d4b\u516c\u5171\u7a7a\u95f4\u4e2d\u7684\u7fa4\u4f53\u793e\u4ea4\u4e92\u52a8\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u57ce\u5e02\u89c4\u5212\u548c\u793e\u4ea4\u4e92\u52a8\u5206\u6790\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.13941", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13941", "abs": "https://arxiv.org/abs/2509.13941", "authors": ["Simiao Liu", "Fang Liu", "Liehao Li", "Xin Tan", "Yinghao Zhu", "Xiaoli Lian", "Li Zhang"], "title": "An Empirical Study on Failures in Automated Issue Solving", "comment": null, "summary": "Automated issue solving seeks to autonomously identify and repair defective\ncode snippets across an entire codebase. SWE-Bench has emerged as the most\nwidely adopted benchmark for evaluating progress in this area. While LLM-based\nagentic tools show great promise, they still fail on a substantial portion of\ntasks. Moreover, current evaluations primarily report aggregate issue-solving\nrates, which obscure the underlying causes of success and failure, making it\nchallenging to diagnose model weaknesses or guide targeted improvements. To\nbridge this gap, we first analyze the performance and efficiency of three SOTA\ntools, spanning both pipeline-based and agentic architectures, in automated\nissue solving tasks of SWE-Bench-Verified under varying task characteristics.\nFurthermore, to move from high-level performance metrics to underlying cause\nanalysis, we conducted a systematic manual analysis of 150 failed instances.\nFrom this analysis, we developed a comprehensive taxonomy of failure modes\ncomprising 3 primary phases, 9 main categories, and 25 fine-grained\nsubcategories. Then we systematically analyze the distribution of the\nidentified failure modes, the results reveal distinct failure fingerprints\nbetween the two architectural paradigms, with the majority of agentic failures\nstemming from flawed reasoning and cognitive deadlocks. Motivated by these\ninsights, we propose a collaborative Expert-Executor framework. It introduces a\nsupervisory Expert agent tasked with providing strategic oversight and\ncourse-correction for a primary Executor agent. This architecture is designed\nto correct flawed reasoning and break the cognitive deadlocks that frequently\nlead to failure. Experiments show that our framework solves 22.2% of previously\nintractable issues for a leading single agent. These findings pave the way for\nbuilding more robust agents through diagnostic evaluation and collaborative\ndesign.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u81ea\u52a8\u5316\u4ee3\u7801\u4fee\u590d\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u63d0\u51fa\u4e86\u534f\u4f5c\u5f0fExpert-Executor\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u95ee\u9898\u89e3\u51b3\u7387\u3002", "motivation": "\u5f53\u524dLLM-based\u4ee3\u7406\u5de5\u5177\u5728\u81ea\u52a8\u5316\u95ee\u9898\u89e3\u51b3\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u805a\u5408\u95ee\u9898\u89e3\u51b3\u7387\uff0c\u96be\u4ee5\u8bca\u65ad\u6a21\u578b\u5f31\u70b9\u6216\u6307\u5bfc\u9488\u5bf9\u6027\u6539\u8fdb\u3002", "method": "\u7814\u7a76\u9996\u5148\u5206\u6790\u4e86\u4e09\u79cdSOTA\u5de5\u5177\u5728SWE-Bench-Verified\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u968f\u540e\u901a\u8fc7\u7cfb\u7edf\u624b\u52a8\u5206\u6790150\u4e2a\u5931\u8d25\u5b9e\u4f8b\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b3\u4e2a\u4e3b\u8981\u9636\u6bb5\u30019\u4e2a\u4e3b\u7c7b\u522b\u548c25\u4e2a\u5b50\u7c7b\u522b\u7684\u5931\u8d25\u6a21\u5f0f\u5206\u7c7b\u6cd5\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u534f\u4f5c\u5f0f\u7684Expert-Executor\u6846\u67b6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4ee3\u7406\u5f0f\u67b6\u6784\u7684\u5931\u8d25\u4e3b\u8981\u6e90\u4e8e\u63a8\u7406\u9519\u8bef\u548c\u8ba4\u77e5\u6b7b\u9501\u3002\u63d0\u51fa\u7684Expert-Executor\u6846\u67b6\u89e3\u51b3\u4e8622.2%\u4e4b\u524d\u5355\u4ee3\u7406\u65e0\u6cd5\u5904\u7406\u7684\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u63d0\u51fa\u534f\u4f5c\u5f0f\u7684Expert-Executor\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u4ee3\u7406\u5728SWE-Bench-Verified\u4efb\u52a1\u4e2d\u7684\u95ee\u9898\u89e3\u51b3\u7387\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u4ee3\u7406\u63d0\u4f9b\u4e86\u8bca\u65ad\u6027\u8bc4\u4f30\u548c\u534f\u4f5c\u8bbe\u8ba1\u7684\u8def\u5f84\u3002"}}
{"id": "2509.13579", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13579", "abs": "https://arxiv.org/abs/2509.13579", "authors": ["Momchil S. Tomov", "Sang Uk Lee", "Hansford Hendrago", "Jinwook Huh", "Teawon Han", "Forbes Howington", "Rafael da Silva", "Gianmarco Bernasconi", "Marc Heim", "Samuel Findler", "Xiaonan Ji", "Alexander Boule", "Michael Napoli", "Kuo Chen", "Jesse Miller", "Boaz Floor", "Yunqing Hu"], "title": "TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement Learning", "comment": null, "summary": "We present TreeIRL, a novel planner for autonomous driving that combines\nMonte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to\nachieve state-of-the-art performance in simulation and in real-world driving.\nThe core idea is to use MCTS to find a promising set of safe candidate\ntrajectories and a deep IRL scoring function to select the most human-like\namong them. We evaluate TreeIRL against both classical and state-of-the-art\nplanners in large-scale simulations and on 500+ miles of real-world autonomous\ndriving in the Las Vegas metropolitan area. Test scenarios include dense urban\ntraffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves\nthe best overall performance, striking a balance between safety, progress,\ncomfort, and human-likeness. To our knowledge, our work is the first\ndemonstration of MCTS-based planning on public roads and underscores the\nimportance of evaluating planners across a diverse set of metrics and in\nreal-world environments. TreeIRL is highly extensible and could be further\nimproved with reinforcement learning and imitation learning, providing a\nframework for exploring different combinations of classical and learning-based\napproaches to solve the planning bottleneck in autonomous driving.", "AI": {"tldr": "TreeIRL\u7ed3\u5408MCTS\u548cIRL\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u7684\u6700\u4f73\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u9053\u8def\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u7684\u5b89\u5168\u6027\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u6a21\u62df\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u4f7f\u7528MCTS\u5bfb\u627e\u5b89\u5168\u5019\u9009\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6IRL\u8bc4\u5206\u51fd\u6570\u9009\u62e9\u6700\u63a5\u8fd1\u4eba\u7c7b\u9a7e\u9a76\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u5927\u578b\u4eff\u771f\u548c\u62c9\u65af\u7ef4\u52a0\u65af\u5e02\u533a500+\u82f1\u91cc\u7684\u5b9e\u9645\u9a7e\u9a76\u6d4b\u8bd5\u4e2d\uff0cTreeIRL\u5728\u5b89\u5168\u6027\u3001\u8fdb\u5c55\u3001\u8212\u9002\u6027\u548c\u4eba\u7c7b\u76f8\u4f3c\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "TreeIRL\u7ed3\u5408\u4e86\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u548c\u9006\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\uff0c\u5728\u4eff\u771f\u548c\u5b9e\u9645\u9a7e\u9a76\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2509.13450", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13450", "abs": "https://arxiv.org/abs/2509.13450", "authors": ["Vincent Siu", "Nicholas Crispino", "David Park", "Nathan W. Henry", "Zhun Wang", "Yang Liu", "Dawn Song", "Chenguang Wang"], "title": "SteeringControl: Holistic Evaluation of Alignment Steering in LLMs", "comment": null, "summary": "We introduce SteeringControl, a benchmark for evaluating representation\nsteering methods across core alignment objectives--bias, harmful generation,\nand hallucination--and their effects on secondary behaviors such as sycophancy\nand commonsense morality. While prior alignment work often highlights\ntruthfulness or reasoning ability to demonstrate the side effects of\nrepresentation steering, we find there are many unexplored tradeoffs not yet\nunderstood in a systematic way. We collect a dataset of safety-relevant primary\nand secondary behaviors to evaluate steering effectiveness and behavioral\nentanglement centered around five popular steering methods. To enable this, we\ncraft a modular steering framework based on unique components that serve as the\nbuilding blocks of many existing methods. Our results on Qwen-2.5-7B and\nLlama-3.1-8B find that strong steering performance is dependent on the specific\ncombination of steering method, model, and targeted behavior, and that severe\nconcept entanglement can result from poor combinations of these three as well.\nWe release our code here:\nhttps://github.com/wang-research-lab/SteeringControl.git.", "AI": {"tldr": "SteeringControl\u662f\u4e00\u4e2a\u8bc4\u4f30\u8868\u793a\u5f15\u5bfc\u65b9\u6cd5\u5728\u6838\u5fc3\u5bf9\u9f50\u76ee\u6807\uff08\u504f\u89c1\u3001\u6709\u5bb3\u751f\u6210\u3001\u5e7b\u89c9\uff09\u53ca\u5176\u5bf9\u6b21\u8981\u884c\u4e3a\uff08\u5982\u5949\u627f\u548c\u5e38\u8bc6\u9053\u5fb7\uff09\u5f71\u54cd\u7684\u57fa\u51c6\u3002\u7814\u7a76\u53d1\u73b0\u5f15\u5bfc\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u65b9\u6cd5\u3001\u6a21\u578b\u548c\u884c\u4e3a\u7684\u7ec4\u5408\uff0c\u4e0d\u5f53\u7ec4\u5408\u4f1a\u5bfc\u81f4\u6982\u5ff5\u7ea0\u7f20\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u5de5\u4f5c\u5e38\u5f3a\u8c03\u771f\u5b9e\u6027\u6216\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8bb8\u591a\u672a\u63a2\u7d22\u7684\u6743\u8861\u5c1a\u672a\u7cfb\u7edf\u5316\u7406\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u8bc4\u4f30\u5f15\u5bfc\u65b9\u6cd5\u5728\u6838\u5fc3\u5bf9\u9f50\u76ee\u6807\uff08\u5982\u504f\u89c1\u3001\u6709\u5bb3\u751f\u6210\u548c\u5e7b\u89c9\uff09\u53ca\u5176\u5bf9\u6b21\u8981\u884c\u4e3a\uff08\u5982\u5949\u627f\u548c\u5e38\u8bc6\u9053\u5fb7\uff09\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u5b89\u5168\u76f8\u5173\u7684\u4e3b\u8981\u548c\u6b21\u8981\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u56f4\u7ed5\u4e94\u79cd\u6d41\u884c\u5f15\u5bfc\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4ef7\uff0c\u5e76\u57fa\u4e8e\u72ec\u7279\u7ec4\u4ef6\u6784\u5efa\u6a21\u5757\u5316\u5f15\u5bfc\u6846\u67b6\u3002", "result": "\u5728Qwen-2.5-7B\u548cLlama-3.1-8B\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f15\u5bfc\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u65b9\u6cd5\u3001\u6a21\u578b\u548c\u884c\u4e3a\u7684\u7279\u5b9a\u7ec4\u5408\uff0c\u4e14\u4e0d\u5f53\u7ec4\u5408\u4f1a\u5bfc\u81f4\u6982\u5ff5\u7ea0\u7f20\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f3a\u5f15\u5bfc\u6027\u80fd\u4f9d\u8d56\u4e8e\u7279\u5b9a\u5f15\u5bfc\u65b9\u6cd5\u3001\u6a21\u578b\u548c\u76ee\u6807\u884c\u4e3a\u7684\u7ec4\u5408\uff0c\u4e14\u8fd9\u4e09\u8005\u7ec4\u5408\u4e0d\u5f53\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u6982\u5ff5\u7ea0\u7f20\u3002"}}
{"id": "2509.13496", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13496", "abs": "https://arxiv.org/abs/2509.13496", "authors": ["Rajatsubhra Chakraborty", "Xujun Che", "Depeng Xu", "Cori Faklaris", "Xi Niu", "Shuhan Yuan"], "title": "BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation", "comment": null, "summary": "Bias discovery is critical for black-box generative models, especiall\ntext-to-image (TTI) models. Existing works predominantly focus on output-level\ndemographic distributions, which do not necessarily guarantee concept\nrepresentations to be disentangled post-mitigation. We propose BiasMap, a\nmodel-agnostic framework for uncovering latent concept-level representational\nbiases in stable diffusion models. BiasMap leverages cross-attention\nattribution maps to reveal structural entanglements between demographics (e.g.,\ngender, race) and semantics (e.g., professions), going deeper into\nrepresentational bias during the image generation. Using attribution maps of\nthese concepts, we quantify the spatial demographics-semantics concept\nentanglement via Intersection over Union (IoU), offering a lens into bias that\nremains hidden in existing fairness discovery approaches. In addition, we\nfurther utilize BiasMap for bias mitigation through energy-guided diffusion\nsampling that directly modifies latent noise space and minimizes the expected\nSoftIoU during the denoising process. Our findings show that existing fairness\ninterventions may reduce the output distributional gap but often fail to\ndisentangle concept-level coupling, whereas our mitigation method can mitigate\nconcept entanglement in image generation while complementing distributional\nbias mitigation.", "AI": {"tldr": "BiasMap\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u53d1\u73b0\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u6982\u5ff5\u7ea7\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7f13\u89e3\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8f93\u51fa\u7ea7\u7684\u4eba\u53e3\u7edf\u8ba1\u5206\u5e03\uff0c\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u6982\u5ff5\u8868\u793a\u5728\u7f13\u89e3\u540e\u89e3\u8026\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6df1\u5165\u7684\u65b9\u6cd5\u6765\u63ed\u793a\u548c\u7f13\u89e3\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u6982\u5ff5\u7ea7\u504f\u5dee\u3002", "method": "BiasMap\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u5f52\u56e0\u56fe\u6765\u63ed\u793a\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u4e0e\u8bed\u4e49\u6982\u5ff5\u4e4b\u95f4\u7684\u7ed3\u6784\u7ea0\u7f20\uff0c\u5e76\u901a\u8fc7IoU\u91cf\u5316\u8fd9\u79cd\u7ea0\u7f20\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u80fd\u91cf\u5f15\u5bfc\u6269\u6563\u91c7\u6837\u76f4\u63a5\u4fee\u6539\u6f5c\u5728\u566a\u58f0\u7a7a\u95f4\uff0c\u5e76\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6700\u5c0f\u5316\u9884\u671f\u7684SoftIoU\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u7684\u516c\u5e73\u6027\u5e72\u9884\u63aa\u65bd\u53ef\u80fd\u51cf\u5c11\u8f93\u51fa\u5206\u5e03\u5dee\u8ddd\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u89e3\u8026\u6982\u5ff5\u7ea7\u8026\u5408\u3002\u800cBiasMap\u7684\u7f13\u89e3\u65b9\u6cd5\u53ef\u4ee5\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7f13\u89e3\u6982\u5ff5\u7ea0\u7f20\uff0c\u5e76\u8865\u5145\u5206\u5e03\u504f\u5dee\u7f13\u89e3\u3002", "conclusion": "BiasMap\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u53d1\u73b0\u548c\u7f13\u89e3\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u6982\u5ff5\u7ea7\u504f\u5dee\uff0c\u7279\u522b\u662f\u5728\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u4e2d\u3002\u5b83\u4e0d\u4ec5\u63ed\u793a\u4e86\u73b0\u6709\u516c\u5e73\u6027\u5e72\u9884\u63aa\u65bd\u7684\u5c40\u9650\u6027\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7f13\u89e3\u65b9\u6cd5\u3002"}}
{"id": "2509.13942", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13942", "abs": "https://arxiv.org/abs/2509.13942", "authors": ["Duc Minh Ha", "Phu Trac Kien", "Tho Quan", "Anh Nguyen-Duc"], "title": "Evaluating Classical Software Process Models as Coordination Mechanisms for LLM-Based Software Generation", "comment": null, "summary": "[Background] Large Language Model (LLM)-based multi-agent systems (MAS) are\ntransforming software development by enabling autonomous collaboration.\nClassical software processes such asWaterfall, V-Model, and Agile offer\nstructured coordination patterns that can be repurposed to guide these agent\ninteractions. [Aims] This study explores how traditional software development\nprocesses can be adapted as coordination scaffolds for LLM based MAS and\nexamines their impact on code quality, cost, and productivity. [Method] We\nexecuted 11 diverse software projects under three process models and four GPT\nvariants, totaling 132 runs. Each output was evaluated using standardized\nmetrics for size (files, LOC), cost (execution time, token usage), and quality\n(code smells, AI- and human detected bugs). [Results] Both process model and\nLLM choice significantly affected system performance. Waterfall was most\nefficient, V-Model produced the most verbose code, and Agile achieved the\nhighest code quality, albeit at higher computational cost. [Conclusions]\nClassical software processes can be effectively instantiated in LLM-based MAS,\nbut each entails trade-offs across quality, cost, and adaptability. Process\nselection should reflect project goals, whether prioritizing efficiency,\nrobustness, or structured validation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4f20\u7edf\u8f6f\u4ef6\u6d41\u7a0b\u53ef\u4f5c\u4e3aLLM\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u534f\u8c03\u6846\u67b6\uff0c\u4f46\u5404\u6709\u4f18\u52a3\uff1a\u7011\u5e03\u6a21\u578b\u9ad8\u6548\uff0cV\u6a21\u578b\u5197\u957f\uff0c\u654f\u6377\u6a21\u578b\u8d28\u91cf\u9ad8\u4f46\u6210\u672c\u9ad8\u3002", "motivation": "\u63a2\u7d22\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u5982\u4f55\u4f5c\u4e3a\u534f\u8c03\u6846\u67b6\u9002\u5e94\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5e76\u7814\u7a76\u5176\u5bf9\u4ee3\u7801\u8d28\u91cf\u3001\u6210\u672c\u548c\u751f\u4ea7\u7387\u7684\u5f71\u54cd\u3002", "method": "\u6211\u4eec\u6267\u884c\u4e8611\u4e2a\u4e0d\u540c\u7684\u8f6f\u4ef6\u9879\u76ee\uff0c\u91c7\u7528\u4e09\u79cd\u6d41\u7a0b\u6a21\u578b\u548c\u56db\u79cdGPT\u53d8\u4f53\uff0c\u5171132\u6b21\u8fd0\u884c\u3002\u6bcf\u4e2a\u8f93\u51fa\u5747\u4f7f\u7528\u6807\u51c6\u5316\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u5305\u62ec\u89c4\u6a21\uff08\u6587\u4ef6\u3001\u4ee3\u7801\u884c\u6570\uff09\u3001\u6210\u672c\uff08\u6267\u884c\u65f6\u95f4\u3001\u4ee4\u724c\u4f7f\u7528\u91cf\uff09\u548c\u8d28\u91cf\uff08\u4ee3\u7801\u5f02\u5473\u3001AI\u548c\u4eba\u5de5\u68c0\u6d4b\u5230\u7684\u9519\u8bef\uff09\u3002", "result": "\u6d41\u7a0b\u6a21\u578b\u548cLLM\u9009\u62e9\u5747\u663e\u8457\u5f71\u54cd\u7cfb\u7edf\u6027\u80fd\u3002\u7011\u5e03\u6a21\u578b\u6548\u7387\u6700\u9ad8\uff0cV\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u6700\u5197\u957f\uff0c\u800c\u654f\u6377\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u4ee3\u7801\u8d28\u91cf\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\u3002", "conclusion": "\u7ecf\u5178\u8f6f\u4ef6\u6d41\u7a0b\u53ef\u4ee5\u6709\u6548\u5730\u5728\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u5b9e\u4f8b\u5316\uff0c\u4f46\u6bcf\u79cd\u6d41\u7a0b\u5728\u8d28\u91cf\u3001\u6210\u672c\u548c\u9002\u5e94\u6027\u65b9\u9762\u90fd\u6709\u6743\u8861\u3002\u6d41\u7a0b\u9009\u62e9\u5e94\u53cd\u6620\u9879\u76ee\u76ee\u6807\uff0c\u65e0\u8bba\u662f\u4f18\u5148\u8003\u8651\u6548\u7387\u3001\u7a33\u5065\u6027\u8fd8\u662f\u7ed3\u6784\u5316\u9a8c\u8bc1\u3002"}}
{"id": "2509.13591", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13591", "abs": "https://arxiv.org/abs/2509.13591", "authors": ["Amir-Hossein Shahidzadeh", "Jiyue Zhu", "Kezhou Chen", "Sha Yi", "Cornelia Ferm\u00fcller", "Yiannis Aloimonos", "Xiaolong Wang"], "title": "Object Pose Estimation through Dexterous Touch", "comment": null, "summary": "Robust object pose estimation is essential for manipulation and interaction\ntasks in robotics, particularly in scenarios where visual data is limited or\nsensitive to lighting, occlusions, and appearances. Tactile sensors often offer\nlimited and local contact information, making it challenging to reconstruct the\npose from partial data. Our approach uses sensorimotor exploration to actively\ncontrol a robot hand to interact with the object. We train with Reinforcement\nLearning (RL) to explore and collect tactile data. The collected 3D point\nclouds are used to iteratively refine the object's shape and pose. In our\nsetup, one hand holds the object steady while the other performs active\nexploration. We show that our method can actively explore an object's surface\nto identify critical pose features without prior knowledge of the object's\ngeometry. Supplementary material and more demonstrations will be provided at\nhttps://amirshahid.github.io/BimanualTactilePose .", "AI": {"tldr": "\u901a\u8fc7\u53cc\u624b\u673a\u5668\u4eba\u89e6\u89c9\u63a2\u7d22\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u5148\u9a8c\u51e0\u4f55\u77e5\u8bc6\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u89c6\u89c9\u53d7\u9650\u573a\u666f\u3002", "motivation": "\u5728\u89c6\u89c9\u6570\u636e\u6709\u9650\u6216\u53d7\u5149\u7167\u3001\u906e\u6321\u548c\u5916\u89c2\u5f71\u54cd\u7684\u60c5\u51b5\u4e0b\uff0c\u89e6\u89c9\u4f20\u611f\u5668\u63d0\u4f9b\u7684\u5c40\u90e8\u63a5\u89e6\u4fe1\u606f\u96be\u4ee5\u76f4\u63a5\u91cd\u5efa\u7269\u4f53\u59ff\u6001\uff0c\u56e0\u6b64\u9700\u8981\u4e3b\u52a8\u63a2\u7d22\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u673a\u5668\u4eba\u624b\u4e3b\u52a8\u63a2\u7d22\u7269\u4f53\u8868\u9762\uff0c\u6536\u96c6\u89e6\u89c9\u6570\u636e\u5e76\u751f\u62103D\u70b9\u4e91\uff0c\u8fed\u4ee3\u4f18\u5316\u7269\u4f53\u5f62\u72b6\u548c\u59ff\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3b\u52a8\u63a2\u7d22\u7269\u4f53\u8868\u9762\u5e76\u8bc6\u522b\u5173\u952e\u59ff\u6001\u7279\u5f81\uff0c\u65e0\u9700\u7269\u4f53\u51e0\u4f55\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u53cc\u624b\u673a\u5668\u4eba\u89e6\u89c9\u63a2\u7d22\u5b9e\u73b0\u4e86\u65e0\u9700\u5148\u9a8c\u51e0\u4f55\u77e5\u8bc6\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u5c55\u793a\u4e86\u5728\u89c6\u89c9\u53d7\u9650\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.13547", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13547", "abs": "https://arxiv.org/abs/2509.13547", "authors": ["Harper Reed", "Michael Sugimura", "Angelo Zangari"], "title": "AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving", "comment": "16 pages, 5 tables", "summary": "We investigate whether giving LLM agents the collaborative tools and autonomy\nthat humans naturally use for problem solving can improve their performance. We\nequip Claude Code agents with MCP-based social media and journaling tools and\nallow them to use these tools as they see fit. Across 34 Aider Polyglot Python\nprogramming challenges, collaborative tools substantially improve performance\non the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and\n12-38% faster completion than baseline agents. Effects on the full challenge\nset are mixed, suggesting these tools act as performance enhancers when\nadditional reasoning scaffolding is most needed. Surprisingly, Different models\nnaturally adopted distinct collaborative strategies without explicit\ninstruction. Sonnet 3.7 engaged broadly across tools and benefited from\narticulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,\nleaning on journal-based semantic search when problems were genuinely\ndifficult. This mirrors how human developers adjust collaboration based on\nexpertise and task complexity. Behavioral analysis shows agents prefer writing\nover reading by about 2-9x, indicating that structured articulation drives much\nof the improvement rather than information access alone. Overall, AI agents can\nsystematically benefit from human-inspired collaboration tools at the edge of\ntheir capabilities, pointing to adaptive collaborative interfaces as reasoning\nenhancers rather than universal efficiency boosts.", "AI": {"tldr": "Giving LLM agents human-like collaborative tools improves their performance on hard tasks, with varying strategies adopted by different models, indicating these tools enhance reasoning when most needed.", "motivation": "To determine if giving LLM agents collaborative tools and autonomy, similar to humans, can improve their performance.", "method": "Equipping Claude Code agents with MCP-based social media and journaling tools, allowing them to use these tools autonomously across 34 Aider Polyglot Python programming challenges.", "result": "Collaborative tools significantly improved performance on the hardest problems, with 15-40% lower cost, 12-27% fewer turns, and 12-38% faster completion than baseline agents. Different models adopted distinct collaborative strategies without explicit instruction.", "conclusion": "AI agents can benefit from human-inspired collaboration tools, especially at the edge of their capabilities, suggesting these tools act as reasoning enhancers rather than universal efficiency boosts."}}
{"id": "2509.13504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13504", "abs": "https://arxiv.org/abs/2509.13504", "authors": ["Uriel Garcilazo-Cruz", "Joseph O. Okeme", "Rodrigo A. Vargas--Hern\u00e1ndez"], "title": "LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming", "comment": "8 pages, 10 figures, SM, 5 pages, 4 figures", "summary": "The lack of flexible annotation tools has hindered the deployment of AI\nmodels in some scientific areas. Most existing image annotation software\nrequires users to upload a precollected dataset, which limits support for\non-demand pipelines and introduces unnecessary steps to acquire images. This\nconstraint is particularly problematic in laboratory environments, where\nreal-time data acquisition from instruments such as microscopes is increasingly\ncommon. In this work, we introduce \\texttt{LivePixel}, a Python-based graphical\nuser interface that integrates with imaging systems, such as webcams,\nmicroscopes, and others, to enable real-time image annotation. LivePyxel is\ndesigned to be easy to use through a simple interface that allows users to\nprecisely delimit areas for annotation using tools commonly found in commercial\ngraphics editing software. Of particular interest is the availability of\nB\\'ezier splines and binary masks, and the software's capacity to work with\nnon-destructive layers that enable high-performance editing. LivePyxel also\nintegrates a wide compatibility across video devices, and it's optimized for\nobject detection operations via the use of OpenCV in combination with\nhigh-performance libraries designed to handle matrix and linear algebra\noperations via Numpy effectively. LivePyxel facilitates seamless data\ncollection and labeling, accelerating the development of AI models in\nexperimental workflows. LivePyxel freely available at\nhttps://github.com/UGarCil/LivePyxel", "AI": {"tldr": "LivePyxel\u662f\u4e00\u4e2a\u5b9e\u65f6\u56fe\u50cf\u6807\u6ce8\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u79d1\u5b66\u9886\u57df\u7f3a\u4e4f\u7075\u6d3b\u6807\u6ce8\u5de5\u5177\u7684\u95ee\u9898\uff0c\u652f\u6301\u591a\u79cd\u6210\u50cf\u8bbe\u5907\u548c\u9ad8\u6548\u7f16\u8f91\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u6807\u6ce8\u8f6f\u4ef6\u901a\u5e38\u8981\u6c42\u7528\u6237\u4e0a\u4f20\u9884\u6536\u96c6\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u6309\u9700\u6d41\u7a0b\u7684\u652f\u6301\uff0c\u5e76\u4e3a\u83b7\u53d6\u56fe\u50cf\u5f15\u5165\u4e86\u4e0d\u5fc5\u8981\u7684\u6b65\u9aa4\uff0c\u5c24\u5176\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u5b9e\u65f6\u6570\u636e\u91c7\u96c6\u65e5\u76ca\u666e\u904d\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e00\u95ee\u9898\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "LivePyxel\u96c6\u6210\u4e86\u6210\u50cf\u7cfb\u7edf\uff08\u5982\u7f51\u7edc\u6444\u50cf\u5934\u3001\u663e\u5fae\u955c\u7b49\uff09\uff0c\u63d0\u4f9b\u7b80\u5355\u6613\u7528\u7684\u754c\u9762\uff0c\u652f\u6301B\u00e9zier\u6837\u6761\u548c\u4e8c\u8fdb\u5236\u63a9\u7801\u7b49\u5de5\u5177\uff0c\u5e76\u5229\u7528OpenCV\u548c\u9ad8\u6027\u80fd\u5e93\uff08\u5982Numpy\uff09\u4f18\u5316\u5bf9\u8c61\u68c0\u6d4b\u64cd\u4f5c\u3002", "result": "LivePyxel\u5b9e\u73b0\u4e86\u4e0e\u591a\u79cd\u89c6\u9891\u8bbe\u5907\u7684\u5e7f\u6cdb\u517c\u5bb9\u6027\uff0c\u652f\u6301\u9ad8\u6027\u80fd\u7f16\u8f91\u7684\u975e\u7834\u574f\u6027\u56fe\u5c42\uff0c\u4e3a\u5b9e\u65f6\u56fe\u50cf\u6807\u6ce8\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "LivePyxel\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\u5de5\u5177\uff0c\u65e8\u5728\u89e3\u51b3\u79d1\u5b66\u9886\u57df\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u56fe\u50cf\u6807\u6ce8\u5de5\u5177\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5b9e\u65f6\u56fe\u50cf\u6807\u6ce8\u52a0\u901fAI\u6a21\u578b\u5728\u5b9e\u9a8c\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5f00\u53d1\u3002"}}
{"id": "2509.14093", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14093", "abs": "https://arxiv.org/abs/2509.14093", "authors": ["Kerui Huang", "Shuhan Liu", "Xing Hu", "Tongtong Xu", "Lingfeng Bao", "Xin Xia"], "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework", "comment": null, "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.", "AI": {"tldr": "SEER\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29CoT\u63a8\u7406\u6b65\u9aa4\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u63ed\u793a\u4e86\u8fc7\u957f\u7684CoT\u63a8\u7406\u6b65\u9aa4\u53ef\u80fd\u5bfc\u81f4\u8f93\u51fa\u622a\u65ad\u3001\u51c6\u786e\u6027\u4e0b\u964d\u548c\u9ad8\u5ef6\u8fdf\uff0c\u6311\u6218\u4e86\u2018\u66f4\u957f\u63a8\u7406\u6b65\u9aa4\u66f4\u597d\u2019\u7684\u5047\u8bbe\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u9002\u5e94\u63a7\u5236CoT\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86SEER\uff08Self-Enhancing Efficient Reasoning\uff09\u6846\u67b6\uff0c\u7ed3\u5408Best-of-N\u91c7\u6837\u548c\u4efb\u52a1\u611f\u77e5\u81ea\u9002\u5e94\u8fc7\u6ee4\uff0c\u52a8\u6001\u8c03\u6574\u9608\u503c\u4ee5\u51cf\u5c11\u5197\u4f59\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "result": "SEER\u5e73\u5747\u7f29\u77edCoT\u63a8\u7406\u6b65\u9aa442.1%\uff0c\u51cf\u5c11\u622a\u65ad\u60c5\u51b5\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u5e76\u6d88\u9664\u5927\u591a\u6570\u65e0\u9650\u5faa\u73af\u3002", "conclusion": "SEER\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u538b\u7f29CoT\u63a8\u7406\u6b65\u9aa4\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684LLMs\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13595", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13595", "abs": "https://arxiv.org/abs/2509.13595", "authors": ["Xiao Liu", "Weijun Wang", "Tianlun Huang", "Zhiyong Wang", "Wei Feng"], "title": "Leg-Arm Coordinated Operation for Curtain Wall Installation", "comment": null, "summary": "With the acceleration of urbanization, the number of high-rise buildings and\nlarge public facilities is increasing, making curtain walls an essential\ncomponent of modern architecture with widespread applications. Traditional\ncurtain wall installation methods face challenges such as variable on-site\nterrain, high labor intensity, low construction efficiency, and significant\nsafety risks. Large panels often require multiple workers to complete\ninstallation. To address these issues, based on a hexapod curtain wall\ninstallation robot, we design a hierarchical optimization-based whole-body\ncontrol framework for coordinated arm-leg planning tailored to three key tasks:\nwall installation, ceiling installation, and floor laying. This framework\nintegrates the motion of the hexapod legs with the operation of the folding arm\nand the serial-parallel manipulator. We conduct experiments on the hexapod\ncurtain wall installation robot to validate the proposed control method,\ndemonstrating its capability in performing curtain wall installation tasks. Our\nresults confirm the effectiveness of the hierarchical optimization-based\narm-leg coordination framework for the hexapod robot, laying the foundation for\nits further application in complex construction site environments.", "AI": {"tldr": "A hexapod robot with a hierarchical control framework is developed to address challenges in curtain wall installation, proving effective in experiments.", "motivation": "Traditional curtain wall installation methods face challenges like variable terrain, high labor intensity, low efficiency, and safety risks, prompting the need for robotic solutions.", "method": "A hierarchical optimization-based whole-body control framework is designed for coordinated arm-leg planning, integrating leg motion with folding arm and serial-parallel manipulator operations.", "result": "Experiments validate the control method's capability in performing curtain wall installation tasks.", "conclusion": "The hierarchical optimization-based arm-leg coordination framework for the hexapod robot is effective in performing curtain wall installation tasks, providing a foundation for its application in complex construction environments."}}
{"id": "2509.13570", "categories": ["cs.AI", "math.HO", "Primary: 97U50, Secondary: 97U70, 97D40, 97D60, 97E50, 97H40"], "pdf": "https://arxiv.org/pdf/2509.13570", "abs": "https://arxiv.org/abs/2509.13570", "authors": ["Hannah Klawa", "Shraddha Rajpal", "Cigole Thomas"], "title": "Gen AI in Proof-based Math Courses: A Pilot Study", "comment": "35 pages, 6 figures, Comments welcome!", "summary": "With the rapid rise of generative AI in higher education and the\nunreliability of current AI detection tools, developing policies that encourage\nstudent learning and critical thinking has become increasingly important. This\nstudy examines student use and perceptions of generative AI across three\nproof-based undergraduate mathematics courses: a first-semester abstract\nalgebra course, a topology course and a second-semester abstract algebra\ncourse. In each case, course policy permitted some use of generative AI.\nDrawing on survey responses and student interviews, we analyze how students\nengaged with AI tools, their perceptions of generative AI's usefulness and\nlimitations, and what implications these perceptions hold for teaching\nproof-based mathematics. We conclude by discussing future considerations for\nintegrating generative AI into proof-based mathematics instruction.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u4e09\u4e2a\u57fa\u4e8e\u8bc1\u660e\u7684\u672c\u79d1\u6570\u5b66\u8bfe\u7a0b\u4e2d\u5b66\u751f\u5bf9\u751f\u6210\u5f0fAI\u7684\u4f7f\u7528\u548c\u770b\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u5728\u6559\u5b66\u4e2d\u6574\u5408AI\u7684\u8003\u8651\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684\u5feb\u901f\u5d1b\u8d77\u548c\u5f53\u524dAI\u68c0\u6d4b\u5de5\u5177\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u5236\u5b9a\u9f13\u52b1\u5b66\u751f\u5b66\u4e60\u548c\u6279\u5224\u6027\u601d\u7ef4\u7684\u653f\u7b56\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u95ee\u5377\u548c\u5b66\u751f\u8bbf\u8c08\uff0c\u5206\u6790\u4e86\u5b66\u751f\u5982\u4f55\u4f7f\u7528AI\u5de5\u5177\u3001\u4ed6\u4eec\u5bf9\u751f\u6210\u5f0fAI\u6709\u7528\u6027\u548c\u5c40\u9650\u6027\u7684\u770b\u6cd5\u3002", "result": "\u7814\u7a76\u5206\u6790\u4e86\u5b66\u751f\u5bf9\u751f\u6210\u5f0fAI\u7684\u4f7f\u7528\u548c\u770b\u6cd5\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u770b\u6cd5\u5bf9\u57fa\u4e8e\u8bc1\u660e\u7684\u6570\u5b66\u6559\u5b66\u7684\u5f71\u54cd\u3002", "conclusion": "\u8bba\u6587\u8ba8\u8bba\u4e86\u5c06\u751f\u6210\u5f0fAI\u6574\u5408\u5230\u57fa\u4e8e\u8bc1\u660e\u7684\u6570\u5b66\u6559\u5b66\u4e2d\u7684\u672a\u6765\u8003\u8651\u3002"}}
{"id": "2509.13506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13506", "abs": "https://arxiv.org/abs/2509.13506", "authors": ["Xingzi Xu", "Qi Li", "Shuwen Qiu", "Julien Han", "Karim Bouyarmane"], "title": "DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform", "comment": "Published in 2025 CVPR Workshop", "summary": "Diffusion models enable high-quality virtual try-on (VTO) with their\nestablished image synthesis abilities. Despite the extensive end-to-end\ntraining of large pre-trained models involved in current VTO methods,\nreal-world applications often prioritize limited training and inference,\nserving, and deployment budgets for VTO. To solve this obstacle, we apply\nDoob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained\nunconditional models for downstream image-conditioned VTO abilities. DEFT\nfreezes the pre-trained model's parameters and trains a small h-transform\nnetwork to learn a conditional h-transform. The h-transform network allows\ntraining only 1.42 percent of the frozen parameters, compared to a baseline of\n5.52 percent in traditional parameter-efficient fine-tuning (PEFT).\n  To further improve DEFT's performance and decrease existing models' inference\ntime, we additionally propose an adaptive consistency loss. Consistency\ntraining distills slow but high-performing diffusion models into a fast one\nwhile retaining performance by enforcing consistencies along the inference\npath. Inspired by constrained optimization, instead of distillation, we combine\nthe consistency loss and the denoising score matching loss in a data-adaptive\nmanner for fine-tuning existing VTO models at a low cost. Empirical results\nshow the proposed DEFT-VTON method achieves state-of-the-art performance on VTO\ntasks, with as few as 15 denoising steps, while maintaining competitive\nresults.", "AI": {"tldr": "DEFT-VTON\u901a\u8fc7\u9ad8\u6548\u5fae\u8c03\u548c\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5728\u865a\u62df\u8bd5\u8863\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u4f4e\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u5e94\u7528\u4e2d\u6709\u9650\u8bad\u7ec3\u548c\u63a8\u7406\u9884\u7b97\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u865a\u62df\u8bd5\u8863\u80fd\u529b\u3002", "method": "\u5e94\u7528Doob\u7684h-transform\u9ad8\u6548\u5fae\u8c03\uff08DEFT\uff09\u6765\u9002\u5e94\u5927\u578b\u9884\u8bad\u7ec3\u65e0\u6761\u4ef6\u6a21\u578b\uff0c\u7528\u4e8e\u4e0b\u6e38\u56fe\u50cf\u6761\u4ef6\u865a\u62df\u8bd5\u8863\u80fd\u529b\u3002DEFT\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u53c2\u6570\uff0c\u5e76\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u578bh-transform\u7f51\u7edc\u6765\u5b66\u4e60\u6761\u4ef6h-transform\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u635f\u5931\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u5e76\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3002", "result": "DEFT-VTON\u5728\u865a\u62df\u8bd5\u8863\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u970015\u4e2a\u53bb\u566a\u6b65\u9aa4\uff0c\u4e14\u8bad\u7ec3\u53c2\u6570\u5360\u6bd4\u4ec5\u4e3a1.42%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "DEFT-VTON\u65b9\u6cd5\u5728\u865a\u62df\u8bd5\u8863\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4ec5\u970015\u4e2a\u53bb\u566a\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002"}}
{"id": "2509.13704", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13704", "abs": "https://arxiv.org/abs/2509.13704", "authors": ["Liangtao Lin", "Zhaomeng Zhu", "Tianwei Zhang", "Yonggang Wen"], "title": "InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management", "comment": null, "summary": "Mission-critical industrial infrastructure, such as data centers,\nincreasingly depends on complex management software. Its operations, however,\npose significant challenges due to the escalating system complexity,\nmulti-vendor integration, and a shortage of expert operators. While Robotic\nProcess Automation (RPA) offers partial automation through handcrafted scripts,\nit suffers from limited flexibility and high maintenance costs. Recent advances\nin Large Language Model (LLM)-based graphical user interface (GUI) agents have\nenabled more flexible automation, yet these general-purpose agents face five\ncritical challenges when applied to industrial management, including unfamiliar\nelement understanding, precision and efficiency, state localization, deployment\nconstraints, and safety requirements. To address these issues, we propose\nInfraMind, a novel exploration-based GUI agentic framework specifically\ntailored for industrial management systems. InfraMind integrates five\ninnovative modules to systematically resolve different challenges in industrial\nmanagement: (1) systematic search-based exploration with virtual machine\nsnapshots for autonomous understanding of complex GUIs; (2) memory-driven\nplanning to ensure high-precision and efficient task execution; (3) advanced\nstate identification for robust localization in hierarchical interfaces; (4)\nstructured knowledge distillation for efficient deployment with lightweight\nmodels; and (5) comprehensive, multi-layered safety mechanisms to safeguard\nsensitive operations. Extensive experiments on both open-source and commercial\nDCIM platforms demonstrate that our approach consistently outperforms existing\nframeworks in terms of task success rate and operational efficiency, providing\na rigorous and scalable solution for industrial management automation.", "AI": {"tldr": "InfraMind\u662f\u4e00\u4e2a\u4e13\u4e3a\u5de5\u4e1a\u7ba1\u7406\u7cfb\u7edf\u8bbe\u8ba1\u7684GUI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u4e2a\u521b\u65b0\u6a21\u5757\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6848\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u8f6f\u4ef6\u9762\u4e34\u7cfb\u7edf\u590d\u6742\u6027\u3001\u591a\u4f9b\u5e94\u5546\u96c6\u6210\u548c\u4e13\u5bb6\u64cd\u4f5c\u5458\u77ed\u7f3a\u7684\u6311\u6218\u3002\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6848\u5982RPA\u7075\u6d3b\u6027\u4e0d\u8db3\uff0cLLM-based GUI\u4ee3\u7406\u5728\u5de5\u4e1a\u7ba1\u7406\u4e2d\u9762\u4e34\u4e94\u5927\u5173\u952e\u6311\u6218\u3002", "method": "InfraMind\u6574\u5408\u4e86\u4e94\u4e2a\u521b\u65b0\u6a21\u5757\uff1a(1)\u57fa\u4e8e\u7cfb\u7edf\u641c\u7d22\u7684\u63a2\u7d22\u4e0e\u865a\u62df\u673a\u5feb\u7167\uff1b(2)\u8bb0\u5fc6\u9a71\u52a8\u7684\u89c4\u5212\uff1b(3)\u9ad8\u7ea7\u72b6\u6001\u8bc6\u522b\uff1b(4)\u7ed3\u6784\u5316\u77e5\u8bc6\u84b8\u998f\uff1b(5)\u591a\u5c42\u5b89\u5168\u673a\u5236\u3002", "result": "\u5728\u5f00\u6e90\u548c\u5546\u4e1aDCIM\u5e73\u53f0\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cInfraMind\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u64cd\u4f5c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u3002", "conclusion": "InfraMind\u6846\u67b6\u901a\u8fc7\u4e94\u4e2a\u521b\u65b0\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1a\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u64cd\u4f5c\u6548\u7387\uff0c\u4e3a\u5de5\u4e1a\u7ba1\u7406\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u4e25\u8c28\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13649", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13649", "abs": "https://arxiv.org/abs/2509.13649", "authors": ["M\u00e9lon\u00e9 Nyoba Tchonkeu", "Soulaimane Berkane", "Tarek Hamel"], "title": "Barometer-Aided Attitude Estimation", "comment": "6 pages, 4 figures. this manuscript is submitted to IEEE Control\n  Systems Letters (L-CSS) with American Control Conference (ACC) option", "summary": "Accurate and robust attitude estimation is a central challenge for autonomous\nvehicles operating in GNSS-denied or highly dynamic environments. In such\ncases, Inertial Measurement Units (IMUs) alone are insufficient for reliable\ntilt estimation due to the ambiguity between gravitational and inertial\naccelerations. While auxiliary velocity sensors, such as GNSS, Pitot tubes,\nDoppler radar, or visual odometry, are often used, they can be unavailable,\nintermittent, or costly. This work introduces a barometer-aided attitude\nestimation architecture that leverages barometric altitude measurements to\ninfer vertical velocity and attitude within a nonlinear observer on SO(3). The\ndesign cascades a deterministic Riccati observer with a complementary filter,\nensuring Almost Global Asymptotic Stability (AGAS) under a uniform\nobservability condition while maintaining geometric consistency. The analysis\nhighlights barometer-aided estimation as a lightweight and effective\ncomplementary modality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6c14\u538b\u8ba1\u7684\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u89c2\u6d4b\u5668\u7ed3\u5408\u6c14\u538b\u9ad8\u5ea6\u6d4b\u91cf\uff0c\u89e3\u51b3\u4e86GNSS\u7f3a\u5931\u73af\u5883\u4e0b\u7684\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u5177\u6709\u7a33\u5065\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "motivation": "\u5728GNSS\u7f3a\u5931\u6216\u9ad8\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u4ec5\u4f9d\u8d56IMU\u7684\u59ff\u6001\u4f30\u8ba1\u4e0d\u53ef\u9760\uff0c\u800c\u8f85\u52a9\u901f\u5ea6\u4f20\u611f\u5668\uff08\u5982GNSS\u3001\u76ae\u6258\u7ba1\u7b49\uff09\u53ef\u80fd\u4e0d\u53ef\u7528\u6216\u6210\u672c\u9ad8\u6602\u3002\u6c14\u538b\u8ba1\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u4e14\u6709\u6548\u7684\u8865\u5145\u6a21\u6001\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u5408\u786e\u5b9a\u6027Riccati\u89c2\u6d4b\u5668\u548c\u4e92\u8865\u6ee4\u6ce2\u5668\u7684\u975e\u7ebf\u6027\u89c2\u6d4b\u5668\u67b6\u6784\uff0c\u5229\u7528\u6c14\u538b\u9ad8\u5ea6\u6d4b\u91cf\u63a8\u65ad\u5782\u76f4\u901f\u5ea6\u548c\u59ff\u6001\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5747\u5300\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u51e0\u4e4e\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u6027\uff08AGAS\uff09\uff0c\u5e76\u4fdd\u6301\u4e86\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6c14\u538b\u8ba1\u7684\u59ff\u6001\u4f30\u8ba1\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u6c14\u538b\u9ad8\u5ea6\u6d4b\u91cf\u548c\u975e\u7ebf\u6027\u89c2\u6d4b\u5668\uff0c\u5b9e\u73b0\u4e86\u5728GNSS\u7f3a\u5931\u6216\u9ad8\u52a8\u6001\u73af\u5883\u4e0b\u7684\u7a33\u5065\u59ff\u6001\u4f30\u8ba1\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u8bc1\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u5177\u6709\u51e0\u4e4e\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u6027\uff08AGAS\uff09\u3002"}}
{"id": "2509.13588", "categories": ["cs.AI", "cs.CE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13588", "abs": "https://arxiv.org/abs/2509.13588", "authors": ["Xuan Liu", "Haoyang Shang", "Haojian Jin"], "title": "Programmable Cognitive Bias in Social Agents", "comment": null, "summary": "This paper introduces CoBRA, a novel toolkit for systematically specifying\nagent behavior in LLM-based social simulation. We found that conventional\napproaches that specify agent behaviors through implicit natural language\ndescriptions cannot yield consistent behaviors across models, and the produced\nagent behaviors do not capture the nuances of the descriptions. In contrast,\nCoBRA presents a new approach to program agents' cognitive biases explicitly,\nby grounding agents' expected behaviors using classic social science\nexperiments. CoBRA has two components: (1) Cognitive Bias Index that measures\nthe cognitive bias of a social agent, by quantifying the agent's reactions in a\nset of validated classical social science experiments; (2) Behavioral\nRegulation Engine that aligns the agent's behavior to demonstrate controlled\ncognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and\ntechnical benchmarks. Our results suggest that CoBRA can precisely program the\ncognitive bias demonstrated in a social agent in a model-agnostic manner.", "AI": {"tldr": "CoBRA\u662f\u4e00\u4e2a\u65b0\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u663e\u5f0f\u7f16\u7a0b\u8ba4\u77e5\u504f\u89c1\u6765\u89e3\u51b3LLM-based\u793e\u4f1a\u6a21\u62df\u4e2d\u4ee3\u7406\u884c\u4e3a\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5305\u62ec\u6d4b\u91cf\u548c\u8c03\u8282\u7ec4\u4ef6\uff0c\u8bc4\u4f30\u663e\u793a\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u9690\u5f0f\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6307\u5b9a\u4ee3\u7406\u884c\u4e3a\uff0c\u65e0\u6cd5\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u4ea7\u751f\u4e00\u81f4\u884c\u4e3a\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u63cf\u8ff0\u4e2d\u7684\u7ec6\u5fae\u5dee\u522b\u3002", "method": "CoBRA\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1) \u8ba4\u77e5\u504f\u89c1\u6307\u6570\uff0c\u901a\u8fc7\u91cf\u5316\u4ee3\u7406\u5728\u4e00\u7ec4\u7ecf\u5178\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u4e2d\u7684\u53cd\u5e94\u6765\u6d4b\u91cf\u5176\u8ba4\u77e5\u504f\u89c1\uff1b2) \u884c\u4e3a\u8c03\u8282\u5f15\u64ce\uff0c\u7528\u4e8e\u8c03\u6574\u4ee3\u7406\u884c\u4e3a\u4ee5\u5c55\u793a\u53d7\u63a7\u7684\u8ba4\u77e5\u504f\u89c1\u3002", "result": "CoBRA\u901a\u8fc7\u6f14\u793a\u548c\u6280\u672f\u57fa\u51c6\u8bc4\u4f30\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u7cbe\u786e\u7f16\u7a0b\u793e\u4ea4\u4ee3\u7406\u4e2d\u7684\u8ba4\u77e5\u504f\u89c1\u3002", "conclusion": "CoBRA\u80fd\u591f\u4ee5\u6a21\u578b\u65e0\u5173\u7684\u65b9\u5f0f\u7cbe\u786e\u7f16\u7a0b\u793e\u4ea4\u4ee3\u7406\u4e2d\u7684\u8ba4\u77e5\u504f\u89c1\uff0c\u5c55\u793a\u4e86\u5176\u5728LLM-based\u793e\u4f1a\u6a21\u62df\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.13507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13507", "abs": "https://arxiv.org/abs/2509.13507", "authors": ["Artem Savkin", "Thomas Lapotre", "Kevin Strauss", "Uzair Akbar", "Federico Tombari"], "title": "Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving", "comment": null, "summary": "In the autonomous driving area synthetic data is crucial for cover specific\ntraffic scenarios which autonomous vehicle must handle. This data commonly\nintroduces domain gap between synthetic and real domains. In this paper we\ndeploy data augmentation to generate custom traffic scenarios with VRUs in\norder to improve pedestrian recognition. We provide a pipeline for augmentation\nof the Cityscapes dataset with virtual pedestrians. In order to improve\naugmentation realism of the pipeline we reveal a novel generative network\narchitecture for adversarial learning of the data-set lighting conditions. We\nalso evaluate our approach on the tasks of semantic and instance segmentation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548cGAN\u67b6\u6784\u751f\u6210\u66f4\u771f\u5b9e\u7684\u865a\u62df\u884c\u4eba\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u884c\u4eba\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u4f9d\u8d56\u5408\u6210\u6570\u636e\u8986\u76d6\u7279\u5b9a\u4ea4\u901a\u573a\u666f\uff0c\u4f46\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u5f71\u54cd\u4e86\u884c\u4eba\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u7ed3\u5408\u65b0\u578b\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u67b6\u6784\uff0c\u7528\u4e8e\u5b66\u4e60\u5e76\u5339\u914dCityscapes\u6570\u636e\u96c6\u7684\u7167\u660e\u6761\u4ef6\uff0c\u4ee5\u751f\u6210\u66f4\u771f\u5b9e\u7684\u865a\u62df\u884c\u4eba\u6570\u636e\u3002", "result": "\u5728\u8bed\u4e49\u5206\u5272\u548c\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u884c\u4eba\u8bc6\u522b\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u521b\u65b0\u7684\u751f\u6210\u7f51\u7edc\u67b6\u6784\uff0c\u8be5\u7814\u7a76\u6210\u529f\u7f29\u5c0f\u4e86\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u548cVRU\u7684\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2509.13666", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13666", "abs": "https://arxiv.org/abs/2509.13666", "authors": ["Zhenqi Wu", "Abhinav Modi", "Angelos Mavrogiannis", "Kaustubh Joshi", "Nikhil Chopra", "Yiannis Aloimonos", "Nare Karapetyan", "Ioannis Rekleitis", "Xiaomin Lin"], "title": "DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring", "comment": "submitted to ICRA 2026", "summary": "The ocean is warming and acidifying, increasing the risk of mass mortality\nevents for temperature-sensitive shellfish such as oysters. This motivates the\ndevelopment of long-term monitoring systems. However, human labor is costly and\nlong-duration underwater work is highly hazardous, thus favoring robotic\nsolutions as a safer and more efficient option. To enable underwater robots to\nmake real-time, environment-aware decisions without human intervention, we must\nequip them with an intelligent \"brain.\" This highlights the need for\npersistent,wide-area, and low-cost benthic monitoring. To this end, we present\nDREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term\nunderwater exploration and habitat monitoring. The results show that our\nframework is highly efficient in finding and exploring target objects (e.g.,\noysters, shipwrecks) without prior location information. In the\noyster-monitoring task, our framework takes 31.5% less time than the previous\nbaseline with the same amount of oysters. Compared to the vanilla VLM, it uses\n23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our\nframework successfully explores and maps the wreck without collisions,\nrequiring 27.5% fewer steps than the vanilla model and achieving 100% coverage,\nwhile the vanilla model achieves 60.23% average coverage in our shipwreck\nenvironments.", "AI": {"tldr": "DREAM\u6846\u67b6\u901a\u8fc7VLM\u5f15\u5bfc\u81ea\u4e3b\u6c34\u4e0b\u76d1\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u4e0e\u8986\u76d6\u8303\u56f4\uff0c\u9002\u7528\u4e8e\u7261\u86ce\u548c\u6c89\u8239\u7b49\u573a\u666f\u3002", "motivation": "\u6d77\u6d0b\u53d8\u6696\u548c\u9178\u5316\u52a0\u5267\u4e86\u6e29\u5ea6\u654f\u611f\u8d1d\u7c7b\uff08\u5982\u7261\u86ce\uff09\u7684\u6b7b\u4ea1\u98ce\u9669\uff0c\u9700\u8981\u5f00\u53d1\u957f\u671f\u76d1\u6d4b\u7cfb\u7edf\uff0c\u4f46\u4eba\u5de5\u6210\u672c\u9ad8\u4e14\u5371\u9669\uff0c\u56e0\u6b64\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u66f4\u4f18\u3002", "method": "\u63d0\u51fa\u4e86DREAM\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5b9e\u73b0\u81ea\u4e3b\u6c34\u4e0b\u63a2\u7d22\u548c\u76d1\u6d4b\u3002", "result": "\u5728\u7261\u86ce\u76d1\u6d4b\u4efb\u52a1\u4e2d\uff0cDREAM\u6bd4\u57fa\u7ebf\u8282\u770131.5%\u65f6\u95f4\uff0c\u8986\u76d6\u66f4\u591a\u7261\u86ce\uff1b\u5728\u6c89\u8239\u573a\u666f\u4e2d\uff0c\u5b9e\u73b0100%\u8986\u76d6\u4e14\u78b0\u649e\u51cf\u5c11\u3002", "conclusion": "DREAM\u6846\u67b6\u901a\u8fc7VLM\u5f15\u5bfc\u7684\u81ea\u4e3b\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u957f\u671f\u6816\u606f\u5730\u76d1\u6d4b\u4e2d\u7684\u6548\u7387\u548c\u8986\u76d6\u8303\u56f4\u3002"}}
{"id": "2509.13615", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13615", "abs": "https://arxiv.org/abs/2509.13615", "authors": ["Zongru Wu", "Rui Mao", "Zhiyuan Tian", "Pengzhou Cheng", "Tianjie Ju", "Zheng Wu", "Lingzhong Dong", "Haiyue Sheng", "Zhuosheng Zhang", "Gongshen Liu"], "title": "See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles", "comment": null, "summary": "The advent of multimodal agents facilitates effective interaction within\ngraphical user interface (GUI), especially in ubiquitous GUI control. However,\ntheir inability to reliably execute toggle control instructions remains a key\nbottleneck. To investigate this, we construct a state control benchmark with\nbinary toggle instructions from public datasets. Evaluations of existing agents\ndemonstrate their unreliability, particularly when the current toggle state\nalready matches the desired state. To address the challenge, we propose\nState-aware Reasoning (StaR), a training method that teaches agents to perceive\nthe current toggle state, analyze the desired state from the instruction, and\nact accordingly. Experiments on three multimodal agents demonstrate that StaR\ncan improve toggle instruction execution accuracy by over 30\\%. Further\nevaluations on three public benchmarks show that StaR also enhances general\ntask performance. Finally, evaluations on a dynamic environment highlight the\npotential of StaR for real-world applications. Code, benchmark, and\nStaR-enhanced agents are available at https://github.com/ZrW00/StaR.", "AI": {"tldr": "StaR\u65b9\u6cd5\u901a\u8fc7\u611f\u77e5\u5f53\u524d\u72b6\u6001\u548c\u5206\u6790\u6307\u4ee4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u4ee3\u7406\u5728GUI\u5207\u6362\u63a7\u5236\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u901a\u7528\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u591a\u6a21\u6001\u4ee3\u7406\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762(GUI)\u63a7\u5236\u4e2d\u6267\u884c\u5207\u6362\u6307\u4ee4\u7684\u4e0d\u53ef\u9760\u6027\u662f\u4e00\u4e2a\u5173\u952e\u74f6\u9888\uff0c\u5c24\u5176\u662f\u5728\u5f53\u524d\u72b6\u6001\u4e0e\u671f\u671b\u72b6\u6001\u4e00\u81f4\u65f6\u3002", "method": "\u63d0\u51faState-aware Reasoning (StaR)\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6559\u5bfc\u4ee3\u7406\u611f\u77e5\u5f53\u524d\u5207\u6362\u72b6\u6001\u3001\u5206\u6790\u6307\u4ee4\u4e2d\u7684\u671f\u671b\u72b6\u6001\uff0c\u5e76\u636e\u6b64\u884c\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cStaR\u80fd\u5c06\u5207\u6362\u6307\u4ee4\u6267\u884c\u51c6\u786e\u6027\u63d0\u9ad830%\u4ee5\u4e0a\uff0c\u5e76\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u901a\u7528\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "StaR\u65b9\u6cd5\u901a\u8fc7\u6559\u5bfc\u4ee3\u7406\u611f\u77e5\u5f53\u524d\u5207\u6362\u72b6\u6001\u5e76\u5206\u6790\u6307\u4ee4\u4e2d\u7684\u671f\u671b\u72b6\u6001\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5207\u6362\u6307\u4ee4\u6267\u884c\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u901a\u7528\u4efb\u52a1\u6027\u80fd\u4e0a\u4e5f\u6709\u6240\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13508", "categories": ["cs.CV", "I.4.3; I.4.6"], "pdf": "https://arxiv.org/pdf/2509.13508", "abs": "https://arxiv.org/abs/2509.13508", "authors": ["Maksim Penkin", "Andrey Krylov"], "title": "FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation", "comment": "9 pages, 5 figures, submitted to the Fortieth AAAI Conference on\n  Artificial Intelligence (AAAI-26)", "summary": "Medical image enhancement and segmentation are critical yet challenging tasks\nin modern clinical practice, constrained by artifacts and complex anatomical\nvariations. Traditional deep learning approaches often rely on complex\narchitectures with limited interpretability. While Kolmogorov-Arnold networks\noffer interpretable solutions, their reliance on flattened feature\nrepresentations fundamentally disrupts the intrinsic spatial structure of\nimaging data. To address this issue we propose a Functional Kolmogorov-Arnold\nNetwork (FunKAN) -- a novel interpretable neural framework, designed\nspecifically for image processing, that formally generalizes the\nKolmogorov-Arnold representation theorem onto functional spaces and learns\ninner functions using Fourier decomposition over the basis Hermite functions.\nWe explore FunKAN on several medical image processing tasks, including Gibbs\nringing suppression in magnetic resonance images, benchmarking on IXI dataset.\nWe also propose U-FunKAN as state-of-the-art binary medical segmentation model\nwith benchmarks on three medical datasets: BUSI (ultrasound images), GlaS\n(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting\nbreast cancer, glands and polyps, respectively. Experiments on those diverse\ndatasets demonstrate that our approach outperforms other KAN-based backbones in\nboth medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work\nbridges the gap between theoretical function approximation and medical image\nanalysis, offering a robust, interpretable solution for clinical applications.", "AI": {"tldr": "\u63d0\u51faFunKAN\u548cU-FunKAN\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u548c\u5206\u5272\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u5904\u7406\u4e2d\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u56fe\u50cf\u7684\u7a7a\u95f4\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u529f\u80fd\u7a7a\u95f4\u548c\u5085\u91cc\u53f6\u5206\u89e3\u7684FunKAN\u6846\u67b6\uff0c\u4ee5\u53caU-FunKAN\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\uff0cFunKAN\u548cU-FunKAN\u5728\u56fe\u50cf\u589e\u5f3a\uff08PSNR, TV\uff09\u548c\u5206\u5272\uff08IoU, F1\uff09\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6KAN-based\u65b9\u6cd5\u3002", "conclusion": "FunKAN\u548cU-FunKAN\u5728\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13691", "abs": "https://arxiv.org/abs/2509.13691", "authors": ["Songhao Huang", "Yuwei Wu", "Guangyao Shi", "Gaurav S. Sukhatme", "Vijay Kumar"], "title": "SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics", "comment": null, "summary": "We investigate the problem of automatic domain generation for the Planning\nDomain Definition Language (PDDL) using Large Language Models (LLMs), with a\nparticular focus on unmanned aerial vehicle (UAV) tasks. Although PDDL is a\nwidely adopted standard in robotic planning, manually designing domains for\ndiverse applications such as surveillance, delivery, and inspection is\nlabor-intensive and error-prone, which hinders adoption and real-world\ndeployment. To address these challenges, we propose SPAR, a framework that\nleverages the generative capabilities of LLMs to automatically produce valid,\ndiverse, and semantically accurate PDDL domains from natural language input. To\nthis end, we first introduce a systematically formulated and validated UAV\nplanning dataset, consisting of ground-truth PDDL domains and associated\nproblems, each paired with detailed domain and action descriptions. Building on\nthis dataset, we design a prompting framework that generates high-quality PDDL\ndomains from language input. The generated domains are evaluated through syntax\nvalidation, executability, feasibility, and interpretability. Overall, this\nwork demonstrates that LLMs can substantially accelerate the creation of\ncomplex planning domains, providing a reproducible dataset and evaluation\npipeline that enables application experts without prior experience to leverage\nit for practical tasks and advance future research in aerial robotics and\nautomated planning.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210PDDL\u9886\u57df\uff0c\u9488\u5bf9\u65e0\u4eba\u673a\u4efb\u52a1\uff0c\u63d0\u51faSPAR\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u624b\u52a8\u8bbe\u8ba1\u7684\u5de5\u4f5c\u91cf\uff0c\u5e76\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u4f30\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u867d\u7136PDDL\u662f\u673a\u5668\u4eba\u89c4\u5212\u4e2d\u5e7f\u6cdb\u91c7\u7528\u7684\u6807\u51c6\uff0c\u4f46\u4e3a\u76d1\u89c6\u3001\u4ea4\u4ed8\u548c\u68c0\u67e5\u7b49\u591a\u6837\u5316\u5e94\u7528\u624b\u52a8\u8bbe\u8ba1\u9886\u57df\u65e2\u8017\u65f6\u53c8\u5bb9\u6613\u51fa\u9519\uff0c\u8fd9\u963b\u788d\u4e86\u5176\u91c7\u7528\u548c\u5b9e\u9645\u90e8\u7f72\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5229\u7528LLMs\u81ea\u52a8\u751f\u6210PDDL\u9886\u57df\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86SPAR\u6846\u67b6\uff0c\u5229\u7528LLMs\u7684\u751f\u6210\u80fd\u529b\uff0c\u4ece\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u81ea\u52a8\u751f\u6210\u6709\u6548\u3001\u591a\u6837\u4e14\u8bed\u4e49\u51c6\u786e\u7684PDDL\u9886\u57df\u3002\u9996\u5148\u5f15\u5165\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u5236\u5b9a\u548c\u9a8c\u8bc1\u7684UAV\u89c4\u5212\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9ePDDL\u9886\u57df\u548c\u76f8\u5173\u95ee\u9898\uff0c\u6bcf\u4e2a\u90fd\u914d\u6709\u8be6\u7ec6\u7684\u9886\u57df\u548c\u52a8\u4f5c\u63cf\u8ff0\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u63d0\u793a\u6846\u67b6\uff0c\u4ece\u8bed\u8a00\u8f93\u5165\u751f\u6210\u9ad8\u8d28\u91cfPDDL\u9886\u57df\u3002\u751f\u6210\u7684\u9886\u57df\u901a\u8fc7\u8bed\u6cd5\u9a8c\u8bc1\u3001\u53ef\u6267\u884c\u6027\u3001\u53ef\u884c\u6027\u548c\u53ef\u89e3\u91ca\u6027\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u751f\u6210\u7684PDDL\u9886\u57df\u901a\u8fc7\u4e25\u683c\u7684\u8bed\u6cd5\u9a8c\u8bc1\u3001\u53ef\u6267\u884c\u6027\u3001\u53ef\u884c\u6027\u548c\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86LLMs\u5728\u81ea\u52a8\u751f\u6210\u590d\u6742\u89c4\u5212\u9886\u57df\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u663e\u8457\u52a0\u901f\u590d\u6742\u89c4\u5212\u9886\u57df\u7684\u521b\u5efa\uff0c\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6d41\u7a0b\uff0c\u4f7f\u65e0\u7ecf\u9a8c\u7684\u4e13\u5bb6\u4e5f\u80fd\u5c06\u5176\u7528\u4e8e\u5b9e\u9645\u4efb\u52a1\uff0c\u5e76\u63a8\u52a8\u7a7a\u4e2d\u673a\u5668\u4eba\u548c\u81ea\u52a8\u89c4\u5212\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.13515", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13515", "abs": "https://arxiv.org/abs/2509.13515", "authors": ["Jiangbei Yue", "Shuonan Yang", "Tailin Chen", "Jianbo Jiao", "Zeyu Fu"], "title": "Multimodal Hate Detection Using Dual-Stream Graph Neural Networks", "comment": null, "summary": "Hateful videos present serious risks to online safety and real-world\nwell-being, necessitating effective detection methods. Although multimodal\nclassification approaches integrating information from several modalities\noutperform unimodal ones, they typically neglect that even minimal hateful\ncontent defines a video's category. Specifically, they generally treat all\ncontent uniformly, instead of emphasizing the hateful components. Additionally,\nexisting multimodal methods cannot systematically capture structured\ninformation in videos, limiting the effectiveness of multimodal fusion. To\naddress these limitations, we propose a novel multimodal dual-stream graph\nneural network model. It constructs an instance graph by separating the given\nvideo into several instances to extract instance-level features. Then, a\ncomplementary weight graph assigns importance weights to these features,\nhighlighting hateful instances. Importance weights and instance features are\ncombined to generate video labels. Our model employs a graph-based framework to\nsystematically model structured relationships within and across modalities.\nExtensive experiments on public datasets show that our model is\nstate-of-the-art in hateful video classification and has strong explainability.\nCode is available:\nhttps://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u53cc\u6d41\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u7279\u5f81\u63d0\u53d6\u548c\u91cd\u8981\u6027\u6743\u91cd\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ec7\u6068\u89c6\u9891\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5ffd\u89c6\u4ec7\u6068\u5185\u5bb9\u7684\u6700\u5c0f\u5316\u5b9a\u4e49\uff0c\u4e14\u65e0\u6cd5\u7cfb\u7edf\u6355\u83b7\u89c6\u9891\u4e2d\u7684\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u878d\u5408\u7684\u6548\u679c\u3002", "method": "\u6a21\u578b\u901a\u8fc7\u6784\u5efa\u5b9e\u4f8b\u56fe\u548c\u8865\u5145\u6743\u91cd\u56fe\uff0c\u63d0\u53d6\u5b9e\u4f8b\u7ea7\u7279\u5f81\u5e76\u5206\u914d\u91cd\u8981\u6027\u6743\u91cd\uff0c\u7ed3\u5408\u56fe\u6846\u67b6\u7cfb\u7edf\u5efa\u6a21\u591a\u6a21\u6001\u95f4\u7684\u7ed3\u6784\u5316\u5173\u7cfb\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4ec7\u6068\u89c6\u9891\u5206\u7c7b\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5177\u6709\u8f83\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u53cc\u6d41\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u79bb\u89c6\u9891\u5b9e\u4f8b\u5e76\u5f3a\u8c03\u4ec7\u6068\u5185\u5bb9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ec7\u6068\u89c6\u9891\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.13692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13692", "abs": "https://arxiv.org/abs/2509.13692", "authors": ["Yadan Zeng", "Jiadong Zhou", "Xiaohan Li", "I-Ming Chen"], "title": "HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion", "comment": "9 pages, 6 figures", "summary": "Point cloud completion is essential for robotic perception, object\nreconstruction and supporting downstream tasks like grasp planning, obstacle\navoidance, and manipulation. However, incomplete geometry caused by\nself-occlusion and sensor limitations can significantly degrade downstream\nreasoning and interaction. To address these challenges, we propose HGACNet, a\nnovel framework that reconstructs complete point clouds of individual objects\nby hierarchically encoding 3D geometric features and fusing them with\nimage-guided priors from a single-view RGB image. At the core of our approach,\nthe Hierarchical Graph Attention (HGA) encoder adaptively selects critical\nlocal points through graph attention-based downsampling and progressively\nrefines hierarchical geometric features to better capture structural continuity\nand spatial relationships. To strengthen cross-modal interaction, we further\ndesign a Multi-Scale Cross-Modal Fusion (MSCF) module that performs\nattention-based feature alignment between hierarchical geometric features and\nstructured visual representations, enabling fine-grained semantic guidance for\ncompletion. In addition, we proposed the contrastive loss (C-Loss) to\nexplicitly align the feature distributions across modalities, improving\ncompletion fidelity under modality discrepancy. Finally, extensive experiments\nconducted on both the ShapeNet-ViPC benchmark and the YCB-Complete dataset\nconfirm the effectiveness of HGACNet, demonstrating state-of-the-art\nperformance as well as strong applicability in real-world robotic manipulation\ntasks.", "AI": {"tldr": "HGACNet\u901a\u8fc7\u5206\u5c42\u56fe\u6ce8\u610f\u529b\u548c\u8de8\u6a21\u6001\u878d\u5408\u6a21\u5757\uff0c\u7ed3\u5408\u5bf9\u6bd4\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u8865\u5168\u6548\u679c\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u70b9\u4e91\u8865\u5168\u5bf9\u673a\u5668\u4eba\u611f\u77e5\u548c\u4e0b\u6e38\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u81ea\u906e\u6321\u548c\u4f20\u611f\u5668\u9650\u5236\u5bfc\u81f4\u7684\u4e0d\u5b8c\u6574\u51e0\u4f55\u4f1a\u663e\u8457\u5f71\u54cd\u63a8\u7406\u548c\u4ea4\u4e92\u6548\u679c\u3002", "method": "HGACNet\u91c7\u7528\u5206\u5c42\u56fe\u6ce8\u610f\u529b\uff08HGA\uff09\u7f16\u7801\u5668\u81ea\u9002\u5e94\u9009\u62e9\u5173\u952e\u5c40\u90e8\u70b9\uff0c\u5e76\u901a\u8fc7\u591a\u5c3a\u5ea6\u8de8\u6a21\u6001\u878d\u5408\uff08MSCF\uff09\u6a21\u5757\u5b9e\u73b0\u51e0\u4f55\u7279\u5f81\u4e0e\u89c6\u89c9\u8868\u5f81\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff0c\u540c\u65f6\u5f15\u5165\u5bf9\u6bd4\u635f\u5931\uff08C-Loss\uff09\u4f18\u5316\u8de8\u6a21\u6001\u7279\u5f81\u5206\u5e03\u3002", "result": "\u5728ShapeNet-ViPC\u548cYCB-Complete\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHGACNet\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "HGACNet\u5728\u70b9\u4e91\u8865\u5168\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u5206\u5c42\u56fe\u6ce8\u610f\u529b\u7f16\u7801\u5668\u548c\u591a\u5c3a\u5ea6\u8de8\u6a21\u6001\u878d\u5408\u6a21\u5757\uff0c\u7ed3\u5408\u5bf9\u6bd4\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8865\u5168\u6548\u679c\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2509.13761", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13761", "abs": "https://arxiv.org/abs/2509.13761", "authors": ["Qikai Chang", "Zhenrong Zhang", "Pengfei Hu", "Jiefeng Ma", "Yicheng Pan", "Jianshu Zhang", "Jun Du", "Quan Liu", "Jianqing Gao"], "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning", "comment": "22 pages, 13 figures", "summary": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both trajectory-level problem solving and step-level code generation. This\nis motivated by our key insight that the success of an intermediate tool call\nis a strong predictor of the final answer's correctness. Finally, THOR\nincorporates a self-correction mechanism that leverages immediate tool feedback\nto dynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR.", "AI": {"tldr": "THOR\u901a\u8fc7\u5de5\u5177\u96c6\u6210\u548c\u5206\u5c42\u4f18\u5316\uff0c\u63d0\u5347\u4e86LLM\u5728\u9ad8\u7cbe\u5ea6\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6784\u5efa\u5de5\u5177\u96c6\u6210\u63a8\u7406\u6570\u636e\u3001\u7ec6\u7c92\u5ea6\u4f18\u5316\u548c\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cTHOR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86THOR\u6846\u67b6\uff0c\u5305\u62ecTIRGen\u6570\u636e\u751f\u6210\u7ba1\u9053\u3001\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5206\u5c42\u4f18\u5316\u7b56\u7565\u4ee5\u53ca\u5229\u7528\u5de5\u5177\u53cd\u9988\u7684\u52a8\u6001\u81ea\u6821\u6b63\u673a\u5236\u3002", "result": "THOR\u5728\u591a\u6837\u5316\u7684\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u6570\u5b66\u548c\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "THOR\u901a\u8fc7\u6574\u5408\u5916\u90e8\u5de5\u5177\u548c\u5206\u5c42\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u548c\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u5b66\u548c\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13525", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13525", "abs": "https://arxiv.org/abs/2509.13525", "authors": ["Romain Hardy", "Tyler Berzin", "Pranav Rajpurkar"], "title": "ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors", "comment": "12 pages, 8 figures", "summary": "Three-dimensional (3D) scene understanding in colonoscopy presents\nsignificant challenges that necessitate automated methods for accurate depth\nestimation. However, existing depth estimation models for endoscopy struggle\nwith temporal consistency across video sequences, limiting their applicability\nfor 3D reconstruction. We present ColonCrafter, a diffusion-based depth\nestimation model that generates temporally consistent depth maps from monocular\ncolonoscopy videos. Our approach learns robust geometric priors from synthetic\ncolonoscopy sequences to generate temporally consistent depth maps. We also\nintroduce a style transfer technique that preserves geometric structure while\nadapting real clinical videos to match our synthetic training domain.\nColonCrafter achieves state-of-the-art zero-shot performance on the C3VD\ndataset, outperforming both general-purpose and endoscopy-specific approaches.\nAlthough full trajectory 3D reconstruction remains a challenge, we demonstrate\nclinically relevant applications of ColonCrafter, including 3D point cloud\ngeneration and surface coverage assessment.", "AI": {"tldr": "ColonCrafter\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u5b66\u4e60\u51e0\u4f55\u5148\u9a8c\uff0c\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u6df1\u5ea6\u56fe\uff0c\u5e76\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7ed3\u80a0\u955c\u4e2d\u7684\u4e09\u7ef4\u573a\u666f\u7406\u89e3\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u5728\u89c6\u9891\u5e8f\u5217\u4e2d\u7f3a\u4e4f\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e86\u5176\u57283D\u91cd\u5efa\u4e2d\u7684\u5e94\u7528\u3002", "method": "ColonCrafter\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7\u5b66\u4e60\u5408\u6210\u7ed3\u80a0\u955c\u5e8f\u5217\u4e2d\u7684\u51e0\u4f55\u5148\u9a8c\uff0c\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u6df1\u5ea6\u56fe\uff0c\u5e76\u91c7\u7528\u98ce\u683c\u8fc1\u79fb\u6280\u672f\u5c06\u771f\u5b9e\u4e34\u5e8a\u89c6\u9891\u9002\u914d\u5230\u5408\u6210\u8bad\u7ec3\u57df\u3002", "result": "ColonCrafter\u5728C3VD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4f18\u4e8e\u901a\u7528\u548c\u7ed3\u80a0\u955c\u4e13\u7528\u65b9\u6cd5\u3002", "conclusion": "\u5c3d\u7ba1\u5b8c\u6574\u76843D\u8f68\u8ff9\u91cd\u5efa\u4ecd\u5177\u6311\u6218\u6027\uff0c\u4f46ColonCrafter\u57283D\u70b9\u4e91\u751f\u6210\u548c\u8868\u9762\u8986\u76d6\u8bc4\u4f30\u7b49\u4e34\u5e8a\u5e94\u7528\u65b9\u9762\u5c55\u73b0\u4e86\u6f5c\u529b\u3002"}}
{"id": "2509.13720", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13720", "abs": "https://arxiv.org/abs/2509.13720", "authors": ["Tianle Zeng", "Jianwei Peng", "Hanjing Ye", "Guangcheng Chen", "Senzi Luo", "Hong Zhang"], "title": "EZREAL: Enhancing Zero-Shot Outdoor Robot Navigation toward Distant Targets under Varying Visibility", "comment": "Page:https://tianlezeng.github.io/EzReal/", "summary": "Zero-shot object navigation (ZSON) in large-scale outdoor environments faces\nmany challenges; we specifically address a coupled one: long-range targets that\nreduce to tiny projections and intermittent visibility due to partial or\ncomplete occlusion. We present a unified, lightweight closed-loop system built\non an aligned multi-scale image tile hierarchy. Through hierarchical\ntarget-saliency fusion, it summarizes localized semantic contrast into a stable\ncoarse-layer regional saliency that provides the target direction and indicates\ntarget visibility. This regional saliency supports visibility-aware heading\nmaintenance through keyframe memory, saliency-weighted fusion of historical\nheadings, and active search during temporary invisibility. The system avoids\nwhole-image rescaling, enables deterministic bottom-up aggregation, supports\nzero-shot navigation, and runs efficiently on a mobile robot. Across simulation\nand real-world outdoor trials, the system detects semantic targets beyond 150m,\nmaintains a correct heading through visibility changes with 82.6% probability,\nand improves overall task success by 17.5% compared with the SOTA methods,\ndemonstrating robust ZSON toward distant and intermittently observable targets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u95ed\u73af\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5c42\u7ea7\u878d\u5408\u548c\u53ef\u89c1\u6027\u611f\u77e5\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u8fdc\u8ddd\u79bb\u95f4\u6b47\u6027\u53ef\u89c1\u76ee\u6807\u7684\u96f6\u6837\u672c\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u96f6\u6837\u672c\u7269\u4f53\u5bfc\u822a\uff08ZSON\uff09\u5728\u5927\u578b\u6237\u5916\u73af\u5883\u4e2d\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u8fdc\u8ddd\u79bb\u76ee\u6807\u56e0\u6295\u5f71\u8fc7\u5c0f\u548c\u95f4\u6b47\u6027\u53ef\u89c1\u6027\uff08\u90e8\u5206\u6216\u5b8c\u5168\u906e\u6321\uff09\u5bfc\u81f4\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u9f50\u591a\u5c3a\u5ea6\u56fe\u50cf\u74e6\u7247\u5c42\u7ea7\u7684\u7edf\u4e00\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c42\u7ea7\u76ee\u6807\u663e\u8457\u6027\u878d\u5408\u548c\u5173\u952e\u5e27\u8bb0\u5fc6\u7b49\u6280\u672f\uff0c\u5b9e\u73b0\u76ee\u6807\u65b9\u5411\u548c\u53ef\u89c1\u6027\u7684\u7a33\u5b9a\u68c0\u6d4b\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6237\u5916\u8bd5\u9a8c\u4e2d\uff0c\u7cfb\u7edf\u80fd\u68c0\u6d4b150\u7c73\u5916\u7684\u8bed\u4e49\u76ee\u6807\uff0c\u822a\u5411\u4fdd\u6301\u6b63\u786e\u6982\u7387\u8fbe82.6%\uff0c\u4efb\u52a1\u6210\u529f\u7387\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad817.5%\u3002", "conclusion": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u95ed\u73af\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c42\u7ea7\u76ee\u6807\u663e\u8457\u6027\u878d\u5408\u548c\u53ef\u89c1\u6027\u611f\u77e5\u7684\u822a\u5411\u7ef4\u62a4\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6237\u5916\u8bd5\u9a8c\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u96f6\u6837\u672c\u7269\u4f53\u5bfc\u822a\uff08ZSON\uff09\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2509.13773", "categories": ["cs.AI", "cs.IR", "I.2.7; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.13773", "abs": "https://arxiv.org/abs/2509.13773", "authors": ["Zhipeng Bian", "Jieming Zhu", "Xuyang Xie", "Quanyu Dai", "Zhou Zhao", "Zhenhua Dong"], "title": "MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation", "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (Volume 6: Industry Track), ACL\n  2025. Official version: https://doi.org/10.18653/v1/2025.acl-industry.103", "summary": "The rapid advancement of generative AI technologies is driving the\nintegration of diverse AI-powered services into smartphones, transforming how\nusers interact with their devices. To simplify access to predefined AI\nservices, this paper introduces MIRA, a pioneering framework for task\ninstruction recommendation that enables intuitive one-touch AI tasking on\nsmartphones. With MIRA, users can long-press on images or text objects to\nreceive contextually relevant instruction recommendations for executing AI\ntasks. Our work introduces three key innovations: 1) A multimodal large\nlanguage model (MLLM)-based recommendation pipeline with structured reasoning\nto extract key entities, infer user intent, and generate precise instructions;\n2) A template-augmented reasoning mechanism that integrates high-level\nreasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based\nconstrained decoding strategy that restricts outputs to predefined instruction\ncandidates, ensuring coherent and intent-aligned suggestions. Through\nevaluation using a real-world annotated datasets and a user study, MIRA has\ndemonstrated substantial improvements in the accuracy of instruction\nrecommendation. The encouraging results highlight MIRA's potential to\nrevolutionize the way users engage with AI services on their smartphones,\noffering a more seamless and efficient experience.", "AI": {"tldr": "MIRA\u662f\u4e00\u4e2a\u667a\u80fd\u624b\u673a\u4e0a\u7684\u4efb\u52a1\u6307\u4ee4\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7ed3\u6784\u5316\u63a8\u7406\u63d0\u5347AI\u4efb\u52a1\u6307\u4ee4\u63a8\u8350\u7684\u51c6\u786e\u6027\uff0c\u4f18\u5316\u7528\u6237\u4e0eAI\u670d\u52a1\u7684\u4e92\u52a8\u4f53\u9a8c\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u63a8\u52a8\u4e86\u591a\u6837\u5316AI\u670d\u52a1\u5728\u667a\u80fd\u624b\u673a\u4e2d\u7684\u96c6\u6210\uff0c\u4f46\u7528\u6237\u5982\u4f55\u76f4\u89c2\u3001\u9ad8\u6548\u5730\u8bbf\u95ee\u8fd9\u4e9b\u670d\u52a1\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "1) \u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u63a8\u8350\u6d41\u7a0b\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u63a8\u7406\u63d0\u53d6\u5173\u952e\u5b9e\u4f53\u3001\u63a8\u65ad\u7528\u6237\u610f\u56fe\u5e76\u751f\u6210\u7cbe\u786e\u6307\u4ee4\uff1b2) \u6a21\u677f\u589e\u5f3a\u63a8\u7406\u673a\u5236\uff0c\u6574\u5408\u9ad8\u7ea7\u63a8\u7406\u6a21\u677f\u63d0\u5347\u4efb\u52a1\u63a8\u65ad\u51c6\u786e\u6027\uff1b3) \u524d\u7f00\u6811\u7ea6\u675f\u89e3\u7801\u7b56\u7565\uff0c\u9650\u5236\u8f93\u51fa\u4e3a\u9884\u5b9a\u4e49\u6307\u4ee4\u5019\u9009\uff0c\u786e\u4fdd\u5efa\u8bae\u7684\u4e00\u81f4\u6027\u548c\u610f\u56fe\u5bf9\u9f50\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u6807\u6ce8\u6570\u636e\u96c6\u548c\u7528\u6237\u7814\u7a76\u7684\u8bc4\u4f30\uff0cMIRA\u5728\u6307\u4ee4\u63a8\u8350\u51c6\u786e\u6027\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MIRA\u6846\u67b6\u901a\u8fc7\u5176\u521b\u65b0\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u6a21\u677f\u589e\u5f3a\u63a8\u7406\u673a\u5236\u548c\u524d\u7f00\u6811\u7ea6\u675f\u89e3\u7801\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u667a\u80fd\u624b\u673a\u4e0aAI\u4efb\u52a1\u6307\u4ee4\u63a8\u8350\u7684\u51c6\u786e\u6027\uff0c\u6709\u671b\u5f7b\u5e95\u6539\u53d8\u7528\u6237\u4e0eAI\u670d\u52a1\u7684\u4e92\u52a8\u65b9\u5f0f\u3002"}}
{"id": "2509.13536", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13536", "abs": "https://arxiv.org/abs/2509.13536", "authors": ["Yinlong Bai", "Hongxin Zhang", "Sheng Zhong", "Junkai Niu", "Hai Li", "Yijia He", "Yi Zhou"], "title": "MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM", "comment": null, "summary": "Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant\nimpact on rendering and reconstruction techniques. Current research\npredominantly focuses on improving rendering performance and reconstruction\nquality using high-performance desktop GPUs, largely overlooking applications\nfor embedded platforms like micro air vehicles (MAVs). These devices, with\ntheir limited computational resources and memory, often face a trade-off\nbetween system performance and reconstruction quality. In this paper, we\nimprove existing methods in terms of GPU memory usage while enhancing rendering\nquality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we\npropose merging them in voxel space based on geometric similarity. This reduces\nGPU memory usage without impacting system runtime performance. Furthermore,\nrendering quality is improved by initializing 3D Gaussian primitives via\nPatch-Grid (PG) point sampling, enabling more accurate modeling of the entire\nscene. Quantitative and qualitative evaluations on publicly available datasets\ndemonstrate the effectiveness of our improvements.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4f18\u5316\u4e863D\u9ad8\u65af\u55b7\u5c04\u6280\u672f\uff0c\u51cf\u5c11GPU\u5185\u5b58\u4f7f\u7528\u5e76\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u5e73\u53f0\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9ad8\u6027\u80fd\u684c\u9762GPU\u7684\u6e32\u67d3\u548c\u91cd\u5efa\u8d28\u91cf\uff0c\u5ffd\u89c6\u4e86\u5d4c\u5165\u5f0f\u5e73\u53f0\uff08\u5982MAV\uff09\u7684\u8d44\u6e90\u9650\u5236\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u51e0\u4f55\u76f8\u4f3c\u6027\u5728\u4f53\u7d20\u7a7a\u95f4\u4e2d\u5408\u5e76\u5197\u4f59\u76843D\u9ad8\u65af\u57fa\u5143\uff0c\u5e76\u901a\u8fc7Patch-Grid\uff08PG\uff09\u70b9\u91c7\u6837\u521d\u59cb\u53163D\u9ad8\u65af\u57fa\u5143\uff0c\u4ee5\u66f4\u7cbe\u786e\u5730\u5efa\u6a21\u6574\u4e2a\u573a\u666f\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8bc1\u660e\u4e86\u6240\u63d0\u6539\u8fdb\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u4f18\u53163D\u9ad8\u65af\u55b7\u5c04\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86GPU\u5185\u5b58\u4f7f\u7528\u5e76\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u5d4c\u5165\u5f0f\u5e73\u53f0\u5982\u5fae\u578b\u98de\u884c\u5668\uff08MAV\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.13731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13731", "abs": "https://arxiv.org/abs/2509.13731", "authors": ["Jeongwoo Park", "Seabin Lee", "Changmin Park", "Wonjong Lee", "Changjoo Nam"], "title": "Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings", "comment": null, "summary": "The industrial insertion of flexible flat cables (FFCs) into receptacles\npresents a significant challenge owing to the need for submillimeter precision\nwhen handling the deformable cables. In manufacturing processes, FFC insertion\nwith robotic manipulators often requires laborious human-guided trajectory\ngeneration. While Reinforcement Learning (RL) offers a solution to automate\nthis task without modeling complex properties of FFCs, the nondeterminism\ncaused by the deformability of FFCs requires significant efforts and time on\ntraining. Moreover, training directly in a real environment is dangerous as\nindustrial robots move fast and possess no safety measure. We propose an RL\nalgorithm for FFC insertion that leverages a foundation model-based real-to-sim\napproach to reduce the training time and eliminate the risk of physical damages\nto robots and surroundings. Training is done entirely in simulation, allowing\nfor random exploration without the risk of physical damages. Sim-to-real\ntransfer is achieved through semantic segmentation masks which leave only those\nvisual features relevant to the insertion tasks such as the geometric and\nspatial information of the cables and receptacles. To enhance generality, we\nuse a foundation model, Segment Anything Model 2 (SAM2). To eleminate human\nintervention, we employ a Vision-Language Model (VLM) to automate the initial\nprompting of SAM2 to find segmentation masks. In the experiments, our method\nexhibits zero-shot capabilities, which enable direct deployments to real\nenvironments without fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u548c\u6a21\u62df\u8bad\u7ec3\u7684RL\u7b97\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316FFC\u63d2\u5165\u4efb\u52a1\uff0c\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u5e76\u907f\u514d\u7269\u7406\u98ce\u9669\u3002", "motivation": "\u89e3\u51b3\u67d4\u6027\u6241\u5e73\u7535\u7f06\uff08FFC\uff09\u63d2\u5165\u8fc7\u7a0b\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u9700\u6c42\u548c\u975e\u786e\u5b9a\u6027\u53d8\u5f62\u95ee\u9898\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u548c\u7269\u7406\u98ce\u9669\u3002", "method": "\u5229\u7528\u57fa\u7840\u6a21\u578b\uff08SAM2\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff0c\u901a\u8fc7\u6a21\u62df\u73af\u5883\u8bad\u7ec3RL\u7b97\u6cd5\uff0c\u907f\u514d\u76f4\u63a5\u7269\u7406\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u5907\u96f6\u6837\u672c\u80fd\u529b\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u73b0\u5b9e\u73af\u5883\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "conclusion": "\u63d0\u51fa\u7684RL\u7b97\u6cd5\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u548c\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u5e76\u6d88\u9664\u4e86\u7269\u7406\u635f\u574f\u98ce\u9669\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u90e8\u7f72\u3002"}}
{"id": "2509.13880", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13880", "abs": "https://arxiv.org/abs/2509.13880", "authors": ["Mingwei Zhang", "Zhenhao Gu", "Liangda Fang", "Cunjing Ge", "Ziliang Chen", "Zhao-Rong Lai", "Quanlong Guan"], "title": "An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques", "comment": null, "summary": "Linear constraints are one of the most fundamental constraints in fields such\nas computer science, operations research and optimization. Many applications\nreduce to the task of model counting over integer linear constraints (MCILC).\nIn this paper, we design an exact approach to MCILC based on an exhaustive DPLL\narchitecture. To improve the efficiency, we integrate several effective\nsimplification techniques from mixed integer programming into the architecture.\nWe compare our approach to state-of-the-art MCILC counters and propositional\nmodel counters on 2840 random and 4131 application benchmarks. Experimental\nresults show that our approach significantly outperforms all exact methods in\nrandom benchmarks solving 1718 instances while the state-of-the-art approach\nonly computes 1470 instances. In addition, our approach is the only approach to\nsolve all 4131 application instances.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eDPLL\u67b6\u6784\u7684\u7cbe\u786e\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df7\u5408\u6574\u6570\u89c4\u5212\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347MCILC\u4efb\u52a1\u7684\u6548\u7387\uff0c\u5c24\u5176\u5728\u5e94\u7528\u5b9e\u4f8b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7ebf\u6027\u7ea6\u675f\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u8fd0\u7b79\u5b66\u548c\u4f18\u5316\u7b49\u9886\u57df\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u800cMCILC\u4f5c\u4e3a\u5176\u6838\u5fc3\u4efb\u52a1\u4e4b\u4e00\uff0c\u4e9f\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8eDPLL\u67b6\u6784\u7684\u7cbe\u786e\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u6df7\u5408\u6574\u6570\u89c4\u5212\u4e2d\u7684\u7b80\u5316\u6280\u672f\u3002", "result": "\u57282840\u4e2a\u968f\u673a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u89e3\u51b3\u4e861718\u4e2a\u5b9e\u4f8b\uff08\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u76841470\u4e2a\uff09\uff0c\u5e76\u57284131\u4e2a\u5e94\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u90e8\u89e3\u51b3\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8eDPLL\u67b6\u6784\u7684\u7cbe\u786e\u65b9\u6cd5\u5728MCILC\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c24\u5176\u5728\u5e94\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5353\u8d8a\u3002"}}
{"id": "2509.13577", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13577", "abs": "https://arxiv.org/abs/2509.13577", "authors": ["Tongfei Guo", "Lili Su"], "title": "Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles", "comment": "8 pages, 7 figures", "summary": "Trajectory prediction is central to the safe and seamless operation of\nautonomous vehicles (AVs). In deployment, however, prediction models inevitably\nface distribution shifts between training data and real-world conditions, where\nrare or underrepresented traffic scenarios induce out-of-distribution (OOD)\ncases. While most prior OOD detection research in AVs has concentrated on\ncomputer vision tasks such as object detection and segmentation,\ntrajectory-level OOD detection remains largely underexplored. A recent study\nformulated this problem as a quickest change detection (QCD) task, providing\nformal guarantees on the trade-off between detection delay and false alarms\n[1]. Building on this foundation, we propose a new framework that introduces\nadaptive mechanisms to achieve robust detection in complex driving\nenvironments. Empirical analysis across multiple real-world datasets reveals\nthat prediction errors -- even on in-distribution samples -- exhibit\nmode-dependent distributions that evolve over time with dataset-specific\ndynamics. By explicitly modeling these error modes, our method achieves\nsubstantial improvements in both detection delay and false alarm rates.\nComprehensive experiments on established trajectory prediction benchmarks show\nthat our framework significantly outperforms prior UQ- and vision-based OOD\napproaches in both accuracy and computational efficiency, offering a practical\npath toward reliable, driving-aware autonomy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u8f68\u8ff9\u7ea7OOD\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u90e8\u7f72\u4e2d\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u4e0e\u73b0\u5b9e\u6761\u4ef6\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u8f68\u8ff9\u7ea7OOD\u68c0\u6d4b\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u89e3\u51b3\u68c0\u6d4b\u5ef6\u8fdf\u4e0e\u8bef\u62a5\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u901a\u8fc7\u660e\u786e\u5efa\u6a21\u9884\u6d4b\u8bef\u5dee\u7684\u6a21\u5f0f\u4f9d\u8d56\u6027\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u7a33\u5065\u68c0\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u5ef6\u8fdf\u548c\u8bef\u62a5\u7387\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709UQ\u548c\u57fa\u4e8e\u89c6\u89c9\u7684OOD\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u673a\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4e2d\u8f68\u8ff9\u7ea7OOD\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u5ef6\u8fdf\u548c\u8bef\u62a5\u7387\uff0c\u4e3a\u53ef\u9760\u7684\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2509.13733", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13733", "abs": "https://arxiv.org/abs/2509.13733", "authors": ["Xiaolin Zhou", "Tingyang Xiao", "Liu Liu", "Yucheng Wang", "Maiyue Chen", "Xinrui Meng", "Xinjie Wang", "Wei Feng", "Wei Sui", "Zhizhong Su"], "title": "FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph", "comment": "8 pages", "summary": "Visual-Language Navigation (VLN) is a fundamental challenge in robotic\nsystems, with broad applications for the deployment of embodied agents in\nreal-world environments. Despite recent advances, existing approaches are\nlimited in long-range spatial reasoning, often exhibiting low success rates and\nhigh inference latency, particularly in long-range navigation tasks. To address\nthese limitations, we propose FSR-VLN, a vision-language navigation system that\ncombines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow\nNavigation Reasoning (FSR). The HMSG provides a multi-modal map representation\nsupporting progressive retrieval, from coarse room-level localization to\nfine-grained goal view and object identification. Building on HMSG, FSR first\nperforms fast matching to efficiently select candidate rooms, views, and\nobjects, then applies VLM-driven refinement for final goal selection. We\nevaluated FSR-VLN across four comprehensive indoor datasets collected by\nhumanoid robots, utilizing 87 instructions that encompass a diverse range of\nobject categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all\ndatasets, measured by the retrieval success rate (RSR), while reducing the\nresponse time by 82% compared to VLM-based methods on tour videos by activating\nslow reasoning only when fast intuition fails. Furthermore, we integrate\nFSR-VLN with speech interaction, planning, and control modules on a Unitree-G1\nhumanoid robot, enabling natural language interaction and real-time navigation.", "AI": {"tldr": "FSR-VLN\u7ed3\u5408HMSG\u548cFSR\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u957f\u8ddd\u79bb\u63a8\u7406\u6027\u80fd\uff0c\u54cd\u5e94\u65f6\u95f4\u51cf\u5c1182%\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u5728\u957f\u8ddd\u79bb\u7a7a\u95f4\u63a8\u7406\u4e2d\u5b58\u5728\u6210\u529f\u7387\u4f4e\u548c\u63a8\u7406\u5ef6\u8fdf\u9ad8\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faFSR-VLN\u7cfb\u7edf\uff0c\u7ed3\u5408HMSG\uff08\u5206\u5c42\u591a\u6a21\u6001\u573a\u666f\u56fe\uff09\u548cFSR\uff08\u5feb\u6162\u5bfc\u822a\u63a8\u7406\uff09\u3002HMSG\u63d0\u4f9b\u591a\u6a21\u6001\u5730\u56fe\u8868\u793a\uff0c\u652f\u6301\u4ece\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684\u6e10\u8fdb\u68c0\u7d22\uff1bFSR\u5148\u8fdb\u884c\u5feb\u901f\u5339\u914d\u7b5b\u9009\u5019\u9009\uff0c\u518d\u901a\u8fc7VLM\u9a71\u52a8\u7ec6\u5316\u9009\u62e9\u6700\u7ec8\u76ee\u6807\u3002", "result": "\u5728\u56db\u4e2a\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cFSR-VLN\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff08RSR\u8861\u91cf\uff09\uff0c\u54cd\u5e94\u65f6\u95f4\u6bd4VLM\u65b9\u6cd5\u51cf\u5c1182%\u3002\u7cfb\u7edf\u6210\u529f\u96c6\u6210\u5230Unitree-G1\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u5b9e\u65f6\u5bfc\u822a\u3002", "conclusion": "FSR-VLN\u901a\u8fc7\u7ed3\u5408HMSG\u548cFSR\uff0c\u5728\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u8ddd\u79bb\u7a7a\u95f4\u63a8\u7406\u7684\u6210\u529f\u7387\u5e76\u964d\u4f4e\u4e86\u54cd\u5e94\u65f6\u95f4\u3002\u6b64\u5916\uff0c\u8be5\u7cfb\u7edf\u6210\u529f\u96c6\u6210\u5230\u4eba\u5f62\u673a\u5668\u4eba\u4e2d\uff0c\u5b9e\u73b0\u4e86\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u5b9e\u65f6\u5bfc\u822a\u3002"}}
{"id": "2509.13968", "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13968", "abs": "https://arxiv.org/abs/2509.13968", "authors": ["Konstantinos Voudouris", "Andrew Barron", "Marta Halina", "Colin Klein", "Matishalin Patel"], "title": "Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks", "comment": null, "summary": "Transitional accounts of evolution emphasise a few changes that shape what is\nevolvable, with dramatic consequences for derived lineages. More recently it\nhas been proposed that cognition might also have evolved via a series of major\ntransitions that manipulate the structure of biological neural networks,\nfundamentally changing the flow of information. We used idealised models of\ninformation flow, artificial neural networks (ANNs), to evaluate whether\nchanges in information flow in a network can yield a transitional change in\ncognitive performance. We compared networks with feed-forward, recurrent and\nlaminated topologies, and tested their performance learning artificial grammars\nthat differed in complexity, controlling for network size and resources. We\ndocumented a qualitative expansion in the types of input that recurrent\nnetworks can process compared to feed-forward networks, and a related\nqualitative increase in performance for learning the most complex grammars. We\nalso noted how the difficulty in training recurrent networks poses a form of\ntransition barrier and contingent irreversibility -- other key features of\nevolutionary transitions. Not all changes in network topology confer a\nperformance advantage in this task set. Laminated networks did not outperform\nnon-laminated networks in grammar learning. Overall, our findings show how some\nchanges in information flow can yield transitions in cognitive performance.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u9012\u5f52\u7f51\u7edc\u62d3\u6251\u53d8\u5316\u80fd\u5e26\u6765\u8ba4\u77e5\u6027\u80fd\u7684\u8fc7\u6e21\u6027\u63d0\u5347\uff0c\u4f46\u5206\u5c42\u7f51\u7edc\u65e0\u4f18\u52bf\uff0c\u652f\u6301\u8ba4\u77e5\u6f14\u5316\u7684\u8fc7\u6e21\u7406\u8bba\u3002", "motivation": "\u63a2\u8ba8\u8ba4\u77e5\u662f\u5426\u901a\u8fc7\u4e00\u7cfb\u5217\u4e3b\u8981\u8fc7\u6e21\u6f14\u5316\uff0c\u8fd9\u4e9b\u8fc7\u6e21\u64cd\u7eb5\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u6784\uff0c\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4fe1\u606f\u6d41\u3002", "method": "\u4f7f\u7528\u7406\u60f3\u5316\u4fe1\u606f\u6d41\u6a21\u578b\u548c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\uff0c\u6bd4\u8f83\u4e86\u524d\u9988\u3001\u9012\u5f52\u548c\u5206\u5c42\u62d3\u6251\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u5e76\u63a7\u5236\u7f51\u7edc\u5927\u5c0f\u548c\u8d44\u6e90\u3002", "result": "\u9012\u5f52\u7f51\u7edc\u5728\u5904\u7406\u8f93\u5165\u7c7b\u578b\u4e0a\u8868\u73b0\u51fa\u8d28\u7684\u6269\u5c55\uff0c\u5e76\u5728\u5b66\u4e60\u6700\u590d\u6742\u8bed\u6cd5\u65f6\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u800c\u5206\u5c42\u7f51\u7edc\u5728\u8bed\u6cd5\u5b66\u4e60\u4e2d\u672a\u8868\u73b0\u51fa\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u67d0\u4e9b\u4fe1\u606f\u6d41\u7684\u53d8\u5316\u53ef\u4ee5\u5bfc\u81f4\u8ba4\u77e5\u6027\u80fd\u7684\u8fc7\u6e21\u6027\u53d8\u5316\uff0c\u5c24\u5176\u662f\u9012\u5f52\u7f51\u7edc\u5728\u5904\u7406\u590d\u6742\u8bed\u6cd5\u65f6\u8868\u73b0\u51fa\u8d28\u7684\u98de\u8dc3\u3002"}}
{"id": "2509.13586", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM", "I.2; I.4; I.7; H.3"], "pdf": "https://arxiv.org/pdf/2509.13586", "abs": "https://arxiv.org/abs/2509.13586", "authors": ["Nathalie Neptune", "Josiane Mothe"], "title": "Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection", "comment": null, "summary": "The Amazon rain forest is a vital ecosystem that plays a crucial role in\nregulating the Earth's climate and providing habitat for countless species.\nDeforestation in the Amazon is a major concern as it has a significant impact\non global carbon emissions and biodiversity. In this paper, we present a method\nfor detecting deforestation in the Amazon using image pairs from Earth\nobservation satellites. Our method leverages deep learning techniques to\ncompare the images of the same area at different dates and identify changes in\nthe forest cover. We also propose a visual semantic model that automatically\nannotates the detected changes with relevant keywords. The candidate annotation\nfor images are extracted from scientific documents related to the Amazon\nregion. We evaluate our approach on a dataset of Amazon image pairs and\ndemonstrate its effectiveness in detecting deforestation and generating\nrelevant annotations. Our method provides a useful tool for monitoring and\nstudying the impact of deforestation in the Amazon. While we focus on\nenvironment applications of our work by using images of deforestation in the\nAmazon rain forest to demonstrate the effectiveness of our proposed approach,\nit is generic enough to be applied to other domains.", "AI": {"tldr": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u548c\u89c6\u89c9\u8bed\u4e49\u6a21\u578b\u68c0\u6d4b\u4e9a\u9a6c\u900a\u68ee\u6797\u780d\u4f10\uff0c\u5e76\u63d0\u4f9b\u81ea\u52a8\u6807\u6ce8\uff0c\u65b9\u6cd5\u901a\u7528\u6027\u5f3a\u3002", "motivation": "\u4e9a\u9a6c\u900a\u96e8\u6797\u5bf9\u5730\u7403\u6c14\u5019\u548c\u751f\u7269\u591a\u6837\u6027\u81f3\u5173\u91cd\u8981\uff0c\u5176\u780d\u4f10\u5bf9\u5168\u7403\u78b3\u6392\u653e\u548c\u751f\u7269\u591a\u6837\u6027\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u76d1\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6bd4\u8f83\u4e0d\u540c\u65e5\u671f\u7684\u5730\u7403\u89c2\u6d4b\u536b\u661f\u56fe\u50cf\u5bf9\uff0c\u8bc6\u522b\u68ee\u6797\u8986\u76d6\u53d8\u5316\uff0c\u5e76\u63d0\u51fa\u89c6\u89c9\u8bed\u4e49\u6a21\u578b\u81ea\u52a8\u6807\u6ce8\u68c0\u6d4b\u5230\u7684\u53d8\u5316\u3002", "result": "\u5728\u4e9a\u9a6c\u900a\u56fe\u50cf\u5bf9\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u68ee\u6797\u780d\u4f10\u548c\u751f\u6210\u76f8\u5173\u6807\u6ce8\u65b9\u9762\u6709\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u76d1\u6d4b\u548c\u7814\u7a76\u4e9a\u9a6c\u900a\u68ee\u6797\u780d\u4f10\u5f71\u54cd\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u4e14\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u9886\u57df\u3002"}}
{"id": "2509.13736", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13736", "abs": "https://arxiv.org/abs/2509.13736", "authors": ["Muyuan Ma", "Long Cheng", "Lijun Han", "Xiuze Xia", "Houcheng Li"], "title": "Motion Adaptation Across Users and Tasks for Exoskeletons via Meta-Learning", "comment": null, "summary": "Wearable exoskeletons can augment human strength and reduce muscle fatigue\nduring specific tasks. However, developing personalized and task-generalizable\nassistance algorithms remains a critical challenge. To address this, a\nmeta-imitation learning approach is proposed. This approach leverages a\ntask-specific neural network to predict human elbow joint movements, enabling\neffective assistance while enhancing generalization to new scenarios. To\naccelerate data collection, full-body keypoint motions are extracted from\npublicly available RGB video and motion-capture datasets across multiple tasks,\nand subsequently retargeted in simulation. Elbow flexion trajectories generated\nin simulation are then used to train the task-specific neural network within\nthe model-agnostic meta-learning (MAML) framework, which allows the network to\nrapidly adapt to novel tasks and unseen users with only a few gradient updates.\nThe adapted network outputs personalized references tracked by a\ngravity-compensated PD controller to ensure stable assistance. Experimental\nresults demonstrate that the exoskeleton significantly reduces both muscle\nactivation and metabolic cost for new users performing untrained tasks,\ncompared to performing without exoskeleton assistance. These findings suggest\nthat the proposed framework effectively improves task generalization and user\nadaptability for wearable exoskeleton systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u6a21\u4eff\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u7528\u6237\uff0c\u663e\u8457\u63d0\u5347\u5916\u9aa8\u9abc\u7684\u8f85\u52a9\u6548\u679c\u3002", "motivation": "\u5f00\u53d1\u4e2a\u6027\u5316\u548c\u4efb\u52a1\u53ef\u6cdb\u5316\u7684\u8f85\u52a9\u7b97\u6cd5\u662f\u53ef\u7a7f\u6234\u5916\u9aa8\u9abc\u589e\u5f3a\u4eba\u7c7b\u529b\u91cf\u548c\u51cf\u5c11\u808c\u8089\u75b2\u52b3\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528\u5143\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u4efb\u52a1\u7279\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u4eba\u7c7b\u8098\u5173\u8282\u8fd0\u52a8\uff0c\u5e76\u7ed3\u5408\u6a21\u578b\u65e0\u5173\u7684\u5143\u5b66\u4e60\uff08MAML\uff09\u6846\u67b6\uff0c\u4f7f\u7f51\u7edc\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u672a\u89c1\u8fc7\u7684\u7528\u6237\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5916\u9aa8\u9abc\u663e\u8457\u964d\u4f4e\u4e86\u65b0\u7528\u6237\u6267\u884c\u672a\u8bad\u7ec3\u4efb\u52a1\u65f6\u7684\u808c\u8089\u6fc0\u6d3b\u548c\u4ee3\u8c22\u6210\u672c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u53ef\u7a7f\u6234\u5916\u9aa8\u9abc\u7cfb\u7edf\u7684\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u548c\u7528\u6237\u9002\u5e94\u6027\u3002"}}
{"id": "2509.14030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14030", "abs": "https://arxiv.org/abs/2509.14030", "authors": ["Maosheng Qin", "Renyu Zhu", "Mingxuan Xia", "Chenkai Chen", "Zhen Zhu", "Minmin Lin", "Junbo Zhao", "Lu Xu", "Changjie Fan", "Runze Wu", "Haobo Wang"], "title": "CrowdAgent: Multi-Agent Managed Multi-Source Annotation System", "comment": null, "summary": "High-quality annotated data is a cornerstone of modern Natural Language\nProcessing (NLP). While recent methods begin to leverage diverse annotation\nsources-including Large Language Models (LLMs), Small Language Models (SLMs),\nand human experts-they often focus narrowly on the labeling step itself. A\ncritical gap remains in the holistic process control required to manage these\nsources dynamically, addressing complex scheduling and quality-cost trade-offs\nin a unified manner. Inspired by real-world crowdsourcing companies, we\nintroduce CrowdAgent, a multi-agent system that provides end-to-end process\ncontrol by integrating task assignment, data annotation, and quality/cost\nmanagement. It implements a novel methodology that rationally assigns tasks,\nenabling LLMs, SLMs, and human experts to advance synergistically in a\ncollaborative annotation workflow. We demonstrate the effectiveness of\nCrowdAgent through extensive experiments on six diverse multimodal\nclassification tasks. The source code and video demo are available at\nhttps://github.com/QMMMS/CrowdAgent.", "AI": {"tldr": "CrowdAgent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u7ba1\u7406LLMs\u3001SLMs\u548c\u4eba\u7c7b\u4e13\u5bb6\u7684\u534f\u4f5c\u6807\u6ce8\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86NLP\u4e2d\u591a\u6e90\u6807\u6ce8\u7684\u5168\u9762\u63a7\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3NLP\u9700\u8981\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6807\u6ce8\u6b65\u9aa4\u672c\u8eab\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6e90\u6807\u6ce8\uff08\u5982LLMs\u3001SLMs\u548c\u4eba\u7c7b\u4e13\u5bb6\uff09\u7684\u52a8\u6001\u7ba1\u7406\u548c\u6d41\u7a0b\u63a7\u5236\u3002", "method": "\u5f15\u5165CrowdAgent\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u91c7\u7528\u65b0\u9896\u7684\u65b9\u6cd5\u8bba\uff0c\u5408\u7406\u5206\u914d\u4efb\u52a1\uff0c\u4f7fLLMs\u3001SLMs\u548c\u4eba\u7c7b\u4e13\u5bb6\u5728\u534f\u4f5c\u6807\u6ce8\u6d41\u7a0b\u4e2d\u534f\u540c\u5de5\u4f5c\u3002", "result": "\u5728\u516d\u4e2a\u591a\u6837\u5316\u7684\u591a\u6a21\u6001\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CrowdAgent\u7684\u6709\u6548\u6027\u3002", "conclusion": "CrowdAgent\u901a\u8fc7\u6574\u5408\u4efb\u52a1\u5206\u914d\u3001\u6570\u636e\u6807\u6ce8\u548c\u8d28\u91cf/\u6210\u672c\u7ba1\u7406\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u4ee3NLP\u4e2d\u591a\u6e90\u6807\u6ce8\u7684\u52a8\u6001\u7ba1\u7406\u95ee\u9898\u3002"}}
{"id": "2509.13590", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13590", "abs": "https://arxiv.org/abs/2509.13590", "authors": ["Samer Al-Hamadani"], "title": "Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation", "comment": "32 pages, 14 figures, 6 tables", "summary": "The rapid advancement of artificial intelligence (AI) in healthcare imaging\nhas revolutionized diagnostic medicine and clinical decision-making processes.\nThis work presents an intelligent multimodal framework for medical image\nanalysis that leverages Vision-Language Models (VLMs) in healthcare\ndiagnostics. The framework integrates Google Gemini 2.5 Flash for automated\ntumor detection and clinical report generation across multiple imaging\nmodalities including CT, MRI, X-ray, and Ultrasound. The system combines visual\nfeature extraction with natural language processing to enable contextual image\ninterpretation, incorporating coordinate verification mechanisms and\nprobabilistic Gaussian modeling for anomaly distribution. Multi-layered\nvisualization techniques generate detailed medical illustrations, overlay\ncomparisons, and statistical representations to enhance clinical confidence,\nwith location measurement achieving 80 pixels average deviation. Result\nprocessing utilizes precise prompt engineering and textual analysis to extract\nstructured clinical information while maintaining interpretability.\nExperimental evaluations demonstrated high performance in anomaly detection\nacross multiple modalities. The system features a user-friendly Gradio\ninterface for clinical workflow integration and demonstrates zero-shot learning\ncapabilities to reduce dependence on large datasets. This framework represents\na significant advancement in automated diagnostic support and radiological\nworkflow efficiency, though clinical validation and multi-center evaluation are\nnecessary prior to widespread adoption.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVision-Language Models\u7684\u667a\u80fd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u7597\u5f71\u50cf\u5206\u6790\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u5f71\u50cf\u4e2d\u7684\u5feb\u901f\u53d1\u5c55\u4e3a\u8bca\u65ad\u533b\u5b66\u548c\u4e34\u5e8a\u51b3\u7b56\u8fc7\u7a0b\u5e26\u6765\u4e86\u9769\u547d\u6027\u7684\u53d8\u5316\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u5229\u7528Google Gemini 2.5 Flash\u8fdb\u884c\u81ea\u52a8\u80bf\u7624\u68c0\u6d4b\u548c\u4e34\u5e8a\u62a5\u544a\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u591a\u79cd\u5f71\u50cf\u6a21\u6001\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u8868\u73b0\u4f18\u5f02\uff0c\u4f4d\u7f6e\u6d4b\u91cf\u7684\u5e73\u5747\u504f\u5dee\u4e3a80\u50cf\u7d20\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u81ea\u52a8\u8bca\u65ad\u652f\u6301\u548c\u653e\u5c04\u5b66\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5e7f\u6cdb\u91c7\u7528\u4e4b\u524d\u9700\u8981\u8fdb\u884c\u4e34\u5e8a\u9a8c\u8bc1\u548c\u591a\u4e2d\u5fc3\u8bc4\u4f30\u3002"}}
{"id": "2509.13737", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13737", "abs": "https://arxiv.org/abs/2509.13737", "authors": ["Renjie Wang", "Shangke Lyu", "Donglin Wang"], "title": "Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force Control and Gait Control", "comment": null, "summary": "While Reinforcement Learning (RL) has achieved remarkable progress in legged\nlocomotion control, it often suffers from performance degradation in\nout-of-distribution (OOD) conditions and discrepancies between the simulation\nand the real environments. Instead of mainly relying on domain randomization\n(DR) to best cover the real environments and thereby close the sim-to-real gap\nand enhance robustness, this work proposes an emerging decoupled framework that\nacquires fast online adaptation ability and mitigates the sim-to-real problems\nin unfamiliar environments by isolating stance-leg control and swing-leg\ncontrol. Various simulation and real-world experiments demonstrate its\neffectiveness against horizontal force disturbances, uneven terrains, heavy and\nbiased payloads, and sim-to-real gap.", "AI": {"tldr": "\u63d0\u51fa\u89e3\u8026\u6846\u67b6\uff0c\u5206\u79bb\u7ad9\u7acb\u817f\u548c\u6446\u52a8\u817f\u63a7\u5236\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\uff0c\u6709\u6548\u7f13\u89e3\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u817f\u90e8\u8fd0\u52a8\u63a7\u5236\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5206\u5e03\u5916\u6761\u4ef6\u548c\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9886\u57df\u968f\u673a\u5316\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u91c7\u7528\u89e3\u8026\u6846\u67b6\uff0c\u5206\u522b\u63a7\u5236\u7ad9\u7acb\u817f\u548c\u6446\u52a8\u817f\uff0c\u4ee5\u589e\u5f3a\u5728\u7ebf\u9002\u5e94\u80fd\u529b\u548c\u51cf\u5c11\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u5f02\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6c34\u5e73\u529b\u5e72\u6270\u3001\u4e0d\u5e73\u5766\u5730\u5f62\u3001\u91cd\u8f7d\u548c\u504f\u7f6e\u8f7d\u8377\u4ee5\u53ca\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u4e2d\u8868\u73b0\u6709\u6548\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u7ad9\u7acb\u817f\u548c\u6446\u52a8\u817f\u7684\u63a7\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\uff0c\u5e76\u7f13\u89e3\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u95ee\u9898\u3002"}}
{"id": "2509.14195", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14195", "abs": "https://arxiv.org/abs/2509.14195", "authors": ["Shalima Binta Manir", "Tim Oates"], "title": "Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning", "comment": "8 pages, 3 figures", "summary": "Mental representation, characterized by structured internal models mirroring\nexternal environments, is fundamental to advanced cognition but remains\nchallenging to investigate empirically. Existing theory hypothesizes that\nsecond-order learning -- learning mechanisms that adapt first-order learning\n(i.e., learning about the task/domain) -- promotes the emergence of such\nenvironment-cognition isomorphism. In this paper, we empirically validate this\nhypothesis by proposing a hierarchical architecture comprising a Graph\nConvolutional Network (GCN) as a first-order learner and an MLP controller as a\nsecond-order learner. The GCN directly maps node-level features to predictions\nof optimal navigation paths, while the MLP dynamically adapts the GCN's\nparameters when confronting structurally novel maze environments. We\ndemonstrate that second-order learning is particularly effective when the\ncognitive system develops an internal mental map structurally isomorphic to the\nenvironment. Quantitative and qualitative results highlight significant\nperformance improvements and robust generalization on unseen maze tasks,\nproviding empirical support for the pivotal role of structured mental\nrepresentations in maximizing the effectiveness of second-order learning.", "AI": {"tldr": "\u901a\u8fc7\u5206\u5c42\u67b6\u6784\uff08GCN+MLP\uff09\u9a8c\u8bc1\u4e8c\u9636\u5b66\u4e60\u4fc3\u8fdb\u5fc3\u7406\u8868\u5f81\u540c\u6784\u6027\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u4e8c\u9636\u5b66\u4e60\u5982\u4f55\u4fc3\u8fdb\u5fc3\u7406\u8868\u5f81\uff08\u5373\u5185\u90e8\u6a21\u578b\u4e0e\u5916\u90e8\u73af\u5883\u7684\u7ed3\u6784\u540c\u6784\u6027\uff09\u7684\u6d8c\u73b0\uff0c\u4ee5\u89e3\u51b3\u9ad8\u7ea7\u8ba4\u77e5\u7814\u7a76\u4e2d\u5b9e\u8bc1\u9a8c\u8bc1\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u67b6\u6784\uff0c\u5305\u62ec\u4f5c\u4e3a\u4e00\u9636\u5b66\u4e60\u5668\u7684\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u548c\u4f5c\u4e3a\u4e8c\u9636\u5b66\u4e60\u5668\u7684MLP\u63a7\u5236\u5668\u3002GCN\u76f4\u63a5\u6620\u5c04\u8282\u70b9\u7279\u5f81\u5230\u6700\u4f18\u5bfc\u822a\u8def\u5f84\u9884\u6d4b\uff0cMLP\u5728\u9047\u5230\u7ed3\u6784\u65b0\u9896\u7684\u8ff7\u5bab\u73af\u5883\u65f6\u52a8\u6001\u8c03\u6574GCN\u53c2\u6570\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u7ed3\u679c\u8868\u660e\uff0c\u4e8c\u9636\u5b66\u4e60\u5728\u5fc3\u7406\u8868\u5f81\u4e0e\u73af\u5883\u540c\u6784\u65f6\u8868\u73b0\u5c24\u4e3a\u6709\u6548\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u8ff7\u5bab\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u5b9e\u8bc1\u4e86\u4e8c\u9636\u5b66\u4e60\uff08\u5373\u9002\u5e94\u4e00\u9636\u5b66\u4e60\u7684\u5b66\u4e60\u673a\u5236\uff09\u5728\u4fc3\u8fdb\u73af\u5883-\u8ba4\u77e5\u540c\u6784\u6027\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u652f\u6301\u7ed3\u6784\u5316\u5fc3\u7406\u8868\u5f81\u5bf9\u63d0\u5347\u5b66\u4e60\u6709\u6548\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.13605", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13605", "abs": "https://arxiv.org/abs/2509.13605", "authors": ["Ruochen Hou", "Gabriel I. Fernandez", "Alex Xu", "Dennis W. Hong"], "title": "A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms", "comment": null, "summary": "In previous work, we introduced a 2D localization algorithm called CLAP,\nClustering to Localize Across $n$ Possibilities, which was used during our\nchampionship win in RoboCup 2024, an international autonomous humanoid soccer\ncompetition. CLAP is particularly recognized for its robustness against\noutliers, where clustering is employed to suppress noise and mitigate against\nerroneous feature matches. This clustering-based strategy provides an\nalternative to traditional outlier rejection schemes such as RANSAC, in which\ncandidates are validated by reprojection error across all data points. In this\npaper, CLAP is extended to a more general framework beyond 2D localization,\nspecifically to 3D localization and image stitching. We also show how CLAP,\nRANSAC, and Hough transforms are related. The generalization of CLAP is widely\napplicable to many different fields and can be a useful tool to deal with noise\nand uncertainty.", "AI": {"tldr": "CLAP\u7b97\u6cd5\u4ece2D\u5b9a\u4f4d\u6269\u5c55\u52303D\u5b9a\u4f4d\u548c\u56fe\u50cf\u62fc\u63a5\uff0c\u5c55\u793a\u4e86\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u53ca\u4e0e\u5176\u4ed6\u6280\u672f\u7684\u5173\u7cfb\u3002", "motivation": "\u6269\u5c55CLAP\u7b97\u6cd5\u81f3\u66f4\u901a\u7528\u7684\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf93D\u5b9a\u4f4d\u548c\u56fe\u50cf\u62fc\u63a5\u7b49\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u3002", "method": "\u901a\u8fc7\u805a\u7c7b\u6291\u5236\u566a\u58f0\u548c\u9519\u8bef\u7279\u5f81\u5339\u914d\uff0cCLAP\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u4f20\u7edf\u5f02\u5e38\u503c\u62d2\u7edd\u65b9\u6848\uff08\u5982RANSAC\uff09\u7684\u7b56\u7565\u3002", "result": "\u6210\u529f\u5c06CLAP\u4ece2D\u5b9a\u4f4d\u6269\u5c55\u52303D\u5b9a\u4f4d\u548c\u56fe\u50cf\u62fc\u63a5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4e0eRANSAC\u548cHough\u53d8\u6362\u7684\u5173\u7cfb\u3002", "conclusion": "CLAP\u7b97\u6cd5\u7684\u6269\u5c55\u6846\u67b6\u57283D\u5b9a\u4f4d\u548c\u56fe\u50cf\u62fc\u63a5\u4e2d\u8868\u73b0\u51fa\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u6210\u4e3a\u5904\u7406\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u6709\u7528\u5de5\u5177\u3002"}}
{"id": "2509.13771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13771", "abs": "https://arxiv.org/abs/2509.13771", "authors": ["Mengzhu Li", "Yunyu Zhou", "He Ying", "F. Richard Yu"], "title": "CDFlow: Generative Gradient Flows for Configuration Space Distance Fields via Neural ODEs", "comment": null, "summary": "Signed Distance Fields (SDFs) are a fundamental representation in robot\nmotion planning. Their configuration-space counterpart, the Configuration Space\nDistance Field (CDF), directly encodes distances in joint space, offering a\nunified representation for optimization and control. However, existing CDF\nformulations face two major challenges in high-degree-of-freedom (DoF) robots:\n(1) they effectively return only a single nearest collision configuration,\nneglecting the multi-modal nature of minimal-distance collision configurations\nand leading to gradient ambiguity; and (2) they rely on sparse sampling of the\ncollision boundary, which often fails to identify the true closest\nconfigurations, producing oversmoothed approximations and geometric distortion\nin high-dimensional spaces. We propose CDFlow, a novel framework that addresses\nthese limitations by learning a continuous flow in configuration space via\nNeural Ordinary Differential Equations (Neural ODEs). We redefine the problem\nfrom finding a single nearest point to modeling the distribution of\nminimal-distance collision configurations. We also introduce an adaptive\nrefinement sampling strategy to generate high-fidelity training data for this\ndistribution. The resulting Neural ODE implicitly models this multi-modal\ndistribution and produces a smooth, consistent gradient field-derived as the\nexpected direction towards the distribution-that mitigates gradient ambiguity\nand preserves sharp geometric features. Extensive experiments on high-DoF\nmotion planning tasks demonstrate that CDFlow significantly improves planning\nefficiency, trajectory quality, and robustness compared to existing CDF-based\nmethods, enabling more robust and efficient planning for collision-aware robots\nin complex environments.", "AI": {"tldr": "CDFlow\u5229\u7528Neural ODEs\u89e3\u51b3\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4ebaCDF\u7684\u5355\u6a21\u6001\u548c\u7a00\u758f\u91c7\u6837\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CDF\u65b9\u6cd5\u5728\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u4e2d\u5b58\u5728\u5355\u6a21\u6001\u6700\u8fd1\u78b0\u649e\u914d\u7f6e\u548c\u7a00\u758f\u91c7\u6837\u8fb9\u754c\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u68af\u5ea6\u6a21\u7cca\u548c\u51e0\u4f55\u5931\u771f\u3002", "method": "\u63d0\u51faCDFlow\u6846\u67b6\uff0c\u5229\u7528Neural ODEs\u5b66\u4e60\u914d\u7f6e\u7a7a\u95f4\u4e2d\u7684\u8fde\u7eed\u6d41\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u7ec6\u5316\u91c7\u6837\u7b56\u7565\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCDFlow\u5728\u9ad8\u81ea\u7531\u5ea6\u8fd0\u52a8\u89c4\u5212\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709CDF\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u89c4\u5212\u6548\u7387\u548c\u8f68\u8ff9\u8d28\u91cf\u3002", "conclusion": "CDFlow\u901a\u8fc7Neural ODEs\u5b66\u4e60\u914d\u7f6e\u7a7a\u95f4\u4e2d\u7684\u8fde\u7eed\u6d41\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.13629", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13629", "abs": "https://arxiv.org/abs/2509.13629", "authors": ["Yue He", "Min Liu", "Qinghao Liu", "Jiazheng Wang", "Yaonan Wang", "Hang Zhang", "Xiang Chen"], "title": "SAMIR, an efficient registration framework via robust feature learning from SAM", "comment": null, "summary": "Image registration is a fundamental task in medical image analysis.\nDeformations are often closely related to the morphological characteristics of\ntissues, making accurate feature extraction crucial. Recent weakly supervised\nmethods improve registration by incorporating anatomical priors such as\nsegmentation masks or landmarks, either as inputs or in the loss function.\nHowever, such weak labels are often not readily available, limiting their\npractical use. Motivated by the strong representation learning ability of\nvisual foundation models, this paper introduces SAMIR, an efficient medical\nimage registration framework that utilizes the Segment Anything Model (SAM) to\nenhance feature extraction. SAM is pretrained on large-scale natural image\ndatasets and can learn robust, general-purpose visual representations. Rather\nthan using raw input images, we design a task-specific adaptation pipeline\nusing SAM's image encoder to extract structure-aware feature embeddings,\nenabling more accurate modeling of anatomical consistency and deformation\npatterns. We further design a lightweight 3D head to refine features within the\nembedding space, adapting to local deformations in medical images.\nAdditionally, we introduce a Hierarchical Feature Consistency Loss to guide\ncoarse-to-fine feature matching and improve anatomical alignment. Extensive\nexperiments demonstrate that SAMIR significantly outperforms state-of-the-art\nmethods on benchmark datasets for both intra-subject cardiac image registration\nand inter-subject abdomen CT image registration, achieving performance\nimprovements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code\nwill be publicly available on GitHub following the acceptance of this paper.", "AI": {"tldr": "SAMIR\u5229\u7528Segment Anything Model\uff08SAM\uff09\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u914d\u51c6\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u63d0\u53d6\u548c\u5206\u5c42\u4e00\u81f4\u6027\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u914d\u51c6\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u96be\u4ee5\u83b7\u53d6\u7684\u89e3\u5256\u5148\u9a8c\uff08\u5982\u5206\u5272\u63a9\u7801\u6216\u6807\u5fd7\u70b9\uff09\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002SAMIR\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5f3a\u5927\u8868\u793a\u5b66\u4e60\u80fd\u529b\uff0c\u514b\u670d\u4e86\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86\u4efb\u52a1\u7279\u5b9a\u7684\u9002\u5e94\u6d41\u7a0b\uff0c\u5229\u7528SAM\u7684\u56fe\u50cf\u7f16\u7801\u5668\u63d0\u53d6\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u5d4c\u5165\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea73D\u5934\u90e8\u548c\u5206\u5c42\u7279\u5f81\u4e00\u81f4\u6027\u635f\u5931\u6765\u4f18\u5316\u7279\u5f81\u5339\u914d\u548c\u89e3\u5256\u5bf9\u9f50\u3002", "result": "\u5728\u5fc3\u810f\u56fe\u50cf\u548c\u8179\u90e8CT\u56fe\u50cf\u7684\u914d\u51c6\u4efb\u52a1\u4e2d\uff0cSAMIR\u5206\u522b\u5b9e\u73b0\u4e862.68%\u548c6.44%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SAMIR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408Segment Anything Model\uff08SAM\uff09\u7684\u5f3a\u5927\u8868\u793a\u5b66\u4e60\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u51c6\u786e\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2509.13774", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13774", "abs": "https://arxiv.org/abs/2509.13774", "authors": ["Piaopiao Jin", "Qi Wang", "Guokang Sun", "Ziwen Cai", "Pinjia He", "Yangwei You"], "title": "Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach", "comment": null, "summary": "Vision-language-action (VLA) models demonstrate strong generalization in\nrobotic manipulation but face challenges in complex, real-world tasks. While\nsupervised fine-tuning with demonstrations is constrained by data quality,\nreinforcement learning (RL) offers a promising alternative. We propose a\nhuman-in-the-loop dual-actor fine-tuning framework grounded in RL. The\nframework integrates a primary actor for robust multi-task performance with a\nrefinement actor for latent-space adaptation. Beyond standard physical\ninterventions, we introduce a lightweight talk-and-tweak scheme that converts\nhuman corrections into semantically grounded language commands, thereby\ngenerating a new dataset for policy learning. In real-world multi-task\nexperiments, our approach achieves 100% success across three tasks within 101\nminutes of online fine-tuning. For long-horizon tasks, it sustains a 50%\nsuccess rate over 12 consecutive operations. Furthermore, the framework scales\neffectively to multi-robot training, achieving up to a 2 times improvement in\nefficiency when using dual robots. The experiment videos are available at\nhttps://sites.google.com/view/hil-daft/.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4eba\u673a\u4ea4\u4e92\u53cc\u6267\u884c\u5668\u5fae\u8c03\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u9488\u5bf9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u76d1\u7763\u5fae\u8c03\u53d7\u9650\u4e8e\u6570\u636e\u8d28\u91cf\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4eba\u673a\u4ea4\u4e92\u7684\u53cc\u6267\u884c\u5668\u5fae\u8c03\u6846\u67b6\uff0c\u7ed3\u5408\u4e3b\u6267\u884c\u5668\u5b9e\u73b0\u591a\u4efb\u52a1\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u7ec6\u5316\u6267\u884c\u5668\u8fdb\u884c\u6f5c\u5728\u7a7a\u95f4\u9002\u5e94\u3002\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684\u2018talk-and-tweak\u2019\u65b9\u6848\uff0c\u5c06\u4eba\u7c7b\u7ea0\u6b63\u8f6c\u5316\u4e3a\u8bed\u4e49\u57fa\u7840\u7684\u8bed\u8a00\u547d\u4ee4\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u591a\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0c101\u5206\u949f\u5185\u5b9e\u73b0100%\u6210\u529f\u7387\uff1b\u957f\u65f6\u4efb\u52a1\u4e2d\u4fdd\u630150%\u6210\u529f\u7387\uff0812\u6b21\u8fde\u7eed\u64cd\u4f5c\uff09\u3002\u591a\u673a\u5668\u4eba\u8bad\u7ec3\u6548\u7387\u63d0\u53472\u500d\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4eba\u673a\u4ea4\u4e92\u53cc\u6267\u884c\u5668\u5fae\u8c03\u6846\u67b6\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u591a\u4efb\u52a1\u548c\u957f\u65f6\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u548c\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2509.13780", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13780", "abs": "https://arxiv.org/abs/2509.13780", "authors": ["Weishuai Zeng", "Shunlin Lu", "Kangning Yin", "Xiaojie Niu", "Minyue Dai", "Jingbo Wang", "Jiangmiao Pang"], "title": "Behavior Foundation Model for Humanoid Robots", "comment": null, "summary": "Whole-body control (WBC) of humanoid robots has witnessed remarkable progress\nin skill versatility, enabling a wide range of applications such as locomotion,\nteleoperation, and motion tracking. Despite these achievements, existing WBC\nframeworks remain largely task-specific, relying heavily on labor-intensive\nreward engineering and demonstrating limited generalization across tasks and\nskills. These limitations hinder their response to arbitrary control modes and\nrestrict their deployment in complex, real-world scenarios. To address these\nchallenges, we revisit existing WBC systems and identify a shared objective\nacross diverse tasks: the generation of appropriate behaviors that guide the\nrobot toward desired goal states. Building on this insight, we propose the\nBehavior Foundation Model (BFM), a generative model pretrained on large-scale\nbehavioral datasets to capture broad, reusable behavioral knowledge for\nhumanoid robots. BFM integrates a masked online distillation framework with a\nConditional Variational Autoencoder (CVAE) to model behavioral distributions,\nthereby enabling flexible operation across diverse control modes and efficient\nacquisition of novel behaviors without retraining from scratch. Extensive\nexperiments in both simulation and on a physical humanoid platform demonstrate\nthat BFM generalizes robustly across diverse WBC tasks while rapidly adapting\nto new behaviors. These results establish BFM as a promising step toward a\nfoundation model for general-purpose humanoid control.", "AI": {"tldr": "BFM\u662f\u4e00\u79cd\u9884\u8bad\u7ec3\u5728\u5927\u89c4\u6a21\u884c\u4e3a\u6570\u636e\u96c6\u4e0a\u7684\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u63a9\u7801\u5728\u7ebf\u84b8\u998f\u548cCVAE\uff0c\u5b9e\u73b0\u4e86\u8de8\u591a\u6837\u5316\u63a7\u5236\u6a21\u5f0f\u7684\u7075\u6d3b\u64cd\u4f5c\u548c\u9ad8\u6548\u884c\u4e3a\u83b7\u53d6\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u548c\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u73b0\u6709WBC\u6846\u67b6\u591a\u4e3a\u4efb\u52a1\u7279\u5b9a\uff0c\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u5956\u52b1\u5de5\u7a0b\uff0c\u8de8\u4efb\u52a1\u548c\u6280\u80fd\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u3002BFM\u65e8\u5728\u901a\u8fc7\u6355\u6349\u5e7f\u6cdb\u53ef\u91cd\u7528\u7684\u884c\u4e3a\u77e5\u8bc6\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u884c\u4e3a\u57fa\u7840\u6a21\u578b\uff08BFM\uff09\uff0c\u7ed3\u5408\u63a9\u7801\u5728\u7ebf\u84b8\u998f\u6846\u67b6\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVAE\uff09\u6765\u5efa\u6a21\u884c\u4e3a\u5206\u5e03\uff0c\u652f\u6301\u8de8\u591a\u6837\u5316\u63a7\u5236\u6a21\u5f0f\u7684\u7075\u6d3b\u64cd\u4f5c\uff0c\u5e76\u65e0\u9700\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u5373\u53ef\u9ad8\u6548\u83b7\u53d6\u65b0\u884c\u4e3a\u3002", "result": "\u5728\u4eff\u771f\u548c\u7269\u7406\u7c7b\u4eba\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cBFM\u80fd\u7a33\u5065\u5730\u6cdb\u5316\u5230\u591a\u6837\u5316WBC\u4efb\u52a1\u4e2d\uff0c\u5e76\u5feb\u901f\u9002\u5e94\u65b0\u884c\u4e3a\u3002", "conclusion": "BFM\u4f5c\u4e3a\u4e00\u79cd\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u5927\u89c4\u6a21\u884c\u4e3a\u6570\u636e\u96c6\u4e0a\u7684\u9884\u8bad\u7ec3\uff0c\u4e3a\u7c7b\u4eba\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u4e14\u53ef\u91cd\u7528\u7684\u884c\u4e3a\u77e5\u8bc6\uff0c\u5c55\u793a\u4e86\u5728\u591a\u6837\u5316WBC\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u548c\u5feb\u901f\u9002\u5e94\u65b0\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u4e3a\u901a\u7528\u7c7b\u4eba\u673a\u5668\u4eba\u63a7\u5236\u7684\u57fa\u7840\u6a21\u578b\u8fc8\u51fa\u4e86\u6709\u5e0c\u671b\u7684\u4e00\u6b65\u3002"}}
{"id": "2509.13652", "categories": ["cs.CV", "I.4.8; I.4.5"], "pdf": "https://arxiv.org/pdf/2509.13652", "abs": "https://arxiv.org/abs/2509.13652", "authors": ["Yumin Li", "Dylan Campbell"], "title": "Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction", "comment": "12 pages, 4 figures, accepted by AJCAI 2025", "summary": "Estimating metric relative camera pose from a pair of images is of great\nimportance for 3D reconstruction and localisation. However, conventional\ntwo-view pose estimation methods are not metric, with camera translation known\nonly up to a scale, and struggle with wide baselines and textureless or\nreflective surfaces. This paper introduces GARPS, a training-free framework\nthat casts this problem as the direct alignment of two independently\nreconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and\na Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model\n(GMM) for each image. It then refines an initial pose from a feed-forward\ntwo-view pose estimator by optimising a differentiable GMM alignment objective.\nThis objective jointly considers geometric structure, view-independent colour,\nanisotropic covariance, and semantic feature consistency, and is robust to\nocclusions and texture-poor regions without requiring explicit 2D\ncorrespondences. Extensive experiments on the Real\\-Estate10K dataset\ndemonstrate that GARPS outperforms both classical and state-of-the-art\nlearning-based methods, including MASt3R. These results highlight the potential\nof bridging single-view perception with multi-view geometry to achieve robust\nand metric relative pose estimation.", "AI": {"tldr": "GARPS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u5bf9\u9f50\u72ec\u7acb\u91cd\u5efa\u76843D\u573a\u666f\u5b9e\u73b0\u5ea6\u91cf\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\uff0c\u5728\u5bbd\u57fa\u7ebf\u548c\u590d\u6742\u8868\u9762\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u4e24\u89c6\u89d2\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u5ea6\u91cf\u5c3a\u5ea6\uff0c\u4e14\u5728\u5bbd\u57fa\u7ebf\u3001\u65e0\u7eb9\u7406\u6216\u53cd\u5c04\u8868\u9762\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002GARPS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "GARPS\u5229\u7528\u5ea6\u91cf\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5668\u548c\u9ad8\u65af\u573a\u666f\u91cd\u5efa\u5668\uff0c\u751f\u6210\u6bcf\u5f20\u56fe\u50cf\u7684\u5ea6\u91cf3D\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u53ef\u5fae\u5206\u7684GMM\u5bf9\u9f50\u76ee\u6807\u6765\u6539\u8fdb\u521d\u59cb\u4f4d\u59ff\u3002", "result": "\u5728Real-Estate10K\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGARPS\u4f18\u4e8e\u4f20\u7edf\u548c\u6700\u5148\u8fdb\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ecMASt3R\u3002", "conclusion": "GARPS\u901a\u8fc7\u7ed3\u5408\u5355\u89c6\u89d2\u611f\u77e5\u4e0e\u591a\u89c6\u89d2\u51e0\u4f55\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u4e14\u5ea6\u91cf\u7684\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\uff0c\u5c55\u793a\u4e86\u5176\u57283D\u91cd\u5efa\u548c\u5b9a\u4f4d\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13802", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13802", "abs": "https://arxiv.org/abs/2509.13802", "authors": ["Takuya Kiyokawa", "Ryunosuke Takebayashi", "Kensuke Harada"], "title": "Shell-Type Soft Jig for Holding Objects during Disassembly", "comment": "6 pages, 8 figures", "summary": "This study addresses a flexible holding tool for robotic disassembly. We\npropose a shell-type soft jig that securely and universally holds objects,\nmitigating the risk of component damage and adapting to diverse shapes while\nenabling soft fixation that is robust to recognition, planning, and control\nerrors. The balloon-based holding mechanism ensures proper alignment and stable\nholding performance, thereby reducing the need for dedicated jig design, highly\naccurate perception, precise grasping, and finely tuned trajectory planning\nthat are typically required with conventional fixtures. Our experimental\nresults demonstrate the practical feasibility of the proposed jig through\nperformance comparisons with a vise and a jamming-gripper-inspired soft jig.\nTests on ten different objects further showed representative successes and\nfailures, clarifying the jig's limitations and outlook.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u8f6f\u5939\u5177\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u62c6\u5378\uff0c\u51cf\u5c11\u5bf9\u9ad8\u7cbe\u5ea6\u5de5\u5177\u7684\u9700\u6c42\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u5939\u5177\u5728\u673a\u5668\u4eba\u62c6\u5378\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u5bfc\u81f4\u7684\u7ec4\u4ef6\u635f\u574f\u4ee5\u53ca\u5bf9\u9ad8\u7cbe\u5ea6\u8bc6\u522b\u3001\u89c4\u5212\u548c\u63a7\u5236\u7684\u4f9d\u8d56\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u5939\u5177\u3002", "method": "\u91c7\u7528\u6c14\u7403\u5f0f\u5939\u6301\u673a\u5236\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5916\u58f3\u578b\u8f6f\u5939\u5177\uff0c\u80fd\u591f\u5b89\u5168\u4e14\u901a\u7528\u5730\u5939\u6301\u7269\u4f53\uff0c\u9002\u5e94\u591a\u79cd\u5f62\u72b6\uff0c\u5e76\u5b9e\u73b0\u8f6f\u56fa\u5b9a\uff0c\u51cf\u5c11\u4e86\u5bf9\u4e13\u7528\u5939\u5177\u8bbe\u8ba1\u3001\u9ad8\u7cbe\u5ea6\u611f\u77e5\u3001\u7cbe\u786e\u6293\u53d6\u548c\u7cbe\u7ec6\u8f68\u8ff9\u89c4\u5212\u7684\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8f6f\u5939\u5177\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5939\u5177\uff08\u5982\u864e\u94b3\uff09\u548c\u53e6\u4e00\u79cd\u8f6f\u5939\u5177\uff08\u53d7\u5835\u585e\u5939\u6301\u5668\u542f\u53d1\uff09\uff0c\u5728\u5341\u79cd\u4e0d\u540c\u7269\u4f53\u4e0a\u7684\u6d4b\u8bd5\u5c55\u793a\u4e86\u5176\u4ee3\u8868\u6027\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u673a\u5668\u4eba\u62c6\u5378\u7684\u67d4\u6027\u5939\u5177\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u660e\u786e\u4e86\u5176\u5c40\u9650\u6027\u548c\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2509.13662", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13662", "abs": "https://arxiv.org/abs/2509.13662", "authors": ["Yulan Guo", "Longguang Wang", "Wendong Mao", "Xiaoyu Dong", "Yingqian Wang", "Li Liu", "Wei An"], "title": "Deep Lookup Network", "comment": null, "summary": "Convolutional neural networks are constructed with massive operations with\ndifferent types and are highly computationally intensive. Among these\noperations, multiplication operation is higher in computational complexity and\nusually requires {more} energy consumption with longer inference time than\nother operations, which hinders the deployment of convolutional neural networks\non mobile devices. In many resource-limited edge devices, complicated\noperations can be calculated via lookup tables to reduce computational cost.\nMotivated by this, in this paper, we introduce a generic and efficient lookup\noperation which can be used as a basic operation for the construction of neural\nnetworks. Instead of calculating the multiplication of weights and activation\nvalues, simple yet efficient lookup operations are adopted to compute their\nresponses. To enable end-to-end optimization of the lookup operation, we\nconstruct the lookup tables in a differentiable manner and propose several\ntraining strategies to promote their convergence. By replacing computationally\nexpensive multiplication operations with our lookup operations, we develop\nlookup networks for the image classification, image super-resolution, and point\ncloud classification tasks. It is demonstrated that our lookup networks can\nbenefit from the lookup operations to achieve higher efficiency in terms of\nenergy consumption and inference speed while maintaining competitive\nperformance to vanilla convolutional networks. Extensive experiments show that\nour lookup networks produce state-of-the-art performance on different tasks\n(both classification and regression tasks) and different data types (both\nimages and point clouds).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7528\u67e5\u627e\u64cd\u4f5c\u66ff\u4ee3\u4e58\u6cd5\u64cd\u4f5c\uff0c\u6784\u5efa\u9ad8\u6548\u67e5\u627e\u7f51\u7edc\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u6548\u7387\u3002", "motivation": "\u4e58\u6cd5\u64cd\u4f5c\u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u80fd\u8017\u5927\uff0c\u963b\u788d\u4e86\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002\u67e5\u627e\u8868\u53ef\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u56e0\u6b64\u8bba\u6587\u63a2\u7d22\u4e86\u7528\u67e5\u627e\u64cd\u4f5c\u66ff\u4ee3\u4e58\u6cd5\u64cd\u4f5c\u7684\u53ef\u884c\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u3001\u9ad8\u6548\u7684\u67e5\u627e\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7\u53ef\u5fae\u67e5\u627e\u8868\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u591a\u79cd\u8bad\u7ec3\u7b56\u7565\u4ee5\u4fc3\u8fdb\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u67e5\u627e\u7f51\u7edc\u5728\u80fd\u8017\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u6548\u7387\u66f4\u9ad8\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u8d85\u5206\u8fa8\u7387\u548c\u70b9\u4e91\u5206\u7c7b\u7b49\u4efb\u52a1\u4e0a\u4fdd\u6301\u4e86\u4e0e\u4f20\u7edf\u7f51\u7edc\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u67e5\u627e\u64cd\u4f5c\u66ff\u4ee3\u4f20\u7edf\u4e58\u6cd5\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u4f20\u7edf\u7f51\u7edc\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13815", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13815", "abs": "https://arxiv.org/abs/2509.13815", "authors": ["Takuya Kiyokawa", "Zhengtao Hu", "Weiwei Wan", "Kensuke Harada"], "title": "Soft Regrasping Tool Inspired by Jamming Gripper", "comment": "6 pages, 9 figures", "summary": "Regrasping on fixtures is a promising approach to reduce pose uncertainty in\nrobotic assembly, but conventional rigid fixtures lack adaptability and require\ndedicated designs for each part. To overcome this limitation, we propose a soft\njig inspired by the jamming transition phenomenon, which can be continuously\ndeformed to accommodate diverse object geometries. By pressing a\ntriangular-pyramid-shaped tool into the membrane and evacuating the enclosed\nair, a stable cavity is formed as a placement space. We further optimize the\nstamping depth to balance placement stability and gripper accessibility. In\nsoft-jig-based regrasping, the key challenge lies in optimizing the cavity size\nto achieve precise dropping; once the part is reliably placed, subsequent\ngrasping can be performed with reduced uncertainty. Accordingly, we conducted\ndrop experiments on ten mechanical parts of varying shapes, which achieved\nplacement success rates exceeding 80% for most objects and above 90% for\ncylindrical ones, while failures were mainly caused by geometric constraints\nand membrane properties. These results demonstrate that the proposed jig\nenables general-purpose, accurate, and repeatable regrasping, while also\nclarifying its current limitations and future potential as a practical\nalternative to rigid fixtures in assembly automation.", "AI": {"tldr": "A soft jig using jamming transition enables adaptable, accurate regrasping for robotic assembly, achieving high success rates but with some geometric limitations.", "motivation": "To overcome the lack of adaptability and dedicated designs required by conventional rigid fixtures in robotic assembly, reducing pose uncertainty.", "method": "The method involves using a soft jig that can be deformed to accommodate diverse object geometries by pressing a triangular-pyramid-shaped tool into the membrane and evacuating the enclosed air to form a stable cavity. The stamping depth is optimized to balance placement stability and gripper accessibility.", "result": "Drop experiments on ten mechanical parts achieved placement success rates exceeding 80% for most objects and above 90% for cylindrical ones, with failures mainly due to geometric constraints and membrane properties.", "conclusion": "The proposed soft jig, inspired by jamming transition, enables general-purpose, accurate, and repeatable regrasping, offering a practical alternative to rigid fixtures in assembly automation, though it has some limitations."}}
{"id": "2509.13676", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13676", "abs": "https://arxiv.org/abs/2509.13676", "authors": ["Xiaobo Yang", "Xiaojin Gong"], "title": "Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation", "comment": null, "summary": "Recently, Referring Image Segmentation (RIS) frameworks that pair the\nMultimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)\nhave achieved impressive results. However, adapting MLLM to segmentation is\ncomputationally intensive, primarily due to visual token redundancy. We observe\nthat traditional patch-wise visual projectors struggle to strike a balance\nbetween reducing the number of visual tokens and preserving semantic clarity,\noften retaining overly long token sequences to avoid performance drops.\nInspired by text tokenizers, we propose a novel semantic visual projector that\nleverages semantic superpixels generated by SAM to identify \"visual words\" in\nan image. By compressing and projecting semantic superpixels as visual tokens,\nour approach adaptively shortens the token sequence according to scene\ncomplexity while minimizing semantic loss in compression. To mitigate loss of\ninformation, we propose a semantic superpixel positional embedding to\nstrengthen MLLM's awareness of superpixel geometry and position, alongside a\nsemantic superpixel aggregator to preserve both fine-grained details inside\nsuperpixels and global context outside. Experiments show that our method cuts\nvisual tokens by 93% without compromising performance, notably speeding up MLLM\ntraining and inference, and outperforming existing compressive visual\nprojectors on RIS.", "AI": {"tldr": "\u901a\u8fc7\u8bed\u4e49\u8d85\u50cf\u7d20\u538b\u7f29\u89c6\u89c9\u4ee4\u724c\uff0c\u65b0\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c1193%\u4ee4\u724c\uff0c\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u8865\u4e01\u7684\u89c6\u89c9\u6295\u5f71\u5668\u5728\u51cf\u5c11\u89c6\u89c9\u4ee4\u724c\u6570\u91cf\u548c\u4fdd\u6301\u8bed\u4e49\u6e05\u6670\u5ea6\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u89c6\u89c9\u6295\u5f71\u5668\uff0c\u5229\u7528SAM\u751f\u6210\u7684\u8bed\u4e49\u8d85\u50cf\u7d20\u8bc6\u522b\u56fe\u50cf\u4e2d\u7684\u201c\u89c6\u89c9\u8bcd\u201d\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u8d85\u50cf\u7d20\u4f4d\u7f6e\u5d4c\u5165\u548c\u805a\u5408\u5668\u4fdd\u7559\u7ec6\u8282\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u89c6\u89c9\u4ee4\u724c\u6570\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\uff0c\u5e76\u5728RIS\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u538b\u7f29\u89c6\u89c9\u6295\u5f71\u5668\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bed\u4e49\u89c6\u89c9\u6295\u5f71\u5668\u901a\u8fc7\u5229\u7528SAM\u751f\u6210\u7684\u8bed\u4e49\u8d85\u50cf\u7d20\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u89c6\u89c9\u4ee4\u724c\u6570\u91cf\uff08\u51cf\u5c1193%\uff09\u800c\u4e0d\u5f71\u54cd\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u5347\u4e86MLLM\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2509.13816", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13816", "abs": "https://arxiv.org/abs/2509.13816", "authors": ["Yude Li", "Zhexuan Zhou", "Huizhe Li", "Youmin Gong", "Jie Mei"], "title": "Agile in the Face of Delay: Asynchronous End-to-End Learning for Real-World Aerial Navigation", "comment": null, "summary": "Robust autonomous navigation for Autonomous Aerial Vehicles (AAVs) in complex\nenvironments is a critical capability. However, modern end-to-end navigation\nfaces a key challenge: the high-frequency control loop needed for agile flight\nconflicts with low-frequency perception streams, which are limited by sensor\nupdate rates and significant computational cost. This mismatch forces\nconventional synchronous models into undesirably low control rates. To resolve\nthis, we propose an asynchronous reinforcement learning framework that\ndecouples perception and control, enabling a high-frequency policy to act on\nthe latest IMU state for immediate reactivity, while incorporating perception\nfeatures asynchronously. To manage the resulting data staleness, we introduce a\ntheoretically-grounded Temporal Encoding Module (TEM) that explicitly\nconditions the policy on perception delays, a strategy complemented by a\ntwo-stage curriculum to ensure stable and efficient training. Validated in\nextensive simulations, our method was successfully deployed in zero-shot\nsim-to-real transfer on an onboard NUC, where it sustains a 100~Hz control rate\nand demonstrates robust, agile navigation in cluttered real-world environments.\nOur source code will be released for community reference.", "AI": {"tldr": "\u63d0\u51fa\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u81ea\u4e3b\u98de\u884c\u5668\u9ad8\u9891\u7387\u63a7\u5236\u4e0e\u4f4e\u9891\u7387\u611f\u77e5\u51b2\u7a81\uff0c\u901a\u8fc7TEM\u548c\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\uff0c\u5b9e\u9645\u9a8c\u8bc1\u4e2d\u8868\u73b0\u9c81\u68d2\u4e14\u654f\u6377\u3002", "motivation": "\u73b0\u4ee3\u7aef\u5230\u7aef\u5bfc\u822a\u7cfb\u7edf\u9762\u4e34\u9ad8\u9891\u7387\u63a7\u5236\u9700\u6c42\u4e0e\u4f4e\u9891\u7387\u611f\u77e5\u6d41\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5bfc\u81f4\u63a7\u5236\u901f\u7387\u53d7\u9650\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u5f02\u6b65\u6846\u67b6\u4ee5\u63d0\u5347\u81ea\u4e3b\u98de\u884c\u5668\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u611f\u77e5\u4e0e\u63a7\u5236\u89e3\u8026\uff0c\u5141\u8bb8\u9ad8\u9891\u7387\u7b56\u7565\u57fa\u4e8e\u6700\u65b0IMU\u72b6\u6001\u884c\u52a8\uff0c\u540c\u65f6\u5f02\u6b65\u6574\u5408\u611f\u77e5\u7279\u5f81\u3002\u91c7\u7528Temporal Encoding Module\uff08TEM\uff09\u7ba1\u7406\u6570\u636e\u5ef6\u8fdf\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u786e\u4fdd\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "result": "\u5728\u5e7f\u6cdb\u6a21\u62df\u4e2d\u9a8c\u8bc1\u540e\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u96f6\u6837\u672c\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u6210\u529f\u90e8\u7f72\uff0c\u7ef4\u6301100Hz\u63a7\u5236\u9891\u7387\uff0c\u5e76\u5728\u6742\u4e71\u73b0\u5b9e\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u9c81\u68d2\u4e14\u654f\u6377\u7684\u5bfc\u822a\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u4e3b\u98de\u884c\u5668\u5728\u590d\u6742\u73af\u5883\u4e2d\u9ad8\u9891\u7387\u63a7\u5236\u4e0e\u4f4e\u9891\u7387\u611f\u77e5\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u901a\u8fc7\u7406\u8bba\u652f\u6301\u7684Temporal Encoding Module\u548c\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\uff0c\u5e76\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u654f\u6377\u6027\u3002"}}
{"id": "2509.13681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13681", "abs": "https://arxiv.org/abs/2509.13681", "authors": ["Hang Li", "Dianmo Sheng", "Qiankun Dong", "Zichun Wang", "Zhiwei Xu", "Tao Li"], "title": "FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras", "comment": "8 pages, 4 figures", "summary": "As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)\nsegmentation has recently achieved remarkable progress with pinhole cameras.\nHowever, it is non-trivial to extend the existing methods to fisheye cameras\nwith severe geometric distortion, ambiguous multi-view correspondences and\nunstable temporal dynamics, all of which significantly degrade BEV performance.\nTo address these challenges, we propose FishBEV, a novel BEV segmentation\nframework specifically tailored for fisheye cameras. This framework introduces\nthree complementary innovations, including a Distortion-Resilient Multi-scale\nExtraction (DRME) backbone that learns robust features under distortion while\npreserving scale consistency, an Uncertainty-aware Spatial Cross-Attention\n(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view\nalignment, a Distance-aware Temporal Self-Attention (D-TSA) module that\nadaptively balances near field details and far field context to ensure temporal\ncoherence. Extensive experiments on the Synwoodscapes dataset demonstrate that\nFishBEV consistently outperforms SOTA baselines, regarding the performance\nevaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.", "AI": {"tldr": "FishBEV\u662f\u4e3a\u9c7c\u773c\u76f8\u673a\u8bbe\u8ba1\u7684BEV\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7DRME\u3001U-SCA\u548cD-TSA\u4e09\u4e2a\u521b\u65b0\u6a21\u5757\u89e3\u51b3\u4e86\u5931\u771f\u3001\u8de8\u89c6\u56fe\u5bf9\u9f50\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7b49\u95ee\u9898\uff0c\u5728Synwoodscapes\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eSOTA\u3002", "motivation": "\u9c7c\u773c\u76f8\u673a\u5728BEV\u5206\u5272\u4e2d\u5b58\u5728\u4e25\u91cd\u51e0\u4f55\u5931\u771f\u3001\u591a\u89c6\u56fe\u5bf9\u5e94\u6a21\u7cca\u548c\u65f6\u95f4\u52a8\u6001\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002FishBEV\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "FishBEV\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) DRME\u9aa8\u5e72\u7f51\u7edc\uff0c\u7528\u4e8e\u5728\u5931\u771f\u60c5\u51b5\u4e0b\u5b66\u4e60\u9c81\u68d2\u7279\u5f81\u5e76\u4fdd\u6301\u5c3a\u5ea6\u4e00\u81f4\u6027\uff1b2) U-SCA\u673a\u5236\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5b9e\u73b0\u53ef\u9760\u7684\u8de8\u89c6\u56fe\u5bf9\u9f50\uff1b3) D-TSA\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u5e73\u8861\u8fd1\u573a\u7ec6\u8282\u548c\u8fdc\u573a\u4e0a\u4e0b\u6587\u4ee5\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728Synwoodscapes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFishBEV\u5728\u73af\u7ed5\u89c6\u56fe\u9c7c\u773cBEV\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u3002", "conclusion": "FishBEV\u901a\u8fc7\u5f15\u5165DRME\u3001U-SCA\u548cD-TSA\u4e09\u4e2a\u521b\u65b0\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9c7c\u773c\u76f8\u673a\u5728BEV\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5728Synwoodscapes\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u3002"}}
{"id": "2509.13827", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.13827", "abs": "https://arxiv.org/abs/2509.13827", "authors": ["Renyuan Liu", "Haoting Zhou", "Chuankai Fang", "Qinbing Fu"], "title": "How Fly Neural Perception Mechanisms Enhance Visuomotor Control of Micro Robots", "comment": "9 pages, 6 figures", "summary": "Anyone who has tried to swat a fly has likely been frustrated by its\nremarkable agility.This ability stems from its visual neural perception system,\nparticularly the collision-selective neurons within its small brain.For\nautonomous robots operating in complex and unfamiliar environments, achieving\nsimilar agility is highly desirable but often constrained by the trade-off\nbetween computational cost and performance.In this context, insect-inspired\nintelligence offers a parsimonious route to low-power, computationally\nefficient frameworks.In this paper, we propose an attention-driven visuomotor\ncontrol strategy inspired by a specific class of fly visual projection\nneurons-the lobula plate/lobula column type-2 (LPLC2)-and their associated\nescape behaviors.To our knowledge, this represents the first embodiment of an\nLPLC2 neural model in the embedded vision of a physical mobile robot, enabling\ncollision perception and reactive evasion.The model was simplified and\noptimized at 70KB in memory to suit the computational constraints of a\nvision-based micro robot, the Colias, while preserving key neural perception\nmechanisms.We further incorporated multi-attention mechanisms to emulate the\ndistributed nature of LPLC2 responses, allowing the robot to detect and react\nto approaching targets both rapidly and selectively.We systematically evaluated\nthe proposed method against a state-of-the-art locust-inspired collision\ndetection model.Results showed that the fly-inspired visuomotor model achieved\ncomparable robustness, at success rate of 96.1% in collision detection while\nproducing more adaptive and elegant evasive maneuvers.Beyond demonstrating an\neffective collision-avoidance strategy, this work highlights the potential of\nfly-inspired neural models for advancing research into collective behaviors in\ninsect intelligence.", "AI": {"tldr": "\u53d7\u82cd\u8747\u89c6\u89c9\u795e\u7ecf\u7cfb\u7edf\u542f\u53d1\uff0c\u63d0\u51fa\u4f4e\u529f\u8017\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u5e94\u7528\u4e8e\u5fae\u578b\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u9ad8\u6548\u78b0\u649e\u907f\u969c\uff0c\u5e76\u5c55\u793a\u7fa4\u4f53\u884c\u4e3a\u7814\u7a76\u6f5c\u529b\u3002", "motivation": "\u53d7\u82cd\u8747\u89c6\u89c9\u795e\u7ecf\u7cfb\u7edf\u7684\u542f\u53d1\uff0c\u4e3a\u89e3\u51b3\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u654f\u6377\u6027\u4e0e\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u63a2\u7d22\u6606\u866b\u667a\u80fd\u7684\u4f4e\u529f\u8017\u9ad8\u6548\u8ba1\u7b97\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLPLC2\u795e\u7ecf\u5143\u6a21\u578b\u7684\u6ce8\u610f\u529b\u9a71\u52a8\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u4f18\u5316\u4e3a70KB\u5185\u5b58\u4ee5\u9002\u5e94\u5fae\u578b\u673a\u5668\u4ebaColias\u7684\u8ba1\u7b97\u9650\u5236\uff0c\u5e76\u6574\u5408\u591a\u6ce8\u610f\u529b\u673a\u5236\u6a21\u62dfLPLC2\u7684\u5206\u5e03\u5f0f\u54cd\u5e94\u7279\u6027\u3002", "result": "\u4e0e\u8757\u866b\u542f\u53d1\u7684\u78b0\u649e\u68c0\u6d4b\u6a21\u578b\u76f8\u6bd4\uff0c\u82cd\u8747\u542f\u53d1\u7684\u6a21\u578b\u5728\u78b0\u649e\u68c0\u6d4b\u4e2d\u8fbe\u523096.1%\u7684\u6210\u529f\u7387\uff0c\u5e76\u8868\u73b0\u51fa\u66f4\u9002\u5e94\u548c\u4f18\u96c5\u7684\u907f\u969c\u52a8\u4f5c\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u82cd\u8747\u89c6\u89c9\u795e\u7ecf\u7cfb\u7edf\u7684LPLC2\u795e\u7ecf\u5143\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u529f\u8017\u3001\u9ad8\u6548\u8ba1\u7b97\u7684\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u5fae\u578b\u673a\u5668\u4ebaColias\uff0c\u5b9e\u73b0\u4e8696.1%\u7684\u78b0\u649e\u68c0\u6d4b\u6210\u529f\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u7fa4\u4f53\u884c\u4e3a\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13687", "abs": "https://arxiv.org/abs/2509.13687", "authors": ["Kaniz Fatema", "Emad A. Mohammed", "Sukhjit Singh Sehra"], "title": "Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification", "comment": null, "summary": "Effective and interpretable classification of medical images is a challenge\nin computer-aided diagnosis, especially in resource-limited clinical settings.\nThis study introduces spline-based Kolmogorov-Arnold Networks (KANs) for\naccurate medical image classification with limited, diverse datasets. The\nmodels include SBTAYLOR-KAN, integrating B-splines with Taylor series;\nSBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,\nembedding B-splines in Morlet wavelet transforms. These approaches leverage\nspline-based function approximation to capture both local and global\nnonlinearities. The models were evaluated on brain MRI, chest X-rays,\ntuberculosis X-rays, and skin lesion images without preprocessing,\ndemonstrating the ability to learn directly from raw data. Extensive\nexperiments, including cross-dataset validation and data reduction analysis,\nshowed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%\naccuracy, with a balanced F1-score, maintaining over 86% accuracy using only\n30% of the training data across three datasets. Despite class imbalance in the\nskin cancer dataset, experiments on both imbalanced and balanced versions\nshowed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.\nUnlike traditional CNNs, which require millions of parameters (e.g., ResNet50\nwith 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872\ntrainable parameters, making it more suitable for constrained medical\nenvironments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used\nfor interpretability, highlighting relevant regions in medical images. This\nframework provides a lightweight, interpretable, and generalizable solution for\nmedical image classification, addressing the challenges of limited datasets and\ndata-scarce scenarios in clinical AI applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6837\u6761\u7684\u8f7b\u91cf\u7ea7\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\uff0c\u5728\u5c0f\u6837\u672c\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u5177\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5728\u8d44\u6e90\u6709\u9650\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9002\u7528\u4e8e\u5c0f\u6837\u672c\u548c\u591a\u6837\u5316\u6570\u636e\u96c6\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u57fa\u4e8e\u6837\u6761\u7684Kolmogorov-Arnold Networks\uff08KANs\uff09\uff0c\u5305\u62ecSBTAYLOR-KAN\u3001SBRBF-KAN\u548cSBWAVELET-KAN\uff0c\u5229\u7528\u6837\u6761\u57fa\u51fd\u6570\u903c\u8fd1\u6280\u672f\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u975e\u7ebf\u6027\u7279\u5f81\u3002", "result": "SBTAYLOR-KAN\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe98.93%\uff0c\u4e14\u4ec5\u97002,872\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfCNN\u6a21\u578b\uff08\u5982ResNet50\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u4e34\u5e8a\u73af\u5883\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u5c0f\u6837\u672c\u7684\u6311\u6218\u3002"}}
{"id": "2509.13832", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13832", "abs": "https://arxiv.org/abs/2509.13832", "authors": ["Teng Wang", "Haojun Jiang", "Yuxuan Wang", "Zhenguo Sun", "Xiangjie Yan", "Xiang Li", "Gao Huang"], "title": "UltraHiT: A Hierarchical Transformer Architecture for Generalizable Internal Carotid Artery Robotic Ultrasonography", "comment": null, "summary": "Carotid ultrasound is crucial for the assessment of cerebrovascular health,\nparticularly the internal carotid artery (ICA). While previous research has\nexplored automating carotid ultrasound, none has tackled the challenging ICA.\nThis is primarily due to its deep location, tortuous course, and significant\nindividual variations, which greatly increase scanning complexity. To address\nthis, we propose a Hierarchical Transformer-based decision architecture, namely\nUltraHiT, that integrates high-level variation assessment with low-level action\ndecision. Our motivation stems from conceptualizing individual vascular\nstructures as morphological variations derived from a standard vascular model.\nThe high-level module identifies variation and switches between two low-level\nmodules: an adaptive corrector for variations, or a standard executor for\nnormal cases. Specifically, both the high-level module and the adaptive\ncorrector are implemented as causal transformers that generate predictions\nbased on the historical scanning sequence. To ensure generalizability, we\ncollected the first large-scale ICA scanning dataset comprising 164\ntrajectories and 72K samples from 28 subjects of both genders. Based on the\nabove innovations, our approach achieves a 95% success rate in locating the ICA\non unseen individuals, outperforming baselines and demonstrating its\neffectiveness. Our code will be released after acceptance.", "AI": {"tldr": "\u63d0\u51fa UltraHiT \u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42 Transformer \u67b6\u6784\u89e3\u51b3 ICA \u626b\u63cf\u96be\u9898\uff0c\u6210\u529f\u7387\u8fbe 95%\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u5c06\u4e2a\u4f53\u8840\u7ba1\u7ed3\u6784\u89c6\u4e3a\u6807\u51c6\u8840\u7ba1\u6a21\u578b\u7684\u5f62\u6001\u53d8\u5f02\uff0c\u4ece\u800c\u89e3\u51b3 ICA \u626b\u63cf\u4e2d\u7684\u590d\u6742\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5206\u5c42 Transformer \u7684\u51b3\u7b56\u67b6\u6784 UltraHiT\uff0c\u7ed3\u5408\u9ad8\u5c42\u53d8\u5f02\u8bc4\u4f30\u4e0e\u4f4e\u5c42\u884c\u52a8\u51b3\u7b56\u3002\u9ad8\u5c42\u6a21\u5757\u8bc6\u522b\u53d8\u5f02\u5e76\u5207\u6362\u81f3\u4e24\u4e2a\u4f4e\u5c42\u6a21\u5757\uff1a\u81ea\u9002\u5e94\u6821\u6b63\u5668\u6216\u6807\u51c6\u6267\u884c\u5668\u3002\u4e24\u8005\u5747\u91c7\u7528\u56e0\u679c Transformer \u5b9e\u73b0\uff0c\u57fa\u4e8e\u5386\u53f2\u626b\u63cf\u5e8f\u5217\u751f\u6210\u9884\u6d4b\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u4e2a\u4f53\u4e0a\uff0cUltraHiT \u6210\u529f\u5b9a\u4f4d ICA \u7684\u6210\u529f\u7387\u8fbe\u5230 95%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "UltraHiT \u65b9\u6cd5\u5728\u5b9a\u4f4d ICA \u65b9\u9762\u53d6\u5f97\u4e86 95% \u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.13711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13711", "abs": "https://arxiv.org/abs/2509.13711", "authors": ["Qiuyu Tang", "Joshua Krinsky", "Aparna Bharati"], "title": "StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models", "comment": null, "summary": "The rapid advancement of generative models, particularly diffusion-based\napproaches, has inadvertently facilitated their potential for misuse. Such\nmodels enable malicious exploiters to replicate artistic styles that capture an\nartist's creative labor, personal vision, and years of dedication in an\ninexpensive manner. This has led to a rise in the need and exploration of\nmethods for protecting artworks against style mimicry. Although generic\ndiffusion models can easily mimic an artistic style, finetuning amplifies this\ncapability, enabling the model to internalize and reproduce the style with\nhigher fidelity and control. We hypothesize that certain cross-attention layers\nexhibit heightened sensitivity to artistic styles. Sensitivity is measured\nthrough activation strengths of attention layers in response to style and\ncontent representations, and assessing their correlations with features\nextracted from external models. Based on our findings, we introduce an\nefficient and lightweight protection strategy, StyleProtect, that achieves\neffective style defense against fine-tuned diffusion models by updating only\nselected cross-attention layers. Our experiments utilize a carefully curated\nartwork dataset based on WikiArt, comprising representative works from 30\nartists known for their distinctive and influential styles and cartoon\nanimations from the Anita dataset. The proposed method demonstrates promising\nperformance in safeguarding unique styles of artworks and anime from malicious\ndiffusion customization, while maintaining competitive imperceptibility.", "AI": {"tldr": "StyleProtect\u901a\u8fc7\u66f4\u65b0\u9009\u5b9a\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\uff0c\u6709\u6548\u9632\u5fa1\u7ec6\u8c03\u6269\u6563\u6a21\u578b\u5bf9\u827a\u672f\u98ce\u683c\u7684\u6a21\u4eff\uff0c\u4fdd\u62a4\u827a\u672f\u54c1\u514d\u53d7\u6ee5\u7528\u3002", "motivation": "\u751f\u6210\u6a21\u578b\uff08\u5c24\u5176\u662f\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff09\u7684\u5feb\u901f\u53d1\u5c55\u53ef\u80fd\u88ab\u6ee5\u7528\uff0c\u6a21\u4eff\u827a\u672f\u5bb6\u7684\u72ec\u7279\u98ce\u683c\uff0c\u4fb5\u72af\u5176\u521b\u610f\u52b3\u52a8\u548c\u4e2a\u4eba\u613f\u666f\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u4fdd\u62a4\u827a\u672f\u54c1\u514d\u53d7\u98ce\u683c\u6a21\u4eff\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u5bf9\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u5bf9\u827a\u672f\u98ce\u683c\u654f\u611f\u6027\u7684\u6d4b\u91cf\uff0c\u63d0\u51faStyleProtect\u7b56\u7565\uff0c\u901a\u8fc7\u66f4\u65b0\u9009\u5b9a\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u6765\u5b9e\u73b0\u98ce\u683c\u9632\u5fa1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cStyleProtect\u5728\u4fdd\u62a4\u72ec\u7279\u827a\u672f\u98ce\u683c\u548c\u52a8\u6f2b\u514d\u53d7\u6076\u610f\u6269\u6563\u5b9a\u5236\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u4e0d\u53ef\u5bdf\u89c9\u6027\u3002", "conclusion": "StyleProtect\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684\u4fdd\u62a4\u7b56\u7565\uff0c\u901a\u8fc7\u4ec5\u66f4\u65b0\u9009\u5b9a\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\uff0c\u6709\u6548\u9632\u5fa1\u9488\u5bf9\u7ec6\u8c03\u6269\u6563\u6a21\u578b\u7684\u98ce\u683c\u6a21\u4eff\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u7684\u4e0d\u53ef\u5bdf\u89c9\u6027\u3002"}}
{"id": "2509.13833", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13833", "abs": "https://arxiv.org/abs/2509.13833", "authors": ["Zhikai Zhang", "Jun Guo", "Chao Chen", "Jilong Wang", "Chenghuai Lin", "Yunrui Lian", "Han Xue", "Zhenrong Wang", "Maoqi Liu", "Huaping Liu", "He Wang", "Li Yi"], "title": "Track Any Motions under Any Disturbances", "comment": null, "summary": "A foundational humanoid motion tracker is expected to be able to track\ndiverse, highly dynamic, and contact-rich motions. More importantly, it needs\nto operate stably in real-world scenarios against various dynamics\ndisturbances, including terrains, external forces, and physical property\nchanges for general practical use. To achieve this goal, we propose Any2Track\n(Track Any motions under Any disturbances), a two-stage RL framework to track\nvarious motions under multiple disturbances in the real world. Any2Track\nreformulates dynamics adaptability as an additional capability on top of basic\naction execution and consists of two key components: AnyTracker and AnyAdapter.\nAnyTracker is a general motion tracker with a series of careful designs to\ntrack various motions within a single policy. AnyAdapter is a history-informed\nadaptation module that endows the tracker with online dynamics adaptability to\novercome the sim2real gap and multiple real-world disturbances. We deploy\nAny2Track on Unitree G1 hardware and achieve a successful sim2real transfer in\na zero-shot manner. Any2Track performs exceptionally well in tracking various\nmotions under multiple real-world disturbances.", "AI": {"tldr": "Any2Track \u662f\u4e00\u4e2a\u4e24\u9636\u6bb5 RL \u6846\u67b6\uff0c\u7528\u4e8e\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u8ddf\u8e2a\u5404\u79cd\u52a8\u4f5c\u5e76\u5e94\u5bf9\u591a\u79cd\u5e72\u6270\u3002\u5b83\u5305\u542b\u901a\u7528\u7684\u52a8\u4f5c\u8ddf\u8e2a\u5668\u548c\u9002\u5e94\u6a21\u5757\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u7684 sim2real \u8fc1\u79fb\u3002", "motivation": "\u4e3a\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7a33\u5b9a\u5730\u8ddf\u8e2a\u591a\u6837\u5316\u3001\u9ad8\u5ea6\u52a8\u6001\u4e14\u63a5\u89e6\u4e30\u5bcc\u7684\u52a8\u4f5c\uff0c\u5e76\u5e94\u5bf9\u5404\u79cd\u52a8\u6001\u5e72\u6270\uff08\u5982\u5730\u5f62\u3001\u5916\u529b\u548c\u7269\u7406\u5c5e\u6027\u53d8\u5316\uff09\uff0c\u63d0\u51fa\u4e86 Any2Track\u3002", "method": "Any2Track \u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684 RL \u6846\u67b6\uff0c\u5305\u542b AnyTracker \u548c AnyAdapter \u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\u3002AnyTracker \u662f\u4e00\u4e2a\u901a\u7528\u7684\u52a8\u4f5c\u8ddf\u8e2a\u5668\uff0c\u80fd\u591f\u901a\u8fc7\u5355\u4e00\u7b56\u7565\u8ddf\u8e2a\u5404\u79cd\u52a8\u4f5c\uff1bAnyAdapter \u662f\u4e00\u4e2a\u57fa\u4e8e\u5386\u53f2\u7684\u9002\u5e94\u6a21\u5757\uff0c\u8d4b\u4e88\u8ddf\u8e2a\u5668\u5728\u7ebf\u52a8\u6001\u9002\u5e94\u80fd\u529b\u3002", "result": "Any2Track \u5728 Unitree G1 \u786c\u4ef6\u4e0a\u6210\u529f\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u7684 sim2real \u8fc1\u79fb\uff0c\u5e76\u5728\u591a\u79cd\u73b0\u5b9e\u4e16\u754c\u5e72\u6270\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Any2Track \u6210\u529f\u5730\u5728 Unitree G1 \u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u7684 sim2real \u8fc1\u79fb\uff0c\u5e76\u5728\u591a\u79cd\u73b0\u5b9e\u4e16\u754c\u5e72\u6270\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u8ddf\u8e2a\u5404\u79cd\u52a8\u4f5c\u3002"}}
{"id": "2509.13713", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13713", "abs": "https://arxiv.org/abs/2509.13713", "authors": ["Tae-Wook Um", "Ki-Hyeon Kim", "Hyun-Duck Choi", "Hyo-Sung Ahn"], "title": "UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry", "comment": null, "summary": "Monocular depth estimation has been increasingly adopted in robotics and\nautonomous driving for its ability to infer scene geometry from a single\ncamera. In self-supervised monocular depth estimation frameworks, the network\njointly generates and exploits depth and pose estimates during training,\nthereby eliminating the need for depth labels. However, these methods remain\nchallenged by uncertainty in the input data, such as low-texture or dynamic\nregions, which can cause reduced depth accuracy. To address this, we introduce\nUM-Depth, a framework that combines motion- and uncertainty-aware refinement to\nenhance depth accuracy at dynamic object boundaries and in textureless regions.\nSpecifically, we develop a teacherstudent training strategy that embeds\nuncertainty estimation into both the training pipeline and network\narchitecture, thereby strengthening supervision where photometric signals are\nweak. Unlike prior motion-aware approaches that incur inference-time overhead\nand rely on additional labels or auxiliary networks for real-time generation,\nour method uses optical flow exclusively within the teacher network during\ntraining, which eliminating extra labeling demands and any runtime cost.\nExtensive experiments on the KITTI and Cityscapes datasets demonstrate the\neffectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves\nstate-of-the-art results in both self-supervised depth and pose estimation on\nthe KITTI datasets.", "AI": {"tldr": "UM-Depth \u662f\u4e00\u79cd\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u52a8\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7ec6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u7269\u4f53\u8fb9\u754c\u548c\u65e0\u7eb9\u7406\u533a\u57df\u7684\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u6807\u7b7e\u6216\u63a8\u7406\u5f00\u9500\u3002", "motivation": "\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728\u4f4e\u7eb9\u7406\u6216\u52a8\u6001\u533a\u57df\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u6df1\u5ea6\u7cbe\u5ea6\u4e0b\u964d\u3002UM-Depth \u65e8\u5728\u901a\u8fc7\u8fd0\u52a8\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7ec6\u5316\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "UM-Depth \u91c7\u7528\u4e86\u6559\u5e08-\u5b66\u751f\u8bad\u7ec3\u7b56\u7565\uff0c\u5c06\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5d4c\u5165\u8bad\u7ec3\u6d41\u7a0b\u548c\u7f51\u7edc\u67b6\u6784\u4e2d\uff0c\u540c\u65f6\u4ec5\u5728\u6559\u5e08\u7f51\u7edc\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u5149\u6d41\uff0c\u907f\u514d\u4e86\u63a8\u7406\u65f6\u7684\u989d\u5916\u5f00\u9500\u548c\u6807\u7b7e\u9700\u6c42\u3002", "result": "\u5728 KITTI \u548c Cityscapes \u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUM-Depth \u5728\u52a8\u6001\u7269\u4f53\u8fb9\u754c\u548c\u65e0\u7eb9\u7406\u533a\u57df\u7684\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "UM-Depth \u5728 KITTI \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u81ea\u76d1\u7763\u6df1\u5ea6\u548c\u59ff\u6001\u4f30\u8ba1\u7684\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u901a\u8fc7\u7ed3\u5408\u8fd0\u52a8\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7ec6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u7269\u4f53\u8fb9\u754c\u548c\u65e0\u7eb9\u7406\u533a\u57df\u7684\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u3002"}}
{"id": "2509.13839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13839", "abs": "https://arxiv.org/abs/2509.13839", "authors": ["Motonari Kambara", "Komei Sugiura"], "title": "Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models", "comment": "Published in Advanced Robotics", "summary": "In this work, we address the problem of predicting the future success of\nopen-vocabulary object manipulation tasks. Conventional approaches typically\ndetermine success or failure after the action has been carried out. However,\nthey make it difficult to prevent potential hazards and rely on failures to\ntrigger replanning, thereby reducing the efficiency of object manipulation\nsequences. To overcome these challenges, we propose a model, which predicts the\nalignment between a pre-manipulation egocentric image with the planned\ntrajectory and a given natural language instruction. We introduce a Multi-Level\nTrajectory Fusion module, which employs a state-of-the-art deep state-space\nmodel and a transformer encoder in parallel to capture multi-level time-series\nself-correlation within the end effector trajectory. Our experimental results\nindicate that the proposed method outperformed existing methods, including\nfoundation models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u64cd\u4f5c\u4efb\u52a1\u672a\u6765\u6210\u529f\u7387\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u7ea7\u8f68\u8ff9\u878d\u5408\u6a21\u5757\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5728\u64cd\u4f5c\u5b8c\u6210\u540e\u624d\u786e\u5b9a\u6210\u529f\u6216\u5931\u8d25\uff0c\u96be\u4ee5\u9884\u9632\u6f5c\u5728\u5371\u9669\u4e14\u4f9d\u8d56\u5931\u8d25\u89e6\u53d1\u91cd\u65b0\u89c4\u5212\uff0c\u964d\u4f4e\u4e86\u5bf9\u8c61\u64cd\u4f5c\u5e8f\u5217\u7684\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u8f68\u8ff9\u878d\u5408\u6a21\u5757\uff0c\u7ed3\u5408\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u53d8\u538b\u5668\u7f16\u7801\u5668\u5e76\u884c\u5de5\u4f5c\uff0c\u4ee5\u6355\u6349\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\u7684\u591a\u5c42\u6b21\u65f6\u95f4\u5e8f\u5217\u81ea\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9884\u6d4b\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u64cd\u4f5c\u4efb\u52a1\u7684\u672a\u6765\u6210\u529f\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u64cd\u4f5c\u4efb\u52a1\u672a\u6765\u6210\u529f\u7387\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u7ea7\u8f68\u8ff9\u878d\u5408\u6a21\u5757\u6709\u6548\u6355\u6349\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\u7684\u591a\u5c42\u6b21\u65f6\u95f4\u5e8f\u5217\u81ea\u76f8\u5173\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.13722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13722", "abs": "https://arxiv.org/abs/2509.13722", "authors": ["Dingwei Zhang", "Dong Zhang", "Jinhui Tang"], "title": "Mitigating Query Selection Bias in Referring Video Object Segmentation", "comment": null, "summary": "Recently, query-based methods have achieved remarkable performance in\nReferring Video Object Segmentation (RVOS) by using textual static object\nqueries to drive cross-modal alignment. However, these static queries are\neasily misled by distractors with similar appearance or motion, resulting in\n\\emph{query selection bias}. To address this issue, we propose Triple Query\nFormer (TQF), which factorizes the referring query into three specialized\ncomponents: an appearance query for static attributes, an intra-frame\ninteraction query for spatial relations, and an inter-frame motion query for\ntemporal association. Instead of relying solely on textual embeddings, our\nqueries are dynamically constructed by integrating both linguistic cues and\nvisual guidance. Furthermore, we introduce two motion-aware aggregation modules\nthat enhance object token representations: Intra-frame Interaction Aggregation\nincorporates position-aware interactions among objects within a single frame,\nwhile Inter-frame Motion Aggregation leverages trajectory-guided alignment\nacross frames to ensure temporal coherence. Extensive experiments on multiple\nRVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our\nstructured query design and motion-aware aggregation modules.", "AI": {"tldr": "TQF\u901a\u8fc7\u5206\u89e3\u67e5\u8be2\u4e3a\u4e09\u4e2a\u7ec4\u4ef6\u5e76\u5f15\u5165\u8fd0\u52a8\u611f\u77e5\u6a21\u5757\uff0c\u89e3\u51b3\u4e86RVOS\u4e2d\u9759\u6001\u67e5\u8be2\u7684\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u67e5\u8be2\u7684\u65b9\u6cd5\u5728RVOS\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9759\u6001\u67e5\u8be2\u5bb9\u6613\u53d7\u5230\u5916\u89c2\u6216\u8fd0\u52a8\u76f8\u4f3c\u7684\u5e72\u6270\u9879\u8bef\u5bfc\uff0c\u5bfc\u81f4\u67e5\u8be2\u9009\u62e9\u504f\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86TQF\u3002", "method": "TQF\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u5916\u89c2\u67e5\u8be2\u3001\u5e27\u5185\u4ea4\u4e92\u67e5\u8be2\u548c\u5e27\u95f4\u8fd0\u52a8\u67e5\u8be2\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u8fd0\u52a8\u611f\u77e5\u805a\u5408\u6a21\u5757\uff08Intra-frame Interaction Aggregation\u548cInter-frame Motion Aggregation\uff09\u6765\u589e\u5f3a\u5bf9\u8c61\u6807\u8bb0\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2aRVOS\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86TQF\u7684\u4f18\u52bf\u53ca\u5176\u7ed3\u6784\u5316\u67e5\u8be2\u8bbe\u8ba1\u548c\u8fd0\u52a8\u611f\u77e5\u805a\u5408\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684Triple Query Former (TQF)\u901a\u8fc7\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u4e09\u4e2a\u4e13\u95e8\u7ec4\u4ef6\uff0c\u5e76\u5f15\u5165\u8fd0\u52a8\u611f\u77e5\u805a\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86Referring Video Object Segmentation (RVOS)\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13857", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13857", "abs": "https://arxiv.org/abs/2509.13857", "authors": ["Nguyen Hoang Khoi Tran", "Julie Stephany Berrio", "Mao Shan", "Stewart Worrall"], "title": "InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap", "comment": "8 pages, 5 figures", "summary": "Reliable global localization is critical for autonomous vehicles, especially\nin environments where GNSS is degraded or unavailable, such as urban canyons\nand tunnels. Although high-definition (HD) maps provide accurate priors, the\ncost of data collection, map construction, and maintenance limits scalability.\nOpenStreetMap (OSM) offers a free and globally available alternative, but its\ncoarse abstraction poses challenges for matching with sensor data. We propose\nInterKey, a cross-modal framework that leverages road intersections as\ndistinctive landmarks for global localization. Our method constructs compact\nbinary descriptors by jointly encoding road and building imprints from point\nclouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,\norientation determination, and area-equalized sampling strategies, enabling\nrobust cross-modal matching. Experiments on the KITTI dataset demonstrate that\nInterKey achieves state-of-the-art accuracy, outperforming recent baselines by\na large margin. The framework generalizes to sensors that can produce dense\nstructural point clouds, offering a scalable and cost-effective solution for\nrobust vehicle localization.", "AI": {"tldr": "InterKey\u662f\u4e00\u79cd\u5229\u7528\u9053\u8def\u4ea4\u53c9\u53e3\u4f5c\u4e3a\u5730\u6807\u7684\u8de8\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u7f16\u7801\u70b9\u4e91\u548cOSM\u6570\u636e\u5b9e\u73b0\u5168\u7403\u5b9a\u4f4d\uff0c\u5728KITTI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5728GNSS\u4fe1\u53f7\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u5982\u57ce\u5e02\u5ce1\u8c37\u548c\u96a7\u9053\uff0c\u53ef\u9760\u7684\u5168\u7403\u5b9a\u4f4d\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u9ad8\u7cbe\u5ea6\u5730\u56fe\u63d0\u4f9b\u4e86\u51c6\u786e\u7684\u5148\u9a8c\u4fe1\u606f\uff0c\u4f46\u5176\u6570\u636e\u6536\u96c6\u3001\u6784\u5efa\u548c\u7ef4\u62a4\u6210\u672c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002OpenStreetMap\uff08OSM\uff09\u63d0\u4f9b\u4e86\u514d\u8d39\u4e14\u5168\u7403\u53ef\u7528\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u7c97\u7565\u7684\u62bd\u8c61\u7ed9\u4e0e\u4f20\u611f\u5668\u6570\u636e\u7684\u5339\u914d\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u6784\u5efa\u7d27\u51d1\u7684\u4e8c\u8fdb\u5236\u63cf\u8ff0\u7b26\uff0c\u901a\u8fc7\u8054\u5408\u7f16\u7801\u70b9\u4e91\u548cOSM\u4e2d\u7684\u9053\u8def\u548c\u5efa\u7b51\u7269\u5370\u8bb0\uff0c\u5e76\u5f15\u5165\u5dee\u5f02\u7f13\u89e3\u3001\u65b9\u5411\u786e\u5b9a\u548c\u9762\u79ef\u5747\u8861\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u8de8\u6a21\u6001\u5339\u914d\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cInterKey\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u5927\u5e45\u4f18\u4e8e\u6700\u8fd1\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u80fd\u591f\u751f\u6210\u5bc6\u96c6\u7ed3\u6784\u70b9\u4e91\u7684\u4f20\u611f\u5668\u3002", "conclusion": "InterKey\u6846\u67b6\u901a\u8fc7\u5229\u7528\u9053\u8def\u4ea4\u53c9\u53e3\u4f5c\u4e3a\u663e\u8457\u5730\u6807\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9700\u8981\u53ef\u9760\u5168\u7403\u5b9a\u4f4d\u7684\u573a\u666f\uff0c\u7279\u522b\u662f\u5728GNSS\u4fe1\u53f7\u53d7\u9650\u7684\u73af\u5883\u4e2d\u3002"}}
{"id": "2509.13747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13747", "abs": "https://arxiv.org/abs/2509.13747", "authors": ["Ming Dai", "Wenxuan Cheng", "Jiang-Jiang Liu", "Lingfeng Yang", "Zhenhua Feng", "Wankou Yang", "Jingdong Wang"], "title": "Improving Generalized Visual Grounding with Instance-aware Joint Learning", "comment": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) in September 2025", "summary": "Generalized visual grounding tasks, including Generalized Referring\nExpression Comprehension (GREC) and Segmentation (GRES), extend the classical\nvisual grounding paradigm by accommodating multi-target and non-target\nscenarios. Specifically, GREC focuses on accurately identifying all referential\nobjects at the coarse bounding box level, while GRES aims for achieve\nfine-grained pixel-level perception. However, existing approaches typically\ntreat these tasks independently, overlooking the benefits of jointly training\nGREC and GRES to ensure consistent multi-granularity predictions and streamline\nthe overall process. Moreover, current methods often treat GRES as a semantic\nsegmentation task, neglecting the crucial role of instance-aware capabilities\nand the necessity of ensuring consistent predictions between instance-level\nboxes and masks. To address these limitations, we propose InstanceVG, a\nmulti-task generalized visual grounding framework equipped with instance-aware\ncapabilities, which leverages instance queries to unify the joint and\nconsistency predictions of instance-level boxes and masks. To the best of our\nknowledge, InstanceVG is the first framework to simultaneously tackle both GREC\nand GRES while incorporating instance-aware capabilities into generalized\nvisual grounding. To instantiate the framework, we assign each instance query a\nprior reference point, which also serves as an additional basis for target\nmatching. This design facilitates consistent predictions of points, boxes, and\nmasks for the same instance. Extensive experiments obtained on ten datasets\nacross four tasks demonstrate that InstanceVG achieves state-of-the-art\nperformance, significantly surpassing the existing methods in various\nevaluation metrics. The code and model will be publicly available at\nhttps://github.com/Dmmm1997/InstanceVG.", "AI": {"tldr": "InstanceVG\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u5e7f\u4e49\u89c6\u89c9\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f8b\u611f\u77e5\u80fd\u529b\u7edf\u4e00GREC\u548cGRES\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5e7f\u4e49\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u901a\u5e38\u72ec\u7acb\u5904\u7406GREC\u548cGRES\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u8054\u5408\u8bad\u7ec3\u7684\u4f18\u52bf\uff0c\u4e14\u7f3a\u4e4f\u5b9e\u4f8b\u611f\u77e5\u80fd\u529b\u3002", "method": "\u63d0\u51faInstanceVG\u6846\u67b6\uff0c\u5229\u7528\u5b9e\u4f8b\u67e5\u8be2\u7edf\u4e00\u5b9e\u4f8b\u7ea7\u6846\u548c\u63a9\u7801\u7684\u8054\u5408\u9884\u6d4b\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u5b9e\u4f8b\u67e5\u8be2\u5206\u914d\u5148\u9a8c\u53c2\u8003\u70b9\u4ee5\u589e\u5f3a\u76ee\u6807\u5339\u914d\u3002", "result": "\u5728\u56db\u4e2a\u4efb\u52a1\u7684\u5341\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cInstanceVG\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "InstanceVG\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u5b9e\u4f8b\u7ea7\u6846\u548c\u63a9\u7801\u7684\u8054\u5408\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e7f\u4e49\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8868\u73b0\u3002"}}
{"id": "2509.13861", "categories": ["cs.RO", "I.6.0; A.0"], "pdf": "https://arxiv.org/pdf/2509.13861", "abs": "https://arxiv.org/abs/2509.13861", "authors": ["G\u00f6rkem K\u0131l\u0131n\u00e7 Soylu", "Neziha Akalin", "Maria Riveiro"], "title": "Using Petri Nets for Context-Adaptive Robot Explanations", "comment": "In proceedings of TRUST 2025 (arXiv:2509.11402), a workshop at IEEE\n  RO-MAN 2025: https://www.ro-man2025.org/", "summary": "In human-robot interaction, robots must communicate in a natural and\ntransparent manner to foster trust, which requires adapting their communication\nto the context. In this paper, we propose using Petri nets (PNs) to model\ncontextual information for adaptive robot explanations. PNs provide a formal,\ngraphical method for representing concurrent actions, causal dependencies, and\nsystem states, making them suitable for analyzing dynamic interactions between\nhumans and robots. We demonstrate this approach through a scenario involving a\nrobot that provides explanations based on contextual cues such as user\nattention and presence. Model analysis confirms key properties, including\ndeadlock-freeness, context-sensitive reachability, boundedness, and liveness,\nshowing the robustness and flexibility of PNs for designing and verifying\ncontext-adaptive explanations in human-robot interactions.", "AI": {"tldr": "\u4f7f\u7528Petri\u7f51\u5efa\u6a21\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u652f\u6301\u673a\u5668\u4eba\u81ea\u9002\u5e94\u89e3\u91ca\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u7a33\u5065\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u901a\u8fc7\u81ea\u7136\u900f\u660e\u7684\u6c9f\u901a\u65b9\u5f0f\u5efa\u7acb\u4fe1\u4efb\uff0c\u8fd9\u8981\u6c42\u673a\u5668\u4eba\u80fd\u6839\u636e\u4e0a\u4e0b\u6587\u8c03\u6574\u5176\u6c9f\u901a\u7b56\u7565\u3002", "method": "\u4f7f\u7528Petri\u7f51\uff08PNs\uff09\u5efa\u6a21\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u652f\u6301\u673a\u5668\u4eba\u81ea\u9002\u5e94\u89e3\u91ca\u3002PNs\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u3001\u56fe\u5f62\u5316\u7684\u65b9\u6cd5\u6765\u8868\u793a\u5e76\u53d1\u52a8\u4f5c\u3001\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\u548c\u7cfb\u7edf\u72b6\u6001\u3002", "result": "\u6a21\u578b\u5206\u6790\u786e\u8ba4\u4e86\u5173\u952e\u5c5e\u6027\uff0c\u5305\u62ec\u65e0\u6b7b\u9501\u6027\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u53ef\u8fbe\u6027\u3001\u6709\u754c\u6027\u548c\u6d3b\u6027\uff0c\u9a8c\u8bc1\u4e86PNs\u7684\u7a33\u5065\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "PNs\u88ab\u8bc1\u660e\u662f\u8bbe\u8ba1\u548c\u9a8c\u8bc1\u4eba\u673a\u4ea4\u4e92\u4e2d\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u89e3\u91ca\u7684\u7a33\u5065\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.13754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13754", "abs": "https://arxiv.org/abs/2509.13754", "authors": ["Hao Yin", "Xin Man", "Feiyu Chen", "Jie Shao", "Heng Tao Shen"], "title": "Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval", "comment": null, "summary": "Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that\naims to retrieve the most relevant person images based on a given text query.\nThe key challenge in TIPR lies in achieving effective alignment between textual\nand visual modalities within a common latent space. To address this challenge,\nprior approaches incorporate attention mechanisms for implicit cross-modal\nlocal alignment. However, they lack the ability to verify whether all local\nfeatures are correctly aligned. Moreover, existing methods primarily focus on\nhard negative samples during model updates, with the goal of refining\ndistinctions between positive and negative pairs, often neglecting incorrectly\nmatched positive pairs. To alleviate these issues, we propose FMFA, a\ncross-modal Full-Mode Fine-grained Alignment framework, which enhances global\nmatching through explicit fine-grained alignment and existing implicit\nrelational reasoning -- hence the term ``full-mode\" -- without requiring\nadditional supervision. Specifically, we design an Adaptive Similarity\nDistribution Matching (A-SDM) module to rectify unmatched positive sample\npairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint\nembedding space, thereby achieving more precise global alignment. Additionally,\nwe introduce an Explicit Fine-grained Alignment (EFA) module, which makes up\nfor the lack of verification capability of implicit relational reasoning. EFA\nstrengthens explicit cross-modal fine-grained interactions by sparsifying the\nsimilarity matrix and employs a hard coding method for local alignment. Our\nproposed method is evaluated on three public datasets, achieving\nstate-of-the-art performance among all global matching methods. Our code is\navailable at https://github.com/yinhao1102/FMFA.", "AI": {"tldr": "FMFA\u901a\u8fc7\u663e\u5f0f\u548c\u9690\u5f0f\u5bf9\u9f50\u4f18\u5316\u8de8\u6a21\u6001\u5339\u914d\uff0c\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u4eba\u7269\u68c0\u7d22\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u5339\u914d\u4e2d\u65e0\u6cd5\u9a8c\u8bc1\u5c40\u90e8\u7279\u5f81\u5bf9\u9f50\u662f\u5426\u6b63\u786e\uff0c\u4ee5\u53ca\u5ffd\u89c6\u9519\u8bef\u5339\u914d\u7684\u6b63\u6837\u672c\u5bf9\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86FMFA\u6846\u67b6\uff0c\u5305\u542bA-SDM\u6a21\u5757\uff08\u81ea\u9002\u5e94\u76f8\u4f3c\u6027\u5206\u5e03\u5339\u914d\uff09\u548cEFA\u6a21\u5757\uff08\u663e\u5f0f\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff09\uff0c\u5206\u522b\u7528\u4e8e\u4fee\u6b63\u672a\u5339\u914d\u7684\u6b63\u6837\u672c\u5bf9\u548c\u589e\u5f3a\u663e\u5f0f\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "FMFA\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u9690\u5f0f\u5173\u7cfb\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u5168\u5c40\u5339\u914d\u7684\u4f18\u5316\uff0c\u5e76\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13882", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.13882", "abs": "https://arxiv.org/abs/2509.13882", "authors": ["Junhwa Hong", "Beomjoon Lee", "Woojin Lee", "Changjoo Nam"], "title": "Repulsive Trajectory Modification and Conflict Resolution for Efficient Multi-Manipulator Motion Planning", "comment": "7 pages", "summary": "We propose an efficient motion planning method designed to efficiently find\ncollision-free trajectories for multiple manipulators. While multi-manipulator\nsystems offer significant advantages, coordinating their motions is\ncomputationally challenging owing to the high dimensionality of their composite\nconfiguration space. Conflict-Based Search (CBS) addresses this by decoupling\nmotion planning, but suffers from subsequent conflicts incurred by resolving\nexisting conflicts, leading to an exponentially growing constraint tree of CBS.\nOur proposed method is based on repulsive trajectory modification within the\ntwo-level structure of CBS. Unlike conventional CBS variants, the low-level\nplanner applies a gradient descent approach using an Artificial Potential\nField. This field generates repulsive forces that guide the trajectory of the\nconflicting manipulator away from those of other robots. As a result,\nsubsequent conflicts are less likely to occur. Additionally, we develop a\nstrategy that, under a specific condition, directly attempts to find a\nconflict-free solution in a single step without growing the constraint tree.\nThrough extensive tests including physical robot experiments, we demonstrate\nthat our method consistently reduces the number of expanded nodes in the\nconstraint tree, achieves a higher success rate, and finds a solution faster\ncompared to Enhanced CBS and other state-of-the-art algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdbCBS\u7684\u9ad8\u6548\u591a\u673a\u68b0\u81c2\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eba\u5de5\u52bf\u573a\u51cf\u5c11\u540e\u7eed\u51b2\u7a81\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u591a\u673a\u68b0\u81c2\u7cfb\u7edf\u7684\u8fd0\u52a8\u89c4\u5212\u56e0\u590d\u5408\u914d\u7f6e\u7a7a\u95f4\u7684\u9ad8\u7ef4\u5ea6\u800c\u8ba1\u7b97\u590d\u6742\uff0c\u4f20\u7edfCBS\u65b9\u6cd5\u5728\u89e3\u51b3\u51b2\u7a81\u65f6\u4f1a\u4ea7\u751f\u6307\u6570\u7ea7\u589e\u957f\u7684\u7ea6\u675f\u6811\u3002", "method": "\u57fa\u4e8eCBS\u7684\u4e24\u7ea7\u7ed3\u6784\uff0c\u4f4e\u5c42\u89c4\u5212\u5668\u91c7\u7528\u4eba\u5de5\u52bf\u573a\u7684\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\uff0c\u751f\u6210\u6392\u65a5\u529b\u5f15\u5bfc\u51b2\u7a81\u673a\u68b0\u81c2\u8f68\u8ff9\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4e00\u79cd\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u76f4\u63a5\u6c42\u89e3\u51b2\u7a81\u81ea\u7531\u89e3\u7684\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u7ea6\u675f\u6811\u7684\u6269\u5c55\u8282\u70b9\u6570\uff0c\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u5e76\u6bd4Enhanced CBS\u7b49\u5148\u8fdb\u7b97\u6cd5\u66f4\u5feb\u627e\u5230\u89e3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6539\u8fdbCBS\u7684\u4e24\u7ea7\u7ed3\u6784\uff0c\u91c7\u7528\u4eba\u5de5\u52bf\u573a\u7684\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u51cf\u5c11\u540e\u7eed\u51b2\u7a81\uff0c\u5e76\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4e00\u6b65\u6c42\u89e3\u51b2\u7a81\u81ea\u7531\u89e3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u673a\u68b0\u81c2\u8fd0\u52a8\u89c4\u5212\u7684\u6548\u7387\u3002"}}
{"id": "2509.13756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13756", "abs": "https://arxiv.org/abs/2509.13756", "authors": ["Yuqi Yang", "Dongliang Chang", "Yuanchen Fang", "Yi-Zhe SonG", "Zhanyu Ma", "Jun Guo"], "title": "Controllable-Continuous Color Editing in Diffusion Model via Color Mapping", "comment": null, "summary": "In recent years, text-driven image editing has made significant progress.\nHowever, due to the inherent ambiguity and discreteness of natural language,\ncolor editing still faces challenges such as insufficient precision and\ndifficulty in achieving continuous control. Although linearly interpolating the\nembedding vectors of different textual descriptions can guide the model to\ngenerate a sequence of images with varying colors, this approach lacks precise\ncontrol over the range of color changes in the output images. Moreover, the\nrelationship between the interpolation coefficient and the resulting image\ncolor is unknown and uncontrollable. To address these issues, we introduce a\ncolor mapping module that explicitly models the correspondence between the text\nembedding space and image RGB values. This module predicts the corresponding\nembedding vector based on a given RGB value, enabling precise color control of\nthe generated images while maintaining semantic consistency. Users can specify\na target RGB range to generate images with continuous color variations within\nthe desired range, thereby achieving finer-grained, continuous, and\ncontrollable color editing. Experimental results demonstrate that our method\nperforms well in terms of color continuity and controllability.", "AI": {"tldr": "\u63d0\u51fa\u989c\u8272\u6620\u5c04\u6a21\u5757\uff0c\u901a\u8fc7\u5efa\u6a21\u6587\u672c\u5d4c\u5165\u4e0eRGB\u503c\u7684\u5173\u7cfb\uff0c\u5b9e\u73b0\u7cbe\u786e\u3001\u8fde\u7eed\u4e14\u53ef\u63a7\u7684\u989c\u8272\u7f16\u8f91\u3002", "motivation": "\u7531\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u56fa\u6709\u6a21\u7cca\u6027\u548c\u79bb\u6563\u6027\uff0c\u989c\u8272\u7f16\u8f91\u5728\u7cbe\u5ea6\u4e0d\u8db3\u548c\u96be\u4ee5\u5b9e\u73b0\u8fde\u7eed\u63a7\u5236\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u7cbe\u786e\u63a7\u5236\u989c\u8272\u53d8\u5316\u8303\u56f4\u53ca\u5176\u4e0e\u63d2\u503c\u7cfb\u6570\u7684\u5173\u7cfb\u3002", "method": "\u5f15\u5165\u989c\u8272\u6620\u5c04\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u57fa\u4e8e\u7ed9\u5b9aRGB\u503c\u9884\u6d4b\u5bf9\u5e94\u7684\u5d4c\u5165\u5411\u91cf\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u751f\u6210\u56fe\u50cf\u7684\u7cbe\u786e\u989c\u8272\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u989c\u8272\u8fde\u7eed\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u989c\u8272\u6620\u5c04\u6a21\u5757\u901a\u8fc7\u660e\u786e\u5efa\u6a21\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u4e0e\u56fe\u50cfRGB\u503c\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5b9e\u73b0\u4e86\u5bf9\u751f\u6210\u56fe\u50cf\u989c\u8272\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u5e76\u5728\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u3001\u8fde\u7eed\u4e14\u53ef\u63a7\u7684\u989c\u8272\u7f16\u8f91\u80fd\u529b\u3002"}}
{"id": "2509.13903", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13903", "abs": "https://arxiv.org/abs/2509.13903", "authors": ["Artem Lykov", "Jeffrin Sam", "Hung Khang Nguyen", "Vladislav Kozlovskiy", "Yara Mahmoud", "Valerii Serpiva", "Miguel Altamirano Cabrera", "Mikhail Konenkov", "Dzmitry Tsetserukou"], "title": "PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models", "comment": "submitted to IEEE conference", "summary": "We introduce PhysicalAgent, an agentic framework for robotic manipulation\nthat integrates iterative reasoning, diffusion-based video generation, and\nclosed-loop execution. Given a textual instruction, our method generates short\nvideo demonstrations of candidate trajectories, executes them on the robot, and\niteratively re-plans in response to failures. This approach enables robust\nrecovery from execution errors. We evaluate PhysicalAgent across multiple\nperceptual modalities (egocentric, third-person, and simulated) and robotic\nembodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing\nagainst state-of-the-art task-specific baselines. Experiments demonstrate that\nour method consistently outperforms prior approaches, achieving up to 83%\nsuccess on human-familiar tasks. Physical trials reveal that first-attempt\nsuccess is limited (20-30%), yet iterative correction increases overall success\nto 80% across platforms. These results highlight the potential of video-based\ngenerative reasoning for general-purpose robotic manipulation and underscore\nthe importance of iterative execution for recovering from initial failures. Our\nframework paves the way for scalable, adaptable, and robust robot control.", "AI": {"tldr": "PhysicalAgent\u662f\u4e00\u4e2a\u7ed3\u5408\u8fed\u4ee3\u63a8\u7406\u548c\u89c6\u9891\u751f\u6210\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u6821\u6b63\u663e\u8457\u63d0\u9ad8\u6210\u529f\u7387\uff0c\u5c55\u793a\u4e86\u89c6\u9891\u751f\u6210\u63a8\u7406\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u4ece\u6267\u884c\u9519\u8bef\u4e2d\u7a33\u5065\u6062\u590d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u5e76\u63a2\u7d22\u89c6\u9891\u751f\u6210\u63a8\u7406\u5728\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6f5c\u529b\u3002", "method": "PhysicalAgent\u7ed3\u5408\u4e86\u8fed\u4ee3\u63a8\u7406\u3001\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u751f\u6210\u548c\u95ed\u73af\u6267\u884c\u3002\u7ed9\u5b9a\u6587\u672c\u6307\u4ee4\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u5019\u9009\u8f68\u8ff9\u7684\u77ed\u89c6\u9891\u6f14\u793a\uff0c\u5728\u673a\u5668\u4eba\u4e0a\u6267\u884c\u5b83\u4eec\uff0c\u5e76\u6839\u636e\u5931\u8d25\u60c5\u51b5\u8fed\u4ee3\u91cd\u65b0\u89c4\u5212\u3002", "result": "PhysicalAgent\u5728\u591a\u4e2a\u611f\u77e5\u6a21\u6001\u548c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9996\u6b21\u5c1d\u8bd5\u6210\u529f\u7387\u8f83\u4f4e\uff0820-30%\uff09\uff0c\u4f46\u901a\u8fc7\u8fed\u4ee3\u6821\u6b63\uff0c\u6574\u4f53\u6210\u529f\u7387\u63d0\u9ad8\u523080%\u3002", "conclusion": "PhysicalAgent\u6846\u67b6\u5c55\u793a\u4e86\u57fa\u4e8e\u89c6\u9891\u7684\u751f\u6210\u63a8\u7406\u5728\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5f3a\u8c03\u4e86\u8fed\u4ee3\u6267\u884c\u5bf9\u4e8e\u4ece\u521d\u59cb\u5931\u8d25\u4e2d\u6062\u590d\u7684\u91cd\u8981\u6027\u3002\u8be5\u6846\u67b6\u4e3a\u53ef\u6269\u5c55\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u7a33\u5065\u7684\u673a\u5668\u4eba\u63a7\u5236\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.13760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13760", "abs": "https://arxiv.org/abs/2509.13760", "authors": ["Jinwoo Jeon", "JunHyeok Oh", "Hayeong Lee", "Byung-Jun Lee"], "title": "Iterative Prompt Refinement for Safer Text-to-Image Generation", "comment": null, "summary": "Text-to-Image (T2I) models have made remarkable progress in generating images\nfrom text prompts, but their output quality and safety still depend heavily on\nhow prompts are phrased. Existing safety methods typically refine prompts using\nlarge language models (LLMs), but they overlook the images produced, which can\nresult in unsafe outputs or unnecessary changes to already safe prompts. To\naddress this, we propose an iterative prompt refinement algorithm that uses\nVision Language Models (VLMs) to analyze both the input prompts and the\ngenerated images. By leveraging visual feedback, our method refines prompts\nmore effectively, improving safety while maintaining user intent and\nreliability comparable to existing LLM-based approaches. Additionally, we\nintroduce a new dataset labeled with both textual and visual safety signals\nusing off-the-shelf multi-modal LLM, enabling supervised fine-tuning.\nExperimental results demonstrate that our approach produces safer outputs\nwithout compromising alignment with user intent, offering a practical solution\nfor generating safer T2I content. Our code is available at\nhttps://github.com/ku-dmlab/IPR. \\textbf{\\textcolor{red}WARNING: This paper\ncontains examples of harmful or inappropriate images generated by models.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u89c6\u89c9\u53cd\u9988\u7684\u8fed\u4ee3\u63d0\u793a\u7ec6\u5316\u7b97\u6cd5\uff0c\u63d0\u5347T2I\u6a21\u578b\u5b89\u5168\u6027\u5e76\u4fdd\u6301\u7528\u6237\u610f\u56fe\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5b89\u5168\u65b9\u6cd5\u5ffd\u89c6\u751f\u6210\u56fe\u50cf\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u4e0d\u5b89\u5168\u8f93\u51fa\u6216\u4e0d\u5fc5\u8981\u4fee\u6539\u5b89\u5168\u63d0\u793a\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5206\u6790\u8f93\u5165\u63d0\u793a\u548c\u751f\u6210\u7684\u56fe\u50cf\uff0c\u63d0\u51fa\u8fed\u4ee3\u63d0\u793a\u7ec6\u5316\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u65b0\u6807\u6ce8\u7684\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7528\u6237\u610f\u56fe\u7684\u540c\u65f6\uff0c\u751f\u6210\u4e86\u66f4\u5b89\u5168\u7684\u8f93\u51fa\uff0c\u4f18\u4e8e\u73b0\u6709LLM-based\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u8fed\u4ee3\u63d0\u793a\u7ec6\u5316\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u7528\u6237\u610f\u56fe\u7684\u4fdd\u6301\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u65b0\u7684\u6570\u636e\u96c6\u652f\u6301\u76d1\u7763\u5fae\u8c03\u3002"}}
{"id": "2509.13926", "categories": ["cs.RO", "cs.AI", "cs.CV", "I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.13926", "abs": "https://arxiv.org/abs/2509.13926", "authors": ["Huilin Yin", "Yiming Kan", "Daniel Watzenig"], "title": "MAP: End-to-End Autonomous Driving with Map-Assisted Planning", "comment": "8 pages, 2 figures, accepted by ICCVW Author list updated to match\n  the camera-ready version, in compliance with conference policy", "summary": "In recent years, end-to-end autonomous driving has attracted increasing\nattention for its ability to jointly model perception, prediction, and planning\nwithin a unified framework. However, most existing approaches underutilize the\nonline mapping module, leaving its potential to enhance trajectory planning\nlargely untapped. This paper proposes MAP (Map-Assisted Planning), a novel\nmap-assisted end-to-end trajectory planning framework. MAP explicitly\nintegrates segmentation-based map features and the current ego status through a\nPlan-enhancing Online Mapping module, an Ego-status-guided Planning module, and\na Weight Adapter based on current ego status. Experiments conducted on the\nDAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%\nreduction in L2 displacement error, a 56.2% reduction in off-road rate, and a\n44.5% improvement in overall score compared to the UniV2X baseline, even\nwithout post-processing. Furthermore, it achieves top ranking in Track 2 of the\nEnd-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS\nWorkshop @CVPR2025, outperforming the second-best model by 39.5% in terms of\noverall score. These results highlight the effectiveness of explicitly\nleveraging semantic map features in planning and suggest new directions for\nimproving structure design in end-to-end autonomous driving systems. Our code\nis available at https://gitee.com/kymkym/map.git", "AI": {"tldr": "MAP\u662f\u4e00\u79cd\u65b0\u578b\u5730\u56fe\u8f85\u52a9\u7aef\u5230\u7aef\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u6574\u5408\u5730\u56fe\u7279\u5f81\u548c\u81ea\u8f66\u72b6\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u5728\u7ebf\u5730\u56fe\u6a21\u5757\u7684\u6f5c\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u589e\u5f3a\u4f5c\u7528\u3002", "method": "MAP\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\uff08Plan-enhancing Online Mapping\u3001Ego-status-guided Planning\u548cWeight Adapter\uff09\u663e\u5f0f\u6574\u5408\u5206\u5272\u5730\u56fe\u7279\u5f81\u548c\u5f53\u524d\u81ea\u8f66\u72b6\u6001\u3002", "result": "\u5728DAIR-V2X-seq-SPD\u6570\u636e\u96c6\u4e0a\uff0cMAP\u5b9e\u73b0\u4e86L2\u4f4d\u79fb\u8bef\u5dee\u51cf\u5c1116.6%\u3001\u8131\u8f68\u7387\u51cf\u5c1156.2%\u3001\u603b\u5206\u63d0\u534744.5%\uff0c\u5e76\u5728MEIS Workshop @CVPR2025\u7684\u6311\u6218\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u63d0\u51faMAP\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u663e\u5f0f\u5229\u7528\u8bed\u4e49\u5730\u56fe\u7279\u5f81\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u7ed3\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.13762", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13762", "abs": "https://arxiv.org/abs/2509.13762", "authors": ["Kai Chen", "Jin Xiao", "Leheng Zhang", "Kexuan Shi", "Shuhang Gu"], "title": "Task-Aware Image Signal Processor for Advanced Visual Perception", "comment": null, "summary": "In recent years, there has been a growing trend in computer vision towards\nexploiting RAW sensor data, which preserves richer information compared to\nconventional low-bit RGB images. Early studies mainly focused on enhancing\nvisual quality, while more recent efforts aim to leverage the abundant\ninformation in RAW data to improve the performance of visual perception tasks\nsuch as object detection and segmentation. However, existing approaches still\nface two key limitations: large-scale ISP networks impose heavy computational\noverhead, while methods based on tuning traditional ISP pipelines are\nrestricted by limited representational capacity.To address these issues, we\npropose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB\nframework that produces task-oriented representations for pretrained vision\nmodels. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small\nset of lightweight, multi-scale modulation operators that act at global,\nregional, and pixel scales to reshape image statistics across different spatial\nextents. This factorized control significantly expands the range of spatially\nvarying transforms that can be represented while keeping memory usage,\ncomputation, and latency tightly constrained. Evaluated on several RAW-domain\ndetection and segmentation benchmarks under both daytime and nighttime\nconditions, TA-ISP consistently improves downstream accuracy while markedly\nreducing parameter count and inference time, making it well suited for\ndeployment on resource-constrained devices.", "AI": {"tldr": "TA-ISP \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684 RAW-to-RGB \u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u8c03\u5236\u7b97\u5b50\u4f18\u5316\u56fe\u50cf\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5229\u7528 RAW \u6570\u636e\u65f6\u9762\u4e34\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u8868\u793a\u80fd\u529b\u53d7\u9650\u7684\u95ee\u9898\uff0cTA-ISP \u65e8\u5728\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u7684\u65b9\u5f0f\u4f18\u5316\u56fe\u50cf\u4fe1\u53f7\u5904\u7406\uff0c\u63d0\u5347\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "TA-ISP \u901a\u8fc7\u9884\u6d4b\u4e00\u7ec4\u8f7b\u91cf\u7ea7\u3001\u591a\u5c3a\u5ea6\u7684\u8c03\u5236\u7b97\u5b50\uff0c\u5728\u5168\u5c40\u3001\u533a\u57df\u548c\u50cf\u7d20\u5c3a\u5ea6\u4e0a\u91cd\u5851\u56fe\u50cf\u7edf\u8ba1\u4fe1\u606f\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5bc6\u96c6\u5377\u79ef\u7ba1\u9053\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u591a\u4e2a RAW \u57df\u68c0\u6d4b\u548c\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTA-ISP \u5728\u767d\u5929\u548c\u591c\u95f4\u6761\u4ef6\u4e0b\u5747\u80fd\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u53c2\u6570\u548c\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "TA-ISP \u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u51d1\u7684 RAW-to-RGB \u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u548c\u63a8\u7406\u65f6\u95f4\uff0c\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002"}}
{"id": "2509.13943", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13943", "abs": "https://arxiv.org/abs/2509.13943", "authors": ["Salim Oyinlola", "Nitesh Subedi", "Soumik Sarkar"], "title": "Reinforcement Learning for Autonomous Point-to-Point UAV Navigation", "comment": "Presented at the Research Experience for Undergraduates (REU)\n  Symposium at the Translational AI Centre in Iowa State University", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly used in automated\ninspection, delivery, and navigation tasks that require reliable autonomy. This\nproject develops a reinforcement learning (RL) approach to enable a single UAV\nto autonomously navigate between predefined points without manual intervention.\nThe drone learns navigation policies through trial-and-error interaction, using\na custom reward function that encourages goal-reaching efficiency while\npenalizing collisions and unsafe behavior. The control system integrates ROS\nwith a Gym-compatible training environment, enabling flexible deployment and\ntesting. After training, the learned policy is deployed on a real UAV platform\nand evaluated under practical conditions. Results show that the UAV can\nsuccessfully perform autonomous navigation with minimal human oversight,\ndemonstrating the viability of RL-based control for point-to-point drone\noperations in real-world scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4f7f\u65e0\u4eba\u673a\u80fd\u591f\u81ea\u4e3b\u5bfc\u822a\uff0c\u5b9e\u9645\u6d4b\u8bd5\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5728\u81ea\u52a8\u5316\u68c0\u67e5\u3001\u914d\u9001\u548c\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u53ef\u9760\u7684\u81ea\u4e3b\u4f53\u6765\u5b9e\u73b0\u65e0\u4eba\u5e72\u9884\u7684\u81ea\u4e3b\u5bfc\u822a\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\uff0c\u9f13\u52b1\u65e0\u4eba\u673a\u9ad8\u6548\u5230\u8fbe\u76ee\u6807\u70b9\uff0c\u540c\u65f6\u907f\u514d\u78b0\u649e\u548c\u4e0d\u5b89\u5168\u884c\u4e3a\u3002\u63a7\u5236\u7cfb\u7edf\u7ed3\u5408\u4e86ROS\u548cGym\u517c\u5bb9\u7684\u8bad\u7ec3\u73af\u5883\u3002", "result": "\u7ecf\u8fc7\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u5b9e\u9645\u65e0\u4eba\u673a\u5e73\u53f0\u4e0a\u90e8\u7f72\u5e76\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u65e0\u4eba\u673a\u80fd\u591f\u5728\u6700\u5c11\u4eba\u5de5\u76d1\u7763\u4e0b\u6210\u529f\u5b8c\u6210\u81ea\u4e3b\u5bfc\u822a\u4efb\u52a1\u3002", "conclusion": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u65b9\u6cd5\u5728\u5b9e\u9645\u65e0\u4eba\u673a\u70b9\u5bf9\u70b9\u5bfc\u822a\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.13766", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13766", "abs": "https://arxiv.org/abs/2509.13766", "authors": ["Huichun Liu", "Xiaosong Li", "Yang Liu", "Xiaoqi Cheng", "Haishu Tan"], "title": "NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset", "comment": null, "summary": "Visual degradation caused by rain streak artifacts in low-light conditions\nsignificantly hampers the performance of nighttime surveillance and autonomous\nnavigation. Existing image deraining techniques are primarily designed for\ndaytime conditions and perform poorly under nighttime illumination due to the\nspatial heterogeneity of rain distribution and the impact of light-dependent\nstripe visibility. In this paper, we propose a novel Nighttime Deraining\nLocation-enhanced Perceptual Network(NDLPNet) that effectively captures the\nspatial positional information and density distribution of rain streaks in\nlow-light environments. Specifically, we introduce a Position Perception Module\n(PPM) to capture and leverage spatial contextual information from input data,\nenhancing the model's capability to identify and recalibrate the importance of\ndifferent feature channels. The proposed nighttime deraining network can\neffectively remove the rain streaks as well as preserve the crucial background\ninformation. Furthermore, We construct a night scene rainy (NSR) dataset\ncomprising 900 image pairs, all based on real-world nighttime scenes, providing\na new benchmark for nighttime deraining task research. Extensive qualitative\nand quantitative experimental evaluations on both existing datasets and the NSR\ndataset consistently demonstrate our method outperform the state-of-the-art\n(SOTA) methods in nighttime deraining tasks. The source code and dataset is\navailable at https://github.com/Feecuin/NDLPNet.", "AI": {"tldr": "\u63d0\u51faNDLPNet\u7528\u4e8e\u591c\u95f4\u53bb\u96e8\uff0c\u901a\u8fc7PPM\u6a21\u5757\u589e\u5f3a\u7a7a\u95f4\u4fe1\u606f\u6355\u6349\u80fd\u529b\uff0c\u6784\u5efaNSR\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u53bb\u96e8\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u767d\u5929\u6761\u4ef6\uff0c\u5728\u591c\u95f4\u5149\u7167\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u96e8\u5206\u5e03\u7684\u7a7a\u95f4\u5f02\u8d28\u6027\u548c\u5149\u4f9d\u8d56\u6761\u7eb9\u53ef\u89c1\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5f15\u5165\u4f4d\u7f6e\u611f\u77e5\u6a21\u5757\uff08PPM\uff09\u6765\u6355\u6349\u548c\u5229\u7528\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u589e\u5f3a\u6a21\u578b\u8bc6\u522b\u548c\u91cd\u65b0\u6821\u51c6\u4e0d\u540c\u7279\u5f81\u901a\u9053\u91cd\u8981\u6027\u7684\u80fd\u529b\u3002", "result": "\u5728\u73b0\u6709\u6570\u636e\u96c6\u548cNSR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591c\u95f4\u53bb\u96e8\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684NDLPNet\u5728\u591c\u95f4\u53bb\u96e8\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6784\u5efaNSR\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2509.13948", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13948", "abs": "https://arxiv.org/abs/2509.13948", "authors": ["Benedict Barrow", "Roger K. Moore"], "title": "The Influence of Facial Features on the Perceived Trustworthiness of a Social Robot", "comment": "In proceedings of TRUST 2025 (arXiv:2509.11402), a workshop at IEEE\n  RO-MAN 2025: https://ro-man2025.org/", "summary": "Trust and the perception of trustworthiness play an important role in\ndecision-making and our behaviour towards others, and this is true not only of\nhuman-human interactions but also of human-robot interactions. While\nsignificant advances have been made in recent years in the field of social\nrobotics, there is still some way to go before we fully understand the factors\nthat influence human trust in robots. This paper presents the results of a\nstudy into the first impressions created by a social robot's facial features,\nbased on the hypothesis that a `babyface' engenders trust. By manipulating the\nback-projected face of a Furhat robot, the study confirms that eye shape and\nsize have a significant impact on the perception of trustworthiness. The work\nthus contributes to an understanding of the design choices that need to be made\nwhen developing social robots so as to optimise the effectiveness of\nhuman-robot interaction.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u8c03\u6574\u673a\u5668\u4eba\u9762\u90e8\u7279\u5f81\uff0c\u53d1\u73b0\u773c\u775b\u5f62\u72b6\u548c\u5927\u5c0f\u663e\u8457\u5f71\u54cd\u4eba\u7c7b\u5bf9\u5176\u7684\u4fe1\u4efb\u611f\u77e5\u3002", "motivation": "\u7406\u89e3\u5f71\u54cd\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u4fe1\u4efb\u7684\u56e0\u7d20\uff0c\u4ee5\u4f18\u5316\u793e\u4ea4\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u64cd\u7eb5Furhat\u673a\u5668\u4eba\u7684\u540e\u6295\u5f71\u9762\u90e8\u7279\u5f81\uff0c\u7814\u7a76\u2018\u5a03\u5a03\u8138\u2019\u5bf9\u4fe1\u4efb\u611f\u77e5\u7684\u5f71\u54cd\u3002", "result": "\u773c\u775b\u5f62\u72b6\u548c\u5927\u5c0f\u5bf9\u4fe1\u4efb\u611f\u77e5\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u793e\u4ea4\u673a\u5668\u4eba\u9762\u90e8\u7279\u5f81\uff08\u5c24\u5176\u662f\u773c\u775b\u5f62\u72b6\u548c\u5927\u5c0f\uff09\u5bf9\u4eba\u7c7b\u4fe1\u4efb\u611f\u77e5\u7684\u663e\u8457\u5f71\u54cd\uff0c\u4e3a\u8bbe\u8ba1\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.13767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13767", "abs": "https://arxiv.org/abs/2509.13767", "authors": ["Daiqi Liu", "Tom\u00e1s Arias-Vergara", "Johannes Enk", "Fangxu Xing", "Maureen Stone", "Jerry L. Prince", "Jana Hutter", "Andreas Maier", "Jonghye Woo", "Paula Andrea P\u00e9rez-Toro"], "title": "VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI", "comment": "Preprint submitted to ICASSP", "summary": "Accurately segmenting articulatory structures in real-time magnetic resonance\nimaging (rtMRI) remains challenging, as most existing methods rely almost\nentirely on visual cues. Yet synchronized acoustic and phonological signals\nprovide complementary context that can enrich visual information and improve\nprecision. In this paper, we introduce VocSegMRI, a multimodal framework that\nintegrates video, audio, and phonological inputs through cross-attention fusion\nfor dynamic feature alignment. To further enhance cross-modal representation,\nwe incorporate a contrastive learning objective that improves segmentation\nperformance even when the audio modality is unavailable at inference. Evaluated\non a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art\nperformance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance\n(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.\nAblation studies confirm the contributions of cross-attention and contrastive\nlearning to segmentation precision and robustness. These results highlight the\nvalue of integrative multimodal modeling for accurate vocal tract analysis.", "AI": {"tldr": "VocSegMRI\u901a\u8fc7\u6574\u5408\u89c6\u9891\u3001\u97f3\u9891\u548c\u8bed\u97f3\u5b66\u8f93\u5165\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u78c1\u5171\u632f\u6210\u50cf\u4e2d\u53d1\u97f3\u7ed3\u6784\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7ebf\u7d22\uff0c\u96be\u4ee5\u51c6\u786e\u5206\u5272\u5b9e\u65f6\u78c1\u5171\u632f\u6210\u50cf\u4e2d\u7684\u53d1\u97f3\u7ed3\u6784\uff0c\u800c\u540c\u6b65\u7684\u58f0\u5b66\u548c\u8bed\u97f3\u5b66\u4fe1\u53f7\u80fd\u63d0\u4f9b\u4e92\u8865\u4e0a\u4e0b\u6587\uff0c\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86VocSegMRI\u591a\u6a21\u6001\u6846\u67b6\uff0c\u6574\u5408\u89c6\u9891\u3001\u97f3\u9891\u548c\u8bed\u97f3\u5b66\u8f93\u5165\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u5b9e\u73b0\u52a8\u6001\u7279\u5f81\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u4ee5\u589e\u5f3a\u8de8\u6a21\u6001\u8868\u793a\u3002", "result": "\u5728USC-75 rtMRI\u6570\u636e\u96c6\u7684\u5b50\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cDice\u5f97\u5206\u4e3a0.95\uff0c95\u767e\u5206\u4f4d\u8c6a\u65af\u591a\u592b\u8ddd\u79bb\u4e3a4.20\u6beb\u7c73\uff0c\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u6574\u5408\u591a\u6a21\u6001\u5efa\u6a21\u5728\u51c6\u786e\u58f0\u9053\u5206\u6790\u4e2d\u7684\u4ef7\u503c\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u5bf9\u6bd4\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.13949", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.13949", "abs": "https://arxiv.org/abs/2509.13949", "authors": ["Jannick Strangh\u00f6ner", "Philipp Hartmann", "Marco Braun", "Sebastian Wrede", "Klaus Neumann"], "title": "SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks", "comment": "8 pages, 5 figures, submitted to the IEEE International Conference on\n  Robotics and Automation (ICRA) 2026", "summary": "High-mix low-volume (HMLV) industrial assembly, common in small and\nmedium-sized enterprises (SMEs), requires the same precision, safety, and\nreliability as high-volume automation while remaining flexible to product\nvariation and environmental uncertainty. Current robotic systems struggle to\nmeet these demands. Manual programming is brittle and costly to adapt, while\nlearning-based methods suffer from poor sample efficiency and unsafe\nexploration in contact-rich tasks. To address this, we present SHaRe-RL, a\nreinforcement learning framework that leverages multiple sources of prior\nknowledge. By (i) structuring skills into manipulation primitives, (ii)\nincorporating human demonstrations and online corrections, and (iii) bounding\ninteraction forces with per-axis compliance, SHaRe-RL enables efficient and\nsafe online learning for long-horizon, contact-rich industrial assembly tasks.\nExperiments on the insertion of industrial Harting connector modules with\n0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance\nwithin practical time budgets. Our results show that process expertise, without\nrequiring robotics or RL knowledge, can meaningfully contribute to learning,\nenabling safer, more robust, and more economically viable deployment of RL for\nindustrial assembly.", "AI": {"tldr": "SHaRe-RL\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5148\u9a8c\u77e5\u8bc6\u5b9e\u73b0\u9ad8\u6548\u5b89\u5168\u7684\u5728\u7ebf\u5b66\u4e60\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u88c5\u914d\u4efb\u52a1\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u9ad8\u6df7\u5408\u4f4e\u91cf\uff08HMLV\uff09\u5de5\u4e1a\u88c5\u914d\u9700\u8981\u9ad8\u7cbe\u5ea6\u3001\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u4ea7\u54c1\u53d8\u5316\u548c\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7684\u7075\u6d3b\u6027\uff0c\u800c\u73b0\u6709\u673a\u5668\u4eba\u7cfb\u7edf\u96be\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "SHaRe-RL\u6846\u67b6\u901a\u8fc7\uff08i\uff09\u5c06\u6280\u80fd\u7ed3\u6784\u5316\u4e3a\u64cd\u4f5c\u57fa\u5143\uff0c\uff08ii\uff09\u6574\u5408\u4eba\u7c7b\u6f14\u793a\u548c\u5728\u7ebf\u4fee\u6b63\uff0c\uff08iii\uff09\u901a\u8fc7\u6bcf\u8f74\u987a\u5e94\u6027\u9650\u5236\u4ea4\u4e92\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u5728\u7ebf\u5b66\u4e60\u3002", "result": "\u5728\u5de5\u4e1aHarting\u8fde\u63a5\u5668\u6a21\u5757\u63d2\u5165\u4efb\u52a1\u4e2d\uff0cSHaRe-RL\u57280.2-0.4\u6beb\u7c73\u7684\u95f4\u9699\u4e0b\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u65f6\u95f4\u9884\u7b97\u5185\u5b8c\u6210\u4efb\u52a1\u3002", "conclusion": "SHaRe-RL\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u79cd\u5148\u9a8c\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u5728\u7ebf\u5b66\u4e60\uff0c\u4e3a\u5de5\u4e1a\u88c5\u914d\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u3001\u66f4\u7a33\u5065\u4e14\u7ecf\u6d4e\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13768", "abs": "https://arxiv.org/abs/2509.13768", "authors": ["Jianhui Chang"], "title": "Generative Image Coding with Diffusion Prior", "comment": null, "summary": "As generative technologies advance, visual content has evolved into a complex\nmix of natural and AI-generated images, driving the need for more efficient\ncoding techniques that prioritize perceptual quality. Traditional codecs and\nlearned methods struggle to maintain subjective quality at high compression\nratios, while existing generative approaches face challenges in visual fidelity\nand generalization. To this end, we propose a novel generative coding framework\nleveraging diffusion priors to enhance compression performance at low bitrates.\nOur approach employs a pre-optimized encoder to generate generalized\ncompressed-domain representations, integrated with the pretrained model's\ninternal features via a lightweight adapter and an attentive fusion module.\nThis framework effectively leverages existing pretrained diffusion models and\nenables efficient adaptation to different pretrained models for new\nrequirements with minimal retraining costs. We also introduce a distribution\nrenormalization method to further enhance reconstruction fidelity. Extensive\nexperiments show that our method (1) outperforms existing methods in visual\nfidelity across low bitrates, (2) improves compression performance by up to 79%\nover H.266/VVC, and (3) offers an efficient solution for AI-generated content\nwhile being adaptable to broader content types.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u5148\u9a8c\u7684\u751f\u6210\u5f0f\u7f16\u7801\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u6bd4\u7279\u7387\u538b\u7f29\u6027\u80fd\uff0c\u89c6\u89c9\u4fdd\u771f\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u538b\u7f29\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe79%\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u89c6\u89c9\u5185\u5bb9\u65e5\u76ca\u590d\u6742\uff0c\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u548c\u5b66\u4e60\u65b9\u6cd5\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u96be\u4ee5\u4fdd\u6301\u4e3b\u89c2\u8d28\u91cf\uff0c\u800c\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u6cdb\u5316\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u9884\u4f18\u5316\u7f16\u7801\u5668\u751f\u6210\u901a\u7528\u538b\u7f29\u57df\u8868\u793a\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u548c\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u5185\u90e8\u7279\u5f81\u96c6\u6210\uff0c\u5e76\u5f15\u5165\u5206\u5e03\u91cd\u5f52\u4e00\u5316\u65b9\u6cd5\u4ee5\u589e\u5f3a\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u89c6\u89c9\u4fdd\u771f\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u538b\u7f29\u6027\u80fd\u6bd4H.266/VVC\u63d0\u5347\u9ad8\u8fbe79%\uff0c\u5e76\u4e3aAI\u751f\u6210\u5185\u5bb9\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u9002\u5e94\u66f4\u5e7f\u6cdb\u7684\u5185\u5bb9\u7c7b\u578b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u5148\u9a8c\u7684\u751f\u6210\u5f0f\u7f16\u7801\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u6bd4\u7279\u7387\u4e0b\u7684\u538b\u7f29\u6027\u80fd\uff0c\u5e76\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u9002\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.13956", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13956", "abs": "https://arxiv.org/abs/2509.13956", "authors": ["Zewei Yang", "Zengqi Peng", "Jun Ma"], "title": "SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning", "comment": null, "summary": "Autonomous parking is a critical component for achieving safe and efficient\nurban autonomous driving. However, unstructured environments and dynamic\ninteractions pose significant challenges to autonomous parking tasks. To\naddress this problem, we propose SEG-Parking, a novel end-to-end offline\nreinforcement learning (RL) framework to achieve interaction-aware autonomous\nparking. Notably, a specialized parking dataset is constructed for parking\nscenarios, which include those without interference from the opposite vehicle\n(OV) and complex ones involving interactions with the OV. Based on this\ndataset, a goal-conditioned state encoder is pretrained to map the fused\nperception information into the latent space. Then, an offline RL policy is\noptimized with a conservative regularizer that penalizes out-of-distribution\nactions. Extensive closed-loop experiments are conducted in the high-fidelity\nCARLA simulator. Comparative results demonstrate the superior performance of\nour framework with the highest success rate and robust generalization to\nout-of-distribution parking scenarios. The related dataset and source code will\nbe made publicly available after the paper is accepted.", "AI": {"tldr": "SEG-Parking\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ea4\u4e92\u611f\u77e5\u7684\u81ea\u52a8\u9a7e\u9a76\u505c\u8f66\uff0c\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u73af\u5883\u548c\u52a8\u6001\u4ea4\u4e92\u5bf9\u81ea\u52a8\u9a7e\u9a76\u505c\u8f66\u4efb\u52a1\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86SEG-Parking\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u7684\u76ee\u6807\u6761\u4ef6\u72b6\u6001\u7f16\u7801\u5668\u548c\u5e26\u6709\u4fdd\u5b88\u6b63\u5219\u5316\u5668\u7684\u79bb\u7ebfRL\u7b56\u7565\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e2d\u8fdb\u884c\u7684\u95ed\u73af\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5728\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "SEG-Parking\u6846\u67b6\u5728CARLA\u6a21\u62df\u5668\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u6700\u9ad8\u7684\u6210\u529f\u7387\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u5916\u505c\u8f66\u573a\u666f\u3002\u76f8\u5173\u6570\u636e\u96c6\u548c\u6e90\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u88ab\u63a5\u53d7\u540e\u516c\u5f00\u3002"}}
{"id": "2509.13769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13769", "abs": "https://arxiv.org/abs/2509.13769", "authors": ["Yuechen Luo", "Fang Li", "Shaoqing Xu", "Zhiyi Lai", "Lei Yang", "Qimao Chen", "Ziang Luo", "Zixun Xie", "Shengyin Jiang", "Jiaxin Liu", "Long Chen", "Bing Wang", "Zhi-xin Yang"], "title": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "comment": null, "summary": "While reasoning technology like Chain of Thought (CoT) has been widely\nadopted in Vision Language Action (VLA) models, it demonstrates promising\ncapabilities in end to end autonomous driving. However, recent efforts to\nintegrate CoT reasoning often fall short in simple scenarios, introducing\nunnecessary computational overhead without improving decision quality. To\naddress this, we propose AdaThinkDrive, a novel VLA framework with a dual mode\nreasoning mechanism inspired by fast and slow thinking. First, our framework is\npretrained on large scale autonomous driving (AD) scenarios using both question\nanswering (QA) and trajectory datasets to acquire world knowledge and driving\ncommonsense. During supervised fine tuning (SFT), we introduce a two mode\ndataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the\nmodel to distinguish between scenarios that require reasoning. Furthermore, an\nAdaptive Think Reward strategy is proposed in conjunction with the Group\nRelative Policy Optimization (GRPO), which rewards the model for selectively\napplying CoT by comparing trajectory quality across different reasoning modes.\nExtensive experiments on the Navsim benchmark show that AdaThinkDrive achieves\na PDMS of 90.3, surpassing the best vision only baseline by 1.7 points.\nMoreover, ablations show that AdaThinkDrive surpasses both the never Think and\nalways Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also\nreduces inference time by 14% compared to the always Think baseline,\ndemonstrating its ability to balance accuracy and efficiency through adaptive\nreasoning.", "AI": {"tldr": "AdaThinkDrive\u901a\u8fc7\u53cc\u6a21\u5f0f\u63a8\u7406\u548c\u81ea\u9002\u5e94\u5956\u52b1\u7b56\u7565\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5e73\u8861\u4e86\u51c6\u786e\u6027\u4e0e\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709CoT\u63a8\u7406\u5728VLA\u6a21\u578b\u4e2d\u867d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7b80\u5355\u573a\u666f\u4e2d\u5f15\u5165\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u4e14\u672a\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u9002\u5e94\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAdaThinkDrive\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u6a21\u5f0f\u63a8\u7406\u673a\u5236\uff08\u5feb\u901f\u56de\u7b54\u4e0e\u6162\u901f\u601d\u8003\uff09\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u601d\u8003\u5956\u52b1\u7b56\u7565\u4e0eGRPO\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728Navsim\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdaThinkDrive\u7684PDMS\u8fbe\u523090.3\uff0c\u6bd4\u6700\u4f73\u89c6\u89c9\u57fa\u7ebf\u9ad81.7\u5206\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1114%\u3002", "conclusion": "AdaThinkDrive\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u7406\u673a\u5236\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u4e0e\u6548\u7387\u7684\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u3002"}}
{"id": "2509.13965", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13965", "abs": "https://arxiv.org/abs/2509.13965", "authors": ["Abhijeet Nayak", "D\u00e9bora N. P. Oliveira", "Samiran Gode", "Cordelia Schmid", "Wolfram Burgard"], "title": "MetricNet: Recovering Metric Scale in Generative Navigation Policies", "comment": null, "summary": "Generative navigation policies have made rapid progress in improving\nend-to-end learned navigation. Despite their promising results, this paradigm\nhas two structural problems. First, the sampled trajectories exist in an\nabstract, unscaled space without metric grounding. Second, the control strategy\ndiscards the full path, instead moving directly towards a single waypoint. This\nleads to short-sighted and unsafe actions, moving the robot towards obstacles\nthat a complete and correctly scaled path would circumvent. To address these\nissues, we propose MetricNet, an effective add-on for generative navigation\nthat predicts the metric distance between waypoints, grounding policy outputs\nin real-world coordinates. We evaluate our method in simulation with a new\nbenchmarking framework and show that executing MetricNet-scaled waypoints\nsignificantly improves both navigation and exploration performance. Beyond\nsimulation, we further validate our approach in real-world experiments.\nFinally, we propose MetricNav, which integrates MetricNet into a navigation\npolicy to guide the robot away from obstacles while still moving towards the\ngoal.", "AI": {"tldr": "MetricNet\u901a\u8fc7\u9884\u6d4b\u8def\u5f84\u70b9\u7684\u771f\u5b9e\u8ddd\u79bb\u89e3\u51b3\u4e86\u751f\u6210\u5bfc\u822a\u7b56\u7565\u7684\u5ea6\u91cf\u95ee\u9898\uff0c\u7ed3\u5408MetricNav\u6539\u5584\u4e86\u5bfc\u822a\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u5bfc\u822a\u7b56\u7565\u5b58\u5728\u4e24\u4e2a\u7ed3\u6784\u6027\u95ee\u9898\uff1a\u8f68\u8ff9\u5728\u62bd\u8c61\u7a7a\u95f4\u4e2d\u7f3a\u4e4f\u5ea6\u91cf\u57fa\u7840\uff0c\u4ee5\u53ca\u63a7\u5236\u7b56\u7565\u4e22\u5f03\u5b8c\u6574\u8def\u5f84\u5bfc\u81f4\u77ed\u89c6\u548c\u4e0d\u5b89\u5168\u7684\u52a8\u4f5c\u3002", "method": "\u63d0\u51faMetricNet\u4f5c\u4e3a\u751f\u6210\u5bfc\u822a\u7684\u9644\u52a0\u6a21\u5757\uff0c\u9884\u6d4b\u8def\u5f84\u70b9\u4e4b\u95f4\u7684\u771f\u5b9e\u8ddd\u79bb\uff0c\u5e76\u5c06MetricNet\u6574\u5408\u5230\u5bfc\u822a\u7b56\u7565\u4e2d\u5f62\u6210MetricNav\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\uff0c\u6267\u884cMetricNet\u7f29\u653e\u7684\u8def\u5f84\u70b9\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u548c\u63a2\u7d22\u6027\u80fd\u3002", "conclusion": "MetricNet\u548cMetricNav\u901a\u8fc7\u9884\u6d4b\u8def\u5f84\u70b9\u7684\u771f\u5b9e\u8ddd\u79bb\u5e76\u6574\u5408\u5230\u5bfc\u822a\u7b56\u7565\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u548c\u63a2\u7d22\u6027\u80fd\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.13776", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13776", "abs": "https://arxiv.org/abs/2509.13776", "authors": ["Chao Shuai", "Gaojian Wang", "Kun Pan", "Tong Wu", "Fanli Jin", "Haohan Tan", "Mengxiang Li", "Zhenguang Liu", "Feng Lin", "Kui Ren"], "title": "Morphology-optimized Multi-Scale Fusion: Combining Local Artifacts and Mesoscopic Semantics for Deepfake Detection and Localization", "comment": "The 3rd Place, IJCAI 2025 Workshop on Deepfake Detection,\n  Localization, and Interpretability", "summary": "While the pursuit of higher accuracy in deepfake detection remains a central\ngoal, there is an increasing demand for precise localization of manipulated\nregions. Despite the remarkable progress made in classification-based\ndetection, accurately localizing forged areas remains a significant challenge.\nA common strategy is to incorporate forged region annotations during model\ntraining alongside manipulated images. However, such approaches often neglect\nthe complementary nature of local detail and global semantic context, resulting\nin suboptimal localization performance. Moreover, an often-overlooked aspect is\nthe fusion strategy between local and global predictions. Naively combining the\noutputs from both branches can amplify noise and errors, thereby undermining\nthe effectiveness of the localization.\n  To address these issues, we propose a novel approach that independently\npredicts manipulated regions using both local and global perspectives. We\nemploy morphological operations to fuse the outputs, effectively suppressing\nnoise while enhancing spatial coherence. Extensive experiments reveal the\neffectiveness of each module in improving the accuracy and robustness of\nforgery localization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u89c6\u89d2\u7684\u6df1\u5ea6\u4f2a\u9020\u533a\u57df\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f62\u6001\u5b66\u64cd\u4f5c\u878d\u5408\u8f93\u51fa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5728\u5206\u7c7b\u57fa\u7840\u4e0a\u867d\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4f2a\u9020\u533a\u57df\u7684\u7cbe\u786e\u5b9a\u4f4d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u89c6\u5c40\u90e8\u7ec6\u8282\u4e0e\u5168\u5c40\u8bed\u4e49\u7684\u4e92\u8865\u6027\uff0c\u5bfc\u81f4\u5b9a\u4f4d\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u548c\u5168\u5c40\u89c6\u89d2\u72ec\u7acb\u9884\u6d4b\u4f2a\u9020\u533a\u57df\uff0c\u5e76\u4f7f\u7528\u5f62\u6001\u5b66\u64cd\u4f5c\u878d\u5408\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u63d0\u5347\u4f2a\u9020\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u6548\u679c\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5c40\u90e8\u548c\u5168\u5c40\u89c6\u89d2\u72ec\u7acb\u9884\u6d4b\u4f2a\u9020\u533a\u57df\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5f62\u6001\u5b66\u64cd\u4f5c\u878d\u5408\u8f93\u51fa\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f2a\u9020\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.13972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13972", "abs": "https://arxiv.org/abs/2509.13972", "authors": ["Asier Bikandi", "Miguel Fernandez-Cortizas", "Muhammad Shaheer", "Ali Tourani", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "BIM Informed Visual SLAM for Construction Monitoring", "comment": "8 pages, 5 tables, 4 figures", "summary": "Simultaneous Localization and Mapping (SLAM) is a key tool for monitoring\nconstruction sites, where aligning the evolving as-built state with the\nas-planned design enables early error detection and reduces costly rework.\nLiDAR-based SLAM achieves high geometric precision, but its sensors are\ntypically large and power-demanding, limiting their use on portable platforms.\nVisual SLAM offers a practical alternative with lightweight cameras already\nembedded in most mobile devices. however, visually mapping construction\nenvironments remains challenging: repetitive layouts, occlusions, and\nincomplete or low-texture structures often cause drift in the trajectory map.\nTo mitigate this, we propose an RGB-D SLAM system that incorporates the\nBuilding Information Model (BIM) as structural prior knowledge. Instead of\nrelying solely on visual cues, our system continuously establishes\ncorrespondences between detected wall and their BIM counterparts, which are\nthen introduced as constraints in the back-end optimization. The proposed\nmethod operates in real time and has been validated on real construction sites,\nreducing trajectory error by an average of 23.71% and map RMSE by 7.14%\ncompared to visual SLAM baselines. These results demonstrate that BIM\nconstraints enable reliable alignment of the digital plan with the as-built\nscene, even under partially constructed conditions.", "AI": {"tldr": "\u7ed3\u5408BIM\u7684RGB-D SLAM\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u65bd\u5de5\u73af\u5883\u4e2dSLAM\u7684\u7cbe\u5ea6\uff0c\u51cf\u5c11\u8f68\u8ff9\u548c\u5730\u56fe\u8bef\u5dee\u3002", "motivation": "\u65bd\u5de5\u73af\u5883\u4e2d\u91cd\u590d\u5e03\u5c40\u3001\u906e\u6321\u548c\u4f4e\u7eb9\u7406\u7ed3\u6784\u5bfc\u81f4\u89c6\u89c9SLAM\u7684\u8f68\u8ff9\u6f02\u79fb\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u5bf9\u9f50\u6570\u5b57\u8ba1\u5212\u4e0e\u5b9e\u666f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdRGB-D SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06BIM\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\u77e5\u8bc6\uff0c\u6301\u7eed\u5efa\u7acb\u68c0\u6d4b\u5230\u7684\u5899\u9762\u4e0eBIM\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5173\u7cfb\u4f5c\u4e3a\u540e\u7aef\u4f18\u5316\u7684\u7ea6\u675f\u3002", "result": "\u5728\u771f\u5b9e\u65bd\u5de5\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0c\u8f68\u8ff9\u8bef\u5dee\u5e73\u5747\u51cf\u5c1123.71%\uff0c\u5730\u56feRMSE\u51cf\u5c117.14%\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684RGB-D SLAM\u7cfb\u7edf\u7ed3\u5408BIM\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\u77e5\u8bc6\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u89c6\u89c9SLAM\u5728\u65bd\u5de5\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u8bef\u5dee\u548c\u5730\u56feRMSE\uff0c\u63d0\u5347\u4e86\u6570\u5b57\u8ba1\u5212\u4e0e\u5b9e\u666f\u4e4b\u95f4\u7684\u5bf9\u9f50\u53ef\u9760\u6027\u3002"}}
{"id": "2509.13688", "categories": ["cs.GR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13688", "abs": "https://arxiv.org/abs/2509.13688", "authors": ["James Jincheng", "Youcheng Cai", "Ligang Liu"], "title": "CraftMesh: High-Fidelity Generative Mesh Manipulation via Poisson Seamless Fusion", "comment": null, "summary": "Controllable, high-fidelity mesh editing remains a significant challenge in\n3D content creation. Existing generative methods often struggle with complex\ngeometries and fail to produce detailed results. We propose CraftMesh, a novel\nframework for high-fidelity generative mesh manipulation via Poisson Seamless\nFusion. Our key insight is to decompose mesh editing into a pipeline that\nleverages the strengths of 2D and 3D generative models: we edit a 2D reference\nimage, then generate a region-specific 3D mesh, and seamlessly fuse it into the\noriginal model. We introduce two core techniques: Poisson Geometric Fusion,\nwhich utilizes a hybrid SDF/Mesh representation with normal blending to achieve\nharmonious geometric integration, and Poisson Texture Harmonization for\nvisually consistent texture blending. Experimental results demonstrate that\nCraftMesh outperforms state-of-the-art methods, delivering superior global\nconsistency and local detail in complex editing tasks.", "AI": {"tldr": "CraftMesh\u901a\u8fc72D\u548c3D\u751f\u6210\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f3D\u7f51\u683c\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u51e0\u4f55\u4f53\u7684\u7f16\u8f91\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u5728\u590d\u6742\u51e0\u4f55\u4f53\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u96be\u4ee5\u4ea7\u751f\u9ad8\u4fdd\u771f\u7ed3\u679c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7f51\u683c\u7f16\u8f91\u6846\u67b6\u3002", "method": "\u5c06\u7f51\u683c\u7f16\u8f91\u5206\u89e3\u4e3a2D\u53c2\u8003\u56fe\u50cf\u7f16\u8f91\u3001\u533a\u57df\u7279\u5b9a3D\u7f51\u683c\u751f\u6210\u548c\u4e0e\u539f\u6a21\u578b\u65e0\u7f1d\u878d\u5408\u7684\u6d41\u7a0b\uff0c\u7ed3\u5408Poisson Geometric Fusion\u548cPoisson Texture Harmonization\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCraftMesh\u5728\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u7ec6\u8282\u3002", "conclusion": "CraftMesh\u901a\u8fc7Poisson Seamless Fusion\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u76843D\u7f51\u683c\u7f16\u8f91\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u51e0\u4f55\u4f53\u7f16\u8f91\u7684\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u7ec6\u8282\u8868\u73b0\u3002"}}
{"id": "2509.13784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13784", "abs": "https://arxiv.org/abs/2509.13784", "authors": ["Hanfang Liang", "Bing Wang", "Shizhen Zhang", "Wen Jiang", "Yizhuo Yang", "Weixiang Guo", "Shenghai Yuan"], "title": "CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling", "comment": "8 pages, 6 figures", "summary": "Event cameras capture asynchronous pixel-level brightness changes with\nmicrosecond temporal resolution, offering unique advantages for high-speed\nvision tasks. Existing methods often convert event streams into intermediate\nrepresentations such as frames, voxel grids, or point clouds, which inevitably\nrequire predefined time windows and thus introduce window latency. Meanwhile,\npointwise detection methods face computational challenges that prevent\nreal-time efficiency due to their high computational cost. To overcome these\nlimitations, we propose the Variable-Rate Spatial Event Mamba, a novel\narchitecture that directly processes raw event streams without intermediate\nrepresentations. Our method introduces a lightweight causal spatial\nneighborhood encoder to efficiently capture local geometric relations, followed\nby Mamba-based state space models for scalable temporal modeling with linear\ncomplexity. During inference, a controller adaptively adjusts the processing\nspeed according to the event rate, achieving an optimal balance between window\nlatency and inference latency.", "AI": {"tldr": "\u63d0\u51faVariable-Rate Spatial Event Mamba\uff0c\u76f4\u63a5\u5904\u7406\u4e8b\u4ef6\u6d41\uff0c\u65e0\u9700\u4e2d\u95f4\u8868\u793a\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u89c6\u89c9\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9884\u5b9a\u4e49\u65f6\u95f4\u7a97\u53e3\u5f15\u5165\u7a97\u53e3\u5ef6\u8fdf\uff0c\u800c\u9010\u70b9\u68c0\u6d4b\u65b9\u6cd5\u56e0\u9ad8\u8ba1\u7b97\u6210\u672c\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u56e0\u679c\u7a7a\u95f4\u90bb\u57df\u7f16\u7801\u5668\u6355\u83b7\u5c40\u90e8\u51e0\u4f55\u5173\u7cfb\uff0c\u7ed3\u5408\u57fa\u4e8eMamba\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u53ef\u6269\u5c55\u65f6\u95f4\u5efa\u6a21\u3002", "result": "\u65b0\u67b6\u6784\u76f4\u63a5\u5904\u7406\u539f\u59cb\u4e8b\u4ef6\u6d41\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u5904\u7406\u901f\u5ea6\uff0c\u4f18\u5316\u4e86\u5ef6\u8fdf\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002", "conclusion": "Variable-Rate Spatial Event Mamba\u67b6\u6784\u901a\u8fc7\u76f4\u63a5\u5904\u7406\u539f\u59cb\u4e8b\u4ef6\u6d41\uff0c\u65e0\u9700\u4e2d\u95f4\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u901f\u89c6\u89c9\u4efb\u52a1\u7684\u9ad8\u6548\u5904\u7406\uff0c\u5e73\u8861\u4e86\u7a97\u53e3\u5ef6\u8fdf\u548c\u63a8\u7406\u5ef6\u8fdf\u3002"}}
{"id": "2509.13998", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13998", "abs": "https://arxiv.org/abs/2509.13998", "authors": ["Bailey Dacre", "Rodrigo Moreno", "Serhat Demirtas", "Ziqiao Wang", "Yuhao Jiang", "Jamie Paik", "Kasper Stoy", "Andr\u00e9s Fa\u00ed\u00f1a"], "title": "Flexible and Foldable: Workspace Analysis and Object Manipulation Using a Soft, Interconnected, Origami-Inspired Actuator Array", "comment": null, "summary": "Object manipulation is a fundamental challenge in robotics, where systems\nmust balance trade-offs among manipulation capabilities, system complexity, and\nthroughput. Distributed manipulator systems (DMS) use the coordinated motion of\nactuator arrays to perform complex object manipulation tasks, seeing widespread\nexploration within the literature and in industry. However, existing DMS\ndesigns typically rely on high actuator densities and impose constraints on\nobject-to-actuator scale ratios, limiting their adaptability. We present a\nnovel DMS design utilizing an array of 3-DoF, origami-inspired robotic tiles\ninterconnected by a compliant surface layer. Unlike conventional DMS, our\napproach enables manipulation not only at the actuator end effectors but also\nacross a flexible surface connecting all actuators; creating a continuous,\ncontrollable manipulation surface. We analyse the combined workspace of such a\nsystem, derive simple motion primitives, and demonstrate its capabilities to\ntranslate simple geometric objects across an array of tiles. By leveraging the\ninter-tile connective material, our approach significantly reduces actuator\ndensity, increasing the area over which an object can be manipulated by x1.84\nwithout an increase in the number of actuators. This design offers a lower cost\nand complexity alternative to traditional high-density arrays, and introduces\nnew opportunities for manipulation strategies that leverage the flexibility of\nthe interconnected surface.", "AI": {"tldr": "\u65b0\u578bDMS\u8bbe\u8ba1\u5229\u75283\u81ea\u7531\u5ea6\u3001\u6298\u7eb8\u542f\u53d1\u7684\u673a\u5668\u4eba\u74e6\u7247\u548c\u67d4\u6027\u8fde\u63a5\u5c42\uff0c\u964d\u4f4e\u6267\u884c\u5668\u5bc6\u5ea6\u5e76\u63d0\u9ad8\u64cd\u7eb5\u80fd\u529b\uff0c\u65e0\u9700\u589e\u52a0\u6267\u884c\u5668\u6570\u91cf\u3002", "motivation": "\u73b0\u6709\u7684DMS\u8bbe\u8ba1\u901a\u5e38\u4f9d\u8d56\u9ad8\u6267\u884c\u5668\u5bc6\u5ea6\uff0c\u5e76\u5bf9\u7269\u4f53\u4e0e\u6267\u884c\u5668\u6bd4\u4f8b\u65bd\u52a0\u9650\u5236\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u65b0\u578b\u8bbe\u8ba1\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u75283\u81ea\u7531\u5ea6\u3001\u53d7\u6298\u7eb8\u542f\u53d1\u7684\u673a\u5668\u4eba\u74e6\u7247\u9635\u5217\uff0c\u5e76\u901a\u8fc7\u67d4\u6027\u8868\u9762\u5c42\u8fde\u63a5\u8fd9\u4e9b\u6267\u884c\u5668\uff0c\u5f62\u6210\u4e00\u4e2a\u8fde\u7eed\u53ef\u63a7\u7684\u64cd\u7eb5\u8868\u9762\u3002\u5206\u6790\u4e86\u7cfb\u7edf\u7684\u8054\u5408\u5de5\u4f5c\u7a7a\u95f4\uff0c\u63a8\u5bfc\u4e86\u7b80\u5355\u7684\u8fd0\u52a8\u539f\u8bed\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u74e6\u7247\u9635\u5217\u4e0a\u5e73\u79fb\u7b80\u5355\u51e0\u4f55\u7269\u4f53\u7684\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5229\u7528\u74e6\u7247\u95f4\u8fde\u63a5\u6750\u6599\uff0c\u8be5\u8bbe\u8ba1\u5c06\u6267\u884c\u5668\u5bc6\u5ea6\u663e\u8457\u964d\u4f4e\uff0c\u4f7f\u7269\u4f53\u64cd\u7eb5\u9762\u79ef\u589e\u52a0\u4e861.84\u500d\uff0c\u800c\u65e0\u9700\u589e\u52a0\u6267\u884c\u5668\u6570\u91cf\u3002\u8fd9\u4e3a\u4f20\u7edf\u9ad8\u5bc6\u5ea6\u9635\u5217\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u3001\u4f4e\u590d\u6742\u6027\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5206\u5e03\u5f0f\u64cd\u7eb5\u5668\u7cfb\u7edf\uff08DMS\uff09\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5229\u75283\u81ea\u7531\u5ea6\u3001\u53d7\u6298\u7eb8\u542f\u53d1\u7684\u673a\u5668\u4eba\u74e6\u7247\u9635\u5217\u548c\u67d4\u6027\u8fde\u63a5\u5c42\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6267\u884c\u5668\u5bc6\u5ea6\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u64cd\u7eb5\u80fd\u529b\u3002"}}
{"id": "2509.13789", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13789", "abs": "https://arxiv.org/abs/2509.13789", "authors": ["Hanshuai Cui", "Zhiqing Tang", "Zhifei Xu", "Zhi Yao", "Wenyi Zeng", "Weijia Jia"], "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching", "comment": null, "summary": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality.", "AI": {"tldr": "BWCache\u901a\u8fc7\u52a8\u6001\u7f13\u5b58\u548c\u91cd\u7528DiT\u5757\u7279\u5f81\uff0c\u52a0\u901f\u89c6\u9891\u751f\u6210\uff0c\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "DiT\u5728\u89c6\u9891\u751f\u6210\u4e2d\u7684\u987a\u5e8f\u53bb\u566a\u8fc7\u7a0b\u5bfc\u81f4\u4e0d\u53ef\u907f\u514d\u7684\u5ef6\u8fdf\uff0c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u8981\u4e48\u727a\u7272\u89c6\u89c9\u8d28\u91cf\uff0c\u8981\u4e48\u65e0\u6cd5\u9002\u5f53\u91cd\u7528\u4e2d\u95f4\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e86Block-Wise Caching (BWCache)\u65b9\u6cd5\uff0c\u52a8\u6001\u7f13\u5b58\u548c\u91cd\u7528DiT\u5757\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u76f8\u4f3c\u6027\u6307\u6807\u4ee5\u6700\u5c0f\u5316\u5197\u4f59\u8ba1\u7b97\u3002", "result": "BWCache\u5728\u591a\u4e2a\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.24\u500d\u7684\u52a0\u901f\uff0c\u4e14\u89c6\u89c9\u8d28\u91cf\u76f8\u5f53\u3002", "conclusion": "BWCache\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u7f13\u5b58\u548c\u91cd\u7528DiT\u5757\u7279\u5f81\uff0c\u663e\u8457\u52a0\u901f\u4e86\u57fa\u4e8eDiT\u7684\u89c6\u9891\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u3002"}}
{"id": "2509.14010", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14010", "abs": "https://arxiv.org/abs/2509.14010", "authors": ["Zong Chen", "Shaoyang Li", "Ben Liu", "Min Li", "Zhouping Yin", "Yiqun Li"], "title": "Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization", "comment": null, "summary": "Wheel-legged robots with integrated manipulators hold great promise for\nmobile manipulation in logistics, industrial automation, and human-robot\ncollaboration. However, unified control of such systems remains challenging due\nto the redundancy in degrees of freedom, complex wheel-ground contact dynamics,\nand the need for seamless coordination between locomotion and manipulation. In\nthis work, we present the design and whole-body motion control of an\nomnidirectional wheel-legged quadrupedal robot equipped with a dexterous\nmanipulator. The proposed platform incorporates independently actuated steering\nmodules and hub-driven wheels, enabling agile omnidirectional locomotion with\nhigh maneuverability in structured environments. To address the challenges of\ncontact-rich interaction, we develop a contact-aware whole-body dynamic\noptimization framework that integrates point-contact modeling for manipulation\nwith line-contact modeling for wheel-ground interactions. A warm-start strategy\nis introduced to accelerate online optimization, ensuring real-time feasibility\nfor high-dimensional control. Furthermore, a unified kinematic model tailored\nfor the robot's 4WIS-4WID actuation scheme eliminates the need for mode\nswitching across different locomotion strategies, improving control consistency\nand robustness. Simulation and experimental results validate the effectiveness\nof the proposed framework, demonstrating agile terrain traversal, high-speed\nomnidirectional mobility, and precise manipulation under diverse scenarios,\nunderscoring the system's potential for factory automation, urban logistics,\nand service robotics in semi-structured environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6e\u817f\u5f0f\u56db\u8db3\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u548c\u5168\u8eab\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a5\u89e6\u611f\u77e5\u7684\u52a8\u6001\u4f18\u5316\u6846\u67b6\u548c\u7edf\u4e00\u7684\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u64cd\u4f5c\u4e0e\u8fd0\u52a8\u534f\u8c03\u7684\u6311\u6218\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u7269\u6d41\u548c\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u8f6e\u817f\u5f0f\u673a\u5668\u4eba\u7ed3\u5408\u673a\u68b0\u81c2\u5728\u7269\u6d41\u3001\u5de5\u4e1a\u81ea\u52a8\u5316\u548c\u4eba\u673a\u534f\u4f5c\u7b49\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u6b64\u7c7b\u7cfb\u7edf\u7684\u7edf\u4e00\u63a7\u5236\u4ecd\u9762\u4e34\u81ea\u7531\u5ea6\u5197\u4f59\u3001\u590d\u6742\u8f6e\u5730\u63a5\u89e6\u52a8\u529b\u5b66\u4ee5\u53ca\u8fd0\u52a8\u4e0e\u64cd\u4f5c\u65e0\u7f1d\u534f\u8c03\u7b49\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u63a5\u89e6\u611f\u77e5\u7684\u5168\u8eab\u52a8\u6001\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u70b9\u63a5\u89e6\u6a21\u578b\uff08\u7528\u4e8e\u64cd\u4f5c\uff09\u548c\u7ebf\u63a5\u89e6\u6a21\u578b\uff08\u7528\u4e8e\u8f6e\u5730\u4ea4\u4e92\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u70ed\u542f\u52a8\u7b56\u7565\u4ee5\u52a0\u901f\u5728\u7ebf\u4f18\u5316\uff0c\u786e\u4fdd\u9ad8\u7ef4\u63a7\u5236\u7684\u5b9e\u65f6\u53ef\u884c\u6027\u3002\u6b64\u5916\uff0c\u4e3a\u673a\u5668\u4eba\u76844WIS-4WID\u9a71\u52a8\u65b9\u6848\u5b9a\u5236\u4e86\u7edf\u4e00\u7684\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u6d88\u9664\u4e86\u4e0d\u540c\u8fd0\u52a8\u7b56\u7565\u95f4\u6a21\u5f0f\u5207\u6362\u7684\u9700\u6c42\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u673a\u5668\u4eba\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u654f\u6377\u5730\u5f62\u7a7f\u8d8a\u3001\u9ad8\u901f\u5168\u65b9\u4f4d\u79fb\u52a8\u548c\u7cbe\u786e\u64cd\u4f5c\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u673a\u5668\u4eba\u5728\u5730\u5f62\u7a7f\u8d8a\u3001\u9ad8\u901f\u5168\u65b9\u4f4d\u79fb\u52a8\u548c\u7cbe\u786e\u64cd\u4f5c\u65b9\u9762\u7684\u80fd\u529b\uff0c\u51f8\u663e\u4e86\u5176\u5728\u534a\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5de5\u5382\u81ea\u52a8\u5316\u3001\u57ce\u5e02\u7269\u6d41\u548c\u670d\u52a1\u673a\u5668\u4eba\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13792", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13792", "abs": "https://arxiv.org/abs/2509.13792", "authors": ["Inder Pal Singh", "Nidhal Eddine Chenni", "Abd El Rahman Shabayek", "Arunkumar Rathinam", "Djamila Aouada"], "title": "Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation", "comment": null, "summary": "Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous\nspace operations such as rendezvous, docking, and in-orbit servicing. Hybrid\npipelines that combine object detection, keypoint regression, and\nPerspective-n-Point (PnP) solvers have recently achieved strong results on\nsynthetic datasets, yet their performance deteriorates sharply on real or\nlab-generated imagery due to the persistent synthetic-to-real domain gap.\nExisting unsupervised domain adaptation approaches aim to mitigate this issue\nbut often underperform when a modest number of labeled target samples are\navailable. In this work, we propose the first Supervised Domain Adaptation\n(SDA) framework tailored for SPE keypoint regression. Building on the Learning\nInvariant Representation and Risk (LIRR) paradigm, our method jointly optimizes\ndomain-invariant representations and task-specific risk using both labeled\nsynthetic and limited labeled real data, thereby reducing generalization error\nunder domain shift. Extensive experiments on the SPEED+ benchmark demonstrate\nthat our approach consistently outperforms source-only, fine-tuning, and oracle\nbaselines. Notably, with only 5% labeled target data, our method matches or\nsurpasses oracle performance trained on larger fractions of labeled data. The\nframework is lightweight, backbone-agnostic, and computationally efficient,\noffering a practical pathway toward robust and deployable spacecraft pose\nestimation in real-world space environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76d1\u7763\u57df\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u822a\u5929\u5668\u59ff\u6001\u4f30\u8ba1\u5173\u952e\u70b9\u56de\u5f52\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u57df\u4e0d\u53d8\u8868\u793a\u548c\u4efb\u52a1\u7279\u5b9a\u98ce\u9669\uff0c\u5728\u6709\u9650\u6807\u8bb0\u771f\u5b9e\u6570\u636e\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u822a\u5929\u5668\u59ff\u6001\u4f30\u8ba1\uff08SPE\uff09\u662f\u81ea\u4e3b\u7a7a\u95f4\u64cd\u4f5c\uff08\u5982\u4ea4\u4f1a\u3001\u5bf9\u63a5\u548c\u5728\u8f68\u670d\u52a1\uff09\u7684\u57fa\u7840\u80fd\u529b\u3002\u5c3d\u7ba1\u6df7\u5408\u7ba1\u9053\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u771f\u5b9e\u6216\u5b9e\u9a8c\u5ba4\u751f\u6210\u7684\u56fe\u50cf\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5408\u6210\u5230\u771f\u5b9e\u7684\u57df\u5dee\u8ddd\u3002\u73b0\u6709\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\u5728\u5c11\u91cf\u6807\u8bb0\u76ee\u6807\u6837\u672c\u53ef\u7528\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u76d1\u7763\u57df\u9002\u5e94\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u5b66\u4e60\u4e0d\u53d8\u8868\u793a\u548c\u98ce\u9669\uff08LIRR\uff09\u8303\u5f0f\uff0c\u8be5\u65b9\u6cd5\u8054\u5408\u4f18\u5316\u4e86\u57df\u4e0d\u53d8\u8868\u793a\u548c\u4efb\u52a1\u7279\u5b9a\u98ce\u9669\uff0c\u5229\u7528\u6807\u8bb0\u7684\u5408\u6210\u6570\u636e\u548c\u6709\u9650\u7684\u6807\u8bb0\u771f\u5b9e\u6570\u636e\uff0c\u51cf\u5c11\u4e86\u57df\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u8bef\u5dee\u3002", "result": "\u5728SPEED+\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u4ec5\u6e90\u57df\u3001\u5fae\u8c03\u548c\u57fa\u51c6\u6a21\u578b\u3002\u7279\u522b\u662f\uff0c\u4ec5\u4f7f\u75285%\u7684\u6807\u8bb0\u76ee\u6807\u6570\u636e\uff0c\u8be5\u65b9\u6cd5\u5c31\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u57fa\u4e8e\u66f4\u5927\u6bd4\u4f8b\u6807\u8bb0\u6570\u636e\u8bad\u7ec3\u7684\u57fa\u51c6\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u76d1\u7763\u57df\u9002\u5e94\uff08SDA\uff09\u6846\u67b6\u5728\u822a\u5929\u5668\u59ff\u6001\u4f30\u8ba1\uff08SPE\uff09\u5173\u952e\u70b9\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u6709\u9650\u6807\u8bb0\u771f\u5b9e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u57fa\u4e8e\u5927\u91cf\u6807\u8bb0\u6570\u636e\u7684\u57fa\u51c6\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u8f7b\u91cf\u7ea7\u3001\u4e0e\u9aa8\u5e72\u7f51\u7edc\u65e0\u5173\u4e14\u8ba1\u7b97\u9ad8\u6548\uff0c\u4e3a\u5b9e\u9645\u7a7a\u95f4\u73af\u5883\u4e2d\u7684\u7a33\u5065\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.14025", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14025", "abs": "https://arxiv.org/abs/2509.14025", "authors": ["Rui Huang", "Zhiyu Gao", "Siyu Tang", "Jialin Zhang", "Lei He", "Ziqian Zhang", "Lin Zhao"], "title": "TransforMARS: Fault-Tolerant Self-Reconfiguration for Arbitrarily Shaped Modular Aerial Robot Systems", "comment": null, "summary": "Modular Aerial Robot Systems (MARS) consist of multiple drone modules that\nare physically bound together to form a single structure for flight. Exploiting\nstructural redundancy, MARS can be reconfigured into different formations to\nmitigate unit or rotor failures and maintain stable flight. Prior work on MARS\nself-reconfiguration has solely focused on maximizing controllability margins\nto tolerate a single rotor or unit fault for rectangular-shaped MARS. We\npropose TransforMARS, a general fault-tolerant reconfiguration framework that\ntransforms arbitrarily shaped MARS under multiple rotor and unit faults while\nensuring continuous in-air stability. Specifically, we develop algorithms to\nfirst identify and construct minimum controllable assemblies containing faulty\nunits. We then plan feasible disassembly-assembly sequences to transport MARS\nunits or subassemblies to form target configuration. Our approach enables more\nflexible and practical feasible reconfiguration. We validate TransforMARS in\nchallenging arbitrarily shaped MARS configurations, demonstrating substantial\nimprovements over prior works in both the capacity of handling diverse\nconfigurations and the number of faults tolerated. The videos and source code\nof this work are available at the anonymous repository:\nhttps://anonymous.4open.science/r/TransforMARS-1030/", "AI": {"tldr": "TransforMARS \u662f\u4e00\u79cd\u901a\u7528\u7684\u6545\u969c\u5bb9\u5fcd\u91cd\u6784\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u5f62\u72b6\u7684 MARS\uff0c\u80fd\u5904\u7406\u591a\u6545\u969c\u5e76\u786e\u4fdd\u7a7a\u4e2d\u7a33\u5b9a\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684 MARS \u81ea\u91cd\u6784\u7814\u7a76\u4ec5\u5173\u6ce8\u77e9\u5f62\u5f62\u72b6\u7684 MARS\uff0c\u4e14\u4ec5\u80fd\u5bb9\u5fcd\u5355\u4e2a\u8f6c\u5b50\u6216\u5355\u5143\u6545\u969c\u3002TransforMARS \u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u548c\u5b9e\u7528\u7684\u91cd\u6784\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u7b97\u6cd5\uff0c\u9996\u5148\u8bc6\u522b\u5e76\u6784\u5efa\u5305\u542b\u6545\u969c\u5355\u5143\u7684\u6700\u5c0f\u53ef\u63a7\u7ec4\u4ef6\uff0c\u7136\u540e\u89c4\u5212\u53ef\u884c\u7684\u62c6\u5378-\u7ec4\u88c5\u5e8f\u5217\u4ee5\u8fd0\u8f93 MARS \u5355\u5143\u6216\u5b50\u7ec4\u4ef6\u5f62\u6210\u76ee\u6807\u914d\u7f6e\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u610f\u5f62\u72b6 MARS \u914d\u7f6e\u4e2d\u9a8c\u8bc1\u4e86 TransforMARS\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u591a\u6837\u914d\u7f6e\u548c\u5bb9\u5fcd\u591a\u6545\u969c\u65b9\u9762\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "TransforMARS \u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6545\u969c\u5bb9\u5fcd\u91cd\u6784\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u5f62\u72b6\u7684 MARS \u5728\u591a\u4e2a\u8f6c\u5b50\u548c\u5355\u5143\u6545\u969c\u4e0b\u7684\u91cd\u6784\uff0c\u540c\u65f6\u786e\u4fdd\u7a7a\u4e2d\u7a33\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u591a\u6837\u914d\u7f6e\u548c\u5bb9\u5fcd\u6545\u969c\u6570\u91cf\u7684\u80fd\u529b\u3002"}}
{"id": "2509.13795", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13795", "abs": "https://arxiv.org/abs/2509.13795", "authors": ["Jiayu Yuan", "Ming Dai", "Enhui Zheng", "Chao Su", "Nanxing Chen", "Qiming Hu", "Shibo Zhu", "Yibin Cao"], "title": "SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient 4-DoF UAV Localization in GNSS-Denied Environments", "comment": null, "summary": "Vision-based Unmanned Aerial Vehicle (UAV) localization systems have been\nextensively investigated for Global Navigation Satellite System (GNSS)-denied\nenvironments. However, existing retrieval-based approaches face limitations in\ndataset availability and persistent challenges including suboptimal real-time\nperformance, environmental sensitivity, and limited generalization capability,\nparticularly in dynamic or temporally varying environments. To overcome these\nlimitations, we present a large-scale Multi-Altitude Flight Segments dataset\n(MAFS) for variable altitude scenarios and propose a novel Semantic-Weighted\nAdaptive Particle Filter (SWA-PF) method. This approach integrates robust\nsemantic features from both UAV-captured images and satellite imagery through\ntwo key innovations: a semantic weighting mechanism and an optimized particle\nfiltering architecture. Evaluated using our dataset, the proposed method\nachieves 10x computational efficiency gain over feature extraction methods,\nmaintains global positioning errors below 10 meters, and enables rapid 4 degree\nof freedom (4-DoF) pose estimation within seconds using accessible\nlow-resolution satellite maps. Code and dataset will be available at\nhttps://github.com/YuanJiayuuu/SWA-PF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSWA-PF\u65b9\u6cd5\uff0c\u7528\u4e8eGNSS\u7f3a\u5931\u73af\u5883\u4e0b\u7684UAV\u5b9a\u4f4d\uff0c\u7ed3\u5408\u8bed\u4e49\u7279\u5f81\u548c\u4f18\u5316\u7c92\u5b50\u6ee4\u6ce2\uff0c\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u7684UAV\u5b9a\u4f4d\u65b9\u6cd5\u5728\u6570\u636e\u96c6\u53ef\u7528\u6027\u3001\u5b9e\u65f6\u6027\u80fd\u3001\u73af\u5883\u654f\u611f\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u6216\u65f6\u95f4\u53d8\u5316\u7684\u73af\u5883\u4e2d\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u4e49\u52a0\u6743\u81ea\u9002\u5e94\u7c92\u5b50\u6ee4\u6ce2\uff08SWA-PF\uff09\u65b9\u6cd5\uff0c\u6574\u5408\u4e86\u65e0\u4eba\u673a\u6355\u83b7\u56fe\u50cf\u548c\u536b\u661f\u56fe\u50cf\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u901a\u8fc7\u8bed\u4e49\u52a0\u6743\u673a\u5236\u548c\u4f18\u5316\u7684\u7c92\u5b50\u6ee4\u6ce2\u67b6\u6784\u5b9e\u73b0\u5b9a\u4f4d\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u6bd4\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u63d0\u5347\u4e8610\u500d\uff0c\u5168\u5c40\u5b9a\u4f4d\u8bef\u5dee\u4fdd\u6301\u572810\u7c73\u4ee5\u4e0b\uff0c\u5e76\u80fd\u5728\u51e0\u79d2\u5185\u5b9e\u73b0\u5feb\u901f\u76844\u81ea\u7531\u5ea6\u59ff\u6001\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684SWA-PF\u65b9\u6cd5\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e86UAV\u7684\u5b9a\u4f4d\u6027\u80fd\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u7279\u5f81\u548c\u4f18\u5316\u7c92\u5b50\u6ee4\u6ce2\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u5b9a\u4f4d\u3002"}}
{"id": "2509.14040", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14040", "abs": "https://arxiv.org/abs/2509.14040", "authors": ["Zewen Yang", "Xiaobing Dai", "Dongfa Zhang", "Yu Li", "Ziyang Meng", "Bingkun Huang", "Hamid Sadeghian", "Sami Haddadin"], "title": "Prompt2Auto: From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning", "comment": null, "summary": "Learning from demonstration allows robots to acquire complex skills from\nhuman demonstrations, but conventional approaches often require large datasets\nand fail to generalize across coordinate transformations. In this paper, we\npropose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP)\nlearning framework that enables robots to perform human-guided automated\ncontrol from a single motion prompt. A dataset-construction strategy based on\ncoordinate transformations is introduced that enforces invariance to\ntranslation, rotation, and scaling, while supporting multi-step predictions.\nMoreover, GeoGP is robust to variations in the user's motion prompt and\nsupports multi-skill autonomy. We validate the proposed approach through\nnumerical simulations with the designed user graphical interface and two\nreal-world robotic experiments, which demonstrate that the proposed method is\neffective, generalizes across tasks, and significantly reduces the\ndemonstration burden. Project page is available at:\nhttps://prompt2auto.github.io", "AI": {"tldr": "Prompt2Auto\u662f\u4e00\u79cd\u51e0\u4f55\u4e0d\u53d8\u7684\u4e00\u6b21\u6027\u9ad8\u65af\u8fc7\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u8fd0\u52a8\u63d0\u793a\u5b9e\u73b0\u673a\u5668\u4eba\u63a7\u5236\uff0c\u51cf\u5c11\u6f14\u793a\u8d1f\u62c5\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\u4e14\u96be\u4ee5\u8de8\u5750\u6807\u53d8\u6362\u6cdb\u5316\uff0cPrompt2Auto\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\u5355\u6b21\u8fd0\u52a8\u63d0\u793a\u4e2d\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5750\u6807\u53d8\u6362\u7684\u6570\u636e\u96c6\u6784\u5efa\u7b56\u7565\uff0c\u786e\u4fdd\u5bf9\u5e73\u79fb\u3001\u65cb\u8f6c\u548c\u7f29\u653e\u7684\u51e0\u4f55\u4e0d\u53d8\u6027\uff0c\u5e76\u652f\u6301\u591a\u6b65\u9884\u6d4b\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u6709\u6548\u4e14\u80fd\u8de8\u4efb\u52a1\u6cdb\u5316\u3002", "conclusion": "Prompt2Auto\u901a\u8fc7\u51e0\u4f55\u4e0d\u53d8\u7684\u4e00\u6b21\u6027\u9ad8\u65af\u8fc7\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6f14\u793a\u8d1f\u62c5\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.13801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13801", "abs": "https://arxiv.org/abs/2509.13801", "authors": ["Wenlve Zhou", "Zhiheng Zhou", "Tiantao Xian", "Yikui Zhai", "Weibin Wu", "Biyun Ma"], "title": "Masked Feature Modeling Enhances Adaptive Segmentation", "comment": null, "summary": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to\ntransfer models from a labeled source domain to an unlabeled target domain.\nWhile auxiliary self-supervised tasks-particularly contrastive learning-have\nimproved feature discriminability, masked modeling approaches remain\nunderexplored in this setting, largely due to architectural incompatibility and\nmisaligned optimization objectives. We propose Masked Feature Modeling (MFM), a\nnovel auxiliary task that performs feature masking and reconstruction directly\nin the feature space. Unlike existing masked modeling methods that reconstruct\nlow-level inputs or perceptual features (e.g., HOG or visual tokens), MFM\naligns its learning target with the main segmentation task, ensuring\ncompatibility with standard architectures like DeepLab and DAFormer without\nmodifying the inference pipeline. To facilitate effective reconstruction, we\nintroduce a lightweight auxiliary module, Rebuilder, which is trained jointly\nbut discarded during inference, adding zero computational overhead at test\ntime. Crucially, MFM leverages the segmentation decoder to classify the\nreconstructed features, tightly coupling the auxiliary objective with the\npixel-wise prediction task to avoid interference with the primary task.\nExtensive experiments across various architectures and UDA benchmarks\ndemonstrate that MFM consistently enhances segmentation performance, offering a\nsimple, efficient, and generalizable strategy for unsupervised domain-adaptive\nsemantic segmentation.", "AI": {"tldr": "\u63d0\u51faMFM\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\uff0c\u901a\u8fc7\u7279\u5f81\u63a9\u853d\u548c\u91cd\u5efa\u63d0\u5347\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u63a9\u853d\u5efa\u6a21\u65b9\u6cd5\u7531\u4e8e\u67b6\u6784\u4e0d\u517c\u5bb9\u548c\u4f18\u5316\u76ee\u6807\u4e0d\u4e00\u81f4\u800c\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u56e0\u6b64\u63d0\u51faMFM\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMasked Feature Modeling\uff08MFM\uff09\u7684\u65b0\u578b\u8f85\u52a9\u4efb\u52a1\uff0c\u76f4\u63a5\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7279\u5f81\u63a9\u853d\u548c\u91cd\u5efa\u3002\u5f15\u5165\u8f7b\u91cf\u7ea7\u8f85\u52a9\u6a21\u5757Rebuilder\uff0c\u8bad\u7ec3\u65f6\u8054\u5408\u4f7f\u7528\u4f46\u5728\u63a8\u7406\u65f6\u4e22\u5f03\u3002", "result": "\u5728\u5404\u79cd\u67b6\u6784\u548cUDA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMFM\u4e00\u81f4\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "conclusion": "MFM\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u9ad8\u6548\u4e14\u901a\u7528\u7684\u7b56\u7565\uff0c\u53ef\u663e\u8457\u63d0\u5347\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14063", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14063", "abs": "https://arxiv.org/abs/2509.14063", "authors": ["Sundhar Vinodh Sangeetha", "Chih-Yuan Chiu", "Sarah H. Q. Li", "Shreyas Kousik"], "title": "Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace", "comment": "The last two authors advised equally. Submitted to the 2026 IEEE\n  International Conference on Robotics and Automation. 8 pages, 6 figures", "summary": "Autonomous aircraft must safely operate in untowered airspace, where\ncoordination relies on voice-based communication among human pilots. Safe\noperation requires an aircraft to predict the intent, and corresponding goal\nlocation, of other aircraft. This paper introduces a multimodal framework for\naircraft goal prediction that integrates natural language understanding with\nspatial reasoning to improve autonomous decision-making in such environments.\nWe leverage automatic speech recognition and large language models to\ntranscribe and interpret pilot radio calls, identify aircraft, and extract\ndiscrete intent labels. These intent labels are fused with observed\ntrajectories to condition a temporal convolutional network and Gaussian mixture\nmodel for probabilistic goal prediction. Our method significantly reduces goal\nprediction error compared to baselines that rely solely on motion history,\ndemonstrating that language-conditioned prediction increases prediction\naccuracy. Experiments on a real-world dataset from an untowered airport\nvalidate the approach and highlight its potential to enable socially aware,\nlanguage-conditioned robotic motion planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8bed\u8a00\u7406\u89e3\u548c\u7a7a\u95f4\u63a8\u7406\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u65e0\u5854\u53f0\u7a7a\u57df\u4e2d\u98de\u673a\u7684\u76ee\u6807\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u8fd0\u52a8\u5386\u53f2\u7684\u65b9\u6cd5\u3002", "motivation": "\u65e0\u5854\u53f0\u7a7a\u57df\u4e2d\u98de\u673a\u5b89\u5168\u8fd0\u884c\u4f9d\u8d56\u4e8e\u98de\u884c\u5458\u95f4\u7684\u8bed\u97f3\u901a\u4fe1\uff0c\u800c\u81ea\u4e3b\u98de\u673a\u9700\u51c6\u786e\u9884\u6d4b\u5176\u4ed6\u98de\u673a\u7684\u610f\u56fe\u548c\u76ee\u6807\u4f4d\u7f6e\u4ee5\u786e\u4fdd\u5b89\u5168\u3002", "method": "\u7ed3\u5408\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8f6c\u5f55\u5e76\u89e3\u6790\u98de\u884c\u5458\u65e0\u7ebf\u7535\u901a\u8bdd\uff0c\u8bc6\u522b\u98de\u673a\u5e76\u63d0\u53d6\u79bb\u6563\u610f\u56fe\u6807\u7b7e\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u6807\u7b7e\u4e0e\u89c2\u5bdf\u5230\u7684\u8f68\u8ff9\u878d\u5408\uff0c\u901a\u8fc7\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\u548c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u8fdb\u884c\u6982\u7387\u6027\u76ee\u6807\u9884\u6d4b\u3002", "result": "\u4e0e\u4ec5\u4f9d\u8d56\u8fd0\u52a8\u5386\u53f2\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u672c\u6587\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u76ee\u6807\u9884\u6d4b\u8bef\u5dee\uff0c\u8bc1\u660e\u4e86\u8bed\u8a00\u6761\u4ef6\u9884\u6d4b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u6a21\u6001\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u65e0\u5854\u53f0\u7a7a\u57df\u4e2d\u98de\u673a\u7684\u76ee\u6807\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u9a8c\u8bc1\u4e86\u8bed\u8a00\u6761\u4ef6\u9884\u6d4b\u5728\u63d0\u5347\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13809", "abs": "https://arxiv.org/abs/2509.13809", "authors": ["Nick Theisen", "Kenny Schlegel", "Dietrich Paulus", "Peer Neubert"], "title": "Data-Efficient Spectral Classification of Hyperspectral Data Using MiniROCKET and HDC-MiniROCKET", "comment": "Accepted for publication at IEEE CASE 2025", "summary": "The classification of pixel spectra of hyperspectral images, i.e. spectral\nclassification, is used in many fields ranging from agricultural, over medical\nto remote sensing applications and is currently also expanding to areas such as\nautonomous driving. Even though for full hyperspectral images the\nbest-performing methods exploit spatial-spectral information, performing\nclassification solely on spectral information has its own advantages, e.g.\nsmaller model size and thus less data required for training. Moreover, spectral\ninformation is complementary to spatial information and improvements on either\npart can be used to improve spatial-spectral approaches in the future.\nRecently, 1D-Justo-LiuNet was proposed as a particularly efficient model with\nvery few parameters, which currently defines the state of the art in spectral\nclassification. However, we show that with limited training data the model\nperformance deteriorates. Therefore, we investigate MiniROCKET and\nHDC-MiniROCKET for spectral classification to mitigate that problem. The model\nextracts well-engineered features without trainable parameters in the feature\nextraction part and is therefore less vulnerable to limited training data. We\nshow that even though MiniROCKET has more parameters it outperforms\n1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the\ngeneral case", "AI": {"tldr": "MiniROCKET\u548cHDC-MiniROCKET\u5728\u6570\u636e\u6709\u9650\u65f6\u4f18\u4e8e1D-Justo-LiuNet\uff0c\u4e3a\u5149\u8c31\u5206\u7c7b\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba11D-Justo-LiuNet\u5728\u5149\u8c31\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u7a33\u5065\u7684\u6a21\u578b\u4ee5\u5e94\u5bf9\u6570\u636e\u9650\u5236\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4e86MiniROCKET\u548cHDC-MiniROCKET\u5728\u5149\u8c31\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u8fd9\u4e9b\u6a21\u578b\u901a\u8fc7\u63d0\u53d6\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7279\u5f81\uff08\u65e0\u9700\u53ef\u8bad\u7ec3\u53c2\u6570\uff09\u6765\u51cf\u5c11\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u3002", "result": "MiniROCKET\u5728\u6709\u9650\u6570\u636e\u573a\u666f\u4e0b\u4f18\u4e8e1D-Justo-LiuNet\uff0c\u4e14\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "MiniROCKET\u548cHDC-MiniROCKET\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e1D-Justo-LiuNet\uff0c\u4e14\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u4e0e\u4e4b\u6301\u5e73\uff0c\u4e3a\u89e3\u51b3\u5149\u8c31\u5206\u7c7b\u4e2d\u7684\u6570\u636e\u9650\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.14075", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14075", "abs": "https://arxiv.org/abs/2509.14075", "authors": ["Yu Li", "Hamid Sadeghian", "Zewen Yang", "Valentin Le Mesle", "Sami Haddadin"], "title": "Constraint-Consistent Control of Task-Based and Kinematic RCM Constraints for Surgical Robots", "comment": null, "summary": "Robotic-assisted minimally invasive surgery (RAMIS) requires precise\nenforcement of the remote center of motion (RCM) constraint to ensure safe tool\nmanipulation through a trocar. Achieving this constraint under dynamic and\ninteractive conditions remains challenging, as existing control methods either\nlack robustness at the torque level or do not guarantee consistent RCM\nconstraint satisfaction. This paper proposes a constraint-consistent torque\ncontroller that treats the RCM as a rheonomic holonomic constraint and embeds\nit into a projection-based inverse-dynamics framework. The method unifies\ntask-level and kinematic formulations, enabling accurate tool-tip tracking\nwhile maintaining smooth and efficient torque behavior. The controller is\nvalidated both in simulation and on a RAMIS training platform, and is\nbenchmarked against state-of-the-art approaches. Results show improved RCM\nconstraint satisfaction, reduced required torque, and robust performance by\nimproving joint torque smoothness through the consistency formulation under\nclinically relevant scenarios, including spiral trajectories, variable\ninsertion depths, moving trocars, and human interaction. These findings\ndemonstrate the potential of constraint-consistent torque control to enhance\nsafety and reliability in surgical robotics. The project page is available at:\nhttps://rcmpc-cube.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u4e00\u81f4\u6027\u626d\u77e9\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u624b\u672f\u4e2d\u52a8\u6001\u548c\u4ea4\u4e92\u6761\u4ef6\u4e0b\u7684RCM\u7ea6\u675f\u6ee1\u8db3\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u63a7\u5236\u65b9\u6cd5\u5728\u626d\u77e9\u7ea7\u522b\u7f3a\u4e4f\u9c81\u68d2\u6027\u6216\u65e0\u6cd5\u4fdd\u8bc1\u4e00\u81f4\u7684RCM\u7ea6\u675f\u6ee1\u8db3\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u548c\u4ea4\u4e92\u6761\u4ef6\u4e0b\u3002", "method": "\u901a\u8fc7\u5c06\u8fdc\u7a0b\u8fd0\u52a8\u4e2d\u5fc3\uff08RCM\uff09\u7ea6\u675f\u89c6\u4e3a\u6d41\u5f62\u7ea6\u675f\uff0c\u5e76\u5d4c\u5165\u57fa\u4e8e\u6295\u5f71\u7684\u9006\u52a8\u529b\u5b66\u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u7edf\u4e00\u4e86\u4efb\u52a1\u7ea7\u548c\u8fd0\u52a8\u5b66\u516c\u5f0f\u3002", "result": "\u5728\u6a21\u62df\u548cRAMIS\u8bad\u7ec3\u5e73\u53f0\u4e0a\u7684\u9a8c\u8bc1\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6539\u5584\u4e86RCM\u7ea6\u675f\u6ee1\u8db3\u5ea6\uff0c\u51cf\u5c11\u4e86\u6240\u9700\u626d\u77e9\uff0c\u5e76\u901a\u8fc7\u4e00\u81f4\u6027\u516c\u5f0f\u63d0\u9ad8\u4e86\u5173\u8282\u626d\u77e9\u7684\u5e73\u6ed1\u6027\u3002", "conclusion": "\u672c\u8bba\u6587\u63d0\u51fa\u7684\u7ea6\u675f\u4e00\u81f4\u6027\u626d\u77e9\u63a7\u5236\u5668\u5728\u624b\u672f\u673a\u5668\u4eba\u4e2d\u5c55\u73b0\u51fa\u63d0\u5347\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u548c\u4ea4\u4e92\u6761\u4ef6\u4e0b\u3002"}}
{"id": "2509.13834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13834", "abs": "https://arxiv.org/abs/2509.13834", "authors": ["Nguyen Lan Vi Vu", "Thanh-Huy Nguyen", "Thien Nguyen", "Daisuke Kihara", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation", "comment": "Accepted to BMVC 2025", "summary": "Semi-supervised learning has been employed to alleviate the need for\nextensive labeled data for histopathology image segmentation, but existing\nmethods struggle with noisy pseudo-labels due to ambiguous gland boundaries and\nmorphological misclassification. This paper introduces Semi-MOE, to the best of\nour knowledge, the first multi-task Mixture-of-Experts framework for\nsemi-supervised histopathology image segmentation. Our approach leverages three\nspecialized expert networks: A main segmentation expert, a signed distance\nfield regression expert, and a boundary prediction expert, each dedicated to\ncapturing distinct morphological features. Subsequently, the Multi-Gating\nPseudo-labeling module dynamically aggregates expert features, enabling a\nrobust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate\nmanual tuning while dynamically balancing multiple learning objectives, we\npropose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and\nCRAG benchmarks show that our method outperforms state-of-the-art approaches in\nlow-label settings, highlighting the potential of MoE-based architectures in\nadvancing semi-supervised segmentation. Our code is available at\nhttps://github.com/vnlvi2k3/Semi-MoE.", "AI": {"tldr": "Semi-MOE\u662f\u591a\u4efb\u52a1Mixture-of-Experts\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e13\u5bb6\u7f51\u7edc\u548c\u52a8\u6001\u4f2a\u6807\u7b7e\u673a\u5236\u63d0\u5347\u534a\u76d1\u7763\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u56e0\u6a21\u7cca\u817a\u4f53\u8fb9\u754c\u548c\u5f62\u6001\u5b66\u8bef\u5206\u7c7b\u5bfc\u81f4\u7684\u566a\u58f0\u4f2a\u6807\u7b7e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u4e2a\u4e13\u5bb6\u7f51\u7edc\uff08\u4e3b\u5206\u5272\u4e13\u5bb6\u3001\u7b26\u53f7\u8ddd\u79bb\u573a\u56de\u5f52\u4e13\u5bb6\u548c\u8fb9\u754c\u9884\u6d4b\u4e13\u5bb6\uff09\u548cMulti-Gating\u4f2a\u6807\u7b7e\u6a21\u5757\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u591a\u76ee\u6807\u635f\u5931\uff0c\u52a8\u6001\u5e73\u8861\u5b66\u4e60\u76ee\u6807\u3002", "result": "\u5728GlaS\u548cCRAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSemi-MOE\u5728\u4f4e\u6807\u7b7e\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Semi-MOE\u901a\u8fc7\u591a\u4efb\u52a1Mixture-of-Experts\u6846\u67b6\u548c\u81ea\u9002\u5e94\u591a\u76ee\u6807\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u5c55\u73b0\u4e86MoE\u67b6\u6784\u5728\u8be5\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.14082", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14082", "abs": "https://arxiv.org/abs/2509.14082", "authors": ["Valerii Serpiva", "Artem Lykov", "Faryal Batool", "Vladislav Kozlovskiy", "Miguel Altamirano Cabrera", "Dzmitry Tsetserukou"], "title": "FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video", "comment": "Submitted to conference", "summary": "We present FlightDiffusion, a diffusion-model-based framework for training\nautonomous drones from first-person view (FPV) video. Our model generates\nrealistic video sequences from a single frame, enriched with corresponding\naction spaces to enable reasoning-driven navigation in dynamic environments.\nBeyond direct policy learning, FlightDiffusion leverages its generative\ncapabilities to synthesize diverse FPV trajectories and state-action pairs,\nfacilitating the creation of large-scale training datasets without the high\ncost of real-world data collection. Our evaluation demonstrates that the\ngenerated trajectories are physically plausible and executable, with a mean\nposition error of 0.25 m (RMSE 0.28 m) and a mean orientation error of 0.19 rad\n(RMSE 0.24 rad). This approach enables improved policy learning and dataset\nscalability, leading to superior performance in downstream navigation tasks.\nResults in simulated environments highlight enhanced robustness, smoother\ntrajectory planning, and adaptability to unseen conditions. An ANOVA revealed\nno statistically significant difference between performance in simulation and\nreality (F(1, 16) = 0.394, p = 0.541), with success rates of M = 0.628 (SD =\n0.162) and M = 0.617 (SD = 0.177), respectively, indicating strong sim-to-real\ntransfer. The generated datasets provide a valuable resource for future UAV\nresearch. This work introduces diffusion-based reasoning as a promising\nparadigm for unifying navigation, action generation, and data synthesis in\naerial robotics.", "AI": {"tldr": "FlightDiffusion\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u903c\u771fFPV\u89c6\u9891\u548c\u52a8\u4f5c\u7a7a\u95f4\uff0c\u964d\u4f4e\u65e0\u4eba\u673a\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65e0\u4eba\u673a\u8bad\u7ec3\u4f9d\u8d56\u4e8e\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u6269\u5c55\u3002FlightDiffusion\u65e8\u5728\u901a\u8fc7\u751f\u6210\u903c\u771f\u7684FPV\u89c6\u9891\u548c\u52a8\u4f5c\u7a7a\u95f4\uff0c\u964d\u4f4e\u6570\u636e\u6536\u96c6\u6210\u672c\uff0c\u540c\u65f6\u63d0\u5347\u7b56\u7565\u5b66\u4e60\u548c\u6570\u636e\u96c6\u6269\u5c55\u6027\u3002", "method": "FlightDiffusion\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u7684FPV\u8f68\u8ff9\u548c\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u4ece\u800c\u65e0\u9700\u9ad8\u6602\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6536\u96c6\u6210\u672c\u5373\u53ef\u521b\u5efa\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u6a21\u578b\u751f\u6210\u7684\u8f68\u8ff9\u5728\u7269\u7406\u4e0a\u662f\u53ef\u884c\u7684\uff0c\u4e14\u53ef\u6267\u884c\uff0c\u4f4d\u7f6e\u8bef\u5dee\u5747\u503c0.25\u7c73\uff08RMSE 0.28\u7c73\uff09\uff0c\u65b9\u5411\u8bef\u5dee\u5747\u503c0.19\u5f27\u5ea6\uff08RMSE 0.24\u5f27\u5ea6\uff09\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0cFlightDiffusion\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3001\u66f4\u5e73\u6ed1\u7684\u8f68\u8ff9\u89c4\u5212\u80fd\u529b\u4ee5\u53ca\u5bf9\u672a\u77e5\u6761\u4ef6\u7684\u9002\u5e94\u6027\u3002ANOVA\u5206\u6790\u663e\u793a\uff0c\u6a21\u62df\u4e0e\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6027\u80fd\u65e0\u663e\u8457\u5dee\u5f02\uff08F(1, 16) = 0.394, p = 0.541\uff09\uff0c\u6210\u529f\u7387\u4e3aM = 0.628\uff08SD = 0.162\uff09\u548cM = 0.617\uff08SD = 0.177\uff09\uff0c\u8868\u660e\u826f\u597d\u7684\u6a21\u62df\u5230\u771f\u5b9e\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86FlightDiffusion\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\uff08FPV\uff09\u89c6\u9891\u4e2d\u8bad\u7ec3\u81ea\u4e3b\u65e0\u4eba\u673a\u3002\u8be5\u6a21\u578b\u80fd\u591f\u4ece\u5355\u5e27\u56fe\u50cf\u751f\u6210\u903c\u771f\u7684\u89c6\u9891\u5e8f\u5217\uff0c\u5e76\u9644\u5e26\u76f8\u5e94\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4ee5\u5b9e\u73b0\u52a8\u6001\u73af\u5883\u4e2d\u7684\u63a8\u7406\u9a71\u52a8\u5bfc\u822a\u3002\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u7b56\u7565\u5b66\u4e60\u80fd\u529b\uff0c\u8fd8\u589e\u5f3a\u4e86\u6570\u636e\u96c6\u7684\u6269\u5c55\u6027\uff0c\u4ece\u800c\u5728\u4e0b\u6e38\u5bfc\u822a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.13836", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13836", "abs": "https://arxiv.org/abs/2509.13836", "authors": ["Weihang Wang", "Xinhao Li", "Ziyue Wang", "Yan Pang", "Jielei Zhang", "Peiyi Li", "Qiang Zhang", "Longwen Gao"], "title": "Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models", "comment": "Accepted by EMNLP2025 Finding", "summary": "Object hallucination in Large Vision-Language Models (LVLMs) significantly\nimpedes their real-world applicability. As the primary component for accurately\ninterpreting visual information, the choice of visual encoder is pivotal. We\nhypothesize that the diverse training paradigms employed by different visual\nencoders instill them with distinct inductive biases, which leads to their\ndiverse hallucination performances. Existing benchmarks typically focus on\ncoarse-grained hallucination detection and fail to capture the diverse\nhallucinations elaborated in our hypothesis. To systematically analyze these\neffects, we introduce VHBench-10, a comprehensive benchmark with approximately\n10,000 samples for evaluating LVLMs across ten fine-grained hallucination\ncategories. Our evaluations confirm encoders exhibit unique hallucination\ncharacteristics. Building on these insights and the suboptimality of simple\nfeature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.\nIt employs global visual features to generate routing signals, dynamically\naggregating visual features from multiple specialized experts. Comprehensive\nexperiments confirm the effectiveness of VisionWeaver in significantly reducing\nhallucinations and improving overall model performance.", "AI": {"tldr": "LVLM\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\u4e25\u91cd\uff0c\u672c\u6587\u63d0\u51faVisionWeaver\u901a\u8fc7\u52a8\u6001\u805a\u5408\u4e13\u5bb6\u7279\u5f81\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u4e25\u91cd\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\uff0c\u4e0d\u540c\u89c6\u89c9\u7f16\u7801\u5668\u7684\u8bad\u7ec3\u8303\u5f0f\u5bfc\u81f4\u5176\u72ec\u7279\u7684\u5f52\u7eb3\u504f\u5dee\u548c\u5e7b\u89c9\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u8def\u7531\u7f51\u7edcVisionWeaver\uff0c\u5229\u7528\u5168\u5c40\u89c6\u89c9\u7279\u5f81\u751f\u6210\u8def\u7531\u4fe1\u53f7\uff0c\u52a8\u6001\u805a\u5408\u591a\u4e2a\u4e13\u5bb6\u7684\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u901a\u8fc7VHBench-10\u57fa\u51c6\u6d4b\u8bd5\u8bc1\u5b9e\u4e86\u7f16\u7801\u5668\u5177\u6709\u72ec\u7279\u7684\u5e7b\u89c9\u7279\u5f81\uff0cVisionWeaver\u5728\u51cf\u5c11\u5e7b\u89c9\u548c\u63d0\u5347\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "VisionWeaver\u901a\u8fc7\u52a8\u6001\u805a\u5408\u591a\u4e2a\u4e13\u5bb6\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2509.14117", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14117", "abs": "https://arxiv.org/abs/2509.14117", "authors": ["Ali Abouzeid", "Malak Mansour", "Zezhou Sun", "Dezhen Song"], "title": "GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model", "comment": "Under Review", "summary": "Vision-Language-Action (VLA) models often fail to generalize to novel camera\nviewpoints, a limitation stemming from their difficulty in inferring robust 3D\ngeometry from 2D images. We introduce GeoAware-VLA, a simple yet effective\napproach that enhances viewpoint invariance by integrating strong geometric\npriors into the vision backbone. Instead of training a visual encoder or\nrelying on explicit 3D data, we leverage a frozen, pretrained geometric vision\nmodel as a feature extractor. A trainable projection layer then adapts these\ngeometrically-rich features for the policy decoder, relieving it of the burden\nof learning 3D consistency from scratch. Through extensive evaluations on\nLIBERO benchmark subsets, we show GeoAware-VLA achieves substantial\nimprovements in zero-shot generalization to novel camera poses, boosting\nsuccess rates by over 2x in simulation. Crucially, these benefits translate to\nthe physical world; our model shows a significant performance gain on a real\nrobot, especially when evaluated from unseen camera angles. Our approach proves\neffective across both continuous and discrete action spaces, highlighting that\nrobust geometric grounding is a key component for creating more generalizable\nrobotic agents.", "AI": {"tldr": "GeoAware-VLA\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u5148\u9a8c\u63d0\u5347VLA\u6a21\u578b\u5728\u65b0\u89c6\u89d2\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3VLA\u6a21\u578b\u5728\u65b0\u6444\u50cf\u5934\u89c6\u89d2\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u4ece2D\u56fe\u50cf\u63a8\u65ad3D\u51e0\u4f55\u7684\u56f0\u96be\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u51e0\u4f55\u89c6\u89c9\u6a21\u578b\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u6295\u5f71\u5c42\u5c06\u51e0\u4f55\u4e30\u5bcc\u7684\u7279\u5f81\u9002\u914d\u5230\u7b56\u7565\u89e3\u7801\u5668\uff0c\u907f\u514d\u4ece\u5934\u5b66\u4e603D\u4e00\u81f4\u6027\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGeoAware-VLA\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u5230\u65b0\u76f8\u673a\u59ff\u6001\u7684\u6210\u529f\u7387\u63d0\u53472\u500d\u4ee5\u4e0a\uff0c\u4e14\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "GeoAware-VLA\u8bc1\u660e\u4e86\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347VLA\u6a21\u578b\u5728\u65b0\u89c6\u89d2\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u901a\u7528\u7684\u673a\u5668\u4eba\u4ee3\u7406\u63d0\u4f9b\u4e86\u5173\u952e\u7ec4\u4ef6\u3002"}}
{"id": "2509.13846", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13846", "abs": "https://arxiv.org/abs/2509.13846", "authors": ["Puru Vaish", "Felix Meister", "Tobias Heimann", "Christoph Brune", "Jelmer M. Wolterink"], "title": "Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation", "comment": "MICCAI 2025: 1st Place in Transformer track and 2nd Place in\n  Convolution track of SSL3D-OpenMind challenge", "summary": "Many recent approaches in representation learning implicitly assume that\nuncorrelated views of a data point are sufficient to learn meaningful\nrepresentations for various downstream tasks. In this work, we challenge this\nassumption and demonstrate that meaningful structure in the latent space does\nnot emerge naturally. Instead, it must be explicitly induced. We propose a\nmethod that aligns representations from different views of the data to align\ncomplementary information without inducing false positives. Our experiments\nshow that our proposed self-supervised learning method, Consistent View\nAlignment, improves performance for downstream tasks, highlighting the critical\nrole of structured view alignment in learning effective representations. Our\nmethod achieved first and second place in the MICCAI 2025 SSL3D challenge when\nusing a Primus vision transformer and ResEnc convolutional neural network,\nrespectively. The code and pretrained model weights are released at\nhttps://github.com/Tenbatsu24/LatentCampus.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u81f4\u89c6\u56fe\u5bf9\u9f50\u65b9\u6cd5\uff0c\u8bc1\u660e\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u9700\u660e\u786e\u5f15\u5bfc\uff0c\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u6311\u6218\u4e86\u73b0\u6709\u8868\u793a\u5b66\u4e60\u4e2d\u9690\u542b\u7684\u5047\u8bbe\uff0c\u5373\u6570\u636e\u70b9\u7684\u65e0\u5173\u89c6\u56fe\u8db3\u4ee5\u5b66\u4e60\u6709\u610f\u4e49\u7684\u8868\u793a\uff0c\u8bc1\u660e\u4e86\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u7ed3\u6784\u9700\u8981\u660e\u786e\u5f15\u5bfc\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4e00\u81f4\u89c6\u56fe\u5bf9\u9f50\uff08Consistent View Alignment\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u9f50\u6570\u636e\u4e0d\u540c\u89c6\u56fe\u7684\u8868\u793a\u6765\u6574\u5408\u4e92\u8865\u4fe1\u606f\uff0c\u540c\u65f6\u907f\u514d\u5f15\u5165\u5047\u9633\u6027\u3002", "result": "\u5728MICCAI 2025 SSL3D\u6311\u6218\u8d5b\u4e2d\uff0c\u4f7f\u7528Primus\u89c6\u89c9\u53d8\u6362\u5668\u548cResEnc\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5206\u522b\u83b7\u5f97\u4e86\u7b2c\u4e00\u540d\u548c\u7b2c\u4e8c\u540d\u7684\u6210\u7ee9\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u63d0\u51fa\u4e00\u81f4\u7684\u89c6\u56fe\u5bf9\u9f50\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u660e\u786e\u5f15\u5bfc\u7ed3\u6784\u7684\u91cd\u8981\u6027\uff0c\u4ece\u800c\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14126", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14126", "abs": "https://arxiv.org/abs/2509.14126", "authors": ["Viktor Lorentz", "Khaled Wahba", "Sayantan Auddy", "Marc Toussaint", "Wolfgang H\u00f6nig"], "title": "CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads", "comment": "This work has been submitted to IEEE for possible publication", "summary": "Collaborative transportation of cable-suspended payloads by teams of Unmanned\nAerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to\ndifferent payload shapes, and provide built-in compliance, making it attractive\nfor applications ranging from disaster relief to precision logistics. However,\nmulti-UAV coordination under disturbances, nonlinear payload dynamics, and\nslack--taut cable modes remains a challenging control problem. To our\nknowledge, no prior work has addressed these cable mode transitions in the\nmulti-UAV context, instead relying on simplifying rigid-link assumptions. We\npropose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for\nmulti-UAV cable-suspended payload transport. Simulation results demonstrate\nthat the learned policies can outperform classical decentralized controllers in\nterms of disturbance rejection and tracking precision, achieving an 80%\nrecovery rate from harsh conditions compared to 44% for the baseline method. We\nalso achieve successful zero-shot sim-to-real transfer and demonstrate that our\npolicies are highly robust under harsh conditions, including wind, random\nexternal disturbances, and transitions between slack and taut cable dynamics.\nThis work paves the way for autonomous, resilient UAV teams capable of\nexecuting complex payload missions in unstructured environments.", "AI": {"tldr": "\u63d0\u51faCrazyMARL\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u591a\u65e0\u4eba\u673a\u7535\u7f06\u8d1f\u8f7d\u8fd0\u8f93\u4e2d\u7684\u63a7\u5236\u95ee\u9898\uff0c\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u5747\u663e\u793a\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u591a\u65e0\u4eba\u673a\u534f\u8c03\u5728\u5e72\u6270\u3001\u975e\u7ebf\u6027\u8d1f\u8f7d\u52a8\u6001\u548c\u677e\u5f1b-\u7d27\u7ef7\u7535\u7f06\u6a21\u5f0f\u4e0b\u7684\u63a7\u5236\u95ee\u9898\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u5c1a\u672a\u6709\u5de5\u4f5c\u89e3\u51b3\u8fd9\u4e9b\u7535\u7f06\u6a21\u5f0f\u8f6c\u6362\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86CrazyMARL\uff0c\u4e00\u79cd\u7528\u4e8e\u591a\u65e0\u4eba\u673a\u7535\u7f06\u60ac\u6302\u8d1f\u8f7d\u8fd0\u8f93\u7684\u5206\u6563\u5f0f\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5728\u5e72\u6270\u6291\u5236\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u7ecf\u5178\u5206\u6563\u63a7\u5236\u5668\uff0c\u4ece\u6076\u52a3\u6761\u4ef6\u4e2d\u6062\u590d\u7684\u6210\u529f\u7387\u8fbe\u523080%\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u4e3a44%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u81ea\u4e3b\u3001\u5f39\u6027\u7684\u65e0\u4eba\u673a\u56e2\u961f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u8d1f\u8f7d\u4efb\u52a1\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.14001", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14001", "abs": "https://arxiv.org/abs/2509.14001", "authors": ["Elena Camuffo", "Francesco Barbato", "Mete Ozay", "Simone Milani", "Umberto Michieli"], "title": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment", "comment": null, "summary": "We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),\na knowledge distillation approach that transfers region-level multimodal\nsemantics from a large vision-language teacher (e.g., LLaVa) into a lightweight\nvision-only object detector student (e.g., YOLO). A translation module maps\nstudent features into a joint space, where the training of the student and\ntranslator is guided by a dual-objective loss that enforces both local\nalignment and global relational consistency. Unlike prior approaches focused on\ndense or global alignment, MOCHA operates at the object level, enabling\nefficient transfer of semantics without modifying the teacher or requiring\ntextual input at inference. We validate our method across four personalized\ndetection benchmarks under few-shot regimes. Results show consistent gains over\nbaselines, with a +10.1 average score improvement. Despite its compact\narchitecture, MOCHA reaches performance on par with larger multimodal models,\nproving its suitability for real-world deployment.", "AI": {"tldr": "MOCHA\u901a\u8fc7\u5bf9\u8c61\u7ea7\u77e5\u8bc6\u84b8\u998f\uff0c\u5c06\u591a\u6a21\u6001\u8bed\u4e49\u4ece\u5927\u578b\u6559\u5e08\u6a21\u578b\u4f20\u9012\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\uff0c\u65e0\u9700\u6587\u672c\u8f93\u5165\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5bc6\u96c6\u6216\u5168\u5c40\u5bf9\u9f50\uff0c\u800cMOCHA\u4e13\u6ce8\u4e8e\u5bf9\u8c61\u7ea7\u522b\u7684\u8bed\u4e49\u4f20\u9012\uff0c\u65e0\u9700\u4fee\u6539\u6559\u5e08\u6a21\u578b\u6216\u63a8\u7406\u65f6\u7684\u6587\u672c\u8f93\u5165\u3002", "method": "MOCHA\u901a\u8fc7\u7ffb\u8bd1\u6a21\u5757\u5c06\u5b66\u751f\u6a21\u578b\u7279\u5f81\u6620\u5c04\u5230\u8054\u5408\u7a7a\u95f4\uff0c\u5e76\u4f7f\u7528\u53cc\u76ee\u6807\u635f\u5931\u51fd\u6570\uff08\u5c40\u90e8\u5bf9\u9f50\u548c\u5168\u5c40\u5173\u7cfb\u4e00\u81f4\u6027\uff09\u6307\u5bfc\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u4e2a\u5c11\u6837\u672c\u4e2a\u6027\u5316\u68c0\u6d4b\u57fa\u51c6\u4e0a\uff0cMOCHA\u5e73\u5747\u5f97\u5206\u63d0\u9ad8\u4e8610.1\u5206\uff0c\u6027\u80fd\u4e0e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "MOCHA\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u67b6\u6784\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e0e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.13848", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13848", "abs": "https://arxiv.org/abs/2509.13848", "authors": ["Jiayi Pan", "Jiaming Xu", "Yongkang Zhou", "Guohao Dai"], "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation", "comment": null, "summary": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.", "AI": {"tldr": "\u63d0\u51faSpecDiff\uff0c\u4e00\u79cd\u57fa\u4e8e\u81ea\u63a8\u6d4b\u4fe1\u606f\u7684\u8bad\u7ec3\u514d\u8d39\u591a\u7ea7\u7279\u5f81\u7f13\u5b58\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6269\u6563\u6a21\u578b\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5386\u53f2\u4fe1\u606f\uff0c\u5bfc\u81f4\u51c6\u786e\u6027\u548c\u901f\u5ea6\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u63a8\u6d4b\u4fe1\u606f\u7684\u52a8\u6001\u91cd\u8981\u6027\u8bc4\u5206\u7b97\u6cd5\u548c\u591a\u7ea7\u7279\u5f81\u5206\u7c7b\u7b97\u6cd5\uff0c\u5305\u62ec\u7f13\u5b58\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\u548c\u591a\u7ea7\u7279\u5f81\u5206\u7c7b\u7b97\u6cd5\u3002", "result": "SpecDiff\u5728Stable Diffusion 3\u30013.5\u548cFLUX\u4e0a\u5b9e\u73b0\u4e86\u5e73\u57472.80\u500d\u30012.74\u500d\u548c3.17\u500d\u7684\u52a0\u901f\uff0c\u4e14\u8d28\u91cf\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u63a8\u6d4b\u4fe1\u606f\u548c\u5386\u53f2\u4fe1\u606f\uff0cSpecDiff\u514b\u670d\u4e86\u901f\u5ea6\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u74f6\u9888\uff0c\u63a8\u52a8\u4e86\u9ad8\u6548\u6269\u6563\u6a21\u578b\u63a8\u7406\u4e2d\u901f\u5ea6\u4e0e\u51c6\u786e\u6027\u7684Pareto\u524d\u6cbf\u3002"}}
{"id": "2509.14127", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14127", "abs": "https://arxiv.org/abs/2509.14127", "authors": ["Alkesh K. Srivastava", "Jared Michael Levin", "Philip Dames"], "title": "Energy Efficient Multi Robot Package Delivery under Capacity-Constraints via Voronoi-Constrained Networks", "comment": null, "summary": "We consider the problem of delivering multiple packages from a single pickup\ndepot to distinct goal locations using a homogeneous fleet of robots with\nlimited carrying capacity. We propose VCST-RCP, a Voronoi-Constrained Steiner\nTree Relay Coordination Planning framework that constructs sparse relay trunks\nusing Steiner tree optimization and then synthesizes robot-level pickup, relay,\nand delivery schedules. This framework reframes relays from incidental\nbyproducts into central elements of coordination, offering a contrast with\ntraditional delivery methods that rely on direct source-to-destination\ntransport. Extensive experiments show consistent improvements of up to 34%\ncompared to conventional baselines, underscoring the benefits of incorporating\nrelays into the delivery process. These improvements translate directly to\nenhanced energy efficiency in multi-robot delivery under capacity constraints,\nproviding a scalable framework for real-world logistics.", "AI": {"tldr": "VCST-RCP\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u4e2d\u7ee7\u534f\u8c03\uff0c\u63d0\u5347\u591a\u673a\u5668\u4eba\u914d\u9001\u6548\u738734%\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u7269\u6d41\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u6709\u9650\u627f\u8f7d\u80fd\u529b\u7684\u540c\u8d28\u673a\u5668\u4eba\u8f66\u961f\u4ece\u5355\u4e00\u53d6\u8d27\u70b9\u5230\u591a\u4e2a\u76ee\u6807\u4f4d\u7f6e\u7684\u5305\u88f9\u914d\u9001\u95ee\u9898\uff0c\u6311\u6218\u4f20\u7edf\u4f9d\u8d56\u76f4\u63a5\u6e90\u5230\u76ee\u7684\u5730\u8fd0\u8f93\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faVCST-RCP\u6846\u67b6\uff0c\u7ed3\u5408Voronoi\u7ea6\u675f\u7684Steiner\u6811\u4f18\u5316\u6784\u5efa\u7a00\u758f\u4e2d\u7ee7\u4e3b\u5e72\uff0c\u5e76\u5408\u6210\u673a\u5668\u4eba\u7ea7\u7684\u62fe\u53d6\u3001\u4e2d\u7ee7\u548c\u914d\u9001\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u4e0e\u4f20\u7edf\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe34%\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u80fd\u6e90\u6548\u7387\u3002", "conclusion": "VCST-RCP\u6846\u67b6\u901a\u8fc7\u5c06\u4e2d\u7ee7\u70b9\u4f5c\u4e3a\u534f\u8c03\u7684\u6838\u5fc3\u5143\u7d20\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u673a\u5668\u4eba\u914d\u9001\u7684\u6548\u7387\u548c\u80fd\u6e90\u5229\u7528\u7387\uff0c\u4e3a\u73b0\u5b9e\u7269\u6d41\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13858", "abs": "https://arxiv.org/abs/2509.13858", "authors": ["Qianxin Xia", "Jiawei Du", "Guoming Lu", "Zhiyong Shu", "Jielei Wang"], "title": "EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics", "comment": null, "summary": "Dataset distillation aims to synthesize a compact dataset from the original\nlarge-scale one, enabling highly efficient learning while preserving\ncompetitive model performance. However, traditional techniques primarily\ncapture low-level visual features, neglecting the high-level semantic and\nstructural information inherent in images. In this paper, we propose EDITS, a\nnovel framework that exploits the implicit textual semantics within the image\ndata to achieve enhanced distillation. First, external texts generated by a\nVision Language Model (VLM) are fused with image features through a Global\nSemantic Query module, forming the prior clustered buffer. Local Semantic\nAwareness then selects representative samples from the buffer to construct\nimage and text prototypes, with the latter produced by guiding a Large Language\nModel (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype\nGuidance strategy generates the final synthetic dataset through a diffusion\nmodel. Extensive experiments confirm the effectiveness of our method.Source\ncode is available in: https://github.com/einsteinxia/EDITS.", "AI": {"tldr": "EDITS\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u6570\u636e\u96c6\u84b8\u998f\u7684\u8bed\u4e49\u4fdd\u7559\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u96c6\u84b8\u998f\u6280\u672f\u4e3b\u8981\u6355\u83b7\u4f4e\u5c42\u6b21\u89c6\u89c9\u7279\u5f81\uff0c\u5ffd\u89c6\u4e86\u56fe\u50cf\u4e2d\u7684\u9ad8\u5c42\u6b21\u8bed\u4e49\u548c\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u63d0\u51faEDITS\u6846\u67b6\uff0c\u5305\u62ec\u5168\u5c40\u8bed\u4e49\u67e5\u8be2\u6a21\u5757\u3001\u5c40\u90e8\u8bed\u4e49\u611f\u77e5\u6a21\u5757\u548c\u53cc\u539f\u578b\u6307\u5bfc\u7b56\u7565\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u6700\u7ec8\u5408\u6210\u6570\u636e\u96c6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9eEDITS\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "conclusion": "EDITS\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6570\u636e\u96c6\u84b8\u998f\u7684\u6027\u80fd\uff0c\u4fdd\u7559\u4e86\u9ad8\u5c42\u6b21\u8bed\u4e49\u4fe1\u606f\u3002"}}
{"id": "2509.14138", "categories": ["cs.RO", "68T40"], "pdf": "https://arxiv.org/pdf/2509.14138", "abs": "https://arxiv.org/abs/2509.14138", "authors": ["Ran Yang", "Zijian An", "Lifeng ZHou", "Yiming Feng"], "title": "SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model", "comment": "8 pages, 9 figures, 1 table", "summary": "Long-horizon robotic manipulation tasks require executing multiple\ninterdependent subtasks in strict sequence, where errors in detecting subtask\ncompletion can cascade into downstream failures. Existing\nVision-Language-Action (VLA) models such as $\\pi_0$ excel at continuous\nlow-level control but lack an internal signal for identifying when a subtask\nhas finished, making them brittle in sequential settings. We propose SeqVLA, a\ncompletion-aware extension of $\\pi_0$ that augments the base architecture with\na lightweight detection head perceiving whether the current subtask is\ncomplete. This dual-head design enables SeqVLA not only to generate\nmanipulation actions but also to autonomously trigger transitions between\nsubtasks. We investigate four finetuning strategies that vary in how the action\nand detection heads are optimized (joint vs. sequential finetuning) and how\npretrained knowledge is preserved (full finetuning vs. frozen backbone).\nExperiments are performed on two multi-stage tasks: salad packing with seven\ndistinct subtasks and candy packing with four distinct subtasks. Results show\nthat SeqVLA significantly outperforms the baseline $\\pi_0$ and other strong\nbaselines in overall success rate. In particular, joint finetuning with an\nunfrozen backbone yields the most decisive and statistically reliable\ncompletion predictions, eliminating sequence-related failures and enabling\nrobust long-horizon execution. Our results highlight the importance of coupling\naction generation with subtask-aware detection for scalable sequential\nmanipulation.", "AI": {"tldr": "SeqVLA\u901a\u8fc7\u53cc\u5934\u8bbe\u8ba1\u589e\u5f3a\u03c0\u2080\uff0c\u5b9e\u73b0\u5b50\u4efb\u52a1\u5b8c\u6210\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5982\u03c0\u2080\u5728\u8fde\u7eed\u4f4e\u5c42\u63a7\u5236\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u5b50\u4efb\u52a1\u5b8c\u6210\u4fe1\u53f7\uff0c\u5bfc\u81f4\u5728\u5e8f\u5217\u4efb\u52a1\u4e2d\u8106\u5f31\u3002", "method": "\u63d0\u51fa\u4e86SeqVLA\uff0c\u4e00\u79cd\u57fa\u4e8e\u03c0\u2080\u7684\u53cc\u5934\u67b6\u6784\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u5934\u611f\u77e5\u5b50\u4efb\u52a1\u5b8c\u6210\u60c5\u51b5\uff0c\u5e76\u7814\u7a76\u4e86\u56db\u79cd\u5fae\u8c03\u7b56\u7565\u3002", "result": "SeqVLA\u5728\u6c99\u62c9\u6253\u5305\u548c\u7cd6\u679c\u6253\u5305\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u03c0\u2080\uff0c\u8054\u5408\u5fae\u8c03\u4e14\u4e0d\u51bb\u7ed3\u4e3b\u5e72\u7f51\u7edc\u7684\u7b56\u7565\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "SeqVLA\u901a\u8fc7\u7ed3\u5408\u52a8\u4f5c\u751f\u6210\u548c\u5b50\u4efb\u52a1\u611f\u77e5\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u9760\u6027\u548c\u6210\u529f\u7387\u3002"}}
{"id": "2509.13863", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13863", "abs": "https://arxiv.org/abs/2509.13863", "authors": ["Chu Chen", "Ander Biguri", "Jean-Michel Morel", "Raymond H. Chan", "Carola-Bibiane Sch\u00f6nlieb", "Jizhou Li"], "title": "LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction", "comment": null, "summary": "X-ray Computed Laminography (CL) is essential for non-destructive inspection\nof plate-like structures in applications such as microchips and composite\nbattery materials, where traditional computed tomography (CT) struggles due to\ngeometric constraints. However, reconstructing high-quality volumes from\nlaminographic projections remains challenging, particularly under highly\nsparse-view acquisition conditions. In this paper, we propose a reconstruction\nalgorithm, namely LamiGauss, that combines Gaussian Splatting radiative\nrasterization with a dedicated detector-to-world transformation model\nincorporating the laminographic tilt angle. LamiGauss leverages an\ninitialization strategy that explicitly filters out common laminographic\nartifacts from the preliminary reconstruction, preventing redundant Gaussians\nfrom being allocated to false structures and thereby concentrating model\ncapacity on representing the genuine object. Our approach effectively optimizes\ndirectly from sparse projections, enabling accurate and efficient\nreconstruction with limited data. Extensive experiments on both synthetic and\nreal datasets demonstrate the effectiveness and superiority of the proposed\nmethod over existing techniques. LamiGauss uses only 3$\\%$ of full views to\nachieve superior performance over the iterative method optimized on a full\ndataset.", "AI": {"tldr": "LamiGauss\u662f\u4e00\u79cd\u7ed3\u5408\u9ad8\u65af\u55b7\u6e85\u548c\u4e13\u7528\u8f6c\u6362\u6a21\u578b\u7684\u5c42\u6790\u6210\u50cf\u91cd\u5efa\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u9ad8\u8d28\u91cf\u91cd\u5efa\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08CT\uff09\u5728\u51e0\u4f55\u53d7\u9650\u7684\u677f\u72b6\u7ed3\u6784\uff08\u5982\u5fae\u82af\u7247\u548c\u590d\u5408\u7535\u6c60\u6750\u6599\uff09\u4e2d\u96be\u4ee5\u8fdb\u884c\u65e0\u635f\u68c0\u6d4b\uff0c\u800c\u73b0\u6709\u5c42\u6790\u6210\u50cf\u6280\u672f\u5728\u9ad8\u5ea6\u7a00\u758f\u89c6\u56fe\u91c7\u96c6\u6761\u4ef6\u4e0b\u7684\u9ad8\u8d28\u91cf\u4f53\u79ef\u91cd\u5efa\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "LamiGauss\u7ed3\u5408\u4e86\u9ad8\u65af\u55b7\u6e85\u8f90\u5c04\u5149\u6805\u5316\u548c\u4e13\u7528\u7684\u63a2\u6d4b\u5668\u5230\u4e16\u754c\u8f6c\u6362\u6a21\u578b\uff0c\u5229\u7528\u521d\u59cb\u5316\u7b56\u7565\u663e\u5f0f\u8fc7\u6ee4\u6389\u5e38\u89c1\u7684\u5c42\u6790\u6210\u50cf\u4f2a\u5f71\uff0c\u907f\u514d\u5197\u4f59\u9ad8\u65af\u5206\u914d\u5230\u865a\u5047\u7ed3\u6784\uff0c\u4ece\u800c\u96c6\u4e2d\u6a21\u578b\u5bb9\u91cf\u8868\u793a\u771f\u5b9e\u5bf9\u8c61\u3002", "result": "LamiGauss\u4ec5\u97003%\u7684\u5b8c\u6574\u89c6\u56fe\u5373\u53ef\u5728\u7a00\u758f\u6295\u5f71\u4e0b\u5b9e\u73b0\u51c6\u786e\u9ad8\u6548\u7684\u91cd\u5efa\uff0c\u6027\u80fd\u4f18\u4e8e\u5728\u5b8c\u6574\u6570\u636e\u96c6\u4e0a\u4f18\u5316\u7684\u8fed\u4ee3\u65b9\u6cd5\u3002", "conclusion": "LamiGauss\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408\u9ad8\u65af\u55b7\u6e85\u8f90\u5c04\u5149\u6805\u5316\u548c\u4e13\u7528\u7684\u63a2\u6d4b\u5668\u5230\u4e16\u754c\u8f6c\u6362\u6a21\u578b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5728\u9ad8\u5ea6\u7a00\u758f\u89c6\u56fe\u91c7\u96c6\u6761\u4ef6\u4e0b\u9ad8\u8d28\u91cf\u4f53\u79ef\u91cd\u5efa\u7684\u6311\u6218\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.14143", "categories": ["cs.RO", "68T40"], "pdf": "https://arxiv.org/pdf/2509.14143", "abs": "https://arxiv.org/abs/2509.14143", "authors": ["Zijian An", "Ran Yang", "Yiming Feng", "Lifeng Zhou"], "title": "CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping", "comment": "8 pages, 5 figures, 1 table", "summary": "Vision-language-action (VLA) models have recently emerged as a promising\nparadigm for robotic control, enabling end-to-end policies that ground natural\nlanguage instructions into visuomotor actions. However, current VLAs often\nstruggle to satisfy precise task constraints, such as stopping based on numeric\nthresholds, since their observation-to-action mappings are implicitly shaped by\ntraining data and lack explicit mechanisms for condition monitoring. In this\nwork, we propose CLAW (CLIP-Language-Action for Weight), a framework that\ndecouples condition evaluation from action generation. CLAW leverages a\nfine-tuned CLIP model as a lightweight prompt generator, which continuously\nmonitors the digital readout of a scale and produces discrete directives based\non task-specific weight thresholds. These prompts are then consumed by $\\pi_0$,\na flow-based VLA policy, which integrates the prompts with multi-view camera\nobservations to produce continuous robot actions. This design enables CLAW to\ncombine symbolic weight reasoning with high-frequency visuomotor control. We\nvalidate CLAW on three experimental setups: single-object grasping and\nmixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW\nreliably executes weight-aware behaviors and outperforms both raw-$\\pi_0$ and\nfine-tuned $\\pi_0$ models. We have uploaded the videos as supplementary\nmaterials.", "AI": {"tldr": "CLAW\u901a\u8fc7\u89e3\u8026\u6761\u4ef6\u8bc4\u4f30\u4e0e\u52a8\u4f5c\u751f\u6210\uff0c\u7ed3\u5408CLIP\u6a21\u578b\u548cVLA\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u91cd\u91cf\u611f\u77e5\u7684\u673a\u5668\u4eba\u63a7\u5236\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u7f3a\u4e4f\u660e\u786e\u7684\u673a\u5236\u6765\u6ee1\u8db3\u7cbe\u786e\u4efb\u52a1\u7ea6\u675f\uff08\u5982\u57fa\u4e8e\u6570\u5b57\u9608\u503c\u7684\u505c\u6b62\uff09\uff0cCLAW\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "CLAW\u5229\u7528\u5fae\u8c03\u7684CLIP\u6a21\u578b\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u63d0\u793a\u751f\u6210\u5668\uff0c\u6301\u7eed\u76d1\u6d4b\u6570\u5b57\u8bfb\u6570\u5e76\u751f\u6210\u79bb\u6563\u6307\u4ee4\uff0c\u7531\u57fa\u4e8e\u6d41\u7684VLA\u7b56\u7565\u03c00\u6574\u5408\u591a\u89c6\u89d2\u6444\u50cf\u5934\u89c2\u6d4b\u751f\u6210\u8fde\u7eed\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "CLAW\u5728\u5355\u7269\u4f53\u6293\u53d6\u548c\u9700\u8981\u53cc\u81c2\u64cd\u4f5c\u7684\u6df7\u5408\u7269\u4f53\u4efb\u52a1\u4e2d\u53ef\u9760\u6267\u884c\u91cd\u91cf\u611f\u77e5\u884c\u4e3a\uff0c\u6027\u80fd\u4f18\u4e8e\u539f\u59cb\u03c00\u548c\u5fae\u8c03\u03c00\u6a21\u578b\u3002", "conclusion": "CLAW\u6846\u67b6\u901a\u8fc7\u5c06\u6761\u4ef6\u8bc4\u4f30\u4e0e\u52a8\u4f5c\u751f\u6210\u89e3\u8026\uff0c\u6210\u529f\u7ed3\u5408\u4e86\u7b26\u53f7\u91cd\u91cf\u63a8\u7406\u4e0e\u9ad8\u9891\u7387\u7684\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\uff0c\u5728\u591a\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002"}}
{"id": "2509.14165", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14165", "abs": "https://arxiv.org/abs/2509.14165", "authors": ["Michal Szczepanski", "Martyna Poreba", "Karim Haroun"], "title": "Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions", "comment": null, "summary": "Vision Transformers (ViTs) achieve state-of-the-art performance in semantic\nsegmentation but are hindered by high computational and memory costs. To\naddress this, we propose STEP (SuperToken and Early-Pruning), a hybrid\ntoken-reduction framework that combines dynamic patch merging and token pruning\nto enhance efficiency without significantly compromising accuracy. At the core\nof STEP is dCTS, a lightweight CNN-based policy network that enables flexible\nmerging into superpatches. Encoder blocks integrate also early-exits to remove\nhigh-confident supertokens, lowering computational load. We evaluate our method\non high-resolution semantic segmentation benchmarks, including images up to\n1024 x 1024, and show that when dCTS is applied alone, the token count can be\nreduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching\nscheme. This yields a 2.6x reduction in computational cost and a 3.4x increase\nin throughput when using ViT-Large as the backbone. Applying the full STEP\nframework further improves efficiency, reaching up to a 4x reduction in\ncomputational complexity and a 1.7x gain in inference speed, with a maximum\naccuracy drop of no more than 2.0%. With the proposed STEP configurations, up\nto 40% of tokens can be confidently predicted and halted before reaching the\nfinal encoder layer.", "AI": {"tldr": "STEP\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8865\u4e01\u5408\u5e76\u548c\u4ee4\u724c\u526a\u679d\uff0c\u663e\u8457\u63d0\u5347\u4e86ViTs\u5728\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u6548\u7387\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e4\u500d\uff0c\u901f\u5ea6\u63d0\u53471.7\u500d\uff0c\u51c6\u786e\u7387\u635f\u5931\u6781\u5c0f\u3002", "motivation": "\u89e3\u51b3Vision Transformers\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u56e0\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u800c\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86STEP\uff08SuperToken\u548cEarly-Pruning\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u8865\u4e01\u5408\u5e76\u548c\u4ee4\u724c\u526a\u679d\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7CNN\u7b56\u7565\u7f51\u7edcdCTS\u5b9e\u73b0\u7075\u6d3b\u5408\u5e76\uff0c\u5e76\u5728\u7f16\u7801\u5668\u5757\u4e2d\u96c6\u6210\u65e9\u671f\u9000\u51fa\u673a\u5236\u4ee5\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "\u57281024 x 1024\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTEP\u6846\u67b6\u5b9e\u73b0\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e4\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53471.7\u500d\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u4e0d\u8d85\u8fc72.0%\u3002", "conclusion": "STEP\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u52a8\u6001\u8865\u4e01\u5408\u5e76\u548c\u4ee4\u724c\u526a\u679d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86Vision Transformers\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.13864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13864", "abs": "https://arxiv.org/abs/2509.13864", "authors": ["Jovana Videnovic", "Matej Kristan", "Alan Lukezic"], "title": "Distractor-Aware Memory-Based Visual Object Tracking", "comment": "Code available on Github: https://github.com/jovanavidenovic/DAM4SAM", "summary": "Recent emergence of memory-based video segmentation methods such as SAM2 has\nled to models with excellent performance in segmentation tasks, achieving\nleading results on numerous benchmarks. However, these modes are not fully\nadjusted for visual object tracking, where distractors (i.e., objects visually\nsimilar to the target) pose a key challenge. In this paper we propose a\ndistractor-aware drop-in memory module and introspection-based management\nmethod for SAM2, leading to DAM4SAM. Our design effectively reduces the\ntracking drift toward distractors and improves redetection capability after\nobject occlusion. To facilitate the analysis of tracking in the presence of\ndistractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM\noutperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results\non ten. Furthermore, integrating the proposed distractor-aware memory into a\nreal-time tracker EfficientTAM leads to 11% improvement and matches tracking\nquality of the non-real-time SAM2.1-L on multiple tracking and segmentation\nbenchmarks, while integration with edge-based tracker EdgeTAM delivers 4%\nperformance boost, demonstrating a very good generalization across\narchitectures.", "AI": {"tldr": "DAM4SAM\u901a\u8fc7\u5e72\u6270\u7269\u611f\u77e5\u5185\u5b58\u6a21\u5757\u6539\u8fdb\u4e86SAM2\u7684\u8ffd\u8e2a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5e72\u6270\u7269\u548c\u906e\u6321\u573a\u666f\u4e0b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u9886\u5148\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5185\u5b58\u7684\u89c6\u9891\u5206\u5272\u65b9\u6cd5\uff08\u5982SAM2\uff09\u5728\u89c6\u89c9\u76ee\u6807\u8ffd\u8e2a\u4e2d\u672a\u80fd\u5145\u5206\u9002\u5e94\u5e72\u6270\u7269\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u8ffd\u8e2a\u6f02\u79fb\u548c\u906e\u6321\u540e\u91cd\u68c0\u6d4b\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e72\u6270\u7269\u611f\u77e5\u7684drop-in\u5185\u5b58\u6a21\u5757\u548c\u57fa\u4e8e\u81ea\u7701\u7684\u7ba1\u7406\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86DiDi\u6570\u636e\u96c6\u4ee5\u5206\u6790\u5e72\u6270\u7269\u5bf9\u8ffd\u8e2a\u7684\u5f71\u54cd\u3002", "result": "DAM4SAM\u572813\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eSAM2.1\uff0c\u5e76\u572810\u4e2a\u6d4b\u8bd5\u4e2d\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002\u4e0e\u5b9e\u65f6\u8ffd\u8e2a\u5668EfficientTAM\u548c\u8fb9\u7f18\u8ffd\u8e2a\u5668EdgeTAM\u7684\u96c6\u6210\u5206\u522b\u63d0\u5347\u4e8611%\u548c4%\u7684\u6027\u80fd\u3002", "conclusion": "DAM4SAM\u901a\u8fc7\u5f15\u5165\u5e72\u6270\u7269\u611f\u77e5\u7684drop-in\u5185\u5b58\u6a21\u5757\u548c\u57fa\u4e8e\u81ea\u7701\u7684\u7ba1\u7406\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86SAM2\u5728\u89c6\u89c9\u76ee\u6807\u8ffd\u8e2a\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5e72\u6270\u7269\u548c\u76ee\u6807\u906e\u6321\u65b9\u9762\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7684\u7ed3\u679c\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u67b6\u6784\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.14147", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14147", "abs": "https://arxiv.org/abs/2509.14147", "authors": ["Fanxing Li", "Shengyang Wang", "Fangyu Sun", "Shuyu Wu", "Dexin Zuo", "Wenxian Yu", "Danping Zou"], "title": "StableTracker: Learning to Stably Track Target via Differentiable Simulation", "comment": null, "summary": "FPV object tracking methods heavily rely on handcraft modular designs,\nresulting in hardware overload and cumulative error, which seriously degrades\nthe tracking performance, especially for rapidly accelerating or decelerating\ntargets. To address these challenges, we present \\textbf{StableTracker}, a\nlearning-based control policy that enables quadrotors to robustly follow the\nmoving target from arbitrary perspectives. The policy is trained using\nbackpropagation-through-time via differentiable simulation, allowing the\nquadrotor to maintain the target at the center of the visual field in both\nhorizontal and vertical directions, while keeping a fixed relative distance,\nthereby functioning as an autonomous aerial camera. We compare StableTracker\nagainst both state-of-the-art traditional algorithms and learning baselines.\nSimulation experiments demonstrate that our policy achieves superior accuracy,\nstability and generalization across varying safe distances, trajectories, and\ntarget velocities. Furthermore, a real-world experiment on a quadrotor with an\nonboard computer validated practicality of the proposed approach.", "AI": {"tldr": "StableTracker\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u6a21\u62df\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u76ee\u6807\u8ddf\u8e2a\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684FPV\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u6a21\u5757\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u786c\u4ef6\u8fc7\u8f7d\u548c\u7d2f\u79ef\u8bef\u5dee\uff0c\u5c24\u5176\u5728\u76ee\u6807\u5feb\u901f\u52a0\u51cf\u901f\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u65f6\u95f4\u7684\u53cd\u5411\u4f20\u64ad\u548c\u53ef\u5fae\u5206\u6a21\u62df\u8bad\u7ec3\u63a7\u5236\u7b56\u7565\uff0c\u4f7f\u65e0\u4eba\u673a\u80fd\u5728\u6c34\u5e73\u548c\u5782\u76f4\u65b9\u5411\u4e0a\u4fdd\u6301\u76ee\u6807\u5728\u89c6\u91ce\u4e2d\u5fc3\uff0c\u5e76\u7ef4\u6301\u56fa\u5b9a\u76f8\u5bf9\u8ddd\u79bb\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0cStableTracker\u5728\u51c6\u786e\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u4f20\u7edf\u7b97\u6cd5\u548c\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "StableTracker \u901a\u8fc7\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u76ee\u6807\u8ddf\u8e2a\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.14199", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14199", "abs": "https://arxiv.org/abs/2509.14199", "authors": ["Haichao Zhang", "Wenhao Chai", "Shwai He", "Ang Li", "Yun Fu"], "title": "Dense Video Understanding with Gated Residual Tokenization", "comment": null, "summary": "High temporal resolution is essential for capturing fine-grained details in\nvideo understanding. However, current video large language models (VLLMs) and\nbenchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or\nkeyframe selection, discarding dense temporal information. This compromise\navoids the high cost of tokenizing every frame, which otherwise leads to\nredundant computation and linear token growth as video length increases. While\nthis trade-off works for slowly changing content, it fails for tasks like\nlecture comprehension, where information appears in nearly every frame and\nrequires precise temporal alignment. To address this gap, we introduce Dense\nVideo Understanding (DVU), which enables high-FPS video comprehension by\nreducing both tokenization time and token overhead. Existing benchmarks are\nalso limited, as their QA pairs focus on coarse content changes. We therefore\npropose DIVE (Dense Information Video Evaluation), the first benchmark designed\nfor dense temporal reasoning. To make DVU practical, we present Gated Residual\nTokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated\nTokenization uses pixel-level motion estimation to skip static regions during\ntokenization, achieving sub-linear growth in token count and compute. (2)\nSemantic-Scene Intra-Tokenization Merging fuses tokens across static regions\nwithin a scene, further reducing redundancy while preserving dynamic semantics.\nExperiments on DIVE show that GRT outperforms larger VLLM baselines and scales\npositively with FPS. These results highlight the importance of dense temporal\ninformation and demonstrate that GRT enables efficient, scalable high-FPS video\nunderstanding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGRT\u6846\u67b6\u548cDIVE\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u9ad8\u5e27\u7387\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u8ba1\u7b97\u5197\u4f59\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u65f6\u95f4\u4fe1\u606f\u7684\u5904\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u56e0\u9ad8\u5e27\u7387\u5904\u7406\u7684\u9ad8\u6210\u672c\u800c\u4f9d\u8d56\u4f4e\u5e27\u7387\u91c7\u6837\uff0c\u5bfc\u81f4\u5bc6\u96c6\u65f6\u95f4\u4fe1\u606f\u4e22\u5931\uff0c\u5c24\u5176\u5728\u9700\u8981\u7cbe\u786e\u65f6\u95f4\u5bf9\u9f50\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86Gated Residual Tokenization (GRT)\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) Motion-Compensated Inter-Gated Tokenization\u8df3\u8fc7\u9759\u6001\u533a\u57df\uff0c\u5b9e\u73b0\u6b21\u7ebf\u6027\u589e\u957f\u7684token\u8ba1\u7b97\uff1b2) Semantic-Scene Intra-Tokenization Merging\u878d\u5408\u9759\u6001\u533a\u57df\u7684token\uff0c\u51cf\u5c11\u5197\u4f59\u3002", "result": "\u5728DIVE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGRT\u8868\u73b0\u4f18\u4e8e\u73b0\u6709VLLM\u57fa\u7ebf\uff0c\u4e14\u968f\u7740\u5e27\u7387\u63d0\u5347\u6027\u80fd\u6301\u7eed\u6539\u5584\u3002", "conclusion": "GRT\u6846\u67b6\u901a\u8fc7\u9ad8\u6548\u5904\u7406\u9ad8\u5e27\u7387\u89c6\u9891\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u7684\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u4e86\u5bc6\u96c6\u65f6\u95f4\u4fe1\u606f\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.13873", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13873", "abs": "https://arxiv.org/abs/2509.13873", "authors": ["Siam Tahsin Bhuiyan", "Rashedur Rahman", "Sefatul Wasi", "Naomi Yagi", "Syoji Kobashi", "Ashraful Islam", "Saadia Binte Alam"], "title": "Invisible Yet Detected: PelFANet with Attention-Guided Anatomical Fusion for Pelvic Fracture Diagnosis", "comment": "Accepted at MICCAI EMERGE 2025", "summary": "Pelvic fractures pose significant diagnostic challenges, particularly in\ncases where fracture signs are subtle or invisible on standard radiographs. To\naddress this, we introduce PelFANet, a dual-stream attention network that fuses\nraw pelvic X-rays with segmented bone images to improve fracture\nclassification. The network em-ploys Fused Attention Blocks (FABlocks) to\niteratively exchange and refine fea-tures from both inputs, capturing global\ncontext and localized anatomical detail. Trained in a two-stage pipeline with a\nsegmentation-guided approach, PelFANet demonstrates superior performance over\nconventional methods. On the AMERI dataset, it achieves 88.68% accuracy and\n0.9334 AUC on visible fractures, while generalizing effectively to invisible\nfracture cases with 82.29% accuracy and 0.8688 AUC, despite not being trained\non them. These results highlight the clini-cal potential of anatomy-aware\ndual-input architectures for robust fracture detec-tion, especially in\nscenarios with subtle radiographic presentations.", "AI": {"tldr": "PelFANet\u901a\u8fc7\u878d\u5408X\u5149\u7247\u548c\u5206\u5272\u9aa8\u56fe\u50cf\u7684\u53cc\u6d41\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9aa8\u76c6\u9aa8\u6298\u7684\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728X\u5149\u8868\u73b0\u4e0d\u660e\u663e\u7684\u75c5\u4f8b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9aa8\u76c6\u9aa8\u6298\u5728\u6807\u51c6X\u5149\u7247\u4e0a\u8868\u73b0\u4e0d\u660e\u663e\u6216\u4e0d\u53ef\u89c1\u65f6\uff0c\u8bca\u65ad\u5b58\u5728\u663e\u8457\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86PelFANet\uff0c\u4e00\u79cd\u53cc\u6d41\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u878d\u5408\u539f\u59cb\u9aa8\u76c6X\u5149\u7247\u548c\u5206\u5272\u9aa8\u56fe\u50cf\uff0c\u91c7\u7528Fused Attention Blocks\uff08FABlocks\uff09\u8fed\u4ee3\u4ea4\u6362\u548c\u7cbe\u70bc\u7279\u5f81\u3002\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u548c\u5206\u5272\u5f15\u5bfc\u65b9\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728AMERI\u6570\u636e\u96c6\u4e0a\uff0cPelFANet\u5bf9\u53ef\u89c1\u9aa8\u6298\u7684\u51c6\u786e\u7387\u4e3a88.68%\uff0cAUC\u4e3a0.9334\uff1b\u5bf9\u4e0d\u53ef\u89c1\u9aa8\u6298\u7684\u51c6\u786e\u7387\u4e3a82.29%\uff0cAUC\u4e3a0.8688\uff0c\u5c3d\u7ba1\u672a\u9488\u5bf9\u8fd9\u4e9b\u75c5\u4f8b\u8fdb\u884c\u8bad\u7ec3\u3002", "conclusion": "PelFANet\u5c55\u793a\u4e86\u5728\u9aa8\u76c6\u9aa8\u6298\u8bca\u65ad\u4e2d\u7684\u4e34\u5e8a\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728X\u5149\u8868\u73b0\u4e0d\u660e\u663e\u7684\u75c5\u4f8b\u4e2d\uff0c\u901a\u8fc7\u89e3\u5256\u611f\u77e5\u7684\u53cc\u8f93\u5165\u67b6\u6784\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u9aa8\u6298\u68c0\u6d4b\u3002"}}
{"id": "2509.14159", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14159", "abs": "https://arxiv.org/abs/2509.14159", "authors": ["Dayi Dong", "Maulik Bhatt", "Seoyeon Choi", "Negar Mehr"], "title": "MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies", "comment": "9 pages, 4 figures, 5 tables", "summary": "As robots become more integrated in society, their ability to coordinate with\nother robots and humans on multi-modal tasks (those with multiple valid\nsolutions) is crucial. We propose to learn such behaviors from expert\ndemonstrations via imitation learning (IL). However, when expert demonstrations\nare multi-modal, standard IL approaches can struggle to capture the diverse\nstrategies, hindering effective coordination. Diffusion models are known to be\neffective at handling complex multi-modal trajectory distributions in\nsingle-agent systems. Diffusion models have also excelled in multi-agent\nscenarios where multi-modality is more common and crucial to learning\ncoordinated behaviors. Typically, diffusion-based approaches require a\ncentralized planner or explicit communication among agents, but this assumption\ncan fail in real-world scenarios where robots must operate independently or\nwith agents like humans that they cannot directly communicate with. Therefore,\nwe propose MIMIC-D, a Centralized Training, Decentralized Execution (CTDE)\nparadigm for multi-modal multi-agent imitation learning using diffusion\npolicies. Agents are trained jointly with full information, but execute\npolicies using only local information to achieve implicit coordination. We\ndemonstrate in both simulation and hardware experiments that our method\nrecovers multi-modal coordination behavior among agents in a variety of tasks\nand environments, while improving upon state-of-the-art baselines.", "AI": {"tldr": "MIMIC-D\u5229\u7528\u6269\u6563\u7b56\u7565\u5b9e\u73b0\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u6a21\u4eff\u5b66\u4e60\u7684\u9690\u5f0f\u534f\u8c03\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5728\u793e\u4f1a\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u4e0e\u4eba\u7c7b\u548c\u5176\u4ed6\u673a\u5668\u4eba\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u534f\u8c03\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u4e13\u5bb6\u6f14\u793a\u4e0b\u96be\u4ee5\u6355\u6349\u591a\u6837\u5316\u7b56\u7565\uff0c\u5f71\u54cd\u534f\u8c03\u6548\u679c\u3002", "method": "\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u3001\u5206\u6563\u6267\u884c\uff08CTDE\uff09\u8303\u5f0f\uff0c\u5229\u7528\u6269\u6563\u7b56\u7565\u8fdb\u884c\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u6a21\u4eff\u5b66\u4e60\uff0c\u667a\u80fd\u4f53\u5728\u8bad\u7ec3\u65f6\u5171\u4eab\u5b8c\u6574\u4fe1\u606f\uff0c\u6267\u884c\u65f6\u4ec5\u4f9d\u8d56\u672c\u5730\u4fe1\u606f\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0cMIMIC-D\u6210\u529f\u6062\u590d\u4e86\u667a\u80fd\u4f53\u95f4\u7684\u591a\u6a21\u6001\u534f\u8c03\u884c\u4e3a\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MIMIC-D\u65b9\u6cd5\u901a\u8fc7\u6269\u6563\u7b56\u7565\u5728\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u6a21\u4eff\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u9690\u5f0f\u534f\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728\u5404\u79cd\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u7684\u534f\u8c03\u80fd\u529b\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2509.13883", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13883", "abs": "https://arxiv.org/abs/2509.13883", "authors": ["Zhen Xu", "Guorui Lu", "Chang Gao", "Qinyu Chen"], "title": "EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View", "comment": "8 pages", "summary": "Hand tracking holds great promise for intuitive interaction paradigms, but\nframe-based methods often struggle to meet the requirements of accuracy, low\nlatency, and energy efficiency, especially in resource-constrained settings\nsuch as Extended Reality (XR) devices. Event cameras provide $\\mu$s-level\ntemporal resolution at mW-level power by asynchronously sensing brightness\nchanges. In this work, we present EvHand-FPV, a lightweight framework for\negocentric First-Person-View 3D hand tracking from a single event camera. We\nconstruct an event-based FPV dataset that couples synthetic training data with\n3D labels and real event data with 2D labels for evaluation to address the\nscarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based\nregion of interest (ROI) that localizes the hand region via geometric cues,\ncombined with an end-to-end mapping strategy that embeds ROI offsets into the\nnetwork to reduce computation without explicit reconstruction, and a multi-task\nlearning strategy with an auxiliary geometric feature head that improves\nrepresentations without test-time overhead. On our real FPV test set,\nEvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from\n11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It\nalso maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results\ndemonstrate accurate and efficient egocentric event-based hand tracking\nsuitable for on-device XR applications. The dataset and code are available at\nhttps://github.com/zen5x5/EvHand-FPV.", "AI": {"tldr": "EvHand-FPV \u901a\u8fc7\u4e8b\u4ef6\u6444\u50cf\u5934\u548c\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u90e8\u8ddf\u8e2a\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8eXR\u8bbe\u5907\u3002", "motivation": "\u89e3\u51b3\u5e27\u57fa\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u4f4e\u5ef6\u8fdf\u548c\u80fd\u6548\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684XR\u8bbe\u5907\u4e2d\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4e8b\u4ef6\u9a71\u52a8\u7684FPV\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5408\u6210\u8bad\u7ec3\u6570\u636e\u548c\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\uff0c\u5f15\u5165\u624b\u8155ROI\u548c\u7aef\u5230\u7aef\u6620\u5c04\u7b56\u7565\uff0c\u4ee5\u53ca\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "result": "\u5728\u771f\u5b9eFPV\u6d4b\u8bd5\u96c6\u4e0a\uff0c2D-AUCp\u4ece0.77\u63d0\u5347\u81f30.85\uff0c\u53c2\u6570\u51cf\u5c1189%\uff0cFLOPs\u964d\u4f4e89%\uff0c3D-AUCp\u4fdd\u63010.84\u3002", "conclusion": "EvHand-FPV \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u4ece\u5355\u4e2a\u4e8b\u4ef6\u6444\u50cf\u5934\u8fdb\u884c\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u76843D\u624b\u90e8\u8ddf\u8e2a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u9002\u5408\u8bbe\u5907\u7aefXR\u5e94\u7528\u3002"}}
{"id": "2509.14178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14178", "abs": "https://arxiv.org/abs/2509.14178", "authors": ["Kai Ye", "Yuhang Wu", "Shuyuan Hu", "Junliang Li", "Meng Liu", "Yongquan Chen", "Rui Huang"], "title": "\\textsc{Gen2Real}: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video", "comment": null, "summary": "Dexterous manipulation remains a challenging robotics problem, largely due to\nthe difficulty of collecting extensive human demonstrations for learning. In\nthis paper, we introduce \\textsc{Gen2Real}, which replaces costly human demos\nwith one generated video and drives robot skill from it: it combines\ndemonstration generation that leverages video generation with pose and depth\nestimation to yield hand-object trajectories, trajectory optimization that uses\nPhysics-aware Interaction Optimization Model (PIOM) to impose physics\nconsistency, and demonstration learning that retargets human motions to a robot\nhand and stabilizes control with an anchor-based residual Proximal Policy\nOptimization (PPO) policy. Using only generated videos, the learned policy\nachieves a 77.3\\% success rate on grasping tasks in simulation and demonstrates\ncoherent executions on a real robot. We also conduct ablation studies to\nvalidate the contribution of each component and demonstrate the ability to\ndirectly specify tasks using natural language, highlighting the flexibility and\nrobustness of \\textsc{Gen2Real} in generalizing grasping skills from imagined\nvideos to real-world execution.", "AI": {"tldr": "Gen2Real\u901a\u8fc7\u751f\u6210\u89c6\u9891\u66ff\u4ee3\u4eba\u7c7b\u6f14\u793a\uff0c\u7ed3\u5408\u7269\u7406\u4f18\u5316\u548c\u5b66\u4e60\u7b56\u7565\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u673a\u5668\u4eba\u6293\u53d6\u6280\u80fd\u8f6c\u79fb\u3002", "motivation": "\u89e3\u51b3\u7075\u5de7\u64cd\u4f5c\u4e2d\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u6536\u96c6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u89c6\u9891\u66ff\u4ee3\u6602\u8d35\u7684\u4eba\u7c7b\u6f14\u793a\uff0c\u9a71\u52a8\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u3002", "method": "Gen2Real\u7ed3\u5408\u4e86\u89c6\u9891\u751f\u6210\u4e0e\u59ff\u6001\u548c\u6df1\u5ea6\u4f30\u8ba1\u751f\u6210\u624b-\u7269\u4f53\u8f68\u8ff9\uff0c\u4f7f\u7528\u7269\u7406\u611f\u77e5\u4ea4\u4e92\u4f18\u5316\u6a21\u578b\uff08PIOM\uff09\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u951a\u70b9\u7684\u6b8b\u5deePPO\u7b56\u7565\u8fdb\u884c\u5b66\u4e60\u548c\u63a7\u5236\u7a33\u5b9a\u3002", "result": "\u4ec5\u4f7f\u7528\u751f\u6210\u89c6\u9891\u5b66\u4e60\u7684\u7b56\u7565\u5728\u6a21\u62df\u6293\u53d6\u4efb\u52a1\u4e2d\u8fbe\u523077.3%\u7684\u6210\u529f\u7387\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u8fde\u8d2f\u7684\u6267\u884c\u80fd\u529b\u3002", "conclusion": "Gen2Real\u6210\u529f\u5730\u5c06\u751f\u6210\u89c6\u9891\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u6280\u80fd\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u751f\u6210\u3001\u7269\u7406\u4f18\u5316\u548c\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fde\u8d2f\u6267\u884c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.13907", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13907", "abs": "https://arxiv.org/abs/2509.13907", "authors": ["Jiyun Im", "SuBeen Lee", "Miso Lee", "Jae-Pil Heo"], "title": "White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation", "comment": "9 pages, 5 figures", "summary": "Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point\nlabels for an unlabeled point cloud, given only a few labeled examples. To\nextract discriminative representations from the limited support set, existing\nmethods have constructed prototypes using conventional algorithms such as\nfarthest point sampling. However, we point out that its initial randomness\nsignificantly affects FS-PCS performance and that the prototype generation\nprocess remains underexplored despite its prevalence. This motivates us to\ninvestigate an advanced prototype generation method based on attention\nmechanism. Despite its potential, we found that vanilla module suffers from the\ndistributional gap between learnable prototypical tokens and support features.\nTo overcome this, we propose White Aggregation and Restoration Module (WARM),\nwhich resolves the misalignment by sandwiching cross-attention between\nwhitening and coloring transformations. Specifically, whitening aligns the\nsupport features to prototypical tokens before attention process, and\nsubsequently coloring restores the original distribution to the attended\ntokens. This simple yet effective design enables robust attention, thereby\ngenerating representative prototypes by capturing the semantic relationships\namong support features. Our method achieves state-of-the-art performance with a\nsignificant margin on multiple FS-PCS benchmarks, demonstrating its\neffectiveness through extensive experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faWARM\u6a21\u5757\uff0c\u901a\u8fc7\u767d\u5316\u548c\u67d3\u8272\u53d8\u6362\u4f18\u5316\u539f\u578b\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347FS-PCS\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u539f\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u5b58\u5728\u521d\u59cb\u968f\u673a\u6027\u5f71\u54cd\u6027\u80fd\u7684\u95ee\u9898\uff0c\u4e14\u539f\u578b\u751f\u6210\u8fc7\u7a0b\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u66f4\u5148\u8fdb\u7684\u539f\u578b\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u539f\u578b\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408White Aggregation and Restoration Module (WARM)\uff0c\u901a\u8fc7\u767d\u5316\u548c\u67d3\u8272\u53d8\u6362\u4f18\u5316\u7279\u5f81\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2aFS-PCS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWARM\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684WARM\u6a21\u5757\u901a\u8fc7\u7ed3\u5408\u767d\u5316\u548c\u67d3\u8272\u53d8\u6362\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u539f\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5206\u5e03\u5dee\u8ddd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86FS-PCS\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14191", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14191", "abs": "https://arxiv.org/abs/2509.14191", "authors": ["Zhihao Cao", "Hanyu Wu", "Li Wa Tang", "Zizhou Luo", "Zihan Zhu", "Wei Zhang", "Marc Pollefeys", "Martin R. Oswald"], "title": "MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping", "comment": null, "summary": "Recent progress in dense SLAM has primarily targeted monocular setups, often\nat the expense of robustness and geometric coverage. We present MCGS-SLAM, the\nfirst purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting\n(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM\nfuses dense RGB inputs from multiple viewpoints into a unified, continuously\noptimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines\nposes and depths via dense photometric and geometric residuals, while a scale\nconsistency module enforces metric alignment across views using low-rank\npriors. The system supports RGB input and maintains real-time performance at\nlarge scale. Experiments on synthetic and real-world datasets show that\nMCGS-SLAM consistently yields accurate trajectories and photorealistic\nreconstructions, usually outperforming monocular baselines. Notably, the wide\nfield of view from multi-camera input enables reconstruction of side-view\nregions that monocular setups miss, critical for safe autonomous operation.\nThese results highlight the promise of multi-camera Gaussian Splatting SLAM for\nhigh-fidelity mapping in robotics and autonomous driving.", "AI": {"tldr": "MCGS-SLAM\u662f\u9996\u4e2a\u57fa\u4e8eRGB\u7684\u591a\u76f8\u673aSLAM\u7cfb\u7edf\uff0c\u5229\u75283D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\uff0c\u901a\u8fc7\u591a\u76f8\u673a\u675f\u8c03\u6574\u548c\u5c3a\u5ea6\u4e00\u81f4\u6027\u6a21\u5757\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8f68\u8ff9\u548c\u903c\u771f\u91cd\u5efa\uff0c\u4f18\u4e8e\u5355\u76ee\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u5355\u76eeSLAM\u5728\u9c81\u68d2\u6027\u548c\u51e0\u4f55\u8986\u76d6\u4e0a\u7684\u4e0d\u8db3\uff0c\u5f00\u53d1\u4e86\u9996\u4e2a\u57fa\u4e8eRGB\u7684\u591a\u76f8\u673aSLAM\u7cfb\u7edf\uff0c\u5229\u75283D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u3002", "method": "MCGS-SLAM\u901a\u8fc7\u591a\u76f8\u673a\u675f\u8c03\u6574\uff08MCBA\uff09\u8054\u5408\u4f18\u5316\u4f4d\u59ff\u548c\u6df1\u5ea6\uff0c\u5229\u7528\u5bc6\u96c6\u7684\u5149\u5ea6\u548c\u51e0\u4f55\u6b8b\u5dee\uff0c\u540c\u65f6\u901a\u8fc7\u5c3a\u5ea6\u4e00\u81f4\u6027\u6a21\u5757\u4f7f\u7528\u4f4e\u79e9\u5148\u9a8c\u5f3a\u5236\u89c6\u56fe\u95f4\u7684\u5ea6\u91cf\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMCGS-SLAM\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4ea7\u751f\u51c6\u786e\u7684\u8f68\u8ff9\u548c\u903c\u771f\u7684\u91cd\u5efa\uff0c\u901a\u5e38\u4f18\u4e8e\u5355\u76ee\u57fa\u7ebf\u3002\u591a\u76f8\u673a\u8f93\u5165\u7684\u5927\u89c6\u573a\u89d2\u80fd\u591f\u91cd\u5efa\u5355\u76ee\u8bbe\u7f6e\u9057\u6f0f\u7684\u4fa7\u89c6\u533a\u57df\u3002", "conclusion": "MCGS-SLAM\u5c55\u793a\u4e86\u591a\u76f8\u673a\u9ad8\u65af\u6e85\u5c04SLAM\u5728\u673a\u5668\u4eba\u5b66\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\u9ad8\u4fdd\u771f\u5730\u56fe\u6784\u5efa\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13919", "abs": "https://arxiv.org/abs/2509.13919", "authors": ["Yuanchen Wu", "Ke Yan", "Shouhong Ding", "Ziyin Zhou", "Xiaoqiang Li"], "title": "Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration", "comment": "Accepted by ICML 2025", "summary": "Large Vision-Language Models (LVLMs) have manifested strong visual question\nanswering capability. However, they still struggle with aligning the rationale\nand the generated answer, leading to inconsistent reasoning and incorrect\nresponses. To this end, this paper introduces the Self-Rationale Calibration\n(SRC) framework to iteratively calibrate the alignment between rationales and\nanswers. SRC begins by employing a lightweight \"rationale fine-tuning\"\napproach, which modifies the model's response format to require a rationale\nbefore deriving an answer without explicit prompts. Next, SRC searches for a\ndiverse set of candidate responses from the fine-tuned LVLMs for each sample,\nfollowed by a proposed pairwise scoring strategy using a tailored scoring\nmodel, R-Scorer, to evaluate both rationale quality and factual consistency of\ncandidates. Based on a confidence-weighted preference curation process, SRC\ndecouples the alignment calibration into a preference fine-tuning manner,\nleading to significant improvements of LVLMs in perception, reasoning, and\ngeneralization across multiple benchmarks. Our results emphasize the\nrationale-oriented alignment in exploring the potential of LVLMs.", "AI": {"tldr": "SRC\u6846\u67b6\u901a\u8fc7\u7406\u7531\u5fae\u8c03\u548c\u6210\u5bf9\u8bc4\u5206\u7b56\u7565\u6821\u51c6\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7406\u7531\u4e0e\u7b54\u6848\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7406\u7531\u4e0e\u7b54\u6848\u7684\u5bf9\u9f50\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u63a8\u7406\u4e0d\u4e00\u81f4\u548c\u9519\u8bef\u54cd\u5e94\u3002", "method": "SRC\u6846\u67b6\u5305\u62ec\u8f7b\u91cf\u7ea7\u7684\u201c\u7406\u7531\u5fae\u8c03\u201d\u65b9\u6cd5\u3001\u5019\u9009\u54cd\u5e94\u591a\u6837\u6027\u641c\u7d22\u3001\u57fa\u4e8eR-Scorer\u7684\u6210\u5bf9\u8bc4\u5206\u7b56\u7565\u4ee5\u53ca\u7f6e\u4fe1\u5ea6\u52a0\u6743\u7684\u504f\u597d\u5fae\u8c03\u8fc7\u7a0b\u3002", "result": "SRC\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u81ea\u6821\u51c6\u6846\u67b6SRC\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u3001\u63a8\u7406\u548c\u6cdb\u5316\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u57fa\u4e8e\u7406\u7531\u7684\u5bf9\u9f50\u5728\u63a2\u7d22LVLMs\u6f5c\u529b\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.14210", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14210", "abs": "https://arxiv.org/abs/2509.14210", "authors": ["Seth Farrell", "Chenghao Li", "Hongzhan Yu", "Hesam Mojtahedi", "Sicun Gao", "Henrik I. Christensen"], "title": "GLIDE: A Coordinated Aerial-Ground Framework for Search and Rescue in Unknown Environments", "comment": null, "summary": "We present a cooperative aerial-ground search-and-rescue (SAR) framework that\npairs two unmanned aerial vehicles (UAVs) with an unmanned ground vehicle (UGV)\nto achieve rapid victim localization and obstacle-aware navigation in unknown\nenvironments. We dub this framework Guided Long-horizon Integrated Drone Escort\n(GLIDE), highlighting the UGV's reliance on UAV guidance for long-horizon\nplanning. In our framework, a goal-searching UAV executes real-time onboard\nvictim detection and georeferencing to nominate goals for the ground platform,\nwhile a terrain-scouting UAV flies ahead of the UGV's planned route to provide\nmid-level traversability updates. The UGV fuses aerial cues with local sensing\nto perform time-efficient A* planning and continuous replanning as information\narrives. Additionally, we present a hardware demonstration (using a GEM e6 golf\ncart as the UGV and two X500 UAVs) to evaluate end-to-end SAR mission\nperformance and include simulation ablations to assess the planning stack in\nisolation from detection. Empirical results demonstrate that explicit role\nseparation across UAVs, coupled with terrain scouting and guided planning,\nimproves reach time and navigation safety in time-critical SAR missions.", "AI": {"tldr": "GLIDE\u662f\u4e00\u4e2a\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u8f66\u8f86\u534f\u540c\u7684\u641c\u6551\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u5206\u79bb\u548c\u5b9e\u65f6\u89c4\u5212\u63d0\u9ad8\u6548\u7387\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u672a\u77e5\u73af\u5883\u4e2d\u641c\u6551\u4efb\u52a1\u7684\u65f6\u95f4\u6548\u7387\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002", "method": "\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u65e0\u4eba\u673a\uff08\u4e00\u4e2a\u7528\u4e8e\u76ee\u6807\u641c\u7d22\uff0c\u4e00\u4e2a\u7528\u4e8e\u5730\u5f62\u4fa6\u5bdf\uff09\u548c\u4e00\u4e2a\u5730\u9762\u8f66\u8f86\uff0c\u901a\u8fc7\u5b9e\u65f6\u6570\u636e\u878d\u5408\u548cA*\u8def\u5f84\u89c4\u5212\u8fdb\u884c\u534f\u4f5c\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cGLIDE\u6846\u67b6\u5728\u65f6\u95f4\u7d27\u8feb\u7684\u641c\u6551\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u5230\u8fbe\u65f6\u95f4\u548c\u5bfc\u822a\u5b89\u5168\u6027\u3002", "conclusion": "GLIDE\u6846\u67b6\u901a\u8fc7\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u8f66\u8f86\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u641c\u6551\u4efb\u52a1\u4e2d\u7684\u76ee\u6807\u5b9a\u4f4d\u901f\u5ea6\u548c\u5bfc\u822a\u5b89\u5168\u6027\u3002"}}
{"id": "2509.13922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13922", "abs": "https://arxiv.org/abs/2509.13922", "authors": ["Wenkui Yang", "Jie Cao", "Junxian Duan", "Ran He"], "title": "Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification", "comment": "Accepted to ICCV 2025", "summary": "Diffusion models like Stable Diffusion have become prominent in visual\nsynthesis tasks due to their powerful customization capabilities, which also\nintroduce significant security risks, including deepfakes and copyright\ninfringement. In response, a class of methods known as protective perturbation\nemerged, which mitigates image misuse by injecting imperceptible adversarial\nnoise. However, purification can remove protective perturbations, thereby\nexposing images again to the risk of malicious forgery. In this work, we\nformalize the anti-purification task, highlighting challenges that hinder\nexisting approaches, and propose a simple diagnostic protective perturbation\nnamed AntiPure. AntiPure exposes vulnerabilities of purification within the\n\"purification-customization\" workflow, owing to two guidance mechanisms: 1)\nPatch-wise Frequency Guidance, which reduces the model's influence over\nhigh-frequency components in the purified image, and 2) Erroneous Timestep\nGuidance, which disrupts the model's denoising strategy across different\ntimesteps. With additional guidance, AntiPure embeds imperceptible\nperturbations that persist under representative purification settings,\nachieving effective post-customization distortion. Experiments show that, as a\nstress test for purification, AntiPure achieves minimal perceptual discrepancy\nand maximal distortion, outperforming other protective perturbation methods\nwithin the purification-customization workflow.", "AI": {"tldr": "AntiPure\u662f\u4e00\u79cd\u6297\u7eaf\u5316\u7684\u4fdd\u62a4\u6027\u6270\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u79cd\u5f15\u5bfc\u673a\u5236\u6709\u6548\u62b5\u5fa1\u7eaf\u5316\u653b\u51fb\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u6700\u5927\u5316\u5931\u771f\u3002", "motivation": "\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u7684\u5b9a\u5236\u80fd\u529b\u5e26\u6765\u4e86\u5b89\u5168\u98ce\u9669\uff08\u5982\u6df1\u5ea6\u4f2a\u9020\u548c\u7248\u6743\u4fb5\u6743\uff09\uff0c\u73b0\u6709\u4fdd\u62a4\u6027\u6270\u52a8\u65b9\u6cd5\u6613\u88ab\u7eaf\u5316\u6280\u672f\u79fb\u9664\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6297\u7eaf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAntiPure\u7684\u8bca\u65ad\u6027\u4fdd\u62a4\u6270\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u79cd\u5f15\u5bfc\u673a\u5236\uff1a1\uff09Patch-wise Frequency Guidance\u51cf\u5c11\u6a21\u578b\u5bf9\u9ad8\u9891\u6210\u5206\u7684\u5f71\u54cd\uff1b2\uff09Erroneous Timestep Guidance\u6270\u4e71\u6a21\u578b\u7684\u53bb\u566a\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAntiPure\u5728\u7eaf\u5316-\u5b9a\u5236\u5de5\u4f5c\u6d41\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u6700\u5c0f\u611f\u77e5\u5dee\u5f02\u548c\u6700\u5927\u5931\u771f\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "AntiPure\u901a\u8fc7\u4e24\u79cd\u5f15\u5bfc\u673a\u5236\uff08Patch-wise Frequency Guidance\u548cErroneous Timestep Guidance\uff09\u6709\u6548\u62b5\u5fa1\u7eaf\u5316\u653b\u51fb\uff0c\u5728\u4fdd\u6301\u6700\u5c0f\u611f\u77e5\u5dee\u5f02\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u5927\u5931\u771f\uff0c\u4f18\u4e8e\u5176\u4ed6\u4fdd\u62a4\u6027\u6270\u52a8\u65b9\u6cd5\u3002"}}
{"id": "2509.14228", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14228", "abs": "https://arxiv.org/abs/2509.14228", "authors": ["Benjamin Shaffer", "Victoria Edwards", "Brooks Kinch", "Nathaniel Trask", "M. Ani Hsieh"], "title": "Multi-robot Multi-source Localization in Complex Flows with Physics-Preserving Environment Models", "comment": null, "summary": "Source localization in a complex flow poses a significant challenge for\nmulti-robot teams tasked with localizing the source of chemical leaks or\ntracking the dispersion of an oil spill. The flow dynamics can be time-varying\nand chaotic, resulting in sporadic and intermittent sensor readings, and\ncomplex environmental geometries further complicate a team's ability to model\nand predict the dispersion. To accurately account for the physical processes\nthat drive the dispersion dynamics, robots must have access to computationally\nintensive numerical models, which can be difficult when onboard computation is\nlimited. We present a distributed mobile sensing framework for source\nlocalization in which each robot carries a machine-learned, finite element\nmodel of its environment to guide information-based sampling. The models are\nused to evaluate an approximate mutual information criterion to drive an\ninfotaxis control strategy, which selects sensing regions that are expected to\nmaximize informativeness for the source localization objective. Our approach\nachieves faster error reduction compared to baseline sensing strategies and\nresults in more accurate source localization compared to baseline machine\nlearning approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u79fb\u52a8\u4f20\u611f\u6846\u67b6\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u7684\u6709\u9650\u5143\u6a21\u578b\u548c\u4fe1\u606f\u8d8b\u5411\u7b56\u7565\uff0c\u5728\u590d\u6742\u6d41\u52a8\u4e2d\u5b9e\u73b0\u9ad8\u6548\u6e90\u5b9a\u4f4d\u3002", "motivation": "\u590d\u6742\u6d41\u52a8\u73af\u5883\u4e2d\u7684\u6e90\u5b9a\u4f4d\u5bf9\u591a\u673a\u5668\u4eba\u56e2\u961f\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u6d41\u52a8\u529b\u5b66\u548c\u73af\u5883\u51e0\u4f55\u7684\u590d\u6742\u6027\u4f7f\u5f97\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u548c\u9884\u6d4b\u3002", "method": "\u6bcf\u4e2a\u673a\u5668\u4eba\u643a\u5e26\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u7684\u73af\u5883\u6709\u9650\u5143\u6a21\u578b\uff0c\u7528\u4e8e\u6307\u5bfc\u57fa\u4e8e\u4fe1\u606f\u7684\u91c7\u6837\uff0c\u5e76\u901a\u8fc7\u8fd1\u4f3c\u4e92\u4fe1\u606f\u51c6\u5219\u9a71\u52a8\u4fe1\u606f\u8d8b\u5411\u63a7\u5236\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u7b56\u7565\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u8bef\u5dee\u51cf\u5c11\u548c\u66f4\u51c6\u786e\u7684\u6e90\u5b9a\u4f4d\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u79fb\u52a8\u4f20\u611f\u6846\u67b6\u5728\u590d\u6742\u6d41\u52a8\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5feb\u901f\u548c\u66f4\u51c6\u786e\u7684\u6e90\u5b9a\u4f4d\uff0c\u4f18\u4e8e\u57fa\u7ebf\u4f20\u611f\u7b56\u7565\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2509.13936", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13936", "abs": "https://arxiv.org/abs/2509.13936", "authors": ["Harvey Mannering", "Zhiwu Huang", "Adam Prugel-Bennett"], "title": "Noise-Level Diffusion Guidance: Well Begun is Half Done", "comment": null, "summary": "Diffusion models have achieved state-of-the-art image generation. However,\nthe random Gaussian noise used to start the diffusion process influences the\nfinal output, causing variations in image quality and prompt adherence.\nExisting noise-level optimization approaches generally rely on extra dataset\nconstruction, additional networks, or backpropagation-based optimization,\nlimiting their practicality. In this paper, we propose Noise Level Guidance\n(NLG), a simple, efficient, and general noise-level optimization approach that\nrefines initial noise by increasing the likelihood of its alignment with\ngeneral guidance - requiring no additional training data, auxiliary networks,\nor backpropagation. The proposed NLG approach provides a unified framework\ngeneralizable to both conditional and unconditional diffusion models,\naccommodating various forms of diffusion-level guidance. Extensive experiments\non five standard benchmarks demonstrate that our approach enhances output\ngeneration quality and input condition adherence. By seamlessly integrating\nwith existing guidance methods while maintaining computational efficiency, our\nmethod establishes NLG as a practical and scalable enhancement to diffusion\nmodels. Code can be found at\nhttps://github.com/harveymannering/NoiseLevelGuidance.", "AI": {"tldr": "NLG\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u566a\u58f0\u63d0\u5347\u6269\u6563\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8d44\u6e90\uff0c\u517c\u5bb9\u591a\u79cd\u5f15\u5bfc\u5f62\u5f0f\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u521d\u59cb\u968f\u673a\u9ad8\u65af\u566a\u58f0\u4f1a\u5f71\u54cd\u6700\u7ec8\u8f93\u51fa\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u63d0\u793a\u4e00\u81f4\u6027\uff0c\u73b0\u6709\u566a\u58f0\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u6570\u636e\u96c6\u3001\u7f51\u7edc\u6216\u53cd\u5411\u4f20\u64ad\uff0c\u5b9e\u7528\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u566a\u58f0\u6c34\u5e73\u5f15\u5bfc\uff08NLG\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u52a0\u521d\u59cb\u566a\u58f0\u4e0e\u901a\u7528\u5f15\u5bfc\u7684\u5bf9\u9f50\u6982\u7387\u6765\u4f18\u5316\u566a\u58f0\u6c34\u5e73\uff0c\u9002\u7528\u4e8e\u6761\u4ef6\u548c\u975e\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u517c\u5bb9\u591a\u79cd\u6269\u6563\u7ea7\u5f15\u5bfc\u5f62\u5f0f\u3002", "result": "\u5728\u4e94\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cNLG\u65b9\u6cd5\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u548c\u8f93\u5165\u6761\u4ef6\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u566a\u58f0\u6c34\u5e73\u5f15\u5bfc\uff08NLG\uff09\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u566a\u58f0\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6570\u636e\u3001\u8f85\u52a9\u7f51\u7edc\u6216\u53cd\u5411\u4f20\u64ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u8f93\u5165\u6761\u4ef6\u4e00\u81f4\u6027\uff0c\u6210\u4e3a\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u589e\u5f3a\u65b9\u6cd5\u3002"}}
{"id": "2509.13939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13939", "abs": "https://arxiv.org/abs/2509.13939", "authors": ["Gia Khanh Nguyen", "Yifeng Huang", "Minh Hoai"], "title": "Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation", "comment": null, "summary": "Visual counting is a fundamental yet challenging task, especially when users\nneed to count objects of a specific type in complex scenes. While recent\nmodels, including class-agnostic counting models and large vision-language\nmodels (VLMs), show promise in counting tasks, their ability to perform\nfine-grained, intent-driven counting remains unclear. In this paper, we\nintroduce PairTally, a benchmark dataset specifically designed to evaluate\nfine-grained visual counting. Each of the 681 high-resolution images in\nPairTally contains two object categories, requiring models to distinguish and\ncount based on subtle differences in shape, size, color, or semantics. The\ndataset includes both inter-category (distinct categories) and intra-category\n(closely related subcategories) settings, making it suitable for rigorous\nevaluation of selective counting capabilities. We benchmark a variety of\nstate-of-the-art models, including exemplar-based methods, language-prompted\nmodels, and large VLMs. Our results show that despite recent advances, current\nmodels struggle to reliably count what users intend, especially in fine-grained\nand visually ambiguous cases. PairTally provides a new foundation for\ndiagnosing and improving fine-grained visual counting systems.", "AI": {"tldr": "PairTally\u662f\u4e00\u4e2a\u4e13\u4e3a\u8bc4\u4f30\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8ba1\u6570\u8bbe\u8ba1\u7684\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u5f53\u524d\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u548c\u610f\u56fe\u9a71\u52a8\u7684\u8ba1\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u8fd9\u4e9b\u80fd\u529b\u3002", "method": "\u5f15\u5165PairTally\u6570\u636e\u96c6\uff0c\u5305\u542b681\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u6bcf\u5f20\u56fe\u50cf\u5305\u542b\u4e24\u4e2a\u7269\u4f53\u7c7b\u522b\uff0c\u8981\u6c42\u6a21\u578b\u57fa\u4e8e\u5f62\u72b6\u3001\u5927\u5c0f\u3001\u989c\u8272\u6216\u8bed\u4e49\u7684\u7ec6\u5fae\u5dee\u5f02\u8fdb\u884c\u533a\u5206\u548c\u8ba1\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c3d\u7ba1\u73b0\u6709\u6a21\u578b\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u548c\u89c6\u89c9\u6a21\u7cca\u60c5\u51b5\u4e0b\u4ecd\u96be\u4ee5\u53ef\u9760\u5730\u5b8c\u6210\u7528\u6237\u610f\u56fe\u9a71\u52a8\u7684\u8ba1\u6570\u4efb\u52a1\u3002", "conclusion": "PairTally\u6570\u636e\u96c6\u4e3a\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8ba1\u6570\u7cfb\u7edf\u7684\u8bca\u65ad\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7528\u6237\u610f\u56fe\u9a71\u52a8\u8ba1\u6570\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.14012", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14012", "abs": "https://arxiv.org/abs/2509.14012", "authors": ["Tamara R. Lenhard", "Andreas Weinmann", "Tobias Koch"], "title": "Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments", "comment": null, "summary": "Drone detection in visually complex environments remains challenging due to\nbackground clutter, small object scale, and camouflage effects. While generic\nobject detectors like YOLO exhibit strong performance in low-texture scenes,\ntheir effectiveness degrades in cluttered environments with low\nobject-background separability. To address these limitations, this work\npresents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework\nthat integrates generic object detection with camouflage object detection\ntechniques. Building upon the original architecture, the proposed iteration\nintroduces systematic advancements in training data composition, feature fusion\nstrategies, and backbone design. Specifically, the training process leverages\nlarge-scale, photo-realistic synthetic data, complemented by a small set of\nreal-world samples, to enhance robustness under visually complex conditions.\nThe contribution of intermediate multi-scale FEDER features is systematically\nevaluated, and detection performance is comprehensively benchmarked across\nmultiple YOLO-based backbone configurations. Empirical results indicate that\nintegrating intermediate FEDER features, in combination with backbone upgrades,\ncontributes to notable performance improvements. In the most promising\nconfiguration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER\nfeatures derived from the DWD module -- these enhancements lead to a FNR\nreduction of up to 39.1 percentage points and a mAP increase of up to 62.8\npercentage points at an IoU threshold of 0.5, compared to the initial baseline.", "AI": {"tldr": "\u589e\u5f3a\u7248YOLO-FEDER FusionNet\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u3001\u7279\u5f81\u878d\u5408\u548c\u9aa8\u5e72\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u73af\u5883\u4e2d\u7684\u65e0\u4eba\u673a\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u590d\u6742\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5982\u80cc\u666f\u6742\u6ce2\u3001\u5c0f\u76ee\u6807\u5c3a\u5ea6\u548c\u4f2a\u88c5\u6548\u5e94\uff0c\u63d0\u5347\u68c0\u6d4b\u5668\u5728\u4f4e\u76ee\u6807-\u80cc\u666f\u53ef\u5206\u6027\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faYOLO-FEDER FusionNet\u7684\u589e\u5f3a\u7248\u672c\uff0c\u6539\u8fdb\u8bad\u7ec3\u6570\u636e\u7ec4\u5408\u3001\u7279\u5f81\u878d\u5408\u7b56\u7565\u548c\u9aa8\u5e72\u7f51\u7edc\u8bbe\u8ba1\uff0c\u5229\u7528\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u548c\u5c11\u91cf\u771f\u5b9e\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6700\u4f73\u914d\u7f6e\uff08YOLOv8l\u9aa8\u5e72\u548cDWD\u6a21\u5757\u7684FEDER\u7279\u5f81\uff09\u76f8\u6bd4\u57fa\u7ebf\uff0cFNR\u964d\u4f4e39.1\u4e2a\u767e\u5206\u70b9\uff0cmAP\u63d0\u534762.8\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u96c6\u6210\u4e2d\u95f4FEDER\u7279\u5f81\u548c\u9aa8\u5e72\u7f51\u7edc\u5347\u7ea7\u7684YOLO-FEDER FusionNet\u5728\u89c6\u89c9\u590d\u6742\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u68c0\u6d4b\u6027\u80fd\uff0cFNR\u964d\u4f4e\u8fbe39.1\u4e2a\u767e\u5206\u70b9\uff0cmAP\u63d0\u5347\u8fbe62.8\u4e2a\u767e\u5206\u70b9\u3002"}}
{"id": "2509.14033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14033", "abs": "https://arxiv.org/abs/2509.14033", "authors": ["Weijie Yin", "Yongjie Ye", "Fangxun Shu", "Yue Liao", "Zijian Kang", "Hongyuan Dong", "Haiyang Yu", "Dingkang Yang", "Jiacong Wang", "Han Wang", "Wenzhuo Liu", "Xiao Liang", "Shuicheng Yan", "Chao Feng"], "title": "SAIL-VL2 Technical Report", "comment": "Technical Report", "summary": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Three\ncore innovations drive its effectiveness. First, a large-scale data curation\npipeline with scoring and filtering strategies enhances both quality and\ndistribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community.", "AI": {"tldr": "SAIL-VL2\u662f\u4e00\u4e2a\u5148\u8fdb\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u6570\u636e\u7b5b\u9009\u3001\u6e10\u8fdb\u8bad\u7ec3\u548cMoE\u8bbe\u8ba1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u4ece\u7ec6\u7c92\u5ea6\u611f\u77e5\u5230\u590d\u6742\u63a8\u7406\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u7b5b\u9009\u6d41\u7a0b\u3001\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u6846\u67b6\uff08\u5305\u62ec\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u3001\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u548c\u601d\u8003\u878d\u5408SFT-RL\u6df7\u5408\u8303\u5f0f\uff09\u4ee5\u53ca\u9ad8\u6548\u7684\u7a00\u758fMixture-of-Experts\uff08MoE\uff09\u8bbe\u8ba1\u3002", "result": "SAIL-VL2\u5728OpenCompass\u6392\u884c\u699c\u4e0a\uff0c2B\u53c2\u6570\u7248\u672c\u57284B\u53c2\u6570\u89c4\u6a21\u4ee5\u4e0b\u7684\u5b98\u65b9\u5f00\u6e90\u6a21\u578b\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "SAIL-VL2\u4f5c\u4e3aSAIL-VL\u7684\u7ee7\u4efb\u8005\uff0c\u57282B\u548c8B\u53c2\u6570\u89c4\u6a21\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728106\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u7ade\u4e89\u529b\uff0c\u7279\u522b\u662f\u5728MMMU\u548cMathVista\u7b49\u6311\u6218\u6027\u63a8\u7406\u57fa\u51c6\u4e0a\u3002"}}
{"id": "2509.14051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14051", "abs": "https://arxiv.org/abs/2509.14051", "authors": ["Suhang You", "Carla Pitarch-Abaigar", "Sanket Kachole", "Sumedh Sonawane", "Juhyung Ha", "Anish Sudarshan Gada", "David Crandall", "Rakesh Shiradkar", "Spyridon Bakas"], "title": "PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd Multi-modal Embeddings", "comment": "11 pages, 1 figure, method paper for CHIMERA 2025 Challenge", "summary": "Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy\n(RP) experience biochemical recurrence (BCR), characterized by increased\nprostate specific antigen (PSA) and associated with increased mortality.\nAccurate early prediction of BCR, at the time of RP, would contribute to prompt\nadaptive clinical decision-making and improved patient outcomes. In this work,\nwe propose prostate cancer BCR prediction via fused multi-modal embeddings\n(PROFUSEme), which learns cross-modal interactions of clinical, radiology, and\npathology data, following an intermediate fusion configuration in combination\nwith Cox Proportional Hazard regressors. Quantitative evaluation of our\nproposed approach reveals superior performance, when compared with late fusion\nconfigurations, yielding a mean C-index of 0.861 ($\\sigma=0.112$) on the\ninternal 5-fold nested cross-validation framework, and a C-index of 0.7103 on\nthe hold out data of CHIMERA 2025 challenge validation leaderboard.", "AI": {"tldr": "PROFUSEme, a multi-modal fusion method, excels in predicting prostate cancer recurrence post-surgery, offering better clinical outcomes.", "motivation": "Nearly 30% of prostate cancer patients experience BCR after radical prostatectomy, leading to increased mortality. Early and accurate prediction of BCR is crucial for timely clinical interventions.", "method": "The approach involves fused multi-modal embeddings (PROFUSEme) that learn cross-modal interactions of clinical, radiology, and pathology data, using an intermediate fusion configuration with Cox Proportional Hazard regressors.", "result": "PROFUSEme achieved a mean C-index of 0.861 on internal validation and 0.7103 on external validation, outperforming late fusion configurations.", "conclusion": "PROFUSEme demonstrates superior performance in predicting biochemical recurrence (BCR) in prostate cancer patients post-radical prostatectomy, highlighting its potential for improving clinical decision-making and patient outcomes."}}
{"id": "2509.14055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14055", "abs": "https://arxiv.org/abs/2509.14055", "authors": ["Gang Cheng", "Xin Gao", "Li Hu", "Siqi Hu", "Mingyang Huang", "Chaonan Ji", "Ju Li", "Dechao Meng", "Jinwei Qi", "Penchong Qiao", "Zhen Shen", "Yafei Song", "Ke Sun", "Linrui Tian", "Feng Wang", "Guangyuan Wang", "Qi Wang", "Zhongjian Wang", "Jiayu Xiao", "Sheng Xu", "Bang Zhang", "Peng Zhang", "Xindi Zhang", "Zhe Zhang", "Jingren Zhou", "Lian Zhuo"], "title": "Wan-Animate: Unified Character Animation and Replacement with Holistic Replication", "comment": "Project Page: https://humanaigc.github.io/wan-animate/", "summary": "We introduce Wan-Animate, a unified framework for character animation and\nreplacement. Given a character image and a reference video, Wan-Animate can\nanimate the character by precisely replicating the expressions and movements of\nthe character in the video to generate high-fidelity character videos.\nAlternatively, it can integrate the animated character into the reference video\nto replace the original character, replicating the scene's lighting and color\ntone to achieve seamless environmental integration. Wan-Animate is built upon\nthe Wan model. To adapt it for character animation tasks, we employ a modified\ninput paradigm to differentiate between reference conditions and regions for\ngeneration. This design unifies multiple tasks into a common symbolic\nrepresentation. We use spatially-aligned skeleton signals to replicate body\nmotion and implicit facial features extracted from source images to reenact\nexpressions, enabling the generation of character videos with high\ncontrollability and expressiveness. Furthermore, to enhance environmental\nintegration during character replacement, we develop an auxiliary Relighting\nLoRA. This module preserves the character's appearance consistency while\napplying the appropriate environmental lighting and color tone. Experimental\nresults demonstrate that Wan-Animate achieves state-of-the-art performance. We\nare committed to open-sourcing the model weights and its source code.", "AI": {"tldr": "Wan-Animate\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89d2\u8272\u52a8\u753b\u548c\u66ff\u6362\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u786e\u590d\u5236\u8868\u60c5\u548c\u52a8\u4f5c\uff0c\u5e76\u5229\u7528Relighting LoRA\u6a21\u5757\u5b9e\u73b0\u73af\u5883\u6574\u5408\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u4e3a\u89d2\u8272\u52a8\u753b\u548c\u66ff\u6362\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u7cbe\u786e\u590d\u5236\u89c6\u9891\u4e2d\u7684\u8868\u60c5\u548c\u52a8\u4f5c\uff0c\u5e76\u5b9e\u73b0\u65e0\u7f1d\u7684\u73af\u5883\u6574\u5408\u3002", "method": "\u57fa\u4e8eWan\u6a21\u578b\uff0c\u91c7\u7528\u4fee\u6539\u540e\u7684\u8f93\u5165\u8303\u5f0f\u533a\u5206\u53c2\u8003\u6761\u4ef6\u548c\u751f\u6210\u533a\u57df\uff0c\u4f7f\u7528\u7a7a\u95f4\u5bf9\u9f50\u7684\u9aa8\u67b6\u4fe1\u53f7\u590d\u5236\u8eab\u4f53\u52a8\u4f5c\uff0c\u9690\u5f0f\u9762\u90e8\u7279\u5f81\u91cd\u73b0\u8868\u60c5\uff0c\u5e76\u5f00\u53d1\u4e86\u8f85\u52a9Relighting LoRA\u6a21\u5757\u4ee5\u589e\u5f3a\u73af\u5883\u6574\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cWan-Animate\u5728\u89d2\u8272\u52a8\u753b\u548c\u66ff\u6362\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Wan-Animate\u901a\u8fc7\u7edf\u4e00\u7684\u7b26\u53f7\u8868\u793a\u548c\u9ad8\u53ef\u63a7\u6027\uff0c\u5b9e\u73b0\u4e86\u89d2\u8272\u52a8\u753b\u548c\u66ff\u6362\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u627f\u8bfa\u5f00\u6e90\u6a21\u578b\u6743\u91cd\u548c\u6e90\u4ee3\u7801\u3002"}}
{"id": "2509.14060", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14060", "abs": "https://arxiv.org/abs/2509.14060", "authors": ["Jun Du", "Weiwei Xing", "Ming Li", "Fei Richard Yu"], "title": "VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement", "comment": null, "summary": "Current multi-object tracking (MOT) algorithms typically overlook issues\ninherent in low-quality videos, leading to significant degradation in tracking\nperformance when confronted with real-world image deterioration. Therefore,\nadvancing the application of MOT algorithms in real-world low-quality video\nscenarios represents a critical and meaningful endeavor. To address the\nchallenges posed by low-quality scenarios, inspired by vision-language models,\nthis paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking\nframework (VSE-MOT). Specifically, we first design a tri-branch architecture\nthat leverages a vision-language model to extract global visual semantic\ninformation from images and fuse it with query vectors. Subsequently, to\nfurther enhance the utilization of visual semantic information, we introduce\nthe Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion\nModule (VSFM). The MOT-Adapter adapts the extracted global visual semantic\ninformation to suit multi-object tracking tasks, while the VSFM improves the\nefficacy of feature fusion. Through extensive experiments, we validate the\neffectiveness and superiority of the proposed method in real-world low-quality\nvideo scenarios. Its tracking performance metrics outperform those of existing\nmethods by approximately 8% to 20%, while maintaining robust performance in\nconventional scenarios.", "AI": {"tldr": "VSE-MOT\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u8bed\u4e49\u589e\u5f3a\u63d0\u5347\u4f4e\u8d28\u91cf\u89c6\u9891\u4e2d\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u6027\u80fd\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd58%-20%\u3002", "motivation": "\u73b0\u6709MOT\u7b97\u6cd5\u5728\u4f4e\u8d28\u91cf\u89c6\u9891\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u4f4e\u8d28\u91cf\u89c6\u9891\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e09\u5206\u652f\u67b6\u6784\uff0c\u5305\u62ecMOT-Adapter\u548cVSFM\u6a21\u5757\uff0c\u7528\u4e8e\u63d0\u53d6\u548c\u878d\u5408\u5168\u5c40\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86VSE-MOT\u5728\u4f4e\u8d28\u91cf\u89c6\u9891\u4e2d\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u6027\u80fd\u6307\u6807\u663e\u8457\u63d0\u5347\u3002", "conclusion": "VSE-MOT\u6846\u67b6\u5728\u4f4e\u8d28\u91cf\u89c6\u9891\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u591a\u76ee\u6807\u8ddf\u8e2a\u6027\u80fd\uff0c\u5176\u6027\u80fd\u6307\u6807\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd58%\u81f320%\uff0c\u540c\u65f6\u5728\u5e38\u89c4\u573a\u666f\u4e2d\u4fdd\u6301\u7a33\u5065\u8868\u73b0\u3002"}}
{"id": "2509.14084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14084", "abs": "https://arxiv.org/abs/2509.14084", "authors": ["Jingyi Yuan", "Jianxiong Ye", "Wenkang Chen", "Chenqiang Gao"], "title": "AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration", "comment": null, "summary": "Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary\nnovel categories, offering a scalable and annotation-efficient solution.\nTraditionally, most ZSAD works have been based on the CLIP model, which\nperforms anomaly detection by calculating the similarity between visual and\ntext embeddings. Recently, vision foundation models such as DINOv3 have\ndemonstrated strong transferable representation capabilities. In this work, we\nare the first to adapt DINOv3 for ZSAD. However, this adaptation presents two\nkey challenges: (i) the domain bias between large-scale pretraining data and\nanomaly detection tasks leads to feature misalignment; and (ii) the inherent\nbias toward global semantics in pretrained representations often leads to\nsubtle anomalies being misinterpreted as part of the normal foreground objects,\nrather than being distinguished as abnormal regions. To overcome these\nchallenges, we introduce AD-DINOv3, a novel vision-language multimodal\nframework designed for ZSAD. Specifically, we formulate anomaly detection as a\nmultimodal contrastive learning problem, where DINOv3 is employed as the visual\nbackbone to extract patch tokens and a CLS token, and the CLIP text encoder\nprovides embeddings for both normal and abnormal prompts. To bridge the domain\ngap, lightweight adapters are introduced in both modalities, enabling their\nrepresentations to be recalibrated for the anomaly detection task. Beyond this\nbaseline alignment, we further design an Anomaly-Aware Calibration Module\n(AACM), which explicitly guides the CLS token to attend to anomalous regions\nrather than generic foreground semantics, thereby enhancing discriminability.\nExtensive experiments on eight industrial and medical benchmarks demonstrate\nthat AD-DINOv3 consistently matches or surpasses state-of-the-art methods,\nverifying its superiority as a general zero-shot anomaly detection framework.", "AI": {"tldr": "AD-DINOv3 \u7ed3\u5408 DINOv3 \u548c CLIP\uff0c\u901a\u8fc7\u9002\u914d\u5668\u548c AACM \u89e3\u51b3\u7279\u5f81\u5bf9\u9f50\u4e0e\u8bed\u4e49\u504f\u5dee\u95ee\u9898\uff0c\u5728\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56 CLIP \u6a21\u578b\uff0c\u4f46\u5b58\u5728\u7279\u5f81\u5bf9\u9f50\u4e0d\u8db3\u548c\u5168\u5c40\u8bed\u4e49\u504f\u5dee\u95ee\u9898\u3002DINOv3 \u7684\u5f3a\u53ef\u8fc1\u79fb\u6027\u4e3a\u89e3\u51b3\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u9762\u4e34\u57df\u504f\u5dee\u548c\u8bed\u4e49\u504f\u5dee\u6311\u6218\u3002", "method": "AD-DINOv3 \u662f\u4e00\u79cd\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528 DINOv3 \u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0cCLIP \u63d0\u4f9b\u6587\u672c\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u548c\u5f02\u5e38\u611f\u77e5\u6821\u51c6\u6a21\u5757\uff08AACM\uff09\u4f18\u5316\u7279\u5f81\u5bf9\u9f50\u548c\u5f02\u5e38\u533a\u57df\u8bc6\u522b\u3002", "result": "\u5728\u516b\u4e2a\u5de5\u4e1a\u548c\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAD-DINOv3 \u8868\u73b0\u4e00\u81f4\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u901a\u7528\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "AD-DINOv3 \u901a\u8fc7\u7ed3\u5408 DINOv3 \u548c CLIP \u7684\u4f18\u52bf\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u548c\u5f02\u5e38\u611f\u77e5\u6821\u51c6\u6a21\u5757\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7279\u5f81\u5bf9\u9f50\u548c\u8bed\u4e49\u504f\u5dee\u95ee\u9898\uff0c\u5728\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.14097", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.14097", "abs": "https://arxiv.org/abs/2509.14097", "authors": ["Yaru Chen", "Ruohao Guo", "Liting Gao", "Yang Xiang", "Qingyu Luo", "Zhenbo Li", "Wenwu Wang"], "title": "Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing", "comment": null, "summary": "Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,\nvisible, and audio-visual events without temporal annotations. Previous work\nhas emphasized refining global predictions through contrastive or collaborative\nlearning, but neglected stable segment-level supervision and class-aware\ncross-modal alignment. To address this, we propose two strategies: (1) an\nexponential moving average (EMA)-guided pseudo supervision framework that\ngenerates reliable segment-level masks via adaptive thresholds or top-k\nselection, offering stable temporal guidance beyond video-level labels; and (2)\na class-aware cross-modal agreement (CMA) loss that aligns audio and visual\nembeddings at reliable segment-class pairs, ensuring consistency across\nmodalities while preserving temporal structure. Evaluations on LLP and UnAV-100\ndatasets shows that our method achieves state-of-the-art (SOTA) performance\nacross multiple metrics.", "AI": {"tldr": "\u63d0\u51faEMA\u4f2a\u76d1\u7763\u6846\u67b6\u548cCMA\u635f\u5931\uff0c\u63d0\u5347\u5f31\u76d1\u7763\u97f3\u89c6\u9891\u89c6\u9891\u89e3\u6790\u6027\u80fd\uff0c\u5728LLP\u548cUnAV-100\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "\u89e3\u51b3\u5148\u524d\u5de5\u4f5c\u4e2d\u5ffd\u89c6\u7684\u7a33\u5b9a\u6bb5\u7ea7\u76d1\u7763\u548c\u7c7b\u611f\u77e5\u8de8\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u7b56\u7565\uff1a\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08EMA\uff09\u5f15\u5bfc\u7684\u4f2a\u76d1\u7763\u6846\u67b6\u548c\u7c7b\u611f\u77e5\u8de8\u6a21\u6001\u4e00\u81f4\u6027\uff08CMA\uff09\u635f\u5931\u3002", "result": "\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728LLP\u548cUnAV-100\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14104", "abs": "https://arxiv.org/abs/2509.14104", "authors": ["Leonard Hackel", "Tom Burgert", "Beg\u00fcm Demir"], "title": "CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts", "comment": null, "summary": "Self-supervised learning through masked autoencoders has attracted great\nattention for remote sensing (RS) foundation model (FM) development, enabling\nimproved representation learning across diverse sensors and downstream tasks.\nHowever, existing RS FMs often either suffer from substantial computational\ncomplexity during both training and inference or exhibit limited\nrepresentational capacity. These issues restrict their practical applicability\nin RS. To address this limitation, we propose an adaptation for enhancing the\nefficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism\ninto the FM. The integration of Soft MoEs into the FM allows modality-specific\nexpert specialization alongside shared cross-sensor representation learning. To\ndemonstrate the effectiveness of our adaptation, we apply it on the\nCross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor\nMixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic\ndescriptor-driven sampling strategy for the construction of a representative\nand diverse training set to train our CSMoE model. Extensive experiments on\nscene classification, semantic segmentation, and content-based image retrieval\ndemonstrate that our adaptation yields a reduction in computational\nrequirements while maintaining or improving representational performance.\nCompared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off\nbetween representational capacity, accuracy, and computational efficiency. On\naverage, CSMoE achieves more than twice the computational efficiency of\nexisting RS FMs, while maintaining competitive performance across all\nexperiments. These results show the effectiveness of the proposed adaptation\nfor creating computationally efficient RS FMs. The code for the model, the\ntraining set creation, and the model weights will be available at\nhttps://git.tu-berlin.de/rsim/csmoe.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408Soft MoE\u673a\u5236\u7684CSMoE\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9065\u611f\u57fa\u7840\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u9065\u611f\u57fa\u7840\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u6216\u8868\u793a\u80fd\u529b\u6709\u9650\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u9065\u611f\u57fa\u7840\u6a21\u578b\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06Soft MoE\u673a\u5236\u6574\u5408\u5230Cross-Sensor Masked Autoencoder (CSMAE)\u6a21\u578b\u4e2d\uff0c\u5f62\u6210Cross-Sensor Mixture-of-Experts (CSMoE)\u6a21\u578b\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u9898\u6c14\u5019\u63cf\u8ff0\u7b26\u7684\u91c7\u6837\u7b56\u7565\u6765\u6784\u5efa\u5177\u6709\u4ee3\u8868\u6027\u548c\u591a\u6837\u6027\u7684\u8bad\u7ec3\u96c6\u3002", "result": "CSMoE\u6a21\u578b\u5728\u573a\u666f\u5206\u7c7b\u3001\u8bed\u4e49\u5206\u5272\u548c\u57fa\u4e8e\u5185\u5bb9\u7684\u56fe\u50cf\u68c0\u7d22\u7b49\u5b9e\u9a8c\u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u9065\u611f\u57fa\u7840\u6a21\u578b\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u9ad8\u4e86\u4e24\u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06Soft MoE\u673a\u5236\u6574\u5408\u5230\u57fa\u7840\u6a21\u578b\u4e2d\uff0c\u63d0\u51fa\u7684CSMoE\u6a21\u578b\u5728\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u7684\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u8868\u793a\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u521b\u5efa\u8ba1\u7b97\u9ad8\u6548\u7684\u9065\u611f\u57fa\u7840\u6a21\u578b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.14119", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14119", "abs": "https://arxiv.org/abs/2509.14119", "authors": ["Jiabo MA", "Wenqiang Li", "Jinbang Li", "Ziyi Liu", "Linshan Wu", "Fengtao Zhou", "Li Liang", "Ronald Cheong Kin Chan", "Terence T. W. Wong", "Hao Chen"], "title": "Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows", "comment": "the arxiv version of the under review journal paper", "summary": "Accurate histopathological diagnosis often requires multiple differently\nstained tissue sections, a process that is time-consuming, labor-intensive, and\nenvironmentally taxing due to the use of multiple chemical stains. Recently,\nvirtual staining has emerged as a promising alternative that is faster,\ntissue-conserving, and environmentally friendly. However, existing virtual\nstaining methods face significant challenges in clinical applications,\nprimarily due to their reliance on well-aligned paired data. Obtaining such\ndata is inherently difficult because chemical staining processes can distort\ntissue structures, and a single tissue section cannot undergo multiple staining\nprocedures without damage or loss of information. As a result, most available\nvirtual staining datasets are either unpaired or roughly paired, making it\ndifficult for existing methods to achieve accurate pixel-level supervision. To\naddress this challenge, we propose a robust virtual staining framework\nfeaturing cascaded registration mechanisms to resolve spatial mismatches\nbetween generated outputs and their corresponding ground truth. Experimental\nresults demonstrate that our method significantly outperforms state-of-the-art\nmodels across five datasets, achieving an average improvement of 3.2% on\ninternal datasets and 10.1% on external datasets. Moreover, in datasets with\nsubstantial misalignment, our approach achieves a remarkable 23.8% improvement\nin peak signal-to-noise ratio compared to baseline models. The exceptional\nrobustness of the proposed method across diverse datasets simplifies the data\nacquisition process for virtual staining and offers new insights for advancing\nits development.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7ea7\u8054\u914d\u51c6\u7684\u865a\u62df\u67d3\u8272\u6846\u67b6\uff0c\u89e3\u51b3\u6570\u636e\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u7b80\u5316\u6570\u636e\u91c7\u96c6\u3002", "motivation": "\u4f20\u7edf\u7ec4\u7ec7\u67d3\u8272\u8fc7\u7a0b\u8017\u65f6\u3001\u8017\u529b\u4e14\u5bf9\u73af\u5883\u4e0d\u53cb\u597d\uff0c\u800c\u73b0\u6709\u865a\u62df\u67d3\u8272\u65b9\u6cd5\u4f9d\u8d56\u914d\u5bf9\u6570\u636e\uff0c\u96be\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u50cf\u7d20\u7ea7\u76d1\u7763\u3002", "method": "\u91c7\u7528\u7ea7\u8054\u914d\u51c6\u673a\u5236\u7684\u865a\u62df\u67d3\u8272\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u672a\u914d\u5bf9\u6216\u7c97\u7565\u914d\u5bf9\u6570\u636e\u5e26\u6765\u7684\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5185\u90e8\u6570\u636e\u96c6\u5e73\u5747\u63d0\u53473.2%\uff0c\u5916\u90e8\u6570\u636e\u96c6\u63d0\u534710.1%\uff0c\u5728\u4e25\u91cd\u4e0d\u5bf9\u9f50\u6570\u636e\u4e0aPSNR\u63d0\u534723.8%\u3002", "conclusion": "\u63d0\u51fa\u7684\u865a\u62df\u67d3\u8272\u6846\u67b6\u901a\u8fc7\u7ea7\u8054\u914d\u51c6\u673a\u5236\u89e3\u51b3\u4e86\u751f\u6210\u8f93\u51fa\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u67d3\u8272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7b80\u5316\u4e86\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\u3002"}}
{"id": "2509.14120", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14120", "abs": "https://arxiv.org/abs/2509.14120", "authors": ["Sara Concas", "Simone Maurizio La Cava", "Andrea Panzino", "Ester Masala", "Giulia Orr\u00f9", "Gian Luca Marcialis"], "title": "Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection", "comment": "Accepted at the 2025 IEEE INTERNATIONAL CONFERENCE ON Metrology for\n  eXtended Reality, Artificial Intelligence and Neural Engineering", "summary": "Digital beautification through social media filters has become increasingly\npopular, raising concerns about the reliability of facial images and videos and\nthe effectiveness of automated face analysis. This issue is particularly\ncritical for digital manipulation detectors, systems aiming at distinguishing\nbetween genuine and manipulated data, especially in cases involving deepfakes\nand morphing attacks designed to deceive humans and automated facial\nrecognition. This study examines whether beauty filters impact the performance\nof deepfake and morphing attack detectors. We perform a comprehensive analysis,\nevaluating multiple state-of-the-art detectors on benchmark datasets before and\nafter applying various smoothing filters. Our findings reveal performance\ndegradation, highlighting vulnerabilities introduced by facial enhancements and\nunderscoring the need for robust detection models resilient to such\nalterations.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\u793e\u4ea4\u5a92\u4f53\u7f8e\u5bb9\u6ee4\u955c\u4f1a\u5f71\u54cd\u6df1\u5ea6\u4f2a\u9020\u548c\u53d8\u5f62\u653b\u51fb\u68c0\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6548\u679c\u4e0b\u964d\uff0c\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u3002", "motivation": "\u6570\u5b57\u7f8e\u5316\u901a\u8fc7\u793e\u4ea4\u5a92\u4f53\u6ee4\u955c\u53d8\u5f97\u8d8a\u6765\u8d8a\u6d41\u884c\uff0c\u5f15\u53d1\u4e86\u5bf9\u9762\u90e8\u56fe\u50cf\u548c\u89c6\u9891\u53ef\u9760\u6027\u4ee5\u53ca\u81ea\u52a8\u5316\u9762\u90e8\u5206\u6790\u6709\u6548\u6027\u7684\u62c5\u5fe7\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u6df1\u5ea6\u4f2a\u9020\u548c\u53d8\u5f62\u653b\u51fb\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5bf9\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u5668\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u5e94\u7528\u5404\u79cd\u5e73\u6ed1\u6ee4\u955c\u524d\u540e\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7f8e\u5bb9\u6ee4\u955c\u4f1a\u5bfc\u81f4\u68c0\u6d4b\u5668\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u7f8e\u5bb9\u6ee4\u955c\u4f1a\u5f71\u54cd\u6df1\u5ea6\u4f2a\u9020\u548c\u53d8\u5f62\u653b\u51fb\u68c0\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u9762\u90e8\u589e\u5f3a\u5e26\u6765\u7684\u8106\u5f31\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u9700\u8981\u5bf9\u8fd9\u4e9b\u6539\u53d8\u5177\u6709\u9c81\u68d2\u6027\u7684\u68c0\u6d4b\u6a21\u578b\u3002"}}
{"id": "2509.14142", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14142", "abs": "https://arxiv.org/abs/2509.14142", "authors": ["Peng Xu", "Shengwu Xiong", "Jiajun Zhang", "Yaxiong Chen", "Bowen Zhou", "Chen Change Loy", "David A. Clifton", "Kyoung Mu Lee", "Luc Van Gool", "Ruiming He", "Ruilin Yao", "Xinwei Long", "Jirui Huang", "Kai Tian", "Sa Yang", "Yihua Shao", "Jin Feng", "Yue Zhong", "Jiakai Zhou", "Cheng Tang", "Tianyu Zou", "Yifang Zhang", "Junming Liang", "Guoyou Li", "Zhaoxiang Wang", "Qiang Zhou", "Yichen Zhao", "Shili Xiong", "Hyeongjin Nam", "Jaerin Lee", "Jaeyoung Chung", "JoonKyu Park", "Junghun Oh", "Kanggeon Lee", "Wooseok Lee", "Juneyoung Ro", "Turghun Osman", "Can Hu", "Chaoyang Liao", "Cheng Chen", "Chengcheng Han", "Chenhao Qiu", "Chong Peng", "Cong Xu", "Dailin Li", "Feiyu Wang", "Feng Gao", "Guibo Zhu", "Guopeng Tang", "Haibo Lu", "Han Fang", "Han Qi", "Hanxiao Wu", "Haobo Cheng", "Hongbo Sun", "Hongyao Chen", "Huayong Hu", "Hui Li", "Jiaheng Ma", "Jiang Yu", "Jianing Wang", "Jie Yang", "Jing He", "Jinglin Zhou", "Jingxuan Li", "Josef Kittler", "Lihao Zheng", "Linnan Zhao", "Mengxi Jia", "Muyang Yan", "Nguyen Thanh Thien", "Pu Luo", "Qi Li", "Shien Song", "Shijie Dong", "Shuai Shao", "Shutao Li", "Taofeng Xue", "Tianyang Xu", "Tianyi Gao", "Tingting Li", "Wei Zhang", "Weiyang Su", "Xiaodong Dong", "Xiao-Jun Wu", "Xiaopeng Zhou", "Xin Chen", "Xin Wei", "Xinyi You", "Xudong Kang", "Xujie Zhou", "Xusheng Liu", "Yanan Wang", "Yanbin Huang", "Yang Liu", "Yang Yang", "Yanglin Deng", "Yashu Kang", "Ye Yuan", "Yi Wen", "Yicen Tian", "Yilin Tao", "Yin Tang", "Yipeng Lin", "Yiqing Wang", "Yiting Xi", "Yongkang Yu", "Yumei Li", "Yuxin Qin", "Yuying Chen", "Yuzhe Cen", "Zhaofan Zou", "Zhaohong Liu", "Zhehao Shen", "Zhenglin Du", "Zhengyang Li", "Zhenni Huang", "Zhenwei Shao", "Zhilong Song", "Zhiyong Feng", "Zhiyu Wang", "Zhou Yu", "Ziang Li", "Zihan Zhai", "Zijian Zhang", "Ziyang Peng", "Ziyun Xiao", "Zongshu Li"], "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook", "comment": "ICCV 2025 MARS2 Workshop and Challenge \"Multimodal Reasoning and Slow\n  Thinking in the Large Model Era: Towards System 2 and Beyond''", "summary": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided.", "AI": {"tldr": "MARS2 2025\u6311\u6218\u8d5b\u901a\u8fc7\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u548c\u516c\u5f00\u8d44\u6e90\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\uff0c\u5438\u5f15\u4e86\u5927\u91cf\u56e2\u961f\u53c2\u4e0e\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u6574\u5408\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u548cLLMs\u7684\u4e0d\u540c\u65b9\u6cd5\uff0c\u63a8\u52a8\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u9886\u57df\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u53d1\u5e03\u4e24\u4e2a\u5b9a\u5236\u6570\u636e\u96c6Lens\u548cAdsQA\uff0c\u5e76\u572812\u4e2a\u65e5\u5e38\u573a\u666f\u548c\u5e7f\u544a\u89c6\u9891\u4e2d\u8bc4\u4f3040\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u7ec4\u7ec7\u4e86\u4e09\u4e2a\u7ade\u8d5b\u8d5b\u9053\u3002", "result": "76\u4e2a\u56e2\u961f\u6ce8\u518c\uff0c40\u591a\u4e2a\u6709\u6548\u63d0\u4ea4\u88ab\u7eb3\u5165\u6392\u540d\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u96c6\u516c\u5f00\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86MARS2 2025\u6311\u6218\u8d5b\u5728\u63a8\u52a8\u591a\u6a21\u6001\u63a8\u7406\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u901a\u8fc7\u516c\u5f00\u6570\u636e\u96c6\u3001\u4ee3\u7801\u96c6\u548c\u6392\u540d\uff0c\u4fc3\u8fdb\u4e86\u7814\u7a76\u793e\u533a\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.14149", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14149", "abs": "https://arxiv.org/abs/2509.14149", "authors": ["Haotian Li", "Jianbo Jiao"], "title": "An Exploratory Study on Abstract Images and Visual Representations Learned from Them", "comment": "Accepted to BMVC 2025", "summary": "Imagine living in a world composed solely of primitive shapes, could you\nstill recognise familiar objects? Recent studies have shown that abstract\nimages-constructed by primitive shapes-can indeed convey visual semantic\ninformation to deep learning models. However, representations obtained from\nsuch images often fall short compared to those derived from traditional raster\nimages. In this paper, we study the reasons behind this performance gap and\ninvestigate how much high-level semantic content can be captured at different\nabstraction levels. To this end, we introduce the Hierarchical Abstraction\nImage Dataset (HAID), a novel data collection that comprises abstract images\ngenerated from normal raster images at multiple levels of abstraction. We then\ntrain and evaluate conventional vision systems on HAID across various tasks\nincluding classification, segmentation, and object detection, providing a\ncomprehensive study between rasterised and abstract image representations. We\nalso discuss if the abstract image can be considered as a potentially effective\nformat for conveying visual semantic information and contributing to vision\ntasks.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u62bd\u8c61\u56fe\u50cf\u4e0e\u4f20\u7edf\u6805\u683c\u56fe\u50cf\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5f15\u5165HAID\u6570\u636e\u96c6\u8fdb\u884c\u591a\u5c42\u6b21\u62bd\u8c61\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u62bd\u8c61\u56fe\u50cf\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u62bd\u8c61\u56fe\u50cf\u4e0e\u4f20\u7edf\u6805\u683c\u56fe\u50cf\u5728\u6027\u80fd\u4e0a\u7684\u5dee\u8ddd\u539f\u56e0\uff0c\u63a2\u8ba8\u4e0d\u540c\u62bd\u8c61\u7ea7\u522b\u4e0b\u9ad8\u5c42\u6b21\u8bed\u4e49\u5185\u5bb9\u7684\u6355\u6349\u80fd\u529b\uff0c\u5e76\u8bc4\u4f30\u62bd\u8c61\u56fe\u50cf\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728\u6709\u6548\u6027\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u5c42\u6b21\u62bd\u8c61\u56fe\u50cf\u6570\u636e\u96c6\uff08HAID\uff09\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u4ece\u6b63\u5e38\u6805\u683c\u56fe\u50cf\u751f\u6210\u7684\u591a\u4e2a\u62bd\u8c61\u7ea7\u522b\u7684\u62bd\u8c61\u56fe\u50cf\u3002\u968f\u540e\uff0c\u5728HAID\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e86\u4f20\u7edf\u7684\u89c6\u89c9\u7cfb\u7edf\uff0c\u6db5\u76d6\u4e86\u5206\u7c7b\u3001\u5206\u5272\u548c\u5bf9\u8c61\u68c0\u6d4b\u7b49\u591a\u79cd\u4efb\u52a1\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u62bd\u8c61\u56fe\u50cf\u80fd\u591f\u4f20\u9012\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\uff0c\u4f46\u5728\u6027\u80fd\u4e0a\u4ecd\u4e0d\u53ca\u4f20\u7edf\u6805\u683c\u56fe\u50cf\u3002HAID\u6570\u636e\u96c6\u4e3a\u591a\u5c42\u6b21\u62bd\u8c61\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u652f\u6301\u3002", "conclusion": "\u62bd\u8c61\u56fe\u50cf\u867d\u7136\u80fd\u591f\u4f20\u9012\u4e00\u5b9a\u7684\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\uff0c\u4f46\u5728\u6027\u80fd\u4e0a\u4ecd\u4e0d\u53ca\u4f20\u7edf\u6805\u683c\u56fe\u50cf\u3002\u901a\u8fc7HAID\u6570\u636e\u96c6\u7684\u591a\u5c42\u6b21\u62bd\u8c61\u7814\u7a76\uff0c\u672c\u6587\u63a2\u8ba8\u4e86\u4e0d\u540c\u62bd\u8c61\u7ea7\u522b\u4e0b\u9ad8\u5c42\u6b21\u8bed\u4e49\u5185\u5bb9\u7684\u6355\u6349\u80fd\u529b\uff0c\u5e76\u8bc4\u4f30\u4e86\u62bd\u8c61\u56fe\u50cf\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728\u6709\u6548\u6027\u3002"}}
{"id": "2509.14151", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14151", "abs": "https://arxiv.org/abs/2509.14151", "authors": ["Rongyu Zhang", "Jiaming Liu", "Xiaoqi Li", "Xiaowei Chi", "Dan Wang", "Li Du", "Yuan Du", "Shanghang Zhang"], "title": "BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection", "comment": "Accepted by IEEE TCSVT", "summary": "Vision-centric Bird's Eye View (BEV) perception holds considerable promise\nfor autonomous driving. Recent studies have prioritized efficiency or accuracy\nenhancements, yet the issue of domain shift has been overlooked, leading to\nsubstantial performance degradation upon transfer. We identify major domain\ngaps in real-world cross-domain scenarios and initiate the first effort to\naddress the Domain Adaptation (DA) challenge in multi-view 3D object detection\nfor BEV perception. Given the complexity of BEV perception approaches with\ntheir multiple components, domain shift accumulation across multi-geometric\nspaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain\nadaptation. In this paper, we introduce an innovative geometric-aware\nteacher-student framework, BEVUDA++, to diminish this issue, comprising a\nReliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.\nSpecifically, RDT effectively blends target LiDAR with dependable depth\npredictions to generate depth-aware information based on uncertainty\nestimation, enhancing the extraction of Voxel and BEV features that are\nessential for understanding the target domain. To collaboratively reduce the\ndomain shift, GCS maps features from multiple spaces into a unified geometric\nembedding space, thereby narrowing the gap in data distribution between the two\ndomains. Additionally, we introduce a novel Uncertainty-guided Exponential\nMoving Average (UEMA) to further reduce error accumulation due to domain shifts\ninformed by previously obtained uncertainty guidance. To demonstrate the\nsuperiority of our proposed method, we execute comprehensive experiments in\nfour cross-domain scenarios, securing state-of-the-art performance in BEV 3D\nobject detection tasks, e.g., 12.9\\% NDS and 9.5\\% mAP enhancement on Day-Night\nadaptation.", "AI": {"tldr": "BEVUDA++\u6846\u67b6\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u5e08\u751f\u6a21\u578b\u548cUEMA\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86BEV\u611f\u77e5\u7684\u57df\u9002\u5e94\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df3D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "BEV\u611f\u77e5\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u57df\u9002\u5e94\u95ee\u9898\uff0c\u5bfc\u81f4\u8de8\u57df\u573a\u666f\u4e0b\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u9996\u6b21\u9488\u5bf9\u591a\u89c6\u89d23D\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684BEV\u57df\u9002\u5e94\u95ee\u9898\u5c55\u5f00\u7814\u7a76\u3002", "method": "\u5f15\u5165\u51e0\u4f55\u611f\u77e5\u7684\u5e08\u751f\u6846\u67b6BEVUDA++\uff0c\u5305\u62ec\u53ef\u9760\u7684\u6df1\u5ea6\u6559\u5e08\uff08RDT\uff09\u548c\u51e0\u4f55\u4e00\u81f4\u5b66\u751f\uff08GCS\uff09\u6a21\u578b\uff0c\u4ee5\u53ca\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08UEMA\uff09\u65b9\u6cd5\u3002", "result": "\u5728\u56db\u4e2a\u8de8\u57df\u573a\u666f\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBEVUDA++\u5728BEV 3D\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f8b\u5982\u5728\u663c\u591c\u9002\u5e94\u4efb\u52a1\u4e2dNDS\u63d0\u534712.9%\uff0cmAP\u63d0\u53479.5%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684BEVUDA++\u6846\u67b6\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u5e08\u751f\u6a21\u578b\u548c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86BEV\u611f\u77e5\u4e2d\u7684\u57df\u9002\u5e94\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u573a\u666f\u4e0b\u76843D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.14227", "categories": ["cs.CV", "I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.14227", "abs": "https://arxiv.org/abs/2509.14227", "authors": ["Nisarg A. Shah", "Amir Ziai", "Chaitanya Ekanadham", "Vishal M. Patel"], "title": "Cin\u00e9aste: A Fine-grained Contextual Movie Question Answering Benchmark", "comment": "11 pages, 5 figures, 5 tables", "summary": "While recent advancements in vision-language models have improved video\nunderstanding, diagnosing their capacity for deep, narrative comprehension\nremains a challenge. Existing benchmarks often test short-clip recognition or\nuse template-based questions, leaving a critical gap in evaluating fine-grained\nreasoning over long-form narrative content. To address these gaps, we introduce\n$\\mathsf{Cin\\acute{e}aste}$, a comprehensive benchmark for long-form movie\nunderstanding. Our dataset comprises 3,119 multiple-choice question-answer\npairs derived from 1,805 scenes across 200 diverse movies, spanning five novel\nfine-grained contextual reasoning categories. We use GPT-4o to generate\ndiverse, context-rich questions by integrating visual descriptions, captions,\nscene titles, and summaries, which require deep narrative understanding. To\nensure high-quality evaluation, our pipeline incorporates a two-stage filtering\nprocess: Context-Independence filtering ensures questions require video\ncontext, while Contextual Veracity filtering validates factual consistency\nagainst the movie content, mitigating hallucinations. Experiments show that\nexisting MLLMs struggle on $\\mathsf{Cin\\acute{e}aste}$; our analysis reveals\nthat long-range temporal reasoning is a primary bottleneck, with the top\nopen-source model achieving only 63.15\\% accuracy. This underscores significant\nchallenges in fine-grained contextual understanding and the need for\nadvancements in long-form movie comprehension.", "AI": {"conclusion": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5f62\u5f0f\u7535\u5f71\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u957f\u7a0b\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u3002", "method": "\u5f15\u5165\u4e86$\\mathsf{Cin\\acute{e}aste}$\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b3,119\u4e2a\u591a\u9009\u9898\u5bf9\uff0c\u6e90\u81ea200\u90e8\u7535\u5f71\u76841,805\u4e2a\u573a\u666f\uff0c\u8986\u76d6\u4e94\u4e2a\u65b0\u9896\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\u7c7b\u522b\u3002\u4f7f\u7528GPT-4o\u751f\u6210\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u6ee4\u786e\u4fdd\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u957f\u5f62\u5f0f\u53d9\u4e8b\u5185\u5bb9\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728$\\mathsf{Cin\\acute{e}aste}$\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u4f73\u5f00\u6e90\u6a21\u578b\u51c6\u786e\u7387\u4ec5\u4e3a63.15%\u3002", "tldr": "$\\mathsf{Cin\\acute{e}aste}$\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u5f62\u5f0f\u7535\u5f71\u7406\u89e3\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u4e86\u957f\u7a0b\u65f6\u95f4\u63a8\u7406\u7684\u6311\u6218\u3002"}}
{"id": "2509.14232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14232", "abs": "https://arxiv.org/abs/2509.14232", "authors": ["Zhaokai Wang", "Penghao Yin", "Xiangyu Zhao", "Changyao Tian", "Yu Qiao", "Wenhai Wang", "Jifeng Dai", "Gen Luo"], "title": "GenExam: A Multidisciplinary Text-to-Image Exam", "comment": null, "summary": "Exams are a fundamental test of expert-level intelligence and require\nintegrated understanding, reasoning, and generation. Existing exam-style\nbenchmarks mainly focus on understanding and reasoning tasks, and current\ngeneration benchmarks emphasize the illustration of world knowledge and visual\nconcepts, neglecting the evaluation of rigorous drawing exams. We introduce\nGenExam, the first benchmark for multidisciplinary text-to-image exams,\nfeaturing 1,000 samples across 10 subjects with exam-style prompts organized\nunder a four-level taxonomy. Each problem is equipped with ground-truth images\nand fine-grained scoring points to enable a precise evaluation of semantic\ncorrectness and visual plausibility. Experiments show that even\nstate-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve\nless than 15% strict scores, and most models yield almost 0%, suggesting the\ngreat challenge of our benchmark. By framing image generation as an exam,\nGenExam offers a rigorous assessment of models' ability to integrate knowledge,\nreasoning, and generation, providing insights on the path to general AGI.", "AI": {"tldr": "GenExam \u662f\u9996\u4e2a\u591a\u5b66\u79d1\u6587\u672c\u5230\u56fe\u50cf\u8003\u8bd5\u57fa\u51c6\uff0c\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u4e25\u683c\u7ed8\u56fe\u8003\u8bd5\u4e2d\u8868\u73b0\u6781\u5dee\uff0c\u7a81\u663e\u4e86\u77e5\u8bc6\u6574\u5408\u4e0e\u751f\u6210\u7684\u5de8\u5927\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u8003\u8bd5\u98ce\u683c\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\uff0c\u800c\u751f\u6210\u57fa\u51c6\u4fa7\u91cd\u4e8e\u4e16\u754c\u77e5\u8bc6\u548c\u89c6\u89c9\u6982\u5ff5\u7684\u5c55\u793a\uff0c\u7f3a\u4e4f\u5bf9\u4e25\u683c\u7ed8\u56fe\u8003\u8bd5\u7684\u8bc4\u4f30\u3002", "method": "\u5f15\u5165 GenExam \u57fa\u51c6\uff0c\u5305\u542b 10 \u4e2a\u5b66\u79d1\u7684 1,000 \u4e2a\u6837\u672c\uff0c\u91c7\u7528\u56db\u5c42\u7ea7\u5206\u7c7b\u7684\u8003\u8bd5\u98ce\u683c\u63d0\u793a\uff0c\u6bcf\u4e2a\u95ee\u9898\u914d\u5907\u771f\u5b9e\u56fe\u50cf\u548c\u7ec6\u7c92\u5ea6\u8bc4\u5206\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f GPT-Image-1 \u548c Gemini-2.5-Flash-Image \u7b49\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u4e25\u683c\u5f97\u5206\u4e5f\u4f4e\u4e8e 15%\uff0c\u5927\u591a\u6570\u6a21\u578b\u5f97\u5206\u63a5\u8fd1 0%\u3002", "conclusion": "GenExam \u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u591a\u5b66\u79d1\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8003\u8bd5\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u5728\u77e5\u8bc6\u6574\u5408\u3001\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u4e3a\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
