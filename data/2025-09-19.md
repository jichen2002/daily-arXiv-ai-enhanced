<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.RO](#cs.RO) [Total: 48]
- [cs.SE](#cs.SE) [Total: 21]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.NI](#cs.NI) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Class-invariant Test-Time Augmentation for Domain Generalization](https://arxiv.org/abs/2509.14420)
*Zhicheng Lin,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

TL;DR: 提出轻量级测试时增强技术CI-TTA，通过类不变变形和置信度过滤提升模型在未见域上的泛化性能，实验验证了其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 深度模型在分布偏移下性能显著下降，现有方法多依赖多域训练或计算密集的测试时适应，因此需要一种轻量级的补充策略。

Method: 提出了一种新颖的类不变测试时增强技术（CI-TTA），通过弹性变形和网格变形生成输入图像的多个变体，并通过置信度引导的过滤方案聚合预测结果。

Result: 在PACS和Office-Home数据集上的广泛实验表明，该方法在不同域泛化算法和主干网络上均取得了稳定的性能提升。

Conclusion: CI-TTA技术通过轻量级的测试时增强，显著提升了模型在未见域上的泛化能力，且与多种域泛化算法和主干网络兼容。

Abstract: Deep models often suffer significant performance degradation under
distribution shifts. Domain generalization (DG) seeks to mitigate this
challenge by enabling models to generalize to unseen domains. Most prior
approaches rely on multi-domain training or computationally intensive test-time
adaptation. In contrast, we propose a complementary strategy: lightweight
test-time augmentation. Specifically, we develop a novel Class-Invariant
Test-Time Augmentation (CI-TTA) technique. The idea is to generate multiple
variants of each input image through elastic and grid deformations that
nevertheless belong to the same class as the original input. Their predictions
are aggregated through a confidence-guided filtering scheme that remove
unreliable outputs, ensuring the final decision relies on consistent and
trustworthy cues. Extensive Experiments on PACS and Office-Home datasets
demonstrate consistent gains across different DG algorithms and backbones,
highlighting the effectiveness and generality of our approach.

</details>


### [2] [AToken: A Unified Tokenizer for Vision](https://arxiv.org/abs/2509.14476)
*Jiasen Lu,Liangchen Song,Mingze Xu,Byeongjoo Ahn,Yanjun Wang,Chen Chen,Afshin Dehghan,Yinfei Yang*

Main category: cs.CV

TL;DR: AToken是首个统一的视觉标记器，通过Transformer架构和渐进训练，在多模态任务中实现了高保真重建和语义理解。


<details>
  <summary>Details</summary>
Motivation: 现有标记器仅针对单一模态或任务，缺乏统一处理多模态和高保真重建与语义理解的框架。

Method: 采用纯Transformer架构和4D旋转位置嵌入技术，结合感知和Gram矩阵损失的对抗自由训练目标，以及渐进式训练课程。

Result: AToken在图像、视频和3D资产上均取得了最先进的性能，如0.21 rFID和82.2% ImageNet准确率。

Conclusion: AToken作为一种统一的视觉标记器，在图像、视频和3D资产上实现了高保真重建和语义理解，为下一代多模态AI系统奠定了基础。

Abstract: We present AToken, the first unified visual tokenizer that achieves both
high-fidelity reconstruction and semantic understanding across images, videos,
and 3D assets. Unlike existing tokenizers that specialize in either
reconstruction or understanding for single modalities, AToken encodes these
diverse visual inputs into a shared 4D latent space, unifying both tasks and
modalities in a single framework. Specifically, we introduce a pure transformer
architecture with 4D rotary position embeddings to process visual inputs of
arbitrary resolutions and temporal durations. To ensure stable training, we
introduce an adversarial-free training objective that combines perceptual and
Gram matrix losses, achieving state-of-the-art reconstruction quality. By
employing a progressive training curriculum, AToken gradually expands from
single images, videos, and 3D, and supports both continuous and discrete latent
tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01
rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9%
classification accuracy for 3D. In downstream applications, AToken enables both
visual generation tasks (e.g., image generation with continuous and discrete
tokens, text-to-video generation, image-to-3D synthesis) and understanding
tasks (e.g., multimodal LLMs), achieving competitive performance across all
benchmarks. These results shed light on the next-generation multimodal AI
systems built upon unified visual tokenization.

</details>


### [3] [MemEvo: Memory-Evolving Incremental Multi-view Clustering](https://arxiv.org/abs/2509.14544)
*Zisen Kong,Bo Zhong,Pengyuan Li,Dongxia Chang,Yiming Wang*

Main category: cs.CV

TL;DR: MemEvo是一种增量多视图聚类方法，通过模拟人脑记忆机制解决稳定性-可塑性困境，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决增量多视图聚类中的稳定性-可塑性困境（SPD），即模型需要快速适应新数据的同时保持足够的稳定性以巩固长期知识。

Method: 提出了一种海马体启发的视图对齐模块来捕捉新视图的增益信息，引入认知遗忘机制模拟人类记忆的衰减模式，以及设计前额叶皮质启发的知识巩固模块利用时间张量稳定性逐步巩固历史知识。

Result: MemEvo在视图数量增长的情况下表现出卓越的知识保留能力，并在实验中优于现有最先进方法。

Conclusion: MemEvo通过整合海马体启发的视图对齐模块、认知遗忘机制和前额叶皮质启发的知识巩固模块，成功解决了增量多视图聚类中的稳定性-可塑性困境（SPD），并在视图数量增长的情况下表现出卓越的知识保留能力。

Abstract: Incremental multi-view clustering aims to achieve stable clustering results
while addressing the stability-plasticity dilemma (SPD) in incremental views.
At the core of SPD is the challenge that the model must have enough plasticity
to quickly adapt to new data, while maintaining sufficient stability to
consolidate long-term knowledge and prevent catastrophic forgetting. Inspired
by the hippocampal-prefrontal cortex collaborative memory mechanism in
neuroscience, we propose a Memory-Evolving Incremental Multi-view Clustering
method (MemEvo) to achieve this balance. First, we propose a
hippocampus-inspired view alignment module that captures the gain information
of new views by aligning structures in continuous representations. Second, we
introduce a cognitive forgetting mechanism that simulates the decay patterns of
human memory to modulate the weights of historical knowledge. Additionally, we
design a prefrontal cortex-inspired knowledge consolidation memory module that
leverages temporal tensor stability to gradually consolidate historical
knowledge. By integrating these modules, MemEvo achieves strong knowledge
retention capabilities in scenarios with a growing number of views. Extensive
experiments demonstrate that MemEvo exhibits remarkable advantages over
existing state-of-the-art methods.

</details>


### [4] [Edge-Aware Normalized Attention for Efficient and Detail-Preserving Single Image Super-Resolution](https://arxiv.org/abs/2509.14550)
*Penghao Rao,Tieyong Zeng*

Main category: cs.CV

TL;DR: 提出边缘引导注意机制和轻量级残差设计，通过复合目标训练，显著提升单图像超分辨率的结构清晰度和感知质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有边缘感知方法中存在的冗余、优化不稳定或结构增益有限的问题，通过更高效的方式注入边缘先验，提升超分辨率的结构保真度。

Method: 提出了一种边缘引导注意机制，通过联合编码边缘特征和中间特征激活来生成自适应调制图，并应用于响应归一化和重加权。该方法集成到轻量级残差设计中，并采用结合像素级、感知和对抗性损失的复合目标进行训练。

Result: 在标准SISR基准测试中，相比SRGAN、ESRGAN和其他边缘注意基线方法，所提方法在模型复杂度相当的情况下，显著提升了结构清晰度和感知质量。

Conclusion: 该论文提出了一种基于边缘引导注意机制的轻量级残差设计，通过复合目标训练，在保持模型复杂度的同时，显著提升了单图像超分辨率（SISR）的结构清晰度和感知质量。

Abstract: Single-image super-resolution (SISR) remains highly ill-posed because
recovering structurally faithful high-frequency content from a single
low-resolution observation is ambiguous. Existing edge-aware methods often
attach edge priors or attention branches onto increasingly complex backbones,
yet ad hoc fusion frequently introduces redundancy, unstable optimization, or
limited structural gains. We address this gap with an edge-guided attention
mechanism that derives an adaptive modulation map from jointly encoded edge
features and intermediate feature activations, then applies it to normalize and
reweight responses, selectively amplifying structurally salient regions while
suppressing spurious textures. In parallel, we integrate this mechanism into a
lightweight residual design trained under a composite objective combining
pixel-wise, perceptual, and adversarial terms to balance fidelity, perceptual
realism, and training stability. Extensive experiments on standard SISR
benchmarks demonstrate consistent improvements in structural sharpness and
perceptual quality over SRGAN, ESRGAN, and prior edge-attention baselines at
comparable model complexity. The proposed formulation provides (i) a
parameter-efficient path to inject edge priors, (ii) stabilized adversarial
refinement through a tailored multiterm loss, and (iii) enhanced edge fidelity
without resorting to deeper or heavily overparameterized architectures. These
results highlight the effectiveness of principled edge-conditioned modulation
for advancing perceptual super-resolution.

</details>


### [5] [Adaptive and Iterative Point Cloud Denoising with Score-Based Diffusion Model](https://arxiv.org/abs/2509.14560)
*Zhaonan Wang,Manyi Li,ShiQing Xin,Changhe Tu*

Main category: cs.CV

TL;DR: 本文提出了一种基于分数扩散模型的自适应迭代点云去噪方法，通过噪声估计和自适应调度优化去噪过程，在合成和真实数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在迭代去噪过程中难以高效处理不同级别或模式的噪声，缺乏明确的自适应调度策略。

Method: 基于分数扩散模型的自适应迭代点云去噪方法，包括噪声变化估计、自适应去噪调度设计、网络架构和两阶段采样策略。

Result: 所提方法在定性和定量上均优于现有最先进方法，适用于不同噪声模式的合成数据集和真实扫描数据集。

Conclusion: 本文提出的自适应迭代点云去噪方法在合成和真实扫描数据集上均表现出色，不仅去噪效果好，还能更好地保留形状边界和细节。

Abstract: Point cloud denoising task aims to recover the clean point cloud from the
scanned data coupled with different levels or patterns of noise. The recent
state-of-the-art methods often train deep neural networks to update the point
locations towards the clean point cloud, and empirically repeat the denoising
process several times in order to obtain the denoised results. It is not clear
how to efficiently arrange the iterative denoising processes to deal with
different levels or patterns of noise. In this paper, we propose an adaptive
and iterative point cloud denoising method based on the score-based diffusion
model. For a given noisy point cloud, we first estimate the noise variation and
determine an adaptive denoising schedule with appropriate step sizes, then
invoke the trained network iteratively to update point clouds following the
adaptive schedule. To facilitate this adaptive and iterative denoising process,
we design the network architecture and a two-stage sampling strategy for the
network training to enable feature fusion and gradient fusion for iterative
denoising. Compared to the state-of-the-art point cloud denoising methods, our
approach obtains clean and smooth denoised point clouds, while preserving the
shape boundary and details better. Our results not only outperform the other
methods both qualitatively and quantitatively, but also are preferable on the
synthetic dataset with different patterns of noises, as well as the
real-scanned dataset.

</details>


### [6] [DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising](https://arxiv.org/abs/2509.14565)
*Li Gao,Hongyang Sun,Liu Liu,Yunhao Li,Yang Cai*

Main category: cs.CV

TL;DR: DiffVL利用扩散模型将视觉定位转化为GPS去噪任务，联合建模GPS、标准地图和视觉信号，实现了无需高精度地图的亚米级定位，超越了传统匹配方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于标准地图的视觉定位方法主要关注鸟瞰图匹配，忽略了普遍存在但噪声较大的GPS信号。尽管GPS在城市场景中易受多路径误差影响，但其可用性为定位提供了潜在优势。

Method: DiffVL框架将视觉定位重新定义为GPS去噪任务，利用扩散模型通过迭代去噪恢复真实姿态分布。该方法联合建模GPS、标准地图和视觉信号，反向学习GPS噪声扰动。

Result: 实验表明，DiffVL在多个数据集上实现了优于现有鸟瞰图匹配基线的定位精度，达到了亚米级精度。

Conclusion: DiffVL通过将视觉定位重新定义为GPS去噪任务，利用扩散模型实现了无需依赖高精度地图的亚米级定位精度，为可扩展的定位方法提供了新范式。

Abstract: Accurate visual localization is crucial for autonomous driving, yet existing
methods face a fundamental dilemma: While high-definition (HD) maps provide
high-precision localization references, their costly construction and
maintenance hinder scalability, which drives research toward
standard-definition (SD) maps like OpenStreetMap. Current SD-map-based
approaches primarily focus on Bird's-Eye View (BEV) matching between images and
maps, overlooking a ubiquitous signal-noisy GPS. Although GPS is readily
available, it suffers from multipath errors in urban environments. We propose
DiffVL, the first framework to reformulate visual localization as a GPS
denoising task using diffusion models. Our key insight is that noisy GPS
trajectory, when conditioned on visual BEV features and SD maps, implicitly
encode the true pose distribution, which can be recovered through iterative
diffusion refinement. DiffVL, unlike prior BEV-matching methods (e.g.,
OrienterNet) or transformer-based registration approaches, learns to reverse
GPS noise perturbations by jointly modeling GPS, SD map, and visual signals,
achieving sub-meter accuracy without relying on HD maps. Experiments on
multiple datasets demonstrate that our method achieves state-of-the-art
accuracy compared to BEV-matching baselines. Crucially, our work proves that
diffusion models can enable scalable localization by treating noisy GPS as a
generative prior-making a paradigm shift from traditional matching-based
methods.

</details>


### [7] [DICE: Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction](https://arxiv.org/abs/2509.14566)
*Leon Suarez-Rodriguez,Roman Jacome,Romario Gualdron-Hurtado,Ana Mantilla-Dulcey,Henry Arguello*

Main category: cs.CV

TL;DR: DICE框架结合扩散模型和数据一致性代理，有效提升稀疏视图CT重建质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图CT重建因欠采样导致病态逆问题，传统方法难以捕捉医学图像中的复杂结构。扩散模型作为生成先验能更准确建模复杂图像分布。

Method: 提出Diffusion Consensus Equilibrium (DICE)框架，交替使用数据一致性代理（近端算子）和生成先验代理（扩散模型）进行迭代优化。

Result: DICE在15、30和60视图的均匀和非均匀稀疏视图设置下，显著优于现有基线方法，重建出高质量CT图像。

Conclusion: DICE框架通过整合数据一致性和生成先验代理，显著提高了稀疏视图CT重建的质量和鲁棒性。

Abstract: Sparse-view computed tomography (CT) reconstruction is fundamentally
challenging due to undersampling, leading to an ill-posed inverse problem.
Traditional iterative methods incorporate handcrafted or learned priors to
regularize the solution but struggle to capture the complex structures present
in medical images. In contrast, diffusion models (DMs) have recently emerged as
powerful generative priors that can accurately model complex image
distributions. In this work, we introduce Diffusion Consensus Equilibrium
(DICE), a framework that integrates a two-agent consensus equilibrium into the
sampling process of a DM. DICE alternates between: (i) a data-consistency
agent, implemented through a proximal operator enforcing measurement
consistency, and (ii) a prior agent, realized by a DM performing a clean image
estimation at each sampling step. By balancing these two complementary agents
iteratively, DICE effectively combines strong generative prior capabilities
with measurement consistency. Experimental results show that DICE significantly
outperforms state-of-the-art baselines in reconstructing high-quality CT images
under uniform and non-uniform sparse-view settings of 15, 30, and 60 views (out
of a total of 180), demonstrating both its effectiveness and robustness.

</details>


### [8] [Domain Adaptation for Ulcerative Colitis Severity Estimation Using Patient-Level Diagnoses](https://arxiv.org/abs/2509.14573)
*Takamasa Yamaguchi,Brian Kenji Iwana,Ryoma Bise,Shota Harada,Takumi Okuo,Kiyohito Tanaka,Kaito Shiku*

Main category: cs.CV

TL;DR: 本文提出一种弱监督域适应方法，利用患者级别诊断结果解决UC严重性估计中的域偏移问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有UC严重性估计方法因成像设备和临床设置的差异导致域偏移问题，且目标域缺乏监督或标注成本高。

Method: 提出了一种基于共享聚合令牌和最大严重性三元组损失的弱监督域适应方法，利用患者级别诊断结果作为弱监督，对齐跨域的类别分布。

Result: 实验证明，该方法在域偏移设置下提高了UC严重性估计的准确性，优于现有域适应方法。

Conclusion: 本文提出了一种新颖的弱监督域适应方法，通过利用患者级别的诊断结果作为弱监督，有效解决了溃疡性结肠炎（UC）严重性估计中的域偏移问题。实验结果表明，该方法在域偏移设置下优于现有域适应方法。

Abstract: The development of methods to estimate the severity of Ulcerative Colitis
(UC) is of significant importance. However, these methods often suffer from
domain shifts caused by differences in imaging devices and clinical settings
across hospitals. Although several domain adaptation methods have been proposed
to address domain shift, they still struggle with the lack of supervision in
the target domain or the high cost of annotation. To overcome these challenges,
we propose a novel Weakly Supervised Domain Adaptation method that leverages
patient-level diagnostic results, which are routinely recorded in UC diagnosis,
as weak supervision in the target domain. The proposed method aligns class-wise
distributions across domains using Shared Aggregation Tokens and a Max-Severity
Triplet Loss, which leverages the characteristic that patient-level diagnoses
are determined by the most severe region within each patient. Experimental
results demonstrate that our method outperforms comparative DA approaches,
improving UC severity estimation in a domain-shifted setting.

</details>


### [9] [Do Vision-Language Models See Urban Scenes as People Do? An Urban Perception Benchmark](https://arxiv.org/abs/2509.14574)
*Rashid Mushkani*

Main category: cs.CV

TL;DR: 研究构建了一个城市感知基准测试，评估VLM在客观与主观维度上的表现，发现模型在客观属性上更优，并发布了工具支持参与式分析。


<details>
  <summary>Details</summary>
Motivation: 理解人们如何阅读城市场景可为设计和规划提供信息。

Method: 使用100张蒙特利尔街道图像（照片与合成场景各半），12名参与者提供230份标注表格，评估7个视觉语言模型（VLM）的零样本性能。采用准确率（单选项）和Jaccard重叠（多标签）作为指标，人类一致性使用Krippendorff's alpha和成对Jaccard。

Result: 模型在可见、客观属性上表现优于主观评价。最佳系统（claude-sonnet）在多标签项目上达到宏平均0.31和平均Jaccard 0.48。人类一致性较高的维度模型得分也更高，合成图像略微降低分数。

Conclusion: 研究发布了一个用于参与式城市分析的基准测试、提示和工具，支持可重复且不确定性感知的评估。

Abstract: Understanding how people read city scenes can inform design and planning. We
introduce a small benchmark for testing vision-language models (VLMs) on urban
perception using 100 Montreal street images, evenly split between photographs
and photorealistic synthetic scenes. Twelve participants from seven community
groups supplied 230 annotation forms across 30 dimensions mixing physical
attributes and subjective impressions. French responses were normalized to
English. We evaluated seven VLMs in a zero-shot setup with a structured prompt
and deterministic parser. We use accuracy for single-choice items and Jaccard
overlap for multi-label items; human agreement uses Krippendorff's alpha and
pairwise Jaccard. Results suggest stronger model alignment on visible,
objective properties than subjective appraisals. The top system (claude-sonnet)
reaches macro 0.31 and mean Jaccard 0.48 on multi-label items. Higher human
agreement coincides with better model scores. Synthetic images slightly lower
scores. We release the benchmark, prompts, and harness for reproducible,
uncertainty-aware evaluation in participatory urban analysis.

</details>


### [10] [Feature-aligned Motion Transformation for Efficient Dynamic Point Cloud Compression](https://arxiv.org/abs/2509.14591)
*Xuan Deng,Xiandong Meng,Longguang Wang,Tiange Zhang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: FMT框架通过隐式时间建模和双向参考策略，显著提升动态点云压缩效率，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动态点云的压缩效率高度依赖运动估计，但现有方法因显式运动向量的局限性难以捕捉复杂动态，且未充分利用时间相关性。

Method: 引入特征对齐运动变换（FMT）框架，采用时空对齐策略隐式建模时间变化，并设计随机访问（RA）参考策略支持双向运动参考和分层编码。

Result: 实验表明，FMT在编码和解码效率上优于D-DPCC和AdaDPCC，BD-Rate分别降低20%和9.4%。

Conclusion: FMT框架通过隐式建模连续时间变化和双向运动参考策略，显著提升了动态点云压缩的效率和性能，实验证明其优于现有方法。

Abstract: Dynamic point clouds are widely used in applications such as immersive
reality, robotics, and autonomous driving. Efficient compression largely
depends on accurate motion estimation and compensation, yet the irregular
structure and significant local variations of point clouds make this task
highly challenging. Current methods often rely on explicit motion estimation,
whose encoded vectors struggle to capture intricate dynamics and fail to fully
exploit temporal correlations. To overcome these limitations, we introduce a
Feature-aligned Motion Transformation (FMT) framework for dynamic point cloud
compression. FMT replaces explicit motion vectors with a spatiotemporal
alignment strategy that implicitly models continuous temporal variations, using
aligned features as temporal context within a latent-space conditional encoding
framework. Furthermore, we design a random access (RA) reference strategy that
enables bidirectional motion referencing and layered encoding, thereby
supporting frame-level parallel compression. Extensive experiments demonstrate
that our method surpasses D-DPCC and AdaDPCC in both encoding and decoding
efficiency, while also achieving BD-Rate reductions of 20% and 9.4%,
respectively. These results highlight the effectiveness of FMT in jointly
improving compression efficiency and processing performance.

</details>


### [11] [HybridMamba: A Dual-domain Mamba for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.14609)
*Weitong Wu,Zhaohu Xing,Jing Gong,Qin Peng,Lei Zhu*

Main category: cs.CV

TL;DR: HybridMamba通过双互补机制平衡全局与局部信息，在3D医学图像分割中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（如CNN和Transformer）在长程依赖建模和计算效率上的不足，同时避免过度关注全局上下文而忽略局部结构信息。

Method: 1) 采用轴向遍历和局部自适应路径的特征扫描策略；2) 结合空间-频率分析的门控模块。

Result: 在MRI和CT数据集上的实验表明，HybridMamba显著优于现有最先进方法。

Conclusion: HybridMamba通过双互补机制显著提升了3D医学图像分割的性能，解决了全局与局部信息平衡的问题，并在多中心数据集上验证了其优越性。

Abstract: In the domain of 3D biomedical image segmentation, Mamba exhibits the
superior performance for it addresses the limitations in modeling long-range
dependencies inherent to CNNs and mitigates the abundant computational overhead
associated with Transformer-based frameworks when processing high-resolution
medical volumes. However, attaching undue importance to global context modeling
may inadvertently compromise critical local structural information, thus
leading to boundary ambiguity and regional distortion in segmentation outputs.
Therefore, we propose the HybridMamba, an architecture employing dual
complementary mechanisms: 1) a feature scanning strategy that progressively
integrates representations both axial-traversal and local-adaptive pathways to
harmonize the relationship between local and global representations, and 2) a
gated module combining spatial-frequency analysis for comprehensive contextual
modeling. Besides, we collect a multi-center CT dataset related to lung cancer.
Experiments on MRI and CT datasets demonstrate that HybridMamba significantly
outperforms the state-of-the-art methods in 3D medical image segmentation.

</details>


### [12] [Enhancing Feature Fusion of U-like Networks with Dynamic Skip Connections](https://arxiv.org/abs/2509.14610)
*Yue Cao,Quansong He,Kaishen Wang,Jianlong Xiong,Tao He*

Main category: cs.CV

TL;DR: 提出动态跳过连接（DSC）模块，通过TTT和DMSK解决传统U型网络的跳过连接限制，实验验证其广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 传统跳过连接存在特征间和特征内约束，限制了多尺度特征交互和全局上下文信息聚合。

Method: DSC模块包含两个互补组件：测试时间训练（TTT）模块解决特征间约束，动态多尺度核（DMSK）模块解决特征内约束。

Result: 实验表明DSC模块在CNN、Transformer、混合CNN-Transformer及Mamba-based U型网络中均有效。

Conclusion: 提出的动态跳过连接（DSC）模块通过自适应机制显著提升了U型网络的跨层连接能力，验证了其在多种U型网络结构中的即插即用有效性。

Abstract: U-like networks have become fundamental frameworks in medical image
segmentation through skip connections that bridge high-level semantics and
low-level spatial details. Despite their success, conventional skip connections
exhibit two key limitations: inter-feature constraints and intra-feature
constraints. The inter-feature constraint refers to the static nature of
feature fusion in traditional skip connections, where information is
transmitted along fixed pathways regardless of feature content. The
intra-feature constraint arises from the insufficient modeling of multi-scale
feature interactions, thereby hindering the effective aggregation of global
contextual information. To overcome these limitations, we propose a novel
Dynamic Skip Connection (DSC) block that fundamentally enhances cross-layer
connectivity through adaptive mechanisms. The DSC block integrates two
complementary components. (1) Test-Time Training (TTT) module. This module
addresses the inter-feature constraint by enabling dynamic adaptation of hidden
representations during inference, facilitating content-aware feature
refinement. (2) Dynamic Multi-Scale Kernel (DMSK) module. To mitigate the
intra-feature constraint, this module adaptively selects kernel sizes based on
global contextual cues, enhancing the network capacity for multi-scale feature
integration. The DSC block is architecture-agnostic and can be seamlessly
incorporated into existing U-like network structures. Extensive experiments
demonstrate the plug-and-play effectiveness of the proposed DSC block across
CNN-based, Transformer-based, hybrid CNN-Transformer, and Mamba-based U-like
networks.

</details>


### [13] [LSTC-MDA: A Unified Framework for Long-Short Term Temporal Convolution and Mixed Data Augmentation in Skeleton-Based Action Recognition](https://arxiv.org/abs/2509.14619)
*Feng Ding,Haisheng Fu,Soroush Oraki,Jie Liang*

Main category: cs.CV

TL;DR: 提出LSTC-MDA框架，结合长短期时间卷积和增强数据扩增，显著提升骨架动作识别性能，在多个数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决骨架动作识别中标记训练样本稀缺和短长期时间依赖建模困难的问题。

Method: 提出LSTC模块，通过并行短长期分支和自适应融合权重，改进时间建模；扩展JMDA方法，通过输入级Additive Mixup增强数据多样性，同时限制混合操作在同一摄像机视角以避免分布偏移。

Result: 在多个数据集上达到最佳性能：NTU 60（X-Sub 94.1%，X-View 97.5%）、NTU 120（X-Sub 90.4%，X-Set 92.0%）、NW-UCLA 97.2%。

Conclusion: LSTC-MDA框架通过结合长短期时间卷积模块和增强的数据扩增方法，显著提升了骨架动作识别的性能，并在多个数据集上取得了最先进的结果。

Abstract: Skeleton-based action recognition faces two longstanding challenges: the
scarcity of labeled training samples and difficulty modeling short- and
long-range temporal dependencies. To address these issues, we propose a unified
framework, LSTC-MDA, which simultaneously improves temporal modeling and data
diversity. We introduce a novel Long-Short Term Temporal Convolution (LSTC)
module with parallel short- and long-term branches, these two feature branches
are then aligned and fused adaptively using learned similarity weights to
preserve critical long-range cues lost by conventional stride-2 temporal
convolutions. We also extend Joint Mixing Data Augmentation (JMDA) with an
Additive Mixup at the input level, diversifying training samples and
restricting mixup operations to the same camera view to avoid distribution
shifts. Ablation studies confirm each component contributes. LSTC-MDA achieves
state-of-the-art results: 94.1% and 97.5% on NTU 60 (X-Sub and X-View), 90.4%
and 92.0% on NTU 120 (X-Sub and X-Set),97.2% on NW-UCLA. Code:
https://github.com/xiaobaoxia/LSTC-MDA.

</details>


### [14] [MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks](https://arxiv.org/abs/2509.14638)
*Mingsong Li,Lin Liu,Hongjun Wang,Haoxing Chen,Xijun Gu,Shizhan Liu,Dong Gong,Junbo Zhao,Zhenzhong Lan,Jianguo Li*

Main category: cs.CV

TL;DR: MultiEdit是一个包含107K高质量图像编辑样本的综合数据集，涵盖6种复杂编辑任务，通过新颖的MLLM流程构建，显著提升了模型在复杂编辑任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前指令式图像编辑方法在复杂编辑任务上表现不佳，现有数据集的编辑类型和样本数量有限，且传统数据集构建存在噪声图像-标题对，可能引入偏见并限制模型能力。

Method: 采用了一种新颖的数据集构建流程，利用两个多模态大型语言模型（MLLMs）生成视觉适应性编辑指令并生成高保真度编辑图像。

Result: 实验表明，使用MultiEdit-Train集微调基础开源模型显著提升了模型在复杂编辑任务上的性能，同时有效保留了其在标准编辑基准上的能力。

Conclusion: MultiEdit数据集为推进更多样化和具有挑战性的指令式图像编辑研究提供了宝贵资源，显著提升了模型在复杂编辑任务上的性能。

Abstract: Current instruction-based image editing (IBIE) methods struggle with
challenging editing tasks, as both editing types and sample counts of existing
datasets are limited. Moreover, traditional dataset construction often contains
noisy image-caption pairs, which may introduce biases and limit model
capabilities in complex editing scenarios. To address these limitations, we
introduce MultiEdit, a comprehensive dataset featuring over 107K high-quality
image editing samples. It encompasses 6 challenging editing tasks through a
diverse collection of 18 non-style-transfer editing types and 38 style transfer
operations, covering a spectrum from sophisticated style transfer to complex
semantic operations like person reference editing and in-image text editing. We
employ a novel dataset construction pipeline that utilizes two multi-modal
large language models (MLLMs) to generate visual-adaptive editing instructions
and produce high-fidelity edited images, respectively. Extensive experiments
demonstrate that fine-tuning foundational open-source models with our
MultiEdit-Train set substantially improves models' performance on sophisticated
editing tasks in our proposed MultiEdit-Test benchmark, while effectively
preserving their capabilities on the standard editing benchmark. We believe
MultiEdit provides a valuable resource for advancing research into more diverse
and challenging IBIE capabilities. Our dataset is available at
https://huggingface.co/datasets/inclusionAI/MultiEdit.

</details>


### [15] [Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model](https://arxiv.org/abs/2509.14664)
*Shinnosuke Hirano,Yuiga Wada,Tsumugi Iida,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出一种结合ALA和AEA机制的新方法，用于视觉基础模型中的解释生成，显著提升了性能指标。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂模型中缺乏适应性，无法有效生成视觉解释。本研究旨在克服这些限制，提出一种既能生成解释又能部分更新模型参数的方法，以增强可解释性。

Method: 提出了一种新颖的解释生成方法，包括两种新机制：Attention Lattice Adapter（ALA）和Alternating Epoch Architect（AEA）。ALA简化了手动选择层的过程，增强了模型的适应性和可解释性；AEA通过每隔一个周期更新ALA参数，解决了注意力区域过小的问题。

Result: 在CUB-200-2011和ImageNet-S数据集上，该方法在平均IoU、插入得分、删除得分和插入-删除得分上均优于基线方法。

Conclusion: 本研究提出的方法在CUB-200-2011和ImageNet-S数据集上均优于基线方法，特别是在CUB-200-2011数据集上实现了53.2点的平均IoU提升。

Abstract: In this study, we consider the problem of generating visual explanations in
visual foundation models. Numerous methods have been proposed for this purpose;
however, they often cannot be applied to complex models due to their lack of
adaptability. To overcome these limitations, we propose a novel explanation
generation method in visual foundation models that is aimed at both generating
explanations and partially updating model parameters to enhance
interpretability. Our approach introduces two novel mechanisms: Attention
Lattice Adapter (ALA) and Alternating Epoch Architect (AEA). ALA mechanism
simplifies the process by eliminating the need for manual layer selection, thus
enhancing the model's adaptability and interpretability. Moreover, the AEA
mechanism, which updates ALA's parameters every other epoch, effectively
addresses the common issue of overly small attention regions. We evaluated our
method on two benchmark datasets, CUB-200-2011 and ImageNet-S. Our results
showed that our method outperformed the baseline methods in terms of mean
intersection over union (IoU), insertion score, deletion score, and
insertion-deletion score on both the CUB-200-2011 and ImageNet-S datasets.
Notably, our best model achieved a 53.2-point improvement in mean IoU on the
CUB-200-2011 dataset compared with the baselines.

</details>


### [16] [DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images](https://arxiv.org/abs/2509.14685)
*Kazuma Nagata,Naoshi Kaneko*

Main category: cs.CV

TL;DR: DACoN通过结合基础模型和CNN特征，支持多参考图像，显著提升了线稿自动上色的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 减少手绘动画制作的劳动成本，同时解决现有深度学习方法在遮挡、姿态变化和视角变化方面的不足。

Method: DACoN融合了基础模型的低分辨率语义特征和CNN的高分辨率空间特征，实现了细粒度且鲁棒的特征提取，并支持任意数量的参考图像。

Result: 定量和定性评估表明，使用多张参考图像的DACoN在颜色化性能上优于之前的方法。

Conclusion: DACoN框架通过结合基础模型和CNN的空间特征，显著提升了线稿自动上色的性能，尤其在处理遮挡、姿态变化和视角变化方面表现优异。

Abstract: Automatic colorization of line drawings has been widely studied to reduce the
labor cost of hand-drawn anime production. Deep learning approaches, including
image/video generation and feature-based correspondence, have improved accuracy
but struggle with occlusions, pose variations, and viewpoint changes. To
address these challenges, we propose DACoN, a framework that leverages
foundation models to capture part-level semantics, even in line drawings. Our
method fuses low-resolution semantic features from foundation models with
high-resolution spatial features from CNNs for fine-grained yet robust feature
extraction. In contrast to previous methods that rely on the Multiplex
Transformer and support only one or two reference images, DACoN removes this
constraint, allowing any number of references. Quantitative and qualitative
evaluations demonstrate the benefits of using multiple reference images,
achieving superior colorization performance. Our code and model are available
at https://github.com/kzmngt/DACoN.

</details>


### [17] [FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction](https://arxiv.org/abs/2509.14739)
*Jinlong Fan,Bingyu Hu,Xingguang Li,Yuxiang Yang,Jing Zhang*

Main category: cs.CV

TL;DR: FMGS-Avatar通过Mesh-Guided 2D高斯泼溅和基础模型知识蒸馏，提升了单目视频中3D人体化身重建的质量，解决了信息不足和优化冲突问题。


<details>
  <summary>Details</summary>
Motivation: 单目视频中几何信息不足导致高保真可动画人体化身重建困难，现有3D Gaussian Splatting方法因自由形式的3D高斯基元难以保持表面细节。

Method: 提出了FMGS-Avatar方法，结合了Mesh-Guided 2D Gaussian Splatting和基于基础模型的多模态先验知识蒸馏，通过选择性梯度隔离的协调训练策略解决优化目标冲突问题。

Result: 实验评估显示，该方法在重建质量上显著优于现有方法，尤其在几何精度和外观保真度上有明显提升，并能实现新颖视角和姿态下空间和时间一致的渲染。

Conclusion: 通过结合增强表示和协调信息蒸馏，FMGS-Avatar方法显著提升了单目3D人体化身重建的质量，在几何精度和外观保真度上均优于现有方法，并提供了丰富的语义信息。

Abstract: Reconstructing high-fidelity animatable human avatars from monocular videos
remains challenging due to insufficient geometric information in single-view
observations. While recent 3D Gaussian Splatting methods have shown promise,
they struggle with surface detail preservation due to the free-form nature of
3D Gaussian primitives. To address both the representation limitations and
information scarcity, we propose a novel method, \textbf{FMGS-Avatar}, that
integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian
Splatting, where 2D Gaussian primitives are attached directly to template mesh
faces with constrained position, rotation, and movement, enabling superior
surface alignment and geometric detail preservation. Second, we leverage
foundation models trained on large-scale datasets, such as Sapiens, to
complement the limited visual cues from monocular videos. However, when
distilling multi-modal prior knowledge from foundation models, conflicting
optimization objectives can emerge as different modalities exhibit distinct
parameter sensitivities. We address this through a coordinated training
strategy with selective gradient isolation, enabling each loss component to
optimize its relevant parameters without interference. Through this combination
of enhanced representation and coordinated information distillation, our
approach significantly advances 3D monocular human avatar reconstruction.
Experimental evaluation demonstrates superior reconstruction quality compared
to existing methods, with notable gains in geometric accuracy and appearance
fidelity while providing rich semantic information. Additionally, the distilled
prior knowledge within a shared canonical space naturally enables spatially and
temporally consistent rendering under novel views and poses.

</details>


### [18] [Chain-of-Thought Re-ranking for Image Retrieval Tasks](https://arxiv.org/abs/2509.14746)
*Shangrong Wu,Yanghong Zhou,Yang Chen,Feng Zhang,P. Y. Mok*

Main category: cs.CV

TL;DR: CoTRR通过链式思维重排序方法，利用MLLM的多模态推理能力提升图像检索性能，在多个任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用MLLM的多模态推理能力，导致图像检索性能不佳，因此提出CoTRR方法以解决这一问题。

Method: 设计了一种列表式排名提示，使MLLM能够直接参与候选图像的重排序，并结合图像评估提示和查询解构提示，实现全局比较、一致推理和可解释的决策。

Result: 在五个数据集上的实验表明，CoTRR在文本到图像检索、组合图像检索和基于聊天的图像检索任务中均达到了最先进性能。

Conclusion: CoTRR方法通过引入链式思维重排序，成功将MLLM的多模态推理能力应用于图像检索的排名过程，显著提升了性能，并在多个任务中达到了最先进水平。

Abstract: Image retrieval remains a fundamental yet challenging problem in computer
vision. While recent advances in Multimodal Large Language Models (MLLMs) have
demonstrated strong reasoning capabilities, existing methods typically employ
them only for evaluation, without involving them directly in the ranking
process. As a result, their rich multimodal reasoning abilities remain
underutilized, leading to suboptimal performance. In this paper, we propose a
novel Chain-of-Thought Re-Ranking (CoTRR) method to address this issue.
Specifically, we design a listwise ranking prompt that enables MLLM to directly
participate in re-ranking candidate images. This ranking process is grounded in
an image evaluation prompt, which assesses how well each candidate aligns with
users query. By allowing MLLM to perform listwise reasoning, our method
supports global comparison, consistent reasoning, and interpretable
decision-making - all of which are essential for accurate image retrieval. To
enable structured and fine-grained analysis, we further introduce a query
deconstruction prompt, which breaks down the original query into multiple
semantic components. Extensive experiments on five datasets demonstrate the
effectiveness of our CoTRR method, which achieves state-of-the-art performance
across three image retrieval tasks, including text-to-image retrieval (TIR),
composed image retrieval (CIR) and chat-based image retrieval (Chat-IR). Our
code is available at https://github.com/freshfish15/CoTRR .

</details>


### [19] [Data Augmentation via Latent Diffusion Models for Detecting Smell-Related Objects in Historical Artworks](https://arxiv.org/abs/2509.14755)
*Ahmed Sheta,Mathias Zinnen,Aline Sindel,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 研究利用扩散模型生成合成数据，有效解决了历史艺术品气味物体检测中的标注稀缺问题，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 历史艺术品中气味相关物体的识别面临标注稀疏和极端类别不平衡的挑战，研究旨在探索合成数据生成如何缓解这些问题。

Method: 研究评估了多种基于扩散模型的数据增强策略，并将合成数据整合到模型训练中。

Result: 实验证明，合成数据的引入能显著提升检测性能，尤其是在标注稀缺且昂贵的领域应用中。

Conclusion: 该研究展示了利用合成数据生成技术，特别是基于扩散模型的方法，可以有效提升历史艺术品中气味相关物体的检测性能，即使在数据稀缺的情况下也能取得良好效果。

Abstract: Finding smell references in historic artworks is a challenging problem.
Beyond artwork-specific challenges such as stylistic variations, their
recognition demands exceptionally detailed annotation classes, resulting in
annotation sparsity and extreme class imbalance. In this work, we explore the
potential of synthetic data generation to alleviate these issues and enable
accurate detection of smell-related objects. We evaluate several
diffusion-based augmentation strategies and demonstrate that incorporating
synthetic data into model training can improve detection performance. Our
findings suggest that leveraging the large-scale pretraining of diffusion
models offers a promising approach for improving detection accuracy,
particularly in niche applications where annotations are scarce and costly to
obtain. Furthermore, the proposed approach proves to be effective even with
relatively small amounts of data, and scaling it up provides high potential for
further enhancements.

</details>


### [20] [Frame Sampling Strategies Matter: A Benchmark for small vision language models](https://arxiv.org/abs/2509.14769)
*Marija Brkic,Anas Filali Razzouki,Yannis Tevissen,Khalil Guetari,Mounim A. El Yacoubi*

Main category: cs.CV

TL;DR: 研究发现视频视觉语言模型评估中存在帧采样偏差，提出了首个帧精确基准测试，强调需要标准化帧采样策略。


<details>
  <summary>Details</summary>
Motivation: 当前视频基准测试存在帧采样偏差，因为模型使用不同的帧选择策略进行评估，这影响了性能的准确比较。

Method: 提出了第一个帧精确的基准测试，用于评估在受控帧采样策略下的小型视觉语言模型在视频问答任务中的表现。

Result: 研究结果证实了帧采样偏差的存在，并揭示了不同帧采样技术下小型视觉语言模型的数据特定和任务特定行为。

Conclusion: 通过开源基准测试代码，本研究为社区提供了一个可重复且无偏见的视频视觉语言模型评估协议，并强调了未来研究中需要针对每个基准数据集定制标准化帧采样策略。

Abstract: Comparing vision language models on videos is particularly complex, as the
performances is jointly determined by the model's visual representation
capacity and the frame-sampling strategy used to construct the input. Current
video benchmarks are suspected to suffer from substantial frame-sampling bias,
as models are evaluated with different frame selection strategies. In this
work, we propose the first frame-accurate benchmark of state-of-the-art small
VLMs for video question-answering, evaluated under controlled frame-sampling
strategies. Our results confirm the suspected bias and highlight both
data-specific and task-specific behaviors of SVLMs under different
frame-sampling techniques. By open-sourcing our benchmarking code, we provide
the community with a reproducible and unbiased protocol for evaluating video
VLMs and emphasize the need for standardized frame-sampling strategies tailored
to each benchmarking dataset in future research.

</details>


### [21] [A Real-Time Multi-Model Parametric Representation of Point Clouds](https://arxiv.org/abs/2509.14773)
*Yuan Gao,Wei Dong*

Main category: cs.CV

TL;DR: 提出了一种实时高精度的多模型参数表示方法，通过高斯混合模型分割和B样条曲面拟合，显著提升了点云处理的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有参数化表示方法在计算成本和精度之间的权衡问题，提出一种实时且高精度的多模型参数表示方法。

Method: 首先使用高斯混合模型分割点云为多个簇，然后选择并合并平坦簇为平面或曲面。平面通过2D体素边界描述方法拟合，曲面则通过B样条曲面和相同边界描述方法拟合。

Result: 在多个公开数据集上的评估显示，该方法在效率上比现有技术提高了3.78倍，精度比高斯混合模型提高了2倍，并在低功耗设备上实现了36.4 fps的运行速度。

Conclusion: 该论文提出的多模型参数表示方法在实时表面检测和拟合中表现出更高的鲁棒性和效率，同时在低功耗设备上实现了较高的运行帧率。

Abstract: In recent years, parametric representations of point clouds have been widely
applied in tasks such as memory-efficient mapping and multi-robot
collaboration. Highly adaptive models, like spline surfaces or quadrics, are
computationally expensive in detection or fitting. In contrast, real-time
methods, such as Gaussian mixture models or planes, have low degrees of
freedom, making high accuracy with few primitives difficult. To tackle this
problem, a multi-model parametric representation with real-time surface
detection and fitting is proposed. Specifically, the Gaussian mixture model is
first employed to segment the point cloud into multiple clusters. Then, flat
clusters are selected and merged into planes or curved surfaces. Planes can be
easily fitted and delimited by a 2D voxel-based boundary description method.
Surfaces with curvature are fitted by B-spline surfaces and the same boundary
description method is employed. Through evaluations on multiple public
datasets, the proposed surface detection exhibits greater robustness than the
state-of-the-art approach, with 3.78 times improvement in efficiency.
Meanwhile, this representation achieves a 2-fold gain in accuracy over Gaussian
mixture models, operating at 36.4 fps on a low-power onboard computer.

</details>


### [22] [Dataset Distillation for Super-Resolution without Class Labels and Pre-trained Models](https://arxiv.org/abs/2509.14777)
*Sunwoo Cho,Yejin Jung,Nam Ik Cho,Jae Woong Soh*

Main category: cs.CV

TL;DR: 该论文提出了一种无需类别标签或预训练SR模型的数据蒸馏方法，通过高梯度块提取和扩散模型微调，显著减少了训练数据量和计算时间，性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 当前基于GAN反转的数据蒸馏方法依赖于预训练的SR网络和类别特定信息，限制了其通用性和适用性。

Method: 通过提取高梯度块并使用CLIP特征分类图像，然后微调扩散模型来学习分布并合成蒸馏训练图像。

Result: 实验结果表明，该方法在使用极少量训练数据和更短的计算时间内实现了最先进的性能。

Conclusion: 该论文提出了一种新的数据蒸馏方法，显著减少了训练深度学习模型所需的数据量和计算时间，同时保持了高性能。

Abstract: Training deep neural networks has become increasingly demanding, requiring
large datasets and significant computational resources, especially as model
complexity advances. Data distillation methods, which aim to improve data
efficiency, have emerged as promising solutions to this challenge. In the field
of single image super-resolution (SISR), the reliance on large training
datasets highlights the importance of these techniques. Recently, a generative
adversarial network (GAN) inversion-based data distillation framework for SR
was proposed, showing potential for better data utilization. However, the
current method depends heavily on pre-trained SR networks and class-specific
information, limiting its generalizability and applicability. To address these
issues, we introduce a new data distillation approach for image SR that does
not need class labels or pre-trained SR models. In particular, we first extract
high-gradient patches and categorize images based on CLIP features, then
fine-tune a diffusion model on the selected patches to learn their distribution
and synthesize distilled training images. Experimental results show that our
method achieves state-of-the-art performance while using significantly less
training data and requiring less computational time. Specifically, when we
train a baseline Transformer model for SR with only 0.68\% of the original
dataset, the performance drop is just 0.3 dB. In this case, diffusion model
fine-tuning takes 4 hours, and SR model training completes within 1 hour, much
shorter than the 11-hour training time with the full dataset.

</details>


### [23] [Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model](https://arxiv.org/abs/2509.14780)
*Sina Amirrajab,Zohaib Salahuddin,Sheng Kuang,Henry C. Woodruff,Philippe Lambin*

Main category: cs.CV

TL;DR: Report2CT是一种基于放射学报告的3D CT生成框架，通过多文本编码器和潜在扩散模型，实现了高质量的合成CT数据，并在多项评估指标中达到最新技术水平。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖于简化的提示，忽略了完整放射学报告中的丰富语义细节，从而降低了文本图像对齐和临床保真度。

Method: Report2CT整合了三种预训练的医学文本编码器（BiomedVLP CXR BERT、MedEmbed和ClinicalBERT）来捕捉细微的临床背景。放射学报告和体素间距信息条件化一个在CT RATE数据集的20000个CT体积上训练的3D潜在扩散模型。

Result: Report2CT生成了解剖学上一致的CT体积，具有出色的视觉质量和文本图像对齐。多编码器条件化提高了CLIP分数，表明在自由文本放射学报告中更好地保留了细粒度的临床细节。无分类器引导进一步增强了对齐，仅在FID上有轻微的权衡。

Conclusion: Report2CT通过利用完整的放射学报告和多编码器文本条件，推进了3D CT合成，生成了临床可信且高质量的合成数据。

Abstract: Text to image latent diffusion models have recently advanced medical image
synthesis, but applications to 3D CT generation remain limited. Existing
approaches rely on simplified prompts, neglecting the rich semantic detail in
full radiology reports, which reduces text image alignment and clinical
fidelity. We propose Report2CT, a radiology report conditional latent diffusion
framework for synthesizing 3D chest CT volumes directly from free text
radiology reports, incorporating both findings and impression sections using
multiple text encoder. Report2CT integrates three pretrained medical text
encoders (BiomedVLP CXR BERT, MedEmbed, and ClinicalBERT) to capture nuanced
clinical context. Radiology reports and voxel spacing information condition a
3D latent diffusion model trained on 20000 CT volumes from the CT RATE dataset.
Model performance was evaluated using Frechet Inception Distance (FID) for real
synthetic distributional similarity and CLIP based metrics for semantic
alignment, with additional qualitative and quantitative comparisons against
GenerateCT model. Report2CT generated anatomically consistent CT volumes with
excellent visual quality and text image alignment. Multi encoder conditioning
improved CLIP scores, indicating stronger preservation of fine grained clinical
details in the free text radiology reports. Classifier free guidance further
enhanced alignment with only a minor trade off in FID. We ranked first in the
VLM3D Challenge at MICCAI 2025 on Text Conditional CT Generation and achieved
state of the art performance across all evaluation metrics. By leveraging
complete radiology reports and multi encoder text conditioning, Report2CT
advances 3D CT synthesis, producing clinically faithful and high quality
synthetic data.

</details>


### [24] [Fracture interactive geodesic active contours for bone segmentation](https://arxiv.org/abs/2509.14817)
*Liheng Wang,Licheng Zhang,Hailin Xu,Jingxin Zhao,Xiuyun Su,Jiantao Li,Miutian Tang,Weilu Gao,Chong Chen*

Main category: cs.CV

TL;DR: 针对骨分割中的边缘阻塞和骨折问题，提出了一种结合骨科知识的改进测地线活动轮廓算法，实验证明其有效性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 经典的测地线活动轮廓模型在骨分割中因特征提取不区分而难以处理边缘阻塞、泄漏和骨折现象。

Method: 提出了一种骨折交互式测地线活动轮廓算法，结合强度与梯度范数的新型边缘检测函数，并将距离信息作为自适应步长引入轮廓演化，以稳定演化并帮助轮廓在骨边缘和骨折处停止。

Result: 实验表明，该算法在骨盆和踝部分割中有效解决了上述问题，表现出准确、稳定和一致的性能。

Conclusion: 该算法通过结合骨科知识和距离信息，提出了一种针对骨分割的改进方法，有效解决了边缘阻塞、泄漏和骨折现象，展示了在骨盆和踝部分割中的准确、稳定和一致性能，并具有在其他骨解剖学中广泛应用的潜力。

Abstract: For bone segmentation, the classical geodesic active contour model is usually
limited by its indiscriminate feature extraction, and then struggles to handle
the phenomena of edge obstruction, edge leakage and bone fracture. Thus, we
propose a fracture interactive geodesic active contour algorithm tailored for
bone segmentation, which can better capture bone features and perform robustly
to the presence of bone fractures and soft tissues. Inspired by orthopedic
knowledge, we construct a novel edge-detector function that combines the
intensity and gradient norm, which guides the contour towards bone edges
without being obstructed by other soft tissues and therefore reduces
mis-segmentation. Furthermore, distance information, where fracture prompts can
be embedded, is introduced into the contour evolution as an adaptive step size
to stabilize the evolution and help the contour stop at bone edges and
fractures. This embedding provides a way to interact with bone fractures and
improves the accuracy in the fracture regions. Experiments in pelvic and ankle
segmentation demonstrate the effectiveness on addressing the aforementioned
problems and show an accurate, stable and consistent performance, indicating a
broader application in other bone anatomies. Our algorithm also provides
insights into combining the domain knowledge and deep neural networks.

</details>


### [25] [Template-Based Cortical Surface Reconstruction with Minimal Energy Deformation](https://arxiv.org/abs/2509.14827)
*Patrick Madlindl,Fabian Bongratz,Christian Wachinger*

Main category: cs.CV

TL;DR: 该论文提出了一种最小能量变形（MED）损失作为正则化器，显著提升了CSR训练的一致性和可重复性，同时保持精度和拓扑正确性。


<details>
  <summary>Details</summary>
Motivation: 尽管学习基础的CSR大大加速了处理速度，但确保学习到的变形在变形能量上最优且在训练运行中保持一致仍是一个挑战。

Method: 设计了最小能量变形（MED）损失作为正则化器，并将其整合到V2C-Flow模型中，以优化变形轨迹并补充常用的Chamfer距离。

Result: 研究表明，MED损失显著改善了以往被忽视的训练一致性和可重复性，同时未损害重建精度和拓扑正确性。

Conclusion: 通过引入最小能量变形（MED）损失作为正则化器，该研究显著提高了CSR训练的一致性和可重复性，同时保持了重建精度和拓扑正确性。

Abstract: Cortical surface reconstruction (CSR) from magnetic resonance imaging (MRI)
is fundamental to neuroimage analysis, enabling morphological studies of the
cerebral cortex and functional brain mapping. Recent advances in learning-based
CSR have dramatically accelerated processing, allowing for reconstructions
through the deformation of anatomical templates within seconds. However,
ensuring the learned deformations are optimal in terms of deformation energy
and consistent across training runs remains a particular challenge. In this
work, we design a Minimal Energy Deformation (MED) loss, acting as a
regularizer on the deformation trajectories and complementing the widely used
Chamfer distance in CSR. We incorporate it into the recent V2C-Flow model and
demonstrate considerable improvements in previously neglected training
consistency and reproducibility without harming reconstruction accuracy and
topological correctness.

</details>


### [26] [ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone Health Classification](https://arxiv.org/abs/2509.14830)
*Alvaro Lopez Pellicer,Andre Mariucci,Plamen Angelov,Marwan Bukhari,Jemma G. Kerns*

Main category: cs.CV

TL;DR: ProtoMedX是一种可解释的多模态模型，结合DEXA扫描和患者记录，在骨健康分类中表现优异且透明。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法在骨健康研究中缺乏可解释性，难以满足医疗需求及欧盟AI法案要求。

Method: 提出ProtoMedX模型，结合DEXA扫描和患者记录，采用原型架构实现可解释性设计。

Result: 在4,160名真实NHS患者数据集上，ProtoMedX在视觉任务中达到87.58%准确率，多模态版本达89.8%，均优于现有方法。

Conclusion: ProtoMedX在多模态骨健康分类中表现出色，不仅准确率高，还能提供可解释的决策支持，适合医疗应用。

Abstract: Bone health studies are crucial in medical practice for the early detection
and treatment of Osteopenia and Osteoporosis. Clinicians usually make a
diagnosis based on densitometry (DEXA scans) and patient history. The
applications of AI in this field are ongoing research. Most successful methods
rely on deep learning models that use vision alone (DEXA/X-ray imagery) and
focus on prediction accuracy, while explainability is often disregarded and
left to post hoc assessments of input contributions. We propose ProtoMedX, a
multi-modal model that uses both DEXA scans of the lumbar spine and patient
records. ProtoMedX's prototype-based architecture is explainable by design,
which is crucial for medical applications, especially in the context of the
upcoming EU AI Act, as it allows explicit analysis of model decisions,
including incorrect ones. ProtoMedX demonstrates state-of-the-art performance
in bone health classification while also providing explanations that can be
visually understood by clinicians. Using a dataset of 4,160 real NHS patients,
the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8%
in its multi-modal variant, both surpassing existing published methods.

</details>


### [27] [MapAnything: Mapping Urban Assets using Single Street-View Images](https://arxiv.org/abs/2509.14839)
*Miriam Louise Carnot,Jonas Kunze,Erik Fastermann,Eric Peukert,André Ludwig,Bogdan Franczyk*

Main category: cs.CV

TL;DR: MapAnything模块通过单个图像自动计算物体地理坐标，验证了其在城市对象和事件映射中的有效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 城市数字化进程中，手动更新和管理大量城市对象（如交通标志、树木）及事件（如涂鸦、道路损坏）的地理坐标数据库需求日益增长，亟需自动化解决方案。

Method: 利用度量深度估计模型，结合物体与相机的距离、几何原理及相机规格，计算地理坐标。

Result: 评估表明，MapAnything在城市场景中与LiDAR点云相比，能准确估计距离，并在不同距离区间和语义区域（如道路、植被）中表现良好。

Conclusion: MapAnything模块通过先进的度量深度估计模型，成功实现了基于单个图像的物体地理坐标自动确定，为城市对象和事件映射的自动化提供了有效解决方案。

Abstract: To maintain an overview of urban conditions, city administrations manage
databases of objects like traffic signs and trees, complete with their
geocoordinates. Incidents such as graffiti or road damage are also relevant. As
digitization increases, so does the need for more data and up-to-date
databases, requiring significant manual effort. This paper introduces
MapAnything, a module that automatically determines the geocoordinates of
objects using individual images. Utilizing advanced Metric Depth Estimation
models, MapAnything calculates geocoordinates based on the object's distance
from the camera, geometric principles, and camera specifications. We detail and
validate the module, providing recommendations for automating urban object and
incident mapping. Our evaluation measures the accuracy of estimated distances
against LiDAR point clouds in urban environments, analyzing performance across
distance intervals and semantic areas like roads and vegetation. The module's
effectiveness is demonstrated through practical use cases involving traffic
signs and road damage.

</details>


### [28] [Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution](https://arxiv.org/abs/2509.14841)
*Hongjun Wang,Jiyuan Chen,Zhengwei Yin,Xuan Song,Yinqiang Zheng*

Main category: cs.CV

TL;DR: 本文发现超分辨率模型主要对噪声过拟合，提出了一种无需修改架构的目标特征去噪框架，显著提升了泛化性能。


<details>
  <summary>Details</summary>
Motivation: 通过细致研究发现，模型在超分辨率任务中主要对噪声过拟合，而现有方法假设模型对所有退化类型过拟合。因此，需要一种针对性解决方案。

Method: 提出了一种包含噪声检测和去噪模块的目标特征去噪框架，能够无缝集成到现有超分辨率模型中。

Result: 在五个传统基准和数据集（包含合成和真实场景）上，该框架表现优于之前的基于正则化的方法。

Conclusion: 本文提出的目标特征去噪框架通过噪声检测和去噪模块，有效解决了模型在超分辨率任务中主要对噪声过拟合的问题，显著提升了模型的泛化能力，且无需修改现有架构即可集成。

Abstract: Generalizable Image Super-Resolution aims to enhance model generalization
capabilities under unknown degradations. To achieve this goal, the models are
expected to focus only on image content-related features instead of overfitting
degradations. Recently, numerous approaches such as Dropout and Feature
Alignment have been proposed to suppress models' natural tendency to overfit
degradations and yield promising results. Nevertheless, these works have
assumed that models overfit to all degradation types (e.g., blur, noise, JPEG),
while through careful investigations in this paper, we discover that models
predominantly overfit to noise, largely attributable to its distinct
degradation pattern compared to other degradation types. In this paper, we
propose a targeted feature denoising framework, comprising noise detection and
denoising modules. Our approach presents a general solution that can be
seamlessly integrated with existing super-resolution models without requiring
architectural modifications. Our framework demonstrates superior performance
compared to previous regularization-based methods across five traditional
benchmarks and datasets, encompassing both synthetic and real-world scenarios.

</details>


### [29] [[Re] Improving Interpretation Faithfulness for Vision Transformers](https://arxiv.org/abs/2509.14846)
*Izabela Kurek,Wojciech Trejter,Stipe Frkovic,Andro Erdelez*

Main category: cs.CV

TL;DR: 该研究验证了DDS在提升Vision Transformers解释方法鲁棒性方面的效果，并评估了其计算成本和环境影响，结果与原始研究基本一致。


<details>
  <summary>Details</summary>
Motivation: 旨在验证arXiv:2311.17983中提出的DDS是否如声称的那样能提升解释方法的鲁棒性，并评估其计算成本和环境影响。

Method: 本研究通过复制Faithful Vision Transformers (FViTs)的结果，并扩展了原始研究，测试了Diffusion Denoised Smoothing (DDS)在不同解释方法中的鲁棒性提升效果。

Result: 结果基本支持原始研究的结论，但发现了一些细微差异。

Conclusion: 研究结果总体上支持了原始研究的发现，尽管存在一些细微差异并进行了讨论。

Abstract: This work aims to reproduce the results of Faithful Vision Transformers
(FViTs) proposed by arXiv:2311.17983 alongside interpretability methods for
Vision Transformers from arXiv:2012.09838 and Xu (2022) et al. We investigate
claims made by arXiv:2311.17983, namely that the usage of Diffusion Denoised
Smoothing (DDS) improves interpretability robustness to (1) attacks in a
segmentation task and (2) perturbation and attacks in a classification task. We
also extend the original study by investigating the authors' claims that adding
DDS to any interpretability method can improve its robustness under attack.
This is tested on baseline methods and the recently proposed Attribution
Rollout method. In addition, we measure the computational costs and
environmental impact of obtaining an FViT through DDS. Our results broadly
agree with the original study's findings, although minor discrepancies were
found and discussed.

</details>


### [30] [MARIC: Multi-Agent Reasoning for Image Classification](https://arxiv.org/abs/2509.14860)
*Wonduk Seo,Minhyeong Yu,Hyunjin An,Seunghyun Lee*

Main category: cs.CV

TL;DR: MARIC是一种多代理协作推理框架，通过分解图像分类任务为多个视角并综合反思，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统图像分类方法依赖于参数密集型模型训练和大规模标注数据集，而现有视觉语言模型（VLMs）又受限于单次表示，无法捕捉视觉内容的互补方面。MARIC旨在通过多代理协作推理解决这些问题。

Method: MARIC采用多代理框架，包括Outliner Agent分析图像全局主题并生成提示，三个Aspect Agent提取不同视觉维度的细粒度描述，以及Reasoning Agent通过综合反思步骤合成这些描述，生成统一表示进行分类。

Result: 在四个不同的图像分类基准数据集上的实验表明，MARIC显著优于基线方法。

Conclusion: MARIC通过多代理协作推理框架显著提升了图像分类的性能，展示了其在鲁棒性和可解释性方面的优势。

Abstract: Image classification has traditionally relied on parameter-intensive model
training, requiring large-scale annotated datasets and extensive fine tuning to
achieve competitive performance. While recent vision language models (VLMs)
alleviate some of these constraints, they remain limited by their reliance on
single pass representations, often failing to capture complementary aspects of
visual content. In this paper, we introduce Multi Agent based Reasoning for
Image Classification (MARIC), a multi agent framework that reformulates image
classification as a collaborative reasoning process. MARIC first utilizes an
Outliner Agent to analyze the global theme of the image and generate targeted
prompts. Based on these prompts, three Aspect Agents extract fine grained
descriptions along distinct visual dimensions. Finally, a Reasoning Agent
synthesizes these complementary outputs through integrated reflection step,
producing a unified representation for classification. By explicitly
decomposing the task into multiple perspectives and encouraging reflective
synthesis, MARIC mitigates the shortcomings of both parameter-heavy training
and monolithic VLM reasoning. Experiments on 4 diverse image classification
benchmark datasets demonstrate that MARIC significantly outperforms baselines,
highlighting the effectiveness of multi-agent visual reasoning for robust and
interpretable image classification.

</details>


### [31] [Controllable Localized Face Anonymization Via Diffusion Inpainting](https://arxiv.org/abs/2509.14866)
*Ali Salar,Qing Liu,Guoying Zhao*

Main category: cs.CV

TL;DR: 提出一种基于潜在扩散模型的图像匿名化框架，通过自适应属性引导模块实现精确控制，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着肖像图像在计算机视觉中的广泛应用，保护个人身份的需求日益增长，同时匿名化后的图像仍需保持对下游任务的有效性。

Method: 利用潜在扩散模型的修复能力生成真实的匿名化图像，设计了自适应属性引导模块，在反向去噪过程中应用梯度校正。

Result: 在CelebA-HQ和FFHQ数据集上的实验表明，该方法无需额外训练即可超越现有最先进方法。

Conclusion: 该论文提出了一种基于潜在扩散模型的统一框架，通过设计自适应属性引导模块，实现了对匿名化过程的完全控制，并在CelebA-HQ和FFHQ数据集上验证了其优于现有方法的性能。

Abstract: The growing use of portrait images in computer vision highlights the need to
protect personal identities. At the same time, anonymized images must remain
useful for downstream computer vision tasks. In this work, we propose a unified
framework that leverages the inpainting ability of latent diffusion models to
generate realistic anonymized images. Unlike prior approaches, we have complete
control over the anonymization process by designing an adaptive
attribute-guidance module that applies gradient correction during the reverse
denoising process, aligning the facial attributes of the generated image with
those of the synthesized target image. Our framework also supports localized
anonymization, allowing users to specify which facial regions are left
unchanged. Extensive experiments conducted on the public CelebA-HQ and FFHQ
datasets show that our method outperforms state-of-the-art approaches while
requiring no additional model training. The source code is available on our
page.

</details>


### [32] [Temporal Representation Learning of Phenotype Trajectories for pCR Prediction in Breast Cancer](https://arxiv.org/abs/2509.14872)
*Ivana Janíčková,Yen Y. Tan,Thomas H. Helbich,Konstantin Miloserdov,Zsuzsanna Bago-Horvath,Ulrike Heber,Georg Langs*

Main category: cs.CV

TL;DR: 该研究通过MRI数据学习治疗早期动态表示，预测乳腺癌新辅助化疗的病理完全缓解，实验显示随时间点增加预测准确率提升。


<details>
  <summary>Details</summary>
Motivation: 由于疾病进展和治疗反应在患者之间存在显著差异，需要能够预测个体治疗反应的模型来支持有效治疗决策。

Method: 利用乳腺癌患者磁共振成像（MRI）数据的纵向变化在潜在空间中形成轨迹，构建多任务模型，该模型能够表征外观、促进时间连续性并考虑非响应队列的高异质性。

Result: 在ISPY-2数据集上，潜在轨迹空间中的线性分类器仅使用治疗前数据（T0）时平衡准确率为0.761，使用早期反应数据（T0 + T1）时为0.811，使用四个成像时间点（T0 -> T3）时为0.861。

Conclusion: 该研究提出了一种通过学习治疗早期动态的影像数据表示来预测乳腺癌患者新辅助化疗病理完全缓解（pCR）的方法，通过公开数据集ISPY-2验证了其有效性。

Abstract: Effective therapy decisions require models that predict the individual
response to treatment. This is challenging since the progression of disease and
response to treatment vary substantially across patients. Here, we propose to
learn a representation of the early dynamics of treatment response from imaging
data to predict pathological complete response (pCR) in breast cancer patients
undergoing neoadjuvant chemotherapy (NACT). The longitudinal change in magnetic
resonance imaging (MRI) data of the breast forms trajectories in the latent
space, serving as basis for prediction of successful response. The multi-task
model represents appearance, fosters temporal continuity and accounts for the
comparably high heterogeneity in the non-responder cohort.In experiments on the
publicly available ISPY-2 dataset, a linear classifier in the latent trajectory
space achieves a balanced accuracy of 0.761 using only pre-treatment data (T0),
0.811 using early response (T0 + T1), and 0.861 using four imaging time points
(T0 -> T3). The code will be made available upon paper acceptance.

</details>


### [33] [NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft Pose Estimation](https://arxiv.org/abs/2509.14890)
*Antoine Legrand,Renaud Detry,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 本文提出了一种基于NeRF的方法，可视化太空船姿态估计网络依赖的3D视觉线索，实验结果验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的太空船姿态估计方法在实际任务中应用受限，因为对其决策过程缺乏理解。本文旨在解决这一问题。

Method: 使用通过姿态估计网络反向传播的梯度训练NeRF-based图像生成器，以渲染姿态估计网络利用的主要3D特征。

Result: 实验表明，该方法能够恢复相关的3D线索，并提供了关于姿态估计网络监督与其对目标太空船隐式表示之间关系的额外见解。

Conclusion: 本文提出了一种可视化太空船姿态估计网络依赖的3D视觉线索的方法，并通过实验验证了该方法能够恢复相关的3D特征。

Abstract: On-orbit operations require the estimation of the relative 6D pose, i.e.,
position and orientation, between a chaser spacecraft and its target. While
data-driven spacecraft pose estimation methods have been developed, their
adoption in real missions is hampered by the lack of understanding of their
decision process. This paper presents a method to visualize the 3D visual cues
on which a given pose estimator relies. For this purpose, we train a NeRF-based
image generator using the gradients back-propagated through the pose estimation
network. This enforces the generator to render the main 3D features exploited
by the spacecraft pose estimation network. Experiments demonstrate that our
method recovers the relevant 3D cues. Furthermore, they offer additional
insights on the relationship between the pose estimation network supervision
and its implicit representation of the target spacecraft.

</details>


### [34] [Pseudo-Label Enhanced Cascaded Framework: 2nd Technical Report for LSVOS 2025 VOS Track](https://arxiv.org/abs/2509.14901)
*An Yan,Leilei Cao,Feng Lu,Ran Hong,Youhai Jiang,Fengjie Zhu*

Main category: cs.CV

TL;DR: 该论文提出了一种结合伪标签训练和级联多模型推理的方法，显著提升了复杂视频对象分割的性能，在LSVOS 2025竞赛中排名第二。


<details>
  <summary>Details</summary>
Motivation: 解决复杂视频对象分割中因小目标、频繁遮挡、快速运动和复杂交互带来的挑战。

Method: 采用伪标签训练策略和级联多模型推理框架，结合SAM2Long和SeC模型的优势。

Result: 在MOSE测试集上J&F得分为0.8616，比基线提高了1.4分。

Conclusion: 该方法在LSVOS 2025 VOS Track中获得了第二名，展示了在长、复杂视频分割场景中的强鲁棒性和准确性。

Abstract: Complex Video Object Segmentation (VOS) presents significant challenges in
accurately segmenting objects across frames, especially in the presence of
small and similar targets, frequent occlusions, rapid motion, and complex
interactions. In this report, we present our solution for the LSVOS 2025 VOS
Track based on the SAM2 framework. We adopt a pseudo-labeling strategy during
training: a trained SAM2 checkpoint is deployed within the SAM2Long framework
to generate pseudo labels for the MOSE test set, which are then combined with
existing data for further training. For inference, the SAM2Long framework is
employed to obtain our primary segmentation results, while an open-source SeC
model runs in parallel to produce complementary predictions. A cascaded
decision mechanism dynamically integrates outputs from both models, exploiting
the temporal stability of SAM2Long and the concept-level robustness of SeC.
Benefiting from pseudo-label training and cascaded multi-model inference, our
approach achieves a J\&F score of 0.8616 on the MOSE test set -- +1.4 points
over our SAM2Long baseline -- securing the 2nd place in the LSVOS 2025 VOS
Track, and demonstrating strong robustness and accuracy in long, complex video
segmentation scenarios.

</details>


### [35] [Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications](https://arxiv.org/abs/2509.14921)
*Tahar Chettaoui,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: CLIP微调后可能因过度专业化丧失泛化能力，较大模型容量有助于缓解此问题。FRoundation模型在FR任务表现优异但泛化性能下降。


<details>
  <summary>Details</summary>
Motivation: 探讨基础模型在高度专业化任务微调后是否丧失其跨领域泛化能力，并量化这种权衡。

Method: 通过评估三种针对FR、MAD和PAD微调的CLIP实例，以及原始CLIP基线，在14个通用视觉数据集和常见FR、MAD、PAD基准上进行零样本和线性探测协议测试。

Result: 微调模型（尤其是复杂任务如FR）表现出过度专业化。FRoundation模型在IJB-C基准上表现最佳，但在ImageNetV2上性能显著下降。较大CLIP架构能更好地保留泛化能力。

Conclusion: 基础模型如CLIP在高度专业化的生物识别任务（如人脸识别、变形攻击检测和呈现攻击检测）微调后，可能会因过度专业化而丧失跨领域泛化能力。模型容量较大的CLIP架构能更好地保留原始泛化能力。

Abstract: Foundation models such as CLIP have demonstrated exceptional zero- and
few-shot transfer capabilities across diverse vision tasks. However, when
fine-tuned for highly specialized biometric tasks, face recognition (FR),
morphing attack detection (MAD), and presentation attack detection (PAD), these
models may suffer from over-specialization. Thus, they may lose one of their
foundational strengths, cross-domain generalization. In this work, we
systematically quantify these trade-offs by evaluating three instances of CLIP
fine-tuned for FR, MAD, and PAD. We evaluate each adapted model as well as the
original CLIP baseline on 14 general vision datasets under zero-shot and
linear-probe protocols, alongside common FR, MAD, and PAD benchmarks. Our
results indicate that fine-tuned models suffer from over-specialization,
especially when fine-tuned for complex tasks of FR. Also, our results pointed
out that task complexity and classification head design, multi-class (FR) vs.
binary (MAD and PAD), correlate with the degree of catastrophic forgetting. The
FRoundation model with the ViT-L backbone outperforms other approaches on the
large-scale FR benchmark IJB-C, achieving an improvement of up to 58.52%.
However, it experiences a substantial performance drop on ImageNetV2, reaching
only 51.63% compared to 69.84% achieved by the baseline CLIP model. Moreover,
the larger CLIP architecture consistently preserves more of the model's
original generalization ability than the smaller variant, indicating that
increased model capacity may help mitigate over-specialization.

</details>


### [36] [GenKOL: Modular Generative AI Framework For Scalable Virtual KOL Generation](https://arxiv.org/abs/2509.14927)
*Tan-Hiep To,Duy-Khang Nguyen,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: GenKOL利用生成式AI技术，通过模块化服务高效生成虚拟KOL图像，降低营销成本并加速内容生产。


<details>
  <summary>Details</summary>
Motivation: 传统与人类KOL合作成本高且存在物流挑战，GenKOL旨在通过生成式AI技术高效创建高质量虚拟KOL图像，优化营销流程。

Method: GenKOL 是一个交互式系统，集成了多种AI功能（如服装生成、妆容迁移、背景合成和发型编辑），通过模块化服务实现灵活部署。

Result: 系统能够显著简化品牌内容生产，降低成本并加速营销工作流程，通过可扩展的虚拟KOL创建实现高效营销。

Conclusion: GenKOL 通过其模块化架构和生成式AI技术，显著降低了虚拟KOL创建的成本和复杂性，为营销内容生产提供了高效、灵活的解决方案。

Abstract: Key Opinion Leader (KOL) play a crucial role in modern marketing by shaping
consumer perceptions and enhancing brand credibility. However, collaborating
with human KOLs often involves high costs and logistical challenges. To address
this, we present GenKOL, an interactive system that empowers marketing
professionals to efficiently generate high-quality virtual KOL images using
generative AI. GenKOL enables users to dynamically compose promotional visuals
through an intuitive interface that integrates multiple AI capabilities,
including garment generation, makeup transfer, background synthesis, and hair
editing. These capabilities are implemented as modular, interchangeable
services that can be deployed flexibly on local machines or in the cloud. This
modular architecture ensures adaptability across diverse use cases and
computational environments. Our system can significantly streamline the
production of branded content, lowering costs and accelerating marketing
workflows through scalable virtual KOL creation.

</details>


### [37] [DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection](https://arxiv.org/abs/2509.14957)
*Zhuokang Shen,Kaisen Zhang,Bohan Jia,Yuan Fang,Zhou Yu,Shaohui Lin*

Main category: cs.CV

TL;DR: DF-LLaVA结合MLLMs的潜在知识和提示训练，实现了高准确性和可解释性的合成图像检测。


<details>
  <summary>Details</summary>
Motivation: 现有检测模型在图像真实性评估中缺乏解释性，且MLLM-based方法在准确性上落后于专家模型。

Method: 提出了DF-LLaVA框架，通过提取MLLMs的潜在知识并通过提示注入训练，提升检测性能。

Result: DF-LLaVA在合成图像检测中实现了超过专家模型的高准确性，同时保持了MLLMs的可解释性。

Conclusion: DF-LLaVA框架通过结合MLLMs的潜在知识和提示训练，在合成图像检测中实现了高准确性和可解释性，超越了专家模型。

Abstract: With the increasing prevalence of synthetic images, evaluating image
authenticity and locating forgeries accurately while maintaining human
interpretability remains a challenging task. Existing detection models
primarily focus on simple authenticity classification, ultimately providing
only a forgery probability or binary judgment, which offers limited explanatory
insights into image authenticity. Moreover, while MLLM-based detection methods
can provide more interpretable results, they still lag behind expert models in
terms of pure authenticity classification accuracy. To address this, we propose
DF-LLaVA, a simple yet effective framework that unlocks the intrinsic
discrimination potential of MLLMs. Our approach first extracts latent knowledge
from MLLMs and then injects it into training via prompts. This framework allows
LLaVA to achieve outstanding detection accuracy exceeding expert models while
still maintaining the interpretability offered by MLLMs. Extensive experiments
confirm the superiority of our DF-LLaVA, achieving both high accuracy and
explainability in synthetic image detection. Code is available online at:
https://github.com/Eliot-Shen/DF-LLaVA.

</details>


### [38] [Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification](https://arxiv.org/abs/2509.14958)
*Xiang Tuo,Xu Xuemiao,Liu Bangzhen,Li Jinyi,Li Yong,He Shengfeng*

Main category: cs.CV

TL;DR: CMGR框架通过几何校正和纹理增强，提升3D少样本类增量学习的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D类增量学习方法在极端数据稀缺情况下因几何错位和纹理偏差导致的性能下降问题。

Method: 提出了Cross-Modal Geometric Rectification (CMGR)框架，包括Structure-Aware Geometric Rectification模块和Texture Amplification模块，以及Base-Novel Discriminator来稳定增量原型。

Result: 实验证明CMGR在跨域和域内设置下均显著提升了3D少样本类增量学习的几何一致性和鲁棒性。

Conclusion: CMGR框架通过跨模态几何校正和纹理增强，显著提升了3D少样本类增量学习的性能，实现了更好的几何一致性和对纹理偏差的鲁棒性。

Abstract: The rapid growth of 3D digital content necessitates expandable recognition
systems for open-world scenarios. However, existing 3D class-incremental
learning methods struggle under extreme data scarcity due to geometric
misalignment and texture bias. While recent approaches integrate 3D data with
2D foundation models (e.g., CLIP), they suffer from semantic blurring caused by
texture-biased projections and indiscriminate fusion of geometric-textural
cues, leading to unstable decision prototypes and catastrophic forgetting. To
address these issues, we propose Cross-Modal Geometric Rectification (CMGR), a
framework that enhances 3D geometric fidelity by leveraging CLIP's hierarchical
spatial semantics. Specifically, we introduce a Structure-Aware Geometric
Rectification module that hierarchically aligns 3D part structures with CLIP's
intermediate spatial priors through attention-driven geometric fusion.
Additionally, a Texture Amplification Module synthesizes minimal yet
discriminative textures to suppress noise and reinforce cross-modal
consistency. To further stabilize incremental prototypes, we employ a
Base-Novel Discriminator that isolates geometric variations. Extensive
experiments demonstrate that our method significantly improves 3D few-shot
class-incremental learning, achieving superior geometric coherence and
robustness to texture bias across cross-domain and within-domain settings.

</details>


### [39] [Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis](https://arxiv.org/abs/2509.14965)
*Junhao Jia,Yunyou Liu,Cheng Yang,Yifei Sun,Feiwei Qin,Changmiao Wang,Yong Peng*

Main category: cs.CV

TL;DR: 该论文提出了一种基于双曲几何的深度学习框架Brain-HGCN，用于高保真建模大脑功能网络的分层结构，并在精神病分类任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于固有的空间限制，标准欧几里得GNN难以无高失真地表示大脑网络的分层结构，限制了其临床性能。

Method: 基于双曲几何的深度学习框架Brain-HGCN，利用负曲率空间的固有特性高保真地建模大脑网络层次结构。采用带符号聚合机制的新型双曲图注意力层处理兴奋性和抑制性连接，并通过几何合理的Fr\'echet均值学习鲁棒的图级表示。

Result: 在两大fMRI数据集上的精神病分类实验中，该方法显著优于多种最先进的欧几里得基线模型。

Conclusion: 该研究开创了fMRI分析的新几何深度学习范式，凸显了双曲GNN在计算精神病学领域的巨大潜力。

Abstract: Functional magnetic resonance imaging (fMRI) provides a powerful non-invasive
window into the brain's functional organization by generating complex
functional networks, typically modeled as graphs. These brain networks exhibit
a hierarchical topology that is crucial for cognitive processing. However, due
to inherent spatial constraints, standard Euclidean GNNs struggle to represent
these hierarchical structures without high distortion, limiting their clinical
performance. To address this limitation, we propose Brain-HGCN, a geometric
deep learning framework based on hyperbolic geometry, which leverages the
intrinsic property of negatively curved space to model the brain's network
hierarchy with high fidelity. Grounded in the Lorentz model, our model employs
a novel hyperbolic graph attention layer with a signed aggregation mechanism to
distinctly process excitatory and inhibitory connections, ultimately learning
robust graph-level representations via a geometrically sound Fr\'echet mean for
graph readout. Experiments on two large-scale fMRI datasets for psychiatric
disorder classification demonstrate that our approach significantly outperforms
a wide range of state-of-the-art Euclidean baselines. This work pioneers a new
geometric deep learning paradigm for fMRI analysis, highlighting the immense
potential of hyperbolic GNNs in the field of computational psychiatry.

</details>


### [40] [RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching](https://arxiv.org/abs/2509.14966)
*Xingwu Zhang,Guanxuan Li,Zhuocheng Zhang,Zijun Long*

Main category: cs.CV

TL;DR: RoboEye是一个两阶段物品识别框架，结合2D语义特征和3D推理，显著提升识别准确率，无需3D输入，降低部署成本。


<details>
  <summary>Details</summary>
Motivation: 大规模电商仓库中物品识别因类内变异性、长尾物品、视觉相似性、多样化包装、遮挡和大视角变化等因素而变得困难，导致依赖2D外观特征的方法性能下降。

Method: RoboEye采用两阶段识别框架：第一阶段训练大型视觉模型提取2D特征生成候选排名；第二阶段通过轻量级3D特征感知模块判断是否需要3D重排序，并使用机器人3D检索变换器进行几何感知密集特征提取和关键点匹配。

Result: 实验表明，RoboEye在Recall@1上比现有最佳方法（RoboLLM）提升了7.1%，且仅需RGB图像，无需显式3D输入。

Conclusion: RoboEye通过结合2D语义特征和领域适应的3D推理，显著提升了大规模电商仓库中物品识别的准确性，且无需依赖显式3D输入，降低了部署成本。

Abstract: The rapidly growing number of product categories in large-scale e-commerce
makes accurate object identification for automated packing in warehouses
substantially more difficult. As the catalog grows, intra-class variability and
a long tail of rare or visually similar items increase, and when combined with
diverse packaging, cluttered containers, frequent occlusion, and large
viewpoint changes-these factors amplify discrepancies between query and
reference images, causing sharp performance drops for methods that rely solely
on 2D appearance features. Thus, we propose RoboEye, a two-stage identification
framework that dynamically augments 2D semantic features with domain-adapted 3D
reasoning and lightweight adapters to bridge training deployment gaps. In the
first stage, we train a large vision model to extract 2D features for
generating candidate rankings. A lightweight 3D-feature-awareness module then
estimates 3D feature quality and predicts whether 3D re-ranking is necessary,
preventing performance degradation and avoiding unnecessary computation. When
invoked, the second stage uses our robot 3D retrieval transformer, comprising a
3D feature extractor that produces geometry-aware dense features and a
keypoint-based matcher that computes keypoint-correspondence confidences
between query and reference images instead of conventional cosine-similarity
scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior
state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images,
avoiding reliance on explicit 3D inputs and reducing deployment costs. The code
used in this paper is publicly available at:
https://github.com/longkukuhi/RoboEye.

</details>


### [41] [Beyond Random Masking: A Dual-Stream Approach for Rotation-Invariant Point Cloud Masked Autoencoders](https://arxiv.org/abs/2509.14975)
*Xuanhua Yin,Dingxin Zhang,Yu Feng,Shunqi Mao,Jianhui Yu,Weidong Cai*

Main category: cs.CV

TL;DR: 双流掩码方法（几何+语义）提升旋转不变点云MAE性能，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有旋转不变点云MAE的随机掩码策略忽略了几何结构和语义连贯性，导致性能受限。

Method: 采用双流掩码策略：3D空间网格掩码通过坐标排序捕获几何关系，渐进语义掩码通过注意力驱动聚类发现语义部分。两者通过动态加权课程学习协调。

Result: 在ModelNet40、ScanObjectNN和OmniObject3D上的实验表明，该方法在不同旋转场景下均优于基线方法。

Conclusion: 提出的双流掩码方法（3D空间网格掩码和渐进语义掩码）显著提升了旋转不变点云MAE的性能，无需架构改动即可广泛兼容现有框架。

Abstract: Existing rotation-invariant point cloud masked autoencoders (MAE) rely on
random masking strategies that overlook geometric structure and semantic
coherence. Random masking treats patches independently, failing to capture
spatial relationships consistent across orientations and overlooking semantic
object parts that maintain identity regardless of rotation. We propose a
dual-stream masking approach combining 3D Spatial Grid Masking and Progressive
Semantic Masking to address these fundamental limitations. Grid masking creates
structured patterns through coordinate sorting to capture geometric
relationships that persist across different orientations, while semantic
masking uses attention-driven clustering to discover semantically meaningful
parts and maintain their coherence during masking. These complementary streams
are orchestrated via curriculum learning with dynamic weighting, progressing
from geometric understanding to semantic discovery. Designed as plug-and-play
components, our strategies integrate into existing rotation-invariant
frameworks without architectural changes, ensuring broad compatibility across
different approaches. Comprehensive experiments on ModelNet40, ScanObjectNN,
and OmniObject3D demonstrate consistent improvements across various rotation
scenarios, showing substantial performance gains over the baseline
rotation-invariant methods.

</details>


### [42] [EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence](https://arxiv.org/abs/2509.14977)
*Chaoyin She,Ruifang Lu,Lida Chen,Wei Wang,Qinghua Huang*

Main category: cs.CV

TL;DR: EchoVLM是一种专为超声医学影像设计的视觉语言模型，通过MoE架构和多区域数据训练，显著提升了报告生成和诊断任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统超声诊断高度依赖医生经验，存在主观性强和诊断效率低的问题；现有通用视觉语言模型在超声医学任务中表现有限。

Method: 采用专家混合（MoE）架构，在七个解剖区域的数据上进行训练，支持多任务处理，包括超声报告生成、诊断和视觉问答（VQA）。

Result: 在超声报告生成任务中，EchoVLM相比Qwen2-VL在BLEU-1和ROUGE-1分数上分别提升了10.15和4.77分。

Conclusion: EchoVLM展示了在超声影像诊断中提升准确性的潜力，为未来临床应用提供了可行的技术方案。

Abstract: Ultrasound imaging has become the preferred imaging modality for early cancer
screening due to its advantages of non-ionizing radiation, low cost, and
real-time imaging capabilities. However, conventional ultrasound diagnosis
heavily relies on physician expertise, presenting challenges of high
subjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer
promising solutions for this issue, but existing general-purpose models
demonstrate limited knowledge in ultrasound medical tasks, with poor
generalization in multi-organ lesion recognition and low efficiency across
multi-task diagnostics. To address these limitations, we propose EchoVLM, a
vision-language model specifically designed for ultrasound medical imaging. The
model employs a Mixture of Experts (MoE) architecture trained on data spanning
seven anatomical regions. This design enables the model to perform multiple
tasks, including ultrasound report generation, diagnosis and visual
question-answering (VQA). The experimental results demonstrated that EchoVLM
achieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and
ROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report
generation task. These findings suggest that EchoVLM has substantial potential
to enhance diagnostic accuracy in ultrasound imaging, thereby providing a
viable technical solution for future clinical applications. Source code and
model weights are available at https://github.com/Asunatan/EchoVLM.

</details>


### [43] [SPATIALGEN: Layout-guided 3D Indoor Scene Generation](https://arxiv.org/abs/2509.14981)
*Chuan Fang,Heng Li,Yixun Liang,Jia Zheng,Yongsen Mao,Yuan Liu,Rui Tang,Zihan Zhou,Ping Tan*

Main category: cs.CV

TL;DR: 论文提出SpatialGen，一种多视角多模态扩散模型，利用新的大规模合成数据集生成高质量3D室内场景，优于现有方法，并开源了数据和模型。


<details>
  <summary>Details</summary>
Motivation: 手动3D建模耗时且劳动密集，现有生成AI方法在视觉质量、多样性、语义一致性和用户控制方面存在挑战，且缺乏大规模高质量数据集。

Method: 提出了SpatialGen，一种基于多视角多模态扩散模型的新方法，能够从3D布局和参考图像生成外观、几何和语义信息。

Result: SpatialGen在实验中表现优于现有方法，能够生成高质量且语义一致的3D室内场景。

Conclusion: 作者开源了数据和模型，以推动室内场景理解和生成领域的发展，并展示了SpatialGen在生成高质量、语义一致的3D室内场景方面的优越性。

Abstract: Creating high-fidelity 3D models of indoor environments is essential for
applications in design, virtual reality, and robotics. However, manual 3D
modeling remains time-consuming and labor-intensive. While recent advances in
generative AI have enabled automated scene synthesis, existing methods often
face challenges in balancing visual quality, diversity, semantic consistency,
and user control. A major bottleneck is the lack of a large-scale, high-quality
dataset tailored to this task. To address this gap, we introduce a
comprehensive synthetic dataset, featuring 12,328 structured annotated scenes
with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this
dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model
that generates realistic and semantically consistent 3D indoor scenes. Given a
3D layout and a reference image (derived from a text prompt), our model
synthesizes appearance (color image), geometry (scene coordinate map), and
semantic (semantic segmentation map) from arbitrary viewpoints, while
preserving spatial consistency across modalities. SpatialGen consistently
generates superior results to previous methods in our experiments. We are
open-sourcing our data and models to empower the community and advance the
field of indoor scene understanding and generation.

</details>


### [44] [Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation](https://arxiv.org/abs/2509.15011)
*Vasiliki Ismiroglou,Malte Pedersen,Stefan H. Bengtson,Andreas Aakerberg,Thomas B. Moeslund*

Main category: cs.CV

TL;DR: 改进的合成数据生成管道在浑浊环境中表现更优，BUCKET数据集为真实浑浊图像处理提供支持。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注变色场景，忽略了模型在高度浑浊环境中捕捉复杂、距离依赖性可见性损失的能力。

Method: 提出了一种改进的合成数据生成管道，包括前向散射项和非均匀介质的考虑，并收集了BUCKET数据集。

Result: 改进模型在增加浑浊度时表现出定性提升，调查参与者的选择率为82.5%。

Conclusion: 改进后的合成数据生成管道在高度浑浊环境中表现出色，特别是在考虑前向散射项和非均匀介质的情况下。BUCKET数据集的收集为真实浑浊环境下的图像处理提供了宝贵资源。

Abstract: In recent years, the underwater image formation model has found extensive use
in the generation of synthetic underwater data. Although many approaches focus
on scenes primarily affected by discoloration, they often overlook the model's
ability to capture the complex, distance-dependent visibility loss present in
highly turbid environments. In this work, we propose an improved synthetic data
generation pipeline that includes the commonly omitted forward scattering term,
while also considering a nonuniform medium. Additionally, we collected the
BUCKET dataset under controlled turbidity conditions to acquire real turbid
footage with the corresponding reference images. Our results demonstrate
qualitative improvements over the reference model, particularly under
increasing turbidity, with a selection rate of 82. 5\% by survey participants.
Data and code can be accessed on the project page:
vap.aau.dk/sea-ing-through-scattered-rays.

</details>


### [45] [PRISM: Product Retrieval In Shopping Carts using Hybrid Matching](https://arxiv.org/abs/2509.14985)
*Arda Kabadayi,Senem Velipasalar,Jiajing Chen*

Main category: cs.CV

TL;DR: PRISM是一种结合视觉语言模型和像素匹配的混合方法，显著提升了零售产品检索的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 零售产品检索面临视觉相似性高和视角差异大的挑战，现有方法（如CLIP/SigLIP和像素匹配）在精度或效率上存在不足。

Method: PRISM是一个三阶段混合方法：1) 使用SigLIP检索语义相似的候选产品；2) 应用YOLO-E分割模型去除背景干扰；3) 使用LightGlue进行像素级精细匹配。

Result: PRISM在ABV数据集上比现有最佳方法提升了4.21%的top-1准确率，且满足实时处理需求。

Conclusion: PRISM方法在ABV数据集上实现了4.21%的top-1准确率提升，同时保持了实时处理的效率，适用于实际零售场景。

Abstract: Compared to traditional image retrieval tasks, product retrieval in retail
settings is even more challenging. Products of the same type from different
brands may have highly similar visual appearances, and the query image may be
taken from an angle that differs significantly from view angles of the stored
catalog images. Foundational models, such as CLIP and SigLIP, often struggle to
distinguish these subtle but important local differences. Pixel-wise matching
methods, on the other hand, are computationally expensive and incur
prohibitively high matching times. In this paper, we propose a new, hybrid
method, called PRISM, for product retrieval in retail settings by leveraging
the advantages of both vision-language model-based and pixel-wise matching
approaches. To provide both efficiency/speed and finegrained retrieval
accuracy, PRISM consists of three stages: 1) A vision-language model (SigLIP)
is employed first to retrieve the top 35 most semantically similar products
from a fixed gallery, thereby narrowing the search space significantly; 2) a
segmentation model (YOLO-E) is applied to eliminate background clutter; 3)
fine-grained pixel-level matching is performed using LightGlue across the
filtered candidates. This framework enables more accurate discrimination
between products with high inter-class similarity by focusing on subtle visual
cues often missed by global models. Experiments performed on the ABV dataset
show that our proposed PRISM outperforms the state-of-the-art image retrieval
methods by 4.21% in top-1 accuracy while still remaining within the bounds of
real-time processing for practical retail deployments.

</details>


### [46] [UCorr: Wire Detection and Depth Estimation for Autonomous Drones](https://arxiv.org/abs/2509.14989)
*Benedikt Kolbeinsson,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: 提出了一种用于电线分割和深度估计的单目端到端模型，通过时间相关层和合成数据训练，表现优于现有方法，提升了无人机安全性。


<details>
  <summary>Details</summary>
Motivation: 在完全自主无人机的领域中，电线的纤细轮廓使其检测成为一个独特且复杂的问题，这对安全导航和防撞至关重要。

Method: 采用了一种创新的单目端到端模型，结合时间相关层和合成数据进行训练，以解决电线检测和深度估计的复杂联合任务。

Result: 论文提出的方法在电线检测和深度估计的联合任务中优于现有竞争方法，验证了其有效性。

Conclusion: 该论文提出的单目端到端模型在电线分割和深度估计任务中表现出色，展示了其在提升自主无人机安全性和精确性方面的潜力，并预示了其实际应用的广阔前景。

Abstract: In the realm of fully autonomous drones, the accurate detection of obstacles
is paramount to ensure safe navigation and prevent collisions. Among these
challenges, the detection of wires stands out due to their slender profile,
which poses a unique and intricate problem. To address this issue, we present
an innovative solution in the form of a monocular end-to-end model for wire
segmentation and depth estimation. Our approach leverages a temporal
correlation layer trained on synthetic data, providing the model with the
ability to effectively tackle the complex joint task of wire detection and
depth estimation. We demonstrate the superiority of our proposed method over
existing competitive approaches in the joint task of wire detection and depth
estimation. Our results underscore the potential of our model to enhance the
safety and precision of autonomous drones, shedding light on its promising
applications in real-world scenarios.

</details>


### [47] [Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models](https://arxiv.org/abs/2509.15156)
*Haobo Yang,Minghao Guo,Dequan Yang,Wenyu Wang*

Main category: cs.CV

TL;DR: 论文提出将几何视觉错觉整合到图像分类训练中，实验表明这能提升模型性能，特别是在复杂视觉任务中，为感知科学和机器学习的结合提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 探索感知驱动的归纳偏见在深度学习中的潜在价值，特别是利用人类感知中经典几何视觉错觉来提升图像分类模型的性能。

Method: 作者提出了一种合成、参数化的几何错觉数据集，并评估了三种多源学习策略，将错觉识别任务与ImageNet分类目标相结合。

Result: 实验表明，将几何错觉作为辅助监督可系统性提高泛化能力，尤其是在视觉挑战性案例中。此外，感知驱动的归纳偏见可以增强CNN和基于Transformer架构的结构敏感性。

Conclusion: 该论文展示了将感知心理学中的几何视觉错觉整合到深度学习模型中，可以显著提升图像分类性能，尤其是在处理复杂轮廓和精细纹理时。这为视觉模型设计提供了一种新的方向，即嵌入感知先验。

Abstract: Contemporary deep learning models have achieved impressive performance in
image classification by primarily leveraging statistical regularities within
large datasets, but they rarely incorporate structured insights drawn directly
from perceptual psychology. To explore the potential of perceptually motivated
inductive biases, we propose integrating classic geometric visual illusions
well-studied phenomena from human perception into standard image-classification
training pipelines. Specifically, we introduce a synthetic, parametric
geometric-illusion dataset and evaluate three multi-source learning strategies
that combine illusion recognition tasks with ImageNet classification
objectives. Our experiments reveal two key conceptual insights: (i)
incorporating geometric illusions as auxiliary supervision systematically
improves generalization, especially in visually challenging cases involving
intricate contours and fine textures; and (ii) perceptually driven inductive
biases, even when derived from synthetic stimuli traditionally considered
unrelated to natural image recognition, can enhance the structural sensitivity
of both CNN and transformer-based architectures. These results demonstrate a
novel integration of perceptual science and machine learning and suggest new
directions for embedding perceptual priors into vision model design.

</details>


### [48] [Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model](https://arxiv.org/abs/2509.15167)
*Pak-Hei Yeung,Jayroop Ramesh,Pengfei Lyu,Ana Namburete,Jagath Rajapakse*

Main category: cs.CV

TL;DR: 本文提出了一种模型无关的框架M&N，通过知识蒸馏和自适应采样策略，显著提升了3D医学图像分割的性能，尤其在标记数据稀缺的情况下。


<details>
  <summary>Details</summary>
Motivation: 探索如何将2D自然图像预训练的通用视觉模型的知识迁移到3D医学图像分割中，以解决标记数据稀缺的问题。

Method: 提出了一种模型无关的框架M&N，通过迭代协同训练2D预训练模型和3D分割模型，并结合学习率引导的采样策略，自适应调整标记和未标记数据的比例。

Result: 在多个公开数据集上的实验表明，M&N在所有不同设置下均优于现有的半监督分割方法。

Conclusion: M&N框架在多种公开数据集上实现了最先进的性能，优于现有的13种半监督分割方法，并且保持了模型无关性，能够与不同架构无缝集成，确保其适应性。

Abstract: This paper explores the transfer of knowledge from general vision models
pretrained on 2D natural images to improve 3D medical image segmentation. We
focus on the semi-supervised setting, where only a few labeled 3D medical
images are available, along with a large set of unlabeled images. To tackle
this, we propose a model-agnostic framework that progressively distills
knowledge from a 2D pretrained model to a 3D segmentation model trained from
scratch. Our approach, M&N, involves iterative co-training of the two models
using pseudo-masks generated by each other, along with our proposed learning
rate guided sampling that adaptively adjusts the proportion of labeled and
unlabeled data in each training batch to align with the models' prediction
accuracy and stability, minimizing the adverse effect caused by inaccurate
pseudo-masks. Extensive experiments on multiple publicly available datasets
demonstrate that M&N achieves state-of-the-art performance, outperforming
thirteen existing semi-supervised segmentation approaches under all different
settings. Importantly, ablation studies show that M&N remains model-agnostic,
allowing seamless integration with different architectures. This ensures its
adaptability as more advanced models emerge. The code is available at
https://github.com/pakheiyeung/M-N.

</details>


### [49] [No Modality Left Behind: Adapting to Missing Modalities via Knowledge Distillation for Brain Tumor Segmentation](https://arxiv.org/abs/2509.15017)
*Shenghao Zhu,Yifei Chen,Weihong Chen,Shuo Jiang,Guanyu Zhou,Yuanhan Wang,Feiwei Qin,Changmiao Wang,Qiyuan Tian*

Main category: cs.CV

TL;DR: AdaMM是一个针对缺失模态场景的脑肿瘤分割框架，通过三个模块提升鲁棒性，实验证明其在单模态和弱模态下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床实践中多模态MRI常存在缺失模态，限制了依赖完整输入的深度学习方法的鲁棒性和泛化能力。为应对这一问题，提出了专门针对缺失模态场景的AdaMM框架。

Method: AdaMM框架包含三个协同模块：Graph-guided Adaptive Refinement Module（显式建模通用和模态特定特征的语义关联）、Bi-Bottleneck Distillation Module（通过全局风格匹配和对抗特征对齐从教师模型向学生模型传递知识）和Lesion-Presence-Guided Reliability Module（通过辅助分类任务预测病变类型的先验概率）。

Result: 在BraTS 2018和2024数据集上的实验表明，AdaMM在单模态和弱模态配置下显著优于现有方法，分割准确性和鲁棒性更优。

Conclusion: AdaMM框架在脑肿瘤分割任务中表现出色，尤其在单模态和弱模态配置下，展现了优越的分割准确性和鲁棒性。此外，系统评估了六类缺失模态策略，确认了知识蒸馏的优越性，并为未来研究提供了实用指导。

Abstract: Accurate brain tumor segmentation is essential for preoperative evaluation
and personalized treatment. Multi-modal MRI is widely used due to its ability
to capture complementary tumor features across different sequences. However, in
clinical practice, missing modalities are common, limiting the robustness and
generalizability of existing deep learning methods that rely on complete
inputs, especially under non-dominant modality combinations. To address this,
we propose AdaMM, a multi-modal brain tumor segmentation framework tailored for
missing-modality scenarios, centered on knowledge distillation and composed of
three synergistic modules. The Graph-guided Adaptive Refinement Module
explicitly models semantic associations between generalizable and
modality-specific features, enhancing adaptability to modality absence. The
Bi-Bottleneck Distillation Module transfers structural and textural knowledge
from teacher to student models via global style matching and adversarial
feature alignment. The Lesion-Presence-Guided Reliability Module predicts prior
probabilities of lesion types through an auxiliary classification task,
effectively suppressing false positives under incomplete inputs. Extensive
experiments on the BraTS 2018 and 2024 datasets demonstrate that AdaMM
consistently outperforms existing methods, exhibiting superior segmentation
accuracy and robustness, particularly in single-modality and weak-modality
configurations. In addition, we conduct a systematic evaluation of six
categories of missing-modality strategies, confirming the superiority of
knowledge distillation and offering practical guidance for method selection and
future research. Our source code is available at
https://github.com/Quanato607/AdaMM.

</details>


### [50] [AutoEdit: Automatic Hyperparameter Tuning for Image Editing](https://arxiv.org/abs/2509.15031)
*Chau Pham,Quan Dao,Mahesh Bhosale,Yunjie Tian,Dimitris Metaxas,David Doermann*

Main category: cs.CV

TL;DR: 本文提出强化学习框架动态调整超参数，显著减少计算成本，提升扩散模型图像编辑的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有编辑方法在超参数识别上面临挑战，需要用户暴力调参，导致高计算成本。

Method: 提出了一种强化学习框架，建立马尔可夫决策过程，在去噪步骤中动态调整超参数，并将编辑目标整合到奖励函数中。

Result: 实验表明，该方法在搜索时间和计算开销上相比现有暴力方法有显著减少。

Conclusion: 本文提出的强化学习框架通过动态调整超参数，显著减少了搜索时间和计算开销，推动了基于扩散的图像编辑框架的实际应用。

Abstract: Recent advances in diffusion models have revolutionized text-guided image
editing, yet existing editing methods face critical challenges in
hyperparameter identification. To get the reasonable editing performance, these
methods often require the user to brute-force tune multiple interdependent
hyperparameters, such as inversion timesteps and attention modification,
\textit{etc.} This process incurs high computational costs due to the huge
hyperparameter search space. We consider searching optimal editing's
hyperparameters as a sequential decision-making task within the diffusion
denoising process. Specifically, we propose a reinforcement learning framework,
which establishes a Markov Decision Process that dynamically adjusts
hyperparameters across denoising steps, integrating editing objectives into a
reward function. The method achieves time efficiency through proximal policy
optimization while maintaining optimal hyperparameter configurations.
Experiments demonstrate significant reduction in search time and computational
overhead compared to existing brute-force approaches, advancing the practical
deployment of a diffusion-based image editing framework in the real world.

</details>


### [51] [Synthetic-to-Real Object Detection using YOLOv11 and Domain Randomization Strategies](https://arxiv.org/abs/2509.15045)
*Luisa Torquato Niño,Hamza A. A. Gardi*

Main category: cs.CV

TL;DR: 该研究通过多样化的合成数据和数据增强训练YOLOv11模型，成功缩小了合成到真实的领域差距，最佳模型在Kaggle竞赛中达到0.910的mAP@50。


<details>
  <summary>Details</summary>
Motivation: 解决合成到真实的领域差距问题，探索仅使用合成数据训练对象检测模型的可行性。

Method: 该方法包括使用合成数据和领域随机化策略训练YOLOv11模型，通过数据增强、数据集组合和模型缩放的广泛实验，以及通过视觉检查和定量评估来指导模型开发。

Result: 最佳配置的YOLOv11l模型在官方Kaggle竞赛的隐藏测试集上实现了0.910的mAP@50分数。

Conclusion: 该研究表明，通过增加合成数据集的多样性和精心调整的数据增强，可以有效缩小合成到真实的领域差距。最佳配置的YOLOv11l模型在官方Kaggle竞赛的隐藏测试集上达到了0.910的mAP@50，展示了仅使用合成数据进行训练的潜力，但也指出了在完全捕捉真实世界变异性方面的挑战。

Abstract: This paper addresses the synthetic-to-real domain gap in object detection,
focusing on training a YOLOv11 model to detect a specific object (a soup can)
using only synthetic data and domain randomization strategies. The methodology
involves extensive experimentation with data augmentation, dataset composition,
and model scaling. While synthetic validation metrics were consistently high,
they proved to be poor predictors of real-world performance. Consequently,
models were also evaluated qualitatively, through visual inspection of
predictions, and quantitatively, on a manually labeled real-world test set, to
guide development. Final mAP@50 scores were provided by the official Kaggle
competition. Key findings indicate that increasing synthetic dataset diversity,
specifically by including varied perspectives and complex backgrounds, combined
with carefully tuned data augmentation, were crucial in bridging the domain
gap. The best performing configuration, a YOLOv11l model trained on an expanded
and diverse dataset, achieved a final mAP@50 of 0.910 on the competition's
hidden test set. This result demonstrates the potential of a synthetic-only
training approach while also highlighting the remaining challenges in fully
capturing real-world variability.

</details>


### [52] [Transplant-Ready? Evaluating AI Lung Segmentation Models in Candidates with Severe Lung Disease](https://arxiv.org/abs/2509.15083)
*Jisoo Lee,Michael R. Harowicz,Yuwen Chen,Hanxue Gu,Isaac S. Alderete,Lin Li,Maciej A. Mazurowski,Matthew G. Hartwig*

Main category: cs.CV

TL;DR: Unet-R231在肺部分割中表现最佳，但所有模型在严重病例中表现下降，需进一步微调。


<details>
  <summary>Details</summary>
Motivation: 评估公开可用的基于深度学习的肺部分割模型在移植适合患者中的性能，以确定它们在不同疾病严重程度、病理类别和肺侧的表现，并识别影响其在肺移植术前规划中使用的局限性。

Method: 这项回顾性研究包括32名患者，共3645张2D轴向切片。使用三种深度学习模型（Unet-R231、TotalSegmentator、MedSAM）进行肺部分割，并通过定量指标（体积相似性、Dice相似系数、Hausdorff距离）和定性测量（四点临床可接受性量表）评估性能。

Result: Unet-R231在总体上、不同严重程度和病理类别中均优于TotalSegmentator和MedSAM（p<0.05）。所有模型在轻度至中度至重度病例中表现显著下降，特别是在体积相似性方面（p<0.05），但在肺侧或病理类型之间无显著差异。

Conclusion: Unet-R231在评估的模型中提供了最准确的自动肺部分割，TotalSegmentator紧随其后，但它们在中等至严重病例中的表现显著下降，强调了在严重病理背景下需要专门的模型微调。

Abstract: This study evaluates publicly available deep-learning based lung segmentation
models in transplant-eligible patients to determine their performance across
disease severity levels, pathology categories, and lung sides, and to identify
limitations impacting their use in preoperative planning in lung
transplantation. This retrospective study included 32 patients who underwent
chest CT scans at Duke University Health System between 2017 and 2019 (total of
3,645 2D axial slices). Patients with standard axial CT scans were selected
based on the presence of two or more lung pathologies of varying severity. Lung
segmentation was performed using three previously developed deep learning
models: Unet-R231, TotalSegmentator, MedSAM. Performance was assessed using
quantitative metrics (volumetric similarity, Dice similarity coefficient,
Hausdorff distance) and a qualitative measure (four-point clinical
acceptability scale). Unet-R231 consistently outperformed TotalSegmentator and
MedSAM in general, for different severity levels, and pathology categories
(p<0.05). All models showed significant performance declines from mild to
moderate-to-severe cases, particularly in volumetric similarity (p<0.05),
without significant differences among lung sides or pathology types. Unet-R231
provided the most accurate automated lung segmentation among evaluated models
with TotalSegmentator being a close second, though their performance declined
significantly in moderate-to-severe cases, emphasizing the need for specialized
model fine-tuning in severe pathology contexts.

</details>


### [53] [RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation](https://arxiv.org/abs/2509.15212)
*Yuming Jiang,Siteng Huang,Shengke Xue,Yaxi Zhao,Jun Cen,Sicong Leng,Kehan Li,Jiayan Guo,Kexiang Wang,Mingxiu Chen,Fan Wang,Deli Zhao,Xin Li*

Main category: cs.CV

TL;DR: RynnVLA-001通过两阶段视频生成预训练和ActionVAE，显著提升VLA模型性能，在机器人任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 旨在通过大规模视频生成预训练和动作表示增强，提升视觉-语言-动作（VLA）模型在下游机器人任务中的表现。

Method: 提出两阶段预训练方法：第一阶段（Ego-Centric Video Generative Pretraining）基于1200万自我中心操作视频训练图像到视频模型；第二阶段（Human-Centric Trajectory-Aware Modeling）联合预测未来关键点轨迹。同时引入ActionVAE压缩动作序列。

Result: 在相同下游机器人数据集上微调后，RynnVLA-001性能优于现有最佳基线。

Conclusion: RynnVLA-001通过两阶段预训练方法（Ego-Centric Video Generative Pretraining和Human-Centric Trajectory-Aware Modeling）及ActionVAE的引入，显著提升了VLA模型的性能，在机器人任务中表现优于现有基线。

Abstract: This paper presents RynnVLA-001, a vision-language-action(VLA) model built
upon large-scale video generative pretraining from human demonstrations. We
propose a novel two-stage pretraining methodology. The first stage, Ego-Centric
Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric
manipulation videos to predict future frames conditioned on an initial frame
and a language instruction. The second stage, Human-Centric Trajectory-Aware
Modeling, extends this by jointly predicting future keypoint trajectories,
thereby effectively bridging visual frame prediction with action prediction.
Furthermore, to enhance action representation, we propose ActionVAE, a
variational autoencoder that compresses sequences of actions into compact
latent embeddings, reducing the complexity of the VLA output space. When
finetuned on the same downstream robotics datasets, RynnVLA-001 achieves
superior performance over state-of-the-art baselines, demonstrating that the
proposed pretraining strategy provides a more effective initialization for VLA
models.

</details>


### [54] [OmniSegmentor: A Flexible Multi-Modal Learning Framework for Semantic Segmentation](https://arxiv.org/abs/2509.15096)
*Bo-Wen Yin,Jiao-Long Cao,Xuying Zhang,Yuming Chen,Ming-Ming Cheng,Qibin Hou*

Main category: cs.CV

TL;DR: 本文提出OmniSegmentor，一个创新的多模态学习框架，通过构建ImageNeXt数据集和高效预训练方式，首次实现通用多模态预训练，显著提升模型性能并在多个数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前多模态线索在表示学习中的优势已被证实，但针对多种视觉模态的灵活预训练与微调流程尚未得到充分探索。

Method: 基于ImageNet构建了一个大规模多模态预训练数据集ImageNeXt，包含五种流行的视觉模态，并提出了一种高效的预训练方式，使模型能够编码不同模态信息。

Result: OmniSegmentor在多个多模态语义分割数据集（如NYU Depthv2、EventScape、MFNet等）上取得了新的最先进记录。

Conclusion: OmniSegmentor首次提出了一个通用的多模态预训练框架，显著提升了模型在各种场景下的感知能力，并在多个多模态语义分割数据集上达到了新的最先进水平。

Abstract: Recent research on representation learning has proved the merits of
multi-modal clues for robust semantic segmentation. Nevertheless, a flexible
pretrain-and-finetune pipeline for multiple visual modalities remains
unexplored. In this paper, we propose a novel multi-modal learning framework,
termed OmniSegmentor. It has two key innovations: 1) Based on ImageNet, we
assemble a large-scale dataset for multi-modal pretraining, called ImageNeXt,
which contains five popular visual modalities. 2) We provide an efficient
pretraining manner to endow the model with the capacity to encode different
modality information in the ImageNeXt. For the first time, we introduce a
universal multi-modal pretraining framework that consistently amplifies the
model's perceptual capabilities across various scenarios, regardless of the
arbitrary combination of the involved modalities. Remarkably, our OmniSegmentor
achieves new state-of-the-art records on a wide range of multi-modal semantic
segmentation datasets, including NYU Depthv2, EventScape, MFNet, DeLiVER,
SUNRGBD, and KITTI-360.

</details>


### [55] [Out-of-Sight Trajectories: Tracking, Fusion, and Prediction](https://arxiv.org/abs/2509.15219)
*Haichao Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: 本研究提出一种新任务OST，预测视线外物体的无噪声轨迹，通过视觉-定位去噪模块在无监督下有效去噪，性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖完整且无噪声的观测数据，忽视了视线外物体和传感器噪声的挑战，导致安全隐患和预测不可靠。

Method: 通过增强的视觉-定位去噪模块，利用相机标定建立视觉-定位映射，以无监督方式有效去噪噪声传感器数据。

Result: 在Vi-Fi和JRDB数据集上，我们的方法在轨迹去噪和预测方面均达到最先进性能，显著超越先前基线。

Conclusion: 本研究首次提出将视觉-定位投影整合用于去噪不可见物体的传感器轨迹，为未来研究奠定了基础。代码和预处理数据集已开源。

Abstract: Trajectory prediction is a critical task in computer vision and autonomous
systems, playing a key role in autonomous driving, robotics, surveillance, and
virtual reality. Existing methods often rely on complete and noise-free
observational data, overlooking the challenges associated with out-of-sight
objects and the inherent noise in sensor data caused by limited camera
coverage, obstructions, and the absence of ground truth for denoised
trajectories. These limitations pose safety risks and hinder reliable
prediction in real-world scenarios. In this extended work, we present
advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the
noise-free visual trajectories of out-of-sight objects using noisy sensor data.
Building on our previous research, we broaden the scope of Out-of-Sight
Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending
its applicability to autonomous driving, robotics, surveillance, and virtual
reality. Our enhanced Vision-Positioning Denoising Module leverages camera
calibration to establish a vision-positioning mapping, addressing the lack of
visual references, while effectively denoising noisy sensor data in an
unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB
datasets, our approach achieves state-of-the-art performance in both trajectory
denoising and prediction, significantly surpassing previous baselines.
Additionally, we introduce comparisons with traditional denoising methods, such
as Kalman filtering, and adapt recent trajectory prediction models to our task,
providing a comprehensive benchmark. This work represents the first initiative
to integrate vision-positioning projection for denoising noisy sensor
trajectories of out-of-sight agents, paving the way for future advances. The
code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST

</details>


### [56] [RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes](https://arxiv.org/abs/2509.15123)
*Fang Li,Hao Zhang,Narendra Ahuja*

Main category: cs.CV

TL;DR: 提出了一种仅需单个RGB视频即可高效优化动态场景相机参数的新方法，通过三个关键组件和两阶段策略实现，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: COLMAP在动态场景中依赖耗时和需要地面真实运动掩码，且现有改进方法通常依赖额外监督信息（如焦距、3D点云等），这些信息在普通RGB视频中难以获取。

Method: 提出了三个关键组件：(1) 基于补丁的跟踪滤波器，(2) 异常感知联合优化，(3) 两阶段优化策略。

Result: 在多个真实和合成数据集上的实验表明，该方法在相机参数估计的效率和准确性上优于现有方法。

Conclusion: 该方法在仅使用单个RGB视频作为监督的情况下，能够更高效、更准确地估计动态场景中的相机参数，并通过多种数据集验证了其优越性。

Abstract: Although COLMAP has long remained the predominant method for camera parameter
optimization in static scenes, it is constrained by its lengthy runtime and
reliance on ground truth (GT) motion masks for application to dynamic scenes.
Many efforts attempted to improve it by incorporating more priors as
supervision such as GT focal length, motion masks, 3D point clouds, camera
poses, and metric depth, which, however, are typically unavailable in casually
captured RGB videos. In this paper, we propose a novel method for more accurate
and efficient camera parameter optimization in dynamic scenes solely supervised
by a single RGB video. Our method consists of three key components: (1)
Patch-wise Tracking Filters, to establish robust and maximally sparse
hinge-like relations across the RGB video. (2) Outlier-aware Joint
Optimization, for efficient camera parameter optimization by adaptive
down-weighting of moving outliers, without reliance on motion priors. (3) A
Two-stage Optimization Strategy, to enhance stability and optimization speed by
a trade-off between the Softplus limits and convex minima in losses. We
visually and numerically evaluate our camera estimates. To further validate
accuracy, we feed the camera estimates into a 4D reconstruction method and
assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform
experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics)
and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates
camera parameters more efficiently and accurately with a single RGB video as
the only supervision.

</details>


### [57] [MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label Augmentation](https://arxiv.org/abs/2509.15154)
*Gengliang Li,Rongyu Chen,Bin Li,Linlin Yang,Guodong Ding*

Main category: cs.CV

TL;DR: MEDFACT-R1是一个两阶段框架，通过SFT和GRPO提升医学模型的事实准确性，实验显示显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决医学视觉语言模型在事实一致性和可靠推理方面的关键挑战。

Method: 两阶段框架：第一阶段使用伪标签监督微调（SFT）引入外部事实知识；第二阶段应用组相对策略优化（GRPO）并设计四种事实奖励信号以促进自洽推理。

Result: 在三个公共医学QA基准测试中，MEDFACT-R1相比之前的最先进方法绝对提升了22.5%的事实准确性。

Conclusion: MEDFACT-R1通过结合外部知识基础与强化学习，显著提升了医学视觉语言模型的事实准确性和可靠推理能力，代码已开源。

Abstract: Ensuring factual consistency and reliable reasoning remains a critical
challenge for medical vision-language models. We introduce MEDFACT-R1, a
two-stage framework that integrates external knowledge grounding with
reinforcement learning to improve the factual medical reasoning. The first
stage uses pseudo-label supervised fine-tuning (SFT) to incorporate external
factual expertise; while the second stage applies Group Relative Policy
Optimization (GRPO) with four tailored factual reward signals to encourage
self-consistent reasoning. Across three public medical QA benchmarks,
MEDFACT-R1 delivers up to 22.5% absolute improvement in factual accuracy over
previous state-of-the-art methods. Ablation studies highlight the necessity of
pseudo-label SFT cold start and validate the contribution of each GRPO reward,
underscoring the synergy between knowledge grounding and RL-driven reasoning
for trustworthy medical AI. Codes are released at
https://github.com/Garfieldgengliang/MEDFACT-R1.

</details>


### [58] [AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt](https://arxiv.org/abs/2509.15159)
*Saket S. Chaturvedi,Gaurav Bagwe,Lan Zhang,Xiaoyong Yuan*

Main category: cs.CV

TL;DR: AIP攻击通过操纵指令提示来影响RAG输出，揭示了RAG系统中的新漏洞，攻击成功率高达95.23%。


<details>
  <summary>Details</summary>
Motivation: 现有RAG攻击主要依赖操纵用户查询，这在实践中往往不可行，因此研究转向更现实和隐蔽的向量：指令提示。

Method: 提出了一种基于遗传算法的联合优化方法，用于通过平衡攻击成功率、清洁任务效用和隐蔽性来演化对抗提示。

Result: 实验结果表明，AIP在保留良性功能的同时，攻击成功率高达95.23%。

Conclusion: 研究发现，AIP攻击揭示了RAG系统中一个关键且先前被忽视的脆弱性，强调了重新评估共享指令提示的必要性。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
retrieving relevant documents from external sources to improve factual accuracy
and verifiability. However, this reliance introduces new attack surfaces within
the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have
exposed such vulnerabilities, they largely rely on manipulating user queries,
which is often infeasible in practice due to fixed or protected user inputs.
This narrow focus overlooks a more realistic and stealthy vector: instructional
prompts, which are widely reused, publicly shared, and rarely audited. Their
implicit trust makes them a compelling target for adversaries to manipulate RAG
behavior covertly.
  We introduce a novel attack for Adversarial Instructional Prompt (AIP) that
exploits adversarial instructional prompts to manipulate RAG outputs by subtly
altering retrieval behavior. By shifting the attack surface to the
instructional prompts, AIP reveals how trusted yet seemingly benign interface
components can be weaponized to degrade system integrity. The attack is crafted
to achieve three goals: (1) naturalness, to evade user detection; (2) utility,
to encourage use of prompts; and (3) robustness, to remain effective across
diverse query variations. We propose a diverse query generation strategy that
simulates realistic linguistic variation in user queries, enabling the
discovery of prompts that generalize across paraphrases and rephrasings.
Building on this, a genetic algorithm-based joint optimization is developed to
evolve adversarial prompts by balancing attack success, clean-task utility, and
stealthiness. Experimental results show that AIP achieves up to 95.23% ASR
while preserving benign functionality. These findings uncover a critical and
previously overlooked vulnerability in RAG systems, emphasizing the need to
reassess the shared instructional prompts.

</details>


### [59] [A Race Bias Free Face Aging Model for Reliable Kinship Verification](https://arxiv.org/abs/2509.15177)
*Ali Nazari,Bardiya Kariminia,Mohsen Ebrahimi Moghaddam*

Main category: cs.CV

TL;DR: RA-GAN是一种无种族偏见的面部老化GAN模型，通过生成同年龄照片显著提高了亲属关系验证的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于父母与孩子的照片存在年龄差距，且同年龄照片通常不可得，加之现有面部老化模型存在种族偏见，影响了照片的相似性。

Method: 提出了RA-GAN模型，包含RACEpSp和特征混合器两个新模块，用于生成无种族偏见的图像。

Result: RA-GAN在所有年龄组的种族准确性上平均优于SAM-GAN 13.14%，在60岁以上年龄组优于CUSP-GAN 9.1%。亲属关系验证的准确性在不同关系上均有提升。

Conclusion: RA-GAN在保持个体身份和种族准确性方面优于SAM-GAN和CUSP-GAN，特别是在60岁以上年龄组。将父母和孩子的图像转换为同一年龄可以显著提高亲属关系验证的准确性。

Abstract: The age gap in kinship verification addresses the time difference between the
photos of the parent and the child. Moreover, their same-age photos are often
unavailable, and face aging models are racially biased, which impacts the
likeness of photos. Therefore, we propose a face aging GAN model, RA-GAN,
consisting of two new modules, RACEpSp and a feature mixer, to produce racially
unbiased images. The unbiased synthesized photos are used in kinship
verification to investigate the results of verifying same-age parent-child
images. The experiments demonstrate that our RA-GAN outperforms SAM-GAN on an
average of 13.14\% across all age groups, and CUSP-GAN in the 60+ age group by
9.1\% in terms of racial accuracy. Moreover, RA-GAN can preserve subjects'
identities better than SAM-GAN and CUSP-GAN across all age groups.
Additionally, we demonstrate that transforming parent and child images from the
KinFaceW-I and KinFaceW-II datasets to the same age can enhance the
verification accuracy across all age groups. The accuracy increases with our
RA-GAN for the kinship relationships of father-son and father-daughter,
mother-son, and mother-daughter, which are 5.22, 5.12, 1.63, and 0.41,
respectively, on KinFaceW-I. Additionally, the accuracy for the relationships
of father-daughter, father-son, and mother-son is 2.9, 0.39, and 1.6 on
KinFaceW-II, respectively. The code is available
at~\href{https://github.com/bardiya2254kariminia/An-Age-Transformation-whitout-racial-bias-for-Kinship-verification}{Github}

</details>


### [60] [Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding](https://arxiv.org/abs/2509.15178)
*Zaiquan Yang,Yuhao Liu,Gerhard Hancke,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 本文利用MLLM提出了一种零样本STVG框架，通过DSTH和TAS策略优化模型注意力，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 探索MLLM在STVG任务中的零样本解决方案，解决其因未能充分整合文本查询线索而导致的次优接地问题。

Method: 提出了基于MLLM的零样本框架，包括分解时空高亮（DSTH）和时序增强组装（TAS）策略，通过解耦查询并学习潜在变量来优化模型注意力。

Result: 在三个常见STVG基准测试中超越了现有最先进方法。

Conclusion: 本文提出的基于MLLM的零样本框架在STVG任务中表现出色，超越了现有最先进方法，并通过DSTH和TAS策略有效提升了模型的推理能力。

Abstract: Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal
tube of a video, as specified by the input text query. In this paper, we
utilize multimodal large language models (MLLMs) to explore a zero-shot
solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to
dynamically assign special tokens, referred to as \textit{grounding tokens},
for grounding the text query; and (2) MLLMs often suffer from suboptimal
grounding due to the inability to fully integrate the cues in the text query
(\textit{e.g.}, attributes, actions) for inference. Based on these insights, we
propose a MLLM-based zero-shot framework for STVG, which includes novel
decomposed spatio-temporal highlighting (DSTH) and temporal-augmented
assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH
strategy first decouples the original query into attribute and action
sub-queries for inquiring the existence of the target both spatially and
temporally. It then uses a novel logit-guided re-attention (LRA) module to
learn latent variables as spatial and temporal prompts, by regularizing token
predictions for each sub-query. These prompts highlight attribute and action
cues, respectively, directing the model's attention to reliable spatial and
temporal related visual regions. In addition, as the spatial grounding by the
attribute sub-query should be temporally consistent, we introduce the TAS
strategy to assemble the predictions using the original video frames and the
temporal-augmented frames as inputs to help improve temporal consistency. We
evaluate our method on various MLLMs, and show that it outperforms SOTA methods
on three common STVG benchmarks.
  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.

</details>


### [61] [Maize Seedling Detection Dataset (MSDD): A Curated High-Resolution RGB Dataset for Seedling Maize Detection and Benchmarking with YOLOv9, YOLO11, YOLOv12 and Faster-RCNN](https://arxiv.org/abs/2509.15181)
*Dewi Endah Kharismawati,Toni Kazic*

Main category: cs.CV

TL;DR: MSDD数据集通过高质量图像和多种模型测试，显著提升了玉米幼苗检测效率，为精准农业奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决传统玉米幼苗检测方法劳动密集且易出错的问题，推动精准农业的自动化监测。

Method: 使用YOLO11和YOLOv9等模型进行检测，重点关注V4-V6生长阶段和天顶视角。

Result: 单株检测精度高达0.984，召回率0.873；多株检测因样本稀少和外观不规则而效果较差。

Conclusion: MSDD数据集为玉米幼苗检测提供了高质量的基础，支持实时决策和精准农业的发展。

Abstract: Accurate maize seedling detection is crucial for precision agriculture, yet
curated datasets remain scarce. We introduce MSDD, a high-quality aerial image
dataset for maize seedling stand counting, with applications in early-season
crop monitoring, yield prediction, and in-field management. Stand counting
determines how many plants germinated, guiding timely decisions such as
replanting or adjusting inputs. Traditional methods are labor-intensive and
error-prone, while computer vision enables efficient, accurate detection. MSDD
contains three classes-single, double, and triple plants-capturing diverse
growth stages, planting setups, soil types, lighting conditions, camera angles,
and densities, ensuring robustness for real-world use. Benchmarking shows
detection is most reliable during V4-V6 stages and under nadir views. Among
tested models, YOLO11 is fastest, while YOLOv9 yields the highest accuracy for
single plants. Single plant detection achieves precision up to 0.984 and recall
up to 0.873, but detecting doubles and triples remains difficult due to rarity
and irregular appearance, often from planting errors. Class imbalance further
reduces accuracy in multi-plant detection. Despite these challenges, YOLO11
maintains efficient inference at 35 ms per image, with an additional 120 ms for
saving outputs. MSDD establishes a strong foundation for developing models that
enhance stand counting, optimize resource allocation, and support real-time
decision-making. This dataset marks a step toward automating agricultural
monitoring and advancing precision agriculture.

</details>


### [62] [Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation](https://arxiv.org/abs/2509.15185)
*Xiaoyu Yue,Zidong Wang,Yuqing Wang,Wenlong Zhang,Xihui Liu,Wanli Ouyang,Lei Bai,Luping Zhou*

Main category: cs.CV

TL;DR: ST-AR框架通过自监督目标提升自回归模型的图像理解和生成质量，FID显著提升。


<details>
  <summary>Details</summary>
Motivation: 研究自回归模型在视觉领域的应用机制，解决其在高层次视觉语义学习中的局限性。

Method: 提出了自回归模型的自引导训练框架ST-AR，通过引入自监督目标解决局部依赖、语义不一致和空间不变性缺陷。

Result: ST-AR显著提升了LlamaGen-L和LlamaGen-XL的生成质量，FID分别提升了42%和49%。

Conclusion: ST-AR框架通过引入自监督目标，显著提升了自回归模型的图像理解能力和生成质量。

Abstract: Recent studies have demonstrated the importance of high-quality visual
representations in image generation and have highlighted the limitations of
generative models in image understanding. As a generative paradigm originally
designed for natural language, autoregressive models face similar challenges.
In this work, we present the first systematic investigation into the mechanisms
of applying the next-token prediction paradigm to the visual domain. We
identify three key properties that hinder the learning of high-level visual
semantics: local and conditional dependence, inter-step semantic inconsistency,
and spatial invariance deficiency. We show that these issues can be effectively
addressed by introducing self-supervised objectives during training, leading to
a novel training framework, Self-guided Training for AutoRegressive models
(ST-AR). Without relying on pre-trained representation models, ST-AR
significantly enhances the image understanding ability of autoregressive models
and leads to improved generation quality. Specifically, ST-AR brings
approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for
LlamaGen-XL, while maintaining the same sampling strategy.

</details>


### [63] [Geometric Image Synchronization with Deep Watermarking](https://arxiv.org/abs/2509.15208)
*Pierre Fernandez,Tomáš Souček,Nikola Jovanović,Hady Elsahar,Sylvestre-Alvise Rebuffi,Valeriu Lacatusu,Tuan Tran,Alexandre Mourachko*

Main category: cs.CV

TL;DR: SyncSeal是一种新型水印方法，通过端到端训练的嵌入器和提取器网络增强现有水印对几何变换的鲁棒性，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 图像同步是估计和反转应用于图像的几何变换的任务，现有水印方法对几何变换的鲁棒性不足，因此需要一种新的方法来增强其鲁棒性。

Method: SyncSeal依赖于一个嵌入器网络和一个提取器网络，两者通过端到端训练以最小化预测与真实变换参数之间的误差，并结合鉴别器保持高感知质量。

Result: 实验证明SyncSeal在多种几何和值变换下能准确同步图像，并能有效提升现有水印方法对几何变换的抵抗能力。

Conclusion: SyncSeal是一种有效的水印方法，能够增强现有水印技术对几何变换的鲁棒性，通过端到端训练的嵌入器和提取器网络实现准确的图像同步。

Abstract: Synchronization is the task of estimating and inverting geometric
transformations (e.g., crop, rotation) applied to an image. This work
introduces SyncSeal, a bespoke watermarking method for robust image
synchronization, which can be applied on top of existing watermarking methods
to enhance their robustness against geometric transformations. It relies on an
embedder network that imperceptibly alters images and an extractor network that
predicts the geometric transformation to which the image was subjected. Both
networks are end-to-end trained to minimize the error between the predicted and
ground-truth parameters of the transformation, combined with a discriminator to
maintain high perceptual quality. We experimentally validate our method on a
wide variety of geometric and valuemetric transformations, demonstrating its
effectiveness in accurately synchronizing images. We further show that our
synchronization can effectively upgrade existing watermarking methods to
withstand geometric transformations to which they were previously vulnerable.

</details>


### [64] [Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model](https://arxiv.org/abs/2509.15220)
*Fangjinhua Wang,Qingshan Xu,Yew-Soon Ong,Marc Pollefeys*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的MVS框架，通过条件扩散过程和高效网络设计，实现了高效且高性能的3D重建。


<details>
  <summary>Details</summary>
Motivation: 为了提高多视图立体（MVS）的计算效率，并利用扩散模型在生成任务中的成功，将扩散模型引入MVS中。

Method: 提出了一种基于扩散模型的多视图立体（MVS）框架，包括条件编码器、轻量级2D U-Net与卷积GRU结合的扩散网络，以及基于置信度的采样策略。

Result: DiffMVS在运行时间和GPU内存上具有竞争力，CasDiffMVS在DTU、Tanks & Temples和ETH3D数据集上达到了最先进的性能。

Conclusion: 提出的DiffMVS和CasDiffMVS方法在效率和性能上均表现出色，CasDiffMVS在多个数据集上达到了最先进的性能。

Abstract: To reconstruct the 3D geometry from calibrated images, learning-based
multi-view stereo (MVS) methods typically perform multi-view depth estimation
and then fuse depth maps into a mesh or point cloud. To improve the
computational efficiency, many methods initialize a coarse depth map and then
gradually refine it in higher resolutions. Recently, diffusion models achieve
great success in generation tasks. Starting from a random noise, diffusion
models gradually recover the sample with an iterative denoising process. In
this paper, we propose a novel MVS framework, which introduces diffusion models
in MVS. Specifically, we formulate depth refinement as a conditional diffusion
process. Considering the discriminative characteristic of depth estimation, we
design a condition encoder to guide the diffusion process. To improve
efficiency, we propose a novel diffusion network combining lightweight 2D U-Net
and convolutional GRU. Moreover, we propose a novel confidence-based sampling
strategy to adaptively sample depth hypotheses based on the confidence
estimated by diffusion model. Based on our novel MVS framework, we propose two
novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive
performance with state-of-the-art efficiency in run-time and GPU memory.
CasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and
ETH3D. Code is available at: https://github.com/cvg/diffmvs.

</details>


### [65] [ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data](https://arxiv.org/abs/2509.15221)
*Zhaoyang Liu,JingJing Xie,Zichen Ding,Zehao Li,Bowen Yang,Zhenyu Wu,Xuehui Wang,Qiushi Sun,Shi Liu,Weiyun Wang,Shenglong Ye,Qingyun Li,Zeyue Tian,Gen Luo,Xiangyu Yue,Biqing Qi,Kai Chen,Bowen Zhou,Yu Qiao,Qifeng Chen,Wenhai Wang*

Main category: cs.CV

TL;DR: ScaleCUA通过大规模数据集和跨平台训练，显著提升了计算机使用代理的性能，并开源数据、模型和代码。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模开源计算机使用数据和基础模型，视觉语言模型（VLMs）在计算机使用代理（CUAs）中的应用进展受限。

Method: 通过封闭循环管道结合自动化代理与人类专家构建大规模数据集，训练ScaleCUA模型。

Result: ScaleCUA在多个基准测试中表现出色（如WebArena-Lite-v2 +26.6，ScreenSpot-Pro +10.7），并创下新记录（如MMBench-GUI L1-Hard 94.4%）。

Conclusion: ScaleCUA展示了数据驱动扩展在通用计算机使用代理中的强大潜力，并提供了数据集、模型和代码以推动未来研究。

Abstract: Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that
operate GUIs autonomously, showing great potential, yet progress is limited by
the lack of large-scale, open-source computer use data and foundation models.
In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It
offers a large-scale dataset spanning 6 operating systems and 3 task domains,
built via a closed-loop pipeline uniting automated agents with human experts.
Trained on this scaled-up data, ScaleCUA can operate seamlessly across
platforms. Specifically, it delivers strong gains over baselines (+26.6 on
WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art
results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on
WebArena-Lite-v2). These findings underscore the power of data-driven scaling
for general-purpose computer use agents. We will release data, models, and code
to advance future research: https://github.com/OpenGVLab/ScaleCUA.

</details>


### [66] [Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation](https://arxiv.org/abs/2509.15224)
*Luca Bartolomei,Enrico Mannocci,Fabio Tosi,Matteo Poggi,Stefano Mattoccia*

Main category: cs.CV

TL;DR: 通过跨模态蒸馏和视觉基础模型，论文解决了事件相机深度估计中缺乏标注数据的问题，实现了高性能且无需昂贵标注。


<details>
  <summary>Details</summary>
Motivation: 事件相机在高速运动和光照变化强烈的环境中表现优异，但缺乏带密集深度标注的大规模数据集，限制了基于学习的单目深度估计方法的发展。

Method: 提出了一种跨模态蒸馏范式，利用视觉基础模型（VFM）生成密集代理标签，并探索了两种VFM架构：标准版（如Depth Anything v2）和一种新颖的循环架构。

Result: 在合成和真实数据集上的评估表明，跨模态蒸馏范式性能接近全监督方法，且基于VFM的模型达到了最先进水平。

Conclusion: 该论文提出的跨模态蒸馏范式及基于视觉基础模型（VFM）的方法在无需昂贵深度标注的情况下，实现了与全监督方法竞争的性能，并在单目事件相机的深度估计中达到最先进水平。

Abstract: Event cameras capture sparse, high-temporal-resolution visual information,
making them particularly suitable for challenging environments with high-speed
motion and strongly varying lighting conditions. However, the lack of large
datasets with dense ground-truth depth annotations hinders learning-based
monocular depth estimation from event data. To address this limitation, we
propose a cross-modal distillation paradigm to generate dense proxy labels
leveraging a Vision Foundation Model (VFM). Our strategy requires an event
stream spatially aligned with RGB frames, a simple setup even available
off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally,
we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2),
or deriving from it a novel recurrent architecture to infer depth from
monocular event cameras. We evaluate our approach with synthetic and real-world
datasets, demonstrating that i) our cross-modal paradigm achieves competitive
performance compared to fully supervised methods without requiring expensive
depth annotations, and ii) our VFM-based models achieve state-of-the-art
performance.

</details>


### [67] [Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2509.15225)
*Silvio Mazzucco,Carl Persson,Mattia Segu,Pier Luigi Dovesi,Federico Tombari,Luc Van Gool,Matteo Poggi*

Main category: cs.CV

TL;DR: VocAlign是一种专为开放词汇语义分割设计的源自由域适应框架，通过学生-教师范式和词汇对齐策略提升性能，同时使用LoRA和Top-K机制优化计算效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决开放词汇语义分割中源自由域适应的挑战，特别是伪标签生成和计算效率的问题。

Method: 采用学生-教师范式，结合词汇对齐策略和Low-Rank Adaptation（LoRA）进行微调，同时引入Top-K类选择机制以减少内存需求。

Result: 在CityScapes数据集上实现了6.11 mIoU的显著提升，并在零样本分割基准测试中表现出色。

Conclusion: VocAlign在开放词汇语义分割中为源自由域适应设立了新标准，特别是在CityScapes数据集上实现了6.11 mIoU的显著提升。

Abstract: We introduce VocAlign, a novel source-free domain adaptation framework
specifically designed for VLMs in open-vocabulary semantic segmentation. Our
method adopts a student-teacher paradigm enhanced with a vocabulary alignment
strategy, which improves pseudo-label generation by incorporating additional
class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to
fine-tune the model, preserving its original capabilities while minimizing
computational overhead. In addition, we propose a Top-K class selection
mechanism for the student model, which significantly reduces memory
requirements while further improving adaptation performance. Our approach
achieves a notable 6.11 mIoU improvement on the CityScapes dataset and
demonstrates superior performance on zero-shot segmentation benchmarks, setting
a new standard for source-free adaptation in the open-vocabulary setting.

</details>


### [68] [Calibration-Aware Prompt Learning for Medical Vision-Language Models](https://arxiv.org/abs/2509.15226)
*Abhishek Basu,Fahad Shamshad,Ashshak Sharifdeen,Karthik Nandakumar,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: CalibPrompt是首个在提示调优中校准Med-VLMs的框架，通过优化提示和设计校准目标，显著提升校准性能而不影响准确性。


<details>
  <summary>Details</summary>
Motivation: Med-VLMs的置信度校准尚未充分探索，可能导致过度自信的错误，影响临床信任和决策可靠性。

Method: 提出CalibPrompt框架，通过优化少量可学习提示并结合校准目标（如平滑准确率对齐和角度分离损失）来校准Med-VLMs。

Result: 在四个公开Med-VLMs和五个医疗影像数据集上的实验表明，CalibPrompt显著改善了校准性能，且不影响清洁准确性。

Conclusion: CalibPrompt通过优化少量可学习提示并结合精心设计的校准目标，显著提升了Med-VLMs的校准性能，而不显著影响其清洁准确性。

Abstract: Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable
performance across diverse medical imaging tasks by leveraging large-scale
image-text pretraining. However, their confidence calibration is largely
unexplored, and so remains a significant challenge. As such, miscalibrated
predictions can lead to overconfident errors, undermining clinical trust and
decision-making reliability. To address this, we introduce CalibPrompt, the
first framework to calibrate Med-VLMs during prompt tuning. CalibPrompt
optimizes a small set of learnable prompts with carefully designed calibration
objectives under scarce labeled data regime. First, we study a regularizer that
attempts to align the smoothed accuracy with the predicted model confidences.
Second, we introduce an angular separation loss to maximize textual feature
proximity toward improving the reliability in confidence estimates of
multimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs
and five diverse medical imaging datasets reveal that CalibPrompt consistently
improves calibration without drastically affecting clean accuracy. Our code is
available at https://github.com/iabh1shekbasu/CalibPrompt.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [69] [Normalized Square Root: Sharper Matrix Factorization Bounds for Differentially Private Continual Counting](https://arxiv.org/abs/2509.14334)
*Monika Henzinger,Nikita P. Kalinin,Jalaj Upadhyay*

Main category: cs.DS

TL;DR: 本文改进了矩阵因子分解的上下界，显著缩小了差距，为私有训练算法提供了更精确的理论支持。


<details>
  <summary>Details</summary>
Motivation: 解决Google在生产级深度神经网络私有训练算法中使用的矩阵因子分解问题，改进现有上下界差距。

Method: 通过改进因子分解方法，提出新的显式因子分解方案。

Result: 将γ₂(M_count)的上下界差距缩小至0.14 + o(1)，γ_F(M_count)的上下界差距缩小至0.047 + o(1)。

Conclusion: 本文通过改进因子分解方法，显著缩小了γ₂(M_count)和γ_F(M_count)的上下界差距，分别降低至0.14 + o(1)和0.047 + o(1)。

Abstract: The factorization norms of the lower-triangular all-ones $n \times n$ matrix,
$\gamma_2(M_{count})$ and $\gamma_{F}(M_{count})$, play a central role in
differential privacy as they are used to give theoretical justification of the
accuracy of the only known production-level private training algorithm of deep
neural networks by Google. Prior to this work, the best known upper bound on
$\gamma_2(M_{count})$ was $1 + \frac{\log n}{\pi}$ by Mathias (Linear Algebra
and Applications, 1993), and the best known lower bound was $\frac{1}{\pi}(2 +
\log(\frac{2n+1}{3})) \approx 0.507 + \frac{\log n}{\pi}$ (Matou\v{s}ek,
Nikolov, Talwar, IMRN 2020), where $\log$ denotes the natural logarithm.
Recently, Henzinger and Upadhyay (SODA 2025) gave the first explicit
factorization that meets the bound of Mathias (1993) and asked whether there
exists an explicit factorization that improves on Mathias' bound. We answer
this question in the affirmative. Additionally, we improve the lower bound
significantly. More specifically, we show that $$
  0.701 + \frac{\log n}{\pi} + o(1) \;\leq\; \gamma_2(M_{count}) \;\leq\; 0.846
+ \frac{\log n}{\pi} + o(1). $$ That is, we reduce the gap between the upper
and lower bound to $0.14 + o(1)$.
  We also show that our factors achieve a better upper bound for
$\gamma_{F}(M_{count})$ compared to prior work, and we establish an improved
lower bound: $$
  0.701 + \frac{\log n}{\pi} + o(1) \;\leq\; \gamma_{F}(M_{count}) \;\leq\;
0.748 + \frac{\log n}{\pi} + o(1). $$ That is, the gap between the lower and
upper bound provided by our explicit factorization is $0.047 + o(1)$.

</details>


### [70] [Fast and Compact Sketch-Based Dynamic Connectivity](https://arxiv.org/abs/2509.14433)
*Quinten De Man,Qamber Jafri,Daniel Delayo,Evan T. West,Michael A. Bender,David Tench*

Main category: cs.DS

TL;DR: 论文提出了一种并行动态连通性算法和系统CUPCaKE，显著提升了密集图上的查询速度、更新吞吐量和内存效率，同时满足了三个性能目标。


<details>
  <summary>Details</summary>
Motivation: 现有的系统在密集图上最多只能同时满足三个性能目标中的两个，无法兼顾快速查询、高更新吞吐量和低内存使用。

Method: 采用了基于图素描技术的并行动态连通性算法，具有O(V log^3 V)的空间复杂度和O(log V/log log V)的查询复杂度。更新操作在O(log^2 V)深度和O(log^4 V)工作量的最坏情况下完成。

Result: 提出的CUPCaKE系统在密集图上比现有无损系统节省了一个数量级的内存，查询延迟为微秒级，更新吞吐量达到每秒数百万次。

Conclusion: 该论文提出了一种并行动态连通性算法和系统CUPCaKE，显著提升了在密集图上的查询速度、更新吞吐量和内存效率，同时满足了三个性能目标。

Abstract: We study the dynamic connectivity problem for massive, dense graphs. Our goal
is to build a system for dense graphs that simultaneously answers connectivity
queries quickly, maintains a fast update throughput, and a uses a small amount
of memory. Existing systems at best achieve two of these three performance
goals at once.
  We present a parallel dynamic connectivity algorithm using graph sketching
techniques that has space complexity $O(V \log^3 V)$ and query complexity
$O(\log V/\log\log V)$. Its updates are fast and parallel: in the worst case,
it performs updates in $O(\log^2 V)$ depth and $O(\log^4 V)$ work. For updates
which don't change the spanning forests maintained by our data structure, the
update complexity is $O(\log V)$ depth and $O(\log^2 V)$ work.
  We also present CUPCaKE (Compact Updating Parallel Connectivity and Sketching
Engine), a dynamic connectivity system based on our parallel algorithm. It uses
an order of magnitude less memory than the best lossless systems on dense graph
inputs, answers queries with microsecond latency, and ingests millions of
updates per second on dense graphs.

</details>


### [71] [Kronecker Powers, Orthogonal Vectors, and the Asymptotic Spectrum](https://arxiv.org/abs/2509.14489)
*Josh Alman,Baitian Li*

Main category: cs.DS

TL;DR: 论文应用Strassen理论优化深度-2线性电路设计，提出多项改进结果，包括电路构造和算法应用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决如何最优应用“再平衡”方法来设计深度-2线性电路的问题，并探索其在矩阵计算和算法中的应用潜力。

Method: 作者利用Strassen的渐近谱理论，特别是对偶定理，来设计和分析深度-2线性电路。他们还提出了“障碍”概念，并通过理论证明这些障碍的完备性。

Result: 论文提出了多项改进结果，包括：1) $N \times N$不相交矩阵的深度-2线性电路大小为$O(N^{1.2495})$；2) 强指数时间假说下的下界；3) 正交向量问题的确定性算法改进。

Conclusion: 该论文通过应用Strassen的渐近谱理论，优化了深度-2线性电路的构造，并展示了其在多个问题中的实际应用，如改进电路构造和算法时间复杂度。

Abstract: We study circuits for computing depth-2 linear transforms defined by
Kronecker power matrices. Recent works have improved on decades-old
constructions in this area using a new ''rebalancing'' approach [Alman, Guan
and Padaki, SODA'23; Sergeev'22], but it was unclear how to apply this approach
optimally.
  We find that Strassen's theory of asymptotic spectra can be applied to
capture the design of these circuits. In particular, in hindsight, we find that
the techniques of recent work on rebalancing were proving special cases of the
duality theorem, which is central to Strassen's theory. We carefully outline a
collection of ''obstructions'' to designing small depth-2 circuits using a
rebalancing approach, and apply Strassen's theory to show that our obstructions
are complete.
  Using this connection, combined with other algorithmic techniques, we give
new improved circuit constructions as well as other applications, including:
  - The $N \times N$ disjointness matrix has a depth-2 linear circuit of size
$O(N^{1.2495})$ over any field. This also yield smaller circuits for many
families of matrices using reductions to disjointness.
  - The Strong Exponential Time Hypothesis implies an $N^{1 + \Omega(1)}$ size
lower bound for depth-2 linear circuits computing the Walsh--Hadamard transform
(and the disjointness matrix with a technical caveat), and proving a $N^{1 +
\Omega(1)}$ depth-2 size lower bound would also imply breakthrough threshold
circuit lower bounds.
  - The Orthogonal Vectors (OV) problem in moderate dimension $d$ can be solved
in deterministic time $\tilde{O}(n \cdot 1.155^d)$, derandomizing an algorithm
of Nederlof and W\k{e}grzycki [STOC'21], and the counting problem can be solved
in time $\tilde{O}(n \cdot 1.26^d)$, improving an algorithm of Williams
[FOCS'24] which runs in time $\tilde{O}(n \cdot 1.35^d)$.

</details>


### [72] [Efficient Algorithms for Disjoint Shortest Paths Problem and its Extensions](https://arxiv.org/abs/2509.14588)
*Keerti Choudhary,Amit Kumar,Lakshay Saggi*

Main category: cs.DS

TL;DR: 本文提出高效算法解决加权有向图中顶点不相交最短路径问题，显著改进时间复杂度，并扩展到更一般的问题。


<details>
  <summary>Details</summary>
Motivation: 解决加权有向图中顶点不相交最短路径问题，改进现有算法的时间复杂度。

Method: 利用多项式代数结构和动态编程技术，在特征为二的域上进行高效评估。

Result: 提出了时间复杂度为O(mn log n)的算法，显著优于先前的O(m^5n)算法，并扩展技术到Min-2-DSP问题。

Conclusion: 本文提出了一种针对加权有向图中2-DSP问题的高效算法，显著改进了先前的时间复杂度，并扩展了技术到Min-2-DSP问题，提供了首个高效解决方案。

Abstract: We study the 2-Disjoint Shortest Paths (2-DSP) problem: given a directed
weighted graph and two terminal pairs $(s_1,t_1)$ and $(s_2,t_2)$, decide
whether there exist vertex-disjoint shortest paths between each pair.
  Building on recent advances in disjoint shortest paths for DAGs and
undirected graphs (Akmal et al. 2024), we present an $O(mn \log n)$ time
algorithm for this problem in weighted directed graphs that do not contain
negative or zero weight cycles. This algorithm presents a significant
improvement over the previously known $O(m^5n)$ time bound (Berczi et al.
2017). Our approach exploits the algebraic structure of polynomials that
enumerate shortest paths between terminal pairs. A key insight is that these
polynomials admit a recursive decomposition, enabling efficient evaluation via
dynamic programming over fields of characteristic two. Furthermore, we
demonstrate how to report the corresponding paths in $O(mn^2 \log n)$ time.
  In addition, we extend our techniques to a more general setting: given two
terminal pairs $(s_1, t_1)$ and $(s_2, t_2)$ in a directed graph, find minimum
possible number of vertex intersections between any shortest path from $s_1$ to
$t_1$ and $s_2$ to $t_2$. We call this the Minimum 2-Disjoint Shortest Paths
(Min-2-DSP) problem. We provide in this paper the first efficient algorithm for
this problem, including an $O(m^2 n^3)$ time algorithm for directed graphs with
positive edge weights, and an $O(m+n)$ time algorithm for DAGs and undirected
graphs. Moreover, if the number of intersecting vertices is at least one, we
show that it is possible to report the paths in the same $O(m+n)$ time. This is
somewhat surprising, as there is no known $o(mn)$ time algorithm for explicitly
reporting the paths if they are vertex disjoint, and is left as an open problem
in (Akmal et al. 2024).

</details>


### [73] [Streaming periodicity with mismatches, wildcards, and edits](https://arxiv.org/abs/2509.14898)
*Taha El Ghazi,Tatiana Starikovskaya*

Main category: cs.DS

TL;DR: 论文提出了一种高效检测噪声字符串周期性的流式算法，改进了现有方法并首次支持编辑距离。


<details>
  <summary>Details</summary>
Motivation: 研究如何高效检测带有噪声的字符串中的周期性，解决了现有方法在通配符和编辑距离下的局限性。

Method: 结合了Clifford等人的汉明距离草图技术和Charalampopoulos等人的结构描述方法，提出了一种新的流式算法。

Result: 提出的算法不仅比Ergün等人的方法更高效，还能处理包含通配符的字符串，并首次实现了在编辑距离下的两遍流式算法。

Conclusion: 该论文提出了一种更高效的流式算法，用于在汉明距离下检测字符串的周期性，并首次引入了在编辑距离下的两遍流式算法。

Abstract: In this work, we study the problem of detecting periodic trends in strings.
While detecting exact periodicity has been studied extensively, real-world data
is often noisy, where small deviations or mismatches occur between repetitions.
This work focuses on a generalized approach to period detection that
efficiently handles noise. Given a string $S$ of length $n$, the task is to
identify integers $p$ such that the prefix and the suffix of $S$, each of
length $n-p+1$, are similar under a given distance measure. Erg\"un et al.
[APPROX-RANDOM 2017] were the first to study this problem in the streaming
model under the Hamming distance. In this work, we combine, in a non-trivial
way, the Hamming distance sketch of Clifford et al. [SODA 2019] and the
structural description of the $k$-mismatch occurrences of a pattern in a text
by Charalampopoulos et al. [FOCS 2020] to present a more efficient streaming
algorithm for period detection under the Hamming distance. As a corollary, we
derive a streaming algorithm for detecting periods of strings which may contain
wildcards, a special symbol that match any character of the alphabet. Our
algorithm is not only more efficient than that of Erg\"un et al. [TCS 2020],
but it also operates without their assumption that the string must be free of
wildcards in its final characters. Additionally, we introduce the first
two-pass streaming algorithm for computing periods under the edit distance by
leveraging and extending the Bhattacharya-Kouck\'y's grammar decomposition
technique [STOC 2023].

</details>


### [74] [Fast and Optimal Incremental Parametric Procedure for the Densest Subgraph Problem: An Experimental Study](https://arxiv.org/abs/2509.14993)
*Dorit S. Hochbaum,Ayleen Irribarra-Cortés,Olivier Goldschmidt,Roberto Asín-Achá*

Main category: cs.DS

TL;DR: IPC算法在密集子图及相关单调比率问题上表现出色，优于现有精确和启发式方法。


<details>
  <summary>Details</summary>
Motivation: 传统的精确算法在计算和可扩展性方面存在局限性，导致采用更快但非最优的启发式方法。IPC算法旨在克服这些限制。

Method: 本文提出了增量参数化割（IPC）算法的首次实验研究，这是一种针对DSP及其他“单调比率问题”的精确方法。

Result: 实验结果表明，IPC不仅克服了以往精确方法的局限性，还在速度和解决方案质量上显著优于领先的启发式方法。

Conclusion: IPC算法被确立为一种快速、可扩展且最优的解决方案框架，适用于密集子图及相关单调比率问题。

Abstract: The Densest Subgraph Problem (DSP) is widely used to identify community
structures and patterns in networks such as bioinformatics and social networks.
While solvable in polynomial time, traditional exact algorithms face
computational and scalability limitations, leading to the adoption of faster,
but non-optimal, heuristic methods. This work presents the first experimental
study of the recently devised Incremental Parametric Cut (IPC) algorithm, which
is an exact method for DSP and other "monotone ratio problems". Our findings
demonstrate that IPC not only overcomes the limitations of previous exact
approaches but also substantially outperforms leading state-of-the-art
heuristics in both speed and solution quality. IPC's performance is also
evaluated here for other "monotone ratio problems" related to conductance,
Cheeger constant and normalized cut. For these, our experimental study on
large-scale instances demonstrate exceptional computational speed. In
particular, comparing IPC with the "fully parametric cut" algorithm, which is
the only other efficient known optimization algorithm for such problems,
demonstrate the superior performance of IPC. We provide here code and
benchmarks, establishing IPC as a fast, scalable, and optimal solution
framework for densest subgraph and related monotone ratio problems.

</details>


### [75] [Minimum Sum Coloring with Bundles in Trees and Bipartite Graphs](https://arxiv.org/abs/2509.15080)
*Takehiro Ito,Naonori Kakimura,Naoyuki Kamiyama,Yusuke Kobayashi,Yoshio Okamoto*

Main category: cs.DS

TL;DR: 本文证明了带束的最小和着色问题在路径上是NP难的，并提供了多种算法解决方案，适用于不同约束条件的树和二分图。


<details>
  <summary>Details</summary>
Motivation: 解决Darbouy和Friggstad提出的开放性问题，即带束的最小和着色问题是否在树上有多项式时间解。

Method: 本文通过证明带束的最小和着色问题在路径上的NP难性，并设计了针对不同情况的算法，包括固定参数算法（适用于树宽有限的图）和多项式时间算法（适用于特定约束的树和二分图）。

Result: 证明了带束的最小和着色问题在路径上是NP难的，并提供了多种算法，包括固定参数算法和多项式时间算法，适用于不同约束条件的树和二分图。

Conclusion: 本文否定了Darbouy和Friggstad提出的开放性问题，证明了即使在路径上，带束的最小和着色问题也是NP难的。同时，提供了多种算法，包括固定参数算法和多项式时间算法，针对不同约束条件的树和二分图。

Abstract: The minimum sum coloring problem with bundles was introduced by Darbouy and
Friggstad (SWAT 2024) as a common generalization of the minimum coloring
problem and the minimum sum coloring problem. During their presentation, the
following open problem was raised: whether the minimum sum coloring problem
with bundles could be solved in polynomial time for trees. We answer their
question in the negative by proving that the minimum sum coloring problem with
bundles is NP-hard even for paths. We complement this hardness by providing
algorithms of the following types. First, we provide a fixed-parameter
algorithm for trees when the number of bundles is a parameter; this can be
extended to graphs of bounded treewidth. Second, we provide a polynomial-time
algorithm for trees when bundles form a partition of the vertex set and the
difference between the number of vertices and the number of bundles is
constant. Third, we provide a polynomial-time algorithm for trees when bundles
form a partition of the vertex set and each bundle induces a connected
subgraph. We further show that for bipartite graphs, the problem with weights
is NP-hard even when the number of bundles is at least three, but is
polynomial-time solvable when the number of bundles is at most two. The
threshold shifts to three versus four for the problem without weights.

</details>


### [76] [Balanced Spanning Tree Distributions Have Separation Fairness](https://arxiv.org/abs/2509.15137)
*Harry Chen,Kamesh Munagala,Govind S. Sankar*

Main category: cs.DS

TL;DR: 论文提出分离公平性概念，证明平滑平衡生成树分布满足该性质，支持ReCom方法的细粒度公平性，并开发了相关分析工具。


<details>
  <summary>Details</summary>
Motivation: 尽管基于抽样的方法（如ReCom）被广泛用于审计选区划分的公平性，但这些样本是否真正具有代表性或存在隐藏偏见仍是一个开放性问题。

Method: 研究聚焦于网格图和两区划分，通过分析平滑平衡生成树分布，证明了其满足分离公平性。同时，开发了分析循环擦除随机游走和划分的工具。

Result: 研究结果表明，平滑平衡生成树分布满足分离公平性，且为ReCom等MCMC方法的公平性提供了理论支持。

Conclusion: 论文证明了平衡生成树分布的平滑变体满足分离公平性，为ReCom等MCMC方法提供了理论支持，表明其在采样过程中保持了细粒度的公平性。

Abstract: Sampling-based methods such as ReCom are widely used to audit redistricting
plans for fairness, with the balanced spanning tree distribution playing a
central role since it favors compact, contiguous, and population-balanced
districts. However, whether such samples are truly representative or exhibit
hidden biases remains an open question. In this work, we introduce the notion
of separation fairness, which asks whether adjacent geographic units are
separated with at most a constant probability (bounded away from one) in
sampled redistricting plans. Focusing on grid graphs and two-district
partitions, we prove that a smooth variant of the balanced spanning tree
distribution satisfies separation fairness. Our results also provide
theoretical support for popular MCMC methods like ReCom, suggesting that they
maintain fairness at a granular level in the sampling process. Along the way,
we develop tools for analyzing loop-erased random walks and partitions that may
be of independent interest.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [77] [AEGIS: Automated Error Generation and Identification for Multi-Agent Systems](https://arxiv.org/abs/2509.14295)
*Fanqi Kong,Ruijie Zhang,Huaxiao Yin,Guibin Zhang,Xiaofei Zhang,Ziang Chen,Zhaowei Zhang,Xiaoyuan Zhang,Song-Chun Zhu,Xue Feng*

Main category: cs.RO

TL;DR: AEGIS框架通过自动化生成多智能体系统错误数据集，显著提升模型鲁棒性，验证其作为关键资源的有效性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）日益自主和复杂，理解其错误模式对确保可靠性和安全性至关重要，但目前缺乏大规模、多样化且带有精确错误标签的数据集。

Method: 采用上下文感知的基于LLM的自适应操纵器，系统注入可控和可追踪的错误，创建丰富的失败数据集，并探索了监督微调、强化学习和对比学习三种学习范式。

Result: 在AEGIS数据上训练的模型在三种学习范式中均取得显著提升，部分微调模型性能与或优于规模大一个数量级的专有系统。

Conclusion: AEGIS框架通过自动化生成和识别多智能体系统中的错误，显著提升了模型的鲁棒性和可解释性，验证了其作为关键资源的价值。

Abstract: As Multi-Agent Systems (MAS) become increasingly autonomous and complex,
understanding their error modes is critical for ensuring their reliability and
safety. However, research in this area has been severely hampered by the lack
of large-scale, diverse datasets with precise, ground-truth error labels. To
address this bottleneck, we introduce \textbf{AEGIS}, a novel framework for
\textbf{A}utomated \textbf{E}rror \textbf{G}eneration and
\textbf{I}dentification for Multi-Agent \textbf{S}ystems. By systematically
injecting controllable and traceable errors into initially successful
trajectories, we create a rich dataset of realistic failures. This is achieved
using a context-aware, LLM-based adaptive manipulator that performs
sophisticated attacks like prompt injection and response corruption to induce
specific, predefined error modes. We demonstrate the value of our dataset by
exploring three distinct learning paradigms for the error identification task:
Supervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning. Our
comprehensive experiments show that models trained on AEGIS data achieve
substantial improvements across all three learning paradigms. Notably, several
of our fine-tuned models demonstrate performance competitive with or superior
to proprietary systems an order of magnitude larger, validating our automated
data generation framework as a crucial resource for developing more robust and
interpretable multi-agent systems. Our project website is available at
https://kfq20.github.io/AEGIS-Website.

</details>


### [78] [FlowDrive: Energy Flow Field for End-to-End Autonomous Driving](https://arxiv.org/abs/2509.14303)
*Hao Jiang,Zhipeng Zhang,Yu Gao,Zhigang Sun,Yiru Wang,Yuwen Heng,Shuo Wang,Jinhao Chai,Zhuo Chen,Hao Zhao,Hao Sun,Xi Zhang,Anqing Jiang,Chuan Hu*

Main category: cs.RO

TL;DR: FlowDrive提出了一个基于能量流场的新框架，通过显式建模风险和车道先验，提升了自动驾驶规划的安全性和可解释性，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有端到端框架通常依赖隐式学习的BEV特征，缺乏对风险和指导先验的显式建模，导致规划缺乏安全性和可解释性。FlowDrive旨在解决这一问题。

Method: FlowDrive提出了一种新颖的框架，利用能量流场（风险势和车道吸引场）来编码语义先验和安全提示，并通过条件扩散规划器解耦运动意图预测和轨迹去噪。

Result: 在NAVSIM v2基准测试中，FlowDrive的EPDMS达到86.3，超越了现有基线，表现出更高的安全性和规划质量。

Conclusion: FlowDrive通过引入物理可解释的能量流场（如风险势和车道吸引场）来增强BEV空间中的语义先验和安全提示，实现了安全且可解释的规划，并在NAVSIM v2基准测试中达到了最先进的性能。

Abstract: Recent advances in end-to-end autonomous driving leverage multi-view images
to construct BEV representations for motion planning. In motion planning,
autonomous vehicles need considering both hard constraints imposed by
geometrically occupied obstacles (e.g., vehicles, pedestrians) and soft,
rule-based semantics with no explicit geometry (e.g., lane boundaries, traffic
priors). However, existing end-to-end frameworks typically rely on BEV features
learned in an implicit manner, lacking explicit modeling of risk and guidance
priors for safe and interpretable planning. To address this, we propose
FlowDrive, a novel framework that introduces physically interpretable
energy-based flow fields-including risk potential and lane attraction fields-to
encode semantic priors and safety cues into the BEV space. These flow-aware
features enable adaptive refinement of anchor trajectories and serve as
interpretable guidance for trajectory generation. Moreover, FlowDrive decouples
motion intent prediction from trajectory denoising via a conditional diffusion
planner with feature-level gating, alleviating task interference and enhancing
multimodal diversity. Experiments on the NAVSIM v2 benchmark demonstrate that
FlowDrive achieves state-of-the-art performance with an EPDMS of 86.3,
surpassing prior baselines in both safety and planning quality. The project is
available at https://astrixdrive.github.io/FlowDrive.github.io/.

</details>


### [79] [Multi-Quadruped Cooperative Object Transport: Learning Decentralized Pinch-Lift-Move](https://arxiv.org/abs/2509.14342)
*Bikram Pandit,Aayam Kumar Shrestha,Alan Fern*

Main category: cs.RO

TL;DR: 该论文提出了一种分层策略和星座奖励方法，使独立机器人团队无需机械耦合或通信即可协调运输不可抓取物体，验证了其在多样化场景中的有效性及仿真到现实的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 解决在无机械耦合、无通信或集中控制的情况下，机械独立的机器人团队如何仅通过接触力协调运输不可抓取物体的挑战性问题。

Method: 采用分层策略架构，分离基础运动与手臂控制，并提出星座奖励公式统一位置和方向跟踪，以强制刚性接触行为。通过精心设计的奖励和训练课程，鼓励机器人表现得像与对象刚性连接。

Result: 在2-10个机器人的团队中展示了鲁棒的运输能力，适用于多样化的对象几何和质量，并成功实现了轻量级对象的仿真到现实迁移。

Conclusion: 该研究通过分层策略架构和星座奖励设计，成功实现了无需机械耦合或通信的机器人团队协调运输，展示了在多样化对象几何和质量上的鲁棒性，并验证了从仿真到现实的迁移能力。

Abstract: We study decentralized cooperative transport using teams of N-quadruped
robots with arm that must pinch, lift, and move ungraspable objects through
physical contact alone. Unlike prior work that relies on rigid mechanical
coupling between robots and objects, we address the more challenging setting
where mechanically independent robots must coordinate through contact forces
alone without any communication or centralized control. To this end, we employ
a hierarchical policy architecture that separates base locomotion from arm
control, and propose a constellation reward formulation that unifies position
and orientation tracking to enforce rigid contact behavior. The key insight is
encouraging robots to behave as if rigidly connected to the object through
careful reward design and training curriculum rather than explicit mechanical
constraints. Our approach enables coordination through shared policy parameters
and implicit synchronization cues - scaling to arbitrary team sizes without
retraining. We show extensive simulation experiments to demonstrate robust
transport across 2-10 robots on diverse object geometries and masses, along
with sim2real transfer results on lightweight objects.

</details>


### [80] [LeVR: A Modular VR Teleoperation Framework for Imitation Learning in Dexterous Manipulation](https://arxiv.org/abs/2509.14349)
*Zhengyang Kris Weng,Matthew L. Elwin,Han Liu*

Main category: cs.RO

TL;DR: LeVR是一个模块化软件框架，通过VR远程操作和LeRobot集成，解决了机器人模仿学习中的数据收集问题，并开源了LeFranX实现。


<details>
  <summary>Details</summary>
Motivation: 解决机器人模仿学习中VR远程操作和数据收集的不足，提升演示数据收集的效率和鲁棒性。

Method: LeVR框架结合了VR远程操作和LeRobot模仿学习框架，实现了从数据收集到策略部署的无缝工作流程。

Result: 成功收集了100个专家演示的公开数据集，并用于微调先进的视觉运动策略，验证了系统的有效性。

Conclusion: LeVR通过提供模块化软件框架，解决了机器人模仿学习中的关键问题，包括VR远程操作和数据收集的集成，并通过LeFranX开源实现验证了其有效性。

Abstract: We introduce LeVR, a modular software framework designed to bridge two
critical gaps in robotic imitation learning. First, it provides robust and
intuitive virtual reality (VR) teleoperation for data collection using robot
arms paired with dexterous hands, addressing a common limitation in existing
systems. Second, it natively integrates with the powerful LeRobot imitation
learning (IL) framework, enabling the use of VR-based teleoperation data and
streamlining the demonstration collection process. To demonstrate LeVR, we
release LeFranX, an open-source implementation for the Franka FER arm and
RobotEra XHand, two widely used research platforms. LeFranX delivers a
seamless, end-to-end workflow from data collection to real-world policy
deployment. We validate our system by collecting a public dataset of 100 expert
demonstrations and use it to successfully fine-tune state-of-the-art visuomotor
policies. We provide our open-source framework, implementation, and dataset to
accelerate IL research for the robotics community.

</details>


### [81] [DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion](https://arxiv.org/abs/2509.14353)
*Dvij Kalaria,Sudarshan S Harithas,Pushkal Katara,Sangkyung Kwak,Sarthak Bhagat,Shankar Sastry,Srinath Sridhar,Sai Vemprala,Ashish Kapoor,Jonathan Chung-Kuan Huang*

Main category: cs.RO

TL;DR: DreamControl结合扩散模型和强化学习，通过人类运动数据指导策略，实现高效人形机器人控制，并在真实机器人上验证。


<details>
  <summary>Details</summary>
Motivation: 探索如何结合扩散模型和强化学习的优势，以解决直接强化学习难以实现的任务，并生成自然的运动模式。

Method: 利用基于人类运动数据的扩散先验模型指导强化学习策略，在仿真中完成特定任务。

Result: DreamControl能够发现直接强化学习无法实现的解决方案，并生成自然的运动，有助于仿真到现实的迁移。

Conclusion: DreamControl通过结合扩散模型和强化学习，成功实现了在仿真中学习自然且高效的人形机器人全身控制技能，并在真实机器人Unitree G1上验证了其有效性。

Abstract: We introduce DreamControl, a novel methodology for learning autonomous
whole-body humanoid skills. DreamControl leverages the strengths of diffusion
models and Reinforcement Learning (RL): our core innovation is the use of a
diffusion prior trained on human motion data, which subsequently guides an RL
policy in simulation to complete specific tasks of interest (e.g., opening a
drawer or picking up an object). We demonstrate that this human motion-informed
prior allows RL to discover solutions unattainable by direct RL, and that
diffusion models inherently promote natural looking motions, aiding in
sim-to-real transfer. We validate DreamControl's effectiveness on a Unitree G1
robot across a diverse set of challenging tasks involving simultaneous lower
and upper body control and object interaction.

</details>


### [82] [CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks](https://arxiv.org/abs/2509.14380)
*Seoyeon Choi,Kanghyun Ryu,Jonghoon Ock,Negar Mehr*

Main category: cs.RO

TL;DR: CRAFT利用基础模型作为“教练”，自动分解多机器人协调任务并训练子任务，成功实现了复杂协调行为的学习和硬件验证。


<details>
  <summary>Details</summary>
Motivation: 受人类通过分阶段课程学习复杂协调行为的启发，为了解决多智能体强化学习（MARL）在机器人应用中面临的高维连续联合动作空间、复杂奖励设计和非平稳过渡等挑战。

Method: CRAFT框架利用大型语言模型（LLMs）的规划能力将长时程协调任务分解为子任务序列，并通过视觉语言模型（VLM）引导的奖励细化循环训练每个子任务。

Result: CRAFT在多四足机器人导航和双手操作任务中表现出学习复杂协调行为的能力，并在真实硬件实验中验证了其导航策略的有效性。

Conclusion: CRAFT框架通过利用基础模型的推理能力，成功实现了多机器人协调任务的自动分解和训练，并在真实硬件实验中验证了其有效性。

Abstract: Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for
learning coordination in multi-agent systems. However, applying MARL to
robotics still remains challenging due to high-dimensional continuous joint
action spaces, complex reward design, and non-stationary transitions inherent
to decentralized settings. On the other hand, humans learn complex coordination
through staged curricula, where long-horizon behaviors are progressively built
upon simpler skills. Motivated by this, we propose CRAFT: Coaching
Reinforcement learning Autonomously using Foundation models for multi-robot
coordination Tasks, a framework that leverages the reasoning capabilities of
foundation models to act as a "coach" for multi-robot coordination. CRAFT
automatically decomposes long-horizon coordination tasks into sequences of
subtasks using the planning capability of Large Language Models (LLMs). In what
follows, CRAFT trains each subtask using reward functions generated by LLM, and
refines them through a Vision Language Model (VLM)-guided reward-refinement
loop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation
tasks, demonstrating its capability to learn complex coordination behaviors. In
addition, we validate the multi-quadruped navigation policy in real hardware
experiments.

</details>


### [83] [RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust Embeddings](https://arxiv.org/abs/2509.14383)
*Yuhong Lu*

Main category: cs.RO

TL;DR: RLBind 是一种两阶段对抗不变性跨模态对齐框架，通过硬化视觉编码器和利用跨模态对应性，显著提升了多模态编码器在对抗性环境下的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器人部署中视觉分支面临对抗性和自然损坏，鲁棒性成为安全的前提。现有防御方法通常仅关注清洁和对抗特征的对齐，忽视了更广泛的跨模态对应性，导致性能提升有限且可能损害零样本迁移能力。

Method: RLBind 采用两阶段对抗不变性跨模态对齐框架：第一阶段通过无监督微调硬化视觉编码器；第二阶段利用跨模态对应性，最小化干净/对抗特征与文本锚点之间的差异，并强制跨模态的类级分布对齐。

Result: 在图像、音频、热成像和视频数据上的广泛实验表明，RLBind 在清洁准确性和规范限制的对抗鲁棒性方面均优于 LanguageBind 主干和标准微调基线。

Conclusion: RLBind 提供了一种在不牺牲泛化能力的情况下提高鲁棒性的实用方法，为机器人在导航、操作等自主场景中的多传感器感知提供了更安全的解决方案。

Abstract: Unified multi-modal encoders that bind vision, audio, and other sensors into
a shared embedding space are attractive building blocks for robot perception
and decision-making. However, on-robot deployment exposes the vision branch to
adversarial and natural corruptions, making robustness a prerequisite for
safety. Prior defenses typically align clean and adversarial features within
CLIP-style encoders and overlook broader cross-modal correspondence, yielding
modest gains and often degrading zero-shot transfer. We introduce RLBind, a
two-stage adversarial-invariant cross-modal alignment framework for robust
unified embeddings. Stage 1 performs unsupervised fine-tuning on
clean-adversarial pairs to harden the visual encoder. Stage 2 leverages
cross-modal correspondence by minimizing the discrepancy between
clean/adversarial features and a text anchor, while enforcing class-wise
distributional alignment across modalities. Extensive experiments on Image,
Audio, Thermal, and Video data show that RLBind consistently outperforms the
LanguageBind backbone and standard fine-tuning baselines in both clean accuracy
and norm-bounded adversarial robustness. By improving resilience without
sacrificing generalization, RLBind provides a practical path toward safer
multi-sensor perception stacks for embodied robots in navigation, manipulation,
and other autonomy settings.

</details>


### [84] [GestOS: Advanced Hand Gesture Interpretation via Large Language Models to control Any Type of Robot](https://arxiv.org/abs/2509.14412)
*Artem Lykov,Oleg Kobzarev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: GestOS是一个基于手势的操作系统，通过语义解释和动态任务分配实现多机器人智能协作，结合视觉感知和LLM推理生成机器人命令。


<details>
  <summary>Details</summary>
Motivation: 现有的系统将手势映射到固定命令或单代理动作，缺乏语义解释和动态任务分配能力。GestOS旨在通过语义解释和动态任务分配，实现上下文感知的自适应控制。

Method: 结合轻量级视觉感知与大型语言模型（LLM）推理：将手势姿势转换为结构化文本描述，LLM用于推断意图并生成针对特定机器人的命令。机器人选择模块确保每个手势触发的任务实时匹配到最合适的代理。

Result: GestOS实现了无需用户明确指定目标或命令的上下文感知、自适应控制，支持多机器人的智能协作。

Conclusion: GestOS通过将手势交互从简单的识别提升到智能编排，支持在动态环境中与机器人系统进行可扩展、灵活且用户友好的协作。

Abstract: We present GestOS, a gesture-based operating system for high-level control of
heterogeneous robot teams. Unlike prior systems that map gestures to fixed
commands or single-agent actions, GestOS interprets hand gestures semantically
and dynamically distributes tasks across multiple robots based on their
capabilities, current state, and supported instruction sets. The system
combines lightweight visual perception with large language model (LLM)
reasoning: hand poses are converted into structured textual descriptions, which
the LLM uses to infer intent and generate robot-specific commands. A robot
selection module ensures that each gesture-triggered task is matched to the
most suitable agent in real time. This architecture enables context-aware,
adaptive control without requiring explicit user specification of targets or
commands. By advancing gesture interaction from recognition to intelligent
orchestration, GestOS supports scalable, flexible, and user-friendly
collaboration with robotic systems in dynamic environments.

</details>


### [85] [Perception-Integrated Safety Critical Control via Analytic Collision Cone Barrier Functions on 3D Gaussian Splatting](https://arxiv.org/abs/2509.14421)
*Dario Tscholl,Yashwanth Nakka,Brian Gunter*

Main category: cs.RO

TL;DR: 提出一种基于3D高斯Splat的碰撞锥CBF方法，实现高效、早激活的避障，适用于实时导航。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于距离的CBF反应滞后、计算成本高的问题，提供一种更高效、更早激活的避障方法，适用于实时导航。

Method: 利用3D高斯Splat的解析几何特性，将其转化为前向碰撞锥，并嵌入二次规划（QP）中的一阶CBF。无需高阶CBF扩展，通过Minkowski和膨胀处理机器人物理尺寸。

Result: 在包含约17万个Splat的大规模合成场景中验证，规划时间减少3倍，轨迹抖动显著降低，同时保持同等安全性。

Conclusion: 该方法通过将3D高斯Splat转化为前向碰撞锥，并嵌入一阶控制屏障函数（CBF），实现了高效、连续且封闭形式的碰撞约束表示。相比传统方法，它能更早激活避障，生成更平滑、安全的轨迹，且计算成本更低。

Abstract: We present a perception-driven safety filter that converts each 3D Gaussian
Splat (3DGS) into a closed-form forward collision cone, which in turn yields a
first-order control barrier function (CBF) embedded within a quadratic program
(QP). By exploiting the analytic geometry of splats, our formulation provides a
continuous, closed-form representation of collision constraints that is both
simple and computationally efficient. Unlike distance-based CBFs, which tend to
activate reactively only when an obstacle is already close, our collision-cone
CBF activates proactively, allowing the robot to adjust earlier and thereby
produce smoother and safer avoidance maneuvers at lower computational cost. We
validate the method on a large synthetic scene with approximately 170k splats,
where our filter reduces planning time by a factor of 3 and significantly
decreased trajectory jerk compared to a state-of-the-art 3DGS planner, while
maintaining the same level of safety. The approach is entirely analytic,
requires no high-order CBF extensions (HOCBFs), and generalizes naturally to
robots with physical extent through a principled Minkowski-sum inflation of the
splats. These properties make the method broadly applicable to real-time
navigation in cluttered, perception-derived extreme environments, including
space robotics and satellite systems.

</details>


### [86] [Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control](https://arxiv.org/abs/2509.14431)
*Keqin Wang,Tao Zhong,David Chang,Christine Allen-Blanchette*

Main category: cs.RO

TL;DR: LEGO框架通过图神经网络和规范化方法，解决了MARL的训练不稳定性与泛化问题，实验和实际应用表现优异。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）在复杂决策协调中表现出强大潜力，但面临训练不稳定性、非动能对策失效以及泛化能力不足等挑战。

Method: LEGO框架结合了图神经网络（捕捉置换等变性和泛化能力）、规范化（强制E(n)-等变性）和异质表示（编码角色特定归纳偏差），并与MAPPO等流行MARL算法无缝集成。

Result: 实验表明，LEGO在合作性和竞争性群体基准测试中优于强基线，并提升了泛化能力。实际应用中，LEGO对团队规模变化和智能体故障表现出鲁棒性。

Conclusion: LEGO框架通过整合图神经网络和规范化的方法，有效解决了MARL在竞争性和合作性任务中的训练不稳定性和泛化能力问题，并在实际应用中表现出色。

Abstract: Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigm
for coordinating swarms of agents in complex decision-making, yet major
challenges remain. In competitive settings such as pursuer-evader tasks,
simultaneous adaptation can destabilize training; non-kinetic countermeasures
often fail under adverse conditions; and policies trained in one configuration
rarely generalize to environments with a different number of agents. To address
these issues, we propose the Local-Canonicalization Equivariant Graph Neural
Networks (LEGO) framework, which integrates seamlessly with popular MARL
algorithms such as MAPPO. LEGO employs graph neural networks to capture
permutation equivariance and generalization to different agent numbers,
canonicalization to enforce E(n)-equivariance, and heterogeneous
representations to encode role-specific inductive biases. Experiments on
cooperative and competitive swarm benchmarks show that LEGO outperforms strong
baselines and improves generalization. In real-world experiments, LEGO
demonstrates robustness to varying team sizes and agent failure.

</details>


### [87] [Online Learning of Deceptive Policies under Intermittent Observation](https://arxiv.org/abs/2509.14453)
*Gokul Puthumanaillam,Ram Padmanabhan,Jose Fuentes,Nicole Cruz,Paulo Padrao,Ruben Hernandez,Hao Jiang,William Schafer,Leonardo Bobadilla,Melkior Ornik*

Main category: cs.RO

TL;DR: 本研究利用心理理论（ToM）引导在线强化学习，实现代理在执行私人目标时保持对监督者策略的表面合规性，并在硬件实验中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于真实人类监督者的行为，探讨在非连续监控环境下，代理如何通过心理理论实现欺骗性行为，同时保持表面合规性。

Method: 通过建模监督者的期望并从中提取一个校准标量（预期偏差证据），并将其作为状态依赖权重注入KL正则化的策略改进步骤中，实现自我利益与合规性的平衡。

Result: 在海洋（ASV）和空中（UAV）导航的实时硬件实验中，ToM引导的RL在线运行，实现了高回报和成功，且观察到的轨迹证据与监督者期望一致。

Conclusion: 本研究表明，通过将心理理论（Theory of Mind）应用于在线强化学习（RL），可以有效地引导代理在执行私人目标的同时保持对监督者参考策略的合规性，从而在硬件实验中实现高回报和成功。

Abstract: In supervisory control settings, autonomous systems are not monitored
continuously. Instead, monitoring often occurs at sporadic intervals within
known bounds. We study the problem of deception, where an agent pursues a
private objective while remaining plausibly compliant with a supervisor's
reference policy when observations occur. Motivated by the behavior of real,
human supervisors, we situate the problem within Theory of Mind: the
representation of what an observer believes and expects to see. We show that
Theory of Mind can be repurposed to steer online reinforcement learning (RL)
toward such deceptive behavior. We model the supervisor's expectations and
distill from them a single, calibrated scalar -- the expected evidence of
deviation if an observation were to happen now. This scalar combines how unlike
the reference and current action distributions appear, with the agent's belief
that an observation is imminent. Injected as a state-dependent weight into a
KL-regularized policy improvement step within an online RL loop, this scalar
informs a closed-form update that smoothly trades off self-interest and
compliance, thus sidestepping hand-crafted or heuristic policies. In
real-world, real-time hardware experiments on marine (ASV) and aerial (UAV)
navigation, our ToM-guided RL runs online, achieves high return and success
with observed-trace evidence calibrated to the supervisor's expectations.

</details>


### [88] [Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring](https://arxiv.org/abs/2509.14460)
*Abhiroop Ajith,Constantinos Chamzas*

Main category: cs.RO

TL;DR: 研究提出了一种从视觉数据自动发现抽象表示的方法，用于机器人规划任务，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 直接从视觉数据中学习抽象表示是机器人学的核心挑战，自动化这一过程可以提升规划框架的可扩展性和适用性。

Method: 结合结构约束和注意力引导的视觉距离，诱导出离散的、图结构的抽象表示。

Result: 在模拟实验中，该方法一致地识别出有意义的抽象表示，支持高效规划并优于现有方法。

Conclusion: 该研究提出了一种通过视觉数据自动发现抽象表示的方法，有效支持了机器人规划任务，并在模拟实验中证明了其优越性。

Abstract: Learning abstractions directly from data is a core challenge in robotics.
Humans naturally operate at an abstract level, reasoning over high-level
subgoals while delegating execution to low-level motor skills -- an ability
that enables efficient problem solving in complex environments. In robotics,
abstractions and hierarchical reasoning have long been central to planning, yet
they are typically hand-engineered, demanding significant human effort and
limiting scalability. Automating the discovery of useful abstractions directly
from visual data would make planning frameworks more scalable and more
applicable to real-world robotic domains. In this work, we focus on
rearrangement tasks where the state is represented with raw images, and propose
a method to induce discrete, graph-structured abstractions by combining
structural constraints with an attention-guided visual distance. Our approach
leverages the inherent bipartite structure of rearrangement problems,
integrating structural constraints and visual embeddings into a unified
framework. This enables the autonomous discovery of abstractions from vision
alone, which can subsequently support high-level planning. We evaluate our
method on two rearrangement tasks in simulation and show that it consistently
identifies meaningful abstractions that facilitate effective planning and
outperform existing approaches.

</details>


### [89] [Object Recognition and Force Estimation with the GelSight Baby Fin Ray](https://arxiv.org/abs/2509.14510)
*Sandra Q. Liu,Yuxiang Ma,Edward H. Adelson*

Main category: cs.RO

TL;DR: 该研究利用机器学习技术，通过GelSight Baby Fin Ray触觉传感器提取高分辨率触觉图像信息，成功应用于纹理分类和力/位置估计，展示了软体机器人在环境交互中的潜力。


<details>
  <summary>Details</summary>
Motivation: 进一步探讨GelSight Baby Fin Ray的潜力，特别是在区分坚果壳纹理以及进行力和位置估计方面的应用。

Method: 通过消融研究比较了流行的神经网络结构，包括ResNet50、GoogLeNet以及3层和5层卷积神经网络（CNN）结构。

Result: 研究表明，GelSight Baby Fin Ray能够通过机器学习有效提取触觉图像中的信息，用于纹理分类和力/位置估计。

Conclusion: 机器学习是一种有前景的技术，可以从高分辨率触觉图像中提取有用信息，并增强软体机器人更好地理解和与环境互动的能力。

Abstract: Recent advances in soft robotic hands and tactile sensing have enabled both
to perform an increasing number of complex tasks with the aid of machine
learning. In particular, we presented the GelSight Baby Fin Ray in our previous
work, which integrates a camera with a soft, compliant Fin Ray structure.
Camera-based tactile sensing gives the GelSight Baby Fin Ray the ability to
capture rich contact information like forces, object geometries, and textures.
Moreover, our previous work showed that the GelSight Baby Fin Ray can dig
through clutter, and classify in-shell nuts. To further examine the potential
of the GelSight Baby Fin Ray, we leverage learning to distinguish nut-in-shell
textures and to perform force and position estimation. We implement ablation
studies with popular neural network structures, including ResNet50, GoogLeNet,
and 3- and 5-layer convolutional neural network (CNN) structures. We conclude
that machine learning is a promising technique to extract useful information
from high-resolution tactile images and empower soft robotics to better
understand and interact with the environments.

</details>


### [90] [Event-LAB: Towards Standardized Evaluation of Neuromorphic Localization Methods](https://arxiv.org/abs/2509.14516)
*Adam D. Hines,Alejandro Fontan,Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: Event-LAB是一个统一框架，简化事件基于定位方法的比较，通过Pixi实现，支持VPR和SLAM流程，强调公平比较的重要性。


<details>
  <summary>Details</summary>
Motivation: 事件基于定位研究的快速增长带来了代码、包依赖和数据格式的多样性，使得比较和可靠实现变得困难和繁琐。

Method: 使用Pixi包和依赖管理器实现Event-LAB框架，支持单一命令行安装和调用多种定位方法和数据集。

Result: 框架成功实现了两种常见的事件基于定位流程（VPR和SLAM），并能够系统地可视化和分析多种方法和数据集的结果。

Conclusion: Event-LAB框架为研究社区提供了一个统一的平台，能够简化事件基于定位方法的比较和分析，强调了公平比较的重要性。

Abstract: Event-based localization research and datasets are a rapidly growing area of
interest, with a tenfold increase in the cumulative total number of published
papers on this topic over the past 10 years. Whilst the rapid expansion in the
field is exciting, it brings with it an associated challenge: a growth in the
variety of required code and package dependencies as well as data formats,
making comparisons difficult and cumbersome for researchers to implement
reliably. To address this challenge, we present Event-LAB: a new and unified
framework for running several event-based localization methodologies across
multiple datasets. Event-LAB is implemented using the Pixi package and
dependency manager, that enables a single command-line installation and
invocation for combinations of localization methods and datasets. To
demonstrate the capabilities of the framework, we implement two common
event-based localization pipelines: Visual Place Recognition (VPR) and
Simultaneous Localization and Mapping (SLAM). We demonstrate the ability of the
framework to systematically visualize and analyze the results of multiple
methods and datasets, revealing key insights such as the association of
parameters that control event collection counts and window sizes for frame
generation to large variations in performance. The results and analysis
demonstrate the importance of fairly comparing methodologies with consistent
event image generation parameters. Our Event-LAB framework provides this
ability for the research community, by contributing a streamlined workflow for
easily setting up multiple conditions.

</details>


### [91] [Learning to Pick: A Visuomotor Policy for Clustered Strawberry Picking](https://arxiv.org/abs/2509.14530)
*Zhenghao Fei,Wenwu Lu,Linsheng Hou,Chen Peng*

Main category: cs.RO

TL;DR: 研究提出一种基于人类演示学习的草莓采摘机器人系统，通过改进的ACT策略在遮挡环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 草莓自然生长时常常被叶片、茎和其他果实遮挡，传统感知-规划-控制系统难以应对，需要灵巧的操作来绕过或移动周围软物体以精确采摘。

Method: 系统采用4自由度SCARA机械臂配合人类远程操作接口进行数据收集，并利用End Pose Assisted Action Chunking Transformer (ACT)开发精细的视觉运动采摘策略。

Result: 实验表明，改进后的方法在各种遮挡场景下显著优于直接实施ACT，验证了其有效性。

Conclusion: 该研究提出的草莓采摘机器人系统通过模仿人类演示，显著提高了在遮挡环境下的采摘效率，展示了其在实践应用中的潜力。

Abstract: Strawberries naturally grow in clusters, interwoven with leaves, stems, and
other fruits, which frequently leads to occlusion. This inherent growth habit
presents a significant challenge for robotic picking, as traditional
percept-plan-control systems struggle to reach fruits amid the clutter.
Effectively picking an occluded strawberry demands dexterous manipulation to
carefully bypass or gently move the surrounding soft objects and precisely
access the ideal picking point located at the stem just above the calyx. To
address this challenge, we introduce a strawberry-picking robotic system that
learns from human demonstrations. Our system features a 4-DoF SCARA arm paired
with a human teleoperation interface for efficient data collection and
leverages an End Pose Assisted Action Chunking Transformer (ACT) to develop a
fine-grained visuomotor picking policy. Experiments under various occlusion
scenarios demonstrate that our modified approach significantly outperforms the
direct implementation of ACT, underscoring its potential for practical
application in occluded strawberry picking.

</details>


### [92] [Dual-Arm Hierarchical Planning for Laboratory Automation: Vibratory Sieve Shaker Operations](https://arxiv.org/abs/2509.14531)
*Haoran Xiao,Xue Wang,Huimin Lu,Zhiwen Zeng,Zirui Guo,Ziqi Ni,Yicong Ye,Wei Dai*

Main category: cs.RO

TL;DR: 提出分层规划框架，显著提升振动筛分机自动化操作的效率和实用性，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决振动筛分机自动化操作中的三大挑战：狭窄空间的双臂盖操作、重叠工作区的双手交接以及受限条件下的粉末样本容器递送。

Method: 结合先验引导路径规划（Prior-Guided Path Planning）和多步轨迹优化（Multi-Step Trajectory Optimization），前者通过有限高斯混合模型提高狭窄通道的采样效率，后者通过路径缩短、简化、关节约束和B样条平滑优化轨迹。

Result: 实验表明，规划时间减少80.4%，路径点减少89.4%，并在物理实验中成功完成完整工作流程。

Conclusion: 该研究提出的分层规划框架在振动筛分机自动化操作中表现出高效性和实用性，成功解决了狭窄空间操作、双手交接和受限容器递送等挑战。

Abstract: This paper addresses the challenges of automating vibratory sieve shaker
operations in a materials laboratory, focusing on three critical tasks: 1)
dual-arm lid manipulation in 3 cm clearance spaces, 2) bimanual handover in
overlapping workspaces, and 3) obstructed powder sample container delivery with
orientation constraints. These tasks present significant challenges, including
inefficient sampling in narrow passages, the need for smooth trajectories to
prevent spillage, and suboptimal paths generated by conventional methods. To
overcome these challenges, we propose a hierarchical planning framework
combining Prior-Guided Path Planning and Multi-Step Trajectory Optimization.
The former uses a finite Gaussian mixture model to improve sampling efficiency
in narrow passages, while the latter refines paths by shortening, simplifying,
imposing joint constraints, and B-spline smoothing. Experimental results
demonstrate the framework's effectiveness: planning time is reduced by up to
80.4%, and waypoints are decreased by 89.4%. Furthermore, the system completes
the full vibratory sieve shaker operation workflow in a physical experiment,
validating its practical applicability for complex laboratory automation.

</details>


### [93] [SimCoachCorpus: A naturalistic dataset with language and trajectories for embodied teaching](https://arxiv.org/abs/2509.14548)
*Emily Sumner,Deepak E. Gopinath,Laporsha Dees,Patricio Reyes Gomez,Xiongyi Cui,Andrew Silva,Jean Costa,Allison Morgan,Mariah Schrum,Tiffany L. Chen,Avinash Balachandran,Guy Rosman*

Main category: cs.RO

TL;DR: SimCoachCorpus是一个独特的赛车模拟驾驶数据集，填补了语言与动作结合领域的数据空白，支持运动学习、语言现象及计算教学模型的研究。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏记录人们通过语言指导随时间获得具体技能的数据集，特别是在语言与物理动作深度交织的领域。

Method: 通过收集29名人类在模拟赛车驾驶中的行为数据，包括车辆状态、输入、地图及教练的同步语音指导，并辅以教练反馈分类、学生遵从度评分及参与者自我报告的情感状态和认知负荷。

Result: 数据集包含超过2万条同步反馈语句、400多条终端反馈语句及40多小时的驾驶数据，展示了其在上下文学习、模仿学习和主题建模中的应用。

Conclusion: SimCoachCorpus数据集填补了语言与物理动作紧密结合领域数据集的空白，为研究运动学习动态、探索语言现象及训练计算教学模型提供了宝贵资源。

Abstract: Curated datasets are essential for training and evaluating AI approaches, but
are often lacking in domains where language and physical action are deeply
intertwined. In particular, few datasets capture how people acquire embodied
skills through verbal instruction over time. To address this gap, we introduce
SimCoachCorpus: a unique dataset of race car simulator driving that allows for
the investigation of rich interactive phenomena during guided and unguided
motor skill acquisition. In this dataset, 29 humans were asked to drive in a
simulator around a race track for approximately ninety minutes. Fifteen
participants were given personalized one-on-one instruction from a professional
performance driving coach, and 14 participants drove without coaching. \name\
includes embodied features such as vehicle state and inputs, map (track
boundaries and raceline), and cone landmarks. These are synchronized with
concurrent verbal coaching from a professional coach and additional feedback at
the end of each lap. We further provide annotations of coaching categories for
each concurrent feedback utterance, ratings on students' compliance with
coaching advice, and self-reported cognitive load and emotional state of
participants (gathered from surveys during the study). The dataset includes
over 20,000 concurrent feedback utterances, over 400 terminal feedback
utterances, and over 40 hours of vehicle driving data. Our naturalistic dataset
can be used for investigating motor learning dynamics, exploring linguistic
phenomena, and training computational models of teaching. We demonstrate
applications of this dataset for in-context learning, imitation learning, and
topic modeling. The dataset introduced in this work will be released publicly
upon publication of the peer-reviewed version of this paper. Researchers
interested in early access may register at
https://tinyurl.com/SimCoachCorpusForm.

</details>


### [94] [Hierarchical Planning and Scheduling for Reconfigurable Multi-Robot Disassembly Systems under Structural Constraints](https://arxiv.org/abs/2509.14564)
*Takuya Kiyokawa,Tomoki Ishikura,Shingo Hamada,Genichiro Matsuda,Kensuke Harada*

Main category: cs.RO

TL;DR: 该研究提出了一种可重构机器人系统集成方法，通过分层优化和遗传算法有效拆解受限结构，避免了局部最优问题。


<details>
  <summary>Details</summary>
Motivation: 可重构机器人系统在拆解受限结构时需要适应目标结构的配置和协调，但大而复杂的搜索空间容易导致局部最优。

Method: 采用分层优化方法，结合多种目标遗传算法进行序列和任务规划，并通过约束编程进行调度。针对序列规划的大搜索空间问题，引入了染色体初始化方法以减少局部最优风险。

Result: 仿真结果表明，所提出的方法能有效解决可重构机器人拆解中的复杂问题。

Conclusion: 该研究提出了一种系统集成方法，通过可重构机器人自动非破坏性地拆解受限结构，有效解决了复杂问题。

Abstract: This study presents a system integration approach for planning schedules,
sequences, tasks, and motions for reconfigurable robots to automatically
disassemble constrained structures in a non-destructive manner. Such systems
must adapt their configuration and coordination to the target structure, but
the large and complex search space makes them prone to local optima. To address
this, we integrate multiple robot arms equipped with different types of tools,
together with a rotary stage, into a reconfigurable setup. This flexible system
is based on a hierarchical optimization method that generates plans meeting
multiple preferred conditions under mandatory requirements within a realistic
timeframe. The approach employs two many-objective genetic algorithms for
sequence and task planning with motion evaluations, followed by constraint
programming for scheduling. Because sequence planning has a much larger search
space, we introduce a chromosome initialization method tailored to constrained
structures to mitigate the risk of local optima. Simulation results demonstrate
that the proposed method effectively solves complex problems in reconfigurable
robotic disassembly.

</details>


### [95] [Toward Embodiment Equivariant Vision-Language-Action Policy](https://arxiv.org/abs/2509.14630)
*Anzhe Chen,Yifei Yang,Zhenjie Zhu,Kechun Xu,Zhongxiang Zhou,Rong Xiong,Yue Wang*

Main category: cs.RO

TL;DR: 论文提出了一种通过设计等变策略解决机器人配置泛化问题的框架，实验证明其有效性和高效微调能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作策略在跨任务、环境和具身的预训练中表现良好，但对新机器人配置的泛化能力有限。论文旨在解决这一配置泛化问题。

Method: 论文提出了一个框架，包括：(i) 建立具身等变理论用于动作空间和策略设计，(ii) 引入一个强制配置等变的动作解码器，(iii) 结合几何感知网络架构以增强与具身无关的空间推理。

Result: 实验表明，该方法在仿真和真实世界设置中均能提升预训练效果，并支持在新机器人配置上的高效微调。

Conclusion: 该论文提出的框架通过设计对机器人配置变换等变的策略，显著提升了跨具身预训练的效果，并在新机器人配置上实现了高效的微调。

Abstract: Vision-language-action policies learn manipulation skills across tasks,
environments and embodiments through large-scale pre-training. However, their
ability to generalize to novel robot configurations remains limited. Most
approaches emphasize model size, dataset scale and diversity while paying less
attention to the design of action spaces. This leads to the configuration
generalization problem, which requires costly adaptation. We address this
challenge by formulating cross-embodiment pre-training as designing policies
equivariant to embodiment configuration transformations. Building on this
principle, we propose a framework that (i) establishes a embodiment
equivariance theory for action space and policy design, (ii) introduces an
action decoder that enforces configuration equivariance, and (iii) incorporates
a geometry-aware network architecture to enhance embodiment-agnostic spatial
reasoning. Extensive experiments in both simulation and real-world settings
demonstrate that our approach improves pre-training effectiveness and enables
efficient fine-tuning on novel robot embodiments. Our code is available at
https://github.com/hhcaz/e2vla

</details>


### [96] [BEV-ODOM2: Enhanced BEV-based Monocular Visual Odometry with PV-BEV Fusion and Dense Flow Supervision for Ground Robots](https://arxiv.org/abs/2509.14636)
*Yufei Wei,Wangtao Lu,Sha Lu,Chenxiao Hu,Fuzhang Han,Rong Xiong,Yue Wang*

Main category: cs.RO

TL;DR: BEV-ODOM2通过密集光流监督和PV-BEV融合提升单目视觉里程计性能，在多个数据集上表现最优。


<details>
  <summary>Details</summary>
Motivation: 现有BEV方法存在稀疏监督信号和信息丢失问题，限制了单目视觉里程计的性能。

Method: 引入密集BEV光流监督和PV-BEV融合，保留6-DoF运动线索，同时维持尺度一致性。采用三级监督机制，仅从姿态数据中提取监督信号。

Result: 在KITTI、NCLT、Oxford和ZJH-VO数据集上，BEV-ODOM2实现了40%的相对平移误差改进，性能优于现有BEV方法。

Conclusion: BEV-ODOM2框架通过密集BEV光流监督和PV-BEV融合，显著提升了单目视觉里程计的性能，并在多个数据集上实现了最先进的性能。

Abstract: Bird's-Eye-View (BEV) representation offers a metric-scaled planar workspace,
facilitating the simplification of 6-DoF ego-motion to a more robust 3-DoF
model for monocular visual odometry (MVO) in intelligent transportation
systems. However, existing BEV methods suffer from sparse supervision signals
and information loss during perspective-to-BEV projection. We present
BEV-ODOM2, an enhanced framework addressing both limitations without additional
annotations. Our approach introduces: (1) dense BEV optical flow supervision
constructed from 3-DoF pose ground truth for pixel-level guidance; (2) PV-BEV
fusion that computes correlation volumes before projection to preserve 6-DoF
motion cues while maintaining scale consistency. The framework employs three
supervision levels derived solely from pose data: dense BEV flow, 5-DoF for the
PV branch, and final 3-DoF output. Enhanced rotation sampling further balances
diverse motion patterns in training. Extensive evaluation on KITTI, NCLT,
Oxford, and our newly collected ZJH-VO multi-scale dataset demonstrates
state-of-the-art performance, achieving 40 improvement in RTE compared to
previous BEV methods. The ZJH-VO dataset, covering diverse ground vehicle
scenarios from underground parking to outdoor plazas, is publicly available to
facilitate future research.

</details>


### [97] [Efficient 3D Perception on Embedded Systems via Interpolation-Free Tri-Plane Lifting and Volume Fusion](https://arxiv.org/abs/2509.14641)
*Sibaek Lee,Jiung Yeon,Hyeonwoo Yu*

Main category: cs.RO

TL;DR: 提出一种高效插值自由三平面框架，适用于嵌入式3D推理，在保持精度的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 密集3D卷积在感知任务中精度高，但计算成本过高，不适合实时机器人系统。现有三平面方法依赖2D图像特征和插值，计算量大且不适合嵌入式3D推理。

Method: 提出了一种无需插值的三平面提升和体积融合框架，通过将3D体素直接投影到平面特征并通过广播和求和重建特征体积，将非线性转移到2D卷积中。添加了一个低分辨率体积分支以捕捉全局上下文，并通过轻量级集成层与提升的特征融合。

Result: 实验表明，分类和补全任务在保持或提高精度的同时，分割和检测任务以轻微精度下降换取显著计算节省。在NVIDIA Jetson Orin nano上的设备基准测试证实了实时吞吐量。

Conclusion: 该论文提出的新型插值自由三平面提升和体积融合框架，在保持高精度的同时显著降低了计算复杂度，适用于嵌入式3D推理。

Abstract: Dense 3D convolutions provide high accuracy for perception but are too
computationally expensive for real-time robotic systems. Existing tri-plane
methods rely on 2D image features with interpolation, point-wise queries, and
implicit MLPs, which makes them computationally heavy and unsuitable for
embedded 3D inference. As an alternative, we propose a novel interpolation-free
tri-plane lifting and volumetric fusion framework, that directly projects 3D
voxels into plane features and reconstructs a feature volume through broadcast
and summation. This shifts nonlinearity to 2D convolutions, reducing complexity
while remaining fully parallelizable. To capture global context, we add a
low-resolution volumetric branch fused with the lifted features through a
lightweight integration layer, yielding a design that is both efficient and
end-to-end GPU-accelerated. To validate the effectiveness of the proposed
method, we conduct experiments on classification, completion, segmentation, and
detection, and we map the trade-off between efficiency and accuracy across
tasks. Results show that classification and completion retain or improve
accuracy, while segmentation and detection trade modest drops in accuracy for
significant computational savings. On-device benchmarks on an NVIDIA Jetson
Orin nano confirm robust real-time throughput, demonstrating the suitability of
the approach for embedded robotic perception.

</details>


### [98] [RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI](https://arxiv.org/abs/2509.14687)
*Cong Tai,Zhaoyu Zheng,Haixu Long,Hansheng Wu,Haodong Xiang,Zhengbin Long,Jun Xiong,Rong Shi,Shizhuang Zhang,Gang Qiu,He Wang,Ruifeng Li,Jun Huang,Bin Chang,Shuai Feng,Tao Shen*

Main category: cs.RO

TL;DR: RealMirror是一个开源VLA平台，通过低成本数据收集和仿真训练，实现了零样本Sim2Real转移，加速人形机器人VLA研究。


<details>
  <summary>Details</summary>
Motivation: 解决VLA领域数据获取成本高、缺乏标准化基准以及仿真与现实世界之间差距大的问题。

Method: 通过整合生成模型和3D高斯泼溅技术来重建真实环境和机器人模型，实现了零样本的Sim2Real转移。

Result: 成功展示了无需微调的零样本Sim2Real转移，模型仅使用仿真数据训练即可在真实机器人上无缝执行任务。

Conclusion: RealMirror提供了一个强大的框架，显著加速了人形机器人VLA模型的开发。

Abstract: The emerging field of Vision-Language-Action (VLA) for humanoid robots faces
several fundamental challenges, including the high cost of data acquisition,
the lack of a standardized benchmark, and the significant gap between
simulation and the real world. To overcome these obstacles, we propose
RealMirror, a comprehensive, open-source embodied AI VLA platform. RealMirror
builds an efficient, low-cost data collection, model training, and inference
system that enables end-to-end VLA research without requiring a real robot. To
facilitate model evolution and fair comparison, we also introduce a dedicated
VLA benchmark for humanoid robots, featuring multiple scenarios, extensive
trajectories, and various VLA models. Furthermore, by integrating generative
models and 3D Gaussian Splatting to reconstruct realistic environments and
robot models, we successfully demonstrate zero-shot Sim2Real transfer, where
models trained exclusively on simulation data can perform tasks on a real robot
seamlessly, without any fine-tuning. In conclusion, with the unification of
these critical components, RealMirror provides a robust framework that
significantly accelerates the development of VLA models for humanoid robots.
Project page: https://terminators2025.github.io/RealMirror.github.io

</details>


### [99] [exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation](https://arxiv.org/abs/2509.14688)
*Yue Xu,Litao Wei,Pengyu An,Qingyu Zhang,Yong-Lu Li*

Main category: cs.RO

TL;DR: 介绍了一种结合硬件（exUMI）和算法（TPP）创新的触觉机器人学习系统，有效解决了数据稀缺和稀疏性问题，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决触觉感知机器人学习中数据稀缺和稀疏性以及现有系统缺乏力反馈的挑战。

Method: 提出了exUMI（一种扩展的数据收集设备）和Tactile Prediction Pretraining（TPP）表示学习框架。

Result: TPP在真实世界实验中优于传统的触觉模仿学习，exUMI实现了100%的数据可用性。

Conclusion: 本研究通过硬件和算法的协同设计，缩小了人类触觉直觉与机器人学习之间的差距，为接触丰富的操作研究提供了开源资源。

Abstract: Tactile-aware robot learning faces critical challenges in data collection and
representation due to data scarcity and sparsity, and the absence of force
feedback in existing systems. To address these limitations, we introduce a
tactile robot learning system with both hardware and algorithm innovations. We
present exUMI, an extensible data collection device that enhances the vanilla
UMI with robust proprioception (via AR MoCap and rotary encoder), modular
visuo-tactile sensing, and automated calibration, achieving 100% data
usability. Building on an efficient collection of over 1 M tactile frames, we
propose Tactile Prediction Pretraining (TPP), a representation learning
framework through action-aware temporal tactile prediction, capturing contact
dynamics and mitigating tactile sparsity. Real-world experiments show that TPP
outperforms traditional tactile imitation learning. Our work bridges the gap
between human tactile intuition and robot learning through co-designed hardware
and algorithms, offering open-source resources to advance contact-rich
manipulation research. Project page: https://silicx.github.io/exUMI.

</details>


### [100] [Wohlhart's Three-Loop Mechanism: An Overconstrained and Shaky Linkage](https://arxiv.org/abs/2509.14698)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 论文重新分析了一个三环空间连杆机构，证明其参考配置具有局部恒定的微分自由度，且构型空间为光滑流形，参考配置非奇异。


<details>
  <summary>Details</summary>
Motivation: 重新分析Karl Wohlhart在ARK 2004论文中提出的三环空间连杆机构（基于Eddie Baker 1980年提出的两环连杆机构扩展），并验证Diez-Martinez等人在ARK 2006论文中的分析。

Method: 通过计算运动学切锥和局部近似构型空间，进行了高阶局部分析。

Result: 局部分析表明，该连杆机构在参考配置下的微分自由度为5，而实际自由度为3（过约束），但其构型空间局部为光滑流形，参考配置不是奇异点。

Conclusion: 这篇论文重新审视了一个三环空间连杆机构，证明了其在参考配置下具有局部恒定的微分自由度，表明该连杆机构是抖动的，且参考配置不是奇异点。

Abstract: This paper revisits a three-loop spatial linkage that was proposed in an ARK
2004 paper by Karl Wohlhart (as extension of a two-loop linkage proposed by
Eddie Baker in 1980) and later analyzed in an ARK 2006 paper by Diez-Martinez
et. al. A local analysis shows that this linkage has a finite degree of freedom
(DOF) 3 (and is thus overconstrained) while in its reference configuration the
differential DOF is 5. It is shown that its configuration space is locally a
smooth manifold so that the reference configuration is not a c-space
singularity. It is shown that the differential DOF is locally constant, which
makes this linkage shaky (so that the reference configuration is not a
singularity). The higher-order local analysis is facilitated by the computation
of the kinematic tangent cone as well as a local approximation of the c-space.

</details>


### [101] [Rethinking Reference Trajectories in Agile Drone Racing: A Unified Reference-Free Model-Based Controller via MPPI](https://arxiv.org/abs/2509.14726)
*Fangguo Zhao,Xin Guan,Shuo Li*

Main category: cs.RO

TL;DR: 本文提出一种无参考的无人机竞速方法，通过MPPI直接优化门进展目标，性能媲美传统参考方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖预计算参考轨迹，限制了性能；强化学习启发下，直接优化竞速目标（门进展）而非替代目标（如轨迹跟踪）。

Method: 将门进展目标直接融入模型预测路径积分（MPPI）公式中，利用其采样特性优化不连续且不可微的目标。

Result: 无参考方法在实时竞速中表现优异，与基于参考方法相比具有竞争力。

Conclusion: 提出的无参考方法在无人机竞速中表现出色，性能媲美或超越基于参考的方法。

Abstract: While model-based controllers have demonstrated remarkable performance in
autonomous drone racing, their performance is often constrained by the reliance
on pre-computed reference trajectories. Conventional approaches, such as
trajectory tracking, demand a dynamically feasible, full-state reference,
whereas contouring control relaxes this requirement to a geometric path but
still necessitates a reference. Recent advancements in reinforcement learning
(RL) have revealed that many model-based controllers optimize surrogate
objectives, such as trajectory tracking, rather than the primary racing goal of
directly maximizing progress through gates. Inspired by these findings, this
work introduces a reference-free method for time-optimal racing by
incorporating this gate progress objective, derived from RL reward shaping,
directly into the Model Predictive Path Integral (MPPI) formulation. The
sampling-based nature of MPPI makes it uniquely capable of optimizing the
discontinuous and non-differentiable objective in real-time. We also establish
a unified framework that leverages MPPI to systematically and fairly compare
three distinct objective functions with a consistent dynamics model and
parameter set: classical trajectory tracking, contouring control, and the
proposed gate progress objective. We compare the performance of these three
objectives when solved via both MPPI and a traditional gradient-based solver.
Our results demonstrate that the proposed reference-free approach achieves
competitive racing performance, rivaling or exceeding reference-based methods.
Videos are available at https://zhaofangguo.github.io/racing_mppi/

</details>


### [102] [Investigating the Effect of LED Signals and Emotional Displays in Human-Robot Shared Workspaces](https://arxiv.org/abs/2509.14748)
*Maria Ibrahim,Alap Kshirsagar,Dorothea Koert,Jan Peters*

Main category: cs.RO

TL;DR: 研究发现，情感显示虽能提升用户参与感，但对共享工作空间中的任务性能影响有限，LED信号已足够有效。


<details>
  <summary>Details</summary>
Motivation: 在共享工作空间中，有效沟通对安全性和效率至关重要。本文研究了非语言沟通对人机交互（HRI）的影响，通过将反应性光信号和情感显示集成到机器人系统中。

Method: 我们为Franka Emika Panda机器人配备了末端执行器上的LED灯带和平板上的动画面部显示，通过颜色编码信号和面部表情传达运动意图。进行了18名参与者的人机协作实验，评估三种条件：仅LED信号、LED信号与反应性情感显示、LED信号与预发性情感显示。

Result: 结果显示，情感显示增加了机器人的感知互动性，但与仅LED信号相比，并未显著改善碰撞预期、沟通清晰度或任务效率。

Conclusion: 研究结果表明，虽然情感线索可以增强用户参与感，但它们对共享工作空间中任务性能的影响有限。

Abstract: Effective communication is essential for safety and efficiency in human-robot
collaboration, particularly in shared workspaces. This paper investigates the
impact of nonverbal communication on human-robot interaction (HRI) by
integrating reactive light signals and emotional displays into a robotic
system. We equipped a Franka Emika Panda robot with an LED strip on its end
effector and an animated facial display on a tablet to convey movement intent
through colour-coded signals and facial expressions. We conducted a human-robot
collaboration experiment with 18 participants, evaluating three conditions: LED
signals alone, LED signals with reactive emotional displays, and LED signals
with pre-emptive emotional displays. We collected data through questionnaires
and position tracking to assess anticipation of potential collisions, perceived
clarity of communication, and task performance. The results indicate that while
emotional displays increased the perceived interactivity of the robot, they did
not significantly improve collision anticipation, communication clarity, or
task efficiency compared to LED signals alone. These findings suggest that
while emotional cues can enhance user engagement, their impact on task
performance in shared workspaces is limited.

</details>


### [103] [Designing Latent Safety Filters using Pre-Trained Vision Models](https://arxiv.org/abs/2509.14758)
*Ihab Tabbara,Yuxuan Yang,Ahmad Hamzeh,Maxwell Astafyev,Hussein Sibai*

Main category: cs.RO

TL;DR: 研究探讨了预训练视觉模型在视觉安全过滤器设计中的应用效果，比较了不同训练策略和模型表现，并提出了实际部署的考虑因素。


<details>
  <summary>Details</summary>
Motivation: 解决视觉控制系统的安全性问题，推动其在关键场景中的应用。

Method: 使用PVRs作为分类器、Hamilton-Jacobi可达性安全过滤器和潜在世界模型的骨干，比较不同训练策略，评估PVRs在不同任务中的表现，并探讨实际部署问题。

Result: PVRs在安全过滤器中表现良好，但需根据任务选择合适训练策略；不同PVRs在不同任务中表现不一；世界模型或Q函数对切换安全策略的决策影响需进一步评估。

Conclusion: 预训练视觉模型（PVRs）在视觉安全过滤器设计中表现出色，但需权衡训练策略（从头训练、微调或冻结）。不同任务中PVRs的表现各异，且在实际部署到资源受限设备时需考虑实用性。

Abstract: Ensuring safety of vision-based control systems remains a major challenge
hindering their deployment in critical settings. Safety filters have gained
increased interest as effective tools for ensuring the safety of classical
control systems, but their applications in vision-based control settings have
so far been limited. Pre-trained vision models (PVRs) have been shown to be
effective perception backbones for control in various robotics domains. In this
paper, we are interested in examining their effectiveness when used for
designing vision-based safety filters. We use them as backbones for classifiers
defining failure sets, for Hamilton-Jacobi (HJ) reachability-based safety
filters, and for latent world models. We discuss the trade-offs between
training from scratch, fine-tuning, and freezing the PVRs when training the
models they are backbones for. We also evaluate whether one of the PVRs is
superior across all tasks, evaluate whether learned world models or Q-functions
are better for switching decisions to safe policies, and discuss practical
considerations for deploying these PVRs on resource-constrained devices.

</details>


### [104] [COMPASS: Confined-space Manipulation Planning with Active Sensing Strategy](https://arxiv.org/abs/2509.14787)
*Qixuan Li,Chen Le,Dongyue Huang,Jincheng Yu,Xinlei Chen*

Main category: cs.RO

TL;DR: COMPASS框架通过智能探索和操作优化，显著提升受限环境中的操作成功率。


<details>
  <summary>Details</summary>
Motivation: 受限和杂乱环境中的操作因部分可观测性和复杂配置空间而具有挑战性，需要智能探索策略以安全理解场景并搜索目标。

Method: 提出COMPASS框架，结合操作感知的采样规划器、近场感知扫描构建局部碰撞地图、多目标效用函数选择视点，以及约束操作优化策略生成操作姿态。

Result: 在仿真中，相比仅考虑信息增益的探索方法，操作成功率提高了24.25%；真实实验验证了框架在受限环境中的主动感知和操作能力。

Conclusion: COMPASS框架通过多阶段探索和操作策略，显著提高了在受限和杂乱环境中的操作成功率，验证了其在主动感知和操作中的有效性。

Abstract: Manipulation in confined and cluttered environments remains a significant
challenge due to partial observability and complex configuration spaces.
Effective manipulation in such environments requires an intelligent exploration
strategy to safely understand the scene and search the target. In this paper,
we propose COMPASS, a multi-stage exploration and manipulation framework
featuring a manipulation-aware sampling-based planner. First, we reduce
collision risks with a near-field awareness scan to build a local collision
map. Additionally, we employ a multi-objective utility function to find
viewpoints that are both informative and conducive to subsequent manipulation.
Moreover, we perform a constrained manipulation optimization strategy to
generate manipulation poses that respect obstacle constraints. To
systematically evaluate method's performance under these difficulties, we
propose a benchmark of confined-space exploration and manipulation containing
four level challenging scenarios. Compared to exploration methods designed for
other robots and only considering information gain, our framework increases
manipulation success rate by 24.25% in simulations. Real-world experiments
demonstrate our method's capability for active sensing and manipulation in
confined environments.

</details>


### [105] [Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution](https://arxiv.org/abs/2509.14816)
*Humphrey Munn,Brendan Tidd,Peter Böhm,Marcus Gallagher,David Howard*

Main category: cs.RO

TL;DR: GCR-PPO通过多目标梯度冲突解决机制，显著提升多任务强化学习的性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习控制器将多目标聚合为单一标量奖励，导致调优成本高且易陷入局部最优，限制了可扩展性。

Method: 提出GCR-PPO，通过多头部评论家分解目标梯度，并根据优先级解决冲突。

Result: 在IsaacLab操作和运动基准测试中，GCR-PPO展现出优于并行PPO的可扩展性和性能（p=0.04），且计算开销无显著增加。

Conclusion: GCR-PPO在解决多目标强化学习任务中的梯度冲突方面表现出色，尤其在冲突性高的任务中性能提升显著（平均提升9.5%）。

Abstract: Reinforcement Learning (RL) robot controllers usually aggregate many task
objectives into one scalar reward. While large-scale proximal policy
optimisation (PPO) has enabled impressive results such as robust robot
locomotion in the real world, many tasks still require careful reward tuning
and are brittle to local optima. Tuning cost and sub-optimality grow with the
number of objectives, limiting scalability. Modelling reward vectors and their
trade-offs can address these issues; however, multi-objective methods remain
underused in RL for robotics because of computational cost and optimisation
difficulty. In this work, we investigate the conflict between gradient
contributions for each objective that emerge from scalarising the task
objectives. In particular, we explicitly address the conflict between
task-based rewards and terms that regularise the policy towards realistic
behaviour. We propose GCR-PPO, a modification to actor-critic optimisation that
decomposes the actor update into objective-wise gradients using a multi-headed
critic and resolves conflicts based on the objective priority. Our methodology,
GCR-PPO, is evaluated on the well-known IsaacLab manipulation and locomotion
benchmarks and additional multi-objective modifications on two related tasks.
We show superior scalability compared to parallel PPO (p = 0.04), without
significant computational overhead. We also show higher performance with more
conflicting tasks. GCR-PPO improves on large-scale PPO with an average
improvement of 9.5%, with high-conflict tasks observing a greater improvement.
The code is available at https://github.com/humphreymunn/GCR-PPO.

</details>


### [106] [CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human](https://arxiv.org/abs/2509.14889)
*Nan Sun,Yongchang Li,Chenxu Wang,Huiying Li,Huaping Liu*

Main category: cs.RO

TL;DR: CollabVLA 是一个自反思的视觉语言动作框架，通过混合专家设计和两阶段训练，显著提升了协作效率、可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言动作框架的领域过拟合、不可解释推理和生成模型高延迟问题，提升协作效率和可解释性。

Method: 通过混合专家设计，结合VLM的反思推理和扩散模型的动作生成，采用两阶段训练（动作基础和反思调优）来优化性能。

Result: 相比生成代理，CollabVLA 将标准化时间缩短约2倍，Dream次数减少约4倍，同时提高了成功率、可解释性并降低了延迟。

Conclusion: CollabVLA 将视觉语言动作框架从传统控制器转变为具备推理、协作和反思能力的辅助代理，为VLA领域的发展开辟了新方向。

Abstract: In this work, we present CollabVLA, a self-reflective vision-language-action
framework that transforms a standard visuomotor policy into a collaborative
assistant. CollabVLA tackles key limitations of prior VLAs, including domain
overfitting, non-interpretable reasoning, and the high latency of auxiliary
generative models, by integrating VLM-based reflective reasoning with
diffusion-based action generation under a mixture-of-experts design. Through a
two-stage training recipe of action grounding and reflection tuning, it
supports explicit self-reflection and proactively solicits human guidance when
confronted with uncertainty or repeated failure. It cuts normalized Time by ~2x
and Dream counts by ~4x vs. generative agents, achieving higher success rates,
improved interpretability, and balanced low latency compared with existing
methods. This work takes a pioneering step toward shifting VLAs from opaque
controllers to genuinely assistive agents capable of reasoning, acting, and
collaborating with humans.

</details>


### [107] [PERAL: Perception-Aware Motion Control for Passive LiDAR Excitation in Spherical Robots](https://arxiv.org/abs/2509.14915)
*Shenghai Yuan,Jason Wai Hao Yee,Weixiang Guo,Zhongyuan Liu,Thien-Minh Nguyen,Lihua Xie*

Main category: cs.RO

TL;DR: PERAL是一种无需硬件的被动LiDAR激发框架，通过非周期性振荡提升扫描多样性，显著改善地形感知和导航性能。


<details>
  <summary>Details</summary>
Motivation: 水平安装的LiDAR（如MID360）在特征稀缺环境中性能受限，现有解决方案（如静态倾斜或主动旋转）存在牺牲水平感知或增加成本的问题。

Method: PERAL通过建模内部差速驱动与传感器姿态的耦合，叠加有界非周期性振荡到目标或轨迹跟踪命令中。

Result: 实验表明，PERAL实现了96%的地图完整性，轨迹跟踪误差减少27%，并在近地面人检测中表现鲁棒。

Conclusion: PERAL框架通过被动激发LiDAR扫描，显著提升了地形感知和导航性能，且无需额外硬件，降低了重量、功耗和成本。

Abstract: Autonomous mobile robots increasingly rely on LiDAR-IMU odometry for
navigation and mapping, yet horizontally mounted LiDARs such as the MID360
capture few near-ground returns, limiting terrain awareness and degrading
performance in feature-scarce environments. Prior solutions - static tilt,
active rotation, or high-density sensors - either sacrifice horizontal
perception or incur added actuators, cost, and power. We introduce PERAL, a
perception-aware motion control framework for spherical robots that achieves
passive LiDAR excitation without dedicated hardware. By modeling the coupling
between internal differential-drive actuation and sensor attitude, PERAL
superimposes bounded, non-periodic oscillations onto nominal goal- or
trajectory-tracking commands, enriching vertical scan diversity while
preserving navigation accuracy. Implemented on a compact spherical robot, PERAL
is validated across laboratory, corridor, and tactical environments.
Experiments demonstrate up to 96 percent map completeness, a 27 percent
reduction in trajectory tracking error, and robust near-ground human detection,
all at lower weight, power, and cost compared with static tilt, active
rotation, and fixed horizontal baselines. The design and code will be
open-sourced upon acceptance.

</details>


### [108] [Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale](https://arxiv.org/abs/2509.14932)
*Tobias Jülg,Pierre Krack,Seongjin Bien,Yannik Blei,Khaled Gamal,Ken Nakahara,Johannes Hechtl,Roberto Calandra,Wolfram Burgard,Florian Walter*

Main category: cs.RO

TL;DR: RCS是一个为支持大规模通用机器人策略研究设计的轻量级生态系统，通过模块化架构和统一接口促进模拟到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 传统机器人软件框架在大规模通用策略研究中成为瓶颈，而模拟环境对真实实验的支持有限，RCS旨在解决这一差距。

Method: RCS采用模块化、易扩展的分层架构，提供统一的模拟和物理机器人接口，支持大规模训练和真实实验。

Result: 实验验证了RCS在VLA和RL策略开发中的可用性和性能，并展示了模拟数据如何提升真实世界策略的表现。

Conclusion: RCS（Robot Control Stack）作为一个轻量级生态系统，成功填补了传统机器人软件框架在支持大规模通用策略研究中的不足，通过模块化设计和统一接口促进了模拟到现实的迁移。

Abstract: Vision-Language-Action models (VLAs) mark a major shift in robot learning.
They replace specialized architectures and task-tailored components of expert
policies with large-scale data collection and setup-specific fine-tuning. In
this machine learning-focused workflow that is centered around models and
scalable training, traditional robotics software frameworks become a
bottleneck, while robot simulations offer only limited support for
transitioning from and to real-world experiments. In this work, we close this
gap by introducing Robot Control Stack (RCS), a lean ecosystem designed from
the ground up to support research in robot learning with large-scale generalist
policies. At its core, RCS features a modular and easily extensible layered
architecture with a unified interface for simulated and physical robots,
facilitating sim-to-real transfer. Despite its minimal footprint and
dependencies, it offers a complete feature set, enabling both real-world
experiments and large-scale training in simulation. Our contribution is
twofold: First, we introduce the architecture of RCS and explain its design
principles. Second, we evaluate its usability and performance along the
development cycle of VLA and RL policies. Our experiments also provide an
extensive evaluation of Octo, OpenVLA, and Pi Zero on multiple robots and shed
light on how simulation data can improve real-world policy performance. Our
code, datasets, weights, and videos are available at:
https://robotcontrolstack.github.io/

</details>


### [109] [CAD-Driven Co-Design for Flight-Ready Jet-Powered Humanoids](https://arxiv.org/abs/2509.14935)
*Punith Reddy Vanteddu,Davide Gorbani,Giuseppe L'Erario,Hosameldin Awadalla Omer Mohamed,Fabio Bergonti,Daniele Pucci*

Main category: cs.RO

TL;DR: 本文提出了一种CAD驱动的协同设计框架，通过多目标优化和聚类分析，优化喷气动力空中人形机器人的设计和控制参数，以实现动态约束轨迹的高效执行。


<details>
  <summary>Details</summary>
Motivation: 优化喷气动力空中人形机器人以执行动态约束轨迹，确保结构有效性和仿真工具兼容性。

Method: 使用CAD驱动的协同设计框架，通过设计实验（DoE）生成5,000个几何变体和机械可行的设计，采用K-means聚类减少计算成本，并通过NSGA-II算法进行多目标优化。

Result: 框架成功生成并验证了一组飞行就绪的人形配置和控制参数，最小化了轨迹跟踪误差和机械能耗。

Conclusion: 该框架输出了一组经过验证的飞行人形机器人配置和控制参数，为选择和实施可行的空中人形设计提供了结构化方法。

Abstract: This paper presents a CAD-driven co-design framework for optimizing
jet-powered aerial humanoid robots to execute dynamically constrained
trajectories. Starting from the iRonCub-Mk3 model, a Design of Experiments
(DoE) approach is used to generate 5,000 geometrically varied and mechanically
feasible designs by modifying limb dimensions, jet interface geometry (e.g.,
angle and offset), and overall mass distribution. Each model is constructed
through CAD assemblies to ensure structural validity and compatibility with
simulation tools. To reduce computational cost and enable parameter sensitivity
analysis, the models are clustered using K-means, with representative centroids
selected for evaluation. A minimum-jerk trajectory is used to assess flight
performance, providing position and velocity references for a momentum-based
linearized Model Predictive Control (MPC) strategy. A multi-objective
optimization is then conducted using the NSGA-II algorithm, jointly exploring
the space of design centroids and MPC gain parameters. The objectives are to
minimize trajectory tracking error and mechanical energy expenditure. The
framework outputs a set of flight-ready humanoid configurations with validated
control parameters, offering a structured method for selecting and implementing
feasible aerial humanoid designs.

</details>


### [110] [A Novel Task-Driven Diffusion-Based Policy with Affordance Learning for Generalizable Manipulation of Articulated Objects](https://arxiv.org/abs/2509.14939)
*Hao Zhang,Zhen Kan,Weiwei Shang,Yongduan Song*

Main category: cs.RO

TL;DR: DART结合扩散策略、可供性学习和LTL，提升关节灵巧操作的泛化能力和效率，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决关节物体操作和跨类别泛化的挑战，提升灵巧操作的效率和泛化能力。

Method: DART采用扩散策略，结合可供性学习识别最佳交互点，并利用线性时序逻辑（LTL）理解任务语义，同时通过基于交互数据的优化方法精炼动作。

Result: DART在操作能力、泛化性能、迁移推理和鲁棒性方面优于现有方法。

Conclusion: DART通过结合扩散策略、可供性学习和线性时序逻辑表示，显著提升了关节灵巧操作的效率和泛化能力，实验证明其在操作能力、泛化性能、迁移推理和鲁棒性方面优于现有方法。

Abstract: Despite recent advances in dexterous manipulations, the manipulation of
articulated objects and generalization across different categories remain
significant challenges. To address these issues, we introduce DART, a novel
framework that enhances a diffusion-based policy with affordance learning and
linear temporal logic (LTL) representations to improve the learning efficiency
and generalizability of articulated dexterous manipulation. Specifically, DART
leverages LTL to understand task semantics and affordance learning to identify
optimal interaction points. The {diffusion-based policy} then generalizes these
interactions across various categories. Additionally, we exploit an
optimization method based on interaction data to refine actions, overcoming the
limitations of traditional diffusion policies that typically rely on offline
reinforcement learning or learning from demonstrations. Experimental results
demonstrate that DART outperforms most existing methods in manipulation
ability, generalization performance, transfer reasoning, and robustness. For
more information, visit our project website at:
https://sites.google.com/view/dart0257/.

</details>


### [111] [Multi-CAP: A Multi-Robot Connectivity-Aware Hierarchical Coverage Path Planning Algorithm for Unknown Environments](https://arxiv.org/abs/2509.14941)
*Zongyuan Shen,Burhanuddin Shirose,Prasanna Sriganesh,Bhaskar Vundurthy,Howie Choset,Matthew Travers*

Main category: cs.RO

TL;DR: Multi-CAP是一种层次化覆盖路径规划算法，通过连通性感知图和VRP模型优化多机器人协调，显著提升覆盖效率。


<details>
  <summary>Details</summary>
Motivation: 多机器人在未知大环境中高效协调覆盖是一个重要挑战，需最小化总覆盖路径长度并减少机器人间冲突。

Method: Multi-CAP通过构建并动态维护一个邻接图来表示环境的连通子区域，将子区域分配问题建模为车辆路径问题（VRP），为每个机器人计算不重叠的路径，并在子区域内独立调整覆盖策略。

Result: 仿真和多机器人硬件实验表明，Multi-CAP在覆盖时间、总路径长度和路径重叠率等指标上显著优于现有方法。

Conclusion: Multi-CAP显著优于现有方法，在覆盖时间、总路径长度和路径重叠率等关键指标上表现出色，验证了其连通性感知图和全局路径规划器的重要性。

Abstract: Efficient coordination of multiple robots for coverage of large, unknown
environments is a significant challenge that involves minimizing the total
coverage path length while reducing inter-robot conflicts. In this paper, we
introduce a Multi-robot Connectivity-Aware Planner (Multi-CAP), a hierarchical
coverage path planning algorithm that facilitates multi-robot coordination
through a novel connectivity-aware approach. The algorithm constructs and
dynamically maintains an adjacency graph that represents the environment as a
set of connected subareas. Critically, we make the assumption that the
environment, while unknown, is bounded. This allows for incremental refinement
of the adjacency graph online to ensure its structure represents the physical
layout of the space, both in observed and unobserved areas of the map as robots
explore the environment. We frame the task of assigning subareas to robots as a
Vehicle Routing Problem (VRP), a well-studied problem for finding optimal
routes for a fleet of vehicles. This is used to compute disjoint tours that
minimize redundant travel, assigning each robot a unique, non-conflicting set
of subareas. Each robot then executes its assigned tour, independently adapting
its coverage strategy within each subarea to minimize path length based on
real-time sensor observations of the subarea. We demonstrate through
simulations and multi-robot hardware experiments that Multi-CAP significantly
outperforms state-of-the-art methods in key metrics, including coverage time,
total path length, and path overlap ratio. Ablation studies further validate
the critical role of our connectivity-aware graph and the global tour planner
in achieving these performance gains.

</details>


### [112] [Human Interaction for Collaborative Semantic SLAM using Extended Reality](https://arxiv.org/abs/2509.14949)
*Laura Ribeiro,Muhammad Shaheer,Miguel Fernandez-Cortizas,Ali Tourani,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: HICS-SLAM是一种人机协作的语义SLAM框架，通过实时交互提升语义地图质量，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有语义SLAM系统在遮挡、数据不完整或几何模糊等现实场景中的局限性。

Method: 提出了一种基于图的语义融合方法，将人类干预与机器人感知结合，实现可扩展的协作。

Result: 在真实建筑工地数据集上的实验表明，该方法在房间检测准确率、地图精度和语义完整性方面优于自动化基线。

Conclusion: HICS-SLAM通过人机协作显著提升了语义SLAM在复杂环境中的性能，展示了其在未来扩展中的潜力。

Abstract: Semantic SLAM (Simultaneous Localization and Mapping) systems enrich robot
maps with structural and semantic information, enabling robots to operate more
effectively in complex environments. However, these systems struggle in
real-world scenarios with occlusions, incomplete data, or ambiguous geometries,
as they cannot fully leverage the higher-level spatial and semantic knowledge
humans naturally apply. We introduce HICS-SLAM, a Human-in-the-Loop semantic
SLAM framework that uses a shared extended reality environment for real-time
collaboration. The system allows human operators to directly interact with and
visualize the robot's 3D scene graph, and add high-level semantic concepts
(e.g., rooms or structural entities) into the mapping process. We propose a
graph-based semantic fusion methodology that integrates these human
interventions with robot perception, enabling scalable collaboration for
enhanced situational awareness. Experimental evaluations on real-world
construction site datasets demonstrate improvements in room detection accuracy,
map precision, and semantic completeness compared to automated baselines,
demonstrating both the effectiveness of the approach and its potential for
future extensions.

</details>


### [113] [Exploratory Movement Strategies for Texture Discrimination with a Neuromorphic Tactile Sensor](https://arxiv.org/abs/2509.14954)
*Xingchen Xu,Ao Li,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: 提出受人类启发的神经形态触觉传感框架，通过多种动作测试，确定滑动+旋转为最佳纹理分类策略，准确率87.33%，功耗极低。


<details>
  <summary>Details</summary>
Motivation: 受人类探索策略启发，开发一种神经形态触觉传感框架，以提升机器人纹理分类的准确性和能效。

Method: 使用NeuroTac传感器捕捉神经形态触觉数据，测试六种不同动作（滑动、旋转、敲击及其组合），并在固定和复杂环境下评估其效果。

Result: 在复杂条件下，滑动+旋转动作达到87.33%的最高准确率，功耗仅为8.04 mW。

Conclusion: 滑动结合旋转动作是神经形态触觉传感在纹理分类任务中的最佳探索策略，具有显著提升机器人环境交互能力的潜力。

Abstract: We propose a neuromorphic tactile sensing framework for robotic texture
classification that is inspired by human exploratory strategies. Our system
utilizes the NeuroTac sensor to capture neuromorphic tactile data during a
series of exploratory motions. We first tested six distinct motions for texture
classification under fixed environment: sliding, rotating, tapping, as well as
the combined motions: sliding+rotating, tapping+rotating, and tapping+sliding.
We chose sliding and sliding+rotating as the best motions based on final
accuracy and the sample timing length needed to reach converged accuracy. In
the second experiment designed to simulate complex real-world conditions, these
two motions were further evaluated under varying contact depth and speeds.
Under these conditions, our framework attained the highest accuracy of 87.33\%
with sliding+rotating while maintaining an extremely low power consumption of
only 8.04 mW. These results suggest that the sliding+rotating motion is the
optimal exploratory strategy for neuromorphic tactile sensing deployment in
texture classification tasks and holds significant promise for enhancing
robotic environmental interaction.

</details>


### [114] [Affordance-Based Disambiguation of Surgical Instructions for Collaborative Robot-Assisted Surgery](https://arxiv.org/abs/2509.14967)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 论文提出了一种手术机器人辅助框架，通过视觉上下文和知识库消除口头指令歧义，采用双重集保形预测确保安全，评估显示60%的歧义消除率。


<details>
  <summary>Details</summary>
Motivation: 手术中口头指令的固有歧义性影响了人机协作的效果，因此需要一种能够解释和消除歧义的机器人辅助系统，以提高手术安全性和效率。

Method: 框架采用基于两层次可供性推理的过程，结合多模态视觉-语言模型分析手术场景，并利用工具能力知识库进行指令推理。双重集保形预测方法用于提供机器人决策的统计严格置信度。

Result: 在胆囊切除术视频的模糊手术请求数据集上评估，框架实现了60%的总体歧义消除率，并提出了一种更安全的人机交互方法。

Conclusion: 该论文提出了一个框架，通过结合视觉上下文和知识库，有效解决手术中口头指令的歧义问题，并采用双重集保形预测方法确保患者安全，实现了60%的总体歧义消除率。

Abstract: Effective human-robot collaboration in surgery is affected by the inherent
ambiguity of verbal communication. This paper presents a framework for a
robotic surgical assistant that interprets and disambiguates verbal
instructions from a surgeon by grounding them in the visual context of the
operating field. The system employs a two-level affordance-based reasoning
process that first analyzes the surgical scene using a multimodal
vision-language model and then reasons about the instruction using a knowledge
base of tool capabilities. To ensure patient safety, a dual-set conformal
prediction method is used to provide a statistically rigorous confidence
measure for robot decisions, allowing it to identify and flag ambiguous
commands. We evaluated our framework on a curated dataset of ambiguous surgical
requests from cholecystectomy videos, demonstrating a general disambiguation
rate of 60% and presenting a method for safer human-robot interaction in the
operating room.

</details>


### [115] [PA-MPPI: Perception-Aware Model Predictive Path Integral Control for Quadrotor Navigation in Unknown Environments](https://arxiv.org/abs/2509.14978)
*Yifan Zhai,Rudolf Reiter,Davide Scaramuzza*

Main category: cs.RO

TL;DR: PA-MPPI通过感知感知调整轨迹，显著提升了四旋翼在未知环境中的导航性能，实验证明其比基线方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼在未知环境中导航的三大挑战：非凸自由空间、四旋翼特定动力学和目标、以及需要探索未知区域以找到通往目标的路径。

Method: 引入Perception-Aware MPPI（PA-MPPI），通过在线调整轨迹基于感知目标，特别是在目标被遮挡时，感知成本会偏向能够感知未知区域的轨迹。

Result: 硬件实验表明，PA-MPPI在50 Hz运行时性能比基线提升高达100%，且在状态最先进的MPPI失败的情况下表现优异。

Conclusion: PA-MPPI通过感知感知调整轨迹，显著提升了在未知环境中的导航性能，尤其是在目标被遮挡时，能够有效探索未知区域并找到替代路径。

Abstract: Quadrotor navigation in unknown environments is critical for practical
missions such as search-and-rescue. Solving it requires addressing three key
challenges: the non-convexity of free space due to obstacles,
quadrotor-specific dynamics and objectives, and the need for exploration of
unknown regions to find a path to the goal. Recently, the Model Predictive Path
Integral (MPPI) method has emerged as a promising solution that solves the
first two challenges. By leveraging sampling-based optimization, it can
effectively handle non-convex free space while directly optimizing over the
full quadrotor dynamics, enabling the inclusion of quadrotor-specific costs
such as energy consumption. However, its performance in unknown environments is
limited, as it lacks the ability to explore unknown regions when blocked by
large obstacles. To solve this issue, we introduce Perception-Aware MPPI
(PA-MPPI). Here, perception-awareness is defined as adapting the trajectory
online based on perception objectives. Specifically, when the goal is occluded,
PA-MPPI's perception cost biases trajectories that can perceive unknown
regions. This expands the mapped traversable space and increases the likelihood
of finding alternative paths to the goal. Through hardware experiments, we
demonstrate that PA-MPPI, running at 50 Hz with our efficient perception and
mapping module, performs up to 100% better than the baseline in our challenging
settings where the state-of-the-art MPPI fails. In addition, we demonstrate
that PA-MPPI can be used as a safe and robust action policy for navigation
foundation models, which often provide goal poses that are not directly
reachable.

</details>


### [116] [M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation](https://arxiv.org/abs/2509.14980)
*Ju Dong,Lei Zhang,Liding Zhang,Yao Ling,Yu Fu,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang*

Main category: cs.RO

TL;DR: M4Diffuser 结合多视角扩散策略和 ReM-QP 控制器，显著提升移动操作性能，成功率高且碰撞少。


<details>
  <summary>Details</summary>
Motivation: 现有单视角方法在无结构环境中因视野有限、探索能力不足和泛化能力差而表现不佳，且经典控制器在奇异点附近效率低。

Method: 提出了一种混合框架 M4Diffuser，结合了多视角扩散策略（生成任务相关的末端执行器目标）和 ReM-QP 控制器（高效执行目标并处理奇异点）。

Result: 在仿真和真实环境中，M4Diffuser 的成功率比基线高 7% 至 56%，碰撞减少 3% 至 31%。

Conclusion: M4Diffuser 通过结合多视角扩散策略和新型 ReM-QP 控制器，显著提升了移动操作的成功率并减少了碰撞，为无结构环境中的可靠移动操作提供了解决方案。

Abstract: Mobile manipulation requires the coordinated control of a mobile base and a
robotic arm while simultaneously perceiving both global scene context and
fine-grained object details. Existing single-view approaches often fail in
unstructured environments due to limited fields of view, exploration, and
generalization abilities. Moreover, classical controllers, although stable,
struggle with efficiency and manipulability near singularities. To address
these challenges, we propose M4Diffuser, a hybrid framework that integrates a
Multi-View Diffusion Policy with a novel Reduced and Manipulability-aware QP
(ReM-QP) controller for mobile manipulation. The diffusion policy leverages
proprioceptive states and complementary camera perspectives with both
close-range object details and global scene context to generate task-relevant
end-effector goals in the world frame. These high-level goals are then executed
by the ReM-QP controller, which eliminates slack variables for computational
efficiency and incorporates manipulability-aware preferences for robustness
near singularities. Comprehensive experiments in simulation and real-world
environments show that M4Diffuser achieves 7 to 56 percent higher success rates
and reduces collisions by 3 to 31 percent over baselines. Our approach
demonstrates robust performance for smooth whole-body coordination, and strong
generalization to unseen tasks, paving the way for reliable mobile manipulation
in unstructured environments. Details of the demo and supplemental material are
available on our project website https://sites.google.com/view/m4diffuser.

</details>


### [117] [The Role of Touch: Towards Optimal Tactile Sensing Distribution in Anthropomorphic Hands for Dexterous In-Hand Manipulation](https://arxiv.org/abs/2509.14984)
*João Damião Almeida,Egidio Falotico,Cecilia Laschi,José Santos-Victor*

Main category: cs.RO

TL;DR: 该论文研究了手部不同区域的触觉反馈对物体重定向任务的影响，通过深度强化学习分析最优传感器配置，为人形机器人末端执行器设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 探讨触觉反馈从手指和手掌不同区域对执行手内物体重定向任务的影响，以优化触觉传感器网络的配置。

Method: 通过分析不同手部区域的触觉反馈对深度强化学习控制策略稳健性的影响，并研究物体特性与最佳传感器放置之间的关系。

Result: 研究确定了有助于提高操作效率和准确性的触觉传感配置，揭示了物体特性与最佳传感器放置之间的关系。

Conclusion: 该研究为设计和使用具有增强操作能力的人形末端执行器提供了宝贵见解，明确了哪些触觉传感配置有助于提高操作的效率和准确性。

Abstract: In-hand manipulation tasks, particularly in human-inspired robotic systems,
must rely on distributed tactile sensing to achieve precise control across a
wide variety of tasks. However, the optimal configuration of this network of
sensors is a complex problem, and while the fingertips are a common choice for
placing sensors, the contribution of tactile information from other regions of
the hand is often overlooked. This work investigates the impact of tactile
feedback from various regions of the fingers and palm in performing in-hand
object reorientation tasks. We analyze how sensory feedback from different
parts of the hand influences the robustness of deep reinforcement learning
control policies and investigate the relationship between object
characteristics and optimal sensor placement. We identify which tactile sensing
configurations contribute to improving the efficiency and accuracy of
manipulation. Our results provide valuable insights for the design and use of
anthropomorphic end-effectors with enhanced manipulation capabilities.

</details>


### [118] [ExT: Towards Scalable Autonomous Excavation via Large-Scale Multi-Task Pretraining and Fine-Tuning](https://arxiv.org/abs/2509.14992)
*Yifan Zhai,Lorenzo Terenzi,Patrick Frey,Diego Garcia Soto,Pascal Egli,Marco Hutter*

Main category: cs.RO

TL;DR: ExT框架通过大规模演示预训练和微调策略，实现了自主挖掘的高精度和快速适应新任务的能力，展示了其在通用自主挖掘中的潜力。


<details>
  <summary>Details</summary>
Motivation: 自主挖掘设备的部署在经济和社会上具有重要意义，但现有方法依赖于高度工程化、任务特定的控制器，需要为每个新场景进行大量手动调整。

Method: ExT是一个统一的开放源代码框架，用于大规模演示收集、预训练和多任务挖掘策略的微调。策略首先通过混合专家收集的大规模演示进行训练，然后通过监督微调（SFT）或强化学习微调（RLFT）进行微调以适应新任务或操作条件。

Result: ExT策略能够以厘米级精度执行完整的挖掘周期，并能从模拟成功转移到真实机器，性能与专业单任务控制器相当。

Conclusion: ExT框架展示了作为可扩展和通用自主挖掘基础的潜力，能够快速适应新任务、非分布条件和机器配置，同时保持先前学习任务的强性能。

Abstract: Scaling up the deployment of autonomous excavators is of great economic and
societal importance. Yet it remains a challenging problem, as effective systems
must robustly handle unseen worksite conditions and new hardware
configurations. Current state-of-the-art approaches rely on highly engineered,
task-specific controllers, which require extensive manual tuning for each new
scenario. In contrast, recent advances in large-scale pretrained models have
shown remarkable adaptability across tasks and embodiments in domains such as
manipulation and navigation, but their applicability to heavy construction
machinery remains largely unexplored. In this work, we introduce ExT, a unified
open-source framework for large-scale demonstration collection, pretraining,
and fine-tuning of multitask excavation policies. ExT policies are first
trained on large-scale demonstrations collected from a mix of experts, then
fine-tuned either with supervised fine-tuning (SFT) or reinforcement learning
fine-tuning (RLFT) to specialize to new tasks or operating conditions. Through
both simulation and real-world experiments, we show that pretrained ExT
policies can execute complete excavation cycles with centimeter-level accuracy,
successfully transferring from simulation to real machine with performance
comparable to specialized single-task controllers. Furthermore, in simulation,
we demonstrate that ExT's fine-tuning pipelines allow rapid adaptation to new
tasks, out-of-distribution conditions, and machine configurations, while
maintaining strong performance on previously learned tasks. These results
highlight the potential of ExT to serve as a foundation for scalable and
generalizable autonomous excavation.

</details>


### [119] [Semantic-LiDAR-Inertial-Wheel Odometry Fusion for Robust Localization in Large-Scale Dynamic Environments](https://arxiv.org/abs/2509.14999)
*Haoxuan Jiang,Peicong Qian,Yusen Xie,Linwei Zheng,Xiaocong Li,Ming Liu,Jun Ma*

Main category: cs.RO

TL;DR: 该论文提出了一种多传感器融合框架，用于大型动态环境中的高精度定位，通过语义信息和自适应策略减少漂移，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 大型动态环境中的可靠、无漂移全局定位对自主导航至关重要，但面临显著挑战。现有方法在长期轨迹漂移和动态环境适应性方面存在不足。

Method: 研究采用了一种高效的语义体素地图表示和改进的扫描匹配算法，结合全局语义信息减少漂移，并通过紧密耦合的多传感器融合迭代误差状态卡尔曼滤波器（iESKF）融合激光雷达、IMU和轮式里程计数据。此外，还引入了3D自适应缩放策略以应对地形变化和动态运动的挑战。

Result: 在100万平方米的自动化港口进行的广泛真实世界实验中，系统在3,575小时的操作数据中表现优异，显著优于当前最先进的基于激光雷达的定位方法。

Conclusion: 该论文提出的紧密耦合的语义-激光雷达-惯性-轮式里程计融合框架在大型动态环境中表现出色，显著减少了长期轨迹漂移，并提供了高精度的状态估计和鲁棒的定位。

Abstract: Reliable, drift-free global localization presents significant challenges yet
remains crucial for autonomous navigation in large-scale dynamic environments.
In this paper, we introduce a tightly-coupled Semantic-LiDAR-Inertial-Wheel
Odometry fusion framework, which is specifically designed to provide
high-precision state estimation and robust localization in large-scale dynamic
environments. Our framework leverages an efficient semantic-voxel map
representation and employs an improved scan matching algorithm, which utilizes
global semantic information to significantly reduce long-term trajectory drift.
Furthermore, it seamlessly fuses data from LiDAR, IMU, and wheel odometry using
a tightly-coupled multi-sensor fusion Iterative Error-State Kalman Filter
(iESKF). This ensures reliable localization without experiencing abnormal
drift. Moreover, to tackle the challenges posed by terrain variations and
dynamic movements, we introduce a 3D adaptive scaling strategy that allows for
flexible adjustments to wheel odometry measurement weights, thereby enhancing
localization precision. This study presents extensive real-world experiments
conducted in a one-million-square-meter automated port, encompassing 3,575
hours of operational data from 35 Intelligent Guided Vehicles (IGVs). The
results consistently demonstrate that our system outperforms state-of-the-art
LiDAR-based localization methods in large-scale dynamic environments,
highlighting the framework's reliability and practical value.

</details>


### [120] [Online Multi-Robot Coordination and Cooperation with Task Precedence Relationships](https://arxiv.org/abs/2509.15052)
*Walker Gosrich,Saurav Agarwal,Kashish Garg,Siddharth Mayya,Matthew Malencia,Mark Yim,Vijay Kumar*

Main category: cs.RO

TL;DR: 提出了一种结合任务优先关系、内部协调和机器人联盟的多机器人任务分配新方法，通过网络流算法和在线迭代优化，显著提高了任务分配的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的多机器人任务分配方法难以处理复杂任务关系和高效协调，因此需要一种新的方法来优化任务分配并提高整体性能。

Method: 采用任务图来定义任务及其关系，并使用奖励函数模型来评估联盟规模和前置任务表现的影响。提出了基于网络流的高效近似算法，以及一种在线迭代重分配算法以应对任务失败和模型不准确性。

Result: 在随机任务和奖励函数的测试平台上，该方法在性能上优于混合整数求解器和贪婪启发式算法。在高级模拟器中验证了方法的有效性，能够生成高保真度的任务计划。

Conclusion: 论文提出了一种新的多机器人任务分配方法，通过结合任务间的复杂优先关系、高效的内部任务协调以及机器人联盟的合作，显著提高了任务分配的效率和鲁棒性。

Abstract: We propose a new formulation for the multi-robot task allocation problem that
incorporates (a) complex precedence relationships between tasks, (b) efficient
intra-task coordination, and (c) cooperation through the formation of robot
coalitions. A task graph specifies the tasks and their relationships, and a set
of reward functions models the effects of coalition size and preceding task
performance. Maximizing task rewards is NP-hard; hence, we propose network
flow-based algorithms to approximate solutions efficiently. A novel online
algorithm performs iterative re-allocation, providing robustness to task
failures and model inaccuracies to achieve higher performance than offline
approaches. We comprehensively evaluate the algorithms in a testbed with random
missions and reward functions and compare them to a mixed-integer solver and a
greedy heuristic. Additionally, we validate the overall approach in an advanced
simulator, modeling reward functions based on realistic physical phenomena and
executing the tasks with realistic robot dynamics. Results establish efficacy
in modeling complex missions and efficiency in generating high-fidelity task
plans while leveraging task relationships.

</details>


### [121] [Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue](https://arxiv.org/abs/2509.15061)
*Xingyao Lin,Xinghao Zhu,Tianyi Lu,Sicheng Xie,Hui Zhang,Xipeng Qiu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.RO

TL;DR: 本文提出Ask-to-Clarify框架，通过多轮对话解决指令模糊性并生成动作，实验证明其在真实任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于VLA的具身智能体多为单向模式，无法处理现实场景中常见的模糊指令。本文旨在通过多轮对话解决指令模糊性问题，并实现端到端的低层动作生成。

Method: Ask-to-Clarify框架包含两个组件：一个用于协作的视觉语言模型（VLM）和一个用于生成动作的扩散模型。通过连接模块将VLM的输出转化为扩散模型的条件，并采用两阶段知识隔离策略进行训练。

Result: 在8项真实任务评估中，Ask-to-Clarify框架表现优于现有最先进的VLAs。

Conclusion: 本文提出的Ask-to-Clarify框架及其训练策略为构建协作型具身智能体提供了一条可行路径，实验结果表明其在8项真实任务中优于现有最先进的VLAs。

Abstract: The ultimate goal of embodied agents is to create collaborators that can
interact with humans, not mere executors that passively follow instructions.
This requires agents to communicate, coordinate, and adapt their actions based
on human feedback. Recently, advances in VLAs have offered a path toward this
goal. However, most current VLA-based embodied agents operate in a one-way
mode: they receive an instruction and execute it without feedback. This
approach fails in real-world scenarios where instructions are often ambiguous.
In this paper, we address this problem with the Ask-to-Clarify framework. Our
framework first resolves ambiguous instructions by asking questions in a
multi-turn dialogue. Then it generates low-level actions end-to-end.
Specifically, the Ask-to-Clarify framework consists of two components, one VLM
for collaboration and one diffusion for action. We also introduce a connection
module that generates conditions for the diffusion based on the output of the
VLM. This module adjusts the observation by instructions to create reliable
conditions. We train our framework with a two-stage knowledge-insulation
strategy. First, we fine-tune the collaboration component using
ambiguity-solving dialogue data to handle ambiguity. Then, we integrate the
action component while freezing the collaboration one. This preserves the
interaction abilities while fine-tuning the diffusion to generate actions. The
training strategy guarantees our framework can first ask questions, then
generate actions. During inference, a signal detector functions as a router
that helps our framework switch between asking questions and taking actions. We
evaluate the Ask-to-Clarify framework in 8 real-world tasks, where it
outperforms existing state-of-the-art VLAs. The results suggest that our
proposed framework, along with the training strategy, provides a path toward
collaborative embodied agents.

</details>


### [122] [Energy-Constrained Navigation for Planetary Rovers under Hybrid RTG-Solar Power](https://arxiv.org/abs/2509.15062)
*Tianxin Hu,Weixiang Guo,Ruimeng Liu,Xinhang Xu,Rui Qian,Jinyu Chen,Shenghai Yuan,Lihua Xie*

Main category: cs.RO

TL;DR: 本文提出了一种能量约束轨迹规划框架，确保行星漫游车在混合动力输入下的轨迹平滑、动态可行且符合功率限制，模拟结果显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 未来的行星探索漫游车需要在混合动力输入下长时间运行，而现有的地形感知规划器通常忽略能量可行性，专注于坡度或可穿越性。

Method: 通过将累积能量预算和瞬时功率约束整合到基于SE（2）的多项式轨迹优化中，结合物理基础的平移、旋转和电阻功率模型与基线子系统负载。

Result: 在类似月球地形的模拟中，该规划器生成的轨迹峰值功率在规定的极限内0.55%以内，而现有方法超出极限超过17%。

Conclusion: 本文提出的能量约束轨迹规划框架为长期行星任务提供了一种原则性和实用性的能量感知自主方法，确保轨迹平滑、动态可行且符合功率限制。

Abstract: Future planetary exploration rovers must operate for extended durations on
hybrid power inputs that combine steady radioisotope thermoelectric generator
(RTG) output with variable solar photovoltaic (PV) availability. While
energy-aware planning has been studied for aerial and underwater robots under
battery limits, few works for ground rovers explicitly model power flow or
enforce instantaneous power constraints. Classical terrain-aware planners
emphasize slope or traversability, and trajectory optimization methods
typically focus on geometric smoothness and dynamic feasibility, neglecting
energy feasibility. We present an energy-constrained trajectory planning
framework that explicitly integrates physics-based models of translational,
rotational, and resistive power with baseline subsystem loads, under hybrid
RTG-solar input. By incorporating both cumulative energy budgets and
instantaneous power constraints into SE(2)-based polynomial trajectory
optimization, the method ensures trajectories that are simultaneously smooth,
dynamically feasible, and power-compliant. Simulation results on lunar-like
terrain show that our planner generates trajectories with peak power within
0.55 percent of the prescribed limit, while existing methods exceed limits by
over 17 percent. This demonstrates a principled and practical approach to
energy-aware autonomy for long-duration planetary missions.

</details>


### [123] [AnoF-Diff: One-Step Diffusion-Based Anomaly Detection for Forceful Tool Use](https://arxiv.org/abs/2509.15153)
*Yating Lin,Zixuan Huang,Fan Yang,Dmitry Berenson*

Main category: cs.RO

TL;DR: AnoF-Diff是一种基于扩散模型的方法，用于从时间序列数据中提取力-扭矩特征并检测异常，在嘈杂数据集中表现更优。


<details>
  <summary>Details</summary>
Motivation: 直接应用现有方法到强力工具使用任务的数据上存在挑战，因为现实世界中的流传感器数据通常噪声大、非平稳且随任务和工具变化。

Method: 基于扩散模型提取力-扭矩特征，并利用这些特征进行异常检测。

Result: 在四个强力工具使用任务上，AnoF-Diff在F1-score和AUROC指标上优于其他最先进方法。

Conclusion: AnoF-Diff方法在嘈杂数据集上表现出更好的性能和更强的鲁棒性，适用于在线异常检测。

Abstract: Multivariate time-series anomaly detection, which is critical for identifying
unexpected events, has been explored in the field of machine learning for
several decades. However, directly applying these methods to data from forceful
tool use tasks is challenging because streaming sensor data in the real world
tends to be inherently noisy, exhibits non-stationary behavior, and varies
across different tasks and tools. To address these challenges, we propose a
method, AnoF-Diff, based on the diffusion model to extract force-torque
features from time-series data and use force-torque features to detect
anomalies. We compare our method with other state-of-the-art methods in terms
of F1-score and Area Under the Receiver Operating Characteristic curve (AUROC)
on four forceful tool-use tasks, demonstrating that our method has better
performance and is more robust to a noisy dataset. We also propose the method
of parallel anomaly score evaluation based on one-step diffusion and
demonstrate how our method can be used for online anomaly detection in several
forceful tool use experiments.

</details>


### [124] [Parallel Simulation of Contact and Actuation for Soft Growing Robots](https://arxiv.org/abs/2509.15180)
*Yitian Gao,Lucas Chen,Priyanka Bhovad,Sicheng Wang,Zachary Kingston,Laura H. Blumenschein*

Main category: cs.RO

TL;DR: 该研究开发了一个统一的建模框架，用于软生长机器人的生长、弯曲、驱动和障碍物接触，并通过优化设计展示了其在复杂环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了在更复杂的环境中成功导航，需要为软生长机器人添加主动转向功能。

Method: 开发了一个统一的建模框架，集成了藤蔓机器人的生长、弯曲、驱动和障碍物接触，并扩展了梁力矩模型以考虑驱动对生长过程中运动学的影响。

Result: 通过模型验证和实际机器人实验，展示了框架的有效性，并在设计优化任务中找到了能够最小化所需驱动器数量的机器人设计。

Conclusion: 通过优化设计和实际部署，研究展示了软生长机器人在复杂环境中的鲁棒性和实用性。

Abstract: Soft growing robots, commonly referred to as vine robots, have demonstrated
remarkable ability to interact safely and robustly with unstructured and
dynamic environments. It is therefore natural to exploit contact with the
environment for planning and design optimization tasks. Previous research has
focused on planning under contact for passively deforming robots with
pre-formed bends. However, adding active steering to these soft growing robots
is necessary for successful navigation in more complex environments. To this
end, we develop a unified modeling framework that integrates vine robot growth,
bending, actuation, and obstacle contact. We extend the beam moment model to
include the effects of actuation on kinematics under growth and then use these
models to develop a fast parallel simulation framework. We validate our model
and simulator with real robot experiments. To showcase the capabilities of our
framework, we apply our model in a design optimization task to find designs for
vine robots navigating through cluttered environments, identifying designs that
minimize the number of required actuators by exploiting environmental contacts.
We show the robustness of the designs to environmental and manufacturing
uncertainties. Finally, we fabricate an optimized design and successfully
deploy it in an obstacle-rich environment.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [125] [Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models](https://arxiv.org/abs/2509.14265)
*Siyuan Chen,Zhichao Lu,Qingfu Zhang*

Main category: cs.SE

TL;DR: EoK是一种基于LLM的进化框架，通过整合历史优化经验和RISC-V特定上下文，成功实现了自动化内核设计，性能优于人类和现有方法。


<details>
  <summary>Details</summary>
Motivation: 新兴硬件平台（如RISC-V）缺乏参考材料，传统LLM方法在CUDA等成熟领域表现良好，但在参考稀缺领域效果未经验证。EoK旨在解决这一问题，实现自动化内核设计。

Method: EoK采用基于LLM的进化程序搜索框架，通过挖掘和形式化现有内核库的可重用优化思想（通用设计原则+可操作思路），并结合RISC-V特定上下文的检索增强生成（RAG）技术，指导并行LLM探索。

Result: EoK在80个内核设计任务中实现了中位数1.27倍的加速，超越了人类专家，并比现有LLM方法提升了20%。

Conclusion: EoK通过整合历史优化经验和RISC-V特定上下文，成功实现了自动化内核设计，其性能超越了人类专家和现有LLM方法，展示了LLM在新兴硬件平台中的巨大潜力。

Abstract: Automated kernel design is critical for overcoming software ecosystem
barriers in emerging hardware platforms like RISC-V. While large language
models (LLMs) have shown promise for automated kernel optimization,
demonstrating success in CUDA domains with comprehensive technical documents
and mature codebases, their effectiveness remains unproven for reference-scarce
domains like RISC-V. We present Evolution of Kernels (EoK), a novel LLM-based
evolutionary program search framework that automates kernel design for domains
with limited reference material. EoK mitigates reference scarcity by mining and
formalizing reusable optimization ideas (general design principles + actionable
thoughts) from established kernel libraries' development histories; it then
guides parallel LLM explorations using these ideas, enriched via
Retrieval-Augmented Generation (RAG) with RISC-V-specific context, prioritizing
historically effective techniques. Empirically, EoK achieves a median 1.27x
speedup, surpassing human experts on all 80 evaluated kernel design tasks and
improving upon prior LLM-based automated kernel design methods by 20%. These
results underscore the viability of incorporating human experience into
emerging domains and highlight the immense potential of LLM-based automated
kernel optimization.

</details>


### [126] [Automated and Context-Aware Code Documentation Leveraging Advanced LLMs](https://arxiv.org/abs/2509.14273)
*Swapnil Sharma Sarker,Tanzina Taher Ifty*

Main category: cs.SE

TL;DR: 研究开发了一个上下文感知的Javadoc数据集，评估了五种开源LLM的性能，发现LLaMA 3.1在自动化Javadoc生成中表现最佳。


<details>
  <summary>Details</summary>
Motivation: Address the gap in template-based documentation generation (e.g., Javadoc) with publicly available LLMs and the lack of a Javadoc-specific dataset incorporating modern language features and contextual information.

Method: Developed a novel, context-aware dataset for Javadoc generation and evaluated five open-source LLMs (LLaMA-3.1, Gemma-2, Phi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups.

Result: LLaMA 3.1 demonstrates consistent performance, making it a reliable choice for automated Javadoc generation.

Conclusion: LLaMA 3.1 consistently performs well and is a reliable candidate for practical, automated Javadoc generation, offering a viable alternative to proprietary systems.

Abstract: Code documentation is essential to improve software maintainability and
comprehension. The tedious nature of manual code documentation has led to much
research on automated documentation generation. Existing automated approaches
primarily focused on code summarization, leaving a gap in template-based
documentation generation (e.g., Javadoc), particularly with publicly available
Large Language Models (LLMs). Furthermore, progress in this area has been
hindered by the lack of a Javadoc-specific dataset that incorporates modern
language features, provides broad framework/library coverage, and includes
necessary contextual information. This study aims to address these gaps by
developing a tailored dataset and assessing the capabilities of publicly
available LLMs for context-aware, template-based Javadoc generation. In this
work, we present a novel, context-aware dataset for Javadoc generation that
includes critical structural and semantic information from modern Java
codebases. We evaluate five open-source LLMs (including LLaMA-3.1, Gemma-2,
Phi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups and
provide a comparative analysis of their performance. Our results demonstrate
that LLaMA 3.1 performs consistently well and is a reliable candidate for
practical, automated Javadoc generation, offering a viable alternative to
proprietary systems.

</details>


### [127] [Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization](https://arxiv.org/abs/2509.14279)
*Robert Tjarko Lange,Qi Sun,Aaditya Prasad,Maxence Faldor,Yujin Tang,David Ha*

Main category: cs.SE

TL;DR: 本文提出robust-kbench基准和agentic框架，优化CUDA内核生成和验证，显著提升性能和正确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）方法在软件工程任务中表现优异，但在低层CUDA内核实现优化方面关注不足，且现有基准存在漏洞和测试条件单一的问题。

Method: 通过一个序列化的工作流程，将PyTorch代码转换为等效的CUDA内核，并采用基于LLM的进化元生成程序进行运行时优化。

Result: 在robust-kbench基准上，该方法生成的CUDA内核性能优于torch实现，并能融合操作和部署多种运行时优化策略。验证器工作流能准确分类错误内核，提升硬件验证效率。

Conclusion: 本文提出的agentic框架和robust-kbench基准在CUDA内核生成和优化方面表现出色，显著提升了性能和正确性评估的严谨性。

Abstract: Recent advances in large language models (LLMs) demonstrate their
effectiveness in scaling test-time compute for software engineering tasks.
However, these approaches often focus on high-level solutions, with limited
attention to optimizing low-level CUDA kernel implementations. Additionally,
existing kernel generation benchmarks suffer from exploitable loopholes and
insufficient diversity in testing conditions, hindering true generalization
assessment. To address these limitations, we introduce robust-kbench, a new
benchmark for rigorous evaluation of kernel performance and correctness across
varied scenarios. Furthermore, we present a comprehensive agentic framework
that automates CUDA kernel discovery, verification, and optimization. This
pipeline enables frontier LLMs to translate torch code to CUDA kernels and
iteratively improve their runtime within our robust evaluation setting. Our
sequential workflow first translates PyTorch code into equivalent CUDA kernels.
It then optimizes their runtime using a novel evolutionary meta-generation
procedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for
correctness and efficient filtering. Evaluated on robust-kbench, our approach
produces CUDA kernels outperforming torch implementations for practical
applications, including forward and backward passes. It can fuse operations and
deploy various runtime optimization strategies. The verifier workflow
accurately classifies incorrect kernels, enhancing hardware verification
efficiency.

</details>


### [128] [SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems](https://arxiv.org/abs/2509.14281)
*Xifeng Yao,Dongyu Lang,Wu Zhang,Xintong Guo,Huarui Xie,Yinhao Ni,Ping Liu,Guang Shen,Yi Bai,Dandan Tu,Changzheng Zhang*

Main category: cs.SE

TL;DR: 提出了一种合成模拟真实世界代码问题的框架，通过整合领域知识和技能，显著提升代码模型的性能。


<details>
  <summary>Details</summary>
Motivation: 由于真实世界编码问题的稀缺性限制了代码大型语言模型的进一步发展，需要一种新方法来合成模拟真实场景的代码问题。

Method: 通过系统整合从Stack Overflow和Kaggle等数据集中提取的领域知识、领域技能和编码技能，构建场景中心图，并设计图采样策略控制代码问题的生成。

Result: 实验结果表明，该方法在多样化的真实世界基准测试中优于现有开源大型语言模型。

Conclusion: 该论文提出的框架能够有效合成模拟真实世界场景的代码问题，显著提升代码大型语言模型的性能。

Abstract: Significant advancements have been made in the capabilities of code large
language models, leading to their rapid adoption and application across a wide
range of domains. However, their further advancements are often constrained by
the scarcity of real-world coding problems. To bridge this gap, we propose a
novel framework for synthesizing code problems that emulate authentic
real-world scenarios. This framework systematically integrates domain
knowledge, domain skills, and coding skills, all of which are meticulously
extracted from real-world programming-related datasets, including Stack
Overflow and Kaggle. The extracted elements serve as the foundational building
blocks for constructing code problems. To align the generated problems with
practical applications, application scenarios are also mined from the
aforementioned datasets. These scenarios are then utilized to construct a
scenario-centric graph that interconnects domain knowledge, domain skills, and
coding skills. Based on this structured representation, a sampling strategy on
the graph is designed, which effectively controls the generation of a code
problem with complexity and diversity, reflects real-world challenges.
Experimental results demonstrate that the proposed method consistently achieves
superior performance over state-of-the-art open-source large language models of
varying sizes and functionalities, including both coders and general-purpose
models, across a diverse set of real-world benchmarks.

</details>


### [129] [Monitoring Machine Learning Systems: A Multivocal Literature Review](https://arxiv.org/abs/2509.14294)
*Hira Naveed,Scott Barnett,Chetan Arora,John Grundy,Hourieh Khalajzadeh,Omar Haggag*

Main category: cs.SE

TL;DR: 该论文通过多声文献综述总结了机器学习监控的实践与不足，为学术界和从业者提供了解决方案选择、当前方法局限及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: Dynamic production environments make it challenging to maintain reliable machine learning (ML) systems. Runtime issues, such as changes in data patterns or operating contexts, that degrade model performance are a common occurrence in production settings. Monitoring enables early detection and mitigation of these runtime issues, helping maintain users' trust and prevent unwanted consequences for organizations.

Method: We conducted a multivocal literature review (MLR) following the well established guidelines by Garousi to investigate various aspects of ML monitoring approaches in 136 papers.

Result: We analyzed selected studies based on four key areas: (1) the motivations, goals, and context; (2) the monitored aspects, specific techniques, metrics, and tools; (3) the contributions and benefits; and (4) the current limitations. We also discuss several insights found in the studies, their implications, and recommendations for future research and practice.

Conclusion: Our MLR identifies and summarizes ML monitoring practices and gaps, emphasizing similarities and disconnects between formal and gray literature. Our study is valuable for both academics and practitioners, as it helps select appropriate solutions, highlights limitations in current approaches, and provides future directions for research and tool development.

Abstract: Context: Dynamic production environments make it challenging to maintain
reliable machine learning (ML) systems. Runtime issues, such as changes in data
patterns or operating contexts, that degrade model performance are a common
occurrence in production settings. Monitoring enables early detection and
mitigation of these runtime issues, helping maintain users' trust and prevent
unwanted consequences for organizations. Aim: This study aims to provide a
comprehensive overview of the ML monitoring literature. Method: We conducted a
multivocal literature review (MLR) following the well established guidelines by
Garousi to investigate various aspects of ML monitoring approaches in 136
papers. Results: We analyzed selected studies based on four key areas: (1) the
motivations, goals, and context; (2) the monitored aspects, specific
techniques, metrics, and tools; (3) the contributions and benefits; and (4) the
current limitations. We also discuss several insights found in the studies,
their implications, and recommendations for future research and practice.
Conclusion: Our MLR identifies and summarizes ML monitoring practices and gaps,
emphasizing similarities and disconnects between formal and gray literature.
Our study is valuable for both academics and practitioners, as it helps select
appropriate solutions, highlights limitations in current approaches, and
provides future directions for research and tool development.

</details>


### [130] [On the Illusion of Success: An Empirical Study of Build Reruns and Silent Failures in Industrial CI](https://arxiv.org/abs/2509.14347)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 本文首次实证研究了CI中的静默失败问题，发现11%的成功作业会被重新运行，并提出了11类静默失败原因及解决方案。


<details>
  <summary>Details</summary>
Motivation: 静默失败（即作业标记为成功但未完成全部任务）往往被忽视，导致错误进入生产环境。此前研究多关注间歇性失败，而静默失败的研究尚属空白。

Method: 通过对142,387个作业和81个工业项目的分析，使用混合效应模型（AUC为85%）评估了32个独立变量，并进一步分析了92个公开问题。

Result: 研究发现11%的成功作业会被重新运行，35%的重新运行发生在24小时后。静默失败的主要类别包括工件操作错误、缓存错误和忽略退出码等。

Conclusion: 本文通过实证研究发现，持续集成(CI)中的静默失败问题普遍存在，11%的成功作业会被重新运行，其中35%的重新运行发生在24小时后。研究提出了11类静默失败原因，并提出了提高CI可靠性的解决方案。

Abstract: Reliability of build outcomes is a cornerstone of effective Continuous
Integration (CI). Yet in practice, developers often struggle with
non-deterministic issues in the code or CI infrastructure, which undermine
trust in build results. When faced with such unexpected outcomes, developers
often repeatedly rerun jobs hoping for true success, but this practice is known
to increase CI costs and reduce productivity. While recent studies have focused
on intermittent job failures, no prior work has investigated silent failures,
where build jobs are marked as successful but fail to complete all or part of
their tasks. Such silent failures often go unnoticed, creating an illusion of
success with detrimental consequences such as bugs escaping into production.
This paper presents the first empirical study of silent failures through the
practice of rerunning successful jobs. An analysis of 142,387 jobs across 81
industrial projects shows that 11% of successful jobs are rerun, with 35% of
these reruns occurring after more than 24 hours. Using mixed-effects models on
32 independent variables (AUC of 85%), we identified key factors associated
with reruns of successful jobs, notably testing and static analysis tasks,
scripting languages like Shell, and developers prior rerun tendencies. A
further analysis of 92 public issues revealed 11 categories of silent failures
aligning with these factors, the most frequent being artifact operation errors,
caching errors, and ignored exit codes. Overall, our findings provide valuable
insights into the circumstances and causes of silent failures to raise
awareness among teams, and present solutions to improve CI reliability.

</details>


### [131] [CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning](https://arxiv.org/abs/2509.14373)
*Huy Le,Phong Nguyen,Hao Do,Tuan Nguyen,Thien Pham,Anh Nguyen-Duc,Tho Quan*

Main category: cs.SE

TL;DR: CodeLSI框架结合低秩优化和领域特定指令调优，提升自动化代码生成的性能，提供安全、经济高效的替代方案，支持更快、更具针对性的软件开发创新。


<details>
  <summary>Details</summary>
Motivation: 自动化代码生成使用基础模型（FMs）为提升软件开发效率提供了有前景的解决方案。然而，在确保领域特异性、成本效益和安全性方面仍存在挑战，尤其是在依赖第三方API时。

Method: CodeLSI应用低秩适应技术来降低模型预训练和微调的计算成本，并采用领域特定指令调优以使代码生成与组织需求对齐。该框架在现实世界的JavaScript编码任务上进行了实现和测试，使用了来自内部软件项目的数据集。

Result: 实验评估表明，CodeLSI生成了高质量、上下文感知的代码。在相关性、准确性和领域适应性方面优于基线模型。低秩优化的使用显著降低了资源需求，使在公司自有基础设施上进行可扩展训练成为可能。

Conclusion: CodeLSI通过结合低秩优化和领域特定调优，证明了其在提升基础模型（FMs）在自动化代码生成中的实用性和性能。该方法提供了一种安全、经济高效的替代商业API解决方案，并支持软件开发中更快、更具针对性的创新。

Abstract: Context: Automated code generation using Foundation Models (FMs) offers
promising solutions for enhancing software development efficiency. However,
challenges remain in ensuring domain specificity, cost-effectiveness, and
security - especially when relying on third-party APIs. This paper introduces
CodeLSI, a framework that combines low-rank optimization and domain-specific
instruction tuning to address these challenges.
  Objectives: The aim of this study is to develop and evaluate CodeLSI, a novel
approach for generating high-quality code tailored to specific domains, using
FMs fine-tuned on company infrastructure without dependence on external APIs.
  Methods: CodeLSI applies low-rank adaptation techniques to reduce the
computational cost of model pre-training and fine-tuning. Domain-specific
instruction tuning is employed to align code generation with organizational
needs. We implemented and tested the framework on real-world JavaScript coding
tasks using datasets drawn from internal software projects.
  Results: Experimental evaluations show that CodeLSI produces high-quality,
context aware code. It outperforms baseline models in terms of relevance,
accuracy, and domain fit. The use of low-rank optimization significantly
reduced resource requirements, enabling scalable training on company-owned
infrastructure.
  Conclusion: CodeLSI demonstrates that combining low-rank optimization with
domain specific tuning can enhance the practicality and performance of FMs for
automated code generation. This approach provides a secure, cost-efficient
alternative to commercial API based solutions and supports faster, more
targeted innovation in software development.

</details>


### [132] [A Taxonomy of Prompt Defects in LLM Systems](https://arxiv.org/abs/2509.14404)
*Haoye Tian,Chong Wang,BoYang Yang,Lyuye Zhang,Yang Liu*

Main category: cs.SE

TL;DR: 本文系统分类了LLM提示设计的六大缺陷维度，提出了缓解策略，并呼吁工程化方法确保LLM系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 提示设计目前主要依赖经验，小错误可能导致不可靠、不安全或低效的行为。因此，需要系统化的方法来识别和缓解提示缺陷。

Method: 本文通过六个维度（规范与意图、输入与内容、结构与格式、上下文与内存、性能与效率、可维护性与工程）对提示缺陷进行了系统调查和分类，并提供了具体示例和根本原因分析。

Result: 作者提出了一个主分类法，将缺陷、影响和补救措施联系起来，并为每种缺陷类型提炼了缓解策略。

Conclusion: 作者呼吁采用严格的工程导向方法，确保LLM驱动的系统在设计上可靠，并提出了开放的研究挑战。

Abstract: Large Language Models (LLMs) have become key components of modern software,
with prompts acting as their de-facto programming interface. However, prompt
design remains largely empirical and small mistakes can cascade into
unreliable, insecure, or inefficient behavior. This paper presents the first
systematic survey and taxonomy of prompt defects, recurring ways that prompts
fail to elicit their intended behavior from LLMs. We organize defects along six
dimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure
and Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6)
Maintainability and Engineering. Each dimension is refined into fine-grained
subtypes, illustrated with concrete examples and root cause analysis. Grounded
in software engineering principles, we show how these defects surface in real
development workflows and examine their downstream effects. For every subtype,
we distill mitigation strategies that span emerging prompt engineering
patterns, automated guardrails, testing harnesses, and evaluation frameworks.
We then summarize these strategies in a master taxonomy that links defect,
impact, and remedy. We conclude with open research challenges and a call for
rigorous engineering-oriented methodologies to ensure that LLM-driven systems
are dependable by design.

</details>


### [133] [An LLM-based multi-agent framework for agile effort estimation](https://arxiv.org/abs/2509.14483)
*Thanh-Long Bui,Hoa Khanh Dam,Rashina Hoda*

Main category: cs.SE

TL;DR: 论文提出了一种基于LLM的多智能体框架，用于敏捷工作量估算，解决了现有方法的主观性和交互不足问题，实验证明了其优越性和良好协作体验。


<details>
  <summary>Details</summary>
Motivation: 现有敏捷工作量估算方法依赖主观评估，导致估算不准确和不一致；而现有的机器学习方法虽准确但缺乏解释性和交互能力。

Method: 提出了一种新型的基于大型语言模型（LLMs）的多智能体框架，该框架不仅能生成估算，还能与人类开发者及其他智能体协调、沟通和讨论以达成共识。

Result: 在真实数据集上的评估表明，该方法在多数情况下优于现有技术；人类研究也显示开发者与智能体协作的体验极为积极。

Conclusion: 论文通过引入基于大型语言模型的多智能体框架，显著提升了敏捷软件开发中的工作量估算准确性和一致性，并增强了与人类开发者的协作体验。

Abstract: Effort estimation is a crucial activity in agile software development, where
teams collaboratively review, discuss, and estimate the effort required to
complete user stories in a product backlog. Current practices in agile effort
estimation heavily rely on subjective assessments, leading to inaccuracies and
inconsistencies in the estimates. While recent machine learning-based methods
show promising accuracy, they cannot explain or justify their estimates and
lack the capability to interact with human team members. Our paper fills this
significant gap by leveraging the powerful capabilities of Large Language
Models (LLMs). We propose a novel LLM-based multi-agent framework for agile
estimation that not only can produce estimates, but also can coordinate,
communicate and discuss with human developers and other agents to reach a
consensus. Evaluation results on a real-life dataset show that our approach
outperforms state-of-the-art techniques across all evaluation metrics in the
majority of the cases. Our human study with software development practitioners
also demonstrates an overwhelmingly positive experience in collaborating with
our agents in agile effort estimation.

</details>


### [134] [Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language](https://arxiv.org/abs/2509.14623)
*Hanlong Wan,Xing Lu,Yan Chen,Karthik Devaprasad,Laura Hinkle*

Main category: cs.SE

TL;DR: 研究探讨了LLM自动化生成Modelica控制模块的可行性，结果显示Claude Sonnet 4在特定条件下表现良好，但仍有局限性，同时显著减少了开发时间。


<details>
  <summary>Details</summary>
Motivation: Modelica是一种广泛使用的基于方程的语言，但开发控制模块需要大量劳动和专业知识，因此研究探索使用大型语言模型（LLMs）自动化生成控制描述语言模块。

Method: 研究开发了一个结构化工作流程，结合标准化提示支架、库感知接地、OpenModelica的自动编译和人工循环评估。实验在四个基本逻辑任务和五个控制模块上进行。

Result: GPT 4o在零样本模式下无法生成可执行的Modelica代码，而Claude Sonnet 4在精心设计的提示下对基本逻辑块实现了完全成功。控制模块的成功率达到83%，失败输出需要中等水平的人工修复。LLM辅助工作流程将平均开发时间从10-20小时减少到4-6小时，节省了40-60%的时间。

Conclusion: 论文强调了LLM辅助Modelica生成的潜力和当前限制，并指出了未来研究的方向，如预模拟验证、更强的接地和闭环评估。

Abstract: Dynamic energy systems and controls require advanced modeling frameworks to
design and test supervisory and fault tolerant strategies. Modelica is a widely
used equation based language, but developing control modules is labor intensive
and requires specialized expertise. This paper examines the use of large
language models (LLMs) to automate the generation of Control Description
Language modules in the Building Modelica Library as a case study. We developed
a structured workflow that combines standardized prompt scaffolds, library
aware grounding, automated compilation with OpenModelica, and human in the loop
evaluation. Experiments were carried out on four basic logic tasks (And, Or,
Not, and Switch) and five control modules (chiller enable/disable, bypass valve
control, cooling tower fan speed, plant requests, and relief damper control).
The results showed that GPT 4o failed to produce executable Modelica code in
zero shot mode, while Claude Sonnet 4 achieved up to full success for basic
logic blocks with carefully engineered prompts. For control modules, success
rates reached 83 percent, and failed outputs required medium level human repair
(estimated one to eight hours). Retrieval augmented generation often produced
mismatches in module selection (for example, And retrieved as Or), while a
deterministic hard rule search strategy avoided these errors. Human evaluation
also outperformed AI evaluation, since current LLMs cannot assess simulation
results or validate behavioral correctness. Despite these limitations, the LLM
assisted workflow reduced the average development time from 10 to 20 hours down
to 4 to 6 hours per module, corresponding to 40 to 60 percent time savings.
These results highlight both the potential and current limitations of LLM
assisted Modelica generation, and point to future research in pre simulation
validation, stronger grounding, and closed loop evaluation.

</details>


### [135] [Evaluating the Effectiveness of Coverage-Guided Fuzzing for Testing Deep Learning Library APIs](https://arxiv.org/abs/2509.14626)
*Feiran Qin,M. M. Abid Naziri,Hengyu Ai,Saikat Dutta,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: FlashFuzz利用LLMs自动合成API测试工具，显著提升深度学习库模糊测试的覆盖率和效率，发现多个未知错误。


<details>
  <summary>Details</summary>
Motivation: 深度学习库中的错误查找重要且具有挑战性，现有方法缺乏覆盖引导，限制了其效果和效率。

Method: 提出FlashFuzz技术，利用大型语言模型（LLMs）自动合成API级测试工具，结合模板、辅助函数和API文档，采用反馈驱动的策略迭代合成和修复测试工具。

Result: FlashFuzz为1,151个PyTorch和662个TensorFlow API合成测试工具，相比现有方法，覆盖率和有效性显著提高，并发现42个未知错误。

Conclusion: 研究表明，覆盖引导的模糊测试（CGF）可以有效地应用于深度学习库，并提供了未来测试方法的强基线。

Abstract: Deep Learning (DL) libraries such as PyTorch provide the core components to
build major AI-enabled applications. Finding bugs in these libraries is
important and challenging. Prior approaches have tackled this by performing
either API-level fuzzing or model-level fuzzing, but they do not use coverage
guidance, which limits their effectiveness and efficiency. This raises an
intriguing question: can coverage guided fuzzing (CGF), in particular
frameworks like LibFuzzer, be effectively applied to DL libraries, and does it
offer meaningful improvements in code coverage, bug detection, and scalability
compared to prior methods?
  We present the first in-depth study to answer this question. A key challenge
in applying CGF to DL libraries is the need to create a test harness for each
API that can transform byte-level fuzzer inputs into valid API inputs. To
address this, we propose FlashFuzz, a technique that leverages Large Language
Models (LLMs) to automatically synthesize API-level harnesses by combining
templates, helper functions, and API documentation. FlashFuzz uses a feedback
driven strategy to iteratively synthesize and repair harnesses. With this
approach, FlashFuzz synthesizes harnesses for 1,151 PyTorch and 662 TensorFlow
APIs. Compared to state-of-the-art fuzzing methods (ACETest, PathFinder, and
TitanFuzz), FlashFuzz achieves up to 101.13 to 212.88 percent higher coverage
and 1.0x to 5.4x higher validity rate, while also delivering 1x to 1182x
speedups in input generation. FlashFuzz has discovered 42 previously unknown
bugs in PyTorch and TensorFlow, 8 of which are already fixed. Our study
confirms that CGF can be effectively applied to DL libraries and provides a
strong baseline for future testing approaches.

</details>


### [136] [SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation](https://arxiv.org/abs/2509.14646)
*Yongpan Wang,Xin Xu,Xiaojie Zhu,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: \saltm 是一种新的二进制反编译方法，通过抽象逻辑特征和微调LLM，显著提升了反编译效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法将汇编代码视为线性指令序列，忽略了二进制文件中的跳转模式和孤立数据段，限制了语义恢复能力。

Method: \saltm 构建了一个源级抽象逻辑树（\salt）来近似高级语言的逻辑结构，并利用微调的LLM生成反编译代码，最后通过错误修正和符号恢复优化输出。

Result: \saltm 在Decompile-Eval数据集上实现了70.4%的TCP率，提升了10.6%，且对四种常用混淆技术表现出鲁棒性。

Conclusion: \saltm 方法通过抽象二进制代码中的逻辑特征，显著提升了反编译的准确性和可读性，优于现有先进方法，并在实际应用中表现出色。

Abstract: Decompilation is widely used in reverse engineering to recover high-level
language code from binary executables. While recent approaches leveraging Large
Language Models (LLMs) have shown promising progress, they typically treat
assembly code as a linear sequence of instructions, overlooking arbitrary jump
patterns and isolated data segments inherent to binary files. This limitation
significantly hinders their ability to correctly infer source code semantics
from assembly code. To address this limitation, we propose \saltm, a novel
binary decompilation method that abstracts stable logical features shared
between binary and source code. The core idea of \saltm is to abstract selected
binary-level operations, such as specific jumps, into a high-level logic
framework that better guides LLMs in semantic recovery. Given a binary
function, \saltm constructs a Source-level Abstract Logic Tree (\salt) from
assembly code to approximate the logic structure of high-level language. It
then fine-tunes an LLM using the reconstructed \salt to generate decompiled
code. Finally, the output is refined through error correction and symbol
recovery to improve readability and correctness. We compare \saltm to three
categories of baselines (general-purpose LLMs, commercial decompilers, and
decompilation methods) using three well-known datasets (Decompile-Eval, MBPP,
Exebench). Our experimental results demonstrate that \saltm is highly effective
in recovering the logic of the source code, significantly outperforming
state-of-the-art methods (e.g., 70.4\% TCP rate on Decompile-Eval with a 10.6\%
improvement). The results further validate its robustness against four commonly
used obfuscation techniques. Additionally, analyses of real-world software and
a user study confirm that our decompiled output offers superior assistance to
human analysts in comprehending binary functions.

</details>


### [137] [Wireless Communication Performance Testing: From Laboratory Environment to Research Vessel](https://arxiv.org/abs/2509.14740)
*Andrei-Raoul Morariu,Andreas Strandberg,Bogdan Iancu,Jerker Bjorkqvist*

Main category: cs.SE

TL;DR: 研究共享频谱中信号传输，发现视线遮挡、距离和位置布置显著影响效率，对环境因素在无线通信中的作用提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 探讨在共享频谱中信号传输的效率，特别是在动态和遮挡环境中，环境因素对无线通信的影响。

Method: 在实验室和户外环境中进行测量，分析视线遮挡物体对信号衰减的影响，并研究距离及位置布置对信号传输效率的作用。

Result: 实验室和户外测量显示，视线遮挡、距离及位置布置均显著影响信号传输效率。

Conclusion: 研究表明，实验室和户外环境中的信号传输受到视线遮挡、距离及位置布置的显著影响，这些环境因素在动态和遮挡环境中对无线通信效率具有重要影响。

Abstract: This study investigates signal transmission within a shared spectrum,
focusing on measurements conducted both in laboratory and outdoor environments.
The objective was to demonstrate how laboratory objects obstructing the line of
sight can attenuate the signal between a transmitter (Tx) and a receiver (Rx).
Additionally, we examined the impact of distance and placement in various
locations aboard an electric research boat on signal transmission efficiency.
These findings contribute to understanding whether the environmental factors
influence wireless communication in dynamic and obstructed environments.

</details>


### [138] [On the Use of Agentic Coding Manifests: An Empirical Study of Claude Code](https://arxiv.org/abs/2509.14744)
*Worawalan Chatlatanagulchai,Kundjanasith Thonglek,Brittany Reid,Yutaro Kashiwa,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Hajimu Iida*

Main category: cs.SE

TL;DR: 研究发现代理清单（如Claude.md）结构简单，内容多为操作命令和技术说明，但缺乏全面文档是开发者主要挑战。


<details>
  <summary>Details</summary>
Motivation: 代理编码工具需要自然语言编写的目标作为输入，但缺乏创建代理清单的全面文档，这对开发者构成挑战。

Method: 分析了来自242个仓库的253个Claude.md文件，以识别结构模式和常见内容。

Result: 发现代理清单通常具有一个主标题和几个子部分的浅层次结构，内容以操作命令、技术实现说明和高层架构为主。

Conclusion: 研究表明，代理清单（如Claude.md）通常具有浅层次结构，内容主要由操作命令、技术实现说明和高层架构组成。缺乏全面且易获取的文档是开发者面临的主要挑战。

Abstract: Agentic coding tools receive goals written in natural language as input,
break them down into specific tasks, and write/execute the actual code with
minimal human intervention. Key to this process are agent manifests,
configuration files (such as Claude.md) that provide agents with essential
project context, identity, and operational rules. However, the lack of
comprehensive and accessible documentation for creating these manifests
presents a significant challenge for developers. We analyzed 253 Claude.md
files from 242 repositories to identify structural patterns and common content.
Our findings show that manifests typically have shallow hierarchies with one
main heading and several subsections, with content dominated by operational
commands, technical implementation notes, and high-level architecture.

</details>


### [139] [On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub](https://arxiv.org/abs/2509.14745)
*Miku Watanabe,Hao Li,Yutaro Kashiwa,Brittany Reid,Hajimu Iida,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: AI代理生成的PR大多被接受，但人类监督仍不可或缺。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理生成的PR在实际项目中的实用性和接受程度。

Method: 通过实证研究分析了567个由Claude Code生成的GitHub PR，涉及157个开源项目。

Result: 83.8%的AI辅助PR被接受并合并，其中54.9%无需修改；45.1%需要人类进一步修订。

Conclusion: 研究发现，虽然AI代理辅助的PR大多被接受，但仍需人类监督和优化。

Abstract: Large language models (LLMs) are increasingly being integrated into software
development processes. The ability to generate code and submit pull requests
with minimal human intervention, through the use of autonomous AI agents, is
poised to become a standard practice. However, little is known about the
practical usefulness of these pull requests and the extent to which their
contributions are accepted in real-world projects. In this paper, we
empirically study 567 GitHub pull requests (PRs) generated using Claude Code,
an agentic coding tool, across 157 diverse open-source projects. Our analysis
reveals that developers tend to rely on agents for tasks such as refactoring,
documentation, and testing. The results indicate that 83.8% of these
agent-assisted PRs are eventually accepted and merged by project maintainers,
with 54.9% of the merged PRs are integrated without further modification. The
remaining 45.1% require additional changes benefit from human revisions,
especially for bug fixes, documentation, and adherence to project-specific
standards. These findings suggest that while agent-assisted PRs are largely
acceptable, they still benefit from human oversight and refinement.

</details>


### [140] [RulER: Automated Rule-Based Semantic Error Localization and Repair for Code Translation](https://arxiv.org/abs/2509.14829)
*Shuo Jin,Songqiang Chen,Xiaoyuan Xie,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: RulER利用LLMs提取代码翻译规则，显著提升自动化代码翻译的调试效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动化调试方法因缺乏可靠参考而影响定位准确性和修复效果，RulER通过引入代码翻译规则解决了这一问题。

Method: RulER 是一种基于规则的调试方法，通过动态组合可扩展节点（如表达式和令牌）上的现有规则，自适应地对齐更多语句。

Result: 在Java-to-C++和Python-to-C++翻译测试中，RulER的错误定位率和修复成功率分别比最佳基线高出20%和272%。

Conclusion: RulER 通过自动从LLMs生成的正向翻译中提取代码翻译规则，显著提升了错误定位和修复的效率，相比现有方法表现出色。

Abstract: Automated code translation aims to convert programs between different
programming languages while maintaining their functionality. Due to the
imperfections of code translation models, the generated translations may
contain errors that compromise their reliability. Existing automated debugging
methods for code translation rely on code alignments and repair patch templates
to locate and fix erroneous translations. However, existing methods lack
reliable references to construct code alignments and design repair patch
templates, which significantly impacts their localization accuracy and repair
effectiveness. To address these limitations, we reintroduce code translation
rules and propose a rule-based debugging method for code translation, called
RulER. RulER automatically derives code translation rules from correct
translations generated by LLMs, enabling the efficient collection of diverse
translation rules. In addition, RulER dynamically combines the existing rules
on expandable nodes like expressions and tokens to further adaptively align
more statements. These rules capture clear and detailed structural
correspondences between source and target programming languages. Therefore,
they can serve as reliable and reusable references for code alignment and
repair template design, enabling RulER to locate and fix translation errors
effectively. Our evaluation of RulER on Java-to-C++ and Python-to-C++
translations produced by four code translation models demonstrates that RulER
outperforms state-of-the-art methods, BatFix and TransMap. Our experimental
results show that RulER outperformed the best baseline by 20% and 272% in terms
of error localization rates and repair success rates, respectively. RulER
exhibits superior repair performance compared to directly prompting LLMs for
patch generation, demonstrating a promising methodology for extracting and
leveraging coding knowledge from LLMs.

</details>


### [141] [CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects](https://arxiv.org/abs/2509.14856)
*Hanyang Guo,Xunjin Zheng,Zihan Liao,Hang Yu,Peng DI,Ziyin Zhang,Hong-Ning Dai*

Main category: cs.SE

TL;DR: 研究团队提出了CodeFuse-CR-Bench基准测试，首次全面评估了LLM在代码审查任务中的表现，发现Gemini 2.5 Pro综合表现最佳，但不同模型在不同方面各有优劣。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试在评估大型语言模型（LLM）时，使用简化的、缺乏上下文的数据，无法反映真实代码审查（CR）的丰富上下文环境。这导致了"现实差距"，阻碍了自动化CR的进展。

Method: 研究团队引入了CodeFuse-CR-Bench，这是一个包含601个高质量实例的基准测试，覆盖了70个Python项目中的九个PR问题领域。每个实例提供了丰富的上下文信息，包括相关issue、PR详情和仓库状态。此外，还提出了一种结合规则检查和模型评估的新框架。

Result: 研究首次对最先进的LLM在这一全面CR任务上进行了大规模评估。结果显示：（1）没有单一LLM在所有CR方面表现最优；（2）Gemini 2.5 Pro的综合性能最高；（3）不同LLM对冗余上下文的鲁棒性不同。

Conclusion: 研究结果表明，当前的LLM在代码审查任务中表现各异，没有单一模型在所有方面都占优。Gemini 2.5 Pro综合表现最佳，而不同模型对冗余上下文的鲁棒性也不同。这些发现强调了进行多维度评估的必要性。

Abstract: Automated code review (CR) is a key application for Large Language Models
(LLMs), but progress is hampered by a "reality gap": existing benchmarks
evaluate models on isolated sub-tasks using simplified, context-poor data. This
fails to reflect the holistic context-rich nature of real-world CR. To bridge
this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware
benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601
high-quality instances from 70 Python projects covering nine Pull-Request (PR)
problem domains, where each instance provides rich, multi-faceted context
including the associated issue, PR details, and repository state, enabling
end-to-end evaluation. Beyond superficial metrics, we also propose a novel
evaluation framework that combines rule-based checks for location and syntax
with model-based judgments of review quality. We present the first large-scale
assessment of state-of-the-art LLMs on this comprehensive CR task. Our results
establish crucial baselines and reveal that (1) no single LLM dominates all
aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive
performance; and (3) different LLMs exhibit varying robustness to redundant
context. These findings highlight the necessity of holistic, multi-dimensional
evaluation and provide actionable insights for advancing truly intelligent yet
practical CR assistants.

</details>


### [142] [CARGO: A Framework for Confidence-Aware Routing of Large Language Models](https://arxiv.org/abs/2509.14899)
*Amine Barrak,Yosr Fourati,Michael Olchawa,Emna Ksontini,Khalil Zoghlami*

Main category: cs.SE

TL;DR: CARGO是一种轻量级动态路由框架，通过嵌入回归器和可选分类器实现高效LLM路由，在多个任务组中表现优异，接近专家水平。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在规模、专业化和延迟特性上的多样化，如何高效路由用户提示以平衡性能和成本成为关键挑战。

Method: CARGO采用两阶段设计：首先使用基于嵌入的回归器预测模型性能，对于不确定的情况可选调用二元分类器。此外，还支持针对五个任务组的类别特定回归器。

Result: 在四个竞争性LLM（GPT-4o、Claude 3.5 Sonnet、DeepSeek V3和Perplexity Sonar）上评估，CARGO实现了76.4%的top-1路由准确率和72%至89%的胜率。

Conclusion: CARGO的轻量级、基于置信度的动态路由框架在无需人工标注监督的情况下，实现了专家级的路由性能，为多模型LLM部署提供了实用解决方案。

Abstract: As large language models (LLMs) proliferate in scale, specialization, and
latency profiles, the challenge of routing user prompts to the most appropriate
model has become increasingly critical for balancing performance and cost. We
introduce CARGO (Category-Aware Routing with Gap-based Optimization), a
lightweight, confidence-aware framework for dynamic LLM selection. CARGO
employs a single embedding-based regressor trained on LLM-judged pairwise
comparisons to predict model performance, with an optional binary classifier
invoked when predictions are uncertain. This two-stage design enables precise,
cost-aware routing without the need for human-annotated supervision. To capture
domain-specific behavior, CARGO also supports category-specific regressors
trained across five task groups: mathematics, coding, reasoning, summarization,
and creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5
Sonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing
accuracy of 76.4% and win rates ranging from 72% to 89% against individual
experts. These results demonstrate that confidence-guided, lightweight routing
can achieve expert-level performance with minimal overhead, offering a
practical solution for real-world, multi-model LLM deployments.

</details>


### [143] ["Let it be Chaos in the Plumbing!" Usage and Efficacy of Chaos Engineering in DevOps Pipelines](https://arxiv.org/abs/2509.14931)
*Stefano Fossati,Damian Andrew Tamburri,Massimiliano Di Penta,Marco Tonnarelli*

Main category: cs.SE

TL;DR: 本文通过灰色文献综述扩展了混沌工程的十大概念，揭示了实践者在DevOps环境中更注重受控实验、自动化和风险缓解。


<details>
  <summary>Details</summary>
Motivation: 研究混沌工程（CE）在行业实践中的采用和适应情况，以提升现代分布式系统的韧性。

Method: 对2019年至2024年初发表的50篇文献进行了系统性灰色文献综述，并开发了一个全面的分类框架。

Result: 研究发现，尽管CE的核心原则仍然具有影响力，但实践者越来越注重受控实验、自动化和风险缓解策略，以适应敏捷和持续演进的DevOps环境。

Conclusion: 该研究通过灰色文献综述，扩展了混沌工程（CE）的基本原则，提出了十个不同的概念，并揭示了实践者在敏捷和持续演进的DevOps环境中如何强调受控实验、自动化和风险缓解策略。

Abstract: Chaos Engineering (CE) has emerged as a proactive method to improve the
resilience of modern distributed systems, particularly within DevOps
environments. Originally pioneered by Netflix, CE simulates real-world failures
to expose weaknesses before they impact production. In this paper, we present a
systematic gray literature review that investigates how industry practitioners
have adopted and adapted CE principles over recent years. Analyzing 50 sources
published between 2019 and early 2024, we developed a comprehensive
classification framework that extends the foundational CE principles into ten
distinct concepts. Our study reveals that while the core tenets of CE remain
influential, practitioners increasingly emphasize controlled experimentation,
automation, and risk mitigation strategies to align with the demands of agile
and continuously evolving DevOps pipelines. Our results enhance the
understanding of how CE is intended and implemented in practice, and offer
guidance for future research and industrial applications aimed at improving
system robustness in dynamic production environments.

</details>


### [144] [Code Less to Code More: Streamlining Language Server Protocol and Type System Development for Language Families](https://arxiv.org/abs/2509.15150)
*Federico Bruzzone,Walter Cazzola,Luca Favalli*

Main category: cs.SE

TL;DR: Typelang和模块化语言服务器生成过程显著简化了语言编辑支持，减少了工作量，提高了效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有语言工作台在模块化、可重用性及利用类型系统生成语言服务器方面的不足，简化语言家族编辑支持。

Method: 提出了Typelang（一组用于模块化、可组合和可重用类型系统实现的领域特定语言）、模块化语言服务器生成过程、变体导向编程范式及跨工件协调层，以及LSP插件生成器。

Result: 实现了类型系统实现字符数的93.48%减少和LSP插件生成的100%自动化，显著降低了语言家族编辑支持的工作量。

Conclusion: 通过Typelang和模块化语言服务器生成过程，本研究显著减少了语言支持编辑的复杂性，实现了类型系统实现字符数的93.48%减少和LSP插件生成的100%自动化。

Abstract: Developing editing support for $L$ languages in $E$ editors is complex and
time-consuming. Some languages do not provide dedicated editors, while others
offer a single native editor. The $\textit{language server protocol}$ (LSP)
reduces the language-editor combinations $L \times E$ to $L + E$, where a
single language server communicates with editors via LSP plugins. However,
overlapping implementations of linguistic components remain an issue. Existing
language workbenches struggle with modularity, reusability, and leveraging type
systems for language server generation. In this work, we propose: (i) Typelang,
a family of domain-specific languages for modular, composable, and reusable
type system implementation, (ii) a modular language server generation process,
producing servers for languages built in a modular workbench, (iii) the
variant-oriented programming paradigm and a cross-artifact coordination layer
to manage interdependent software variants, and (iv) an LSP plugin generator,
reducing $E$ to $1$ by automating plugin creation for multiple editors. To
simplify editing support for language families, each language artifact
integrates its own Typelang variant, used to generate language servers. This
reduces combinations to $T \times 1$, where $T = L$ represents the number of
type systems. Further reuse of language artifacts across languages lowers this
to $N \times 1$, where $N << T$, representing unique type systems. We implement
Typelang in Neverlang, generating language servers for each artifact and LSP
plugins for three editors. Empirical evaluation shows a 93.48% reduction in
characters needed for type system implementation and 100% automation of LSP
plugin generation, significantly lowering effort for editing support in
language families, especially when artifacts are reused.

</details>


### [145] [Orion: Fuzzing Workflow Automation](https://arxiv.org/abs/2509.15195)
*Max Bazalii,Marius Fleischer*

Main category: cs.SE

TL;DR: Orion框架结合LLM与传统工具，自动化模糊测试流程，显著减少人工干预并发现新漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代模糊测试工具虽能自动生成输入和监控执行，但整体工作流程仍需大量人工干预。Orion旨在解决这一瓶颈。

Method: Orion利用LLM进行代码推理和语义指导，同时依赖确定性工具进行验证、迭代优化和精确任务。

Result: Orion在基准测试中减少了46-204倍的人工工作量，并成功发现了开源clib库中的两个未知漏洞。

Conclusion: Orion框架通过结合LLM推理与传统工具，显著减少了模糊测试中的人工工作量，并在实际应用中发现了新的漏洞。

Abstract: Fuzz testing is one of the most effective techniques for finding software
vulnerabilities. While modern fuzzers can generate inputs and monitor
executions automatically, the overall workflow, from analyzing a codebase, to
configuring harnesses, to triaging results, still requires substantial manual
effort. Prior attempts focused on single stages such as harness synthesis or
input minimization, leaving researchers to manually connect the pieces into a
complete fuzzing campaign.
  We introduce Orion, a framework that automates the the manual bottlenecks of
fuzzing by integrating LLM reasoning with traditional tools, allowing campaigns
to scale to settings where human effort alone was impractical. Orion uses LLMs
for code reasoning and semantic guidance, while relying on deterministic tools
for verification, iterative refinement, and tasks that require precision.
Across our benchmark suite, Orion reduces human effort by 46-204x depending on
the workflow stage, and we demonstrate its effectiveness through the discovery
of two previously unknown vulnerabilities in the widely used open-source clib
library.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [146] [Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless and GPU-Based Training Architectures](https://arxiv.org/abs/2509.14920)
*Amine Barrak,Fabio Petrillo,Fehmi Jaafar*

Main category: cs.DC

TL;DR: 该论文比较了多种服务器无分布式ML架构，发现SPIRT在训练时间和通信开销上表现最优，长期成本效益显著，为未来研究提供了方向。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习领域对可扩展且经济高效的训练解决方案需求日益增长，尤其是在大型复杂模型背景下。服务器无计算因其动态可扩展性和资源高效执行成为解决这些挑战的有前景范式。

Method: 本研究对SPIRT、ScatterReduce、AllReduce和MLLess等服务器无分布式ML架构进行了比较分析，重点关注训练时间效率、成本效益、通信开销和容错能力等关键指标。

Result: 研究发现SPIRT通过并行批处理和RedisAI支持的数据内操作等策略，显著减少了训练时间和通信开销。传统架构则面临可扩展性挑战，并对故障和对抗攻击表现出不同程度的脆弱性。成本分析显示SPIRT尽管初始成本较高，但长期经济效益显著。

Conclusion: 该研究强调了SPIRT架构在减少训练时间和通信开销方面的显著优势，尽管初始设置成本较高，但长期经济效益显著。同时，指出了传统架构的可扩展性和容错性挑战，为未来结合现有系统最优特性的新模型研究奠定了基础。

Abstract: The field of distributed machine learning (ML) faces increasing demands for
scalable and cost-effective training solutions, particularly in the context of
large, complex models. Serverless computing has emerged as a promising paradigm
to address these challenges by offering dynamic scalability and
resource-efficient execution. Building upon our previous work, which introduced
the Serverless Peer Integrated for Robust Training (SPIRT) architecture, this
paper presents a comparative analysis of several serverless distributed ML
architectures. We examine SPIRT alongside established architectures like
ScatterReduce, AllReduce, and MLLess, focusing on key metrics such as training
time efficiency, cost-effectiveness, communication overhead, and fault
tolerance capabilities. Our findings reveal that SPIRT provides significant
improvements in reducing training times and communication overhead through
strategies such as parallel batch processing and in-database operations
facilitated by RedisAI. However, traditional architectures exhibit scalability
challenges and varying degrees of vulnerability to faults and adversarial
attacks. The cost analysis underscores the long-term economic benefits of SPIRT
despite its higher initial setup costs. This study not only highlights the
strengths and limitations of current serverless ML architectures but also sets
the stage for future research aimed at developing new models that combine the
most effective features of existing systems.

</details>


### [147] [Conditional Prior-based Non-stationary Channel Estimation Using Accelerated Diffusion Models](https://arxiv.org/abs/2509.15182)
*Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Umer,Asad Aali,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 该论文提出了一种条件先验扩散方法用于非平稳无线信道估计，通过历史条件得分解噪，在3GPP测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 移动和散射体动态导致无线信道非平稳性，传统和深度估计器性能下降，需要一种能适应信道动态变化的估计方法。

Method: 提出了一种条件先验扩散方法，通过历史条件得分解噪噪声信道快照。使用跨时间注意力的时间编码器将短观测窗口压缩为上下文向量，捕捉信道的瞬时相干性并通过特征调制引导去噪器。推理时采用SNR匹配初始化和几何间隔的缩短调度，保留信噪比轨迹。

Result: 在3GPP基准测试中，该方法在所有SNR下均表现出更低的NMSE，优于现有基线方法。

Conclusion: 条件先验扩散方法在3GPP基准测试中表现出色，在所有SNR下均优于LMMSE、GMM、LSTM和LDAMP基线，展现了稳定的性能和在高SNR下的强保真度。

Abstract: Wireless channels in motion-rich urban microcell (UMi) settings are
non-stationary; mobility and scatterer dynamics shift the distribution over
time, degrading classical and deep estimators. This work proposes conditional
prior diffusion for channel estimation, which learns a history-conditioned
score to denoise noisy channel snapshots. A temporal encoder with cross-time
attention compresses a short observation window into a context vector, which
captures the channel's instantaneous coherence and steers the denoiser via
feature-wise modulation. In inference, an SNR-matched initialization selects
the diffusion step whose marginal aligns with the measured input SNR, and the
process follows a shortened, geometrically spaced schedule, preserving the
signal-to-noise trajectory with far fewer iterations. Temporal
self-conditioning with the previous channel estimate and a training-only
smoothness penalty further stabilizes evolution without biasing the test-time
estimator. Evaluations on a 3GPP benchmark show lower NMSE across all SNRs than
LMMSE, GMM, LSTM, and LDAMP baselines, demonstrating stable performance and
strong high SNR fidelity.

</details>


### [148] [Channel Prediction under Network Distribution Shift Using Continual Learning-based Loss Regularization](https://arxiv.org/abs/2509.15192)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ibtsaam Qadir,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 提出持续学习框架解决信道预测中的灾难性遗忘，SI和EWC分别降低NMSE，SI内存效率更高。


<details>
  <summary>Details</summary>
Motivation: 现代无线网络中，移动用户在异构网络配置间切换时，传统预测器因分布偏移导致性能下降，需解决信道预测中的灾难性遗忘问题。

Method: 基于损失正则化的持续学习框架，研究了两种正则化策略：弹性权重巩固（EWC）和突触智能（SI）。

Result: 在3GPP场景和多架构测试中，SI将高信噪比NMSE下限降低了1.8 dB（约32-34%），EWC降低了1.4 dB（约17-28%）。SI的内存复杂度为O(M)，适合资源受限的无线基础设施。

Conclusion: 提出的持续学习框架通过损失正则化有效缓解了信道预测中的灾难性遗忘问题，SI和EWC在不同场景下均表现出色，其中SI在资源受限的无线基础设施中更具优势。

Abstract: Modern wireless networks face critical challenges when mobile users traverse
heterogeneous network configurations with varying antenna layouts, carrier
frequencies, and scattering statistics. Traditional predictors degrade under
distribution shift, with NMSE rising by 37.5\% during cross-configuration
handovers. This work addresses catastrophic forgetting in channel prediction by
proposing a continual learning framework based on loss regularization. The
approach augments standard training objectives with penalty terms that
selectively preserve network parameters essential for previous configurations
while enabling adaptation to new environments. Two prominent regularization
strategies are investigated: Elastic Weight Consolidation (EWC) and Synaptic
Intelligence (SI). Across 3GPP scenarios and multiple architectures, SI lowers
the high-SNR NMSE floor by up to 1.8 dB ($\approx$32--34\%), while EWC achieves
up to 1.4 dB ($\approx$17--28\%). Notably, standard EWC incurs
$\mathcal{O}(MK)$ complexity (storing $M$ Fisher diagonal entries and
corresponding parameter snapshots across $K$ tasks) unless consolidated,
whereas SI maintains $\mathcal{O}(M)$ memory complexity (storing $M$ model
parameters), independent of task sequence length, making it suitable for
resource-constrained wireless infrastructure

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [149] [Taming Serverless Cold Starts Through OS Co-Design](https://arxiv.org/abs/2509.14292)
*Ben Holmes,Baltasar Dinis,Lana Honcharuk,Joshua Fried,Adam Belay*

Main category: cs.OS

TL;DR: Spice通过直接与OS集成和专用内存恢复原语，解决了无服务器冷启动问题，实现了接近热启动的性能，延迟显著降低。


<details>
  <summary>Details</summary>
Motivation: 当前无服务器计算中的冷启动问题，即函数在闲置后首次调用需经历昂贵的初始化过程，普遍认为实现亚毫秒级冷启动需保持状态常驻内存。本文挑战这一假设，指出操作系统级别的限制才是从磁盘快速恢复的真正障碍。

Method: Spice是一个专为无服务器快照/恢复设计的执行引擎，直接与操作系统集成，无需昂贵的重放即可恢复内核状态，并引入了专用原语以高效可靠地恢复内存映射。

Result: Spice从磁盘冷恢复时实现了接近热启动的性能，比最先进的基于进程的系统延迟降低了14.9倍，比基于虚拟机的系统降低了10.6倍。

Conclusion: Spice证明了在无服务器计算中，高性能和内存弹性不再需要权衡，通过直接从磁盘恢复实现接近热启动的性能。

Abstract: Serverless computing promises fine-grained elasticity and operational
simplicity, fueling widespread interest from both industry and academia. Yet
this promise is undercut by the cold setart problem, where invoking a function
after a period of inactivity triggers costly initialization before any work can
begin. Even with today's high-speed storage, the prevailing view is that
achieving sub-millisecond cold starts requires keeping state resident in
memory.
  This paper challenges that assumption. Our analysis of existing
snapshot/restore mechanisms show that OS-level limitations, not storage speed,
are the real barrier to ultra-fast restores from disk. These limitations force
current systems to either restore state piecemeal in a costly manner or capture
too much state, leading to longer restore times and unpredictable performance.
Futhermore, current memory primitives exposed by the OS make it difficult to
reliably fetch data into memory and avoid costly runtime page faults.
  To overcome these barriers, we present Spice, an execution engine
purpose-built for serverless snapshot/restore. Spice integrates directly with
the OS to restore kernel state without costly replay and introduces dedicated
primitives for restoring memory mappings efficiently and reliably. As a result,
Spice delivers near-warm performance on cold restores from disk, reducing
latency by up to 14.9x over state-of-the-art process-based systems and 10.6x
over VM-based systems. This proves that high performance and memory elasticity
no longer need to be a trade-off in serverless computing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [150] [Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity](https://arxiv.org/abs/2509.14251)
*Qihang Chen*

Main category: cs.AI

TL;DR: 本文提出了一种多线地铁乘务员规划和重新规划的优化框架，通过层次化时空网络模型和高效算法，显著提升了跨线协调和应急管理的效率。


<details>
  <summary>Details</summary>
Motivation: 随着地铁网络的快速扩展，有效的多线调度和应急管理对于大规模无缝运营至关重要。然而，现有研究主要关注单线地铁，对跨线协调和中断期间的快速重新规划关注不足。

Method: 提出了一个层次化的时空网络模型来表示统一的乘务员行动空间，并推导了计算高效的约束条件和公式，考虑了乘务员的异质资格和偏好。进一步开发了基于列生成和最短路径调整的求解算法。

Result: 实验结果表明，所提出的方法在成本降低和任务完成方面优于基准启发式算法，并通过纳入跨线操作实现了显著的效率提升，特别是在中断期间的紧急任务中。

Conclusion: 本研究强调了全局优化和跨线协调在多线地铁系统运营中的作用，为智能城市公共交通的高效可靠运行提供了见解。

Abstract: Metro crew planning is a key component of smart city development as it
directly impacts the operational efficiency and service reliability of public
transportation. With the rapid expansion of metro networks, effective
multi-line scheduling and emergency management have become essential for
large-scale seamless operations. However, current research focuses primarily on
individual metro lines,with insufficient attention on cross-line coordination
and rapid replanning during disruptions. Here, a unified optimization framework
is presented for multi-line metro crew planning and replanning with
heterogeneous workforce. Specifically, a hierarchical time-space network model
is proposed to represent the unified crew action space, and computationally
efficient constraints and formulations are derived for the crew's heterogeneous
qualifications and preferences. Solution algorithms based on column generation
and shortest path adjustment are further developed, utilizing the proposed
network model. Experiments with real data from Shanghai and Beijing Metro
demonstrate that the proposed methods outperform benchmark heuristics in both
cost reduction and task completion,and achieve notable efficiency gains by
incorporating cross-line operations, particularly for urgent tasks during
disruptions. This work highlights the role of global optimization and
cross-line coordination in multi-line metro system operations, providing
insights into the efficient and reliable functioning of public transportation
in smart cities.

</details>


### [151] [From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing](https://arxiv.org/abs/2509.14289)
*Lanxiao Huang,Daksh Dave,Ming Jin,Tyler Cody,Peter Beling*

Main category: cs.AI

TL;DR: 研究评估了多种LLM代理在渗透测试中的表现，并通过五项核心功能增强显著提升了复杂任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）越来越多地用于自动化或增强渗透测试，但其在不同攻击阶段的有效性和可靠性仍不明确。

Method: 通过目标增强分离了五种核心功能能力的影响：全局上下文记忆（GCM）、代理间消息传递（IAM）、上下文条件调用（CCI）、自适应规划（AP）和实时监控（RTM）。

Result: 结果表明，虽然某些架构天生具备这些特性的子集，但针对性的增强显著改善了模块化代理的性能。

Conclusion: 针对性的功能增强显著提升了模块化代理在复杂、多步骤和实时渗透测试任务中的表现。

Abstract: Large language models (LLMs) are increasingly used to automate or augment
penetration testing, but their effectiveness and reliability across attack
phases remain unclear. We present a comprehensive evaluation of multiple
LLM-based agents, from single-agent to modular designs, across realistic
penetration testing scenarios, measuring empirical performance and recurring
failure patterns. We also isolate the impact of five core functional
capabilities via targeted augmentations: Global Context Memory (GCM),
Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive
Planning (AP), and Real-Time Monitoring (RTM). These interventions support,
respectively: (i) context coherence and retention, (ii) inter-component
coordination and state management, (iii) tool use accuracy and selective
execution, (iv) multi-step strategic planning, error detection, and recovery,
and (v) real-time dynamic responsiveness. Our results show that while some
architectures natively exhibit subsets of these properties, targeted
augmentations substantially improve modular agent performance, especially in
complex, multi-step, and real-time penetration testing tasks.

</details>


### [152] [Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents](https://arxiv.org/abs/2509.14382)
*Daniel Röder,Akhil Juneja,Roland Roller,Sven Schmeier*

Main category: cs.AI

TL;DR: 研究提出模块化评估框架，通过分解智能体流程进行详细错误分析，揭示了标准指标遗漏的弱点，助力更健壮的网络智能体开发。


<details>
  <summary>Details</summary>
Motivation: 当前评估主要关注整体成功率，忽略了中间错误，限制了失败模式的洞察和系统改进的可能性。

Method: 使用SeeAct框架和Mind2Web数据集进行案例研究，提出模块化评估框架，分解智能体流程为可解释阶段。

Result: 该方法揭示了标准指标遗漏的可操作弱点，为更健壮和通用的网络智能体提供了改进方向。

Conclusion: 该研究提出了一种模块化评估框架，通过分解智能体流程为可解释的阶段进行详细错误分析，揭示了标准指标遗漏的可操作弱点，为构建更健壮和通用的网络智能体铺平了道路。

Abstract: Web agents powered by large language models (LLMs) can autonomously perform
complex, multistep tasks in dynamic web environments. However, current
evaluations mostly focus on the overall success while overlooking intermediate
errors. This limits insight into failure modes and hinders systematic
improvement. This work analyzes existing benchmarks and highlights the lack of
fine-grained diagnostic tools. To address this gap, we propose a modular
evaluation framework that decomposes agent pipelines into interpretable stages
for detailed error analysis. Using the SeeAct framework and the Mind2Web
dataset as a case study, we show how this approach reveals actionable
weaknesses missed by standard metrics - paving the way for more robust and
generalizable web agents.

</details>


### [153] [VCBench: Benchmarking LLMs in Venture Capital](https://arxiv.org/abs/2509.14448)
*Rick Chen,Joseph Ternasky,Afriyie Samuel Kwesi,Ben Griffin,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: VCBench是首个用于预测风险投资创始人成功的基准测试，提供匿名创始人资料并评估多种LLMs，结果表明多数模型表现优于人类基准。


<details>
  <summary>Details</summary>
Motivation: 风险投资领域信号稀疏、结果不确定，即使是顶级投资者表现也一般，因此需要共享数据集以加速AGI进展。

Method: VCBench提供了9,000个匿名创始人资料，标准化以保留预测特征同时抵抗身份泄露，并通过对抗性测试显示超过90%的重新识别风险降低。评估了九种最先进的大型语言模型（LLMs）。

Result: DeepSeek-V3的精度超过基线六倍，GPT-4o达到最高的F0.5，大多数模型超过人类基准。

Conclusion: VCBench作为首个用于预测风险投资创始人成功的基准测试，为AGI在早期风险预测中的可重复和隐私保护评估设立了社区驱动的标准。

Abstract: Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets
accelerate progress toward artificial general intelligence (AGI). We introduce
VCBench, the first benchmark for predicting founder success in venture capital
(VC), a domain where signals are sparse, outcomes are uncertain, and even top
investors perform modestly. At inception, the market index achieves a precision
of 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1
firms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,
standardized to preserve predictive features while resisting identity leakage,
with adversarial tests showing more than 90% reduction in re-identification
risk. We evaluate nine state-of-the-art large language models (LLMs).
DeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the
highest F0.5, and most models surpass human benchmarks. Designed as a public
and evolving resource available at vcbench.com, VCBench establishes a
community-driven standard for reproducible and privacy-preserving evaluation of
AGI in early-stage venture forecasting.

</details>


### [154] [From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence](https://arxiv.org/abs/2509.14474)
*Meltem Subasioglu,Nevzat Subasioglu*

Main category: cs.AI

TL;DR: 论文提出了一种新的AGI定义框架，基于六个核心组件（包括意识和主观体验），并提出了五级分类法，认为第五级AGI与真正智能（TI）在功能上等价。


<details>
  <summary>Details</summary>
Motivation: 当前基于性能的AGI定义缺乏明确的研究路线图，且未能准确定义真正智能的质性特征。作者从人脑获取灵感，旨在提出一种更全面的、基于机制的AGI定义。

Method: 作者提出了一种新的范式，将重点从外部模仿转向基础认知架构的开发，并定义了真正智能（TI）的六个核心组件。此外，还提出了一个基于前五个可测量组件的五级AGI分类法。

Result: 该研究提供了一个清晰的框架，包括发展里程碑，直接解决了构建真正智能系统的挑战，并为研究社区提供了可操作的路径。

Conclusion: 作者认为，一旦系统实现了所有五个可测量组件（达到第五级AGI），它与真正智能（TI）之间的区别就仅剩哲学层面的讨论。从功能和实践角度来看，第五级AGI等同于TI。

Abstract: The debate around Artificial General Intelligence (AGI) remains open due to
two fundamentally different goals: replicating human-like performance versus
replicating human-like cognitive processes. We argue that current
performance-based definitions are inadequate because they provide no clear,
mechanism-focused roadmap for research, and they fail to properly define the
qualitative nature of genuine intelligence. Drawing inspiration from the human
brain, we propose a new paradigm that shifts the focus from external mimicry to
the development of foundational cognitive architectures. We define True
Intelligence (TI) as a system characterized by six core components: embodied
sensory fusion, core directives, dynamic schemata creation, a
highly-interconnected multi-expert architecture, an orchestration layer, and
lastly, the unmeasurable quality of Interconnectedness, which we hypothesize
results in consciousness and a subjective experience. We propose a practical,
five-level taxonomy of AGI based on the number of the first five measurable
components a system exhibits. This framework provides a clear path forward with
developmental milestones that directly address the challenge of building
genuinely intelligent systems. We contend that once a system achieves Level-5
AGI by implementing all five measurable components, the difference between it
and TI remains as a purely philosophical debate. For practical purposes - and
given theories indicate consciousness is an emergent byproduct of integrated,
higher-order cognition - we conclude that a fifth-level AGI is functionally and
practically equivalent to TI. This work synthesizes diverse insights from
analytical psychology, schema theory, metacognition, modern brain architectures
and latest works in AI to provide the first holistic, mechanism-based
definition of AGI that offers a clear and actionable path for the research
community.

</details>


### [155] [Beyond the high score: Prosocial ability profiles of multi-agent populations](https://arxiv.org/abs/2509.14485)
*Marko Tesic,Yue Zhao,Joel Z. Leibo,Rakshit S. Trivedi,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: 论文提出了一种 Bayesian 方法 Measurement Layouts，用于评估 AI 系统的合作能力，发现亲社会能力与表现的关系复杂，并揭示了竞赛中可能存在的评估框架漏洞。


<details>
  <summary>Details</summary>
Motivation: 开发一个更透明和通用的方法来评估 AI 系统在复杂社交环境中的合作能力。

Method: 应用 Bayesian 方法 Measurement Layouts 来推断多智能体系统在 Melting Pot 竞赛中的能力概况。

Result: 能力概况不仅能预测未来表现，还揭示了智能体的潜在亲社会能力。但发现高亲社会能力并不总是与更好的表现相关，且某些高分团队可能在不需要合作的场景中表现更好。

Conclusion: Measurement Layouts 提供了强大的预测准确性和可操作的见解，有助于在复杂社交环境中更透明和通用地评估 AI 系统。

Abstract: The development and evaluation of social capabilities in AI agents require
complex environments where competitive and cooperative behaviours naturally
emerge. While game-theoretic properties can explain why certain teams or agent
populations outperform others, more abstract behaviours, such as convention
following, are harder to control in training and evaluation settings. The
Melting Pot contest is a social AI evaluation suite designed to assess the
cooperation capabilities of AI systems. In this paper, we apply a Bayesian
approach known as Measurement Layouts to infer the capability profiles of
multi-agent systems in the Melting Pot contest. We show that these capability
profiles not only predict future performance within the Melting Pot suite but
also reveal the underlying prosocial abilities of agents. Our analysis
indicates that while higher prosocial capabilities sometimes correlate with
better performance, this is not a universal trend-some lower-scoring agents
exhibit stronger cooperation abilities. Furthermore, we find that
top-performing contest submissions are more likely to achieve high scores in
scenarios where prosocial capabilities are not required. These findings,
together with reports that the contest winner used a hard-coded solution
tailored to specific environments, suggest that at least one top-performing
team may have optimised for conditions where cooperation was not necessary,
potentially exploiting limitations in the evaluation framework. We provide
recommendations for improving the annotation of cooperation demands and propose
future research directions to account for biases introduced by different
testing environments. Our results demonstrate that Measurement Layouts offer
both strong predictive accuracy and actionable insights, contributing to a more
transparent and generalisable approach to evaluating AI systems in complex
social settings.

</details>


### [156] [DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction](https://arxiv.org/abs/2509.14507)
*Jian Chen,Zhenyan Chen,Xuming Hu,Peilin Zhou,Yining Hua,Han Fang,Cissy Hing Yee Choy,Xinmei Ke,Jingfeng Luo,Zixuan Yuan*

Main category: cs.AI

TL;DR: DeKeySQL通过DeKeyNLU数据集微调，显著提升NL2SQL准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有NL2SQL方法中任务分解和关键词提取不准确的问题，提升SQL生成精度。

Method: 提出DeKeySQL流程，包含三个模块：用户问题理解、实体检索和生成，并使用DeKeyNLU数据集微调。

Result: 在BIRD和Spider开发数据集上，SQL生成准确率分别从62.31%提升至69.10%和84.2%提升至88.7%。

Conclusion: DeKeySQL，一种基于RAG的NL2SQL流程，通过DeKeyNLU数据集微调，显著提高了SQL生成准确性，在BIRD和Spider数据集上表现优异。

Abstract: Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that
simplifies database access for non-technical users by converting natural
language queries into SQL commands. Recent advancements, particularly those
integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)
reasoning, have made significant strides in enhancing NL2SQL performance.
However, challenges such as inaccurate task decomposition and keyword
extraction by LLMs remain major bottlenecks, often leading to errors in SQL
generation. While existing datasets aim to mitigate these issues by fine-tuning
models, they struggle with over-fragmentation of tasks and lack of
domain-specific keyword annotations, limiting their effectiveness. To address
these limitations, we present DeKeyNLU, a novel dataset which contains 1,500
meticulously annotated QA pairs aimed at refining task decomposition and
enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with
DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three
distinct modules for user question understanding, entity retrieval, and
generation to improve SQL generation accuracy. We benchmarked multiple model
configurations within DeKeySQL RAG pipeline. Experimental results demonstrate
that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy
on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.

</details>


### [157] [Rationality Check! Benchmarking the Rationality of Large Language Models](https://arxiv.org/abs/2509.14546)
*Zhilun Zhou,Jing Yi Wang,Nicholas Sukiennik,Chen Gao,Fengli Xu,Yong Li,James Evans*

Main category: cs.AI

TL;DR: 论文提出了首个评估LLMs全面理性的基准，包括工具包和实验结果，帮助理解LLMs与人类理性的异同。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在模拟人类行为和作为AI助手方面的广泛应用，评估其在理论和实践理性方面是否接近人类行为变得至关重要。

Method: 论文提出了一个涵盖多个领域和LLMs的基准，包括易用的工具包、广泛的实验结果和分析。

Result: 基准工具揭示了LLMs在理想化人类理性方面的收敛和分歧点。

Conclusion: 该论文提出了一个评估大语言模型（LLMs）全面理性能力的基准工具，为开发者和用户提供了基础性工具。

Abstract: Large language models (LLMs), a recent advance in deep learning and machine
intelligence, have manifested astonishing capacities, now considered among the
most promising for artificial general intelligence. With human-like
capabilities, LLMs have been used to simulate humans and serve as AI assistants
across many applications. As a result, great concern has arisen about whether
and under what circumstances LLMs think and behave like real human agents.
Rationality is among the most important concepts in assessing human behavior,
both in thinking (i.e., theoretical rationality) and in taking action (i.e.,
practical rationality). In this work, we propose the first benchmark for
evaluating the omnibus rationality of LLMs, covering a wide range of domains
and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental
results, and analysis that illuminates where LLMs converge and diverge from
idealized human rationality. We believe the benchmark can serve as a
foundational tool for both developers and users of LLMs.

</details>


### [158] [(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration](https://arxiv.org/abs/2509.14547)
*Yi Lin,Lujin Zhao,Yijie Shi*

Main category: cs.AI

TL;DR: 提出动态框架优化LLM工作流构建，结合先验决策与历史经验，实验显示性能提升4.05%，成本降至30.68%-48.31%。


<details>
  <summary>Details</summary>
Motivation: 现有工作流构建方法过度依赖历史经验，导致效率和适应性受限。本文认为工作流构建应灵活响应任务特性。

Method: 提出了一种先验动态框架，结合Q-table学习优化决策空间，并引入冷启动初始化、早期停止和剪枝等机制以提高系统效率。

Result: 在四个基准数据集上验证了方法的有效性，相较于现有方法平均提升4.05%，成本显著降低。

Conclusion: 实验评估表明，该方法在四个基准数据集上均表现出可行性和有效性，相较于现有方法平均提升4.05%，同时将工作流构建和推理成本降至30.68%-48.31%。

Abstract: Recent studies have shown that carefully designed workflows coordinating
large language models(LLMs) significantly enhance task-solving capabilities
compared to using a single model. While an increasing number of works focus on
autonomous workflow construction, most existing approaches rely solely on
historical experience, leading to limitations in efficiency and adaptability.
We argue that while historical experience is valuable, workflow construction
should also flexibly respond to the unique characteristics of each task. To
this end, we propose an a priori dynamic framework for automated workflow
construction. Our framework first leverages Q-table learning to optimize the
decision space, guiding agent decisions and enabling effective use of
historical experience. At the same time, agents evaluate the current task
progress and make a priori decisions regarding the next executing agent,
allowing the system to proactively select the more suitable workflow structure
for each given task. Additionally, we incorporate mechanisms such as cold-start
initialization, early stopping, and pruning to further improve system
efficiency. Experimental evaluations on four benchmark datasets demonstrate the
feasibility and effectiveness of our approach. Compared to state-of-the-art
baselines, our method achieves an average improvement of 4.05%, while reducing
workflow construction and inference costs to only 30.68%-48.31% of those
required by existing methods.

</details>


### [159] [SynBench: A Benchmark for Differentially Private Text Generation](https://arxiv.org/abs/2509.14594)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Yulong Wu,Hao Li,Jie Zhang,Warren Del-Pinto,Goran Nenadic,Siew Kei Lam,Anil Anthony Bharath*

Main category: cs.AI

TL;DR: 本文提出一个评估框架和实证研究，揭示在差分隐私下生成高质量领域特定合成数据的挑战，并开发针对合成文本的隐私攻击方法。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动决策支持在医疗和金融等高风险领域中面临的数据共享障碍，以及现有匿名化方法对非结构化文本的不足。

Method: 通过三个关键贡献：引入一个全面的评估框架，进行大规模实证研究，以及开发针对合成文本的成员推理攻击方法。

Result: 研究表明，在差分隐私约束下生成高质量领域特定合成数据仍是一个未解决的挑战，且性能随领域复杂性增加而下降。

Conclusion: 研究发现，在隐私敏感的高风险环境中，生成式AI的负责任部署需要严格的隐私审计，并揭示了开放领域与专业领域评估之间的持续差距。

Abstract: Data-driven decision support in high-stakes domains like healthcare and
finance faces significant barriers to data sharing due to regulatory,
institutional, and privacy concerns. While recent generative AI models, such as
large language models, have shown impressive performance in open-domain tasks,
their adoption in sensitive environments remains limited by unpredictable
behaviors and insufficient privacy-preserving datasets for benchmarking.
Existing anonymization methods are often inadequate, especially for
unstructured text, as redaction and masking can still allow re-identification.
Differential Privacy (DP) offers a principled alternative, enabling the
generation of synthetic data with formal privacy assurances. In this work, we
address these challenges through three key contributions. First, we introduce a
comprehensive evaluation framework with standardized utility and fidelity
metrics, encompassing nine curated datasets that capture domain-specific
complexities such as technical jargon, long-context dependencies, and
specialized document structures. Second, we conduct a large-scale empirical
study benchmarking state-of-the-art DP text generation methods and LLMs of
varying sizes and different fine-tuning strategies, revealing that high-quality
domain-specific synthetic data generation under DP constraints remains an
unsolved challenge, with performance degrading as domain complexity increases.
Third, we develop a membership inference attack (MIA) methodology tailored for
synthetic text, providing first empirical evidence that the use of public
datasets - potentially present in pre-training corpora - can invalidate claimed
privacy guarantees. Our findings underscore the urgent need for rigorous
privacy auditing and highlight persistent gaps between open-domain and
specialist evaluations, informing responsible deployment of generative AI in
privacy-sensitive, high-stakes settings.

</details>


### [160] [AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production](https://arxiv.org/abs/2509.14647)
*NVJK Kartik,Garvit Sapra,Rishav Hada,Nikhil Pareek*

Main category: cs.AI

TL;DR: AgentCompass是一个专为后部署监控和调试代理工作流设计的框架，通过多阶段分析和双记忆系统，有效识别和改进问题，在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在多代理工作流中的广泛应用，现有评估方法难以捕捉错误、涌现行为和系统故障，因此需要一种专门的后部署监控和调试框架。

Method: AgentCompass采用多阶段分析流程，包括错误识别与分类、主题聚类、定量评分和战略总结，并结合双记忆系统（情景记忆和语义记忆）实现持续学习。

Result: AgentCompass在TRAIL基准测试中取得了最先进的结果，并发现了人工标注中遗漏的关键问题，验证了其作为开发人员工具的实用性。

Conclusion: AgentCompass作为一种专为代理工作流设计的后部署监控和调试框架，通过其结构化分析流程和双记忆系统，实现了对复杂多代理工作流中错误和系统故障的高效识别与改进，展现了其在生产环境中的实用性和可靠性。

Abstract: With the growing adoption of Large Language Models (LLMs) in automating
complex, multi-agent workflows, organizations face mounting risks from errors,
emergent behaviors, and systemic failures that current evaluation methods fail
to capture. We present AgentCompass, the first evaluation framework designed
specifically for post-deployment monitoring and debugging of agentic workflows.
AgentCompass models the reasoning process of expert debuggers through a
structured, multi-stage analytical pipeline: error identification and
categorization, thematic clustering, quantitative scoring, and strategic
summarization. The framework is further enhanced with a dual memory
system-episodic and semantic-that enables continual learning across executions.
Through collaborations with design partners, we demonstrate the framework's
practical utility on real-world deployments, before establishing its efficacy
against the publicly available TRAIL benchmark. AgentCompass achieves
state-of-the-art results on key metrics, while uncovering critical issues
missed in human annotations, underscoring its role as a robust,
developer-centric tool for reliable monitoring and improvement of agentic
systems in production.

</details>


### [161] [Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory](https://arxiv.org/abs/2509.14662)
*Ming Li,Nan Zhang,Chenrui Fan,Hong Jiao,Yanbin Fu,Sydney Peters,Qingshu Xu,Robert Lissitz,Tianyi Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种基于Schoenfeld的Episode Theory的新方法，用于分析大型推理模型的推理结构，并创建了首个公开的细粒度机器推理分析基准。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型（LRMs）生成了大量的链式推理，但目前缺乏一个原则性的框架来理解这些思维的结构。

Method: 应用Schoenfeld的Episode Theory（一种经典的人类数学问题解决认知框架）来分析大型推理模型的推理轨迹，并对数千个模型生成的数学问题解决方案中的句子和段落进行了七种认知标签（如计划、实施、验证）的标注。

Result: 创建了第一个公开可用的细粒度机器推理分析基准，包括大量标注语料库和详细的标注指南。初步分析揭示了大型推理模型推理中的独特模式，如认知状态之间的转换动态。

Conclusion: 该框架为解释大型推理模型的认知提供了理论基础，并为未来开发更可控和透明的推理系统奠定了基础。

Abstract: While Large Reasoning Models (LRMs) generate extensive chain-of-thought
reasoning, we lack a principled framework for understanding how these thoughts
are structured. In this paper, we introduce a novel approach by applying
Schoenfeld's Episode Theory, a classic cognitive framework for human
mathematical problem-solving, to analyze the reasoning traces of LRMs. We
annotated thousands of sentences and paragraphs from model-generated solutions
to math problems using seven cognitive labels (e.g., Plan, Implement, Verify).
The result is the first publicly available benchmark for the fine-grained
analysis of machine reasoning, including a large annotated corpus and detailed
annotation guidebooks. Our preliminary analysis reveals distinct patterns in
LRM reasoning, such as the transition dynamics between cognitive states. This
framework provides a theoretically grounded methodology for interpreting LRM
cognition and enables future work on more controllable and transparent
reasoning systems.

</details>


### [162] [RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning](https://arxiv.org/abs/2509.14693)
*Song Xu,Yilun Liu,Minggui He,Mingchen Dai,Ziang Chen,Chunguang Zhao,Jingzhou Du,Shimin Tao,Weibin Meng,Shenglin Zhang,Yongqian Sun,Boxing Chen,Daimeng Wei*

Main category: cs.AI

TL;DR: RationAnomaly通过CoT微调和强化学习提升日志异常检测性能，超越现有方法并提供透明分析。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法存在局限性：传统深度学习模型缺乏可解释性和泛化能力，而基于大语言模型的方法常受不可靠性和事实错误困扰。

Method: 提出了RationAnomaly框架，结合CoT引导的监督微调和多面奖励函数的强化学习阶段，以优化准确性和逻辑一致性。

Result: RationAnomaly在关键基准测试中实现了更高的F1分数，并提供了逐步透明的分析输出。

Conclusion: RationAnomaly通过结合Chain-of-Thought（CoT）微调和强化学习，显著提升了日志异常检测的性能，并在实验中超越了现有方法，同时提供了透明的分析输出。

Abstract: Logs constitute a form of evidence signaling the operational status of
software systems. Automated log anomaly detection is crucial for ensuring the
reliability of modern software systems. However, existing approaches face
significant limitations: traditional deep learning models lack interpretability
and generalization, while methods leveraging Large Language Models are often
hindered by unreliability and factual inaccuracies. To address these issues, we
propose RationAnomaly, a novel framework that enhances log anomaly detection by
synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our
approach first instills expert-like reasoning patterns using CoT-guided
supervised fine-tuning, grounded in a high-quality dataset corrected through a
rigorous expert-driven process. Subsequently, a reinforcement learning phase
with a multi-faceted reward function optimizes for accuracy and logical
consistency, effectively mitigating hallucinations. Experimentally,
RationAnomaly outperforms state-of-the-art baselines, achieving superior
F1-scores on key benchmarks while providing transparent, step-by-step
analytical outputs. We have released the corresponding resources, including
code and datasets.

</details>


### [163] [The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs](https://arxiv.org/abs/2509.14704)
*Masaharu Mizumoto,Dat Nguyen,Zhiheng Han,Jiyuan Fang,Heyuan Guan,Xingfu Li,Naoya Shiraishi,Xuyang Tian,Yo Nakawake,Le Minh Nguyen*

Main category: cs.AI

TL;DR: Nazonazo基准测试通过儿童谜语评估模型洞察力推理，发现除GPT-5外模型表现不及人类，推理能力是关键，模型大小无关。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试的饱和和污染问题削弱了对大型语言模型评估的信心，需要一种新的测试方法来评估洞察力推理。

Method: 构建自日本儿童谜语的Nazonazo基准测试，评估了38个前沿模型和126名成年人，分析模型在洞察力推理上的表现。

Result: 除GPT-5外，没有模型能与人类表现相媲美（人类平均准确率为52.9%）。推理模型显著优于非推理模型，而模型大小与准确率无可靠关联。

Conclusion: Nazonazo提供了一种成本效益高、可扩展且易于更新的基准测试格式，解决了当前的评估危机，并揭示了未来控制和校准方法的明确目标。

Abstract: Benchmark saturation and contamination undermine confidence in LLM
evaluation. We present Nazonazo, a cost-effective and extensible benchmark
built from Japanese children's riddles to test insight-based reasoning. Items
are short (mostly one sentence), require no specialized domain knowledge, and
can be generated at scale, enabling rapid refresh of blind sets when leakage is
suspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No
model except for GPT-5 is comparable to human performance, which achieves a
52.9% mean accuracy. Model comparison on extended 201 items shows that
reasoning models significantly outperform non-reasoning peers, while model size
shows no reliable association with accuracy. Beyond aggregate accuracy, an
informal candidate-tracking analysis of thought logs reveals many cases of
verification failure: models often produce the correct solution among
intermediate candidates yet fail to select it as the final answer, which we
illustrate with representative examples observed in multiple models. Nazonazo
thus offers a cost-effective, scalable, and easily renewable benchmark format
that addresses the current evaluation crisis while also suggesting a recurrent
meta-cognitive weakness, providing clear targets for future control and
calibration methods.

</details>


### [164] [Enhancing Retrieval Augmentation via Adversarial Collaboration](https://arxiv.org/abs/2509.14750)
*Letian Zhang,Guanghao Meng,Xudong Ren,Yiming Wang,Shu-Tao Xia*

Main category: cs.AI

TL;DR: AC-RAG通过对抗性协作解决RAG中的‘检索幻觉’问题，显著提升检索准确性和性能。


<details>
  <summary>Details</summary>
Motivation: 针对RAG方法中存在的‘检索幻觉’问题，即模型无法识别和处理低质量检索文档，导致性能下降。

Method: AC-RAG采用两个异构代理：一个通用检测器识别知识缺口，一个领域专业解决器提供精确解决方案，通过对抗性协作动态优化检索过程。

Result: 实验表明，AC-RAG显著提高了检索准确性，并在多个领域中优于现有RAG方法。

Conclusion: AC-RAG框架通过对抗性协作显著提高了检索准确性，并在多个垂直领域中超越了现有最先进的RAG方法。

Abstract: Retrieval-augmented Generation (RAG) is a prevalent approach for
domain-specific LLMs, yet it is often plagued by "Retrieval Hallucinations"--a
phenomenon where fine-tuned models fail to recognize and act upon poor-quality
retrieved documents, thus undermining performance. To address this, we propose
the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two
heterogeneous agents: a generalist Detector that identifies knowledge gaps, and
a domain-specialized Resolver that provides precise solutions. Guided by a
moderator, these agents engage in an adversarial collaboration, where the
Detector's persistent questioning challenges the Resolver's expertise. This
dynamic process allows for iterative problem dissection and refined knowledge
retrieval. Extensive experiments show that AC-RAG significantly improves
retrieval accuracy and outperforms state-of-the-art RAG methods across various
vertical domains.

</details>


### [165] [OpenLens AI: Fully Autonomous Research Agent for Health Infomatics](https://arxiv.org/abs/2509.14778)
*Yuxiao Cheng,Jinli Suo*

Main category: cs.AI

TL;DR: OpenLens AI 是一个针对健康信息学的自动化框架，整合了多种专业代理和视觉语言反馈，解决了现有系统的局限性，并自动化了从研究到发表的全流程。


<details>
  <summary>Details</summary>
Motivation: 健康信息学研究需要跨学科整合，现有系统在医学可视化和领域特定质量要求上存在不足。

Method: OpenLens AI 整合了文献综述、数据分析、代码生成和稿件准备等专业代理，并通过视觉语言反馈和质量控制增强其功能。

Result: OpenLens AI 能够自动化整个研究流程，生成可发表的 LaTeX 稿件，并提供透明且可追溯的工作流程。

Conclusion: OpenLens AI 提供了一个针对健康信息学的自动化框架，通过整合专业代理和视觉语言反馈，解决了现有系统在医学可视化和领域特定质量要求上的不足。

Abstract: Health informatics research is characterized by diverse data modalities,
rapid knowledge expansion, and the need to integrate insights across biomedical
science, data analytics, and clinical practice. These characteristics make it
particularly well-suited for agent-based approaches that can automate knowledge
exploration, manage complex workflows, and generate clinically meaningful
outputs. Recent progress in large language model (LLM)-based agents has
demonstrated promising capabilities in literature synthesis, data analysis, and
even end-to-end research execution. However, existing systems remain limited
for health informatics because they lack mechanisms to interpret medical
visualizations and often overlook domain-specific quality requirements. To
address these gaps, we introduce OpenLens AI, a fully automated framework
tailored to health informatics. OpenLens AI integrates specialized agents for
literature review, data analysis, code generation, and manuscript preparation,
enhanced by vision-language feedback for medical visualization and quality
control for reproducibility. The framework automates the entire research
pipeline, producing publication-ready LaTeX manuscripts with transparent and
traceable workflows, thereby offering a domain-adapted solution for advancing
health informatics research.

</details>


### [166] [Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers](https://arxiv.org/abs/2509.14942)
*Minh-Khoi Pham,Tai Tan Mai,Martin Crane,Rob Brennan,Marie E. Ward,Una Geary,Declan Byrne,Brian O Connell,Colm Bergin,Donncha Creagh,Nick McDonald,Marija Bezbradica*

Main category: cs.AI

TL;DR: 研究利用可解释AI框架分析电子病历数据，发现Transformer模型（如TabTransformer）在预测CPE相关结果中表现优异，并识别出关键风险因素如感染相关特征和网络变量。


<details>
  <summary>Details</summary>
Motivation: 碳青霉烯酶产生肠杆菌科对医院感染防控构成严重威胁，但此前关于CPE相关风险（如再入院、死亡率和住院时间延长）的预测建模研究较少，尤其是现代深度学习方法的应用不足。

Method: 研究引入了一个可解释的AI建模框架，利用爱尔兰医院的电子病历数据，分析住院患者数据集，包括诊断代码、病房转换、患者人口统计、感染相关变量和接触网络特征。对多种基于Transformer的架构与传统机器学习模型进行了基准测试。

Result: 研究框架成功展示了基于Transformer模型的实用性，其中TabTransformer在多个临床预测任务中表现优于基线模型，尤其是在CPE获取（AUROC和敏感性）方面。感染相关特征（包括历史医院暴露、入院背景和网络中心性指标）对预测患者结果和CPE获取风险具有高度影响。

Conclusion: 本研究提出了一个稳健且可解释的AI框架，用于分析复杂的电子病历数据，以识别关键风险因素并预测CPE相关结果。研究结果强调了Transformer模型的优越性能，并突出了多样化的临床和网络特征的重要性。

Abstract: Carbapenemase-Producing Enterobacteriace poses a critical concern for
infection prevention and control in hospitals. However, predictive modeling of
previously highlighted CPE-associated risks such as readmission, mortality, and
extended length of stay (LOS) remains underexplored, particularly with modern
deep learning approaches. This study introduces an eXplainable AI modeling
framework to investigate CPE impact on patient outcomes from Electronic Medical
Records data of an Irish hospital. We analyzed an inpatient dataset from an
Irish acute hospital, incorporating diagnostic codes, ward transitions, patient
demographics, infection-related variables and contact network features. Several
Transformer-based architectures were benchmarked alongside traditional machine
learning models. Clinical outcomes were predicted, and XAI techniques were
applied to interpret model decisions. Our framework successfully demonstrated
the utility of Transformer-based models, with TabTransformer consistently
outperforming baselines across multiple clinical prediction tasks, especially
for CPE acquisition (AUROC and sensitivity). We found infection-related
features, including historical hospital exposure, admission context, and
network centrality measures, to be highly influential in predicting patient
outcomes and CPE acquisition risk. Explainability analyses revealed that
features like "Area of Residence", "Admission Ward" and prior admissions are
key risk factors. Network variables like "Ward PageRank" also ranked highly,
reflecting the potential value of structural exposure information. This study
presents a robust and explainable AI framework for analyzing complex EMR data
to identify key risk factors and predict CPE-related outcomes. Our findings
underscore the superior performance of the Transformer models and highlight the
importance of diverse clinical and network features.

</details>


### [167] [Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems](https://arxiv.org/abs/2509.14956)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 论文提出结合哨兵代理和协调代理的双层次安全架构，通过模拟实验证明其能有效防御多智能体系统的多种威胁。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统面临多种安全威胁（如提示注入、幻觉生成等），需动态且自适应的防御机制。

Method: 采用哨兵代理网络作为分布式安全层，结合语义分析、行为分析等技术，以及协调代理进行策略管理和威胁应对。

Result: 模拟实验中，哨兵代理成功检测了162种合成攻击，验证了框架的实用性。

Conclusion: 该论文提出的双层次安全架构（哨兵代理和协调代理）有效增强了多智能体系统的安全性和可靠性，并通过模拟实验验证了其可行性。

Abstract: This paper proposes a novel architectural framework aimed at enhancing
security and reliability in multi-agent systems (MAS). A central component of
this framework is a network of Sentinel Agents, functioning as a distributed
security layer that integrates techniques such as semantic analysis via large
language models (LLMs), behavioral analytics, retrieval-augmented verification,
and cross-agent anomaly detection. Such agents can potentially oversee
inter-agent communications, identify potential threats, enforce privacy and
access controls, and maintain comprehensive audit records. Complementary to the
idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator
Agent supervises policy implementation, and manages agent participation. In
addition, the Coordinator also ingests alerts from Sentinel Agents. Based on
these alerts, it can adapt policies, isolate or quarantine misbehaving agents,
and contain threats to maintain the integrity of the MAS ecosystem. This
dual-layered security approach, combining the continuous monitoring of Sentinel
Agents with the governance functions of Coordinator Agents, supports dynamic
and adaptive defense mechanisms against a range of threats, including prompt
injection, collusive agent behavior, hallucinations generated by LLMs, privacy
breaches, and coordinated multi-agent attacks. In addition to the architectural
design, we present a simulation study where 162 synthetic attacks of different
families (prompt injection, hallucination, and data exfiltration) were injected
into a multi-agent conversational environment. The Sentinel Agents successfully
detected the attack attempts, confirming the practical feasibility of the
proposed monitoring approach. The framework also offers enhanced system
observability, supports regulatory compliance, and enables policy evolution
over time.

</details>


### [168] [Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles](https://arxiv.org/abs/2509.14963)
*Filip Naudot,Andreas Brännström,Vicenç Torra,Timotheus Kampik*

Main category: cs.AI

TL;DR: 本文扩展了论点贡献函数，从单一论点泛化到一组论点，并提出了新的集合贡献原则及其在推荐系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 旨在量化一组论点对目标论点的集体贡献，填补现有方法仅关注单一论点贡献的不足。

Method: 通过泛化现有贡献函数的原理，提出了适用于集合贡献函数的新原则，重点关注集合内论点间的交互特性。

Result: 提出了新的集合贡献函数原则，并通过推荐系统应用场景展示了不同函数的表现。

Conclusion: 本文提出了量化一组论点在定量双极论证图中对目标论点（主题）最终强度的贡献函数，并扩展了现有单一论点贡献函数的原理分析。

Abstract: We present functions that quantify the contribution of a set of arguments in
quantitative bipolar argumentation graphs to (the final strength of) an
argument of interest, a so-called topic. Our set contribution functions are
generalizations of existing functions that quantify the contribution of a
single contributing argument to a topic. Accordingly, we generalize existing
contribution function principles for set contribution functions and provide a
corresponding principle-based analysis. We introduce new principles specific to
set-based functions that focus on properties pertaining to the interaction of
arguments within a set. Finally, we sketch how the principles play out across
different set contribution functions given a recommendation system application
scenario.

</details>


### [169] [A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making](https://arxiv.org/abs/2509.14998)
*Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Yanyuan Qiao,Imran Razzak,Yutong Xie*

Main category: cs.AI

TL;DR: KAMAC是一个知识驱动的自适应多代理协作框架，通过动态组建专家团队提升复杂临床决策能力，实验证明其在癌症预后等场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的多代理协作框架由于静态预分配角色限制了适应性和动态知识整合能力，无法满足复杂临床场景的需求。

Method: 提出了KAMAC框架，使LLM代理能够根据诊断上下文动态组建和扩展专家团队，通过知识驱动的讨论填补知识空白。

Result: 在两项真实世界医学基准测试中，KAMAC显著优于单代理和先进的多代理方法，尤其是在癌症预后等复杂场景中。

Conclusion: KAMAC框架通过动态组建和扩展专家团队，显著提升了在复杂临床场景中的决策能力，特别是在需要跨专业知识的情况下。

Abstract: Medical decision-making often involves integrating knowledge from multiple
clinical specialties, typically achieved through multidisciplinary teams.
Inspired by this collaborative process, recent work has leveraged large
language models (LLMs) in multi-agent collaboration frameworks to emulate
expert teamwork. While these approaches improve reasoning through agent
interaction, they are limited by static, pre-assigned roles, which hinder
adaptability and dynamic knowledge integration. To address these limitations,
we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration
framework that enables LLM agents to dynamically form and expand expert teams
based on the evolving diagnostic context. KAMAC begins with one or more expert
agents and then conducts a knowledge-driven discussion to identify and fill
knowledge gaps by recruiting additional specialists as needed. This supports
flexible, scalable collaboration in complex clinical scenarios, with decisions
finalized through reviewing updated agent comments. Experiments on two
real-world medical benchmarks demonstrate that KAMAC significantly outperforms
both single-agent and advanced multi-agent methods, particularly in complex
clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty
expertise. Our code is publicly available at:
https://github.com/XiaoXiao-Woo/KAMAC.

</details>


### [170] [Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews](https://arxiv.org/abs/2509.15035)
*Gabriela C. Zapata,Bill Cope,Mary Kalantzis,Duane Searsmith*

Main category: cs.AI

TL;DR: 研究表明生成式AI能有效模拟人类反馈特征，支持学生参与同行评审，提升反馈质量。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI如何通过机器生成的同行评审反馈支持形成性评估，以提升在线研究生课程中的反馈质量和学生参与。

Method: 基于系统功能语言学与评价理论，分析了120份元评价，探讨生成式AI反馈在概念、人际和文本维度上的意义构建。

Result: 生成式AI反馈能够平衡表扬与建设性批评，符合评分标准，结构化展示学生能动性，展现出模拟人类反馈关键特征的潜力。

Conclusion: 生成式AI在形成性评估中能够模拟人类反馈的关键修辞和关系特征，提供清晰指导的同时保持支持性立场，有助于提升学生的反馈素养和参与度。

Abstract: This study investigates the use of generative AI to support formative
assessment through machine generated reviews of peer reviews in graduate online
courses in a public university in the United States. Drawing on Systemic
Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to
explore how generative AI feedback constructs meaning across ideational,
interpersonal, and textual dimensions. The findings suggest that generative AI
can approximate key rhetorical and relational features of effective human
feedback, offering directive clarity while also maintaining a supportive
stance. The reviews analyzed demonstrated a balance of praise and constructive
critique, alignment with rubric expectations, and structured staging that
foregrounded student agency. By modeling these qualities, AI metafeedback has
the potential to scaffold feedback literacy and enhance leaner engagement with
peer review.

</details>


### [171] [From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support](https://arxiv.org/abs/2509.15084)
*Doreen Jirak,Pieter Maes,Armeen Saroukanoff,Dirk van Rooy*

Main category: cs.AI

TL;DR: 本文探讨了海上领域XAI的重要性，提出用户调查以提升AI系统的透明度和信任度，促进人机协作。


<details>
  <summary>Details</summary>
Motivation: 随着自主技术日益影响海上操作，理解AI系统决策的原因变得与其决策本身同等重要，尤其是在复杂动态的海上环境中，信任依赖于透明度和可解释性。

Method: 提出了一项针对海上专业人士的领域特定调查，以捕捉他们对信任、可用性和可解释性的看法。

Result: 通过调查，旨在提高意识并指导开发符合海员和海上团队需求的用户中心XAI系统。

Conclusion: 本文强调了可解释人工智能（XAI）在海上领域作为人机有效协作基础的的重要性，旨在通过用户为中心的调查促进对XAI系统的信任和可用性。

Abstract: As autonomous technologies increasingly shape maritime operations,
understanding why an AI system makes a decision becomes as crucial as what it
decides. In complex and dynamic maritime environments, trust in AI depends not
only on performance but also on transparency and interpretability. This paper
highlights the importance of Explainable AI (XAI) as a foundation for effective
human-machine teaming in the maritime domain, where informed oversight and
shared understanding are essential. To support the user-centered integration of
XAI, we propose a domain-specific survey designed to capture maritime
professionals' perceptions of trust, usability, and explainability. Our aim is
to foster awareness and guide the development of user-centric XAI systems
tailored to the needs of seafarers and maritime teams.

</details>


### [172] [Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment](https://arxiv.org/abs/2509.15172)
*Ankur Samanta,Akshayaa Magesh,Youliang Yu,Runzhe Wu,Ayush Jain,Daniel Jiang,Boris Vidolov,Paul Sajda,Yonathan Efroni,Kaveh Hassani*

Main category: cs.AI

TL;DR: MACA通过多智能体辩论和自我对齐提升语言模型推理的一致性和性能，显著改进多项任务表现。


<details>
  <summary>Details</summary>
Motivation: 语言模型在推理时存在不一致性，当前推理时方法无法解决核心问题，即模型难以可靠选择导致一致结果的推理路径。

Method: 提出了多智能体共识对齐（MACA）框架，通过强化学习后训练模型，使其偏好与内部共识一致的理由轨迹，利用多智能体辩论中的多数/少数结果。

Result: MACA在自我一致性（GSM8K +27.6%）、单智能体推理（MATH +23.7%）、基于采样的推理（MATH Pass@20 +22.4%）和多智能体集成决策（MathQA +42.7%）等方面带来显著改进，并在未见过的基准测试中表现出强泛化能力（GPQA +16.3%，CommonsenseQA +11.6%）。

Conclusion: MACA框架通过多智能体辩论和自我对齐显著提升了语言模型在推理任务中的一致性和性能，展示了其在解锁模型潜在推理能力方面的有效性。

Abstract: Language Models (LMs) are inconsistent reasoners, often generating
contradictory responses to identical prompts. While inference-time methods can
mitigate these inconsistencies, they fail to address the core problem: LMs
struggle to reliably select reasoning pathways leading to consistent outcomes
under exploratory sampling. To address this, we formalize self-consistency as
an intrinsic property of well-aligned reasoning models and introduce
Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that
post-trains models to favor reasoning trajectories aligned with their internal
consensus using majority/minority outcomes from multi-agent debate. These
trajectories emerge from deliberative exchanges where agents ground reasoning
in peer arguments, not just aggregation of independent attempts, creating
richer consensus signals than single-round majority voting. MACA enables agents
to teach themselves to be more decisive and concise, and better leverage peer
insights in multi-agent settings without external supervision, driving
substantial improvements across self-consistency (+27.6% on GSM8K),
single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%
Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).
These findings, coupled with strong generalization to unseen benchmarks (+16.3%
on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more
reliably unlocks latent reasoning potential of language models.

</details>


### [173] [Generalizable Geometric Image Caption Synthesis](https://arxiv.org/abs/2509.15217)
*Yue Xin,Wenyuan Wang,Rui Pan,Ruida Wang,Howard Meng,Renjie Pi,Shizhe Diao,Tong Zhang*

Main category: cs.AI

TL;DR: 该研究通过RLVR方法优化几何图像标题生成，显著提升了多模态大语言模型的几何问题解决和一般推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在复杂几何问题上的局限性，尤其是缺乏高质量几何图像-文本对数据集的问题。

Method: 采用Reinforcement Learning with Verifiable Rewards (RLVR)方法，结合50种基本几何关系合成的图像，通过数学问题解决任务生成的奖励信号优化标题生成。

Result: 在非几何输入的MathVista和MathVerse任务中准确率提升2.8%-4.8%，在MMMU的艺术、设计、技术和工程任务中提升2.4%-3.9%。

Conclusion: 通过引入RLVR方法，该研究成功提升了多模态大语言模型在几何问题解决和一般推理任务中的性能，显著提高了多项任务的准确率。

Abstract: Multimodal large language models have various practical applications that
demand strong reasoning abilities. Despite recent advancements, these models
still struggle to solve complex geometric problems. A key challenge stems from
the lack of high-quality image-text pair datasets for understanding geometric
images. Furthermore, most template-based data synthesis pipelines typically
fail to generalize to questions beyond their predefined templates. In this
paper, we bridge this gap by introducing a complementary process of
Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation
pipeline. By adopting RLVR to refine captions for geometric images synthesized
from 50 basic geometric relations and using reward signals derived from
mathematical problem-solving tasks, our pipeline successfully captures the key
features of geometry problem-solving. This enables better task generalization
and yields non-trivial improvements. Furthermore, even in out-of-distribution
scenarios, the generated dataset enhances the general reasoning capabilities of
multimodal large language models, yielding accuracy improvements of
$2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks
with non-geometric input images of MathVista and MathVerse, along with
$2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks
in MMMU.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [174] [WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance](https://arxiv.org/abs/2509.15130)
*Chenxi Song,Yanming Yang,Tong Zhao,Ruibo Li,Chi Zhang*

Main category: cs.GR

TL;DR: WorldForge是一个无需训练的推理框架，通过三个模块实现精确运动控制和逼真视频生成，解决了现有模型的可控性和几何一致性问题。


<details>
  <summary>Details</summary>
Motivation: 解决当前视频扩散模型在3D/4D任务中可控性和几何一致性不足的问题，避免重新训练或微调带来的计算成本和预训练知识退化。

Method: Intra-Step Recursive Refinement、Flow-Gated Latent Fusion和Dual-Path Self-Corrective Guidance三个模块的组合。

Result: 在多个基准测试中验证了方法在真实性、轨迹一致性和视觉保真度方面的优越性。

Conclusion: WorldForge提出了一种无需训练、推理时的框架，通过三个紧密耦合的模块实现了精确的运动控制和逼真的内容生成，为可控视频合成提供了新的视角。

Abstract: Recent video diffusion models demonstrate strong potential in spatial
intelligence tasks due to their rich latent world priors. However, this
potential is hindered by their limited controllability and geometric
inconsistency, creating a gap between their strong priors and their practical
use in 3D/4D tasks. As a result, current approaches often rely on retraining or
fine-tuning, which risks degrading pretrained knowledge and incurs high
computational costs. To address this, we propose WorldForge, a training-free,
inference-time framework composed of three tightly coupled modules. Intra-Step
Recursive Refinement introduces a recursive refinement mechanism during
inference, which repeatedly optimizes network predictions within each denoising
step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages
optical flow similarity to decouple motion from appearance in the latent space
and selectively inject trajectory guidance into motion-related channels.
Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths
to adaptively correct trajectory drift caused by noisy or misaligned structural
signals. Together, these components inject fine-grained, trajectory-aligned
guidance without training, achieving both accurate motion control and
photorealistic content generation. Extensive experiments across diverse
benchmarks validate our method's superiority in realism, trajectory
consistency, and visual fidelity. This work introduces a novel plug-and-play
paradigm for controllable video synthesis, offering a new perspective on
leveraging generative priors for spatial intelligence.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [175] [A Software-Defined Radio Testbed for Distributed LiDAR Point Cloud Sharing with IEEE 802.11p in V2V Networks](https://arxiv.org/abs/2509.14523)
*Mario Hernandez,Elijah Bryce,Peter Stubberud,Ebrahim Saberinia,Brendan Morris*

Main category: cs.NI

TL;DR: 提出了一种低成本SDR-based V2V通信测试平台，验证了协作感知功能，并研究了去中心化存储系统的性能。


<details>
  <summary>Details</summary>
Motivation: 解决网络仿真与实际部署之间的差距，提供一个灵活且成本效益高的V2V通信测试平台，并探索去中心化存储系统在V2V通信中的应用潜力。

Method: 采用模块化代码库配置ADALM-Pluto SDR，支持Docker、ROS和Matlab，构建了一个成本效益高的V2V通信测试平台。通过共享LiDAR点云和融合数据实现了协作感知。

Result: 成功构建了SDR-based IEEE 802.11p测试平台，验证了协作感知的可行性，并分析了去中心化存储系统的性能限制。

Conclusion: 该论文提出了一种基于SDR的IEEE 802.11p测试平台，用于分布式V2V通信，并通过共享LiDAR点云和融合数据验证了其有效性。同时，研究了去中心化存储系统的潜力与限制。

Abstract: We present a Software Defined Radio (SDR)-based IEEE 802.11p testbed for
distributed Vehicle-to-Vehicle (V2V) communication. The platform bridges the
gap between network simulation and deployment by providing a modular codebase
configured for cost-effective ADALM-Pluto SDRs. Any device capable of running a
Docker with ROS, executing Matlab and interface with a Pluto via USB can act as
a communication node. To demonstrate collaborative sensing, we share LiDAR
point clouds between nodes and fuse them into a collective perception
environment. We evaluated a theoretical model for leveraging decentralized
storage systems (IPFS and Filecoin), analyzing constraints such as node storage
convergence, latency, and scalability. In addition, we provide a channel
quality study.

</details>


### [176] [Chameleon: Integrated Sensing and Communication with Sub-Symbol Beam Switching in mmWave Networks](https://arxiv.org/abs/2509.14628)
*Zhihui Gao,Zhecun Liu,Tingjun Chen*

Main category: cs.NI

TL;DR: Chameleon框架通过快速切换波束形成器，在5G毫米波网络中实现了集成感知与通信，同时保持多用户通信和高精度感知。


<details>
  <summary>Details</summary>
Motivation: 当前5G网络中的波束形成通常仅针对通信或感知设计，缺乏集成感知与通信的能力。Chameleon旨在填补这一空白，实现高效的ISAC。

Method: Chameleon框架通过在每次解调参考信号（DMRS）符号期间增强和快速切换波束形成器，引入额外的感知波束，同时维持对多用户的通信波束。

Result: 实验表明，Chameleon在开放环境中实现了高达0.80 Gbps的多用户通信速率，同时以0.24微秒的波束切换间隔实现了31x31点的2D成像，并通过机器学习实现了高精度的物体定位和材料分类。

Conclusion: Chameleon框架成功实现了5G毫米波网络中的集成感知与通信（ISAC），通过快速切换波束形成器，在保持多用户通信的同时，实现了高精度的感知功能。

Abstract: Next-generation cellular networks are envisioned to integrate sensing
capabilities with communication, particularly in the millimeter-wave (mmWave)
spectrum, where beamforming using large-scale antenna arrays enables
directional signal transmissions for improved spatial multiplexing. In current
5G networks, however, beamforming is typically designed either for
communication or sensing (e.g., beam training during link establishment). In
this paper, we present Chameleon, a novel framework that augments and rapidly
switches beamformers during each demodulation reference signal (DMRS) symbol to
achieve integrated sensing and communication (ISAC) in 5G mmWave networks. Each
beamformer introduces an additional sensing beam toward target angles while
maintaining the communication beams toward multiple users. We implement
Chameleon on a 28 GHz software-defined radio testbed supporting over-the-air 5G
physical downlink shared channel (PDSCH) transmissions. Extensive experiments
in open environments show that Chameleon achieves multi-user communication with
a sum data rate of up to 0.80 Gbps across two users. Simultaneously, Chameleon
employs a beamformer switching interval of only 0.24 {\mu}s, therefore
producing a 31x31-point 2D imaging within just 0.875 ms. Leveraging machine
learning, Chameleon further enables object localization with median errors of
0.14 m (distance) and 0.24{\deg} (angle), and material classification with
99.0% accuracy.

</details>


### [177] [1Q: First-Generation Wireless Systems Integrating Classical and Quantum Communication](https://arxiv.org/abs/2509.14731)
*Petar Popovski,Čedomir Stefanović,Beatriz Soret,Israel Leyva-Mayorga,Shashi Raj Pandey,René Bødker Christensen,Jakob Kaltoft Søndergaard,Kristian Skafte Jensen,Thomas Garm Pedersen,Angela Sara Cacciapuoti,Lajos Hanzo*

Main category: cs.NI

TL;DR: 1Q框架是第一代无线集成经典和量子通信的概念，通过量子基站支持纠缠分发和传统通信，扩展量子互联网到无线蜂窝网络。


<details>
  <summary>Details</summary>
Motivation: 探讨并扩展量子互联网到无线蜂窝网络，支持量子密钥分发、盲量子计算和分布式量子传感等应用场景。

Method: 1Q框架引入了量子单元、量子用户设备（QUEs）以及跨越经典时频和量子纠缠领域的混合资源分配。

Result: 识别了一系列独特的量子约束，如退相干时间、保真度要求以及量子与经典错误概率的相互作用。协议扩展了蜂窝连接管理，纳入纠缠生成、分发和切换程序。

Conclusion: 1Q框架提出了将经典和量子通信整合的第一代无线通信概念，通过量子基站（QBSs）支持自由空间光学链路的纠缠分发和传统无线电通信。

Abstract: We sketch out the concept of 1Q, the first wireless generation of integrated
classical and quantum communication. The 1Q framework features quantum base
stations (QBSs) that support entanglement distribution via free-space optical
links alongside traditional radio communications. Key new components include
quantum cells, quantum user equipment (QUEs), and hybrid resource allocation
spanning classical time-frequency and quantum entanglement domains. Several
application scenarios are discussed and illustrated through system design
requirements for quantum key distribution, blind quantum computing, and
distributed quantum sensing. A range of unique quantum constraints are
identified, including decoherence timing, fidelity requirements, and the
interplay between quantum and classical error probabilities. Protocol
adaptations extend cellular connection management to incorporate entanglement
generation, distribution, and handover procedures, expanding the Quantum
Internet to the cellular wireless.

</details>


### [178] [AI-Driven Multi-Agent Vehicular Planning for Battery Efficiency and QoS in 6G Smart Cities](https://arxiv.org/abs/2509.14877)
*Rohin Gillgallon,Giacomo Bergami,Reham Almutairi,Graham Morgan*

Main category: cs.NI

TL;DR: 扩展SO模拟器，引入AI算法优化车载通信，减少电池消耗并提升QoS性能，测试显示优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有车载物联网节点与云端通过边缘节点在完全模拟渗透架构中通信的模拟器缺乏动态代理规划和优化支持，无法有效减少车载电池消耗并确保公平通信时间。

Method: 通过扩展SimulatorOrchestrator（SO）架构，引入AI算法进行交通预测和动态代理规划，并在现实城市数据集上进行初步测试。

Result: 初步结果显示，与传统最短路径算法相比，利用车载规划算法可以改善电池和QoS性能；引入理想区域后，更多救护车能以更低能耗到达目的地。

Conclusion: 扩展SimulatorOrchestrator（SO）以支持动态代理规划和优化的AI算法，可以显著减少车载电池消耗并确保公平通信时间，同时提高QoS性能。

Abstract: While simulators exist for vehicular IoT nodes communicating with the Cloud
through Edge nodes in a fully-simulated osmotic architecture, they often lack
support for dynamic agent planning and optimisation to minimise vehicular
battery consumption while ensuring fair communication times. Addressing these
challenges requires extending current simulator architectures with AI
algorithms for both traffic prediction and dynamic agent planning. This paper
presents an extension of SimulatorOrchestrator (SO) to meet these requirements.
Preliminary results over a realistic urban dataset show that utilising
vehicular planning algorithms can lead to improved battery and QoS performance
compared with traditional shortest path algorithms. The additional inclusion of
desirability areas enabled more ambulances to be routed to their target
destinations while utilising less energy to do so, compared to traditional and
weighted algorithms without desirability considerations.

</details>
