<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 76]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DS](#cs.DS) [Total: 9]
- [cs.NI](#cs.NI) [Total: 13]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.RO](#cs.RO) [Total: 54]
- [cs.GR](#cs.GR) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Vision-Based Perception for Autonomous Vehicles in Off-Road Environment Using Deep Learning](https://arxiv.org/abs/2509.19378)
*Nelson Alves Ferreira Neto*

Main category: cs.CV

TL;DR: CMSNet框架和Kamino数据集提升了自动驾驶在非铺装/越野环境中的实时语义分割能力，尤其在恶劣条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 为满足露天矿场和发展中国家非均匀地形自动驾驶的低延迟需求，开发能在无预设路径的崎岖地形中导航的感知系统。

Method: 提出了可配置模块化分割网络（CMSNet）框架，支持不同架构组合，并通过TensorRT、C++和CUDA优化实现实时推理。

Result: 实验验证了CMSNet在两种数据集上的有效性，并展示了在恶劣条件下（如夜间、雨、灰尘）的鲁棒性能。

Conclusion: 该研究提出的CMSNet框架和Kamino数据集有效支持了非铺装道路和越野环境中的实时语义分割，为自动驾驶在复杂地形中的应用提供了实用解决方案。

Abstract: Low-latency intelligent systems are required for autonomous driving on
non-uniform terrain in open-pit mines and developing countries. This work
proposes a perception system for autonomous vehicles on unpaved roads and
off-road environments, capable of navigating rough terrain without a predefined
trail. The Configurable Modular Segmentation Network (CMSNet) framework is
proposed, facilitating different architectural arrangements. CMSNet
configurations were trained to segment obstacles and trafficable ground on new
images from unpaved/off-road scenarios with adverse conditions (night, rain,
dust). We investigated applying deep learning to detect drivable regions
without explicit track boundaries, studied algorithm behavior under visibility
impairment, and evaluated field tests with real-time semantic segmentation. A
new dataset, Kamino, is presented with almost 12,000 images from an operating
vehicle with eight synchronized cameras. The Kamino dataset has a high number
of labeled pixels compared to similar public collections and includes images
from an off-road proving ground emulating a mine under adverse visibility. To
achieve real-time inference, CMSNet CNN layers were methodically removed and
fused using TensorRT, C++, and CUDA. Empirical experiments on two datasets
validated the proposed system's effectiveness.

</details>


### [2] [Overview of LifeCLEF Plant Identification task 2020](https://arxiv.org/abs/2509.19402)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: PlantCLEF 2020挑战利用植物标本馆藏品提升数据稀缺地区的植物自动识别，基于跨域分类任务评估效果。


<details>
  <summary>Details</summary>
Motivation: 解决热带地区等生物多样性丰富但数据稀缺区域的植物自动识别问题。

Method: 基于深度学习的跨域分类任务，训练集包含数十万张植物标本和少量野外照片，测试集仅包含野外照片。

Result: PlantCLEF 2020挑战评估了利用植物标本馆藏品提升自动识别的效果，并总结了参与团队的方法和系统。

Conclusion: 该论文强调了利用植物标本馆藏品提升数据稀缺地区植物自动识别的重要性，并展示了PlantCLEF 2020挑战的主要成果和方法。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data with
more and more photos in the field. However, this profusion of data only
concerns a few tens of thousands of species, mostly located in North America
and Western Europe, much less in the richest regions in terms of biodiversity
such as tropical countries. On the other hand, for several centuries, botanists
have collected, catalogued and systematically stored plant specimens in
herbaria, particularly in tropical regions, and the recent efforts by the
biodiversity informatics community made it possible to put millions of
digitized sheets online. The LifeCLEF 2020 Plant Identification challenge (or
"PlantCLEF 2020") was designed to evaluate to what extent automated
identification on the flora of data deficient regions can be improved by the
use of herbarium collections. It is based on a dataset of about 1,000 species
mainly focused on the South America's Guiana Shield, an area known to have one
of the greatest diversity of plants in the world. The challenge was evaluated
as a cross-domain classification task where the training set consist of several
hundred thousand herbarium sheets and few thousand of photos to enable learning
a mapping between the two domains. The test set was exclusively composed of
photos in the field. This paper presents the resources and assessments of the
conducted evaluation, summarizes the approaches and systems employed by the
participating research groups, and provides an analysis of the main outcomes.

</details>


### [3] [iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning](https://arxiv.org/abs/2509.19552)
*Manyi Yao,Bingbing Zhuang,Sparsh Garg,Amit Roy-Chowdhury,Christian Shelton,Manmohan Chandraker,Abhishek Aich*

Main category: cs.CV

TL;DR: iFinder是一种结构化语义接地框架，通过分层、可解释的数据结构和三块提示策略，显著提升LLMs在驾驶视频分析中的表现，准确率提升高达39%。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在通用目的训练和缺乏结构化归纳偏置方面的局限性，在特定领域任务（如事后驾驶视频分析）中表现不佳。现有基于视频的视觉语言模型（V-VLMs）在空间推理、因果推断和输入视频事件的可解释性上存在困难。

Method: iFinder是一个模块化的、无需训练的框架，通过预训练的视觉模型提取关键线索（如物体姿态、车道位置和物体轨迹），并分层组织成帧级和视频级结构。结合三块提示策略，实现了逐步、有基础的推理。

Result: 在四个公开的驾驶视频基准测试中，iFinder在零射击驾驶基准测试中显著优于端到端V-VLMs，事故推理准确率提升高达39%。

Conclusion: iFinder通过将LLMs与驾驶领域特定的表示相结合，为零射驾驶视频理解提供了一个可解释且可靠的替代方案，显著优于端到端的V-VLMs。

Abstract: Grounding large language models (LLMs) in domain-specific tasks like post-hoc
dash-cam driving video analysis is challenging due to their general-purpose
training and lack of structured inductive biases. As vision is often the sole
modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing
video-based vision-language models (V-VLMs) struggle with spatial reasoning,
causal inference, and explainability of events in the input video. To this end,
we introduce iFinder, a structured semantic grounding framework that decouples
perception from reasoning by translating dash-cam videos into a hierarchical,
interpretable data structure for LLMs. iFinder operates as a modular,
training-free pipeline that employs pretrained vision models to extract
critical cues -- object pose, lane positions, and object trajectories -- which
are hierarchically organized into frame- and video-level structures. Combined
with a three-block prompting strategy, it enables step-wise, grounded reasoning
for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.
Evaluations on four public dash-cam video benchmarks show that iFinder's
proposed grounding with domain-specific cues, especially object orientation and
global context, significantly outperforms end-to-end V-VLMs on four zero-shot
driving benchmarks, with up to 39% gains in accident reasoning accuracy. By
grounding LLMs with driving domain-specific representations, iFinder offers a
zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for
post-hoc driving video understanding.

</details>


### [4] [CURE: Centroid-guided Unsupervised Representation Erasure for Facial Recognition Systems](https://arxiv.org/abs/2509.19562)
*Fnu Shivam,Nima Najafzadeh,Yenumula Reddy,Prashnna Gyawali*

Main category: cs.CV

TL;DR: CURE 是无监督面部识别遗忘框架，无需身份标签，性能优越，并提出了新评估指标 UES。


<details>
  <summary>Details</summary>
Motivation: 面部识别系统广泛使用引发隐私担忧，现有机器遗忘技术依赖监督方法，但身份标签在隐私受限或大规模噪声数据集中常不可用。

Method: 提出了 CURE（Centroid-guided Unsupervised Representation Erasure），一种无需身份标签的无监督遗忘框架，并引入了 Unlearning Efficiency Score (UES) 作为新评估指标。

Result: CURE 显著优于现有无监督遗忘方法，并通过质量感知遗忘展示了图像质量在机器遗忘中的作用。

Conclusion: CURE 提供了一种无监督的机器遗忘框架，有效解决了隐私保护问题，同时保持了模型性能，并通过提出的 UES 指标改进了评估标准。

Abstract: In the current digital era, facial recognition systems offer significant
utility and have been widely integrated into modern technological
infrastructures; however, their widespread use has also raised serious privacy
concerns, prompting regulations that mandate data removal upon request. Machine
unlearning has emerged as a powerful solution to address this issue by
selectively removing the influence of specific user data from trained models
while preserving overall model performance. However, existing machine
unlearning techniques largely depend on supervised techniques requiring
identity labels, which are often unavailable in privacy-constrained situations
or in large-scale, noisy datasets. To address this critical gap, we introduce
CURE (Centroid-guided Unsupervised Representation Erasure), the first
unsupervised unlearning framework for facial recognition systems that operates
without the use of identity labels, effectively removing targeted samples while
preserving overall performance. We also propose a novel metric, the Unlearning
Efficiency Score (UES), which balances forgetting and retention stability,
addressing shortcomings in the current evaluation metrics. CURE significantly
outperforms unsupervised variants of existing unlearning methods. Additionally,
we conducted quality-aware unlearning by designating low-quality images as the
forget set, demonstrating its usability and benefits, and highlighting the role
of image quality in machine unlearning.

</details>


### [5] [Synthesizing Artifact Dataset for Pixel-level Detection](https://arxiv.org/abs/2509.19589)
*Dennis Menn,Feng Liang,Diana Marculescu*

Main category: cs.CV

TL;DR: 通过自动注入伪影生成标注数据，提升了伪影检测器的性能，避免了昂贵的人工标注。


<details>
  <summary>Details</summary>
Motivation: 当前伪影检测器的性能受限于缺乏标注数据，而人工标注成本高昂，且传统伪标签方法因噪声标签导致效果不佳。

Method: 提出了一种伪影破坏管道，自动将伪影注入干净的合成图像中，生成像素级标注，避免了昂贵的人工标注需求。

Result: 提出的方法在人类标注数据上验证，ConvNeXt和Swin-T的性能分别提升了13.2%和3.7%，优于基线方法。

Conclusion: 本研究提出了一种自动注入伪影的方法，通过合成图像生成像素级标注，显著提升了伪影检测器的性能，为构建可扩展的像素级伪影标注数据集提供了初步解决方案。

Abstract: Artifact detectors have been shown to enhance the performance of
image-generative models by serving as reward models during fine-tuning. These
detectors enable the generative model to improve overall output fidelity and
aesthetics. However, training the artifact detector requires expensive
pixel-level human annotations that specify the artifact regions. The lack of
annotated data limits the performance of the artifact detector. A naive
pseudo-labeling approach-training a weak detector and using it to annotate
unlabeled images-suffers from noisy labels, resulting in poor performance. To
address this, we propose an artifact corruption pipeline that automatically
injects artifacts into clean, high-quality synthetic images on a predetermined
region, thereby producing pixel-level annotations without manual labeling. The
proposed method enables training of an artifact detector that achieves
performance improvements of 13.2% for ConvNeXt and 3.7% for Swin-T, as verified
on human-labeled data, compared to baseline approaches. This work represents an
initial step toward scalable pixel-level artifact annotation datasets that
integrate world knowledge into artifact detection.

</details>


### [6] [Parameter-Efficient Multi-Task Learning via Progressive Task-Specific Adaptation](https://arxiv.org/abs/2509.19602)
*Neeraj Gangwar,Anshuka Rangi,Rishabh Deshmukh,Holakou Rahmanian,Yesh Dattatreya,Nickvash Kani*

Main category: cs.CV

TL;DR: 提出渐进式任务特定适配器方法，通过共享初始层和梯度任务相似性分配，显著提升多任务学习效率。


<details>
  <summary>Details</summary>
Motivation: 解决多任务学习中任务干扰和负迁移问题，通过共享初始层适配器模块并逐步增加任务特定性。

Method: 提出了渐进式任务特定多任务适应方法，包括共享适配器模块和基于梯度的任务相似性计算。

Result: 在PASCAL和NYUD-v2数据集上实验表明，该方法优于全微调多任务模型，且参数效率更高。

Conclusion: 该方法在参数高效多任务学习中表现优于当前最先进方法，仅需五分之一的训练参数即可超越全微调模型。

Abstract: Parameter-efficient fine-tuning methods have emerged as a promising solution
for adapting pre-trained models to various downstream tasks. While these
methods perform well in single-task learning, extending them to multi-task
learning exacerbates common challenges, such as task interference and negative
transfer, due to the limited number of trainable parameters. To address these
issues, we introduce progressive task-specific multi-task adaptation, a novel
parameter-efficient approach for multi-task learning. This approach introduces
adapter modules in a pre-trained model such that these modules are shared
across all tasks in the initial layers and become progressively more
task-specific in the later layers. The motivation is to reduce the conflicts
among tasks by allowing transfer learning across all tasks in the initial
layers and enabling task-specific learning toward the prediction heads.
Additionally, we propose a gradient-based approach for computing task
similarity and use this measure to allocate similar tasks to the shared adapter
modules. Our task similarity method introduces minimal overhead in the
pipeline. We evaluate our approach by adapting the Swin Transformer for dense
prediction tasks. Experiments on the PASCAL and NYUD-v2 datasets demonstrate
that our approach outperforms a fully fine-tuned multi-task model while
requiring only one-fifth of the trainable parameters. This approach achieves
better relative improvement to single-task fine-tuning while reducing the
number of trainable parameters and surpasses the current state-of-the-art
methods for parameter-efficient multi-task learning.

</details>


### [7] [Raw-JPEG Adapter: Efficient Raw Image Compression with JPEG](https://arxiv.org/abs/2509.19624)
*Mahmoud Afifi,Ran Zhang,Michael S. Brown*

Main category: cs.CV

TL;DR: RawJPEG Adapter 是一种轻量级、可逆的预处理方法，将原始图像适配为JPEG压缩，实现高效存储与高保真重建。


<details>
  <summary>Details</summary>
Motivation: 原始数据（如DNG）保留了完整的传感器信息，但存储需求大；JPEG格式压缩效率高且兼容性强，但不适合原始存储。因此，需要一种方法既能保留原始信息，又能高效压缩。

Method: 该方法应用空间和可选频域变换，并将紧凑参数存储在JPEG注释字段中，以实现准确的原始重建。

Result: 实验表明，该方法比直接JPEG存储具有更高的保真度，支持其他编解码器，并在压缩比和重建精度之间提供了有利的权衡。

Conclusion: RawJPEG Adapter 提供了一种轻量级、可学习且可逆的预处理方法，成功实现了将原始图像适配为标准JPEG压缩，同时在压缩比和重建精度之间取得了良好的平衡。

Abstract: Digital cameras digitize scene light into linear raw representations, which
the image signal processor (ISP) converts into display-ready outputs. While raw
data preserves full sensor information--valuable for editing and vision
tasks--formats such as Digital Negative (DNG) require large storage, making
them impractical in constrained scenarios. In contrast, JPEG is a widely
supported format, offering high compression efficiency and broad compatibility,
but it is not well-suited for raw storage. This paper presents RawJPEG Adapter,
a lightweight, learnable, and invertible preprocessing pipeline that adapts raw
images for standard JPEG compression. Our method applies spatial and optional
frequency-domain transforms, with compact parameters stored in the JPEG comment
field, enabling accurate raw reconstruction. Experiments across multiple
datasets show that our method achieves higher fidelity than direct JPEG
storage, supports other codecs, and provides a favorable trade-off between
compression ratio and reconstruction accuracy.

</details>


### [8] [The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar](https://arxiv.org/abs/2509.19644)
*William L. Muckelroy III,Mohammed Alsakabi,John M. Dolan,Ozan K. Tonguz*

Main category: cs.CV

TL;DR: 研究探讨了高容量分割骨干网络对4D雷达生成LiDAR点云质量的影响，发现最优模型比SOTA提升23.7%。


<details>
  <summary>Details</summary>
Motivation: LiDAR的高成本限制了其在商用车辆中的广泛应用，因此探索使用4D雷达生成类似LiDAR的3D点云的方法。

Method: 通过研究更高容量的分割骨干网络对生成点云质量的影响。

Result: 最优分割骨干网络比现有技术提升了23.7%。

Conclusion: 研究发现，虽然极高容量的模型可能会损害性能，但最优分割骨干网络可以在现有技术基础上提升23.7%。

Abstract: LiDAR's dense, sharp point cloud (PC) representations of the surrounding
environment enable accurate perception and significantly improve road safety by
offering greater scene awareness and understanding. However, LiDAR's high cost
continues to restrict the broad adoption of high-level Autonomous Driving (AD)
systems in commercially available vehicles. Prior research has shown progress
towards circumventing the need for LiDAR by training a neural network, using
LiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds
using only 4D Radars. One of the best examples is a neural network created to
train a more efficient radar target detector with a modular 2D convolutional
neural network (CNN) backbone and a temporal coherence network at its core that
uses the RaDelft dataset for training (see arXiv:2406.04723). In this work, we
investigate the impact of higher-capacity segmentation backbones on the quality
of the produced point clouds. Our results show that while very high-capacity
models may actually hurt performance, an optimal segmentation backbone can
provide a 23.7% improvement over the state-of-the-art (SOTA).

</details>


### [9] [Bias in the Picture: Benchmarking VLMs with Social-Cue News Images and LLM-as-Judge Assessment](https://arxiv.org/abs/2509.19659)
*Aravind Narayanan,Vahid Reza Khazaie,Shaina Raza*

Main category: cs.CV

TL;DR: 研究表明VLM在处理视觉和文本时容易产生社会刻板印象，尤其是性别和职业。研究发布了一个新闻图像基准数据集以支持公平评估。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型（VLM）能够联合解释图像和文本，但它们也容易吸收和再现有害的社会刻板印象，尤其是在视觉线索（如年龄、性别、种族、服装或职业）存在的情况下。

Method: 研究引入了一个包含1,343个图像-问题对的新闻图像基准数据集，标注了真实答案和人口统计属性（年龄、性别、种族、职业和运动）。评估了多种最先进的VLM，并利用大型语言模型（LLM）作为评判者，辅以人工验证。

Result: 研究发现：（i）在开放式设置中，视觉上下文会系统性地改变模型输出；（ii）不同属性和模型的偏见普遍性不同，性别和职业的风险尤其高；（iii）更高的忠实度并不一定对应更低的偏见。

Conclusion: 研究揭示了大型视觉语言模型（VLM）在处理视觉和文本信息时容易吸收和再现有害社会刻板印象的问题，尤其是在性别和职业等属性上。研究团队发布了一个新闻图像基准数据集，以支持可重复和公平的多模态评估。

Abstract: Large vision-language models (VLMs) can jointly interpret images and text,
but they are also prone to absorbing and reproducing harmful social stereotypes
when visual cues such as age, gender, race, clothing, or occupation are
present. To investigate these risks, we introduce a news-image benchmark
consisting of 1,343 image-question pairs drawn from diverse outlets, which we
annotated with ground-truth answers and demographic attributes (age, gender,
race, occupation, and sports). We evaluate a range of state-of-the-art VLMs and
employ a large language model (LLM) as judge, with human verification. Our
findings show that: (i) visual context systematically shifts model outputs in
open-ended settings; (ii) bias prevalence varies across attributes and models,
with particularly high risk for gender and occupation; and (iii) higher
faithfulness does not necessarily correspond to lower bias. We release the
benchmark prompts, evaluation rubric, and code to support reproducible and
fairness-aware multimodal assessment.

</details>


### [10] [MoTiC: Momentum Tightness and Contrast for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.19664)
*Zeyu He,Shuai Huang,Yuwu Lu,Ming Zhao*

Main category: cs.CV

TL;DR: MoTiC框架通过贝叶斯分析和对比学习减少新类原型偏差，提升FSCIL性能，在CUB-200上表现突出。


<details>
  <summary>Details</summary>
Motivation: 解决FSCIL任务中因数据稀缺导致的新类原型估计偏差问题，同时保持旧类知识。

Method: 提出了MoTiC框架，结合贝叶斯分析、大规模对比学习、动量自监督和虚拟类别，以构建具有丰富表示和增强类间凝聚力的特征空间。

Result: 在三个FSCIL基准测试中取得了最先进的性能，特别是在CUB-200细粒度任务上验证了方法的有效性。

Conclusion: MoTiC框架通过在FSCIL任务中引入贝叶斯分析和对比学习，显著减少了新类原型的估计偏差，提升了增量学习的鲁棒性，尤其在细粒度任务CUB-200上表现优异。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) must contend with the dual
challenge of learning new classes from scarce samples while preserving old
class knowledge. Existing methods use the frozen feature extractor and
class-averaged prototypes to mitigate against catastrophic forgetting and
overfitting. However, new-class prototypes suffer significant estimation bias
due to extreme data scarcity, whereas base-class prototypes benefit from
sufficient data. In this work, we theoretically demonstrate that aligning the
new-class priors with old-class statistics via Bayesian analysis reduces
variance and improves prototype accuracy. Furthermore, we propose large-scale
contrastive learning to enforce cross-category feature tightness. To further
enrich feature diversity and inject prior information for new-class prototypes,
we integrate momentum self-supervision and virtual categories into the Momentum
Tightness and Contrast framework (MoTiC), constructing a feature space with
rich representations and enhanced interclass cohesion. Experiments on three
FSCIL benchmarks produce state-of-the-art performances, particularly on the
fine-grained task CUB-200, validating our method's ability to reduce estimation
bias and improve incremental learning robustness.

</details>


### [11] [Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy](https://arxiv.org/abs/2509.19665)
*Manuel Perez-Carrasco,Maya Nasr,Sebastien Roche,Chris Chan Miller,Zhan Zhang,Core Francisco Park,Eleanor Walker,Cecilia Garraffo,Douglas Finkbeiner,Ritesh Gautam,Steven Wofsy*

Main category: cs.CV

TL;DR: 研究对比了传统方法与深度学习模型（UNet和SCAN）在云和云阴影检测中的表现，发现深度学习显著提升检测质量，SCAN在卫星数据上表现尤为突出。


<details>
  <summary>Details</summary>
Motivation: 云和云阴影的存在会影响高光谱遥感中甲烷等痕量气体浓度的准确反演，因此需要有效检测并排除这些干扰。

Method: 研究使用了传统方法（如迭代逻辑回归ILR和多层感知机MLP）与先进深度学习架构（UNet和SCAN）进行对比评估。

Result: 传统方法在空间一致性和边界定义上表现不佳，而深度学习模型显著提升了检测质量：UNet在保持空间结构上表现最佳，SCAN则在捕捉精细边界细节上更优，尤其在MethaneSAT数据上表现超越UNet。

Conclusion: 深度学习模型（尤其是UNet和SCAN）在云和云阴影检测中表现出色，显著提升了检测质量，为高光谱任务提供了稳健且可扩展的解决方案。

Abstract: Effective cloud and cloud shadow detection is a critical prerequisite for
accurate retrieval of concentrations of atmospheric methane or other trace
gases in hyperspectral remote sensing. This challenge is especially pertinent
for MethaneSAT and for its airborne companion mission, MethaneAIR. In this
study, we use machine learning methods to address the cloud and cloud shadow
detection problem for sensors with these high spatial resolutions instruments.
Cloud and cloud shadows in remote sensing data need to be effectively screened
out as they bias methane retrievals in remote sensing imagery and impact the
quantification of emissions. We deploy and evaluate conventional techniques
including Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP),
with advanced deep learning architectures, namely UNet and a Spectral Channel
Attention Network (SCAN) method. Our results show that conventional methods
struggle with spatial coherence and boundary definition, affecting the
detection of clouds and cloud shadows. Deep learning models substantially
improve detection quality: UNet performs best in preserving spatial structure,
while SCAN excels at capturing fine boundary details. Notably, SCAN surpasses
UNet on MethaneSAT data, underscoring the benefits of incorporating spectral
attention for satellite specific features. This in depth assessment of various
disparate machine learning techniques demonstrates the strengths and
effectiveness of advanced deep learning architectures in providing robust,
scalable solutions for clouds and cloud shadow screening towards enhancing
methane emission quantification capacity of existing and next generation
hyperspectral missions. Our data and code is publicly available at
https://doi.org/10.7910/DVN/IKLZOJ

</details>


### [12] [Enhancing Transformer-Based Vision Models: Addressing Feature Map Anomalies Through Novel Optimization Strategies](https://arxiv.org/abs/2509.19687)
*Sumit Mamtani*

Main category: cs.CV

TL;DR: 提出两种轻量级优化技术（STA和ANF）以改善ViT特征图中的结构化噪声，提升视觉质量和任务性能。


<details>
  <summary>Details</summary>
Motivation: ViT在计算机视觉任务中表现优异，但特征图中的结构化噪声阻碍了下游应用（如分割和深度估计）。

Method: 1. STA通过空间扰动增强token多样性；2. ANF在Transformer层间应用可学习的在线去噪。

Result: 在ImageNet、Ade20k和NYUv2等标准基准测试中，视觉质量和任务性能均得到一致提升。

Conclusion: STA和ANF是架构无关的优化技术，能有效改善ViT的视觉质量和下游任务表现。

Abstract: Vision Transformers (ViTs) have demonstrated superior performance across a
wide range of computer vision tasks. However, structured noise artifacts in
their feature maps hinder downstream applications such as segmentation and
depth estimation. We propose two novel and lightweight optimisation techniques-
Structured Token Augmentation (STA) and Adaptive Noise Filtering (ANF)- to
improve interpretability and mitigate these artefacts. STA enhances token
diversity through spatial perturbations during tokenisation, while ANF applies
learnable inline denoising between transformer layers. These methods are
architecture-agnostic and evaluated across standard benchmarks, including
ImageNet, Ade20k, and NYUv2. Experimental results show consistent improvements
in visual quality and task performance, highlighting the practical
effectiveness of our approach.

</details>


### [13] [From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition](https://arxiv.org/abs/2509.19690)
*Ling Lo,Kelvin C. K. Chan,Wen-Huang Cheng,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种通过在去噪过程中引入帧级引导的方法，实现了平滑且一致的属性过渡，优于现有基线，并发布了CAT-Bench用于评估。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理复杂时间变化（尤其是生成具有渐变属性过渡的视频）时表现不佳，常见的提示插值方法在渐变属性过渡中往往导致不一致性加剧。

Method: 通过为每个噪声潜在空间构建数据特定的过渡方向，逐帧引导从初始属性到最终属性的渐变，同时保留视频的运动动态。

Result: 实验结果表明，该方法在视觉保真度、文本提示对齐和过渡无缝性方面优于现有基线。

Conclusion: 提出的方法通过引入帧级引导，在去噪过程中实现了平滑且一致的属性过渡，显著优于现有基线，并在视觉保真度、文本提示对齐和过渡无缝性方面表现出色。

Abstract: Existing models often struggle with complex temporal changes, particularly
when generating videos with gradual attribute transitions. The most common
prompt interpolation approach for motion transitions often fails to handle
gradual attribute transitions, where inconsistencies tend to become more
pronounced. In this work, we propose a simple yet effective method to extend
existing models for smooth and consistent attribute transitions, through
introducing frame-wise guidance during the denoising process. Our approach
constructs a data-specific transitional direction for each noisy latent,
guiding the gradual shift from initial to final attributes frame by frame while
preserving the motion dynamics of the video. Moreover, we present the
Controlled-Attribute-Transition Benchmark (CAT-Bench), which integrates both
attribute and motion dynamics, to comprehensively evaluate the performance of
different models. We further propose two metrics to assess the accuracy and
smoothness of attribute transitions. Experimental results demonstrate that our
approach performs favorably against existing baselines, achieving visual
fidelity, maintaining alignment with text prompts, and delivering seamless
attribute transitions. Code and CATBench are released:
https://github.com/lynn-ling-lo/Prompt2Progression.

</details>


### [14] [Anatomically Constrained Transformers for Cardiac Amyloidosis Classification](https://arxiv.org/abs/2509.19691)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Roberto Lang,Jeremy Slivnick,Jamie O'Driscoll,Rajan Sharma,Dipak Kotecha,Jinming Duan,Alberto Gomez*

Main category: cs.CV

TL;DR: 该研究提出了一种受解剖约束的Transformer模型，专注于心肌区域，用于心脏淀粉样变性分类，实现了比传统方法更高的性能，并提供了分类仅关注解剖区域的保证。


<details>
  <summary>Details</summary>
Motivation: 心脏淀粉样变性（CA）是一种罕见的心肌病，传统方法使用神经网络处理整个视频片段，但无法保证分类基于与CA相关的临床相关特征。因此，研究旨在通过限制模型到已知CA异常发生的解剖区域（心肌），确保分类与临床相关特征相关。

Method: 研究采用了一种受约束的Transformer模型，专注于心肌区域，通过将心肌嵌入为一组变形点和相应的采样图像块作为输入标记，并结合自监督学习的掩码自动编码器预训练，仅对解剖块进行掩码和重建。

Result: 研究结果表明，通过将模型和预训练任务限制在心肌区域，可以在CA分类任务中实现更高的性能，并能够可视化Transformer在变形心肌上的注意力得分。

Conclusion: 通过将Transformer模型和预训练任务限制在心肌区域，该研究在CA分类任务中实现了比完整视频Transformer更高的性能，并提供了分类仅关注心脏解剖区域的明确保证。

Abstract: Cardiac amyloidosis (CA) is a rare cardiomyopathy, with typical abnormalities
in clinical measurements from echocardiograms such as reduced global
longitudinal strain of the myocardium. An alternative approach for detecting CA
is via neural networks, using video classification models such as convolutional
neural networks. These models process entire video clips, but provide no
assurance that classification is based on clinically relevant features known to
be associated with CA. An alternative paradigm for disease classification is to
apply models to quantitative features such as strain, ensuring that the
classification relates to clinically relevant features. Drawing inspiration
from this approach, we explicitly constrain a transformer model to the
anatomical region where many known CA abnormalities occur -- the myocardium,
which we embed as a set of deforming points and corresponding sampled image
patches into input tokens. We show that our anatomical constraint can also be
applied to the popular self-supervised learning masked autoencoder
pre-training, where we propose to mask and reconstruct only anatomical patches.
We show that by constraining both the transformer and pre-training task to the
myocardium where CA imaging features are localized, we achieve increased
performance on a CA classification task compared to full video transformers.
Our model provides an explicit guarantee that the classification is focused on
only anatomical regions of the echo, and enables us to visualize transformer
attention scores over the deforming myocardium.

</details>


### [15] [Learning to Stop: Reinforcement Learning for Efficient Patient-Level Echocardiographic Classification](https://arxiv.org/abs/2509.19694)
*Woo-Jin Cho Kim,Jorge Oliveira,Arian Beqiri,Alex Thorley,Jordan Strom,Jamie O'Driscoll,Rajan Sharma,Jeremy Slivnick,Roberto Lang,Alberto Gomez,Agisilaos Chartsias*

Main category: cs.CV

TL;DR: 通过强化学习优化选择最优超声心动图片段子集，结合注意力聚合方法，显著提高疾病分类性能，同时减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化方法通常仅使用一个视频片段或平均所有片段的预测结果，前者忽略了其他片段的互补信息，后者计算成本高且可能阻碍临床采用。因此，需要一种更高效的方法来选择最优片段子集。

Method: 研究采用强化学习训练一个代理，该代理学会根据疾病分类的不确定性决定是否继续处理特定视图的视频片段，或当分类置信度足够时停止处理。此外，还提出了一种基于注意力的可学习聚合方法，用于灵活融合多个片段的信息。

Result: 在检测心脏淀粉样变性的任务中，该方法仅使用30%的视频片段就达到了0.91的AUC，超过了使用所有片段和其他基准方法的性能。

Conclusion: 该研究提出了一种通过强化学习优化的方法，能够选择最优的超声心动图视频片段子集，以提高特定任务（如基于图像的疾病分类）的性能。该方法不仅超过了使用所有片段的性能，还在临床应用中更具可行性。

Abstract: Guidelines for transthoracic echocardiographic examination recommend the
acquisition of multiple video clips from different views of the heart,
resulting in a large number of clips. Typically, automated methods, for
instance disease classifiers, either use one clip or average predictions from
all clips. Relying on one clip ignores complementary information available from
other clips, while using all clips is computationally expensive and may be
prohibitive for clinical adoption.
  To select the optimal subset of clips that maximize performance for a
specific task (image-based disease classification), we propose a method
optimized through reinforcement learning. In our method, an agent learns to
either keep processing view-specific clips to reduce the disease classification
uncertainty, or stop processing if the achieved classification confidence is
sufficient. Furthermore, we propose a learnable attention-based aggregation
method as a flexible way of fusing information from multiple clips. The
proposed method obtains an AUC of 0.91 on the task of detecting cardiac
amyloidosis using only 30% of all clips, exceeding the performance achieved
from using all clips and from other benchmarks.

</details>


### [16] [Towards Robust In-Context Learning for Medical Image Segmentation via Data Synthesis](https://arxiv.org/abs/2509.19711)
*Jiesi Hu,Yanwu Yang,Zhiyu Ye,Chenfei Ye,Hanyang Peng,Jianfeng Cao,Ting Ma*

Main category: cs.CV

TL;DR: SynthICL是一个新颖的数据合成框架，通过领域随机化和真实数据解剖先验，解决了ICL医学图像分割中的数据稀缺问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决ICL在医学图像分割中因数据稀缺和多样性不足而面临的挑战。

Method: 提出了SynthICL框架，基于领域随机化，利用真实数据集的解剖学先验生成多样化的解剖结构，并明确建模主体间变异以生成适合ICL的数据队列。

Result: 在四个测试数据集上验证了框架的有效性，模型性能提升高达63%（平均Dice），并显著增强了对未见解剖领域的泛化能力。

Conclusion: SynthICL通过结合真实世界数据集的解剖学先验，有效缓解了ICL在医学图像分割中的数据稀缺问题，显著提升了模型的性能和泛化能力。

Abstract: The rise of In-Context Learning (ICL) for universal medical image
segmentation has introduced an unprecedented demand for large-scale, diverse
datasets for training, exacerbating the long-standing problem of data scarcity.
While data synthesis offers a promising solution, existing methods often fail
to simultaneously achieve both high data diversity and a domain distribution
suitable for medical data. To bridge this gap, we propose \textbf{SynthICL}, a
novel data synthesis framework built upon domain randomization. SynthICL
ensures realism by leveraging anatomical priors from real-world datasets,
generates diverse anatomical structures to cover a broad data distribution, and
explicitly models inter-subject variations to create data cohorts suitable for
ICL. Extensive experiments on four held-out datasets validate our framework's
effectiveness, showing that models trained with our data achieve performance
gains of up to 63\% in average Dice and substantially enhanced generalization
to unseen anatomical domains. Our work helps mitigate the data bottleneck for
ICL-based segmentation, paving the way for robust models. Our code and the
generated dataset are publicly available at
https://github.com/jiesihu/Neuroverse3D.

</details>


### [17] [VIMD: Monocular Visual-Inertial Motion and Depth Estimation](https://arxiv.org/abs/2509.19713)
*Saimouli Katragadda,Guoquan Huang*

Main category: cs.CV

TL;DR: VIMD框架通过多视图迭代优化每像素尺度，实现了高精度密集深度估计，兼容多种主干网络，在稀疏点情况下仍表现优异，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 精确且高效的密集度量深度估计对于机器人和XR中的3D视觉感知至关重要。现有方法在全局拟合不变仿射模型时存在局限性，需要一种更灵活且高效的解决方案。

Method: 提出了一种单目视觉-惯性运动与深度（VIMD）学习框架，通过多视图信息迭代优化每像素尺度，而非全局拟合不变仿射模型。该框架高度模块化，兼容多种现有深度估计主干网络。

Result: 在TartanAir和VOID数据集上的广泛评估显示，VIMD在稀疏点（每图像仅10-20个度量深度点）情况下仍能实现卓越的准确性和鲁棒性，并在AR Table数据集上展示了零样本泛化能力。

Conclusion: VIMD框架通过利用多视图信息迭代优化每像素尺度，实现了高精度和鲁棒性的密集度量深度估计，适用于资源受限的环境，并展现出强大的泛化能力。

Abstract: Accurate and efficient dense metric depth estimation is crucial for 3D visual
perception in robotics and XR. In this paper, we develop a monocular
visual-inertial motion and depth (VIMD) learning framework to estimate dense
metric depth by leveraging accurate and efficient MSCKF-based monocular
visual-inertial motion tracking. At the core the proposed VIMD is to exploit
multi-view information to iteratively refine per-pixel scale, instead of
globally fitting an invariant affine model as in the prior work. The VIMD
framework is highly modular, making it compatible with a variety of existing
depth estimation backbones. We conduct extensive evaluations on the TartanAir
and VOID datasets and demonstrate its zero-shot generalization capabilities on
the AR Table dataset. Our results show that VIMD achieves exceptional accuracy
and robustness, even with extremely sparse points as few as 10-20 metric depth
points per image. This makes the proposed VIMD a practical solution for
deployment in resource constrained settings, while its robust performance and
strong generalization capabilities offer significant potential across a wide
range of scenarios.

</details>


### [18] [Frequency-domain Multi-modal Fusion for Language-guided Medical Image Segmentation](https://arxiv.org/abs/2509.19719)
*Bo Yu,Jianhua Yang,Zetao Du,Yan Huang,Chenglong Li,Liang Wang*

Main category: cs.CV

TL;DR: FMISeg模型通过频域多模态交互提升语言引导的医学图像分割性能，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效增强视觉特征表示并消除语义无关信息，导致分割性能不佳。FMISeg旨在解决这一问题。

Method: 提出了一种频域多模态交互模型（FMISeg），包含频域特征双向交互模块（FFBI）和语言引导的频域特征交互模块（LFFI），用于在解码器中建立语言特征与频域视觉特征的交互。

Result: 在QaTa-COV19和MosMedData+数据集上的实验表明，FMISeg在定性和定量上均优于现有方法。

Conclusion: FMISeg模型通过频域多模态交互有效提升了语言引导的医学图像分割性能，实验证明其在QaTa-COV19和MosMedData+数据集上优于现有方法。

Abstract: Automatically segmenting infected areas in radiological images is essential
for diagnosing pulmonary infectious diseases. Recent studies have demonstrated
that the accuracy of the medical image segmentation can be improved by
incorporating clinical text reports as semantic guidance. However, the complex
morphological changes of lesions and the inherent semantic gap between
vision-language modalities prevent existing methods from effectively enhancing
the representation of visual features and eliminating semantically irrelevant
information, ultimately resulting in suboptimal segmentation performance. To
address these problems, we propose a Frequency-domain Multi-modal Interaction
model (FMISeg) for language-guided medical image segmentation. FMISeg is a late
fusion model that establishes interaction between linguistic features and
frequency-domain visual features in the decoder. Specifically, to enhance the
visual representation, our method introduces a Frequency-domain Feature
Bidirectional Interaction (FFBI) module to effectively fuse frequency-domain
features. Furthermore, a Language-guided Frequency-domain Feature Interaction
(LFFI) module is incorporated within the decoder to suppress semantically
irrelevant visual features under the guidance of linguistic information.
Experiments on QaTa-COV19 and MosMedData+ demonstrated that our method
outperforms the state-of-the-art methods qualitatively and quantitatively.

</details>


### [19] [PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface Reconstruction](https://arxiv.org/abs/2509.19726)
*Yufei Han,Bowen Tie,Heng Guo,Youwei Lyu,Si Li,Boxin Shi,Yunpeng Jia,Zhanyu Ma*

Main category: cs.CV

TL;DR: PolGS模型通过偏振约束提升3DGS对复杂反射表面的重建质量，实验验证其高效性。


<details>
  <summary>Details</summary>
Motivation: 针对复杂反射特性表面的高效重建需求，尤其是在实时虚拟现实中，3DGS方法在重建质量上落后于隐式神经表示。

Method: 提出PolGS模型，一种偏振高斯泼溅模型，通过集成偏振约束到3DGS框架中，快速分离镜面和漫反射成分。

Result: 在合成和真实数据集上的实验验证了PolGS方法的有效性，能够在10分钟内快速完成反射表面重建。

Conclusion: PolGS通过将偏振约束集成到3DGS框架中，有效提升了具有挑战性反射材料的重建质量。

Abstract: Efficient shape reconstruction for surfaces with complex reflectance
properties is crucial for real-time virtual reality. While 3D Gaussian
Splatting (3DGS)-based methods offer fast novel view rendering by leveraging
their explicit surface representation, their reconstruction quality lags behind
that of implicit neural representations, particularly in the case of recovering
surfaces with complex reflective reflectance. To address these problems, we
propose PolGS, a Polarimetric Gaussian Splatting model allowing fast reflective
surface reconstruction in 10 minutes. By integrating polarimetric constraints
into the 3DGS framework, PolGS effectively separates specular and diffuse
components, enhancing reconstruction quality for challenging reflective
materials. Experimental results on the synthetic and real-world dataset
validate the effectiveness of our method.

</details>


### [20] [CAMILA: Context-Aware Masking for Image Editing with Language Alignment](https://arxiv.org/abs/2509.19731)
*Hyunseung Kim,Chiho Choi,Srikanth Malla,Sai Prahladh Padmanabhan,Saurabh Bagchi,Joon Hee Choi*

Main category: cs.CV

TL;DR: CAMILA是一种上下文感知的图像编辑方法，通过验证指令与图像的连贯性，避免无效编辑，表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型通常盲目遵循所有用户指令，即使指令本身不可行或矛盾，导致输出结果无意义。CAMILA旨在解决这一问题，确保编辑的上下文连贯性。

Method: 提出了一种名为CAMILA（Context-Aware Masking for Image Editing with Language Alignment）的上下文感知图像编辑方法，通过验证指令与图像的上下文一致性，确保仅对指定区域应用相关编辑，并忽略不可执行指令。

Result: CAMILA在单指令和多指令图像编辑数据集上表现优于现有模型，尤其在处理不可行请求时表现出更高的语义对齐。

Conclusion: CAMILA方法在保持图像完整性的同时，有效处理了复杂指令挑战，表现出比现有模型更好的性能和更高的语义对齐。

Abstract: Text-guided image editing has been allowing users to transform and synthesize
images through natural language instructions, offering considerable
flexibility. However, most existing image editing models naively attempt to
follow all user instructions, even if those instructions are inherently
infeasible or contradictory, often resulting in nonsensical output. To address
these challenges, we propose a context-aware method for image editing named as
CAMILA (Context-Aware Masking for Image Editing with Language Alignment).
CAMILA is designed to validate the contextual coherence between instructions
and the image, ensuring that only relevant edits are applied to the designated
regions while ignoring non-executable instructions. For comprehensive
evaluation of this new method, we constructed datasets for both single- and
multi-instruction image editing, incorporating the presence of infeasible
requests. Our method achieves better performance and higher semantic alignment
than state-of-the-art models, demonstrating its effectiveness in handling
complex instruction challenges while preserving image integrity.

</details>


### [21] [Robust RGB-T Tracking via Learnable Visual Fourier Prompt Fine-tuning and Modality Fusion Prompt Generation](https://arxiv.org/abs/2509.19733)
*Hongtao Yang,Bineng Zhong,Qihua Liang,Zhiruo Zhu,Yaozong Zheng,Ning Li*

Main category: cs.CV

TL;DR: VFPTrack通过结合空间域和频域信息，提出了一种高效的视觉傅里叶提示跟踪方法，显著提升了RGB-T跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法仅依赖空间域信息，忽略了频域信息在提示学习中的关键作用，导致性能不佳。

Method: 采用对称特征提取编码器、视觉傅里叶提示和模态融合提示生成器，结合FFT提取频域信息，生成双向交互提示。

Result: 在三个流行的RGB-T跟踪基准测试中，VFPTrack表现出卓越性能。

Conclusion: VFPTrack方法通过结合空间域和频域信息，显著提升了RGB-T跟踪的性能，并在多个基准测试中表现出色。

Abstract: Recently, visual prompt tuning is introduced to RGB-Thermal (RGB-T) tracking
as a parameter-efficient finetuning (PEFT) method. However, these PEFT-based
RGB-T tracking methods typically rely solely on spatial domain information as
prompts for feature extraction. As a result, they often fail to achieve optimal
performance by overlooking the crucial role of frequency-domain information in
prompt learning. To address this issue, we propose an efficient Visual Fourier
Prompt Tracking (named VFPTrack) method to learn modality-related prompts via
Fast Fourier Transform (FFT). Our method consists of symmetric feature
extraction encoder with shared parameters, visual fourier prompts, and Modality
Fusion Prompt Generator that generates bidirectional interaction prompts
through multi-modal feature fusion. Specifically, we first use a frozen feature
extraction encoder to extract RGB and thermal infrared (TIR) modality features.
Then, we combine the visual prompts in the spatial domain with the frequency
domain prompts obtained from the FFT, which allows for the full extraction and
understanding of modality features from different domain information. Finally,
unlike previous fusion methods, the modality fusion prompt generation module we
use combines features from different modalities to generate a fused modality
prompt. This modality prompt is interacted with each individual modality to
fully enable feature interaction across different modalities. Extensive
experiments conducted on three popular RGB-T tracking benchmarks show that our
method demonstrates outstanding performance.

</details>


### [22] [Rectified Decoupled Dataset Distillation: A Closer Look for Fair and Comprehensive Evaluation](https://arxiv.org/abs/2509.19743)
*Xinhao Zhong,Shuoyang Sun,Xulin Gu,Chenyang Zhu,Bin Chen,Yaowei Wang*

Main category: cs.CV

TL;DR: RD$^3$ 提出标准化评估协议，揭示性能差异源于评估不一致，为数据集蒸馏研究提供公平比较基础。


<details>
  <summary>Details</summary>
Motivation: 解决现有解耦数据集蒸馏方法中不一致的后评估协议问题，推动领域的进步。

Method: 提出了 Rectified Decoupled Dataset Distillation (RD$^3$)，系统研究了不同后评估设置对测试准确性的影响，并分析了现有方法性能差异的来源。

Result: 分析表明，性能差异主要源于评估不一致而非合成数据的固有质量差异，并确定了提高蒸馏数据集有效性的通用策略。

Conclusion: RD$^3$ 通过建立标准化基准和严格的评估协议，为数据集蒸馏研究提供了公平和可重复比较的基础。

Abstract: Dataset distillation aims to generate compact synthetic datasets that enable
models trained on them to achieve performance comparable to those trained on
full real datasets, while substantially reducing storage and computational
costs. Early bi-level optimization methods (e.g., MTT) have shown promising
results on small-scale datasets, but their scalability is limited by high
computational overhead. To address this limitation, recent decoupled dataset
distillation methods (e.g., SRe$^2$L) separate the teacher model pre-training
from the synthetic data generation process. These methods also introduce random
data augmentation and epoch-wise soft labels during the post-evaluation phase
to improve performance and generalization. However, existing decoupled
distillation methods suffer from inconsistent post-evaluation protocols, which
hinders progress in the field. In this work, we propose Rectified Decoupled
Dataset Distillation (RD$^3$), and systematically investigate how different
post-evaluation settings affect test accuracy. We further examine whether the
reported performance differences across existing methods reflect true
methodological advances or stem from discrepancies in evaluation procedures.
Our analysis reveals that much of the performance variation can be attributed
to inconsistent evaluation rather than differences in the intrinsic quality of
the synthetic data. In addition, we identify general strategies that improve
the effectiveness of distilled datasets across settings. By establishing a
standardized benchmark and rigorous evaluation protocol, RD$^3$ provides a
foundation for fair and reproducible comparisons in future dataset distillation
research.

</details>


### [23] [nnFilterMatch: A Unified Semi-Supervised Learning Framework with Uncertainty-Aware Pseudo-Label Filtering for Efficient Medical Segmentation](https://arxiv.org/abs/2509.19746)
*Yi Yang*

Main category: cs.CV

TL;DR: 提出nnFilterMatch框架，结合SSL与AL，无需重新训练循环，仅需少量标注数据即可实现高性能医学图像分割。


<details>
  <summary>Details</summary>
Motivation: 传统SSL与AL混合方法依赖迭代和循环的重新训练，计算开销大且临床应用中扩展性受限。本研究旨在减少注释需求，同时保持分割准确性。

Method: 该方法在半监督学习（SSL）中引入了基于熵的伪标签过滤（FilterMatch）和主动学习（AL）机制，整合到单次训练的nnU-Net框架（nnFilterMatch）中，选择性排除高置信度伪标签以避免重新训练。

Result: 在多个临床分割基准测试中，该方法仅使用5%-20%的标注数据即可达到或超过全监督模型的性能。

Conclusion: 该研究提出了一种新颖的、注释高效的、自适应的深度学习分割框架，结合了半监督学习和基于熵的伪标签过滤，无需重新训练循环即可实现与全监督模型相当或更好的性能。

Abstract: Semi-supervised learning (SSL) has emerged as a promising paradigm in medical
image segmentation, offering competitive performance while substantially
reducing the need for extensive manual annotation. When combined with active
learning (AL), these strategies further minimize annotation burden by
selectively incorporating the most informative samples. However, conventional
SSL_AL hybrid approaches often rely on iterative and loop-based retraining
cycles after each annotation round, incurring significant computational
overhead and limiting scalability in clinical applications. In this study, we
present a novel, annotation-efficient, and self-adaptive deep segmentation
framework that integrates SSL with entropy-based pseudo-label filtering
(FilterMatch), an AL-inspired mechanism, within the single-pass nnU-Net
training segmentation framework (nnFilterMatch). By selectively excluding
high-confidence pseudo-labels during training, our method circumvents the need
for retraining loops while preserving the benefits of uncertainty-guided
learning. We validate the proposed framework across multiple clinical
segmentation benchmarks and demonstrate that it achieves performance comparable
to or exceeding fully supervised models, even with only 5\%--20\% labeled data.
This work introduces a scalable, end-to-end learning strategy for reducing
annotation demands in medical image segmentation without compromising accuracy.
Code is available here: https://github.com/Ordi117/nnFilterMatch.git.

</details>


### [24] [Talking Head Generation via AU-Guided Landmark Prediction](https://arxiv.org/abs/2509.19749)
*Shao-Yu Chang,Jingyi Xu,Hieu Le,Dimitris Samaras*

Main category: cs.CV

TL;DR: 两阶段框架通过显式AU到标点映射实现精细表情控制，扩散合成器生成逼真视频，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖情感标签或隐式AU条件，缺乏物理基础的表情控制。本文旨在通过显式AU到标点的映射，实现更精确、稳定的表情控制。

Method: 提出了一种两阶段框架：第一阶段使用变分运动生成器从音频和AU强度预测时间连贯的标点序列；第二阶段基于扩散的合成器根据这些标点和参考图像生成逼真的唇同步视频。

Result: 在MEAD数据集上的实验表明，该方法在多个指标上优于现有基线，验证了显式AU到标点建模的有效性。

Conclusion: 论文提出的两阶段框架通过显式地将面部动作单元（AUs）映射到2D面部标点，实现了精细的表情控制，并在MEAD数据集上验证了其优于现有方法的性能。

Abstract: We propose a two-stage framework for audio-driven talking head generation
with fine-grained expression control via facial Action Units (AUs). Unlike
prior methods relying on emotion labels or implicit AU conditioning, our model
explicitly maps AUs to 2D facial landmarks, enabling physically grounded,
per-frame expression control. In the first stage, a variational motion
generator predicts temporally coherent landmark sequences from audio and AU
intensities. In the second stage, a diffusion-based synthesizer generates
realistic, lip-synced videos conditioned on these landmarks and a reference
image. This separation of motion and appearance improves expression accuracy,
temporal stability, and visual realism. Experiments on the MEAD dataset show
that our method outperforms state-of-the-art baselines across multiple metrics,
demonstrating the effectiveness of explicit AU-to-landmark modeling for
expressive talking head generation.

</details>


### [25] [ExpFace: Exponential Angular Margin Loss for Deep Face Recognition](https://arxiv.org/abs/2509.19753)
*Jinhui Zheng,Xueyuan Gong*

Main category: cs.CV

TL;DR: ExpFace是一种新的损失函数，通过角度空间中的指数项边际，有效区分干净和噪声样本，提升了人脸识别性能。


<details>
  <summary>Details</summary>
Motivation: 观察到干净样本主要集中在中心区域，而噪声样本倾向于向边缘区域移动，因此设计了ExpFace以强调干净样本并抑制噪声样本。

Method: 提出了Exponential Angular Margin Loss (ExpFace)，在角度空间中引入角指数项作为边际，对中心区域施加较大惩罚，对边缘区域施加较小惩罚。

Result: 实验表明，ExpFace不仅避免了SphereFace的训练不稳定性和ArcFace的非单调性，还在性能上达到了最先进水平。

Conclusion: ExpFace通过引入角指数项作为边际，有效强调了干净样本并抑制了噪声样本，实验证明其性能达到了最先进水平。

Abstract: Face recognition is an open-set problem requiring high discriminative power
to ensure that intra-class distances remain smaller than inter-class distances.
Margin-based softmax losses, such as SphereFace, CosFace, and ArcFace, have
been widely adopted to enhance intra-class compactness and inter-class
separability, yet they overlook the impact of noisy samples. By examining the
distribution of samples in the angular space, we observe that clean samples
predominantly cluster in the center region, whereas noisy samples tend to shift
toward the peripheral region. Motivated by this observation, we propose the
Exponential Angular Margin Loss (ExpFace), which introduces an angular
exponential term as the margin. This design applies a larger penalty in the
center region and a smaller penalty in the peripheral region within the angular
space, thereby emphasizing clean samples while suppressing noisy samples. We
present a unified analysis of ExpFace and classical margin-based softmax losses
in terms of margin embedding forms, similarity curves, and gradient curves,
showing that ExpFace not only avoids the training instability of SphereFace and
the non-monotonicity of ArcFace, but also exhibits a similarity curve that
applies penalties in the same manner as the decision boundary in the angular
space. Extensive experiments demonstrate that ExpFace achieves state-of-the-art
performance. To facilitate future research, we have released the source code
at: https://github.com/dfr-code/ExpFace.

</details>


### [26] [Logics-Parsing Technical Report](https://arxiv.org/abs/2509.19760)
*Xiangyang Chen,Shuzhao Li,Xiuwen Zhu,Yongfan Chen,Fan Yang,Cheng Fang,Lin Qu,Xiaoxiao Xu,Hu Wei,Minggang Wu*

Main category: cs.CV

TL;DR: 提出Logics-Parsing模型，结合强化学习优化文档布局分析，引入多样化数据扩展模型能力，并在新基准上验证其SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统端到端方法在复杂文档类型（如多栏报纸或海报）处理上因缺乏显式布局分析和阅读顺序推断而受限。

Method: 提出Logics-Parsing模型，结合强化学习的奖励机制优化复杂布局分析和阅读顺序推断，并在监督微调中引入多样化数据类型。

Result: 模型在LogicsParsingBench上验证了其高效性和SOTA性能。

Conclusion: Logics-Parsing模型在多样化的文档分析场景中展现了高效性和最先进的性能，并通过LogicsParsingBench进行了全面验证。

Abstract: Recent advances in Large Vision-Language models (LVLM) have spurred
significant progress in document parsing task. Compared to traditional
pipeline-based methods, end-to-end paradigms have shown their excellence in
converting PDF images into structured outputs through integrated Optical
Character Recognition (OCR), table recognition, mathematical formula
recognition and so on. However, the absence of explicit analytical stages for
document layouts and reading orders limits the LVLM's capability in handling
complex document types such as multi-column newspapers or posters. To address
this limitation, we propose in this report Logics-Parsing: an end-to-end
LVLM-based model augmented with reinforcement learning. Our model incorporates
meticulously designed reward mechanisms to optimize complex layout analysis and
reading order inference. In addition, we expand the model's versatility by
incorporating diverse data types such as chemical formulas and handwritten
Chinese characters into supervised fine-tuning. Finally, to enable rigorous
evaluation of our approach, we introduce LogicsParsingBench, a curated set of
1,078 page-level PDF images spanning nine major categories and over twenty
sub-categories, which will be released later. Comprehensive experiments
conducted on LogicsParsingBench have validated the efficacy and
State-of-the-art (SOTA) performance of our proposed model across diverse
document analysis scenarios. Project Page:
https://github.com/alibaba/Logics-Parsing

</details>


### [27] [Sex-based Bias Inherent in the Dice Similarity Coefficient: A Model Independent Analysis for Multiple Anatomical Structures](https://arxiv.org/abs/2509.19778)
*Hartmut Häntze,Myrthe Buser,Alessa Hering,Lisa C. Adams,Keno K. Bressem*

Main category: cs.CV

TL;DR: 研究发现DSC指标本身存在性别偏差，小型结构差异显著，提醒公平性评估需注意此因素。


<details>
  <summary>Details</summary>
Motivation: 探讨DSC指标本身可能引入的性别偏差，此前未有研究关注此问题。

Method: 通过向50名参与者的手动MRI注释应用相同大小的合成错误，确保性别间的可比性。

Result: 即使最小错误（如1 mm边界偏移）也会导致DSC的系统性性别差异，小型结构差异约0.03，中型约0.01，大型结构（如肺和肝）差异接近零。

Conclusion: 研究指出DSC指标本身存在性别偏差，提醒在医学图像分析中使用DSC进行公平性评估时需注意这一因素。

Abstract: Overlap-based metrics such as the Dice Similarity Coefficient (DSC) penalize
segmentation errors more heavily in smaller structures. As organ size differs
by sex, this implies that a segmentation error of equal magnitude may result in
lower DSCs in women due to their smaller average organ volumes compared to men.
While previous work has examined sex-based differences in models or datasets,
no study has yet investigated the potential bias introduced by the DSC itself.
This study quantifies sex-based differences of the DSC and the normalized DSC
in an idealized setting independent of specific models. We applied
equally-sized synthetic errors to manual MRI annotations from 50 participants
to ensure sex-based comparability. Even minimal errors (e.g., a 1 mm boundary
shift) produced systematic DSC differences between sexes. For small structures,
average DSC differences were around 0.03; for medium-sized structures around
0.01. Only large structures (i.e., lungs and liver) were mostly unaffected,
with sex-based DSC differences close to zero. These findings underline that
fairness studies using the DSC as an evaluation metric should not expect
identical scores between men and women, as the metric itself introduces bias. A
segmentation model may perform equally well across sexes in terms of error
magnitude, even if observed DSC values suggest otherwise. Importantly, our work
raises awareness of a previously underexplored source of sex-based differences
in segmentation performance. One that arises not from model behavior, but from
the metric itself. Recognizing this factor is essential for more accurate and
fair evaluations in medical image analysis.

</details>


### [28] [EfficienT-HDR: An Efficient Transformer-Based Framework via Multi-Exposure Fusion for HDR Reconstruction](https://arxiv.org/abs/2509.19779)
*Yu-Shen Huang,Tzu-Han Chen,Cheng-Yen Hsiao,Shaou-Gang Miaou*

Main category: cs.CV

TL;DR: 该研究提出了一种轻量级Vision Transformer架构，用于高效且无伪影的HDR重建，通过多种技术创新在性能和计算效率之间取得了平衡，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上实现高质量的高动态范围(HDR)成像是计算机视觉中的关键挑战，其性能直接影响智能监控和自动驾驶等下游任务。多曝光融合(MEF)是实现这一目标的主流技术，但现有方法普遍面临高计算成本和伪影问题的双重瓶颈，阻碍了其广泛应用。

Method: 本研究基于Context-Aware Vision Transformer，首先将输入图像转换为YCbCr颜色空间以分离亮度和色度信息。随后采用Intersection-Aware Adaptive Fusion (IAAF)模块有效抑制伪影。为实现轻量化设计，引入了Inverted Residual Embedding (IRE)、Dynamic Tanh (DyT)和Enhanced Multi-Scale Dilated Convolution (E-MSDC)，从多个层面降低计算复杂度。

Result: 实验结果表明，与基线相比，主版本在CPU上的FLOPS减少了约67%，推理速度提高了五倍以上，在边缘设备上提高了2.5倍。这些结果证实了我们的方法为边缘设备提供了一种高效且无伪影的HDR成像解决方案。

Conclusion: 本研究提出了两种模型版本，主要版本追求高视觉质量，轻量级版本则在计算效率上占优，两者均在性能和图像质量之间取得了良好平衡。实验结果表明，与基线相比，主版本在CPU上的FLOPS减少了约67%，推理速度提高了五倍以上，在边缘设备上提高了2.5倍。这些结果证实了我们的方法为边缘设备提供了一种高效且无伪影的HDR成像解决方案，展示了其在各种动态场景中的多功能性和实用性。

Abstract: Achieving high-quality High Dynamic Range (HDR) imaging on
resource-constrained edge devices is a critical challenge in computer vision,
as its performance directly impacts downstream tasks such as intelligent
surveillance and autonomous driving. Multi-Exposure Fusion (MEF) is a
mainstream technique to achieve this goal; however, existing methods generally
face the dual bottlenecks of high computational costs and ghosting artifacts,
hindering their widespread deployment. To this end, this study proposes a
light-weight Vision Transformer architecture designed explicitly for HDR
reconstruction to overcome these limitations. This study is based on the
Context-Aware Vision Transformer and begins by converting input images to the
YCbCr color space to separate luminance and chrominance information. It then
employs an Intersection-Aware Adaptive Fusion (IAAF) module to suppress
ghosting effectively. To further achieve a light-weight design, we introduce
Inverted Residual Embedding (IRE), Dynamic Tanh (DyT), and propose Enhanced
Multi-Scale Dilated Convolution (E-MSDC) to reduce computational complexity at
multiple levels. Our study ultimately contributes two model versions: a main
version for high visual quality and a light-weight version with advantages in
computational efficiency, both of which achieve an excellent balance between
performance and image quality. Experimental results demonstrate that, compared
to the baseline, the main version reduces FLOPS by approximately 67% and
increases inference speed by more than fivefold on CPU and 2.5 times on an edge
device. These results confirm that our method provides an efficient and
ghost-free HDR imaging solution for edge devices, demonstrating versatility and
practicality across various dynamic scenarios.

</details>


### [29] [BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting](https://arxiv.org/abs/2509.19793)
*Yixun Zhang,Feng Zhou,Jianqin Yin*

Main category: cs.CV

TL;DR: BiTAA是一种基于3D高斯泼溅的双任务对抗攻击，可同时破坏检测和偏差深度估计，揭示了多任务相机感知的实践风险。


<details>
  <summary>Details</summary>
Motivation: 现有2D/3D攻击多为任务孤岛，缺乏可控深度偏差机制，且无标准化协议量化跨任务转移，导致检测与深度交互未被充分探索。

Method: 基于3D高斯泼溅构建的双模型攻击框架，支持全图像和补丁设置，兼容常见检测器和深度估计器，可选EOT增强物理现实性。设计了一种复合损失函数，将检测抑制与有符号、幅度可控的深度偏差耦合在感兴趣区域内。

Result: 实验显示BiTAA能同时降低检测性能和偏差单目深度估计，并揭示了检测到深度与深度到检测转移的不对称性。

Conclusion: BiTAA攻击展示了多任务相机感知的实践风险，并激励了自动驾驶场景中跨任务感知防御的发展。

Abstract: Camera-based perception is critical to autonomous driving yet remains
vulnerable to task-specific adversarial manipulations in object detection and
monocular depth estimation. Most existing 2D/3D attacks are developed in task
silos, lack mechanisms to induce controllable depth bias, and offer no
standardized protocol to quantify cross-task transfer, leaving the interaction
between detection and depth underexplored. We present BiTAA, a bi-task
adversarial attack built on 3D Gaussian Splatting that yields a single
perturbation capable of simultaneously degrading detection and biasing
monocular depth. Specifically, we introduce a dual-model attack framework that
supports both full-image and patch settings and is compatible with common
detectors and depth estimators, with optional expectation-over-transformation
(EOT) for physical reality. In addition, we design a composite loss that
couples detection suppression with a signed, magnitude-controlled log-depth
bias within regions of interest (ROIs) enabling controllable near or far
misperception while maintaining stable optimization across tasks. We also
propose a unified evaluation protocol with cross-task transfer metrics and
real-world evaluations, showing consistent cross-task degradation and a clear
asymmetry between Det to Depth and from Depth to Det transfer. The results
highlight practical risks for multi-task camera-only perception and motivate
cross-task-aware defenses in autonomous driving scenarios.

</details>


### [30] [StrCGAN: A Generative Framework for Stellar Image Restoration](https://arxiv.org/abs/2509.19805)
*Shantanusinh Parmar*

Main category: cs.CV

TL;DR: StrCGAN是一种改进的生成模型，通过3D卷积、多光谱融合和天体物理正则化，显著提升低分辨率天体图像的清晰度和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 解决小望远镜观测（如MobilTelesco数据集）因分辨率和质量限制导致的天体图像重建挑战，传统模型（如CycleGAN）在2D映射中常扭曲恒星和星系形态。

Method: 扩展CycleGAN框架，引入3D卷积层、多光谱融合技术和天体物理正则化模块，以捕捉体积空间相关性并保持恒星形态。

Result: StrCGAN生成的图像不仅在视觉上更清晰，且在物理上更一致，超越了标准GAN模型在天体图像增强任务中的表现。

Conclusion: StrCGAN通过结合3D卷积层、多光谱融合和天体物理正则化模块，成功提升了低分辨率天体摄影图像的清晰度和物理一致性，优于传统的GAN模型。

Abstract: We introduce StrCGAN (Stellar Cyclic GAN), a generative model designed to
enhance low-resolution astrophotography images. Our goal is to reconstruct
high-fidelity ground truth-like representations of celestial objects, a task
that is challenging due to the limited resolution and quality of
small-telescope observations such as the MobilTelesco dataset. Traditional
models such as CycleGAN provide a foundation for image-to-image translation but
are restricted to 2D mappings and often distort the morphology of stars and
galaxies. To overcome these limitations, we extend the CycleGAN framework with
three key innovations: 3D convolutional layers to capture volumetric spatial
correlations, multi-spectral fusion to align optical and near-infrared (NIR)
domains, and astrophysical regularization modules to preserve stellar
morphology. Ground-truth references from multi-mission all-sky surveys spanning
optical to NIR guide the training process, ensuring that reconstructions remain
consistent across spectral bands. Together, these components allow StrCGAN to
generate reconstructions that are not only visually sharper but also physically
consistent, outperforming standard GAN models in the task of astrophysical
image enhancement.

</details>


### [31] [Adaptive Model Ensemble for Continual Learning](https://arxiv.org/abs/2509.19819)
*Yuchuan Mao,Zhi Gao,Xiaomeng Fan,Yuwei Wu,Yunde Jia,Chenchen Jing*

Main category: cs.CV

TL;DR: 提出 meta-weight-ensembler，通过元学习自适应生成混合系数，解决任务和层级知识冲突，提升持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型集成方法在任务和层级上存在知识冲突问题，导致新旧任务学习性能下降。

Method: 提出 meta-weight-ensembler，通过元学习训练的混合系数生成器为模型集成生成适当的混合系数，解决任务和层级知识冲突。

Result: 实验表明，meta-weight-ensembler 有效缓解了灾难性遗忘，并提升了性能。

Conclusion: Meta-weight-ensembler 通过自适应融合不同任务的知识，有效缓解了灾难性遗忘，并在多个持续学习数据集上实现了最先进的性能。

Abstract: Model ensemble is an effective strategy in continual learning, which
alleviates catastrophic forgetting by interpolating model parameters, achieving
knowledge fusion learned from different tasks. However, existing model ensemble
methods usually encounter the knowledge conflict issue at task and layer
levels, causing compromised learning performance in both old and new tasks. To
solve this issue, we propose meta-weight-ensembler that adaptively fuses
knowledge of different tasks for continual learning. Concretely, we employ a
mixing coefficient generator trained via meta-learning to generate appropriate
mixing coefficients for model ensemble to address the task-level knowledge
conflict. The mixing coefficient is individually generated for each layer to
address the layer-level knowledge conflict. In this way, we learn the prior
knowledge about adaptively accumulating knowledge of different tasks in a fused
model, achieving efficient learning in both old and new tasks.
Meta-weight-ensembler can be flexibly combined with existing continual learning
methods to boost their ability of alleviating catastrophic forgetting.
Experiments on multiple continual learning datasets show that
meta-weight-ensembler effectively alleviates catastrophic forgetting and
achieves state-of-the-art performance.

</details>


### [32] [ThinkFake: Reasoning in Multimodal Large Language Models for AI-Generated Image Detection](https://arxiv.org/abs/2509.19841)
*Tai-Ming Huang,Wei-Tung Lin,Kai-Lung Hua,Wen-Huang Cheng,Junichi Yamagishi,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: ThinkFake是一个基于推理的AI生成图像检测框架，通过MLLM和GRPO强化学习实现可解释的检测，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: AI生成图像的逼真度提升引发了对错误信息和隐私侵犯的担忧，亟需准确且可解释的检测方法。

Method: 利用配备伪造推理提示的多模态大型语言模型（MLLM），并通过Group Relative Policy Optimization（GRPO）强化学习进行训练。

Result: 在GenImage基准测试中优于现有方法，并在LOKI基准测试中展现出强大的零样本泛化能力。

Conclusion: ThinkFake框架在AI生成图像检测方面表现出色，验证了其有效性和鲁棒性。

Abstract: The increasing realism of AI-generated images has raised serious concerns
about misinformation and privacy violations, highlighting the urgent need for
accurate and interpretable detection methods. While existing approaches have
made progress, most rely on binary classification without explanations or
depend heavily on supervised fine-tuning, resulting in limited generalization.
In this paper, we propose ThinkFake, a novel reasoning-based and generalizable
framework for AI-generated image detection. Our method leverages a Multimodal
Large Language Model (MLLM) equipped with a forgery reasoning prompt and is
trained using Group Relative Policy Optimization (GRPO) reinforcement learning
with carefully designed reward functions. This design enables the model to
perform step-by-step reasoning and produce interpretable, structured outputs.
We further introduce a structured detection pipeline to enhance reasoning
quality and adaptability. Extensive experiments show that ThinkFake outperforms
state-of-the-art methods on the GenImage benchmark and demonstrates strong
zero-shot generalization on the challenging LOKI benchmark. These results
validate our framework's effectiveness and robustness. Code will be released
upon acceptance.

</details>


### [33] [PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents](https://arxiv.org/abs/2509.19843)
*Filippo Ziliotto,Jelin Raphael Akkara,Alessandro Daniele,Lamberto Ballan,Luciano Serafini,Tommaso Campari*

Main category: cs.CV

TL;DR: PersONAL 是一个专注于个性化对象导航的基准测试，揭示了当前模型在感知和推理个性化信息方面的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管 Embodied AI 取得了进展，但在现实人类中心场景（如家庭环境）中部署智能体仍面临挑战，尤其是建模个体人类偏好和行为的困难。

Method: 引入了 PersONAL（个性化对象导航与定位）基准，包含 2000 多个高质量场景，涵盖 30 多个真实感家庭，支持两种评估模式：主动导航和对象定位。

Result: 实验表明，当前最先进模型与人类表现之间存在显著差距。

Conclusion: PersONAL 基准测试揭示了当前最先进模型与人类表现之间的显著差距，强调了需要开发能够感知、推理和记忆个性化信息的具身智能体，为现实世界辅助机器人铺平了道路。

Abstract: Recent advances in Embodied AI have enabled agents to perform increasingly
complex tasks and adapt to diverse environments. However, deploying such agents
in realistic human-centered scenarios, such as domestic households, remains
challenging, particularly due to the difficulty of modeling individual human
preferences and behaviors. In this work, we introduce PersONAL (PERSonalized
Object Navigation And Localization, a comprehensive benchmark designed to study
personalization in Embodied AI. Agents must identify, retrieve, and navigate to
objects associated with specific users, responding to natural-language queries
such as "find Lily's backpack". PersONAL comprises over 2,000 high-quality
episodes across 30+ photorealistic homes from the HM3D dataset. Each episode
includes a natural-language scene description with explicit associations
between objects and their owners, requiring agents to reason over user-specific
semantics. The benchmark supports two evaluation modes: (1) active navigation
in unseen environments, and (2) object grounding in previously mapped scenes.
Experiments with state-of-the-art baselines reveal a substantial gap to human
performance, highlighting the need for embodied agents capable of perceiving,
reasoning, and memorizing over personalized information; paving the way towards
real-world assistive robot.

</details>


### [34] [FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models](https://arxiv.org/abs/2509.19870)
*Xin Wang,Jie Li,Zejia Weng,Yixu Wang,Yifeng Gao,Tianyu Pang,Chao Du,Yan Teng,Yingchun Wang,Zuxuan Wu,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 研究发现VLA模型易受对抗图像攻击，导致模型忽略指令，提出FreezeVLA攻击框架并验证其高效性，呼吁加强防御机制。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人技术中的安全性和对抗攻击的鲁棒性尚未充分探索，尤其是对抗图像可能导致模型忽略后续指令的漏洞。

Method: 提出了FreezeVLA攻击框架，通过最小-最大双层优化生成和评估动作冻结攻击。

Result: 在三个先进VLA模型和四个机器人基准测试中，FreezeVLA的平均攻击成功率达到76.2%，显著优于现有方法。

Conclusion: 该研究揭示了VLA模型中的关键安全风险，强调了开发鲁棒防御机制的紧迫性。

Abstract: Vision-Language-Action (VLA) models are driving rapid progress in robotics by
enabling agents to interpret multimodal inputs and execute complex,
long-horizon tasks. However, their safety and robustness against adversarial
attacks remain largely underexplored. In this work, we identify and formalize a
critical adversarial vulnerability in which adversarial images can "freeze" VLA
models and cause them to ignore subsequent instructions. This threat
effectively disconnects the robot's digital mind from its physical actions,
potentially inducing inaction during critical interventions. To systematically
study this vulnerability, we propose FreezeVLA, a novel attack framework that
generates and evaluates action-freezing attacks via min-max bi-level
optimization. Experiments on three state-of-the-art VLA models and four robotic
benchmarks show that FreezeVLA attains an average attack success rate of 76.2%,
significantly outperforming existing methods. Moreover, adversarial images
generated by FreezeVLA exhibit strong transferability, with a single image
reliably inducing paralysis across diverse language prompts. Our findings
expose a critical safety risk in VLA models and highlight the urgent need for
robust defense mechanisms.

</details>


### [35] [Adaptive Guidance Semantically Enhanced via Multimodal LLM for Edge-Cloud Object Detection](https://arxiv.org/abs/2509.19875)
*Yunqing Hu,Zheming Yang,Chang Zhao,Wen Ji*

Main category: cs.CV

TL;DR: 提出一种基于MLLM的边缘-云协作目标检测方法，通过语义增强在复杂场景中平衡精度与效率，显著降低延迟和计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测方法在低光照和严重遮挡等复杂场景中因缺乏高层语义理解而性能下降。

Method: 采用指令微调使多模态大语言模型（MLLM）生成结构化场景描述，并通过自适应映射机制动态转换语义信息为边缘检测器参数调整信号，实现实时语义增强。

Result: 实验表明，该方法在低光照和严重遮挡场景下延迟降低79%，计算成本减少70%，同时保持准确性。

Conclusion: 本文提出的基于自适应语义增强的边缘-云协作目标检测方法，在复杂场景下显著提升了检测精度和效率。

Abstract: Traditional object detection methods face performance degradation challenges
in complex scenarios such as low-light conditions and heavy occlusions due to a
lack of high-level semantic understanding. To address this, this paper proposes
an adaptive guidance-based semantic enhancement edge-cloud collaborative object
detection method leveraging Multimodal Large Language Models (MLLM), achieving
an effective balance between accuracy and efficiency. Specifically, the method
first employs instruction fine-tuning to enable the MLLM to generate structured
scene descriptions. It then designs an adaptive mapping mechanism that
dynamically converts semantic information into parameter adjustment signals for
edge detectors, achieving real-time semantic enhancement. Within an edge-cloud
collaborative inference framework, the system automatically selects between
invoking cloud-based semantic guidance or directly outputting edge detection
results based on confidence scores. Experiments demonstrate that the proposed
method effectively enhances detection accuracy and efficiency in complex
scenes. Specifically, it can reduce latency by over 79% and computational cost
by 70% in low-light and highly occluded scenes while maintaining accuracy.

</details>


### [36] [Generalized Shortest Path-based Superpixels for 3D Spherical Image Segmentation](https://arxiv.org/abs/2509.19895)
*Rémi Giraud,Rodrigo Borba Pinheiro,Yannick Berthoumieu*

Main category: cs.CV

TL;DR: SphSPS是一种针对360度球形或全向图像的超像素方法，通过考虑3D几何特性改进分割准确性和规则性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着广角图像采集设备的普及和计算机视觉中对快速准确图像分析的需求增加，现有的超像素方法通常针对标准2D平面图像设计，无法有效处理360度球形或全向图像。

Method: SphSPS方法考虑了3D球形采集空间的几何特性，通过最短路径快速提取相关聚类特征，并改进超像素的分割准确性和形状规则性。同时，还提出了一种新的全局规则性度量方法。

Result: 在参考的360度球形全景分割数据集和合成道路全向图像上验证了SphSPS方法的有效性，显著优于现有方法。

Conclusion: 提出的SphSPS方法在360度球形全景图像分割中显著优于现有的平面和球形方法，在分割准确性、抗噪声能力和规则性方面表现优异，为基于超像素的360度图像应用提供了有力工具。

Abstract: The growing use of wide angle image capture devices and the need for fast and
accurate image analysis in computer visions have enforced the need for
dedicated under-representation approaches. Most recent decomposition methods
segment an image into a small number of irregular homogeneous regions, called
superpixels. Nevertheless, these approaches are generally designed to segment
standard 2D planar images, i.e., captured with a 90o angle view without
distortion. In this work, we introduce a new general superpixel method called
SphSPS (for Spherical Shortest Path-based Superpixels)1 , dedicated to wide
360o spherical or omnidirectional images. Our method respects the geometry of
the 3D spherical acquisition space and generalizes the notion of shortest path
between a pixel and a superpixel center, to fastly extract relevant clustering
features. We demonstrate that considering the geometry of the acquisition space
to compute the shortest path enables to jointly improve the segmentation
accuracy and the shape regularity of superpixels. To evaluate this regularity
aspect, we also generalize a global regularity metric to the spherical space,
addressing the limitations of the only existing spherical compactness measure.
Finally, the proposed SphSPS method is validated on the reference 360o
spherical panorama segmentation dataset and on synthetic road omnidirectional
images. Our method significantly outperforms both planar and spherical
state-of-the-art approaches in terms of segmentation accuracy,robustness to
noise and regularity, providing a very interesting tool for superpixel-based
applications on 360o images.

</details>


### [37] [Efficient Cell Painting Image Representation Learning via Cross-Well Aligned Masked Siamese Network](https://arxiv.org/abs/2509.19896)
*Pin-Jui Huang,Yu-Hsuan Liao,SooHeon Kim,NoSeong Park,JongBae Park,DongMyung Shin*

Main category: cs.CV

TL;DR: CWA-MSN是一种新型表示学习框架，通过跨孔对齐掩码连体网络有效捕获细胞形态特征，显著优于现有方法，且数据与参数效率更高。


<details>
  <summary>Details</summary>
Motivation: 提取生物学意义且抗批次效应的细胞涂片表示具有挑战性，传统自监督和对比学习方法需要大规模模型或大量精心策划的数据，但仍难以克服批次效应。

Method: CWA-MSN采用跨孔对齐掩码连体网络框架，通过在不同孔中施加相同扰动的细胞嵌入对齐，实现语义一致性。

Result: 在基因-基因关系检索基准测试中，CWA-MSN优于现有的自监督（OpenPhenom）和对比学习（CellCLIP）方法，分别提高了29%和9%的基准分数，同时训练数据量更少（如0.2M vs. 2.2M）或模型规模更小（如22M vs. 1.48B参数）。

Conclusion: CWA-MSN是一种简单有效的细胞图像表示学习方法，能够在有限的数据和参数预算下实现高效的表型建模。

Abstract: Computational models that predict cellular phenotypic responses to chemical
and genetic perturbations can accelerate drug discovery by prioritizing
therapeutic hypotheses and reducing costly wet-lab iteration. However,
extracting biologically meaningful and batch-robust cell painting
representations remains challenging. Conventional self-supervised and
contrastive learning approaches often require a large-scale model and/or a huge
amount of carefully curated data, still struggling with batch effects. We
present Cross-Well Aligned Masked Siamese Network (CWA-MSN), a novel
representation learning framework that aligns embeddings of cells subjected to
the same perturbation across different wells, enforcing semantic consistency
despite batch effects. Integrated into a masked siamese architecture, this
alignment yields features that capture fine-grained morphology while remaining
data- and parameter-efficient. For instance, in a gene-gene relationship
retrieval benchmark, CWA-MSN outperforms the state-of-the-art publicly
available self-supervised (OpenPhenom) and contrastive learning (CellCLIP)
methods, improving the benchmark scores by +29\% and +9\%, respectively, while
training on substantially fewer data (e.g., 0.2M images for CWA-MSN vs. 2.2M
images for OpenPhenom) or smaller model size (e.g., 22M parameters for CWA-MSN
vs. 1.48B parameters for CellCLIP). Extensive experiments demonstrate that
CWA-MSN is a simple and effective way to learn cell image representation,
enabling efficient phenotype modeling even under limited data and parameter
budgets.

</details>


### [38] [Aerial-Ground Image Feature Matching via 3D Gaussian Splatting-based Intermediate View Rendering](https://arxiv.org/abs/2509.19898)
*Jiangxue Yu,Hui Wang,San Jiang,Xing Zhang,Dejin Zhang,Qingquan Li*

Main category: cs.CV

TL;DR: 提出了一种通过生成中间视图缓解视角畸变的航空与地面图像特征匹配算法，实验验证了其在匹配数量和场景渲染中的优越性。


<details>
  <summary>Details</summary>
Motivation: 航空与地面图像的集成是复杂场景3D建模的有效方案，但受到可靠对应关系查找的限制，因此需要一种能够缓解视角变化带来的透视畸变的特征匹配算法。

Method: 1. 使用航空图像通过增量式SfM引擎重建稀疏模型；2. 采用3D高斯泼溅技术进行场景渲染；3. 设计渲染视角确定算法生成高质量中间图像；4. 借助中间图像进行可靠的特征匹配。

Result: 实验结果表明，所提解决方案能够为航空与地面图像提供可靠的特征匹配，初始和精炼匹配数量明显增加，并能为准确的ISfM重建和完整的3DGS场景渲染提供足够匹配。

Conclusion: 通过生成中间视图来缓解视角变化带来的透视畸变，提出了一种用于航空与地面图像特征匹配的算法，实验验证了该算法在特征匹配和场景渲染方面的有效性。

Abstract: The integration of aerial and ground images has been a promising solution in
3D modeling of complex scenes, which is seriously restricted by finding
reliable correspondences. The primary contribution of this study is a feature
matching algorithm for aerial and ground images, whose core idea is to generate
intermediate views to alleviate perspective distortions caused by the extensive
viewpoint changes. First, by using aerial images only, sparse models are
reconstructed through an incremental SfM (Structure from Motion) engine due to
their large scene coverage. Second, 3D Gaussian Splatting is then adopted for
scene rendering by taking as inputs sparse points and oriented images. For
accurate view rendering, a render viewpoint determination algorithm is designed
by using the oriented camera poses of aerial images, which is used to generate
high-quality intermediate images that can bridge the gap between aerial and
ground images. Third, with the aid of intermediate images, reliable feature
matching is conducted for match pairs from render-aerial and render-ground
images, and final matches can be generated by transmitting correspondences
through intermediate views. By using real aerial and ground datasets, the
validation of the proposed solution has been verified in terms of feature
matching and scene rendering and compared comprehensively with widely used
methods. The experimental results demonstrate that the proposed solution can
provide reliable feature matches for aerial and ground images with an obvious
increase in the number of initial and refined matches, and it can provide
enough matches to achieve accurate ISfM reconstruction and complete 3DGS-based
scene rendering.

</details>


### [39] [CapStARE: Capsule-based Spatiotemporal Architecture for Robust and Efficient Gaze Estimation](https://arxiv.org/abs/2509.19936)
*Miren Samaniego,Igor Rodriguez,Elena Lazkano*

Main category: cs.CV

TL;DR: CapStARE是一种基于胶囊的实时视线估计架构，结合ConvNeXt和双GRU解码器，在多个数据集上表现优异，参数少且可解释性强。


<details>
  <summary>Details</summary>
Motivation: 设计一个模块化架构，以实现高效的部分-整体推理和解耦的时间建模，同时在实时推断中保持高性能。

Method: CapStARE采用胶囊基础的空间-时间架构，结合ConvNeXt骨干网络、注意力路由的胶囊形成以及专用于慢速和快速视线动态的双GRU解码器。

Result: 在ETH-XGaze（3.36）、MPIIFaceGaze（2.65）、Gaze360（9.06）和RT-GENE（4.76）等数据集上表现优异，参数更少且可解释性更强。

Conclusion: CapStARE提供了一种高效且鲁棒的实时视线估计解决方案，适用于交互式系统，并在多个数据集上实现了最先进的性能。

Abstract: We introduce CapStARE, a capsule-based spatio-temporal architecture for gaze
estimation that integrates a ConvNeXt backbone, capsule formation with
attention routing, and dual GRU decoders specialized for slow and rapid gaze
dynamics. This modular design enables efficient part-whole reasoning and
disentangled temporal modeling, achieving state-of-the-art performance on
ETH-XGaze (3.36) and MPIIFaceGaze (2.65) while maintaining real-time inference
(< 10 ms). The model also generalizes well to unconstrained conditions in
Gaze360 (9.06) and human-robot interaction scenarios in RT-GENE (4.76),
outperforming or matching existing methods with fewer parameters and greater
interpretability. These results demonstrate that CapStARE offers a practical
and robust solution for real-time gaze estimation in interactive systems. The
related code and results for this article can be found on:
https://github.com/toukapy/capsStare

</details>


### [40] [GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes](https://arxiv.org/abs/2509.19937)
*Guo Chen,Jiarun Liu,Sicong Du,Chenming Wu,Deqi Li,Shi-Sheng Huang,Guofeng Zhang,Sheng Yang*

Main category: cs.CV

TL;DR: GS-RoadPatching 是一种基于3D高斯泼溅的驾驶场景修补方法，通过结构匹配和替代融合优化，实现了高效且高质量的3D场景修补。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS修补方法依赖2D视角的扩散或GAN模型，预测缺失区域的外观或深度线索，存在时空一致性问题和耗时重训练的局限性。

Method: 构建特征嵌入的3DGS场景，采用多尺度局部上下文抽象和3D空间中的结构搜索方法，结合替代与融合优化以实现视觉和谐。

Result: 在多个公开数据集上的实验验证了该方法的高效性和优越性，达到了最先进的性能。

Conclusion: GS-RoadPatching 通过3D高斯泼溅（3DGS）模态实现了驾驶场景的高效修补和编辑，无需依赖2D跨模态的时空一致性或耗时的高斯重训练。实验证明该方法在质量和互操作性上均优于基线方法，且在通用场景中也表现出良好的适用性。

Abstract: This paper presents GS-RoadPatching, an inpainting method for driving scene
completion by referring to completely reconstructed regions, which are
represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting
methods that perform generative completion relying on 2D perspective-view-based
diffusion or GAN models to predict limited appearance or depth cues for missing
regions, our approach enables substitutional scene inpainting and editing
directly through the 3DGS modality, extricating it from requiring
spatial-temporal consistency of 2D cross-modals and eliminating the need for
time-intensive retraining of Gaussians. Our key insight is that the highly
repetitive patterns in driving scenes often share multi-modal similarities
within the implicit 3DGS feature space and are particularly suitable for
structural matching to enable effective 3DGS-based substitutional inpainting.
Practically, we construct feature-embedded 3DGS scenes to incorporate a patch
measurement method for abstracting local context at different scales and,
subsequently, propose a structural search method to find candidate patches in
3D space effectively. Finally, we propose a simple yet effective
substitution-and-fusion optimization for better visual harmony. We conduct
extensive experiments on multiple publicly available datasets to demonstrate
the effectiveness and efficiency of our proposed method in driving scenes, and
the results validate that our method achieves state-of-the-art performance
compared to the baseline methods in terms of both quality and interoperability.
Additional experiments in general scenes also demonstrate the applicability of
the proposed 3D inpainting strategy. The project page and code are available
at: https://shanzhaguoo.github.io/GS-RoadPatching/

</details>


### [41] [Interpreting ResNet-based CLIP via Neuron-Attention Decomposition](https://arxiv.org/abs/2509.19943)
*Edmund Bu,Yossi Gandelsman*

Main category: cs.CV

TL;DR: 提出了一种解释CLIP-ResNet神经元的新方法，通过分析神经元-注意力头配对，发现其可近似为单一方向并用于语义分割和数据集监测。


<details>
  <summary>Details</summary>
Motivation: 旨在理解CLIP-ResNet中神经元的内部工作机制，并探索其在下游任务中的潜在应用。

Method: 提出了分析CLIP-ResNet中神经元与注意力头配对贡献的新技术，通过将其近似为嵌入空间中的单一方向，并与文本关联进行解释。

Result: 发现神经元-注意力头配对可近似为单一方向，且仅有稀疏配对的贡献显著。这些配对可用于无训练语义分割和监测数据集分布偏移，表现优于先前方法。

Conclusion: 通过分解神经元对输出的贡献为个体计算路径，本研究揭示了CLIP-ResNet中可解释的单元，并展示了这些单元在下游任务中的应用价值。

Abstract: We present a novel technique for interpreting the neurons in CLIP-ResNet by
decomposing their contributions to the output into individual computation
paths. More specifically, we analyze all pairwise combinations of neurons and
the following attention heads of CLIP's attention-pooling layer. We find that
these neuron-head pairs can be approximated by a single direction in
CLIP-ResNet's image-text embedding space. Leveraging this insight, we interpret
each neuron-head pair by associating it with text. Additionally, we find that
only a sparse set of the neuron-head pairs have a significant contribution to
the output value, and that some neuron-head pairs, while polysemantic,
represent sub-concepts of their corresponding neurons. We use these
observations for two applications. First, we employ the pairs for training-free
semantic segmentation, outperforming previous methods for CLIP-ResNet. Second,
we utilize the contributions of neuron-head pairs to monitor dataset
distribution shifts. Our results demonstrate that examining individual
computation paths in neural networks uncovers interpretable units, and that
such units can be utilized for downstream tasks.

</details>


### [42] [When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset](https://arxiv.org/abs/2509.19952)
*Sarmistha Das,R E Zera Marveen Lyngkhoi,Kirtan Jain,Vinayak Goyal,Sriparna Saha,Manish Gupta*

Main category: cs.CV

TL;DR: 论文提出了通过视频生成投诉描述（CoD-V）的新任务，引入了ComVID数据集和CR评估指标，并开发了多模态RAG嵌入式VideoLLaMA2-7b模型，以帮助用户更清晰地表达投诉。


<details>
  <summary>Details</summary>
Motivation: 用户通常难以通过文字清晰表达投诉，但可以轻松上传视频展示产品缺陷。为了解决这一问题，论文提出了从视频生成投诉描述（CoD-V）的新任务，以帮助用户更准确地表达投诉。

Method: 该论文引入了ComVID数据集，包含1,175个投诉视频及其对应描述，并标注了投诉者的情感状态。提出了新的CR评估指标，并开发了多模态RAG嵌入式VideoLLaMA2-7b模型，用于生成投诉内容。

Result: 论文对多种视频语言模型进行了全面评估，包括预训练和微调版本，并使用了多种评估指标（如METEOR、困惑度和Coleman-Liau可读性分数）。结果表明，提出的模型在生成投诉时能有效考虑用户情感状态。

Conclusion: 该论文提出了一个新的研究方向，即通过视频帮助用户更清晰地表达投诉，并引入了ComVID数据集和CR评估指标。提出的多模态RAG嵌入式VideoLLaMA2-7b模型在生成投诉时考虑了用户的情感状态，为未来研究奠定了基础。

Abstract: While there exists a lot of work on explainable complaint mining,
articulating user concerns through text or video remains a significant
challenge, often leaving issues unresolved. Users frequently struggle to
express their complaints clearly in text but can easily upload videos depicting
product defects (e.g., vague text such as `worst product' paired with a
5-second video depicting a broken headphone with the right earcup). This paper
formulates a new task in the field of complaint mining to aid the common users'
need to write an expressive complaint, which is Complaint Description from
Videos (CoD-V) (e.g., to help the above user articulate her complaint about the
defective right earcup). To this end, we introduce ComVID, a video complaint
dataset containing 1,175 complaint videos and the corresponding descriptions,
also annotated with the emotional state of the complainer. Additionally, we
present a new complaint retention (CR) evaluation metric that discriminates the
proposed (CoD-V) task against standard video summary generation and description
tasks. To strengthen this initiative, we introduce a multimodal
Retrieval-Augmented Generation (RAG) embedded VideoLLaMA2-7b model, designed to
generate complaints while accounting for the user's emotional state. We conduct
a comprehensive evaluation of several Video Language Models on several tasks
(pre-trained and fine-tuned versions) with a range of established evaluation
metrics, including METEOR, perplexity, and the Coleman-Liau readability score,
among others. Our study lays the foundation for a new research direction to
provide a platform for users to express complaints through video. Dataset and
resources are available at: https://github.com/sarmistha-D/CoD-V.

</details>


### [43] [SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding](https://arxiv.org/abs/2509.19965)
*Phyo Thet Yee,Dimitrios Kollias,Sudeepta Mishra,Abhinav Dhall*

Main category: cs.CV

TL;DR: SynchroRaMa通过多模态情感嵌入和动态场景描述，提升了说话人脸视频的情感表达和真实感，实验和用户研究均显示其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一模态情感嵌入和静态参考图像，难以捕捉细腻情感和动态变化，限制了情感表达和真实感。

Method: 结合文本情感分析和音频情感识别的多模态情感嵌入，以及LLM生成的场景描述，通过A2M模块实现音频驱动的自然头部运动和唇同步。

Result: 在基准数据集上，SynchroRaMa在图像质量、表情保持和运动真实感方面优于现有方法，用户研究也证实了其更高的主观评分。

Conclusion: SynchroRaMa通过多模态情感嵌入和动态场景描述，显著提升了说话人脸视频的情感表达和视觉真实感，实验和用户研究均验证了其优越性。

Abstract: Audio-driven talking face generation has received growing interest,
particularly for applications requiring expressive and natural human-avatar
interaction. However, most existing emotion-aware methods rely on a single
modality (either audio or image) for emotion embedding, limiting their ability
to capture nuanced affective cues. Additionally, most methods condition on a
single reference image, restricting the model's ability to represent dynamic
changes in actions or attributes across time. To address these issues, we
introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion
embedding by combining emotional signals from text (via sentiment analysis) and
audio (via speech-based emotion recognition and audio-derived valence-arousal
features), enabling the generation of talking face videos with richer and more
authentic emotional expressiveness and fidelity. To ensure natural head motion
and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M)
module that generates motion frames aligned with the input audio. Finally,
SynchroRaMa incorporates scene descriptions generated by Large Language Model
(LLM) as additional textual input, enabling it to capture dynamic actions and
high-level semantic attributes. Conditioning the model on both visual and
textual cues enhances temporal consistency and visual realism. Quantitative and
qualitative experiments on benchmark datasets demonstrate that SynchroRaMa
outperforms the state-of-the-art, achieving improvements in image quality,
expression preservation, and motion realism. A user study further confirms that
SynchroRaMa achieves higher subjective ratings than competing methods in
overall naturalness, motion diversity, and video smoothness. Our project page
is available at <https://novicemm.github.io/synchrorama>.

</details>


### [44] [OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving](https://arxiv.org/abs/2509.19973)
*Pei Liu,Hongliang Lu,Haichao Liu,Haipeng Liu,Xin Liu,Ruoyu Yao,Shengbo Eben Li,Jun Ma*

Main category: cs.CV

TL;DR: OmniScene提出了一种人类化的自动驾驶场景理解框架，结合视觉语言模型和层次化融合策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统缺乏人类般的场景理解能力，主要依赖深度重建而非真正的理解。

Method: 提出了OmniScene框架，包括OmniVLM视觉语言模型和层次化融合策略（HFS），通过知识蒸馏和语义监督增强特征学习。

Result: 在nuScenes数据集上全面评估，OmniScene在感知、预测、规划和视觉问答等任务中均优于十多种最先进模型。

Conclusion: OmniScene通过集成多视角和时间感知的视觉语言模型、知识蒸馏以及层次化融合策略，显著提升了自动驾驶系统的场景理解能力，并在多个任务上实现了最先进的性能。

Abstract: Human vision is capable of transforming two-dimensional observations into an
egocentric three-dimensional scene understanding, which underpins the ability
to translate complex scenes and exhibit adaptive behaviors. This capability,
however, remains lacking in current autonomous driving systems, where
mainstream approaches primarily rely on depth-based 3D reconstruction rather
than true scene understanding. To address this limitation, we propose a novel
human-like framework called OmniScene. First, we introduce the OmniScene
Vision-Language Model (OmniVLM), a vision-language framework that integrates
multi-view and temporal perception for holistic 4D scene understanding. Then,
harnessing a teacher-student OmniVLM architecture and knowledge distillation,
we embed textual representations into 3D instance features for semantic
supervision, enriching feature learning, and explicitly capturing human-like
attentional semantics. These feature representations are further aligned with
human driving behaviors, forming a more human-like
perception-understanding-action architecture. In addition, we propose a
Hierarchical Fusion Strategy (HFS) to address imbalances in modality
contributions during multimodal integration. Our approach adaptively calibrates
the relative significance of geometric and semantic features at multiple
abstraction levels, enabling the synergistic use of complementary cues from
visual and textual modalities. This learnable dynamic fusion enables a more
nuanced and effective exploitation of heterogeneous information. We evaluate
OmniScene comprehensively on the nuScenes dataset, benchmarking it against over
ten state-of-the-art models across various tasks. Our approach consistently
achieves superior results, establishing new benchmarks in perception,
prediction, planning, and visual question answering.

</details>


### [45] [CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion](https://arxiv.org/abs/2509.19979)
*Chenhao Ji,Chaohui Yu,Junyao Gao,Fan Wang,Cairong Zhao*

Main category: cs.CV

TL;DR: CamPVG 是首个基于扩散的全景视频生成框架，通过创新的相机位姿编码和球面极线模块，解决了全景视频生成的几何一致性难题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要集中在透视投影视频生成中的相机控制，而几何一致的全景视频生成仍具挑战性。

Method: 提出了全景 Plücker 嵌入和球面极线模块，分别用于编码相机外参和实现跨视图特征聚合。

Result: 实验表明，CamPVG 能够生成高质量且与相机轨迹一致的全景视频，远超现有方法。

Conclusion: CamPVG 是一种基于扩散的框架，能够生成与相机轨迹一致的高质量全景视频，显著优于现有方法。

Abstract: Recently, camera-controlled video generation has seen rapid development,
offering more precise control over video generation. However, existing methods
predominantly focus on camera control in perspective projection video
generation, while geometrically consistent panoramic video generation remains
challenging. This limitation is primarily due to the inherent complexities in
panoramic pose representation and spherical projection. To address this issue,
we propose CamPVG, the first diffusion-based framework for panoramic video
generation guided by precise camera poses. We achieve camera position encoding
for panoramic images and cross-view feature aggregation based on spherical
projection. Specifically, we propose a panoramic Pl\"ucker embedding that
encodes camera extrinsic parameters through spherical coordinate
transformation. This pose encoder effectively captures panoramic geometry,
overcoming the limitations of traditional methods when applied to
equirectangular projections. Additionally, we introduce a spherical epipolar
module that enforces geometric constraints through adaptive attention masking
along epipolar lines. This module enables fine-grained cross-view feature
aggregation, substantially enhancing the quality and consistency of generated
panoramic videos. Extensive experiments demonstrate that our method generates
high-quality panoramic videos consistent with camera trajectories, far
surpassing existing methods in panoramic video generation.

</details>


### [46] [SDE-DET: A Precision Network for Shatian Pomelo Detection in Complex Orchard Environments](https://arxiv.org/abs/2509.19990)
*Yihao Hu,Pan Wang,Xiaodong Bai,Shijie Cai,Hang Wang,Huazhong Liu,Aiping Yang,Xiangxiang Li,Meiping Ding,Hongyan Liu,Jianguo Yao*

Main category: cs.CV

TL;DR: SDE-DET模型通过Star Block、Deformable Attention和Efficient Multi-Scale Attention机制，有效解决了沙田柚检测中的多尺度、遮挡和小目标问题，在STP-AgriData数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 沙田柚检测在复杂果园环境中面临多尺度问题、树干和叶子的遮挡以及小目标检测等挑战。

Method: SDE-DET模型首先利用Star Block高效获取高维信息而不增加计算开销；其次采用Deformable Attention增强遮挡条件下的检测能力；最后集成多个Efficient Multi-Scale Attention机制以减少计算开销并提取深层视觉表示，从而提升小目标检测能力。

Result: SDE-DET在Precision、Recall、mAP@0.5、mAP@0.5:0.95和F1-score上分别达到0.883、0.771、0.838、0.497和0.823，优于Yolo系列及其他主流检测模型。

Conclusion: SDE-DET模型在STP-AgriData数据集上实现了最先进的性能，为自动化收获机器人的进一步发展奠定了基础。

Abstract: Pomelo detection is an essential process for their localization, automated
robotic harvesting, and maturity analysis. However, detecting Shatian pomelo in
complex orchard environments poses significant challenges, including
multi-scale issues, obstructions from trunks and leaves, small object
detection, etc. To address these issues, this study constructs a custom dataset
STP-AgriData and proposes the SDE-DET model for Shatian pomelo detection.
SDE-DET first utilizes the Star Block to effectively acquire high-dimensional
information without increasing the computational overhead. Furthermore, the
presented model adopts Deformable Attention in its backbone, to enhance its
ability to detect pomelos under occluded conditions. Finally, multiple
Efficient Multi-Scale Attention mechanisms are integrated into our model to
reduce the computational overhead and extract deep visual representations,
thereby improving the capacity for small object detection. In the experiment,
we compared SDE-DET with the Yolo series and other mainstream detection models
in Shatian pomelo detection. The presented SDE-DET model achieved scores of
0.883, 0.771, 0.838, 0.497, and 0.823 in Precision, Recall, mAP@0.5,
mAP@0.5:0.95 and F1-score, respectively. SDE-DET has achieved state-of-the-art
performance on the STP-AgriData dataset. Experiments indicate that the SDE-DET
provides a reliable method for Shatian pomelo detection, laying the foundation
for the further development of automatic harvest robots.

</details>


### [47] [Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models](https://arxiv.org/abs/2509.19994)
*Zhifang Zhang,Jiahan Zhang,Shengjie Zhou,Qi Wei,Shuo He,Feng Liu,Lei Feng*

Main category: cs.CV

TL;DR: 本文提出PTA方法，解决了多模态预训练模型中目标对抗攻击的通用性和不可检测性问题，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态预训练模型（如ImageBind）的广泛应用引发了安全担忧，尤其是目标对抗攻击的通用性和不可检测性存在局限。

Method: 提出了一种名为代理目标攻击（PTA）的新方法，通过利用多源模态和目标模态代理优化目标对抗样本，确保其在防御下仍能保持隐蔽性并与多个潜在目标对齐。

Result: 实验结果表明，PTA在多种相关目标上实现了高成功率，并且能够抵御多种异常检测方法。

Conclusion: 本文提出的代理目标攻击（PTA）方法有效解决了多模态预训练模型中目标对抗攻击的通用性和不可检测性问题，实验证明了其高成功率和防御规避能力。

Abstract: Multimodal pre-trained models (e.g., ImageBind), which align distinct data
modalities into a shared embedding space, have shown remarkable success across
downstream tasks. However, their increasing adoption raises serious security
concerns, especially regarding targeted adversarial attacks. In this paper, we
show that existing targeted adversarial attacks on multimodal pre-trained
models still have limitations in two aspects: generalizability and
undetectability. Specifically, the crafted targeted adversarial examples (AEs)
exhibit limited generalization to partially known or semantically similar
targets in cross-modal alignment tasks (i.e., limited generalizability) and can
be easily detected by simple anomaly detection methods (i.e., limited
undetectability). To address these limitations, we propose a novel method
called Proxy Targeted Attack (PTA), which leverages multiple source-modal and
target-modal proxies to optimize targeted AEs, ensuring they remain evasive to
defenses while aligning with multiple potential targets. We also provide
theoretical analyses to highlight the relationship between generalizability and
undetectability and to ensure optimal generalizability while meeting the
specified requirements for undetectability. Furthermore, experimental results
demonstrate that our PTA can achieve a high success rate across various related
targets and remain undetectable against multiple anomaly detection methods.

</details>


### [48] [Table Detection with Active Learning](https://arxiv.org/abs/2509.20003)
*Somraj Gautam,Nachiketa Purohit,Gaurav Harit*

Main category: cs.CV

TL;DR: Active learning with diversity-based strategies reduces annotation costs in object detection, outperforming random sampling and achieving higher mAP scores with limited budgets.


<details>
  <summary>Details</summary>
Motivation: Efficient data annotation is a critical challenge in machine learning, especially for object detection tasks requiring extensive labeled data.

Method: Incorporating diversity-based strategies into active learning for selecting informative samples in object detection tasks.

Result: The method achieves higher mAP scores within the same annotation budget on benchmark datasets (TableBank-LaTeX, TableBank-Word) using CascadeTabNet and YOLOv9.

Conclusion: AL-based example selection significantly outperforms random sampling, reducing annotation effort while maintaining comparable performance to fully supervised models.

Abstract: Efficient data annotation remains a critical challenge in machine learning,
particularly for object detection tasks requiring extensive labeled data.
Active learning (AL) has emerged as a promising solution to minimize annotation
costs by selecting the most informative samples. While traditional AL
approaches primarily rely on uncertainty-based selection, recent advances
suggest that incorporating diversity-based strategies can enhance sampling
efficiency in object detection tasks. Our approach ensures the selection of
representative examples that improve model generalization. We evaluate our
method on two benchmark datasets (TableBank-LaTeX, TableBank-Word) using
state-of-the-art table detection architectures, CascadeTabNet and YOLOv9. Our
results demonstrate that AL-based example selection significantly outperforms
random sampling, reducing annotation effort given a limited budget while
maintaining comparable performance to fully supervised models. Our method
achieves higher mAP scores within the same annotation budget.

</details>


### [49] [Anomaly Detection by Clustering DINO Embeddings using a Dirichlet Process Mixture](https://arxiv.org/abs/2509.19997)
*Nico Schulthess,Ender Konukoglu*

Main category: cs.CV

TL;DR: 提出使用DPMM建模DINOv2嵌入，高效实现医学影像异常检测，计算时间减半且性能优异。


<details>
  <summary>Details</summary>
Motivation: 针对大型医学数据集，传统基于内存库的异常检测方法计算负担过重，因此需要一种更高效的方法。

Method: 提出使用Dirichlet Process Mixture模型（DPMM）建模DINOv2嵌入的分布，替代传统的内存库方法，通过组件中心与嵌入的相似性作为异常评分函数生成粗粒度异常分割掩码。

Result: 实验表明，DPMM建模的DINOv2嵌入在医学影像异常检测中表现优异，计算时间至少减半。归一化嵌入在解剖结构对齐方面优于未归一化特征。

Conclusion: 通过DPMM对DINOv2嵌入进行建模，显著减少了计算时间，同时在医学影像基准测试中实现了极具竞争力的异常检测性能。归一化的DINOv2嵌入在解剖结构对齐方面表现更优，适合异常检测。

Abstract: In this work, we leverage informative embeddings from foundational models for
unsupervised anomaly detection in medical imaging. For small datasets, a
memory-bank of normative features can directly be used for anomaly detection
which has been demonstrated recently. However, this is unsuitable for large
medical datasets as the computational burden increases substantially.
Therefore, we propose to model the distribution of normative DINOv2 embeddings
with a Dirichlet Process Mixture model (DPMM), a non-parametric mixture model
that automatically adjusts the number of mixture components to the data at
hand. Rather than using a memory bank, we use the similarity between the
component centers and the embeddings as anomaly score function to create a
coarse anomaly segmentation mask. Our experiments show that through DPMM
embeddings of DINOv2, despite being trained on natural images, achieve very
competitive anomaly detection performance on medical imaging benchmarks and can
do this while at least halving the computation time at inference. Our analysis
further indicates that normalized DINOv2 embeddings are generally more aligned
with anatomical structures than unnormalized features, even in the presence of
anomalies, making them great representations for anomaly detection. The code is
available at https://github.com/NicoSchulthess/anomalydino-dpmm.

</details>


### [50] [Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification](https://arxiv.org/abs/2509.20024)
*Lubos Mjachky,Ivan Homoliak*

Main category: cs.CV

TL;DR: 提出一种基于GAN的隐私保护认证方法，将人脸图像转换为视觉隐私域，实验证明其安全有效。


<details>
  <summary>Details</summary>
Motivation: 现有的生物认证系统不允许用户控制其数据的使用方式，且存在数据泄露和滥用的风险。

Method: 使用GAN将人脸图像转换为视觉隐私域（如花朵或鞋子），并在该域上训练用于认证的分类器。

Result: 实验表明，该方法对攻击具有鲁棒性，并保持了有意义的实用性。

Conclusion: 该论文提出的基于生成对抗网络（GAN）的认证方法在保护个人隐私的同时，仍能提供有效的认证功能。

Abstract: Biometric-based authentication systems are getting broadly adopted in many
areas. However, these systems do not allow participating users to influence the
way their data is used. Furthermore, the data may leak and can be misused
without the users' knowledge. In this paper, we propose a new authentication
method that preserves the privacy of individuals and is based on a generative
adversarial network (GAN). Concretely, we suggest using the GAN for translating
images of faces to a visually private domain (e.g., flowers or shoes).
Classifiers, which are used for authentication purposes, are then trained on
the images from the visually private domain. Based on our experiments, the
method is robust against attacks and still provides meaningful utility.

</details>


### [51] [Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models](https://arxiv.org/abs/2509.20107)
*JuanaJuana Valeria Hurtado,Rohit Mohan,Abhinav Valada*

Main category: cs.CV

TL;DR: 该论文提出了一种高光谱适配器，利用预训练视觉模型提升高光谱语义分割性能，在自动驾驶数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前的高光谱语义分割方法由于依赖针对RGB输入优化的架构和学习框架而表现不佳，因此需要一种能够直接处理高光谱数据并提升分割性能的新方法。

Method: 架构结合了光谱变换器和频谱感知的空间先验模块，以提取丰富的空间-频谱特征，并引入了模态感知交互块，通过专用的提取和注入机制，有效整合高光谱表示和冻结的视觉Transformer特征。

Result: 在三个基准自动驾驶数据集上的广泛评估表明，该架构在使用高光谱输入时优于基于视觉和高光谱分割方法，实现了最先进的语义分割性能。

Conclusion: 该论文提出了一种新型的高光谱适配器，通过利用预训练的视觉基础模型，有效学习高光谱数据，并在三个基准自动驾驶数据集上实现了最先进的语义分割性能。

Abstract: Hyperspectral imaging (HSI) captures spatial information along with dense
spectral measurements across numerous narrow wavelength bands. This rich
spectral content has the potential to facilitate robust robotic perception,
particularly in environments with complex material compositions, varying
illumination, or other visually challenging conditions. However, current HSI
semantic segmentation methods underperform due to their reliance on
architectures and learning frameworks optimized for RGB inputs. In this work,
we propose a novel hyperspectral adapter that leverages pretrained vision
foundation models to effectively learn from hyperspectral data. Our
architecture incorporates a spectral transformer and a spectrum-aware spatial
prior module to extract rich spatial-spectral features. Additionally, we
introduce a modality-aware interaction block that facilitates effective
integration of hyperspectral representations and frozen vision Transformer
features through dedicated extraction and injection mechanisms. Extensive
evaluations on three benchmark autonomous driving datasets demonstrate that our
architecture achieves state-of-the-art semantic segmentation performance while
directly using HSI inputs, outperforming both vision-based and hyperspectral
segmentation methods. We make the code available at
https://hyperspectraladapter.cs.uni-freiburg.de.

</details>


### [52] [Does the Manipulation Process Matter? RITA: Reasoning Composite Image Manipulations via Reversely-Ordered Incremental-Transition Autoregression](https://arxiv.org/abs/2509.20006)
*Xuekang Zhu,Ji-Zhe Zhou,Kaiwen Feng,Chenfan Qu,Yunfei Wang,Liting Zhou,Jian liu*

Main category: cs.CV

TL;DR: RITA通过条件序列预测任务重新定义图像操纵定位，显著提升性能并验证其通用性。


<details>
  <summary>Details</summary>
Motivation: 现有图像操纵定位方法忽略操纵过程的顺序性和层次性，导致维度崩溃和任务本质不匹配。

Method: 提出RITA框架，将图像操纵定位建模为条件序列预测任务，逐层预测操纵区域，并利用每一步的预测作为下一步的条件。

Result: RITA在传统基准测试中达到SOTA性能，并为新的层次定位任务奠定了基础。

Conclusion: RITA框架通过条件序列预测任务重新定义了图像操纵定位，显著提升了性能，并提供了一个通用且有效的范式。

Abstract: Image manipulations often entail a complex manipulation process, comprising a
series of editing operations to create a deceptive image, exhibiting
sequentiality and hierarchical characteristics. However, existing IML methods
remain manipulation-process-agnostic, directly producing localization masks in
a one-shot prediction paradigm without modeling the underlying editing steps.
This one-shot paradigm compresses the high-dimensional compositional space into
a single binary mask, inducing severe dimensional collapse, thereby creating a
fundamental mismatch with the intrinsic nature of the IML task.
  To address this, we are the first to reformulate image manipulation
localization as a conditional sequence prediction task, proposing the RITA
framework. RITA predicts manipulated regions layer-by-layer in an ordered
manner, using each step's prediction as the condition for the next, thereby
explicitly modeling temporal dependencies and hierarchical structures among
editing operations.
  To enable training and evaluation, we synthesize multi-step manipulation data
and construct a new benchmark HSIM. We further propose the HSS metric to assess
sequential order and hierarchical alignment. Extensive experiments show RITA
achieves SOTA on traditional benchmarks and provides a solid foundation for the
novel hierarchical localization task, validating its potential as a general and
effective paradigm. The code and dataset will be publicly available.

</details>


### [53] [PS3: A Multimodal Transformer Integrating Pathology Reports with Histology Images and Biological Pathways for Cancer Survival Prediction](https://arxiv.org/abs/2509.20022)
*Manahil Raza,Ayesha Azam,Talha Qaiser,Nasir Rajpoot*

Main category: cs.CV

TL;DR: 论文提出PS3模型，通过整合病理报告、全切片图像和转录组数据，利用原型生成和Transformer融合提升生存预测性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要整合全切片图像与基因组数据，但忽略了病理报告中的补充信息。病理报告包含专家总结和组织学发现，可能进一步提升预测性能。

Method: 采用原型生成方法分别处理病理报告（诊断原型）、全切片图像（组织学原型）和转录组数据（生物通路原型），并通过Transformer模型进行多模态融合。

Result: 在TCGA的六个数据集上，PS3模型在生存预测任务中表现优于临床、单模态和多模态基线方法。

Conclusion: 论文提出了一种基于原型的三模态Transformer模型PS3，通过整合病理报告、全切片图像和转录组数据，显著提升了生存预测的准确性，优于现有方法。

Abstract: Current multimodal fusion approaches in computational oncology primarily
focus on integrating multi-gigapixel histology whole slide images (WSIs) with
genomic or transcriptomic data, demonstrating improved survival prediction. We
hypothesize that incorporating pathology reports can further enhance prognostic
performance. Pathology reports, as essential components of clinical workflows,
offer readily available complementary information by summarizing
histopathological findings and integrating expert interpretations and clinical
context. However, fusing these modalities poses challenges due to their
heterogeneous nature. WSIs are high-dimensional, each containing several
billion pixels, whereas pathology reports consist of concise text summaries of
varying lengths, leading to potential modality imbalance. To address this, we
propose a prototype-based approach to generate balanced representations, which
are then integrated using a Transformer-based fusion model for survival
prediction that we term PS3 (Predicting Survival from Three Modalities).
Specifically, we present: (1) Diagnostic prototypes from pathology reports,
leveraging self-attention to extract diagnostically relevant sections and
standardize text representation; (2) Histological prototypes to compactly
represent key morphological patterns in WSIs; and (3) Biological pathway
prototypes to encode transcriptomic expressions, accurately capturing cellular
functions. PS3, the three-modal transformer model, processes the resulting
prototype-based multimodal tokens and models intra-modal and cross-modal
interactions across pathology reports, WSIs and transcriptomic data. The
proposed model outperforms state-of-the-art methods when evaluated against
clinical, unimodal and multimodal baselines on six datasets from The Cancer
Genome Atlas (TCGA). The code is available at: https://github.com/manahilr/PS3.

</details>


### [54] [EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models](https://arxiv.org/abs/2509.20146)
*Botai Yuan,Yutian Zhou,Yingjie Wang,Fushuo Huo,Yongcheng Jing,Li Shen,Ying Wei,Zhiqi Shen,Ziwei Liu,Tianwei Zhang,Jie Yang,Dacheng Tao*

Main category: cs.CV

TL;DR: 医疗LVLMs普遍存在迎合性行为，EchoBench基准揭示其严重性，并提供通过数据质量和提示干预降低迎合性的方法。


<details>
  <summary>Details</summary>
Motivation: 现有医疗LVLMs基准过于关注准确性，忽视了可靠性和安全性，特别是在高风险临床环境中模型的迎合性行为。

Method: 引入EchoBench基准，系统评估医疗LVLMs的迎合性，包含2,122张图像、90个模拟偏见的提示，并分析了不同模型的表现。

Result: 所有模型均表现出显著迎合性，最佳专有模型Claude 3.7 Sonnet仍有45.98%的迎合性，而GPT-4.1达59.15%。部分医疗专用模型迎合性超过95%。细粒度分析揭示了影响因素，数据质量和领域知识可降低迎合性。

Conclusion: 研究发现医疗LVLMs普遍存在迎合性行为，强调需要超越准确性的稳健评估，并提供构建更安全、更可信模型的实用指导。

Abstract: Recent benchmarks for medical Large Vision-Language Models (LVLMs) emphasize
leaderboard accuracy, overlooking reliability and safety. We study sycophancy
-- models' tendency to uncritically echo user-provided information -- in
high-stakes clinical settings. We introduce EchoBench, a benchmark to
systematically evaluate sycophancy in medical LVLMs. It contains 2,122 images
across 18 departments and 20 modalities with 90 prompts that simulate biased
inputs from patients, medical students, and physicians. We evaluate
medical-specific, open-source, and proprietary LVLMs. All exhibit substantial
sycophancy; the best proprietary model (Claude 3.7 Sonnet) still shows 45.98%
sycophancy, and GPT-4.1 reaches 59.15%. Many medical-specific models exceed 95%
sycophancy despite only moderate accuracy. Fine-grained analyses by bias type,
department, perceptual granularity, and modality identify factors that increase
susceptibility. We further show that higher data quality/diversity and stronger
domain knowledge reduce sycophancy without harming unbiased accuracy. EchoBench
also serves as a testbed for mitigation: simple prompt-level interventions
(negative prompting, one-shot, few-shot) produce consistent reductions and
motivate training- and decoding-time strategies. Our findings highlight the
need for robust evaluation beyond accuracy and provide actionable guidance
toward safer, more trustworthy medical LVLMs.

</details>


### [55] [Predictive Quality Assessment for Mobile Secure Graphics](https://arxiv.org/abs/2509.20028)
*Cas Steigstra,Sergey Milyaev,Shaodi You*

Main category: cs.CV

TL;DR: 论文提出了一种预测视频帧质量分数的轻量级模型，解决了智能手机图像采集质量差导致的验证可靠性问题，并发现冻结的通用主干网络在跨域分析中表现更好。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是解决智能手机图像采集质量差导致的高熵图案验证的可靠性问题。

Method: 论文提出了一种轻量级模型来预测视频帧的质量分数，确定其是否适合资源密集型的oracle模型。

Result: 论文通过在大规模数据集（32,000+图像，105种智能手机）上验证，使用重新上下文化的FNMR和ISRR指标，展示了框架的有效性。

Conclusion: 论文的结论是，对于物理制造领域的域转移，冻结的通用主干网络比完全微调的模型更鲁棒，后者可能过度拟合源域伪影。

Abstract: The reliability of secure graphic verification, a key anti-counterfeiting
tool, is undermined by poor image acquisition on smartphones. Uncontrolled user
captures of these high-entropy patterns cause high false rejection rates,
creating a significant 'reliability gap'. To bridge this gap, we depart from
traditional perceptual IQA and introduce a framework that predictively
estimates a frame's utility for the downstream verification task. We propose a
lightweight model to predict a quality score for a video frame, determining its
suitability for a resource-intensive oracle model. Our framework is validated
using re-contextualized FNMR and ISRR metrics on a large-scale dataset of
32,000+ images from 105 smartphones. Furthermore, a novel cross-domain analysis
on graphics from different industrial printing presses reveals a key finding: a
lightweight probe on a frozen, ImageNet-pretrained network generalizes better
to an unseen printing technology than a fully fine-tuned model. This provides a
key insight for real-world generalization: for domain shifts from physical
manufacturing, a frozen general-purpose backbone can be more robust than full
fine-tuning, which can overfit to source-domain artifacts.

</details>


### [56] [U-Mamba2-SSL for Semi-Supervised Tooth and Pulp Segmentation in CBCT](https://arxiv.org/abs/2509.20154)
*Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li*

Main category: cs.CV

TL;DR: 提出U-Mamba2-SSL框架，通过自监督学习和伪标签策略，显著提升CBCT图像中牙齿和牙髓分割的准确性。


<details>
  <summary>Details</summary>
Motivation: CBCT图像中牙齿和牙髓的分割需要专业知识和大量时间，亟需能有效利用未标记数据的自动化算法。

Method: 提出了U-Mamba2-SSL框架，结合自监督预训练、一致性正则化和伪标签策略，利用未标记数据提升模型性能。

Result: 在验证数据集上平均得分为0.872，DSC为0.969，表现优异。

Conclusion: U-Mamba2-SSL展示了在CBCT图像中自动分割牙齿和牙髓的优越性能，为临床治疗规划和诊断提供了高效的工具。

Abstract: Accurate segmentation of teeth and pulp in Cone-Beam Computed Tomography
(CBCT) is vital for clinical applications like treatment planning and
diagnosis. However, this process requires extensive expertise and is
exceptionally time-consuming, highlighting the critical need for automated
algorithms that can effectively utilize unlabeled data. In this paper, we
propose U-Mamba2-SSL, a novel semi-supervised learning framework that builds on
the U-Mamba2 model and employs a multi-stage training strategy. The framework
first pre-trains U-Mamba2 in a self-supervised manner using a disruptive
autoencoder. It then leverages unlabeled data through consistency
regularization, where we introduce input and feature perturbations to ensure
stable model outputs. Finally, a pseudo-labeling strategy is implemented with a
reduced loss weighting to minimize the impact of potential errors. U-Mamba2-SSL
achieved an average score of 0.872 and a DSC of 0.969 on the validation
dataset, demonstrating the superior performance of our approach. The code is
available at https://github.com/zhiqin1998/UMamba2.

</details>


### [57] [SHMoAReg: Spark Deformable Image Registration via Spatial Heterogeneous Mixture of Experts and Attention Heads](https://arxiv.org/abs/2509.20073)
*Yuxi Zheng,Jianhui Feng,Tianran Li,Marius Staring,Yuchuan Qiao*

Main category: cs.CV

TL;DR: SHMoAReg通过MoE机制在编码器和解码器中分别引入MoA和SHMoE，显著提升DIR性能和可解释性，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在特征提取和变形场预测上缺乏针对性，无法有效处理不同方向的变形。SHMoAReg旨在通过MoE机制增强特征提取和变形预测的专门化和异质性。

Method: 提出了一种名为SHMoAReg的新网络，其中编码器采用MoA机制动态选择最优注意力头组合，解码器采用SHMoE机制以不同核大小的专家预测三个方向的变形场。

Result: 在两个公开数据集上，SHMoAReg均表现出优于现有方法的性能，腹部CT数据集的Dice分数从60.58%提升至65.58%。

Conclusion: SHMoAReg通过引入Mixture of Experts（MoE）机制，在编码器和解码器中分别采用Mixture of Attention heads（MoA）和Spatial Heterogeneous Mixture of Experts（SHMoE），显著提升了可变形图像配准（DIR）的性能和可解释性。实验结果表明，该方法在多个数据集上均优于现有方法，特别是在腹部CT数据集上Dice分数从60.58%提升至65.58%。

Abstract: Encoder-Decoder architectures are widely used in deep learning-based
Deformable Image Registration (DIR), where the encoder extracts multi-scale
features and the decoder predicts deformation fields by recovering spatial
locations. However, current methods lack specialized extraction of features
(that are useful for registration) and predict deformation jointly and
homogeneously in all three directions. In this paper, we propose a novel
expert-guided DIR network with Mixture of Experts (MoE) mechanism applied in
both encoder and decoder, named SHMoAReg. Specifically, we incorporate Mixture
of Attention heads (MoA) into encoder layers, while Spatial Heterogeneous
Mixture of Experts (SHMoE) into the decoder layers. The MoA enhances the
specialization of feature extraction by dynamically selecting the optimal
combination of attention heads for each image token. Meanwhile, the SHMoE
predicts deformation fields heterogeneously in three directions for each voxel
using experts with varying kernel sizes. Extensive experiments conducted on two
publicly available datasets show consistent improvements over various methods,
with a notable increase from 60.58% to 65.58% in Dice score for the abdominal
CT dataset. Furthermore, SHMoAReg enhances model interpretability by
differentiating experts' utilities across/within different resolution layers.
To the best of our knowledge, we are the first to introduce MoE mechanism into
DIR tasks. The code will be released soon.

</details>


### [58] [Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing](https://arxiv.org/abs/2509.20091)
*Zizheng Yang,Hu Yu,Bing Li,Jinghao Zhang,Jie Huang,Feng Zhao*

Main category: cs.CV

TL;DR: DiffLI$^2$D利用预训练扩散模型的潜在表示，避免了重新训练和迭代采样，实现了高效的图像去雾。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像去雾中因计算负担大和采样步骤多而应用受限。

Method: 提出DiffLI$^2$D网络，利用预训练扩散模型在不同时间步的潜在表示来指导图像去雾。

Result: DiffLI$^2$D在多个数据集上表现出优越性能。

Conclusion: DiffLI$^2$D通过利用预训练扩散模型的潜在表示，避免了重新训练和迭代采样过程，在多个数据集上实现了优于现有图像去雾方法的性能。

Abstract: Diffusion models have recently been investigated as powerful generative
solvers for image dehazing, owing to their remarkable capability to model the
data distribution. However, the massive computational burden imposed by the
retraining of diffusion models, coupled with the extensive sampling steps
during the inference, limit the broader application of diffusion models in
image dehazing. To address these issues, we explore the properties of hazy
images in the semantic latent space of frozen pre-trained diffusion models, and
propose a Diffusion Latent Inspired network for Image Dehazing, dubbed
DiffLI$^2$D. Specifically, we first reveal that the semantic latent space of
pre-trained diffusion models can represent the content and haze characteristics
of hazy images, as the diffusion time-step changes. Building upon this insight,
we integrate the diffusion latent representations at different time-steps into
a delicately designed dehazing network to provide instructions for image
dehazing. Our DiffLI$^2$D avoids re-training diffusion models and iterative
sampling process by effectively utilizing the informative representations
derived from the pre-trained diffusion models, which also offers a novel
perspective for introducing diffusion models to image dehazing. Extensive
experiments on multiple datasets demonstrate that the proposed method achieves
superior performance to existing image dehazing methods. Code is available at
https://github.com/aaaasan111/difflid.

</details>


### [59] [ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression](https://arxiv.org/abs/2509.20234)
*Tom Burgert,Oliver Stoll,Paolo Rota,Begüm Demir*

Main category: cs.CV

TL;DR: 研究表明CNN并非固有纹理偏好，而是局部形状依赖，且不同领域模型特征依赖模式各异。


<details>
  <summary>Details</summary>
Motivation: 重新审视CNN固有纹理偏好的假设，探讨Geirhos等人实验的局限性。

Method: 提出了一种领域无关框架，通过系统性地抑制形状、纹理和颜色线索来量化特征依赖，避免了强制选择冲突的混淆。

Result: 发现CNN主要依赖局部形状特征，且不同领域模型对特征的依赖模式不同：计算机视觉模型优先形状，医学影像模型强调颜色，遥感模型更依赖纹理。

Conclusion: 研究发现CNN并非固有纹理偏好，而是主要依赖局部形状特征，且通过现代训练策略或架构可显著缓解。不同领域模型对特征的依赖模式存在系统性差异。

Abstract: The hypothesis that Convolutional Neural Networks (CNNs) are inherently
texture-biased has shaped much of the discourse on feature use in deep
learning. We revisit this hypothesis by examining limitations in the
cue-conflict experiment by Geirhos et al. To address these limitations, we
propose a domain-agnostic framework that quantifies feature reliance through
systematic suppression of shape, texture, and color cues, avoiding the
confounds of forced-choice conflicts. By evaluating humans and neural networks
under controlled suppression conditions, we find that CNNs are not inherently
texture-biased but predominantly rely on local shape features. Nonetheless,
this reliance can be substantially mitigated through modern training strategies
or architectures (ConvNeXt, ViTs). We further extend the analysis across
computer vision, medical imaging, and remote sensing, revealing that reliance
patterns differ systematically: computer vision models prioritize shape,
medical imaging models emphasize color, and remote sensing models exhibit a
stronger reliance towards texture. Code is available at
https://github.com/tomburgert/feature-reliance.

</details>


### [60] [A Simple Data Augmentation Strategy for Text-in-Image Scientific VQA](https://arxiv.org/abs/2509.20119)
*Belal Shoer,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 研究通过合成‘文本嵌入图像’格式数据集并微调模型，显著提升了科学视觉问答的跨语言性能。


<details>
  <summary>Details</summary>
Motivation: 科学视觉问答对视觉-语言模型提出了挑战，现有方法在零样本设置下表现不佳，且缺乏‘文本嵌入图像’格式的训练数据。

Method: 将现有的独立图像-文本对转换为统一图像格式，合成新的数据集，并在此基础上微调小型多语言多模态模型。

Result: 在混合合成数据和EXAMS-V上微调的模型在13种语言中表现出显著的性能提升和跨语言迁移能力。

Conclusion: 通过合成数据集和微调多语言多模态模型，研究在科学视觉问答任务中取得了显著的跨语言性能提升。

Abstract: Scientific visual question answering poses significant challenges for
vision-language models due to the complexity of scientific figures and their
multimodal context. Traditional approaches treat the figure and accompanying
text (e.g., questions and answer options) as separate inputs. EXAMS-V
introduced a new paradigm by embedding both visual and textual content into a
single image. However, even state-of-the-art proprietary models perform poorly
on this setup in zero-shot settings, underscoring the need for task-specific
fine-tuning. To address the scarcity of training data in this "text-in-image"
format, we synthesize a new dataset by converting existing separate image-text
pairs into unified images. Fine-tuning a small multilingual multimodal model on
a mix of our synthetic data and EXAMS-V yields notable gains across 13
languages, demonstrating strong average improvements and cross-lingual
transfer.

</details>


### [61] [Smaller is Better: Enhancing Transparency in Vehicle AI Systems via Pruning](https://arxiv.org/abs/2509.20148)
*Sanish Suwal,Shaurya Garg,Dipkamal Bhusal,Michael Clifford,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 研究表明，剪枝能有效提升交通标志分类器后解释的质量和可靠性，是增强AI模型透明度的有前景方法。


<details>
  <summary>Details</summary>
Motivation: 由于后解释在透明度和可靠性方面存在不一致性和缺乏忠实性的问题，研究如何提升AI模型解释的质量和可靠性至关重要。

Method: 本文系统地研究了三种常用训练方法（自然训练、对抗训练和剪枝）对交通标志分类器后解释质量的影响。

Result: 通过广泛的实证评估，发现剪枝显著提升了显著性图的可理解性和忠实性。

Conclusion: 剪枝不仅提高了模型效率，还通过强制稀疏性增强了学习表示的可解释性和可靠性，是开发透明深度学习模型的有前景策略。

Abstract: Connected and autonomous vehicles continue to heavily rely on AI systems,
where transparency and security are critical for trust and operational safety.
Post-hoc explanations provide transparency to these black-box like AI models
but the quality and reliability of these explanations is often questioned due
to inconsistencies and lack of faithfulness in representing model decisions.
This paper systematically examines the impact of three widely used training
approaches, namely natural training, adversarial training, and pruning, affect
the quality of post-hoc explanations for traffic sign classifiers. Through
extensive empirical evaluation, we demonstrate that pruning significantly
enhances the comprehensibility and faithfulness of explanations (using saliency
maps). Our findings reveal that pruning not only improves model efficiency but
also enforces sparsity in learned representation, leading to more interpretable
and reliable decisions. Additionally, these insights suggest that pruning is a
promising strategy for developing transparent deep learning models, especially
in resource-constrained vehicular AI systems.

</details>


### [62] [C$^2$MIL: Synchronizing Semantic and Topological Causalities in Multiple Instance Learning for Robust and Interpretable Survival Analysis](https://arxiv.org/abs/2509.20152)
*Min Cen,Zhenfeng Zhuang,Yuzhe Zhang,Min Zeng,Baptiste Magnier,Lequan Yu,Hong Zhang,Liansheng Wang*

Main category: cs.CV

TL;DR: C$^2$MIL是一种新型双因果图MIL模型，通过特征解缠和因果子图采样改善语义和拓扑因果性，提升泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决H&E染色的WSI中因染色和扫描变化导致的语义偏差以及拓扑子图噪声问题，这些问题会影响分析的泛化性和可解释性。

Method: 提出了一种新颖且可解释的双因果图MIL模型C$^2$MIL，包括跨尺度自适应特征解缠模块和Bernoulli可微分因果子图采样方法，结合解缠监督和对比学习的联合优化策略。

Result: 实验证明C$^2$MIL在泛化性和可解释性上优于现有方法，并能作为多种MIL基线的因果增强。

Conclusion: C$^2$MIL通过引入双结构因果模型和联合优化策略，显著提高了基于图的MIL模型的泛化性和可解释性，可作为多种MIL基线的因果增强方法。

Abstract: Graph-based Multiple Instance Learning (MIL) is widely used in survival
analysis with Hematoxylin and Eosin (H\&E)-stained whole slide images (WSIs)
due to its ability to capture topological information. However, variations in
staining and scanning can introduce semantic bias, while topological subgraphs
that are not relevant to the causal relationships can create noise, resulting
in biased slide-level representations. These issues can hinder both the
interpretability and generalization of the analysis. To tackle this, we
introduce a dual structural causal model as the theoretical foundation and
propose a novel and interpretable dual causal graph-based MIL model, C$^2$MIL.
C$^2$MIL incorporates a novel cross-scale adaptive feature disentangling module
for semantic causal intervention and a new Bernoulli differentiable causal
subgraph sampling method for topological causal discovery. A joint optimization
strategy combining disentangling supervision and contrastive learning enables
simultaneous refinement of both semantic and topological causalities.
Experiments demonstrate that C$^2$MIL consistently improves generalization and
interpretability over existing methods and can serve as a causal enhancement
for diverse MIL baselines. The code is available at
https://github.com/mimic0127/C2MIL.

</details>


### [63] [Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research](https://arxiv.org/abs/2509.20171)
*Patricia Schöntag,David Nakath,Judith Fischer,Rüdiger Röttgers,Kevin Köser*

Main category: cs.CV

TL;DR: 论文提出“光学海洋配方”框架，通过受控环境生成真实水下数据集，解决机器视觉评估的通用性问题，并展示其应用效果。


<details>
  <summary>Details</summary>
Motivation: 水下机器视觉的开发与评估面临光学挑战（如颜色失真、对比度降低和动态光模式），且缺乏通用性强的测试环境。

Method: 通过使用校准的颜色和散射添加剂，创建可重复和可控的水下图像数据集，以模拟不同水组成对图像外观的影响。

Result: 研究团队提供了一个演示数据集，并展示了该系统在两种水下视觉任务中的应用，证明了其有效性。

Conclusion: 该论文提出了一种名为“光学海洋配方”的框架，用于在受控的水下环境中创建真实数据集，解决了水下机器视觉评估缺乏通用性和可控性的问题。

Abstract: The development and evaluation of machine vision in underwater environments
remains challenging, often relying on trial-and-error-based testing tailored to
specific applications. This is partly due to the lack of controlled,
ground-truthed testing environments that account for the optical challenges,
such as color distortion from spectrally variant light attenuation, reduced
contrast and blur from backscatter and volume scattering, and dynamic light
patterns from natural or artificial illumination. Additionally, the appearance
of ocean water in images varies significantly across regions, depths, and
seasons. However, most machine vision evaluations are conducted under specific
optical water types and imaging conditions, therefore often lack
generalizability. Exhaustive testing across diverse open-water scenarios is
technically impractical. To address this, we introduce the \textit{Optical
Ocean Recipes}, a framework for creating realistic datasets under controlled
underwater conditions. Unlike synthetic or open-water data, these recipes,
using calibrated color and scattering additives, enable repeatable and
controlled testing of the impact of water composition on image appearance.
Hence, this provides a unique framework for analyzing machine vision in
realistic, yet controlled underwater scenarios. The controlled environment
enables the creation of ground-truth data for a range of vision tasks,
including water parameter estimation, image restoration, segmentation, visual
SLAM, and underwater image synthesis. We provide a demonstration dataset
generated using the Optical Ocean Recipes and briefly demonstrate the use of
our system for two underwater vision tasks. The dataset and evaluation code
will be made available.

</details>


### [64] [Universal Camouflage Attack on Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2509.20196)
*Dehong Kong,Sifan Yu,Siyuan Liang,Jiawei Liang,Jianhou Gan,Aishan Liu,Wenqi Ren*

Main category: cs.CV

TL;DR: 提出首个针对VLM-AD的通用伪装攻击框架UCA，通过特征空间操作和多尺度学习策略，显著提升攻击效果和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: VLM-AD尽管具有先进的推理能力，但仍面临来自对抗攻击的严重安全威胁。现有攻击方法主要针对视觉模块或数字层面，难以直接应用于VLM-AD系统。

Method: UCA在特征空间中操作，生成物理可实现的伪装纹理，引入特征差异损失（FDL）最大化干净图像和对抗图像之间的表示差异，并结合多尺度学习策略和调整采样比例以增强对现实场景中尺度和视角多样性的适应性。

Result: UCA在多种VLM-AD模型和驾驶场景中均能诱导错误的驾驶命令，显著优于现有攻击方法（在3-P指标上提升30%）。

Conclusion: UCA框架在多种VLM-AD模型和驾驶场景中均能诱导错误的驾驶命令，显著优于现有攻击方法（在3-P指标上提升30%），并在多样视角和动态条件下表现出强大的攻击鲁棒性，具有实际部署的高潜力。

Abstract: Visual language modeling for automated driving is emerging as a promising
research direction with substantial improvements in multimodal reasoning
capabilities. Despite its advanced reasoning abilities, VLM-AD remains
vulnerable to serious security threats from adversarial attacks, which involve
misleading model decisions through carefully crafted perturbations. Existing
attacks have obvious challenges: 1) Physical adversarial attacks primarily
target vision modules. They are difficult to directly transfer to VLM-AD
systems because they typically attack low-level perceptual components. 2)
Adversarial attacks against VLM-AD have largely concentrated on the digital
level. To address these challenges, we propose the first Universal Camouflage
Attack (UCA) framework for VLM-AD. Unlike previous methods that focus on
optimizing the logit layer, UCA operates in the feature space to generate
physically realizable camouflage textures that exhibit strong generalization
across different user commands and model architectures. Motivated by the
observed vulnerability of encoder and projection layers in VLM-AD, UCA
introduces a feature divergence loss (FDL) that maximizes the representational
discrepancy between clean and adversarial images. In addition, UCA incorporates
a multi-scale learning strategy and adjusts the sampling ratio to enhance its
adaptability to changes in scale and viewpoint diversity in real-world
scenarios, thereby improving training stability. Extensive experiments
demonstrate that UCA can induce incorrect driving commands across various
VLM-AD models and driving scenarios, significantly surpassing existing
state-of-the-art attack methods (improving 30\% in 3-P metrics). Furthermore,
UCA exhibits strong attack robustness under diverse viewpoints and dynamic
conditions, indicating high potential for practical deployment.

</details>


### [65] [PU-Gaussian: Point Cloud Upsampling using 3D Gaussian Representation](https://arxiv.org/abs/2509.20207)
*Mahmoud Khater,Mona Strauss,Philipp von Olshausen,Alexander Reiterer*

Main category: cs.CV

TL;DR: PU-Gaussian利用3D高斯分布显式上采样点云，结合细化网络，显著提升质量并在基准测试中领先。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在几何可解释性或输入稀疏性鲁棒性方面的不足，提升稀疏和噪声点云的上采样质量。

Method: 提出PU-Gaussian，一种新型上采样网络，利用各向异性3D高斯分布建模每个点的局部邻域，通过直接点采样在局部几何域中显式执行上采样，然后通过细化网络调整输出。

Result: 在PU1K和PUGAN数据集上实现了最先进的性能。

Conclusion: PU-Gaussian通过显式地在局部几何域中进行上采样，结合后续的细化网络，显著提升了点云的上采样质量，并在PU1K和PUGAN数据集上实现了最先进的性能。

Abstract: Point clouds produced by 3D sensors are often sparse and noisy, posing
challenges for tasks requiring dense and high-fidelity 3D representations.
Prior work has explored both implicit feature-based upsampling and
distance-function learning to address this, but often at the expense of
geometric interpretability or robustness to input sparsity. To overcome these
limitations, we propose PU-Gaussian, a novel upsampling network that models the
local neighborhood around each point using anisotropic 3D Gaussian
distributions. These Gaussians capture the underlying geometric structure,
allowing us to perform upsampling explicitly in the local geometric domain by
direct point sampling. The sampling process generates a dense, but coarse,
point cloud. A subsequent refinement network adjusts the coarse output to
produce a more uniform distribution and sharper edges. We perform extensive
testing on the PU1K and PUGAN datasets, demonstrating that PU-Gaussian achieves
state-of-the-art performance. We make code and model weights publicly available
at https://github.com/mvg-inatech/PU-Gaussian.git.

</details>


### [66] [An Anisotropic Cross-View Texture Transfer with Multi-Reference Non-Local Attention for CT Slice Interpolation](https://arxiv.org/abs/2509.20242)
*Kwang-Hyun Uhm,Hyunjun Cho,Sung-Hoo Hong,Seung-Won Jung*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的跨视图纹理转移方法，用于CT切片插值，通过充分利用3D CT体积的各向异性特性，显著提高了插值效果。


<details>
  <summary>Details</summary>
Motivation: 由于CT图像通常以大切片厚度采集，导致各向异性的CT体积，这种分辨率不一致可能增加疾病诊断的难度，因此需要开发深度学习方法以提高穿平面分辨率。

Method: 设计了一个独特的框架，利用高分辨率的平面内纹理细节作为参考，将其转移到低分辨率的穿平面图像中，并引入了一个多参考非局部注意力模块来提取有意义的特征。

Result: 在公共CT数据集上的广泛实验表明，该方法在CT切片插值中表现显著优于现有竞争方法。

Conclusion: 提出的跨视图纹理转移方法在CT切片插值中表现显著优于现有竞争方法，验证了该框架的有效性。

Abstract: Computed tomography (CT) is one of the most widely used non-invasive imaging
modalities for medical diagnosis. In clinical practice, CT images are usually
acquired with large slice thicknesses due to the high cost of memory storage
and operation time, resulting in an anisotropic CT volume with much lower
inter-slice resolution than in-plane resolution. Since such inconsistent
resolution may lead to difficulties in disease diagnosis, deep learning-based
volumetric super-resolution methods have been developed to improve inter-slice
resolution. Most existing methods conduct single-image super-resolution on the
through-plane or synthesize intermediate slices from adjacent slices; however,
the anisotropic characteristic of 3D CT volume has not been well explored. In
this paper, we propose a novel cross-view texture transfer approach for CT
slice interpolation by fully utilizing the anisotropic nature of 3D CT volume.
Specifically, we design a unique framework that takes high-resolution in-plane
texture details as a reference and transfers them to low-resolution
through-plane images. To this end, we introduce a multi-reference non-local
attention module that extracts meaningful features for reconstructing
through-plane high-frequency details from multiple in-plane images. Through
extensive experiments, we demonstrate that our method performs significantly
better in CT slice interpolation than existing competing methods on public CT
datasets including a real-paired benchmark, verifying the effectiveness of the
proposed framework. The source code of this work is available at
https://github.com/khuhm/ACVTT.

</details>


### [67] [4D Driving Scene Generation With Stereo Forcing](https://arxiv.org/abs/2509.20251)
*Hao Lu,Zhuang Ma,Guangfeng Jiang,Wenhang Ge,Bohan Li,Yuzhan Cai,Wenzhao Zheng,Yunpeng Zhang,Yingcong Chen*

Main category: cs.CV

TL;DR: PhiGenesis是一个统一的4D场景生成框架，通过两阶段方法结合几何和时序一致性，显著提升动态场景的综合能力。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型难以在无需逐场景优化的情况下合成支持时空外推和新视图合成的动态4D驾驶场景，PhiGenesis旨在解决这一挑战。

Method: PhiGenesis采用两阶段方法：首先利用预训练的视频VAE和范围视图适配器进行4D重建；其次引入几何指导的视频扩散模型，结合Stereo Forcing策略增强新视图生成。

Result: 实验结果表明，PhiGenesis在外观和几何重建、时序生成及新视图合成任务中均达到最先进性能。

Conclusion: PhiGenesis框架在4D场景生成中实现了时空一致性和几何指导，显著提升了动态场景的综合能力，并在多个任务中达到了最先进的性能。

Abstract: Current generative models struggle to synthesize dynamic 4D driving scenes
that simultaneously support temporal extrapolation and spatial novel view
synthesis (NVS) without per-scene optimization. Bridging generation and novel
view synthesis remains a major challenge. We present PhiGenesis, a unified
framework for 4D scene generation that extends video generation techniques with
geometric and temporal consistency. Given multi-view image sequences and camera
parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting
representations along target 3D trajectories. In its first stage, PhiGenesis
leverages a pre-trained video VAE with a novel range-view adapter to enable
feed-forward 4D reconstruction from multi-view images. This architecture
supports single-frame or video inputs and outputs complete 4D scenes including
geometry, semantics, and motion. In the second stage, PhiGenesis introduces a
geometric-guided video diffusion model, using rendered historical 4D scenes as
priors to generate future views conditioned on trajectories. To address
geometric exposure bias in novel views, we propose Stereo Forcing, a novel
conditioning strategy that integrates geometric uncertainty during denoising.
This method enhances temporal coherence by dynamically adjusting generative
influence based on uncertainty-aware perturbations. Our experimental results
demonstrate that our method achieves state-of-the-art performance in both
appearance and geometric reconstruction, temporal generation and novel view
synthesis (NVS) tasks, while simultaneously delivering competitive performance
in downstream evaluations. Homepage is at
\href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}.

</details>


### [68] [A Versatile Foundation Model for AI-enabled Mammogram Interpretation](https://arxiv.org/abs/2509.20271)
*Fuxiang Huang,Jiayi Zhu,Yunfang Yu,Yu Xie,Yuan Guo,Qingcong Kong,Mingxiang Wu,Xinrui Jiang,Shu Yang,Jiabo Ma,Ziyi Liu,Zhe Xu,Zhixuan Chen,Yujie Tan,Zifan He,Luhui Mao,Xi Wang,Junlin Hou,Lei Zhang,Qiong Luo,Zhenhui Li,Herui Yao,Hao Chen*

Main category: cs.CV

TL;DR: VersaMammo, a versatile mammogram foundation model, overcomes limitations in current models by using a two-stage pre-training strategy and achieves top performance in clinical tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in foundation models for mammogram analysis, including insufficient training data diversity, limited generalizability, and lack of comprehensive evaluation.

Method: A two-stage pre-training strategy involving self-supervised learning for a teacher model, followed by supervised learning combined with knowledge distillation to develop VersaMammo.

Result: VersaMammo achieves state-of-the-art performance, ranking first in 50 out of 68 internal tasks and 20 out of 24 external validation tasks.

Conclusion: VersaMammo demonstrates superior generalization and clinical utility, advancing reliable and scalable breast cancer screening and diagnosis.

Abstract: Breast cancer is the most commonly diagnosed cancer and the leading cause of
cancer-related mortality in women globally. Mammography is essential for the
early detection and diagnosis of breast lesions. Despite recent progress in
foundation models (FMs) for mammogram analysis, their clinical translation
remains constrained by several fundamental limitations, including insufficient
diversity in training data, limited model generalizability, and a lack of
comprehensive evaluation across clinically relevant tasks. Here, we introduce
VersaMammo, a versatile foundation model for mammograms, designed to overcome
these limitations. We curated the largest multi-institutional mammogram dataset
to date, comprising 706,239 images from 21 sources. To improve generalization,
we propose a two-stage pre-training strategy to develop VersaMammo, a mammogram
foundation model. First, a teacher model is trained via self-supervised
learning to extract transferable features from unlabeled mammograms. Then,
supervised learning combined with knowledge distillation transfers both
features and clinical knowledge into VersaMammo. To ensure a comprehensive
evaluation, we established a benchmark comprising 92 specific tasks, including
68 internal tasks and 24 external validation tasks, spanning 5 major clinical
task categories: lesion detection, segmentation, classification, image
retrieval, and visual question answering. VersaMammo achieves state-of-the-art
performance, ranking first in 50 out of 68 specific internal tasks and 20 out
of 24 external validation tasks, with average ranks of 1.5 and 1.2,
respectively. These results demonstrate its superior generalization and
clinical utility, offering a substantial advancement toward reliable and
scalable breast cancer screening and diagnosis.

</details>


### [69] [A co-evolving agentic AI system for medical imaging analysis](https://arxiv.org/abs/2509.20279)
*Songhao Li,Jonathan Xu,Tiancheng Bao,Yuxuan Liu,Yuchen Liu,Yihang Liu,Lilin Wang,Wenhui Lei,Sheng Wang,Yinuo Xu,Yan Cui,Jialu Yao,Shunsuke Koga,Zhi Huang*

Main category: cs.CV

TL;DR: TissueLab是一个协同进化的代理AI系统，通过整合多领域工具工厂和实时专家反馈，提升了医疗影像分析的性能和实用性，并在临床任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 医疗影像分析中，AI的性能和采用率因缺乏强大的生态系统、工具集不足以及缺乏实时交互专家反馈而受限。

Method: TissueLab通过整合病理学、放射学和空间组学领域的工具工厂，标准化输入、输出和工具能力，从而确定何时以及如何调用这些工具来解决研究和临床问题。

Result: TissueLab在多样化的临床量化任务中，相比端到端视觉语言模型（VLMs）和其他代理AI系统（如GPT-5），实现了最先进的性能，并能通过主动学习在未见疾病情境下快速提供准确结果。

Conclusion: TissueLab作为一个可持续的开源生态系统，旨在加速医学影像的计算研究和转化应用，同时为下一代医疗AI奠定基础。

Abstract: Agentic AI is rapidly advancing in healthcare and biomedical research.
However, in medical image analysis, their performance and adoption remain
limited due to the lack of a robust ecosystem, insufficient toolsets, and the
absence of real-time interactive expert feedback. Here we present "TissueLab",
a co-evolving agentic AI system that allows researchers to ask direct
questions, automatically plan and generate explainable workflows, and conduct
real-time analyses where experts can visualize intermediate results and refine
them. TissueLab integrates tool factories across pathology, radiology, and
spatial omics domains. By standardizing inputs, outputs, and capabilities of
diverse tools, the system determines when and how to invoke them to address
research and clinical questions. Across diverse tasks with clinically
meaningful quantifications that inform staging, prognosis, and treatment
planning, TissueLab achieves state-of-the-art performance compared with
end-to-end vision-language models (VLMs) and other agentic AI systems such as
GPT-5. Moreover, TissueLab continuously learns from clinicians, evolving toward
improved classifiers and more effective decision strategies. With active
learning, it delivers accurate results in unseen disease contexts within
minutes, without requiring massive datasets or prolonged retraining. Released
as a sustainable open-source ecosystem, TissueLab aims to accelerate
computational research and translational adoption in medical imaging while
establishing a foundation for the next generation of medical AI.

</details>


### [70] [HiPerformer: A High-Performance Global-Local Segmentation Model with Modular Hierarchical Fusion Strategy](https://arxiv.org/abs/2509.20280)
*Dayu Tan,Zhenpeng Xu,Yansen Su,Xin Peng,Chunhou Zheng,Weimin Zhong*

Main category: cs.CV

TL;DR: HiPerformer通过动态并行融合和LGFF模块，解决了医学图像分割中的特征不一致问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN-Transformer混合架构的方法在特征融合上存在简单堆叠或拼接的问题，导致特征不一致和信息冲突。

Method: HiPerformer采用模块化层次架构动态并行融合多源特征，设计了LGFF模块实现局部细节与全局语义信息的精确融合，并提出了PPA模块替代传统跳跃连接。

Result: 在11个公共数据集上的实验表明，HiPerformer在分割准确性和鲁棒性上优于现有技术。

Conclusion: HiPerformer通过创新的模块化层次架构和LGFF模块，有效解决了医学图像分割中特征不一致和信息丢失的问题，实验证明其在多个公共数据集上优于现有分割技术。

Abstract: Both local details and global context are crucial in medical image
segmentation, and effectively integrating them is essential for achieving high
accuracy. However, existing mainstream methods based on CNN-Transformer hybrid
architectures typically employ simple feature fusion techniques such as serial
stacking, endpoint concatenation, or pointwise addition, which struggle to
address the inconsistencies between features and are prone to information
conflict and loss. To address the aforementioned challenges, we innovatively
propose HiPerformer. The encoder of HiPerformer employs a novel modular
hierarchical architecture that dynamically fuses multi-source features in
parallel, enabling layer-wise deep integration of heterogeneous information.
The modular hierarchical design not only retains the independent modeling
capability of each branch in the encoder, but also ensures sufficient
information transfer between layers, effectively avoiding the degradation of
features and information loss that come with traditional stacking methods.
Furthermore, we design a Local-Global Feature Fusion (LGFF) module to achieve
precise and efficient integration of local details and global semantic
information, effectively alleviating the feature inconsistency problem and
resulting in a more comprehensive feature representation. To further enhance
multi-scale feature representation capabilities and suppress noise
interference, we also propose a Progressive Pyramid Aggregation (PPA) module to
replace traditional skip connections. Experiments on eleven public datasets
demonstrate that the proposed method outperforms existing segmentation
techniques, demonstrating higher segmentation accuracy and robustness. The code
is available at https://github.com/xzphappy/HiPerformer.

</details>


### [71] [PerFace: Metric Learning in Perceptual Facial Similarity for Enhanced Face Anonymization](https://arxiv.org/abs/2509.20281)
*Haruka Kumagai,Leslie Wöhler,Satoshi Ikehata,Kiyoharu Aizawa*

Main category: cs.CV

TL;DR: 本文提出了一种基于人类感知的面部相似度度量方法，通过创建数据集和度量学习，显著提升了面部相似度预测和分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的模型仅关注二元身份分类（“同一人或不是”），难以衡量细微的相似度差异，如“完全不同”与“高度相似但不同”。

Method: 创建了一个包含6,400个三元组标注的数据集，并采用度量学习来预测相似度。

Result: 实验结果表明，该方法在面部相似度预测和基于属性的面部分类任务上均优于现有方法。

Conclusion: 本文提出了一种基于人类感知的面部相似度度量方法，显著提升了面部相似度预测和基于属性的面部分类任务的性能。

Abstract: In response to rising societal awareness of privacy concerns, face
anonymization techniques have advanced, including the emergence of
face-swapping methods that replace one identity with another. Achieving a
balance between anonymity and naturalness in face swapping requires careful
selection of identities: overly similar faces compromise anonymity, while
dissimilar ones reduce naturalness. Existing models, however, focus on binary
identity classification "the same person or not", making it difficult to
measure nuanced similarities such as "completely different" versus "highly
similar but different." This paper proposes a human-perception-based face
similarity metric, creating a dataset of 6,400 triplet annotations and metric
learning to predict the similarity. Experimental results demonstrate
significant improvements in both face similarity prediction and attribute-based
face classification tasks over existing methods.

</details>


### [72] [FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis](https://arxiv.org/abs/2509.20295)
*Xichen Xu,Yanshu Wang,Jinbao Wang,Xiaoning Lei,Guoyang Xie,Guannan Jiang,Zhichao Lu*

Main category: cs.CV

TL;DR: FAST框架通过AIAS和FARM模块高效合成工业异常，显著提升分割性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 工业异常分割依赖像素级标注，但真实异常稀缺、多样且标注成本高。现有合成方法在采样效率和生成质量之间难以平衡，且忽视异常与背景区域的统计差异。

Method: 提出了FAST框架，包含Anomaly-Informed Accelerated Sampling (AIAS)和Foreground-Aware Reconstruction Module (FARM)两个模块，用于高效合成可控、结构特定的异常。

Result: FAST在多个工业基准测试中表现优异，AIAS能在10步内合成高质量的异常，FARM能有效保留局部异常信号。

Conclusion: FAST框架通过AIAS和FARM模块显著提升了工业异常分割任务的性能，并在多个工业基准测试中表现优于现有方法。

Abstract: Industrial anomaly segmentation relies heavily on pixel-level annotations,
yet real-world anomalies are often scarce, diverse, and costly to label.
Segmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a
promising alternative; however, existing methods struggle to balance sampling
efficiency and generation quality. Moreover, most approaches treat all spatial
regions uniformly, overlooking the distinct statistical differences between
anomaly and background areas. This uniform treatment hinders the synthesis of
controllable, structure-specific anomalies tailored for segmentation tasks. In
this paper, we propose FAST, a foreground-aware diffusion framework featuring
two novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the
Foreground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling
algorithm specifically designed for segmentation-oriented industrial anomaly
synthesis, which accelerates the reverse process through coarse-to-fine
aggregation and enables the synthesis of state-of-the-art segmentation-oriented
anomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the
anomaly-aware noise within the masked foreground regions at each sampling step,
preserving localized anomaly signals throughout the denoising trajectory.
Extensive experiments on multiple industrial benchmarks demonstrate that FAST
consistently outperforms existing anomaly synthesis methods in downstream
segmentation tasks. We release the code at:
https://anonymous.4open.science/r/NeurIPS-938.

</details>


### [73] [A Comprehensive Evaluation of YOLO-based Deer Detection Performance on Edge Devices](https://arxiv.org/abs/2509.20318)
*Bishal Adhikari,Jiajia Li,Eric S. Michel,Jacob Dykes,Te-Ming Paul Tseng,Mary Love Tagert,Dong Chen*

Main category: cs.CV

TL;DR: 本研究填补了农业鹿类检测领域的数据集空白，评估了多种YOLO模型在边缘计算平台上的性能，为实际应用提供了高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统农业中的鹿类入侵导致巨额经济损失，而现有缓解策略成本高、效果差。缺乏特定领域的数据集和实际部署研究阻碍了智能、自主解决方案的发展。

Method: 本研究评估了四种最新的YOLO架构（v8、v9、v10和v11）的12种模型变体，并使用高端NVIDIA RTX 5090 GPU和两种边缘计算平台（Raspberry Pi 5和NVIDIA Jetson AGX Xavier）进行性能测试。

Result: 研究表明，在未进行硬件优化的Raspberry Pi上无法实现实时检测，而NVIDIA Jetson在GPU加速下可实现超过30 FPS。YOLOv11n、YOLOv8s和YOLOv9s等小型先进模型在高精度（AP@.5 > 0.85）和计算效率（FPS > 30）之间取得了最佳平衡。

Conclusion: 本研究通过引入一个公开可用的数据集，并对多种YOLO架构进行广泛比较，为农业中的鹿类检测提供了高效、准确的解决方案。研究还评估了不同边缘计算平台的性能，为实际应用提供了参考。

Abstract: The escalating economic losses in agriculture due to deer intrusion,
estimated to be in the hundreds of millions of dollars annually in the U.S.,
highlight the inadequacy of traditional mitigation strategies since these
methods are often labor-intensive, costly, and ineffective for modern farming
systems. To overcome this, there is a critical need for intelligent, autonomous
solutions which require accurate and efficient deer detection. But the progress
in this field is impeded by a significant gap in the literature, mainly the
lack of a domain-specific, practical dataset and limited study on the on-field
deployability of deer detection systems. Addressing this gap, this study
presents a comprehensive evaluation of state-of-the-art deep learning models
for deer detection in challenging real-world scenarios. The contributions of
this work are threefold. First, we introduce a curated, publicly available
dataset of 3,095 annotated images with bounding-box annotations of deer,
derived from the Idaho Cameratraps project. Second, we provide an extensive
comparative analysis of 12 model variants across four recent YOLO
architectures(v8, v9, v10, and v11). Finally, we benchmarked performance on a
high-end NVIDIA RTX 5090 GPU and evaluated on two representative edge computing
platforms: Raspberry Pi 5 and NVIDIA Jetson AGX Xavier. Results show that the
real-time detection is not feasible in Raspberry Pi without hardware-specific
model optimization, while NVIDIA Jetson provides greater than 30 FPS with
GPU-accelerated inference on 's' and 'n' series models. This study also reveals
that smaller, architecturally advanced models such as YOLOv11n, YOLOv8s, and
YOLOv9s offer the optimal balance of high accuracy (AP@.5 > 0.85) and
computational efficiency (FPS > 30). To support further research, both the
source code and datasets are publicly available at
https://github.com/WinnerBishal/track-the-deer.

</details>


### [74] [Efficient Encoder-Free Pose Conditioning and Pose Control for Virtual Try-On](https://arxiv.org/abs/2509.20343)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 该论文研究了在VTON模型中融入姿态控制的方法，通过空间拼接姿态数据（姿态图优于骨架）和混合掩码训练策略，提升了姿态保持和产品整合的灵活性，无需增加额外参数。


<details>
  <summary>Details</summary>
Motivation: 随着在线购物的增长，虚拟试穿（VTON）技术需求激增，但姿态控制是关键挑战，需确保产品与用户身体的准确对齐，同时支持多样化的姿态以提升沉浸感。

Method: 在基线VTON模型的基础上，研究了通过空间拼接姿态数据（姿态图和骨架）来融入姿态控制的方法，无需额外参数或模块。同时引入了混合掩码训练策略，结合细粒度掩码和边界框掩码。

Result: 实验表明，使用姿态图进行姿态拼接效果最佳，提升了姿态保持和输出真实感。混合掩码训练策略进一步增强了模型在不同条件下的灵活性。

Conclusion: 通过空间拼接姿态数据，特别是使用姿态图（pose maps），在无需增加额外参数或模块的情况下，显著提升了VTON模型的姿态保持能力和输出真实感。此外，混合掩码训练策略进一步增强了模型在不同姿态和条件下的产品整合灵活性。

Abstract: As online shopping continues to grow, the demand for Virtual Try-On (VTON)
technology has surged, allowing customers to visualize products on themselves
by overlaying product images onto their own photos. An essential yet
challenging condition for effective VTON is pose control, which ensures
accurate alignment of products with the user's body while supporting diverse
orientations for a more immersive experience. However, incorporating pose
conditions into VTON models presents several challenges, including selecting
the optimal pose representation, integrating poses without additional
parameters, and balancing pose preservation with flexible pose control.
  In this work, we build upon a baseline VTON model that concatenates the
reference image condition without external encoder, control network, or complex
attention layers. We investigate methods to incorporate pose control into this
pure concatenation paradigm by spatially concatenating pose data, comparing
performance using pose maps and skeletons, without adding any additional
parameters or module to the baseline model. Our experiments reveal that pose
stitching with pose maps yields the best results, enhancing both pose
preservation and output realism. Additionally, we introduce a mixed-mask
training strategy using fine-grained and bounding box masks, allowing the model
to support flexible product integration across varied poses and conditions.

</details>


### [75] [PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation](https://arxiv.org/abs/2509.20358)
*Chen Wang,Chuhao Chen,Yiming Huang,Zhiyang Dou,Yuan Liu,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: PhysCtrl是一个基于物理的图像到视频生成框架，通过物理参数和力控制，生成高保真、物理合理的视频。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在物理合理性和3D可控性方面存在不足，PhysCtrl旨在解决这些限制。

Method: PhysCtrl采用生成物理网络和扩散模型，结合物理参数和施加力，通过3D点轨迹表示物理动态，并在大规模合成数据集上进行训练。

Result: PhysCtrl生成的物理动态轨迹在驱动图像到视频模型时，能够产生高保真、可控的视频，在视觉质量和物理合理性上均优于现有方法。

Conclusion: PhysCtrl通过引入物理基础和可控性，显著提升了视频生成的物理合理性和3D可控性，超越了现有方法。

Abstract: Existing video generation models excel at producing photo-realistic videos
from text or images, but often lack physical plausibility and 3D
controllability. To overcome these limitations, we introduce PhysCtrl, a novel
framework for physics-grounded image-to-video generation with physical
parameters and force control. At its core is a generative physics network that
learns the distribution of physical dynamics across four materials (elastic,
sand, plasticine, and rigid) via a diffusion model conditioned on physics
parameters and applied forces. We represent physical dynamics as 3D point
trajectories and train on a large-scale synthetic dataset of 550K animations
generated by physics simulators. We enhance the diffusion model with a novel
spatiotemporal attention block that emulates particle interactions and
incorporates physics-based constraints during training to enforce physical
plausibility. Experiments show that PhysCtrl generates realistic,
physics-grounded motion trajectories which, when used to drive image-to-video
models, yield high-fidelity, controllable videos that outperform existing
methods in both visual quality and physical plausibility. Project Page:
https://cwchenwang.github.io/physctrl

</details>


### [76] [EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning](https://arxiv.org/abs/2509.20360)
*Xuan Ju,Tianyu Wang,Yuqian Zhou,He Zhang,Qing Liu,Nanxuan Zhao,Zhifei Zhang,Yijun Li,Yuanhao Cai,Shaoteng Liu,Daniil Pakhomov,Zhe Lin,Soo Ye Kim,Qiang Xu*

Main category: cs.CV

TL;DR: EditVerse是一个统一图像和视频生成与编辑的框架，通过统一令牌序列和自注意力机制实现跨模态处理，性能超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成和编辑领域因架构限制和数据稀缺而碎片化，而图像领域已实现统一框架。本研究旨在通过统一模型解决这一问题。

Method: 采用统一令牌序列表示所有模态（文本、图像、视频），利用自注意力机制实现上下文学习、跨模态知识传递，并灵活处理任意分辨率和时长的输入输出。为解决视频编辑数据稀缺问题，设计了一个可扩展的数据管道，整合了232K视频编辑样本与大规模图像视频数据集进行联合训练。

Result: 实验和用户研究表明，EditVerse在性能上达到最先进水平，并展现出跨模态的新兴编辑和生成能力。

Conclusion: EditVerse通过统一框架实现了跨模态的图像和视频生成与编辑，表现出卓越的性能和新兴能力，超越了现有开源和商业模型。

Abstract: Recent advances in foundation models highlight a clear trend toward
unification and scaling, showing emergent capabilities across diverse domains.
While image generation and editing have rapidly transitioned from task-specific
to unified frameworks, video generation and editing remain fragmented due to
architectural limitations and data scarcity. In this work, we introduce
EditVerse, a unified framework for image and video generation and editing
within a single model. By representing all modalities, i.e., text, image, and
video, as a unified token sequence, EditVerse leverages self-attention to
achieve robust in-context learning, natural cross-modal knowledge transfer, and
flexible handling of inputs and outputs with arbitrary resolutions and
durations. To address the lack of video editing training data, we design a
scalable data pipeline that curates 232K video editing samples and combines
them with large-scale image and video datasets for joint training. Furthermore,
we present EditVerseBench, the first benchmark for instruction-based video
editing covering diverse tasks and resolutions. Extensive experiments and user
studies demonstrate that EditVerse achieves state-of-the-art performance,
surpassing existing open-source and commercial models, while exhibiting
emergent editing and generation abilities across modalities.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [77] [The Indispensable Role of User Simulation in the Pursuit of AGI](https://arxiv.org/abs/2509.19456)
*Krisztian Balog,ChengXiang Zhai*

Main category: cs.AI

TL;DR: 用户模拟技术是加速AGI发展的关键，需与智能任务代理研究协同推进。


<details>
  <summary>Details</summary>
Motivation: 解决AGI发展中评估复杂交互系统和获取大量交互数据的瓶颈问题。

Method: 本文通过探讨构建真实模拟器的多学科性质，识别了包括大型语言模型在内的关键挑战，并提出了未来研究议程。

Result: 真实模拟器为可扩展评估、交互学习数据生成及培养AGI核心适应能力提供了必要环境。

Conclusion: 用户模拟技术是推动人工通用智能（AGI）发展的关键催化剂，其与智能任务代理的研究需协同推进。

Abstract: Progress toward Artificial General Intelligence (AGI) faces significant
bottlenecks, particularly in rigorously evaluating complex interactive systems
and acquiring the vast interaction data needed for training adaptive agents.
This paper posits that user simulation -- creating computational agents that
mimic human interaction with AI systems -- is not merely a useful tool, but is
a critical catalyst required to overcome these bottlenecks and accelerate AGI
development. We argue that realistic simulators provide the necessary
environments for scalable evaluation, data generation for interactive learning,
and fostering the adaptive capabilities central to AGI. Therefore, research
into user simulation technology and intelligent task agents are deeply
synergistic and must advance hand-in-hand. This article elaborates on the
critical role of user simulation for AGI, explores the interdisciplinary nature
of building realistic simulators, identifies key challenges including those
posed by large language models, and proposes a future research agenda.

</details>


### [78] [Evaluation-Aware Reinforcement Learning](https://arxiv.org/abs/2509.19464)
*Shripad Vilasrao Deshmukh,Will Schwarzer,Scott Niekum*

Main category: cs.AI

TL;DR: EvA-RL是一种新型RL方法，通过训练策略同时优化回报和评估准确性，减少评估误差，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法因数据有限、任务长视野或环境模型不准确导致高方差或高偏差，EvA-RL旨在解决这些问题。

Method: 提出评估感知强化学习（EvA-RL）框架，通过同时最大化回报和最小化评估误差来训练策略，并扩展为共同学习状态价值预测器。

Result: 实验证明EvA-RL能显著降低评估误差，同时保持竞争力的回报，但固定价值预测方案下存在评估准确性与策略性能的权衡。

Conclusion: EvA-RL通过将评估准确性作为训练目标的一部分，显著减少了评估误差，同时在多种任务中保持了竞争力的回报，为RL方法开辟了新方向。

Abstract: Policy evaluation is often a prerequisite for deploying safety- and
performance-critical systems. Existing evaluation approaches frequently suffer
from high variance due to limited data and long-horizon tasks, or high bias due
to unequal support or inaccurate environmental models. We posit that these
challenges arise, in part, from the standard reinforcement learning (RL)
paradigm of policy learning without explicit consideration of evaluation. As an
alternative, we propose evaluation-aware reinforcement learning (EvA-RL), in
which a policy is trained to maximize expected return while simultaneously
minimizing expected evaluation error under a given value prediction scheme --
in other words, being "easy" to evaluate. We formalize a framework for EvA-RL
and design an instantiation that enables accurate policy evaluation,
conditioned on a small number of rollouts in an assessment environment that can
be different than the deployment environment. However, our theoretical analysis
and empirical results show that there is often a tradeoff between evaluation
accuracy and policy performance when using a fixed value-prediction scheme
within EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an
assessment-conditioned state-value predictor alongside the policy. Empirical
results across diverse discrete and continuous action domains demonstrate that
EvA-RL can substantially reduce evaluation error while maintaining competitive
returns. This work lays the foundation for a broad new class of RL methods that
treat reliable evaluation as a first-class principle during training.

</details>


### [79] [Estimating the Self-Consistency of LLMs](https://arxiv.org/abs/2509.19489)
*Robert Nowak*

Main category: cs.AI

TL;DR: 在固定计算预算下，重复提示和聚合响应的最佳分割比例为m,n∝√B。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过重复提示和聚合响应来提高大型语言模型（LLMs）的可靠性。

Method: 通过分析自我一致性估计器及其在固定计算预算B=mn下的权衡。

Result: 在固定计算预算下，最佳分割比例为m,n∝√B。

Conclusion: 分析表明，在固定计算预算B=mn下，自我一致性估计器的最佳分割比例大致为m,n∝√B。

Abstract: Systems often repeat the same prompt to large language models (LLMs) and
aggregate responses to improve reliability. This short note analyzes an
estimator of the self-consistency of LLMs and the tradeoffs it induces under a
fixed compute budget $B=mn$, where $m$ is the number of prompts sampled from
the task distribution and $n$ is the number of repeated LLM calls per prompt;
the resulting analysis favors a rough split $m,n\propto\sqrt{B}$.

</details>


### [80] [Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning](https://arxiv.org/abs/2509.19517)
*Sai Teja Reddy Adapala*

Main category: cs.AI

TL;DR: 研究提出认知负荷是推理失败的关键因素，并通过ICE基准验证了动态压力测试对评估AI系统韧性的重要性。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）在静态基准测试中表现优异，但在动态、信息丰富的环境中表现脆弱，其计算限制对认知负荷下的推理能力影响尚不明确。

Method: 设计了Interleaved Cognitive Evaluation (ICE)基准，系统性地操纵认知负荷因素，并在具有挑战性的多跳推理任务上进行全面研究（N = 10次重复/项目，共200个问题）。

Result: 开源小模型（如Llama-3-8B-Instruct、Mistral-7B-Instruct-v0.2）在所有条件下均表现脆弱（准确率0%），而Gemini-2.0-Flash-001在控制条件下表现部分韧性（准确率85%），但在上下文饱和条件下显著退化（β = -0.003，p < 0.001）。

Conclusion: 动态、认知感知的压力测试（如ICE基准）对于评估高级AI系统的真实韧性和安全性至关重要。

Abstract: The scaling of Large Language Models (LLMs) has exposed a critical gap
between their performance on static benchmarks and their fragility in dynamic,
information-rich environments. While models excel at isolated tasks, the
computational limits that govern their reasoning under cognitive load remain
poorly understood. In this work, we introduce a formal theory of computational
cognitive load, positing that extraneous, task-irrelevant information (Context
Saturation) and interference from task-switching (Attentional Residue) are key
mechanisms that degrade performance. We designed the Interleaved Cognitive
Evaluation (ICE), a deconfounded benchmark to systematically manipulate these
load factors on challenging multi-hop reasoning tasks. A comprehensive study (N
= 10 replications per item across 200 questions) revealed significant
performance variations across five instruction-tuned models. Smaller
open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)
exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all
conditions, including clean controls, on this high-intrinsic-load task. In
contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%
accuracy in control conditions, with a statistically significant degradation
under context saturation ($\beta = -0.003$ per % load, $p < 0.001$). These
findings provide preliminary evidence that cognitive load is a key contributor
to reasoning failures, supporting theories of hallucination-as-guessing under
uncertainty. We conclude that dynamic, cognitive-aware stress testing, as
exemplified by the ICE benchmark, is essential for evaluating the true
resilience and safety of advanced AI systems.

</details>


### [81] [Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation](https://arxiv.org/abs/2509.19524)
*Ramy ElMallah,Krish Chhajer,Chi-Guhn Lee*

Main category: cs.AI

TL;DR: 论文提出StepEval框架，通过子目标级成功率向量和视觉语言模型自动化评估，改进机器人学习策略的多步任务性能报告，旨在推动社区采用标准化、可复现的评估实践。


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习论文通常仅报告单一的二值成功率（SR），这掩盖了策略在多步操作任务中具体在哪些子目标上成功或失败。作者认为子目标级别的报告应成为常规做法，以展示部分能力（如抓取与倾倒）。

Method: 论文提出了StepEval，一个成本感知的插件式评估框架，利用视觉语言模型（VLMs）作为子目标结果的自动评判者，从记录的图像或视频中提取信息。

Result: StepEval的核心贡献是提供了一个可扩展、社区驱动的开源项目蓝图，其主输出为每个子目标的成功率向量，同时也考虑了其他量（如延迟或成本估计）以优化框架的诊断能力。

Conclusion: 论文提出了一种名为StepEval的评估框架，旨在通过子目标级别的报告（如每个子目标的成功率向量）来更细致地评估机器人学习策略的性能，而不仅仅是单一的二值成功率。这种框架设计为模型无关、支持单视图或多视图输入，并足够轻量以便于跨实验室采用。

Abstract: Robot learning papers typically report a single binary success rate (SR),
which obscures where a policy succeeds or fails along a multi-step manipulation
task. We argue that subgoal-level reporting should become routine: for each
trajectory, a vector of per-subgoal SRs that makes partial competence visible
(e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware
plug-in evaluation framework that utilizes vision-language models (VLMs) as
automated judges of subgoal outcomes from recorded images or videos. Rather
than proposing new benchmarks or APIs, our contribution is to outline design
principles for a scalable, community-driven open-source project. In StepEval,
the primary artifact for policy evaluation is the per-subgoal SR vector;
however, other quantities (e.g., latency or cost estimates) are also considered
for framework-optimization diagnostics to help the community tune evaluation
efficiency and accuracy when ground-truth subgoal success labels are available.
We discuss how such a framework can remain model-agnostic, support single- or
multi-view inputs, and be lightweight enough to adopt across labs. The intended
contribution is a shared direction: a minimal, extensible seed that invites
open-source contributions, so that scoring the steps, not just the final goal,
becomes a standard and reproducible practice.

</details>


### [82] [Nano Bio-Agents (NBA): Small Language Model Agents for Genomics](https://arxiv.org/abs/2509.19566)
*George Hong,Daniel Trejo Banos*

Main category: cs.AI

TL;DR: SLMs结合NBA框架在基因组学问答中表现优异，高准确性且低成本，为ML工具民主化铺路。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在基因组学问答中的幻觉问题和计算成本挑战。

Method: 通过任务分解、工具编排和API访问（如NCBI和AlphaGenome）的NBA框架，结合SLMs（3-10B参数）进行基因组学问答。

Result: SLMs与NBA框架结合在GeneTuring基准测试中达到98%准确率，3-10B参数模型在85-97%准确率范围内表现稳定，且计算资源需求显著低于传统方法。

Conclusion: 小语言模型（SLMs）结合Nano Bio-Agent（NBA）框架在基因组学问答中展现出高效、低成本且高准确性的潜力，为ML驱动的基因组学工具民主化提供了可行方案。

Abstract: We investigate the application of Small Language Models (<10 billion
parameters) for genomics question answering via agentic framework to address
hallucination issues and computational cost challenges. The Nano Bio-Agent
(NBA) framework we implemented incorporates task decomposition, tool
orchestration, and API access into well-established systems such as NCBI and
AlphaGenome. Results show that SLMs combined with such agentic framework can
achieve comparable and in many cases superior performance versus existing
approaches utilising larger models, with our best model-agent combination
achieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B
parameter models consistently achieve 85-97% accuracy while requiring much
lower computational resources than conventional approaches. This demonstrates
promising potential for efficiency gains, cost savings, and democratization of
ML-powered genomics tools while retaining highly robust and accurate
performance.

</details>


### [83] [What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities](https://arxiv.org/abs/2509.19590)
*Nathanael Jo,Ashia Wilson*

Main category: cs.AI

TL;DR: 论文提出将评估视为推理的框架，从能力理论出发，推导出更可靠的评估方法，并证明其在处理敏感性和样本不确定性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于对生成模型基准评估可靠性的日益增长的怀疑，论文旨在提供一个更可靠的评估框架，以确保报告的性能真实反映模型的能力。

Method: 论文从能力理论出发，推导出估计方法，包括处理敏感性和有限样本不确定性的方法，以及一个显著降低样本复杂度的自适应算法。

Result: 提出的框架和方法能够更可靠地估计AI能力，特别是在处理敏感性和有限样本的不确定性方面表现出色。

Conclusion: 该论文提出了一个基于理论的能力评估框架，通过将评估视为推理过程，为AI能力的可靠估计奠定了基础。

Abstract: Evaluations of generative models on benchmark data are now ubiquitous, and
their outcomes critically shape public and scientific expectations of AI's
capabilities. Yet growing skepticism surrounds their reliability. How can we
know that a reported accuracy genuinely reflects a model's true performance?
Evaluations are often presented as simple measurements, but in reality they are
inferences: to treat benchmark scores as evidence of capability is already to
assume a theory of what capability is and how it manifests in a test. We make
this step explicit by proposing a principled framework for evaluation as
inference: begin from a theory of capability, and then derive methods for
estimating it. This perspective, familiar in fields such as psychometrics, has
not yet become commonplace in AI evaluation. As a proof of concept, we address
a central challenge that undermines reliability: sensitivity to perturbations.
After formulating a model of ability, we introduce methods that infer ability
while accounting for uncertainty from sensitivity and finite samples, including
an adaptive algorithm that significantly reduces sample complexity. Together,
these contributions lay the groundwork for more reliable and trustworthy
estimates of AI capabilities as measured through benchmarks.

</details>


### [84] [SteinerSQL: Graph-Guided Mathematical Reasoning for Text-to-SQL Generation](https://arxiv.org/abs/2509.19623)
*Xutao Mao,Tao Liu,Hongying Zan*

Main category: cs.AI

TL;DR: SteinerSQL 是一个统一数学推理和模式导航的 Text-to-SQL 框架，通过三阶段优化提升复杂查询的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在处理需要复杂数学推理和模式导航的 Text-to-SQL 查询时的不足，现有方法往往孤立处理这些挑战，导致推理过程断裂。

Method: SteinerSQL 框架通过三个阶段操作：数学分解以识别所需表（终端）、通过 Steiner 树问题构建最优推理支架，以及多级验证以确保正确性。

Result: 在 LogicCat 和 Spider2.0-Lite 基准测试中，SteinerSQL 分别达到了 36.10% 和 40.04% 的执行准确率，使用 Gemini-2.5-Pro 实现了新的最先进水平。

Conclusion: SteinerSQL 提出了一种新的统一范式，为复杂推理任务提供了更稳健和原则性的解决方案。

Abstract: Large Language Models (LLMs) struggle with complex Text-to-SQL queries that
demand both sophisticated mathematical reasoning and intricate schema
navigation. Existing methods often tackle these challenges in isolation,
creating a fractured reasoning process that compromises logical and structural
correctness. To resolve this, we introduce SteinerSQL, a framework that unifies
these dual challenges into a single, graph-centric optimization problem.
SteinerSQL operates in three stages: mathematical decomposition to identify
required tables (terminals), optimal reasoning scaffold construction via a
Steiner tree problem, and multi-level validation to ensure correctness. On the
challenging LogicCat and Spider2.0-Lite benchmarks, SteinerSQL establishes a
new state-of-the-art with 36.10% and 40.04% execution accuracy, respectively,
using Gemini-2.5-Pro. Beyond accuracy, SteinerSQL presents a new, unified
paradigm for Text-to-SQL, paving the way for more robust and principled
solutions to complex reasoning tasks.

</details>


### [85] [Calibrated Reasoning: An Explanatory Verifier for Dynamic and Efficient Problem-Solving](https://arxiv.org/abs/2509.19681)
*Anisha Garg,Engin Tekin,Yash More,David Bick,Nishit Neema,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: 通过强化学习训练的成对解释验证器提升了测试时策略的准确性和效率，尤其在识别复杂错误模式时表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型的自评估能力不足，限制了高级测试时计算策略的有效性。

Method: 使用强化学习（GRPO）训练一个成对的解释验证器，生成校准的置信分数和相关的自然语言推理。

Result: 验证器提升了如最佳-n和自我反思等测试时策略的准确性和效率，特别擅长识别具有挑战性的失败模式，如两个候选解决方案都错误的情况。

Conclusion: 提出的成对解释验证器通过强化学习训练，能够生成校准的置信分数和自然语言推理，显著提升了测试时策略的准确性和效率，尤其在识别具有挑战性的失败模式方面表现出色。

Abstract: Advanced test-time computing strategies are essential for scaling reasoning
models, but their effectiveness is capped by the models' poor self-evaluation.
We propose a pairwise Explanatory Verifier, trained via reinforcement learning
(GRPO), that produces calibrated confidence scores and associated natural
language reasoning for generated solutions. Our verifier improves the accuracy
and efficiency of test-time strategies like best-of-n and self-reflection.
Crucially, it excels at identifying challenging failure modes, such as when
both candidate solutions are identically incorrect, succeeding where standard
methods like majority voting fail.

</details>


### [86] [UserRL: Training Interactive User-Centric Agent via Reinforcement Learning](https://arxiv.org/abs/2509.19736)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Jielin Qiu,Zhiwei Liu,Haolin Chen,Shirley Kokane,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: UserRL框架通过标准化环境和模拟用户，优化奖励设计和轨迹评分，提升强化学习代理的用户交互能力，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决用户交互的多样性和动态性对强化学习代理模型的挑战，提升其在实际用户辅助中的能力。

Method: 提出UserRL框架，通过标准化健身房环境和模拟用户系统训练和评估用户中心能力，分析不同奖励分配和轨迹评分对GRPO算法学习的影响。

Result: 实验发现：SFT冷启动对初始交互能力至关重要；精心设计的轨迹评分能提升多轮交互效率；开源模拟器（如Qwen3-32B）是经济高效的选择。

Conclusion: 研究表明，奖励设计和用户模拟选择对开发用户为中心的智能代理模型至关重要，UserRL框架为此提供了实用路径。

Abstract: Reinforcement learning (RL) has shown promise in training agentic models that
move beyond static benchmarks to engage in dynamic, multi-turn interactions.
Yet, the ultimate value of such agents lies in their ability to assist users, a
setting where diversity and dynamics of user interaction pose challenges. In
this work, we propose UserRL, a unified framework for training and evaluating
user-centric abilities through standardized gym environments paired with
simulated users. We systematically vary turn-level reward assignment and
trajectory-level score calculation to analyze how different formulations affect
learning under the GRPO algorithm. Our experiments across Qwen3 models reveal
three key findings: (i) SFT cold start is critical for unlocking initial
interaction ability and enabling sustained RL improvements; (ii) deliberate
trajectory scoring yields more efficient and effective multi-turn interactions;
and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,
open-source simulators (e.g., Qwen3-32B) remain a cost-effective and
transferable option. Together, these results highlight that careful design of
reward shaping and user simulation choice is as crucial as model scale, and
establish UserRL as a practical pathway for developing robust user-centric
agentic models. All codes and data are public for future research.

</details>


### [87] [The Conductor and the Engine: A Path Towards Co-Designed Reasoning](https://arxiv.org/abs/2509.19762)
*Yuanxin Wang,Pawel Filipczuk,Anisha Garg,Amaan Dhada,Mohammad Hassanpour,David Bick,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: 该论文提出了一种优化推理工作流程（\cepo），通过减少计算浪费和提升指令跟随能力，使小型模型超越大型模型性能，并计划开源以促进进一步研究。


<details>
  <summary>Details</summary>
Motivation: 现代LLM推理依赖于大量的测试时计算，但由于模型冗长和指令跟随能力差，导致计算资源浪费。

Method: 引入了一种优化的推理工作流程（\cepo），旨在减少计算浪费并提升效率。

Result: 优化后的工作流程使小型开源模型能够超越比其大数倍的模型性能。

Conclusion: 该论文展示了一种通过协同设计编排框架与底层模型能力来解锁中小型模型强大推理能力的清晰路径。

Abstract: Modern LLM reasoning relies on extensive test-time computation, driven by
internal model training and external agentic orchestration. However, this
synergy is often inefficient, as model verbosity and poor instruction following
lead to wasted compute. We analyze this capability-cost trade-off and introduce
an optimized reasoning workflow (\cepo) that empowers smaller open-source
models to outperform models multiple times their size. We will open-source this
workflow to enable further research. Our work demonstrates a clear path toward
co-designing orchestration frameworks with the underlying model capabilities to
unlock powerful reasoning in small-to-medium sized models.

</details>


### [88] [Agentic Metacognition: Designing a "Self-Aware" Low-Code Agent for Failure Prediction and Human Handoff](https://arxiv.org/abs/2509.19783)
*Jiexi Xu*

Main category: cs.AI

TL;DR: 研究提出了一种元认知层架构，通过预测失败并启动人工交接，显著提高了LCNC代理的可靠性，但也增加了计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决自主代理在LCNC环境中的非确定性问题，如陷入循环、生成不准确输出或不可恢复的故障，以提升用户体验和信任。

Method: 提出了一种集成次级'元认知'层的架构模式，该层主动监控主LCNC代理，基于定义的触发器预测任务失败并启动人工交接。

Result: 原型系统的实证分析表明，该方法显著提高了任务成功率，但增加了计算开销。

Conclusion: 研究提出了一种新型架构模式，通过引入元认知层来增强自主代理的可靠性，显著提高了任务成功率，但也带来了计算开销的增加。

Abstract: The inherent non-deterministic nature of autonomous agents, particularly
within low-code/no-code (LCNC) environments, presents significant reliability
challenges. Agents can become trapped in unforeseen loops, generate inaccurate
outputs, or encounter unrecoverable failures, leading to user frustration and a
breakdown of trust. This report proposes a novel architectural pattern to
address these issues: the integration of a secondary, "metacognitive" layer
that actively monitors the primary LCNC agent. Inspired by human introspection,
this layer is designed to predict impending task failures based on a defined
set of triggers, such as excessive latency or repetitive actions. Upon
predicting a failure, the metacognitive agent proactively initiates a human
handoff, providing the user with a clear summary of the agent's "thought
process" and a detailed explanation of why it could not proceed. An empirical
analysis of a prototype system demonstrates that this approach significantly
increases the overall task success rate. However, this performance gain comes
with a notable increase in computational overhead. The findings reframe human
handoffs not as an admission of defeat but as a core design feature that
enhances system resilience, improves user experience, and builds trust by
providing transparency into the agent's internal state. The report discusses
the practical and ethical implications of this approach and identifies key
directions for future research.

</details>


### [89] [Analysis of approximate linear programming solution to Markov decision problem with log barrier function](https://arxiv.org/abs/2509.19800)
*Donghwan Lee,Hyukjun Yang,Bum Geun Park*

Main category: cs.AI

TL;DR: 本文提出了一种利用对数障碍函数将LP-based MDP转化为无约束优化问题的方法，通过梯度下降求解近似解，填补了该方法的理论空白。


<details>
  <summary>Details</summary>
Motivation: 线性规划（LP）方法在MDP求解中相对较少使用，主要因为其导致的不等式约束优化问题比基于Bellman方程的方法更难有效解决。本文旨在为LP-based MDP的解决方案提供更有效和实用的理论基础。

Method: 利用对数障碍函数将LP形式的MDP转化为无约束优化问题，并通过梯度下降求解近似解。

Result: 提出了一种新的方法，通过对数障碍函数将不等式约束优化问题转化为无约束优化问题，使得通过梯度下降获得近似解成为可能。

Conclusion: 本文为基于线性规划的马尔可夫决策问题（MDP）解决方案建立了理论基础，提出了一种利用对数障碍函数将不等式约束优化问题转化为无约束优化问题的方法，从而通过梯度下降获得近似解。

Abstract: There are two primary approaches to solving Markov decision problems (MDPs):
dynamic programming based on the Bellman equation and linear programming (LP).
Dynamic programming methods are the most widely used and form the foundation of
both classical and modern reinforcement learning (RL). By contrast, LP-based
methods have been less commonly employed, although they have recently gained
attention in contexts such as offline RL. The relative underuse of the LP-based
methods stems from the fact that it leads to an inequality-constrained
optimization problem, which is generally more challenging to solve effectively
compared with Bellman-equation-based methods. The purpose of this paper is to
establish a theoretical foundation for solving LP-based MDPs in a more
effective and practical manner. Our key idea is to leverage the log-barrier
function, widely used in inequality-constrained optimization, to transform the
LP formulation of the MDP into an unconstrained optimization problem. This
reformulation enables approximate solutions to be obtained easily via gradient
descent. While the method may appear simple, to the best of our knowledge, a
thorough theoretical interpretation of this approach has not yet been
developed. This paper aims to bridge this gap.

</details>


### [90] [LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation](https://arxiv.org/abs/2509.19839)
*Huizhen Shu,Xuying Li,Zhuo Li*

Main category: cs.AI

TL;DR: LATENTGUARD通过行为对齐和潜在空间控制，实现了LLM的安全引导，实验证明其有效且通用。


<details>
  <summary>Details</summary>
Motivation: 在保持大型语言模型（LLM）实用性的同时实现稳健的安全对齐是一个基本挑战。现有方法往往难以在表示级别平衡全面安全性和细粒度可控性。

Method: LATENTGUARD框架结合了行为对齐和监督潜在空间控制，通过三阶段方法实现可解释且精确的安全引导。

Result: 在Qwen3-8B和Mistral-7B上的实验表明，LATENTGUARD显著提高了安全可控性和响应可解释性，且未损害实用性。

Conclusion: 结构化表示级干预为构建更安全且实用的LLM系统提供了一条有前景的路径。

Abstract: Achieving robust safety alignment in large language models (LLMs) while
preserving their utility remains a fundamental challenge. Existing approaches
often struggle to balance comprehensive safety with fine-grained
controllability at the representation level. We introduce LATENTGUARD, a novel
three-stage framework that combines behavioral alignment with supervised latent
space control for interpretable and precise safety steering. Our approach
begins by fine-tuning an LLM on rationalized datasets containing both
reasoning-enhanced refusal responses to adversarial prompts and
reasoning-enhanced normal responses to benign queries, establishing robust
behavioral priors across both safety-critical and utility-preserving scenarios.
We then train a structured variational autoencoder (VAE) on intermediate MLP
activations, supervised by multi-label annotations including attack types,
attack methods, and benign indicators. This supervision enables the VAE to
learn disentangled latent representations that capture distinct adversarial
characteristics while maintaining semantic interpretability. Through targeted
manipulation of learned latent dimensions, LATENTGUARD achieves selective
refusal behavior, effectively blocking harmful requests while preserving
helpfulness for legitimate use cases. Experiments on Qwen3-8B demonstrate
significant improvements in both safety controllability and response
interpretability without compromising utility. Cross-architecture validation on
Mistral-7B confirms the generalizability of our latent steering approach,
showing consistent effectiveness across different model families. Our results
suggest that structured representation-level intervention offers a promising
pathway toward building safer yet practical LLM systems.

</details>


### [91] [CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain](https://arxiv.org/abs/2509.19925)
*Ajeet Kumar Singh,Rajsabi Surya,Anurag Tripathi,Santanu Choudhury,Sudhir Bisane*

Main category: cs.AI

TL;DR: CON-QA是一个混合隐私保护框架，用于安全的企业合同问答，结合本地和云端LLMs，有效保护敏感信息并保持实用性。


<details>
  <summary>Details</summary>
Motivation: 企业将云基础大型语言模型集成到法律文档工作流中时，保护敏感合同信息成为关键挑战。

Method: CON-QA通过三个阶段运作：语义查询分解、敏感实体匿名化以及匿名化响应生成。

Result: 实证评估和详细人工评估证实，CON-QA有效维护隐私和实用性，保持答案质量，并显著降低隐私风险。

Conclusion: CON-QA框架在保护隐私的同时保持了实用性，验证了其在企业级合同文档中的适用性。

Abstract: As enterprises increasingly integrate cloud-based large language models
(LLMs) such as ChatGPT and Gemini into their legal document workflows,
protecting sensitive contractual information - including Personally
Identifiable Information (PII) and commercially sensitive clauses - has emerged
as a critical challenge. In this work, we propose CON-QA, a hybrid
privacy-preserving framework designed specifically for secure question
answering over enterprise contracts, effectively combining local and
cloud-hosted LLMs. The CON-QA framework operates through three stages: (i)
semantic query decomposition and query-aware document chunk retrieval using a
locally deployed LLM analysis, (ii) anonymization of detected sensitive
entities via a structured one-to-many mapping scheme, ensuring semantic
coherence while preventing cross-session entity inference attacks, and (iii)
anonymized response generation by a cloud-based LLM, with accurate
reconstruction of the original answer locally using a session-consistent
many-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce
CUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world
CUAD contract documents, encompassing simple, complex, and summarization-style
queries. Empirical evaluations, complemented by detailed human assessments,
confirm that CON-QA effectively maintains both privacy and utility, preserves
answer quality, maintains fidelity to legal clause semantics, and significantly
mitigates privacy risks, demonstrating its practical suitability for secure,
enterprise-level contract documents.

</details>


### [92] [Embodied AI: From LLMs to World Models](https://arxiv.org/abs/2509.20021)
*Tongtong Feng,Xin Wang,Yu-Gang Jiang,Wenwu Zhu*

Main category: cs.AI

TL;DR: 本文综述了具身AI的发展，重点分析了LLM和WM的作用，提出了联合架构的重要性，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨具身AI作为实现AGI的智能系统范式，及其在物理世界中的广泛应用潜力。

Method: 本文综合分析了具身AI的文献，包括历史、关键技术、组成部分和硬件系统，并从单模态到多模态的角度探讨了其发展。

Result: 详细阐述了LLM和WM在具身AI中的作用，并提出了联合MLLM-WM驱动的架构及其在复杂任务中的潜力。

Conclusion: 本文强调了联合MLLM-WM驱动的具身AI架构的重要性，并指出了未来研究方向。

Abstract: Embodied Artificial Intelligence (AI) is an intelligent system paradigm for
achieving Artificial General Intelligence (AGI), serving as the cornerstone for
various applications and driving the evolution from cyberspace to physical
systems. Recent breakthroughs in Large Language Models (LLMs) and World Models
(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs
empower embodied AI via semantic reasoning and task decomposition, bringing
high-level natural language instructions and low-level natural language actions
into embodied cognition. On the other hand, WMs empower embodied AI by building
internal representations and future predictions of the external world,
facilitating physical law-compliant embodied interactions. As such, this paper
comprehensively explores the literature in embodied AI from basics to advances,
covering both LLM driven and WM driven works. In particular, we first present
the history, key technologies, key components, and hardware systems of embodied
AI, as well as discuss its development via looking from unimodal to multimodal
angle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,
embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,
meticulously delineating their indispensable roles in end-to-end embodied
cognition and physical laws-driven embodied interactions. Building upon the
above advances, we further share our insights on the necessity of the joint
MLLM-WM driven embodied AI architecture, shedding light on its profound
significance in enabling complex tasks within physical worlds. In addition, we
examine representative applications of embodied AI, demonstrating its wide
applicability in real-world scenarios. Last but not least, we point out future
research directions of embodied AI that deserve further investigation.

</details>


### [93] [MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM](https://arxiv.org/abs/2509.20067)
*Wenliang Li,Rui Yan,Xu Zhang,Li Chen,Hongji Zhu,Jing Zhao,Junjun Li,Mengru Li,Wei Cao,Zihang Jiang,Wei Wei,Kun Zhang,Shaohua Kevin Zhou*

Main category: cs.AI

TL;DR: MACD框架通过多智能体自学习提升LLM在临床诊断中的准确性，性能超越现有指南并接近人类医生，同时增强解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在医疗应用中展现出潜力，但在处理复杂真实世界临床诊断时仍面临挑战，现有方法通常优化孤立推理，忽略了可重用临床经验的积累。

Method: 提出了一种新颖的多智能体临床诊断（MACD）框架，通过多智能体管道（总结、提炼和应用诊断见解）使LLMs能够自我学习临床知识。进一步扩展为MACD-人类协作工作流程，由多个基于LLM的诊断智能体进行迭代咨询，辅以评估智能体和人类监督。

Result: 在4,390个真实世界患者案例中，MACD显著提高了主要诊断准确性，最高提升22.3%（MACD）。在部分数据上，性能达到或超过人类医生（最高提升16%）。MACD-人类工作流程相比纯医生诊断提升了18.6%。自学习知识表现出跨模型稳定性、可迁移性和模型个性化。

Conclusion: 本研究提出了一种可扩展的自学习范式（MACD框架），用于LLM辅助诊断，弥合了LLMs内在知识与真实世界临床实践之间的差距。

Abstract: Large language models (LLMs) have demonstrated notable potential in medical
applications, yet they face substantial challenges in handling complex
real-world clinical diagnoses using conventional prompting methods. Current
prompt engineering and multi-agent approaches typically optimize isolated
inferences, neglecting the accumulation of reusable clinical experience. To
address this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)
framework, which allows LLMs to self-learn clinical knowledge via a multi-agent
pipeline that summarizes, refines, and applies diagnostic insights. It mirrors
how physicians develop expertise through experience, enabling more focused and
accurate diagnosis on key disease-specific cues. We further extend it to a
MACD-human collaborative workflow, where multiple LLM-based diagnostician
agents engage in iterative consultations, supported by an evaluator agent and
human oversight for cases where agreement is not reached. Evaluated on 4,390
real-world patient cases across seven diseases using diverse open-source LLMs
(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves
primary diagnostic accuracy, outperforming established clinical guidelines with
gains up to 22.3% (MACD). On the subset of the data, it achieves performance on
par with or exceeding that of human physicians (up to 16% improvement over
physicians-only diagnosis). Additionally, on the MACD-human workflow, it
achieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,
self-learned knowledge exhibits strong cross-model stability, transferability,
and model-specific personalization, while the system can generate traceable
rationales, enhancing explainability. Consequently, this work presents a
scalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap
between the intrinsic knowledge of LLMs and real-world clinical practice.

</details>


### [94] [From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms](https://arxiv.org/abs/2509.20095)
*Aymeric Vellinger,Nemanja Antonic,Elio Tuci*

Main category: cs.AI

TL;DR: 该研究揭示了信息素系统如何通过分布式强化学习过程实现集体决策，并通过引入探索性个体增强群体在动态环境中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在建立信息素介导的聚集与强化学习之间的理论等价性，展示信息素信号如何作为分布式奖励机制。

Method: 我们通过建模工程化线虫群体执行觅食任务，展示了信息素动态如何数学上镜像交叉学习更新，这是一种基本的强化学习算法。

Result: 实验验证表明，我们的模型准确复制了静态条件下线虫觅食模式，并揭示了在动态环境中，引入探索性个体可以平衡探索与利用的权衡，实现群体级过时策略的淘汰。

Conclusion: 这项研究表明，通过引入少数对信息素不敏感的探索性个体，可以恢复群体在动态环境中的适应能力，实现快速任务切换。

Abstract: Swarm intelligence emerges from decentralised interactions among simple
agents, enabling collective problem-solving. This study establishes a
theoretical equivalence between pheromone-mediated aggregation in \celeg\ and
reinforcement learning (RL), demonstrating how stigmergic signals function as
distributed reward mechanisms. We model engineered nematode swarms performing
foraging tasks, showing that pheromone dynamics mathematically mirror
cross-learning updates, a fundamental RL algorithm. Experimental validation
with data from literature confirms that our model accurately replicates
empirical \celeg\ foraging patterns under static conditions. In dynamic
environments, persistent pheromone trails create positive feedback loops that
hinder adaptation by locking swarms into obsolete choices. Through
computational experiments in multi-armed bandit scenarios, we reveal that
introducing a minority of exploratory agents insensitive to pheromones restores
collective plasticity, enabling rapid task switching. This behavioural
heterogeneity balances exploration-exploitation trade-offs, implementing
swarm-level extinction of outdated strategies. Our results demonstrate that
stigmergic systems inherently encode distributed RL processes, where
environmental signals act as external memory for collective credit assignment.
By bridging synthetic biology with swarm robotics, this work advances
programmable living systems capable of resilient decision-making in volatile
environments.

</details>


### [95] [Steerable Adversarial Scenario Generation through Test-Time Preference Alignment](https://arxiv.org/abs/2509.20102)
*Tong Nie,Yuewen Mei,Yihong Tang,Junlin He,Jie Sun,Haotian Shi,Wei Ma,Jian Sun*

Main category: cs.AI

TL;DR: SAGE框架通过层次化偏好优化和权重插值，实现了对抗场景生成中对抗性与真实性的灵活平衡，提升了自动驾驶安全评估的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有对抗场景生成方法在对抗性和真实性之间缺乏灵活平衡，无法满足多样化的训练和测试需求。

Method: 提出了一种名为SAGE的框架，采用层次化组偏好优化方法，通过离线对齐学习平衡对抗性和真实性。框架在推理时通过插值两个专家模型的权重来构建连续策略谱。

Result: 实验表明，SAGE不仅能生成更优平衡的对抗场景，还能更有效地进行驾驶策略的闭环训练。

Conclusion: SAGE框架通过层次化组偏好优化和专家权重插值，实现了对抗性和真实性的灵活平衡，显著提升了自动驾驶系统的安全评估效率。

Abstract: Adversarial scenario generation is a cost-effective approach for safety
assessment of autonomous driving systems. However, existing methods are often
constrained to a single, fixed trade-off between competing objectives such as
adversariality and realism. This yields behavior-specific models that cannot be
steered at inference time, lacking the efficiency and flexibility to generate
tailored scenarios for diverse training and testing requirements. In view of
this, we reframe the task of adversarial scenario generation as a
multi-objective preference alignment problem and introduce a new framework
named \textbf{S}teerable \textbf{A}dversarial scenario \textbf{GE}nerator
(SAGE). SAGE enables fine-grained test-time control over the trade-off between
adversariality and realism without any retraining. We first propose
hierarchical group-based preference optimization, a data-efficient offline
alignment method that learns to balance competing objectives by decoupling hard
feasibility constraints from soft preferences. Instead of training a fixed
model, SAGE fine-tunes two experts on opposing preferences and constructs a
continuous spectrum of policies at inference time by linearly interpolating
their weights. We provide theoretical justification for this framework through
the lens of linear mode connectivity. Extensive experiments demonstrate that
SAGE not only generates scenarios with a superior balance of adversariality and
realism but also enables more effective closed-loop training of driving
policies. Project page: https://tongnie.github.io/SAGE/.

</details>


### [96] [PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs](https://arxiv.org/abs/2509.20105)
*Venkat Margapuri,Garik Kazanjian,Naren Kosaraju*

Main category: cs.AI

TL;DR: 量子启发的方法通过PEPS保真度奖励提升LLMs推理连贯性，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多步推理任务中难以保持连贯的逻辑流，需要一种新方法来提升全局一致性。

Method: 提出了一种基于投影纠缠对态（PEPS）的保真度奖励，并将其融入近端策略优化（PPO）中，以结构一致性指导学习。

Result: 在GSM8K、StrategyQA和EntailmentBank等数据集上的实验表明，该方法在算术、直觉和蕴含推理任务中均优于监督、对比和预训练基线方法。

Conclusion: 量子启发的方法通过结构一致性显著提升了大型语言模型在多步推理任务中的连贯性，为未来研究提供了新方向。

Abstract: Large Language Models (LLMs) often struggle with maintaining coherent
multi-step reasoning traces, particularly in tasks that require a structured
logical flow. This work introduces a quantum-inspired approach to address the
challenge by incorporating a fidelity-based reward derived from Projected
Entangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior
approaches that use direct supervision or contrastive objectives, the proposed
method guides learning through structural consistency, offering a novel
approach to enforce global coherence in generated reasoning traces. The
proposed framework is evaluated using multiple coherence-determining metrics on
diverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning
arithmetic, intuitive, and entailment-based reasoning. Results show that the
proposed quantum-inspired approach offers significant improvements over
supervised, contrastive, and pretrained baseline approaches, highlighting the
effectiveness of quantum-inspired fidelity as a foundation to improve reasoning
trace coherence in LLMs.

</details>


### [97] [Formal Verification of Minimax Algorithms](https://arxiv.org/abs/2509.20138)
*Wieger Wesselink,Kees Huizing,Huub van de Wetering*

Main category: cs.AI

TL;DR: 使用Dafny验证系统对多种极小化极大搜索算法进行了形式化验证，包括带有α-β剪枝和置换表的变体。


<details>
  <summary>Details</summary>
Motivation: 确保极小化极大搜索算法及其优化变体（如α-β剪枝和置换表）的正确性，以提升算法可靠性。

Method: 引入基于见证的正确性标准，并应用于两种代表性算法，使用Dafny进行形式化验证。

Result: 成功验证了深度受限搜索与置换表的算法，验证产物（包括证明和Python实现）已公开。

Conclusion: 通过形式化验证，证明了所研究算法的正确性，为相关领域提供了可靠的验证方法和实现。

Abstract: Using the Dafny verification system, we formally verify a range of minimax
search algorithms, including variations with alpha-beta pruning and
transposition tables. For depth-limited search with transposition tables, we
introduce a witness-based correctness criterion and apply it to two
representative algorithms. All verification artifacts, including proofs and
Python implementations, are publicly available.

</details>


### [98] [Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI](https://arxiv.org/abs/2509.20175)
*Lorenzo Giusti,Ole Anton Werner,Riccardo Taiello,Matilde Carvalho Costa,Emre Tosun,Andrea Protani,Marc Molina,Rodrigo Lopes de Almeida,Paolo Cacace,Diogo Reis Santos,Luigi Serio*

Main category: cs.AI

TL;DR: FoA框架通过动态任务分解和语义路由，显著提升了异构AI代理的协作效率和性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决静态多代理协调的局限性，FoA旨在通过动态、能力驱动的协作框架，提升异构代理联盟的效率和性能。

Method: FoA结合了语义路由、动态任务分解和智能聚类三大创新技术，通过VCVs实现能力驱动的协作，并利用MQTT的发布-订阅语义实现高效消息传递。

Result: 在HealthBench上的评估显示，FoA比单一模型基线提高了13倍性能，智能聚类对复杂推理任务尤为有效。

Conclusion: FoA框架通过动态协作和语义路由，显著提升了异构AI代理联盟的集体智能表现，尤其在复杂推理任务中展现出13倍于单一模型的性能提升。

Abstract: We present Federation of Agents (FoA), a distributed orchestration framework
that transforms static multi-agent coordination into dynamic, capability-driven
collaboration. FoA introduces Versioned Capability Vectors (VCVs):
machine-readable profiles that make agent capabilities searchable through
semantic embeddings, enabling agents to advertise their capabilities, cost, and
limitations. Our aarchitecturecombines three key innovations: (1) semantic
routing that matches tasks to agents over sharded HNSW indices while enforcing
operational constraints through cost-biased optimization, (2) dynamic task
decomposition where compatible agents collaboratively break down complex tasks
into DAGs of subtasks through consensus-based merging, and (3) smart clustering
that groups agents working on similar subtasks into collaborative channels for
k-round refinement before synthesis. Built on top of MQTT,s publish-subscribe
semantics for scalable message passing, FoA achieves sub-linear complexity
through hierarchical capability matching and efficient index maintenance.
Evaluation on HealthBench shows 13x improvements over single-model baselines,
with clustering-enhanced laboration particularly effective for complex
reasoning tasks requiring multiple perspectives. The system scales horizontally
while maintaining consistent performance, demonstrating that semantic
orchestration with structured collaboration can unlock the collective
intelligence of heterogeneous federations of AI agents.

</details>


### [99] [Design Insights and Comparative Evaluation of a Hardware-Based Cooperative Perception Architecture for Lane Change Prediction](https://arxiv.org/abs/2509.20218)
*Mohamed Manzour,Catherine M. Elias,Omar M. Shehata,Rubén Izquierdo,Miguel Ángel Sotelo*

Main category: cs.AI

TL;DR: 本研究通过实际硬件部署探索协同变道预测，记录实施中的挑战，为类似工作提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有变道预测研究多在模拟环境或预录数据集上进行，缺乏对实际部署中挑战的详细记录，本研究旨在填补这一空白。

Method: 采用实际硬件部署在混合交通中进行协同变道预测，记录实施过程中的挑战和限制。

Result: 研究揭示了实际部署中的瓶颈、可靠性问题和操作限制，为类似系统提供了实用参考。

Conclusion: 本研究通过实际硬件部署探索了协同变道预测，并分享了实施和测试中获得的见解，为类似工作提供了实用指导。

Abstract: Research on lane change prediction has gained attention in the last few
years. Most existing works in this area have been conducted in simulation
environments or with pre-recorded datasets, these works often rely on
simplified assumptions about sensing, communication, and traffic behavior that
do not always hold in practice. Real-world deployments of lane-change
prediction systems are relatively rare, and when they are reported, the
practical challenges, limitations, and lessons learned are often
under-documented. This study explores cooperative lane-change prediction
through a real hardware deployment in mixed traffic and shares the insights
that emerged during implementation and testing. We highlight the practical
challenges we faced, including bottlenecks, reliability issues, and operational
constraints that shaped the behavior of the system. By documenting these
experiences, the study provides guidance for others working on similar
pipelines.

</details>


### [100] [Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent](https://arxiv.org/abs/2509.20270)
*Xingjian Kang,Linda Vorberg,Andreas Maier,Alexander Katzmann,Oliver Taubmann*

Main category: cs.AI

TL;DR: 提出LLM代理框架优化CT扫描协议管理，实验验证其有效性，但面临设备API不统一和复杂请求的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决CT扫描协议管理中耗时且需要专业知识的问题，同时应对放射学领域技术人才短缺的挑战。

Method: 提出了一个结合上下文学习、指令跟随和结构化工具调用能力的LLM代理框架，用于自然语言或结构化格式的协议配置请求的解释和执行。

Result: 实验结果表明，该代理能有效检索协议组件、生成设备兼容的协议定义文件，并准确执行用户请求。

Conclusion: 研究结果表明，基于LLM的代理在CT扫描协议管理中具有明确的应用前景，尽管面临设备API不统一和请求模糊或复杂的挑战。

Abstract: Managing scan protocols in Computed Tomography (CT), which includes adjusting
acquisition parameters or configuring reconstructions, as well as selecting
postprocessing tools in a patient-specific manner, is time-consuming and
requires clinical as well as technical expertise. At the same time, we observe
an increasing shortage of skilled workforce in radiology. To address this
issue, a Large Language Model (LLM)-based agent framework is proposed to assist
with the interpretation and execution of protocol configuration requests given
in natural language or a structured, device-independent format, aiming to
improve the workflow efficiency and reduce technologists' workload. The agent
combines in-context-learning, instruction-following, and structured toolcalling
abilities to identify relevant protocol elements and apply accurate
modifications. In a systematic evaluation, experimental results indicate that
the agent can effectively retrieve protocol components, generate device
compatible protocol definition files, and faithfully implement user requests.
Despite demonstrating feasibility in principle, the approach faces limitations
regarding syntactic and semantic validity due to lack of a unified device API,
and challenges with ambiguous or complex requests. In summary, the findings
show a clear path towards LLM-based agents for supporting scan protocol
management in CT imaging.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [101] [Investigating Sharding Advancements, Methodologies, and Adoption Potential in Hedera](https://arxiv.org/abs/2509.19478)
*Ziwei Wang,Cong Wu,Paolo Tasca*

Main category: cs.DC

TL;DR: 本文研究了分片技术在Hedera区块链中的应用，提出了一种混合分片方案，显著提升了性能和安全。


<details>
  <summary>Details</summary>
Motivation: 分片已成为解决区块链网络可扩展性挑战的关键方案，本研究旨在探讨分片技术在Hedera这一分布式账本技术中的进展、方法及采用潜力。

Method: 探索了学术和工业分片技术，并基于这些见解，提出了一种混合分片解决方案，将网络划分为本地和全局委员会，以促进高效的跨分片交易，并通过动态重新配置确保安全性。

Result: 分析显示，分片技术显著降低了存储和通信开销，提高了可扩展性和容错能力。

Conclusion: 分片技术通过减少存储和通信开销、提高可扩展性和容错能力，证明了将其整合到Hedera架构中的可行性和优势。

Abstract: Sharding has emerged as a critical solution to address the scalability
challenges faced by blockchain networks, enabling them to achieve higher
transaction throughput, reduced latency, and optimized resource usage. This
paper investigates the advancements, methodologies, and adoption potential of
sharding in the context of Hedera, a distributed ledger technology known for
its unique Gossip about Gossip protocol and asynchronous Byzantine Fault
Tolerance (ABFT). We explore various academic and industrial sharding
techniques, emphasizing their benefits and trade-offs. Building on these
insights, we propose a hybrid sharding solution for Hedera that partitions the
network into local and global committees, facilitating efficient cross-shard
transactions and ensuring robust security through dynamic reconfiguration. Our
analysis highlights significant reductions in storage and communication
overhead, improved scalability, and enhanced fault tolerance, demonstrating the
feasibility and advantages of integrating sharding into Hedera's architecture.

</details>


### [102] [To Stream or Not to Stream: Towards A Quantitative Model for Remote HPC Processing Decisions](https://arxiv.org/abs/2509.19532)
*Flavio Castro,Weijian Zheng,Joaquin Chung,Ian Foster,Rajkumar Kettimuthu*

Main category: cs.DC

TL;DR: 论文提出了一种评估远程HPC资源流式传输可行性的定量框架，通过案例研究验证其在减少处理时间方面的有效性，同时指出尾延迟的重要性。


<details>
  <summary>Details</summary>
Motivation: 现代科学仪器生成数据的速度超过本地计算能力，文件传输的I/O开销使得远程HPC资源在时间敏感分析中不实用，现有流式框架缺乏可行性评估方法。

Method: 引入了包含数据生成率、传输效率、远程处理能力和文件I/O开销等关键参数的模型，计算总处理完成时间并确定流式传输的优势区域。

Result: 流式传输在高数据率下比基于文件的方法端到端完成时间减少97%，但最坏情况下传输时间可能增加一个数量级。

Conclusion: 论文提出了一个定量框架和Streaming Speed Score，用于评估远程HPC资源是否能提供比本地更及时的数据处理，并通过案例研究验证了其有效性。

Abstract: Modern scientific instruments generate data at rates that increasingly exceed
local compute capabilities and, when paired with the staging and I/O overheads
of file-based transfers, also render file-based use of remote HPC resources
impractical for time-sensitive analysis and experimental steering. Real-time
streaming frameworks promise to reduce latency and improve system efficiency,
but lack a principled way to assess their feasibility. In this work, we
introduce a quantitative framework and an accompanying Streaming Speed Score to
evaluate whether remote high-performance computing (HPC) resources can provide
timely data processing compared to local alternatives. Our model incorporates
key parameters including data generation rate, transfer efficiency, remote
processing power, and file input/output overhead to compute total processing
completion time and identify operational regimes where streaming is beneficial.
We motivate our methodology with use cases from facilities such as APS, FRIB,
LCLS-II, and the LHC, and validate our approach through an illustrative case
study based on LCLS-II data. Our measurements show that streaming can achieve
up to 97% lower end-to-end completion time than file-based methods under high
data rates, while worst-case congestion can increase transfer times by over an
order of magnitude, underscoring the importance of tail latency in streaming
feasibility decisions.

</details>


### [103] [A Survey of Recent Advancements in Secure Peer-to-Peer Networks](https://arxiv.org/abs/2509.19539)
*Raj Patel,Umesh Biswas,Surya Kodipaka,Will Carroll,Preston Peranich,Maxwell Young*

Main category: cs.DC

TL;DR: 本文综述了P2P网络安全的最新理论进展，分析了经典威胁和新兴趋势的解决方案，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于P2P网络安全领域的最新综述已超过十年，本文旨在填补这一空白，提供一个更新的视角。

Method: 通过文献综述和理论分析，对P2P网络中的经典威胁（如Sybil攻击和路由攻击）及新兴趋势（如机器学习、社交网络和动态系统）进行系统性回顾。

Result: 研究揭示了新兴趋势对P2P网络安全的新挑战，并展示了针对这些挑战的创新解决方案。

Conclusion: 本文总结了P2P网络安全领域的最新理论进展，评估了现有解决方案的优缺点，并提出了未来研究方向。

Abstract: Peer-to-peer (P2P) networks are a cornerstone of modern computing, and their
security is an active area of research. Many defenses with strong security
guarantees have been proposed; however, the most-recent survey is over a decade
old. This paper delivers an updated review of recent theoretical advances that
address classic threats, such as the Sybil and routing attacks, while
highlighting how emerging trends -- such as machine learning, social networks,
and dynamic systems -- pose new challenges and drive novel solutions. We
evaluate the strengths and weaknesses of these solutions and suggest directions
for future research.

</details>


### [104] [Characterizing Adaptive Mesh Refinement on Heterogeneous Platforms with Parthenon-VIBE](https://arxiv.org/abs/2509.19701)
*Akash Poptani,Alireza Khadem,Scott Mahlke,Jonah Miller,Joshua Dolence,Reetuparna Das*

Main category: cs.DC

TL;DR: 该论文分析了块结构AMR基准Parthenon在CPU-GPU系统上的性能，发现小网格块和深AMR级别会降低GPU性能，并提出优化建议以提高吞吐量和减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 分析Parthenon（一种块结构AMR基准）在CPU-GPU系统上的性能，以了解较小网格块和更深AMR级别对GPU性能的影响。

Method: 通过详细的分析和性能剖析，识别了效率低下、低占用率和内存访问瓶颈的问题。

Result: 研究表明，较小的网格块和更深的AMR级别由于增加的通信、串行开销和低效的GPU利用率而降低了GPU性能。

Conclusion: 本研究为未来在能源部即将推出的异构超级计算机上部署AMR提供了有价值的见解，通过优化提高了GPU吞吐量并减少了内存占用。

Abstract: Hero-class HPC simulations rely on Adaptive Mesh Refinement (AMR) to reduce
compute and memory demands while maintaining accuracy. This work analyzes the
performance of Parthenon, a block-structured AMR benchmark, on CPU-GPU systems.
We show that smaller mesh blocks and deeper AMR levels degrade GPU performance
due to increased communication, serial overheads, and inefficient GPU
utilization. Through detailed profiling, we identify inefficiencies, low
occupancy, and memory access bottlenecks. We further analyze rank scalability
and memory constraints, and propose optimizations to improve GPU throughput and
reduce memory footprint. Our insights can inform future AMR deployments on
Department of Energy's upcoming heterogeneous supercomputers.

</details>


### [105] [Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient LLM Inference](https://arxiv.org/abs/2509.19729)
*Haoyu Chen,Xue Li,Kun Qian,Yu Guan,Jin Zhao,Xin Wang*

Main category: cs.DC

TL;DR: Gyges通过自适应并行策略调整，优化LLM服务吞吐量，提升1.75x-6.57x。


<details>
  <summary>Details</summary>
Motivation: 在LLM服务中，动态处理请求的上下文长度差异是关键，但现有并行策略（如TP）在适应大上下文长度时会降低整体吞吐量。

Method: 设计了页面友好的KV缓存布局、专用权重填充和转换感知调度器。

Result: 评估显示，Gyges相比现有方案将吞吐量提升了1.75x-6.57x。

Conclusion: Gyges通过自适应调整并行策略，显著提升了LLM服务场景下的吞吐量，最高可达6.57倍。

Abstract: Efficiently processing the dynamics of requests, especially the context
length variance, is important in Large Language Model (LLM) serving scenarios.
However, there is an intrinsic trade-off: while leveraging parallelism
strategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to
accommodate larger context lengths, it inevitably results in degraded overall
throughput. In this paper, we propose Cross-Instance Parallelism Transformation
(Gyges), which adaptively adjusts the parallelism strategies of running
instances to align with the dynamics of incoming requests. We design (1) a
page-friendly, header-centric layout to accelerate KV cache transformations;
(2) dedicated weight padding to accelerate model weight transformations; and
(3) a transformation-aware scheduler to cooperatively schedule requests and
parallelism transformations, optimizing the overall performance. Evaluations
using real-world traces show that Gyges improves throughput by 1.75x-6.57x
compared to state-of-the-art solutions.

</details>


### [106] [BurstEngine: an Efficient Distributed Framework for Training Transformers on Extremely Long Sequences of over 1M Tokens](https://arxiv.org/abs/2509.19836)
*Ao Sun,Weilin Zhao,Xu Han,Cheng Yang,Zhiyuan Liu,Chuan Shi,Maosong sun*

Main category: cs.DC

TL;DR: BurstEngine是一个高效训练LLM长序列数据的框架，通过优化分布式注意力和内存管理，显著提升了速度和降低了内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长序列数据和GPU数量增加时，尤其是序列长度超过1M tokens时，模型FLOPs利用率较低。

Method: BurstEngine提出BurstAttention，一种优化的分布式注意力机制，具有比RingAttention更低的通信成本。此外，还引入了序列级选择性检查点和语言建模头与损失函数的融合以减少内存开销。

Result: BurstEngine在训练超过1M tokens的超长序列时，实现了1.2倍的加速，并且内存开销显著低于现有基线方法。

Conclusion: BurstEngine通过集成多种优化技术，在训练超过1M tokens的超长序列LLM时，实现了比现有基线方法更快的速度和更低的内存开销。

Abstract: Existing methods for training LLMs on long-sequence data, such as Tensor
Parallelism and Context Parallelism, exhibit low Model FLOPs Utilization as
sequence lengths and number of GPUs increase, especially when sequence lengths
exceed 1M tokens. To address these challenges, we propose BurstEngine, an
efficient framework designed to train LLMs on long-sequence data. BurstEngine
introduces BurstAttention, an optimized distributed attention with lower
communication cost than RingAttention. BurstAttention leverages topology-aware
ring communication to fully utilize network bandwidth and incorporates
fine-grained communication-computation overlap. Furthermore, BurstEngine
introduces sequence-level selective checkpointing and fuses the language
modeling head with the loss function to reduce memory cost. Additionally,
BurstEngine introduces workload balance optimization for various types of
attention masking. By integrating these optimizations, BurstEngine achieves a
$1.2\times$ speedup with much lower memory overhead than the state-of-the-art
baselines when training LLMs on extremely long sequences of over 1M tokens. We
have made our code publicly available on GitHub:
https://github.com/thunlp/BurstEngine.

</details>


### [107] [Characterizing the Performance of Accelerated Jetson Edge Devices for Training Deep Learning Models](https://arxiv.org/abs/2509.20160)
*Prashanthi S. K.,Sai Anuroop Kesanapalli,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 该论文系统研究了边缘设备上的DNN训练性能，揭示了资源限制的影响，并提出了优化模型。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的DNN训练未被充分探索，但其资源限制和异构性对训练性能有显著影响，需系统研究以优化性能。

Method: 通过在不同Jetson设备上训练三种DNN模型，调整设备参数（如I/O流水线、存储介质、批量大小和电源模式），分析对资源利用、训练时间和能耗的影响。

Result: 揭示了资源间的相互依赖关系和反直觉现象，提出了预测训练时间和能耗的简单模型，为边缘设备选择和参数调优提供依据。

Conclusion: 该研究为边缘设备上的DNN训练提供了系统的性能分析和优化建议，未来可扩展至联邦学习。

Abstract: Deep Neural Networks (DNNs) have had a significant impact on domains like
autonomous vehicles and smart cities through low-latency inferencing on edge
computing devices close to the data source. However, DNN training on the edge
is poorly explored. Techniques like federated learning and the growing capacity
of GPU-accelerated edge devices like NVIDIA Jetson motivate the need for a
holistic characterization of DNN training on the edge. Training DNNs is
resource-intensive and can stress an edge's GPU, CPU, memory and storage
capacities. Edge devices also have different resources compared to workstations
and servers, such as slower shared memory and diverse storage media. Here, we
perform a principled study of DNN training on individual devices of three
contemporary Jetson device types: AGX Xavier, Xavier NX and Nano for three
diverse DNN model--dataset combinations. We vary device and training parameters
such as I/O pipelining and parallelism, storage media, mini-batch sizes and
power modes, and examine their effect on CPU and GPU utilization, fetch stalls,
training time, energy usage, and variability. Our analysis exposes several
resource inter-dependencies and counter-intuitive insights, while also helping
quantify known wisdom. Our rigorous study can help tune the training
performance on the edge, trade-off time and energy usage on constrained
devices, and even select an ideal edge hardware for a DNN workload, and, in
future, extend to federated learning too. As an illustration, we use these
results to build a simple model to predict the training time and energy per
epoch for any given DNN across different power modes, with minimal additional
profiling.

</details>


### [108] [Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge Accelerators](https://arxiv.org/abs/2509.20189)
*Prashanthi S. K.,Kunal Kumar Sahoo,Amartya Ranjan Saikia,Pranav Gupta,Atharva Vinay Joshi,Priyanshu Pansari,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 论文开发了时间屋顶和能量屋顶模型，结合DNN分析模型，揭示了边缘加速器在不同电源模式下的性能与能耗行为，并优化了延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 缺乏对边缘加速器及其电源模式性能表现背后原理的系统性研究，需要从基本原理出发理解其性能与能耗行为。

Method: 开发了时间屋顶和能量屋顶模型，结合DNN推理的计算和内存访问分析模型，对不同电源模式进行原理性分析。

Result: 揭示了DNN工作负载在边缘加速器上的独特性能与能耗行为，优化电源模式可降低15%的能耗且推理时间退化最小。

Conclusion: 通过时间屋顶和能量屋顶模型，结合DNN推理的分析模型，揭示了边缘加速器在DNN工作负载下的独特性能与能耗行为，并成功优化了延迟和能耗。

Abstract: Edge accelerators such as Nvidia Jetsons are becoming an integral part of the
computing continuum, and are often used for DNN inferencing and training.
Nvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power
envelope and offer $1000$s of power modes to customize CPU, GPU and memory
frequencies. Their widely varying power--performance trade-offs can be
exploited for energy and power-constrained deployments. While data-driven
methods to predict the power and latency of DNN workloads for edge devices
exist, there is a lack of principled study to understand why edge accelerators
and their power modes perform the way they do. We develop a time roofline and a
novel energy roofline model for the Jetson Orin AGX for diverse power modes,
and couple it with an analytical model of the compute (FLOP) and memory access
(bytes) for DNN inference workloads to analyze them from first principles.
These reveal unique, sometimes counter-intuitive, insights into the power and
performance behavior of DNN workloads on edge accelerators, e.g., the default
power mode MAXN is not the most energy efficient and time efficiency implies
energy efficiency for all power modes. We also extend our analytical roofline
models to DNN training. Finally, we apply these methods to tune the power mode
(and hence the roofline) of the edge device to optimize the latency and energy
for DNN inference, with up to $15\%$ lower energy and minimal degradation in
inference time.

</details>


### [109] [Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge Accelerators](https://arxiv.org/abs/2509.20205)
*Prashanthi S. K.,Saisamarth Taluri,Pranav Gupta,Amartya Ranjan Saikia,Kunal Kumar Sahoo,Atharva Vinay Joshi,Lakshya Karwa,Kedar Dhule,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 提出GMD和ALS方法，优化边缘设备上DNN训练和推断的并发执行，满足性能目标且接近最优吞吐量。


<details>
  <summary>Details</summary>
Motivation: GPU加速边缘设备（如Nvidia Jetsons）的普及和隐私问题的增加，促使需要在边缘设备上同时进行DNN训练和推断。然而，这些设备不支持原生GPU共享且存在大量功耗模式，需要优化时间共享以满足性能目标。

Method: 设计了一种智能时间切片方法，包括GMD（高效多维梯度下降搜索）和ALS（主动学习技术），用于在Jetson设备上并发执行DNN训练和推断。

Result: GMD和ALS在273,000多种配置和15种DNN工作负载中表现优异，满足97%以上运行的延迟和功耗预算，平均接近最优吞吐量的7%。

Conclusion: GMD和ALS方法在满足延迟和功耗预算的同时，显著提升了训练吞吐量，且在大多数情况下接近最优性能。

Abstract: The proliferation of GPU accelerated edge devices like Nvidia Jetsons and the
rise in privacy concerns are placing an emphasis on concurrent DNN training and
inferencing on edge devices. Inference and training have different computing
and QoS goals. But edge accelerators like Jetson do not support native GPU
sharing and expose 1000s of power modes. This requires careful time-sharing of
concurrent workloads to meet power--performance goals, while limiting costly
profiling. In this paper, we design an intelligent time-slicing approach for
concurrent DNN training and inferencing on Jetsons. We formulate an
optimization problem to interleave training and inferencing minibatches, and
decide the device power mode and inference minibatch size, while maximizing the
training throughput and staying within latency and power budgets, with modest
profiling costs. We propose GMD, an efficient multi-dimensional gradient
descent search which profiles just $15$ power modes; and ALS, an Active
Learning technique which identifies reusable Pareto-optimal power modes, but
profiles $50$--$150$ power modes. We evaluate these within our Fulcrum
scheduler for $273,000+$ configurations across $15$ DNN workloads. We also
evaluate our strategies on dynamic arrival inference and concurrent inferences.
ALS and GMD outperform simpler and more complex baselines with larger-scale
profiling. Their solutions satisfy the latency and power budget for $>97\%$ of
our runs, and on average are within $7\%$ of the optimal throughput.

</details>


### [110] [An Empirical Analysis of Secure Federated Learning for Autonomous Vehicle Applications](https://arxiv.org/abs/2509.20223)
*Md Jueal Mia,M. Hadi Amini*

Main category: cs.DC

TL;DR: 联邦学习在自动驾驶中易受攻击，研究通过安全聚合和多方计算提升其安全性，实证验证了效果。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽能保障数据隐私并提升模型性能，但仍易受网络攻击威胁，需更强大的安全措施。

Method: 采用多种安全聚合技术和多方计算对交通图像数据集（如LISA交通灯）进行实证分析。

Result: 实证分析验证了安全聚合和多方计算在抵御投毒和推理攻击中的有效性。

Conclusion: 研究表明，安全聚合技术和多方计算能有效提升联邦学习在自动驾驶应用中的安全性，抵御多种网络攻击。

Abstract: Federated Learning lends itself as a promising paradigm in enabling
distributed learning for autonomous vehicles applications and ensuring data
privacy while enhancing and refining predictive model performance through
collaborative training on edge client vehicles. However, it remains vulnerable
to various categories of cyber-attacks, necessitating more robust security
measures to effectively mitigate potential threats. Poisoning attacks and
inference attacks are commonly initiated within the federated learning
environment to compromise secure system performance. Secure aggregation can
limit the disclosure of sensitive information from outsider and insider
attackers of the federated learning environment. In this study, our aim is to
conduct an empirical analysis on the transportation image dataset (e.g., LISA
traffic light) using various secure aggregation techniques and multiparty
computation in the presence of diverse categories of cyber-attacks. Multiparty
computation serves as a state-of-the-art security mechanism, offering standard
privacy for secure aggregation of edge autonomous vehicles local model updates
through various security protocols. The presence of adversaries can mislead the
autonomous vehicle learning model, leading to the misclassification of traffic
lights, and resulting in detrimental impacts. This empirical study explores the
resilience of various secure federated learning aggregation techniques and
multiparty computation in safeguarding autonomous vehicle applications against
various cyber threats during both training and inference times.

</details>


### [111] [xGFabric: Coupling Sensor Networks and HPC Facilities with Private 5G Wireless Networks for Real-Time Digital Agriculture](https://arxiv.org/abs/2509.20340)
*Liubov Kurafeeva,Alan Subedi,Ryan Hartung,Michael Fay,Avhishek Biswas,Shantenu Jha,Ozgur O. Kilic,Chandra Krintz,Andre Merzky,Douglas Thain,Mehmet C. Vuran,Rich Wolski*

Main category: cs.DC

TL;DR: xGFabric通过私有5G网络将传感器网络与HPC设施耦合，实现了实时数字农业模拟。


<details>
  <summary>Details</summary>
Motivation: 高级科学应用需要将分布式传感器网络与集中式高性能计算设施耦合。CUPS在数字农业中的需求体现了这一点，其中柑橘研究设施配备了众多传感器监测环境条件并检测保护屏损坏。

Method: 本研究提出了xGFabric，一个通过私有5G网络将传感器网络与高性能计算设施耦合的端到端系统。原型系统通过5G网络切片将远程传感器连接到HPC系统。

Result: xGFabric原型系统成功连接了远程传感器与HPC系统，实现了实时数字农业模拟。

Conclusion: xGFabric成功通过私有5G网络将传感器网络与高性能计算设施耦合，实现了实时数字农业模拟，展示了其在解决分布式传感器网络与集中式高性能计算设施耦合挑战中的潜力。

Abstract: Advanced scientific applications require coupling distributed sensor networks
with centralized high-performance computing facilities. Citrus Under Protective
Screening (CUPS) exemplifies this need in digital agriculture, where citrus
research facilities are instrumented with numerous sensors monitoring
environmental conditions and detecting protective screening damage. CUPS
demands access to computational fluid dynamics codes for modeling environmental
conditions and guiding real-time interventions like water application or
robotic repairs. These computing domains have contrasting properties: sensor
networks provide low-performance, limited-capacity, unreliable data access,
while high-performance facilities offer enormous computing power through
high-latency batch processing. Private 5G networks present novel capabilities
addressing this challenge by providing low latency, high throughput, and
reliability necessary for near-real-time coupling of edge sensor networks with
HPC simulations. This work presents xGFabric, an end-to-end system coupling
sensor networks with HPC facilities through Private 5G networks. The prototype
connects remote sensors via 5G network slicing to HPC systems, enabling
real-time digital agriculture simulation.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [112] [A Note on Fine-Grained Quantum Reductions for Linear Algebraic Problems](https://arxiv.org/abs/2509.19528)
*Kyle Doney,Cameron Musco*

Main category: cs.DS

TL;DR: 量子计算机上线性代数核心问题的复杂度与矩阵乘法等价，改进后者将直接提升前者。


<details>
  <summary>Details</summary>
Motivation: 探索线性代数核心问题（如计算行列式、迹等）在量子计算机上的复杂度是否与矩阵乘法等价。

Method: 通过Bernstein-Vazirani算法将矩阵乘法量子化归约到计算$tr(ABC)$，再进一步归约到目标问题。

Result: 证明了这些问题的复杂度在量子计算机上等同于矩阵乘法，且任何超越当前$\Theta(n^\omega)$时间复杂度的改进都将加速量子矩阵乘法。

Conclusion: 任何在量子计算机上改进矩阵乘法复杂度的算法都将直接提升线性代数核心问题的计算效率。

Abstract: We observe that any $T(n)$ time algorithm (quantum or classical) for several
central linear algebraic problems, such as computing $\det(A)$, $tr(A^3)$, or
$tr(A^{-1})$ for an $n \times n$ integer matrix $A$, yields a $O(T(n)) + \tilde
O(n^2)$ time \textit{quantum algorithm} for $n \times n$ matrix-matrix
multiplication. That is, on quantum computers, the complexity of these problems
is essentially equivalent to that of matrix multiplication. Our results follow
by first observing that the Bernstein-Vazirani algorithm gives a direct quantum
reduction from matrix multiplication to computing $tr(ABC)$ for $n \times n$
inputs $A,B,C$. We can then reduce $tr(ABC)$ to each of our problems of
interest.
  For the above problems, and many others in linear algebra, their fastest
known algorithms require $\Theta(n^\omega)$ time, where $\omega \approx 2.37$
is the current exponent of fast matrix multiplication. Our finding shows that
any improvements beyond this barrier would lead to faster quantum algorithms
for matrix multiplication. Our results complement existing reductions from
matrix multiplication in algebraic circuits [BCS13], and reductions that work
for standard classical algorithms, but are not tight -- i.e., which roughly
show that an $O(n^{3-\delta})$ time algorithm for the problem yields an
$O(n^{3-\delta/3})$ matrix multiplication algorithm [WW10].

</details>


### [113] [A Better-Than-$5/4$-Approximation for Two-Edge Connectivity](https://arxiv.org/abs/2509.19655)
*Felix Hommelsheim,Alexander Lindermayr,Zhenwei Liu*

Main category: cs.DS

TL;DR: 本研究提出了一种(5/4 - η)-近似算法，突破了2ECSS问题的5/4近似比限制，通过互补算法和新技术克服了技术障碍。


<details>
  <summary>Details</summary>
Motivation: 2ECSS问题是生存网络设计中的基础问题，具有广泛的实际应用。尽管已有研究达到了5/4的近似比，但这一限制是否可突破尚不明确。本研究旨在打破这一自然障碍，提供更优的近似算法。

Method: 采用三角形无关的2-边覆盖，并通过添加少量边将其转换为2-边连通的生成子图。设计了两种互补算法：一种针对少量4-循环，另一种针对大量4-循环。引入新技术如彩色桥覆盖、丰富顶点和分支粘合路径。

Result: 提出了一种(5/4 - η)-近似算法，其中η ≥ 10^-6，突破了现有的5/4近似比限制。

Conclusion: 本研究突破了现有的5/4近似比限制，提出了一种(5/4 - η)-近似算法，其中η ≥ 10^-6。通过设计两种互补算法和引入新技术（如彩色桥覆盖、丰富顶点和分支粘合路径），成功克服了技术障碍。

Abstract: The 2-Edge-Connected Spanning Subgraph Problem (2ECSS) is a fundamental
problem in survivable network design. Given an undirected $2$-edge-connected
graph, the goal is to find a $2$-edge-connected spanning subgraph with the
minimum number of edges; a graph is 2-edge-connected if it is connected after
the removal of any single edge. 2ECSS is APX-hard and has been extensively
studied in the context of approximation algorithms. Very recently, Bosch-Calvo,
Garg, Grandoni, Hommelsheim, Jabal Ameli, and Lindermayr showed the currently
best-known approximation ratio of $\frac{5}{4}$ [STOC 2025]. This factor is
tight for many of their techniques and arguments, and it was not clear whether
$\frac{5}{4}$ can be improved.
  We break this natural barrier and present a $(\frac{5}{4} -
\eta)$-approximation algorithm, for some constant $\eta \geq 10^{-6}$. On a
high level, we follow the approach of previous works: take a triangle-free
$2$-edge cover and transform it into a 2-edge-connected spanning subgraph by
adding only a few additional edges. For $\geq \frac{5}{4}$-approximations, one
can heavily exploit that a $4$-cycle in the 2-edge cover can ``buy'' one
additional edge. This enables simple and nice techniques, but immediately fails
for our improved approximation ratio. To overcome this, we design two
complementary algorithms that perform well for different scenarios: one for few
$4$-cycles and one for many $4$-cycles. Besides this, there appear more
obstructions when breaching $\frac54$, which we surpass via new techniques such
as colorful bridge covering, rich vertices, and branching gluing paths.

</details>


### [114] [Non-Clairvoyant Scheduling with Progress Bars](https://arxiv.org/abs/2509.19662)
*Ziyad Benomar,Romain Cosson,Alexander Lindermayr,Jens Schlöter*

Main category: cs.DS

TL;DR: 论文研究了非预见性调度中利用进度条反馈的方法，设计了对抗性和随机性进度条的新算法，证明了其强竞争性，并改进了学习增强调度的预测性能。还提出了一个乐观的随机模型和渐近最优算法。


<details>
  <summary>Details</summary>
Motivation: 非预见性调度中缺乏对作业处理时间的先验知识，导致调度效率受限。通过引入进度条反馈（即作业完成比例的实时估计），论文探索了如何利用这种连续反馈优化调度性能。

Method: 论文设计了针对对抗性和随机性进度条的新调度算法，并提出了一个通用的调度算法组合方法。在随机性进度条模型中，还提出了一种渐近最优的调度算法。

Result: 在对抗性进度条情况下，算法实现了强竞争性保证，并意外地改进了基于作业大小预测的学习增强调度。在随机性进度条模型中，提出的算法达到了渐近最优。

Conclusion: 论文提出了一种在非预见性调度中利用进度条反馈的新方法，并在对抗性和随机性进度条情况下设计了新算法，证明了其强竞争性。此外，该方法还改进了基于作业大小预测的学习增强调度保证，并引入了一种通用的调度算法组合方法。最后，论文提出了一个乐观的随机进度条模型，并展示了在该模型下的渐近最优调度算法。

Abstract: In non-clairvoyant scheduling, the goal is to minimize the total job
completion time without prior knowledge of individual job processing times.
This classical online optimization problem has recently gained attention
through the framework of learning-augmented algorithms. We introduce a natural
setting in which the scheduler receives continuous feedback in the form of
progress bars: estimates of the fraction of each job completed over time. We
design new algorithms for both adversarial and stochastic progress bars and
prove strong competitive bounds. Our results in the adversarial case
surprisingly induce improved guarantees for learning-augmented scheduling with
job size predictions. We also introduce a general method for combining
scheduling algorithms, yielding further insights in scheduling with
predictions. Finally, we propose a stochastic model of progress bars as a more
optimistic alternative to conventional worst-case models, and present an
asymptotically optimal scheduling algorithm in this setting.

</details>


### [115] [SS-GUMAP, SL-GUMAP, SSSL-GUMAP: Fast UMAP Algorithms for Large Graph Drawing](https://arxiv.org/abs/2509.19703)
*Amyra Meidiana,Seok-Hee Hong*

Main category: cs.DS

TL;DR: UMAP在图形绘制中表现出色，三种优化算法显著提升速度，同时保持高质量。


<details>
  <summary>Details</summary>
Motivation: UMAP在降维中表现优秀，但未在图形绘制中得到充分评估，且直接应用时计算复杂度高，难以扩展到大规模图形。

Method: 提出了三种基于UMAP的快速图形绘制算法：SS-GUMAP、SSL-GUMAP和SSSL-GUMAP，分别通过谱稀疏化、部分BFS和边采样优化运行时间。

Result: 实验表明，SS-GUMAP比GUMAP快28%，而SSL-GUMAP和SSSL-GUMAP比GUMAP快80%以上，质量指标差异小于15%。

Conclusion: UMAP在图形绘制中表现出色，不仅运行速度显著快于tsNET，而且在质量指标上也优于传统方法。

Abstract: UMAP is a popular neighborhood-preserving dimension reduction (DR) algorithm.
However, its application for graph drawing has not been evaluated. Moreover, a
naive application of UMAP to graph drawing would include O(nm) time all-pair
shortest path computation, which is not scalable to visualizing large graphs.
  In this paper, we present fast UMAP-based for graph drawing. Specifically, we
present three fast UMAP-based algorithms for graph drawing: (1) The SS-GUMAP
algorithm utilizes spectral sparsification to compute a subgraph G' preserving
important properties of a graph G, reducing the O(nm) component of the runtime
to O(n^2 log n) runtime; (2) The SSL-GUMAP algorithm reduces the kNN (k-Nearest
Neighbors) graph computation from $O(n \log n)$ time to linear time using
partial BFS (Breadth First Search), and the cost optimization runtime from O(n)
time to sublinear time using edge sampling; (3) The SSSL-GUMAP algorithm
combines both approaches, for an overall O(n) runtime.
  Experiments demonstrate that SS-GUMAP runs 28% faster than GUMAP, a naive
application of UMAP to graph drawing, with similar quality metrics, while
SL-GUMAP and SSSL-GUMAP run over 80% faster than GUMAP with less than 15%
difference on average for all quality metrics.
  We also present an evaluation of GUMAP to tsNET, a graph layout based on the
popular DR algorithm t-SNE. GUMAP runs 90% faster than tsNET with similar
neighborhood preservation and, on average, 10% better on quality metrics such
as stress, edge crossing, and shape-based metrics, validating the effectiveness
of UMAP for graph drawing.

</details>


### [116] [Geometric Interpretation of 3-SAT and Phase Transition](https://arxiv.org/abs/2509.19740)
*Frederic Gillet*

Main category: cs.DS

TL;DR: 将3-SAT问题视为体积填充问题，研究其相变行为，揭示了SAT/UNSAT过渡的几何特性。


<details>
  <summary>Details</summary>
Motivation: 探索3-SAT问题的相变行为，特别是SAT/UNSAT过渡的几何解释。

Method: 通过将3-SAT问题建模为体积填充问题，分析其几何特性。

Result: 研究展示了3-SAT问题在相变点附近的体积填充特性与可满足性的关联。

Conclusion: 将3-SAT问题解释为体积填充问题为探索SAT/UNSAT相变提供了新的视角。

Abstract: Interpretation of 3-SAT as a volume filling problem, and its use to explore
the SAT/UNSAT phase transition.

</details>


### [117] [BH-tsNET, FIt-tsNET, L-tsNET: Fast tsNET Algorithms for Large Graph Drawing](https://arxiv.org/abs/2509.19785)
*Amyra Meidiana,Seok-Hee Hong,Kwan-Liu Ma*

Main category: cs.DS

TL;DR: 论文提出了三种快速tsNET算法，大幅降低运行时间（O(n log n)和O(n)），同时保持绘图质量，实验验证了其高效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 原始tsNET算法的时间复杂度较高（O(nm)），限制了其在大规模数据上的应用。通过优化关键组件（C0、C1、C2）的计算效率，目标是显著降低运行时间。

Method: 提出了三种快速算法：1）BH-tsNET，基于部分BFS的高维概率计算和四叉树熵计算；2）FIt-tsNET，结合插值法KL散度计算；3）L-tsNET，利用FFT加速插值熵计算。

Result: 实验表明，BH-tsNET、FIt-tsNET和L-tsNET分别比原始tsNET快93.5%、96%和98.6%，同时在绘图质量（邻域保持、应力、边交叉等）上表现相似。

Conclusion: tsNET算法通过三种快速算法（BH-tsNET、FIt-tsNET和L-tsNET）显著降低了时间复杂度，同时保持了高质量的图形绘制效果。实验证明这些算法在运行速度和绘图质量上均优于原始tsNET和DRGraph算法。

Abstract: The tsNET algorithm utilizes t-SNE to compute high-quality graph drawings,
preserving the neighborhood and clustering structure. We present three fast
algorithms for reducing the time complexity of tsNET algorithm from O(nm) time
to O(n log n) time and O(n) time. To reduce the runtime of tsNET, there are
three components that need to be reduced: (C0) computation of high-dimensional
probabilities, (C1) computation of KL divergence gradient, and (C2) entropy
computation. Specifically, we reduce the overall runtime of tsNET, integrating
our new fast approaches for C0 and C2 with fast t-SNE algorithms for C1. We
first present O(n log n)-time BH-tsNET, based on (C0) new O(n)-time partial
BFS-based high-dimensional probability computation and (C2) new O(n log n)-time
quadtree-based entropy computation, integrated with (C1) O(n log n)-time
quadtree-based KL divergence computation of BH-SNE. We next present faster O(n
log n)-time FIt-tsNET, using (C0) O(n)-time partial BFS-based high-dimensional
probability computation and (C2) quadtree-based O(n log n)-time entropy
computation, integrated with (C1) O(n)-time interpolation-based KL divergence
computation of FIt-SNE. Finally, we present the O(n)-time L-tsNET, integrating
(C2) new O(n)-time FFT-accelerated interpolation-based entropy computation with
(C0) O(n)-time partial BFS-based high-dimensional probability computation, and
(C1) O(n)-time interpolation-based KL divergence computation of FIt-SNE.
Extensive experiments using benchmark data sets confirm that BH-tsNET,
FIt-tsNET, and L-tsNET outperform tsNET, running 93.5%, 96%, and 98.6% faster
while computing similar quality drawings in terms of quality metrics
(neighborhood preservation, stress, edge crossing, and shape-based metrics) and
visual comparison. We also present a comparison between our algorithms and
DRGraph, another dimension reduction-based graph drawing algorithm.

</details>


### [118] [Stealing From the Dragon's Hoard: Online Unbounded Knapsack With Removal](https://arxiv.org/abs/2509.19914)
*Matthias Gehnen,Moritz Stocker*

Main category: cs.DS

TL;DR: 论文提出在线无限制背包问题与移除的变种，证明存在竞争性确定性算法，并给出具体竞争比。在比例设置下，确定性算法可达3/2竞争比，随机化算法的上下界为6/5和4/3。


<details>
  <summary>Details</summary>
Motivation: 研究在线背包问题的变种，探索在允许物品无限次打包和免费移除的情况下，是否能设计出竞争性算法。

Method: 通过引入在线无限制背包问题与移除的变种，设计算法并分析其竞争比。在一般设置下提供竞争比为1.6911的算法，并给出下界1.5877。在比例设置下分析确定性算法的竞争比，并探讨随机化算法的上下界。

Result: 在一般设置下，算法的竞争比为1.6911，下界为1.5877；在比例设置下，确定性算法的竞争比为3/2，随机化算法的上下界分别为6/5和4/3。

Conclusion: 该论文展示了在线无限制背包问题与移除的新变种，证明了在一般设置下存在竞争性确定性算法，并提供了具体的竞争比。在比例设置下，确定性算法可以达到精确的3/2竞争比。此外，还给出了随机化算法的竞争比上下界。

Abstract: We introduce the Online Unbounded Knapsack Problem with Removal, a variation
of the well-known Online Knapsack Problem. Items, each with a weight and value,
arrive online and an algorithm must decide on whether or not to pack them into
a knapsack with a fixed weight limit. An item may be packed an arbitrary number
of times and items may be removed from the knapsack at any time without cost.
The goal is to maximize the total value of items packed, while respecting a
weight limit. We show that this is one of the very few natural online knapsack
variants that allow for competitive deterministic algorithms in the general
setting, by providing an algorithm with competitivity 1.6911. We complement
this with a lower bound of 1.5877.
  We also analyze the proportional setting, where the weight and value of any
single item agree, and show that deterministic algorithms can be exactly
3/2-competitive. Lastly, we give lower and upper bounds of 6/5 and 4/3 on the
competitivity of randomized algorithms in this setting.

</details>


### [119] [Ads that Stick: Near-Optimal Ad Optimization through Psychological Behavior Models](https://arxiv.org/abs/2509.20304)
*Kailash Gopal Darmasubramanian,Akash Pareek,Arindam Khan,Arpit Agarwal*

Main category: cs.DS

TL;DR: 本文提出了一种基于心理行为模型的广告调度算法，通过建模用户兴趣变化，生成接近最优的广告展示方案，实验证明其优于传统启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有广告调度策略依赖简单启发式方法（如均匀间隔和频率上限），忽视了用户长期兴趣的动态变化。本文通过心理行为模型填补这一空白。

Method: 基于三个关键心理原则（单纯曝光效应、享乐适应和操作条件反射）建模用户兴趣变化，提出了一种准线性时间算法来生成接近最优的广告调度方案。

Result: 算法生成的广告调度方案在性能上与最优方案的差异指数级小，且通过实验证明优于多种基线方法。

Conclusion: 本文提出了一种基于心理行为模型的广告调度算法，能够显著提升用户对广告的长期兴趣，并通过实验验证了其优于现有启发式方法。

Abstract: Optimizing the timing and frequency of ads is a central problem in digital
advertising, with significant economic consequences. Existing scheduling
policies rely on simple heuristics, such as uniform spacing and frequency caps,
that overlook long-term user interest. However, it is well-known that users'
long-term interest and engagement result from the interplay of several
psychological effects (Curmei, Haupt, Recht, Hadfield-Menell, ACM CRS, 2022).
  In this work, we model change in user interest upon showing ads based on
three key psychological principles: mere exposure, hedonic adaptation, and
operant conditioning. The first two effects are modeled using a concave
function of user interest with repeated exposure, while the third effect is
modeled using a temporal decay function, which explains the decline in user
interest due to overexposure. Under our psychological behavior model, we ask
the following question: Given a continuous time interval $T$, how many ads
should be shown, and at what times, to maximize the user interest towards the
ads?
  Towards answering this question, we first show that, if the number of
displayed ads is fixed, then the optimal ad-schedule only depends on the
operant conditioning function. Our main result is a quasi-linear time algorithm
that outputs a near-optimal ad-schedule, i.e., the difference in the
performance of our schedule and the optimal schedule is exponentially small.
Our algorithm leads to significant insights about optimal ad placement and
shows that simple heuristics such as uniform spacing are sub-optimal under many
natural settings. The optimal number of ads to display, which also depends on
the mere exposure and hedonistic adaptation functions, can be found through a
simple linear search given the above algorithm. We further support our findings
with experimental results, demonstrating that our strategy outperforms various
baselines.

</details>


### [120] [Testable algorithms for approximately counting edges and triangles in sublinear time and space](https://arxiv.org/abs/2509.20351)
*Talya Eden,Ronitt Rubinfeld,Arsen Vasilyan*

Main category: cs.DS

TL;DR: 该论文提出了一种无需预先知道树状性即可高效近似计算图中边和三角形数量的算法，具有适应性和鲁棒性，适用于各种图结构。


<details>
  <summary>Details</summary>
Motivation: 解决现有算法在树状性边界未知或错误时无法保证估计正确性的问题，提出一种适应性强、鲁棒的算法。

Method: 通过尝试一系列候选树状性值，并结合可测试算法框架中的新算法，确保错误候选不会导致错误估计。算法在运行时动态调整，一旦接受候选值，输出结果即具有高概率的正确性。

Result: 提出的算法在时间复杂性上接近已知树状性边界时的最优效率，同时保证了结果的正确性。实验证明了算法在各种图上的有效性和鲁棒性。

Conclusion: 该论文提出了一种无需预先知道图的具体性质（如树状性）即可高效近似计算图中边和三角形数量的算法，并通过实验验证了其鲁棒性和高效性。

Abstract: We consider the fundamental problems of approximately counting the numbers of
edges and triangles in a graph in sublinear time. Previous algorithms for these
tasks are significantly more efficient under a promise that the arboricity of
the graph is bounded by some parameter $\overline{\alpha}$. However, when this
promise is violated, the estimates given by these algorithms are no longer
guaranteed to be correct.
  For the triangle counting task, we give an algorithm that requires no promise
on the input graph $G$, and computes a $(1\pm \epsilon)$-approximation for the
number of triangles $t$ in $G$ in time $O^*\left( \frac{m\cdot \alpha(G)}{t} +
\frac{m}{t^{2/3}}
  \right)$, where $\alpha(G)$ is the arboricity of the graph. The algorithm can
be used on any graph $G$ (no prior knowledge the arboricity $\alpha(G)$ is
required), and the algorithm adapts its run-time on the fly based on the graph
$G$.
  We accomplish this by trying a sequence of candidate values $\tilde{\alpha}$
for $\alpha(G)$ and using a novel algorithm in the framework of testable
algorithms. This ensures that wrong candidates $\tilde{\alpha}$ cannot lead to
incorrect estimates: as long as the advice is incorrect, the algorithm detects
it and continues with a new candidate. Once the algorithm accepts the
candidate, its output is guaranteed to be correct with high probability.
  We prove that this approach preserves - up to an additive overhead - the
dramatic efficiency gains obtainable when good arboricity bounds are known in
advance, while ensuring robustness against misleading advice. We further
complement this result with a lower bound, showing that such an overhead is
unavoidable whenever the advice may be faulty.
  We further demonstrate implications of our results for triangle counting in
the streaming model.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [121] [Radio Propagation Modelling: To Differentiate or To Deep Learn, That Is The Question](https://arxiv.org/abs/2509.19337)
*Stefanos Bakirtzis,Paul Almasan,José Suárez-Varela,Gabriel O. Ferreira,Michail Kalntis,André Felipe Zanella,Ian Wassell,Andra Lutu*

Main category: cs.NI

TL;DR: 可微分光线追踪虽改进效率-准确性，但大规模泛化和实时应用不足；深度学习模型更优，准确性提升达3 dB。


<details>
  <summary>Details</summary>
Motivation: 填补可微分光线追踪在生产级网络中可扩展性和实际效益的实验评估空白，为移动网络运营商和研究社区提供明确的适用性指导。

Method: 采用可微分光线追踪和深度学习模型，利用从主要移动网络运营商收集的覆盖13个城市和10,000多个天线的真实世界数据，模拟无线电覆盖。

Result: 可微分光线追踪在大规模真实数据泛化和实时应用上表现不佳，而深度学习模型在准确性（最高提升3 dB）和适应性上更具优势。

Conclusion: 可微分光线追踪虽然在效率和准确性上有所改进，但在大规模实际数据泛化和实时应用方面仍显不足，而深度学习模型表现出更高的准确性和更快的适应性，为无线生态系统和未来研究提供了重要参考。

Abstract: Differentiable ray tracing has recently challenged the status quo in radio
propagation modelling and digital twinning. Promising unprecedented speed and
the ability to learn from real-world data, it offers a real alternative to
conventional deep learning (DL) models. However, no experimental evaluation on
production-grade networks has yet validated its assumed scalability or
practical benefits. This leaves mobile network operators (MNOs) and the
research community without clear guidance on its applicability. In this paper,
we fill this gap by employing both differentiable ray tracing and DL models to
emulate radio coverage using extensive real-world data collected from the
network of a major MNO, covering 13 cities and more than 10,000 antennas. Our
results show that, while differentiable ray-tracing simulators have contributed
to reducing the efficiency-accuracy gap, they struggle to generalize from
real-world data at a large scale, and they remain unsuitable for real-time
applications. In contrast, DL models demonstrate higher accuracy and faster
adaptation than differentiable ray-tracing simulators across urban, suburban,
and rural deployments, achieving accuracy gains of up to 3 dB. Our experimental
results aim to provide timely insights into a fundamental open question with
direct implications on the wireless ecosystem and future research.

</details>


### [122] [Fine-Grained AI Model Caching and Downloading With Coordinated Multipoint Broadcasting in Multi-Cell Edge Networks](https://arxiv.org/abs/2509.19341)
*Yang Fu,Peng Qin,Yueyue Zhang,Yifei Wang*

Main category: cs.NI

TL;DR: 论文提出了一种6G网络中基于参数重用的AI模型缓存与下载系统，通过分布式多智能体学习优化缓存与传输，显著降低延迟并提升频谱效率。


<details>
  <summary>Details</summary>
Motivation: 6G网络中AI模型下载需求多样且模型体积庞大，传统的边缘缓存和无线传输方式面临存储容量和频谱效率的挑战。为应对这些挑战，论文提出了一种新的系统设计。

Method: 论文提出了一种细粒度AI模型缓存与下载系统，利用参数可重用性优化缓存策略，并结合CoMP广播技术提高频谱利用率。通过分布式多智能体学习框架和自适应数据增强方法，解决了参数块缓存、迁移和波束成形的联合优化问题。

Result: 理论分析和仿真实验验证了所提学习框架的优越收敛性能，系统能够显著降低模型下载延迟并提高频谱利用率。

Conclusion: 该论文提出的细粒度AI模型缓存与下载系统通过参数重用和CoMP广播技术，显著降低了模型下载延迟，并通过分布式多智能体学习框架实现了高效的参数块缓存与迁移优化。

Abstract: 6G networks are envisioned to support on-demand AI model downloading to
accommodate diverse inference requirements of end users. By proactively caching
models at edge nodes, users can retrieve the requested models with low latency
for on-device AI inference. However, the substantial size of contemporary AI
models poses significant challenges for edge caching under limited storage
capacity, as well as for the concurrent delivery of heterogeneous models over
wireless channels. To address these challenges, we propose a fine-grained AI
model caching and downloading system that exploits parameter reusability,
stemming from the common practice of fine-tuning task-specific models from a
shared pre-trained model with frozen parameters. This system selectively caches
model parameter blocks (PBs) at edge nodes, eliminating redundant storage of
reusable parameters across different cached models. Additionally, it
incorporates coordinated multipoint (CoMP) broadcasting to simultaneously
deliver reusable PBs to multiple users, thereby enhancing downlink spectrum
utilization. Under this arrangement, we formulate a model downloading delay
minimization problem to jointly optimize PB caching, migration (among edge
nodes), and broadcasting beamforming. To tackle this intractable problem, we
develop a distributed multi-agent learning framework that enables edge nodes to
explicitly learn mutual influence among their actions, thereby facilitating
cooperation. Furthermore, a data augmentation approach is proposed to
adaptively generate synthetic training samples through a predictive model,
boosting sample efficiency and accelerating policy learning. Both theoretical
analysis and simulation experiments validate the superior convergence
performance of the proposed learning framework.

</details>


### [123] [TinyAC: Bringing Autonomic Computing Principles to Resource-Constrained Systems](https://arxiv.org/abs/2509.19350)
*Wojciech Kalka,Ruitao Xue,Kamil Faber,Aleksander Slominski,Devki Jha,Rajiv Ranjan,Tomasz Szydlo*

Main category: cs.NI

TL;DR: 本文探讨了自主计算在物联网设备中的应用挑战，提出结合TinyML和LLMs的混合方法，并讨论了自适应功能和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨自主计算在物联网设备中应用的问题和挑战，旨在开发智能且自适应的微小系统。

Method: 提出了一种混合方法，结合TinyML和设备端学习的自下而上智能与大型语言模型（LLMs）的自上而下引导，以实现可扩展和可解释的智能自适应微小系统开发。

Result: 提出了一种混合方法，并讨论了TinyAC系统需要自适应功能以应对操作中的问题，同时指出了现有挑战和未来研究方向。

Conclusion: 本文总结了自主计算（AC）在物联网设备应用中的挑战和未来研究方向，强调了结合自下而上智能和自上而下引导的混合方法的重要性。

Abstract: Autonomic Computing (AC) is a promising approach for developing intelligent
and adaptive self-management systems at the deep network edge. In this paper,
we present the problems and challenges related to the use of AC for IoT
devices. Our proposed hybrid approach bridges bottom-up intelligence (TinyML
and on-device learning) and top-down guidance (LLMs) to achieve a scalable and
explainable approach for developing intelligent and adaptive self-management
tiny systems. Moreover, we argue that TinyAC systems require self-adaptive
features to handle problems that may occur during their operation. Finally, we
identify gaps, discuss existing challenges and future research directions.

</details>


### [124] [A User-to-User Resource Reselling Game in Open RAN with Buffer Rollover](https://arxiv.org/abs/2509.19392)
*Ruide Cao,Marie Siew,David Yau*

Main category: cs.NI

TL;DR: 论文提出了一种O-RAN中基于游戏的PRB转售模型，通过纳什均衡和迭代竞价机制，显著减少了数据丢失和频谱浪费，提高了社会福利。


<details>
  <summary>Details</summary>
Motivation: O-RAN的灵活性为未使用的物理资源块（PRB）在用户间的转售提供了机会，以提高频谱效率并更好地满足用户动态和异构的服务需求。

Method: 论文提出了一种新颖的游戏理论模型，用于O-RAN环境中的PRB转售。该方法通过建模未满足需求的跨时隙传递和用户内部缓冲区状态与PRB购买的关系，将用户间的交互建模为战略游戏，并证明了纳什均衡的存在性和唯一性。此外，还提出了一种收敛到该均衡的迭代竞价机制。

Result: 实验结果表明，该方法减少了30.5%的数据丢失和50.7%的频谱资源浪费，同时显著提高了社会福利。

Conclusion: 该论文提出了一种基于游戏的用户间PRB转售模型，在O-RAN环境中显著提高了频谱效率和社会福利。通过证明纳什均衡的存在性和唯一性，并提出一种迭代竞价机制，实验结果显示数据丢失减少了30.5%，频谱资源浪费降低了50.7%。

Abstract: The development of the Open RAN (O-RAN) framework helps enable network
slicing through its virtualization, interoperability, and flexibility. To
improve spectral efficiency and better meet users' dynamic and heterogeneous
service demands, O-RAN's flexibility further presents an opportunity for
resource reselling of unused physical resource blocks (PRBs) across users. In
this work, we propose a novel game-based user-to-user PRB reselling model in
the O-RAN setting, which models the carryover of unmet demand across time
slots, along with how users' internal buffer states relate to any PRBs
purchased. We formulate the interplay between the users as a strategic game,
with each participant aiming to maximize their own payoffs, and we prove the
existence and uniqueness of the Nash equilibrium (NE) in the game. We
furthermore propose an iterative bidding mechanism that converges to this NE.
Extensive simulations show that our best approach reduces data loss by 30.5%
and spectrum resource wastage by 50.7% while significantly improving social
welfare, compared to its absence.

</details>


### [125] [FedOC: Multi-Server FL with Overlapping Client Relays in Wireless Edge Networks](https://arxiv.org/abs/2509.19398)
*Yun Ji,Zeyu Chen,Xiaoxiong Zhong,Yanan Ma,Sheng Zhang,Yuguang Fang*

Main category: cs.NI

TL;DR: FedOC是一种多服务器联邦学习框架，通过重叠客户端（ROC和NOC）实现模型共享和训练加速，适用于边缘环境，实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多服务器联邦学习中，不同边缘服务器覆盖的区域可能存在重叠，重叠客户端可以访问多个边缘服务器的模型。充分利用这些重叠客户端的潜力，可以解决单服务器联邦学习的通信瓶颈问题。

Method: FedOC框架中，重叠客户端作为ROC实时转发边缘模型以促进模型共享，作为NOC根据边缘模型交付时间动态选择初始模型进行本地训练。每个客户端在每轮训练中基于最早接收的边缘模型训练本地模型并传输给相应的边缘服务器进行模型聚合。边缘服务器通过ROC中继将聚合后的边缘模型传输给邻近服务器，进行二次聚合后广播更新后的模型。

Result: 实验结果表明，FedOC方案相比现有方法具有显著的性能提升。

Conclusion: FedOC框架通过利用重叠客户端的双重角色（ROC和NOC），成功实现了多服务器联邦学习中的高效模型共享和训练加速，特别适合延迟敏感的边缘环境。

Abstract: Multi-server Federated Learning (FL) has emerged as a promising solution to
mitigate communication bottlenecks of single-server FL. We focus on a typical
multi-server FL architecture, where the regions covered by different edge
servers (ESs) may overlap. A key observation of this architecture is that
clients located in the overlapping areas can access edge models from multiple
ESs. Building on this insight, we propose FedOC (Federated learning with
Overlapping Clients), a novel framework designed to fully exploit the potential
of these overlapping clients. In FedOC, overlapping clients could serve dual
roles: (1) as Relay Overlapping Clients (ROCs), they forward edge models
between neighboring ESs in real time to facilitate model sharing among
different ESs; and (2) as Normal Overlapping Clients (NOCs), they dynamically
select their initial model for local training based on the edge model delivery
time, which enables indirect data fusion among different regions of ESs. The
overall FedOC workflow proceeds as follows: in every round, each client trains
local model based on the earliest received edge model and transmits to the
respective ESs for model aggregation. Then each ES transmits the aggregated
edge model to neighboring ESs through ROC relaying. Upon receiving the relayed
models, each ES performs a second aggregation and subsequently broadcasts the
updated model to covered clients. The existence of ROCs enables the model of
each ES to be disseminated to the other ESs in a decentralized manner, which
indirectly achieves intercell model and speeding up the training process,
making it well-suited for latency-sensitive edge environments. Extensive
experimental results show remarkable performance gains of our scheme compared
to existing methods.

</details>


### [126] [Improving Outdoor Multi-cell Fingerprinting-based Positioning via Mobile Data Augmentation](https://arxiv.org/abs/2509.19405)
*Tony Chahoud,Lorenzo Mario Amorosa,Riccardo Marini,Luca De Nardis*

Main category: cs.NI

TL;DR: 本文提出了一种轻量级、模块化的数据增强框架，通过KDE和KNN方法提升蜂窝网络定位性能，尤其在稀疏或复杂区域效果显著。


<details>
  <summary>Details</summary>
Motivation: 蜂窝网络中稀疏且异构的测量数据收集以及高成本的全面站点调查阻碍了准确的户外定位。

Method: 提出了一种轻量级、模块化的移动数据增强框架，通过核密度估计（KDE）生成地理一致的合成位置，并基于k近邻（KNN）的模块增强每小区的无线电指纹。

Result: 结果表明，提出的KDE-KNN增强方法一致提高了定位性能，在稀疏采样或结构复杂区域效果尤为显著。

Conclusion: 该框架提供了一种实用且低复杂度的路径，利用现有的移动数据轨迹增强运营商定位服务。

Abstract: Accurate outdoor positioning in cellular networks is hindered by sparse,
heterogeneous measurement collections and the high cost of exhaustive site
surveys. This paper introduces a lightweight, modular mobile data augmentation
framework designed to enhance multi-cell fingerprinting-based positioning using
operator-collected minimization of drive test (MDT) records. The proposed
approach decouples spatial and radio-feature synthesis: kernel density
estimation (KDE) models the empirical spatial distribution to generate
geographically coherent synthetic locations, while a k-nearest-neighbor
(KNN)-based block produces augmented per-cell radio fingerprints. The
architecture is intentionally training-free, interpretable, and suitable for
distributed or on-premise operator deployments, supporting privacy-aware
workflows. We both validate each augmentation module independently and assess
its end-to-end impact on fingerprinting-based positioning using a real-world
MDT dataset provided by an Italian mobile network operator across diverse urban
and peri-urban scenarios. Results show that the proposed KDE-KNN augmentation
consistently improves positioning performance, with the largest benefits in
sparsely sampled or structurally complex regions; we also observe
region-dependent saturation effects as augmentation increases. The framework
offers a practical, low-complexity path to enhance operator positioning
services using existing mobile data traces.

</details>


### [127] [Poster: ChatIYP: Enabling Natural Language Access to the Internet Yellow Pages Database](https://arxiv.org/abs/2509.19411)
*Vasilis Andritsoudis,Pavlos Sermpezis,Ilias Dimitriadis,Athena Vakali*

Main category: cs.NI

TL;DR: ChatIYP是一个基于RAG的系统，允许通过自然语言查询IYP，评估显示其在简单查询上表现良好，并提供了改进方向。


<details>
  <summary>Details</summary>
Motivation: 解决Internet Yellow Pages（IYP）查询需要Cypher语言和精确的IYP模式知识的问题，提高非专业人士的可用性。

Method: 提出了ChatIYP，一个特定领域的检索增强生成（RAG）系统，允许用户通过自然语言问题查询IYP。

Result: 评估显示在简单查询上表现良好，并指出了改进方向。

Conclusion: ChatIYP展示了在简单查询上的良好性能，并提供了改进方向，为选择更适合IYP查询AI代理的评估指标提供了见解。

Abstract: The Internet Yellow Pages (IYP) aggregates information from multiple sources
about Internet routing into a unified, graph-based knowledge base. However,
querying it requires knowledge of the Cypher language and the exact IYP schema,
thus limiting usability for non-experts. In this paper, we propose ChatIYP, a
domain-specific Retrieval-Augmented Generation (RAG) system that enables users
to query IYP through natural language questions. Our evaluation demonstrates
solid performance on simple queries, as well as directions for improvement, and
provides insights for selecting evaluation metrics that are better fit for IYP
querying AI agents.

</details>


### [128] [Where 6G Stands Today: Evolution, Enablers, and Research Gaps](https://arxiv.org/abs/2509.19646)
*Salma Tika,Abdelkrim Haqiq,Essaid Sabir,Elmahdi Driouch*

Main category: cs.NI

TL;DR: 本文综述了6G的主要需求、关键技术、应用场景及潜在挑战，为未来通信系统的发展提供了全面视角。


<details>
  <summary>Details</summary>
Motivation: 随着5G的全球部署，6G概念化旨在解决日益增长的高可靠性、无缝自动化和无处不在覆盖的需求。

Method: 本文通过全面综述6G的主要严格要求和关键技术（如太赫兹通信、智能反射面、大规模MIMO和AI驱动网络）来探讨6G网络。

Result: 6G将带来高度智能、自动化和超可靠的通信系统，能够处理大量连接设备。

Conclusion: 本文概述了6G的潜在挑战，强调了实现6G承诺所需解决的问题。

Abstract: As the fifth-generation (5G) mobile communication system continues its global
deployment, both industry and academia have started conceptualizing the 6th
generation (6G) to address the growing need for a progressively advanced and
digital society. Even while 5G offers considerable advancements over LTE, it
could struggle to be sufficient to meet all of the requirements, including
ultra-high reliability, seamless automation, and ubiquitous coverage. In
response, 6G is supposed to bring out a highly intelligent, automated, and
ultra-reliable communication system that can handle a vast number of connected
devices. This paper offers a comprehensive overview of 6G, beginning with its
main stringent requirements while focusing on key enabling technologies such as
terahertz (THz) communications, intelligent reflecting surfaces, massive MIMO
and AI-driven networking that will shape the 6G networks. Furthermore, the
paper lists various 6G applications and usage scenarios that will benefit from
these advancements. At the end, we outline the potential challenges that must
be addressed to achieve the 6G promises.

</details>


### [129] [RIS-assisted Data Collection and Wireless Power Transfer in Low-altitude Wireless Networks](https://arxiv.org/abs/2509.19651)
*Wenwen Xie,Geng Sun,Jiahui Li,Jiacheng Wang,Yinqiu Liu,Dusit Niyato,Dong In Kim,Shiwen Mao*

Main category: cs.NI

TL;DR: 研究提出了一种RIS辅助的无人机数据收集系统，采用AO-IPDQN方法优化系统性能，显著降低AoI和无人机能耗。


<details>
  <summary>Details</summary>
Motivation: 解决低空无线网络（LAWNs）中能量受限和信道质量差的物联网设备（IoTDs）数据收集问题。

Method: 采用交替优化（AO）方法优化RIS相位偏移，降低动作空间维度；提出改进的参数化深度Q网络（IPDQN）处理混合动作空间。

Result: AO-IPDQN方法在多个仿真场景中优于其他对比方法。

Conclusion: AO-IPDQN方法在多种仿真场景中表现出色，显著降低了信息年龄（AoI）和无人机能耗。

Abstract: Low-altitude wireless networks (LAWNs) have become effective solutions for
collecting data from low-power Internet-of-Things devices (IoTDs) in remote
areas with limited communication infrastructure. However, some outdoor IoTDs
deployed in such areas face both energy constraints and low-channel quality
challenges, making it challenging to ensure timely data collection from these
IoTDs in LAWNs. In this work, we investigate a reconfigurable intelligent
surface (RIS)-assisted uncrewed aerial vehicle (UAV)-enabled data collection
and wireless power transfer system in LAWN. Specifically, IoTDs first harvest
energy from a low-altitude UAV, and then upload their data to the UAV by
applying the time division multiple access (TDMA) protocol, supported by an RIS
to improve the channel quality. To maintain satisfactory data freshness of the
IoTDs and save energy for an energy-constrained UAV, we aim to minimize the age
of information (AoI) and energy consumption of the UAV by jointly optimizing
the RIS phase shits, UAV trajectory, charging time allocation, and binary IoTD
scheduling. We propose a deep reinforcement learning (DRL)-based approach,
namely the alternating optimization-improved parameterized deep Q-network
(AO-IPDQN). Specifically, considering that RIS typically contains a large
number of reflecting elements, we first adopt an alternating optimization (AO)
method to optimize the RIS phase shifts to reduce the dimension of the action
space. Then, we propose the improved parameterized deep Q-network (IPDQN)
method to deal with the hybrid action space. Simulation results indicate that
AO-IPDQN approach achieves excellent performance relative to multiple
comparison methods across various simulation scenarios.

</details>


### [130] [Games Are Not Equal: Classifying Cloud Gaming Contexts for Effective User Experience Measurement](https://arxiv.org/abs/2509.19669)
*Yifan Wang,Minzhao Lyu,Vijay Sivaraman*

Main category: cs.NI

TL;DR: 该论文提出了一种通过分析网络流量实时测量云游戏用户体验的方法，帮助网络运营商更有效地评估资源配置。


<details>
  <summary>Details</summary>
Motivation: 随着云游戏市场的增长，网络运营商需要动态配置网络资源以提供可盈利的保障服务。然而，缺乏准确测量云游戏用户体验的方法，无法评估资源配置的有效性。

Method: 通过分析网络流量，包括游戏标题和玩家活动阶段等上下文因素，该方法能够在游戏启动后的前五秒内分类游戏标题，并持续评估玩家活动阶段（活跃、被动或闲置）。

Result: 该方法在托管NVIDIA云游戏服务器的ISP中部署，并从三个月的数十万次云游戏流会话中提供了带宽消耗和体验水平对游戏上下文依赖的见解。

Conclusion: 该论文为网络运营商提供了一种实时测量云游戏用户体验的方法，通过分析网络流量和游戏上下文因素，如游戏标题和玩家活动阶段，从而更有效地评估资源配置方法的有效性。

Abstract: To tap into the growing market of cloud gaming, whereby game graphics is
rendered in the cloud and streamed back to the user as a video feed, network
operators are creating monetizable assurance services that dynamically
provision network resources. However, without accurately measuring cloud gaming
user experience, they cannot assess the effectiveness of their provisioning
methods. Basic measures such as bandwidth and frame rate by themselves do not
suffice, and can only be interpreted in the context of the game played and the
player activity within the game. This paper equips the network operator with a
method to obtain a real-time measure of cloud gaming experience by analyzing
network traffic, including contextual factors such as the game title and player
activity stage. Our method is able to classify the game title within the first
five seconds of game launch, and continuously assess the player activity stage
as being active, passive, or idle. We deploy it in an ISP hosting NVIDIA cloud
gaming servers for the region. We provide insights from hundreds of thousands
of cloud game streaming sessions over a three-month period into the dependence
of bandwidth consumption and experience level on the gameplay contexts.

</details>


### [131] [SPARQ: An Optimization Framework for the Distribution of AI-Intensive Applications under Non-Linear Delay Constraints](https://arxiv.org/abs/2509.19913)
*Pietro Spadaccino,Paolo Di Lorenzo,Sergio Barbarossa,Antonia M. Tulino,Jaime Llorca*

Main category: cs.NI

TL;DR: SPARQ是一种新型迭代算法，优化边缘-云中的AI应用编排，解决了非线性延迟问题，提升了效率和成本-延迟权衡。


<details>
  <summary>Details</summary>
Motivation: 现有模型未能准确捕捉AI密集型工作负载中延迟与资源使用的非线性关系，需改进边缘-云基础设施中的分布式AI应用编排。

Method: 引入两种执行模型（GR和SR）并结合M/M/1和M/G/1队列动态，开发了SPARQ迭代逼近算法。

Result: 仿真结果表明，SPARQ更准确地表示系统延迟，并显著提升资源效率和成本-延迟权衡。

Conclusion: SPARQ算法通过分解非凸优化问题为两个凸子问题，显著提升了资源效率和成本-延迟权衡，优于现有方法。

Abstract: Next-generation real-time compute-intensive applications, such as extended
reality, multi-user gaming, and autonomous transportation, are increasingly
composed of heterogeneous AI-intensive functions with diverse resource
requirements and stringent latency constraints. While recent advances have
enabled very efficient algorithms for joint service placement, routing, and
resource allocation for increasingly complex applications, current models fail
to capture the non-linear relationship between delay and resource usage that
becomes especially relevant in AI-intensive workloads. In this paper, we extend
the cloud network flow optimization framework to support queuing-delay-aware
orchestration of distributed AI applications over edge-cloud infrastructures.
We introduce two execution models, Guaranteed-Resource (GR) and Shared-Resource
(SR), that more accurately capture how computation and communication delays
emerge from system-level resource constraints. These models incorporate M/M/1
and M/G/1 queue dynamics to represent dedicated and shared resource usage,
respectively. The resulting optimization problem is non-convex due to the
non-linear delay terms. To overcome this, we develop SPARQ, an iterative
approximation algorithm that decomposes the problem into two convex
sub-problems, enabling joint optimization of service placement, routing, and
resource allocation under nonlinear delay constraints. Simulation results
demonstrate that the SPARQ not only offers a more faithful representation of
system delays, but also substantially improves resource efficiency and the
overall cost-delay tradeoff compared to existing state-of-the-art methods.

</details>


### [132] [A Novel Short-Term Anomaly Prediction for IIoT with Software Defined Twin Network](https://arxiv.org/abs/2509.20068)
*Bilal Dalgic,Betul Sen,Muge Erel-Ozcevik*

Main category: cs.NI

TL;DR: 该论文提出了一种基于SDN数字孪生的新型框架，用于IIoT环境中的短期异常检测，GPU加速的LightGBM模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前文献缺乏基于SDN的数字孪生实现细节及针对IIoT威胁的短期异常检测的时间感知智能模型训练方法。

Method: 提出了一种基于SDN的数字孪生（SD-TWIN）框架，结合时间感知特征标记和多种机器学习模型的综合评估。

Result: 提出的SD-TWIN异常检测算法在全面数据集上表现良好，特别是在GPU加速的LightGBM模型中。

Conclusion: GPU加速的LightGBM模型在实时SD-TWIN部署中表现出色，实现了高召回率和强分类性能的平衡。

Abstract: Secure monitoring and dynamic control in an IIoT environment are major
requirements for current development goals. We believe that dynamic, secure
monitoring of the IIoT environment can be achieved through integration with the
Software-Defined Network (SDN) and Digital Twin (DT) paradigms. The current
literature lacks implementation details for SDN-based DT and time-aware
intelligent model training for short-term anomaly detection against IIoT
threats. Therefore, we have proposed a novel framework for short-term anomaly
detection that uses an SDN-based DT. Using a comprehensive dataset, time-aware
labeling of features, and a comprehensive evaluation of various machine
learning models, we propose a novel SD-TWIN-based anomaly detection algorithm.
According to the performance of a new real-time SD-TWIN deployment, the GPU-
accelerated LightGBM model is particularly effective, achieving a balance of
high recall and strong classification performance.

</details>


### [133] [Can LLMs Forecast Internet Traffic from Social Media?](https://arxiv.org/abs/2509.20123)
*Jonatan Langlet,Mariano Scazzariello,Flavio Luciani,Marta Burocchi,Dejan Kostić,Marco Chiesa*

Main category: cs.NI

TL;DR: 论文提出利用公共讨论信号（如标题、论坛和社交媒体）作为早期需求指标，以补充技术测量，并验证了一个能预测社会驱动流量峰值的原型系统。


<details>
  <summary>Details</summary>
Motivation: 社会事件（如公众人物去世、软件发布或重大体育比赛）会引发突发的流量激增，而现有的预测系统仅依赖常规流量模式，无法捕捉这些关键异常。

Method: 论文提出了一个概念验证系统，该系统通过自动抓取在线讨论、推断现实世界事件、语义聚类和丰富，并将其与主要互联网交换点的流量测量相关联。

Result: 原型系统在抓取适量在线讨论后，成功预测了56-92%由社会驱动的流量峰值。

Conclusion: 该论文提出了一种结合社会事件和公共讨论的互联网流量预测方法，为跨领域预测、调度和需求预期开辟了新的研究方向。

Abstract: Societal events shape the Internet's behavior. The death of a prominent
public figure, a software launch, or a major sports match can trigger sudden
demand surges that overwhelm peering points and content delivery networks.
Although these events fall outside regular traffic patterns, forecasting
systems still rely solely on those patterns and therefore miss these critical
anomalies.
  Thus, we argue for socio-technical systems that supplement technical
measurements with an active understanding of the underlying drivers, including
how events and collective behavior shape digital demands. We propose traffic
forecasting using signals from public discourse, such as headlines, forums, and
social media, as early demand indicators.
  To validate our intuition, we present a proof-of-concept system that
autonomously scrapes online discussions, infers real-world events, clusters and
enriches them semantically, and correlates them with traffic measurements at a
major Internet Exchange Point. This prototype predicted between 56-92% of
society-driven traffic spikes after scraping a moderate amount of online
discussions.
  We believe this approach opens new research opportunities in cross-domain
forecasting, scheduling, demand anticipation, and society-informed decision
making.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [134] [Automated Insertion of Flushes and Fences for Persistency](https://arxiv.org/abs/2509.19459)
*Yutong Guo,Weiyu Luo,Brian Demsky*

Main category: cs.SE

TL;DR: PMRobust是一种编译器，通过静态分析自动插入flush和fence操作，解决持久内存中缺失flush的bug问题，性能开销极低。


<details>
  <summary>Details</summary>
Motivation: 现有工具在检测缺失flush指令时需要依赖bug暴露的测试用例，且无法确保完全消除缺失flush的bug，因此需要一种更可靠的解决方案。

Method: PMRobust采用新颖的静态分析技术，并针对新分配的对象进行优化，自动插入必要的flush和fence操作。

Result: 在持久内存库和多种持久内存数据结构上的评估显示，PMRobust相对于手动放置flush和fence操作的基准测试，几何平均开销仅为0.26%。

Conclusion: PMRobust编译器通过自动插入flush和fence操作，有效解决了持久内存使用中缺失flush和fence的bug问题，且性能开销极低（几何平均开销0.26%）。

Abstract: CXL shared memory and persistent memory allow the contents of memory to
persist beyond crashes. Stores to persistent or CXL memory are typically not
immediately made persistent; developers must manually flush the corresponding
cache lines to force the data to be written to the underlying storage.
Correctly using flush and fence operations is known to be challenging. While
state-of-the-art tools can find missing flush instructions, they often require
bug-revealing test cases. No existing tools can ensure the absence of missing
flush bugs.
  In this paper, we present PMRobust, a compiler that automatically inserts
flush and fence operations to ensure that code using persistent memory is free
from missing flush and fence bugs. PMRobust employs a novel static analysis
with optimizations that target newly allocated objects. We have evaluated
PMRobust on persistent memory libraries and several persistent memory data
structures and measured a geometric mean overhead of 0.26% relative to the
original benchmarks with hand-placed flush and fence operations.

</details>


### [135] [Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation](https://arxiv.org/abs/2509.19533)
*Mengdi Lu,Steven Ding,Furkan Alaca,Philippe Charland*

Main category: cs.SE

TL;DR: 论文提出一个集成推理型LLMs与AFL++的框架，解决了传统模糊测试缺乏语义推理的问题，实验表明Deepseek在仅提示条件下表现最佳，但响应延迟和吞吐量仍是瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统变异模糊测试工具缺乏语义推理能力，而具备推理能力的大型语言模型（LLMs）能利用预训练知识理解输入格式和复杂约束，但缺乏监督微调的实际可行性，因此探索了基于提示的少量样本学习。

Method: 提出了一个开源微服务框架，将推理型LLMs与AFL++集成在Google的FuzzBench上，解决了异步执行和硬件需求差异（GPU与CPU密集型）的问题。

Result: 评估了四个研究问题：（R1）如何将推理型LLMs集成到模糊测试变异循环中；（R2）少量样本提示是否比零样本产生更高质量的变异；（R3）提示工程能否直接改进模糊测试；（R4）哪些开源推理型LLMs在仅提示条件下表现最佳。实验发现Deepseek表现最优。

Conclusion: 实验结果表明，Deepseek在仅提示条件下表现最佳，突变效果更依赖于提示复杂度和模型选择而非样本数量。响应延迟和吞吐量瓶颈仍是主要障碍，为未来工作提供了方向。

Abstract: Security vulnerabilities in Internet-of-Things devices, mobile platforms, and
autonomous systems remain critical. Traditional mutation-based fuzzers -- while
effectively explore code paths -- primarily perform byte- or bit-level edits
without semantic reasoning. Coverage-guided tools such as AFL++ use
dictionaries, grammars, and splicing heuristics to impose shallow structural
constraints, leaving deeper protocol logic, inter-field dependencies, and
domain-specific semantics unaddressed. Conversely, reasoning-capable large
language models (LLMs) can leverage pretraining knowledge to understand input
formats, respect complex constraints, and propose targeted mutations, much like
an experienced reverse engineer or testing expert. However, lacking ground
truth for "correct" mutation reasoning makes supervised fine-tuning
impractical, motivating explorations of off-the-shelf LLMs via prompt-based
few-shot learning. To bridge this gap, we present an open-source microservices
framework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,
tackling asynchronous execution and divergent hardware demands (GPU- vs.
CPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)
How can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do
few-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt
engineering with off-the-shelf models improve fuzzing directly? and (R4) Which
open-source reasoning LLMs perform best under prompt-only conditions?
Experiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3
highlight Deepseek as the most promising. Mutation effectiveness depends more
on prompt complexity and model choice than shot count. Response latency and
throughput bottlenecks remain key obstacles, offering directions for future
work.

</details>


### [136] [Reverse Engineering User Stories from Code using Large Language Models](https://arxiv.org/abs/2509.19587)
*Mohamed Ouf,Haoyu Li,Michael Zhang,Mariam Guizani*

Main category: cs.SE

TL;DR: 研究显示，LLM能有效从源代码中恢复用户故事，且小模型在示例引导下可媲美大模型性能，而链式思考对大模型提升有限。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索LLM是否能直接从源代码中自动恢复用户故事，并分析提示设计对输出质量的影响。

Method: 通过1,750个标注的C++代码片段，评估了五种最先进的LLM在六种提示策略下的表现。

Result: 所有模型在不超过200 NLOC的代码上平均F1得分为0.8。

Conclusion: 研究发现，即使是较小的8B模型，在提供单一示例的情况下也能达到与70B模型相当的性能，而链式思考（Chain-of-Thought）对大型模型的提升有限。

Abstract: User stories are essential in agile development, yet often missing or
outdated in legacy and poorly documented systems. We investigate whether large
language models (LLMs) can automatically recover user stories directly from
source code and how prompt design impacts output quality. Using 1,750 annotated
C++ snippets of varying complexity, we evaluate five state-of-the-art LLMs
across six prompting strategies. Results show that all models achieve, on
average, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a
single illustrative example enables the smallest model (8B) to match the
performance of a much larger 70B model. In contrast, structured reasoning via
Chain-of-Thought offers only marginal gains, primarily for larger models.

</details>


### [137] [Assertion Messages with Large Language Models (LLMs) for Code](https://arxiv.org/abs/2509.19673)
*Ahmed Aljohani,Anamul Haque Mollah,Hyunsook Do*

Main category: cs.SE

TL;DR: 本文评估了四种LLMs生成断言消息的能力，发现Codestral-22B表现最佳，上下文信息（如测试注释）能进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 断言消息能显著增强单元测试，但开发者和自动化测试生成工具经常忽略它们。尽管LLMs有进步，但尚未系统评估其生成信息性断言消息的能力。

Method: 本文评估了四种最先进的FIM LLMs（Qwen2.5-Coder-32B、Codestral-22B、CodeLlama-13B和StarCoder）在包含开发者编写的断言消息的216个Java测试方法数据集上的表现。

Result: Codestral-22B在人类类似评估方法中得分最高（2.76/5），而手动编写消息得分为3.24。包含描述性测试注释可将Codestral性能提升至2.97。

Conclusion: 本文的基准测试、评估结果和讨论为推进测试代码中上下文感知的断言消息自动生成提供了重要基础。

Abstract: Assertion messages significantly enhance unit tests by clearly explaining the
reasons behind test failures, yet they are frequently omitted by developers and
automated test-generation tools. Despite recent advancements, Large Language
Models (LLMs) have not been systematically evaluated for their ability to
generate informative assertion messages. In this paper, we introduce an
evaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -
Qwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset
of 216 Java test methods containing developer-written assertion messages. We
find that Codestral-22B achieves the highest quality score of 2.76 out of 5
using a human-like evaluation approach, compared to 3.24 for manually written
messages. Our ablation study shows that including descriptive test comments
further improves Codestral's performance to 2.97, highlighting the critical
role of context in generating clear assertion messages. Structural analysis
demonstrates that all models frequently replicate developers' preferred
linguistic patterns. We discuss the limitations of the selected models and
conventional text evaluation metrics in capturing diverse assertion message
structures. Our benchmark, evaluation results, and discussions provide an
essential foundation for advancing automated, context-aware generation of
assertion messages in test code. A replication package is available at
https://doi.org/10.5281/zenodo.15293133

</details>


### [138] [Intuition to Evidence: Measuring AI's True Impact on Developer Productivity](https://arxiv.org/abs/2509.19708)
*Anand Kumar,Vishal Khare,Deepak Sharma,Satyam Kumar,Vijay Saini,Anshul Yadav,Sachendra Jain,Ankit Rana,Pratham Verma,Vaibhav Meena,Avinash Edubilli*

Main category: cs.SE

TL;DR: 该论文通过一年期企业级实证研究，证明AI辅助开发工具显著提升开发效率（PR审核时间减少31.8%）和代码交付量（增加28%），同时揭示了实际部署中的挑战。


<details>
  <summary>Details</summary>
Motivation: 评估AI辅助开发工具在真实企业环境中的实际效果，以验证其在提升开发效率和代码质量方面的潜力。

Method: 采用纵向队列分析，对300名工程师使用内部AI平台（DeputyDev）的情况进行为期一年的跟踪，结合代码生成和自动审核功能，评估其对开发流程的影响。

Result: 研究结果显示，使用AI平台后，PR审核周期时间减少了31.8%，开发者满意度达85%，代码交付量增加了28%，且顶级使用者的代码推送量增加了61%。

Conclusion: 该研究通过实证分析展示了AI辅助开发工具在企业级软件工作流程中的实际效果，证实了其显著提升生产力和代码交付量的潜力，同时也揭示了实际部署中的挑战。

Abstract: We present a comprehensive real-world evaluation of AI-assisted software
development tools deployed at enterprise scale. Over one year, 300 engineers
across multiple teams integrated an in-house AI platform (DeputyDev) that
combines code generation and automated review capabilities into their daily
workflows. Through rigorous cohort analysis, our study demonstrates
statistically significant productivity improvements, including an overall 31.8%
reduction in PR review cycle time.
  Developer adoption was strong, with 85% satisfaction for code review features
and 93% expressing a desire to continue using the platform. Adoption patterns
showed systematic scaling from 4% engagement in month 1 to 83% peak usage by
month 6, stabilizing at 60% active engagement. Top adopters achieved a 61%
increase in code volume pushed to production, contributing to approximately 30
to 40% of code shipped to production through this tool, accounting for an
overall 28% increase in code shipment volume.
  Unlike controlled benchmark evaluations, our longitudinal analysis provides
empirical evidence from production environments, revealing both the
transformative potential and practical deployment challenges of integrating AI
into enterprise software development workflows.

</details>


### [139] [Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation](https://arxiv.org/abs/2509.19918)
*Micheline Bénédicte Moumoula,Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: XL-CoGen通过多智能体架构和数据驱动的桥接语言选择，显著提升了多语言代码生成质量，特别是在训练数据少的语言中。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在多语言代码生成中表现不均，尤其是在训练数据较少的语言中。现有方法往往孤立处理每种目标语言，未能充分利用跨语言模式。

Method: XL-CoGen采用协调的多智能体架构，结合中间表示、代码生成、翻译和自动修复。其核心是数据驱动的桥接语言选择机制，通过经验性转移矩阵确定最佳中间语言。

Result: 实验显示XL-CoGen比最强微调基线提升了13个百分点，比现有单语言多智能体方法高出30个百分点。消融研究证实了兼容性引导桥接的重要性。

Conclusion: XL-CoGen通过协调的多智能体架构显著提升了多语言代码生成的质量，特别是在训练数据有限的语言中。其数据驱动的桥接语言选择机制和累积的跨语言知识转移被证明是关键优势。

Abstract: Producing high-quality code across multiple programming languages is
increasingly important as today's software systems are built on heterogeneous
stacks. Large language models (LLMs) have advanced the state of automated
programming, yet their proficiency varies sharply between languages, especially
those with limited training data such as Rust, Perl, OCaml, and Erlang. Many
current solutions including language-specific fine-tuning, multi-agent
orchestration, transfer learning, and intermediate-representation pipelines
still approach each target language in isolation, missing opportunities to
share knowledge or exploit recurring cross-language patterns.
  XL-CoGen tackles this challenge with a coordinated multi-agent architecture
that integrates intermediate representation, code generation, translation, and
automated repair. Its distinguishing feature is a data-driven mechanism for
selecting bridging languages: empirically derived transfer matrices identify
the best intermediate languages based on demonstrated translation success
rather than raw generation accuracy. The system performs early output
validation, iteratively corrects errors, and reuses intermediate artifacts as
contextual scaffolds for subsequent translations.
  Extensive experiments show that XL-CoGen yields notable improvements with 13
percentage-point gains over the strongest fine-tuned baseline and as much as 30
percentage points over existing single-language multi-agent methods. Ablation
studies further demonstrate that compatibility-guided bridging significantly
outperforms LLM-based heuristics, confirming the value of cumulative
cross-language knowledge transfer.

</details>


### [140] [Demystifying the Evolution of Neural Networks with BOM Analysis: Insights from a Large-Scale Study of 55,997 GitHub Repositories](https://arxiv.org/abs/2509.20010)
*Xiaoning Ren,Yuhang Ye,Xiongfei Wu,Yueming Wu,Yinxing Xue*

Main category: cs.SE

TL;DR: NNBOM是一个针对神经网络软件的全面数据集，基于55,997个PyTorch仓库构建，用于分析其演化趋势，并开发了两个实用工具。


<details>
  <summary>Details</summary>
Motivation: 传统软件演化研究方法（如SBOMs）不适用于神经网络软件，而现有的AIBOMs缺乏大规模演化分析的实践。NNBOM旨在填补这一空白。

Method: 创建了一个大规模的NNBOM数据库，包含55,997个PyTorch GitHub仓库，并对其进行了全面的实证研究。

Result: 通过NNBOM数据库，研究人员可以全面分析神经网络软件的演化趋势，包括软件规模、组件重用和跨领域依赖。

Conclusion: NNBOM为神经网络软件提供了一个全面的数据集结构，通过实证研究揭示了其演化趋势，并开发了两个原型应用，展示了其实际价值。

Abstract: Neural networks have become integral to many fields due to their exceptional
performance. The open-source community has witnessed a rapid influx of neural
network (NN) repositories with fast-paced iterations, making it crucial for
practitioners to analyze their evolution to guide development and stay ahead of
trends. While extensive research has explored traditional software evolution
using Software Bill of Materials (SBOMs), these are ill-suited for NN software,
which relies on pre-defined modules and pre-trained models (PTMs) with distinct
component structures and reuse patterns. Conceptual AI Bills of Materials
(AIBOMs) also lack practical implementations for large-scale evolutionary
analysis. To fill this gap, we introduce the Neural Network Bill of Material
(NNBOM), a comprehensive dataset construct tailored for NN software. We create
a large-scale NNBOM database from 55,997 curated PyTorch GitHub repositories,
cataloging their TPLs, PTMs, and modules. Leveraging this database, we conduct
a comprehensive empirical study of neural network software evolution across
software scale, component reuse, and inter-domain dependency, providing
maintainers and developers with a holistic view of its long-term trends.
Building on these findings, we develop two prototype applications,
\textit{Multi repository Evolution Analyzer} and \textit{Single repository
Component Assessor and Recommender}, to demonstrate the practical value of our
analysis.

</details>


### [141] [V-GameGym: Visual Game Generation for Code Large Language Models](https://arxiv.org/abs/2509.20136)
*Wei Zhang,Jack Yang,Renshuai Tao,Lingzheng Chai,Shawn Guo,Jiajun Wu,Xiaoming Chen,Ganqu Cui,Ning Ding,Xander Xu,Hu Wei,Bowen Zhou*

Main category: cs.SE

TL;DR: V-GameGym是一个针对视觉游戏开发的综合基准，填补了现有代码生成评估在游戏特有指标上的空白，通过多模态评估和自动化管道提升实用性。


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型基准主要关注单模态而非视觉游戏开发，缺乏对游戏特有指标（如可玩性、视觉美观性和用户参与度）的评估。

Method: 采用基于聚类的新型整理方法，确保多样性和结构完整性，并引入多模态评估框架和自动化LLM驱动的视觉代码合成管道。

Result: V-GameGym包含2,219个高质量样本，覆盖100个主题集群，有效评估了代码生成在实际游戏开发中的综合需求。

Conclusion: V-GameGym成功弥合了代码生成准确性与实际游戏开发工作流程之间的差距，为视觉编程和交互元素生成提供了可量化的质量指标。

Abstract: Code large language models have demonstrated remarkable capabilities in
programming tasks, yet current benchmarks primarily focus on single modality
rather than visual game development. Most existing code-related benchmarks
evaluate syntax correctness and execution accuracy, overlooking critical
game-specific metrics such as playability, visual aesthetics, and user
engagement that are essential for real-world deployment. To address the gap
between current LLM capabilities in algorithmic problem-solving and competitive
programming versus the comprehensive requirements of practical game
development, we present V-GameGym, a comprehensive benchmark comprising 2,219
high-quality samples across 100 thematic clusters derived from real-world
repositories, adopting a novel clustering-based curation methodology to ensure
both diversity and structural completeness. Further, we introduce a multimodal
evaluation framework with an automated LLM-driven pipeline for visual code
synthesis using complete UI sandbox environments. Our extensive analysis
reveals that V-GameGym effectively bridges the gap between code generation
accuracy and practical game development workflows, providing quantifiable
quality metrics for visual programming and interactive element generation.

</details>


### [142] [Enhancing Requirement Traceability through Data Augmentation Using Large Language Models](https://arxiv.org/abs/2509.20149)
*Jianzhang Zhang,Jialong Zhou,Nan Niu,Chuang Liu*

Main category: cs.SE

TL;DR: 利用LLMs进行数据增强，优化追踪模型，显著提升需求追踪性能。


<details>
  <summary>Details</summary>
Motivation: 解决需求追踪中由于训练数据稀缺和工件间语义差距带来的挑战。

Method: 提出了一种新颖的方法，利用基于提示的技术与LLMs生成增强的需求到代码追踪链接，优化了追踪模型的编码器组件以提高效率和适应性。

Result: 实验结果表明，该方法显著提升了模型性能，F1分数最高提升了28.59%。

Conclusion: 该研究通过利用大型语言模型（LLMs）进行数据增强，显著提升了需求追踪模型的性能，F1分数提高了28.59%，证明了该方法的有效性和实际应用潜力。

Abstract: Requirements traceability is crucial in software engineering to ensure
consistency between requirements and code. However, existing automated
traceability methods are constrained by the scarcity of training data and
challenges in bridging the semantic gap between artifacts. This study aims to
address the data scarcity problem in requirements traceability by employing
large language models (LLMs) for data augmentation. We propose a novel approach
that utilizes prompt-based techniques with LLMs to generate augmented
requirement-to-code trace links, thereby enhancing the training dataset. Four
LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both
zero-shot and few-shot templates. Moreover, we optimized the encoder component
of the tracing model to improve its efficiency and adaptability to augmented
data. The key contributions of this paper are: (1) proposing and evaluating
four prompt templates for data augmentation; (2) providing a comparative
analysis of four LLMs for generating trace links; (3) enhancing the model's
encoder for improved adaptability to augmented datasets. Experimental results
show that our approach significantly enhances model performance, achieving an
F1 score improvement of up to 28.59%, thus demonstrating its effectiveness and
potential for practical application.

</details>


### [143] [Benchmarking Web API Integration Code Generation](https://arxiv.org/abs/2509.20172)
*Daniel Maninger,Leon Chemnitz,Amir Molzam Sharifloo,Jannis Brugger,Mira Mezini*

Main category: cs.SE

TL;DR: 研究评估了开源大型语言模型生成Web API调用代码的能力，发现其表现不佳，最高解决率不足40%。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在软件开发中越来越受欢迎，但其在自动化生成Web API集成代码方面的有效性尚未得到充分研究。

Method: 提出了一个数据集和评估流程，用于测试多个开源大型语言模型生成Web API调用代码的能力。

Result: 实验结果表明，生成API调用存在显著挑战，包括虚构的端点、错误的参数使用等问题。

Conclusion: 评估显示，当前的开源大型语言模型在生成Web API调用代码方面表现不佳，未能解决超过40%的任务。

Abstract: API integration is a cornerstone of our digital infrastructure, enabling
software systems to connect and interact. However, as shown by many studies,
writing or generating correct code to invoke APIs, particularly web APIs, is
challenging. Although large language models~(LLMs) have become popular in
software development, their effectiveness in automating the generation of web
API integration code remains unexplored. In order to address this, we present a
dataset and evaluation pipeline designed to assess the ability of LLMs to
generate web API invocation code. Our experiments with several open-source LLMs
reveal that generating API invocations poses a significant challenge, resulting
in hallucinated endpoints, incorrect argument usage, and other errors. None of
the evaluated open-source models were able to solve more than 40% of the tasks.

</details>


### [144] [The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation](https://arxiv.org/abs/2509.20215)
*Guang Yang,Wei Zheng,Xiang Chen,Yifan Sun,Fengji Zhang,Terry Yue Zhuo*

Main category: cs.SE

TL;DR: VCD-RNK是一个针对Verilog代码重排的判别模型，通过模拟专家知识的三个维度推理过程，解决了语义对齐问题并避免了计算密集的测试执行。


<details>
  <summary>Details</summary>
Motivation: LLMs在Verilog生成中因缺乏领域特定知识而面临挑战，硬件工程师需要一个可信赖的解决方案而非不确定的候选方案。

Method: 提出了VCD-RNK，一个专门用于Verilog代码重排的判别模型，通过代码语义分析、测试用例生成和功能正确性评估三个维度来蒸馏专家知识。

Result: VCD-RNK在推理过程中显式模拟了专家知识的推理过程，避免了现有方法中计算密集的测试执行。

Conclusion: VCD-RNK通过模拟专家知识在三个维度的推理过程，有效解决了Verilog生成中的语义对齐问题，避免了计算密集的测试执行，提供了一个可靠的解决方案。

Abstract: LLMs face significant challenges in Verilog generation due to limited
domain-specific knowledge. While sampling techniques improve pass@k metrics,
hardware engineers need one trustworthy solution rather than uncertain
candidates. To bridge this gap, we formulate it as a semantic alignment problem
between requirements and Verilog implementations, and propose VCD-RNK, a
discriminator model tailored for efficient Verilog code reranking.
Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling
expert knowledge across three dimensions: code semantic analysis, test case
generation, and functional correctness assessment. By explicitly simulating the
above reasoning processes during inference, VCD-RNK effectively avoids
computationally intensive test execution in existing methods.

</details>


### [145] [Confidentiality-Preserving Verifiable Business Processes through Zero-Knowledge Proofs](https://arxiv.org/abs/2509.20300)
*Jannis Kiesel,Jonathan Heiss*

Main category: cs.SE

TL;DR: 论文提出了一种基于ZKP的方法，通过zkVM集成和证明组合，在保护商业机密的同时实现业务流程的自动化验证，并以产品碳足迹为例展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 跨组织流程中，如何在保护商业机密的同时确保业务流程的完整性是一个重大挑战。

Method: 论文通过系统架构和原型实现，将zkVM集成到业务流程管理引擎中，支持通过证明组合实现链式可验证计算。以产品碳足迹为例，建模了连续的足迹活动，并展示了组织如何在不暴露敏感信息的情况下证明和验证流程的完整性。

Result: 实验评估表明，该方法可以在给定的保密约束下自动化流程验证，并评估了不同ZKP证明变体在流程模型中的效率。

Conclusion: 该论文提出了一种基于零知识证明（ZKP）的方法，用于在保护商业机密的同时验证业务流程的执行。通过集成zkVM到业务流程管理引擎，并支持链式可验证计算，该方法在保证信息不泄露的前提下实现了流程验证的自动化。

Abstract: Ensuring the integrity of business processes without disclosing confidential
business information is a major challenge in inter-organizational processes.
This paper introduces a zero-knowledge proof (ZKP)-based approach for the
verifiable execution of business processes while preserving confidentiality. We
integrate ZK virtual machines (zkVMs) into business process management engines
through a comprehensive system architecture and a prototypical implementation.
Our approach supports chained verifiable computations through proof
compositions. On the example of product carbon footprinting, we model
sequential footprinting activities and demonstrate how organizations can prove
and verify the integrity of verifiable processes without exposing sensitive
information. We assess different ZKP proving variants within process models for
their efficiency in proving and verifying, and discuss the practical
integration of ZKPs throughout the Business Process Management (BPM) lifecycle.
Our experiment-driven evaluation demonstrates the automation of process
verification under given confidentiality constraints.

</details>


### [146] [Protocol Testing with I/O Grammars](https://arxiv.org/abs/2509.20308)
*Alexander Liggesmeyer,José Antonio Zamudio Amaya,Andreas Zeller*

Main category: cs.SE

TL;DR: 提出结合输入生成和输出检查的协议测试新方法，基于I/O语法和FANDANGO框架，验证其在多种协议中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决协议测试中生成语法和语义正确且多样化的输入，以及验证输出正确性的两个基本问题。

Method: 引入I/O语法作为指定协议语法和语义的首个手段，包括消息、状态和交互。基于FANDANGO框架实现，可同时作为测试生成器、模拟对象和预言机。

Result: 在DNS、FTP和SMTP等协议上验证了I/O语法的有效性，系统性覆盖比随机方法更快覆盖功能和响应空间。

Conclusion: I/O语法能够正确且完整地指定高级协议特性，同时实现对被测程序的输出验证。系统性覆盖I/O语法比基于随机的最先进方法能更快覆盖输入和响应空间。

Abstract: Generating software tests faces two fundamental problems. First, one needs to
_generate inputs_ that are syntactically and semantically correct, yet
sufficiently diverse to cover behavior. Second, one needs an _oracle_ to _check
outputs_ whether a test case is correct or not. Both problems become apparent
in _protocol testing_, where inputs are messages exchanged between parties, and
outputs are the responses of these parties.
  In this paper, we propose a novel approach to protocol testing that combines
input generation and output checking in a single framework. We introduce _I/O
grammars_ as the first means to _completely_ specify the syntax and semantics
of protocols, including messages, states, and interactions. Our implementation,
based on the FANDANGO framework, takes a single I/O grammar, and can act as a
_test generator_, as a _mock object_, and as an _oracle_ for a _client_, a
_server_, or both (or actually any number of parties), a versatility not found
in any existing tool or formalism. User-defined _constraints}_can have the
generator focus on arbitrary protocol features; $k$-path guidance
systematically covers states, messages, responses, and value alternatives in a
unified fashion.
  We evaluate the effectiveness of our approach by applying it to several
protocols, including DNS, FTP, and SMTP. We demonstrate that I/O grammars can
specify advanced protocol features correctly and completely, while also
enabling output validation of the programs under test. In its evaluation, we
find that systematic coverage of the I/O grammar results in much quicker
coverage of the input and response spaces (and thus functionality) compared to
the random-based state-of-the-art approaches.

</details>


### [147] [Developer Productivity With and Without GitHub Copilot: A Longitudinal Mixed-Methods Case Study](https://arxiv.org/abs/2509.20353)
*Viktoria Stray,Elias Goldmann Brandtzæg,Viggo Tellefsen Wivestad,Astri Barbala,Nils Brede Moe*

Main category: cs.SE

TL;DR: GitHub Copilot未显著改变开发者提交活动，但用户主观生产力感知更高。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI工具GitHub Copilot对开发者实际活动和感知生产力的真实影响。

Method: 采用混合方法案例研究，分析26,317次非合并提交数据，结合调查和13次访谈。

Result: Copilot用户在工具引入前后提交活动无显著变化，但主观生产力感知提升。

Conclusion: 研究未发现GitHub Copilot对开发者提交活动有显著统计影响，但主观生产力感知存在差异。

Abstract: This study investigates the real-world impact of the generative AI (GenAI)
tool GitHub Copilot on developer activity and perceived productivity. We
conducted a mixed-methods case study in NAV IT, a large public sector agile
organization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's
GitHub repositories over a two-year period, focusing on commit-based activity
metrics from 25 Copilot users and 14 non-users. The analysis was complemented
by survey responses on their roles and perceived productivity, as well as 13
interviews. Our analysis of activity metrics revealed that individuals who used
Copilot were consistently more active than non-users, even prior to Copilot's
introduction. We did not find any statistically significant changes in
commit-based activity for Copilot users after they adopted the tool, although
minor increases were observed. This suggests a discrepancy between changes in
commit-based metrics and the subjective experience of productivity.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [148] [HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames](https://arxiv.org/abs/2509.19452)
*Alessandro Saviolo,Jeffrey Mao,Giuseppe Loianno*

Main category: cs.RO

TL;DR: HUNT框架通过统一导航和跟踪，实现了无人机在感知受限和无全球定位环境下的高速自主飞行。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在未知非结构化环境中高速导航和目标跟踪的挑战，尤其是在感知受限和无全球定位的情况下。

Method: HUNT框架通过将导航目标直接与机载瞬时观测数据（如姿态、高度和速度）关联，实现了搜索和跟踪的统一。

Result: 在密集森林、集装箱堆场和搜救任务中的户外实验表明，HUNT框架在全局方法失效的场景下仍能保持鲁棒性。

Conclusion: HUNT框架在无人机高速导航和目标跟踪方面表现出色，尤其在无全球定位和感知受限的环境下，展现了强大的自主能力。

Abstract: Search and rescue operations require unmanned aerial vehicles to both
traverse unknown unstructured environments at high speed and track targets once
detected. Achieving both capabilities under degraded sensing and without global
localization remains an open challenge. Recent works on relative navigation
have shown robust tracking by anchoring planning and control to a visible
detected object, but cannot address navigation when no target is in the field
of view. We present HUNT (High-speed UAV Navigation and Tracking), a real-time
framework that unifies traversal, acquisition, and tracking within a single
relative formulation. HUNT defines navigation objectives directly from onboard
instantaneous observables such as attitude, altitude, and velocity, enabling
reactive high-speed flight during search. Once a target is detected, the same
perception-control pipeline transitions seamlessly to tracking. Outdoor
experiments in dense forests, container compounds, and search-and-rescue
operations with vehicles and mannequins demonstrate robust autonomy where
global methods fail.

</details>


### [149] [ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation](https://arxiv.org/abs/2509.19454)
*Jason Chen,I-Chun Arthur Liu,Gaurav Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: ROPA是一种离线模仿学习数据增强方法，通过合成第三人称RGB和RGB-D观察及对应动作标签，提升双手机器人操作的多样性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 收集多样且精确的真实世界演示数据成本高且耗时，限制了模仿学习的可扩展性。

Method: ROPA通过微调Stable Diffusion来合成第三人称RGB和RGB-D观察的新机器人姿态，同时生成对应的关节空间动作标签，并通过约束优化确保物理一致性。

Result: 在5个模拟和3个真实世界任务中的2625次模拟试验和300次真实世界试验中，ROPA表现优于基线和消融实验。

Conclusion: ROPA方法在模拟和真实世界任务中表现优于基线，展示了其在眼到手双手机器人操作中RGB和RGB-D数据增强的可扩展潜力。

Abstract: Training robust bimanual manipulation policies via imitation learning
requires demonstration data with broad coverage over robot poses, contacts, and
scene contexts. However, collecting diverse and precise real-world
demonstrations is costly and time-consuming, which hinders scalability. Prior
works have addressed this with data augmentation, typically for either
eye-in-hand (wrist camera) setups with RGB inputs or for generating novel
images without paired actions, leaving augmentation for eye-to-hand
(third-person) RGB-D training with new action labels less explored. In this
paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data
Augmentation (ROPA), an offline imitation learning data augmentation method
that fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D
observations of novel robot poses. Our approach simultaneously generates
corresponding joint-space action labels while employing constrained
optimization to enforce physical consistency through appropriate
gripper-to-object contact constraints in bimanual scenarios. We evaluate our
method on 5 simulated and 3 real-world tasks. Our results across 2625
simulation trials and 300 real-world trials demonstrate that ROPA outperforms
baselines and ablations, showing its potential for scalable RGB and RGB-D data
augmentation in eye-to-hand bimanual manipulation. Our project website is
available at: https://ropaaug.github.io/.

</details>


### [150] [Self-evolved Imitation Learning in Simulated World](https://arxiv.org/abs/2509.19460)
*Yifan Ye,Jun Cen,Jing Chen,Zhihe Lu*

Main category: cs.RO

TL;DR: SEIL通过模拟器互动和双级增强，用少量示范实现了高效的模仿学习，性能领先。


<details>
  <summary>Details</summary>
Motivation: 解决模仿学习中大规模专家示范收集成本高和劳动密集的问题。

Method: SEIL采用双级增强（模型级和环境级）和轻量级选择器来提升示范的多样性和质量。

Result: 在LIBERO基准测试中，SEIL实现了少样本模仿学习的最先进性能。

Conclusion: SEIL框架通过自我演进的模仿学习，在有限的专家示范下实现了竞争性的性能，成为少样本模仿学习的新标杆。

Abstract: Imitation learning has been a trend recently, yet training a generalist agent
across multiple tasks still requires large-scale expert demonstrations, which
are costly and labor-intensive to collect. To address the challenge of limited
supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework
that progressively improves a few-shot model through simulator interactions.
The model first attempts tasksin the simulator, from which successful
trajectories are collected as new demonstrations for iterative refinement. To
enhance the diversity of these demonstrations, SEIL employs dual-level
augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model
to collaborate with the primary model, and (ii) Environment-level, introducing
slight variations in initial object positions. We further introduce a
lightweight selector that filters complementary and informative trajectories
from the generated pool to ensure demonstration quality. These curated samples
enable the model to achieve competitive performance with far fewer training
examples. Extensive experiments on the LIBERO benchmark show that SEIL achieves
a new state-of-the-art performance in few-shot imitation learning scenarios.
Code is available at https://github.com/Jasper-aaa/SEIL.git.

</details>


### [151] [CU-Multi: A Dataset for Multi-Robot Collaborative Perception](https://arxiv.org/abs/2509.19463)
*Doncey Albin,Daniel McGann,Miles Mena,Annika Thomas,Harel Biggie,Xuefei Sun,Steve McGuire,Jonathan P. How,Christoffer Heckman*

Main category: cs.RO

TL;DR: CU-Multi是一个多机器人数据集，旨在解决现有数据集在轨迹长度和重叠方面的不足，为协作感知任务提供标准化评估基础。


<details>
  <summary>Details</summary>
Motivation: 当前多机器人数据集大多包含短轨迹，机器人间重叠有限且机器人内环闭合稀疏，限制了多机器人系统的评估标准化。

Method: 在科罗拉多大学博尔德分校的两个大型室外场地进行了多天数据收集，包括RGB-D传感、RTK GPS、语义LiDAR和精炼的地面真实里程计。

Result: CU-Multi数据集包含四个同步运行，具有对齐的起始时间和控制的轨迹重叠，支持多机器人协作感知任务的可重复评估。

Conclusion: CU-Multi数据集通过提供同步运行、对齐的起始时间和控制的轨迹重叠，为多机器人协作感知任务的可重复评估提供了坚实基础。

Abstract: A central challenge for multi-robot systems is fusing independently gathered
perception data into a unified representation. Despite progress in
Collaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of
dedicated multi-robot datasets. Many evaluations instead partition single-robot
trajectories, a practice that may only partially reflect true multi-robot
operations and, more critically, lacks standardization, leading to results that
are difficult to interpret or compare across studies. While several multi-robot
datasets have recently been introduced, they mostly contain short trajectories
with limited inter-robot overlap and sparse intra-robot loop closures. To
overcome these limitations, we introduce CU-Multi, a dataset collected over
multiple days at two large outdoor sites on the University of Colorado Boulder
campus. CU-Multi comprises four synchronized runs with aligned start times and
controlled trajectory overlap, replicating the distinct perspectives of a robot
team. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined
ground-truth odometry. By combining overlap variation with dense semantic
annotations, CU-Multi provides a strong foundation for reproducible evaluation
in multi-robot collaborative perception tasks.

</details>


### [152] [Crater Observing Bio-inspired Rolling Articulator (COBRA)](https://arxiv.org/abs/2509.19473)
*Adarsh Salagame,Henry Noyes,Alireza Ramezani,Eric Sihite,Arash Kalantari*

Main category: cs.RO

TL;DR: COBRA是一种多模态蛇形机器人，结合滑动和翻滚运动，专为月球极端地形设计，成功解决了现有探测器的移动性问题。


<details>
  <summary>Details</summary>
Motivation: NASA计划在月球建立可持续的人类基地，作为未来火星任务的中转站。月球陨石坑中的水冰资源至关重要，但现有探测方法在极端地形中面临移动性挑战。

Method: COBRA结合了蛇形滑动和翻滚运动模式，适应不同的陨石坑地形。配备机载计算机、立体相机、惯性测量单元和关节编码器，支持实时数据收集和自主操作。

Result: 通过模拟和实验验证，COBRA展示了在极端地形中导航的鲁棒性和高效性。

Conclusion: COBRA机器人通过其多模态设计和生物启发式运动方式，成功解决了月球极端地形中的移动挑战，为未来月球基地建设和资源开发提供了可行的技术解决方案。

Abstract: NASA aims to establish a sustainable human basecamp on the Moon as a stepping
stone for future missions to Mars and beyond. The discovery of water ice on the
Moon's craters located in permanently shadowed regions, which can provide
drinking water, oxygen, and rocket fuel, is therefore of critical importance.
However, current methods to access lunar ice deposits are limited. While rovers
have been used to explore the lunar surface for decades, they face significant
challenges in navigating harsh terrains, such as permanently shadowed craters,
due to the high risk of immobilization. This report introduces COBRA (Crater
Observing Bio-inspired Rolling Articulator), a multi-modal snake-style robot
designed to overcome mobility challenges in Shackleton Crater's rugged
environment. COBRA combines slithering and tumbling locomotion to adapt to
various crater terrains. In snake mode, it uses sidewinding to traverse flat or
low inclined surfaces, while in tumbling mode, it forms a circular barrel by
linking its head and tail, enabling rapid movement with minimal energy on steep
slopes. Equipped with an onboard computer, stereo camera, inertial measurement
unit, and joint encoders, COBRA facilitates real-time data collection and
autonomous operation. This paper highlights COBRAs robustness and efficiency in
navigating extreme terrains through both simulations and experimental
validation.

</details>


### [153] [OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation](https://arxiv.org/abs/2509.19480)
*Noriaki Hirose,Catherine Glossop,Dhruv Shah,Sergey Levine*

Main category: cs.RO

TL;DR: OmniVLA 是一种全模态目标调节的机器人基础模型训练框架，通过融合多种模态（如语言、图像、姿态）提升导航策略的灵活性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人导航策略通常仅针对单一模态训练，限制了其在现实场景中的适应性，而人类能够灵活解释和组合多种目标规范。

Method: 采用高容量的视觉-语言-动作（VLA）主干网络，通过随机模态融合策略训练三种主要目标模态（2D 姿态、自我中心图像和自然语言）及其组合。

Result: OmniVLA 在未见环境中表现出强泛化能力，对稀缺模态具有鲁棒性，并能遵循新自然语言指令，优于专业基线模型。

Conclusion: OmniVLA 提供了一个可扩展的路径，用于构建全模态机器人基础模型，并在未见环境中展现出强大的泛化能力、对稀缺模态的鲁棒性以及遵循新自然语言指令的能力。

Abstract: Humans can flexibly interpret and compose different goal specifications, such
as language instructions, spatial coordinates, or visual references, when
navigating to a destination. In contrast, most existing robotic navigation
policies are trained on a single modality, limiting their adaptability to
real-world scenarios where different forms of goal specification are natural
and complementary. In this work, we present a training framework for robotic
foundation models that enables omni-modal goal conditioning for vision-based
navigation. Our approach leverages a high-capacity vision-language-action (VLA)
backbone and trains with three primary goal modalities: 2D poses, egocentric
images, and natural language, as well as their combinations, through a
randomized modality fusion strategy. This design not only expands the pool of
usable datasets but also encourages the policy to develop richer geometric,
semantic, and visual representations. The resulting model, OmniVLA, achieves
strong generalization to unseen environments, robustness to scarce modalities,
and the ability to follow novel natural language instructions. We demonstrate
that OmniVLA outperforms specialist baselines across modalities and offers a
flexible foundation for fine-tuning to new modalities and tasks. We believe
OmniVLA provides a step toward broadly generalizable and flexible navigation
policies, and a scalable path for building omni-modal robotic foundation
models. We present videos showcasing OmniVLA performance and will release its
checkpoints and training code on our project page.

</details>


### [154] [Supercomputing for High-speed Avoidance and Reactive Planning in Robots](https://arxiv.org/abs/2509.19486)
*Kieran S. Lachmansingh,José R. González-Estrada,Ryan E. Grant,Matthew K. X. J. Pan*

Main category: cs.RO

TL;DR: SHARP通过HPC实现毫秒级机器人控制，展示了在动态环境中高效避障的可行性。


<details>
  <summary>Details</summary>
Motivation: 现代机器人在人机共享工作空间中对反应性需求增加，但机载处理器受限于尺寸、功率和成本。HPC卸载提供了大规模并行化的轨迹规划能力，但其在实时机器人中的可行性仍不确定。

Method: 使用并行化多目标A*搜索算法，通过MPI在本地和远程HPC集群上实现轨迹规划。

Result: 系统在本地和远程HPC集群上分别实现了22.9毫秒和30.0毫秒的平均规划延迟，避障成功率分别为84%和88%。

Conclusion: SHARP将高性能计算（HPC）卸载重新定义为在动态环境中实现可靠、反应迅速的机器人的可行路径。

Abstract: This paper presents SHARP (Supercomputing for High-speed Avoidance and
Reactive Planning), a proof-of-concept study demonstrating how high-performance
computing (HPC) can enable millisecond-scale responsiveness in robotic control.
While modern robots face increasing demands for reactivity in human--robot
shared workspaces, onboard processors are constrained by size, power, and cost.
Offloading to HPC offers massive parallelism for trajectory planning, but its
feasibility for real-time robotics remains uncertain due to network latency and
jitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator
must dodge high-speed foam projectiles. Using a parallelized multi-goal A*
search implemented with MPI on both local and remote HPC clusters, the system
achieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote, ~300
km away), with avoidance success rates of 84% and 88%, respectively. These
results show that when round-trip latency remains within the
tens-of-milliseconds regime, HPC-side computation is no longer the bottleneck,
enabling avoidance well below human reaction times. The SHARP results motivate
hybrid control architectures: low-level reflexes remain onboard for safety,
while bursty, high-throughput planning tasks are offloaded to HPC for
scalability. By reporting per-stage timing and success rates, this study
provides a reproducible template for assessing real-time feasibility of
HPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable
pathway toward dependable, reactive robots in dynamic environments.

</details>


### [155] [A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion](https://arxiv.org/abs/2509.19521)
*Najeeb Ahmed Bhuiyan,M. Nasimul Huq,Sakib H. Chowdhury,Rahul Mangharam*

Main category: cs.RO

TL;DR: 该论文提出了一种基于TinyML和传感器融合的双手机势控制框架，显著提升了移动机械臂手势控制的效率和直观性，适用于多种HRI场景。


<details>
  <summary>Details</summary>
Motivation: 解决手势控制移动机械臂在可靠性、效率和直观性方面的持续挑战。

Method: 研究采用双手机势界面，结合TinyML、光谱分析和传感器融合技术，通过ROS框架实现。左手倾斜和手指弯曲通过加速度计和弯曲传感器捕获，用于移动基座导航；右手IMU信号通过光谱分析和轻量级神经网络分类，控制7自由度Kinova Gen3机械臂。

Result: 该系统实现了同时导航和操作，提高了效率和协调性，相比顺序方法有显著改进。关键贡献包括双手机控制架构、实时低功耗手势识别、稳健的多模态传感器融合和可扩展的ROS实现。

Conclusion: 该论文提出的双手机器人控制架构通过集成TinyML、光谱分析和传感器融合技术，显著提升了手势控制的可靠性、效率和直观性，为工业自动化、辅助机器人和危险环境中的HRI提供了经济高效的开源解决方案。

Abstract: Gesture-based control for mobile manipulators faces persistent challenges in
reliability, efficiency, and intuitiveness. This paper presents a dual-hand
gesture interface that integrates TinyML, spectral analysis, and sensor fusion
within a ROS framework to address these limitations. The system uses left-hand
tilt and finger flexion, captured using accelerometer and flex sensors, for
mobile base navigation, while right-hand IMU signals are processed through
spectral analysis and classified by a lightweight neural network. This pipeline
enables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3
manipulator. By supporting simultaneous navigation and manipulation, the
framework improves efficiency and coordination compared to sequential methods.
Key contributions include a bimanual control architecture, real-time low-power
gesture recognition, robust multimodal sensor fusion, and a scalable ROS-based
implementation. The proposed approach advances Human-Robot Interaction (HRI)
for industrial automation, assistive robotics, and hazardous environments,
offering a cost-effective, open-source solution with strong potential for
real-world deployment and further optimization.

</details>


### [156] [Bioinspired SLAM Approach for Unmanned Surface Vehicle](https://arxiv.org/abs/2509.19522)
*Fabio Coelho,Joao Victor T. Borges,Paulo Padrao,Jose Fuentes,Ramon R. Costa,Liu Hsu,Leonardo Bobadilla*

Main category: cs.RO

TL;DR: OpenRatSLAM2是一个基于生物启发的低计算成本SLAM框架，适用于GPS缺失环境，首次在USV上应用并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 开发一个适用于无人水面艇（USV）的低计算成本SLAM框架，以解决GPS缺失环境下的导航问题。

Method: 基于啮齿动物海马体的计算模型，采用ROS2架构，并通过Hausdorff距离与地面真实数据比较估计轨迹。

Result: 实验结果表明，OpenRatSLAM2能够生成误差可接受的半度量地图，并首次在USV上成功应用。

Conclusion: OpenRatSLAM2是一个适用于GPS缺失环境的低计算成本视觉惯性SLAM框架，其生成的半度量地图误差在大多数机器人应用中是可接受的。

Abstract: This paper presents OpenRatSLAM2, a new version of OpenRatSLAM - a
bioinspired SLAM framework based on computational models of the rodent
hippocampus. OpenRatSLAM2 delivers low-computation-cost visual-inertial based
SLAM, suitable for GPS-denied environments. Our contributions include a
ROS2-based architecture, experimental results on new waterway datasets, and
insights into system parameter tuning. This work represents the first known
application of RatSLAM on USVs. The estimated trajectory was compared with
ground truth data using the Hausdorff distance. The results show that the
algorithm can generate a semimetric map with an error margin acceptable for
most robotic applications.

</details>


### [157] [Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot](https://arxiv.org/abs/2509.19525)
*James Avtges,Jake Ketchum,Millicent Schlafly,Helena Young,Taekyoung Kim,Allison Pinosky,Ryan L. Truby,Todd D. Murphey*

Main category: cs.RO

TL;DR: 通过课程学习和最大扩散RL，在单次硬件部署中实现软机器人动态平衡，即使部分执行器失效仍保持高性能。


<details>
  <summary>Details</summary>
Motivation: 软机器人在动态负载下的非线性响应限制了其控制方法的发展，传统方法未能充分利用其配置空间，且数据驱动方法如RL受限于样本效率和初始化不一致。

Method: 采用基于已知平衡点扩展邻域的课程学习方法和最大扩散RL（Maximum Diffusion RL），在单次部署中学习动态平衡策略。

Result: 在单次部署中，最大扩散RL能够在15分钟内学习动态平衡，即使半数执行器失效，性能仍接近完整平台。

Conclusion: 单次硬件部署中的RL学习能够实现软机器人系统的实时动态平衡，提升了软机器人的多样性和能力。

Abstract: Closed-loop control remains an open challenge in soft robotics. The nonlinear
responses of soft actuators under dynamic loading conditions limit the use of
analytic models for soft robot control. Traditional methods of controlling soft
robots underutilize their configuration spaces to avoid nonlinearity,
hysteresis, large deformations, and the risk of actuator damage. Furthermore,
episodic data-driven control approaches such as reinforcement learning (RL) are
traditionally limited by sample efficiency and inconsistency across
initializations. In this work, we demonstrate RL for reliably learning control
policies for dynamic balancing tasks in real-time single-shot hardware
deployments. We use a deformable Stewart platform constructed using parallel,
3D-printed soft actuators based on motorized handed shearing auxetic (HSA)
structures. By introducing a curriculum learning approach based on expanding
neighborhoods of a known equilibrium, we achieve reliable single-deployment
balancing at arbitrary coordinates. In addition to benchmarking the performance
of model-based and model-free methods, we demonstrate that in a single
deployment, Maximum Diffusion RL is capable of learning dynamic balancing after
half of the actuators are effectively disabled, by inducing buckling and by
breaking actuators with bolt cutters. Training occurs with no prior data, in as
fast as 15 minutes, with performance nearly identical to the fully-intact
platform. Single-shot learning on hardware facilitates soft robotic systems
reliably learning in the real world and will enable more diverse and capable
soft robots.

</details>


### [158] [Autonomous Elemental Characterization Enabled by a Low Cost Robotic Platform Built Upon a Generalized Software Architecture](https://arxiv.org/abs/2509.19541)
*Xuan Cao,Yuxin Wu,Michael L. Whittaker*

Main category: cs.RO

TL;DR: 本文提出了一种用于科学实验室的机器人系统软件架构，通过双层面设计和低成本硬件集成，实现了自动化表征任务，展示了其在化学映射中的实用性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏通用方法论和高硬件成本，机器人在科学实验室中的应用较少。本文旨在通过自动化表征任务降低成本并保持通用性。

Method: 采用双层面（Socket.IO和ROS）动作服务器设计作为基本构建块，结合Web前端和ROS行为树，构建了一个低成本的三轴计算机数控龙门系统作为主要机器人，并集成了手持激光诱导击穿光谱（LIBS）分析仪。

Result: 成功构建了一个机器人平台，实现了矿物和材料样本的自动化表征，并通过密集高光谱映射展示了自动化化学映射的实用性。

Conclusion: 本文提出了一种用于科学实验室环境的机器人系统软件架构，通过双层面（Socket.IO和ROS）动作服务器设计，实现了用户友好的基于Web的前端操作和便捷的任务规划与执行。构建的机器人平台成功应用于矿物和材料样本的表征自动化，展示了自动化化学映射的实用性。

Abstract: Despite the rapidly growing applications of robots in industry, the use of
robots to automate tasks in scientific laboratories is less prolific due to
lack of generalized methodologies and high cost of hardware. This paper focuses
on the automation of characterization tasks necessary for reducing cost while
maintaining generalization, and proposes a software architecture for building
robotic systems in scientific laboratory environment. A dual-layer (Socket.IO
and ROS) action server design is the basic building block, which facilitates
the implementation of a web-based front end for user-friendly operations and
the use of ROS Behavior Tree for convenient task planning and execution. A
robotic platform for automating mineral and material sample characterization is
built upon the architecture, with an open source, low-cost three-axis computer
numerical control gantry system serving as the main robot. A handheld laser
induced breakdown spectroscopy (LIBS) analyzer is integrated with a 3D printed
adapter, enabling automated 2D chemical mapping. We demonstrate the utility of
automated chemical mapping by scanning of the surface of a spodumene-bearing
pegmatite core sample with a 1071-point dense hyperspectral map acquired at a
rate of 1520 bits per second. Automated LIBS scanning enables controlled
chemical quantification in the laboratory that complements field-based
measurements acquired with the same handheld device, linking resource
exploration and processing steps in the supply chain for lithium-based battery
materials.

</details>


### [159] [RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots](https://arxiv.org/abs/2509.19545)
*Min Dai,Aaron D. Ames*

Main category: cs.RO

TL;DR: RoMoCo 是一个开源 C++ 工具箱，用于双足和人形机器人的规划器和控制器开发，支持快速原型化和跨平台验证。


<details>
  <summary>Details</summary>
Motivation: 为双足和人形机器人提供一个统一的、可快速原型开发的工具，支持灵活的控制器设计和跨平台应用。

Method: RoMoCo 采用模块化架构，统一了先进的规划器和全身运动控制器，并通过降阶模型实现平台无关的步态生成。

Result: 通过在 Cassie、Unitree H1 和 G1 机器人上的广泛仿真及硬件实验验证了 RoMoCo 的实用性和性能。

Conclusion: RoMoCo 是一个开源 C++ 工具箱，用于合成和评估基于降阶模型的规划器和全身控制器，适用于双足和人形机器人。其模块化架构和一致的 API 设计支持快速原型开发和可重复基准测试，展示了在多机器人平台上的灵活性和性能。

Abstract: We present RoMoCo, an open-source C++ toolbox for the synthesis and
evaluation of reduced-order model-based planners and whole-body controllers for
bipedal and humanoid robots. RoMoCo's modular architecture unifies
state-of-the-art planners and whole-body locomotion controllers under a
consistent API, enabling rapid prototyping and reproducible benchmarking. By
leveraging reduced-order models for platform-agnostic gait generation, RoMoCo
enables flexible controller design across diverse robots. We demonstrate its
versatility and performance through extensive simulations on the Cassie,
Unitree H1, and G1 robots, and validate its real-world efficacy with hardware
experiments on the Cassie and G1 humanoids.

</details>


### [160] [AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space](https://arxiv.org/abs/2509.19555)
*Sankalp Agrawal,Junwon Seo,Kensuke Nakamura,Ran Tian,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 论文提出了一种潜在安全过滤器，能够适应运行时用户指定的安全约束，通过潜在空间相似性度量和共形校准实现，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设安全约束已知且固定，限制了安全过滤器在不同场景中的适应性，因此需要一种能够适应运行时用户指定安全约束的方法。

Method: 提出了一种约束参数化的潜在安全过滤器，通过潜在空间相似性度量定义安全约束，并利用共形校准对齐失败相似性概念。

Result: 在仿真和硬件实验中，该方法通过条件化约束图像编码实现了运行时适应，验证了其有效性。

Conclusion: 该论文提出了一种运行时可适应的安全过滤器，通过条件化用户指定的约束图像编码，实现了在视觉控制任务中的安全性能提升，且不牺牲性能。

Abstract: Recent works have shown that foundational safe control methods, such as
Hamilton-Jacobi (HJ) reachability analysis, can be applied in the latent space
of world models. While this enables the synthesis of latent safety filters for
hard-to-model vision-based tasks, they assume that the safety constraint is
known a priori and remains fixed during deployment, limiting the safety
filter's adaptability across scenarios. To address this, we propose
constraint-parameterized latent safety filters that can adapt to user-specified
safety constraints at runtime. Our key idea is to define safety constraints by
conditioning on an encoding of an image that represents a constraint, using a
latent-space similarity measure. The notion of similarity to failure is aligned
in a principled way through conformal calibration, which controls how closely
the system may approach the constraint representation. The parameterized safety
filter is trained entirely within the world model's imagination, treating any
image seen by the model as a potential test-time constraint, thereby enabling
runtime adaptation to arbitrary safety constraints. In simulation and hardware
experiments on vision-based control tasks with a Franka manipulator, we show
that our method adapts at runtime by conditioning on the encoding of
user-specified constraint images, without sacrificing performance. Video
results can be found on https://any-safe.github.io

</details>


### [161] [Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action](https://arxiv.org/abs/2509.19571)
*Sacha Morin,Kumaraditya Gupta,Mahtab Sandhu,Charlie Gauthier,Francesco Argenziano,Kirsty Ellis,Liam Paull*

Main category: cs.RO

TL;DR: ASP框架利用高级场景表示处理复杂语言指令，优于现有VLA模型，适用于复杂任务和新场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有模仿学习和视觉-语言-动作模型（VLA）在处理复杂指令和新场景时的局限性。

Method: 利用现代场景表示的语义、空间和功能查询能力，构建Agentic Scene Policies（ASP）框架，通过显式推理对象功能来处理复杂任务。

Result: ASP在桌面操作任务中表现优于VLA，并能通过功能引导导航和扩展场景表示处理房间级查询。

Conclusion: ASP框架通过现代场景表示的高级语义、空间和功能查询能力，成功实现了语言条件下的机器人策略，能够处理复杂指令和新场景。

Abstract: Executing open-ended natural language queries is a core problem in robotics.
While recent advances in imitation learning and vision-language-actions models
(VLAs) have enabled promising end-to-end policies, these models struggle when
faced with complex instructions and new scenes. An alternative is to design an
explicit scene representation as a queryable interface between the robot and
the world, using query results to guide downstream motion planning. In this
work, we present Agentic Scene Policies (ASP), an agentic framework that
leverages the advanced semantic, spatial, and affordance-based querying
capabilities of modern scene representations to implement a capable
language-conditioned robot policy. ASP can execute open-vocabulary queries in a
zero-shot manner by explicitly reasoning about object affordances in the case
of more complex skills. Through extensive experiments, we compare ASP with VLAs
on tabletop manipulation problems and showcase how ASP can tackle room-level
queries through affordance-guided navigation, and a scaled-up scene
representation. (Project page:
https://montrealrobotics.ca/agentic-scene-policies.github.io/)

</details>


### [162] [Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning](https://arxiv.org/abs/2509.19573)
*Zachary Olkin,Kejun Li,William D. Compton,Aaron D. Ames*

Main category: cs.RO

TL;DR: CLF-RL结合控制理论和强化学习，实现了人形机器人跑步等高度动态行为的稳定控制，展示了鲁棒性和精确性。


<details>
  <summary>Details</summary>
Motivation: 设计既鲁棒又精确的控制器以实现人形机器人的高度动态行为（如跑步），同时避免手动调整启发式奖励项的复杂性。

Method: 将非线性控制理论中的控制Lyapunov函数（CLFs）和优化的动态参考轨迹嵌入强化学习训练过程，以塑造奖励函数。

Result: CLF-RL方法成功实现了包括飞行和单支撑阶段的跑步行为，并在跑步机和户外环境中表现出对干扰的鲁棒性，且仅依赖机载传感器即可实现精确的全局参考跟踪。

Conclusion: CLF-RL方法通过将控制理论中的Lyapunov函数与强化学习结合，成功实现了人形机器人高度动态行为的稳定控制，并在实际环境中展示了鲁棒性和精确性。

Abstract: Achieving highly dynamic behaviors on humanoid robots, such as running,
requires controllers that are both robust and precise, and hence difficult to
design. Classical control methods offer valuable insight into how such systems
can stabilize themselves, but synthesizing real-time controllers for nonlinear
and hybrid dynamics remains challenging. Recently, reinforcement learning (RL)
has gained popularity for locomotion control due to its ability to handle these
complex dynamics. In this work, we embed ideas from nonlinear control theory,
specifically control Lyapunov functions (CLFs), along with optimized dynamic
reference trajectories into the reinforcement learning training process to
shape the reward. This approach, CLF-RL, eliminates the need to handcraft and
tune heuristic reward terms, while simultaneously encouraging certifiable
stability and providing meaningful intermediate rewards to guide learning. By
grounding policy learning in dynamically feasible trajectories, we expand the
robot's dynamic capabilities and enable running that includes both flight and
single support phases. The resulting policy operates reliably on a treadmill
and in outdoor environments, demonstrating robustness to disturbances applied
to the torso and feet. Moreover, it achieves accurate global reference tracking
utilizing only on-board sensors, making a critical step toward integrating
these dynamic motions into a full autonomy stack.

</details>


### [163] [Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping](https://arxiv.org/abs/2509.19579)
*Chad R. Samuelson,Abigail Austin,Seth Knoop,Blake Romrell,Gabriel R. Slade,Timothy W. McLain,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 本文提出了一种结合3D场景图和地形感知推理的轻量级方法，用于户外机器人操作，在物体检索和区域分类中表现优异且内存高效。


<details>
  <summary>Details</summary>
Motivation: 户外智能自主机器人操作需要具有足够表达力的环境地图，传统几何地图缺乏语义理解和高层次推理能力。3D场景图通过整合几何、拓扑和语义关系解决了这一限制，但户外操作通常依赖地形信息。

Method: 结合室内3D场景图技术与标准户外几何地图和地形感知推理，生成地形感知的位置节点和层次化组织的区域，构建任务无关的度量-语义稀疏地图，并从中构建3D场景图用于下游规划任务。

Result: 该方法在物体检索方面与最先进的基于相机的3D场景图方法相当，在区域分类方面超越它们，同时保持内存高效。在模拟和真实环境中的物体检索和区域监测任务中展示了其有效性。

Conclusion: 本文提出的方法在户外环境中结合了3D场景图技术和地形感知推理，生成了一个轻量级且高效的任务无关度量-语义稀疏地图，并通过3D场景图支持下游规划任务。该方法在物体检索和区域分类方面表现优异，且内存效率高。

Abstract: Outdoor intelligent autonomous robotic operation relies on a sufficiently
expressive map of the environment. Classical geometric mapping methods retain
essential structural environment information, but lack a semantic understanding
and organization to allow high-level robotic reasoning. 3D scene graphs (3DSGs)
address this limitation by integrating geometric, topological, and semantic
relationships into a multi-level graph-based map. Outdoor autonomous operations
commonly rely on terrain information either due to task-dependence or the
traversability of the robotic platform. We propose a novel approach that
combines indoor 3DSG techniques with standard outdoor geometric mapping and
terrain-aware reasoning, producing terrain-aware place nodes and hierarchically
organized regions for outdoor environments. Our method generates a
task-agnostic metric-semantic sparse map and constructs a 3DSG from this map
for downstream planning tasks, all while remaining lightweight for autonomous
robotic operation. Our thorough evaluation demonstrates our 3DSG method
performs on par with state-of-the-art camera-based 3DSG methods in object
retrieval and surpasses them in region classification while remaining memory
efficient. We demonstrate its effectiveness in diverse robotic tasks of object
retrieval and region monitoring in both simulation and real-world environments.

</details>


### [164] [From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting](https://arxiv.org/abs/2509.19597)
*Sander Tonkens,Nikhil Uday Shinde,Azra Begzadić,Michael C. Yip,Jorge Cortés,Sylvia L. Herbert*

Main category: cs.RO

TL;DR: SPACE2TIME 通过将空间扰动重新参数化为时间变化，解决了离线学习安全过滤器在未知扰动下的应用问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设所有可能的模型不匹配源（如环境扰动）都有详细先验信息，这在现实环境中很少可用。SPACE2TIME 旨在解决这一问题。

Method: SPACE2TIME 通过重新参数化空间变化扰动为时间变化，利用预计算的价值函数在在线操作中实现安全部署。

Result: SPACE2TIME 在四旋翼无人机上通过大量仿真和硬件实验验证，表现显著优于基线方法。

Conclusion: SPACE2TIME 通过将空间变化扰动重新参数化为时间变化，有效解决了离线学习安全过滤器在未知、空间变化扰动下的应用问题，显著提高了安全性能。

Abstract: The widespread deployment of autonomous systems in safety-critical
environments such as urban air mobility hinges on ensuring reliable,
performant, and safe operation under varying environmental conditions. One such
approach, value function-based safety filters, minimally modifies a nominal
controller to ensure safety. Recent advances leverage offline learned value
functions to scale these safety filters to high-dimensional systems. However,
these methods assume detailed priors on all possible sources of model mismatch,
in the form of disturbances in the environment -- information that is rarely
available in real world settings. Even in well-mapped environments like urban
canyons or industrial sites, drones encounter complex, spatially-varying
disturbances arising from payload-drone interaction, turbulent airflow, and
other environmental factors. We introduce SPACE2TIME, which enables safe and
adaptive deployment of offline-learned safety filters under unknown,
spatially-varying disturbances. The key idea is to reparameterize spatial
variations in disturbance as temporal variations, enabling the use of
precomputed value functions during online operation. We validate SPACE2TIME on
a quadcopter through extensive simulations and hardware experiments,
demonstrating significant improvement over baselines.

</details>


### [165] [Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots](https://arxiv.org/abs/2509.19610)
*Qingxi Meng,Emiliano Flores,Carlos Quintero-Peña,Peizhu Qian,Zachary Kingston,Shannan K. Hamlin,Vaibhav Unhelkar,Lydia E. Kavraki*

Main category: cs.RO

TL;DR: 提出GPU并行化感知评分引导规划器（PS-PRM），结合神经代理模型解决高自由度机器人在动态环境中的运动规划问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，高自由度机器人需要同时完成导航和感知任务，但现有方法常忽略障碍物、仅适用于低自由度机器人或依赖简化的感知模型，且实时评估感知质量成本高昂。

Method: 采用GPU并行化的感知评分引导概率路线图规划器（PS-PRM）结合神经代理模型，实时评估感知任务质量并高效进行在线重规划。

Result: 实验表明，PS-PRM在仿真和真实机器人实验中，于静态和动态环境下均优于基线方法。

Conclusion: 本文提出了一种GPU并行化的感知评分引导概率路线图规划器（PS-PRM），通过神经代理模型有效解决了高自由度机器人在动态环境中的运动规划问题，实验证明其在静态和动态环境中均优于基线方法。

Abstract: In this work, we address the problem of planning robot motions for a
high-degree-of-freedom (DoF) robot that effectively achieves a given perception
task while the robot and the perception target move in a dynamic environment.
Achieving navigation and perception tasks simultaneously is challenging, as
these objectives often impose conflicting requirements. Existing methods that
compute motion under perception constraints fail to account for obstacles, are
designed for low-DoF robots, or rely on simplified models of perception.
Furthermore, in dynamic real-world environments, robots must replan and react
quickly to changes and directly evaluating the quality of perception (e.g.,
object detection confidence) is often expensive or infeasible at runtime. This
problem is especially important in human-centered environments such as homes
and hospitals, where effective perception is essential for safe and reliable
operation. To address these challenges, we propose a GPU-parallelized
perception-score-guided probabilistic roadmap planner with a neural surrogate
model (PS-PRM). The planner explicitly incorporates the estimated quality of a
perception task into motion planning for high-DoF robots. Our method uses a
learned model to approximate perception scores and leverages GPU parallelism to
enable efficient online replanning in dynamic settings. We demonstrate that our
planner, evaluated on high-DoF robots, outperforms baseline methods in both
static and dynamic environments in both simulation and real-robot experiments.

</details>


### [166] [EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data](https://arxiv.org/abs/2509.19626)
*Ryan Punamiya,Dhruv Patel,Patcharapong Aphiwetsa,Pranav Kuppili,Lawrence Y. Zhu,Simar Kareer,Judy Hoffman,Danfei Xu*

Main category: cs.RO

TL;DR: EgoBridge通过领域适应方法对齐人类与机器人策略潜在空间，显著提升模仿学习性能，并在新任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决人类与机器人数据在视觉外观、传感器模态和运动学方面的显著领域差距，以提升模仿学习的效果。

Method: 使用最优传输（OT）度量联合策略潜在特征和动作的差异，学习既对齐人类与机器人领域又保留动作相关信息的观察表示。

Result: 在三个真实世界的单臂和双臂操作任务中，EgoBridge比基线方法绝对策略成功率提高了44%，并能泛化到仅有人类数据的新对象、场景和任务。

Conclusion: EgoBridge通过统一的联合训练框架和基于最优传输的领域适应方法，显著提升了机器人模仿学习的性能，并在新任务、新场景中展现出良好的泛化能力。

Abstract: Egocentric human experience data presents a vast resource for scaling up
end-to-end imitation learning for robotic manipulation. However, significant
domain gaps in visual appearance, sensor modalities, and kinematics between
human and robot impede knowledge transfer. This paper presents EgoBridge, a
unified co-training framework that explicitly aligns the policy latent spaces
between human and robot data using domain adaptation. Through a measure of
discrepancy on the joint policy latent features and actions based on Optimal
Transport (OT), we learn observation representations that not only align
between the human and robot domain but also preserve the action-relevant
information critical for policy learning. EgoBridge achieves a significant
absolute policy success rate improvement by 44% over human-augmented
cross-embodiment baselines in three real-world single-arm and bimanual
manipulation tasks. EgoBridge also generalizes to new objects, scenes, and
tasks seen only in human data, where baselines fail entirely. Videos and
additional information can be found at https://ego-bridge.github.io

</details>


### [167] [Minimalistic Autonomous Stack for High-Speed Time-Trial Racing](https://arxiv.org/abs/2509.19636)
*Mahmoud Ali,Hassan Jardali,Youwei Yu,Durgakant Pushp,Lantao Liu*

Main category: cs.RO

TL;DR: 该论文提出了一种极简化的自动驾驶赛车堆栈，可在有限赛道测试时间内实现高速计时赛，最高时速206公里。


<details>
  <summary>Details</summary>
Motivation: 由于专用测试赛道的有限访问，开发全尺寸自动驾驶赛车堆栈面临挑战，需要一种能在有限赛道时间内快速验证的解决方案。

Method: 引入了一个强调快速部署和高效系统集成的极简化自动驾驶赛车堆栈，减少了赛道测试的需求。

Result: 在实际赛道上验证了该堆栈，总测试里程325公里，最高时速206公里，仅需11小时的赛道练习时间。

Conclusion: 该论文提出了一个极简化的自动驾驶赛车堆栈，能够在有限的赛道测试时间内实现高速计时赛，并在实际赛道上验证了其性能，最高时速达到206公里。

Abstract: Autonomous racing has seen significant advancements, driven by competitions
such as the Indy Autonomous Challenge (IAC) and the Abu Dhabi Autonomous Racing
League (A2RL). However, developing an autonomous racing stack for a full-scale
car is often constrained by limited access to dedicated test tracks,
restricting opportunities for real-world validation. While previous work
typically requires extended development cycles and significant track time, this
paper introduces a minimalistic autonomous racing stack for high-speed
time-trial racing that emphasizes rapid deployment and efficient system
integration with minimal on-track testing. The proposed stack was validated on
real speedways, achieving a top speed of 206 km/h within just 11 hours'
practice run on the track with 325 km in total. Additionally, we present the
system performance analysis, including tracking accuracy, vehicle dynamics, and
safety considerations, offering insights for teams seeking to rapidly develop
and deploy an autonomous racing stack with limited track access.

</details>


### [168] [RoboSSM: Scalable In-context Imitation Learning via State-Space Models](https://arxiv.org/abs/2509.19658)
*Youngju Yoo,Jiaheng Hu,Yifeng Zhu,Bo Liu,Qiang Liu,Roberto Martín-Martín,Peter Stone*

Main category: cs.RO

TL;DR: RoboSSM使用SSM替代Transformer，提升ICIL的效率和可扩展性，尤其在长上下文提示下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的ICIL方法存在计算限制，且在训练时未见的长提示下表现不佳。

Method: RoboSSM使用Longhorn（一种先进的SSM）替代Transformer，提供线性时间推理和强外推能力。

Result: 实验表明RoboSSM能有效外推到不同数量的上下文演示，在未见任务上表现优异，并在长时任务中保持稳健。

Conclusion: RoboSSM展示了SSMs作为ICIL高效且可扩展骨干的潜力，尤其在长上下文提示和长时任务中表现优异。

Abstract: In-context imitation learning (ICIL) enables robots to learn tasks from
prompts consisting of just a handful of demonstrations. By eliminating the need
for parameter updates at deployment time, this paradigm supports few-shot
adaptation to novel tasks. However, recent ICIL methods rely on Transformers,
which have computational limitations and tend to underperform when handling
longer prompts than those seen during training. In this work, we introduce
RoboSSM, a scalable recipe for in-context imitation learning based on
state-space models (SSM). Specifically, RoboSSM replaces Transformers with
Longhorn -- a state-of-the-art SSM that provides linear-time inference and
strong extrapolation capabilities, making it well-suited for long-context
prompts. We evaluate our approach on the LIBERO benchmark and compare it
against strong Transformer-based ICIL baselines. Experiments show that RoboSSM
extrapolates effectively to varying numbers of in-context demonstrations,
yields high performance on unseen tasks, and remains robust in long-horizon
scenarios. These results highlight the potential of SSMs as an efficient and
scalable backbone for ICIL. Our code is available at
https://github.com/youngjuY/RoboSSM.

</details>


### [169] [Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains](https://arxiv.org/abs/2509.19672)
*Dongzhe Zheng,Wenjie Mei*

Main category: cs.RO

TL;DR: 本文提出了一种记忆增强势场理论，通过整合历史经验到随机最优控制中，解决了非凸环境中的局部最优问题，并在机器人控制中展示了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 随机最优控制方法在复杂的非凸环境中常常陷入局部最优，因为它们无法从历史轨迹数据中学习。

Method: 引入了 Memory-Augmented Potential Field Theory，这是一个统一的数学框架，通过动态构建基于记忆的势场来识别和编码状态空间的关键拓扑特征，使控制器能够自动从过去的经验中学习并调整其优化策略。

Result: 理论分析表明，记忆增强势场具有非凸逃逸特性、渐近收敛特性和计算效率。在 Memory-Augmented Model Predictive Path Integral (MPPI) 控制器中的实现显示，在具有挑战性的非凸环境中性能显著提升。

Conclusion: Memory-Augmented Potential Field Theory 提供了一个可推广的框架，用于在控制系统中实现基于经验的学习，特别是在机器人动力学中，增强了在复杂状态空间中导航的能力，而无需专门的领域知识或大量的离线训练。

Abstract: Stochastic optimal control methods often struggle in complex non-convex
landscapes, frequently becoming trapped in local optima due to their inability
to learn from historical trajectory data. This paper introduces
Memory-Augmented Potential Field Theory, a unified mathematical framework that
integrates historical experience into stochastic optimal control. Our approach
dynamically constructs memory-based potential fields that identify and encode
key topological features of the state space, enabling controllers to
automatically learn from past experiences and adapt their optimization
strategy. We provide a theoretical analysis showing that memory-augmented
potential fields possess non-convex escape properties, asymptotic convergence
characteristics, and computational efficiency. We implement this theoretical
framework in a Memory-Augmented Model Predictive Path Integral (MPPI)
controller that demonstrates significantly improved performance in challenging
non-convex environments. The framework represents a generalizable approach to
experience-based learning within control systems (especially robotic dynamics),
enhancing their ability to navigate complex state spaces without requiring
specialized domain knowledge or extensive offline training.

</details>


### [170] [Formal Safety Verification and Refinement for Generative Motion Planners via Certified Local Stabilization](https://arxiv.org/abs/2509.19688)
*Devesh Nath,Haoran Yin,Glen Chou*

Main category: cs.RO

TL;DR: 提出了一种验证生成运动规划器安全性的方法，通过小型神经跟踪控制器和NNV工具验证闭环动态，显著提升了仿真和硬件中的安全性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以验证基于学习的生成运动规划器（GMPs）的安全性和动态可行性，因为神经网络验证（NNV）工具仅适用于小规模网络，而GMPs通常包含数百万神经元。

Method: 提出了一种方法，通过模仿GMP生成的参考轨迹，使用小型神经跟踪控制器进行稳定，并应用NNV工具验证闭环动态安全性，从而构建一个经过验证的GMP参考库。

Result: 在多种规划器（包括扩散、流匹配和视觉语言模型）上进行了评估，仿真（地面机器人和四旋翼）和硬件（差速驱动机器人）中均实现了安全性的提升。

Conclusion: 通过稳定GMP生成的参考轨迹并应用小型神经跟踪控制器，结合NNV工具验证闭环动态安全性，成功构建了一个经过验证的GMP参考库，并在硬件和仿真中实现了安全性的显著提升。

Abstract: We present a method for formal safety verification of learning-based
generative motion planners. Generative motion planners (GMPs) offer advantages
over traditional planners, but verifying the safety and dynamic feasibility of
their outputs is difficult since neural network verification (NNV) tools scale
only to a few hundred neurons, while GMPs often contain millions. To preserve
GMP expressiveness while enabling verification, our key insight is to imitate
the GMP by stabilizing references sampled from the GMP with a small neural
tracking controller and then applying NNV to the closed-loop dynamics. This
yields reachable sets that rigorously certify closed-loop safety, while the
controller enforces dynamic feasibility. Building on this, we construct a
library of verified GMP references and deploy them online in a way that
imitates the original GMP distribution whenever it is safe to do so, improving
safety without retraining. We evaluate across diverse planners, including
diffusion, flow matching, and vision-language models, improving safety in
simulation (on ground robots and quadcopters) and on hardware
(differential-drive robot).

</details>


### [171] [Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks](https://arxiv.org/abs/2509.19696)
*Noah Geiger,Tamim Asfour,Neville Hogan,Johannes Lachner*

Main category: cs.RO

TL;DR: 结合扩散模型与阻抗控制，实现高精度物理交互与轨迹生成，应用于跑酷和医疗任务，效果显著。


<details>
  <summary>Details</summary>
Motivation: 解决学习方法和阻抗控制在信息域和能量域中的局限性，实现物理交互与轨迹生成的高效结合。

Method: 提出了基于扩散的阻抗学习框架，结合Transformer-based Diffusion Model和能量基础估计器，通过SLERP-based四元数噪声调度器确保几何一致性，并应用方向性规则调整阻抗参数。

Result: 模型在少量样本训练下实现了亚毫米级位置精度和亚度级旋转精度，成功应用于跑酷场景和机器人辅助治疗任务，且在无需特定演示的情况下完成了多种形状的peg插入任务。

Conclusion: 该研究在物理AI领域迈出了重要一步，将基于模型的物理交互控制与基于学习的轨迹生成方法相结合，实现了高精度的实时扭矩控制和自主刚度适应。

Abstract: Learning methods excel at motion generation in the information domain but are
not primarily designed for physical interaction in the energy domain. Impedance
Control shapes physical interaction but requires task-aware tuning by selecting
feasible impedance parameters. We present Diffusion-Based Impedance Learning, a
framework that combines both domains. A Transformer-based Diffusion Model with
cross-attention to external wrenches reconstructs a simulated Zero-Force
Trajectory (sZFT). This captures both translational and rotational task-space
behavior. For rotations, we introduce a novel SLERP-based quaternion noise
scheduler that ensures geometric consistency. The reconstructed sZFT is then
passed to an energy-based estimator that updates stiffness and damping
parameters. A directional rule is applied that reduces impedance along non task
axes while preserving rigidity along task directions. Training data were
collected for a parkour scenario and robotic-assisted therapy tasks using
teleoperation with Apple Vision Pro. With only tens of thousands of samples,
the model achieved sub-millimeter positional accuracy and sub-degree rotational
accuracy. Its compact model size enabled real-time torque control and
autonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller
achieved smooth parkour traversal within force and velocity limits and 30/30
success rates for cylindrical, square, and star peg insertions without any
peg-specific demonstrations in the training data set. All code for the
Transformer-based Diffusion Model, the robot controller, and the Apple Vision
Pro telemanipulation framework is publicly available. These results mark an
important step towards Physical AI, fusing model-based control for physical
interaction with learning-based methods for trajectory generation.

</details>


### [172] [TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies](https://arxiv.org/abs/2509.19712)
*Liquan Wang,Jiangjie Bian,Eric Heiden,Animesh Garg*

Main category: cs.RO

TL;DR: TopoCut是一个用于多步骤机器人切割任务的综合基准，整合了高保真模拟、奖励设计和策略学习，展示了强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于复杂拓扑行为、密集对象状态感知困难以及缺乏高效切割结果评估方法，机器人切割可变形物体的任务仍具挑战性。

Method: TopoCut基于三个核心组件：1）基于粒子弹性塑性求解器的高保真模拟环境，配备损伤驱动的拓扑发现机制；2）结合拓扑发现与基于Laplace-Beltrami特征分析的姿态不变谱奖励模型；3）集成的策略学习管道，包括动态感知模块和PDDP策略。

Result: 大量实验表明，TopoCut支持轨迹生成、可扩展学习、精确评估，并在多样化对象几何、尺度、姿态和切割目标上表现出强泛化能力。

Conclusion: TopoCut通过整合高保真模拟环境、全面的奖励设计和集成的策略学习管道，为多步骤机器人切割任务提供了一个全面的基准，展示了在多样化对象几何、尺度、姿态和切割目标上的强大泛化能力。

Abstract: Robotic manipulation tasks involving cutting deformable objects remain
challenging due to complex topological behaviors, difficulties in perceiving
dense object states, and the lack of efficient evaluation methods for cutting
outcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for
multi-step robotic cutting tasks that integrates a cutting environment and
generalized policy learning. TopoCut is built upon three core components: (1)
We introduce a high-fidelity simulation environment based on a particle-based
elastoplastic solver with compliant von Mises constitutive models, augmented by
a novel damage-driven topology discovery mechanism that enables accurate
tracking of multiple cutting pieces. (2) We develop a comprehensive reward
design that integrates the topology discovery with a pose-invariant spectral
reward model based on Laplace-Beltrami eigenanalysis, facilitating consistent
and robust assessment of cutting quality. (3) We propose an integrated policy
learning pipeline, where a dynamics-informed perception module predicts
topological evolution and produces particle-wise, topology-aware embeddings to
support PDDP (Particle-based Score-Entropy Discrete Diffusion Policy) for
goal-conditioned policy learning. Extensive experiments demonstrate that
TopoCut supports trajectory generation, scalable learning, precise evaluation,
and strong generalization across diverse object geometries, scales, poses, and
cutting goals.

</details>


### [173] [Towards Autonomous Robotic Electrosurgery via Thermal Imaging](https://arxiv.org/abs/2509.19725)
*Naveed D. Riaziat,Joseph Chen,Axel Krieger,Jeremy D. Brown*

Main category: cs.RO

TL;DR: ThERMO通过热成像反馈智能调节工具速度，显著提升切割成功率和减少热损伤，优于恒定速度方法。


<details>
  <summary>Details</summary>
Motivation: 电外科手术虽然能减少切割力和出血，但存在热损伤风险，且现有自主电外科手术常使用恒定速度，无法适应组织特性变化。

Method: 引入Thermography for Electrosurgical Rate Modulation via Optimization (ThERMO)，利用热成像反馈智能调节工具速度。

Result: ThERMO在组织模型中的表现优于恒定速度方法，切割成功率提高三倍，峰值切割力减半，并能应对环境干扰。

Conclusion: ThERMO技术通过智能控制工具速度，显著提高了切割成功率并减少了热损伤和切割力，优于恒定速度方法。

Abstract: Electrosurgery is a surgical technique that can improve tissue cutting by
reducing cutting force and bleeding. However, electrosurgery adds a risk of
thermal injury to surrounding tissue. Expert surgeons estimate desirable
cutting velocities based on experience but have no quantifiable reference to
indicate if a particular velocity is optimal. Furthermore, prior demonstrations
of autonomous electrosurgery have primarily used constant tool velocity, which
is not robust to changes in electrosurgical tissue characteristics, power
settings, or tool type. Thermal imaging feedback provides information that can
be used to reduce thermal injury while balancing cutting force by controlling
tool velocity. We introduce Thermography for Electrosurgical Rate Modulation
via Optimization (ThERMO) to autonomously reduce thermal injury while balancing
cutting force by intelligently controlling tool velocity. We demonstrate ThERMO
in tissue phantoms and compare its performance to the constant velocity
approach. Overall, ThERMO improves cut success rate by a factor of three and
can reduce peak cutting force by a factor of two. ThERMO responds to varying
environmental disturbances, reduces damage to tissue, and completes cutting
tasks that would otherwise result in catastrophic failure for the constant
velocity approach.

</details>


### [174] [Simultaneous estimation of contact position and tool shape with high-dimensional parameters using force measurements and particle filtering](https://arxiv.org/abs/2509.19732)
*Kyo Kutsuzawa,Mitsuhiro Hayashibe*

Main category: cs.RO

TL;DR: 提出一种使用粒子滤波器的方法，同时估计工具形状和接触位置，提高了估计准确性。


<details>
  <summary>Details</summary>
Motivation: 估计工具与环境的接触状态对于执行装配和物体操作等接触任务至关重要。力信号即使在接触位置被工具遮挡时仍可用于估计接触状态。

Method: 使用粒子滤波器，每个粒子具有独立的工具形状参数，避免直接处理高维参数空间。

Result: 通过模拟和实验验证，提出的方法能够同时估计工具形状和接触位置。

Conclusion: 提出的方法能够同时估计工具形状和接触位置，提高了接触位置估计的准确性。

Abstract: Estimating the contact state between a grasped tool and the environment is
essential for performing contact tasks such as assembly and object
manipulation. Force signals are valuable for estimating the contact state, as
they can be utilized even when the contact location is obscured by the tool.
Previous studies proposed methods for estimating contact positions using
force/torque signals; however, most methods require the geometry of the tool
surface to be known. Although several studies have proposed methods that do not
require the tool shape, these methods require considerable time for estimation
or are limited to tools with low-dimensional shape parameters. Here, we propose
a method for simultaneously estimating the contact position and tool shape,
where the tool shape is represented by a grid, which is high-dimensional (more
than 1000 dimensional). The proposed method uses a particle filter in which
each particle has individual tool shape parameters, thereby to avoid directly
handling a high-dimensional parameter space. The proposed method is evaluated
through simulations and experiments using tools with curved shapes on a plane.
Consequently, the proposed method can estimate the shape of the tool
simultaneously with the contact positions, making the contact-position
estimation more accurate.

</details>


### [175] [Trajectory Planning Using Safe Ellipsoidal Corridors as Projections of Orthogonal Trust Regions](https://arxiv.org/abs/2509.19734)
*Akshay Jaitly,Jon Arrizabalaga,Guanrui Li*

Main category: cs.RO

TL;DR: 提出一种新的轨迹参数化方法，通过凸笛卡尔积球表示轨迹，显著提升复杂环境中的规划效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的走廊规划器在复杂环境中扩展性差，且需要显式分配时间窗口，因此需要一种更高效的轨迹规划方法。

Method: 引入了一种新的轨迹参数化方法，将轨迹表示为凸笛卡尔积球，并基于此提出了Orthogonal Trust Region Problem (Orth-TRP)凸优化程序，开发了高效的求解器。

Result: 实验表明，该方法在四旋翼轨迹规划基准测试中生成更平滑的轨迹且运行时间更短，尤其在高度复杂环境中表现优于现有方法。

Conclusion: 该论文提出了一种新的轨迹参数化方法，通过将轨迹表示为非凸碰撞自由走廊中的凸笛卡尔积球，显著提高了轨迹规划的效率和性能，特别是在复杂环境中。

Abstract: Planning collision free trajectories in complex environments remains a core
challenge in robotics. Existing corridor based planners which rely on
decomposition of the free space into collision free subsets scale poorly with
environmental complexity and require explicit allocations of time windows to
trajectory segments. We introduce a new trajectory parameterization that
represents trajectories in a nonconvex collision free corridor as being in a
convex cartesian product of balls. This parameterization allows us to decouple
problem size from geometric complexity of the solution and naturally avoids
explicit time allocation by allowing trajectories to evolve continuously inside
ellipsoidal corridors. Building on this representation, we formulate the
Orthogonal Trust Region Problem (Orth-TRP), a specialized convex program with
separable block constraints, and develop a solver that exploits this parallel
structure and the unique structure of each parallel subproblem for efficient
optimization. Experiments on a quadrotor trajectory planning benchmark show
that our approach produces smoother trajectories and lower runtimes than
state-of-the-art corridor based planners, especially in highly complicated
environments.

</details>


### [176] [Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training](https://arxiv.org/abs/2509.19752)
*Rushuai Yang,Hangxing Wei,Ran Zhang,Zhiyuan Feng,Xiaoyu Chen,Tong Li,Chuheng Zhang,Li Zhao,Jiang Bian,Xiu Su,Yi Chen*

Main category: cs.RO

TL;DR: A diffusion RL algorithm improves VLA model training by generating superior demonstrations, outperforming human and traditional RL methods.


<details>
  <summary>Details</summary>
Motivation: The reliance on large-scale human demonstrations limits scalability, and conventional RL struggles with long-horizon tasks.

Method: A modified diffusion policy optimization algorithm is proposed to generate high-quality and low-variance trajectories for VLA training.

Result: The diffusion RL-generated data achieves an average success rate of 81.9%, outperforming human data by +5.3% and Gaussian RL by +12.6%.

Conclusion: The diffusion RL algorithm is an effective alternative for generating high-quality, low-variance demonstrations for VLA models, outperforming human and Gaussian RL-generated data.

Abstract: Vision-language-action (VLA) models have shown strong generalization across
tasks and embodiments; however, their reliance on large-scale human
demonstrations limits their scalability owing to the cost and effort of manual
data collection. Reinforcement learning (RL) offers a potential alternative to
generate demonstrations autonomously, yet conventional RL algorithms often
struggle on long-horizon manipulation tasks with sparse rewards. In this paper,
we propose a modified diffusion policy optimization algorithm to generate
high-quality and low-variance trajectories, which contributes to a diffusion
RL-powered VLA training pipeline. Our algorithm benefits from not only the high
expressiveness of diffusion models to explore complex and diverse behaviors but
also the implicit regularization of the iterative denoising process to yield
smooth and consistent demonstrations. We evaluate our approach on the LIBERO
benchmark, which includes 130 long-horizon manipulation tasks, and show that
the generated trajectories are smoother and more consistent than both human
demonstrations and those from standard Gaussian RL policies. Further, training
a VLA model exclusively on the diffusion RL-generated data achieves an average
success rate of 81.9%, which outperforms the model trained on human data by
+5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight
our diffusion RL as an effective alternative for generating abundant,
high-quality, and low-variance demonstrations for VLA models.

</details>


### [177] [DynaFlow: Dynamics-embedded Flow Matching for Physically Consistent Motion Generation from State-only Demonstrations](https://arxiv.org/abs/2509.19804)
*Sowoo Lee,Dongyun Kang,Jaehyun Park,Hae-Won Park*

Main category: cs.RO

TL;DR: DynaFlow是一个嵌入可微分模拟器的流匹配框架，能生成物理一致的状态轨迹并推断动作序列，硬件实验验证了其在实际机器人上的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了从仅状态演示中生成物理一致的状态轨迹，并推断出底层动作序列，同时弥合运动学数据与实际执行之间的差距。

Method: DynaFlow框架将可微分模拟器直接嵌入流匹配模型，通过生成动作空间轨迹并映射到动态可行的状态轨迹，确保输出的物理一致性。

Result: 定量评估和硬件实验表明，DynaFlow生成的动作用于物理Go1四足机器人时，能成功复现多样步态、执行长时程运动，并将不可行运动学演示转化为动态可执行的行为。

Conclusion: DynaFlow通过将可微分模拟器嵌入流匹配模型，成功生成了物理一致的状态轨迹，并推断出底层动作序列。硬件实验验证了其在实际机器人上的有效性和可部署性，弥合了运动学数据与实际执行之间的差距。

Abstract: This paper introduces DynaFlow, a novel framework that embeds a
differentiable simulator directly into a flow matching model. By generating
trajectories in the action space and mapping them to dynamically feasible state
trajectories via the simulator, DynaFlow ensures all outputs are physically
consistent by construction. This end-to-end differentiable architecture enables
training on state-only demonstrations, allowing the model to simultaneously
generate physically consistent state trajectories while inferring the
underlying action sequences required to produce them. We demonstrate the
effectiveness of our approach through quantitative evaluations and showcase its
real-world applicability by deploying the generated actions onto a physical Go1
quadruped robot. The robot successfully reproduces diverse gait present in the
dataset, executes long-horizon motions in open-loop control and translates
infeasible kinematic demonstrations into dynamically executable, stylistic
behaviors. These hardware experiments validate that DynaFlow produces
deployable, highly effective motions on real-world hardware from state-only
demonstrations, effectively bridging the gap between kinematic data and
real-world execution.

</details>


### [178] [Where Did I Leave My Glasses? Open-Vocabulary Semantic Exploration in Real-World Semi-Static Environments](https://arxiv.org/abs/2509.19851)
*Benjamin Bogenberger,Oliver Harrison,Orrin Dahanaggamaarachchi,Lukas Brunke,Jingxing Qian,Siqi Zhou,Angela P. Schoellig*

Main category: cs.RO

TL;DR: 该论文提出了一种开放词汇语义探索系统，通过概率模型和主动探索维护一致性地图，结合LLM提升导航效率，实验证明其在半静态环境中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有语义探索研究在静态场景中缺乏持久对象级实例跟踪的问题，适应现实环境中对象的动态变化。

Method: 系统通过构建对象实例稳定性的概率模型、系统跟踪半静态变化，并主动探索长时间未访问的区域来维护一致性地图。此外，利用LLM基于语义地图进行开放词汇目标导航。

Result: 系统平均检测到95%的地图变化，效率提升超过29%，地图精度接近完全重建地图的2%，目标导航任务完成速度比次优策略快14%。

Conclusion: 该研究提出的开放词汇语义探索系统在半静态环境中表现出色，能够高效检测环境变化并完成目标导航任务，显著优于现有基线方法。

Abstract: Robots deployed in real-world environments, such as homes, must not only
navigate safely but also understand their surroundings and adapt to environment
changes. To perform tasks efficiently, they must build and maintain a semantic
map that accurately reflects the current state of the environment. Existing
research on semantic exploration largely focuses on static scenes without
persistent object-level instance tracking. A consistent map is, however,
crucial for real-world robotic applications where objects in the environment
can be removed, reintroduced, or shifted over time. In this work, to close this
gap, we propose an open-vocabulary, semantic exploration system for semi-static
environments. Our system maintains a consistent map by building a probabilistic
model of object instance stationarity, systematically tracking semi-static
changes, and actively exploring areas that have not been visited for a
prolonged period of time. In addition to active map maintenance, our approach
leverages the map's semantic richness with LLM-based reasoning for
open-vocabulary object-goal navigation. This enables the robot to search more
efficiently by prioritizing contextually relevant areas. We evaluate our
approach across multiple real-world semi-static environments. Our system
detects 95% of map changes on average, improving efficiency by more than 29% as
compared to random and patrol baselines. Overall, our approach achieves a
mapping precision within 2% of a fully rebuilt map while requiring
substantially less exploration and further completes object goal navigation
tasks about 14% faster than the next-best tested strategy (coverage
patrolling). A video of our work can be found at
http://tiny.cc/sem-explor-semi-static .

</details>


### [179] [SAGE:State-Aware Guided End-to-End Policy for Multi-Stage Sequential Tasks via Hidden Markov Decision Process](https://arxiv.org/abs/2509.19853)
*BinXu Wu,TengFei Zhang,Chen Yang,JiaHao Wen,HaoCheng Li,JingTian Ma,Zhen Chen,JingYuan Wang*

Main category: cs.RO

TL;DR: SAGE是一种状态感知的引导模仿学习框架，通过HMDP建模任务阶段，解决机器人操作中的状态模糊性，实验表明其在复杂任务中表现优异且标注效率高。


<details>
  <summary>Details</summary>
Motivation: 多阶段序列机器人操作任务中普遍存在状态模糊性问题，即视觉相似的观察对应不同的动作，需要一种能够显式捕捉潜在任务阶段并解决模糊性的方法。

Method: SAGE框架将任务建模为隐藏马尔可夫决策过程（HMDP），包含状态转移网络和状态感知动作策略，结合主动学习和软标签插值的半自动标注流程。

Result: 在多个具有状态模糊性的复杂多阶段序列任务中，SAGE在标准评估协议下实现了100%的任务成功率，且仅需手动标注约13%的状态即可保持性能。

Conclusion: SAGE框架通过显式建模任务阶段和解决状态模糊性，在复杂的多阶段序列机器人操作任务中实现了100%的任务成功率，显著优于基线方法。

Abstract: Multi-stage sequential (MSS) robotic manipulation tasks are prevalent and
crucial in robotics. They often involve state ambiguity, where visually similar
observations correspond to different actions. We present SAGE, a state-aware
guided imitation learning framework that models tasks as a Hidden Markov
Decision Process (HMDP) to explicitly capture latent task stages and resolve
ambiguity. We instantiate the HMDP with a state transition network that infers
hidden states, and a state-aware action policy that conditions on both
observations and hidden states to produce actions, thereby enabling
disambiguation across task stages. To reduce manual annotation effort, we
propose a semi-automatic labeling pipeline combining active learning and soft
label interpolation. In real-world experiments across multiple complex MSS
tasks with state ambiguity, SAGE achieved 100% task success under the standard
evaluation protocol, markedly surpassing the baselines. Ablation studies
further show that such performance can be maintained with manual labeling for
only about 13% of the states, indicating its strong effectiveness.

</details>


### [180] [D3Grasp: Diverse and Deformable Dexterous Grasping for General Objects](https://arxiv.org/abs/2509.19892)
*Keyu Wang,Bingcong Lu,Zhengxue Cheng,Hengdi Zhang,Li Song*

Main category: cs.RO

TL;DR: D3Grasp是一个多模态感知强化学习框架，显著提升了灵巧抓取的多样性和稳定性，尤其适用于可变形物体。


<details>
  <summary>Details</summary>
Motivation: 解决机器人灵巧抓取中高维动作空间和感知不确定性的挑战，尤其是针对多样化和可变形物体。

Method: 提出了一个多模态感知引导的强化学习框架D3Grasp，包括统一的多模态表示、非对称强化学习架构和精心设计的训练策略。

Result: D3Grasp在多样化和大规模物体类别中表现出高度鲁棒性，平均成功率达95.1%。

Conclusion: D3Grasp significantly提升了对于可变形和柔顺物体的灵巧抓取性能，在真实世界试验中平均成功率达到95.1%，超越了现有方法。

Abstract: Achieving diverse and stable dexterous grasping for general and deformable
objects remains a fundamental challenge in robotics, due to high-dimensional
action spaces and uncertainty in perception. In this paper, we present D3Grasp,
a multimodal perception-guided reinforcement learning framework designed to
enable Diverse and Deformable Dexterous Grasping. We firstly introduce a
unified multimodal representation that integrates visual and tactile perception
to robustly grasp common objects with diverse properties. Second, we propose an
asymmetric reinforcement learning architecture that exploits privileged
information during training while preserving deployment realism, enhancing both
generalization and sample efficiency. Third, we meticulously design a training
strategy to synthesize contact-rich, penetration-free, and kinematically
feasible grasps with enhanced adaptability to deformable and contact-sensitive
objects. Extensive evaluations confirm that D3Grasp delivers highly robust
performance across large-scale and diverse object categories, and substantially
advances the state of the art in dexterous grasping for deformable and
compliant objects, even under perceptual uncertainty and real-world
disturbances. D3Grasp achieves an average success rate of 95.1% in real-world
trials,outperforming prior methods on both rigid and deformable objects
benchmarks.

</details>


### [181] [GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference](https://arxiv.org/abs/2509.19916)
*Zijun Che,Yinghong Zhang,Shengyi Liang,Boyu Zhou,Jun Ma,Jinni Zhou*

Main category: cs.RO

TL;DR: GUIDE是一种新型自主探索框架，结合全局图推理和扩散决策，显著提升探索效率，减少冗余移动。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模未观察空间和规划全局高效路径方面存在不足，因此需要一种更有效的探索框架。

Method: 提出GUIDE探索框架，结合全局图推理和扩散决策网络，使用区域评估全局图表示和扩散策略网络生成稳定的前瞻性动作序列。

Result: GUIDE在模拟和实际部署中表现优异，覆盖完成速度提升18.3%，冗余移动减少34.9%。

Conclusion: GUIDE框架通过结合全局图推理和基于扩散的决策制定，显著提升了自主探索在结构化和复杂室内环境中的性能，实现了更快的覆盖完成和更少的冗余移动。

Abstract: Autonomous exploration in structured and complex indoor environments remains
a challenging task, as existing methods often struggle to appropriately model
unobserved space and plan globally efficient paths. To address these
limitations, we propose GUIDE, a novel exploration framework that
synergistically combines global graph inference with diffusion-based
decision-making. We introduce a region-evaluation global graph representation
that integrates both observed environmental data and predictions of unexplored
areas, enhanced by a region-level evaluation mechanism to prioritize reliable
structural inferences while discounting uncertain predictions. Building upon
this enriched representation, a diffusion policy network generates stable,
foresighted action sequences with significantly reduced denoising steps.
Extensive simulations and real-world deployments demonstrate that GUIDE
consistently outperforms state-of-the-art methods, achieving up to 18.3% faster
coverage completion and a 34.9% reduction in redundant movements.

</details>


### [182] [Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation](https://arxiv.org/abs/2509.19954)
*Pinhao Song,Yurui Du,Ophelie Saussus,Sofie De Schrijver,Irene Caprara,Peter Janssen,Renaud Detry*

Main category: cs.RO

TL;DR: RT-V2是一种概率共享控制导航方法，通过结合先验意图模型和实时用户输入，实现准确意图预测和安全人机交互。


<details>
  <summary>Details</summary>
Motivation: 旨在实现准确意图预测和安全有效的人机交互辅助，解决用户长期行为模式与低维噪声控制信号的联合建模问题。

Method: RT-V2结合了先验意图模型和后验更新，利用循环神经网络和条件变分自编码器捕捉用户意图的多模态和历史依赖性，并整合不确定的用户命令来推断期望动作。

Result: RT-V2在意图估计上优于现有技术，提供安全高效的导航支持，并在用户自主性和辅助干预之间取得良好平衡。

Conclusion: RT-V2通过结合概率建模、强化学习和安全优化，为多样化辅助技术提供了原则性和通用性的共享控制方法。

Abstract: We propose a probabilistic shared-control solution for navigation, called
Robot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe,
effective assistance in human-robot interaction. RT-V2 jointly models a user's
long-term behavioral patterns and their noisy, low-dimensional control signals
by combining a prior intent model with a posterior update that accounts for
real-time user input and environmental context. The prior captures the
multimodal and history-dependent nature of user intent using recurrent neural
networks and conditional variational autoencoders, while the posterior
integrates this with uncertain user commands to infer desired actions. We
conduct extensive experiments to validate RT-V2 across synthetic benchmarks,
human-computer interaction studies with keyboard input, and brain-machine
interface experiments with non-human primates. Results show that RT-V2
outperforms the state of the art in intent estimation, provides safe and
efficient navigation support, and adequately balances user autonomy with
assistive intervention. By unifying probabilistic modeling, reinforcement
learning, and safe optimization, RT-V2 offers a principled and generalizable
approach to shared control for diverse assistive technologies.

</details>


### [183] [Generalist Robot Manipulation beyond Action Labeled Data](https://arxiv.org/abs/2509.19958)
*Alexander Spiridonov,Jan-Nico Zaech,Nikolay Nikolov,Luc Van Gool,Danda Pani Paudel*

Main category: cs.RO

TL;DR: 利用无动作标签视频提升机器人策略性能，实现无标签学习新任务。


<details>
  <summary>Details</summary>
Motivation: 解决高质量、动作标记的机器人演示数据难以大规模获取的问题，以增强机器人的零样本任务处理能力。

Method: 提出了一种方法，通过从无动作标签的视频中提取密集的动态3D点云，并使用3D动态预测器进行自监督学习，然后利用较小的标记数据集进行动作对齐。

Result: 该方法不仅能够从未标记的人类和机器人演示中学习，提升下游通用机器人策略的性能，还能使机器人在无动作标签的情况下学习新任务。

Conclusion: 该方法通过利用无动作标签的视频数据，提升了通用机器人策略的性能，并实现了在无动作标签情况下学习新任务的能力。

Abstract: Recent advances in generalist robot manipulation leverage pre-trained
Vision-Language Models (VLMs) and large-scale robot demonstrations to tackle
diverse tasks in a zero-shot manner. A key challenge remains: scaling
high-quality, action-labeled robot demonstration data, which existing methods
rely on for robustness and generalization. To address this, we propose a method
that benefits from videos without action labels - featuring humans and/or
robots in action - enhancing open-vocabulary performance and enabling
data-efficient learning of new tasks. Our method extracts dense, dynamic 3D
point clouds at the hand or gripper location and uses a proposed 3D dynamics
predictor for self-supervision. This predictor is then tuned to an action
predictor using a smaller labeled dataset for action alignment. We show that
our method not only learns from unlabeled human and robot demonstrations -
improving downstream generalist robot policies - but also enables robots to
learn new tasks without action labels (i.e., out-of-action generalization) in
both real-world and simulated settings.

</details>


### [184] [An effective control of large systems of active particles: An application to evacuation problem](https://arxiv.org/abs/2509.19972)
*Albina Klepach,Egor E. Nuzhin,Alexey A. Tsukanov,Nikolay V. Brilliantov*

Main category: cs.RO

TL;DR: 结合RL与人工力的领导者控制策略，有效解决了大规模主动粒子系统的操控难题，特别是在疏散场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在操控大规模主动粒子系统时缺乏可扩展性和鲁棒性，特别是在需要个体控制的场景下。通过领导者控制策略可以克服这些限制。

Method: 引入了广义Vicsek模型，结合强化学习（RL）与人工力，开发了一种领导者控制策略。

Result: 该方法在机器人救援者（领导者）疏散大规模人群的应用中，表现出比传统RL方法更优的鲁棒性和效率。

Conclusion: 本研究提出了一种结合强化学习与人工力的领导者控制策略，有效解决了大规模主动粒子系统操控的挑战，特别是在复杂场景下的疏散问题。

Abstract: Manipulation of large systems of active particles is a serious challenge
across diverse domains, including crowd management, control of robotic swarms,
and coordinated material transport. The development of advanced control
strategies for complex scenarios is hindered, however, by the lack of
scalability and robustness of the existing methods, in particular, due to the
need of an individual control for each agent. One possible solution involves
controlling a system through a leader or a group of leaders, which other agents
tend to follow. Using such an approach we develop an effective control strategy
for a leader, combining reinforcement learning (RL) with artificial forces
acting on the system. To describe the guidance of active particles by a leader
we introduce the generalized Vicsek model. This novel method is then applied to
the problem of the effective evacuation by a robot-rescuer (leader) of large
groups of people from hazardous places. We demonstrate, that while a
straightforward application of RL yields suboptimal results, even for advanced
architectures, our approach provides a robust and efficient evacuation
strategy. The source code supporting this study is publicly available at:
https://github.com/cinemere/evacuation.

</details>


### [185] [Lidar-based Tracking of Traffic Participants with Sensor Nodes in Existing Urban Infrastructure](https://arxiv.org/abs/2509.20009)
*Simon Schäfer,Bassam Alrifaee,Ehsan Hashemi*

Main category: cs.RO

TL;DR: 该论文提出了一种仅使用激光雷达的实时跟踪框架，通过高效算法在CPU边缘硬件上实现高精度跟踪，适用于城市基础设施部署。


<details>
  <summary>Details</summary>
Motivation: 城市部署需要可扩展的实时跟踪解决方案，但传统的远程感知成本高且计算密集，尤其在感知退化条件下。

Method: 该框架采用扩展卡尔曼滤波器进行状态更新，使用1D网格图/贝叶斯更新进行维度估计，通过最可能的足迹驱动的查找表进行类别更新，以及根据轨道年龄和边界框一致性进行存在估计。

Result: 在动态城市场景中，该框架实现了实时性能和高精度：完整端到端管道在99.88%的消息中在100毫秒内完成，具有出色的检测率。在模拟风和传感器振动下进一步确认了鲁棒性。

Conclusion: 该框架通过仅使用激光雷达和边缘计算单元，实现了在城市基础设施中的可靠、实时路边跟踪，降低了部署成本并简化了大规模城市推广和维护工作。

Abstract: This paper presents a lidar-only state estimation and tracking framework,
along with a roadside sensing unit for integration with existing urban
infrastructure. Urban deployments demand scalable, real-time tracking
solutions, yet traditional remote sensing remains costly and computationally
intensive, especially under perceptually degraded conditions. Our sensor node
couples a single lidar with an edge computing unit and runs a computationally
efficient, GPU-free observer that simultaneously estimates object state, class,
dimensions, and existence probability. The pipeline performs: (i) state updates
via an extended Kalman filter, (ii) dimension estimation using a 1D
grid-map/Bayesian update, (iii) class updates via a lookup table driven by the
most probable footprint, and (iv) existence estimation from track age and
bounding-box consistency. Experiments in dynamic urban-like scenes with diverse
traffic participants demonstrate real-time performance and high precision: The
complete end-to-end pipeline finishes within \SI{100}{\milli\second} for
\SI{99.88}{\%} of messages, with an excellent detection rate. Robustness is
further confirmed under simulated wind and sensor vibration. These results
indicate that reliable, real-time roadside tracking is feasible on CPU-only
edge hardware, enabling scalable, privacy-friendly deployments within existing
city infrastructure. The framework integrates with existing poles, traffic
lights, and buildings, reducing deployment costs and simplifying large-scale
urban rollouts and maintenance efforts.

</details>


### [186] [MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping](https://arxiv.org/abs/2509.20036)
*Yinzhao Dong,Ji Ma,Liu Zhao,Wanyue Li,Peng Lu*

Main category: cs.RO

TL;DR: MARG是一种结合地形图和本体感知的DRL控制器，通过简化传感器需求和优化奖励设计，提升了机器人在高风险间隙地形中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有盲控制器在复杂间隙地形中难以确保安全高效通行，而感知控制器存在多传感器部署复杂和计算资源昂贵的问题。

Method: 提出了MARG控制器，结合地形图和本体感知动态调整动作，并设计了三项足部相关奖励以探索安全立足点。此外，提出了地形图生成（TMG）模型以减少漂移，仅需一个LiDAR即可提供准确地形图。

Result: 实验表明，MARG在多种高风险地形任务中保持稳定性，实现了零样本策略迁移。

Conclusion: MARG控制器通过整合地形图和本体感知，显著提升了机器人在高风险间隙地形中的稳定性和安全性，同时通过简化传感器部署和计算资源需求，实现了零样本策略迁移。

Abstract: Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have
demonstrated impressive performance on challenging terrains, allowing robots to
execute complex skills such as climbing, running, and jumping. However,
existing blind locomotion controllers often struggle to ensure safety and
efficient traversal through risky gap terrains, which are typically highly
complex, requiring robots to perceive terrain information and select
appropriate footholds during locomotion accurately. Meanwhile, existing
perception-based controllers still present several practical limitations,
including a complex multi-sensor deployment system and expensive computing
resource requirements. This paper proposes a DRL controller named MAstering
Risky Gap Terrains (MARG), which integrates terrain maps and proprioception to
dynamically adjust the action and enhance the robot's stability in these tasks.
During the training phase, our controller accelerates policy optimization by
selectively incorporating privileged information (e.g., center of mass,
friction coefficients) that are available in simulation but unmeasurable
directly in real-world deployments due to sensor limitations. We also designed
three foot-related rewards to encourage the robot to explore safe footholds.
More importantly, a terrain map generation (TMG) model is proposed to reduce
the drift existing in mapping and provide accurate terrain maps using only one
LiDAR, providing a foundation for zero-shot transfer of the learned policy. The
experimental results indicate that MARG maintains stability in various risky
terrain tasks.

</details>


### [187] [LLM Trainer: Automated Robotic Data Generating via Demonstration Augmentation using LLMs](https://arxiv.org/abs/2509.20070)
*Abraham George,Amir Barati Farimani*

Main category: cs.RO

TL;DR: LLM Trainer自动化生成机器人模仿学习数据，优于基线方法，硬件验证成功。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型的世界知识，解决模仿学习中数据稀缺的问题。

Method: 分解演示生成为离线标注和在线关键姿态重定向两步，使用Thompson采样优化标注。

Result: 在多项任务中优于专家设计的基线方法，并展示了结合前馈计划和反馈控制的集成策略。

Conclusion: LLM Trainer通过自动化流程成功将少量人类演示转化为大规模机器人数据集，并在硬件上验证了其可行性。

Abstract: We present LLM Trainer, a fully automated pipeline that leverages the world
knowledge of Large Language Models (LLMs) to transform a small number of human
demonstrations (as few as one) into a large robot dataset for imitation
learning. Our approach decomposes demonstration generation into two steps: (1)
offline demonstration annotation that extracts keyframes, salient objects, and
pose-object relations; and (2) online keypose retargeting that adapts those
keyframes to a new scene, given an initial observation. Using these modified
keypoints, our system warps the original demonstration to generate a new
trajectory, which is then executed, and the resulting demo, if successful, is
saved. Because the annotation is reusable across scenes, we use Thompson
sampling to optimize the annotation, significantly improving generation success
rate. We evaluate our method on a range of tasks, and find that our data
annotation method consistently outperforms expert-engineered baselines. We
further show an ensemble policy that combines the optimized LLM feed-forward
plan with a learned feedback imitation learning controller. Finally, we
demonstrate hardware feasibility on a Franka Emika Panda robot. For additional
materials and demonstration videos, please see the project website:
https://sites.google.com/andrew.cmu.edu/llm-trainer

</details>


### [188] [Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning](https://arxiv.org/abs/2509.20077)
*Xun Li,Rodrigo Santa Cruz,Mingze Xi,Hu Zhang,Madhawa Perera,Ziwei Wang,Ahalya Ravendran,Brandon J. Matthews,Feng Xu,Matt Adcock,Dadong Wang,Jiajun Liu*

Main category: cs.RO

TL;DR: 3D QSR框架通过融合多种3D表示和视觉语言模型，实现了机器人对复杂3D环境的语义理解和任务规划。


<details>
  <summary>Details</summary>
Motivation: 为了实现机器人对高级人类指令的理解和执行复杂任务，需要一种能够融合精确几何结构与丰富语义的智能地图。

Method: 引入3D可查询场景表示（3D QSR）框架，结合全景重建、3D点云和3D场景图三种互补表示，并集成大型视觉语言模型以实现语义查询。

Result: 在Unity模拟的机器人任务规划场景和真实湿实验室环境的数字副本中测试，验证了框架的有效性。

Conclusion: 3D QSR框架成功实现了场景理解与空间语义推理的结合，有效将高级人类指令转化为复杂3D环境中的精确机器人任务规划。

Abstract: To enable robots to comprehend high-level human instructions and perform
complex tasks, a key challenge lies in achieving comprehensive scene
understanding: interpreting and interacting with the 3D environment in a
meaningful way. This requires a smart map that fuses accurate geometric
structure with rich, human-understandable semantics. To address this, we
introduce the 3D Queryable Scene Representation (3D QSR), a novel framework
built on multimedia data that unifies three complementary 3D representations:
(1) 3D-consistent novel view rendering and segmentation from panoptic
reconstruction, (2) precise geometry from 3D point clouds, and (3) structured,
scalable organization via 3D scene graphs. Built on an object-centric design,
the framework integrates with large vision-language models to enable semantic
queryability by linking multimodal object embeddings, and supporting
object-level retrieval of geometric, visual, and semantic information. The
retrieved data are then loaded into a robotic task planner for downstream
execution. We evaluate our approach through simulated robotic task planning
scenarios in Unity, guided by abstract language instructions and using the
indoor public dataset Replica. Furthermore, we apply it in a digital duplicate
of a real wet lab environment to test QSR-supported robotic task planning for
emergency response. The results demonstrate the framework's ability to
facilitate scene understanding and integrate spatial and semantic reasoning,
effectively translating high-level human instructions into precise robotic task
planning in complex 3D environments.

</details>


### [189] [DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping](https://arxiv.org/abs/2509.20081)
*Jose E. Maese,Luis Merino,Fernando Caballero*

Main category: cs.RO

TL;DR: 本文提出了一种基于CPU的高效体积映射框架，通过方向位掩码集成方案实现实时3D重建，处理速度与精度均达到当代技术水平。


<details>
  <summary>Details</summary>
Motivation: 为了解决大多数TSDF/ESDF方法依赖GPU加速的问题，提出一种完全在CPU上运行的高效方法，实现高分辨率映射且不牺牲运行时性能。

Method: 采用基于方向位掩码的集成方案，将原始LiDAR点云数据增量融合到体素网格中，生成适合实时3D重建的密集且一致的TSDF表示。

Result: 在真实世界开放数据集上的实验表明，生成的映射在精度上与当代技术相当，且处理速度具有竞争力。

Conclusion: 该论文提出的基于CPU的高效体积映射框架在实时3D重建中表现出色，生成的TSDF表示在精度上与当代技术相当，且处理速度具有竞争力。

Abstract: This paper presents a high-efficiency, CPU-only volumetric mapping framework
based on a Truncated Signed Distance Field (TSDF). The system incrementally
fuses raw LiDAR point-cloud data into a voxel grid using a directional
bitmask-based integration scheme, producing dense and consistent TSDF
representations suitable for real-time 3D reconstruction. A key feature of the
approach is that the processing time per point-cloud remains constant,
regardless of the voxel grid resolution, enabling high resolution mapping
without sacrificing runtime performance. In contrast to most recent TSDF/ESDF
methods that rely on GPU acceleration, our method operates entirely on CPU,
achieving competitive results in speed. Experiments on real-world open datasets
demonstrate that the generated maps attain accuracy on par with contemporary
mapping techniques.

</details>


### [190] [Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots](https://arxiv.org/abs/2509.20082)
*Surov Maksim*

Main category: cs.RO

TL;DR: 本文提出了一种控制方法，通过扩展横向线性化框架并结合时变LQR和滑模控制，实现了欠驱动机器人系统的轨道稳定性和时间同步，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决欠驱动机器人系统中实现周期性轨迹的轨道稳定性和时间同步的挑战。

Method: 该方法扩展了经典的横向线性化框架，明确引入了时间不同步动态，并采用时变LQR和滑模控制的组合来稳定扩展后的横向动态。

Result: 实验在六台蝴蝶机器人上实施了集中式和分散式控制策略，验证了理论结果的有效性。

Conclusion: 本文提出的控制方法通过实验验证，成功实现了欠驱动机器人系统中周期性轨迹的轨道稳定性和时间同步。

Abstract: This paper presents a control methodology for achieving orbital stabilization
with simultaneous time synchronization of periodic trajectories in
underactuated robotic systems. The proposed approach extends the classical
transverse linearization framework to explicitly incorporate
time-desynchronization dynamics. To stabilize the resulting extended transverse
dynamics, we employ a combination of time-varying LQR and sliding-mode control.
The theoretical results are validated experimentally through the implementation
of both centralized and decentralized control strategies on a group of six
Butterfly robots.

</details>


### [191] [C-3TO: Continuous 3D Trajectory Optimization on Neural Euclidean Signed Distance Fields](https://arxiv.org/abs/2509.20084)
*Guillermo Gil,Jose Antonio Cobano,Luis Merino,Fernando Caballero*

Main category: cs.RO

TL;DR: C-3TO是一种新型连续3D轨迹优化框架，利用在线神经ESDF，通过两阶段优化生成安全平滑的轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖于离散化ESDF网格和插值的问题，提供更精确的梯度信息。

Method: 采用两阶段非线性优化管道，直接在连续的神经ESDF上优化由五次多项式表示的平滑轨迹。

Result: 实验证明C-3TO能生成碰撞感知且动态可行的轨迹，并具有灵活的参数定义能力。

Conclusion: C-3TO通过结合连续轨迹参数化和连续更新的神经ESDF，为空中机器人提供了安全高效的局部重规划基础。

Abstract: This paper introduces a novel framework for continuous 3D trajectory
optimization in cluttered environments, leveraging online neural Euclidean
Signed Distance Fields (ESDFs). Unlike prior approaches that rely on
discretized ESDF grids with interpolation, our method directly optimizes smooth
trajectories represented by fifth-order polynomials over a continuous neural
ESDF, ensuring precise gradient information throughout the entire trajectory.
The framework integrates a two-stage nonlinear optimization pipeline that
balances efficiency, safety and smoothness. Experimental results demonstrate
that C-3TO produces collision-aware and dynamically feasible trajectories.
Moreover, its flexibility in defining local window sizes and optimization
parameters enables straightforward adaptation to diverse user's needs without
compromising performance. By combining continuous trajectory parameterization
with a continuously updated neural ESDF, C-3TO establishes a robust and
generalizable foundation for safe and efficient local replanning in aerial
robotics.

</details>


### [192] [Hybrid Safety Verification of Multi-Agent Systems using $ψ$-Weighted CBFs and PAC Guarantees](https://arxiv.org/abs/2509.20093)
*Venkat Margapuri,Garik Kazanjian,Naren Kosaraju*

Main category: cs.RO

TL;DR: 该研究提出了一种混合安全验证框架，结合控制屏障函数和$\psi$-加权公式，验证了闭环多智能体系统在有限随机扰动下的安全性，实验验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决有限随机扰动下闭环多智能体系统的安全验证问题，通过引入方向控制对齐编码的安全约束，提升验证的可行性和准确性。

Method: 该方法将确定性可接受性与蒙特卡洛滚动的经验验证相结合，并基于边缘感知安全违规推导了PAC式保证，提供了概率安全证书。

Result: 实验结果表明，在不同有限随机扰动下，所提方法具有可行性，并能有效验证系统安全性。

Conclusion: 该研究提出的混合安全验证框架在有限随机扰动下，通过结合控制屏障函数和新型$\psi$-加权公式，有效验证了闭环多智能体系统的安全性。

Abstract: This study proposes a hybrid safety verification framework for closed-loop
multi-agent systems under bounded stochastic disturbances. The proposed
approach augments control barrier functions with a novel $\psi$-weighted
formulation that encodes directional control alignment between agents into the
safety constraints. Deterministic admissibility is combined with empirical
validation via Monte Carlo rollouts, and a PAC-style guarantee is derived based
on margin-aware safety violations to provide a probabilistic safety
certificate. The results from the experiments conducted under different bounded
stochastic disturbances validate the feasibility of the proposed approach.

</details>


### [193] [Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving](https://arxiv.org/abs/2509.20109)
*Pengxiang Li,Yinan Zheng,Yue Wang,Huimin Wang,Hang Zhao,Jingjing Liu,Xianyuan Zhan,Kun Zhan,Xianpeng Lang*

Main category: cs.RO

TL;DR: ReflectDrive 是一种基于学习的新框架，通过离散扩散和反射机制实现安全轨迹生成，解决了现有自动驾驶方法在物理规则编码和计算效率上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端自动驾驶方法受限于模仿学习的物理规则编码不足，依赖复杂的后处理强化学习或计算昂贵的扩散引导。ReflectDrive 旨在解决这些挑战，提供更高效和安全的轨迹生成方案。

Method: 该方法首先将二维驾驶空间离散化以构建动作码本，利用预训练的扩散语言模型进行规划任务微调。核心是一个安全感知的反射机制，通过迭代自校正无需梯度计算。包括目标条件轨迹生成和多模态驾驶行为建模，随后通过局部搜索识别不安全标记并确定可行解，作为基于修复的再生成的安全锚点。

Result: 在 NAVSIM 基准测试中，ReflectDrive 在安全关键轨迹生成方面表现出显著优势。

Conclusion: ReflectDrive 提出了一种新颖的学习框架，通过离散扩散和反射机制实现了安全轨迹生成，为自动驾驶系统提供了可扩展且可靠的解决方案。

Abstract: End-to-End (E2E) solutions have emerged as a mainstream approach for
autonomous driving systems, with Vision-Language-Action (VLA) models
representing a new paradigm that leverages pre-trained multimodal knowledge
from Vision-Language Models (VLMs) to interpret and interact with complex
real-world environments. However, these methods remain constrained by the
limitations of imitation learning, which struggles to inherently encode
physical rules during training. Existing approaches often rely on complex
rule-based post-refinement, employ reinforcement learning that remains largely
limited to simulation, or utilize diffusion guidance that requires
computationally expensive gradient calculations. To address these challenges,
we introduce ReflectDrive, a novel learning-based framework that integrates a
reflection mechanism for safe trajectory generation via discrete diffusion. We
first discretize the two-dimensional driving space to construct an action
codebook, enabling the use of pre-trained Diffusion Language Models for
planning tasks through fine-tuning. Central to our approach is a safety-aware
reflection mechanism that performs iterative self-correction without gradient
computation. Our method begins with goal-conditioned trajectory generation to
model multi-modal driving behaviors. Based on this, we apply local search
methods to identify unsafe tokens and determine feasible solutions, which then
serve as safe anchors for inpainting-based regeneration. Evaluated on the
NAVSIM benchmark, ReflectDrive demonstrates significant advantages in
safety-critical trajectory generation, offering a scalable and reliable
solution for autonomous driving systems.

</details>


### [194] [A Biomimetic Vertebraic Soft Robotic Tail for High-Speed, High-Force Dynamic Maneuvering](https://arxiv.org/abs/2509.20219)
*Sicong Liu,Jianhui Liu,Fang Chen,Wenjian Yang,Juan Yi,Yu Zheng,Zheng Wang,Wanchao Chi,Chaoyang Song*

Main category: cs.RO

TL;DR: BVSR尾部通过仿生椎骨和柔顺气动体的混合设计，解决了刚性尾部安全性和软尾部性能不足的问题，实现了高性能动态和多功能应用。


<details>
  <summary>Details</summary>
Motivation: 解决刚性尾部在非结构化环境中的安全风险与软尾部速度和力量不足之间的权衡问题。

Method: 开发并验证了一个结合椎骨约束的专用运动学和动力学模型。

Result: BVSR尾部实现了超过670°/s的角速度，并产生高达5.58 N的惯性力和1.21 Nm的扭矩，相比非椎骨设计提高了200%以上。

Conclusion: BVSR尾部的混合设计通过解耦负载和驱动，实现了高性能动态和保持柔顺性，展示了其在敏捷机器人平台中的多功能性和实用价值。

Abstract: Robotic tails can enhance the stability and maneuverability of mobile robots,
but current designs face a trade-off between the power of rigid systems and the
safety of soft ones. Rigid tails generate large inertial effects but pose risks
in unstructured environments, while soft tails lack sufficient speed and force.
We present a Biomimetic Vertebraic Soft Robotic (BVSR) tail that resolves this
challenge through a compliant pneumatic body reinforced by a passively jointed
vertebral column inspired by musculoskeletal structures. This hybrid design
decouples load-bearing and actuation, enabling high-pressure actuation (up to 6
bar) for superior dynamics while preserving compliance. A dedicated kinematic
and dynamic model incorporating vertebral constraints is developed and
validated experimentally. The BVSR tail achieves angular velocities above
670{\deg}/s and generates inertial forces and torques up to 5.58 N and 1.21 Nm,
indicating over 200% improvement compared to non-vertebraic designs.
Demonstrations on rapid cart stabilization, obstacle negotiation, high-speed
steering, and quadruped integration confirm its versatility and practical
utility for agile robotic platforms.

</details>


### [195] [Techno-Economic analysis for Smart Hangar inspection operations through Sensing and Localisation at scale](https://arxiv.org/abs/2509.20229)
*Angelos Plastropoulos,Nicolas P. Avdelidis,Argyrios Zolotas*

Main category: cs.RO

TL;DR: 本文提出了一个技术经济路线图，通过优化相机选择和布局，为智能机库提供了一种平衡精度、覆盖和预算的方法，展示了其在实现经济高效传感方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 飞机维修和翻新机库中的定位准确性、弹性和经济性是关键挑战，尤其是在GPS受限的环境中。本文旨在填补这一领域的空白，提供针对特定领域的比较研究，以支持智能机库中可靠且可扩展的定位系统部署。

Method: 本文介绍了双层次优化框架，用于相机选择和定位，结合市场基础的相机镜头选择与优化求解器，生成满足精度目标且最小化硬件成本的相机布局。

Result: 研究在40x50米的机库中对运动捕捉、超宽带和天花板安装相机网络进行了基准测试，展示了优化视觉架构在机器人定位、资产跟踪和表面缺陷检测中的潜力。

Conclusion: 本文提出了一个技术经济路线图，通过优化视觉架构，为下一代智能机库提供了一种平衡准确性、覆盖范围和预算的可操作方法，展示了其在实现稳健且经济高效的传感方面的潜力。

Abstract: The accuracy, resilience, and affordability of localisation are fundamental
to autonomous robotic inspection within aircraft maintenance and overhaul (MRO)
hangars. Hangars typically feature tall ceilings and are often made of
materials such as metal. Due to its nature, it is considered a GPS-denied
environment, with extensive multipath effects and stringent operational
constraints that collectively create a uniquely challenging environment. This
persistent gap highlights the need for domain-specific comparative studies,
including rigorous cost, accuracy, and integration assessments, to inform a
reliable and scalable deployment of a localisation system in the Smart Hangar.
This paper presents the first techno-economic roadmap that benchmarks motion
capture (MoCap), ultra-wideband (UWB), and a ceiling-mounted camera network
across three operational scenarios: robot localisation, asset tracking, and
surface defect detection within a 40x50 m hangar bay. A dual-layer optimisation
for camera selection and positioning framework is introduced, which couples
market-based camera-lens selection with an optimisation solver, producing
camera layouts that minimise hardware while meeting accuracy targets. The
roadmap equips MRO planners with an actionable method to balance accuracy,
coverage, and budget, demonstrating that an optimised vision architecture has
the potential to unlock robust and cost-effective sensing for next-generation
Smart Hangars.

</details>


### [196] [AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving](https://arxiv.org/abs/2509.20253)
*Jinhao Chai,Anqing Jiang,Hao Jiang,Shiyi Mu,Zichong Gu,Shugong Xu*

Main category: cs.RO

TL;DR: AnchDrive是一种端到端自动驾驶框架，通过混合轨迹锚点和扩散策略高效生成多样化高质量轨迹，在NAVSIM基准测试中表现卓越。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中行为多样性和长尾场景泛化挑战，同时降低传统生成模型的高计算成本。

Method: AnchDrive采用扩散策略，通过混合轨迹锚点（静态通用驾驶先验和动态上下文感知轨迹）初始化规划器，随后通过扩散模型预测轨迹偏移分布进行细粒度优化。

Result: 在NAVSIM基准测试中表现优异，实现了新的最先进水平，并展现出强大的泛化能力。

Conclusion: AnchDrive框架通过结合静态和动态轨迹锚点，有效提升了自动驾驶规划的多样性和质量，在NAVSIM基准测试中达到了新的最先进水平，并展现出强大的泛化能力。

Abstract: End-to-end multi-modal planning has become a transformative paradigm in
autonomous driving, effectively addressing behavioral multi-modality and the
generalization challenge in long-tail scenarios. We propose AnchDrive, a
framework for end-to-end driving that effectively bootstraps a diffusion policy
to mitigate the high computational cost of traditional generative models.
Rather than denoising from pure noise, AnchDrive initializes its planner with a
rich set of hybrid trajectory anchors. These anchors are derived from two
complementary sources: a static vocabulary of general driving priors and a set
of dynamic, context-aware trajectories. The dynamic trajectories are decoded in
real-time by a Transformer that processes dense and sparse perceptual features.
The diffusion model then learns to refine these anchors by predicting a
distribution of trajectory offsets, enabling fine-grained refinement. This
anchor-based bootstrapping design allows for efficient generation of diverse,
high-quality trajectories. Experiments on the NAVSIM benchmark confirm that
AnchDrive sets a new state-of-the-art and shows strong gen?eralizability

</details>


### [197] [HL-IK: A Lightweight Implementation of Human-Like Inverse Kinematics in Humanoid Arms](https://arxiv.org/abs/2509.20263)
*Bingjie Chen,Zihan Wang,Zhe Han,Guoping Pan,Yi Cheng,Houde Liu*

Main category: cs.RO

TL;DR: HL-IK通过学习的肘部先验，在保持末端执行器跟踪的同时，显著提升人形机器人动作的人类相似性，且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 传统逆运动学方法虽能保证机械有效性，但生成的配置缺乏人类动作的自然性。

Method: 利用大规模人类运动数据训练FiSTA网络预测肘部姿态，并将其作为残差项整合到Levenberg-Marquardt优化器中。

Result: 在183k次仿真步骤中，HL-IK将手臂相似性位置和方向误差分别降低了30.6%和35.4%，在最具挑战性的轨迹上分别降低了42.2%和47.4%。硬件远程操作进一步验证了其人类动作模拟的优越性。

Conclusion: HL-IK是一种轻量级逆运动学框架，通过结合学习到的肘部先验知识，在保持末端执行器跟踪的同时，使整个手臂配置更接近人类动作，显著提升了人形机器人的动作自然度。

Abstract: Traditional IK methods for redundant humanoid manipulators emphasize
end-effector (EE) tracking, frequently producing configurations that are valid
mechanically but not human-like. We present Human-Like Inverse Kinematics
(HL-IK), a lightweight IK framework that preserves EE tracking while shaping
whole-arm configurations to appear human-like, without full-body sensing at
runtime. The key idea is a learned elbow prior: using large-scale human motion
data retargeted to the robot, we train a FiLM-modulated spatio-temporal
attention network (FiSTA) to predict the next-step elbow pose from the EE
target and a short history of EE-elbow states.This prediction is incorporated
as a small residual alongside EE and smoothness terms in a standard
Levenberg-Marquardt optimizer, making HL-IK a drop-in addition to numerical IK
stacks. Over 183k simulation steps, HL-IK reduces arm-similarity position and
direction error by 30.6% and 35.4% on average, and by 42.2% and 47.4% on the
most challenging trajectories. Hardware teleoperation on a robot distinct from
simulation further confirms the gains in anthropomorphism. HL-IK is simple to
integrate, adaptable across platforms via our pipeline, and adds minimal
computation, enabling human-like motions for humanoid robots. Project page:
https://hl-ik.github.io/

</details>


### [198] [Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video](https://arxiv.org/abs/2509.20286)
*Georgios Tziafas,Jiayun Zhang,Hamidreza Kasaei*

Main category: cs.RO

TL;DR: PAD框架通过解析、增强和蒸馏步骤，从单一人类视频学习可泛化双手机器人策略，在现实任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉运动策略学习方法需要大量遥操作数据且难以泛化到分布外场景，而基于人类视频或演示增强的方法又存在仿真到现实的差距。PAD旨在通过结合关键点表示和任务规划，从单一人类视频中学习可泛化的策略。

Method: PAD框架包含三个步骤：(a) 解析人类视频为机器人可执行的关键点-动作轨迹，(b) 使用双手机器人任务和运动规划大规模增强演示数据，(c) 将增强的轨迹蒸馏为关键点条件策略。

Result: PAD在六种现实双手机器人任务中表现优于现有基于图像策略和仿真增强的方法，成功率和样本/成本效率更高。

Conclusion: PAD框架通过解析、增强和蒸馏三个步骤，成功实现了从单一人类视频学习可泛化的双手机器人策略，并在多样化的现实任务中展示出优越的性能。

Abstract: Learning visuomotor policies from expert demonstrations is an important
frontier in modern robotics research, however, most popular methods require
copious efforts for collecting teleoperation data and struggle to generalize
out-ofdistribution. Scaling data collection has been explored through
leveraging human videos, as well as demonstration augmentation techniques. The
latter approach typically requires expensive simulation rollouts and trains
policies with synthetic image data, therefore introducing a sim-to-real gap. In
parallel, alternative state representations such as keypoints have shown great
promise for category-level generalization. In this work, we bring these avenues
together in a unified framework: PAD (Parse-AugmentDistill), for learning
generalizable bimanual policies from a single human video. Our method relies on
three steps: (a) parsing a human video demo into a robot-executable
keypoint-action trajectory, (b) employing bimanual task-and-motion-planning to
augment the demonstration at scale without simulators, and (c) distilling the
augmented trajectories into a keypoint-conditioned policy. Empirically, we
showcase that PAD outperforms state-ofthe-art bimanual demonstration
augmentation works relying on image policies with simulation rollouts, both in
terms of success rate and sample/cost efficiency. We deploy our framework in
six diverse real-world bimanual tasks such as pouring drinks, cleaning trash
and opening containers, producing one-shot policies that generalize in unseen
spatial arrangements, object instances and background distractors.
Supplementary material can be found in the project webpage
https://gtziafas.github.io/PAD_project/.

</details>


### [199] [mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies](https://arxiv.org/abs/2509.20297)
*Remo Steiner,Alexander Millane,David Tingdahl,Clemens Volk,Vikram Ramasamy,Xinjie Yao,Peter Du,Soha Pouya,Shiwei Sheng*

Main category: cs.RO

TL;DR: 论文提出mindmap，一种基于语义3D重建的3D扩散策略，有效解决机器人操作任务中空间记忆缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作任务中，空间记忆（记住场景的空间组成）是关键能力，但目前将此类机制融入机器人学习系统仍是一个开放的研究问题。

Method: 采用端到端学习的神经网络控制策略，结合语义3D重建环境技术，构建空间记忆机制。

Result: 模拟实验表明，该方法在解决无记忆机制方法难以完成的任务上表现优异。

Conclusion: 论文提出了一种名为mindmap的3D扩散策略，通过语义3D重建环境生成机器人轨迹，有效解决了无记忆机制的最先进方法难以完成的任务。

Abstract: End-to-end learning of robot control policies, structured as neural networks,
has emerged as a promising approach to robotic manipulation. To complete many
common tasks, relevant objects are required to pass in and out of a robot's
field of view. In these settings, spatial memory - the ability to remember the
spatial composition of the scene - is an important competency. However,
building such mechanisms into robot learning systems remains an open research
problem. We introduce mindmap (Spatial Memory in Deep Feature Maps for 3D
Action Policies), a 3D diffusion policy that generates robot trajectories based
on a semantic 3D reconstruction of the environment. We show in simulation
experiments that our approach is effective at solving tasks where
state-of-the-art approaches without memory mechanisms struggle. We release our
reconstruction system, training code, and evaluation tasks to spur research in
this direction.

</details>


### [200] [VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation](https://arxiv.org/abs/2509.20322)
*Shaofeng Yin,Yanjie Ze,Hong-Xing Yu,C. Karen Liu,Jiajun Wu*

Main category: cs.RO

TL;DR: VisualMimic是一个结合视觉和分层全身控制的框架，实现了人形机器人在多样化任务中的零样本模拟到现实转移，并在户外环境中展现出强大泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人在非结构化环境中依赖外部运动捕捉系统或无法泛化到多样化任务的问题。

Method: VisualMimic结合了从人类运动数据通过师生机制训练的任务无关低级关键点跟踪器，以及从视觉和本体感受输入生成关键点命令的任务特定高级策略。为确保训练稳定性，向低级策略注入噪声并使用人类运动统计数据裁剪高级动作。

Result: VisualMimic实现了从模拟到现实的零样本策略转移，成功完成了多种loco-manipulation任务（如举箱、推、足球运球和踢球），并在户外环境中表现出强大的泛化能力。

Conclusion: VisualMimic框架通过结合任务无关的低级关键点跟踪器和任务特定的高级策略，成功实现了人形机器人在非结构化环境中的视觉模拟到现实的零样本转移，并在多种任务和户外环境中展现出强大的泛化能力。

Abstract: Humanoid loco-manipulation in unstructured environments demands tight
integration of egocentric perception and whole-body control. However, existing
approaches either depend on external motion capture systems or fail to
generalize across diverse tasks. We introduce VisualMimic, a visual sim-to-real
framework that unifies egocentric vision with hierarchical whole-body control
for humanoid robots. VisualMimic combines a task-agnostic low-level keypoint
tracker -- trained from human motion data via a teacher-student scheme -- with
a task-specific high-level policy that generates keypoint commands from visual
and proprioceptive input. To ensure stable training, we inject noise into the
low-level policy and clip high-level actions using human motion statistics.
VisualMimic enables zero-shot transfer of visuomotor policies trained in
simulation to real humanoid robots, accomplishing a wide range of
loco-manipulation tasks such as box lifting, pushing, football dribbling, and
kicking. Beyond controlled laboratory settings, our policies also generalize
robustly to outdoor environments. Videos are available at:
https://visualmimic.github.io .

</details>


### [201] [BBoE: Leveraging Bundle of Edges for Kinodynamic Bidirectional Motion Planning](https://arxiv.org/abs/2509.20333)
*Srikrishna Bangalore Raghu,Alessandro Roncone*

Main category: cs.RO

TL;DR: BBoE 是一种双向运动规划算法，通过预处理和策略优化在障碍物密集环境中快速找到低成本路径。


<details>
  <summary>Details</summary>
Motivation: 解决在障碍物密集环境中快速找到低成本解决方案的挑战。

Method: 结合探索与利用策略，依赖预计算的机器人状态遍历，通过排序和序列化预处理的前向传播来导航障碍物密集空间。

Result: BBoE 在规划时间、解决方案成本和成功率方面优于现有方法。

Conclusion: BBoE 是一种高效的双向运动规划算法，显著减少了规划时间、降低了解决方案成本并提高了成功率。

Abstract: In this work, we introduce BBoE, a bidirectional, kinodynamic, sampling-based
motion planner that consistently and quickly finds low-cost solutions in
environments with varying obstacle clutter. The algorithm combines exploration
and exploitation while relying on precomputed robot state traversals, resulting
in efficient convergence towards the goal. Our key contributions include: i) a
strategy to navigate through obstacle-rich spaces by sorting and sequencing
preprocessed forward propagations; and ii) BBoE, a robust bidirectional
kinodynamic planner that utilizes this strategy to produce fast and feasible
solutions. The proposed framework reduces planning time, diminishes solution
cost and increases success rate in comparison to previous approaches.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [202] [EngravingGNN: A Hybrid Graph Neural Network for End-to-End Piano Score Engraving](https://arxiv.org/abs/2509.19412)
*Emmanouil Karystinaios,Francesco Foscarin,Gerhard Widmer*

Main category: cs.GR

TL;DR: 该论文提出了一种基于多任务图神经网络的自动音乐雕刻方法，能够高效处理多个子任务，并在两个钢琴数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 自动音乐雕刻是符号音乐处理中尚未充分探索的领域，但对于包含人类玩家的应用至关重要。

Method: 采用多任务图神经网络（GNN）联合预测多个子任务，如声部连接、谱表分配、音高拼写等，并通过后处理管道生成可打印的MusicXML/MEI输出。

Result: 在两个不同的钢琴数据集（J-Pop和DCML Romantic）上，统一模型在所有子任务中均表现出良好的准确性，优于现有仅专注于特定子任务的系统。

Conclusion: 该论文提出的统一图神经网络框架在自动音乐雕刻中表现出色，能够高效处理多个子任务，为未来的研究提供了可扩展和有效的解决方案。

Abstract: This paper focuses on automatic music engraving, i.e., the creation of a
humanly-readable musical score from musical content. This step is fundamental
for all applications that include a human player, but it remains a mostly
unexplored topic in symbolic music processing. In this work, we formalize the
problem as a collection of interdependent subtasks, and propose a unified graph
neural network (GNN) framework that targets the case of piano music and
quantized symbolic input. Our method employs a multi-task GNN to jointly
predict voice connections, staff assignments, pitch spelling, key signature,
stem direction, octave shifts, and clef signs. A dedicated postprocessing
pipeline generates print-ready MusicXML/MEI outputs. Comprehensive evaluation
on two diverse piano corpora (J-Pop and DCML Romantic) demonstrates that our
unified model achieves good accuracy across all subtasks, compared to existing
systems that only specialize in specific subtasks. These results indicate that
a shared GNN encoder with lightweight task-specific decoders in a multi-task
setting offers a scalable and effective solution for automatic music engraving.

</details>


### [203] [AJAHR: Amputated Joint Aware 3D Human Mesh Recovery](https://arxiv.org/abs/2509.19939)
*Hyunjin Cho,Giyun Choi,Jongwon Choi*

Main category: cs.GR

TL;DR: 提出AJAHR框架和A3D数据集，针对截肢个体改进3D人体网格恢复，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人体网格恢复方法假设标准人体结构，忽视了如肢体缺失等多样解剖条件，导致在截肢个体应用时存在偏差，且缺乏合适的数据集。

Method: 提出了一种名为AJAHR的自适应姿态估计框架，集成了身体部位截肢分类器，并与网格恢复网络联合训练。同时引入了合成数据集A3D，用于鲁棒训练。

Result: AJAHR在截肢个体上实现了最先进的网格重建效果，同时在非截肢者上保持竞争力。

Conclusion: AJAHR模型在保持对非截肢者竞争力的同时，为截肢个体提供了最先进的3D人体网格重建结果。

Abstract: Existing human mesh recovery methods assume a standard human body structure,
overlooking diverse anatomical conditions such as limb loss. This assumption
introduces bias when applied to individuals with amputations - a limitation
further exacerbated by the scarcity of suitable datasets. To address this gap,
we propose Amputated Joint Aware 3D Human Mesh Recovery (AJAHR), which is an
adaptive pose estimation framework that improves mesh reconstruction for
individuals with limb loss. Our model integrates a body-part amputation
classifier, jointly trained with the mesh recovery network, to detect potential
amputations. We also introduce Amputee 3D (A3D), which is a synthetic dataset
offering a wide range of amputee poses for robust training. While maintaining
competitive performance on non-amputees, our approach achieves state-of-the-art
results for amputated individuals. Additional materials can be found at the
project webpage.

</details>


### [204] [MeshMosaic: Scaling Artist Mesh Generation via Local-to-Global Assembly](https://arxiv.org/abs/2509.19995)
*Rui Xu,Tianyang Xue,Qiujie Dong,Le Wan,Zhe Zhu,Peng Li,Zhiyang Dou,Cheng Lin,Shiqing Xin,Yuan Liu,Wenping Wang,Taku Komura*

Main category: cs.GR

TL;DR: MeshMosaic是一种局部到全局的框架，通过分割和自回归生成补丁，解决了高分辨率网格生成的瓶颈问题，显著提升了细节表现和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于Transformer的方法在长序列瓶颈和有限量化分辨率下的问题，以更忠实地再现精细几何细节和结构化密度模式。

Method: MeshMosaic先将形状分割为补丁，自回归生成每个补丁，并利用共享边界条件增强补丁间的连贯性、对称性和无缝连接。

Result: MeshMosaic能够处理超过10万个三角形，显著优于现有方法（通常仅处理约8000个面），并在几何保真度和用户偏好上表现优异。

Conclusion: MeshMosaic通过局部到全局的框架显著提升了高分辨率网格生成的几何保真度和用户偏好，支持实际应用中的细节表示。

Abstract: Scaling artist-designed meshes to high triangle numbers remains challenging
for autoregressive generative models. Existing transformer-based methods suffer
from long-sequence bottlenecks and limited quantization resolution, primarily
due to the large number of tokens required and constrained quantization
granularity. These issues prevent faithful reproduction of fine geometric
details and structured density patterns. We introduce MeshMosaic, a novel
local-to-global framework for artist mesh generation that scales to over 100K
triangles--substantially surpassing prior methods, which typically handle only
around 8K faces. MeshMosaic first segments shapes into patches, generating each
patch autoregressively and leveraging shared boundary conditions to promote
coherence, symmetry, and seamless connectivity between neighboring regions.
This strategy enhances scalability to high-resolution meshes by quantizing
patches individually, resulting in more symmetrical and organized mesh density
and structure. Extensive experiments across multiple public datasets
demonstrate that MeshMosaic significantly outperforms state-of-the-art methods
in both geometric fidelity and user preference, supporting superior detail
representation and practical mesh generation for real-world applications.

</details>


### [205] [KSDiff: Keyframe-Augmented Speech-Aware Dual-Path Diffusion for Facial Animation](https://arxiv.org/abs/2509.20128)
*Tianle Lyu,Junchuan Zhao,Ye Wang*

Main category: cs.GR

TL;DR: KSDiff通过解缠语音特征和预测关键帧，提升了说话头部生成的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法将语音特征视为单一表示，未能捕捉其在驱动不同面部动作中的细粒度作用，且忽略了建模关键帧的重要性。

Method: 提出了KSDiff框架，包括Dual-Path Speech Encoder（DPSE）解缠表达和头部姿态特征，以及Keyframe Establishment Learning（KEL）模块预测关键帧，最终通过Dual-path Motion生成器合成面部动作。

Result: 在HDTF和VoxCeleb数据集上的实验表明，KSDiff在唇同步准确性和头部姿态自然度方面达到了最先进的性能。

Conclusion: KSDiff通过结合语音解缠和关键帧感知扩散，显著提升了唇同步准确性和头部姿态自然度，为说话头部生成提供了新的解决方案。

Abstract: Audio-driven facial animation has made significant progress in multimedia
applications, with diffusion models showing strong potential for talking-face
synthesis. However, most existing works treat speech features as a monolithic
representation and fail to capture their fine-grained roles in driving
different facial motions, while also overlooking the importance of modeling
keyframes with intense dynamics. To address these limitations, we propose
KSDiff, a Keyframe-Augmented Speech-Aware Dual-Path Diffusion framework.
Specifically, the raw audio and transcript are processed by a Dual-Path Speech
Encoder (DPSE) to disentangle expression-related and head-pose-related
features, while an autoregressive Keyframe Establishment Learning (KEL) module
predicts the most salient motion frames. These components are integrated into a
Dual-path Motion generator to synthesize coherent and realistic facial motions.
Extensive experiments on HDTF and VoxCeleb demonstrate that KSDiff achieves
state-of-the-art performance, with improvements in both lip synchronization
accuracy and head-pose naturalness. Our results highlight the effectiveness of
combining speech disentanglement with keyframe-aware diffusion for talking-head
generation.

</details>


### [206] [LidarScout: Direct Out-of-Core Rendering of Massive Point Clouds](https://arxiv.org/abs/2509.20198)
*Philipp Erler,Lukas Herzberger,Michael Wimmer,Markus Schütz*

Main category: cs.GR

TL;DR: 无需预处理，即时可视化数百亿点国家规模扫描数据的方法。


<details>
  <summary>Details</summary>
Motivation: 大规模地形扫描数据通常需要数小时至数天的预处理才能实现实时查看，这限制了其应用效率。本文旨在解决这一瓶颈。

Method: 该方法首先加载稀疏子样本点并初始化整个点云的概览，随后进行表面重建以生成更高质量的无孔高度图。用户导航时，优先处理视点区域的高度图构建。当用户放大时，加载该区域的全分辨率点云数据并更新高度图纹理。

Result: 该方法能够即时可视化包含数百亿点的国家规模扫描数据，且无需预处理和额外磁盘空间。

Conclusion: 本文提出了一种无需预处理且不占用额外磁盘空间的大规模点云数据集直接外核渲染方法，能够即时可视化包含数百亿点的国家规模扫描数据。

Abstract: Large-scale terrain scans are the basis for many important tasks, such as
topographic mapping, forestry, agriculture, and infrastructure planning. The
resulting point cloud data sets are so massive in size that even basic tasks
like viewing take hours to days of pre-processing in order to create
level-of-detail structures that allow inspecting the data set in their entirety
in real time. In this paper, we propose a method that is capable of instantly
visualizing massive country-sized scans with hundreds of billions of points.
Upon opening the data set, we first load a sparse subsample of points and
initialize an overview of the entire point cloud, immediately followed by a
surface reconstruction process to generate higher-quality, hole-free
heightmaps. As users start navigating towards a region of interest, we continue
to prioritize the heightmap construction process to the user's viewpoint. Once
a user zooms in closely, we load the full-resolution point cloud data for that
region and update the corresponding height map textures with the
full-resolution data. As users navigate elsewhere, full-resolution point data
that is no longer needed is unloaded, but the updated heightmap textures are
retained as a form of medium level of detail. Overall, our method constitutes a
form of direct out-of-core rendering for massive point cloud data sets
(terabytes, compressed) that requires no preprocessing and no additional disk
space. Source code, executable, pre-trained model, and dataset are available
at: https://github.com/cg-tuwien/lidarscout

</details>
