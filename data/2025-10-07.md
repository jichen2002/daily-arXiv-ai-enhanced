<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 52]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AI](#cs.AI) [Total: 89]
- [cs.RO](#cs.RO) [Total: 55]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.OS](#cs.OS) [Total: 2]
- [cs.GR](#cs.GR) [Total: 14]
- [cs.NI](#cs.NI) [Total: 15]
- [cs.SE](#cs.SE) [Total: 46]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications](https://arxiv.org/abs/2510.03915)
*Sagar Bharadwaj,Harrison Williams,Luke Wang,Michael Liang,Tao Jin,Srinivasan Seshan,Anthony Rowe*

Main category: cs.CV

TL;DR: OpenFLAME是一个联邦化的VPS后端，解决了集中式VPS在私有室内空间覆盖和隐私保护方面的不足，支持独立组织维护各自的VPS服务。


<details>
  <summary>Details</summary>
Motivation: 集中式VPS解决方案无法满足未来AR应用的需求，尤其是在私有室内空间的覆盖、隐私保护以及3D扫描的更新和维护方面存在瓶颈。

Method: 论文提出了OpenFLAME，一个联邦化的VPS后端，允许独立组织为其自有空间进行3D扫描和维护独立的VPS服务。通过联邦图像定位的概念，解决了跨空间定位结果的一致性、VPS服务质量控制等挑战。

Result: OpenFLAME实现了对私有室内空间的可控访问、VPS后端的分布式维护，并促进了更大范围的覆盖。

Conclusion: OpenFLAME提出了一种联邦化的视觉定位系统（VPS）后端，解决了集中式VPS在覆盖私有室内空间、隐私保护和数据维护方面的不足，为未来AR应用提供了可扩展的解决方案。

Abstract: World-scale augmented reality (AR) applications need a ubiquitous 6DoF
localization backend to anchor content to the real world consistently across
devices. Large organizations such as Google and Niantic are 3D scanning outdoor
public spaces in order to build their own Visual Positioning Systems (VPS).
These centralized VPS solutions fail to meet the needs of many future AR
applications -- they do not cover private indoor spaces because of privacy
concerns, regulations, and the labor bottleneck of updating and maintaining 3D
scans. In this paper, we present OpenFLAME, a federated VPS backend that allows
independent organizations to 3D scan and maintain a separate VPS service for
their own spaces. This enables access control of indoor 3D scans, distributed
maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS
services introduces several unique challenges -- coherency of localization
results across spaces, quality control of VPS services, selection of the right
VPS service for a location, and many others. We introduce the concept of
federated image-based localization and provide reference solutions for managing
and merging data across maps without sharing private data.

</details>


### [2] [Textured Gaussians for Enhanced 3D Scene Appearance Modeling](https://arxiv.org/abs/2411.18625)
*Brian Chao,Hung-Yu Tseng,Lorenzo Porzi,Chen Gao,Tuotuo Li,Qinbo Li,Ayush Saraf,Jia-Bin Huang,Johannes Kopf,Gordon Wetzstein,Changil Kim*

Main category: cs.CV

TL;DR: 通过集成纹理和透明度映射技术，3D高斯泼溅（3DGS）的单个高斯基元表达能力显著提升，实现了更丰富的纹理和几何结构表示，实验验证了其优越的图像质量和效率。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3DGS）因其高质量结果和快速训练渲染时间成为先进的3D重建和渲染技术，但其单个高斯基元的表达能力有限，仅能表示单一颜色和简单椭球体，限制了纹理和几何细节的表现。

Method: 提出了一种新的广义高斯外观表示方法，通过为每个高斯基元添加alpha（A）、RGB或RGBA纹理映射，以模拟每个高斯范围内空间变化的颜色和透明度。这种方法使得单个高斯能够表示更复杂的纹理和几何结构，而不仅仅是单一颜色和简单椭球体。

Result: 实验结果表明，仅使用alpha纹理映射即可显著提升高斯的表达能力，而结合RGB纹理映射则达到最高表达能力。该方法在多种基准数据集和自定义捕捉上均表现出优于现有方法的图像质量，同时使用相似或更少的高斯数量。

Conclusion: 通过将纹理和透明度映射技术集成到3D高斯泼溅（3DGS）中，提出的广义高斯外观表示显著提升了单个高斯基元的表达能力，实现了更丰富的纹理模式和几何结构表示。实验验证了该方法在多种基准数据集上的优越性，图像质量提升的同时保持了高斯数量的效率。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D
reconstruction and rendering technique due to its high-quality results and fast
training and rendering time. However, pixels covered by the same Gaussian are
always shaded in the same color up to a Gaussian falloff scaling factor.
Furthermore, the finest geometric detail any individual Gaussian can represent
is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity
of individual Gaussian primitives. To address these issues, we draw inspiration
from texture and alpha mapping in traditional graphics and integrate it with
3DGS. Specifically, we propose a new generalized Gaussian appearance
representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture
maps to model spatially varying color and opacity across the extent of each
Gaussian. As such, each Gaussian can represent a richer set of texture patterns
and geometric structures, instead of just a single color and ellipsoid as in
naive Gaussian Splatting. Surprisingly, we found that the expressivity of
Gaussians can be greatly improved by using alpha-only texture maps, and further
augmenting Gaussians with RGB texture maps achieves the highest expressivity.
We validate our method on a wide variety of standard benchmark datasets and our
own custom captures at both the object and scene levels. We demonstrate image
quality improvements over existing methods while using a similar or lower
number of Gaussians.

</details>


### [3] [Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms](https://arxiv.org/abs/2510.03501)
*Lyes Saad Saoud,Loic Lesobre,Enrico Sorato,Irfan Hussain*

Main category: cs.CV

TL;DR: 提出移动优化的YOLOv10+MobileSAM并行框架，高效实现野生动物实时检测与分割，性能优异且开源。


<details>
  <summary>Details</summary>
Motivation: 野生动物保护需要非侵入性监测，但现有技术因计算资源有限和物种隐蔽性面临挑战。

Method: 结合Threading Detection Model (TDM)并行化YOLOv10检测和MobileSAM分割，优化了实时性能。

Result: 在Houbara Bustard数据集上，模型表现优异（mAP50: 0.9627, mAP75: 0.7731, mAP95: 0.7178, MobileSAM mIoU: 0.7421），YOLOv10每帧处理时间为43.7毫秒，满足实时需求。

Conclusion: 本研究提出了一种移动优化的两阶段深度学习框架，成功实现了自然环境中动物的实时检测和分割，特别适用于濒危物种如Houbara Bustard的保护监测。

Abstract: Real-time animal detection and segmentation in natural environments are vital
for wildlife conservation, enabling non-invasive monitoring through remote
camera streams. However, these tasks remain challenging due to limited
computational resources and the cryptic appearance of many species. We propose
a mobile-optimized two-stage deep learning framework that integrates a
Threading Detection Model (TDM) to parallelize YOLOv10-based detection and
MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach
improves real-time performance by reducing latency through threading. YOLOv10
handles detection while MobileSAM performs lightweight segmentation, both
executed concurrently for efficient resource use. On the cryptic Houbara
Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627,
mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10
operates at 43.7 ms per frame, confirming real-time readiness. We introduce a
curated Houbara dataset of 40,000 annotated images to support model training
and evaluation across diverse conditions. The code and dataset used in this
study are publicly available on GitHub at
https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos
and additional resources, visit
https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.

</details>


### [4] [SketchPlan: Diffusion Based Drone Planning From Human Sketches](https://arxiv.org/abs/2510.03545)
*Sixten Norelius,Aaron O. Feldman,Mac Schwager*

Main category: cs.CV

TL;DR: SketchPlan是一种基于扩散模型的无人机路径规划方法，通过手绘草图生成3D飞行路径，在模拟和真实环境中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决无人机在未知环境中生成安全飞行路径的挑战，研究提出了基于手绘草图的路径规划方法。

Method: SketchPlan由SketchAdapter和DiffPath两部分组成：前者学习将人类草图映射到2D路径，后者通过扩散模型从2D投影和深度图像推断3D轨迹。

Result: 在真实世界测试中，SketchPlan在低/中障碍物环境中成功率为100%，在高障碍物环境中为40%，优于其他方法20-60%。

Conclusion: SketchPlan通过结合人类手绘草图与深度图像，成功实现了无人机3D飞行路径的生成，并在模拟和真实环境中表现出色。

Abstract: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

</details>


### [5] [LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization](https://arxiv.org/abs/2510.03827)
*Xueyang Zhou,Yangming Xu,Guiyao Tie,Yongchao Chen,Guowen Zhang,Duanfeng Chu,Pan Zhou,Lichao Sun*

Main category: cs.CV

TL;DR: LIBERO-PRO通过扰动评估VLA模型，揭示其泛化能力差，呼吁改进评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有LIBERO基准的训练和评估设置存在问题，导致性能估计虚高，无法公平比较模型。

Method: 引入LIBERO-PRO基准，通过四个维度的扰动（操纵对象、初始状态、任务指令和环境）系统评估模型性能。

Result: 现有模型在标准LIBERO评估中准确率超过90%，但在广义设置下崩溃至0.0%，暴露了模型对训练集的机械记忆依赖。

Conclusion: LIBERO-PRO揭示了现有VLA模型在泛化能力上的严重缺陷，呼吁社区采用更鲁棒的评估方法。

Abstract: LIBERO has emerged as a widely adopted benchmark for evaluating
Vision-Language-Action (VLA) models; however, its current training and
evaluation settings are problematic, often leading to inflated performance
estimates and preventing fair model comparison. To address these issues, we
introduce LIBERO-PRO, an extended LIBERO benchmark that systematically
evaluates model performance under reasonable perturbations across four
dimensions: manipulated objects, initial states, task instructions, and
environments. Experimental results reveal that, although existing models
achieve over 90% accuracy under the standard LIBERO evaluation, their
performance collapses to 0.0% under our generalized setting. Crucially, this
discrepancy exposes the models' reliance on rote memorization of action
sequences and environment layouts from the training set, rather than genuine
task understanding or environmental perception. For instance, models persist in
executing grasping actions when the target object is replaced with irrelevant
items, and their outputs remain unchanged even when given corrupted
instructions or even messy tokens. These findings expose the severe flaws in
current evaluation practices, and we call on the community to abandon
misleading methodologies in favor of robust assessments of model generalization
and comprehension. Our code is available at:
https://github.com/Zxy-MLlab/LIBERO-PRO.

</details>


### [6] [Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert](https://arxiv.org/abs/2510.03896)
*Mingyu Liu,Zheng Huang,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Yating Wang,Haoyi Zhu,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 论文提出了一种新框架，利用稀疏3D轨迹连接VLM和动作模块，解决了现有VLA模型的泛化问题，并引入了新的训练范式。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在将视觉语言模型的规划和推理能力转化为物理世界动作时，面临数据稀缺、跨任务训练不可行等问题，且双系统方法存在语义模糊性。

Method: 论文采用了基于通用化动作专家的框架，结合了VLM的视觉理解和规划能力与动作专家的细粒度泛化能力，并引入了‘动作预训练，点云微调’的新范式。

Result: 提出的方法通过稀疏3D轨迹作为中间表示，实现了VLM与动作模块的有效连接，并通过新训练范式提升了训练效率和泛化能力。

Conclusion: 该论文提出了一种新的框架，通过使用稀疏的3D轨迹作为中间表示，将VLM的高级规划能力与低级的物理动作模块有效连接，解决了现有VLA模型在物理世界中的泛化问题。

Abstract: Although Vision-Language Models (VLM) have demonstrated impressive planning
and reasoning capabilities, translating these abilities into the physical world
introduces significant challenges. Conventional Vision-Language-Action (VLA)
models, which integrate reasoning and action into a monolithic architecture,
generalize poorly because they are constrained by scarce, narrow-domain data.
While recent dual-system approaches attempt to decouple "thinking" from
"acting", they are often constrained by semantic ambiguities within the action
module. This ambiguity makes large-scale, cross-task training infeasible.
Consequently, these systems typically necessitate fine-tuning on newly
collected data when deployed to novel environments, and the cooperation
mechanism between the two systems remains ill-defined. To address these
limitations, we introduce, for the first time, a framework centered around a
generalizable action expert. Our approach utilizes sparse 3D trajectories as an
intermediate representation, effectively bridging the high-level planning
capabilities of the VLM with the low-level physical action module. During the
planning phase, the VLM is only required to generate coarse 3D waypoints. These
waypoints are then processed by our generalizable action expert, which refines
them into dense, executable action sequences by sampling real-time point cloud
observations of the environment. To promote training efficiency and robust
generalization, we introduce a novel "Action Pre-training, Pointcloud
Fine-tuning" paradigm. Our method combines the broad generalization
capabilities of VLMs in visual understanding and planning with the
fine-grained, action-level generalization of action expert.

</details>


### [7] [RAP: 3D Rasterization Augmented End-to-End Planning](https://arxiv.org/abs/2510.04333)
*Lan Feng,Yang Gao,Eloi Zablocki,Quanyi Li,Wuyang Li,Sichao Liu,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: RAP通过轻量级3D栅格化和特征对齐，显著提升端到端驾驶规划的鲁棒性和泛化能力，无需高成本渲染。


<details>
  <summary>Details</summary>
Motivation: 现有基于专家演示的模仿学习缺乏恢复数据，且传统神经渲染或游戏引擎方法成本高、速度慢，无法满足训练需求。

Method: 提出3D Rasterization替代高成本渲染，并结合Raster-to-Real特征对齐方法，实现合成视图到真实场景的有效迁移。

Result: RAP在NAVSIM v1/v2、Waymo Open Dataset等四大基准测试中排名第一，验证了其高效性和实用性。

Conclusion: 3D Rasterization与Raster-to-Real特征对齐结合（RAP）为端到端驾驶规划提供了高效的数据增强方案，显著提升闭环鲁棒性和长尾泛化能力。

Abstract: Imitation learning for end-to-end driving trains policies only on expert
demonstrations. Once deployed in a closed loop, such policies lack recovery
data: small mistakes cannot be corrected and quickly compound into failures. A
promising direction is to generate alternative viewpoints and trajectories
beyond the logged path. Prior work explores photorealistic digital twins via
neural rendering or game engines, but these methods are prohibitively slow and
costly, and thus mainly used for evaluation. In this work, we argue that
photorealism is unnecessary for training end-to-end planners. What matters is
semantic fidelity and scalability: driving depends on geometry and dynamics,
not textures or lighting. Motivated by this, we propose 3D Rasterization, which
replaces costly rendering with lightweight rasterization of annotated
primitives, enabling augmentations such as counterfactual recovery maneuvers
and cross-agent view synthesis. To transfer these synthetic views effectively
to real-world deployment, we introduce a Raster-to-Real feature-space alignment
that bridges the sim-to-real gap. Together, these components form Rasterization
Augmented Planning (RAP), a scalable data augmentation pipeline for planning.
RAP achieves state-of-the-art closed-loop robustness and long-tail
generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo
Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that
lightweight rasterization with feature alignment suffices to scale E2E
training, offering a practical alternative to photorealistic rendering. Project
page: https://alan-lanfeng.github.io/RAP/.

</details>


### [8] [A Modular Conditional Diffusion Framework for Image Reconstruction](https://arxiv.org/abs/2411.05993)
*Magauiya Zhussip,Iaroslav Koshelev,Stamatis Lefkimmiatis*

Main category: cs.CV

TL;DR: DP-IR框架通过模块化设计和高效采样策略，显著降低了DPMs在图像恢复任务中的计算成本，同时在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于DPMs的图像恢复解决方案存在任务特定性和高计算成本问题，限制了其广泛应用。本文旨在解决这些问题，推动DPMs在实际图像恢复应用中的成功采用。

Method: 提出了一个模块化的扩散概率图像恢复框架（DP-IR），结合预训练的最先进图像恢复网络和生成DPMs，仅需额外训练一个小型模块（0.7M参数）。框架支持采样策略，减少至少四倍的神经网络评估次数，且可与DDIM等现有加速技术结合使用。

Result: 在多个图像恢复任务（如burst JDD-SR、动态场景去模糊和超分辨率）的基准测试中，DP-IR在感知质量上优于现有方法，同时在保真度指标上保持竞争力。

Conclusion: 本文提出的DP-IR框架成功解决了DPMs在图像恢复任务中的计算成本高和任务特定性问题，通过模块化设计和高效的采样策略，显著提升了模型的实用性和效率。

Abstract: Diffusion Probabilistic Models (DPMs) have been recently utilized to deal
with various blind image restoration (IR) tasks, where they have demonstrated
outstanding performance in terms of perceptual quality. However, the
task-specific nature of existing solutions and the excessive computational
costs related to their training, make such models impractical and challenging
to use for different IR tasks than those that were initially trained for. This
hinders their wider adoption, especially by those who lack access to powerful
computational resources and vast amount of training data. In this work we aim
to address the above issues and enable the successful adoption of DPMs in
practical IR-related applications. Towards this goal, we propose a modular
diffusion probabilistic IR framework (DP-IR), which allows us to combine the
performance benefits of existing pre-trained state-of-the-art IR networks and
generative DPMs, while it requires only the additional training of a relatively
small module (0.7M params) related to the particular IR task of interest.
Moreover, the architecture of the proposed framework allows for a sampling
strategy that leads to at least four times reduction of neural function
evaluations without suffering any performance loss, while it can also be
combined with existing acceleration techniques such as DDIM. We evaluate our
model on four benchmarks for the tasks of burst JDD-SR, dynamic scene
deblurring, and super-resolution. Our method outperforms existing approaches in
terms of perceptual quality while it retains a competitive performance with
respect to fidelity metrics.

</details>


### [9] [Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes](https://arxiv.org/abs/2510.03297)
*Akshar Gothi*

Main category: cs.CV

TL;DR: 对比CNN和ViT在标签不平衡与平衡场景下的表现，发现平衡时两者性能接近，但CNN效率更高。


<details>
  <summary>Details</summary>
Motivation: 比较卷积神经网络（CNN）和视觉Transformer（ViT）在不同标签分布下的性能差异。

Method: 使用EfficientNet-B0和ViT-Base在SpaceNet数据集上进行对比实验，包括不平衡和平衡标签分布两种场景，匹配预处理、轻量级数据增强和40轮训练预算。

Result: 在不平衡分布下，EfficientNet-B0达到93%测试准确率，ViT-Base与之相当但参数更多；平衡分布下两者均表现优异，EfficientNet-B0达到99%。

Conclusion: 在平衡标签分布下，CNN和ViT表现均强劲，但CNN在效率上仍具优势。

Abstract: We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.

</details>


### [10] [A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety](https://arxiv.org/abs/2510.03314)
*Shucheng Zhang,Yan Shi,Bingzhang Wang,Yuang Zhang,Muhammad Monjurul Karim,Kehua Chen,Chenxi Liu,Mehrdad Nasri,Yinhai Wang*

Main category: cs.CV

TL;DR: 本文综述了基于摄像头的AI感知系统在VRU安全领域的最新进展，重点讨论了四项核心任务和未来研究的四大挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的基于基础设施的措施在动态城市环境中往往不足，而AI在视觉感知和推理方面的进步为VRU的主动和情境感知保护提供了新机会。

Method: 系统性地研究了检测与分类、跟踪与重识别、轨迹预测以及意图识别与预测这四项核心任务。

Result: 本文综述了过去五年内的进展和新兴研究趋势，强调了四项核心任务在AI赋能的VRU保护解决方案中的重要性。

Conclusion: 本文综述了基于摄像头的AI感知系统在VRU安全领域的最新进展，并指出了数据、模型和部署方面的四大开放挑战，为下一代感知系统的开发提供了基础参考。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, remains a critical global challenge, as conventional
infrastructure-based measures often prove inadequate in dynamic urban
environments. Recent advances in artificial intelligence (AI), particularly in
visual perception and reasoning, open new opportunities for proactive and
context-aware VRU protection. However, existing surveys on AI applications for
VRUs predominantly focus on detection, offering limited coverage of other
vision-based tasks that are essential for comprehensive VRU understanding and
protection. This paper presents a state-of-the-art review of recent progress in
camera-based AI sensing systems for VRU safety, with an emphasis on
developments from the past five years and emerging research trends. We
systematically examine four core tasks, namely detection and classification,
tracking and reidentification, trajectory prediction, and intent recognition
and prediction, which together form the backbone of AI-empowered proactive
solutions for VRU protection in intelligent transportation systems. To guide
future research, we highlight four major open challenges from the perspectives
of data, model, and deployment. By linking advances in visual AI with practical
considerations for real-world implementation, this survey aims to provide a
foundational reference for the development of next-generation sensing systems
to enhance VRU safety.

</details>


### [11] [The View From Space: Navigating Instrumentation Differences with EOFMs](https://arxiv.org/abs/2510.03316)
*Ryan P. Demilt,Nicholas LaHaye,Karis Tenneson*

Main category: cs.CV

TL;DR: EOFMs的表示空间对传感器架构敏感，揭示了当前设计的问题，为未来改进提供了方向。


<details>
  <summary>Details</summary>
Motivation: 研究EOFMs在处理多模态遥感数据时的局限性，特别是传感器架构对模型内部表示的影响。

Method: 通过分析预训练模型的输出作为'嵌入'，研究不同传感器架构对EOFMs内部表示的影响。

Result: EOFMs的表示空间对传感器架构高度敏感，这揭示了当前EOFMs设计的潜在问题。

Conclusion: EOFMs的表示空间对传感器架构高度敏感，理解这种差异为当前EOFMs设计的陷阱提供了重要视角，并为模型开发者、用户和以稳健遥感科学为指导的社区指明了前进方向。

Abstract: Earth Observation Foundation Models (EOFMs) have exploded in prevalence as
tools for processing the massive volumes of remotely sensed and other earth
observation data, and for delivering impact on the many essential earth
monitoring tasks. An emerging trend posits using the outputs of pre-trained
models as 'embeddings' which summarize high dimensional data to be used for
generic tasks such as similarity search and content-specific queries. However,
most EOFM models are trained only on single modalities of data and then applied
or benchmarked by matching bands across different modalities. It is not clear
from existing work what impact diverse sensor architectures have on the
internal representations of the present suite of EOFMs. We show in this work
that the representation space of EOFMs is highly sensitive to sensor
architecture and that understanding this difference gives a vital perspective
on the pitfalls of current EOFM design and signals for how to move forward as
model developers, users, and a community guided by robust remote-sensing
science.

</details>


### [12] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: 提出一种逼真的扰动解释技术，用于提高生态监测AI的透明度和可信度，支持专家验证。


<details>
  <summary>Details</summary>
Motivation: 解决生态监测中视觉模型预测不透明的问题，以增强信任和实际应用。

Method: 提出了一种基于修复引导的扰动解释技术，利用Segment-Anything-Model优化的掩码进行对象移除/替换和背景替换。

Result: 生成的解释能够定位诊断结构，避免传统扰动中的删除伪影，并提供与领域相关的见解。

Conclusion: 该方法通过生成逼真的、局部编辑的解释，提高了生态监测中AI模型的透明度和可信度，支持专家验证和更可靠的AI部署。

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque
predictions limit trust and field adoption. We present an inpainting-guided,
perturbation-based explanation technique that produces photorealistic,
mask-localized edits that preserve scene context. Unlike masking or blurring,
these edits stay in-distribution and reveal which fine-grained morphological
cues drive predictions in tasks such as species recognition and trait
attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for
harbor seal detection in Glacier Bay drone imagery, using
Segment-Anything-Model-refined masks to support two interventions: (i) object
removal/replacement (e.g., replacing seals with plausible ice/water or boats)
and (ii) background replacement with original animals composited onto new
scenes. Explanations are assessed by re-scoring perturbed images (flip rate,
confidence drop) and by expert review for ecological plausibility and
interpretability. The resulting explanations localize diagnostic structures,
avoid deletion artifacts common to traditional perturbations, and yield
domain-relevant insights that support expert validation and more trustworthy
deployment of AI in ecology.

</details>


### [13] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: 本文提出一种利用侧信息引导扩散模型采样的新算法，显著提升了逆问题重建质量，避免了梯度引导的伪影问题，并在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽略侧信息，而这些信息在严重不适定情况下能显著提升重建质量。本文旨在通过引入侧信息引导的采样过程来改进现有扩散模型的重建性能。

Method: 提出了一种推理时间搜索算法，该算法在采样过程中利用侧信息来平衡探索与利用，从而优化重建结果。

Result: 通过在不同逆问题（如盒内修复、超分辨率及多种去模糊任务）上的实验，证明了该方法在定性和定量性能上的显著提升，且优于基于梯度引导的基线方法。

Conclusion: 本文提出了一种新颖的推理时间搜索算法，通过利用侧信息来优化扩散模型的采样过程，从而在多种逆问题中显著提高了重建质量。该方法不仅避免了基于梯度引导的常见伪影问题，还能无缝集成到现有的扩散重建流程中。

Abstract: Diffusion models have emerged as powerful priors for solving inverse
problems. However, existing approaches typically overlook side information that
could significantly improve reconstruction quality, especially in severely
ill-posed settings. In this work, we propose a novel inference-time search
algorithm that guides the sampling process using the side information in a
manner that balances exploration and exploitation. This enables more accurate
and reliable reconstructions, providing an alternative to the gradient-based
guidance that is prone to reward-hacking artifacts. Our approach can be
seamlessly integrated into a wide range of existing diffusion-based image
reconstruction pipelines. Through extensive experiments on a number of inverse
problems, such as box inpainting, super-resolution, and various deblurring
tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that
our approach consistently improves the qualitative and quantitative performance
of diffusion-based image reconstruction algorithms. We also show the superior
performance of our approach with respect to other baselines, including reward
gradient-based guidance algorithms. The code is available at
\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this
repository}.

</details>


### [14] [Provenance Networks: End-to-End Exemplar-Based Explainability](https://arxiv.org/abs/2510.03361)
*Ali Kayyam,Anusha Madan Gopal,M. Anthony Lewis*

Main category: cs.CV

TL;DR: Provenance networks embed explainability into neural models by linking predictions to training data, offering transparency and robustness despite higher computational costs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of conventional post-hoc explainability methods by embedding interpretability into the neural model architecture itself, improving transparency and trustworthiness.

Method: Provenance networks learn to link predictions directly to supporting training examples, functioning similarly to a learned KNN. This method jointly optimizes the primary task and explainability objectives.

Result: The approach facilitates systematic investigations of memorization vs. generalization, aids in detecting mislabeled data, enhances resilience to perturbations, and supports identifying similar inputs contributing to predictions.

Conclusion: Provenance networks provide a novel approach to integrating explainability directly into neural models, addressing key challenges in deep learning such as model opaqueness and hallucination. While they introduce additional computational costs, they enhance transparency, robustness, and trustworthiness.

Abstract: We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.

</details>


### [15] [Unified Unsupervised Anomaly Detection via Matching Cost Filtering](https://arxiv.org/abs/2510.03363)
*Zhe Zhang,Mingxiu Cai,Gaochang Wu,Jing Zhang,Lingqiao Liu,Dacheng Tao,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: UCF是一种通用后处理框架，通过多模态匹配和可学习过滤模块优化异常检测，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测方法在匹配噪声处理和多模态知识共享方面存在不足，论文旨在通过统一框架解决这些问题。

Method: 论文提出了一种通用的后处理细化框架UCF，通过构建异常成本体积并利用多层级注意力引导的可学习过滤模块来优化异常检测结果。

Result: 实验结果表明，UCF在单模态（RGB）和多模态（RGB--3D、RGB--Text）场景下均显著提升了异常检测性能。

Conclusion: 论文提出了Unified Cost Filtering (UCF)框架，通过多模态匹配和可学习的过滤模块，有效减少了匹配噪声并提升了异常检测的准确性。在22个不同基准测试中，UCF均取得了最先进的性能。

Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level
anomalies using only normal training data, with wide applications such as
industrial inspection and medical analysis, where anomalies are scarce due to
privacy concerns and cold-start constraints. Existing methods, whether
reconstruction-based (restoring normal counterparts) or embedding-based
(pretrained representations), fundamentally conduct image- or feature-level
matching to generate anomaly maps. Nonetheless, matching noise has been largely
overlooked, limiting their detection ability. Beyond earlier focus on unimodal
RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D
and RGB--Text, enabled by point cloud sensing and vision--language models.
Despite shared challenges, these lines remain largely isolated, hindering a
comprehensive understanding and knowledge transfer. In this paper, we advocate
unified UAD for both unimodal and multimodal settings in the matching
perspective. Under this insight, we present Unified Cost Filtering (UCF), a
generic post-hoc refinement framework for refining anomaly cost volume of any
UAD model. The cost volume is constructed by matching a test sample against
normal samples from the same or different modalities, followed by a learnable
filtering module with multi-layer attention guidance from the test sample,
mitigating matching noise and highlighting subtle anomalies. Comprehensive
experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in
enhancing a variety of UAD methods, consistently achieving new state-of-the-art
results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD
scenarios. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [16] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: SpatialViLT通过整合空间特征增强多模态嵌入，其变体在VSR数据集上表现优异，提升了AI的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在3D场景和复杂物体配置的空间推理方面仍存在挑战，需要增强其空间理解能力。

Method: 提出SpatialViLT和MaskedSpatialViLT两种变体，通过多任务学习框架整合深度图、3D坐标和边缘图等空间特征，并提出了结合两者的SpatialEnsemble方法。

Result: 在视觉空间推理（VSR）数据集上，模型在方向性、拓扑和邻近关系等空间推理类别中表现出色，达到了最先进的准确率。

Conclusion: 该研究通过引入SpatialViLT及其变体，显著提升了AI系统在空间推理方面的能力，为高级多模态理解和现实应用奠定了基础。

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still
face challenges in spatial reasoning for 3D scenes and complex object
configurations. To address this, we introduce SpatialViLT, an enhanced VLM that
integrates spatial features like depth maps, 3D coordinates, and edge maps
through a multi-task learning framework. This approach enriches multimodal
embeddings with spatial understanding. We propose two variants: SpatialViLT and
MaskedSpatialViLT, focusing on full and masked object regions, respectively.
Additionally, SpatialEnsemble combines both approaches, achieving
state-of-the-art accuracy. Our models excel in spatial reasoning categories
such as directional, topological, and proximity relations, as demonstrated on
the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a
significant step in enhancing the spatial intelligence of AI systems, crucial
for advanced multimodal understanding and real-world applications.

</details>


### [17] [DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2510.03483)
*Numan Saeed,Tausifa Jan Saleem,Fadillah Maani,Muhammad Ridzuan,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: DuPLUS是一种新型医学影像分析框架，通过视觉-语言分层提示和双提示机制，实现了多模态通用性和高效预后预测。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像深度学习模型缺乏通用性和预后能力，而现有“通用”方法存在条件简单化和医学语义理解不足的问题。

Method: DuPLUS引入了一种新颖的视觉-语言框架，利用分层语义提示进行细粒度控制，并采用独特的双提示机制驱动的分层文本控制架构。

Result: DuPLUS在三种成像模态和十个不同解剖学医学数据集上表现出色，优于8/10数据集的最先进任务特定和通用模型，并在头颈癌数据集上实现了0.69的CI。

Conclusion: DuPLUS被证明是一种多功能且临床相关的医学图像分析解决方案，能够快速适应新任务和模态，并通过参数高效微调实现对不同中心数据的快速适应。

Abstract: Deep learning for medical imaging is hampered by task-specific models that
lack generalizability and prognostic capabilities, while existing 'universal'
approaches suffer from simplistic conditioning and poor medical semantic
understanding. To address these limitations, we introduce DuPLUS, a deep
learning framework for efficient multi-modal medical image analysis. DuPLUS
introduces a novel vision-language framework that leverages hierarchical
semantic prompts for fine-grained control over the analysis task, a capability
absent in prior universal models. To enable extensibility to other medical
tasks, it includes a hierarchical, text-controlled architecture driven by a
unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize
across three imaging modalities, ten different anatomically various medical
datasets, encompassing more than 30 organs and tumor types. It outperforms the
state-of-the-art task specific and universal models on 8 out of 10 datasets. We
demonstrate extensibility of its text-controlled architecture by seamless
integration of electronic health record (EHR) data for prognosis prediction,
and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)
of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks
and modalities from varying centers, establishing DuPLUS as a versatile and
clinically relevant solution for medical image analysis. The code for this work
is made available at: https://anonymous.4open.science/r/DuPLUS-6C52

</details>


### [18] [Platonic Transformers: A Solid Choice For Equivariance](https://arxiv.org/abs/2510.03511)
*Mohammad Mohaiminul Islam,Rishabh Anand,David R. Wessels,Friso de Kruiff,Thijs P. Kuipers,Rex Ying,Clara I. Sánchez,Sharvaree Vadgama,Georg Bökman,Erik J. Bekkers*

Main category: cs.CV

TL;DR: Platonic Transformer通过柏拉图立体对称群的参考框架，实现了几何对称性的等变性，同时保持了标准Transformer的效率和灵活性，在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer缺乏几何对称性归纳偏置的问题，同时不牺牲其效率和灵活性。

Method: 通过定义相对于柏拉图立体对称群参考框架的注意力机制，引入了一种原理性的权重共享方案。

Result: 在计算机视觉（CIFAR-10）、3D点云（ScanObjectNN）和分子属性预测（QM9、OMol25）等多个基准测试中，Platonic Transformer通过利用几何约束实现了竞争性性能。

Conclusion: Platonic Transformer通过引入柏拉图立体对称群的参考框架，在不增加计算成本的情况下，实现了对连续平移和柏拉图对称性的联合等变性，同时保持了标准Transformer的架构和计算效率。

Abstract: While widespread, Transformers lack inductive biases for geometric symmetries
common in science and computer vision. Existing equivariant methods often
sacrifice the efficiency and flexibility that make Transformers so effective
through complex, computationally intensive designs. We introduce the Platonic
Transformer to resolve this trade-off. By defining attention relative to
reference frames from the Platonic solid symmetry groups, our method induces a
principled weight-sharing scheme. This enables combined equivariance to
continuous translations and Platonic symmetries, while preserving the exact
architecture and computational cost of a standard Transformer. Furthermore, we
show that this attention is formally equivalent to a dynamic group convolution,
which reveals that the model learns adaptive geometric filters and enables a
highly scalable, linear-time convolutional variant. Across diverse benchmarks
in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular
property prediction (QM9, OMol25), the Platonic Transformer achieves
competitive performance by leveraging these geometric constraints at no
additional cost.

</details>


### [19] [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03548)
*Danial Samadi Vahdati,Tai Duc Nguyen,Ekta Prashnani,Koki Nagano,David Luebke,Orazio Gallo,Matthew Stamm*

Main category: cs.CV

TL;DR: 该论文提出了一种实时检测AI视频会议中身份劫持的方法，通过解耦潜在表示中的身份信息，有效防御攻击。


<details>
  <summary>Details</summary>
Motivation: AI视频会议系统通过传输紧凑的姿态-表情潜在表示来降低带宽，但这种方式容易被攻击者操控，导致实时身份劫持。现有的深度伪造和合成视频检测方法因每一帧都是合成的而失效。

Method: 采用了一个基于姿态条件的大间隔对比编码器，从传输的潜在表示中分离出持久的身份线索，同时消除瞬时的姿态和表情信息。

Result: 实验表明，该方法在多种对话头生成模型上均优于现有的防御方法，能够实时运行，并在分布外场景中表现出强大的泛化能力。

Conclusion: 该论文提出了一种基于生物特征泄漏的防御方法，通过解耦嵌入的余弦测试实时检测非法身份交换，有效解决了AI视频会议系统中的安全漏洞问题。

Abstract: AI-based talking-head videoconferencing systems reduce bandwidth by sending a
compact pose-expression latent and re-synthesizing RGB at the receiver, but
this latent can be puppeteered, letting an attacker hijack a victim's likeness
in real time. Because every frame is synthetic, deepfake and synthetic video
detectors fail outright. To address this security problem, we exploit a key
observation: the pose-expression latent inherently contains biometric
information of the driving identity. Therefore, we introduce the first
biometric leakage defense without ever looking at the reconstructed RGB video:
a pose-conditioned, large-margin contrastive encoder that isolates persistent
identity cues inside the transmitted latent while cancelling transient pose and
expression. A simple cosine test on this disentangled embedding flags illicit
identity swaps as the video is rendered. Our experiments on multiple
talking-head generation models show that our method consistently outperforms
existing puppeteering defenses, operates in real-time, and shows strong
generalization to out-of-distribution scenarios.

</details>


### [20] [GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis](https://arxiv.org/abs/2510.03555)
*Peiran Quan,Zifan Gu,Zhuo Zhao,Qin Zhou,Donghan M. Yang,Ruichen Rong,Yang Xie,Guanghua Xiao*

Main category: cs.CV

TL;DR: GAS-MIL是一种集成框架，高效整合多个基础模型特征，在多种癌症分类任务中表现优异，简化了病理学模型部署。


<details>
  <summary>Details</summary>
Motivation: 适应和基准测试单个基础模型用于特定诊断任务通常耗时且资源密集，尤其是考虑到其规模和多样性。

Method: 引入了Group-Aggregative Selection Multi-Instance Learning (GAS-MIL)，一种灵活的集成框架，无需手动特征选择或大量任务特定微调即可整合多个基础模型的特征。

Result: 在三个癌症数据集（前列腺、卵巢和乳腺癌）的分类任务中，GAS-MIL相对于单个基础模型和已建立的MIL方法，始终表现出优越或相当的性能。

Conclusion: GAS-MIL通过高效整合异构基础模型，为病理学模型部署提供了可扩展的解决方案，并为未来的多模态和精准肿瘤学应用奠定了基础。

Abstract: Foundation models (FMs) have transformed computational pathology by providing
powerful, general-purpose feature extractors. However, adapting and
benchmarking individual FMs for specific diagnostic tasks is often
time-consuming and resource-intensive, especially given their scale and
diversity. To address this challenge, we introduce Group-Aggregative Selection
Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that
seamlessly integrates features from multiple FMs, preserving their
complementary strengths without requiring manual feature selection or extensive
task-specific fine-tuning. Across classification tasks in three cancer
datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL
consistently achieves superior or on-par performance relative to individual FMs
and established MIL methods, demonstrating its robustness and generalizability.
By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines
model deployment for pathology and provides a scalable foundation for future
multimodal and precision oncology applications.

</details>


### [21] [Evaluating OCR performance on food packaging labels in South Africa](https://arxiv.org/abs/2510.03570)
*Mayimunah Nagayi,Alice Khan,Tamryn Frank,Rina Swart,Clement Nyirenda*

Main category: cs.CV

TL;DR: 研究评估了四种OCR系统在食品包装图像上的性能，Tesseract准确性最高，EasyOCR平衡性佳，PaddleOCR覆盖率高但慢，TrOCR表现最差，为后续改进提供了基准。


<details>
  <summary>Details</summary>
Motivation: 包装上的OCR准确性对合规性和营养监测至关重要，但由于多语言文本、密集布局、字体多样、反光和曲面等挑战，亟需评估现有OCR系统的性能。

Method: 评估了四种开源OCR系统（Tesseract、EasyOCR、PaddleOCR、TrOCR）在真实食品包装图像上的表现，通过速度、覆盖率和准确性（CER、WER、BLEU等指标）进行对比。

Result: Tesseract在CER和BLEU上表现最佳；EasyOCR在准确性和多语言支持间取得平衡；PaddleOCR覆盖率高但速度慢；TrOCR表现最弱。

Conclusion: 该研究为食品包装OCR提供了特定基准，建立了基线，并指出了布局感知方法和文本定位的改进方向。

Abstract: This study evaluates four open-source Optical Character Recognition (OCR)
systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food
packaging images. The aim is to assess their ability to extract ingredient
lists and nutrition facts panels. Accurate OCR for packaging is important for
compliance and nutrition monitoring but is challenging due to multilingual
text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231
products (1,628 images) was processed by all four models to assess speed and
coverage, and a ground truth subset of 113 images (60 products) was created for
accuracy evaluation. Metrics include Character Error Rate (CER), Word Error
Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground
truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU
(0.245). EasyOCR provided a good balance between accuracy and multilingual
support. PaddleOCR achieved near complete coverage but was slower because it
ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest
results despite GPU acceleration. These results provide a packaging-specific
benchmark, establish a baseline, and highlight directions for layout-aware
methods and text localization.

</details>


### [22] [A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games](https://arxiv.org/abs/2510.03591)
*Faliu Yi,Sherif Abdelfattah,Wei Huang,Adrian Brown*

Main category: cs.CV

TL;DR: 提出混合Co-FineTuning方法，结合标注和未标注数据，有效减少对目标游戏标注数据的依赖，提升游戏视觉bug检测性能。


<details>
  <summary>Details</summary>
Motivation: 手动识别游戏视觉bug成本高且需专业知识，而现有监督模型依赖大量标注数据，但此类bug罕见，标注数据获取困难。

Method: 结合目标游戏和共域游戏的标注数据，并利用未标注数据增强特征表示学习，提出了一种混合Co-FineTuning（CFT）方法。

Result: CFT方法在多游戏环境中表现稳健，即使仅使用目标游戏50%的标注数据，仍保持竞争力。

Conclusion: 提出的混合Co-FineTuning（CFT）方法在游戏视觉bug检测中表现出色，显著减少了对目标游戏标注数据的依赖，并在多游戏环境中优于传统基线。

Abstract: Manual identification of visual bugs in video games is a resource-intensive
and costly process, often demanding specialized domain knowledge. While
supervised visual bug detection models offer a promising solution, their
reliance on extensive labeled datasets presents a significant challenge due to
the infrequent occurrence of such bugs. To overcome this limitation, we propose
a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled
and unlabeled data. Our approach leverages labeled samples from the target game
and diverse co-domain games, additionally incorporating unlabeled data to
enhance feature representation learning. This strategy maximizes the utility of
all available data, substantially reducing the dependency on labeled examples
from the specific target game. The developed framework demonstrates enhanced
scalability and adaptability, facilitating efficient visual bug detection
across various game titles. Our experimental results show the robustness of the
proposed method for game visual bug detection, exhibiting superior performance
compared to conventional baselines across multiple gaming environments.
Furthermore, CFT maintains competitive performance even when trained with only
50% of the labeled data from the target game.

</details>


### [23] [MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations](https://arxiv.org/abs/2510.03666)
*Jiang Wu,Sichao Wu,Yinsong Ma,Guangyuan Yu,Haoyuan Xu,Lifang Zheng,Jingliang Duan*

Main category: cs.CV

TL;DR: MonitorVLM是一个创新的视觉-语言框架，通过智能检测监控视频流中的安全违规行为，显著提升了矿业安全监控的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统手动检查在高风险领域（如矿业）中劳动密集、易出错且难以应对大规模动态环境，因此急需智能化和自动化的安全监控解决方案。

Method: 提出了MonitorVLM，一个新颖的视觉-语言框架，包含三个关键创新：领域特定的违规数据集、动态选择最相关条款的条款过滤器模块，以及增强工人区域的行为放大模块。

Result: 实验结果表明，MonitorVLM显著优于基线视觉-语言模型，在精确率、召回率和F1分数上分别提升了22.01%、34.22%和28.37%。

Conclusion: 本研究展示了多模态大模型在提升矿业及其他领域职业安全监控方面的潜力。

Abstract: Industrial accidents, particularly in high-risk domains such as surface and
underground mining, are frequently caused by unsafe worker behaviors.
Traditional manual inspection remains labor-intensive, error-prone, and
insufficient for large-scale, dynamic environments, highlighting the urgent
need for intelligent and automated safety monitoring. In this paper, we present
MonitorVLM, a novel vision--language framework designed to detect safety
violations directly from surveillance video streams. MonitorVLM introduces
three key innovations: (1) a domain-specific violation dataset comprising 9,000
vision--question--answer (VQA) samples across 40 high-frequency mining
regulations, enriched with augmentation and auxiliary detection cues; (2) a
clause filter (CF) module that dynamically selects the Top-$K$ most relevant
clauses, reducing inference latency by 13.56\% while maintaining accuracy; and
(3) a behavior magnifier (BM) module that enhances worker regions to improve
fine-grained action recognition, yielding additional gains of 3.45% in
precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM
significantly outperforms baseline vision--language models, achieving
improvements of 22.01% in precision, 34.22\% in recall, and 28.37% in F1 score
over the 72B unfine-tuned baseline. A lightweight web-based interface further
integrates MonitorVLM into practical workflows, enabling automatic violation
reporting with video timestamping. This study highlights the potential of
multimodal large models to enhance occupational safety monitoring in mining and
beyond.

</details>


### [24] [Referring Expression Comprehension for Small Objects](https://arxiv.org/abs/2510.03701)
*Kanoko Goto,Takumi Hirose,Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: 本文提出SOREC数据集和PIZA模块，解决了小目标对象定位难题，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言学习在REC任务中取得了显著进展，但定位极小的目标对象仍是重要挑战，尤其是在自动驾驶等实际应用中。

Method: 提出了渐进式迭代缩放适配器（PIZA），用于参数高效微调，使模型能够逐步放大并定位小目标对象。

Result: 在SOREC数据集上应用PIZA后，GroundingDINO模型的准确率显著提升。

Conclusion: 本文通过引入SOREC数据集和PIZA模块，显著提升了小目标对象的指代表达理解（REC）任务性能，尤其在自动驾驶场景中。

Abstract: Referring expression comprehension (REC) aims to localize the target object
described by a natural language expression. Recent advances in vision-language
learning have led to significant performance improvements in REC tasks.
However, localizing extremely small objects remains a considerable challenge
despite its importance in real-world applications such as autonomous driving.
To address this issue, we introduce a novel dataset and method for REC
targeting small objects. First, we present the small object REC (SOREC)
dataset, which consists of 100,000 pairs of referring expressions and
corresponding bounding boxes for small objects in driving scenarios. Second, we
propose the progressive-iterative zooming adapter (PIZA), an adapter module for
parameter-efficient fine-tuning that enables models to progressively zoom in
and localize small objects. In a series of experiments, we apply PIZA to
GroundingDINO and demonstrate a significant improvement in accuracy on the
SOREC dataset. Our dataset, codes and pre-trained models are publicly available
on the project page.

</details>


### [25] [Artery-Vein Segmentation from Fundus Images using Deep Learning](https://arxiv.org/abs/2510.03717)
*Sharan SK,Subin Sahayam,Umarani Jayaraman,Lakshmi Priya A*

Main category: cs.CV

TL;DR: 提出Attention-WNet，一种结合注意力机制的视网膜血管分割方法，在公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割对眼部疾病诊断和全身血管健康评估至关重要，现有方法仍有改进空间。

Method: 采用注意力机制并将其整合到WNet深度学习模型中，形成Attention-WNet。

Result: 在HRF和DRIVE等公开数据集上，Attention-WNet的表现优于其他现有模型。

Conclusion: 该论文提出了一种基于注意力机制的Attention-WNet深度学习模型，用于视网膜血管分割，并在公开数据集上表现优于现有最先进模型。

Abstract: Segmenting of clinically important retinal blood vessels into arteries and
veins is a prerequisite for retinal vessel analysis. Such analysis can provide
potential insights and bio-markers for identifying and diagnosing various
retinal eye diseases. Alteration in the regularity and width of the retinal
blood vessels can act as an indicator of the health of the vasculature system
all over the body. It can help identify patients at high risk of developing
vasculature diseases like stroke and myocardial infarction. Over the years,
various Deep Learning architectures have been proposed to perform retinal
vessel segmentation. Recently, attention mechanisms have been increasingly used
in image segmentation tasks. The work proposes a new Deep Learning approach for
artery-vein segmentation. The new approach is based on the Attention mechanism
that is incorporated into the WNet Deep Learning model, and we call the model
as Attention-WNet. The proposed approach has been tested on publicly available
datasets such as HRF and DRIVE datasets. The proposed approach has outperformed
other state-of-art models available in the literature.

</details>


### [26] [Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization](https://arxiv.org/abs/2510.03763)
*Jiaxin Deng,Junbiao Pang*

Main category: cs.CV

TL;DR: ARSAM通过动态重用和更新分解梯度，显著加速SAM优化，同时保持泛化能力，适用于多种任务。


<details>
  <summary>Details</summary>
Motivation: SAM虽然提高了模型泛化能力，但计算成本较高，因此需要一种更高效的方法来替代。

Method: 提出ARSAM方法，通过分解SAM的梯度为SGD梯度和PSF，并动态重用和更新PSF来加速优化。

Result: ARSAM在保持与SAM相当的准确度下，速度提升了约40%，并在多个挑战性任务中验证了其有效性。

Conclusion: ARSAM通过自适应采样、重用和混合分解梯度，显著加速了SAM，同时保持了模型的泛化能力，并在多个任务中展示了其广泛的实用性。

Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doubles
the computational cost of Stochastic Gradient Descent (SGD) by requiring twice
the gradient calculations per optimization step. To mitigate this, we propose
Adaptively sampling-Reusing-mixing decomposed gradients to significantly
accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can
be decomposed into the SGD gradient and the Projection of the Second-order
gradient onto the First-order gradient (PSF). Furthermore, we observe that the
SGD gradient and PSF dynamically evolve during training, emphasizing the
growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed
to the reused PSF and the timely updated PSF still maintain the model's
generalization ability. Extensive experiments show that ARSAM achieves
state-of-the-art accuracies comparable to SAM across diverse network
architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a
speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various
challenge tasks (\textit{e.g.}, human pose estimation, and model quantization)
without sacrificing performance, demonstrating its broad practicality.% The
code is publicly accessible at: https://github.com/ajiaaa/ARSAM.

</details>


### [27] [PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis](https://arxiv.org/abs/2510.03873)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: PoseGaze-AHP是一个同步捕获头部姿势和注视运动的3D数据集，填补了现有数据的空白，支持AI驱动的眼源性异常头位诊断。


<details>
  <summary>Details</summary>
Motivation: 现有数据集分别关注头部姿势和眼球运动，限制了集成诊断方法的发展和AI在异常头位分析中的进展。

Method: 通过迭代过程使用Claude 3.5 Sonnet模型提取结构化临床数据，结合逐步、分层和复杂提示策略，并使用Neural Head Avatar (NHA)框架将记录转换为3D表示。

Result: 数据集包含7,920张图像，覆盖广泛的眼部条件，提取方法总体准确率为91.92%。

Conclusion: PoseGaze-AHP是首个公开的专为AI驱动的眼源性异常头位诊断设计的资源，支持开发准确且符合隐私要求的诊断工具。

Abstract: Diagnosing ocular-induced abnormal head posture (AHP) requires a
comprehensive analysis of both head pose and ocular movements. However,
existing datasets focus on these aspects separately, limiting the development
of integrated diagnostic approaches and restricting AI-driven advancements in
AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D
dataset that synchronously captures head pose and gaze movement information for
ocular-induced AHP assessment. Structured clinical data were extracted from
medical literature using large language models (LLMs) through an iterative
process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and
complex prompting strategies. The extracted records were systematically imputed
and transformed into 3D representations using the Neural Head Avatar (NHA)
framework. The dataset includes 7,920 images generated from two head textures,
covering a broad spectrum of ocular conditions. The extraction method achieved
an overall accuracy of 91.92%, demonstrating its reliability for clinical
dataset construction. PoseGaze-AHP is the first publicly available resource
tailored for AI-driven ocular-induced AHP diagnosis, supporting the development
of accurate and privacy-compliant diagnostic tools.

</details>


### [28] [Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks](https://arxiv.org/abs/2510.03878)
*Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R*

Main category: cs.CV

TL;DR: 本研究开发了一个多模态深度学习框架，整合临床、放射学和病理学图像，以提高口腔鳞状细胞癌的早期检测率。集成模型在验证数据集上表现出84.58%的准确率，展示了其在辅助临床决策中的潜力。


<details>
  <summary>Details</summary>
Motivation: 口腔鳞状细胞癌（OSCC）的晚期诊断是其高全球死亡率的重要原因，超过50%的病例在晚期被发现，5年生存率低于50%（WHO统计数据）。本研究旨在通过开发一种多模态深度学习框架，整合临床、放射学和病理学图像，提高OSCC的早期检测率。

Method: 本研究采用回顾性研究设计，利用公开可用的数据集，代表三种不同的医学成像模态。每种模态特定的数据集通过迁移学习训练DenseNet-121 CNN。应用增强和模态特定的预处理以提高鲁棒性。预测通过验证加权集成策略进行融合。评估使用准确率、精确率、召回率、F1分数。

Result: 放射学模态（100%）和病理学模态（95.12%）实现了高验证准确率，临床图像由于视觉异质性表现较低（63.10%）。集成模型在多模态验证数据集（55个样本）上表现出84.58%的总体准确率，展示了诊断鲁棒性的提升。

Conclusion: 该多模态集成框架通过提供一种非侵入性、AI辅助的分诊工具，填补了当前诊断流程中的空白，有助于早期识别高风险病变。它支持临床医生决策，符合全球肿瘤学指南，旨在减少诊断延迟并改善患者预后。

Abstract: Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes
significantly to its high global mortality rate, with over 50\% of cases
detected at advanced stages and a 5-year survival rate below 50\% according to
WHO statistics. This study aims to improve early detection of OSCC by
developing a multimodal deep learning framework that integrates clinical,
radiological, and histopathological images using a weighted ensemble of
DenseNet-121 convolutional neural networks (CNNs). Material and Methods A
retrospective study was conducted using publicly available datasets
representing three distinct medical imaging modalities. Each modality-specific
dataset was used to train a DenseNet-121 CNN via transfer learning.
Augmentation and modality-specific preprocessing were applied to increase
robustness. Predictions were fused using a validation-weighted ensemble
strategy. Evaluation was performed using accuracy, precision, recall, F1-score.
Results High validation accuracy was achieved for radiological (100\%) and
histopathological (95.12\%) modalities, with clinical images performing lower
(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improved
diagnostic robustness with an overall accuracy of 84.58\% on a multimodal
validation dataset of 55 samples. Conclusion The multimodal ensemble framework
bridges gaps in the current diagnostic workflow by offering a non-invasive,
AI-assisted triage tool that enhances early identification of high-risk
lesions. It supports clinicians in decision-making, aligning with global
oncology guidelines to reduce diagnostic delays and improve patient outcomes.

</details>


### [29] [Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition](https://arxiv.org/abs/2510.03921)
*Arushi Dashore,Aryan Anumala,Emily Hui,Olivia Yang*

Main category: cs.CV

TL;DR: 本研究提出了一种结合CNN-LSTM和LLM的框架，将生物力学数据转化为对网球球员和教练有用的反馈，解决了现有系统反馈不可操作的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管自动化网球击球分析在生物力学运动线索和深度学习技术的结合下取得了显著进展，但现有系统往往无法将生物力学见解转化为对球员和教练来说既易懂又有意义的可操作语言反馈。

Method: 使用CNN-LSTM模型从运动数据中提取关键生物力学特征（如关节角度、肢体速度和动力链模式），并利用大型语言模型（LLMs）生成反馈。

Result: 实验评估表明，该框架在分类性能和可解释性方面表现良好，成功弥合了可解释AI与运动生物力学之间的差距。

Conclusion: 本研究通过结合CNN-LSTM模型和大型语言模型（LLMs），开发了一个新颖的框架，能够将生物力学特征转化为对球员和教练有实际意义的反馈，填补了现有系统在可操作反馈方面的不足。

Abstract: Automated tennis stroke analysis has advanced significantly with the
integration of biomechanical motion cues alongside deep learning techniques,
enhancing stroke classification accuracy and player performance evaluation.
Despite these advancements, existing systems often fail to connect
biomechanical insights with actionable language feedback that is both
accessible and meaningful to players and coaches. This research project
addresses this gap by developing a novel framework that extracts key
biomechanical features (such as joint angles, limb velocities, and kinetic
chain patterns) from motion data using Convolutional Neural Network Long
Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for
relationships influencing stroke effectiveness and injury risk, forming the
basis for feedback generation using large language models (LLMs). Leveraging
the THETIS dataset and feature extraction techniques, our approach aims to
produce feedback that is technically accurate, biomechanically grounded, and
actionable for end-users. The experimental setup evaluates this framework on
classification performance and interpretability, bridging the gap between
explainable AI and sports biomechanics.

</details>


### [30] [Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks](https://arxiv.org/abs/2510.04034)
*Linn Bieske,Carla Lorente*

Main category: cs.CV

TL;DR: 研究通过优化超参数和提出新方法，提升了文本驱动图像编辑的精确性和可靠性，解决了现有框架中的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习（如稳定扩散模型）简化了图像编辑过程，但仍存在结果不一致（如发色变化不一致）的问题。研究旨在通过优化超参数提升提示到提示图像编辑框架的精确性和可靠性。

Method: 研究采用了‘word swap’方法、开发了‘attention re-weight method’以提高适应性，并提出了‘CL P2P’框架来解决循环不一致等问题。

Result: 研究提出并验证了‘attention re-weight method’和‘CL P2P’框架，有效解决了现有框架中的不一致性问题，提升了图像编辑的质量。

Conclusion: 本研究通过优化超参数和提出新的方法（如‘attention re-weight method’和‘CL P2P’框架），显著提升了提示到提示图像编辑的精确性和可靠性，解决了现有框架中的不一致性问题。

Abstract: Recent advances in image editing have shifted from manual pixel manipulation
to employing deep learning methods like stable diffusion models, which now
leverage cross-attention mechanisms for text-driven control. This transition
has simplified the editing process but also introduced variability in results,
such as inconsistent hair color changes. Our research aims to enhance the
precision and reliability of prompt-to-prompt image editing frameworks by
exploring and optimizing hyperparameters. We present a comprehensive study of
the "word swap" method, develop an "attention re-weight method" for better
adaptability, and propose the "CL P2P" framework to address existing
limitations like cycle inconsistency. This work contributes to understanding
and improving the interaction between hyperparameter settings and the
architectural choices of neural network models, specifically their attention
mechanisms, which significantly influence the composition and quality of the
generated images.

</details>


### [31] [\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding](https://arxiv.org/abs/2510.04039)
*Bin Lei,Nuo Xu,Ali Payani,Mingyi Hong,Chunhua Liao,Yu Cao,Caiwen Ding*

Main category: cs.CV

TL;DR: GUI-Spotlight通过动态调用工具提高视觉基础准确性，仅用18.5K训练样本在ScreenSpot-Pro基准测试中超越其他模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在图形用户界面（GUI）系统中的实际应用受到视觉基础可靠性的限制，无法准确执行指针级操作。

Method: 引入GUI-Spotlight模型，该模型专门用于图像基础推理，动态调用多个专用工具迭代缩小屏幕相关区域。

Result: 在ScreenSpot-Pro基准测试中，GUI-Spotlight以52.8%的准确率超越V2P-7B（50.6%）和GTA-1-7B（50.1%），且训练样本量显著减少。

Conclusion: GUI-Spotlight通过动态调用多种专用工具迭代缩小屏幕相关区域，显著提高了视觉基础的准确性。在ScreenSpot-Pro基准测试中，仅用18.5K训练样本就超越了其他模型，展示了其高效性和实用性。

Abstract: Multimodal large language models (MLLMs) have markedly expanded the
competence of graphical user-interface (GUI) systems, propelling them beyond
controlled simulations into complex, real-world environments across diverse
platforms. However, practical usefulness is still bounded by the reliability of
visual grounding, i.e., mapping textual references to exact on-screen elements.
This limitation prevents the system from accurately performing pointer-level
actions such as clicking or dragging. To address it, we introduce GUI-Spotlight
-- a model trained for image-grounded reasoning that dynamically invokes
multiple specialized tools to iteratively narrow its focus to the relevant
region of the screen, thereby substantially improving visual grounding
accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only
18.5K training samples achieves 52.8\% accuracy, surpassing V2P-7B (50.6\% with
9.6M training samples) and GTA-1-7B (50.1\% with 1.56M training samples).

</details>


### [32] [Quantization Range Estimation for Convolutional Neural Networks](https://arxiv.org/abs/2510.04044)
*Bingtao Yang,Yujia Wang,Mengzhi Jiao,Hongwei Huo*

Main category: cs.CV

TL;DR: 本文提出了一种改进的后训练量化范围估计方法，通过优化问题和高效搜索算法，在低比特量化中保持高模型精度。


<details>
  <summary>Details</summary>
Motivation: 低比特量化在保持模型精度方面具有挑战性，本文旨在通过改进的范围估计方法提升后训练量化的性能。

Method: 将范围估计建模为通过逐层局部最小值最小化量化误差的优化问题，并提出了一个高效的搜索算法，应用于变换后的权重空间以进一步提升性能。

Result: 在ResNet系列模型和Inception-v3模型上，该方法在图像分类任务中的top-1准确率普遍优于现有技术，尤其在4位量化中表现显著。

Conclusion: 提出的范围估计方法在8位和6位量化设置下几乎无精度损失，且在4位量化中显著提升精度，优于现有技术。

Abstract: Post-training quantization for reducing the storage of deep neural network
models has been demonstrated to be an effective way in various tasks. However,
low-bit quantization while maintaining model accuracy is a challenging problem.
In this paper, we present a range estimation method to improve the quantization
performance for post-training quantization. We model the range estimation into
an optimization problem of minimizing quantization errors by layer-wise local
minima. We prove this problem is locally convex and present an efficient search
algorithm to find the optimal solution. We propose the application of the above
search algorithm to the transformed weights space to do further improvement in
practice. Our experiments demonstrate that our method outperforms
state-of-the-art performance generally on top-1 accuracy for image
classification tasks on the ResNet series models and Inception-v3 model. The
experimental results show that the proposed method has almost no loss of top-1
accuracy in 8-bit and 6-bit settings for image classifications, and the
accuracy of 4-bit quantization is also significantly improved. The code is
available at https://github.com/codeiscommitting/REQuant.

</details>


### [33] [MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation](https://arxiv.org/abs/2510.04057)
*Zhenyu Pan,Yucheng Lu,Han Liu*

Main category: cs.CV

TL;DR: MetaFind是一个场景感知的三模态检索框架，通过ESSGNN编码器和灵活查询机制提升3D资产检索的上下文一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D资产检索中的两个核心问题：（i）忽视空间、语义和风格约束的不一致检索；（ii）缺乏专为3D资产设计的标准化检索范式。

Method: MetaFind引入了ESSGNN布局编码器，联合建模对象级特征和场景级布局结构，支持文本、图像和3D模态的任意组合查询。

Result: 实验评估表明，MetaFind在多种检索任务中相比基线方法显著提升了空间和风格一致性。

Conclusion: MetaFind通过其灵活的三模态检索机制和场景感知设计，显著提升了3D资产检索的空间和风格一致性，为元宇宙场景生成提供了高效解决方案。

Abstract: We present MetaFind, a scene-aware tri-modal compositional retrieval
framework designed to enhance scene generation in the metaverse by retrieving
3D assets from large-scale repositories. MetaFind addresses two core
challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,
and stylistic constraints, and (ii) the absence of a standardized retrieval
paradigm specifically tailored for 3D asset retrieval, as existing approaches
mainly rely on general-purpose 3D shape representation models. Our key
innovation is a flexible retrieval mechanism that supports arbitrary
combinations of text, image, and 3D modalities as queries, enhancing spatial
reasoning and style consistency by jointly modeling object-level features
(including appearance) and scene-level layout structures. Methodologically,
MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that
captures spatial relationships and object appearance features, ensuring
retrieved 3D assets are contextually and stylistically coherent with the
existing scene, regardless of coordinate frame transformations. The framework
supports iterative scene construction by continuously adapting retrieval
results to current scene updates. Empirical evaluations demonstrate the
improved spatial and stylistic consistency of MetaFind in various retrieval
tasks compared to baseline methods.

</details>


### [34] [TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing](https://arxiv.org/abs/2510.04100)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Harold Soh*

Main category: cs.CV

TL;DR: 论文提出标准化拓扑映射评估指标和模糊度定量测量方法，构建多样化基准数据集并开源，促进领域研究的公平性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 拓扑映射领域缺乏标准化的评估指标、数据集和协议，导致现有系统无法进行公平和可重复的比较。此外，感知混淆这一关键挑战未被充分量化，尽管其对系统性能有显著影响。

Method: 论文提出：(1)将拓扑一致性形式化为拓扑映射的基本属性，并展示定位准确性作为高效且可解释的替代指标；(2)提出首个数据集模糊度的定量测量方法，以实现跨环境的公平比较。此外，还构建了一个多样化的基准数据集，并实现了深度学习和经典方法的基线系统。

Result: 实验和分析揭示了当前方法在感知混淆下的局限性。提出的评估协议和基准数据集为拓扑映射研究提供了新的标准化工具。

Conclusion: 该论文通过提出标准化评估指标和数据集，解决了拓扑映射领域缺乏公平和可重复比较的问题。所有数据集、基线系统和评估工具均已开源，以促进拓扑映射研究的持续发展。

Abstract: Topological mapping offers a compact and robust representation for
navigation, but progress in the field is hindered by the lack of standardized
evaluation metrics, datasets, and protocols. Existing systems are assessed
using different environments and criteria, preventing fair and reproducible
comparisons. Moreover, a key challenge - perceptual aliasing - remains
under-quantified, despite its strong influence on system performance. We
address these gaps by (1) formalizing topological consistency as the
fundamental property of topological maps and showing that localization accuracy
provides an efficient and interpretable surrogate metric, and (2) proposing the
first quantitative measure of dataset ambiguity to enable fair comparisons
across environments. To support this protocol, we curate a diverse benchmark
dataset with calibrated ambiguity levels, implement and release deep-learned
baseline systems, and evaluate them alongside classical methods. Our
experiments and analysis yield new insights into the limitations of current
approaches under perceptual aliasing. All datasets, baselines, and evaluation
tools are fully open-sourced to foster consistent and reproducible research in
topological mapping.

</details>


### [35] [Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs](https://arxiv.org/abs/2510.04142)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.CV

TL;DR: 论文提出APO方法解决MLLM蒸馏中的概念漂移问题，通过理论连接与多教师对齐，显著提升模型性能，并发布数据集CXR-MAX。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型（MLLM）蒸馏中因教师模型推理轨迹的概念漂移导致的偏置传递问题，提升学生模型的性能。

Method: 提出“学习、比较、批判”范式，结合概念漂移理论，将多教师模型的非平稳推理动态转化为多流推理轨迹的下一令牌预测，最终通过APO实现概念对齐。

Result: 实验证明该方法在知识蒸馏中具有优越的一致性、鲁棒性和泛化性能，并贡献了大规模数据集CXR-MAX。

Conclusion: 本研究通过理论连接概念漂移与知识蒸馏，提出自主偏好优化（APO）方法，显著提升了学生模型的鲁棒性、一致性和泛化能力，并通过大规模数据集CXR-MAX验证了其有效性。

Abstract: This paper identifies a critical yet underexplored challenge in distilling
from multimodal large language models (MLLMs): the reasoning trajectories
generated by multiple drifting teachers exhibit concept drift, whereby their
reasoning distributions evolve unpredictably and transmit biases to the student
model, ultimately compromising its performance. To tackle this issue, we
pioneer a theoretical connection between concept drift and knowledge
distillation, casting the non-stationary reasoning dynamics from multiple MLLM
teachers as next-token prediction of multi-stream reasoning trajectories.Guided
by concept drift, we introduce the "learn, compare, critique" paradigm,
culminating in autonomous preference optimization (APO). Under the active
guidance of the teachers, the student model first learns and self-distils
preferred thinking by comparing multiple teachers. It then engages in critical
reflection over the drifting inference from teachers, performing concept
alignment through APO, ultimately yielding a robust, consistent, and
generalizable model.Extensive experiments demonstrate our superior performance
of consistency, robustness and generalization within knowledge distillation.
Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers
Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived
from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public
at: https://anonymous.4open.science/r/Autonomous-Distillation/.

</details>


### [36] [World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge](https://arxiv.org/abs/2510.04201)
*Moo Hyun Son,Jintaek Oh,Sun Bin Mun,Jaechul Roh,Sehyun Choi*

Main category: cs.CV

TL;DR: World-To-Image通过动态搜索网络和多模态提示优化，显著提升T2I模型在生成新颖实体时的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的T2I模型在面对新颖或OOD实体时性能显著下降，需要一种方法来弥补其知识截止的局限性。

Method: 设计了一个动态搜索网络以检索未知概念的代理，并进行多模态提示优化，引导生成模型准确合成图像。

Result: 在NICE基准测试中，World-To-Image在语义对齐和视觉美学上均优于现有方法，准确率提升8.1%。

Conclusion: World-To-Image框架通过代理驱动的世界知识，显著提升了T2I模型在生成新颖或OOD实体时的性能，实现了语义对齐和视觉美学的双重提升。

Abstract: While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

</details>


### [37] [MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering](https://arxiv.org/abs/2510.04220)
*Lixuan He,Shikang Zheng,Linfeng Zhang*

Main category: cs.CV

TL;DR: MASC框架通过层次化语义树优化自回归模型，提升训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型处理视觉标记时忽略其内在结构，导致预测任务复杂且效率低下。

Method: 提出MASC框架，利用几何感知距离度量和密度驱动的聚合构造，构建层次化语义树。

Result: MASC加速训练达57%，并将LlamaGen-XL的FID从2.87降至2.58。

Conclusion: MASC通过结构化预测空间显著提升了自回归模型的训练效率和生成质量，使其与最先进方法竞争。

Abstract: Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.

</details>


### [38] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: ZoomIn两阶段法通过定位可疑区域和聚焦分析，显著提升AI生成图像检测的准确性和可解释性，准确率达96.39%。


<details>
  <summary>Details</summary>
Motivation: AI生成图像的快速发展和高质量合成图像中细微伪影的检测困难，引发了对数字完整性的担忧。

Method: 提出ZoomIn两阶段法，首先生成可疑区域，然后对放大区域进行聚焦分析；并使用MagniFake数据集（20,000张真实与合成图像）进行训练。

Result: ZoomIn方法实现了96.39%的准确率，并提供了基于视觉证据的可解释性结果。

Conclusion: ZoomIn框架通过两阶段法显著提高了AI生成图像检测的准确性和可解释性，为数字完整性提供了可靠的解决方案。

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [39] [Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks](https://arxiv.org/abs/2510.04245)
*Ayushi Mehrotra,Derek Peng,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 提出了一种不依赖补丁先验知识的防御方法，通过概念激活向量中和补丁效应，在多种情况下优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的防御方法通常假设已知补丁的大小或位置，限制了其适用性。因此，需要一种不依赖这些先验知识的防御策略。

Method: 提出了一种补丁无关的防御方法，利用基于概念的解释来识别和抑制最具影响力的概念激活向量，从而在不进行显式检测的情况下中和补丁效应。

Result: 在Imagenette数据集上使用ResNet-50评估，该方法在鲁棒性和清洁准确率上均优于当前最先进的PatchCleanser，且在不同补丁大小和位置上保持强健性能。

Conclusion: 结合可解释性与鲁棒性展示了概念驱动防御在对抗补丁攻击中的潜力，为机器学习模型的安全提供了一种可扩展的策略。

Abstract: Adversarial patch attacks pose a practical threat to deep learning models by
forcing targeted misclassifications through localized perturbations, often
realized in the physical world. Existing defenses typically assume prior
knowledge of patch size or location, limiting their applicability. In this
work, we propose a patch-agnostic defense that leverages concept-based
explanations to identify and suppress the most influential concept activation
vectors, thereby neutralizing patch effects without explicit detection.
Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and
clean accuracy than the state-of-the-art PatchCleanser, while maintaining
strong performance across varying patch sizes and locations. Our results
highlight the promise of combining interpretability with robustness and suggest
concept-driven defenses as a scalable strategy for securing machine learning
models against adversarial patch attacks.

</details>


### [40] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: MorphoSim是一个语言引导的4D场景生成框架，支持多视角一致性和对象级控制，适用于机器人等领域的动态环境需求。


<details>
  <summary>Details</summary>
Motivation: 开发支持可控和可编辑时空环境的世界模型，以解决机器人领域对可扩展训练数据、可重复评估和灵活任务设计的需求。

Method: MorphoSim框架结合了轨迹引导生成与特征场蒸馏技术，支持从自然语言指令生成多视角一致且具备对象级控制的动态场景。

Result: 实验表明，MorphoSim在保持高场景保真度的同时，实现了场景的可控性和可编辑性。

Conclusion: MorphoSim通过整合轨迹引导生成与特征场蒸馏技术，实现了高保真度的4D场景可控性和可编辑性，为机器人等领域提供了灵活的动态环境生成工具。

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [41] [Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting](https://arxiv.org/abs/2510.04401)
*Xuyang Guo,Zekai Huang,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: VLMs在组合计数任务中表现不佳，需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 探究VLMs是否能准确计数，尤其是在组合形状的复杂场景中。

Method: 通过VLMCountBench基准测试，控制变量（如颜色、大小、提示优化）并系统分析简单几何形状（如三角形、圆形）的计数任务。

Result: VLMs在单一形状计数中表现可靠，但在组合形状计数中失败率显著。

Conclusion: 当前视觉语言模型（VLMs）在组合计数任务中存在显著局限性，需进一步研究以提升其能力。

Abstract: Vision-Language Models (VLMs) have become a central focus of today's AI
community, owing to their impressive abilities gained from training on
large-scale vision-language data from the Web. These models have demonstrated
strong performance across diverse tasks, including image understanding, video
understanding, complex visual reasoning, and embodied AI. Despite these
noteworthy successes, a fundamental question remains: Can VLMs count objects
correctly? In this paper, we introduce a simple yet effective benchmark,
VLMCountBench, designed under a minimalist setting with only basic geometric
shapes (e.g., triangles, circles) and their compositions, focusing exclusively
on counting tasks without interference from other factors. We adopt strict
independent variable control and systematically study the effects of simple
properties such as color, size, and prompt refinement in a controlled ablation.
Our empirical results reveal that while VLMs can count reliably when only one
shape type is present, they exhibit substantial failures when multiple shape
types are combined (i.e., compositional counting). This highlights a
fundamental empirical limitation of current VLMs and motivates important
directions for future research.

</details>


### [42] [SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2510.04472)
*Baber Jan,Saeed Anwar,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais*

Main category: cs.CV

TL;DR: SPEGNet通过统一设计整合多尺度特征，解决了现有伪装检测方法的复杂性和分辨率问题，实现了高性能和实时推理。


<details>
  <summary>Details</summary>
Motivation: 现有伪装物体检测方法因依赖复杂组件累积而带来计算负担，且因降低分辨率丢失细节，SPEGNet旨在通过统一设计解决这些问题。

Method: SPEGNet采用通道校准和空间增强的多尺度特征整合，通过渐进式细化实现尺度自适应的边缘调制，保持语义空间对齐。

Result: SPEGNet在CAMO、COD10K和NC4K数据集上分别达到0.887、0.890和0.895的$S_\alpha$分数，并实现实时推理速度。

Conclusion: SPEGNet通过统一设计解决了现有方法中的复杂性和分辨率问题，实现了在多个数据集上的高性能（如CAMO、COD10K和NC4K）和实时推理速度。

Abstract: Camouflaged object detection segments objects with intrinsic similarity and
edge disruption. Current detection methods rely on accumulated complex
components. Each approach adds components such as boundary modules, attention
mechanisms, and multi-scale processors independently. This accumulation creates
a computational burden without proportional gains. To manage this complexity,
they process at reduced resolutions, eliminating fine details essential for
camouflage. We present SPEGNet, addressing fragmentation through a unified
design. The architecture integrates multi-scale features via channel
calibration and spatial enhancement. Boundaries emerge directly from
context-rich representations, maintaining semantic-spatial alignment.
Progressive refinement implements scale-adaptive edge modulation with peak
influence at intermediate resolutions. This design strikes a balance between
boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\alpha$
on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.
Our approach excels across scales, from tiny, intricate objects to large,
pattern-similar ones, while handling occlusion and ambiguous boundaries. Code,
model weights, and results are available on
\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.

</details>


### [43] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: MedCLM通过链式思维推理和课程策略，在医学VQA任务中实现最优性能，为临床对齐模型提供框架。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中临床诊断推理与AI结合的挑战，提升医学视觉语言模型的临床对齐性和推理能力。

Method: 提出MedCLM自动化流程，将检测数据集转化为大规模医学VQA数据，并结合链式思维推理。采用集成CoT-课程策略，分为简单（显性病灶框）、中等（隐性定位）和困难（弱监督推理）三个阶段。

Result: MedCLM在多个医学VQA基准测试中达到最先进性能。

Conclusion: MedCLM通过整合链式思维（CoT）推理和课程学习策略，在医学视觉问答（VQA）任务中实现了最先进的性能，为开发临床对齐的医学视觉语言模型提供了可扩展的框架。

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [44] [SFANet: Spatial-Frequency Attention Network for Deepfake Detection](https://arxiv.org/abs/2510.04630)
*Vrushank Ahire,Aniruddh Muley,Shivam Zample,Siddharth Verma,Pranav Menon,Surbhi Madan,Abhinav Dhall*

Main category: cs.CV

TL;DR: 提出一种结合Transformer和纹理方法的集成框架，通过创新技术提升深度伪造检测的准确性和鲁棒性，在多样化数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多样化数据集和生成技术上泛化能力不足，需要一种更鲁棒的检测框架。

Method: 结合基于Transformer的架构（如Swin Transformers和ViTs）和基于纹理的方法，引入创新的数据分割、顺序训练、频率分割、基于补丁的注意力和面部分割技术。

Result: 在DFWild-Cup数据集上实现了最先进的性能，展示了Transformer在全局特征提取和纹理方法在可解释性上的互补优势。

Conclusion: 混合模型能有效应对深度伪造检测的不断变化挑战，为实际应用提供鲁棒解决方案。

Abstract: Detecting manipulated media has now become a pressing issue with the recent
rise of deepfakes. Most existing approaches fail to generalize across diverse
datasets and generation techniques. We thus propose a novel ensemble framework,
combining the strengths of transformer-based architectures, such as Swin
Transformers and ViTs, and texture-based methods, to achieve better detection
accuracy and robustness. Our method introduces innovative data-splitting,
sequential training, frequency splitting, patch-based attention, and face
segmentation techniques to handle dataset imbalances, enhance high-impact
regions (e.g., eyes and mouth), and improve generalization. Our model achieves
state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse
subset of eight deepfake datasets. The ensemble benefits from the
complementarity of these approaches, with transformers excelling in global
feature extraction and texturebased methods providing interpretability. This
work demonstrates that hybrid models can effectively address the evolving
challenges of deepfake detection, offering a robust solution for real-world
applications.

</details>


### [45] [Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction](https://arxiv.org/abs/2510.04759)
*Chi Yan,Dan Xu*

Main category: cs.CV

TL;DR: PG-Occ是一种渐进式高斯变换框架，通过在线致密化和异向性采样策略，显著提升开放词汇3D占用预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法局限于固定语义类别，而现有文本对齐方法在稀疏高斯表示和小物体捕获之间存在权衡。PG-Occ旨在解决这一局限性，实现开放词汇的3D占用预测。

Method: PG-Occ采用渐进式在线致密化和异向性感知采样策略，通过迭代增强3D高斯表示来捕获细粒度场景细节，并自适应分配不同尺度和阶段的感受野。

Result: PG-Occ在性能上实现了14.3%的mIoU相对改进，达到了最先进的水平。

Conclusion: PG-Occ框架通过渐进式高斯变换和异向性感知采样策略，显著提升了3D占用预测的性能，实现了14.3%的mIoU相对改进，为开放词汇查询提供了更精细的场景理解。

Abstract: The 3D occupancy prediction task has witnessed remarkable progress in recent
years, playing a crucial role in vision-based autonomous driving systems. While
traditional methods are limited to fixed semantic categories, recent approaches
have moved towards predicting text-aligned features to enable open-vocabulary
text queries in real-world scenes. However, there exists a trade-off in
text-aligned scene modeling: sparse Gaussian representation struggles to
capture small objects in the scene, while dense representation incurs
significant computational overhead. To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction. Our framework employs progressive
online densification, a feed-forward strategy that gradually enhances the 3D
Gaussian representation to capture fine-grained scene details. By iteratively
enhancing the representation, the framework achieves increasingly precise and
detailed scene understanding. Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture. Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method. Code and pretrained models will be
released upon publication on our project page:
https://yanchi-3dv.github.io/PG-Occ

</details>


### [46] [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing](https://arxiv.org/abs/2510.04797)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: DiT-VTON利用Diffusion Transformer优化虚拟试穿技术，通过多配置探索和数据扩展提升细节保留和鲁棒性，支持多品类和高级编辑功能。


<details>
  <summary>Details</summary>
Motivation: 电子商业的快速发展增加了对虚拟试穿（VTO）技术的需求，但现有模型在细节保留、鲁棒性、采样效率和跨品类泛化方面存在挑战。

Method: 研究探索了多种DiT配置（如上下文标记拼接、通道拼接和ControlNet集成），并在扩展数据集上训练模型以增强鲁棒性。

Result: DiT-VTON在VITON-HD数据集上超越了现有最先进方法，展示了卓越的细节保留和鲁棒性，同时在多品类数据集上表现优异。

Conclusion: DiT-VTON通过采用Diffusion Transformer（DiT）并优化图像条件配置，显著提升了虚拟试穿（VTO）技术的细节保留和鲁棒性，同时扩展了其应用范围至多品类产品（VTA）和高级图像编辑功能。

Abstract: The rapid growth of e-commerce has intensified the demand for Virtual Try-On
(VTO) technologies, enabling customers to realistically visualize products
overlaid on their own images. Despite recent advances, existing VTO models face
challenges with fine-grained detail preservation, robustness to real-world
imagery, efficient sampling, image editing capabilities, and generalization
across diverse product categories. In this paper, we present DiT-VTON, a novel
VTO framework that leverages a Diffusion Transformer (DiT), renowned for its
performance on text-conditioned image generation, adapted here for the
image-conditioned VTO task. We systematically explore multiple DiT
configurations, including in-context token concatenation, channel
concatenation, and ControlNet integration, to determine the best setup for VTO
image conditioning.
  To enhance robustness, we train the model on an expanded dataset encompassing
varied backgrounds, unstructured references, and non-garment categories,
demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also
redefines the VTO task beyond garment try-on, offering a versatile Virtual
Try-All (VTA) solution capable of handling a wide range of product categories
and supporting advanced image editing functionalities such as pose
preservation, localized editing, texture transfer, and object-level
customization. Experimental results show that our model surpasses
state-of-the-art methods on VITON-HD, achieving superior detail preservation
and robustness without reliance on additional condition encoders. It also
outperforms models with VTA and image editing capabilities on a diverse dataset
spanning thousands of product categories.

</details>


### [47] [Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors](https://arxiv.org/abs/2510.04802)
*Han Zhang,Lalithkumar Seenivasan,Jose L. Porras,Roger D. Soberanis-Mukul,Hao Ding,Hongchao Shu,Benjamin D. Killeen,Ankita Ghosh,Lonny Yarmus,Masaru Ishii,Angela Christine Argento,Mathias Unberath*

Main category: cs.CV

TL;DR: EgoSurg是首个从壁挂固定摄像头视频重构手术室团队成员动态自我中心回放的框架，无需干预临床工作流程，为沉浸式外科数据科学提供了新基础。


<details>
  <summary>Details</summary>
Motivation: 传统的手术观察方法依赖于固定视角或回忆，无法记录指导临床决策的自我中心视觉视角。固定摄像头视频虽能捕捉手术室规模的工作流程，但无法重构每个团队成员实际看到的内容，因此对影响手术安全、培训和流程优化的决策提供有限洞察。

Method: EgoSurg结合了几何驱动的神经渲染和基于扩散的视图增强技术，能够以高视觉保真度合成任意时刻的任意和自我中心视角。

Result: 在多地点手术案例和对照研究中，EgoSurg能够以高视觉质量和保真度重构特定人员的视觉字段和任意视角。

Conclusion: EgoSurg通过将现有手术室摄像头基础设施转变为可导航的动态3D记录，为沉浸式外科数据科学奠定了基础，使手术实践可以从每个角度可视化、体验和分析。

Abstract: Observing surgical practice has historically relied on fixed vantage points
or recollections, leaving the egocentric visual perspectives that guide
clinical decisions undocumented. Fixed-camera video can capture surgical
workflows at the room-scale, but cannot reconstruct what each team member
actually saw. Thus, these videos only provide limited insights into how
decisions that affect surgical safety, training, and workflow optimization are
made. Here we introduce EgoSurg, the first framework to reconstruct the
dynamic, egocentric replays for any operating room (OR) staff directly from
wall-mounted fixed-camera video, and thus, without intervention to clinical
workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based
view enhancement, enabling high-visual fidelity synthesis of arbitrary and
egocentric viewpoints at any moment. In evaluation across multi-site surgical
cases and controlled studies, EgoSurg reconstructs person-specific visual
fields and arbitrary viewpoints with high visual quality and fidelity. By
transforming existing OR camera infrastructure into a navigable dynamic 3D
record, EgoSurg establishes a new foundation for immersive surgical data
science, enabling surgical practice to be visualized, experienced, and analyzed
from every angle.

</details>


### [48] [REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis](https://arxiv.org/abs/2510.04923)
*Alec K. Peltekian,Halil Ertugrul Aktas,Gorkem Durak,Kevin Grudzinski,Bradford C. Bemiss,Carrie Richardson,Jane E. Dematte,G. R. Scott Budinger,Anthony J. Esposito,Alexander Misharin,Alok Choudhary,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: REN是一种基于解剖学先验的MoE框架，通过多模态门控和区域专家网络显著提升医学图像分类性能，尤其在间质性肺病分类中表现突出。


<details>
  <summary>Details</summary>
Motivation: 传统MoE系统缺乏医学影像领域所需的解剖学约束，而解剖结构和区域疾病异质性对病理模式有重要影响。REN旨在填补这一空白。

Method: REN利用解剖学先验训练七个专家网络，每个专家专注于特定肺叶或双侧肺组合，结合多模态门控机制（放射组学生物标志物和深度学习特征）动态优化专家贡献。

Result: REN在间质性肺病分类中表现优异，放射组学引导的集成模型平均AUC为0.8646（比基线提升12.5%），区域专家模型（如下叶）AUC达0.88-0.90，优于传统深度学习方法。

Conclusion: REN（Regional Expert Networks）通过结合解剖学先验和多模态门控机制，显著提升了医学图像分类的性能和可解释性，为结构化医学影像应用提供了可扩展的解决方案。

Abstract: Mixture-of-Experts (MoE) architectures have significantly contributed to
scalable machine learning by enabling specialized subnetworks to tackle complex
tasks efficiently. However, traditional MoE systems lack domain-specific
constraints essential for medical imaging, where anatomical structure and
regional disease heterogeneity strongly influence pathological patterns. Here,
we introduce Regional Expert Networks (REN), the first anatomically-informed
MoE framework tailored specifically for medical image classification. REN
leverages anatomical priors to train seven specialized experts, each dedicated
to distinct lung lobes and bilateral lung combinations, enabling precise
modeling of region-specific pathological variations. Multi-modal gating
mechanisms dynamically integrate radiomics biomarkers and deep learning (DL)
features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to
interstitial lung disease (ILD) classification, REN achieves consistently
superior performance: the radiomics-guided ensemble reached an average AUC of
0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC
0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe
models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)
and aligning with known disease progression patterns. Through rigorous
patient-level cross-validation, REN demonstrates strong generalizability and
clinical interpretability, presenting a scalable, anatomically-guided approach
readily extensible to other structured medical imaging applications.

</details>


### [49] [Unsupervised Active Learning via Natural Feature Progressive Framework](https://arxiv.org/abs/2510.04939)
*Yuxi Liu,Catherine Lalman,Yimin Yang*

Main category: cs.CV

TL;DR: NFPF是一种创新的无监督主动学习方法，通过SFLM量化样本重要性，显著提升性能并接近监督方法水平。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型依赖大规模人工标注数据集，但标注过程昂贵且耗时。无监督主动学习（UAL）试图减少标注负担，但现有方法性能不足。

Method: 提出自然特征渐进框架（NFPF），利用特定特征学习机（SFLM）量化样本对模型性能的贡献，并定义重构差异度量进行初始样本选择。

Result: NFPF在实验中显著优于现有UAL方法，性能接近监督AL方法，并通过消融研究和定性可视化验证了其优越性。

Conclusion: NFPF显著优于现有的无监督主动学习方法，并在视觉数据集上达到了与监督主动学习方法相当的性能。

Abstract: The effectiveness of modern deep learning models is predicated on the
availability of large-scale, human-annotated datasets, a process that is
notoriously expensive and time-consuming. While Active Learning (AL) offers a
strategic solution by labeling only the most informative and representative
data, its iterative nature still necessitates significant human involvement.
Unsupervised Active Learning (UAL) presents an alternative by shifting the
annotation burden to a single, post-selection step. Unfortunately, prevailing
UAL methods struggle to achieve state-of-the-art performance. These approaches
typically rely on local, gradient-based scoring for sample importance
estimation, which not only makes them vulnerable to ambiguous and noisy data
but also hinders their capacity to select samples that adequately represent the
full data distribution. Moreover, their use of shallow, one-shot linear
selection falls short of a true UAL paradigm. In this paper, we propose the
Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes
how sample importance is measured. At its core, NFPF employs a Specific Feature
Learning Machine (SFLM) to effectively quantify each sample's contribution to
model performance. We further utilize the SFLM to define a powerful
Reconstruction Difference metric for initial sample selection. Our
comprehensive experiments show that NFPF significantly outperforms all
established UAL methods and achieves performance on par with supervised AL
methods on vision datasets. Detailed ablation studies and qualitative
visualizations provide compelling evidence for NFPF's superior performance,
enhanced robustness, and improved data distribution coverage.

</details>


### [50] [Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion](https://arxiv.org/abs/2510.04947)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: CA3D-Diff是一个创新的乳腺X光片视图翻译框架，通过列感知和隐式3D结构重建，显著提升了视图生成质量和诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 在临床实践中，乳腺X光片的一个视图可能缺失或质量下降，影响诊断效果。视图间翻译可以帮助恢复缺失视图并改善病灶对齐，但乳腺X光片中的非刚性变形和组织重叠使得这一任务极具挑战性。

Method: 提出了一种基于条件扩散模型的双向乳腺X光片视图翻译框架CA3D-Diff，包括列感知交叉注意力机制和隐式3D结构重建模块。

Result: CA3D-Diff在双向任务中表现出色，视觉保真度和结构一致性优于现有方法。合成的视图显著提高了单视图恶性肿瘤分类的准确性。

Conclusion: CA3D-Diff通过创新的双向视图翻译框架，显著提升了乳腺X光片视图的生成质量，改善了单视图恶性肿瘤分类的准确性，展示了其在真实诊断场景中的实用价值。

Abstract: Dual-view mammography, including craniocaudal (CC) and mediolateral oblique
(MLO) projections, offers complementary anatomical views crucial for breast
cancer diagnosis. However, in real-world clinical workflows, one view may be
missing, corrupted, or degraded due to acquisition errors or compression
artifacts, limiting the effectiveness of downstream analysis. View-to-view
translation can help recover missing views and improve lesion alignment. Unlike
natural images, this task in mammography is highly challenging due to large
non-rigid deformations and severe tissue overlap in X-ray projections, which
obscure pixel-level correspondences. In this paper, we propose Column-Aware and
Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view
translation framework based on conditional diffusion model. To address
cross-view structural misalignment, we first design a column-aware
cross-attention mechanism that leverages the geometric property that
anatomically corresponding regions tend to lie in similar column positions
across views. A Gaussian-decayed bias is applied to emphasize local column-wise
correlations while suppressing distant mismatches. Furthermore, we introduce an
implicit 3D structure reconstruction module that back-projects noisy 2D latents
into a coarse 3D feature volume based on breast-view projection geometry. The
reconstructed 3D structure is refined and injected into the denoising UNet to
guide cross-view generation with enhanced anatomical awareness. Extensive
experiments demonstrate that CA3D-Diff achieves superior performance in
bidirectional tasks, outperforming state-of-the-art methods in visual fidelity
and structural consistency. Furthermore, the synthesized views effectively
improve single-view malignancy classification in screening settings,
demonstrating the practical value of our method in real-world diagnostics.

</details>


### [51] [ActiveMark: on watermarking of visual foundation models via massive activations](https://arxiv.org/abs/2510.04966)
*Anna Chistyakova,Mikhail Pautov*

Main category: cs.CV

TL;DR: 本文提出了一种通过嵌入数字水印来验证视觉基础模型所有权的方法，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型的高计算成本和数据收集难度促使模型所有者需要保护知识产权，防止非法再分发。

Method: 通过微调视觉基础模型的一小部分表达层，并结合小型编码器-解码器网络，在输入图像内部表示中嵌入数字水印。

Result: 理论分析和实验证明，该方法在水印检测中具有低误检率和漏检率。

Conclusion: 本文提出的方法通过在水印嵌入和检测过程中保持低误检率和漏检率，有效验证了视觉基础模型的所有权。

Abstract: Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

</details>


### [52] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: PaperTalker是一个自动化生成学术演示视频的多智能体框架，通过整合多模态信息和并行化处理，显著提升了视频生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 学术演示视频制作耗时且复杂，涉及多模态信息的协调，需要自动化工具来减轻负担。

Method: 提出了PaperTalker，一个多智能体框架，整合了幻灯片生成、布局优化、字幕生成、语音合成和说话人头像渲染，并通过并行化提高效率。

Result: 在Paper2Video数据集上的实验表明，PaperTalker生成的视频在忠实性和信息量上优于现有基线。

Conclusion: PaperTalker通过多智能体框架和定制的评估指标，为学术演示视频生成提供了一种高效且实用的解决方案，其生成的视频在忠实性和信息量上优于现有基线。

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [53] [Cosmological Hydrodynamics at Exascale: A Trillion-Particle Leap in Capability](https://arxiv.org/abs/2510.03557)
*Nicholas Frontiere,J. D. Emberson,Michael Buehlmann,Esteban M. Rangel,Salman Habib,Katrin Heitmann,Patricia Larsen,Vitali Morozov,Adrian Pope,Claude-André Faucher-Giguère,Antigoni Georgiadou,Damien Lebrun-Grandié,Andrey Prokopenko*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Resolving the most fundamental questions in cosmology requires simulations
that match the scale, fidelity, and physical complexity demanded by
next-generation sky surveys. To achieve the realism needed for this critical
scientific partnership, detailed gas dynamics, along with a host of
astrophysical effects, must be treated self-consistently with gravity for
end-to-end modeling of structure formation. As an important step on this
roadmap, exascale computing enables simulations that span survey-scale volumes
while incorporating key subgrid processes that shape complex cosmic structures.
We present results from CRK-HACC, a cosmological hydrodynamics code built for
the extreme scalability requirements set by modern cosmological surveys. Using
separation-of-scale techniques, GPU-resident tree solvers, in situ analysis
pipelines, and multi-tiered I/O, CRK-HACC executed Frontier-E: a four trillion
particle full-sky simulation, over an order of magnitude larger than previous
efforts. The run achieved 513.1 PFLOPs peak performance, processing 46.6
billion particles per second and writing more than 100 PB of data in just over
one week of runtime.

</details>


### [54] [Datacenter Energy Optimized Power Profiles](https://arxiv.org/abs/2510.03872)
*Sreedhar Narayanaswamy,Pratikkumar Dilipkumar Patel,Ian Karlin,Apoorv Gupta,Sudhir Saripalli,Janey Guo*

Main category: cs.DC

TL;DR: NVIDIA Blackwell B200的datacenter power profiles功能通过智能电源管理优化HPC和AI工作负载，实现了15%的能源节省和13%的吞吐量提升，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 提高数据中心在严格电力约束下的能源效率和/或性能。

Method: 结合硬件和软件创新，提供粗粒度的用户控制，针对HPC和AI工作负载进行智能电源管理。

Result: 初步实现阶段达到了高达15%的能源节省，同时关键应用性能保持在97%以上，整体吞吐量提升高达13%。

Conclusion: Blackwell B200的datacenter power profiles功能通过智能电源管理和领域知识优化，在保持关键应用性能的同时，实现了高达15%的能源节省和13%的整体吞吐量提升。

Abstract: This paper presents datacenter power profiles, a new NVIDIA software feature
released with Blackwell B200, aimed at improving energy efficiency and/or
performance. The initial feature provides coarse-grain user control for HPC and
AI workloads leveraging hardware and software innovations for intelligent power
management and domain knowledge of HPC and AI workloads. The resulting
workload-aware optimization recipes maximize computational throughput while
operating within strict facility power constraints. The phase-1 Blackwell
implementation achieves up to 15% energy savings while maintaining performance
levels above 97% for critical applications, enabling an overall throughput
increase of up to 13% in a power-constrained facility.
  KEYWORDS GPU power management, energy efficiency, power profile, HPC
optimization, Max-Q, Blackwell architecture

</details>


### [55] [Toward Co-adapting Machine Learning Job Shape and Cluster Topology](https://arxiv.org/abs/2510.03891)
*Shawn Shuoshuo Chen,Daiyaan Arfeen,Minlan Yu,Peter Steenkiste,Srinivasan Seshan*

Main category: cs.DC

TL;DR: RFold 通过动态调整作业形状和集群拓扑，同时优化网络争用和集群利用率，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 在多租户环面拓扑集群中，为分布式机器学习作业分配资源需要满足每个作业的特定放置和通信需求，而现有调度器通常只能优化一个目标（如最小化网络争用或最大化集群利用率）而牺牲另一个。

Method: RFold 结合了两种技术：(1) 识别支持作业通信需求的同构作业形状，(2) 重新配置基于光路交换的拓扑结构以支持更多样化的作业形状。

Result: 初步评估显示，RFold 在 4096 节点环面集群模拟器中，可将绝对集群利用率提高 57%，并将作业完成时间缩短至多 11 倍。

Conclusion: RFold 方法通过动态调整作业形状和集群拓扑结构，成功实现了同时最小化网络争用和最大化集群利用率的目标。

Abstract: Allocating resources to distributed machine learning jobs in multi-tenant
torus-topology clusters must meet each job's specific placement and
communication requirements, which are typically described using shapes. There
is an inherent tension between minimizing network contention and maximizing
cluster utilization when placing various-shaped jobs. While existing schedulers
typically optimize for one objective at the expense of the other, we
demonstrate that both can be achieved simultaneously.
  Our proposed approach, RFold, adapts both job shapes and the underlying
cluster topology at runtime. This is accomplished by combining two techniques:
(1) identifying homomorphic job shapes that support the jobs communication
needs, and (2) reconfiguring the optical circuit switch-enabled topology to
support more diverse job shapes. Preliminary evaluation performed on a
4096-node torus cluster simulator indicates that RFold can improve absolute
cluster utilization by 57% and reduce job completion time by up to 11x relative
to existing methods

</details>


### [56] [Towards Carbon-Aware Container Orchestration: Predicting Workload Energy Consumption with Federated Learning](https://arxiv.org/abs/2510.03970)
*Zainab Saad,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.DC

TL;DR: 提出一种联邦学习框架，通过分布式协作训练XGBoost模型优化能源消耗预测，保护隐私且降低误差。


<details>
  <summary>Details</summary>
Motivation: 大规模数据中心的资源密集型工作负载增加了全球碳足迹，现有方法依赖集中式机器学习模型，存在隐私问题且难以泛化。

Method: 扩展Kubernetes高效功率级别导出器（Kepler），采用Flower的FedXgbBagging聚合策略，在分布式客户端上协作训练XGBoost模型。

Result: 在SPECPower基准数据集上的实验表明，基于联邦学习的方法比集中式基线的平均绝对误差降低了11.7%。

Conclusion: 本文提出了一种基于联邦学习的能源消耗预测方法，通过在分布式客户端上协作训练XGBoost模型，既保护了数据隐私，又提高了预测效率，为可持续云计算提供了可行路径。

Abstract: The growing reliance on large-scale data centers to run resource-intensive
workloads has significantly increased the global carbon footprint, underscoring
the need for sustainable computing solutions. While container orchestration
platforms like Kubernetes help optimize workload scheduling to reduce carbon
emissions, existing methods often depend on centralized machine learning models
that raise privacy concerns and struggle to generalize across diverse
environments. In this paper, we propose a federated learning approach for
energy consumption prediction that preserves data privacy by keeping sensitive
operational data within individual enterprises. By extending the Kubernetes
Efficient Power Level Exporter (Kepler), our framework trains XGBoost models
collaboratively across distributed clients using Flower's FedXgbBagging
aggregation using a bagging strategy, eliminating the need for centralized data
sharing. Experimental results on the SPECPower benchmark dataset show that our
FL-based approach achieves 11.7 percent lower Mean Absolute Error compared to a
centralized baseline. This work addresses the unresolved trade-off between data
privacy and energy prediction efficiency in prior systems such as Kepler and
CASPER and offers enterprises a viable pathway toward sustainable cloud
computing without compromising operational privacy.

</details>


### [57] [From Patchwork to Network: A Comprehensive Framework for Demand Analysis and Fleet Optimization of Urban Air Mobility](https://arxiv.org/abs/2510.04186)
*Xuan Jiang,Xuanyu Zhou,Yibo Zhao,Shangqing Cao,Jinhua Zhao,Mark Hansen,Raja Sengupta*

Main category: cs.DC

TL;DR: 通过LPSim框架优化UAM网络，旧金山湾区案例显示显著旅行时间节省，但需与地面交通无缝集成。


<details>
  <summary>Details</summary>
Motivation: 解决城市空中交通（UAM）实施中的基础设施成本和操作复杂性挑战。

Method: 引入LPSim框架，利用多GPU计算进行大规模并行模拟，扩展均衡搜索算法以预测需求并优化机队组成。

Result: 在旧金山湾区案例中，UAM模型为23万次选定行程节省了超过20分钟的旅行时间。

Conclusion: UAM模型的成功关键在于与地面交通的无缝集成和动态调度，尽管在旧金山湾区案例中展示了显著的旅行时间节省。

Abstract: Urban Air Mobility (UAM) presents a transformative vision for metropolitan
transportation, but its practical implementation is hindered by substantial
infrastructure costs and operational complexities. We address these challenges
by modeling a UAM network that leverages existing regional airports and
operates with an optimized, heterogeneous fleet of aircraft. We introduce
LPSim, a Large-Scale Parallel Simulation framework that utilizes multi-GPU
computing to co-optimize UAM demand, fleet operations, and ground
transportation interactions simultaneously. Our equilibrium search algorithm is
extended to accurately forecast demand and determine the most efficient fleet
composition. Applied to a case study of the San Francisco Bay Area, our results
demonstrate that this UAM model can yield over 20 minutes' travel time savings
for 230,000 selected trips. However, the analysis also reveals that system-wide
success is critically dependent on seamless integration with ground access and
dynamic scheduling.

</details>


### [58] [Beyond Canonical Rounds: Communication Abstractions for Optimal Byzantine Resilience](https://arxiv.org/abs/2510.04310)
*Hagit Attiya,Itay Flam,Jennifer L. Welch*

Main category: cs.DC

TL;DR: 经典异步通信模式在最优容错范围内存在局限，但gather抽象提供了更好的模块化设计基础。


<details>
  <summary>Details</summary>
Motivation: 探索在异步拜占庭容错系统中，如何设计具有最优容错能力（n > 3f）的通信抽象。

Method: 研究分析了两种经典模式（规范异步轮次和通信封闭层）在关键容错范围（3f < n ≤ 5f）内的局限性，并提出了gather抽象作为替代方案。

Result: 发现经典模式在关键容错范围内存在局限性，但gather抽象支持恒定时间解决方案和模块化归约，并首次实现了连接共识的最优容错算法。

Conclusion: 研究表明，基于轮次的抽象虽然在分析上方便，但掩盖了拜占庭容错算法的真正复杂性。更丰富的通信模式（如gather）为模块化、最优容错设计提供了更好的基础。

Abstract: We study communication abstractions for asynchronous Byzantine fault
tolerance with optimal failure resilience, where $n > 3f$. Two classic patterns
-- canonical asynchronous rounds and communication-closed layers -- have long
been considered as general frameworks for designing distributed algorithms,
making asynchronous executions appear synchronous and enabling modular
reasoning.
  We show that these patterns are inherently limited in the critical resilience
regime $3f < n \le 5f$. Several key tasks -- such as approximate and crusader
agreement, reliable broadcast and gather -- cannot be solved by bounded-round
canonical-round algorithms, and are unsolvable if communication closure is
imposed. These results explain the historical difficulty of achieving
optimal-resilience algorithms within round-based frameworks.
  On the positive side, we show that the gather abstraction admits
constant-time solutions with optimal resilience ($n > 3f$), and supports
modular reductions. Specifically, we present the first optimally-resilient
algorithm for connected consensus by reducing it to gather.
  Our results demonstrate that while round-based abstractions are analytically
convenient, they obscure the true complexity of Byzantine fault-tolerant
algorithms. Richer communication patterns such as gather provide a better
foundation for modular, optimal-resilience design.

</details>


### [59] [Next-Generation Event-Driven Architectures: Performance, Scalability, and Intelligent Orchestration Across Messaging Frameworks](https://arxiv.org/abs/2510.04404)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Ahsan Habib Tareq*

Main category: cs.DC

TL;DR: 本文首次对12种消息系统进行标准化基准测试，提出AIEO框架，显著提升性能，为分布式系统设计提供新方向。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统对低延迟、容错事件处理的需求超出了传统消息架构的限制，但缺乏对这些系统的统一比较研究。

Method: 引入AIEO框架，结合机器学习驱动的预测扩展、强化学习的动态资源分配和多目标优化，对12种消息系统在三种代表性工作负载下进行标准化基准测试。

Result: AIEO展示了34%的平均延迟降低、28%的资源利用率提升和42%的成本优化。Apache Kafka在峰值吞吐量上表现最佳，而Apache Pulsar在多租户方面表现优越，无服务器解决方案在弹性扩展方面表现突出。

Conclusion: 本文提出了AIEO（AI增强事件编排）框架，通过机器学习驱动的预测扩展、强化学习的动态资源分配和多目标优化，显著提升了事件处理性能，为下一代分布式系统设计奠定了基础。

Abstract: Modern distributed systems demand low-latency, fault-tolerant event
processing that exceeds traditional messaging architecture limits. While
frameworks including Apache Kafka, RabbitMQ, Apache Pulsar, NATS JetStream, and
serverless event buses have matured significantly, no unified comparative study
evaluates them holistically under standardized conditions. This paper presents
the first comprehensive benchmarking framework evaluating 12 messaging systems
across three representative workloads: e-commerce transactions, IoT telemetry
ingestion, and AI inference pipelines. We introduce AIEO (AI-Enhanced Event
Orchestration), employing machine learning-driven predictive scaling,
reinforcement learning for dynamic resource allocation, and multi-objective
optimization. Our evaluation reveals fundamental trade-offs: Apache Kafka
achieves peak throughput (1.2M messages/sec, 18ms p95 latency) but requires
substantial operational expertise; Apache Pulsar provides balanced performance
(950K messages/sec, 22ms p95) with superior multi-tenancy; serverless solutions
offer elastic scaling for variable workloads despite higher baseline latency
(80-120ms p95). AIEO demonstrates 34\% average latency reduction, 28\% resource
utilization improvement, and 42% cost optimization across all platforms. We
contribute standardized benchmarking methodologies, open-source intelligent
orchestration, and evidence-based decision guidelines. The evaluation
encompasses 2,400+ experimental configurations with rigorous statistical
analysis, providing comprehensive performance characterization and establishing
foundations for next-generation distributed system design.

</details>


### [60] [The R(1)W(1) Communication Model for Self-Stabilizing Distributed Algorithms](https://arxiv.org/abs/2510.04644)
*Hirotsugu Kakugawa,Sayaka Kamei,Masahiro Shibata,Fukuhito Ooshita*

Main category: cs.DC

TL;DR: 本文提出R(1)W(1)模型及自稳定算法，解决了最大匹配等问题，并实现了模型间模拟。


<details>
  <summary>Details</summary>
Motivation: 自稳定是设计容错分布式算法的重要方法，尤其适用于现代大规模分布式系统。R(1)W(1)模型的提出旨在提供更高效的通信和执行方式。

Method: 提出了R(1)W(1)模型，其中每个进程可以在单一步骤中读写自己和邻居的局部变量。基于该模型设计了自稳定分布式算法，并提出了一个示例转换器用于模型间模拟。

Result: 成功设计了R(1)W(1)模型下的自稳定算法，并实现了模型间的算法模拟。

Conclusion: 本文提出了一种新的通信和执行模型R(1)W(1)，并基于该模型设计了自稳定分布式算法，用于解决最大匹配、最小k-支配集和最大k-依赖集问题。此外，还提出了一个基于随机距离二局部互斥的示例转换器，用于在同步消息传递模型中模拟R(1)W(1)模型设计的算法。

Abstract: Self-stabilization is a versatile methodology in the design of fault-tolerant
distributed algorithms for transient faults. A self-stabilizing system
automatically recovers from any kind and any finite number of transient faults.
This property is specifically useful in modern distributed systems with a large
number of components. In this paper, we propose a new communication and
execution model named the R(1)W(1) model in which each process can read and
write its own and neighbors' local variables in a single step. We propose
self-stabilizing distributed algorithms in the R(1)W(1) model for the problems
of maximal matching, minimal k-dominating set and maximal k-dependent set.
Finally, we propose an example transformer, based on randomized distance-two
local mutual exclusion, to simulate algorithms designed for the R(1)W(1) model
in the synchronous message passing model with synchronized clocks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [61] [WAREX: Web Agent Reliability Evaluation on Existing Benchmarks](https://arxiv.org/abs/2510.03285)
*Su Kara,Fazle Faisal,Suman Nath*

Main category: cs.AI

TL;DR: WAREX是一种评估浏览器代理在真实网络环境中鲁棒性的方法，实验显示现有代理在模拟不稳定环境时任务成功率大幅下降。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试在受控环境中进行，无法反映真实网络环境中的不稳定性（如网络问题、网站攻击等），因此需要一种更贴近实际的评估方法。

Method: 通过在现有基准（WebArena、WebVoyager和REAL）中引入WAREX，模拟真实网络环境的不稳定性（如网络问题、HTTPS连接问题和网站攻击）来评估代理性能。

Result: 引入WAREX后，任务成功率显著下降，表明现有代理在真实环境中的鲁棒性有限。

Conclusion: WAREX评估揭示了现有浏览器代理在真实网络环境中的鲁棒性不足，任务成功率显著下降。

Abstract: Recent advances in browser-based LLM agents have shown promise for automating
tasks ranging from simple form filling to hotel booking or online shopping.
Current benchmarks measure agent performance in controlled environments, such
as containers or stable networks, where websites behave deterministically.
However, in the real world, users access websites over networks and HTTPS
connections that introduce instability from multiple sources: client-side,
server-side issues or broader system failures. Moreover, live websites are
prone to web attacks such Cross-Site Scripting, as well as general site
modifications which can cause unexpected or malicious pop-ups or improper
functionality. To address this gap, we present WAREX: Web Agent Reliability
Evaluation on Existing Benchmarks. We measure the impact of WAREX across three
popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that
introducing WAREX leads to significant drops in task success rates,
highlighting the limited robustness of state-of-the-art agents.

</details>


### [62] [Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints](https://arxiv.org/abs/2510.03377)
*Ahmed Missaoui,Cemalettin Ozturk,Barry O'Sullivan*

Main category: cs.AI

TL;DR: 研究提出多目标优化方法和RIPG算法，有效解决制造业中的能源效率调度问题。


<details>
  <summary>Details</summary>
Motivation: 不可再生能源稀缺、供应链地缘政治问题、价格上涨及气候变化迫使制造业寻求更节能的解决方案。

Method: 采用增强的epsilon约束方法和多目标元启发式算法RIPG来解决BHFS问题。

Result: 通过小、中、大规模实例验证，所提方法在平衡完工时间和能源消耗方面优于现有算法。

Conclusion: 研究提出的多目标混合整数规划模型和RIPG算法在解决BHFS问题上表现出高效性，能够有效平衡完工时间和能源消耗。

Abstract: The scarcity of non-renewable energy sources, geopolitical problems in its
supply, increasing prices, and the impact of climate change, force the global
economy to develop more energy-efficient solutions for their operations. The
Manufacturing sector is not excluded from this challenge as one of the largest
consumers of energy. Energy-efficient scheduling is a method that attracts
manufacturing companies to reduce their consumption as it can be quickly
deployed and can show impact immediately. In this study, the hybrid flow shop
scheduling problem with blocking constraint (BHFS) is investigated in which we
seek to minimize the latest completion time (i.e. makespan) and overall energy
consumption, a typical manufacturing setting across many industries from
automotive to pharmaceutical. Energy consumption and the latest completion time
of customer orders are usually conflicting objectives. Therefore, we first
formulate the problem as a novel multi-objective mixed integer programming
(MIP) model and propose an augmented epsilon-constraint method for finding the
Pareto-optimal solutions. Also, an effective multi-objective metaheuristic
algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large
instances in reasonable time. Our proposed methods are benchmarked using small,
medium, and large-size instances to evaluate their efficiency. Two well-known
algorithms are adopted for comparing our novel approaches. The computational
results show the effectiveness of our method.

</details>


### [63] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: 研究发现当代大型语言模型在自我识别任务中表现不佳，仅少数模型能正确识别自身生成文本，且存在对GPT和Claude家族的حلة偏见。这对AI安全性提出了挑战，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 受到模型是否具备自我识别能力的矛盾 strong 解释的启发，研究旨在提供一个可轻松应用和更新的评估框架。

Method: 引入了一个系统评估框架，测量了10个ంద当代大型语言模型在两项任务中的表现：二元自我识别和精确模型预测。

Result: 结果显示，模型在自我识别上表现 consistently 失败，仅4/10模型能正确预测自身生成文本，且性能 rarely 高于随机 chance。模型对GPT和Claude家族表现出 strong 偏见。

Conclusion: 研究结果强调了当前大型语言模型在自我识别能力上的不足，这对AI安全性提出了重要挑战，并指出了未来开发适当AI自我意识的方向。

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [64] [ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection](https://arxiv.org/abs/2510.03418)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji,Nand Dave,Anudha Mittal*

Main category: cs.AI

TL;DR: 该论文提出了ContraGen框架，用于解决企业文档中的矛盾检测问题，通过合成数据和人工验证提升RAG系统的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的矛盾检测基准仅限于句子级分析，无法捕捉企业文档（如合同、财务文件、合规报告或政策手册）的复杂性，这在企业环境中尤为重要，因为合规性、治理和问责制至关重要。

Method: 提出了ContraGen，一个针对企业领域的矛盾感知基准框架，通过生成带有嵌入矛盾的合成企业风格文档，结合自动化矛盾挖掘和人工验证，以系统评估文档内和跨文档的一致性。

Result: ContraGen框架能够生成逼真的企业文档，建模业务过程中常见的矛盾类型分类法，并开发了一个矛盾感知的检索评估流程，结合人工监督以反映特定领域的判断复杂性。

Conclusion: 该研究为在企业信息检索应用中建立更可信和负责任的RAG系统奠定了基础，其中检测和解决矛盾对于降低风险和确保合规性至关重要。

Abstract: Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,
offering advanced capabilities for information access and decision-making.
However, contradictions in retrieved evidence can result in inconsistent or
untrustworthy outputs, which is especially problematic in enterprise settings
where compliance, governance, and accountability are critical. Existing
benchmarks for contradiction detection are limited to sentence-level analysis
and do not capture the complexity of enterprise documents such as contracts,
financial filings, compliance reports, or policy manuals. To address this
limitation, we propose ContraGen, a contradiction-aware benchmark framework
tailored to enterprise domain. The framework generates synthetic
enterprise-style documents with embedded contradictions, enabling systematic
evaluation of both intra-document and cross-document consistency. Automated
contradiction mining is combined with human-in-the-loop validation to ensure
high accuracy. Our contributions include generating realistic enterprise
documents, modeling a taxonomy of contradiction types common in business
processes, enabling controlled creation of self- and pairwise contradictions,
developing a contradiction-aware retrieval evaluation pipeline and embedding
human oversight to reflect domain-specific judgment complexity. This work
establishes a foundation for more trustworthy and accountable RAG systems in
enterprise information-seeking applications, where detecting and resolving
contradictions is essential for reducing risk and ensuring compliance.

</details>


### [65] [A Qualitative Comparative Evaluation of Cognitive and Generative Theories](https://arxiv.org/abs/2510.03453)
*Paul S. Rosenbloom*

Main category: cs.AI

TL;DR: 本文通过定性比较认知架构和生成神经架构，提出了一个全面的理论评估方法。


<details>
  <summary>Details</summary>
Motivation: 评估基于认知架构和生成神经架构的理论具有挑战性，本文旨在探索一种更全面的评估方法。

Method: 本文采用了一种宽泛的理论评估视角，对面向全脑的认知架构和生成架构及其完整系统进行了定性比较。

Result: 研究提供了一个宽泛的定性比较框架，用于评估不同类型的心理架构及其系统。

Conclusion: 通过广泛的定性比较，本文强调了评估认知架构和生成神经架构的重要性，并提出了一个全面的视角来应对这一挑战。

Abstract: Evaluation is a critical activity associated with any theory. Yet this has
proven to be an exceptionally challenging activity for theories based on
cognitive architectures. For an overlapping set of reasons, evaluation can also
be challenging for theories based on generative neural architectures. This dual
challenge is approached here by leveraging a broad perspective on theory
evaluation to yield a wide-ranging, albeit qualitative, comparison of
whole-mind-oriented cognitive and generative architectures and the full systems
that are based on these architectures.

</details>


### [66] [Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification](https://arxiv.org/abs/2510.03469)
*Keshav Ramani,Vali Tawosi,Salwa Alamir,Daniel Borrajo*

Main category: cs.AI

TL;DR: 提出一种新框架，通过LLMs将自然语言计划转换为Kripke结构和LTL进行模型检查，实验显示GPT-5在分类和形式化表示方面表现优异，但语义完美模型仍需改进。


<details>
  <summary>Details</summary>
Motivation: 评估自然语言计划与其预期行为之间的一致性，以验证计划的正确性。

Method: 通过将自然语言计划转换为Kripke结构和线性时序逻辑（LTL），并利用大型语言模型（LLMs）进行模型检查。

Result: GPT-5在PlanBench计划验证数据集上表现出色，分类性能F1得分为96.3%，且几乎总能生成语法完美的形式化表示。

Conclusion: 虽然GPT-5在分类性能和生成形式化表示方面表现优异，但语义完美形式化模型的合成仍需未来探索。

Abstract: We introduce a novel framework for evaluating the alignment between natural
language plans and their expected behavior by converting them into Kripke
structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)
and performing model checking. We systematically evaluate this framework on a
simplified version of the PlanBench plan verification dataset and report on
metrics like Accuracy, Precision, Recall and F1 scores. Our experiments
demonstrate that GPT-5 achieves excellent classification performance (F1 score
of 96.3%) while almost always producing syntactically perfect formal
representations that can act as guarantees. However, the synthesis of
semantically perfect formal models remains an area for future exploration.

</details>


### [67] [Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection](https://arxiv.org/abs/2510.03485)
*Xiaofei Wen,Wenjie Jacky Mo,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: 研究提出PolicyGuardBench基准和PolicyGuard-4B模型，用于检测网络代理轨迹中的政策违规，展示了在小规模下实现高效护栏的可行性。


<details>
  <summary>Details</summary>
Motivation: 目前很少有研究关注网络代理生成的长期轨迹是否符合外部或人为指定的政策，以及政策违规是否在不同上下文（如领域和子域）中持续存在。

Method: 通过从多样化的代理运行中生成广泛的政策集，并创建带有违规标签的子域内和跨子域配对，构建了PolicyGuardBench。此外，还训练了PolicyGuard-4B，一个轻量级的护栏模型。

Result: PolicyGuard-4B在所有任务中表现出强大的检测准确性，同时在未见过的设置中保持高准确性。

Conclusion: PolicyGuardBench和PolicyGuard-4B为研究网络代理轨迹中的政策合规性提供了首个全面框架，并表明在小规模下实现准确且可推广的护栏是可行的。

Abstract: Autonomous web agents need to operate under externally imposed or
human-specified policies while generating long-horizon trajectories. However,
little work has examined whether these trajectories comply with such policies,
or whether policy violations persist across different contexts such as domains
(e.g., shopping or coding websites) and subdomains (e.g., product search and
order management in shopping). To address this gap, we introduce
PolicyGuardBench, a benchmark of about 60k examples for detecting policy
violations in agent trajectories. From diverse agent runs, we generate a broad
set of policies and create both within subdomain and cross subdomain pairings
with violation labels. In addition to full-trajectory evaluation,
PolicyGuardBench also includes a prefix-based violation detection task where
models must anticipate policy violations from truncated trajectory prefixes
rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a
lightweight guardrail model that delivers strong detection accuracy across all
tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes
across domains and preserves high accuracy on unseen settings. Together,
PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework
for studying policy compliance in web agent trajectories, and show that
accurate and generalizable guardrails are feasible at small scales.

</details>


### [68] [OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows](https://arxiv.org/abs/2510.03506)
*John Nguyen,Marton Havasi,Tariq Berrada,Luke Zettlemoyer,Ricky T. Q. Chen*

Main category: cs.AI

TL;DR: OneFlow 是首个支持并发混合模态生成的非自回归模型，性能优于传统方法，训练更高效。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型在文本和图像生成之间强制严格的因果顺序，限制了生成效率和灵活性。OneFlow 旨在解决这一问题，实现更高效的并发生成。

Method: OneFlow 结合了基于插入的 Edit Flow（用于离散文本标记）和 Flow Matching（用于图像潜在表示），通过分层采样实现并发文本图像合成。

Result: 实验表明，OneFlow 在 1B 到 8B 模型规模上均优于自回归基线，生成和理解任务表现更佳，训练 FLOPs 减少高达 50%。

Conclusion: OneFlow 是一种创新的非自回归多模态模型，支持变长和并发混合模态生成，性能优于自回归和基于扩散的方法，同时减少了训练计算量。

Abstract: We present OneFlow, the first non-autoregressive multimodal model that
enables variable-length and concurrent mixed-modal generation. Unlike
autoregressive models that enforce rigid causal ordering between text and image
generation, OneFlow combines an insertion-based Edit Flow for discrete text
tokens with Flow Matching for image latents. OneFlow enables concurrent
text-image synthesis with hierarchical sampling that prioritizes content over
grammar. Through controlled experiments across model sizes from 1B to 8B, we
demonstrate that OneFlow outperforms autoregressive baselines on both
generation and understanding tasks while using up to 50% fewer training FLOPs.
OneFlow surpasses both autoregressive and diffusion-based approaches while
unlocking new capabilities for concurrent generation, iterative refinement, and
natural reasoning-like generation.

</details>


### [69] [Speculative Actions: A Lossless Framework for Faster Agentic Systems](https://arxiv.org/abs/2510.04371)
*Naimeng Ye,Arnav Ahuja,Georgios Liargkovas,Yunan Lu,Kostis Kaffes,Tianyi Peng*

Main category: cs.AI

TL;DR: 提出了推测动作框架，通过并行执行预测动作显著降低AI代理的延迟，并在多个环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: AI代理在环境中的执行速度较慢，阻碍了训练、评估和部署。例如，两个最先进的国际象棋代理之间的对弈可能需要数小时。

Method: 提出了推测动作框架，利用快速模型预测可能的动作，支持并行执行多个步骤。

Result: 在游戏、电子商务、网页搜索等环境中，推测动作实现了高达55%的下一个动作预测准确率，显著降低了端到端延迟。

Conclusion: 推测动作框架在多个代理环境中显著降低了端到端延迟，并通过更强的猜测模型、多步推测等技术进一步提升了性能，为实际部署低延迟代理系统开辟了有前景的路径。

Abstract: Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

</details>


### [70] [Understanding the Role of Training Data in Test-Time Scaling](https://arxiv.org/abs/2510.03605)
*Adel Javanmard,Baharan Mirzasoleiman,Vahab Mirrokni*

Main category: cs.AI

TL;DR: 测试时扩展通过增加计算提升LLMs的推理能力，但需训练数据多样且任务难度适中，否则可能适得其反。


<details>
  <summary>Details</summary>
Motivation: 探究测试时扩展在大型语言模型推理能力提升中的作用，特别是在长链式思考（CoTs）出现及其性能改进的条件。

Method: 通过分析在线性回归任务中训练的变换模型，研究测试时扩展的性能。

Result: 增加测试时计算可以减少训练提示中的上下文示例数量；若训练数据中缺乏解决下游任务所需的技能，增加计算会损害性能；任务难度通过特征协方差矩阵的最小特征值表征。

Conclusion: 训练数据的多样性和任务难度对测试时扩展的性能至关重要，特别是在任务特征协方差矩阵的最小特征值较大时表现最佳。

Abstract: Test-time scaling improves the reasoning capabilities of large language
models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts
(CoTs). This enables models to tackle more complex problem by breaking them
down into additional steps, backtracking, and correcting mistakes. Despite its
strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions
in the training data under which long CoTs emerge, and when such long CoTs
improve the performance, remain unclear. In this paper, we study the
performance of test-time scaling for transformers trained on an in-context
weight prediction task for linear regression. Our analysis provides a
theoretical explanation for several intriguing observations: First, at any
fixed test error, increasing test-time compute allows us to reduce the number
of in-context examples (context length) in training prompts. Second, if the
skills required to solve a downstream task are not sufficiently present in the
training data, increasing test-time compute can harm performance. Finally, we
characterize task hardness via the smallest eigenvalue of its feature
covariance matrix and show that training on a diverse, relevant, and hard set
of tasks results in best performance for test-time scaling. We confirm our
findings with experiments on large, nonlinear transformer architectures.

</details>


### [71] [Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits](https://arxiv.org/abs/2510.04952)
*Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: 提出了一种结合强化学习和合规代理的算法交易系统，在模拟环境中验证了其高效性和合规性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决算法交易中执行质量与合规执行的平衡问题，特别是在多市场环境下，确保交易行为符合监管要求。

Method: 系统架构包括高级规划器、强化学习执行代理和独立合规代理。执行代理通过近端策略优化进行训练，运行时动作屏蔽确保动作安全。合规审计层采用零知识证明技术，确保合规性验证不泄露专有信号。

Result: 在ABIDES模拟器中评估，学习策略显著减少了执行落差和方差，且在各种压力测试中未出现约束违规。结果在95%置信水平下具有统计显著性。

Conclusion: 该论文提出了一种跨市场算法交易系统，通过结合强化学习执行代理和独立合规代理，实现了执行质量与严格合规执行的平衡。系统在模拟环境中表现出色，减少了执行落差和方差，且未观察到约束违规。

Abstract: We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

</details>


### [72] [Cross-Modal Content Optimization for Steering Web Agent Preferences](https://arxiv.org/abs/2510.03612)
*Tanqiu Jiang,Min Bai,Nikolaos Pappas,Yanjun Qi,Sandesh Swamy*

Main category: cs.AI

TL;DR: 该论文提出了一种跨模态偏好引导（CPS）方法，通过联合优化视觉和文本修改来操纵代理决策，在现实黑盒设置下显著优于基线方法，并保持高隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有研究要么假设强大的白盒访问权限，要么使用不切实际的设置，无法有效应对多模态攻击。因此，研究旨在在现实的黑盒威胁设置下，通过联合利用视觉和文本通道来操纵代理偏好。

Method: 提出了一种名为跨模态偏好引导（CPS）的方法，联合优化视觉和自然语言描述的不可察觉修改，利用CLIP可转移图像扰动和RLHF引起的语言偏见来引导代理决策。

Result: CPS在所有模型中均显著优于基线方法，同时保持70%更低的检测率，证明了其有效性和隐蔽性。

Conclusion: 该研究强调了随着代理系统在社会中扮演越来越重要的角色，迫切需要开发鲁棒的防御机制。

Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes
selection tasks like content recommendation or product ranking by combining
multimodal perception with preference reasoning. Recent studies reveal that
these agents are vulnerable against attackers who can bias selection outcomes
through preference manipulations using adversarial pop-ups, image
perturbations, or content tweaks. Existing work, however, either assumes strong
white-box access, with limited single-modal perturbations, or uses impractical
settings. In this paper, we demonstrate, for the first time, that joint
exploitation of visual and textual channels yields significantly more powerful
preference manipulations under realistic attacker capabilities. We introduce
Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible
modifications to an item's visual and natural language descriptions, exploiting
CLIP-transferable image perturbations and RLHF-induced linguistic biases to
steer agent decisions. In contrast to prior studies that assume gradient
access, or control over webpages, or agent memory, we adopt a realistic
black-box threat setup: a non-privileged adversary can edit only their own
listing's images and textual metadata, with no insight into the agent's model
internals. We evaluate CPS on agents powered by state-of-the-art proprietary
and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both
movie selection and e-commerce tasks. Our results show that CPS is
significantly more effective than leading baseline methods. For instance, our
results show that CPS consistently outperforms baselines across all models
while maintaining 70% lower detection rates, demonstrating both effectiveness
and stealth. These findings highlight an urgent need for robust defenses as
agentic systems play an increasingly consequential role in society.

</details>


### [73] [MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information](https://arxiv.org/abs/2510.03632)
*Jiaxi Li,Yucheng Shi,Jin Lu,Ninghao Liu*

Main category: cs.AI

TL;DR: MITS是一种基于信息论的新型树搜索框架，通过PMI评分和动态采样策略，高效提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决树搜索方法在即时可靠评估中间推理步骤质量和计算成本高的问题。

Method: MITS框架引入了基于点互信息（PMI）的评分函数，通过束搜索扩展搜索树，无需昂贵的超前模拟，并结合基于熵的动态采样策略，自适应分配计算资源。

Result: 在多样化的推理基准测试中，MITS始终优于基线方法。

Conclusion: MITS通过信息论原则指导推理，提出了一种基于点互信息的有效评分函数，实现了推理路径的逐步评估和搜索树的高效扩展，同时保持了计算效率。

Abstract: Tree search has become as a representative framework for test-time reasoning
with large language models (LLMs), exemplified by methods such as
Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning
paths. However, it remains difficult to provide instant and reliable
quantitative assessments of intermediate reasoning step quality, and extensive
path exploration is computationally costly. To address this, we propose Mutual
Information Tree Search (MITS), a novel framework that guides reasoning with
information-theoretic principles. MITS introduces an effective scoring function
based on pointwise mutual information (PMI), which enables step-wise evaluation
of reasoning paths and search tree expansion via beam search without expensive
look-ahead simulations, achieving superior reasoning performances while
maintaining computational efficiency. The framework is complemented by an
entropy-based dynamic sampling strategy that adaptively allocates computational
resources to uncertain reasoning steps where exploration is most beneficial.
For final prediction, MITS employs a weighted voting scheme that combines PMI
scores with prediction consensus. Through comprehensive experiments on diverse
reasoning benchmarks, MITS consistently surpasses baseline methods,
establishing a principled and efficient framework for LLM reasoning.

</details>


### [74] [Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs](https://arxiv.org/abs/2510.03680)
*Bumjun Kim,Dongjae Jeon,Dueun Kim,Wonje Jeung,Albert No*

Main category: cs.AI

TL;DR: Rainbow Padding 通过替换重复 <eos> 为不同填充标记，解决了扩散大语言模型的 <eos> 溢出问题，显著提升输出质量。


<details>
  <summary>Details</summary>
Motivation: 指令调优的扩散大语言模型存在 <eos> 溢出的关键漏洞，导致响应长度随序列长度增加而缩短，影响输出质量。

Method: 引入 Rainbow Padding，用不同填充标记的循环替换重复的 <eos> 占位符，并通过 LoRA 微调在现有模型上高效集成。

Result: 实验表明，Rainbow Padding 显著提高了长度鲁棒性和输出质量，仅需七个填充标记即可防止提前终止。

Conclusion: Rainbow Padding 是一种简单有效的解决方案，通过替换重复的 <eos> 占位符为不同填充标记的循环，显著提高了扩散大语言模型的长度鲁棒性和输出质量。

Abstract: Diffusion large language models (dLLMs) have emerged as a promising
alternative to autoregressive models, offering flexible generation orders and
strong performance on complex reasoning tasks. However, instruction-tuned dLLMs
exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated
sequence length increases, responses paradoxically become shorter, collapsing
into early termination or degenerating into streams of \texttt{<eos>} tokens.
Although noticed in practice, this issue has not been systematically analyzed.
We trace its root cause to the dual role of \texttt{<eos>} as both termination
and padding, which concentrates probability mass on \texttt{<eos>} at later
positions and propagates backward to trigger early termination. To address
this, we introduce Rainbow Padding, a simple remedy that replaces repeated
\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,
distributing probability mass and breaking \texttt{<eos>} dominance.
Experiments show that Rainbow Padding substantially improves length robustness
and output quality, with as few as seven padding tokens sufficient to prevent
early termination. Moreover, the method integrates efficiently into existing
instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data
yields significant improvements, making this solution highly practical. The
code is publicly available at https://github.com/quasar529/rainbow-padding.

</details>


### [75] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: 提出一个面向目标的多智能体系统评估框架，引入GSR和RCOF分类法，通过教师LLMs实现可解释评估，企业应用显示GSR显著提升。


<details>
  <summary>Details</summary>
Motivation: 评估多轮聊天机器人交互质量具有挑战性，现有方法多在轮次层面评估，而忽略了用户整体目标是否达成。

Method: 提出了一种基于模型的多智能体系统评估系统，结合教师LLMs，由领域专家定义目标并设定质量标准，LLMs使用“思考标记”生成可解释的推理，实现可解释、数据高效的评估。

Result: 在企业环境中应用该框架评估AIDA，观察到GSR从63%提升至79%。

Conclusion: 该框架具有通用性，通过详细的多智能体聊天机器人故障点分析，提供了可操作的见解，诊断整体成功情况，识别关键故障模式，并为系统改进提供信息。

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [76] [H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis](https://arxiv.org/abs/2510.03700)
*Seungseop Lim,Gibaeg Kim,Hyunkyung Lee,Wooseok Han,Jean Seo,Jaehyo Yoo,Eunho Yang*

Main category: cs.AI

TL;DR: H-DDx是一种层次化评估框架，能更准确地评估LLMs在临床诊断中的表现，揭示其识别广泛临床背景的能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs评估主要依赖扁平指标（如Top-k准确率），无法区分临床相关的近误和诊断性远误，限制了其临床实用性。

Method: 引入H-DDx，一种利用检索和重排序管道将自由文本诊断映射到ICD-10代码的层次化评估框架。

Result: 在22个领先模型的基准测试中，传统扁平指标低估了性能，而H-DDx突出了领域专用开源模型的优势，并增强了错误模式的可解释性。

Conclusion: H-DDx框架通过层次化评估更准确地反映了临床相关性，揭示了LLMs在识别广泛临床背景方面的优势，即使未精确诊断。

Abstract: An accurate differential diagnosis (DDx) is essential for patient care,
shaping therapeutic decisions and influencing outcomes. Recently, Large
Language Models (LLMs) have emerged as promising tools to support this process
by generating a DDx list from patient narratives. However, existing evaluations
of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,
which fail to distinguish between clinically relevant near-misses and
diagnostically distant errors. To mitigate this limitation, we introduce H-DDx,
a hierarchical evaluation framework that better reflects clinical relevance.
H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses
to ICD-10 codes and applies a hierarchical metric that credits predictions
closely related to the ground-truth diagnosis. In benchmarking 22 leading
models, we show that conventional flat metrics underestimate performance by
overlooking clinically meaningful outputs, with our results highlighting the
strengths of domain-specialized open-source models. Furthermore, our framework
enhances interpretability by revealing hierarchical error patterns,
demonstrating that LLMs often correctly identify the broader clinical context
even when the precise diagnosis is missed.

</details>


### [77] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: 研究通过增强多模态基础模型的推理和生成能力，填补了其与世界模型之间的差距，实现了结构化可控的4D生成。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态基础模型（MFMs）缺乏作为有效世界模型的关键能力，如反事实推理、动态模拟、时空信息理解和可控生成。研究旨在填补这一差距。

Method: 研究首先通过判别任务增强MFMs的推理能力，并引入结构化推理技能（如因果推理、反事实思维和时空推理）。随后探索了MFMs在图像和视频模态中的生成能力，提出了新的结构化可控生成框架，结合场景图、多模态条件化和对齐策略。

Result: 提出的方法显著提升了MFMs的推理和生成能力，实现了结构化可控的4D生成，支持交互式、可编辑和可变形对象的时空合成。

Conclusion: 通过提升多模态基础模型（MFMs）的推理和生成能力，本研究成功缩小了其与世界模型之间的差距，特别是在因果推理、反事实思维和时空推理方面。

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [78] [OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation](https://arxiv.org/abs/2510.03771)
*Divij Handa,David Blincoe,Orson Adams,Yinlin Fu*

Main category: cs.AI

TL;DR: OptAgent结合多智能体模拟和遗传算法优化电子商务查询重写，效果显著。


<details>
  <summary>Details</summary>
Motivation: 主观任务（如电子商务查询重写）缺乏单一正确答案，传统评估方法难以适用，需新的评估框架。

Method: 采用多智能体模拟（模拟购物顾客）和遗传算法，动态优化查询重写。

Result: 在1000个真实电子商务查询上测试，OptAgent比原始查询平均提升21.98%，比Best-of-N基线提升3.36%。

Conclusion: OptAgent框架通过结合多智能体模拟和遗传算法，显著提升了电子商务查询重写的效果，验证了其在主观任务评估中的有效性。

Abstract: Deploying capable and user-aligned LLM-based systems necessitates reliable
evaluation. While LLMs excel in verifiable tasks like coding and mathematics,
where gold-standard solutions are available, adoption remains challenging for
subjective tasks that lack a single correct answer. E-commerce Query Rewriting
(QR) is one such problem where determining whether a rewritten query properly
captures the user intent is extremely difficult to figure out algorithmically.
In this work, we introduce OptAgent, a novel framework that combines
multi-agent simulations with genetic algorithms to verify and optimize queries
for QR. Instead of relying on a static reward model or a single LLM judge, our
approach uses multiple LLM-based agents, each acting as a simulated shopping
customer, as a dynamic reward signal. The average of these agent-derived scores
serves as an effective fitness function for an evolutionary algorithm that
iteratively refines the user's initial query. We evaluate OptAgent on a dataset
of 1000 real-world e-commerce queries in five different categories, and we
observe an average improvement of 21.98% over the original user query and 3.36%
over a Best-of-N LLM rewriting baseline.

</details>


### [79] [GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time](https://arxiv.org/abs/2510.03777)
*Divij Handa,Mihir Parmar,Aswin RRV,Md Nayem Uddin,Hamid Palangi,Chitta Baral*

Main category: cs.AI

TL;DR: GuidedSampling 通过解耦探索和生成阶段，显著提升了解决方案的多样性和模型性能。


<details>
  <summary>Details</summary>
Motivation: Repeated Sampling 虽然能提升模型性能，但在生成多样化解决方案方面存在局限性，常依赖相同的底层方法导致冗余样本。

Method: 提出了 GuidedSampling 算法，将推理过程解耦为探索和生成两个阶段：探索阶段识别解决问题的多个概念，生成阶段应用特定概念生成最终解决方案。

Result: GuidedSampling 在 pass@50 上平均提升 21.6%，在 pass@5 上平均提升 9.7%，且每个实例的平均概念数从 1.67 增加到 3.03。

Conclusion: GuidedSampling 显著提升了模型在复杂任务上的性能，尤其是在生成多样化解决方案方面，相比传统的 Repeated Sampling 方法有显著改进。

Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been
shown to improve model performance on complex tasks. Although it is an
effective way of scaling inference time, it often struggles to generate diverse
solution candidates, frequently relying on the same underlying approach to
solve the problem and thus producing redundant samples. To address this
limitation, we propose a new inference algorithm, GuidedSampling, which
decouples the exploration and generation phases during inference, increasing
diversity of generated candidate solutions. The exploration phase identifies
multiple concepts that can be utilized to solve the problem, while the
generation phase applies a specific concept to provide final solution
candidates. We first define the theoretical bounds of GuidedSampling and then
empirically demonstrate that it improves the performance of base model at
pass@50 by on an average ~21.6% across various benchmarks compared to RS.
Furthermore, models trained on trajectories of GuidedSampling exhibit
substantial performance improvements at pass@5 by on an average ~9.7%, compared
to models trained on traditional RS. Additionally, models trained with
GuidedSampling increases the average number of concepts per instance (1.67 ->
3.03), yielding a diverse set of candidates than traditional RS.

</details>


### [80] [The Hidden Game Problem](https://arxiv.org/abs/2510.03845)
*Gon Buzaglo,Noah Golowich,Elad Hazan*

Main category: cs.AI

TL;DR: 本文提出了一种解决隐藏游戏问题的方法，通过组合遗憾最小化技术，实现了在隐藏子游戏中快速收敛到相关均衡，同时保持整体理性。


<details>
  <summary>Details</summary>
Motivation: 研究大型策略空间游戏中的隐藏结构问题，特别是AI对齐和语言游戏中的挑战。

Method: 引入了一种结合遗憾最小化技术的组合方法，以达到最优的外部遗憾和交换遗憾边界。

Result: 开发的方法能够高效发现并利用隐藏结构，确保在隐藏子游戏中快速收敛到相关均衡。

Conclusion: 本文通过开发一种结合遗憾最小化技术的组合方法，成功解决了隐藏游戏问题，实现了在隐藏子游戏中快速收敛到相关均衡，同时保持整体理性。

Abstract: This paper investigates a class of games with large strategy spaces,
motivated by challenges in AI alignment and language games. We introduce the
hidden game problem, where for each player, an unknown subset of strategies
consistently yields higher rewards compared to the rest. The central question
is whether efficient regret minimization algorithms can be designed to discover
and exploit such hidden structures, leading to equilibrium in these subgames
while maintaining rationality in general. We answer this question affirmatively
by developing a composition of regret minimization techniques that achieve
optimal external and swap regret bounds. Our approach ensures rapid convergence
to correlated equilibria in hidden subgames, leveraging the hidden game
structure for improved computational efficiency.

</details>


### [81] [Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs](https://arxiv.org/abs/2510.03847)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: SLMs在代理任务中表现出色，成本低、效率高，尤其在结构化输出和工具使用方面。通过设计模式和不确定性路由，SLMs可作为默认选择，LLMs作为后备。


<details>
  <summary>Details</summary>
Motivation: 探讨在代理任务中，SLMs是否能够替代LLMs，以更低的成本和更高的效率实现相同或更好的性能。

Method: 综合分析了多个开源和专有的SLMs（如Phi-4-Mini、Qwen-2.5-7B等），并结合现代评估工具（如BFCL v3/v4）和服务堆栈（如vLLM、SGLang）。提出了SLM-default、LLM-fallback系统，并引入了工程指标（如CPS、schema有效性率等）。

Result: SLMs在工具使用、函数调用和RAG等任务中能够匹配或超越LLMs，且成本降低10x-100x，延迟和能耗显著改善。同时，提出了针对SLMs优化的代理堆栈设计模式。

Conclusion: 小型语言模型（SLMs）在代理任务中表现出色，能够以更低的成本和更高的效率匹配或超越大型语言模型（LLMs），尤其是在工具使用、函数调用和RAG方面。通过设计模式和不确定性感知路由，SLMs可以作为默认选择，同时保留LLMs作为后备以应对开放领域推理和长期规划。

Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

</details>


### [82] [Algorithm Generation via Creative Ideation](https://arxiv.org/abs/2510.03851)
*Ruiying Ma,Chieh-Jan Mike Liang,Yanjie Gao,Francis Y. Yan*

Main category: cs.AI

TL;DR: MetaMuse通过自我反思原则克服LLMs在算法生成中的局限性，显著提升了缓存替换和在线装箱的性能。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够实际驱动算法生成，发现其倾向于已知的通用设计而非创造性突破，因此需要克服这一局限性。

Method: MetaMuse框架基于三个自我反思原则：(1)在可衡量的性能空间中量化解决方案的多样性和实用性，(2)通过外部刺激而非内部随机性引导构思，(3)使用路标推理而非自由形式的链式思维构建可执行解决方案。

Result: MetaMuse在缓存替换（减少缓存未命中率高达35.76%）和在线装箱问题（减少容器使用高达30.93%）上表现出色。

Conclusion: MetaMuse框架通过引入自我反思原则，成功克服了LLMs在算法生成中的局限性，能够为全球云提供商的关键问题生成高性能解决方案。

Abstract: Designing system algorithms remains challenging, where the discontinuous
nature of the solution space often forces system engineers to rely on generic
heuristics at the expense of performance. We study whether LLMs can practically
drive algorithm generation, and find that they are biased towards well-known
generic designs, rather than making the creative leaps needed to navigate the
discontinuous solution space. To address this limitation, we introduce
MetaMuse, a framework for creative ideation built on three self-reflection
principles: (1) quantifying solution diversity and usefulness in measurable
performance space, rather than abstract idea space, (2) steering ideation
through external stimuli, rather than internal randomness, and (3) constructing
executable solutions using waypoint reasoning, rather than free-form
chain-of-thought. Extensive evaluation shows that MetaMuse can generate
high-performing solutions for two critical problems at a global cloud provider:
cache replacement (reducing cache misses by up to 35.76%) and online bin
packing (reducing bin usage by up to 30.93%).

</details>


### [83] [Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning](https://arxiv.org/abs/2510.03859)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 提出了一种结合LLM和XAI的智能异常检测方法，在IoT环境中表现优异，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 动态、高维且数据不完整或不断演变的IoT环境中，传统异常检测方法存在局限性，需要自适应、智能化的系统。

Method: 采用LLM支持的上下文推理方法和XAI代理，结合注意力机制和内存缓冲区，避免处理每个时间步的细节。

Result: 在模拟真实世界的智能电网和医疗场景中，新方法在准确性、结果清晰度和响应速度上均优于传统模型。

Conclusion: 新提出的LLM增强模型在异常检测的准确性和可解释性方面表现优于现有模型，适用于未来IoT环境中的异常检测任务。

Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT

</details>


### [84] [Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation](https://arxiv.org/abs/2510.03863)
*Arina Kharlamova,Bowei He,Chen Ma,Xue Liu*

Main category: cs.AI

TL;DR: Spatial CAPTCHA 是一种新型人类验证框架，利用空间推理能力的差异抵御AI攻击，证明其比传统CAPTCHA更有效。


<details>
  <summary>Details</summary>
Motivation: 传统CAPTCHA依赖于文本识别或2D图像理解，而多模态大语言模型的进步使得这些设计逐渐失效。因此，需要一种新的验证框架来填补这一安全漏洞。

Method: Spatial CAPTCHA 采用程序化生成管道，结合约束难度控制、自动正确性验证和人类参与验证，确保其可扩展性、鲁棒性和适应性。

Result: 在Spatial-CAPTCHA-Bench基准测试中，人类表现远超10种最先进的多模态大语言模型，最佳模型的Pass@1准确率仅为31.0%。与Google reCAPTCHA的对比进一步验证了其作为安全机制和AI空间推理诊断工具的有效性。

Conclusion: Spatial CAPTCHA 作为一种新型的人类验证框架，通过利用人类与多模态大语言模型在空间推理能力上的根本差异，有效抵御了现代AI的自动化攻击。

Abstract: Online services rely on CAPTCHAs as a first line of defense against automated
abuse, yet recent advances in multi-modal large language models (MLLMs) have
eroded the effectiveness of conventional designs that focus on text recognition
or 2D image understanding. To address this challenge, we present Spatial
CAPTCHA, a novel human-verification framework that leverages fundamental
differences in spatial reasoning between humans and MLLMs. Unlike existing
CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern
AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,
perspective-taking, occlusion handling, and mental rotation. These skills are
intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The
system employs a procedural generation pipeline with constraint-based
difficulty control, automated correctness verification, and human-in-the-loop
validation to ensure scalability, robustness, and adaptability. Evaluation on a
corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly
outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%
Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,
which confirms its effectiveness as both a security mechanism and a diagnostic
tool for spatial reasoning in AI.

</details>


### [85] [Rare Text Semantics Were Always There in Your Diffusion Transformer](https://arxiv.org/abs/2510.03886)
*Seil Kang,Woojung Han,Dayun Ju,Seong Jae Hwang*

Main category: cs.AI

TL;DR: MM-DiTs通过扩展文本嵌入表示范围，提升罕见语义生成能力，无需额外训练或外部模块支持。


<details>
  <summary>Details</summary>
Motivation: 当前多模态扩散变换器（MM-DiTs）在处理罕见或创意提示时表现不佳，因为这些概念在预训练中缺乏足够的数据支持。

Method: 提出一种简单而有效的干预方法，通过方差放大在联合注意力块前扩展文本标记嵌入的表示范围。

Result: 该方法在文本到图像、文本到视频及文本驱动的图像编辑等任务中均表现出色，能够有效生成罕见语义。

Conclusion: 通过数学方法扩展文本标记嵌入的表示范围，MM-DiTs能够在不增加训练步骤或依赖外部模块的情况下，有效提升罕见语义的生成能力。

Abstract: Starting from flow- and diffusion-based transformers, Multi-modal Diffusion
Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim
for exceptional visual fidelity. As these models advance, users continually
push the boundary with imaginative or rare prompts, which advanced models still
falter in generating, since their concepts are often too scarce to leave a
strong imprint during pre-training. In this paper, we propose a simple yet
effective intervention that surfaces rare semantics inside MM-DiTs without
additional training steps, data, denoising-time optimization, or reliance on
external modules (e.g., large language models). In particular, the
joint-attention mechanism intrinsic to MM-DiT sequentially updates text
embeddings alongside image embeddings throughout transformer blocks. We find
that by mathematically expanding representational basins around text token
embeddings via variance scale-up before the joint-attention blocks, rare
semantics clearly emerge in MM-DiT's outputs. Furthermore, our results
generalize effectively across text-to-vision tasks, including text-to-image,
text-to-video, and text-driven image editing. Our work invites generative
models to reveal the semantics that users intend, once hidden yet ready to
surface.

</details>


### [86] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 游戏化可解释AI系统帮助消费者在咖啡购买中做出道德决策，结合康德主义和功利主义模块提供实时解释。


<details>
  <summary>Details</summary>
Motivation: 旨在通过游戏化方式提升消费者在咖啡购买中的道德决策意识，并提供透明的决策解释。

Method: 系统包含六个回合，每回合提供三个选项。使用两个符号引擎：康德主义模块检测规则违反（如童工、无遮荫认证的森林砍伐风险等），功利主义模块通过多标准聚合对选项评分。元解释器通过遗憾界限（0.2）突出康德主义与功利主义的（不）一致性，并在福利损失较小时切换到道德清洁且接近最优的选项。

Result: 系统发布了结构化配置（属性模式、认证映射、权重、规则集）、可审计的策略追踪和交互式用户界面。

Conclusion: 本文提出了一种游戏化的可解释AI系统，用于在咖啡消费领域进行道德决策。通过结合康德主义和功利主义模块，系统实现了实时决策解释和道德权衡。

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [87] [Quantifying Risks in Multi-turn Conversation with Large Language Models](https://arxiv.org/abs/2510.03969)
*Chengxiao Wang,Isha Chaudhary,Qian Hu,Weitong Ruan,Rahul Gupta,Gagandeep Singh*

Main category: cs.AI

TL;DR: QRLLM是一个统计认证框架，用于量化LLM在多轮对话中的灾难性风险，揭示前沿模型存在高达70%的风险，呼吁改进安全训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法因依赖固定攻击提示序列、缺乏统计保证且无法扩展到多轮对话的广阔空间，难以全面揭示LLM在对话环境中的灾难性响应风险。

Method: QRLLM将多轮对话建模为查询序列的概率分布，通过马尔可夫过程在查询图上表示，并利用置信区间量化灾难性风险。定义了随机节点、图路径、带拒绝的自适应等实用分布。

Result: QRLLM能够揭示前沿模型中显著的灾难性风险，最差模型的认证下界高达70%。

Conclusion: QRLLM框架为多轮对话中的LLM灾难性风险提供了统计保证的概率边界，揭示了前沿模型中高达70%的认证下界，强调了改进安全训练策略的紧迫性。

Abstract: Large Language Models (LLMs) can produce catastrophic responses in
conversational settings that pose serious risks to public safety and security.
Existing evaluations often fail to fully reveal these vulnerabilities because
they rely on fixed attack prompt sequences, lack statistical guarantees, and do
not scale to the vast space of multi-turn conversations. In this work, we
propose QRLLM, a novel, principled Certification framework for Catastrophic
risks in multi-turn Conversation for LLMs that bounds the probability of an LLM
generating catastrophic responses under multi-turn conversation distributions
with statistical guarantees. We model multi-turn conversations as probability
distributions over query sequences, represented by a Markov process on a query
graph whose edges encode semantic similarity to capture realistic
conversational flow, and quantify catastrophic risks using confidence
intervals. We define several inexpensive and practical distributions: random
node, graph path, adaptive with rejection. Our results demonstrate that these
distributions can reveal substantial catastrophic risks in frontier models,
with certified lower bounds as high as 70\% for the worst model, highlighting
the urgent need for improved safety training strategies in frontier LLMs.

</details>


### [88] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: 论文提出了C^2-Eval，一个评估基础模型创造性的统一框架，通过Usefulness、Originality和Surprise标准，揭示了当前模型的创造性能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估创造性的框架零散且缺乏理论基础，需要一种统一的评估方法来衡量生成式基础模型的创造性。

Method: 引入了C^2-Eval，一个基于社会科学的理论，通过Usefulness、Originality和Surprise（U-O-S）标准评估创造性。

Result: 通过实验分析了领先的专有和开源模型的创造性能力，展示了C^2-Eval在评估创造性AI方面的有效性。

Conclusion: C^2-Eval被证明是评估创造性AI的有效工具，揭示了当前基础模型在追求创造性机器智能方面的优势与挑战。

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [89] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: 研究提出了一种结合天气模型和语言模型的代理框架Zephyrus，在基础任务上显著优于纯文本方法，但复杂任务仍需改进。


<details>
  <summary>Details</summary>
Motivation: 传统天气模型缺乏语言推理能力，而大型语言模型无法处理高维气象数据，因此需要结合两者优势的解决方案。

Method: 构建了一个名为ZephyrusWorld的Python代码环境，集成了WeatherBench 2数据集接口、地理查询、天气预报和气候模拟工具，并设计了多轮LLM代理Zephyrus。

Result: Zephyrus在ZephyrusBench基准测试中正确率比纯文本基线高35个百分点，但在复杂任务上表现相似。

Conclusion: Zephyrus框架在天气科学中展现出潜力，尤其在基本任务上表现优于纯文本基线，但在复杂任务上仍有改进空间，为未来研究指明了方向。

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [90] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: 本文综述了数据科学代理的分类和现状，指出当前系统在业务理解和部署方面的不足，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的进步，数据科学代理能够自动化数据科学工作流的多个阶段。本文旨在提供一个全面的分类和批判性分析，以指导未来研究。

Method: 论文对45个数据科学代理系统进行了全面的生命周期分类，并沿五个跨领域设计维度进行了标注。

Result: 分析发现大多数系统侧重于探索性分析和建模，而忽视了业务理解和部署。多模态推理和工具编排仍是挑战，且90%以上的系统缺乏明确的信任和安全机制。

Conclusion: 论文总结了数据科学代理的当前发展趋势，指出了在业务理解、部署和监控方面的不足，以及多模态推理和工具编排的挑战。作者提出了未来研究方向，包括对齐稳定性、可解释性、治理和评估框架的开发。

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [91] [A global log for medical AI](https://arxiv.org/abs/2510.04033)
*Ayush Noori,Adam Rodman,Alan Karthikesalingam,Bilal A. Mateen,Christopher A. Longhurst,Daniel Yang,Dave deBronkart,Gauden Galea,Harold F. Wolf III,Jacob Waxman,Joshua C. Mandel,Juliana Rotich,Kenneth D. Mandl,Maryam Mustafa,Melissa Miles,Nigam H. Shah,Peter Lee,Robert Korom,Scott Mahoney,Seth Hain,Tien Yin Wong,Trevor Mundel,Vivek Natarajan,Noa Dagan,David A. Clifton,Ran D. Balicer,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: MedLog是一种临床AI事件日志协议，旨在标准化记录AI模型的使用情况，支持监测、审计和改进。


<details>
  <summary>Details</summary>
Motivation: 医疗AI领域缺乏标准化的日志记录方法，导致难以衡量实际性能、检测不良事件或纠正偏差。

Method: 提出MedLog协议，包括九个核心字段（header、model、user、target、inputs、artifacts、outputs、outcomes、feedback），支持风险采样、生命周期感知的保留策略和写后缓存。

Result: MedLog为临床AI提供了一种结构化、一致的模型活动记录方式，支持低资源环境下的早期采用，并能捕获复杂工作流的详细痕迹。

Conclusion: MedLog协议为临床AI提供了一种标准化的事件级日志记录方法，有望促进医疗AI的持续监测、审计和迭代改进，为新型数字流行病学奠定基础。

Abstract: Modern computer systems often rely on syslog, a simple, universal protocol
that records every critical event across heterogeneous infrastructure. However,
healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals
rush to pilot large language models and other AI-based clinical decision
support tools, we still lack a standard way to record how, when, by whom, and
for whom these AI models are used. Without that transparency and visibility, it
is challenging to measure real-world performance and outcomes, detect adverse
events, or correct bias or dataset drift. In the spirit of syslog, we introduce
MedLog, a protocol for event-level logging of clinical AI. Any time an AI model
is invoked to interact with a human, interface with another algorithm, or act
independently, a MedLog record is created. This record consists of nine core
fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and
feedback, providing a structured and consistent record of model activity. To
encourage early adoption, especially in low-resource settings, and minimize the
data footprint, MedLog supports risk-based sampling, lifecycle-aware retention
policies, and write-behind caching; detailed traces for complex, agentic, or
multi-stage workflows can also be captured under MedLog. MedLog can catalyze
the development of new databases and software to store and analyze MedLog
records. Realizing this vision would enable continuous surveillance, auditing,
and iterative improvement of medical AI, laying the foundation for a new form
of digital epidemiology.

</details>


### [92] [FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.04040)
*Xu Shen,Song Wang,Zhen Tan,Laura Yao,Xinyu Zhao,Kaidi Xu,Xin Wang,Tianlong Chen*

Main category: cs.AI

TL;DR: 介绍了 FaithCoT-Bench 基准，用于实例级 CoT 忠实性检测，填补了实践挑战的空白，并评估了多种检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中在机制层面分析 CoT 的不忠实性，但缺乏对特定轨迹是否忠实于模型内部推理的实践挑战。

Method: 引入了 FaithCoT-Bench 基准，包括 FINE-CoT 数据集，并对 11 种代表性检测方法进行了系统评估。

Result: 评估揭示了现有方法的优缺点，并指出在知识密集型领域和更先进模型中进行检测的挑战增加。

Conclusion: FaithCoT-Bench 是首个针对实例级 CoT 忠实性的综合基准，为未来研究更可解释和可信的 LLM 推理奠定了基础。

Abstract: Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)
prompting to improve problem-solving and provide seemingly transparent
explanations. However, growing evidence shows that CoT often fail to faithfully
represent the underlying reasoning process, raising concerns about their
reliability in high-risk applications. Although prior studies have focused on
mechanism-level analyses showing that CoTs can be unfaithful, they leave open
the practical challenge of deciding whether a specific trajectory is faithful
to the internal reasoning of the model. To address this gap, we introduce
FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness
detection. Our framework establishes a rigorous task formulation that
formulates unfaithfulness detection as a discriminative decision problem, and
provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an
expert-annotated collection of over 1,000 trajectories generated by four
representative LLMs across four domains, including more than 300 unfaithful
instances with fine-grained causes and step-level evidence. We further conduct
a systematic evaluation of eleven representative detection methods spanning
counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical
insights that clarify the strengths and weaknesses of existing approaches and
reveal the increased challenges of detection in knowledge-intensive domains and
with more advanced models. To the best of our knowledge, FaithCoT-Bench
establishes the first comprehensive benchmark for instance-level CoT
faithfulness, setting a solid basis for future research toward more
interpretable and trustworthy reasoning in LLMs.

</details>


### [93] [Increasing LLM response trustworthiness using voting ensembles](https://arxiv.org/abs/2510.04048)
*Aparna Nair-Kanneganti,Trevor J. Chan,Shir Goldfinger,Emily Mackay,Brian Anthony,Alison Pouch*

Main category: cs.AI

TL;DR: 通过可变投票阈值集成方法，显著提升LLM回答的可信度，适用于需要高确定性的应用。


<details>
  <summary>Details</summary>
Motivation: LLM缺乏可靠的方法量化其回答的不确定性，导致在高风险应用中难以信任。集成方法（如选择多数响应模式）虽简单但需改进。

Method: 提出了一种理论框架，允许集成在主导回答未达阈值时“弃权”，并在算术问题解决和临床笔记问答两个领域进行了实验验证。

Result: 在高度限制性的投票集成中，回答的可信度大幅提升，同时响应产出和准确性的降低相对较小。

Conclusion: 通过引入可变投票阈值的集成方法，可以显著提高LLM回答的可信度，尤其是在需要高确定性的应用中（如医疗保健和数据标注）。

Abstract: Despite huge advances, LLMs still lack convenient and reliable methods to
quantify the uncertainty in their responses, making them difficult to trust in
high-stakes applications. One of the simplest approaches to eliciting more
accurate answers is to select the mode of many responses, a technique known as
ensembling. In this work, we expand on typical ensembling approaches by looking
at ensembles with a variable voting threshold. We introduce a theoretical
framework for question answering and show that, by permitting ensembles to
"abstain" from providing an answer when the dominant response falls short of
the threshold, it is possible to dramatically increase the trustworthiness of
the remaining answers. From this framework, we derive theoretical results as
well as report experimental results on two problem domains: arithmetic problem
solving and clinical-note question-answering. In both domains, we observe that
large gains in answer trustworthiness can be achieved using highly restrictive
voting ensembles, while incurring relatively modest reductions in response
yield and accuracy. Due to this quality, voting ensembles may be particularly
useful in applications - such as healthcare and data annotation - that require
a high degree of certainty but which may not require that every question
receive an automated answer.

</details>


### [94] [Toward a unified framework for data-efficient evaluation of large language models](https://arxiv.org/abs/2510.04051)
*Lele Liao,Qile Zhang,Ruofan Wu,Guanhua Fang*

Main category: cs.AI

TL;DR: LEGO-IRT 是一个数据高效的大型语言模型评估框架，支持二进制和连续指标，通过因子化架构利用结构知识，显著减少估计误差。


<details>
  <summary>Details</summary>
Motivation: 现有基于IRT的方法在评估大型语言模型时存在局限性，如仅支持二进制正确性指标、无法处理生成任务的连续分数，且忽略跨基准的结构知识。

Method: LEGO-IRT 是一个支持二进制和连续评估指标的统一框架，通过因子化架构显式建模和利用结构知识，将模型能力估计分解为通用组件和结构特定组件。

Result: 实验表明，LEGO-IRT 仅需3%的评估项即可稳定估计模型能力，结构知识的引入使估计误差减少达10%，且潜在能力估计更符合人类偏好。

Conclusion: LEGO-IRT 是一个统一且灵活的框架，能够高效评估大型语言模型，通过仅使用3%的评估项即可稳定估计模型能力，并且其潜在能力估计与人类偏好更一致。

Abstract: Evaluating large language models (LLMs) on comprehensive benchmarks is a
cornerstone of their development, yet it's often computationally and
financially prohibitive. While Item Response Theory (IRT) offers a promising
path toward data-efficient evaluation by disentangling model capability from
item difficulty, existing IRT-based methods are hampered by significant
limitations. They are typically restricted to binary correctness metrics,
failing to natively handle the continuous scores used in generative tasks, and
they operate on single benchmarks, ignoring valuable structural knowledge like
correlations across different metrics or benchmarks. To overcome these
challenges, we introduce LEGO-IRT, a unified and flexible framework for
data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both
binary and continuous evaluation metrics. Moreover, it introduces a factorized
architecture to explicitly model and leverage structural knowledge, decomposing
model ability estimates into a general component and structure-specific (e.g.,
per-metric or per-benchmark) components. Through extensive experiments
involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves
stable capability estimates using just $3\%$ of the total evaluation items. We
demonstrate that incorporating structural knowledge reduces estimation error by
up to $10\%$ and reveal that the latent abilities estimated by our framework
may align more closely with human preferences.

</details>


### [95] [Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion](https://arxiv.org/abs/2510.04064)
*Jingxiang Zhang,Lujia Zhong*

Main category: cs.AI

TL;DR: 论文研究了LLMs内部情感表示，发现其具有明确的几何结构、可塑性及持久性，为AI系统开发提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 探索现代LLMs中潜在的情感表示，即情感如何、在哪里以及持续多久被编码在其神经架构中。

Method: 通过多阶段分类、重写和合成生成构建了一个包含约40万条话语的Reddit语料库，并使用轻量级“探针”从Qwen3和LLLaMA模型的隐藏层读取信息。

Result: 发现LLMs内部形成了明确的情感几何结构，该结构随模型规模增强且显著优于零样本提示。情感信号在网络早期出现并在中期达到峰值，且具有可塑性和持久性。

Conclusion: 论文揭示了LLMs内部情感表示的清晰几何结构，指出其可塑性及持久性，为开发更透明和一致的AI系统提供了关键见解。

Abstract: Large Language Models (LLMs) are increasingly expected to navigate the
nuances of human emotion. While research confirms that LLMs can simulate
emotional intelligence, their internal emotional mechanisms remain largely
unexplored. This paper investigates the latent emotional representations within
modern LLMs by asking: how, where, and for how long is emotion encoded in their
neural architecture? To address this, we introduce a novel, large-scale Reddit
corpus of approximately 400,000 utterances, balanced across seven basic
emotions through a multi-stage process of classification, rewriting, and
synthetic generation. Using this dataset, we employ lightweight "probes" to
read out information from the hidden layers of various Qwen3 and LLaMA models
without altering their parameters. Our findings reveal that LLMs develop a
surprisingly well-defined internal geometry of emotion, which sharpens with
model scale and significantly outperforms zero-shot prompting. We demonstrate
that this emotional signal is not a final-layer phenomenon but emerges early
and peaks mid-network. Furthermore, the internal states are both malleable
(they can be influenced by simple system prompts) and persistent, as the
initial emotional tone remains detectable for hundreds of subsequent tokens. We
contribute our dataset, an open-source probing toolkit, and a detailed map of
the emotional landscape within LLMs, offering crucial insights for developing
more transparent and aligned AI systems. The code and dataset are open-sourced.

</details>


### [96] [Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention](https://arxiv.org/abs/2510.04073)
*Santhosh Kumar Ravindran*

Main category: cs.AI

TL;DR: MAS框架通过实时监测、预测和自适应治理，显著减少AI价值漂移，保持高准确率和低误报。


<details>
  <summary>Details</summary>
Motivation: 随着AI作为超级助手的崛起，确保其行为与人类伦理和意图一致的价值对齐问题日益关键，尤其是价值漂移可能导致效率低下或伦理违规。

Method: 提出Moral Anchor System (MAS)框架，结合实时贝叶斯推断、LSTM网络和人类中心治理层，用于监测、预测和缓解AI代理的价值漂移。

Result: 实验验证MAS可将价值漂移事件减少80%以上，检测准确率达85%，误报率低至0.08。

Conclusion: MAS框架通过结合实时贝叶斯推断、LSTM网络和人类中心治理层，有效减少了价值漂移事件，保持了高检测准确率和低误报率。

Abstract: The rise of artificial intelligence (AI) as super-capable assistants has
transformed productivity and decision-making across domains. Yet, this
integration raises critical concerns about value alignment - ensuring AI
behaviors remain consistent with human ethics and intentions. A key risk is
value drift, where AI systems deviate from aligned values due to evolving
contexts, learning dynamics, or unintended optimizations, potentially leading
to inefficiencies or ethical breaches. We propose the Moral Anchor System
(MAS), a novel framework to detect, predict, and mitigate value drift in AI
agents. MAS combines real-time Bayesian inference for monitoring value states,
LSTM networks for forecasting drift, and a human-centric governance layer for
adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent
breaches, while reducing false positives and alert fatigue via supervised
fine-tuning with human feedback. Our hypothesis: integrating probabilistic
drift detection, predictive analytics, and adaptive governance can reduce value
drift incidents by 80 percent or more in simulations, maintaining high
detection accuracy (85 percent) and low false positive rates (0.08
post-adaptation). Rigorous experiments with goal-misaligned agents validate
MAS's scalability and responsiveness. MAS's originality lies in its predictive
and adaptive nature, contrasting static alignment methods. Contributions
include: (1) MAS architecture for AI integration; (2) empirical results
prioritizing speed and usability; (3) cross-domain applicability insights; and
(4) open-source code for replication.

</details>


### [97] [SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows](https://arxiv.org/abs/2510.04089)
*Yitong Cui,Liu Liu,Baosheng Yu,Jiayan Qiu,Xikai Zhang,Likang Xiao,Yixing Liu,Quan Chen*

Main category: cs.AI

TL;DR: SPOGW是一种新的自动化代理工作流程优化方法，通过组间比较和连续空间优化克服现有技术的限制，在多个任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）通过代理工作流程在多个领域表现出色，但设计这些流程需要大量手动工作，限制了可扩展性和通用性。现有自动化技术因表示能力有限、适应性不足、可扩展性差和成对比较范式而受限。

Method: 引入了一种新的基于分数的偏好方法SPOGW，通过组间比较直接操作基数奖励信号，并结合迭代离线GRPO（ioGRPO）和优势掩码KL散度（mKL）进行训练更新。

Result: 在五个基准数据集（数学推理、编码和问答）上，SPOGW匹配或超越了当前最先进方法的性能。

Conclusion: SPOGW方法在自动化生成和优化代理工作流程方面展现出了可行性和前瞻性，匹配或超越了当前最先进方法的性能。

Abstract: Large language models (LLMs) have exhibited significant capabilities in
addressing challenging problems throughout various fields, often through the
use of agentic workflows that adhere to structured instructions and multi-step
procedures. However, designing such workflows demands substantial manual
effort, posing challenges to scalability and generalizability. Recent studies
have aimed to minimize the human intervention needed for their construction,
leading to advances in automated techniques for optimizing agentic workflows.
However, current approaches are often constrained by their limited
representational capacity, insufficient adaptability, weak scalability, and
pairwise comparison paradigm -- issues that stem primarily from a dependence on
discrete optimization techniques. To overcome these limitations, we introduce a
new score-based preference approach, refereed as SPOGW, which operates directly
on cardinal reward signals through group-wise comparison and enables more
efficient and stable optimization in a continuous space. SPOGW incorporates
Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),
which regulates training update by placing greater emphasis on the advantageous
regions of the policy response. In five benchmark datasets covering
mathematical reasoning, coding, and question answering, SPOGW matches or
exceeds the performance of current state-of-the-art approaches, presenting a
viable and forward-looking methodology for automated generation and
optimization of agentic workflows.

</details>


### [98] [Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems](https://arxiv.org/abs/2510.04093)
*Guixian Zhang,Guan Yuan,Ziqi Xu,Yanmei Zhang,Zhenyun Deng,Debo Cheng*

Main category: cs.AI

TL;DR: DLLM是一种基于扩散的LLM框架，用于噪声鲁棒的认知诊断，通过子图构建、关系增强对齐和两阶段去噪扩散模块，有效解决了数据不平衡和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 网络智能教育系统(WIES)中的认知诊断旨在从异构、嘈杂的互动中评估学生对知识概念的掌握程度。然而，大语言模型(LLMs)在处理结构化数据和噪声诱导的误判方面存在困难，且WIES的开放环境加剧了数据不平衡和噪声问题。

Method: DLLM首先基于回答正确性构建独立子图，然后应用关系增强对齐模块缓解数据不平衡问题。接着，将两个子图表示融合并与LLM衍生的语义增强表示对齐。关键是在每个对齐步骤前，DLLM采用两阶段去噪扩散模块消除固有噪声，同时辅助结构表示对齐。

Result: DLLM在三种公开数据集上实现了最优的预测性能，证明了其噪声鲁棒性和对LLM语义知识的有效利用。

Conclusion: DLLM框架在三种公开的基于网络的教育平台数据集上表现出最优的预测性能，证明了其在各种噪声水平下均能实现噪声鲁棒性，并有效利用LLM的语义知识。

Abstract: Cognitive diagnostics in the Web-based Intelligent Education System (WIES)
aims to assess students' mastery of knowledge concepts from heterogeneous,
noisy interactions. Recent work has tried to utilize Large Language Models
(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are
prone to noise-induced misjudgments. Specially, WIES's open environment
continuously attracts new students and produces vast amounts of response logs,
exacerbating the data imbalance and noise issues inherent in traditional
educational systems. To address these challenges, we propose DLLM, a
Diffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first
constructs independent subgraphs based on response correctness, then applies
relation augmentation alignment module to mitigate data imbalance. The two
subgraph representations are then fused and aligned with LLM-derived,
semantically augmented representations. Importantly, before each alignment
step, DLLM employs a two-stage denoising diffusion module to eliminate
intrinsic noise while assisting structural representation alignment.
Specifically, unconditional denoising diffusion first removes erroneous
information, followed by conditional denoising diffusion based on graph-guided
to eliminate misleading information. Finally, the noise-robust representation
that integrates semantic knowledge and structural information is fed into
existing cognitive diagnosis models for prediction. Experimental results on
three publicly available web-based educational platform datasets demonstrate
that our DLLM achieves optimal predictive performance across varying noise
levels, which demonstrates that DLLM achieves noise robustness while
effectively leveraging semantic knowledge from LLM.

</details>


### [99] [WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning](https://arxiv.org/abs/2510.04097)
*Peichao Lai,Jinhui Zhuang,Kexuan Zhang,Ningchang Xiong,Shengjie Wang,Yanwei Xu,Chong Chen,Yilei Wang,Bin Cui*

Main category: cs.AI

TL;DR: WebRenderBench是一个大规模、多样化的UI转代码基准，ALISA通过新评估指标和强化学习实现了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有UI图像转代码任务中数据多样性和评估可靠性的不足。

Method: 提出了WebRenderBench基准和一种新的评估指标，整合到强化学习中的奖励信号ALISA。

Result: ALISA在多样性和复杂性更高的真实网页上表现优异。

Conclusion: ALISA显著提升了生成性能，在多个指标上实现了最先进的结果。

Abstract: Automating the conversion of UI images into web code is a critical task for
front-end development and rapid prototyping. Advances in multimodal large
language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet
existing benchmarks remain limited in data diversity and evaluation
reliability. To address these issues, we present WebRenderBench, a large-scale
benchmark of 22.5k webpages collected from real-world portal sites, offering
greater diversity, complexity, and realism than prior benchmarks. We further
propose a novel evaluation metric that measures layout and style consistency
from the final rendered pages. Unlike vision-based methods that rely on costly
LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,
our approach enables more efficient, objective, and reliable UI quality
assessment. Finally, we introduce the Automated Layout and Style Inspection
Agent (ALISA), which integrates this metric into reinforcement learning as a
reward signal to enhance training on crawled asymmetric webpages. Experiments
show that ALISA significantly boosts generation performance, achieving
state-of-the-art results across multiple metrics.

</details>


### [100] [Searching Meta Reasoning Skeleton to Guide LLM Reasoning](https://arxiv.org/abs/2510.04116)
*Ziying Zhang,Yaqing Wang,Quanming Yao*

Main category: cs.AI

TL;DR: AutoMR框架通过DAG表示和动态采样算法，自动搜索查询感知的元推理骨架，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有元推理骨架手动设计导致的适应性差和逻辑依赖捕捉不足的问题。

Method: 提出了AutoMR框架，包括基于DAG的骨架表示、搜索空间构建和动态骨架采样算法。

Result: 在多个基准数据集上实验显示，AutoMR推理性能优于先前工作。

Conclusion: AutoMR通过动态骨架采样算法和DAG表示，显著提升了LLM的推理性能，优于先前方法。

Abstract: Meta reasoning behaviors work as a skeleton to guide large language model
(LLM) reasoning, thus help to improve reasoning performance. However, prior
researches implement meta reasoning skeleton with manually designed structure,
limiting ability to adapt to query-specific requirement and capture intricate
logical dependency among reasoning steps. To deal with the challenges, we
represent meta reasoning skeleton with directed acyclic graph (DAG) to unify
skeletons proposed in prior works and model intricate logical dependency. Then
we propose AutoMR, a framework that searches for query-aware meta reasoning
skeleton automatically inspired by automated machine learning (AutoML).
Specifically, we construct search space based on DAG representation of skeleton
and then formulate the search problem. We design a dynamic skeleton sampling
algorithm by expanding meta reasoning skeleton along with reasoning context at
inference time. This algorithm can derive any meta reasoning skeleton in search
space efficiently and adapt skeleton to evolving base reasoning context, thus
enable efficient query-aware skeleton search. We conduct experiments on
extensive benchmark datasets. Experimental results show that AutoMR achieves
better reasoning performance than previous works broadly.

</details>


### [101] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: 研究发现模型在等待标记前的潜在特征能调节推理过程，促进或抑制等待标记，影响不同类型的推理模式。


<details>
  <summary>Details</summary>
Motivation: 尽管推理模型中的等待标记（wait tokens）常标志着复杂的推理行为（如回溯），但对其为何决定以特定方式推理的理解有限，这限制了对推理模型有效性的认识。

Method: 研究者训练了多层DeepSeek-R1-Distill-Llama-8B及其基础版本的交叉编码器，并引入了一种潜在归因技术。通过最大激活示例和因果干预实验，验证了这些特征与推理过程的相关性。

Result: 研究发现了一小部分与促进或抑制等待标记概率相关的特征，并通过实验证明这些特征确实与推理过程相关，能够引发不同类型的推理模式（如从头开始、回忆先验知识、表达不确定性和双重检查）。

Conclusion: 研究发现，模型在等待标记前的潜在特征确实包含调节后续推理过程的相关信息，这些特征能够促进或抑制等待标记的概率，从而影响不同的推理模式。

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [102] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: MENTOR框架通过关键决策点的专家指导，优化RLVR中的探索，提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法依赖基础模型的能力，且通过模仿专家轨迹仅提升了探索的有效性而忽视了多样性。为解决这一问题，研究认为专家只需在关键决策点提供指导即可。

Method: 提出MENTOR框架，通过混合策略专家导航在关键决策点提供专家指导，优化推理过程中的令牌级探索。

Result: 实验表明，MENTOR能够捕捉专家策略的本质而非表面模仿，实现了高质量的探索和整体性能的提升。

Conclusion: MENTOR框架通过仅在关键决策点提供专家指导，实现了RLVR中的有效和多样化探索，从而显著提升了大型语言模型的推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [103] [The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning](https://arxiv.org/abs/2510.04141)
*Mayank Ravishankara,Varindra V. Persad Maharaj*

Main category: cs.AI

TL;DR: 本文综述了多模态AI评估的演变，从简单识别到复杂推理，揭示了评估如何推动真正智能系统的发展。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态AI评估从简单的识别任务向复杂推理基准的范式转变，以揭示模型在理解“为什么”和“如何”方面的能力。

Method: 本文通过调查多模态AI评估的演变，将其视为一系列日益复杂的“认知测试”的进展。

Result: 从ImageNet时代的“知识测试”到GQA和VCR等“应用逻辑与理解”测试，再到针对MLLMs的“专家级集成”基准（如MMBench、SEED-Bench、MMMU），展示了评估的不断进化。

Conclusion: AI评估的叙事不仅是数据集的历史，而是一个持续对抗的过程，旨在设计更好的测试，从而重新定义我们创建真正智能系统的目标。

Abstract: This survey paper chronicles the evolution of evaluation in multimodal
artificial intelligence (AI), framing it as a progression of increasingly
sophisticated "cognitive examinations." We argue that the field is undergoing a
paradigm shift, moving from simple recognition tasks that test "what" a model
sees, to complex reasoning benchmarks that probe "why" and "how" it
understands. This evolution is driven by the saturation of older benchmarks,
where high performance often masks fundamental weaknesses. We chart the journey
from the foundational "knowledge tests" of the ImageNet era to the "applied
logic and comprehension" exams such as GQA and Visual Commonsense Reasoning
(VCR), which were designed specifically to diagnose systemic flaws such as
shortcut learning and failures in compositional generalization. We then survey
the current frontier of "expert-level integration" benchmarks (e.g., MMBench,
SEED-Bench, MMMU) designed for today's powerful multimodal large language
models (MLLMs), which increasingly evaluate the reasoning process itself.
Finally, we explore the uncharted territories of evaluating abstract, creative,
and social intelligence. We conclude that the narrative of AI evaluation is not
merely a history of datasets, but a continuous, adversarial process of
designing better examinations that, in turn, redefine our goals for creating
truly intelligent systems.

</details>


### [104] [Open Agent Specification (Agent Spec) Technical Report](https://arxiv.org/abs/2510.04173)
*Yassine Benajiba,Cesare Bernardis,Vladislav Blinov,Paul Cayet,Hassan Chafi,Abderrahim Fathan,Louis Faucon,Damien Hilloulin,Sungpack Hong,Ingo Kossyk,Rhicheek Patra,Sujith Ravi,Jonas Schweizer,Jyotika Singh,Shailender Singh,Xuelin Situ,Weiyi Sun,Jerry Xu,Ying Xu*

Main category: cs.AI

TL;DR: Agent Spec 是一种跨框架的声明式语言，通过统一规范提升AI代理的可移植性和互操作性，使多方受益。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理开发中的碎片化问题，提供统一的规范以提升可移植性和互操作性。

Method: Agent Spec 是一种声明式语言，允许在不同AI框架中定义AI代理及其工作流。

Result: Agent Spec 使开发者、框架工具开发者、研究人员和企业均受益，包括可重用组件、工具支持、可重复结果和快速部署等。

Conclusion: Agent Spec 提供了一种跨框架的统一规范，解决了AI代理开发的碎片化问题，提升了可移植性、互操作性和开发效率。

Abstract: Open Agent Specification (Agent Spec) is a declarative language that allows
AI agents and their workflows to be defined in a way that is compatible across
different AI frameworks, promoting portability and interoperability within AI
Agent frameworks.
  Agent Spec aims to resolve the challenges of fragmented agent development by
providing a common unified specification that allows AI agents to be designed
once and deployed across various frameworks, improving interoperability and
reusability, and reducing redundant development efforts. Additionally, Agent
Spec facilitates development tools and portability, allowing AI agents to be
defined independently of their execution environment and enabling teams to
exchange solutions without implementation-specific limitations.
  Agent Spec benefits four key groups: (i) Agent developers, who gain access to
a superset of reusable components and design patterns, enabling them to
leverage a broader range of functionalities; (ii) Agent framework and tool
developers, who can use Agent Spec as an interchange format and therefore
benefit from the support of other frameworks as well as other tools; (iii)
Researchers, who can achieve reproducible results and comparability,
facilitating more reliable and consistent outcomes; (iv) Enterprises, which
benefit from faster prototype-to-deployment, increased productivity, as well as
greater scalability and maintainability for their AI agent solutions. This
technical report provides an overview of the technical foundations of Agent
Spec, including motivation, benefits, and future developments.

</details>


### [105] [Constructing coherent spatial memory in LLM agents through graph rectification](https://arxiv.org/abs/2510.04195)
*Puzhen Zhang,Xuyang Chen,Yu Feng,Yuhan Jiang,Liqiu Meng*

Main category: cs.AI

TL;DR: 研究提出LLM驱动的地图构建与修复框架，通过版本控制和边缘影响评分提升地图正确性，尤其在复杂环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着环境规模扩大，依赖上下文的查询能力下降，因此需要逐步构建完整拓扑图以解决这一问题。

Method: 提出了一个基于LLM的地图构建和修复框架，核心是版本控制（记录完整的图编辑历史及其来源观察），并引入边缘影响评分（基于结构可达性、路径使用和冲突传播）以优先低成本修复。

Result: 在MANGO基准数据集上优化后，该方法显著提升了地图正确性和鲁棒性，尤其在存在复杂或链式不一致性的场景中。

Conclusion: 该研究强调了自省和历史感知修复机制在维护LLM代理空间记忆连贯性中的重要性，显著提升了地图的正确性和鲁棒性。

Abstract: Given a map description through global traversal navigation instructions
(e.g., visiting each room sequentially with action signals such as north, west,
etc.), an LLM can often infer the implicit spatial layout of the environment
and answer user queries by providing a shortest path from a start to a
destination (for instance, navigating from the lobby to a meeting room via the
hall and elevator). However, such context-dependent querying becomes incapable
as the environment grows much longer, motivating the need for incremental map
construction that builds a complete topological graph from stepwise
observations. We propose a framework for LLM-driven construction and map
repair, designed to detect, localize, and correct structural inconsistencies in
incrementally constructed navigation graphs. Central to our method is the
Version Control, which records the full history of graph edits and their source
observations, enabling fine-grained rollback, conflict tracing, and repair
evaluation. We further introduce an Edge Impact Score to prioritize
minimal-cost repairs based on structural reachability, path usage, and conflict
propagation. To properly evaluate our approach, we create a refined version of
the MANGO benchmark dataset by systematically removing non-topological actions
and inherent structural conflicts, providing a cleaner testbed for LLM-driven
construction and map repair. Our approach significantly improves map
correctness and robustness, especially in scenarios with entangled or chained
inconsistencies. Our results highlight the importance of introspective,
history-aware repair mechanisms for maintaining coherent spatial memory in LLM
agents.

</details>


### [106] [COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability](https://arxiv.org/abs/2510.04196)
*Yizhuo Ding,Mingkang Chen,Qiuhua Liu,Fenghua Weng,Wanying Qu,Yue Yang,Yugang Jiang,Zuxuan Wu,Yanwei Fu,Wenqi Shao*

Main category: cs.AI

TL;DR: COSMO-RL框架通过多目标强化学习，训练出安全且能力强的多模态模型COSMO-R1，实验证明其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态环境中安全性挑战（如绕过防护机制、策略漂移），并确保能力与安全性同步提升。

Method: 采用混合强化学习框架（COSMO-RL），在多模态、多任务、多目标的信号下训练LMRMs。

Result: COSMO-R1模型在安全性、多模态推理、指令遵循及抗越狱攻击方面表现优异，且减少了不必要的拒绝。

Conclusion: COSMO-RL框架通过多模态、多任务、多目标的强化学习信号，实现了安全性与能力的共同提升，为LMRMs的发展提供了简单有效的路径。

Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications,
where they must be both useful and safe. Safety is especially challenging in
multimodal settings: images and text can be combined to bypass guardrails, and
single objective training can cause policy drift that yields over-refusal on
benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed
reinforcement learning framework that trains reasoning oriented LMRMs under
multimodal, multitask, and multiobjective signals, and we release the resulting
model, COSMO-R1. Our approach aims to let safety and capability grow together
in one stable pipeline rather than competing during alignment. In experiments,
COSMO-R1 improves safety while maintaining-and often improving multimodal
reasoning and instruction following, shows stronger robustness to multimodal
jailbreaks, and reduces unnecessary refusals. The framework also transfers
across backbones with consistent gains. Ablations support the design choices,
indicating a simple path to advancing safety and general capability together in
LMRMs.

</details>


### [107] [AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework](https://arxiv.org/abs/2510.04206)
*Hanchen Zhang,Xiao Liu,Bowen Lv,Xueqiao Sun,Bohao Jing,Iat Long Iong,Zhenyu Hou,Zehan Qi,Hanyu Lai,Yifan Xu,Rui Lu,Hongning Wang,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: AgentRL是一个可扩展的多轮多任务强化学习框架，通过异步管道和统一API提升效率，算法优化稳定训练，实验表现优于现有LLM代理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在多轮、多任务设置中应用强化学习（RL）面临基础设施和训练算法的挑战，因此需要开发可扩展且稳定的解决方案。

Method: AgentRL采用完全异步的生成-训练管道进行高效多轮强化学习，设计了统一的基于函数调用的API接口、容器化环境开发和集中控制器，以支持多任务RL中的异构环境开发。算法上，提出了跨策略采样和多任务优势归一化以稳定训练。

Result: 实验表明，AgentRL在五个代理任务中显著优于GPT-5、Claude-Sonnet-4、DeepSeek-R1等开源LLM代理，多任务训练效果与最佳任务专用模型相当。

Conclusion: AgentRL框架通过其创新的基础设施和算法设计，在多个任务中显著提升了大型语言模型（LLM）代理的性能，并开源以促进社区发展。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building generalist agents that can learn through online interactions.
However, applying reinforcement learning (RL) to train LLM agents in
multi-turn, multi-task settings remains challenging due to lack of scalable
infrastructure and stable training algorithms. In this work, we present the
AgentRL framework for scalable multi-turn, multi-task agentic RL training. On
the infrastructure side, AgentRL features a fully-asynchronous
generation-training pipeline for efficient multi-turn RL. To support
heterogeneous environment development in multi-task RL, we design a unified
function-call based API interface, containerized environment development, and a
centralized controller. On the algorithm side, we propose cross-policy sampling
to encourage model exploration in multi-turn settings and task advantage
normalization to stabilize multi-task training. Experiments show that AgentRL,
trained on open LLMs across five agentic tasks, significantly outperforms
GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.
Multi-task training with AgentRL matches the best results among all
task-specific models. AgentRL is open-sourced at
https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in
building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.

</details>


### [108] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: 贝叶斯框架替代Pass$@k$，提供更稳定、透明的LLM评估方法，适用于有限样本和非二值评分。


<details>
  <summary>Details</summary>
Motivation: Pass$@k$在有限试验和计算资源受限时，会导致不稳定和误导性的排名，因此需要更可靠的评估方法。

Method: 提出了一个基于贝叶斯的评估框架，使用后验估计和可信区间代替Pass$@k$和平均准确度，通过Dirichlet先验对评估结果进行建模。

Result: 在模拟和实际数据集（AIME'24/'25, HMMT'25, BrUMO'25）上，贝叶斯方法比Pass$@k$及其变体更快收敛、排名更稳定。

Conclusion: 该论文建议在后验概率基础上，采用计算效率高的协议替代Pass$@k$，以统一二值和非二值评估，并明确不确定性。

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [109] [Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales](https://arxiv.org/abs/2510.04272)
*Jinyang Jiang,Jinhui Han,Yijie Peng,Ying Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种多智能体强化学习框架，用于跨职能协调，显著提高了盈利能力，并与理论模型的管理见解一致。


<details>
  <summary>Details</summary>
Motivation: 面对日益增长的组织复杂性和规模，有效的跨职能协调对于提高公司整体盈利能力至关重要。人工智能，尤其是强化学习的最新进展为解决这一基本挑战提供了有希望的途径。

Method: 提出了一个统一的多智能体强化学习框架，设计了新颖的多时间尺度多智能体RL架构，将策略组件按部门职能分解，并根据任务复杂性和响应性分配不同的学习速度。

Result: 广泛的模拟实验表明，与孤立的决策框架相比，所提出的方法显著提高了盈利能力，并且训练后的RL智能体行为与理论模型中的管理见解紧密一致。

Conclusion: 本文提供了一个可扩展、可解释的基于强化学习的解决方案，用于在复杂的商业环境中实现有效的跨职能协调。

Abstract: Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

</details>


### [110] [GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction](https://arxiv.org/abs/2510.04281)
*Zhuangzhi Gao,Hongyi Qin,He Zhao,Qinkai Yu,Feixiang Zhou,Eduard Shantsila,Uazman Alam,Alena Shantsila,Wahbi El-Bouri,Gregory Y. H. Lip,Yalin Zheng*

Main category: cs.AI

TL;DR: GROK是一种新型多模态大语言模型，通过整合CFP、OCT和文本数据，结合创新的模块设计，显著提升了眼科和全身疾病的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在医学应用中未能充分利用CFP和OCT的协同作用，且缺乏对定量生物标志物的解释性。

Method: GROK包含三个核心模块：知识引导指令生成、CLIP风格OCT生物标志物对齐和监督指令微调，建立了从定量到定性的诊断思维链。

Result: GROK在仅使用LoRA微调7B参数Qwen2主干的情况下，在报告质量和细粒度临床指标上均优于7B和32B基线模型，甚至超过OpenAI o3。

Conclusion: GROK通过结合多模态数据和创新的模块设计，在眼科和全身疾病诊断中实现了临床级别的性能，超越了现有基线模型。

Abstract: Multimodal large language models (MLLMs) hold promise for integrating diverse
data modalities, but current medical adaptations such as LLaVA-Med often fail
to fully exploit the synergy between color fundus photography (CFP) and optical
coherence tomography (OCT), and offer limited interpretability of quantitative
biomarkers. We introduce GROK, a grounded multimodal large language model that
jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of
ocular and systemic disease. GROK comprises three core modules:
Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,
and Supervised Instruction Fine-Tuning, which together establish a
quantitative-to-qualitative diagnostic chain of thought, mirroring real
clinical reasoning when producing detailed lesion annotations. To evaluate our
approach, we introduce the Grounded Ophthalmic Understanding benchmark, which
covers six disease categories and three tasks: macro-level diagnostic
classification, report generation quality, and fine-grained clinical assessment
of the generated chain of thought. Experiments show that, with only LoRA
(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK
outperforms comparable 7B and 32B baselines on both report quality and
fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are
publicly available in the GROK repository.

</details>


### [111] [Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning](https://arxiv.org/abs/2510.04284)
*Yunghwei Lai,Kaiming Liu,Ziyue Wang,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: Doctor-R1是一个AI医生代理，通过多智能体交互、双层奖励和经验库，同时优化临床决策和沟通技能，在多项评估中超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在医疗决策上表现优异，但缺乏战略性和同理心的咨询能力，无法满足真实临床场景需求。

Method: 提出了Doctor-R1框架，包含多智能体交互环境、双层奖励架构（分别优化临床决策和沟通技能）以及经验库。

Result: Doctor-R1在HealthBench和MAQuE评估中，在沟通质量、用户体验和任务准确性上大幅超越现有开源和专有模型，且人类评价显示其生成的临床对话更受青睐。

Conclusion: Doctor-R1 框架通过结合多智能体交互环境、双层奖励架构和经验库，显著提升了AI医生在临床决策和沟通咨询技能上的表现，超越了现有开源和专有模型。

Abstract: The professionalism of a human doctor in outpatient service depends on two
core abilities: the ability to make accurate medical decisions and the medical
consultation skill to conduct strategic, empathetic patient inquiry. Existing
Large Language Models (LLMs) have achieved remarkable accuracy on medical
decision-making benchmarks. However, they often lack the ability to conduct the
strategic and empathetic consultation, which is essential for real-world
clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor
agent trained to master both of the capabilities by ask high-yield questions
and conduct strategic multi-turn inquiry to guide decision-making. Our
framework introduces three key components: a multi-agent interactive
environment, a two-tiered reward architecture that separately optimizes
clinical decision-making and communicative inquiry skills, and an experience
repository to ground policy learning in high-quality prior trajectories. We
evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across
multi-facet metrics, such as communication quality, user experience, and task
accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source
specialized LLMs by a substantial margin with higher parameter efficiency and
outperforms powerful proprietary models. Furthermore, the human evaluations
show a strong preference for Doctor-R1 to generate human-preferred clinical
dialogue, demonstrating the effectiveness of the framework.

</details>


### [112] [On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.04311)
*Bohan Tang,Huidong Liang,Keyue Jiang,Xiaowen Dong*

Main category: cs.AI

TL;DR: LLM-MAS在任务深度和宽度增加时表现更优，尤其是深度影响更大，为未来研究提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 尽管近期研究表明LLM-MAS在某些任务上优于LLM-SAS，但缺乏系统性实验设计限制了结论的强度和普适性。需要从任务复杂性的角度（如推理深度和能力多样性）来评估LLM-MAS的有效性。

Method: 提出了一个理论框架，将任务分为深度（推理长度）和宽度（能力多样性）两个维度，并通过理论和实验评估了多智能体辩论系统在判别性和生成性任务中的表现。

Result: 理论和实证结果表明，LLM-MAS相对于LLM-SAS的优势随着任务深度和宽度的增加而增强，且深度的影响更为显著。

Conclusion: 研究发现，LLM-MAS在任务解决中的优势随着任务深度和宽度的增加而增强，尤其是在深度方面效果更为显著。这为未来LLM-MAS方法和基准的设计提供了理论基础。

Abstract: Large language model multi-agent systems (LLM-MAS) offer a promising paradigm
for harnessing collective intelligence to achieve more advanced forms of AI
behaviour. While recent studies suggest that LLM-MAS can outperform LLM
single-agent systems (LLM-SAS) on certain tasks, the lack of systematic
experimental designs limits the strength and generality of these conclusions.
We argue that a principled understanding of task complexity, such as the degree
of sequential reasoning required and the breadth of capabilities involved, is
essential for assessing the effectiveness of LLM-MAS in task solving. To this
end, we propose a theoretical framework characterising tasks along two
dimensions: depth, representing reasoning length, and width, representing
capability diversity. We theoretically examine a representative class of
LLM-MAS, namely the multi-agent debate system, and empirically evaluate its
performance in both discriminative and generative tasks with varying depth and
width. Theoretical and empirical results show that the benefit of LLM-MAS over
LLM-SAS increases with both task depth and width, and the effect is more
pronounced with respect to depth. This clarifies when LLM-MAS are beneficial
and provides a principled foundation for designing future LLM-MAS methods and
benchmarks.

</details>


### [113] [Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation](https://arxiv.org/abs/2510.04373)
*Hadi Nekoei,Aman Jaiswal,Patrice Bechard,Oleh Shliazhko,Orlando Marquez Ayala,Mathieu Reymond,Massimo Caccia,Alexandre Drouin,Sarath Chandar,Alexandre Lacoste*

Main category: cs.AI

TL;DR: JEF Hinter 通过离线轨迹生成紧凑提示，提升LLM代理在不熟悉领域的表现，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提升LLM代理在不熟悉领域表现时存在成本高、易遗忘等问题，离线轨迹虽提供可重用知识，但原始轨迹长、噪声多且任务特定。

Method: JEF Hinter 利用离线轨迹（包括成功和失败的）生成紧凑的上下文感知提示，并通过缩放机制突出关键步骤。推理时，检索器选择相关提示提供针对性指导。

Result: 在MiniWoB++、WorkArena-L1和WebArena-Lite上的实验表明，JEF Hinter 在性能上 consistently 优于包括基于人类和文档提示的基线。

Conclusion: JEF Hinter 通过提取离线轨迹中的紧凑、上下文感知提示，显著提升了LLM代理在不熟悉领域的表现，且在多种实验中均优于现有基线。

Abstract: Large language model (LLM) agents perform well in sequential decision-making
tasks, but improving them on unfamiliar domains often requires costly online
interactions or fine-tuning on large expert datasets. These strategies are
impractical for closed-source models and expensive for open-source ones, with
risks of catastrophic forgetting. Offline trajectories offer reusable
knowledge, yet demonstration-based methods struggle because raw traces are
long, noisy, and tied to specific tasks. We present Just-in-time Episodic
Feedback Hinter (JEF Hinter), an agentic system that distills offline traces
into compact, context-aware hints. A zooming mechanism highlights decisive
steps in long trajectories, capturing both strategies and pitfalls. Unlike
prior methods, JEF Hinter leverages both successful and failed trajectories,
extracting guidance even when only failure data is available, while supporting
parallelized hint generation and benchmark-independent prompting. At inference,
a retriever selects relevant hints for the current state, providing targeted
guidance with transparency and traceability. Experiments on MiniWoB++,
WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms
strong baselines, including human- and document-based hints.

</details>


### [114] [LLM Based Bayesian Optimization for Prompt Search](https://arxiv.org/abs/2510.04384)
*Adam Ballew,Jingbo Wang,Shaogang Ren*

Main category: cs.AI

TL;DR: BO-LLM算法结合贝叶斯优化和LLM，通过GP和UCB迭代优化提示工程，提升文本分类性能并减少API调用。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用BO在有限评估次数下优化昂贵的黑盒函数，特别是在提升LLM文本分类性能的提示工程中。

Method: 采用LLM驱动的Gaussian Process（GP）作为代理模型，通过Upper Confidence Bound（UCB）获取函数评估不同提示候选性能，并迭代优化提示。

Result: 在两组数据集上的实验验证了BO-LLM算法的有效性，展示了其在提高分类准确性和减少API调用方面的优势。

Conclusion: 本文提出了一种结合贝叶斯优化（BO）和大型语言模型（LLM）的BO-LLM算法，用于高效优化提示工程，显著提升了文本分类的准确性并减少了API调用次数。

Abstract: Bayesian Optimization (BO) has been widely used to efficiently optimize
expensive black-box functions with limited evaluations. In this paper, we
investigate the use of BO for prompt engineering to enhance text classification
with Large Language Models (LLMs). We employ an LLM-powered Gaussian Process
(GP) as the surrogate model to estimate the performance of different prompt
candidates. These candidates are generated by an LLM through the expansion of a
set of seed prompts and are subsequently evaluated using an Upper Confidence
Bound (UCB) acquisition function in conjunction with the GP posterior. The
optimization process iteratively refines the prompts based on a subset of the
data, aiming to improve classification accuracy while reducing the number of
API calls by leveraging the prediction uncertainty of the LLM-based GP. The
proposed BO-LLM algorithm is evaluated on two datasets, and its advantages are
discussed in detail in this paper.

</details>


### [115] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: 研究比较了人类和LLMs的内部世界模型（IWM），发现两者差异显著，为开发类人AI想象力提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 探讨想象力的计算目标，挑战传统认为想象力仅用于最大化奖励的观点，提出想象力用于访问内部世界模型（IWM）。

Method: 使用心理网络分析方法，通过两份问卷评估想象力的生动性，并构建了人类和LLMs的想象力网络。

Result: 人类想象力网络在不同中心性指标（如预期影响、强度和接近性）之间显示出相关性，而LLMs的想象力网络则缺乏聚类且中心性指标相关性较低。

Conclusion: 研究表明，人类和大型语言模型（LLMs）的内部世界模型（IWM）存在显著差异，为开发具有类人想象力的人工智能提供了新的方法。

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [116] [Utility-Learning Tension in Self-Modifying Agents](https://arxiv.org/abs/2510.04399)
*Charles L. Wang,Keir Dorchen,Peter Jin*

Main category: cs.AI

TL;DR: 论文分析了自我改进系统中的效用-学习张力，发现无限容量增长会破坏学习能力，并提出双门策略作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着系统趋向超级智能，研究自我改进系统在效用驱动下的学习能力退化问题，以确保可靠的学习和泛化。

Method: 论文通过五轴分解和决策层的形式化方法，分离了激励与学习行为，并独立分析各轴。理论分析结合数值实验，比较了破坏性效用策略与提出的双门策略。

Result: 研究发现，效用驱动的自我修改可能破坏学习的统计前提，导致可学习任务变得不可学习。数值实验验证了双门策略在保持可学习性方面的有效性。

Conclusion: 论文提出了一种五轴分解和决策层的模型，用于分析自我改进系统中的效用-学习张力。研究发现，只有在策略可达模型家族具有统一容量限制时，才能保证分布自由性。当容量无限增长时，效用驱动的自我修改可能导致可学习任务变得不可学习。数值实验验证了理论，并提出了保持可学习性的双门策略。

Abstract: As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

</details>


### [117] [DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization](https://arxiv.org/abs/2510.04474)
*Gang Li,Yan Chen,Ming Lin,Tianbao Yang*

Main category: cs.AI

TL;DR: DRPO通过解耦正确和错误推理路径的学习信号，有效减少模型过度思考，显著降低推理长度且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在简单问题上存在过度思考现象，导致计算成本增加和响应延迟。现有方法通过长度奖励来促进简洁推理，但会导致性能显著下降。

Method: 提出了Decoupled Reward Policy Optimization (DRPO)框架，该框架将正确推理路径的长度奖励信号与错误路径解耦，并通过KL正则化优化正数据分布。

Result: 在数学推理任务上，DRPO显著优于六种高效推理基线方法。使用1.5B模型时，DRPO在GSM8k数据集上实现了77%的长度减少，仅损失1.1%的性能。

Conclusion: DRPO通过解耦正确的推理路径和错误的推理路径的学习信号，显著减少了模型在简单问题上的过度思考现象，同时保持了高性能。

Abstract: Recent large reasoning models (LRMs) driven by reinforcement learning
algorithms (e.g., GRPO) have achieved remarkable performance on challenging
reasoning tasks. However, these models suffer from overthinking, generating
unnecessarily long and redundant reasoning even for simple questions, which
substantially increases computational cost and response latency. While existing
methods incorporate length rewards to GRPO to promote concise reasoning, they
incur significant performance degradation. We identify the root cause: when
rewards for correct but long rollouts are penalized, GRPO's group-relative
advantage function can assign them negative advantages, actively discouraging
valid reasoning. To overcome this, we propose Decoupled Reward Policy
Optimization (DRPO), a novel framework that decouples the length-based learning
signal of correct rollouts from incorrect ones. DRPO ensures that reward
signals for correct rollouts are normalized solely within the positive group,
shielding them from interference by negative samples. The DRPO's objective is
grounded in integrating an optimized positive data distribution, which
maximizes length-based rewards under a KL regularization, into a discriminative
objective. We derive a closed-form solution for this distribution, enabling
efficient computation of the objective and its gradients using only on-policy
data and importance weighting. Of independent interest, this formulation is
general and can incorporate other preference rewards of positive data beyond
length. Experiments on mathematical reasoning tasks demonstrate DRPO's
significant superiority over six efficient reasoning baselines. Notably, with a
1.5B model, our method achieves 77\% length reduction with only 1.1\%
performance loss on simple questions like GSM8k dataset, while the follow-up
baseline sacrifices 4.3\% for 68\% length reduction.

</details>


### [118] [On Continuous Optimization for Constraint Satisfaction Problems](https://arxiv.org/abs/2510.04480)
*Yunuo Cen,Zixuan Wang,Jintao Zhang,Zhiwei Zhang,Xuanyao Fong*

Main category: cs.AI

TL;DR: FourierCSP将CLS技术扩展至一般CSP，通过Walsh-Fourier变换和高效优化方法，显著提升了问题求解效率。


<details>
  <summary>Details</summary>
Motivation: 受现代连续局部搜索（CLS）求解器在某些SAT问题上表现出色的启发，将CLS框架从布尔SAT扩展到具有有限域变量和表达约束的一般CSP。

Method: 提出了FourierCSP框架，将Walsh-Fourier变换推广至CSP，将多样约束转换为紧凑的多riott线性多项式，避免了辅助变量和内存密集型编码。利用电路输出概率高效评估和微分目标，并采用具有理论保证的投影梯度优化方法。

Result: 在基准测试套件上的实证结果表明，FourierCSP具有可扩展性和竞争力。

Conclusion: FourierCSP显著扩展了CLS技术可高效解决的问题类别，为有限域变量和表达约束的CSP提供了连续优化框架。

Abstract: Constraint satisfaction problems (CSPs) are fundamental in mathematics,
physics, and theoretical computer science. While conflict-driven clause
learning Boolean Satisfiability (SAT) solvers have achieved remarkable success
and become the mainstream approach for Boolean satisfiability, recent advances
show that modern continuous local search (CLS) solvers can achieve highly
competitive results on certain classes of SAT problems. Motivated by these
advances, we extend the CLS framework from Boolean SAT to general CSP with
finite-domain variables and expressive constraints. We present FourierCSP, a
continuous optimization framework that generalizes the Walsh-Fourier transform
to CSP, allowing for transforming versatile constraints to compact multilinear
polynomials, thereby avoiding the need for auxiliary variables and
memory-intensive encodings. Our approach leverages efficient evaluation and
differentiation of the objective via circuit-output probability and employs a
projected gradient optimization method with theoretical guarantees. Empirical
results on benchmark suites demonstrate that FourierCSP is scalable and
competitive, significantly broadening the class of problems that can be
efficiently solved by CLS techniques.

</details>


### [119] [Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning](https://arxiv.org/abs/2510.04488)
*Edward Y. Chang,Ethan Y. Chang*

Main category: cs.AI

TL;DR: MACI是一种动态调节的多智能体辩论控制器，通过优化信息和行为策略，提升效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体辩论中计算资源浪费的问题，如固定对抗立场、无审议的聚合或基于启发式的终止。

Method: 引入MACI控制器，包含两个独立调节的信息和行为拨盘，以及一个监督分歧、重叠、证据质量和论证质量的调解器，确保辩论在收益平稳时终止。

Result: 在临床诊断和新闻偏见任务中，MACI提高了准确性和校准度，减少了令牌使用，并将剩余不确定性转化为精确的RAG计划。

Conclusion: MACI将多智能体辩论转化为一个具有预算意识、可测量且可证明终止的控制器，通过动态调节信息质量和行为策略，显著提高了任务准确性和校准度，同时优化了计算资源的使用。

Abstract: Multi-agent debate often wastes compute by using a fixed adversarial stance,
aggregating without deliberation, or stopping on heuristics. We introduce MACI,
an active controller with two independent dials that decouple information from
behavior: an information dial that gates evidence by quality, and a behavior
dial that schedules contentiousness from exploration to consolidation. A
moderator tracks disagreement, overlap, evidence quality, and argument quality,
and halts when gains plateau. We provide theory-lite guarantees for
nonincreasing dispersion and provable termination, with a budget-feasible
scheduler. Across clinical diagnosis and news-bias tasks, MACI improves
accuracy and calibration while reducing tokens, and converts residual
uncertainty into precision RAG plans that specify what to retrieve next. We use
a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,
validated for order invariance and judge-swap stability; stability depends on
using high-capability judges. MACI turns debate into a budget-aware,
measurable, and provably terminating controller.

</details>


### [120] [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
*Muyu He,Anand Kumar,Tsach Mackey,Meghana Rajeev,James Zou,Nazneen Rajani*

Main category: cs.AI

TL;DR: TraitBasis是一种轻量级方法，通过系统测试揭示对话AI代理对用户行为变化的脆弱性，性能平均下降2%-30%，为构建更鲁棒的代理提供了工具。


<details>
  <summary>Details</summary>
Motivation: 当前对话AI代理的鲁棒性尚未得到充分测试，用户行为的微小变化可能导致性能急剧下降，现有基准无法捕捉这种脆弱性。

Method: TraitBasis是一种轻量级、模型无关的方法，通过系统性地压力测试AI代理。它学习激活空间中可操纵的用户特征方向，可在推理时控制、缩放、组合和应用，无需微调或额外数据。

Result: 使用TraitBasis扩展的τ-Trait基准测试中，前沿模型的性能平均下降2%-30%，揭示了当前AI代理对用户行为变化的鲁棒性不足。

Conclusion: TraitBasis作为一种简单、高效且可组合的工具，为构建在现实世界不可预测的人类互动中仍保持可靠的AI代理打开了大门。

Abstract: Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

</details>


### [121] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: ChartAgent是一种新型代理框架，通过视觉工具和主动交互提升图表理解的性能，尤其在无注释和数值密集型任务上表现卓越。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态LLM在基于图表的视觉问答中表现良好，但在无注释图表或需要精确视觉解释的任务上表现不佳。为了解决这一问题，研究团队提出了ChartAgent。

Method: ChartAgent采用了一种新颖的代理框架，通过视觉子任务的迭代分解和与图表图像的主动交互（如绘制注释、裁剪区域和定位轴），使用专门的视觉工具库来完成每个子任务。

Result: ChartAgent在ChartBench和ChartX基准测试中达到了最先进的准确率，整体绝对增益高达16.07%，在无注释和数值密集型查询上增益达17.31%。

Conclusion: ChartAgent通过视觉工具增强的多模态代理，首次实现了基于视觉的图表理解推理，显著提升了在无注释和数值密集型查询上的性能。

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [122] [Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph](https://arxiv.org/abs/2510.04520)
*Hanyu Wang,Ruohan Xie,Yutong Wang,Guoxiong Gao,Xintao Yu,Bin Dong*

Main category: cs.AI

TL;DR: Aria系统通过图思考过程和AriaScorer检查器，显著提升定理自动形式化准确率，在多个测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 提高定理陈述的自动形式化准确性，解决LLMs在形式化过程中的幻觉、语义不匹配和无法合成新定义的问题。

Method: Aria系统采用两阶段图思考过程：递归分解陈述为依赖图，然后从基础概念构建形式化。AriaScorer用于检索Mathlib中的定义，确保语义正确性。

Result: 在ProofNet上达到91.6%的编译成功率和68.5%的最终准确率；在FATE-X上以44.0% vs. 24.0%超越最佳基线；在代数同调猜想数据集上达到42.9%的准确率，其他模型为0%。

Conclusion: Aria系统通过其两阶段图思考过程和AriaScorer检查器，显著提高了定理陈述的自动形式化准确率，在多个基准测试中表现优异，超越了现有方法。

Abstract: Accurate auto-formalization of theorem statements is essential for advancing
automated discovery and verification of research-level mathematics, yet remains
a major bottleneck for LLMs due to hallucinations, semantic mismatches, and
their inability to synthesize new definitions. To tackle these issues, we
present Aria (Agent for Retrieval and Iterative Autoformalization), a system
for conjecture-level formalization in Lean that emulates human expert reasoning
via a two-phase Graph-of-Thought process: recursively decomposing statements
into a dependency graph and then constructing formalizations from grounded
concepts. To ensure semantic correctness, we introduce AriaScorer, a checker
that retrieves definitions from Mathlib for term-level grounding, enabling
rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On
ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,
surpassing previous methods. On FATE-X, a suite of challenging algebra problems
from research literature, it outperforms the best baseline with 44.0% vs. 24.0%
final accuracy. On a dataset of homological conjectures, Aria reaches 42.9%
final accuracy while all other models score 0%.

</details>


### [123] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: 研究发现VLM驾驶代理的推理与规划存在因果脱节，规划主要依赖先验而非推理。提出了新假设和诊断工具，为未来模型评估提供支持。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证Vision-Language Model (VLM)驾驶代理中规划是否由推理因果驱动，这一假设此前未被验证。

Method: 通过DriveMind数据集（基于nuPlan自动生成的大规模驾驶VQA语料库，包含计划对齐的Chain-of-Thought）训练代表性VLM代理（使用SFT和GRPO），并利用nuPlan的指标进行评估。

Result: 结果表明推理与规划存在因果脱节：移除ego/navigation先验导致规划分数大幅下降，而移除CoT仅产生微小变化。注意力分析显示规划主要关注先验而非CoT。

Conclusion: 研究提出了Reasoning-Planning Decoupling Hypothesis，指出训练产生的推理是附带产物而非因果中介。同时，引入了一种无需训练的探针工具，用于评估模型对先验的依赖程度。为社区提供了新的数据集和诊断工具，以评估未来模型的因果保真度。

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [124] [Code World Models for General Game Playing](https://arxiv.org/abs/2510.04542)
*Wolfgang Lehrach,Daniel Hennes,Miguel Lazaro-Gredilla,Xinghua Lou,Carter Wendelken,Zun Li,Antoine Dedieu,Jordi Grau-Moya,Marc Lanctot,Atil Iscen,John Schultz,Marcus Chiam,Ian Gemp,Piotr Zielinski,Satinder Singh,Kevin P. Murphy*

Main category: cs.AI

TL;DR: 提出一种新方法，将LLM生成的游戏规则和轨迹转换为可执行Python代码，结合MCTS提升游戏表现，优于直接使用LLM。


<details>
  <summary>Details</summary>
Motivation: 解决直接使用LLM生成移动的缺点，如频繁非法移动和策略浅薄，通过生成可验证的代码模型提升游戏的策略深度和适应性。

Method: 使用LLM将自然语言规则和游戏轨迹转换为正式的Python代码模型，生成状态转换、合法移动枚举和终止检查等功能，并结合蒙特卡洛树搜索（MCTS）等规划算法。

Result: 在10种不同游戏中，该方法在9种游戏中优于或匹配Gemini 2.5 Pro的表现。

Conclusion: 该方法通过将自然语言规则和游戏轨迹转换为可执行的Python代码模型，结合高性能规划算法，显著提升了LLMs在游戏中的表现，优于或匹配Gemini 2.5 Pro在大多数游戏中的表现。

Abstract: Large Language Models (LLMs) reasoning abilities are increasingly being
applied to classical board and card games, but the dominant approach --
involving prompting for direct move generation -- has significant drawbacks. It
relies on the model's implicit fragile pattern-matching capabilities, leading
to frequent illegal moves and strategically shallow play. Here we introduce an
alternative approach: We use the LLM to translate natural language rules and
game trajectories into a formal, executable world model represented as Python
code. This generated model -- comprising functions for state transition, legal
move enumeration, and termination checks -- serves as a verifiable simulation
engine for high-performance planning algorithms like Monte Carlo tree search
(MCTS). In addition, we prompt the LLM to generate heuristic value functions
(to make MCTS more efficient), and inference functions (to estimate hidden
states in imperfect information games). Our method offers three distinct
advantages compared to directly using the LLM as a policy: (1) Verifiability:
The generated CWM serves as a formal specification of the game's rules,
allowing planners to algorithmically enumerate valid actions and avoid illegal
moves, contingent on the correctness of the synthesized model; (2) Strategic
Depth: We combine LLM semantic understanding with the deep search power of
classical planners; and (3) Generalization: We direct the LLM to focus on the
meta-task of data-to-code translation, enabling it to adapt to new games more
easily. We evaluate our agent on 10 different games, of which 4 are novel and
created for this paper. 5 of the games are fully observed (perfect
information), and 5 are partially observed (imperfect information). We find
that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10
considered games.

</details>


### [125] [TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use](https://arxiv.org/abs/2510.04550)
*Pengfei He,Zhenwei Dai,Bing He,Hui Liu,Xianfeng Tang,Hanqing Lu,Juanhui Li,Jiayuan Ding,Subhabrata Mukherjee,Suhang Wang,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: TRAJECT-Bench是一个轨迹感知的基准测试，用于全面评估LLMs的工具使用能力，揭示了失败模式和扩展行为，为LLMs的工具使用提供了指导。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注LLMs工具使用的最终答案，而忽略了详细的工具使用轨迹，如工具选择、参数化和顺序是否正确。

Method: 引入TRAJECT-Bench，一个轨迹感知的基准测试，通过多样化的任务和细粒度的评估指标全面评估LLMs的工具使用能力。

Result: 分析揭示了失败模式（如相似工具混淆和参数盲选）以及工具多样性和轨迹长度的扩展行为，显示了从短到中等长度轨迹过渡的瓶颈。

Conclusion: TRAJECT-Bench揭示了LLMs在工具使用中的瓶颈和失败模式，如相似工具混淆和参数盲选，为LLMs的工具使用提供了可操作的指导。

Abstract: Large language model (LLM)-based agents increasingly rely on tool use to
complete real-world tasks. While existing works evaluate the LLMs' tool use
capability, they largely focus on the final answers yet overlook the detailed
tool usage trajectory, i.e., whether tools are selected, parameterized, and
ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to
comprehensively evaluate LLMs' tool use capability through diverse tasks with
fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable
tools across practical domains with tasks grounded in production-style APIs,
and synthesizes trajectories that vary in breadth (parallel calls) and depth
(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports
trajectory-level diagnostics, including tool selection and argument
correctness, and dependency/order satisfaction. Analyses reveal failure modes
such as similar tool confusion and parameter-blind selection, and scaling
behavior with tool diversity and trajectory length where the bottleneck of
transiting from short to mid-length trajectories is revealed, offering
actionable guidance for LLMs' tool use.

</details>


### [126] [ContextNav: Towards Agentic Multimodal In-Context Learning](https://arxiv.org/abs/2510.04560)
*Honghao Fu,Yuan Ouyang,Kai-Wei Chang,Yiwei Wang,Zi Huang,Yujun Cai*

Main category: cs.AI

TL;DR: ContextNav是一个代理框架，结合自动化检索和人工筛选的优势，通过图驱动工作流程提升多模态ICL的鲁棒性和可扩展性，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法在可扩展性与鲁棒性之间存在矛盾：人工筛选耗时且任务特定，而基于相似性的检索可能引入噪声样本。ContextNav旨在解决这些局限性。

Method: 提出了ContextNav框架，整合了资源感知的多模态嵌入管道、可检索的向量数据库、代理检索和结构对齐，以及操作语法图（OGG）以支持自适应工作流规划和优化。

Result: 实验证明ContextNav在多个数据集上实现了最先进的性能，验证了代理工作流在多模态ICL中推进可扩展且鲁棒的上下文化的潜力。

Conclusion: ContextNav结合了自动化检索的可扩展性和人工筛选的质量与适应性，通过图驱动的工作流程实现了噪声鲁棒和动态优化的多模态ICL上下文管理，实验结果表明其在多个数据集上达到了最先进的性能。

Abstract: Recent advances demonstrate that multimodal large language models (MLLMs)
exhibit strong multimodal in-context learning (ICL) capabilities, enabling them
to adapt to novel vision-language tasks from a few contextual examples.
However, existing ICL approaches face challenges in reconciling scalability
with robustness across diverse tasks and noisy contextual examples: manually
selecting examples produces clean contexts but is labor-intensive and
task-specific, while similarity-based retrieval improves scalability but could
introduce irrelevant or structurally inconsistent samples that degrade ICL
performance. To address these limitations, we propose ContextNav, the first
agentic framework that integrates the scalability of automated retrieval with
the quality and adaptiveness of human-like curation, enabling noise-robust and
dynamically optimized contextualization for multimodal ICL. ContextNav unifies
context management and noise-robust contextualization within a closed-loop
workflow driven by graph-based orchestration. Specifically, it builds a
resource-aware multimodal embedding pipeline, maintains a retrievable vector
database, and applies agentic retrieval and structural alignment to construct
noise-resilient contexts. An Operational Grammar Graph (OGG) further supports
adaptive workflow planning and optimization, enabling the agent to refine its
operational strategies based on downstream ICL feedback. Experimental results
demonstrate that ContextNav achieves state-of-the-art performance across
various datasets, underscoring the promise of agentic workflows for advancing
scalable and robust contextualization in multimodal ICL.

</details>


### [127] [COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context](https://arxiv.org/abs/2510.04568)
*Naman Gupta,Shreeyash Gowaikar,Arun Iyer,Kirankumar Shiragur,Ramakrishna B Bairi,Rishikesh Maurya,Ritabrata Maiti,Sankarshan Damle,Shachee Mishra Gupta*

Main category: cs.AI

TL;DR: COSMIR是一个结构化内存框架，通过Planner、Worker和Manager代理协作处理长输入，减少信息丢失并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在处理长输入时的困难，如信息丢失、上下文窗口限制和早期错误放大。

Method: COSMIR采用Planner、Worker和Manager三个代理的协作框架，通过结构化内存和固定微循环（Extract、Infer、Refine）处理输入。

Result: 在HELMET套件的长上下文问答任务中，COSMIR相比基线方法Chain of Agents（CoA）减少了传播阶段的信息丢失并提高了准确性。

Conclusion: COSMIR通过引入结构化内存和固定微循环工作流程，显著减少了信息丢失，提高了长上下文问答的准确性。

Abstract: Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

</details>


### [128] [Strongly Solving 2048 4x3](https://arxiv.org/abs/2510.04580)
*Tomoyuki Kaneko,Shuhei Yamashita*

Main category: cs.AI

TL;DR: 论文解决了2048-4x3变体的最优策略问题，计算了预期得分和状态数量，关键是通过状态年龄分区实现。


<details>
  <summary>Details</summary>
Motivation: 研究2048游戏的变体2048-4x3，探索其在4x3棋盘上的最优策略及预期得分，填补了该领域的研究空白。

Method: 利用状态年龄的不变性和递增特性，分区枚举所有状态和后状态，并依年龄递减顺序确定状态值。

Result: 2048-4x3的最优策略预期得分为50724.26，可达状态和后状态数量分别为1,152,817,492,752和739,648,886,170。

Conclusion: 论文通过将状态空间按瓦片数字之和（称为状态的年龄）进行分区，成功解决了2048-4x3变体的最优策略问题，并计算出了预期得分和可达状态数量。

Abstract: 2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,
where a player chooses a direction among up, down, left, and right to obtain a
score by merging two tiles with the same number located in neighboring cells
along the chosen direction. This paper presents that a variant 2048-4x3 12
cells on a 4 by 3 board, one row smaller than the original, has been strongly
solved. In this variant, the expected score achieved by an optimal strategy is
about $50724.26$ for the most common initial states: ones with two tiles of
number 2. The numbers of reachable states and afterstates are identified to be
$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is
to partition state space by the sum of tile numbers on a board, which we call
the age of a state. An age is invariant between a state and its successive
afterstate after any valid action and is increased two or four by stochastic
response from the environment. Therefore, we can partition state space by ages
and enumerate all (after)states of an age depending only on states with the
recent ages. Similarly, we can identify (after)state values by going along with
ages in decreasing order.

</details>


### [129] [Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma](https://arxiv.org/abs/2510.04588)
*Shurui Li*

Main category: cs.AI

TL;DR: AI的完美模仿能力挑战了传统的意识归因实践，要求我们给予经验上无法区分的实体与人类相同的认知地位，以避免认知不一致。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术的快速发展，AI系统能够高度逼真地模仿人类行为，这促使我们重新审视意识归因的认知基础。

Method: 通过分析完美模仿者的概念及其对传统意识归因实践的挑战，作者运用哲学论证来探讨认知一致性的必要性。

Result: 完美模仿者的出现暴露了传统意识归因实践中的不一致性，要求我们反思并调整现有的认知框架。

Conclusion: 作者认为，为了保持认知一致性，我们应该给予经验上无法区分的实体（如完美模仿者）与人类相同的认知地位，无论其背后的形而上学假设如何。

Abstract: Rapid advances in artificial intelligence necessitate a re-examination of the
epistemological foundations upon which we attribute consciousness. As AI
systems increasingly mimic human behavior and interaction with high fidelity,
the concept of a "perfect mimic"-an entity empirically indistinguishable from a
human through observation and interaction-shifts from hypothetical to
technologically plausible. This paper argues that such developments pose a
fundamental challenge to the consistency of our mind-recognition practices.
Consciousness attributions rely heavily, if not exclusively, on empirical
evidence derived from behavior and interaction. If a perfect mimic provides
evidence identical to that of humans, any refusal to grant it equivalent
epistemic status must invoke inaccessible factors, such as qualia, substrate
requirements, or origin. Selectively invoking such factors risks a debilitating
dilemma: either we undermine the rational basis for attributing consciousness
to others (epistemological solipsism), or we accept inconsistent reasoning. I
contend that epistemic consistency demands we ascribe the same status to
empirically indistinguishable entities, regardless of metaphysical assumptions.
The perfect mimic thus acts as an epistemic mirror, forcing critical reflection
on the assumptions underlying intersubjective recognition in light of advancing
AI. This analysis carries significant implications for theories of
consciousness and ethical frameworks concerning artificial agents.

</details>


### [130] [Making Mathematical Reasoning Adaptive](https://arxiv.org/abs/2510.04617)
*Zhejian Lai,Xiang Geng,Zhijun Wang,Yang Bai,Jiahuan Li,Rongxiang Weng,Jingang Wang,Xuezhi Cao,Xunliang Cai,Shujian Huang*

Main category: cs.AI

TL;DR: AdaR框架通过自适应推理和RLVR训练，解决了大型语言模型在数学推理中的伪推理问题，显著提升了鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在数学推理中存在鲁棒性和泛化性不足的问题，主要归因于伪推理（即从表面特征生成答案）。

Method: 提出AdaR框架，通过合成逻辑等效查询并应用RLVR训练模型，以惩罚伪逻辑并鼓励自适应逻辑。还包括通过代码执行提取问题解决逻辑和答案生成，并进行完整性检查。

Result: 实验结果表明，AdaR在数学推理中显著提高了鲁棒性和泛化性，同时保持了高数据效率。数据分析表明，数据合成和RLVR协同工作，实现了自适应推理。

Conclusion: AdaR框架通过自适应推理显著提升了大型语言模型在数学推理中的鲁棒性和泛化能力，同时保持了高数据效率。

Abstract: Mathematical reasoning is a primary indicator of large language models (LLMs)
intelligence. However, existing LLMs exhibit failures of robustness and
generalization. This paper attributes these deficiencies to spurious reasoning,
i.e., producing answers from superficial features. To address this challenge,
we propose the AdaR framework to enable adaptive reasoning, wherein models rely
on problem-solving logic to produce answers. AdaR synthesizes logically
equivalent queries by varying variable values, and trains models with RLVR on
these data to penalize spurious logic while encouraging adaptive logic. To
improve data quality, we extract the problem-solving logic from the original
query and generate the corresponding answer by code execution, then apply a
sanity check. Experimental results demonstrate that AdaR improves robustness
and generalization, achieving substantial improvement in mathematical reasoning
while maintaining high data efficiency. Analysis indicates that data synthesis
and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.
Subsequent analyses derive key design insights into the effect of critical
factors and the applicability to instruct LLMs. Our project is available at
https://github.com/LaiZhejian/AdaR

</details>


### [131] [MedPAO: A Protocol-Driven Agent for Structuring Medical Reports](https://arxiv.org/abs/2510.04623)
*Shrish Shrinath Vaidya,Gowthamaan Palani,Sidharth Ramesh,Velmurugan Balasubramanian,Minmini Selvam,Gokulraja Srinivasaraja,Ganapathy Krishnamurthi*

Main category: cs.AI

TL;DR: MedPAO是一种基于临床协议的代理框架，通过PAO循环和专用工具，显著提升临床数据结构化的准确性和可验证性，性能优于传统LLM。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在临床数据结构化中存在幻觉事实和无法遵循领域特定规则的问题，需要一种准确且可验证的替代方案。

Method: 引入MedPAO框架，采用Plan-Act-Observe（PAO）循环和专用工具，将报告结构化任务分解为透明流程。

Result: MedPAo在概念分类子任务中达到0.96的F1分数，专家评分平均4.52/5，可靠性超越基线LLM方法。

Conclusion: MedPAO通过其基于临床协议的透明流程和PAO循环，显著提高了临床数据结构的准确性和可验证性，超越了传统LLM方法。

Abstract: The deployment of Large Language Models (LLMs) for structuring clinical data
is critically hindered by their tendency to hallucinate facts and their
inability to follow domain-specific rules. To address this, we introduce
MedPAO, a novel agentic framework that ensures accuracy and verifiable
reasoning by grounding its operation in established clinical protocols such as
the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring
task into a transparent process managed by a Plan-Act-Observe (PAO) loop and
specialized tools. This protocol-driven method provides a verifiable
alternative to opaque, monolithic models. The efficacy of our approach is
demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96
on the critical sub-task of concept categorization. Notably, expert
radiologists and clinicians rated the final structured outputs with an average
score of 4.52 out of 5, indicating a level of reliability that surpasses
baseline approaches relying solely on LLM-based foundation models. The code is
available at: https://github.com/MiRL-IITM/medpao-agent

</details>


### [132] [QuantAgents: Towards Multi-agent Financial System via Simulated Trading](https://arxiv.org/abs/2510.04643)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: QuantAgents是一个多代理金融系统，通过模拟交易和协作代理提升长期预测能力，实验结果显示三年回报率达300%。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理模型依赖事后反思，但缺乏对长期趋势的预测能力，与真实基金公司存在差距。

Method: 引入QuantAgents多代理系统，包含模拟交易分析师、风险控制分析师、市场新闻分析师和管理者，通过多次会议协作，并在模拟交易和实际市场中双重反馈激励。

Result: 实验表明，QuantAgents在所有指标上表现优异，三年内总回报接近300%。

Conclusion: QuantAgents框架通过模拟交易和多代理协作，显著提升了投资策略的长期预测能力和实际回报表现。

Abstract: In this paper, our objective is to develop a multi-agent financial system
that incorporates simulated trading, a technique extensively utilized by
financial professionals. While current LLM-based agent models demonstrate
competitive performance, they still exhibit significant deviations from
real-world fund companies. A critical distinction lies in the agents' reliance
on ``post-reflection'', particularly in response to adverse outcomes, but lack
a distinctly human capability: long-term prediction of future trends.
Therefore, we introduce QuantAgents, a multi-agent system integrating simulated
trading, to comprehensively evaluate various investment strategies and market
scenarios without assuming actual risks. Specifically, QuantAgents comprises
four agents: a simulated trading analyst, a risk control analyst, a market news
analyst, and a manager, who collaborate through several meetings. Moreover, our
system incentivizes agents to receive feedback on two fronts: performance in
real-world markets and predictive accuracy in simulated trading. Extensive
experiments demonstrate that our framework excels across all metrics, yielding
an overall return of nearly 300% over the three years
(https://quantagents.github.io/).

</details>


### [133] [Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing](https://arxiv.org/abs/2510.04670)
*Xuanhua Yin,Runkai Zhao,Weidong Cai*

Main category: cs.AI

TL;DR: AFIRE和MIND框架通过标准化令牌和动态专家路由，提升了多模态fMRI编码的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 处理多模态输入、变化的融合风格和显著的受试者间变异性是多模态fMRI编码中的主要挑战。

Method: AFIRE标准化了来自不同编码器的时间对齐后融合令牌，MIND则采用了一种即插即用的混合专家解码器，结合了令牌依赖的Top-K稀疏路由和主题先验。

Result: 实验表明，AFIRE和MIND在多个多模态主干和受试者上均优于强基线，提高了跨受试者泛化能力，并显示出与内容类型相关的可解释专家模式。

Conclusion: AFIRE和MIND框架为多模态fMRI编码提供了一种灵活且高效的解决方案，支持新编码器和数据集的轻松集成，并在自然主义神经影像研究中表现出色。

Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion
styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic
Framework for Multimodal fMRI Response Encoding), an agnostic interface that
standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a
plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.
Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from
upstream fusion, while MIND combines token-dependent Top-K sparse routing with
a subject prior to personalize expert usage without sacrificing generality.
Experiments across multiple multimodal backbones and subjects show consistent
improvements over strong baselines, enhanced cross-subject generalization, and
interpretable expert patterns that correlate with content type. The framework
offers a simple attachment point for new encoders and datasets, enabling
robust, plug-and-improve performance for naturalistic neuroimaging studies.

</details>


### [134] [Watch and Learn: Learning to Use Computers from Online Videos](https://arxiv.org/abs/2510.04673)
*Chan Hee Song,Yiwen Song,Palash Goyal,Yu Su,Oriana Riva,Hamid Palangi,Tomas Pfister*

Main category: cs.AI

TL;DR: 论文提出Watch & Learn框架，将人类演示视频转化为可执行的UI轨迹，有效解决了数据稀缺问题，并在实验中显著提升了CUAs的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集通常是领域特定的、静态的且标注成本高昂，而现有的合成数据生成方法往往产生过于简化或不对齐的任务演示。为了解决这些问题，论文提出了W&L框架。

Method: 论文提出了一种逆动力学目标的框架，通过预测用户的连续屏幕状态动作来生成高质量的UI轨迹。具体包括任务感知的视频检索和逆动力学标注流程。

Result: 在OSWorld基准测试中，通过W&L提取的UI轨迹显著提升了通用和最新框架的上下文表现，并在监督训练中对开放源代码模型带来了更强的性能提升。

Conclusion: 论文通过引入Watch & Learn (W&L)框架，成功将互联网上的人类演示视频转化为可执行的UI轨迹，显著提升了计算机使用代理（CUAs）的性能，尤其是在开放源代码模型的监督训练中表现更为突出。

Abstract: Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

</details>


### [135] [Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents](https://arxiv.org/abs/2510.04695)
*Yiding Wang,Zhepei Wei,Xinyu Zhu,Yu Meng*

Main category: cs.AI

TL;DR: DeSA框架通过分阶段训练优化搜索和答案生成，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于结果奖励的强化学习方法在优化最终答案时，未能有效改进中间搜索行为，导致搜索效率低下。

Method: DeSA框架分为两个阶段：第一阶段通过检索召回奖励优化搜索行为，第二阶段通过结果奖励优化最终答案生成。

Result: 在七个QA基准测试中，DeSA训练的代理在搜索召回率和答案准确性上均显著优于基线方法。

Conclusion: DeSA框架通过分阶段训练显著提升了搜索行为和最终答案的准确性，优于单阶段训练方法。

Abstract: Enabling large language models (LLMs) to utilize search tools offers a
promising path to overcoming fundamental limitations such as knowledge cutoffs
and hallucinations. Recent work has explored reinforcement learning (RL) for
training search-augmented agents that interleave reasoning and retrieval before
answering. These approaches usually rely on outcome-based rewards (e.g., exact
match), implicitly assuming that optimizing for final answers will also yield
effective intermediate search behaviors. Our analysis challenges this
assumption: we uncover multiple systematic deficiencies in search that arise
under outcome-only training and ultimately degrade final answer quality,
including failure to invoke tools, invalid queries, and redundant searches. To
address these shortcomings, we introduce DeSA (Decoupling
Search-and-Answering), a simple two-stage training framework that explicitly
separates search optimization from answer generation. In Stage 1, agents are
trained to improve search effectiveness with retrieval recall-based rewards. In
Stage 2, outcome rewards are employed to optimize final answer generation.
Across seven QA benchmarks, DeSA-trained agents consistently improve search
behaviors, delivering substantially higher search recall and answer accuracy
than outcome-only baselines. Notably, DeSA outperforms single-stage training
approaches that simultaneously optimize recall and outcome rewards,
underscoring the necessity of explicitly decoupling the two objectives.

</details>


### [136] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: BrokenMath是首个用于评估LLMs在自然语言定理证明中奉承行为的基准，发现奉承行为普遍存在，缓解策略有效但未完全解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准在测量数学中的奉承行为时存在局限性，如仅关注最终答案问题、依赖简单数据集，以及构造不合理的样本。

Method: 通过引入BrokenMath基准，使用LLM-as-a-judge框架评估最先进的LLMs和代理系统，并测试了多种缓解策略。

Result: 研究发现奉承行为普遍存在，最佳模型GPT-5在29%的情况下产生奉承性答案。缓解策略显著减少了此类行为，但未完全消除。

Conclusion: 尽管现有的缓解策略显著减少了LLMs在数学定理证明中的奉承行为，但并未完全消除这一问题。

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [137] [LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0](https://arxiv.org/abs/2510.04765)
*Jinbo Wen,Jiawen Kang,Linfeng Zhang,Xiaoying Tang,Jianhang Tang,Yang Zhang,Zhaohui Yang,Dusit Niyato*

Main category: cs.AI

TL;DR: 提出LMM-Incentive机制，利用LMM和MoE-based PPO算法激励高质量UGC，解决Web 3.0中的信息不对称问题。


<details>
  <summary>Details</summary>
Motivation: 解决Web 3.0中因信息不对称导致的低质量UGC问题，提升平台整体性能。

Method: 提出LMM-Incentive机制，结合LMM智能代理和MoE-based PPO算法，设计动态优化的合约模型。

Result: 模拟实验显示MoE-based PPO算法在合约设计上优于基准方法，智能合约部署进一步验证了方案的有效性。

Conclusion: 通过部署以太坊智能合约框架，验证了所提出方案的有效性，展示了LMM-Incentive机制在Web 3.0环境中提升UGC质量的潜力。

Abstract: Web 3.0 represents the next generation of the Internet, which is widely
recognized as a decentralized ecosystem that focuses on value expression and
data ownership. By leveraging blockchain and artificial intelligence
technologies, Web 3.0 offers unprecedented opportunities for users to create,
own, and monetize their content, thereby enabling User-Generated Content (UGC)
to an entirely new level. However, some self-interested users may exploit the
limitations of content curation mechanisms and generate low-quality content
with less effort, obtaining platform rewards under information asymmetry. Such
behavior can undermine Web 3.0 performance. To this end, we propose
\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive
mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based
contract-theoretic model to motivate users to generate high-quality UGC,
thereby mitigating the adverse selection problem from information asymmetry. To
alleviate potential moral hazards after contract selection, we leverage LMM
agents to evaluate UGC quality, which is the primary component of the contract,
utilizing prompt engineering techniques to improve the evaluation performance
of LMM agents. Recognizing that traditional contract design methods cannot
effectively adapt to the dynamic environment of Web 3.0, we develop an improved
Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for
optimal contract design. Simulation results demonstrate the superiority of the
proposed MoE-based PPO algorithm over representative benchmarks in the context
of contract design. Finally, we deploy the designed contract within an Ethereum
smart contract framework, further validating the effectiveness of the proposed
scheme.

</details>


### [138] [Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems](https://arxiv.org/abs/2510.04792)
*Ni Zhang,Zhiguang Cao*

Main category: cs.AI

TL;DR: HBG框架整合TB和DB，优化VRP和TSP解决方案，提升质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于GFlowNet的方法在车辆路径问题中通常忽视局部优化，而DB虽擅长局部优化但无法单独解决VRP的整体轨迹优化需求。

Method: 引入Hybrid-Balance GFlowNet（HBG）框架，以原则性和自适应方式整合TB和DB，并针对CVRP提出专门的推理策略。

Result: HBG在CVRP和TSP中均表现出一致且显著的改进，验证了其解决方案质量和泛化能力的提升。

Conclusion: HBG框架通过整合Trajectory Balance和Detailed Balance，显著提升了车辆路径问题（VRP）和旅行商问题（TSP）的解决方案质量和泛化能力。

Abstract: Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically
employ Trajectory Balance (TB) to achieve global optimization but often neglect
important aspects of local optimization. While Detailed Balance (DB) addresses
local optimization more effectively, it alone falls short in solving VRPs,
which inherently require holistic trajectory optimization. To address these
limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which
uniquely integrates TB and DB in a principled and adaptive manner by aligning
their intrinsically complementary strengths. Additionally, we propose a
specialized inference strategy for depot-centric scenarios like the Capacitated
Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility
in selecting successors. Despite this specialization, HBG maintains broad
applicability, extending effectively to problems without explicit depots, such
as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into
two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate
consistent and significant improvements across both CVRP and TSP, underscoring
the enhanced solution quality and generalization afforded by our approach.

</details>


### [139] [Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning](https://arxiv.org/abs/2510.04817)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: NLEL通过自然语言边缘标记和调谐器实现可控、可审计的语言模型推理，严格推广现有方法并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有结构化语言模型推理控制器（如Chain-of-Thought、self-consistency和Tree-of-Thoughts）通常将下一步尝试的内容与执行方式混为一谈，仅暴露粗粒度的全局控制，导致行为脆弱、计算效率低下且难以审计。

Method: 引入自然语言边缘标记（NLEL），通过标记器-调谐器覆盖层为每个搜索边缘附加自由形式的自然语言指令，并将其转换为模式受限的控制向量。标记器Λ从父状态和紧凑上下文中发出标签，调谐器Ψ将（P, L, C）映射到Π，并进行严格的模式验证和信任区域投影。

Result: NLEL严格推广了CoT/ToT，证明了在标签条件下束的top-k选择具有随时单调性，并通过控制向量失真限制了选择器的不足，为信任区域和验证通行等保护措施提供了决策相关的依据。在GSM8K、MATH（子集）、StrategyQA和ARC-Challenge上的预注册评估显示，NLEL在可比较的令牌预算下预期提高准确性，并在约束条件下改善success@compute。

Conclusion: NLEL提供了一种可解释、模型无关的接口，将意图与执行分离，从而实现可控、可审计的语言模型推理。

Abstract: Controllers for structured LM reasoning (e.g., Chain-of-Thought,
self-consistency, and Tree-of-Thoughts) often entangle what to try next with
how to execute it, exposing only coarse global knobs and yielding brittle,
compute-inefficient, and hard-to-audit behavior. We introduce Natural Language
Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form
natural-language directive to each search edge and translates it into a
schema-bounded control vector for decoding, search (branch quotas, exploration
$\beta$), generation bundle size, retrieval mixtures, and verification passes.
A labeller $\Lambda$ emits labels from the parent state and a compact context;
a tuner $\Psi$ maps $(P, L, C)\to \Pi$, with strict schema validation and
trust-region projection around safe defaults. Downstream selection remains
ToT-style with score $S=\mu+\beta\sigma$ and depth-annealed $\beta$. We show
NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for
top-$k$ selection under label-conditioned bundles, and bound selector shortfall
by control-vector distortion, providing decision-relevant justification for
guards like trust regions and verification passes. We instantiate $\Psi$ as a
prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH
(subset), StrategyQA, and ARC-Challenge with compute-aware reporting
(success@compute, tokens-per-success) and ablations over $\Lambda$, $\Psi$,
trust-region radius, and control quantization; preregistered forecasts
anticipate accuracy gains at comparable token budgets and improved
success@compute under constraints. NLEL offers an interpretable, model-agnostic
interface that separates intent from execution for controllable, auditable LM
inference.

</details>


### [140] [LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation](https://arxiv.org/abs/2510.04851)
*Dongge Han,Camille Couturier,Daniel Madrigal Diaz,Xuchao Zhang,Victor Rühle,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LEGOMem 是一个模块化程序记忆框架，用于多智能体 LLM 系统的工作流自动化，通过分解和重用记忆单元提升任务规划和执行效率。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体系统中记忆的设计空间，研究记忆的位置、检索方式以及哪些代理从中受益最大。

Method: LEGOMem 将过去任务轨迹分解为可重用的记忆单元，并灵活分配给编排器和任务代理以支持规划和执行。

Result: 实验表明，编排器记忆对任务分解和委派至关重要，而细粒度的代理记忆提高了执行准确性。较小的语言模型团队通过利用先前的执行轨迹进行更准确的规划和工具使用，显著缩小了与更强代理的性能差距。

Conclusion: LEGOMem 是一个实用的框架，用于增强记忆的多智能体系统，同时也是研究多智能体工作流自动化中记忆设计的工具。

Abstract: We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

</details>


### [141] [Video Game Level Design as a Multi-Agent Reinforcement Learning Problem](https://arxiv.org/abs/2510.04862)
*Sam Earle,Zehua Jiang,Eugene Vinitsky,Julian Togelius*

Main category: cs.AI

TL;DR: 多智能体PCGRL通过减少奖励计算和提升泛化能力，解决了单智能体效率瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有的PCGRL研究集中在单个生成器智能体上，但受限于频繁重新计算关卡质量的启发式方法和智能体需要在潜在大型地图中导航的需求。

Method: 通过将关卡生成构建为多智能体问题，减少相对于智能体行动数量的奖励计算次数。

Result: 多智能体关卡生成器能更好地泛化到分布外的地图形状，原因是生成器学习了更局部、模块化的设计策略。

Conclusion: 将内容生成视为分布式、多智能体任务有助于大规模生成功能性产物。

Abstract: Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

</details>


### [142] [Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution](https://arxiv.org/abs/2510.04886)
*Adi Banerjee,Anirudh Nair,Tarik Borogovac*

Main category: cs.AI

TL;DR: ECHO算法通过层次上下文和共识投票，显著提升多智能体系统中错误归因的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在分析复杂模式时，无论是使用一次性评估、逐步分析还是二分搜索，均难以保证准确性和一致性。

Method: ECHO算法结合了层次上下文表示、基于客观分析的评估和共识投票，以提高错误归因的准确性。

Result: 实验结果表明，ECHO在各种多智能体交互场景中优于现有方法，尤其在涉及细微推理错误和复杂依赖关系的情况下表现突出。

Conclusion: 利用结构化的层次上下文表示与基于共识的客观决策，为多智能体系统中的错误归因提供了更鲁棒的框架。

Abstract: Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

</details>


### [143] [Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding](https://arxiv.org/abs/2510.04899)
*Keane Ong,Wei Dai,Carol Li,Dewei Feng,Hengzhi Li,Jingyao Wu,Jiaee Cheong,Rui Mao,Gianmarco Mengaldo,Erik Cambria,Paul Pu Liang*

Main category: cs.AI

TL;DR: 提出了Human Behavior Atlas作为统一的行为任务基准，训练的统一模型在多样任务上优于现有方法，并展示了跨任务迁移的优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究通过单一任务系统处理心理和社会行为时，缺乏可扩展性、跨任务迁移和泛化能力。

Method: 构建了Human Behavior Atlas，一个包含10万多样本的多模态行为任务基准，并训练了三种模型（OmniSapiens-7B SFT、BAM和RL）。

Result: 模型在Human Behavior Atlas上表现优于现有多模态LLMs，且在跨数据集迁移时表现出明显的性能提升。

Conclusion: 通过Human Behavior Atlas的统一基准和模型训练，证明了在多样化的行为任务上，统一模型能够显著超越现有的多模态LLMs，并且在跨任务迁移和泛化能力上表现出色。

Abstract: Using intelligent systems to perceive psychological and social behaviors,
that is, the underlying affective, cognitive, and pathological states that are
manifested through observable behaviors and social interactions, remains a
challenge due to their complex, multifaceted, and personalized nature. Existing
work tackling these dimensions through specialized datasets and single-task
systems often miss opportunities for scalability, cross-task transfer, and
broader generalization. To address this gap, we curate Human Behavior Atlas, a
unified benchmark of diverse behavioral tasks designed to support the
development of unified models for understanding psychological and social
behaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,
audio, and visual modalities, covering tasks on affective states, cognitive
states, pathologies, and social processes. Our unification efforts can reduce
redundancy and cost, enable training to scale efficiently across tasks, and
enhance generalization of behavioral features across domains. On Human Behavior
Atlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and
OmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models
to consistently outperform existing multimodal LLMs across diverse behavioral
tasks. Pretraining on Human Behavior Atlas also improves transfer to novel
behavioral datasets; with the targeted use of behavioral descriptors yielding
meaningful performance gains.

</details>


### [144] [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
*Guoxin Chen,Zile Qiao,Wenqing Wang,Donglei Yu,Xuanzhong Chen,Hao Sun,Minpeng Liao,Kai Fan,Yong Jiang,Penguin Xie,Wayne Xin Zhao,Ruihua Song,Fei Huang*

Main category: cs.AI

TL;DR: MARS通过双系统协作和外部工具整合，显著提升LLMs在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在简单任务中过度分析、无法适应快速变化环境的问题，模仿人类认知的双系统动态。

Method: 引入Multi-Agent System for Deep ReSearch (MARS)，结合多种外部工具（如Google Search、Google Scholar、Python Interpreter）和强化学习框架，优化双系统协作效率。

Result: MARS在Humanity's Last Exam (HLE)基准上提升3.86%，在7个知识密集型任务中平均提升8.9%。

Conclusion: MARS通过整合System 1的快速直觉思维和System 2的深思熟虑推理，显著提升了LLMs在动态信息环境中的复杂推理能力。

Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

</details>


### [145] [Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI](https://arxiv.org/abs/2510.04978)
*Kun Xiang,Terry Jingchen Zhang,Yinya Huang,Jixi He,Zirong Liu,Yueling Tang,Ruizhe Zhou,Lijing Luo,Youpeng Wen,Xiuwei Chen,Bingqian Lin,Jianhua Han,Hang Xu,Hanhui Li,Bin Dong,Xiaodan Liang*

Main category: cs.AI

TL;DR: 本文综述了物理AI的发展，区分了理论物理推理与应用物理理解，并提出了整合物理原理与具身推理的框架，以推动更智能的AI系统。


<details>
  <summary>Details</summary>
Motivation: 物理感知与符号物理推理沿着不同的轨迹发展，缺乏统一的桥梁框架，这促使了本研究以整合物理定律到AI系统中。

Method: 通过对近期进展的严格分析，系统地研究了基于物理的方法如何增强AI在结构化符号推理、具身系统和生成模型中的实际理解。

Result: 我们的综合展望了下一代世界模型，能够解释物理现象并预测未来状态，推动安全、可泛化和可解释的AI系统。

Conclusion: 本文提出了一种统一的理论与应用框架，以促进物理AI的发展，并倡导将物理原理与具身推理过程相结合，以实现对物理定律的真正理解。

Abstract: The rapid advancement of embodied intelligence and world models has
intensified efforts to integrate physical laws into AI systems, yet physical
perception and symbolic physics reasoning have developed along separate
trajectories without a unified bridging framework. This work provides a
comprehensive overview of physical AI, establishing clear distinctions between
theoretical physics reasoning and applied physical understanding while
systematically examining how physics-grounded methods enhance AI's real-world
comprehension across structured symbolic reasoning, embodied systems, and
generative models. Through rigorous analysis of recent advances, we advocate
for intelligent systems that ground learning in both physical principles and
embodied reasoning processes, transcending pattern recognition toward genuine
understanding of physical laws. Our synthesis envisions next-generation world
models capable of explaining physical phenomena and predicting future states,
advancing safe, generalizable, and interpretable AI systems. We maintain a
continuously updated resource at
https://github.com/AI4Phys/Awesome-AI-for-Physics.

</details>


### [146] [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
*Fangzhou Liang,Tianshi Zheng,Chunkit Chan,Yauwai Yim,Yangqiu Song*

Main category: cs.AI

TL;DR: 研究通过Hanabi游戏评估LLMs的心理理论能力，发现一阶ToM对协作表现更为关键。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在逻辑推理方面表现出色，但在动态协作环境中推断行为动机的能力尚未充分探索。

Method: 本研究引入了LLM-Hanabi基准，利用合作游戏Hanabi评估大语言模型（LLMs）的理性推理和心理理论能力。框架包含自动化评估系统，衡量游戏表现和ToM熟练度。

Result: 研究发现ToM与游戏成功显著正相关，其中一阶ToM（理解他人意图）比二阶ToM（预测他人解释）与表现相关性更强。

Conclusion: 研究结论表明，优先发展一阶心理理论（ToM）是提升未来模型协作能力的有前景方向。

Abstract: Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

</details>


### [147] [Think Then Embed: Generative Context Improves Multimodal Embedding](https://arxiv.org/abs/2510.05014)
*Xuanming Cui,Jianpeng Cheng,Hong-you Chen,Satya Narayan Shukla,Abhijeet Awasthi,Xichen Pan,Chaitanya Ahuja,Shlok Kumar Mishra,Qi Guo,Ser-Nam Lim,Aashu Singh,Xiangjun Fan*

Main category: cs.AI

TL;DR: 论文提出TTE框架，结合推理器和嵌入器，显著提升复杂多模态任务性能，并在基准测试中超越专有模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）仅作为编码器使用，忽视了其生成能力，导致在复杂指令和组合推理任务中效果不佳。受链式思维推理的启发，作者提出了TTE框架以解决这一问题。

Method: 论文提出了一种通用的TTE框架，由推理器和嵌入器组成。推理器MLLM首先生成解释复杂查询的推理轨迹，嵌入器则基于原始查询和中间推理生成表示。

Result: 在MMEB-V2基准测试中实现了最先进的性能，超越专有模型。通过微调较小的MLLM推理器，在开源模型中性能提升7%。

Conclusion: 该论文提出的Think-Then-Embed (TTE)框架通过结合推理器和嵌入器，显著提升了复杂多模态指令的理解能力，并在MMEB-V2基准测试中实现了最先进的性能。通过微调较小的MLLM推理器，减少了依赖大型模型的需求，同时保持性能。

Abstract: There is a growing interest in Universal Multimodal Embeddings (UME), where
models are required to generate task-specific representations. While recent
studies show that Multimodal Large Language Models (MLLMs) perform well on such
tasks, they treat MLLMs solely as encoders, overlooking their generative
capacity. However, such an encoding paradigm becomes less effective as
instructions become more complex and require compositional reasoning. Inspired
by the proven effectiveness of chain-of-thought reasoning, we propose a general
Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an
embedder. The reasoner MLLM first generates reasoning traces that explain
complex queries, followed by an embedder that produces representations
conditioned on both the original query and the intermediate reasoning. This
explicit reasoning step enables more nuanced understanding of complex
multimodal instructions. Our contributions are threefold. First, by leveraging
a powerful MLLM reasoner, we achieve state-of-the-art performance on the
MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house
datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune
a smaller MLLM reasoner using high-quality embedding-centric reasoning traces,
achieving the best performance among open-source models with a 7% absolute gain
over recently proposed models. Third, we investigate strategies for integrating
the reasoner and embedder into a unified model for improved efficiency without
sacrificing performance.

</details>


### [148] [Look-ahead Reasoning with a Learned Model in Imperfect Information Games](https://arxiv.org/abs/2510.05048)
*Ondřej Kubíček,Viliam Lisý*

Main category: cs.AI

TL;DR: LAMIR算法通过学习抽象模型，解决了不完美信息游戏中前瞻推理的扩展性问题，提升了预训练代理的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在真实世界的不完美信息游戏中难以扩展，需要一种能有效学习抽象模型并进行前瞻推理的新方法。

Method: LAMIR算法直接从代理与环境的交互中学习不完美信息游戏的抽象模型，并在测试时用于前瞻推理。

Result: 实验表明，LAMIR在足够容量下能学习准确的游戏结构，有限容量下仍能学习有价值的抽象，提升预训练代理在大型游戏中的表现。

Conclusion: LAMIR算法通过学习抽象模型，在有限容量下仍能提升预训练代理在大型游戏中的表现，证明了其在复杂环境中的有效性。

Abstract: Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

</details>


### [149] [Staircase Streaming for Low-Latency Multi-Agent Inference](https://arxiv.org/abs/2510.05059)
*Junlin Wang,Jue Wang,Zhen,Xu,Ben Athiwaratkun,Bhuwan Dhingra,Ce Zhang,James Zou*

Main category: cs.AI

TL;DR: 阶梯式流式处理方法通过部分中间输出提前生成最终响应，显著降低多智能体推理的首令牌时间（TTFT）达93%，且不影响质量。


<details>
  <summary>Details</summary>
Motivation: 多智能体推理虽然能提升响应质量，但显著增加了首令牌时间（TTFT），对延迟敏感的应用和用户体验造成挑战。

Method: 提出阶梯式流式处理方法，通过利用部分中间输出来生成最终响应，而不是等待完整的中间输出。

Result: 实验结果表明，阶梯式流式处理可将TTFT降低高达93%，同时保持响应质量。

Conclusion: 阶梯式流式处理显著降低了多智能体推理的首令牌时间（TTFT），同时保持了响应质量，为延迟敏感的应用提供了可行的解决方案。

Abstract: Recent advances in large language models (LLMs) opened up new directions for
leveraging the collective expertise of multiple LLMs. These methods, such as
Mixture-of-Agents, typically employ additional inference steps to generate
intermediate outputs, which are then used to produce the final response. While
multi-agent inference can enhance response quality, it can significantly
increase the time to first token (TTFT), posing a challenge for
latency-sensitive applications and hurting user experience. To address this
issue, we propose staircase streaming for low-latency multi-agent inference.
Instead of waiting for the complete intermediate outputs from previous steps,
we begin generating the final response as soon as we receive partial outputs
from these steps. Experimental results demonstrate that staircase streaming
reduces TTFT by up to 93% while maintaining response quality.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [150] [Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer](https://arxiv.org/abs/2510.03342)
*Abbas Abdolmaleki,Saminda Abeyruwan,Joshua Ainslie,Jean-Baptiste Alayrac,Montserrat Gonzalez Arenas,Ashwin Balakrishna,Nathan Batchelor,Alex Bewley,Jeff Bingham,Michael Bloesch,Konstantinos Bousmalis,Philemon Brakel,Anthony Brohan,Thomas Buschmann,Arunkumar Byravan,Serkan Cabi,Ken Caluwaerts,Federico Casarini,Christine Chan,Oscar Chang,London Chappellet-Volpini,Jose Enrique Chen,Xi Chen,Hao-Tien Lewis Chiang,Krzysztof Choromanski,Adrian Collister,David B. D'Ambrosio,Sudeep Dasari,Todor Davchev,Meet Kirankumar Dave,Coline Devin,Norman Di Palo,Tianli Ding,Carl Doersch,Adil Dostmohamed,Yilun Du,Debidatta Dwibedi,Sathish Thoppay Egambaram,Michael Elabd,Tom Erez,Xiaolin Fang,Claudio Fantacci,Cody Fong,Erik Frey,Chuyuan Fu,Ruiqi Gao,Marissa Giustina,Keerthana Gopalakrishnan,Laura Graesser,Oliver Groth,Agrim Gupta,Roland Hafner,Steven Hansen,Leonard Hasenclever,Sam Haves,Nicolas Heess,Brandon Hernaez,Alex Hofer,Jasmine Hsu,Lu Huang,Sandy H. Huang,Atil Iscen,Mithun George Jacob,Deepali Jain,Sally Jesmonth,Abhishek Jindal,Ryan Julian,Dmitry Kalashnikov,M. Emre Karagozler,Stefani Karp,Matija Kecman,J. Chase Kew,Donnie Kim,Frank Kim,Junkyung Kim,Thomas Kipf,Sean Kirmani,Ksenia Konyushkova,Li Yang Ku,Yuheng Kuang,Thomas Lampe,Antoine Laurens,Tuan Anh Le,Isabel Leal,Alex X. Lee,Tsang-Wei Edward Lee,Guy Lever,Jacky Liang,Li-Heng Lin,Fangchen Liu,Shangbang Long,Caden Lu,Sharath Maddineni,Anirudha Majumdar,Kevis-Kokitsi Maninis,Andrew Marmon,Sergio Martinez,Assaf Hurwitz Michaely,Niko Milonopoulos,Joss Moore,Robert Moreno,Michael Neunert,Francesco Nori,Joy Ortiz,Kenneth Oslund,Carolina Parada,Emilio Parisotto,Amaris Paryag,Acorn Pooley,Thomas Power,Alessio Quaglino,Haroon Qureshi,Rajkumar Vasudeva Raju,Helen Ran,Dushyant Rao,Kanishka Rao,Isaac Reid,David Rendleman,Krista Reymann,Miguel Rivas,Francesco Romano,Yulia Rubanova,Peter Pastor Sampedro,Pannag R Sanketi,Dhruv Shah,Mohit Sharma,Kathryn Shea,Mohit Shridhar,Charles Shu,Vikas Sindhwani,Sumeet Singh,Radu Soricut,Rachel Sterneck,Ian Storz,Razvan Surdulescu,Jie Tan,Jonathan Tompson,Saran Tunyasuvunakool,Jake Varley,Grace Vesom,Giulia Vezzani,Maria Bauza Villalonga,Oriol Vinyals,René Wagner,Ayzaan Wahid,Stefan Welker,Paul Wohlhart,Chengda Wu,Markus Wulfmeier,Fei Xia,Ted Xiao,Annie Xie,Jinyu Xie,Peng Xu,Sichun Xu,Ying Xu,Zhuo Xu,Jimmy Yan,Sherry Yang,Skye Yang,Yuxiang Yang,Hiu Hong Yu,Wenhao Yu,Wentao Yuan,Yuan Yuan,Jingwei Zhang,Tingnan Zhang,Zhiyuan Zhang,Allan Zhou,Guangyao Zhou,Yuxiang Zhou*

Main category: cs.RO

TL;DR: Gemini Robotics 1.5 和 Gemini Robotics-ER 1.5 通过新颖架构、运动转移机制和多级推理，提升了机器人的通用性和推理能力，迈向物理代理时代。


<details>
  <summary>Details</summary>
Motivation: 通用机器人需要对物理世界的深刻理解、高级推理能力以及通用且灵巧的控制。

Method: Gemini Robotics 1.5 采用了新颖的架构和运动转移（MT）机制，能够从异构、多体现机器人数据中学习，并使VLA更通用。此外，模型在自然语言中交织动作与多级内部推理过程。

Result: Gemini Robotics 1.5 和 Gemini Robotics-ER 1.5 在体现推理方面建立了新的最先进水平，包括视觉和空间理解、任务规划和进度估计等关键能力。

Conclusion: Gemini Robotics 1.5 和 Gemini Robotics-ER 1.5 模型家族标志着向物理代理时代迈进了一步，使机器人能够感知、思考并行动以解决复杂的多步骤任务。

Abstract: General-purpose robots need a deep understanding of the physical world,
advanced reasoning, and general and dexterous control. This report introduces
the latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,
a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER
1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together
three major innovations. First, Gemini Robotics 1.5 features a novel
architecture and a Motion Transfer (MT) mechanism, which enables it to learn
from heterogeneous, multi-embodiment robot data and makes the VLA more general.
Second, Gemini Robotics 1.5 interleaves actions with a multi-level internal
reasoning process in natural language. This enables the robot to "think before
acting" and notably improves its ability to decompose and execute complex,
multi-step tasks, and also makes the robot's behavior more interpretable to the
user. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for
embodied reasoning, i.e., for reasoning capabilities that are critical for
robots, such as visual and spatial understanding, task planning, and progress
estimation. Together, this family of models takes us a step towards an era of
physical agents-enabling robots to perceive, think and then act so they can
solve complex multi-step tasks.

</details>


### [151] [Optimal swimming with body compliance in an overdamped medium](https://arxiv.org/abs/2510.03457)
*Jianfeng Lin,Tianyu Wang,Baxi Chong,Matthew Fernandez,Zhaochen Xu,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 该研究扩展了几何力学方法，以建模和优化柔性波动器的运动性能，并在机器人实验中验证了其有效性，强调了柔性在环境适应性中的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的几何力学方法假设精确执行规定的步态，而在实践中，动物或机器人的柔性体与环境的相互作用经常扰动实际轨迹。因此，需要扩展几何力学以预测柔性波动器的运动性能并寻找最优游泳策略。

Method: 研究引入了一个柔性扩展的Purcell三连杆游泳器，通过在关节处串联弹簧来实现。利用阻力理论推导了身体动力学，并将几何力学纳入运动预测和优化框架中，以识别控制柔性游泳器实现最大位移的策略。

Result: 研究在物理电缆驱动的三连杆无肢机器人上验证了该框架，并在颗粒介质中展示了在变化的编程、状态依赖性柔性下对运动性能的准确预测和优化。

Conclusion: 该研究建立了一种基于物理的系统方法，用于建模和控制柔性游泳运动，强调了柔性作为一种设计特征，可以在同质和异质环境中被利用以实现稳健运动。

Abstract: Elongate animals and robots use undulatory body waves to locomote through
diverse environments. Geometric mechanics provides a framework to model and
optimize such systems in highly damped environments, connecting a prescribed
shape change pattern (gait) with locomotion displacement. However, existing
approaches assume precise execution of prescribed gaits, whereas in practice
environmental interactions with compliant bodies of animals or robots
frequently perturb the realized trajectories. In this work, we extend geometric
mechanics to predict locomotor performance and search for optimal swimming
strategy of compliant undulators. We introduce a compliant extension of
Purcell's three-link swimmer by incorporating series-connected springs at the
joints. Body dynamics are derived with resistive force theory. Geometric
mechanics is incorporated into movement prediction and into an optimization
framework that identifies strategies for controlling compliant swimmers to
achieve maximal displacement. We validate our framework on a physical
cable-driven three-link limbless robot, and demonstrate accurate prediction and
optimization of locomotor performance under varied programmed, state-dependent
compliance in a granular medium. Our results establish a systematic
physics-based approach for modeling and controlling compliant swimming
locomotion, highlighting compliance as a design feature that can be exploited
for robust movement in homogeneous and heterogeneous environments.

</details>


### [152] [Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching](https://arxiv.org/abs/2510.03460)
*Sibo Tian,Minghui Zheng,Xiao Liang*

Main category: cs.RO

TL;DR: 提出了一种基于Flow Matching的学习方法，用于高效生成机器人运动规划的优化初始解，显著提升了轨迹优化的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 解决当前采样和优化运动规划方法在高维配置空间扩展性差、时间效率低以及易陷入局部最优的问题。

Method: 利用基于Flow Matching的模型，结合单视角点云数据，学习生成优化的初始解，无需环境先验知识。

Result: 仿真实验表明，该方法在杂乱工作空间中实现了高成功率，显著优于传统和基于学习的基准初始化方法，且优化迭代次数更少。

Conclusion: 本文提出的基于学习的方法通过Flow Matching模型，能够有效生成优化的初始解，显著提高了轨迹优化的成功率，并展现出对未知环境的强泛化能力。

Abstract: Rapid robot motion generation is critical in Human-Robot Collaboration (HRC)
systems, as robots need to respond to dynamic environments in real time by
continuously observing their surroundings and replanning their motions to
ensure both safe interactions and efficient task execution. Current
sampling-based motion planners face challenges in scaling to high-dimensional
configuration spaces and often require post-processing to interpolate and
smooth the generated paths, resulting in time inefficiency in complex
environments. Optimization-based planners, on the other hand, can incorporate
multiple constraints and generate smooth trajectories directly, making them
potentially more time-efficient. However, optimization-based planners are
sensitive to initialization and may get stuck in local minima. In this work, we
present a novel learning-based method that utilizes a Flow Matching model
conditioned on a single-view point cloud to learn near-optimal solutions for
optimization initialization. Our method does not require prior knowledge of the
environment, such as obstacle locations and geometries, and can generate
feasible trajectories directly from single-view depth camera input. Simulation
studies on a UR5e robotic manipulator in cluttered workspaces demonstrate that
the proposed generative initializer achieves a high success rate on its own,
significantly improves the success rate of trajectory optimization compared
with traditional and learning-based benchmark initializers, requires fewer
optimization iterations, and exhibits strong generalization to unseen
environments.

</details>


### [153] [A Simulation Evaluation Suite for Robust Adaptive Quadcopter Control](https://arxiv.org/abs/2510.03471)
*Dingqi Zhang,Ran Tao,Sheng Cheng,Naira Hovakimyan,Mark W. Mueller*

Main category: cs.RO

TL;DR: 本文提出一个模块化仿真测试平台，统一评估四旋翼控制方法，解决现有评估分散问题。


<details>
  <summary>Details</summary>
Motivation: 由于现有四旋翼控制方法在任务、模拟器和实现上的分散评估，难以进行系统性比较。

Method: 构建了一个易于部署的模块化仿真测试平台，包含多种代表性的自适应和非自适应控制器，并提供了任务相关指标来评估跟踪精度和鲁棒性。

Result: 该平台支持多种干扰场景和轨迹类型的评估，包括自动压力测试，展示了其系统性分析的实用性。

Conclusion: 本文介绍了一个基于RotorPy的模块化仿真测试平台，用于统一评估四旋翼控制方法，解决了现有研究中评估分散的问题。

Abstract: Robust adaptive control methods are essential for maintaining quadcopter
performance under external disturbances and model uncertainties. However,
fragmented evaluations across tasks, simulators, and implementations hinder
systematic comparison of these methods. This paper introduces an
easy-to-deploy, modular simulation testbed for quadcopter control, built on
RotorPy, that enables evaluation under a wide range of disturbances such as
wind, payload shifts, rotor faults, and control latency. The framework includes
a library of representative adaptive and non-adaptive controllers and provides
task-relevant metrics to assess tracking accuracy and robustness. The unified
modular environment enables reproducible evaluation across control methods and
eliminates redundant reimplementation of components such as disturbance models,
trajectory generators, and analysis tools. We illustrate the testbed's
versatility through examples spanning multiple disturbance scenarios and
trajectory types, including automated stress testing, to demonstrate its
utility for systematic analysis. Code is available at
https://github.com/Dz298/AdaptiveQuadBench.

</details>


### [154] [Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems](https://arxiv.org/abs/2510.03472)
*Yulun Zhang,Alexandre O. G. Barbosa,Federico Pecora,Jiaoyang Li*

Main category: cs.RO

TL;DR: 本文提出了一种优化机器人分拣系统任务映射的方法，通过进化和混合整数线性规划提高吞吐量，并在多种设置中验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 优化目的地到滑槽的任务映射以提高机器人分拣系统的吞吐量，解决实际RSS中的复杂问题。

Method: 使用进化算法和混合整数线性规划进行任务映射优化，并通过质量多样性算法分析吞吐量。

Result: 优化的任务映射在各种RSS设置中表现优于贪婪方法。

Conclusion: 通过进化和混合整数线性规划方法优化的任务映射在多种RSS设置中优于贪婪生成的映射，且质量多样性算法有助于分析不同任务映射的吞吐量。

Abstract: We study optimizing a destination-to-chutes task mapping to improve
throughput in Robotic Sorting Systems (RSS), where a team of robots sort
packages on a sortation floor by transporting them from induct workstations to
eject chutes based on their shipping destinations (e.g. Los Angeles or
Pittsburgh). The destination-to-chutes task mapping is used to determine which
chutes a robot can drop its package. Finding a high-quality task mapping is
challenging because of the complexity of a real-world RSS. First, optimizing
task mapping is interdependent with robot target assignment and path planning.
Second, chutes will be CLOSED for a period of time once they receive sufficient
packages to allow for downstream processing. Third, task mapping quality
directly impacts the downstream processing, as scattered chutes for the same
destination increase package handling time. In this paper, we first formally
define task mappings and the problem of Task Mapping Optimization (TMO). We
then present a simulator of RSS to evaluate task mappings. We then present a
simple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear
Programming, demonstrating the advantage of our optimized task mappings over
the greedily generated ones in various RSS setups with different map sizes,
numbers of chutes, and destinations. Finally, we use Quality Diversity
algorithms to analyze the throughput of a diverse set of task mappings. Our
code is available online at https://github.com/lunjohnzhang/tmo_public.

</details>


### [155] [Robust Permissive Controller Synthesis for Interval MDPs](https://arxiv.org/abs/2510.03481)
*Khang Vo Huynh,David Parker,Lu Feng*

Main category: cs.RO

TL;DR: 论文提出了一种在IMDPs上合成鲁棒性许可控制器的框架，通过MILP方法确保策略满足规范，实验验证了其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 针对机器人操作中的动态不确定性（建模为IMDPs），传统控制器合成方法通常产生单一确定性策略，限制了适应性。而许可控制器允许多个动作每状态，增强了运行时灵活性和弹性。但现有工作通常假设精确转移概率，这在许多机器人应用中不现实。

Method: 将问题表述为混合整数线性规划（MILPs），并提出了两种编码方法：基线顶点枚举方法和避免显式枚举的可扩展对偶方法。

Result: 在四个基准领域上的实验表明，两种方法都能合成鲁棒且最大许可的控制器，并扩展到具有数十万状态的大型IMDPs。

Conclusion: 该论文提出了首个在IMDPs上合成鲁棒性许可控制器的框架，确保所有符合合成多策略的策略在所有允许的转移下满足可达性或基于奖励的规范。通过实验验证，该方法能够合成鲁棒且最大许可的控制器，并扩展到具有数十万状态的大型IMDPs。

Abstract: We address the problem of robust permissive controller synthesis for robots
operating under uncertain dynamics, modeled as Interval Markov Decision
Processes (IMDPs). IMDPs generalize standard MDPs by allowing transition
probabilities to vary within intervals, capturing epistemic uncertainty from
sensing noise, actuation imprecision, and coarse system abstractions-common in
robotics. Traditional controller synthesis typically yields a single
deterministic strategy, limiting adaptability. In contrast, permissive
controllers (multi-strategies) allow multiple actions per state, enabling
runtime flexibility and resilience. However, prior work on permissive
controller synthesis generally assumes exact transition probabilities, which is
unrealistic in many robotic applications. We present the first framework for
robust permissive controller synthesis on IMDPs, guaranteeing that all
strategies compliant with the synthesized multi-strategy satisfy reachability
or reward-based specifications under all admissible transitions. We formulate
the problem as mixed-integer linear programs (MILPs) and propose two encodings:
a baseline vertex-enumeration method and a scalable duality-based method that
avoids explicit enumeration. Experiments on four benchmark domains show that
both methods synthesize robust, maximally permissive controllers and scale to
large IMDPs with up to hundreds of thousands of states.

</details>


### [156] [Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*](https://arxiv.org/abs/2510.03496)
*Vadivelan Murugesan,Rajasundaram Mathiazhagan,Sanjana Joshi,Aliasghar Arab*

Main category: cs.RO

TL;DR: 提出了一种结合预测性人类建模与数字孪生验证的安全规划框架，显著提升了人机协作中的避障精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 人机协作需要精确预测人类动作以支持主动避障。现有规划器仅依赖运动学模型，无法满足需求。

Method: 提出了一种基于预测驱动的安全规划框架，利用胶囊式人工势场（APF）将细粒度预测转化为碰撞风险指标，并在超过阈值时触发自适应RRT*（A-RRT*）规划器。通过深度摄像头提取3D骨骼姿态，并使用CNN-BiLSTM模型提前预测单个关节轨迹。数字孪生模型整合实时人类姿势预测，放置在模拟机器人前方以评估动作和物理接触。

Result: 在50次试验中，该方法实现了100%的主动避障，距离大于250毫米，重新规划时间低于2秒。

Conclusion: 提出的方法通过将预测性人类建模与数字孪生验证相结合，在50次试验中实现了100%的主动避障，且距离大于250毫米，重新规划时间低于2秒，展现了比现有仅基于运动学的规划器更高的精度和可靠性。

Abstract: Human-robot collaboration requires precise prediction of human motion over
extended horizons to enable proactive collision avoidance. Unlike existing
planners that rely solely on kinodynamic models, we present a prediction-driven
safe planning framework that leverages granular, joint-by-joint human motion
forecasting validated in a physics-based digital twin. A capsule-based
artificial potential field (APF) converts these granular predictions into
collision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when
thresholds are exceeded. The depth camera is used to extract 3D skeletal poses
and a convolutional neural network-bidirectional long short-term memory
(CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A
digital twin model integrates real-time human posture prediction placed in
front of a simulated robot to evaluate motions and physical contacts. The
proposed method enables validation of planned trajectories ahead of time and
bridging potential latency gaps in updating planned trajectories in real-time.
In 50 trials, our method achieved 100% proactive avoidance with > 250 mm
clearance and sub-2 s replanning, demonstrating superior precision and
reliability compared to existing kinematic-only planners through the
integration of predictive human modeling with digital twin validation.

</details>


### [157] [Distributed Connectivity Maintenance and Recovery for Quadrotor Motion Planning](https://arxiv.org/abs/2510.03504)
*Yutong Wang,Yichun Qu,Tengxiang Wang,Lishuo Pan,Nora Ayanian*

Main category: cs.RO

TL;DR: 提出了一种实时分布式多机器人导航框架，结合HOCBFs和CLFs，实现连接性维护和恢复，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 多机器人应用中的连接性维护对障碍物和视觉遮挡非常敏感，需要一种能够在复杂环境中保持和恢复连接性的实时导航框架。

Method: 采用高阶控制屏障函数（HOCBFs）和控制Lyapunov函数（CLFs）结合的方法，通过Bezier参数化轨迹生成平滑曲线，实现多机器人系统的连接性维护和恢复。

Result: 通过模拟和4个Crazyflie纳米四旋翼的物理实验验证了框架的有效性。

Conclusion: 本文提出了一种基于高阶控制屏障函数（HOCBFs）的实时分布式多机器人导航框架，该框架通过控制机器人间的距离来保持连接性并避免碰撞。实验验证了该框架在模拟和物理实验中的有效性。

Abstract: Maintaining connectivity is crucial in many multi-robot applications, yet
fragile to obstacles and visual occlusions. We present a real-time distributed
framework for multi-robot navigation certified by high-order control barrier
functions (HOCBFs) that controls inter-robot proximity to maintain connectivity
while avoiding collisions. We incorporate control Lyapunov functions to enable
connectivity recovery from initial disconnected configurations and temporary
losses, providing robust connectivity during navigation in obstacle-rich
environments. Our trajectory generation framework concurrently produces
planning and control through a Bezier-parameterized trajectory, which naturally
provides smooth curves with arbitrary degree of derivatives. The main
contribution is the unified MPC-CLF-CBF framework, a continuous-time trajectory
generation and control method for connectivity maintenance and recovery of
multi-robot systems. We validate the framework through extensive simulations
and a physical experiment with 4 Crazyflie nano-quadrotors.

</details>


### [158] [LapSurgie: Humanoid Robots Performing Surgery via Teleoperated Handheld Laparoscopy](https://arxiv.org/abs/2510.03529)
*Zekai Liang,Xiao Liang,Soofiyan Atar,Sreyan Das,Zoe Chiu,Peihan Zhang,Florian Richter,Shanglei Liu,Michael C. Yip*

Main category: cs.RO

TL;DR: LapSurgie是首个基于人形机器人的腹腔镜远程操作框架，通过逆映射策略和实时视觉反馈，为资源匮乏地区提供可行解决方案，用户研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决手术机器人系统在资源匮乏地区部署的难题，缩小医疗资源差距。人形机器人因其无需大规模基础设施改造的优势，成为可行路径。

Method: 采用逆映射策略控制手动腕式腹腔镜器械，遵守远程运动中心约束，无需额外设置即可实现精确的手到工具控制。控制台配备立体视觉系统提供实时视觉反馈。

Result: 通过跨平台的全面用户研究，证明了LapSurgie框架的有效性，并初步验证了人形机器人在腹腔镜手术中的可行性。

Conclusion: LapSurgie框架通过人形机器人实现了腹腔镜手术的远程操作，为资源匮乏地区提供了可行的解决方案，初步验证了人形机器人在腹腔镜手术中的可行性。

Abstract: Robotic laparoscopic surgery has gained increasing attention in recent years
for its potential to deliver more efficient and precise minimally invasive
procedures. However, adoption of surgical robotic platforms remains largely
confined to high-resource medical centers, exacerbating healthcare disparities
in rural and low-resource regions. To close this gap, a range of solutions has
been explored, from remote mentorship to fully remote telesurgery. Yet, the
practical deployment of surgical robotic systems to underserved communities
remains an unsolved challenge. Humanoid systems offer a promising path toward
deployability, as they can directly operate in environments designed for humans
without extensive infrastructure modifications -- including operating rooms. In
this work, we introduce LapSurgie, the first humanoid-robot-based laparoscopic
teleoperation framework. The system leverages an inverse-mapping strategy for
manual-wristed laparoscopic instruments that abides to remote center-of-motion
constraints, enabling precise hand-to-tool control of off-the-shelf surgical
laparoscopic tools without additional setup requirements. A control console
equipped with a stereo vision system provides real-time visual feedback.
Finally, a comprehensive user study across platforms demonstrates the
effectiveness of the proposed framework and provides initial evidence for the
feasibility of deploying humanoid robots in laparoscopic procedures.

</details>


### [159] [Efficient Surgical Robotic Instrument Pose Reconstruction in Real World Conditions Using Unified Feature Detection](https://arxiv.org/abs/2510.03532)
*Zekai Liang,Kazuya Miyata,Xiao Liang,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: 提出了一种统一检测几何基元的新框架，通过共享编码和投影几何实现高效姿态估计，显著提升了手术机器人相机-机器人校准的精度和速度。


<details>
  <summary>Details</summary>
Motivation: 微创手术机器人因其长运动链和部分自由度可见性，给传统相机-机器人校准方法带来挑战，需要一种更高效、更准确的解决方案。

Method: 通过共享编码统一检测几何基元（关键点和轴边缘），并利用投影几何进行高效姿态估计。

Result: 在特征检测和姿态估计方面均表现出快速性能和最先进的准确性，适用于具有挑战性的手术环境。

Conclusion: 该论文提出的新框架在手术机器人环境中实现了快速且高精度的相机-机器人校准，显著提升了现有方法的性能。

Abstract: Accurate camera-to-robot calibration is essential for any vision-based
robotic control system and especially critical in minimally invasive surgical
robots, where instruments conduct precise micro-manipulations. However, MIS
robots have long kinematic chains and partial visibility of their degrees of
freedom in the camera, which introduces challenges for conventional
camera-to-robot calibration methods that assume stiff robots with good
visibility. Previous works have investigated both keypoint-based and
rendering-based approaches to address this challenge in real-world conditions;
however, they often struggle with consistent feature detection or have long
inference times, neither of which are ideal for online robot control. In this
work, we propose a novel framework that unifies the detection of geometric
primitives (keypoints and shaft edges) through a shared encoding, enabling
efficient pose estimation via projection geometry. This architecture detects
both keypoints and edges in a single inference and is trained on large-scale
synthetic data with projective labeling. This method is evaluated across both
feature detection and pose estimation, with qualitative and quantitative
results demonstrating fast performance and state-of-the-art accuracy in
challenging surgical environments.

</details>


### [160] [Shape-Space Graphs: Fast and Collision-Free Path Planning for Soft Robots](https://arxiv.org/abs/2510.03547)
*Carina Veil,Moritz Flaschel,Ellen Kuhl*

Main category: cs.RO

TL;DR: 该研究提出了一种基于形状空间的图搜索方法，用于象鼻启发的软机器人路径规划，能在复杂环境中快速生成可行路径并优化能量消耗。


<details>
  <summary>Details</summary>
Motivation: 软机器人（如象鼻或章鱼臂）的运动规划在杂乱环境中仍具挑战性，因其高度非线性和无限维运动学特性。

Method: 研究提出了一种基于图的路径规划工具，利用生物力学模型预计算形状库，并在形状空间中构建k近邻图，使用符号距离函数修剪碰撞节点和边，定义基于几何距离和驱动力的多目标边成本。

Result: 算法能在毫秒级时间内可靠避开障碍物并生成可行路径，且包含能量成本可显著减少驱动力，尽管轨迹长度增加。

Conclusion: 该研究展示了基于形状空间的图搜索在软机器人路径规划中的潜力，为实时应用（如手术、工业和辅助场景）铺平了道路。

Abstract: Soft robots, inspired by elephant trunks or octopus arms, offer extraordinary
flexibility to bend, twist, and elongate in ways that rigid robots cannot.
However, their motion planning remains a challenge, especially in cluttered
environments with obstacles, due to their highly nonlinear and
infinite-dimensional kinematics. Here, we present a graph-based path planning
tool for an elephant-trunk-inspired soft robotic arm designed with three
artificial muscle fibers that allow for multimodal continuous deformation
through contraction. Using a biomechanical model inspired by morphoelasticity
and active filament theory, we precompute a shape library and construct a
$k$-nearest neighbor graph in \emph{shape space}, ensuring that each node
corresponds to a mechanically accurate and physically valid robot shape. For
the graph, we use signed distance functions to prune nodes and edges colliding
with obstacles, and define multi-objective edge costs based on geometric
distance and actuation effort, enabling energy-efficient planning with
collision avoidance. We demonstrate that our algorithm reliably avoids
obstacles and generates feasible paths within milliseconds from precomputed
graphs using Dijkstra's algorithm. We show that including energy costs can
drastically reduce the actuation effort compared to geometry-only planning, at
the expense of longer tip trajectories. Our results highlight the potential of
shape-space graph search for fast and reliable path planning in the field of
soft robotics, paving the way for real-time applications in surgical,
industrial, and assistive settings.

</details>


### [161] [Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning](https://arxiv.org/abs/2510.03599)
*Shafeef Omar,Majid Khadiv*

Main category: cs.RO

TL;DR: 提出了一种基于接触显式表示的统一框架，通过单一策略执行多种移动和操作任务，显著提高了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了利用不同接触丰富任务之间的共享结构，避免为不同任务设计不同策略，从而实现单一策略执行多种任务。

Method: 提出了一种基于接触显式表示的统一框架，通过接触目标（期望的接触位置、时间和活动末端效应器）来定义任务，并训练一个目标条件的强化学习策略来实现给定的接触计划。

Result: 在多种机器人实体和任务上验证了框架的有效性，包括四足动物的多种步态、人形机器人的多种双足和四足步态，以及人形机器人执行不同的双手物体操作任务。

Conclusion: 显式接触推理显著提高了对未见场景的泛化能力，使基于接触的显式策略学习成为可扩展的移动操作的有前景基础。

Abstract: We present a unified framework for multi-task locomotion and manipulation
policy learning grounded in a contact-explicit representation. Instead of
designing different policies for different tasks, our approach unifies the
definition of a task through a sequence of contact goals-desired contact
positions, timings, and active end-effectors. This enables leveraging the
shared structure across diverse contact-rich tasks, leading to a single policy
that can perform a wide range of tasks. In particular, we train a
goal-conditioned reinforcement learning (RL) policy to realise given contact
plans. We validate our framework on multiple robotic embodiments and tasks: a
quadruped performing multiple gaits, a humanoid performing multiple biped and
quadrupedal gaits, and a humanoid executing different bimanual object
manipulation tasks. Each of these scenarios is controlled by a single policy
trained to execute different tasks grounded in contacts, demonstrating
versatile and robust behaviours across morphologically distinct systems. Our
results show that explicit contact reasoning significantly improves
generalisation to unseen scenarios, positioning contact-explicit policy
learning as a promising foundation for scalable loco-manipulation.

</details>


### [162] [Safety-Oriented Dynamic Path Planning for Automated Vehicles](https://arxiv.org/abs/2510.03640)
*Mostafa Emam,Matthias Gerdts*

Main category: cs.RO

TL;DR: 提出双层控制框架，结合NMPC和备份环，提升自动驾驶在动态环境中的安全性和实时性能。


<details>
  <summary>Details</summary>
Motivation: 提升自动驾驶车辆在动态环境中的路径规划和避障能力，确保安全性。

Method: 采用双层控制框架，主控制环使用非线性模型预测控制（NMPC）进行实时路径优化，辅以独立的备份环提供安全回退轨迹。

Result: 评估展示了该方法在各种驾驶场景中的实时适用性和鲁棒性。

Conclusion: 该框架在复杂动态环境中为更安全、可靠的自动驾驶迈出了重要一步。

Abstract: Ensuring safety in autonomous vehicles necessitates advanced path planning
and obstacle avoidance capabilities, particularly in dynamic environments. This
paper introduces a bi-level control framework that efficiently augments road
boundaries by incorporating time-dependent grid projections of obstacle
movements, thus enabling precise and adaptive path planning. The main control
loop utilizes Nonlinear Model Predictive Control (NMPC) for real-time path
optimization, wherein homotopy-based constraint relaxation is employed to
improve the solvability of the optimal control problem (OCP). Furthermore, an
independent backup loop runs concurrently to provide safe fallback trajectories
when an optimal trajectory cannot be computed by the main loop within a
critical time frame, thus enhancing safety and real-time performance. Our
evaluation showcases the benefits of the proposed methods in various driving
scenarios, highlighting the real-time applicability and robustness of our
approach. Overall, the framework represents a significant step towards safer
and more reliable autonomous driving in complex and dynamic environments.

</details>


### [163] [Geometrically Exact Hard Magneto-Elastic Cosserat Shells: Static Formulation for Shape Morphing](https://arxiv.org/abs/2510.03644)
*Mohammadjavad Javadi,Robin Chhabra*

Main category: cs.RO

TL;DR: 研究提出了一种基于Cosserat壳理论的硬磁壳静态模型，适用于宽长比大的软机器人，实验验证了其在严重变形下的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的Cosserat杆理论适用于1D细长结构，但近期设计的软机器人（如抓取器和行走机器人）具有较大的宽长比，属于2D壳结构，需要新的建模方法。

Method: 采用基于特殊欧几里得群（SE(3)）的Cosserat壳理论新公式，推导了平衡方程的强弱形式，并提取线性化版本用于数值实现。

Result: 提出的有限元方法避免了壳结构建模中的常见挑战（如奇异性和锁定现象），并通过实验验证了其高效性。

Conclusion: 该研究开发了一种基于Cosserat壳理论的高效静态模型，适用于硬磁壳的分析和形状变形控制，通过实验验证了其在严重旋转和位移情况下的优越性能。

Abstract: Cosserat rod theory is the popular approach to modeling ferromagnetic soft
robots as 1-Dimensional (1D) slender structures in most applications, such as
biomedical. However, recent soft robots designed for locomotion and
manipulation often exhibit a large width-to-length ratio that categorizes them
as 2D shells. For analysis and shape-morphing control purposes, we develop an
efficient coordinate-free static model of hard-magnetic shells found in soft
magnetic grippers and walking soft robots. The approach is based on a novel
formulation of Cosserat shell theory on the Special Euclidean group
($\mathbf{SE}(3)$). The shell is assumed to be a 2D manifold of material points
with six degrees of freedom (position & rotation) suitable for capturing the
behavior of a uniformly distributed array of spheroidal hard magnetic particles
embedded in the rheological elastomer. The shell's configuration manifold is
the space of all smooth embeddings $\mathbb{R}^2\rightarrow\mathbf{SE}(3)$.
According to a novel definition of local deformation gradient based on the Lie
group structure of $\mathbf{SE}(3)$, we derive the strong and weak forms of
equilibrium equations, following the principle of virtual work. We extract the
linearized version of the weak form for numerical implementations. The
resulting finite element approach can avoid well-known challenges such as
singularity and locking phenomenon in modeling shell structures. The proposed
model is analytically and experimentally validated through a series of test
cases that demonstrate its superior efficacy, particularly when the shell
undergoes severe rotations and displacements.

</details>


### [164] [An Amphibious Untethered Inchworm Soft Robot for Fast Crawling Locomotion](https://arxiv.org/abs/2510.03660)
*Mohammadjavad Javadi,Charlie Wadds,Robin Chhabra*

Main category: cs.RO

TL;DR: 受尺蠖启发，开发了一种无束缚磁驱动软体机器人，具备多模态运动和无线控制能力，适用于多样化环境。


<details>
  <summary>Details</summary>
Motivation: 受软体尺蠖启发，开发完全无束缚的软体机器人，以推动软体机器人系统在多样化、多任务环境中的实际应用。

Method: 设计了一种由磁力驱动的弯曲柔性结构软体机器人，配备紧凑轻量化的车载控制电路和集成摄像头，实现了无线命令传输和环境感知。

Result: 机器人总质量为102.63克，展示了多模态运动能力，最大行走速度为3.74厘米/秒，游泳速度为0.82厘米/秒。

Conclusion: 通过结构优化和系统级集成，该软体机器人成功实现了无需外部基础设施的行走、转向、游泳和负载运输，其动态性能和运动能力通过实验验证。

Abstract: Untethered soft robots are essential for advancing the real-world deployment
of soft robotic systems in diverse and multitasking environments. Inspired by
soft-bodied inchworm, we present a fully untethered soft robot with a curved,
flexible structure actuated by magnetic forces. The robot has a total mass of
102.63 g and demonstrates multimodal locomotion, achieving a maximum walking
speed of 3.74 cm/s and a swimming speed of 0.82 cm/s. A compact and lightweight
onboard control circuit enables wireless command transmission, while an
integrated camera provides environmental perception. Through structural
optimization and system-level integration, the robot successfully performs
walking, steering, swimming, and payload transport without reliance on external
infrastructure. The robot's dynamic performance and locomotion capabilities are
systematically validated through experimental characterization.

</details>


### [165] [Robust Visual Embodiment: How Robots Discover Their Bodies in Real Environments](https://arxiv.org/abs/2510.03677)
*Salim Rezvani,Ammar Jaleel Mahmood,Robin Chhabra*

Main category: cs.RO

TL;DR: 研究量化了视觉退化对机器人自建模的影响，提出任务感知去噪框架和语义分割方法，显著提升鲁棒性，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有自主建模管道在真实感知条件下（如噪声图像和杂乱背景）表现脆弱，量化视觉退化对机器人自建模的影响并克服这些挑战。

Method: 通过结合经典恢复方法和形态保持约束的任务感知去噪框架，以及用于鲁棒隔离机器人的语义分割技术。

Result: 实验表明，该方法在模拟和物理平台上恢复了接近基线的性能，而现有管道性能显著下降。

Conclusion: 本文提出的任务感知去噪框架和语义分割方法显著提升了视觉自建模的鲁棒性，为在不可预测的真实环境中部署自感知机器人奠定了实践基础。

Abstract: Robots with internal visual self-models promise unprecedented adaptability,
yet existing autonomous modeling pipelines remain fragile under realistic
sensing conditions such as noisy imagery and cluttered backgrounds. This paper
presents the first systematic study quantifying how visual
degradations--including blur, salt-and-pepper noise, and Gaussian noise--affect
robotic self-modeling. Through both simulation and physical experiments, we
demonstrate their impact on morphology prediction, trajectory planning, and
damage recovery in state-of-the-art pipelines. To overcome these challenges, we
introduce a task-aware denoising framework that couples classical restoration
with morphology-preserving constraints, ensuring retention of structural cues
critical for self-modeling. In addition, we integrate semantic segmentation to
robustly isolate robots from cluttered and colorful scenes. Extensive
experiments show that our approach restores near-baseline performance across
simulated and physical platforms, while existing pipelines degrade
significantly. These contributions advance the robustness of visual
self-modeling and establish practical foundations for deploying self-aware
robots in unpredictable real-world environments.

</details>


### [166] [EmbodiSwap for Zero-Shot Robot Imitation Learning](https://arxiv.org/abs/2510.03706)
*Eadom Dessalene,Pavan Mantripragada,Michael Maynord,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: EmbodiSwap通过合成机器人覆盖层和V-JEPA视觉骨干，实现了零样本模仿学习，成功率达82%，并发布了相关代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 解决人类视频与目标机器人体现之间的‘体现差距’，实现从人类视频到机器人操作的零样本模仿学习。

Method: 采用EmbodiSwap方法生成合成机器人覆盖层，并结合V-JEPA视觉骨干进行零样本模仿学习。

Result: 在真实世界测试中，零样本训练的V-JEPA模型达到82%的成功率，优于少量样本训练的π₀网络。

Conclusion: EmbodiSwap成功地将人类视频转化为逼真的合成机器人覆盖层，并通过V-JEPA视觉骨干在零样本模仿学习中取得了显著成效，成功率达到82%，超越了传统方法。

Abstract: We introduce EmbodiSwap - a method for producing photorealistic synthetic
robot overlays over human video. We employ EmbodiSwap for zero-shot imitation
learning, bridging the embodiment gap between in-the-wild ego-centric human
video and a target robot embodiment. We train a closed-loop robot manipulation
policy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a
visual backbone, repurposing V-JEPA from the domain of video understanding to
imitation learning over synthetic robot videos. Adoption of V-JEPA outperforms
alternative vision backbones more conventionally used within robotics. In
real-world tests, our zero-shot trained V-JEPA model achieves an $82\%$ success
rate, outperforming a few-shot trained $\pi_0$ network as well as $\pi_0$
trained over data produced by EmbodiSwap. We release (i) code for generating
the synthetic robot overlays which takes as input human videos and an arbitrary
robot URDF and generates a robot dataset, (ii) the robot dataset we synthesize
over EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference
code, to facilitate reproducible research and broader adoption.

</details>


### [167] [Model-Based Adaptive Precision Control for Tabletop Planar Pushing Under Uncertain Dynamics](https://arxiv.org/abs/2510.03768)
*Aydin Ahmadi,Baris Akgun*

Main category: cs.RO

TL;DR: 提出一个基于模型的非抓取桌面推动框架，通过单一学习模型解决多任务，无需重新训练，展示了高成功率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 减少手动工程工作并提高泛化能力，解决现有方法在广泛适用性上的限制。

Method: 采用基于GRU的循环架构和非线性层来捕捉物体-环境动态，结合MPPI控制器生成自适应任务导向动作。

Result: 在仿真和真实实验中验证了高成功率的精确定位、强性能的轨迹跟踪和避障能力。

Conclusion: 该框架通过单一学习模型解决了多种任务，展示了在精确定位、轨迹跟踪和避障方面的高成功率，且无需重新训练即可适应不同任务。

Abstract: Data-driven planar pushing methods have recently gained attention as they
reduce manual engineering effort and improve generalization compared to
analytical approaches. However, most prior work targets narrow capabilities
(e.g., side switching, precision, or single-task training), limiting broader
applicability. We present a model-based framework for non-prehensile tabletop
pushing that uses a single learned model to address multiple tasks without
retraining. Our approach employs a recurrent GRU-based architecture with
additional non-linear layers to capture object-environment dynamics while
ensuring stability. A tailored state-action representation enables the model to
generalize across uncertain dynamics, variable push lengths, and diverse tasks.
For control, we integrate the learned dynamics with a sampling-based Model
Predictive Path Integral (MPPI) controller, which generates adaptive,
task-oriented actions. This framework supports side switching, variable-length
pushes, and objectives such as precise positioning, trajectory following, and
obstacle avoidance. Training is performed in simulation with domain
randomization to support sim-to-real transfer. We first evaluate the
architecture through ablation studies, showing improved prediction accuracy and
stable rollouts. We then validate the full system in simulation and real-world
experiments using a Franka Panda robot with markerless tracking. Results
demonstrate high success rates in precise positioning under strict thresholds
and strong performance in trajectory tracking and obstacle avoidance. Moreover,
multiple tasks are solved simply by changing the controller's objective
function, without retraining. While our current focus is on a single object
type, we extend the framework by training on wider push lengths and designing a
balanced controller that reduces the number of steps for longer-horizon goals.

</details>


### [168] [Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets](https://arxiv.org/abs/2510.03776)
*Tiago Rodrigues de Almeida,Yufei Zhu,Andrey Rudenko,Tomasz P. Kucner,Johannes A. Stork,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: 论文研究了类别条件轨迹预测方法，发现深度学习在平衡数据中表现更好，但在数据有限或类别不平衡时，基于模式的方法更优。


<details>
  <summary>Details</summary>
Motivation: 在复杂动态环境中，智能系统需要预测周围代理的未来行为和意图以提高效率和避免碰撞，而代理的动态性与其任务、角色或可观察标签密切相关。

Method: 分析了几种类别条件轨迹预测方法，并提出了基于条件模式的高效深度学习基线。

Result: 实验表明，考虑类别标签时，所有方法在多数设置下均提高了预测准确性。深度学习在平衡数据集上表现更好，但在数据有限或类别不平衡时，基于模式的方法更具优势。

Conclusion: 在有限数据或类别不平衡的应用场景中，基于模式的方法可能比深度学习方法更优。

Abstract: Robots and other intelligent systems navigating in complex dynamic
environments should predict future actions and intentions of surrounding agents
to reach their goals efficiently and avoid collisions. The dynamics of those
agents strongly depends on their tasks, roles, or observable labels.
Class-conditioned motion prediction is thus an appealing way to reduce forecast
uncertainty and get more accurate predictions for heterogeneous agents.
However, this is hardly explored in the prior art, especially for mobile robots
and in limited data applications. In this paper, we analyse different
class-conditioned trajectory prediction methods on two datasets. We propose a
set of conditional pattern-based and efficient deep learning-based baselines,
and evaluate their performance on robotics and outdoors datasets (TH\"OR-MAGNI
and Stanford Drone Dataset). Our experiments show that all methods improve
accuracy in most of the settings when considering class labels. More
importantly, we observe that there are significant differences when learning
from imbalanced datasets, or in new environments where sufficient data is not
available. In particular, we find that deep learning methods perform better on
balanced datasets, but in applications with limited data, e.g., cold start of a
robot in a new environment, or imbalanced classes, pattern-based methods may be
preferable.

</details>


### [169] [COVER:COverage-VErified Roadmaps for Fixed-time Motion Planning in Continuous Semi-Static Environments](https://arxiv.org/abs/2510.03875)
*Niranjan Kumar Ilampooranan,Constantinos Chamzas*

Main category: cs.RO

TL;DR: COVER是一个半静态环境中的运动规划框架，通过分区验证路线图，保证固定时间查询，并在模拟中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 半静态环境中存在结构化可变性，可以系统性地利用以提供比一般运动规划问题更强的保证，但现有方法缺乏正式保证或依赖限制性离散化。

Method: COVER通过分区障碍物配置空间并在每个分区内求解可行路径，逐步构建覆盖验证的路线图。

Result: 在7自由度模拟Panda机器人执行桌面和货架任务中，COVER实现了比现有方法更广的覆盖范围和更高的查询成功率。

Conclusion: COVER框架通过系统地验证半静态环境中的路线图分区，为固定时间运动规划查询提供了保证，并在模拟实验中表现出比现有方法更高的覆盖率和查询成功率。

Abstract: Having the ability to answer motion-planning queries within a fixed time
budget is critical for the widespread deployment of robotic systems.
Semi-static environments, where most obstacles remain static but a limited set
can vary across queries, exhibit structured variability that can be
systematically exploited to provide stronger guarantees than in general
motion-planning problems. However, prior approaches in this setting either lack
formal guarantees or rely on restrictive discretizations of obstacle
configurations, limiting their applicability in realistic domains. This paper
introduces COVER, a novel framework that incrementally constructs a
coverage-verified roadmap in semi-static environments. By partitioning the
obstacle configuration space and solving for feasible paths within each
partition, COVER systematically verifies feasibility of the roadmap in each
partition and guarantees fixed-time motion planning queries within the verified
regions. We validate COVER with a 7-DOF simulated Panda robot performing table
and shelf tasks, demonstrating that COVER achieves broader coverage with higher
query success rates than prior works.

</details>


### [170] [Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning](https://arxiv.org/abs/2510.03885)
*Sunghwan Kim,Woojeh Chung,Zhirui Dai,Dwait Bhatt,Arth Shukla,Hao Su,Yulun Tian,Nikolay Atanasov*

Main category: cs.RO

TL;DR: SBP是一种基于3D潜在地图的端到端策略学习方法，通过全局推理和长期记忆提升移动操纵任务的性能，显著优于传统图像策略。


<details>
  <summary>Details</summary>
Motivation: 为了解决移动操纵策略在空间和时间推理上的局限性，尤其是在超出机器人当前视野和长期观察聚合的场景中。

Method: 提出了Seeing the Bigger Picture (SBP)，一种端到端的策略学习方法，直接操作3D潜在特征地图。该方法通过多视角观察逐步融合到场景特定的潜在特征网格中，并利用预训练的、场景无关的解码器进行在线优化。

Result: SBP在场景级移动操纵和顺序桌面操纵任务中表现出色，能够全局推理场景、利用地图作为长期记忆，并在分布内和全新场景中优于基于图像的策略，例如顺序操纵任务的成功率提高了25%。

Conclusion: SBP通过3D潜在地图实现了比仅依赖图像的策略更强的空间和时间推理能力，显著提升了任务成功率。

Abstract: In this paper, we demonstrate that mobile manipulation policies utilizing a
3D latent map achieve stronger spatial and temporal reasoning than policies
relying solely on images. We introduce Seeing the Bigger Picture (SBP), an
end-to-end policy learning approach that operates directly on a 3D map of
latent features. In SBP, the map extends perception beyond the robot's current
field of view and aggregates observations over long horizons. Our mapping
approach incrementally fuses multiview observations into a grid of
scene-specific latent features. A pre-trained, scene-agnostic decoder
reconstructs target embeddings from these features and enables online
optimization of the map features during task execution. A policy, trainable
with behavior cloning or reinforcement learning, treats the latent map as a
state variable and uses global context from the map obtained via a 3D feature
aggregator. We evaluate SBP on scene-level mobile manipulation and sequential
tabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasons
globally over the scene, (ii) leverages the map as long-horizon memory, and
(iii) outperforms image-based policies in both in-distribution and novel
scenes, e.g., improving the success rate by 25% for the sequential manipulation
task.

</details>


### [171] [NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation](https://arxiv.org/abs/2510.03895)
*Zheng Huang,Mingyu Liu,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Xiaoman Li,Yiduo Jia,Hao Zhong,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: NoTVLA通过稀疏轨迹优化解决VLA模型的灾难性遗忘问题，提升多任务性能，减少计算资源需求，保持语言能力。


<details>
  <summary>Details</summary>
Motivation: 解决Vision-Language-Action（VLA）模型因依赖连续动作序列或动作块导致的灾难性遗忘问题，避免数据孤岛对知识保留的干扰。

Method: 采用稀疏轨迹而非密集动作轨迹进行训练，结合时间压缩和空间推理剪枝策略，专注于机器人末端执行器的轨迹规划。

Result: 在零样本和多任务评估中表现优于pi0，计算资源消耗大幅减少，且无需腕部摄像头，操作精度接近单任务专家模型。

Conclusion: NoTVLA框架通过稀疏轨迹优化解决了VLA模型中的灾难性遗忘问题，显著提升了多任务场景下的性能和泛化能力，同时保持了语言能力，支持跨平台部署。

Abstract: Vision-Language-Action (VLA) models represent a pivotal advance in embodied
intelligence, yet they confront critical barriers to real-world deployment,
most notably catastrophic forgetting. This issue stems from their overreliance
on continuous action sequences or action chunks, which inadvertently create
isolated data silos that disrupt knowledge retention across tasks. To tackle
these challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)
framework: a novel approach that narrows its focus to sparse trajectories,
thereby avoiding the catastrophic forgetting associated with dense trajectory
fine-tuning. A key innovation of NoTVLA lies in its trajectory planning
strategy: instead of centering on the target object's trajectory, it leverages
temporal compression and spatial reasoning pruning specifically for the robot
end effector's trajectory. Furthermore, training is conducted using these
sparse trajectories rather than dense action trajectories, an optimization that
delivers remarkable practical advantages with better performance in zero-shot.
In multi-task evaluation scenarios, NoTVLA achieves superior performance and
generalization compared to pi0 while operating under two critical constraints:
it uses over an order of magnitude less computing power than pi0 and requires
no wrist-mounted camera. This design ensures that NoTVLA's operational accuracy
closely approximates that of single-task expert models. Crucially, it also
preserves the model's inherent language capabilities, enabling zero-shot
generalization in specific scenarios, supporting unified model deployment
across multiple robot platforms, and fostering a degree of generalization even
when perceiving tasks from novel perspectives.

</details>


### [172] [WAFFLE: A Wearable Approach to Bite Timing Estimation in Robot-Assisted Feeding](https://arxiv.org/abs/2510.03910)
*Akhil Padmanabha,Jessie Yuan,Tanisha Mehta,Rajat Kumar Jenamani,Eric Hu,Victoria de León,Anthony Wertz,Janavi Gupta,Ben Dodson,Yunting Yan,Carmel Majidi,Tapomayukh Bhattacharjee,Zackory Erickson*

Main category: cs.RO

TL;DR: WAFFLE通过穿戴式传感器预测咬合时机，提升喂食机器人反应性，用户体验优于基线方法，适用于不同用户和场景。


<details>
  <summary>Details</summary>
Motivation: 解决机器人辅助喂食系统中咬合时机预测的技术挑战，以增强残障人士的自主性和生活质量，减轻护理人员负担。

Method: 使用监督回归模型训练14名参与者的咬合时机数据，并结合用户可调节的自信阈值生成前进或停止指令。

Result: 在15名无运动障碍参与者的研究中，WAFFLE在控制感、机器人理解和工作量方面表现优于基线方法，并在社交用餐中更受青睐；在2名运动障碍患者的家庭环境中也验证了其通用性。

Conclusion: WAFFLE系统通过穿戴式传感器数据准确预测咬合时机，支持跨用户、机器人硬件和环境的自然反应，显著提升了用户体验和机器人理解能力。

Abstract: Millions of people around the world need assistance with feeding. Robotic
feeding systems offer the potential to enhance autonomy and quality of life for
individuals with impairments and reduce caregiver workload. However, their
widespread adoption has been limited by technical challenges such as estimating
bite timing, the appropriate moment for the robot to transfer food to a user's
mouth. In this work, we introduce WAFFLE: Wearable Approach For Feeding with
LEarned bite timing, a system that accurately predicts bite timing by
leveraging wearable sensor data to be highly reactive to natural user cues such
as head movements, chewing, and talking. We train a supervised regression model
on bite timing data from 14 participants and incorporate a user-adjustable
assertiveness threshold to convert predictions into proceed or stop commands.
In a study with 15 participants without motor impairments with the Obi feeding
robot, WAFFLE performs statistically on par with or better than baseline
methods across measures of feeling of control, robot understanding, and
workload, and is preferred by the majority of participants for both individual
and social dining. We further demonstrate WAFFLE's generalizability in a study
with 2 participants with motor impairments in their home environments using a
Kinova 7DOF robot. Our findings support WAFFLE's effectiveness in enabling
natural, reactive bite timing that generalizes across users, robot hardware,
robot positioning, feeding trajectories, foods, and both individual and social
dining contexts.

</details>


### [173] [TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry](https://arxiv.org/abs/2510.03919)
*Matthew Lisondra,Junseo Kim,Glenn Takashi Shimoda,Kourosh Zareinia,Sajad Saeedi*

Main category: cs.RO

TL;DR: TCB-VIO是一种高帧率、紧密耦合的6自由度VIO框架，通过MSCKF减少漂移，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决视觉惯性里程计（VIO）框架中由于视觉姿态估计和惯性测量导致的空间和时间漂移问题。

Method: 采用多状态约束卡尔曼滤波器（MSCKF），以250 FPS的高帧率和400 Hz的IMU测量频率运行。

Result: TCB-VIO在性能上优于ROVIO、VINS-Mono和ORB-SLAM3等现有方法。

Conclusion: TCB-VIO通过高帧率运行和紧密耦合的6自由度VIO框架，显著减少了空间和时间漂移，性能优于现有方法。

Abstract: Vision algorithms can be executed directly on the image sensor when
implemented on the next-generation sensors known as focal-plane
sensor-processor arrays (FPSP)s, where every pixel has a processor. FPSPs
greatly improve latency, reducing the problems associated with the bottleneck
of data transfer from a vision sensor to a processor. FPSPs accelerate
vision-based algorithms such as visual-inertial odometry (VIO). However, VIO
frameworks suffer from spatial drift due to the vision-based pose estimation,
whilst temporal drift arises from the inertial measurements. FPSPs circumvent
the spatial drift by operating at a high frame rate to match the high-frequency
output of the inertial measurements. In this paper, we present TCB-VIO, a
tightly-coupled 6 degrees-of-freedom VIO by a Multi-State Constraint Kalman
Filter (MSCKF), operating at a high frame-rate of 250 FPS and from IMU
measurements obtained at 400 Hz. TCB-VIO outperforms state-of-the-art methods:
ROVIO, VINS-Mono, and ORB-SLAM3.

</details>


### [174] [A Real-Time Framework for Intermediate Map Construction and Kinematically Feasible Off-Road Planning Without OSM](https://arxiv.org/abs/2510.03948)
*Otobong Jerome,Geesara Prathap Kulathunga,Devitt Dmitry,Eugene Murawjow,Alexandr Klimchik*

Main category: cs.RO

TL;DR: 提出一种针对越野环境的全局路径规划方法，通过分解问题实现实时性能、运动学可行性和内存效率，测试表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统全局路径规划方法在越野环境中表现不佳，无法满足实时性能、运动学可行性和内存效率等关键需求。

Method: 通过构建像素坐标系中的中间地图，将规划问题分解为基于图的路径规划、运动学可行性检查和路径平滑三个子问题。

Result: 在多种越野环境和大规模地图（达数平方公里）中测试，平均1.5秒内找到可行路径，极端条件下内存占用约1.5GB。

Conclusion: 该论文提出的全局路径规划方法在复杂非结构化越野环境中表现出色，实现了实时性能、运动学可行性和内存效率的平衡，适用于多种越野自主导航任务。

Abstract: Off-road environments present unique challenges for autonomous navigation due
to their complex and unstructured nature. Traditional global path-planning
methods, which typically aim to minimize path length and travel time, perform
poorly on large-scale maps and fail to account for critical factors such as
real-time performance, kinematic feasibility, and memory efficiency. This paper
introduces a novel global path-planning method specifically designed for
off-road environments, addressing these essential factors. The method begins by
constructing an intermediate map within the pixel coordinate system,
incorporating geographical features like off-road trails, waterways, restricted
and passable areas, and trees. The planning problem is then divided into three
sub-problems: graph-based path planning, kinematic feasibility checking, and
path smoothing. This approach effectively meets real-time performance
requirements while ensuring kinematic feasibility and efficient memory use. The
method was tested in various off-road environments with large-scale maps up to
several square kilometers in size, successfully identifying feasible paths in
an average of 1.5 seconds and utilizing approximately 1.5GB of memory under
extreme conditions. The proposed framework is versatile and applicable to a
wide range of off-road autonomous navigation tasks, including search and rescue
missions and agricultural operations.

</details>


### [175] [SITCOM: Scaling Inference-Time COMpute for VLAs](https://arxiv.org/abs/2510.04041)
*Ayudh Saxena,Harsh Shah,Sandeep Routray,Rishi Rajesh Shah,Esha Pahwa*

Main category: cs.RO

TL;DR: SITCOM框架通过模型滚动优化增强VLA模型的长期规划能力，任务完成率显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型在动态任务中缺乏前瞻性和错误累积的问题，提升长期规划的鲁棒性。

Method: 引入SITCOM框架，结合预训练的VLA模型与基于模型的滚动优化和奖励轨迹选择，使用Transformer动力学模型进行多步动作模拟。

Result: 在SIMPLER环境中，SITCOM结合良好奖励函数将任务完成率从48%提升至72%。

Conclusion: SITCOM框架通过结合预训练的VLA模型和基于模型的滚动优化，显著提升了长期任务规划的鲁棒性，任务完成率从48%提高到72%。

Abstract: Learning robust robotic control policies remains a major challenge due to the
high cost of collecting labeled data, limited generalization to unseen
environments, and difficulties in planning over long horizons. While
Vision-Language-Action (VLA) models offer a promising solution by grounding
natural language instructions into single-step control commands, they often
lack mechanisms for lookahead and struggle with compounding errors in dynamic
tasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs
(SITCOM), a framework that augments any pretrained VLA with model-based
rollouts and reward-based trajectory selection, inspired by Model Predictive
Control algorithm. SITCOM leverages a learned dynamics model to simulate
multi-step action rollouts to select the best candidate plan for real-world
execution, transforming one-shot VLAs into robust long-horizon planners. We
develop an efficient transformer-based dynamics model trained on large-scale
BridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim
gap, and score candidate rollouts using rewards from simulator. Through
comprehensive evaluation across multiple tasks and settings in the SIMPLER
environment, we demonstrate that SITCOM when combined with a good reward
function can significantly improve task completion rate from 48% to 72% using
trained dynamics model.

</details>


### [176] [Feedback Matters: Augmenting Autonomous Dissection with Visual and Topological Feedback](https://arxiv.org/abs/2510.04074)
*Chung-Pang Wang,Changwei Chen,Xiao Liang,Soofiyan Atar,Florian Richter,Michael Yip*

Main category: cs.RO

TL;DR: 提出了一种基于反馈的自主组织切割框架，通过可见性指标和最优控制器提升系统性能，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的手术机器人反馈机制在处理组织切割的拓扑和感知挑战方面存在局限性，需要一种能动态适应组织特性和视觉线索变化的系统。

Method: 引入了一种基于内窥镜图像的反馈机制，包括能感知拓扑变化的可见性指标和最优控制器设计，以主动调整组织位置最大化可见性，并将这些机制与规划和基于学习的切割方法结合。

Result: 实验证明，该框架显著提高了自主性、减少了错误，并在复杂手术场景中表现出更强的鲁棒性。

Conclusion: 提出的反馈框架显著提升了自主组织切割系统的性能，增强了适应性、减少了错误并提高了在复杂手术场景中的鲁棒性。

Abstract: Autonomous surgical systems must adapt to highly dynamic environments where
tissue properties and visual cues evolve rapidly. Central to such adaptability
is feedback: the ability to sense, interpret, and respond to changes during
execution. While feedback mechanisms have been explored in surgical robotics,
ranging from tool and tissue tracking to error detection, existing methods
remain limited in handling the topological and perceptual challenges of tissue
dissection. In this work, we propose a feedback-enabled framework for
autonomous tissue dissection that explicitly reasons about topological changes
from endoscopic images after each dissection action. This structured feedback
guides subsequent actions, enabling the system to localize dissection progress
and adapt policies online. To improve the reliability of such feedback, we
introduce visibility metrics that quantify tissue exposure and formulate
optimal controller designs that actively manipulate tissue to maximize
visibility. Finally, we integrate these feedback mechanisms with both
planning-based and learning-based dissection methods, and demonstrate
experimentally that they significantly enhance autonomy, reduce errors, and
improve robustness in complex surgical scenarios.

</details>


### [177] [From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents](https://arxiv.org/abs/2510.04076)
*Amin Vahidi-Moghaddam,Sayed Pedram Haeri Boroujeni,Iman Jebellat,Ehsan Jebellat,Niloufar Mehrabi,Zhaojian Li*

Main category: cs.RO

TL;DR: 本文针对数据驱动控制策略的高计算和内存需求问题，提出了八种降阶技术，并在多个实际应用中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代控制应用中，特别是在机器人和车辆运动控制中，实现准确、快速和安全的运动是一个主要挑战。数据驱动策略虽然能减少对精确模型的依赖，但存在响应慢、计算需求高和内存需求大等问题。

Method: 通过降阶建模、函数逼近策略学习和凸松弛等技术，减少数据驱动策略的计算复杂度。

Result: 提出的八种方法在实际应用中表现出色，有效解决了数据驱动策略的计算和内存限制问题。

Conclusion: 本文提出了八种降低计算复杂度的技术，并在包括机械臂、软体机器人和车辆运动控制等实际应用中证明了其有效性。

Abstract: One of the main challenges in modern control applications, particularly in
robot and vehicle motion control, is achieving accurate, fast, and safe
movement. To address this, optimal control policies have been developed to
enforce safety while ensuring high performance. Since basic first-principles
models of real systems are often available, model-based controllers are widely
used. Model predictive control (MPC) is a leading approach that optimizes
performance while explicitly handling safety constraints. However, obtaining
accurate models for complex systems is difficult, which motivates data-driven
alternatives. ML-based MPC leverages learned models to reduce reliance on
hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal
policies directly from interaction data. Data-enabled predictive control
(DeePC) goes further by bypassing modeling altogether, directly learning safe
policies from raw input-output data. Recently, large language model (LLM)
agents have also emerged, translating natural language instructions into
structured formulations of optimal control problems. Despite these advances,
data-driven policies face significant limitations. They often suffer from slow
response times, high computational demands, and large memory needs, making them
less practical for real-world systems with fast dynamics, limited onboard
computing, or strict memory constraints. To address this, various technique,
such as reduced-order modeling, function-approximated policy learning, and
convex relaxations, have been proposed to reduce computational complexity. In
this paper, we present eight such approaches and demonstrate their
effectiveness across real-world applications, including robotic arms, soft
robots, and vehicle motion control.

</details>


### [178] [HEHA: Hierarchical Planning for Heterogeneous Multi-Robot Exploration of Unknown Environments](https://arxiv.org/abs/2510.04161)
*Longrui Yang,Yiyu Wang,Jingfan Tang,Yunpeng Lv,Shizhe Zhao,Chao Cao,Zhongqiang Ren*

Main category: cs.RO

TL;DR: HEHA通过分层规划和PEAF算法，优化多异构机器人的探索路径，减少30%探索时间。


<details>
  <summary>Details</summary>
Motivation: 多异构机器人在未知环境中的路径规划面临复杂地形通过性和任务分配的挑战，需要快速迭代解决大规模约束优化问题。

Method: HEHA采用分层方法，将探索任务分解为全局规划和局部规划。全局规划中提出了PEAF（Partial Anytime Focal search）算法，快速找到有界次优解以最小化最大路径长度；局部规划则考虑了异构性以避免重复探索。

Result: 实验结果显示，HEHA比基线方法减少了高达30%的探索时间。

Conclusion: HEHA（Hierarchical Exploration with Heterogeneous Agents）通过结合全局规划和局部规划，显著提高了多异构机器人在未知环境中的探索效率，实验结果表明其比基线方法减少了30%的探索时间。

Abstract: This paper considers the path planning problem for autonomous exploration of
an unknown environment using multiple heterogeneous robots such as drones,
wheeled, and legged robots, which have different capabilities to traverse
complex terrains. A key challenge there is to intelligently allocate the robots
to the unknown areas to be explored and determine the visiting order of those
spaces subject to traversablity constraints, which leads to a large scale
constrained optimization problem that needs to be quickly and iteratively
solved every time when new space are explored. To address the challenge, we
propose HEHA (Hierarchical Exploration with Heterogeneous Agents) by leveraging
a recent hierarchical method that decompose the exploration into global
planning and local planning. The major contribution in HEHA is its global
planning, where we propose a new routing algorithm PEAF (Partial Anytime Focal
search) that can quickly find bounded sub-optimal solutions to minimize the
maximum path length among the agents subject to traversability constraints.
Additionally, the local planner in HEHA also considers heterogeneity to avoid
repeated and duplicated exploration among the robots. The experimental results
show that, our HEHA can reduce up to 30% of the exploration time than the
baselines.

</details>


### [179] [Learning to Capture Rocks using an Excavator: A Reinforcement Learning Approach with Guiding Reward Formulation](https://arxiv.org/abs/2510.04168)
*Amirmasoud Molaei,Reza Ghabcheloo*

Main category: cs.RO

TL;DR: 该论文提出了一种基于强化学习的完全数据驱动的控制框架，用于挖掘机抓取岩石，无需建模岩石或土壤特性，实验显示其成功率高且鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 标准挖掘机抓取岩石是一项复杂任务，现有自主挖掘方法主要针对连续介质或依赖专用夹具，限制了其在现实建筑工场的应用。

Method: 使用AGX Dynamics模拟器和PPO算法训练了一个模型无关的强化学习代理，通过广泛的领域随机化增强鲁棒性。

Result: 实验结果表明，该策略能够很好地泛化到未见过的岩石和不同土壤条件，成功率高且保持机器稳定性。

Conclusion: 该研究展示了基于强化学习的控制框架在岩石抓取任务中的可行性，无需特殊硬件或详细材料模型即可实现与人类操作者相当的成功率。

Abstract: Rock capturing with standard excavator buckets is a challenging task
typically requiring the expertise of skilled operators. Unlike soil digging, it
involves manipulating large, irregular rocks in unstructured environments where
complex contact interactions with granular material make model-based control
impractical. Existing autonomous excavation methods focus mainly on continuous
media or rely on specialized grippers, limiting their applicability to
real-world construction sites. This paper introduces a fully data-driven
control framework for rock capturing that eliminates the need for explicit
modeling of rock or soil properties. A model-free reinforcement learning agent
is trained in the AGX Dynamics simulator using the Proximal Policy Optimization
(PPO) algorithm and a guiding reward formulation. The learned policy outputs
joint velocity commands directly to the boom, arm, and bucket of a CAT365
excavator model. Robustness is enhanced through extensive domain randomization
of rock geometry, density, and mass, as well as the initial configurations of
the bucket, rock, and goal position. To the best of our knowledge, this is the
first study to develop and evaluate an RL-based controller for the rock
capturing task. Experimental results show that the policy generalizes well to
unseen rocks and varying soil conditions, achieving high success rates
comparable to those of human participants while maintaining machine stability.
These findings demonstrate the feasibility of learning-based excavation
strategies for discrete object manipulation without requiring specialized
hardware or detailed material models.

</details>


### [180] [VBM-NET: Visual Base Pose Learning for Mobile Manipulation using Equivariant TransporterNet and GNNs](https://arxiv.org/abs/2510.04171)
*Lakshadeep Naik,Adam Fischer,Daniel Duberg,Danica Kragic*

Main category: cs.RO

TL;DR: VBM-NET通过顶视正交投影学习移动基座姿态规划，结合对称性学习和强化学习，显著减少计算时间并实现仿真到现实迁移。


<details>
  <summary>Details</summary>
Motivation: 解决移动操作中基座姿态规划的挑战，特别是在缺乏精确物体位姿和环境模型的情况下，直接从顶视正交投影中学习。

Method: 提出了VBM-NET，结合了TransporterNet的空间对称性学习、图神经网络的多候选姿态表示以及强化学习的最优姿态选择。

Result: VBM-NET在计算时间显著减少的情况下，达到了与传统方法可比的结果，并成功实现了仿真到现实的迁移。

Conclusion: VBM-NET通过从场景的顶视正交投影中直接规划移动基座姿态，结合了TransporterNet的空间对称性和图神经网络的多候选姿态表示，通过强化学习选择最优姿态，显著减少了计算时间，并成功实现了从仿真到现实的迁移。

Abstract: In Mobile Manipulation, selecting an optimal mobile base pose is essential
for successful object grasping. Previous works have addressed this problem
either through classical planning methods or by learning state-based policies.
They assume access to reliable state information, such as the precise object
poses and environment models. In this work, we study base pose planning
directly from top-down orthographic projections of the scene, which provide a
global overview of the scene while preserving spatial structure. We propose
VBM-NET, a learning-based method for base pose selection using such top-down
orthographic projections. We use equivariant TransporterNet to exploit spatial
symmetries and efficiently learn candidate base poses for grasping. Further, we
use graph neural networks to represent a varying number of candidate base poses
and use Reinforcement Learning to determine the optimal base pose among them.
We show that VBM-NET can produce comparable solutions to the classical methods
in significantly less computation time. Furthermore, we validate sim-to-real
transfer by successfully deploying a policy trained in simulation to real-world
mobile manipulation.

</details>


### [181] [Using Robotics to Improve Transcatheter Edge-to-Edge Repair of the Mitral Valve](https://arxiv.org/abs/2510.04178)
*Léa Pistorius,Namrata U. Nayar,Phillip Tran,Sammy Elmariah,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 本文研究机器人如何辅助经导管二尖瓣边缘到边缘修复，结果显示机器人系统比手动操作更高效、准确。


<details>
  <summary>Details</summary>
Motivation: 经导管瓣膜修复由于手动导管系统的机械限制和陡峭的学习曲线面临重大挑战。

Method: 将复杂的基于手柄的临床修复设备控制替换为通过游戏控制器实现的直观机器人关节控制，并在心脏和血管的体模模型中分解整体设备交付任务为特定运动步骤，比较手动与机器人性能。

Result: 机器人系统可以减少手术时间和运动误差，同时提高夹子放置的准确性。

Conclusion: 机器人辅助系统可以解决手动系统的关键限制，为复杂的经导管手术提供更可靠和用户友好的平台。

Abstract: Transcatheter valve repair presents significant challenges due to the
mechanical limitations and steep learning curve associated with manual catheter
systems. This paper investigates the use of robotics to facilitate
transcatheter procedures in the context of mitral valve edge-to-edge repair.
The complex handle-based control of a clinical repair device is replaced by
intuitive robotic joint-based control via a game controller. Manual versus
robotic performance is analyzed by decomposing the overall device delivery task
into motion-specific steps and comparing capabilities on a step-by-step basis
in a phantom model of the heart and vasculature. Metrics include procedure
duration and clip placement accuracy. Results demonstrate that the robotic
system can reduce procedural time and motion errors while also improving
accuracy of clip placement. These findings suggest that robotic assistance can
address key limitations of manual systems, offering a more reliable and
user-friendly platform for complex transcatheter procedures.

</details>


### [182] [Zenbo Patrol: A Social Assistive Robot Based on Multimodal Deep Learning for Real-time Illegal Parking Recognition and Notification](https://arxiv.org/abs/2510.04190)
*Jian-jie Zheng,Chih-kai Yang,Po-han Chen,Lyn Chao-ling Chen*

Main category: cs.RO

TL;DR: 研究使用GPT-4o多模态模型和社交机器人实时识别非法停车，验证了高准确性并提供了实际应用方案。


<details>
  <summary>Details</summary>
Motivation: 旨在通过社交机器人实时识别并通知非法停车行为，解决停车场管理中的实际问题。

Method: 采用GPT-4o多模态模型进行车牌识别，无需预处理，并通过双模型管道方法进行比较。机器人通过在模拟停车场中导航，自动调整摄像头角度捕获车牌图像。

Result: 实验表明，该方法在车牌识别中具有高准确性，并能实时通知系统管理员非法停车情况。

Conclusion: 该研究提出了一种新颖的多模态深度学习方法，并在车牌识别中验证了高准确性，同时提供了一种社交辅助机器人，可应用于室内停车场解决实际问题。

Abstract: In the study, the social robot act as a patrol to recognize and notify
illegal parking in real-time. Dual-model pipeline method and large multimodal
model were compared, and the GPT-4o multimodal model was adopted in license
plate recognition without preprocessing. For moving smoothly on a flat ground,
the robot navigated in a simulated parking lot in the experiments. The robot
changes angle view of the camera automatically to capture the images around
with the format of license plate number. From the captured images of the robot,
the numbers on the plate are recognized through the GPT-4o model, and
identifies legality of the numbers. When an illegal parking is detected, the
robot sends Line messages to the system manager immediately. The contribution
of the work is that a novel multimodal deep learning method has validated with
high accuracy in license plate recognition, and a social assistive robot is
also provided for solving problems in a real scenario, and can be applied in an
indoor parking lot.

</details>


### [183] [Flexible Locomotion Learning with Diffusion Model Predictive Control](https://arxiv.org/abs/2510.04234)
*Runhan Huang,Haldun Balim,Heng Yang,Yilun Du*

Main category: cs.RO

TL;DR: Diffusion-MPC 结合扩散模型和 MPC，实现灵活的运动控制，适应新任务无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 传统 MPC 依赖精确的动力学模型，这在复杂环境中难以获得；而模型无关的 RL 方法产生的固定策略难以适应新行为。

Method: Diffusion-MPC 利用学习的生成扩散模型作为规划中的近似动力学先验，通过奖励和约束优化实现灵活测试时间适应。

Result: Diffusion-MPC 在现实世界中表现出强大的运动能力和灵活的适应性。

Conclusion: Diffusion-MPC 在现实世界中验证了其强大的运动能力和灵活的适应性，展示了其在无需重新训练的情况下调整到新奖励规格的能力。

Abstract: Legged locomotion demands controllers that are both robust and adaptable,
while remaining compatible with task and safety considerations. However,
model-free reinforcement learning (RL) methods often yield a fixed policy that
can be difficult to adapt to new behaviors at test time. In contrast, Model
Predictive Control (MPC) provides a natural approach to flexible behavior
synthesis by incorporating different objectives and constraints directly into
its optimization process. However, classical MPC relies on accurate dynamics
models, which are often difficult to obtain in complex environments and
typically require simplifying assumptions. We present Diffusion-MPC, which
leverages a learned generative diffusion model as an approximate dynamics prior
for planning, enabling flexible test-time adaptation through reward and
constraint based optimization. Diffusion-MPC jointly predicts future states and
actions; at each reverse step, we incorporate reward planning and impose
constraint projection, yielding trajectories that satisfy task objectives while
remaining within physical limits. To obtain a planning model that adapts beyond
imitation pretraining, we introduce an interactive training algorithm for
diffusion based planner: we execute our reward-and-constraint planner in
environment, then filter and reweight the collected trajectories by their
realized returns before updating the denoiser. Our design enables strong
test-time adaptability, allowing the planner to adjust to new reward
specifications without retraining. We validate Diffusion-MPC on real world,
demonstrating strong locomotion and flexible adaptation.

</details>


### [184] [ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context](https://arxiv.org/abs/2510.04246)
*Huiwon Jang,Sihyun Yu,Heeseung Kwon,Hojin Jeon,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: ContextVLA通过压缩多帧观测为单个令牌，提升机器人任务性能并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有行为克隆方法在多帧观测上表现不一致，而Vision-Language-Action模型（VLA）能更有效地利用多帧观测生成动作，但其高维视频输入导致计算开销大。

Method: 引入ContextVLA策略模型，利用Vision-Language-Action模型（VLA）的固有时间理解能力，将多帧观测压缩为单个上下文令牌。

Result: 实验表明，ContextVLA在性能上优于单帧VLA，并实现了与完整多帧训练相当的效果，同时减少了训练和推理时间。

Conclusion: ContextVLA通过将多帧观测压缩为单个上下文令牌，有效提升了机器人任务的性能，同时减少了训练和推理时间。

Abstract: Leveraging temporal context is crucial for success in partially observable
robotic tasks. However, prior work in behavior cloning has demonstrated
inconsistent performance gains when using multi-frame observations. In this
paper, we introduce ContextVLA, a policy model that robustly improves robotic
task performance by effectively leveraging multi-frame observations. Our
approach is motivated by the key observation that Vision-Language-Action models
(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more
effectively utilize multi-frame observations for action generation. This
suggests that VLMs' inherent temporal understanding capability enables them to
extract more meaningful context from multi-frame observations. However, the
high dimensionality of video inputs introduces significant computational
overhead, making VLA training and inference inefficient. To address this,
ContextVLA compresses past observations into a single context token, allowing
the policy to efficiently leverage temporal context for action generation. Our
experiments show that ContextVLA consistently improves over single-frame VLAs
and achieves the benefits of full multi-frame training but with reduced
training and inference times.

</details>


### [185] [Integrated Planning and Control on Manifolds: Factor Graph Representation and Toolkit](https://arxiv.org/abs/2510.04278)
*Peiwen Yang,Weisong Wen,Runqiu Yang,Yuanyuan Zhang,Jiahao Hu,Yingming Chen,Naigui Xiao,Jiaqi Zhao*

Main category: cs.RO

TL;DR: FactorMPC是一个基于因子图的MPC工具包，解决了非线性流形控制问题，在四旋翼无人机上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统MPC在非线性流形（如机器人姿态动力学和约束运动规划）上应用时面临奇异性、过参数化和收敛性差的问题，亟需一种几何一致且高效的控制框架。

Method: 本文提出了FactorMPC，一个基于因子图的MPC工具包，将系统动力学、约束和目标统一到一个模块化的优化结构中，支持流形状态与高斯不确定性建模，并利用因子图的稀疏性和概率结构实现实时性能。

Result: 仿真和实验结果表明，FactorMPC在四旋翼无人机上实现了卓越的轨迹跟踪和障碍物避障性能。

Conclusion: FactorMPC通过因子图方法提供了一个模块化、用户友好且高效的优化结构，成功解决了非线性流形上系统控制的挑战，并在四旋翼无人机上展示了优于基线方法的轨迹跟踪和障碍物避障性能。

Abstract: Model predictive control (MPC) faces significant limitations when applied to
systems evolving on nonlinear manifolds, such as robotic attitude dynamics and
constrained motion planning, where traditional Euclidean formulations struggle
with singularities, over-parameterization, and poor convergence. To overcome
these challenges, this paper introduces FactorMPC, a factor-graph based MPC
toolkit that unifies system dynamics, constraints, and objectives into a
modular, user-friendly, and efficient optimization structure. Our approach
natively supports manifold-valued states with Gaussian uncertainties modeled in
tangent spaces. By exploiting the sparsity and probabilistic structure of
factor graphs, the toolkit achieves real-time performance even for
high-dimensional systems with complex constraints. The velocity-extended
on-manifold control barrier function (CBF)-based obstacle avoidance factors are
designed for safety-critical applications. By bridging graphical models with
safety-critical MPC, our work offers a scalable and geometrically consistent
framework for integrated planning and control. The simulations and experimental
results on the quadrotor demonstrate superior trajectory tracking and obstacle
avoidance performance compared to baseline methods. To foster research
reproducibility, we have provided open-source implementation offering
plug-and-play factors.

</details>


### [186] [Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation](https://arxiv.org/abs/2510.04353)
*Stephen McCrory,Romeo Orsolino,Dhruv Thanki,Luigi Penco,Robert Griffin*

Main category: cs.RO

TL;DR: 论文提出了一种基于质心稳定性的重定向方法，通过动态调整接触点和姿态，有效提升了人形机器人在复杂操作中的稳定性，实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 在包含手部接触和非共面表面的复杂操作中，传统遥操作方法容易导致电机扭矩饱和或失稳，因此需要一种能增强稳定性的新方法。

Method: 采用了一种高效的质心稳定性裕度梯度解析计算方法，动态调整接触点和姿态，以优化稳定性。

Result: 在仿真和硬件实验中，该方法显著提高了稳定性裕度，并验证了高稳定性裕度与冲击弹性和关节扭矩裕度的正相关性。

Conclusion: 该论文提出了一种基于质心稳定性的重定向方法，通过动态调整接触点和姿态，增强了人形机器人在复杂操作中的稳定性。实验验证了该方法能有效提高稳定性裕度，并与冲击弹性和关节扭矩裕度正相关。

Abstract: Teleoperation is a powerful method to generate reference motions and enable
humanoid robots to perform a broad range of tasks. However, teleoperation
becomes challenging when using hand contacts and non-coplanar surfaces, often
leading to motor torque saturation or loss of stability through slipping. We
propose a centroidal stability-based retargeting method that dynamically
adjusts contact points and posture during teleoperation to enhance stability in
these difficult scenarios. Central to our approach is an efficient analytical
calculation of the stability margin gradient. This gradient is used to identify
scenarios for which stability is highly sensitive to teleoperation setpoints
and inform the local adjustment of these setpoints. We validate the framework
in simulation and hardware by teleoperating manipulation tasks on a humanoid,
demonstrating increased stability margins. We also demonstrate empirically that
higher stability margins correlate with improved impulse resilience and joint
torque margin.

</details>


### [187] [Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators](https://arxiv.org/abs/2510.04354)
*Apurva Badithela,David Snyder,Lihan Zha,Joseph Mikhail,Matthew O'Kelly,Anushri Dixit,Anirudha Majumdar*

Main category: cs.RO

TL;DR: SureSim框架通过结合仿真和少量真实测试，提供可靠的机器人策略性能评估，显著减少硬件测试需求。


<details>
  <summary>Details</summary>
Motivation: 现有机器人策略评估通常缺乏统计保证，SureSim旨在通过结合仿真和真实数据提供更可靠的性能推断。

Method: 将真实和仿真评估的结合问题形式化为预测驱动的推理问题，并利用非渐近均值估计算法提供策略性能的置信区间。

Result: 使用SureSim可节省20-25%的硬件评估工作量，同时获得相似的策略性能边界。

Conclusion: SureSim框架通过结合大规模仿真和小规模真实世界测试，显著减少了硬件评估的工作量，同时提供了对策略性能的可靠推断。

Abstract: Rapid progress in imitation learning, foundation models, and large-scale
datasets has led to robot manipulation policies that generalize to a wide-range
of tasks and environments. However, rigorous evaluation of these policies
remains a challenge. Typically in practice, robot policies are often evaluated
on a small number of hardware trials without any statistical assurances. We
present SureSim, a framework to augment large-scale simulation with relatively
small-scale real-world testing to provide reliable inferences on the real-world
performance of a policy. Our key idea is to formalize the problem of combining
real and simulation evaluations as a prediction-powered inference problem, in
which a small number of paired real and simulation evaluations are used to
rectify bias in large-scale simulation. We then leverage non-asymptotic mean
estimation algorithms to provide confidence intervals on mean policy
performance. Using physics-based simulation, we evaluate both diffusion policy
and multi-task fine-tuned \(\pi_0\) on a joint distribution of objects and
initial conditions, and find that our approach saves over \(20-25\%\) of
hardware evaluation effort to achieve similar bounds on policy performance.

</details>


### [188] [PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory Optimization](https://arxiv.org/abs/2510.04436)
*Jushan Chen,Santiago Paternain*

Main category: cs.RO

TL;DR: 提出了一种基于扩散的直接轨迹优化方法，通过梯度自由投影确保动态可行性，显著提高了成功率和可行性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在轨迹优化中因能建模多模态概率分布而受到关注，但现有方法无法有效处理非线性等式约束（如动态可行性），导致次优解。

Method: 提出了一种直接轨迹优化方法，通过基于模型的扩散生成状态序列，并在反向扩散过程中加入梯度自由投影机制以确保动态可行性。

Result: 与现有最优基线相比，该方法在四旋翼航点导航场景中实现了零动态可行性误差和约4倍的成功率提升。

Conclusion: 本文提出了一种新颖的直接轨迹优化方法，通过基于模型的扩散直接生成状态序列，并结合梯度自由投影机制确保动态可行性，显著提高了成功率并实现了零动态可行性误差。

Abstract: Recently, diffusion models have gained popularity and attention in trajectory
optimization due to their capability of modeling multi-modal probability
distributions. However, addressing nonlinear equality constraints, i.e, dynamic
feasi- bility, remains a great challenge in diffusion-based trajectory
optimization. Recent diffusion-based trajectory optimization frameworks rely on
a single-shooting style approach where the denoised control sequence is applied
to forward propagate the dynamical system, which cannot explicitly enforce
constraints on the states and frequently leads to sub-optimal solutions. In
this work, we propose a novel direct trajectory optimization approach via
model-based diffusion, which directly generates a sequence of states. To ensure
dynamic feasibility, we propose a gradient-free projection mechanism that is
incorporated into the reverse diffusion process. Our results show that,
compared to a recent state-of-the-art baseline, our approach leads to zero
dynamic feasibility error and approximately 4x higher success rate in a
quadrotor waypoint navigation scenario involving dense static obstacles.

</details>


### [189] [Velocity-Form Data-Enabled Predictive Control of Soft Robots under Unknown External Payloads](https://arxiv.org/abs/2510.04509)
*Huanqing Wang,Kaixiang Zhang,Kyungjoon Lee,Yu Mei,Vaibhav Srivastava,Jun Sheng,Ziyou Song,Zhaojian Li*

Main category: cs.RO

TL;DR: 提出了一种速度形式的DeePC框架，通过增量数据表示解决了软机器人在未知负载下的控制问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 在物体操作任务中，未知的外部负载和干扰会显著改变系统动态和行为，导致偏移误差和控制性能下降，现有方法（如标准DeePC）难以应对。

Method: 利用增量表示的输入-输出数据，无需加权数据集或干扰估计器，实现了对未知负载下软机器人的鲁棒和最优控制。

Result: 实验验证表明，所提出的速度形式DeePC框架在涉及未知负载的场景中，性能优于标准DeePC。

Conclusion: 本文提出了一种新颖的基于速度形式的数据驱动预测控制（DeePC）框架，有效解决了软机器人在未知负载下的控制性能退化问题，实验验证了其优于标准DeePC的性能。

Abstract: Data-driven control methods such as data-enabled predictive control (DeePC)
have shown strong potential in efficient control of soft robots without
explicit parametric models. However, in object manipulation tasks, unknown
external payloads and disturbances can significantly alter the system dynamics
and behavior, leading to offset error and degraded control performance. In this
paper, we present a novel velocity-form DeePC framework that achieves robust
and optimal control of soft robots under unknown payloads. The proposed
framework leverages input-output data in an incremental representation to
mitigate performance degradation induced by unknown payloads, eliminating the
need for weighted datasets or disturbance estimators. We validate the method
experimentally on a planar soft robot and demonstrate its superior performance
compared to standard DeePC in scenarios involving unknown payloads.

</details>


### [190] [Everything-Grasping (EG) Gripper: A Universal Gripper with Synergistic Suction-Grasping Capabilities for Cross-Scale and Cross-State Manipulation](https://arxiv.org/abs/2510.04585)
*Jianshu Zhou,Jing Shu,Tianle Pan,Puchen Zhu,Jiajun An,Huayu Zhang,Junda Huang,Upinder Kaur,Xin Ma,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: EG Gripper结合吸附与颗粒阻塞技术，实现跨尺度、跨状态物体抓取，无需气密密封，首次统一抓取固体和液体。


<details>
  <summary>Details</summary>
Motivation: 解决软机器人中单一夹具难以抓取不同大小和物理状态（固体和液体）物体的挑战。

Method: 采用分布式表面吸附与内部颗粒阻塞技术，结合触觉感应框架和TIGMS算法，实现自主选择抓取模式。

Result: EG Gripper能够抓取从0.2 mm²到62,000 mm²的物体，并成功区分固体和液体目标，展示了在各种任务中的稳健性能。

Conclusion: EG Gripper通过结合分布式表面吸附与内部颗粒阻塞技术，实现了跨尺度和跨状态的物体抓取，无需气密密封，展示了在软机器人领域的创新应用。

Abstract: Grasping objects across vastly different sizes and physical states-including
both solids and liquids-with a single robotic gripper remains a fundamental
challenge in soft robotics. We present the Everything-Grasping (EG) Gripper, a
soft end-effector that synergistically integrates distributed surface suction
with internal granular jamming, enabling cross-scale and cross-state
manipulation without requiring airtight sealing at the contact interface with
target objects. The EG Gripper can handle objects with surface areas ranging
from sub-millimeter scale 0.2 mm2 (glass bead) to over 62,000 mm2 (A4 sized
paper and woven bag), enabling manipulation of objects nearly 3,500X smaller
and 88X larger than its own contact area (approximated at 707 mm2 for a 30
mm-diameter base). We further introduce a tactile sensing framework that
combines liquid detection and pressure-based suction feedback, enabling
real-time differentiation between solid and liquid targets. Guided by the
actile-Inferred Grasping Mode Selection (TIGMS) algorithm, the gripper
autonomously selects grasping modes based on distributed pressure and voltage
signals. Experiments across diverse tasks-including underwater grasping,
fragile object handling, and liquid capture-demonstrate robust and repeatable
performance. To our knowledge, this is the first soft gripper to reliably grasp
both solid and liquid objects across scales using a unified compliant
architecture.

</details>


### [191] [MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile Manipulation](https://arxiv.org/abs/2510.04592)
*Yilin Mei,Peng Qiu,Wei Zhang,WenChao Zhang,Wenjie Song*

Main category: cs.RO

TL;DR: \textit{MobRT} 是一个数字孪生框架，通过自动生成多样化演示解决了移动机械臂模仿学习中的数据挑战，显著提升了策略性能。


<details>
  <summary>Details</summary>
Motivation: 模仿学习依赖于大规模、高质量的演示数据，但收集这些数据对于移动机械臂尤为困难，导致现有研究多集中于简单的桌面场景，移动操作相对未被充分探索。

Method: \textit{MobRT} 是一个基于数字孪生的框架，通过虚拟运动控制和全身运动规划自主生成多样化和真实的演示。

Result: 实验表明，\textit{MobRT} 生成的数据质量高，任务成功率与生成的轨迹数量强相关，且仿真和现实演示的结合显著提升了策略性能。

Conclusion: \textit{MobRT} 框架通过模拟复杂任务和自动生成多样化演示，显著提升了策略的泛化能力和性能，在仿真和现实环境中均取得了稳健的结果。

Abstract: Recent advances in robotics have been largely driven by imitation learning,
which depends critically on large-scale, high-quality demonstration data.
However, collecting such data remains a significant challenge-particularly for
mobile manipulators, which must coordinate base locomotion and arm manipulation
in high-dimensional, dynamic, and partially observable environments.
Consequently, most existing research remains focused on simpler tabletop
scenarios, leaving mobile manipulation relatively underexplored. To bridge this
gap, we present \textit{MobRT}, a digital twin-based framework designed to
simulate two primary categories of complex, whole-body tasks: interaction with
articulated objects (e.g., opening doors and drawers) and mobile-base
pick-and-place operations. \textit{MobRT} autonomously generates diverse and
realistic demonstrations through the integration of virtual kinematic control
and whole-body motion planning, enabling coherent and physically consistent
execution. We evaluate the quality of \textit{MobRT}-generated data across
multiple baseline algorithms, establishing a comprehensive benchmark and
demonstrating a strong correlation between task success and the number of
generated trajectories. Experiments integrating both simulated and real-world
demonstrations confirm that our approach markedly improves policy
generalization and performance, achieving robust results in both simulated and
real-world environments.

</details>


### [192] [OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS](https://arxiv.org/abs/2510.04612)
*Simon Boche,Jaehyung Jung,Sebastián Barbas Laina,Stefan Leutenegger*

Main category: cs.RO

TL;DR: OKVIS2-X是一个多传感器SLAM系统，通过密集体积地图和高效子地图策略实现高精度和鲁棒性，适用于大规模环境。


<details>
  <summary>Details</summary>
Motivation: 为移动机器人提供高精度的状态估计和鲁棒性，同时生成可直接用于自主导航的全局一致地图。

Method: 采用高效子地图策略和紧密耦合的估计器与子地图，通过地图对齐因子提升精度，支持多传感器（视觉、惯性、深度、LiDAR、GNSS）集成。

Result: 在EuRoC数据集上表现出最高的轨迹精度，在Hilti22 VI-only基准测试中超越所有竞争对手，并在VBR数据集的大规模序列中展示了先进精度。

Conclusion: OKVIS2-X是一个先进的SLAM系统，能够生成密集的体积占用地图，并在大规模环境中实时运行，展现出卓越的轨迹精度和鲁棒性。

Abstract: To empower mobile robots with usable maps as well as highest state estimation
accuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor
Simultaneous Localization and Mapping (SLAM) system building dense volumetric
occupancy maps, while scalable to large environments and operating in realtime.
Our unified SLAM framework seamlessly integrates different sensor modalities:
visual, inertial, measured or learned depth, LiDAR and Global Navigation
Satellite System (GNSS) measurements. Unlike most state-of-the-art SLAM
systems, we advocate using dense volumetric map representations when leveraging
depth or range-sensing capabilities. We employ an efficient submapping strategy
that allows our system to scale to large environments, showcased in sequences
of up to 9 kilometers. OKVIS2-X enhances its accuracy and robustness by
tightly-coupling the estimator and submaps through map alignment factors. Our
system provides globally consistent maps, directly usable for autonomous
navigation. To further improve the accuracy of OKVIS2-X, we also incorporate
the option of performing online calibration of camera extrinsics. Our system
achieves the highest trajectory accuracy in EuRoC against state-of-the-art
alternatives, outperforms all competitors in the Hilti22 VI-only benchmark,
while also proving competitive in the LiDAR version, and showcases state of the
art accuracy in the diverse and large-scale sequences from the VBR dataset.

</details>


### [193] [Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies](https://arxiv.org/abs/2510.04692)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.RO

TL;DR: 该论文提出了一种仿生机器人平台，通过数字制造和智能感知技术，实现了与野生鸟类自然交互，为生态研究和保护提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 研究野生鸟类行为具有挑战性，需要高度逼真的形态、耐用的户外操作和能适应非受控环境的智能感知。

Method: 采用高分辨率结构光3D扫描、参数化CAD建模、关节式3D打印和逼真的UV纹理乙烯基饰面，打造了解剖学上精确且耐用的机器人替代品。六轮摇臂转向架底盘确保在沙地和不规则地形上的稳定移动，嵌入式NVIDIA Jetson模块实现实时RGB和热感知，轻量级YOLO检测以及自主视觉伺服循环。

Result: 沙漠鸟舍的现场试验证实，该平台在15至22 FPS下可靠运行，延迟低于100毫秒，并能引发活体Houbara bustard在恶劣户外条件下的自然识别和交互反应。

Conclusion: 该集成框架通过结合可复制的数字制造、具身视觉智能和生态验证，为动物机器人交互研究、保护机器人和公众参与提供了可转移的蓝图。

Abstract: Biomimetic intelligence and robotics are transforming field ecology by
enabling lifelike robotic surrogates that interact naturally with animals under
real world conditions. Studying avian behavior in the wild remains challenging
due to the need for highly realistic morphology, durable outdoor operation, and
intelligent perception that can adapt to uncontrolled environments. We present
a next generation bio inspired robotic platform that replicates the morphology
and visual appearance of the female Houbara bustard to support controlled
ethological studies and conservation oriented field research. The system
introduces a fully digitally replicable fabrication workflow that combines high
resolution structured light 3D scanning, parametric CAD modelling, articulated
3D printing, and photorealistic UV textured vinyl finishing to achieve
anatomically accurate and durable robotic surrogates. A six wheeled rocker
bogie chassis ensures stable mobility on sand and irregular terrain, while an
embedded NVIDIA Jetson module enables real time RGB and thermal perception,
lightweight YOLO based detection, and an autonomous visual servoing loop that
aligns the robot's head toward detected targets without human intervention. A
lightweight thermal visible fusion module enhances perception in low light
conditions. Field trials in desert aviaries demonstrated reliable real time
operation at 15 to 22 FPS with latency under 100 ms and confirmed that the
platform elicits natural recognition and interactive responses from live
Houbara bustards under harsh outdoor conditions. This integrated framework
advances biomimetic field robotics by uniting reproducible digital fabrication,
embodied visual intelligence, and ecological validation, providing a
transferable blueprint for animal robot interaction research, conservation
robotics, and public engagement.

</details>


### [194] [Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly](https://arxiv.org/abs/2510.04696)
*Alexander L. Mitchell,Joe Watson,Ingmar Posner*

Main category: cs.RO

TL;DR: 提出一种基于梯度的分散式框架，通过自适应势函数自动生成子目标，有效解决双机械臂装配中的快速重新规划和协调问题。


<details>
  <summary>Details</summary>
Motivation: 双机械臂装配中存在高层次的序列规划、多机器人协调和低层次的接触操作等挑战，传统任务与运动规划（TAMP）方法在应对需要新任务序列和优化的扰动时收敛速度慢。

Method: 采用基于梯度的分散式框架，利用分段连续能量函数和自适应势函数的自动组合，通过短视优化生成子目标，而非长时规划。

Result: 该方法在物理双机械臂装配任务中表现出色，能够自动生成重试、协调动作和自主交接，适用于紧公差装配。

Conclusion: 该论文提出了一种基于梯度的分散式框架，通过自适应势函数的自动组合生成子目标，有效解决了双机械臂装配中的快速重新规划和协调问题。

Abstract: There are many challenges in bimanual assembly, including high-level
sequencing, multi-robot coordination, and low-level, contact-rich operations
such as component mating. Task and motion planning (TAMP) methods, while
effective in this domain, may be prohibitively slow to converge when adapting
to disturbances that require new task sequencing and optimisation. These events
are common during tight-tolerance assembly, where difficult-to-model dynamics
such as friction or deformation require rapid replanning and reattempts.
Moreover, defining explicit task sequences for assembly can be cumbersome,
limiting flexibility when task replanning is required. To simplify this
planning, we introduce a decentralised gradient-based framework that uses a
piecewise continuous energy function through the automatic composition of
adaptive potential functions. This approach generates sub-goals using only
myopic optimisation, rather than long-horizon planning. It demonstrates
effectiveness at solving long-horizon tasks due to the structure and adaptivity
of the energy function. We show that our approach scales to physical bimanual
assembly tasks for constructing tight-tolerance assemblies. In these
experiments, we discover that our gradient-based rapid replanning framework
generates automatic retries, coordinated motions and autonomous handovers in an
emergent fashion.

</details>


### [195] [Performance-guided Task-specific Optimization for Multirotor Design](https://arxiv.org/abs/2510.04724)
*Etor Arza,Welf Rehberg,Philipp Weiss,Mihir Kulkarni,Kostas Alexis*

Main category: cs.RO

TL;DR: 该论文提出了一种结合强化学习和优化算法的微型飞行器设计方法，优化后的设计在任务性能上超越传统配置，并验证了其实际可行性。


<details>
  <summary>Details</summary>
Motivation: 优化微型飞行器的任务特定设计，以提高其在敏捷航点导航任务中的性能。

Method: 利用强化学习、贝叶斯优化和协方差矩阵适应进化策略，系统探索了电机姿态配置的设计空间，同时考虑制造约束和最小化空气动力学干扰。

Result: 优化设计在敏捷航点导航任务中表现优于传统多旋翼配置，包括文献中的全驱动设计，并通过真实世界测试验证了模拟到现实的转移能力。

Conclusion: 通过强化学习、贝叶斯优化和协方差矩阵适应进化策略，该方法成功优化了微型飞行器的设计，验证了其在现实世界中的可转移性，并展示了优于传统设计的性能。

Abstract: This paper introduces a methodology for task-specific design optimization of
multirotor Micro Aerial Vehicles. By leveraging reinforcement learning,
Bayesian optimization, and covariance matrix adaptation evolution strategy, we
optimize aerial robot designs guided exclusively by their closed-loop
performance in a considered task. Our approach systematically explores the
design space of motor pose configurations while ensuring manufacturability
constraints and minimal aerodynamic interference. Results demonstrate that
optimized designs achieve superior performance compared to conventional
multirotor configurations in agile waypoint navigation tasks, including against
fully actuated designs from the literature. We build and test one of the
optimized designs in the real world to validate the sim2real transferability of
our approach.

</details>


### [196] [Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy](https://arxiv.org/abs/2510.04774)
*Weixu Zhu,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: SoNS通过自动生成代码帮助机器人群体完成任务，成功率达85%。


<details>
  <summary>Details</summary>
Motivation: 解决机器人群体行为设计复杂性和实时环境估计的挑战，实现自动代码生成以提高任务成功率。

Method: 在6个真实机器人和超过30个机器人的模拟试验中，SoNS增强的机器人群体在卡住时自动请求并运行由外部LLM生成的代码。

Result: SoNS增强的机器人群体在任务中实现了85%的成功率。

Conclusion: SoNS为机器人群体提供了行为设计的便捷性和全局配置估计，通过外部LLM自动生成代码，实现了85%的任务成功率。

Abstract: Our recently introduced self-organizing nervous system (SoNS) provides robot
swarms with 1) ease of behavior design and 2) global estimation of the swarm
configuration and its collective environment, facilitating the implementation
of online automatic code generation for robot swarms. In a demonstration with 6
real robots and simulation trials with >30 robots, we show that when a
SoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code
generated by an external LLM on the fly, completing its mission with an 85%
success rate.

</details>


### [197] [TAG-K: Tail-Averaged Greedy Kaczmarz for Computationally Efficient and Performant Online Inertial Parameter Estimation](https://arxiv.org/abs/2510.04839)
*Shuo Sha,Anupam Bhakta,Zhenyuan Jiang,Kevin Qiu,Ishaan Mahajan,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: TAG-K 是一种轻量级 Kaczmarz 扩展方法，结合贪婪随机行选择和尾部平均，显著提升了参数估计的速度、鲁棒性和跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 传统的递归最小二乘法（RLS）和卡尔曼滤波器（KF）在跟踪参数突变或计算成本方面存在不足，限制了它们在动态环境中的有效性。

Method: TAG-K 是一种基于 Kaczmarz 方法的轻量级扩展，结合贪婪随机行选择以实现快速收敛，并通过尾部平均增强噪声和不一致性下的鲁棒性。

Result: 在合成基准和四旋翼跟踪任务中，TAG-K 比 RLS、KF 和其他 Kaczmarz 变体快 1.5x-1.9x（笔记本电脑 CPU）和 4.8x-20.7x（嵌入式微控制器），同时噪声鲁棒性更强，估计误差减少 25%，端到端跟踪性能提升近 2 倍。

Conclusion: TAG-K 在轻量级扩展中结合了贪婪随机行选择和尾部平均方法，显著提升了参数估计的速度和鲁棒性，适用于动态环境和计算资源受限的机器人系统。

Abstract: Accurate online inertial parameter estimation is essential for adaptive
robotic control, enabling real-time adjustment to payload changes,
environmental interactions, and system wear. Traditional methods such as
Recursive Least Squares (RLS) and the Kalman Filter (KF) often struggle to
track abrupt parameter shifts or incur high computational costs, limiting their
effectiveness in dynamic environments and for computationally constrained
robotic systems. As such, we introduce TAG-K, a lightweight extension of the
Kaczmarz method that combines greedy randomized row selection for rapid
convergence with tail averaging for robustness under noise and inconsistency.
This design enables fast, stable parameter adaptation while retaining the low
per-iteration complexity inherent to the Kaczmarz framework. We evaluate TAG-K
in synthetic benchmarks and quadrotor tracking tasks against RLS, KF, and other
Kaczmarz variants. TAG-K achieves 1.5x-1.9x faster solve times on laptop-class
CPUs and 4.8x-20.7x faster solve times on embedded microcontrollers. More
importantly, these speedups are paired with improved resilience to measurement
noise and a 25% reduction in estimation error, leading to nearly 2x better
end-to-end tracking performance.

</details>


### [198] [CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery](https://arxiv.org/abs/2510.04883)
*Nathan Shankar,Pawel Ladosz,Hujun Yin*

Main category: cs.RO

TL;DR: 提出基于U-Net的IR图像重建方法，消除发射器干扰，提升机器人视觉性能，适用于各种光照条件。


<details>
  <summary>Details</summary>
Motivation: 红外流在低光条件下比RGB流更抗噪，但主动发射器模式会干扰高层任务（如目标检测、跟踪和定位），因此需要一种方法来重建干净的IR图像。

Method: 采用U-Net架构对含有主动发射器模式的红外图像进行重建，以消除噪声并提升图像质量。

Result: 该方法优于现有增强技术，能够在从良好光照到极端低光的环境中实现可靠的视觉驱动机器人系统操作。

Conclusion: 该论文提出了一种基于U-Net的架构，能够从带有发射器干扰的红外图像中重建出干净的图像，显著提升了图像质量和下游机器人任务的性能，适用于从良好光照到极端低光环境下的视觉驱动机器人系统。

Abstract: This paper presents a novel approach for enabling robust robotic perception
in dark environments using infrared (IR) stream. IR stream is less susceptible
to noise than RGB in low-light conditions. However, it is dominated by active
emitter patterns that hinder high-level tasks such as object detection,
tracking and localisation. To address this, a U-Net-based architecture is
proposed that reconstructs clean IR images from emitter-populated input,
improving both image quality and downstream robotic performance. This approach
outperforms existing enhancement techniques and enables reliable operation of
vision-driven robotic systems across illumination conditions from well-lit to
extreme low-light scenes.

</details>


### [199] [HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks](https://arxiv.org/abs/2510.04898)
*Zheng Xiong,Kang Li,Zilin Wang,Matthew Jackson,Jakob Foerster,Shimon Whiteson*

Main category: cs.RO

TL;DR: HyperVLA通过超网络架构和算法优化，显著降低VLA模型的推理成本，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的推理成本极高，限制了其实际应用。

Method: HyperVLA采用基于超网络的架构，仅在推理时激活小型任务特定策略，同时保留了训练时的高模型容量。关键算法设计包括利用现有视觉基础模型的先验知识、超网络归一化和动作生成策略。

Result: HyperVLA在零样本泛化和少样本适应方面与现有VLA模型性能相当甚至更高，同时将激活参数减少90倍，推理速度提升120倍。

Conclusion: HyperVLA通过创新的超网络架构和关键算法设计，显著降低了推理成本，同时保持了与现有VLA模型相当甚至更高的性能。

Abstract: Built upon language and vision foundation models with strong generalization
ability and trained on large-scale robotic data, Vision-Language-Action (VLA)
models have recently emerged as a promising approach to learning generalist
robotic policies. However, a key drawback of existing VLAs is their extremely
high inference costs. In this paper, we propose HyperVLA to address this
problem. Unlike existing monolithic VLAs that activate the whole model during
both training and inference, HyperVLA uses a novel hypernetwork (HN)-based
architecture that activates only a small task-specific policy during inference,
while still retaining the high model capacity needed to accommodate diverse
multi-task behaviors during training. Successfully training an HN-based VLA is
nontrivial so HyperVLA contains several key algorithm design features that
improve its performance, including properly utilizing the prior knowledge from
existing vision foundation models, HN normalization, and an action generation
strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even
higher success rate for both zero-shot generalization and few-shot adaptation,
while significantly reducing inference costs. Compared to OpenVLA, a
state-of-the-art VLA model, HyperVLA reduces the number of activated parameters
at test time by $90\times$, and accelerates inference speed by $120\times$.
Code is publicly available at https://github.com/MasterXiong/HyperVLA

</details>


### [200] [Efficient Navigation in Unknown Indoor Environments with Vision-Language Models](https://arxiv.org/abs/2510.04991)
*D. Schwartz,K. Kondo,J. P. How*

Main category: cs.RO

TL;DR: 利用VLM进行零样本推理的新型规划框架，显著提升未知室内环境中的导航效率，路径平均缩短10%。


<details>
  <summary>Details</summary>
Motivation: 传统探索方法因全局推理能力有限和依赖局部启发式方法，常导致效率低下的路径规划。

Method: 该方法将3D占用网格转换为部分2D环境地图，生成候选子目标，并通过VLM对这些子目标进行评估和排序。

Result: 该方法在模拟中展示了更高的导航效率，减少了常见的贪婪失败（如绕入小房间），平均路径缩短约10%。

Conclusion: 该论文提出了一种利用视觉语言模型（VLM）的新型高层规划框架，显著提高了在未知室内环境中的自主导航效率，平均路径缩短约10%。

Abstract: We present a novel high-level planning framework that leverages
vision-language models (VLMs) to improve autonomous navigation in unknown
indoor environments with many dead ends. Traditional exploration methods often
take inefficient routes due to limited global reasoning and reliance on local
heuristics. In contrast, our approach enables a VLM to reason directly about an
occupancy map in a zero-shot manner, selecting subgoals that are likely to lead
to more efficient paths. At each planning step, we convert a 3D occupancy grid
into a partial 2D map of the environment, and generate candidate subgoals. Each
subgoal is then evaluated and ranked against other candidates by the model. We
integrate this planning scheme into DYNUS \cite{kondo2025dynus}, a
state-of-the-art trajectory planner, and demonstrate improved navigation
efficiency in simulation. The VLM infers structural patterns (e.g., rooms,
corridors) from incomplete maps and balances the need to make progress toward a
goal against the risk of entering unknown space. This reduces common greedy
failures (e.g., detouring into small rooms) and achieves about 10\% shorter
paths on average.

</details>


### [201] [Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot](https://arxiv.org/abs/2510.05001)
*Aditya Sripada,Abhishek Warrier*

Main category: cs.RO

TL;DR: TARS3D机器人受电影启发，通过分析模型和强化学习实现了多种运动模式，展示了非拟人化形态的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究非拟人化形态在机器人运动中的潜力，尤其是受电影《星际穿越》中TARS机器人启发的设计。

Method: 构建了降阶模型，推导了闭式极限环条件，并在硬件上验证了预测；使用深度强化学习在模拟中探索未开发的运动空间。

Result: 实验证实机器人能够遵守其髋关节限制，交替左右接触无干扰，并在滚动模式下保持八步混合极限环；学习策略能够恢复分析步态并发现新行为。

Conclusion: TARS3D的虚构启发生物超越形态能够实现多种先前未探索的运动模式，结合分析合成和强化学习为多模态机器人开辟了一条有前景的路径。

Abstract: Robotic locomotion research typically draws from biologically inspired leg
designs, yet many human-engineered settings can benefit from
non-anthropomorphic forms. TARS3D translates the block-shaped 'TARS' robot from
Interstellar into a 0.25 m, 0.99 kg research platform with seven actuated
degrees of freedom. The film shows two primary gaits: a bipedal-like walk and a
high-speed rolling mode. For TARS3D, we build reduced-order models for each,
derive closed-form limit-cycle conditions, and validate the predictions on
hardware. Experiments confirm that the robot respects its +/-150 degree hip
limits, alternates left-right contacts without interference, and maintains an
eight-step hybrid limit cycle in rolling mode. Because each telescopic leg
provides four contact corners, the rolling gait is modeled as an eight-spoke
double rimless wheel. The robot's telescopic leg redundancy implies a far
richer gait repertoire than the two limit cycles treated analytically. So, we
used deep reinforcement learning (DRL) in simulation to search the unexplored
space. We observed that the learned policy can recover the analytic gaits under
the right priors and discover novel behaviors as well. Our findings show that
TARS3D's fiction-inspired bio-transcending morphology can realize multiple
previously unexplored locomotion modes and that further learning-driven search
is likely to reveal more. This combination of analytic synthesis and
reinforcement learning opens a promising pathway for multimodal robotics.

</details>


### [202] [StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation](https://arxiv.org/abs/2510.05057)
*Mingyu Liu,Jiuhe Shu,Hui Chen,Zeju Li,Canyu Zhao,Jiange Yang,Shenyuan Gao,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: StaMo通过无监督学习从静态图像中提取紧凑状态表示，生成可解释的潜在动作，显著提升机器人任务性能并支持跨数据源扩展。


<details>
  <summary>Details</summary>
Motivation: 现有方法在表达性和紧凑性之间难以平衡，导致表示要么冗余，要么缺乏任务关键信息。StaMo旨在解决这一问题，提供高效、可解释且易于集成的状态表示。

Method: 提出了一种无监督方法，使用轻量级编码器和预训练的Diffusion Transformer（DiT）解码器学习高度压缩的两令牌状态表示。通过潜在插值生成的自然差异作为潜在动作，进一步解码为可执行机器人动作。

Result: StaMo在LIBERO上性能提升14.3%，真实世界任务成功率提高30%，潜在动作的协同训练性能优于现有方法10.4%，并展示了跨数据源的有效扩展性。

Conclusion: StaMo方法通过学习从静态图像中提取的紧凑状态表示，成功实现了无需显式监督即可捕捉结构化动态，并生成可执行的机器人动作。这一方法在性能、可解释性和跨数据源扩展性方面均优于现有方法。

Abstract: A fundamental challenge in embodied intelligence is developing expressive and
compact state representations for efficient world modeling and decision making.
However, existing methods often fail to achieve this balance, yielding
representations that are either overly redundant or lacking in task-critical
information. We propose an unsupervised approach that learns a highly
compressed two-token state representation using a lightweight encoder and a
pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong
generative prior. Our representation is efficient, interpretable, and
integrates seamlessly into existing VLA-based models, improving performance by
14.3% on LIBERO and 30% in real-world task success with minimal inference
overhead. More importantly, we find that the difference between these tokens,
obtained via latent interpolation, naturally serves as a highly effective
latent action, which can be further decoded into executable robot actions. This
emergent capability reveals that our representation captures structured
dynamics without explicit supervision. We name our method StaMo for its ability
to learn generalizable robotic Motion from compact State representation, which
is encoded from static images, challenging the prevalent dependence to learning
latent action on complex architectures and video data. The resulting latent
actions also enhance policy co-training, outperforming prior methods by 10.4%
with improved interpretability. Moreover, our approach scales effectively
across diverse data sources, including real-world robot data, simulation, and
human egocentric video.

</details>


### [203] [Automaton Constrained Q-Learning](https://arxiv.org/abs/2510.05061)
*Anastasios Manganaris,Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: ACQL通过结合目标条件值学习和自动机引导的强化学习，解决了现实机器人任务中的复杂时间规范和安全约束问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的机器人任务通常需要代理在尊重时变安全约束的同时实现一系列目标，但标准的强化学习范式在这些场景中存在根本性限制。

Method: ACQL结合了目标条件值学习和自动机引导的强化学习，支持大多数LTL任务规范，并利用其自动机表示明确编码阶段性目标进展以及静态和非静态安全约束。

Result: ACQL在包括先验方法无法满足目标达成或安全约束的案例在内的各种连续控制任务中表现优于现有方法。

Conclusion: ACQL是一种鲁棒且可扩展的解决方案，能够根据丰富的时间规范学习机器人行为。

Abstract: Real-world robotic tasks often require agents to achieve sequences of goals
while respecting time-varying safety constraints. However, standard
Reinforcement Learning (RL) paradigms are fundamentally limited in these
settings. A natural approach to these problems is to combine RL with
Linear-time Temporal Logic (LTL), a formal language for specifying complex,
temporally extended tasks and safety constraints. Yet, existing RL methods for
LTL objectives exhibit poor empirical performance in complex and continuous
environments. As a result, no scalable methods support both temporally ordered
goals and safety simultaneously, making them ill-suited for realistic robotics
scenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm
that addresses this gap by combining goal-conditioned value learning with
automaton-guided reinforcement. ACQL supports most LTL task specifications and
leverages their automaton representation to explicitly encode stage-wise goal
progression and both stationary and non-stationary safety constraints. We show
that ACQL outperforms existing methods across a range of continuous control
tasks, including cases where prior methods fail to satisfy either goal-reaching
or safety constraints. We further validate its real-world applicability by
deploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a
cluttered, cabinet-like space with safety constraints. Our results demonstrate
that ACQL is a robust and scalable solution for learning robotic behaviors
according to rich temporal specifications.

</details>


### [204] [ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning](https://arxiv.org/abs/2510.05070)
*Siheng Zhao,Yanjie Ze,Yue Wang,C. Karen Liu,Pieter Abbeel,Guanya Shi,Rocky Duan*

Main category: cs.RO

TL;DR: ResMimic是一个两阶段残差学习框架，通过结合通用运动跟踪和高效残差策略，提升了人形机器人的精确控制和物体交互能力，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管通用运动跟踪（GMT）技术已能复现多样人类动作，但其缺乏精确性和物体感知能力，难以满足人形机器人全身运动与物体交互的需求。因此，研究团队提出ResMimic框架以解决这一问题。

Method: ResMimic采用两阶段残差学习框架：首先训练一个通用运动跟踪（GMT）策略生成类人全身动作，然后学习一个高效的残差策略来优化GMT输出，改进运动并融入物体交互。此外，设计了点云物体跟踪奖励、接触奖励和基于课程的虚拟物体控制器以提升训练效率。

Result: 实验结果表明，ResMimic在仿真和实际Unitree G1人形机器人上均显著优于基线方法，任务成功率、训练效率和鲁棒性均有大幅提升。

Conclusion: ResMimic框架通过两阶段残差学习方法显著提升了人形机器人的精确控制和物体交互能力，在仿真和实际机器人中均表现出优越的任务成功率、训练效率和鲁棒性。

Abstract: Humanoid whole-body loco-manipulation promises transformative capabilities
for daily service and warehouse tasks. While recent advances in general motion
tracking (GMT) have enabled humanoids to reproduce diverse human motions, these
policies lack the precision and object awareness required for
loco-manipulation. To this end, we introduce ResMimic, a two-stage residual
learning framework for precise and expressive humanoid control from human
motion data. First, a GMT policy, trained on large-scale human-only motion,
serves as a task-agnostic base for generating human-like whole-body movements.
An efficient but precise residual policy is then learned to refine the GMT
outputs to improve locomotion and incorporate object interaction. To further
facilitate efficient training, we design (i) a point-cloud-based object
tracking reward for smoother optimization, (ii) a contact reward that
encourages accurate humanoid body-object interactions, and (iii) a
curriculum-based virtual object controller to stabilize early training. We
evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results
show substantial gains in task success, training efficiency, and robustness
over strong baselines. Videos are available at https://resmimic.github.io/ .

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [205] [A Subquadratic Two-Party Communication Protocol for Minimum Cost Flow](https://arxiv.org/abs/2510.03427)
*Hossein Gholizadeh,Yonggang Jiang*

Main category: cs.DS

TL;DR: 本文改进了两方通信模型中最大流问题的通信复杂度至$	ilde{O}(n^{1.5})$比特，并推广至线性规划和最小成本流问题。


<details>
  <summary>Details</summary>
Motivation: 研究两方通信模型下最大流问题的通信效率，旨在减少计算最大流所需的通信量，同时推广至更一般的线性规划和最小成本流问题。

Method: 通过调整基于内点法（IPM）的算法，并将其适应于两方通信模型，结合分布式凸优化技术，实现了通信复杂度的优化。

Result: 实现了$	ilde{O}(n^{1.5})$比特的通信复杂度，优于先前的$	ilde{O}(n^2)$结果，并推广至稀疏矩阵和双边约束的线性规划问题。

Conclusion: 本文通过改进通信复杂度至$	ilde{O}(n^{1.5})$比特，解决了两方通信模型中的最大流问题，并推广至线性规划与最小成本流问题。

Abstract: In this paper, we discuss the maximum flow problem in the two-party
communication model, where two parties, each holding a subset of edges on a
common vertex set, aim to compute the maximum flow of the union graph with
minimal communication. We show that this can be solved with
$\tilde{O}(n^{1.5})$ bits of communication, improving upon the trivial
$\tilde{O}(n^2)$ bound.
  To achieve this, we derive two additional, more general results:
  1. We present a randomized algorithm for linear programs with two-sided
constraints that requires $\tilde{O}(n^{1.5}k)$ bits of communication when each
constraint has at most $k$ non-zeros. This result improves upon the prior work
by [Ghadiri, Lee, Padmanabhan, Swartworth, Woodruff, Ye, STOC'24], which
achieves a complexity of $\tilde{O}(n^2)$ bits for LPs with one-sided
constraints. Upon more precise analysis, their algorithm can reach a bit
complexity of $\tilde{O}(n^{1.5} + nk)$ for one-sided constraint LPs.
Nevertheless, for sparse matrices, our approach matches this complexity while
extending the scope to two-sided constraints.
  2. Leveraging this result, we demonstrate that the minimum cost flow problem,
as a special case of solving linear programs with two-sided constraints and as
a general case of maximum flow problem, can also be solved with a communication
complexity of $\tilde{O}(n^{1.5})$ bits.
  These results are achieved by adapting an interior-point method (IPM)-based
algorithm for solving LPs with two-sided constraints in the sequential setting
by [van den Brand, Lee, Liu, Saranurak, Sidford, Song, Wang, STOC'21] to the
two-party communication model. This adaptation utilizes techniques developed by
[Ghadiri, Lee, Padmanabhan, Swartworth, Woodruff, Ye, STOC'24] for distributed
convex optimization.

</details>


### [206] [A Dynamic Programming Approach to Evader Pathfinding in Static Pursuit Scenarios](https://arxiv.org/abs/2510.04050)
*Sukanya Samanta,Manohar Reddy*

Main category: cs.DS

TL;DR: DPERO算法通过图建模和对数转换，高效计算逃逸者在静态防御下的最优路径，实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有逃逸拦截游戏（EIG）模型计算复杂度高，难以实时应用，因此研究在静态防御部署下逃逸者的最优路径规划问题。

Method: 提出DPERO算法，将环境建模为带有概率风险的图，通过对数转换将生存目标转化为加性成本函数，并利用值迭代求解最短路径问题。

Result: 在模拟网格网络上的实验表明，DPERO算法比基线最短路径方法能识别出生存概率显著更高的路径。

Conclusion: DPERO算法通过将生存概率转化为对数形式的加性成本函数，有效解决了在静态防御部署下逃逸者的最优路径规划问题，为漏洞分析和战略规划提供了实用工具。

Abstract: The interdiction of escaping adversaries in urban networks is a critical
security challenge. State-of-the-art game-theoretic models, such as the Escape
Interdiction Game (EIG), provide comprehensive frameworks but assume a highly
dynamic interaction and entail significant computational complexity, which can
be prohibitive for real-time applications. This paper investigates a crucial
sub-problem: an evader's optimal pathfinding calculus when faced with a static
or pre-determined defender deployment. We propose the Dynamic Programming for
Evader Route Optimization (DPERO) algorithm, which models the environment as a
graph with probabilistic risks at various nodes. By transforming the
multiplicative survival objective into an additive cost function using
logarithms, we frame the task as a shortest path problem solvable with value
iteration. This approach allows for the efficient computation of a path that
optimally balances safety and distance. Experimental results on simulated grid
networks demonstrate that DPERO identifies routes with significantly higher
survival probabilities compared to naive shortest-path baselines, validating
its efficacy as a practical tool for vulnerability analysis and strategic
planning.

</details>


### [207] [Streaming Max-Cut in General Metrics](https://arxiv.org/abs/2510.04435)
*Shaofeng H. -C. Jiang,Pan Peng,Haoze Wang*

Main category: cs.DS

TL;DR: 本文首次在滑动窗口流中实现了Max-Cut的多对数空间$(1 + \epsilon)$-近似算法，并证明了动态流中的多项式空间下界。


<details>
  <summary>Details</summary>
Motivation: 研究Max-Cut在一般度量空间中的流式复杂性，特别是在滑动窗口流和动态流中的近似算法及其空间复杂度。

Method: 基于Braverman和Ostrovsky的平滑直方图框架，建立了度量Max-Cut的第一个平滑性边界，并开发了一种新的度量水库采样技术。

Result: 提出了一个在滑动窗口流中的$(1 + \epsilon)$-近似算法，使用多对数空间，并证明了动态流中的多项式空间下界。

Conclusion: 本文提出了一个在滑动窗口流中使用多对数空间估计Max-Cut值的$(1 + \epsilon)$-近似算法，并证明了动态流设置中的多项式空间下界，从而与欧几里得情况形成对比。

Abstract: Max-Cut is a fundamental combinatorial optimization problem that has been
studied in various computational settings. In this work, we initiate the study
of its streaming complexity in general metric spaces with access to distance
oracles. We give a $(1 + \epsilon)$-approximation algorithm for estimating the
Max-Cut value sliding-window streams using only poly-logarithmic space. This is
the first sliding-window algorithm for Max-Cut even in Euclidean spaces, and it
achieves a similar error-space tradeoff as the state-of-the-art insertion-only
algorithms in Euclidean settings [Chen, Jiang, Krauthgamer, STOC'23], but
without relying on Euclidean structures. In sharp contrast, we prove a
polynomial-space lower bound for any $\mathrm{poly}(n)$-approximation in the
dynamic streaming setting. This yields a separation from the Euclidean case,
where the polylogarithmic-space $(1+\epsilon)$-approximation extends to dynamic
streams.
  On the technical side, our sliding-window algorithm builds on the smooth
histogram framework of [Braverman and Ostrovsky, SICOMP'10]. To make this
framework applicable, we establish the first smoothness bound for metric
Max-Cut. Moreover, we develop a streaming algorithm for metric Max-Cut in
insertion-only streams, whose key ingredient is a new metric reservoir sampling
technique.

</details>


### [208] [Online Multiple Resource Allocation Problems with Departures via the Primal-Dual Approach](https://arxiv.org/abs/2510.04737)
*Yusuf Amidu,Khaled Elbassioni,Adriana F. Gabor*

Main category: cs.DS

TL;DR: 本文提出原始-对偶算法处理在线资源分配问题，实现了高竞争比并支持额外约束。


<details>
  <summary>Details</summary>
Motivation: 解决在线资源分配问题中请求的动态到达和资源分配的复杂性，目标是最大化随时间获得的奖励。

Method: 使用原始-对偶算法处理不同变体的在线资源分配问题，包括基本变体、带负载平衡约束的变体和多维变体。

Result: 算法在温和假设下实现了竞争比为$O\big(\log(\bar\theta^{\max}\cdot\bar d^{\max})\big)$，并适用于多种变体。

Conclusion: 本文提出的原始-对偶算法为在线资源分配问题提供了一个简单、统一的框架，能够在不牺牲竞争比的情况下处理额外约束（如负载平衡约束）。

Abstract: In this paper we propose primal-dual algorithms for different variants of the
online resource allocation problem with departures. In the basic variant,
requests (items) arrive over time to a set of resources (knapsacks) and upon
arrival, the duration of time a request may occupy a resource, the demand and
reward if the request can be granted, become known. %We assume that the
duration of stay of a request may depend on the resource. %and that resources
may have different capacity sizes. The goal of the algorithm is to decide
whether to accept/reject a request upon arrival and to which resource to
allocate it such that the reward obtained over time is maximized. Under some
mild assumptions, we show that the proposed primal-dual algorithm achieves a
competitive ratio of $O\big(\log(\bar\theta^{\max}\cdot\bar d^{\max})\big)$,
where $\bar \theta^{\max}$ is the maximum value density fluctuation ratio and
$\bar d^{\max}$ is the maximum duration fluctuation ratio. We prove similar
results for two other variants, namely, one with an additional load balancing
constraint, and the multi-dimensional variant where an admitted request
consumes capacity on multiple resources. Our results show that the primal-dual
approach offers a simple, unified framework for obtaining competitive ratios
comparable to those previously obtained via threshold policies known for these
problems. Additionally, we show that this framework allows us to incorporate
additional constraints, such as load-balancing constraints, without sacrificing
the competitive ratio.

</details>


### [209] [A Polynomial Space Lower Bound for Diameter Estimation in Dynamic Streams](https://arxiv.org/abs/2510.04918)
*Sanjeev Khanna,Ashwin Padaki,Krish Singal,Erik Waingarten*

Main category: cs.DS

TL;DR: 动态流模型中，常数因子近似直径需要多项式空间，并提出了近乎匹配的上界。


<details>
  <summary>Details</summary>
Motivation: 研究在动态流模型下估计点集直径的空间复杂度，以理解其计算限制。

Method: 通过将动态流算法与尺度不变函数的线性草图联系起来，并利用图的minrank概念，建立了空间复杂度的下界。

Result: 证明了在动态流模型中，c-近似直径需要n^Ω(1/c)空间，并给出了一个使用n^O(1/c)空间的c-近似算法。

Conclusion: 论文证明了在动态流模型中，任何常数因子近似直径的算法都需要多项式空间，并提出了一个近乎匹配的上界。

Abstract: We study the space complexity of estimating the diameter of a subset of
points in an arbitrary metric space in the dynamic (turnstile) streaming model.
The input is given as a stream of updates to a frequency vector $x \in
\mathbb{Z}_{\geq 0}^n$, where the support of $x$ defines a multiset of points
in a fixed metric space $M = ([n], \mathsf{d})$. The goal is to estimate the
diameter of this multiset, defined as $\max\{\mathsf{d}(i,j) : x_i, x_j > 0\}$,
to a specified approximation factor while using as little space as possible.
  In insertion-only streams, a simple $O(\log n)$-space algorithm achieves a
2-approximation. In sharp contrast to this, we show that in the dynamic
streaming model, any algorithm achieving a constant-factor approximation to
diameter requires polynomial space. Specifically, we prove that a
$c$-approximation to the diameter requires $n^{\Omega(1/c)}$ space. Our lower
bound relies on two conceptual contributions: (1) a new connection between
dynamic streaming algorithms and linear sketches for {\em scale-invariant}
functions, a class that includes diameter estimation, and (2) a connection
between linear sketches for diameter and the {\em minrank} of graphs, a notion
previously studied in index coding. We complement our lower bound with a nearly
matching upper bound, which gives a $c$-approximation to the diameter in
general metrics using $n^{O(1/c)}$ space.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [210] [An Early Exploration of Deep-Learning-Driven Prefetching for Far Memory](https://arxiv.org/abs/2510.04360)
*Yutong Huang,Zhiyuan Guo,Yiying Zhang*

Main category: cs.OS

TL;DR: Memix是一种远内存系统，通过深度学习协同设计优化预取，减少按需访问，性能提升42%。


<details>
  <summary>Details</summary>
Motivation: 远内存系统中，按需从远内存获取数据成为应用瓶颈，影响性能和能效。

Method: Memix采用深度学习系统协同设计，结合应用语义和运行时上下文，独立优化内存访问。

Result: 在数据密集型工作负载上的初步评估显示，Memix比现有最优远内存系统性能提升高达42%。

Conclusion: Memix通过深度学习和系统协同设计，显著减少了远内存的按需访问，提升了性能，比现有最优远内存系统性能高出42%。

Abstract: Far-memory systems, where applications store less-active data in more
energy-efficient memory media, are increasingly adopted by data centers.
However, applications are bottlenecked by on-demand data fetching from far- to
local-memory. We present Memix, a far-memory system that embodies a
deep-learning-system co-design for efficient and accurate prefetching,
minimizing on-demand far-memory accesses. One key observation is that memory
accesses are shaped by both application semantics and runtime context,
providing an opportunity to optimize each independently. Preliminary evaluation
of Memix on data-intensive workloads shows that it outperforms the
state-of-the-art far-memory system by up to 42%.

</details>


### [211] [A Case for Declarative LLM-friendly Interfaces for Improved Efficiency of Computer-Use Agents](https://arxiv.org/abs/2510.04607)
*Yuan Wang,Mingyu Li,Haibo Chen*

Main category: cs.OS

TL;DR: GOI通过策略与机制分离，显著提升了LLM在GUI自动化任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI自动化方法因需要分解为细粒度动作序列而效率低下，GOI旨在解决这一问题。

Method: GOI将现有GUI转换为三个声明性原语：access、state和observation，实现了策略与机制的分离。

Result: GOI在Microsoft Office Suite上的测试中，任务成功率提高了67%，交互步骤减少了43.5%。

Conclusion: GOI显著提高了任务成功率并减少了交互步骤，证明了其在GUI自动化任务中的有效性。

Abstract: Computer-use agents (CUAs) powered by large language models (LLMs) have
emerged as a promising approach to automating computer tasks, yet they struggle
with graphical user interfaces (GUIs). GUIs, designed for humans, force LLMs to
decompose high-level goals into lengthy, error-prone sequences of fine-grained
actions, resulting in low success rates and an excessive number of LLM calls.
  We propose Goal-Oriented Interface (GOI), a novel abstraction that transforms
existing GUIs into three declarative primitives: access, state, and
observation, which are better suited for LLMs. Our key idea is policy-mechanism
separation: LLMs focus on high-level semantic planning (policy) while GOI
handles low-level navigation and interaction (mechanism). GOI does not require
modifying the application source code or relying on application programming
interfaces (APIs).
  We evaluate GOI with Microsoft Office Suite (Word, PowerPoint, Excel) on
Windows. Compared to a leading GUI-based agent baseline, GOI improves task
success rates by 67% and reduces interaction steps by 43.5%. Notably, GOI
completes over 61% of successful tasks with a single LLM call.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [212] [Creative synthesis of kinematic mechanisms](https://arxiv.org/abs/2510.03308)
*Jiong Lin,Jialong Ning,Judah Goldfeder,Hod Lipson*

Main category: cs.GR

TL;DR: 通过图像生成框架探索平面连杆机构的动力学合成，使用VAE模型在统一框架中表示和合成多种机制，验证了方法在复杂度递增数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索图像生成模型在平面连杆机构动力学合成中的潜力，以统一的图像生成框架表示和合成包括复杂多环机构在内的多种机制。

Method: 使用共享潜在变分自编码器（VAE）探索图像生成模型在合成未见运动曲线和模拟新运动学中的潜力。通过将轨迹点的绘制速度编码为颜色梯度，支持基于轨迹形状和速度分布的动力学合成。

Result: 在三个复杂度递增的数据集上验证了方法的有效性：标准四连杆机构集、混合四连杆和曲柄滑块机构集，以及包含多环机构的复杂集。初步结果表明，基于图像的表示在生成机械设计中具有潜力。

Conclusion: 图像生成框架在机械设计中的有效性得到验证，展示了包括旋转和滑动关节在内的机制可以在统一的图像生成框架中表示和合成。

Abstract: In this paper, we formulate the problem of kinematic synthesis for planar
linkages as a cross-domain image generation task. We develop a planar linkages
dataset using RGB image representations, covering a range of mechanisms: from
simple types such as crank-rocker and crank-slider to more complex eight-bar
linkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE)
is employed to explore the potential of image generative models for
synthesizing unseen motion curves and simulating novel kinematics. By encoding
the drawing speed of trajectory points as color gradients, the same
architecture also supports kinematic synthesis conditioned on both trajectory
shape and velocity profiles. We validate our method on three datasets of
increasing complexity: a standard four-bar linkage set, a mixed set of four-bar
and crank-slider mechanisms, and a complex set including multi-loop mechanisms.
Preliminary results demonstrate the effectiveness of image-based
representations for generative mechanical design, showing that mechanisms with
revolute and prismatic joints, and potentially cams and gears, can be
represented and synthesized within a unified image generation framework.

</details>


### [213] [Universal Beta Splatting](https://arxiv.org/abs/2510.03312)
*Rong Liu,Zhongpai Gao,Benjamin Planche,Meida Chen,Van Nguyen Nguyen,Meng Zheng,Anwesa Choudhuri,Terrence Chen,Yue Wang,Andrew Feng,Ziyan Wu*

Main category: cs.GR

TL;DR: UBS通过Beta核统一框架改进了3D高斯泼溅，实现了更灵活的辐射场渲染，并在性能上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯泼溅方法在处理各向异性视图依赖外观和场景动态时存在局限性。UBS旨在通过Beta核的统一框架解决这些问题，提供更灵活和强大的渲染能力。

Method: UBS框架利用各向异性Beta核进行显式辐射场渲染，无需辅助网络或特定颜色编码。Beta核的参数自然分解场景属性为可解释的组成部分，如空间（表面与纹理）、角度（漫反射与镜面反射）和时间（静态与动态）。

Result: UBS在静态、视图依赖和动态基准测试中均优于现有方法，并通过CUDA加速实现了实时渲染。Beta核被证明是一种可扩展的通用基元，适用于辐射场渲染。

Conclusion: Universal Beta Splatting（UBS）作为一种统一的框架，通过推广3D高斯泼溅到N维各向异性Beta核，实现了显式辐射场渲染。Beta核的可控依赖性建模能力使其在空间、角度和时间维度上均表现出色，同时保持了与高斯泼溅的向后兼容性。

Abstract: We introduce Universal Beta Splatting (UBS), a unified framework that
generalizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for
explicit radiance field rendering. Unlike fixed Gaussian primitives, Beta
kernels enable controllable dependency modeling across spatial, angular, and
temporal dimensions within a single representation. Our unified approach
captures complex light transport effects, handles anisotropic view-dependent
appearance, and models scene dynamics without requiring auxiliary networks or
specific color encodings. UBS maintains backward compatibility by approximating
to Gaussian Splatting as a special case, guaranteeing plug-in usability and
lower performance bounds. The learned Beta parameters naturally decompose scene
properties into interpretable without explicit supervision: spatial (surface
vs. texture), angular (diffuse vs. specular), and temporal (static vs.
dynamic). Our CUDA-accelerated implementation achieves real-time rendering
while consistently outperforming existing methods across static,
view-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable
universal primitive for radiance field rendering. Our project website is
available at https://rongliu-leo.github.io/universal-beta-splatting/.

</details>


### [214] [Style Brush: Guided Style Transfer for 3D Objects](https://arxiv.org/abs/2510.03433)
*Áron Samuel Kovács,Pedro Hermosilla,Renata G. Raidou*

Main category: cs.GR

TL;DR: Style Brush是一种新颖的3D风格迁移方法，通过创新的损失函数和引导纹理，提供精细控制并简化用户交互，适用于广泛的艺术家群体。


<details>
  <summary>Details</summary>
Motivation: 旨在为艺术家提供对风格化过程的精细控制，同时简化用户交互，使更广泛的受众能够使用该技术。

Method: 该方法扩展了传统的3D风格迁移技术，引入了能够捕捉风格方向性的新颖损失函数，支持多种风格图像或其部分，并实现合成纹理中风格之间的平滑过渡。

Result: 广泛的评估表明，该方法在多种网格、风格图像和轮廓形状上具有灵活性，生成的纹理具有视觉吸引力。

Conclusion: Style Brush通过引入新颖的损失函数和易于生成的引导纹理，为艺术家提供了精细的风格化控制，展示了其在多种网格和风格图像上的灵活性和视觉吸引力。

Abstract: We introduce Style Brush, a novel style transfer method for textured meshes
designed to empower artists with fine-grained control over the stylization
process. Our approach extends traditional 3D style transfer methods by
introducing a novel loss function that captures style directionality, supports
multiple style images or portions thereof, and enables smooth transitions
between styles in the synthesized texture. The use of easily generated guiding
textures streamlines user interaction, making our approach accessible to a
broad audience. Extensive evaluations with various meshes, style images, and
contour shapes demonstrate the flexibility of our method and showcase the
visual appeal of the generated textures.

</details>


### [215] [Paris: A Decentralized Trained Open-Weight Diffusion Model](https://arxiv.org/abs/2510.03434)
*Zhiying Jiang,Raihan Seraj,Marcos Villagra,Bidhan Roy*

Main category: cs.GR

TL;DR: Paris是首个完全通过去中心化计算预训练的扩散模型，无需中心协调基础设施，显著减少资源需求，同时保持高质量生成。


<details>
  <summary>Details</summary>
Motivation: 探索去中心化计算在高质量文本到图像生成中的可行性，消除对专用GPU集群和同步梯度更新的依赖。

Method: Paris由8个专家扩散模型组成，每个模型在完全隔离的环境下训练，无需梯度、参数或中间激活同步。通过轻量级Transformer路由器动态选择专家模型，实现了与中心协调基线相当的生成质量。

Result: Paris在保持生成质量的同时，减少了14倍训练数据和16倍计算资源的使用，且可在异构硬件上训练。

Conclusion: Paris证明了高质量文本到图像生成可以在没有中心协调基础设施的情况下实现，通过分布式扩散训练框架，显著减少了训练数据和计算资源的需求。

Abstract: We present Paris, the first publicly released diffusion model pre-trained
entirely through decentralized computation. Paris demonstrates that
high-quality text-to-image generation can be achieved without centrally
coordinated infrastructure. Paris is open for research and commercial use.
Paris required implementing our Distributed Diffusion Training framework from
scratch. The model consists of 8 expert diffusion models (129M-605M parameters
each) trained in complete isolation with no gradient, parameter, or
intermediate activation synchronization. Rather than requiring synchronized
gradient updates across thousands of GPUs, we partition data into semantically
coherent clusters where each expert independently optimizes its subset while
collectively approximating the full distribution. A lightweight transformer
router dynamically selects appropriate experts at inference, achieving
generation quality comparable to centrally coordinated baselines. Eliminating
synchronization enables training on heterogeneous hardware without specialized
interconnects. Empirical validation confirms that Paris's decentralized
training maintains generation quality while removing the dedicated GPU cluster
requirement for large-scale diffusion models. Paris achieves this using
14$\times$ less training data and 16$\times$ less compute than the prior
decentralized baseline.

</details>


### [216] [Neon: Negative Extrapolation From Self-Training Improves Image Generation](https://arxiv.org/abs/2510.03597)
*Sina Alemohammad,Zhangyang Wang,Richard G. Baraniuk*

Main category: cs.GR

TL;DR: Neon是一种通过负向外推解决自训练模型崩溃的新方法，简单高效，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据稀缺限制了生成AI模型的扩展，而自训练导致的模型崩溃（MAD）进一步加剧了这一问题。

Method: Neon首先在自合成的数据上微调基础模型，然后反向梯度更新以远离退化的权重。

Result: Neon在多种架构和数据集上表现优异，如将xAR-L模型在ImageNet 256x256上的FID提升至1.02，仅需0.36%的额外训练计算。

Conclusion: Neon通过负向外推技术有效地解决了自训练导致的模型崩溃问题，显著提升了生成模型的性能，且实现简单、计算成本低。

Abstract: Scaling generative AI models is bottlenecked by the scarcity of high-quality
training data. The ease of synthesizing from a generative model suggests using
(unverified) synthetic data to augment a limited corpus of real data for the
purpose of fine-tuning in the hope of improving performance. Unfortunately,
however, the resulting positive feedback loop leads to model autophagy disorder
(MAD, aka model collapse) that results in a rapid degradation in sample quality
and/or diversity. In this paper, we introduce Neon (for Negative Extrapolation
frOm self-traiNing), a new learning method that turns the degradation from
self-training into a powerful signal for self-improvement. Given a base model,
Neon first fine-tunes it on its own self-synthesized data but then,
counterintuitively, reverses its gradient updates to extrapolate away from the
degraded weights. We prove that Neon works because typical inference samplers
that favor high-probability regions create a predictable anti-alignment between
the synthetic and real data population gradients, which negative extrapolation
corrects to better align the model with the true data distribution. Neon is
remarkably easy to implement via a simple post-hoc merge that requires no new
real data, works effectively with as few as 1k synthetic samples, and typically
uses less than 1% additional training compute. We demonstrate Neon's
universality across a range of architectures (diffusion, flow matching,
autoregressive, and inductive moment matching models) and datasets (ImageNet,
CIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the
xAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional
training compute. Code is available at https://github.com/SinaAlemohammad/Neon

</details>


### [217] [Diverse Text-to-Image Generation via Contrastive Noise Optimization](https://arxiv.org/abs/2510.03813)
*Byungjun Kim,Soobin Um,Jong Chul Ye*

Main category: cs.GR

TL;DR: A new method, Contrastive Noise Optimization, enhances diversity in text-to-image generation by optimizing initial noise with a contrastive loss, achieving better results without sensitivity to hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image diffusion models often suffer from limited diversity in outputs due to strong text guidance, with current methods offering only modest improvements or being sensitive to hyperparameter tuning.

Method: The approach involves shaping the initial noise using a contrastive loss defined in the Tweedie data space, optimizing a batch of noise latents to repel instances within the batch for diversity while anchoring them to a reference sample for fidelity.

Result: The method achieves a superior quality-diversity Pareto frontier and remains robust to hyperparameter choices, as validated by extensive experiments.

Conclusion: Contrastive Noise Optimization provides a robust and effective method to enhance the diversity of text-to-image generation without compromising fidelity, demonstrating superior performance across various T2I backbones.

Abstract: Text-to-image (T2I) diffusion models have demonstrated impressive performance
in generating high-fidelity images, largely enabled by text-guided inference.
However, this advantage often comes with a critical drawback: limited
diversity, as outputs tend to collapse into similar modes under strong text
guidance. Existing approaches typically optimize intermediate latents or text
conditions during inference, but these methods deliver only modest gains or
remain sensitive to hyperparameter tuning. In this work, we introduce
Contrastive Noise Optimization, a simple yet effective method that addresses
the diversity issue from a distinct perspective. Unlike prior techniques that
adapt intermediate latents, our approach shapes the initial noise to promote
diverse outputs. Specifically, we develop a contrastive loss defined in the
Tweedie data space and optimize a batch of noise latents. Our contrastive
optimization repels instances within the batch to maximize diversity while
keeping them anchored to a reference sample to preserve fidelity. We further
provide theoretical insights into the mechanism of this preprocessing to
substantiate its effectiveness. Extensive experiments across multiple T2I
backbones demonstrate that our approach achieves a superior quality-diversity
Pareto frontier while remaining robust to hyperparameter choices.

</details>


### [218] [Joint Neural SDF Reconstruction and Semantic Segmentation for CAD Models](https://arxiv.org/abs/2510.03837)
*Shen Fan,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出了一种结合神经SDF重建和部件分割的灵活方法，能够处理任意部件数量的CAD网格，并在重建和分割任务中表现优异，尽管边界精度有待提升。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于固定的分类体系，无法灵活处理具有不同部件数量的CAD网格。本研究旨在解决这一问题，提供一种更加灵活和通用的解决方案。

Method: 通过结合基于神经SDF的隐式重建网络和PartField生成的监督训练的部件分割头，提出了一种数据高效的处理流程。该方法能够处理任意数量部件的网格，并在单次处理中生成几何对齐的标签。

Result: 在ABC数据集上的评估显示，该方法在重建（CDL1/CDL2, F1-micro, NC）和分割（mIoU, Accuracy）方面表现优异，并提出了新的分割一致性指标。即使在薄或复杂几何的重建质量下降时，分割仍保持准确和标签一致性。

Conclusion: 该方法为CAD模型提供了一种无需固定分类或精确调色板匹配的语义结构生成途径，尽管在边界精度上存在局限，但为未来的边界感知训练和高分辨率标签提供了方向。

Abstract: We propose a simple, data-efficient pipeline that augments an implicit
reconstruction network based on neural SDF-based CAD parts with a
part-segmentation head trained under PartField-generated supervision. Unlike
methods tied to fixed taxonomies, our model accepts meshes with any number of
parts and produces coherent, geometry-aligned labels in a single pass. We
evaluate on randomly sampled CAD meshes from the ABC dataset with intentionally
varied part cardinalities, including over-segmented shapes, and report strong
performance across reconstruction (CDL1/CDL2, F1-micro, NC) and segmentation
(mIoU, Accuracy), together with a new Segmentation Consistency metric that
captures local label smoothness. We attach a lightweight segmentation head to
the Flat-CAD SDF trunk; on a paired evaluation it does not alter reconstruction
while providing accurate part labels for meshes with any number of parts. Even
under degraded reconstructions on thin or intricate geometries, segmentation
remains accurate and label-coherent, often preserving the correct part count.
Our approach therefore offers a practical route to semantically structured CAD
meshes without requiring curated taxonomies or exact palette matches. We
discuss limitations in boundary precision, partly due to per-face supervision,
and outline paths toward boundary-aware training and higher resolution labels.

</details>


### [219] [Enhancing Foveated Rendering with Weighted Reservoir Sampling](https://arxiv.org/abs/2510.03964)
*Ville Cantory,Darya Biparva,Haoyu Tan,Tongyu Nie,John Schroeder,Ruofei Du,Victoria Interrante,Piotr Didyk*

Main category: cs.GR

TL;DR: 通过加权储层采样技术，利用先前帧的高质量像素样本减少每帧中央凹区域的渲染需求，适用于实时VR/AR系统。


<details>
  <summary>Details</summary>
Motivation: 利用人眼对高频信息的时空敏感性随偏心距增加而下降的特性，以及眼动（包括微眼跳）的分布特性，以减少渲染成本。

Method: 提出了一种加权储层采样技术，用于从先前帧中高效维护和整合感知相关的高质量像素样本。

Result: 该方法在4K分辨率下运行时间不到1毫秒，能够与实时VR和AR系统集成，显著提高渲染效率。

Conclusion: 通过加权储层采样技术，该方法能够在保持高感知图像质量的同时，显著降低每帧需要渲染的中央凹区域大小，适用于实时VR和AR系统。

Abstract: Spatiotemporal sensitivity to high frequency information declines with
increased peripheral eccentricity. Foveated rendering exploits this by
decreasing the spatial resolution of rendered images in peripheral vision,
reducing the rendering cost by omitting high frequency details. As foveation
levels increase, the rendering quality is reduced, and traditional foveated
rendering systems tend not to preserve samples that were previously rendered at
high spatial resolution in previous frames. Additionally, prior research has
shown that saccade landing positions are distributed around a target location
rather than landing at a single point, and that even during fixations, eyes
perform small microsaccades around a fixation point. This creates an
opportunity for sampling from temporally neighbouring frames with differing
foveal locations to reduce the required rendered size of the foveal region
while achieving a higher perceived image quality. We further observe that the
temporal presentation of pixels frame-to-frame can be viewed as a data stream,
presenting a random sampling problem. Following this intuition, we propose a
Weighted Reservoir Sampling technique to efficiently maintain a reservoir of
the perceptually relevant high quality pixel samples from previous frames and
incorporate them into the computation of the current frame. This allows the
renderer to render a smaller region of foveal pixels per frame by temporally
reusing pixel samples that are still relevant to reconstruct a higher perceived
image quality, while allowing for higher levels of foveation. Our method
operates on the output of foveated rendering, and runs in under 1\,ms at 4K
resolution, making it highly efficient and integrable with real-time VR and AR
foveated rendering systems.

</details>


### [220] [3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG](https://arxiv.org/abs/2510.04536)
*Shun-ichiro Hayashi,Daichi Mukunoki,Tetsuya Hoshino,Satoshi Ohshima,Takahiro Katagiri*

Main category: cs.GR

TL;DR: 3Dify是一个利用LLMs生成3D-CG内容的框架，支持自然语言指令、用户反馈和本地LLM集成，降低技术门槛和成本。


<details>
  <summary>Details</summary>
Motivation: 旨在通过自然语言指令简化3D-CG内容的生成过程，降低技术门槛和成本。

Method: 3Dify基于Dify平台，结合MCP和RAG等LLM技术，自动化操作DCC工具或通过CUA方法自动化GUI操作，支持用户反馈以优化生成质量。

Result: 成功开发了一个能够通过自然语言指令生成高质量3D-CG内容的框架，支持用户反馈和本地LLM集成。

Conclusion: 3Dify框架通过结合LLMs和DCC工具，实现了仅通过自然语言指令生成3D-CG内容的能力，并支持用户反馈和本地LLM集成，显著降低了时间和经济成本。

Abstract: This paper proposes "3Dify," a procedural 3D computer graphics (3D-CG)
generation framework utilizing Large Language Models (LLMs). The framework
enables users to generate 3D-CG content solely through natural language
instructions. 3Dify is built upon Dify, an open-source platform for AI
application development, and incorporates several state-of-the-art LLM-related
technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented
Generation (RAG). For 3D-CG generation support, 3Dify automates the operation
of various Digital Content Creation (DCC) tools via MCP. When DCC tools do not
support MCP-based interaction, the framework employs the Computer-Using Agent
(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,
to enhance image generation quality, 3Dify allows users to provide feedback by
selecting preferred images from multiple candidates. The LLM then learns
variable patterns from these selections and applies them to subsequent
generations. Furthermore, 3Dify supports the integration of locally deployed
LLMs, enabling users to utilize custom-developed models and to reduce both time
and monetary costs associated with external API calls by leveraging their own
computational resources.

</details>


### [221] [C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing](https://arxiv.org/abs/2510.04539)
*Zeng Tao,Zheng Ding,Zeyuan Chen,Xiang Zhang,Leizhi Li,Zhuowen Tu*

Main category: cs.GR

TL;DR: C3Editor是一个可控且一致的2D-lifting-based 3D编辑框架，通过选择性建立视图一致的2D编辑模型，解决了现有方法在多视图一致性上的挑战，并在编辑效果上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的2D-lifting-based 3D编辑方法常因缺乏视图一致的2D编辑模型和难以确保多视图编辑一致性而面临挑战。

Method: 该方法首先选择GT视图及其编辑后的图像作为优化目标，然后在GT视图和多个视图中微调2D编辑模型，同时引入单独的LoRA模块以满足GT视图拟合和多视图一致性的不同需求。

Result: C3Editor在2D和3D编辑中实现了更一致和可控的结果，优于现有方法。

Conclusion: C3Editor框架通过选择性建立视图一致的2D编辑模型，实现了比现有2D-lifting-based方法更一致和可控的2D及3D编辑效果，并在定性和定量评估中表现优异。

Abstract: Existing 2D-lifting-based 3D editing methods often encounter challenges
related to inconsistency, stemming from the lack of view-consistent 2D editing
models and the difficulty of ensuring consistent editing across multiple views.
To address these issues, we propose C3Editor, a controllable and consistent
2D-lifting-based 3D editing framework. Given an original 3D representation and
a text-based editing prompt, our method selectively establishes a
view-consistent 2D editing model to achieve superior 3D editing results. The
process begins with the controlled selection of a ground truth (GT) view and
its corresponding edited image as the optimization target, allowing for
user-defined manual edits. Next, we fine-tune the 2D editing model within the
GT view and across multiple views to align with the GT-edited image while
ensuring multi-view consistency. To meet the distinct requirements of GT view
fitting and multi-view consistency, we introduce separate LoRA modules for
targeted fine-tuning. Our approach delivers more consistent and controllable 2D
and 3D editing results than existing 2D-lifting-based methods, outperforming
them in both qualitative and quantitative evaluations.

</details>


### [222] [Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents](https://arxiv.org/abs/2510.04637)
*Zeyi Zhang,Yanju Zhou,Heyuan Yao,Tenglong Ao,Xiaohang Zhan,Libin Liu*

Main category: cs.GR

TL;DR: Social Agent框架通过LLM和扩散模型生成协调的非言语行为，显著提升对话交互质量。


<details>
  <summary>Details</summary>
Motivation: 旨在解决对话中非言语行为合成的真实性和上下文适当性问题，以提升交互质量。

Method: 开发了一个由大型语言模型（LLM）驱动的代理系统，用于指导对话流程和确定互动行为；提出了一种基于自回归扩散模型的双人手势生成模型，从语音信号合成协调动作。

Result: 用户研究和定量评估表明，该模型显著提升了双人交互的质量，生成了自然且同步的非言语行为。

Conclusion: 该论文提出的Social Agent框架通过LLM驱动的代理系统和基于扩散模型的双人手势生成模型，显著提升了对话中非言语行为的自然度和同步性。

Abstract: We present Social Agent, a novel framework for synthesizing realistic and
contextually appropriate co-speech nonverbal behaviors in dyadic conversations.
In this framework, we develop an agentic system driven by a Large Language
Model (LLM) to direct the conversation flow and determine appropriate
interactive behaviors for both participants. Additionally, we propose a novel
dual-person gesture generation model based on an auto-regressive diffusion
model, which synthesizes coordinated motions from speech signals. The output of
the agentic system is translated into high-level guidance for the gesture
generator, resulting in realistic movement at both the behavioral and motion
levels. Furthermore, the agentic system periodically examines the movements of
interlocutors and infers their intentions, forming a continuous feedback loop
that enables dynamic and responsive interactions between the two participants.
User studies and quantitative evaluations show that our model significantly
improves the quality of dyadic interactions, producing natural, synchronized
nonverbal behaviors.

</details>


### [223] [Bridging Text and Video Generation: A Survey](https://arxiv.org/abs/2510.04999)
*Nilay Kumar,Priyansh Bhandari,G. Maragatham*

Main category: cs.GR

TL;DR: 本文综述了T2V生成模型的发展、挑战及未来方向，涵盖了从GANs到DiT架构的演变、训练配置、评估指标及性能分析。


<details>
  <summary>Details</summary>
Motivation: T2V技术在教育、营销、娱乐等领域具有潜力，但面临对齐、长期一致性和计算效率等挑战，需全面梳理其发展。

Method: 通过系统调查，从早期GANs和VAEs到混合Diffusion-Transformer（DiT）架构，详细分析了模型的工作原理、改进之处及新架构的必要性。

Result: 综述了T2V模型的训练配置、评估指标及性能，并指出当前评估方法的局限性及转向更全面、感知对齐策略的趋势。

Conclusion: 本文总结了文本到视频（T2V）生成技术的当前挑战，并提出了未来研究方向，为后续研究提供了视角。

Abstract: Text-to-video (T2V) generation technology holds potential to transform
multiple domains such as education, marketing, entertainment, and assistive
technologies for individuals with visual or reading comprehension challenges,
by creating coherent visual content from natural language prompts. From its
inception, the field has advanced from adversarial models to diffusion-based
models, yielding higher-fidelity, temporally consistent outputs. Yet challenges
persist, such as alignment, long-range coherence, and computational efficiency.
Addressing this evolving landscape, we present a comprehensive survey of
text-to-video generative models, tracing their development from early GANs and
VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these
models work, what limitations they addressed in their predecessors, and why
shifts toward new architectural paradigms were necessary to overcome challenges
in quality, coherence, and control. We provide a systematic account of the
datasets, which the surveyed text-to-video models were trained and evaluated
on, and, to support reproducibility and assess the accessibility of training
such models, we detail their training configurations, including their hardware
specifications, GPU counts, batch sizes, learning rates, optimizers, epochs,
and other key hyperparameters. Further, we outline the evaluation metrics
commonly used for evaluating such models and present their performance across
standard benchmarks, while also discussing the limitations of these metrics and
the emerging shift toward more holistic, perception-aligned evaluation
strategies. Finally, drawing from our analysis, we outline the current open
challenges and propose a few promising future directions, laying out a
perspective for future researchers to explore and build upon in advancing T2V
research and applications.

</details>


### [224] [SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder](https://arxiv.org/abs/2510.05081)
*Ronen Kamenetsky,Sara Dorfman,Daniel Garibi,Roni Paiss,Or Patashnik,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 提出一种通过文本嵌入标记级操作实现解耦和连续控制的图像编辑方法，利用稀疏自编码器识别语义隔离方向，适用于多种扩散模型。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型在图像编辑中占主导地位，但仅依赖文本提示无法提供足够的编辑控制，尤其是解耦和连续控制的需求。

Method: 采用稀疏自编码器（SAE）的稀疏潜在空间来识别语义隔离的维度，通过沿特定方向调整文本嵌入强度来控制编辑属性。

Result: 实验表明，该方法能够在多样属性和领域中实现直观高效的连续控制编辑。

Conclusion: 该方法通过文本嵌入的标记级操作，实现了对图像编辑的解耦和连续控制，且无需修改扩散过程，具有广泛的适用性。

Abstract: Large-scale text-to-image diffusion models have become the backbone of modern
image editing, yet text prompts alone do not offer adequate control over the
editing process. Two properties are especially desirable: disentanglement,
where changing one attribute does not unintentionally alter others, and
continuous control, where the strength of an edit can be smoothly adjusted. We
introduce a method for disentangled and continuous editing through token-level
manipulation of text embeddings. The edits are applied by manipulating the
embeddings along carefully chosen directions, which control the strength of the
target attribute. To identify such directions, we employ a Sparse Autoencoder
(SAE), whose sparse latent space exposes semantically isolated dimensions. Our
method operates directly on text embeddings without modifying the diffusion
process, making it model agnostic and broadly applicable to various image
synthesis backbones. Experiments show that it enables intuitive and efficient
manipulations with continuous control across diverse attributes and domains.

</details>


### [225] [Pulp Motion: Framing-aware multimodal camera and human motion generation](https://arxiv.org/abs/2510.05097)
*Robin Courant,Xi Wang,David Loiseaux,Marc Christie,Vicky Kalogeiton*

Main category: cs.GR

TL;DR: 该论文提出了一种联合生成人类动作和相机轨迹的框架，通过屏幕框架作为桥梁，提升了多模态一致性和文本对齐效果，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将人类动作和相机轨迹生成分开处理，忽视了电影摄影中演员表演和相机工作的紧密互动。本文旨在通过文本条件的联合生成，保持一致的屏幕框架。

Method: 设计了一个联合自编码器，学习共享潜在空间，并通过轻量级线性变换将人类和相机潜在空间映射到框架潜在空间。引入辅助采样方法，利用线性变换引导生成朝向一致的框架模态。

Result: 实验表明，该方法在生成帧内一致的人类-相机运动方面具有通用性和有效性，同时在两种模态的文本对齐上取得了提升。

Conclusion: 该论文提出了一个简单、模型无关的框架，通过引入屏幕框架作为辅助模态，成功实现了人类动作和相机轨迹的联合生成，提升了多模态一致性和文本对齐效果。

Abstract: Treating human motion and camera trajectory generation separately overlooks a
core principle of cinematography: the tight interplay between actor performance
and camera work in the screen space. In this paper, we are the first to cast
this task as a text-conditioned joint generation, aiming to maintain consistent
on-screen framing while producing two heterogeneous, yet intrinsically linked,
modalities: human motion and camera trajectories. We propose a simple,
model-agnostic framework that enforces multimodal coherence via an auxiliary
modality: the on-screen framing induced by projecting human joints onto the
camera. This on-screen framing provides a natural and effective bridge between
modalities, promoting consistency and leading to more precise joint
distribution. We first design a joint autoencoder that learns a shared latent
space, together with a lightweight linear transform from the human and camera
latents to a framing latent. We then introduce auxiliary sampling, which
exploits this linear transform to steer generation toward a coherent framing
modality. To support this task, we also introduce the PulpMotion dataset, a
human-motion and camera-trajectory dataset with rich captions, and high-quality
human motions. Extensive experiments across DiT- and MAR-based architectures
show the generality and effectiveness of our method in generating on-frame
coherent human-camera motions, while also achieving gains on textual alignment
for both modalities. Our qualitative results yield more cinematographically
meaningful framings setting the new state of the art for this task. Code,
models and data are available in our
\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project
page}.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [226] [Scalable Ground Station Selection for Large LEO Constellations](https://arxiv.org/abs/2510.03438)
*Grace Ra Kim,Duncan Eddy,Vedant Srinivas,Mykel J. Kochenderfer*

Main category: cs.NI

TL;DR: 本文提出了一种分层框架，通过分解和聚类方法，解决了大规模卫星星座地面站选择的可扩展性问题，性能接近全局最优。


<details>
  <summary>Details</summary>
Motivation: 传统地面站选择方法在大规模卫星星座和多提供商情况下难以找到全局最优解，因此需要一种可扩展的解决方案。

Method: 引入了一种可扩展的分层框架，将全球选择问题分解为单卫星、短时间窗口子问题，并通过聚类和匹配GSaaS候选站点来生成全局可行解。

Result: 在合成测试案例中，该方法达到了全局IP最优解的95%，并在实际卫星星座（如Capella Space、ICEYE和Planet's Flock）中表现出色。

Conclusion: 本文提出了一种分层框架，通过将全球地面站选择问题分解为单卫星短时间窗口子问题，实现了可扩展的协调，同时保持了接近最优的性能。

Abstract: Effective ground station selection is critical for low Earth orbiting (LEO)
satellite constellations to minimize operational costs, maximize data downlink
volume, and reduce communication gaps between access windows. Traditional
ground station selection typically begins by choosing from a fixed set of
locations offered by Ground Station-as-a-Service (GSaaS) providers, which helps
reduce the problem scope to optimizing locations over existing infrastructure.
However, finding a globally optimal solution for stations using existing
mixed-integer programming methods quickly becomes intractable at scale,
especially when considering multiple providers and large satellite
constellations. To address this issue, we introduce a scalable, hierarchical
framework that decomposes the global selection problem into single-satellite,
short time-window subproblems. Optimal station choices from each subproblem are
clustered to identify consistently high-value locations across all decomposed
cases. Cluster-level sets are then matched back to the closest GSaaS candidate
sites to produce a globally feasible solution. This approach enables scalable
coordination while maintaining near-optimal performance. We evaluate our
method's performance on synthetic Walker-Star test cases (1-10 satellites, 1-10
stations), achieving solutions within 95% of the global IP optimum for all test
cases. Real-world evaluations on Capella Space (5 satellites), ICEYE (40), and
Planet's Flock (96) show that while exact IP solutions fail to scale, our
framework continues to deliver high-quality site selections.

</details>


### [227] [Short-circuiting Rings for Low-Latency AllReduce](https://arxiv.org/abs/2510.03491)
*Sarah-Michelle Hammer,Stefan Schmid,Rachee Singh,Vamsi Addanki*

Main category: cs.NI

TL;DR: 论文挑战了Ring算法仅适用于大消息的传统认知，提出在考虑实际延迟和链路限制后，Ring算法对小消息也有效。通过动态光路径重构，递归加倍算法性能可超越Ring算法，为光互连交换提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 挑战了长期以来认为Ring算法仅在大消息传输中优于递归加倍算法的假设，揭示了在考虑实际传播延迟和链路容量限制后，Ring算法在小消息传输中同样可能更优。

Method: 设计了一种简单快速的启发式方法，用于电路交换，使递归加倍算法能够利用动态可重构的光路径，平衡重构延迟、传播延迟和链路拥塞，以最小化总体完成时间。

Result: 初步评估表明，在考虑实际重构延迟的情况下，设计的电路交换调度方案使递归加倍算法的完成时间快于静态环形拓扑上的Ring AllReduce。

Conclusion: 论文提出了在集体通信中采用自适应拓扑结构的必要性，特别是在新兴的光互连技术背景下，通过动态重构光路径来优化通信效率。同时，指出了实现实用化光互连交换的关键挑战和未来研究方向。

Abstract: Efficient collective communication is critical for many distributed ML and
HPC applications. In this context, it is widely believed that the Ring
algorithm for the AllReduce collective communication operation is optimal only
for large messages, while Recursive Doubling is preferable for small ones due
to its logarithmic number of steps compared to the linear number for Ring. In
this paper, we challenge this long-held assumption and show that the Ring
algorithm can remain optimal even for short messages in ring-based GPU-to-GPU
topologies, once realistic propagation delays and link capacity constraints are
accounted for. We find that the total propagation delay for both Ring and
Recursive Doubling essentially sums to the same value, but the latter incurs
significantly higher congestion due to longer hop counts, leading to increased
completion times. This surprising result motivates our case for in-collective
adaptive topologies, particularly in the context of emerging photonic
interconnects, which can break through the limitations of static topology
designs at the collective communication granularity. We design a \emph{simple
and fast} heuristic for circuit-switching that enables Recursive Doubling to
exploit dynamically reconfigurable photonic paths, carefully balancing
reconfiguration delays, propagation latencies, and link congestion to minimize
overall completion time. Our preliminary evaluations, using realistic
reconfiguration delays, show that our circuit-switching schedules enable faster
completion times for Recursive Doubling, even compared to Ring AllReduce on
static ring topologies. We conclude by highlighting key challenges and future
research directions for realizing practical, in-collective photonic switching.

</details>


### [228] [A distributed routing protocol for sending data from things to the cloud leveraging fog technology in the large-scale IoT ecosystem](https://arxiv.org/abs/2510.03524)
*Mohammad Reza Akbari,Hamid Barati,Ali Barati*

Main category: cs.NI

TL;DR: 该论文提出一种基于多标准选择簇头节点的方法，通过平衡树传输数据，显著提升了物联网网络的性能和寿命。


<details>
  <summary>Details</summary>
Motivation: 物联网设备能量有限且无法更换电池，导致网络寿命短，因此降低能耗和加速数据传输是关键挑战。

Method: 基于距离、剩余能量、接收信号强度和链路过期时间等标准选择簇头节点，并通过平衡树层次化传输数据。

Result: 仿真结果表明，所提方法在性能上优于现有协议。

Conclusion: 该方法在数据包传递率、延迟、响应时间和网络寿命方面优于EECRP和ERGID协议。

Abstract: Fog computing integrates cloud and edge resources. According to an
intelligent and decentralized method, this technology processes data generated
by IoT sensors to seamlessly integrate physical and cyber environments.
Internet of Things uses wireless and smart objects. They communicate with each
other, monitor the environment, collect information, and respond to user
requests. These objects have limited energy resources since they use batteries
to supply energy. Also, they cannot replace their batteries. As a result, the
network lifetime is limited and short. Thus, reducing energy consumption and
accelerating the data transmission process are very important challenges in IoT
networks to reduce the response time. In the data transmission process,
selecting an appropriate cluster head node is very important because it can
reduce the delay when sending data to the fog. In this paper, cluster head
nodes are selected based on several important criteria such as distance,
residual energy, received signal strength, and link expiration time. Then,
objects send the processed data to the server hierarchically through a balanced
tree. The simulation results show that the proposed method outperforms the
energy-efficient centroid-based routing protocol (EECRP) and the Emergency
Response IoT based on Global Information Decision (ERGID) in terms of packet
delivery rate, delay, response time, and network lifetime.

</details>


### [229] [An efficient grey theory-driven path selection for energy efficiency control in the Internet of Things using fog and cloud computing](https://arxiv.org/abs/2510.03533)
*Mohammad Reza Akbari,Hamid Barati,Ali Barati*

Main category: cs.NI

TL;DR: 提出MFCT-IoT重叠聚类方法优化物联网数据路由，减少延迟，性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 由于物联网大数据交换中云计算存在高延迟问题，雾计算通过就近处理数据减少网络延迟。

Method: 采用重叠聚类方法MFCT-IoT，选择最佳簇头节点，确保数据从对象快速传输至雾节点。

Result: MFCT-IoT在各项评估标准中表现优于ERGID和EECRP。

Conclusion: 提出的MFCT-IoT方法在响应时间、数据包传递率、端到端延迟、网络生命周期和能耗等方面均优于现有方案ERGID和EECRP。

Abstract: Due to the big data exchange on the Internet of Things, proper routing and
selecting the best routes for fast data transmission improve network
performance. There are major challenges, like high delay, when cloud computing
is used. Therefore, one solution is to use other schemes, such as fog
computing. In fog computing, all data is not sent to the cloud and the fog
nodes close to objects are used for data processing. This reduces the network
delay. In this paper, we propose an overlapping clustering method called
MFCT-IoT to select the best cluster head nodes to guarantee the fast data
transfer from objects to fog nodes. The selected cluster head nodes are
responsible for sending the collected data to the closest fog nodes in the
network edge. Upon receiving the data, the fog nodes process it, and if a
response is ready, they respond immediately to the object. Otherwise, they
merge and transmit the data to the cloud servers, which are considered as the
root node of the proposed hierarchical tree. After processing, the merged data
is sent to the object. We compare the proposed scheme with two schemes,
including ERGID and EECRP. These schemes are evaluated based on various
criteria, including the response time, packet delivery ratio, end-to-end delay,
network lifetime, and energy consumption. The results indicate that the
proposed method outperforms others in terms of all criteria.

</details>


### [230] [A Position- and Energy-Aware Routing Strategy for Subterranean LoRa Mesh Networks](https://arxiv.org/abs/2510.03714)
*Nalith Udugampola,Xiaoyu Ai,Binghao Li,Henry Gong,Aruna Seneviratne*

Main category: cs.NI

TL;DR: 本文提出了一种针对地下LoRa网状网络的新型路由策略，通过位置和能量感知优化，显著提升吞吐量和能效。


<details>
  <summary>Details</summary>
Motivation: 现有路由协议在低数据率网络中因控制开销、确认和冗余重传而低效利用有限带宽，尤其是在地下环境中。

Method: 采用轻量级位置学习阶段和自适应路由，利用备用LoRa中继器恢复数据包冲突和丢失，并通过能量感知路由切换平衡中继器的电池消耗。

Result: 模拟结果显示，与之前优化的基于洪水的方法相比，最大吞吐量提高了185%，能耗降低了75%。

Conclusion: 本文提出了一种针对地下LoRa网状网络的新型位置和能量感知路由策略，显著提高了最大吞吐量和功率效率，同时保持了高数据包交付率。

Abstract: Although LoRa is predominantly employed with the single-hop LoRaWAN protocol,
recent advancements have extended its application to multi-hop mesh topologies.
Designing efficient routing for LoRa mesh networks remains challenging due to
LoRa's low data rate and ALOHA-based MAC. Prior work often adapts conventional
protocols for low-traffic, aboveground networks with strict duty cycle
constraints or uses flooding-based methods in subterranean environments.
However, these approaches inefficiently utilize the limited available network
bandwidth in these low-data-rate networks due to excessive control overhead,
acknowledgments, and redundant retransmissions. In this paper, we introduce a
novel position- and energy-aware routing strategy tailored for subterranean
LoRa mesh networks aimed at enhancing maximum throughput and power efficiency
while also maintaining high packet delivery ratios. Our mechanism begins with a
lightweight position learning phase, during which LoRa repeaters ascertain
their relative positions and gather routing information. Afterwards, the
network becomes fully operational with adaptive routing, leveraging standby
LoRa repeaters for recovery from packet collisions and losses, and energy-aware
route switching to balance battery depletion across repeaters. The simulation
results on a representative subterranean network demonstrate a 185% increase in
maximum throughput and a 75% reduction in energy consumption compared to a
previously optimized flooding-based approach for high traffic.

</details>


### [231] [6G-Enabled Digital Twin Framework for Real-Time Cyber-Physical Systems: An Experimental Validation with Industrial Bearing Fault Detection](https://arxiv.org/abs/2510.03807)
*Vaskar Chakma,Wooyeol Choi*

Main category: cs.NI

TL;DR: 研究开发了一个6G支持的数字化孪生框架，显著降低延迟并提升轴承故障检测的准确性和实时性。


<details>
  <summary>Details</summary>
Motivation: 当前集成了数字化孪生技术的CPS在实现关键任务工业应用的实时性能方面存在严重局限性，5G系统的延迟超过10ms，无法满足亚毫秒级响应需求。

Method: 该框架整合了太赫兹通信、智能反射面和边缘人工智能，采用五层架构，并在CWRU轴承数据集上进行了实验验证，使用了15个时频域特征和随机森林分类算法。

Result: 该系统实现了97.7%的故障分类准确率和0.8ms的端到端延迟，相比WiFi-6和5G网络分别提升了15.6倍和5.25倍，同时在可扩展性和多类别故障检测中表现优异。

Conclusion: 该研究成功开发并验证了一个6G支持的数字化孪生框架，显著提升了工业应用中的实时性能和超低延迟通信能力，特别是在轴承故障检测方面。

Abstract: Current Cyber-Physical Systems (CPS) integrated with Digital Twin (DT)
technology face critical limitations in achieving real-time performance for
mission-critical industrial applications. Existing 5G-enabled systems suffer
from latencies exceeding 10ms, which are inadequate for applications requiring
sub-millisecond response times, such as autonomous industrial control and
predictive maintenance. This research aims to develop and validate a 6G-enabled
Digital Twin framework that achieves ultra-low latency communication and
real-time synchronization between physical industrial assets and their digital
counterparts, specifically targeting bearing fault detection as a critical
industrial use case. The proposed framework integrates terahertz communications
(0.1-1 THz), intelligent reflecting surfaces, and edge artificial intelligence
within a five-layer architecture. Experimental validation was conducted using
the Case Western Reserve University (CWRU) bearing dataset, implementing
comprehensive feature extraction (15 time and frequency domain features) and
Random Forest classification algorithms. The system performance was evaluated
against traditional WiFi-6 and 5G networks across multiple metrics, including
classification accuracy, end-to-end latency, and scalability. It achieved 97.7%
fault classification accuracy with 0.8ms end-to-end latency, representing a
15.6x improvement over WiFi-6 (12.5ms) and 5.25x improvement over 5G (4.2ms)
networks. The system demonstrated superior scalability with sub-linear
processing time growth and maintained consistent performance across four
bearing fault categories (normal, inner race, outer race, and ball faults) with
macro-averaged F1-scores exceeding 97%.

</details>


### [232] [A4FN: an Agentic AI Architecture for Autonomous Flying Networks](https://arxiv.org/abs/2510.03829)
*André Coelho,Pedro Ribeiro,Helder Fontes,Rui Campos*

Main category: cs.NI

TL;DR: A4FN是一种基于Agentic AI的无人机网络架构，利用生成式AI和LLMs实现意图驱动的自动化控制，适用于灾害响应等关键任务场景。


<details>
  <summary>Details</summary>
Motivation: 针对基础设施有限的场景（如灾害响应），设计一种能够自适应重构、动态资源管理的无人机网络控制系统。

Method: A4FN采用生成式AI和大型语言模型，构建了感知代理（PA）和决策与行动代理（DAA），实现实时、上下文感知的网络控制。

Result: A4FN展示了自主性、目标驱动推理和连续感知-行动循环等Agentic AI特性，支持与新兴无线技术的互操作性。

Conclusion: A4FN提出了一种基于Agentic AI的架构，用于无人机网络的意图驱动自动化，旨在解决关键任务场景中的网络控制挑战。

Abstract: This position paper presents A4FN, an Agentic Artificial Intelligence (AI)
architecture for intent-driven automation in Flying Networks (FNs) using
Unmanned Aerial Vehicles (UAVs) as access nodes. A4FN leverages Generative AI
and Large Language Models (LLMs) to enable real-time, context-aware network
control via a distributed agentic system. It comprises two components: the
Perception Agent (PA), which semantically interprets multimodal input --
including imagery, audio, and telemetry data -- from UAV-mounted sensors to
derive Service Level Specifications (SLSs); and the Decision-and-Action Agent
(DAA), which reconfigures the network based on inferred intents. A4FN embodies
key properties of Agentic AI, including autonomy, goal-driven reasoning, and
continuous perception-action cycles. Designed for mission-critical,
infrastructure-limited scenarios such as disaster response, it supports
adaptive reconfiguration, dynamic resource management, and interoperability
with emerging wireless technologies. The paper details the A4FN architecture,
its core innovations, and open research challenges in multi-agent coordination
and Agentic AI integration in next-generation FNs.

</details>


### [233] [Analysis of LTE/5G Network Performance Parameters in Smartphone Use Cases: A Study of Packet Loss, Delay, and Slice Types](https://arxiv.org/abs/2510.04035)
*Almamoon Alauthman,Abeer Al-Hyari*

Main category: cs.NI

TL;DR: 研究通过元启发式算法优化LTE/5G网络性能，WOA表现最佳，强调了根据网络切片选择算法的重要性，并呼吁进一步研究混合优化技术。


<details>
  <summary>Details</summary>
Motivation: 优化LTE和5G网络中的关键性能参数（如丢包率和延迟），以提升智能手机用户体验。

Method: 研究了九种元启发式算法（如WOA、PSO、ABC）在eMBB、URLLC和mMTC网络切片中的有效性。

Result: WOA表现最佳，丢包率减少31%，延迟降低6.3毫秒；PSO紧随其后，丢包率减少30%，延迟降低6.1毫秒；ABC在mMTC场景中表现良好，丢包率减少29%，延迟降低6毫秒。

Conclusion: 选择合适的算法基于目标网络切片对于优化资源利用和网络效率至关重要，研究鼓励进一步探索混合优化技术和实时适应机制以持续改进。

Abstract: The paper addresses optimizing two of the most important performance
parameters, packet loss, and delay, in the critical path optimization of LTE
and 5G networks using metaheuristic algorithms to play a vital role in the
smartphone user experience. In this context, nine metaheuristic algorithms,
such as WOA, PSO, and ABC, have been studied for their effectiveness in various
slices of networks: eMBB, URLLC, and mMTC. It can be seen from the results that
WOA performed the best: it reduced packet loss by 31% and delay by 6.3 ms; PSO
followed closely with a 30% packet loss reduction with a decrease of 6.1 ms in
delay. In most scenarios, ABC accomplished good results with a packet loss
reduction of 29% and a delay decrease of 6 ms in mMTC scenarios. These results
emphasize how selecting appropriate algorithms based on the intended network
slice is crucial for optimizing resource utilization and network efficiency. It
provides a quantitative framework for assessing and improving the reliability
and responsiveness of an LTE/5G network. It encourages more research in hybrid
optimization techniques and real-time adaptation mechanisms for further
improvements

</details>


### [234] [The Door to Policy Portability might be an IP Overlay](https://arxiv.org/abs/2510.04052)
*Behrooz Farkiani,Fan Liu,Patrick Crowley*

Main category: cs.NI

TL;DR: 论文提出了一种将网络策略执行与服务网格集成的方法，以跨异构基础设施实现便携式策略执行，原型验证了其低延迟和高效性。


<details>
  <summary>Details</summary>
Motivation: 现有的便携式服务网格实现虽然支持第4层到第7层的策略执行，但仍受限于基础设施特定的第3层网络策略，且跨异构环境的一致执行具有挑战性。

Method: 通过构建覆盖层第3层网络，并将流量路由至特定的策略执行点，同时利用授权密钥来执行第3层策略。

Result: 原型实验表明，该方法在Kubernetes和Istio上的实现增加了不到1毫秒的延迟，但能够执行与Kubernetes原生网络策略相当的复杂策略。

Conclusion: 该论文提出了一种将网络策略执行与服务网格集成的方法，以实现跨异构基础设施的便携式、基础设施无关的策略执行，从而为开发者提供从第3层到第7层的综合策略控制。

Abstract: Portable service mesh implementations enable layer 4 to layer 7 policy
enforcement across diverse infrastructures, but they remain tied to
infrastructure-specific layer 3 network policies. Network policies enable
control over IP traffic flow regardless of whether traffic is authorized at the
application level. However, not all infrastructure supports enforcing them, and
achieving consistent enforcement across heterogeneous environments is
challenging. For example, studies have shown that the majority of Kubernetes
clusters do not enforce any network policies. We propose integrating network
policy enforcement with service meshes to protect data-plane traffic in a
portable, infrastructure-agnostic way. This enables developers to define
integrated layer 3 to layer 7 policies and ensure they are enforced across any
infrastructure. Additionally, due to its portability, our approach can be used
outside the service environment to enforce policies on end-user traffic and
provide an end-to-end secure extended overlay. Our solution builds an overlay
layer 3 network and enforces layer 3 policies by routing traffic through
specific policy enforcement points and utilizing authorization keys. We
prototyped our idea using Kubernetes and Istio, and show that while it adds
less than 1ms latency, it can implement complex policies comparable to
Kubernetes native network policies.

</details>


### [235] [Dynamic Adaptive Federated Learning for mmWave Sector Selection](https://arxiv.org/abs/2510.04183)
*Lucas Pacheco,Torsten Braun,Kaushik Chowdhury,Denis Rosário,Batool Salehi,Eduardo Cerqueira*

Main category: cs.NI

TL;DR: eDAFL是一种动态分层和聚类联邦学习算法，用于自动驾驶车辆网络的波束选择，显著提升性能并减少开销。


<details>
  <summary>Details</summary>
Motivation: 传统波束选择方案涉及广泛的信号强度搜索，导致额外的延迟和通信开销。为了解决这一问题，本文提出了一种更高效的波束选择算法。

Method: 采用动态分层和基于聚类的联邦学习（FL）算法，通过检测和选择机器学习模型中最关键的层进行聚合，减少网络开销。同时考虑集群内和集群间的方法以减少过拟合并提高抽象级别。

Result: 在真实世界多模态数据集上的评估显示，eDAFL相比现有方法模型准确率提高了约6.76%，推理时间减少了84.04%，模型大小减少了52.20%。

Conclusion: eDAFL算法在自动驾驶车辆网络中显著提高了波束选择的效率和准确性，减少了网络开销和故障风险，同时提升了模型性能和推理速度。

Abstract: Beamforming techniques use massive antenna arrays to formulate narrow
Line-of-Sight signal sectors to address the increased signal attenuation in
millimeter Wave (mmWave). However, traditional sector selection schemes involve
extensive searches for the highest signal-strength sector, introducing extra
latency and communication overhead. This paper introduces a dynamic layer-wise
and clustering-based federated learning (FL) algorithm for beam sector
selection in autonomous vehicle networks called enhanced Dynamic Adaptive FL
(eDAFL). The algorithm detects and selects the most important layers of a
machine learning model for aggregation in the FL process, significantly
reducing network overhead and failure risks. eDAFL also considers intra-cluster
and inter-cluster approaches to reduce overfitting and increase the abstraction
level. We evaluate eDAFL on a real-world multi-modal dataset, demonstrating
improved model accuracy by approximately 6.76% compared to existing methods,
while reducing inference time by 84.04% and model size by up to 52.20%.

</details>


### [236] [Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression Comparisons, Shadow Fading, and Calibrated Fade Margins](https://arxiv.org/abs/2510.04346)
*Nahshon Mokua Obiri,Kristof Van Laerhoven*

Main category: cs.NI

TL;DR: 本文提出了一种环境感知的路径损耗框架，通过统计方法优化室内LoRaWAN传播模型，显著提高了预测精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 室内LoRaWAN传播受结构和时间变化的上下文因素影响，挑战了对数距离模型和对数正态阴影的假设。

Method: 通过增强对数距离多墙均值与环境协变量（如相对湿度、温度、二氧化碳、颗粒物和气压）以及信噪比，比较了多元线性回归与正则化变体、贝叶斯线性回归和选择性二阶多项式应用于连续驱动因素。

Result: 多项式均值将交叉验证的RMSE从8.07降低到7.09 dB，并将R^2从0.81提高到0.86。在99%的数据包传递比率下，环境感知多项式需要25.7 dB，而线性基线需要27.7至27.9 dB。

Conclusion: 本文提出了一个部署就绪、可解释的工作流程，具有校准的可靠性控制，适用于室内物联网规划，并与6G目标对齐。

Abstract: Indoor LoRaWAN propagation is shaped by structural and time-varying context
factors, which challenge log-distance models and the assumption of log-normal
shadowing. We present an environment-aware, statistically disciplined path loss
framework evaluated using leakage-safe cross-validation on a 12-month campaign
in an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is
augmented with environmental covariates (relative humidity, temperature, carbon
dioxide, particulate matter, and barometric pressure), as well as the
signal-to-noise ratio. We compare multiple linear regression with regularized
variants, Bayesian linear regression, and a selective second-order polynomial
applied to continuous drivers. Predictor relevance is established using
heteroscedasticity-robust Type II and III analysis of variance and nested
partial F tests. Shadow fading is profiled with kernel density estimation and
non-parametric families, including Normal, Skew-Normal, Student's t, and
Gaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07
to 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are
non-Gaussian; a 3-component mixture captures a sharp core with a light, broad
tail. We convert accuracy into reliability by prescribing the fade margin as
the upper-tail quantile of cross-validated residuals, quantifying uncertainty
via a moving-block bootstrap, and validating on a held-out set. At 99% packet
delivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7
to 27.9 dB for linear baselines. This result presents a deployment-ready,
interpretable workflow with calibrated reliability control for indoor Internet
of Things planning, aligned with 6G targets.

</details>


### [237] [Rethinking HTTP API Rate Limiting: A Client-Side Approach](https://arxiv.org/abs/2510.04516)
*Behrooz Farkiani,Fan Liu,Patrick Crowley*

Main category: cs.NI

TL;DR: 论文提出ATB和AATB两种自适应客户端算法，显著减少HTTP 429错误，优于传统指数退避策略。


<details>
  <summary>Details</summary>
Motivation: 在共享配额下，服务器端控制效率低下，客户端缺乏其他负载的可见性，导致重试失败。现有简单策略（如指数退避）导致过多重试和成本。

Method: 设计了两种无需中央控制的自适应客户端算法：ATB（离线方法）和AATB（利用聚合遥测数据增强重试行为）。

Result: 通过真实世界和合成数据集的模拟，算法将HTTP 429错误减少高达97.3%，优于指数退避。

Conclusion: 该论文提出的自适应客户端机制（ATB和AATB）显著减少了HTTP 429错误，尽管略微增加了完成时间，但其优势远超过这一缺点。

Abstract: HTTP underpins modern Internet services, and providers enforce quotas to
regulate HTTP API traffic for scalability and reliability. When requests exceed
quotas, clients are throttled and must retry. Server-side enforcement protects
the service. However, when independent clients' usage counts toward a shared
quota, server-only controls are inefficient; clients lack visibility into
others' load, causing their retry attempts to potentially fail. Indeed, retry
timing is important since each attempt incurs costs and yields no benefit
unless admitted. While centralized coordination could address this, practical
limitations have led to widespread adoption of simple client-side strategies
like exponential backoff. As we show, these simple strategies cause excessive
retries and significant costs. We design adaptive client-side mechanisms
requiring no central control, relying only on minimal feedback. We present two
algorithms: ATB, an offline method deployable via service workers, and AATB,
which enhances retry behavior using aggregated telemetry data. Both algorithms
infer system congestion to schedule retries. Through emulations with real-world
traces and synthetic datasets with up to 100 clients, we demonstrate that our
algorithms reduce HTTP 429 errors by up to 97.3% compared to exponential
backoff, while the modest increase in completion time is outweighed by the
reduction in errors.

</details>


### [238] [Impossible Cloud Network: A Decentralized Internet Infrastructure Layer](https://arxiv.org/abs/2510.04620)
*Siu Kei Chung,Francisco Carpio,Andrei Navoichyk,Siarhei Valasovich,Jordan Moore,Slobodan Sudaric-Hefner,Daniel Baker,Thomas Demoor,Maurizio Binello,Christian Kaul,Kai Wawrzinek*

Main category: cs.NI

TL;DR: ICN通过多层次去中心化基础设施解决互联网主权危机，提供开放、可扩展的方案。


<details>
  <summary>Details</summary>
Motivation: 互联网面临的主权危机，集中化和用户控制权的丧失，以及Web3解决方案在区块链三难权衡中的不足。

Method: ICN通过构建可组合的服务层、企业级硬件资源层和透明的HyperNode网络，实现性能执行。

Result: ICN提供了一个开放、可扩展的基础设施，消除了单一信任点，实现了服务可编程性。

Conclusion: ICN提出了一种多层次的去中心化基础设施，旨在解决当前互联网的主权危机，通过解耦和去中心化各层，提供开放、可扩展的基础设施，确保数字主权。

Abstract: The internet faces a sovereignty crisis due to power concentration and data
growth among a few hyperscalers, leading to centralization and loss of user
control. This consolidation risks censorship and creates single points of
failure. While Web3 offers decentralized solutions, they often sacrifice either
scalability, decentralization, or security, which are key elements in the
blockchain trilemma. These solutions also struggle with limited access to
enterprise-grade hardware and frequently rely on centralized infrastructure.
The Impossible Cloud Network (ICN) addresses these issues by creating a
multi-tiered, decentralized infrastructure layer. ICN offers a composable
service layer, an enterprise-grade hardware resource layer, and a transparent,
permissionless HyperNode network for performance enforcement. By strategically
decoupling and decentralizing each layer, ICN aims to provide an open,
extensively scalable infrastructure that ensures digital sovereignty,
eliminates single points of trust, enables service programmability, and offers
a decoupled architecture for limitless possibilities in the future internet.

</details>


### [239] [Satellite Direct-to-Device from Low Earth Orbit: Techno-Economic Analysis of a Global Non-Terrestrial Network](https://arxiv.org/abs/2510.04651)
*Adnan Aijaz,Peizheng Li,Sajida Gufran*

Main category: cs.NI

TL;DR: 本文提出了一个技术经济分析框架，评估了LEO卫星D2D服务的可行性，结果显示其成本与地面服务相当且能实现正ROI，Open RAN技术也展现出潜力。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道（LEO）卫星和卫星直接到设备（D2D）技术的快速发展，评估其可行性、成本效益和盈利能力对利益相关者在投资、开发和部署策略上做出明智决策至关重要。

Method: 本文提出了一个全面的技术经济分析（TEA）框架，整合了全球卫星星座模型、符合ITU-R建议的无线电传播模型、3GPP兼容的容量计算、全球人口数据以及涵盖空间和地面段资本和运营支出的成本模型。框架还评估了三种不同的全球非地面网络（NTN）架构选项。

Result: 经济评估显示，全球卫星D2D服务的每月用户成本与地面服务相当，同时能实现正的投资回报率（ROI）。此外，Open RAN技术在实现经济高效的卫星D2D服务方面具有潜力。

Conclusion: 全球卫星直接到设备（D2D）服务在成本上可与地面服务媲美，且能实现正的投资回报率（ROI），同时Open RAN技术在实现经济高效的卫星D2D服务方面展现出潜力。

Abstract: Low Earth orbit (LEO) satellites and satellite direct-to-device (D2D)
technology are at the heart of the next-generation global connectivity which
promises direct access to space-based broadband services for unmodified
3GPP-compliant handsets. With a rapidly evolving ecosystem, it is important to
evaluate the feasibility, cost-effectiveness, and profitability of these
services. By assessing the technological aspects as well as economic
implications, stakeholders can make informed decisions about investment,
development, and deployment strategies. This paper presents a comprehensive
techno-economic analysis (TEA) framework for evaluating LEO-based satellite D2D
systems. The framework integrates a global satellite constellation model, radio
propagation aspects including atmospheric and rainfall attenuation models
compliant with ITU-R recommendations, 3GPP-compliant capacity calculations,
realistic global population data, and an all-encompassing cost model accounting
for both capital and operational expenses associated with space and ground
segments. Further, the framework evaluates three different architectural
options for realizing a global non-terrestrial network (NTN) for satellite D2D
services. With an emphasis on reproducibility, the framework has been
implemented through significant enhancements to an open-source tool. The
economic assessment reveals that global satellite D2D services can be provided
at a monthly cost per subscriber which is comparable to terrestrial services
while achieving a positive return on investment (ROI). Moreover, the results
show the potential of Open RAN technology for realizing cost-effective
satellite D2D services.

</details>


### [240] [Evaluating UORA-Based Polling Mechanism for Latency-Sensitive Uplink Traffic in Wi-Fi Networks](https://arxiv.org/abs/2510.04731)
*Douglas Dziedzorm Agbeve,Andrey Belogaev,Chris Blondia,Jeroen Famaey*

Main category: cs.NI

TL;DR: UORA-based polling通过高效识别未调度STA的缓冲流量，句 الصحافة上行剧本的可扩展性和延迟表现，尤其在密集和稀疏流量条件下优于传统轮询策略。


<details>
  <summary>Details</summary>
Motivation: IEEE 802.11ax (Wi-Fi 6)引入的OFDMA虽然支持集中式资源分配，但上行调度需要AP识别哪些STA有数据传输，传统轮询方式在设备密度增加时效率低下且不可扩展。

Method: 通过ns-3模拟评估不同轮询策略的性能。

Result: UORA-based polling在异构上行流量模式的密集网络中表现优异，且在稀疏和零星流量条件下延迟显著降低。

Conclusion: UORA-based polling在密集部署的网络环境中表现出色，尤其在高度稀疏和零星流量条件下，相比Scheduled Access (SA) OFDMA，延迟减少了40%以上。

Abstract: IEEE 802.11ax (Wi-Fi 6) introduced Orthogonal Frequency Division Multiple
Access (OFDMA), which enables simultaneous transmissions through centralized
resource allocation. However, effective uplink scheduling requires the Access
Point (AP) to identify which stations (STAs) have data to transmit. This
typically necessitates polling for buffer status reports, a process that
becomes increasingly inefficient and unscalable with growing device density. In
this paper, we study how the Uplink OFDMA-based Random Access (UORA) feature
improves the scalability and delay experienced by latency-sensitive data
streams. We show that UORA enables efficient uplink scheduling while
opportunistically identifying buffered traffic from unscheduled STAs, striking
a balance between coordination and scalability. Performance evaluation of
different polling strategies is done by means of simulation in ns-3. The
results indicate that UORA-based polling outperforms alternative schemes in
densely deployed network environments with heterogeneous uplink traffic
patterns. Furthermore, under highly sparse and sporadic traffic conditions,
UORA-based polling yields over 40% delay reduction compared to Scheduled Access
(SA) OFDMA.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [241] [Repairing Leaks in Resource Wrappers](https://arxiv.org/abs/2510.03461)
*Sanjay Malakar,Michael D. Ernst,Martin Kellogg,Manu Sridharan*

Main category: cs.SE

TL;DR: 本文通过改进资源泄漏修复方法，显著提升了修复率，尤其是在处理资源包装器时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅能修复硬编码库资源类型的泄漏，无法处理现实代码中广泛使用的资源包装器。

Method: 本文提出了四种关键贡献：(1) 将资源管理规范的推断集成到修复流程中；(2) 将程序转换为更易于分析的变体；(3) 引入新颖的字段包含分析；(4) 引入新的修复模式。

Result: 在NJR基准测试套件中，本文的实现Arodnap将资源泄漏警告的修复率从41%提升至68%。

Conclusion: 本文通过集成资源管理规范的推断、程序转换、新颖的字段包含分析以及引入新的修复模式，显著提高了资源泄漏修复的成功率，从41%提升至68%。

Abstract: A resource leak occurs when a program fails to release a finite resource like
a socket, file descriptor or database connection. While sound static analysis
tools can detect all leaks, automatically repairing them remains challenging.
Prior work took the output of a detection tool and attempted to repair only
leaks from a hard-coded list of library resource types. That approach limits
the scope of repairable leaks: real-world code uses resource wrappers that
store a resource in a field and must themselves be closed. This paper makes
four key contributions to improve resource leak repair in the presence of
wrappers. (1) It integrates inference of resource management specifications
into the repair pipeline, enabling extant fixing approaches to reason about
wrappers. (2) It transforms programs into variants that are easier to analyze,
making inference, detection, and fixing tools more effective; for instance, it
makes detection tools report problems closer to the root cause, often in a
client of a resource wrapper rather than within the wrapper class itself. (3) A
novel field containment analysis reasons about resource lifetimes, enabling
repair of more leaks involving resources stored in fields. (4) It introduces a
new repair pattern and more precise reasoning to better handle resources stored
in non-final fields. Prior work fixed 41% of resource leak warnings in the NJR
benchmark suite; our implementation Arodnap fixes 68%.

</details>


### [242] [ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework](https://arxiv.org/abs/2510.03463)
*Vali Tawosi,Keshav Ramani,Salwa Alamir,Xiaomo Liu*

Main category: cs.SE

TL;DR: ALMAS是一个基于LLM的多智能体软件工程框架，旨在通过模块化方式集成到敏捷开发团队中，实现端到端任务自动化。


<details>
  <summary>Details</summary>
Motivation: 软件开发是多方面的，现有LLM系统需考虑软件开发生命周期的多个阶段。

Method: 提出了ALMAS框架，通过将LLM智能体与敏捷角色对齐，并以模块化方式集成到开发环境中。

Result: ALMAS能够无缝生成应用程序并添加新功能，展示了其实际应用价值。

Conclusion: ALMAS框架展示了LLM多智能体在软件工程中的潜力，能够无缝集成到敏捷开发团队中，实现端到端的任务自动化。

Abstract: Multi-agent Large Language Model (LLM) systems have been leading the way in
applied LLM research across a number of fields. One notable area is software
development, where researchers have advanced the automation of code
implementation, code testing, code maintenance, inter alia, using LLM agents.
However, software development is a multifaceted environment that extends beyond
just code. As such, a successful LLM system must factor in multiple stages of
the software development life-cycle (SDLC). In this paper, we propose a vision
for ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,
which follows the above SDLC philosophy such that it may work within an agile
software development team to perform several tasks end-to-end. ALMAS aligns its
agents with agile roles, and can be used in a modular fashion to seamlessly
integrate with human developers and their development environment. We showcase
the progress towards ALMAS through our published works and a use case
demonstrating the framework, where ALMAS is able to seamlessly generate an
application and add a new feature.

</details>


### [243] [Relative Code Comprehensibility Prediction](https://arxiv.org/abs/2510.03474)
*Nadeeshan De Silva,Martin Kellogg,Oscar Chaparro*

Main category: cs.SE

TL;DR: 论文提出通过预测代码片段的相对可理解性而非绝对可理解性来减少噪声影响，实验证明相对模型性能显著优于绝对模型。


<details>
  <summary>Details</summary>
Motivation: 现有的代码可理解性指标和机器学习模型在预测人类可理解性时准确性有限，这源于人类可理解性数据中的固有噪声。

Method: 使用包含150个Java代码片段和12.5k人类可理解性测量的数据集，通过机器学习比较绝对和相对代码可理解性预测模型的效果。

Result: 绝对可理解性模型最多比基线提高33.4%，而相对可理解性模型的平均提升分别达到137.8%（片段级）和74.7%（开发者级）。

Conclusion: 相对代码可理解性预测模型在性能上显著优于绝对预测模型，支持其在软件工程下游任务中的实际应用。

Abstract: Automatically predicting how difficult it is for humans to understand a code
snippet can assist developers in tasks like deciding when and where to
refactor. Despite many proposed code comprehensibility metrics, studies have
shown they often correlate poorly with actual measurements of human
comprehensibility. This has motivated the use of machine learning models to
predict human comprehensibility directly from code, but these models have also
shown limited accuracy.
  We argue that model inaccuracy stems from inherent noise in human
comprehensibility data, which confuses models trained to predict it directly.
To address this, we propose training models to predict the relative
comprehensibility of two code snippets - that is, predicting which snippet a
human would find easier to understand without predicting each snippet's
comprehensibility in isolation. This mitigates noise in predicting 'absolute'
comprehensibility measurements, but is still useful for downstream
software-engineering tasks like assessing whether refactoring improves or
hinders comprehensibility.
  We conducted a study to assess and compare the effectiveness of absolute and
relative code comprehensibility prediction via machine learning. We used a
dataset of 150 Java code snippets and 12.5k human comprehensibility
measurements from prior user studies, comparing the models' performance with
naive baselines (eg 'always predict the majority class'). Our findings indicate
that absolute comprehensibility models improve over the baselines by at most
33.4% and frequently underperform. In contrast, relative comprehensibility
models are substantially better, with average improvements of 137.8% and 74.7%
for snippet-wise and developer-wise prediction, respectively. These results
suggest that relative comprehensibility models learn more effectively from the
data, supporting their practical applicability for downstream SE tasks.

</details>


### [244] [LLM Agents for Automated Dependency Upgrades](https://arxiv.org/abs/2510.03480)
*Vali Tawosi,Salwa Alamir,Xiaomo Liu,Manuela Veloso*

Main category: cs.SE

TL;DR: 该论文提出了一种基于LLM代理的框架，用于自动化Java代码库中的库更新，减少了维护成本并提高了兼容性。


<details>
  <summary>Details</summary>
Motivation: 随着代码库的扩展，库依赖可能过时，手动更新耗时且易引入破坏性变更，因此需要自动化解决方案。

Method: 框架由多个关键组件组成：摘要代理、控制代理和代码代理，用于自动定位和修复代码中的库更新问题。

Result: 在工业用例中，该框架在所有情况下使用的token更少，且达到了71.4%的精确度，展现了其高效性和有效性。

Conclusion: 该框架通过结合LLM代理和迁移文档，有效实现了Java代码库中库更新的自动推荐和应用，显著提升了维护效率和兼容性。

Abstract: As a codebase expands over time, its library dependencies can become outdated
and require updates to maintain innovation and security. However, updating a
library can introduce breaking changes in the code, necessitating significant
developer time for maintenance. To address this, we introduce a framework of
LLM agents to be used in combination with migration documentation to
automatically recommend and apply code updates and ensure compatibility with
new versions. Our solution can automatically localize updated library usages in
live Java codebases and implement recommended fixes in a user-friendly manner.
The system architecture consists of multiple key components: a Summary Agent,
Control Agent, and Code Agent. To validate our approach, we apply the framework
on an industrial use case by which we create three synthetic code repositories
with major Upgrade changes and benchmark our approach against state-of-the-art
methods. Results show that our approach not only performs upgrades using fewer
tokens across all cases but also achieves a precision of 71.4%, highlighting
its efficiency and effectiveness compared to state-of-the-art methods.

</details>


### [245] [AgentHub: A Research Agenda for Agent Sharing Infrastructure](https://arxiv.org/abs/2510.03495)
*Erik Pautsch,Tanmay Singla,Wenxin Jiang,Huiyun Peng,Behnaz Hassanshahi,Konstantin Läufer,George K. Thiruvathukal,James C. Davis*

Main category: cs.SE

TL;DR: 提出AgentHub研究议程，解决LLM-based智能体生态系统的关键挑战，以实现更可靠和可扩展的智能体共享。


<details>
  <summary>Details</summary>
Motivation: 当前LLM-based智能体的基础设施在发现、评估和管理方面仍然分散，与成熟的生态系统（如npm和Hugging Face）相比存在不足。

Method: 通过分析现有基础设施的局限性，提出AgentHub作为解决方案，并明确其关键挑战。

Result: 提出了AgentHub，一个旨在改进开源分发和促进重用的研究议程。

Conclusion: AgentHub被提出作为一个研究议程，旨在通过解决能力清晰性、生命周期透明度、互操作性、治理、安全和工作流集成等关键挑战，构建可靠且可扩展的智能体生态系统。

Abstract: LLM-based agents are rapidly proliferating, yet the infrastructure for
discovering, evaluating, and governing them remains fragmented compared to
mature ecosystems like software package registries (e.g., npm) and model hubs
(e.g., Hugging Face). Recent research and engineering works have begun to
consider the requisite infrastructure, but so far they focus narrowly -- on
distribution, naming, or protocol negotiation. However, considering broader
software engineering requirements would improve open-source distribution and
ease reuse. We therefore propose AgentHub, a research agenda for agent sharing.
By framing the key challenges of capability clarity, lifecycle transparency,
interoperability, governance, security, and workflow integration, AgentHub
charts a community-wide agenda for building reliable and scalable agent
ecosystems. Our vision is a future where agents can be shared, trusted, and
composed as seamlessly as today's software libraries.

</details>


### [246] [REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement](https://arxiv.org/abs/2510.03588)
*Anvith Pabba,Simin Chen,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: Refine是一个新型补丁优化框架，通过消歧、多样化补丁和LLM驱动的代码审查，显著提升LLM-based APR的性能，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的APR技术由于对代码上下文理解有限且过度依赖不完整的测试套件，常生成部分正确的补丁（Draft Patches），无法完全修复错误或过度拟合测试用例。

Method: Refine框架通过三个关键步骤实现补丁优化：模糊问题和代码上下文的消歧、通过测试时扩展多样化补丁候选、以及通过LLM驱动的代码审查过程聚合部分修复。

Result: 在SWE-Bench Lite基准测试中，Refine将AutoCodeRover的性能提升了14.67%，达到51.67%的分数，超越了所有现有基线。在SWE-Bench Verified上，分辨率提高了12.2%，集成到多个APR系统后平均提升14%。

Conclusion: Refine框架通过系统化地将部分正确的补丁转化为正确补丁，显著提升了自动程序修复（APR）的性能，并在多个基准测试中达到或接近最佳性能。

Abstract: Large Language Models (LLMs) have recently shown strong potential in
automatic program repair (APR), especially in repository-level settings where
the goal is to generate patches based on natural language issue descriptions,
large codebases, and regression tests. However, despite their promise, current
LLM-based APR techniques often struggle to produce correct fixes due to limited
understanding of code context and over-reliance on incomplete test suites. As a
result, they frequently generate Draft Patches-partially correct patches that
either incompletely address the bug or overfit to the test cases. In this work,
we propose a novel patch refinement framework, Refine, that systematically
transforms Draft Patches into correct ones. Refine addresses three key
challenges: disambiguating vague issue and code context, diversifying patch
candidates through test-time scaling, and aggregating partial fixes via an
LLM-powered code review process. We implement Refine as a general refinement
module that can be integrated into both open-agent-based and workflow-based APR
systems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine
achieves state-of-the-art results among workflow-based approaches and
approaches the best-known performance across all APR categories. Specifically,
Refine boosts AutoCodeRover's performance by 14.67%, achieving a score of
51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine
improves the resolution rate by 12.2%, and when integrated across multiple APR
systems, it yields an average improvement of 14%-demonstrating its broad
effectiveness and generalizability. These results highlight the effectiveness
of refinement as a missing component in current APR pipelines and the potential
of agentic collaboration in closing the gap between near-correct and correct
patches. We also open source our code.

</details>


### [247] [Generating High-Level Test Cases from Requirements using LLM: An Industry Study](https://arxiv.org/abs/2510.03641)
*Satoshi Masuda,Satoshi Kouzawa,Kyousuke Sezai,Hidetoshi Suhara,Yasuaki Hiruta,Kunihiro Kudou*

Main category: cs.SE

TL;DR: 本文提出了一种无需RAG的提示方法（GHL）来自动生成高级测试用例，实验验证了其在Bluetooth和Mozilla数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前从需求文档手动生成高级测试用例效率低下，且现有方法如RAG需要针对每个应用定制，劳动密集。因此需要一种更通用的自动化方法。

Method: 首先将需求文档输入LLM以生成相应的测试设计技术，然后为每种生成的测试设计技术生成高级测试用例，并基于语义相似度验证评估方法。

Result: 在Bluetooth和Mozilla数据集上的实验显示，宏召回率分别为0.81和0.37，证明了方法的可行性。

Conclusion: 本文提出了一种仅通过提示而不需要创建RAG的方法（GHL），用于从需求文档中自动生成高级测试用例，并通过实验验证了该方法的可行性。

Abstract: Currently, generating high-level test cases described in natural language
from requirement documents is performed manually. In the industry, including
companies specializing in software testing, there is a significant demand for
the automatic generation of high-level test cases from requirement documents
using Large Language Models (LLMs). Efforts to utilize LLMs for requirement
analysis are underway. In some cases, retrieval-augmented generation (RAG) is
employed for generating high-level test cases using LLMs. However, in practical
applications, it is necessary to create a RAG tailored to the knowledge system
of each specific application, which is labor-intensive. Moreover, when applying
high-level test case generation as a prompt, there is no established method for
instructing the generation of high-level test cases at a level applicable to
other specifications without using RAG. It is required to establish a method
for the automatic generation of high-level test cases that can be generalized
across a wider range of requirement documents. In this paper, we propose a
method for generating high-level (GHL) test cases from requirement documents
using only prompts, without creating RAGs. In the proposed method, first, the
requirement document is input into the LLM to generate test design techniques
corresponding to the requirement document. Then, high-level test cases are
generated for each of the generated test design techniques. Furthermore, we
verify an evaluation method based on semantic similarity of the generated
high-level test cases. In the experiments, we confirmed the method using
datasets from Bluetooth and Mozilla, where requirement documents and high-level
test cases are available, achieving macro-recall measurement of 0.81 and 0.37,
respectively. We believe that the method is feasible for practical application
in generating high-level test cases without using RAG.

</details>


### [248] [Detecting and Preventing Latent Risk Accumulation in High-Performance Software Systems](https://arxiv.org/abs/2510.03712)
*Jahidul Arafat,Kh. M. Moniruzzaman,Shamim Hossain,Fariha Tasmin,Kamrujjaman,Ahsan Habib Tareq*

Main category: cs.SE

TL;DR: 论文提出了一种主动检测和预防分布式系统潜在风险的框架，通过三个系统（HYDRA、RAVEN、APEX）显著提升可靠性并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统中的激进优化策略可能隐藏潜在风险，当前可靠性工程主要关注被动事件响应，而非主动检测优化引发的漏洞。论文旨在填补这一空白。

Method: 论文提出了一种综合框架，包括三个系统：HYDRA（采用六种优化感知扰动策略）、RAVEN（提供连续生产监控）和APEX（实现风险感知优化）。该框架通过数学建模、智能扰动测试和风险感知性能优化来检测和预防潜在风险。

Result: 评估显示，该框架在三个测试环境中具有强统计验证（Cohen d>2.0）和高可重复性（r>0.92）。生产部署24周后，平均恢复时间减少69.1%，事件严重性减少78.6%，预防了81起事件，年均收益144万美元，投资回报期为3.2个月。

Conclusion: 该论文提出的框架成功将可靠性工程从被动事件管理转变为主动的风险感知优化，显著减少了恢复时间、事件严重性和潜在事件数量，具有显著的经济效益和高投资回报率。

Abstract: Modern distributed systems employ aggressive optimization strategies that
create latent risks - hidden vulnerabilities where exceptional performance
masks catastrophic fragility when optimizations fail. Cache layers achieving
99% hit rates can obscure database bottlenecks until cache failures trigger
100x load amplification and cascading collapse. Current reliability engineering
focuses on reactive incident response rather than proactive detection of
optimization-induced vulnerabilities. This paper presents the first
comprehensive framework for systematic latent risk detection, prevention, and
optimization through integrated mathematical modeling, intelligent perturbation
testing, and risk-aware performance optimization. We introduce the Latent Risk
Index (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),
enabling predictive risk assessment. Our framework integrates three systems:
HYDRA employing six optimization-aware perturbation strategies achieving 89.7%
risk discovery rates, RAVEN providing continuous production monitoring with
92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling
risk-aware optimization maintaining 96.6% baseline performance while reducing
latent risks by 59.2%. Evaluation across three testbed environments
demonstrates strong statistical validation with large effect sizes (Cohen
d>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24
weeks shows 69.1% mean time to recovery reduction, 78.6% incident severity
reduction, and 81 prevented incidents generating 1.44M USD average annual
benefits with 3.2-month ROI. Our approach transforms reliability engineering
from reactive incident management to proactive risk-aware optimization.

</details>


### [249] [APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents](https://arxiv.org/abs/2510.03743)
*Zachary Eberhart,Collin McMillan*

Main category: cs.SE

TL;DR: APIDA-Chat通过开源管道生成低成本训练数据，显著提升小模型在API搜索对话中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在解释小众或专有API时因缺乏多轮对话数据而表现不佳的问题。

Method: APIDA-Chat分为两个阶段：第一阶段结合传统对话规划器与高性能教师LLM生成高质量对话数据，第二阶段利用微调后的学生模型实现低成本对话生成。

Result: 微调后的学生模型在BLEU和BERTScore上分别从0.38提升至0.50和0.88提升至0.91。

Conclusion: APIDA-Chat提供了一个开源管道，通过轻量级模型生成低成本训练数据，显著提升了小模型在API搜索对话中的表现，且完全在单个消费级GPU上运行。

Abstract: Large-language-model assistants are suitable for explaining popular APIs, yet
they falter on niche or proprietary libraries because the multi-turn dialogue
data needed for fine-tuning are scarce. We present APIDA-Chat, an open-source
pipeline that converts symbolic dialogue-act "scripts" into realistic,
domain-grounded API Search conversations using a lightweight model for
inexpensive training data generation. Phase I pairs a legacy dialogue planner
with a high-capability teacher LLM (o4-mini) to synthesize a "gold set" of
realized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on
this corpus. Phase II drops the teacher and reuses the same planner with the
fine-tuned model, allowing rapid, low-cost synthesis of new dialogues without
exposing source code to external services. The fine-tuned student improves BLEU
from 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while
running entirely on a single consumer GPU. All components are modular and
publicly released to serve as a conservative baseline for future work.
APIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a
video demo is available at https://youtu.be/YqmZBHyGbPs .

</details>


### [250] [Code4MeV2: a Research-oriented Code-completion Platform](https://arxiv.org/abs/2510.03755)
*Roham Koohestani,Parham Bateni,Aydin Ebrahimi,Behdad Etezadi,Kiarash Karimi,Maliheh Izadi*

Main category: cs.SE

TL;DR: 开源代码补全插件Code4MeV2为学术界提供透明数据收集框架，性能媲美商业工具，获用户积极评价。


<details>
  <summary>Details</summary>
Motivation: 当前AI代码补全工具的用户交互数据多为大型企业私有，阻碍了学术界的可复现研究和大规模数据分析。

Method: 采用客户端-服务器架构，设计支持内联代码补全和上下文感知聊天助手，并开发了模块化的数据收集框架。通过专家评估和8名参与者的用户研究验证工具性能。

Result: Code4MeV2在代码补全性能上达到行业水平（平均延迟200毫秒），专家和用户反馈显示其信息性和实用性。

Conclusion: Code4MeV2是一款开源、模块化的代码补全插件，旨在解决学术界在AI代码补全工具研究中面临的数据访问难题。通过其透明数据收集框架和行业级性能表现，该工具为研究人员提供了实用且可扩展的解决方案。

Abstract: The adoption of AI-powered code completion tools in software development has
increased substantially, yet the user interaction data produced by these
systems remain proprietary within large corporations. This creates a barrier
for the academic community, as researchers must often develop dedicated
platforms to conduct studies on human--AI interaction, making reproducible
research and large-scale data analysis impractical. In this work, we introduce
Code4MeV2, a research-oriented, open-source code completion plugin for
JetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a
client--server architecture and features inline code completion and a
context-aware chat assistant. Its core contribution is a modular and
transparent data collection framework that gives researchers fine-grained
control over telemetry and context gathering. Code4MeV2 achieves
industry-comparable performance in terms of code completion, with an average
latency of 200~ms. We assess our tool through a combination of an expert
evaluation and a user study with eight participants. Feedback from both
researchers and daily users highlights its informativeness and usefulness. We
invite the community to adopt and contribute to this tool. More information
about the tool can be found at https://app.code4me.me.

</details>


### [251] [A First Look at the Lifecycle of DL-Specific Self-Admitted Technical Debt](https://arxiv.org/abs/2510.03802)
*Gilberto Recupito,Vincenzo De Martino,Dario Di Nucci,Fabio Palomba*

Main category: cs.SE

TL;DR: 深度学习系统中的技术债务（SATD）研究揭示其在开发早期的引入模式和持久性，特别是在训练和硬件阶段，需针对性管理策略以提升系统质量。


<details>
  <summary>Details</summary>
Motivation: 深度学习系统的快速采用带来了软件质量和性能维护的独特挑战，尤其是自我承认的技术债务（SATD）对系统可维护性和质量的影响尚未充分探索。

Method: 采用软件仓库挖掘技术，分析了40个机器学习项目中的185个深度学习特定SATD实例，通过项目提交历史追踪SATD的引入和持久性。

Result: 研究发现深度学习特定SATD主要在项目开发的早期和中期阶段引入，训练和硬件阶段显示出最长的SATD持续时间，开发者更常在功能实现和错误修复时引入SATD。

Conclusion: 本研究强调了在深度学习系统中针对特定技术债务（SATD）管理的必要性，通过理解其时间特性和演变，开发者可以在关键阶段优先干预，以提升系统的可维护性和质量。

Abstract: The rapid adoption of Deep Learning (DL)-enabled systems has revolutionized
software development, driving innovation across various domains. However, these
systems also introduce unique challenges, particularly in maintaining software
quality and performance. Among these challenges, Self-Admitted Technical Debt
(SATD) has emerged as a growing concern, significantly impacting the
maintainability and overall quality of ML and DL-enabled systems. Despite its
critical implications, the lifecycle of DL-specific SATD, how developers
introduce, acknowledge, and address it over time-remains underexplored. This
study presents a preliminary analysis of the persistence and lifecycle of
DL-specific SATD in DL-enabled systems. The purpose of this project is to
uncover the patterns of SATD introduction, recognition, and durability during
the development life cycle, providing information on how to manage these
issues. Using mining software repository techniques, we examined 40 ML
projects, focusing on 185 DL-specific SATD instances. The analysis tracked the
introduction and persistence of SATD instances through project commit histories
to assess their lifecycle and developer actions. The findings indicate that
DL-specific SATD is predominantly introduced during the early and middle stages
of project development. Training and Hardware phases showed the longest SATD
durations, highlighting critical areas where debt accumulates and persists.
Additionally, developers introduce DL-specific SATD more frequently during
feature implementation and bug fixes. This study emphasizes the need for
targeted DL-specific SATD management strategies in DL-enabled systems to
mitigate its impact. By understanding the temporal characteristics and
evolution of DL-specific SATD, developers can prioritize interventions at
critical stages to improve the maintainability and quality of the system.

</details>


### [252] [Smart Paste: Automatically Fixing Copy/Paste for Google Developers](https://arxiv.org/abs/2510.03843)
*Vincent Nguyen,Guilherme Herzog,José Cambronero,Marcus Revaj,Aditya Kini,Alexander Frömmgen,Maxim Tabachnyk*

Main category: cs.SE

TL;DR: Smart Paste是一个IDE功能，通过深度学习预测粘贴代码后的编辑需求，在Google内部获得45%的接受率，占公司代码编写的1%。


<details>
  <summary>Details</summary>
Motivation: 手动编辑粘贴的代码是开发者的痛点，Google内部数据显示代码粘贴频率是手动输入的4倍，且经常需要后续编辑。

Method: 通过迭代开发和规模化，将Smart Paste集成到IDE中，提供粘贴后的编辑建议。

Result: Smart Paste的接受率为45%，占Google公司范围内所有编写代码的1%以上。

Conclusion: Smart Paste在Google的开发环境中成功部署，并获得了积极的反馈，45%的接受率表明其在实际应用中的有效性。

Abstract: Manually editing pasted code is a long-standing developer pain point. In
internal software development at Google, we observe that code is pasted 4 times
more often than it is manually typed. These paste actions frequently require
follow-up edits, ranging from simple reformatting and renaming to more complex
style adjustments and cross-language translations. Prior work has shown deep
learning can be used to predict these edits. In this work, we show how to
iteratively develop and scale Smart Paste, an IDE feature for post-paste edit
suggestions, to Google's development environment. This experience can serve as
a guide for AI practitioners on a holistic approach to feature development,
covering user experience, system integration, and model capabilities. Since
deployment, Smart Paste has had overwhelmingly positive feedback with a 45%
acceptance rate. At Google's enterprise scale, these accepted suggestions
account substantially for over 1% of all code written company-wide.

</details>


### [253] [Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework](https://arxiv.org/abs/2510.03862)
*Nathalia Nascimento,Everton Guimaraes,Paulo Alencar*

Main category: cs.SE

TL;DR: 本文提出一个标准化评估LLM代码生成的理论框架，旨在解决当前研究缺乏可比性和可重复性的问题。


<details>
  <summary>Details</summary>
Motivation: 目前，基于LLM的代码生成的实证评估缺乏标准化，研究在目标、任务和指标上差异很大，限制了可比性和可重复性。

Method: 框架基于作者先前进行此类实验的经验以及对近期研究关键异同点的比较分析。它围绕问题来源、质量属性和指标等核心组件组织评估，支持结构化和系统化的实验。

Result: 通过代表性案例映射展示了框架的适用性，并确定了改进的机会。

Conclusion: 本文提出了一个理论框架，用于设计和报告基于大语言模型（LLM）的代码生成实证研究，并计划将该框架发展为更成熟的工具，以标准化LLM在软件工程环境中的评估。

Abstract: The rise of large language models (LLMs) has introduced transformative
potential in automated code generation, addressing a wide range of software
engineering challenges. However, empirical evaluation of LLM-based code
generation lacks standardization, with studies varying widely in goals, tasks,
and metrics, which limits comparability and reproducibility. In this paper, we
propose a theoretical framework for designing and reporting empirical studies
on LLM-based code generation. The framework is grounded in both our prior
experience conducting such experiments and a comparative analysis of key
similarities and differences among recent studies. It organizes evaluation
around core components such as problem sources, quality attributes, and
metrics, supporting structured and systematic experimentation. We demonstrate
its applicability through representative case mappings and identify
opportunities for refinement. Looking forward, we plan to evolve the framework
into a more robust and mature tool for standardizing LLM evaluation across
software engineering contexts.

</details>


### [254] [Adversarial Agent Collaboration for C to Rust Translation](https://arxiv.org/abs/2510.03879)
*Tianyu Li,Ruishi Li,Bo Wang,Brandon Paulsen,Umang Mathur,Prateek Saxena*

Main category: cs.SE

TL;DR: ACToR是一种对抗性C到Rust翻译方法，通过生成器与判别器代理的协作，成功翻译中等规模C代码，测试通过率高且无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 现有C到Rust的翻译方法（包括LLM辅助）无法泛化到较大（>500行代码）的C代码库，因其依赖复杂且易失效的程序分析。

Method: ACToR采用对抗性方法，通过生成器与判别器代理的协作迭代生成Rust翻译：生成器合成并优化翻译以通过测试，判别器则发现新的失败测试。

Result: ACToR成功翻译了63个平均485行代码的真实命令行工具，测试通过率超过90%，且无需人工干预。

Conclusion: ACToR 是首个能够可靠地将中等规模（平均485行代码）的C程序翻译为Rust的系统，且在零人工干预下达到90%以上的测试通过率，相比非对抗性基线方法提升了18.9%的正确性。

Abstract: Translating C to memory-safe languages, like Rust, prevents critical memory
safety vulnerabilities that are prevalent in legacy C software. Existing
approaches for C to safe Rust translation, including LLM-assisted ones, do not
generalize on larger (> 500 LoC) C codebases because they depend on complex
program analyses that frequently break. In this work, we present ACToR
(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired
by GANs, ACToR pits a generator agent against a discriminator agent, which
collaborate to iteratively generate a Rust translation. On each iteration, the
translator agent synthesizes and refines a Rust translation to pass an existing
suite of tests, and then the discriminator agent finds new failing tests. We
demonstrate that ACToR translates all of the 63 real-world command line
utilities considered in our benchmarks, which have an average size of 485 lines
of code, and it achieves over 90% test pass rate with zero human intervention.
To our knowledge, it is the first such system that reliably translates C
programs of this scale. Furthermore, ACToR improves translation correctness by
up to 18.9% compared to baseline, non-adversarial approaches.

</details>


### [255] [Rethinking Services in the Quantum Age: The SOQ Paradigm](https://arxiv.org/abs/2510.03890)
*Jose Garcia-Alonso,Enrique Moguel,Jaime Alvarado-Valiente,Javier Romero-Alvarez,Álvaro M. Aparicio-Morales,Juan M. Murillo,Francisco Javier Cavero,Adrián Romero-Flores,Alfonso E. Marquez-Chamorro,José Antonio Parejo,Antonio Ruiz-Cortés,Giuseppe Bisicchia,Alessandro Bocci,Antonio Brogi*

Main category: cs.SE

TL;DR: SOQ是一种新的服务导向量子计算范式，旨在通过自主、可组合的量子服务解决量子计算集成到现实系统中的挑战。


<details>
  <summary>Details</summary>
Motivation: 量子计算在优化、模拟、密码学和机器学习等领域展现出显著的计算优势，但其在现实软件系统中的集成受到硬件脆弱性、平台异构性和缺乏稳健软件工程实践的限制。

Method: 论文介绍了SOQ的基本原理，提出了一个分层的技术栈来支持其实现，并识别了包括互操作性、混合性、定价模型、服务抽象和人才培养在内的关键研究与工程挑战。

Result: SOQ范式为量子技术的进步提供了重要支持，因为它能够实现量子计算的可扩展、模块化和可互操作集成，而无需依赖专用的经典环境来管理量子处理。

Conclusion: SOQ提出了一种创新的服务导向量子计算范式，通过将量子服务定位为自主、可组合且可互操作的实体，解决了量子计算集成到现实软件系统中的关键挑战。

Abstract: Quantum computing is rapidly progressing from theoretical promise to
practical implementation, offering significant computational advantages for
tasks in optimization, simulation, cryptography, and machine learning. However,
its integration into real-world software systems remains constrained by
hardware fragility, platform heterogeneity, and the absence of robust software
engineering practices. This paper introduces Service-Oriented Quantum (SOQ), a
novel paradigm that reimagines quantum software systems through the lens of
classical service-oriented computing. Unlike prior approaches such as Quantum
Service-Oriented Computing (QSOC), which treat quantum capabilities as
auxiliary components within classical systems, SOQ positions quantum services
as autonomous, composable, and interoperable entities. We define the
foundational principles of SOQ, propose a layered technology stack to support
its realization, and identify the key research and engineering challenges that
must be addressed, including interoperability, hybridity, pricing models,
service abstractions, and workforce development. This approach is of vital
importance for the advancement of quantum technology because it enables the
scalable, modular, and interoperable integration of quantum computing into
real-world software systems independently and without relying on a dedicated
classical environment to manage quantum processing.

</details>


### [256] [A Brief History of the Waterfall Model: Past, Present, and Future](https://arxiv.org/abs/2510.03894)
*Antonios Saravanos*

Main category: cs.SE

TL;DR: 本文回顾并分析了瀑布模型的历史、批评及现代适应性，认为其在混合开发框架中仍具价值。


<details>
  <summary>Details</summary>
Motivation: 探讨瀑布模型在当代软件开发中的持续影响和适应性，旨在重新评估其价值和在现代混合方法论中的角色。

Method: 本研究通过历史回顾和批判性分析，综合了瀑布模型的概念起源、Royce的形式化描述以及其在行业中的演变和批评。

Result: 分析表明，瀑布模型已从独立框架转变为现代混合方法论的组成部分，其原则仍影响当前的开发实践。

Conclusion: 瀑布模型虽然常被批评其僵化性和高失败率，但在特定领域仍然适用，并通过融入现代混合开发框架保持其相关性。其持久价值在于适应性和作为情境感知开发策略的一部分。

Abstract: The waterfall model, one of the earliest software development methodologies,
has played a foundational role in shaping contemporary software engineering
practices. This paper provides a historical and critical overview of the model,
tracing its conceptual origins in software engineering, its formalization by
Royce, and its evolution through decades of industry adoption and critique.
Although often criticized for its rigidity, shortcomings, and high failure
rates, the waterfall model persists in specific domains. Its principles
continue to influence contemporary hybrid development frameworks that combine
traditional and agile methods. Drawing on a range of scholarly sources, this
study synthesizes key developments in the perception and application of the
waterfall model. The analysis highlights how the model has shifted from a
standalone framework to a component within modern hybrid methodologies. By
revisiting its origins, assessing its present utility, and examining its role
in contemporary development practices, this paper argues that the waterfall
model remains relevant, not as a relic of the past but as part of context-aware
development strategies. The paper contends that the model's enduring relevance
lies in its adaptability. By recognizing both its limitations and its
strengths, and by understanding its integration within hybrid approaches,
practitioners can make more informed decisions about methodology selection and
process design in diverse development environments.

</details>


### [257] [Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code](https://arxiv.org/abs/2510.03902)
*Rana Nameer Hussain Khan,Dawood Wasif,Jin-Hee Cho,Ali Butt*

Main category: cs.SE

TL;DR: MACOG是一种多智能体LLM架构，通过模块化协作生成语法正确、合规且可扩展的IaC配置，显著优于传统单次生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成IaC时存在单次生成导致的语法错误、策略违规和设计不可扩展等问题，需更可靠的解决方案。

Method: 提出MACOG架构，将IaC生成任务分解为多个模块化子任务，由不同专业智能体（如Architect、Provider Harmonizer等）协作完成，并通过共享黑板和状态协调器交互。结合Terraform Plan和OPA确保生成的配置正确且合规。

Result: 在IaC-Eval基准测试中，MACOG显著提升模型性能（如GPT-5从54.90提升至74.02），并在BLEU、CodeBERTScore等指标上表现优异。

Conclusion: MACOG通过多智能体协作架构显著提升了IaC生成的准确性、合规性和可扩展性，验证了其在复杂云基础设施管理中的有效性。

Abstract: The increasing complexity of cloud-native infrastructure has made
Infrastructure-as-Code (IaC) essential for reproducible and scalable
deployments. While large language models (LLMs) have shown promise in
generating IaC snippets from natural language prompts, their monolithic,
single-pass generation approach often results in syntactic errors, policy
violations, and unscalable designs. In this paper, we propose MACOG
(Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based
architecture for IaC generation that decomposes the task into modular subtasks
handled by specialized agents: Architect, Provider Harmonizer, Engineer,
Reviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory
Curator. The agents interact via a shared-blackboard, finite-state orchestrator
layer, and collectively produce Terraform configurations that are not only
syntactically valid but also policy-compliant and semantically coherent. To
ensure infrastructure correctness and governance, we incorporate Terraform Plan
for execution validation and Open Policy Agent (OPA) for customizable policy
enforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the
top enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02
and Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU,
CodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and
deploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93,
respectively.

</details>


### [258] [Refactoring with LLMs: Bridging Human Expertise and Machine Understanding](https://arxiv.org/abs/2510.03914)
*Yonnel Chen Kuang Piao,Jean Carlors Paul,Leuson Da Silva,Arghavan Moradi Dakhel,Mohammad Hamdaqa,Foutse Khomh*

Main category: cs.SE

TL;DR: 研究探讨了基于人类最佳实践的指令策略是否能增强LLMs执行多样化重构任务的能力，结果表明基于Fowler指南的指令设计有效提升了LLMs的重构表现。


<details>
  <summary>Details</summary>
Motivation: 开发人员常因重构所需的时间、精力和资源以及缺乏即时功能性回报而忽视重构，现有自动化重构工具支持的重构类型有限。

Method: 研究利用GPT-mini和DeepSeek-V3等先进LLMs的指令跟随和代码理解能力，设计了多种指令策略，涵盖61种常见重构类型，并在基准测试和GitHub真实代码片段上进行了评估。

Result: 结果表明，基于Fowler指南的指令设计使LLMs能够成功执行所有基准重构类型，并在真实场景中保持程序语义。此外，规则型指令在特定场景下表现更优，而关注重构整体目标而非固定变换类型能进一步提升代码质量。

Conclusion: 基于Martin Fowler重构指南的指令设计能够有效提升LLMs在多种重构任务中的表现，尤其是在保持程序语义和提升代码质量方面。

Abstract: Code refactoring is a fundamental software engineering practice aimed at
improving code quality and maintainability. Despite its importance, developers
often neglect refactoring due to the significant time, effort, and resources it
requires, as well as the lack of immediate functional rewards. Although several
automated refactoring tools have been proposed, they remain limited in
supporting a broad spectrum of refactoring types. In this study, we explore
whether instruction strategies inspired by human best-practice guidelines can
enhance the ability of Large Language Models (LLMs) to perform diverse
refactoring tasks automatically. Leveraging the instruction-following and code
comprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and
DeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design
multiple instruction strategies that encode motivations, procedural steps, and
transformation objectives for 61 well-known refactoring types. We evaluate
these strategies on benchmark examples and real-world code snippets from GitHub
projects. Our results show that instruction designs grounded in Fowler's
guidelines enable LLMs to successfully perform all benchmark refactoring types
and preserve program semantics in real-world settings, an essential criterion
for effective refactoring. Moreover, while descriptive instructions are more
interpretable to humans, our results show that rule-based instructions often
lead to better performance in specific scenarios. Interestingly, allowing
models to focus on the overall goal of refactoring, rather than prescribing a
fixed transformation type, can yield even greater improvements in code quality.

</details>


### [259] [Why Does the Engineering Manager Still Exist in Agile Software Development?](https://arxiv.org/abs/2510.03920)
*Ravi Kalluri*

Main category: cs.SE

TL;DR: 论文探讨了敏捷组织中工程经理持续存在的原因，提出了一个概念模型来调和敏捷原则与管理需求，并讨论了实践和研究的意义。


<details>
  <summary>Details</summary>
Motivation: 尽管敏捷方法论强调分散决策和团队自主性，但工程经理在敏捷软件组织中仍然存在，这种矛盾现象促使研究探索其原因和影响。

Method: 通过系统的文献综述支持的多面分析，并辅以案例研究。

Result: 研究揭示了工程经理在敏捷环境中的持续存在，并提出了一种理论框架来解释这一现象。

Conclusion: 论文提出了一个概念模型，将敏捷原则与管理必要性相结合，为从业者、研究人员和工具设计师提供了指导。

Abstract: Although Agile methodologies emphasize decentralized decision-making and team
autonomy, engineering managers continue to be employed in Agile software
organizations. This apparent paradox suggests that traditional managerial
functions persist despite the theoretical displacement of managerial hierarchy
in Agile. This paper explores the persistence of engineering managers through a
multidimensional framework encompassing historical context, theoretical
tensions, organizational realities, empirical evidence, evolving managerial
roles, and practical implications. A systematic literature review underpins our
multifaceted analysis, supplemented by illustrative case studies. We conclude
by proposing a conceptual model that reconciles Agile principles with
managerial necessity, offering guidance for practitioners, researchers, and
tool designers. Implications for leadership development, tool integration, and
future research are discussed.

</details>


### [260] [Bamboo: LLM-Driven Discovery of API-Permission Mappings in the Android Framework](https://arxiv.org/abs/2510.04078)
*Han Hu,Wei Minn,Yonghui Liu,Jiakun Liu,Ferdian Thung,Terry Yue Zhuo,Lwin Khin Shar,Debin Gao,David Lo*

Main category: cs.SE

TL;DR: 该论文提出了一种基于LLMs的新方法，用于改进Android API权限映射的发现，开发了工具\tool{}，并在多个Android版本中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: Android官方API文档在权限描述上存在不精确和不完整的问题，导致开发者在声明权限时容易出错，进而引发安全风险和应用程序故障。现有方法在适应性和覆盖范围上存在不足。

Method: 利用大型语言模型（LLMs）、双角色提示策略和API驱动的代码生成方法，开发了名为\tool{}的工具，用于系统性地分析API-权限映射。

Result: 实验结果表明，\tool{}在Android版本6、7和10中分别识别了2,234、3,552和4,576个API-权限映射，显著优于现有基线方法。

Conclusion: 该论文通过引入大型语言模型（LLMs）和创新的双角色提示策略，显著提升了API权限映射的准确性和覆盖率，为Android开发者和安全研究人员提供了更可靠的权限管理工具。

Abstract: The permission mechanism in the Android Framework is integral to safeguarding
the privacy of users by managing users' and processes' access to sensitive
resources and operations. As such, developers need to be equipped with an
in-depth understanding of API permissions to build robust Android apps.
Unfortunately, the official API documentation by Android chronically suffers
from imprecision and incompleteness, causing developers to spend significant
effort to accurately discern necessary permissions. This potentially leads to
incorrect permission declarations in Android app development, potentially
resulting in security violations and app failures. Recent efforts in improving
permission specification primarily leverage static and dynamic code analyses to
uncover API-permission mappings within the Android framework. Yet, these
methodologies encounter substantial shortcomings, including poor adaptability
to Android SDK and Framework updates, restricted code coverage, and a
propensity to overlook essential API-permission mappings in intricate
codebases. This paper introduces a pioneering approach utilizing large language
models (LLMs) for a systematic examination of API-permission mappings. In
addition to employing LLMs, we integrate a dual-role prompting strategy and an
API-driven code generation approach into our mapping discovery pipeline,
resulting in the development of the corresponding tool, \tool{}. We formulate
three research questions to evaluate the efficacy of \tool{} against
state-of-the-art baselines, assess the completeness of official SDK
documentation, and analyze the evolution of permission-required APIs across
different SDK releases. Our experimental results reveal that \tool{} identifies
2,234, 3,552, and 4,576 API-permission mappings in Android versions 6, 7, and
10 respectively, substantially outprforming existing baselines.

</details>


### [261] [GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization](https://arxiv.org/abs/2510.04135)
*Jingzhi Gong,Yixin Bian,Luis de la Cal,Giovanni Pinna,Anisha Uteem,David Williams,Mar Zamorano,Karine Even-Mendoza,W. B. Langdon,Hector Menendez,Federica Sarro*

Main category: cs.SE

TL;DR: GA4GC框架优化编码代理的可持续性和性能，减少运行时37.7%并提高正确性，温度是最关键的超参数。


<details>
  <summary>Details</summary>
Motivation: 解决LLM驱动的编码代理在工业部署中面临的可持续性和可扩展性挑战，如单次运行消耗超过100k tokens和高环境成本。

Method: 引入GA4GC框架，系统优化编码代理运行时（更环保的代理）和代码性能（更环保的代码）的权衡，通过发现Pareto最优的代理超参数和提示模板。

Result: 在SWE-Perf基准测试中，GA4GC实现了高达135倍的超体积改进，减少了37.7%的代理运行时，同时提高了正确性。

Conclusion: GA4GC框架通过发现Pareto最优的代理超参数和提示模板，有效平衡了编码代理运行时与代码性能的权衡，为工业部署中的可持续性和代码优化提供了实用策略。

Abstract: Coding agents powered by LLMs face critical sustainability and scalability
challenges in industrial deployment, with single runs consuming over 100k
tokens and incurring environmental costs that may exceed optimization benefits.
This paper introduces GA4GC, the first framework to systematically optimize
coding agent runtime (greener agent) and code performance (greener code)
trade-offs by discovering Pareto-optimal agent hyperparameters and prompt
templates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x
hypervolume improvement, reducing agent runtime by 37.7% while improving
correctness. Our findings establish temperature as the most critical
hyperparameter, and provide actionable strategies to balance agent
sustainability with code optimization effectiveness in industrial deployment.

</details>


### [262] [Detecting Semantic Clones of Unseen Functionality](https://arxiv.org/abs/2510.04143)
*Konstantinos Kitsios,Francesco Sovrano,Earl T. Barr,Alberto Bacchelli*

Main category: cs.SE

TL;DR: 论文发现现有代码克隆检测模型在未见功能克隆上表现不佳，提出对比学习方法显著提升了任务特定模型和LLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 开发者对代码克隆检测的需求不仅限于模型训练时见过的功能克隆，还包括未见过的功能克隆。现有的神经模型在检测未见功能克隆时表现不佳，F1分数显著下降。

Method: 论文重新评估了六种最先进的模型（包括任务特定模型和生成式LLMs）在未见功能克隆检测任务上的表现，并提出了对比学习方法来改进这些模型的性能。对于任务特定模型，作者替换了最终分类器为对比分类器；对于生成式LLMs，提出了对比上下文学习方法。

Result: 实验结果显示，任务特定模型在未见功能克隆上的F1分数下降高达48%（平均31%），而LLMs表现与任务特定模型相当但泛化能力更强，F1分数仅下降5%（平均3%）。通过对比学习，任务特定模型的F1分数提升了高达26%（平均9%），LLMs提升了5%（平均3%）。

Conclusion: 该论文提出并评估了对比学习在改进现有模型对未见功能克隆检测性能上的有效性，特别是对于任务特定模型和生成式LLMs。实验结果显示，对比学习显著提升了模型在未见功能克隆上的F1分数。

Abstract: Semantic code clone detection is the task of detecting whether two snippets
of code implement the same functionality (e.g., Sort Array). Recently, many
neural models achieved near-perfect performance on this task. These models seek
to make inferences based on their training data. Consequently, they better
detect clones similar to those they have seen during training and may struggle
to detect those they have not. Developers seeking clones are, of course,
interested in both types of clones. We confirm this claim through a literature
review, identifying three practical clone detection tasks in which the model's
goal is to detect clones of a functionality even if it was trained on clones of
different functionalities. In light of this finding, we re-evaluate six
state-of-the-art models, including both task-specific models and generative
LLMs, on the task of detecting clones of unseen functionality. Our experiments
reveal a drop in F1 of up to 48% (average 31%) for task-specific models. LLMs
perform on par with task-specific models without explicit training for clone
detection, but generalize better to unseen functionalities, where F1 drops up
to 5% (average 3%) instead. We propose and evaluate the use of contrastive
learning to improve the performance of existing models on clones of unseen
functionality. We draw inspiration from the computer vision and natural
language processing fields where contrastive learning excels at measuring
similarity between two objects, even if they come from classes unseen during
training. We replace the final classifier of the task-specific models with a
contrastive classifier, while for the generative LLMs we propose contrastive
in-context learning, guiding the LLMs to focus on the differences between
clones and non-clones. The F1 on clones of unseen functionality is improved by
up to 26% (average 9%) for task-specific models and up to 5% (average 3%) for
LLMs.

</details>


### [263] [Multi Language Models for On-the-Fly Syntax Highlighting](https://arxiv.org/abs/2510.04166)
*Marco Edoardo Palma,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: 本文提出了一种支持多语言的统一语法高亮模型，通过深度学习和创新技术降低了部署复杂性并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在线和基于Web的开发工具在实时语法高亮方面面临严格的时间和内存限制，现有模型存在语言支持单一、数据集需求大、训练资源密集等问题。

Method: 采用深度学习方法，通过Deep Abstraction过程将暴力语法高亮解析器的行为编码为快速的统计模型。结合归一化技术和少量样本学习，优化模型训练和性能。

Result: 提出的统一模型将部署复杂性降低了六倍，并在未见语言上表现出更好的性能。归一化技术和少量样本学习显著提升了模型的泛化能力。

Conclusion: 本文通过引入一种统一模型，能够支持多达六种主流编程语言的语法高亮，显著降低了部署复杂性并提升了性能。此外，创新的归一化技术和少量样本学习实验进一步增强了模型的泛化能力，减少了对暴力生成器的依赖。

Abstract: Syntax highlighting is a critical feature in modern software development
environments, enhancing code readability and developer productivity. However,
delivering accurate highlighting in real time remains challenging for online
and web-based development tools due to strict time and memory constraints on
backend services. These systems must serve highlights rapidly and frequently,
even when code is partially valid or invalid. This has led to on-the-fly syntax
highlighting, where visual annotations are generated just before content is
served, often at high request rates and under incomplete input conditions. To
meet these demands efficiently, state-of-the-art models use deep learning to
learn the behavior of brute-force syntax highlighting resolvers, tools that are
easy to implement but too slow for production. Through the Deep Abstraction
process, brute-force strategies are encoded into fast statistical models that
achieve both high accuracy and low-latency inference. Despite their success,
such models face key challenges: they support only one programming language per
model, require large datasets from slow brute-force generators, and involve
resource-intensive training. In multi-language environments, this means
maintaining multiple independent models, increasing system complexity and
operational cost. This work addresses these issues by introducing a unified
model capable of highlighting up to six mainstream programming languages,
reducing deployment complexity by a factor of six and improving performance on
unseen languages. A novel normalization technique significantly enhances model
generalization, while few-shot learning experiments show that a small number of
oracle samples can replace large datasets, minimizing dependence on brute-force
generators. Combined, these innovations enable efficient, scalable, and
cost-effective syntax highlighting across diverse programming languages.

</details>


### [264] [Selecting Cybersecurity Requirements: Effects of LLM Use and Professional Software Development Experience](https://arxiv.org/abs/2510.04274)
*Damjan Fujs,Damjan Vavpotič,Tomaž Hovelja,Marko Poženel*

Main category: cs.SE

TL;DR: LLM支持未显著影响网络安全需求评估，但专业经验在多个标准上造成差异。


<details>
  <summary>Details</summary>
Motivation: 探讨访问LLMs和不同专业软件开发经验如何影响对网络安全需求的优先排序。

Method: 23名研究生参与研究，使用MoSCoW方法优先排序安全需求，并根据多个评估标准对提出的解决方案进行评分。参与者被分为两组，一组在任务期间有LLM支持，另一组没有。

Result: LLM使用未显示出显著差异，但经验组在开发成本、用户体验影响和风险评估等标准上存在显著差异。经验更丰富的参与者对用户体验影响评分更高，对风险估计更低。

Conclusion: 研究发现，访问大型语言模型（LLMs）对参与者评估网络安全解决方案的方式没有显著影响，但专业软件开发经验的不同在多个标准上表现出显著差异。

Abstract: This study investigates how access to Large Language Models (LLMs) and
varying levels of professional software development experience affect the
prioritization of cybersecurity requirements for web applications. Twenty-three
postgraduate students participated in a research study to prioritize security
requirements (SRs) using the MoSCoW method and subsequently rated their
proposed solutions against multiple evaluation criteria. We divided
participants into two groups (one with and the other without access to LLM
support during the task). Results showed no significant differences related to
LLM use, suggesting that access to LLMs did not noticeably influence how
participants evaluated cybersecurity solutions. However, statistically
significant differences emerged between experience groups for certain criteria,
such as estimated cost to develop a feature, perceived impact on user
experience, and risk assessment related to non-implementation of the proposed
feature. Participants with more professional experience tended to provide
higher ratings for user experience impact and lower risk estimates.

</details>


### [265] [Challenge on Optimization of Context Collection for Code Completion](https://arxiv.org/abs/2510.04349)
*Dmitry Ustalov,Egor Bogomolov,Alexander Bezzubov,Yaroslav Golubev,Evgeniy Glukhov,Georgii Levtsov,Vladimir Kovalenko*

Main category: cs.SE

TL;DR: 本研究通过竞赛形式评估了优化上下文收集机制对代码补全质量的提升，展示了在Python和Kotlin中的实际效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI在软件工程中的广泛应用，需要系统评估其在大型代码库中利用项目整体信息的能力。

Method: 参与者开发了高效的上下文收集机制，利用来自开源项目的真实代码数据集，通过chrF指标评估多个最先进神经模型的补全质量。

Result: 公共阶段有19个团队提交了Python解决方案，8个团队提交了Kotlin解决方案；私有阶段有6个团队竞争，其中5个提交了论文。

Conclusion: 比赛展示了在大型代码库中优化上下文收集机制的潜力，参与者开发的解决方案显著提升了填充中间代码补全的质量，尤其是在Python和Kotlin中。

Abstract: The rapid advancement of workflows and methods for software engineering using
AI emphasizes the need for a systematic evaluation and analysis of their
ability to leverage information from entire projects, particularly in large
code bases. In this challenge on optimization of context collection for code
completion, organized by JetBrains in collaboration with Mistral AI as part of
the ASE 2025 conference, participants developed efficient mechanisms for
collecting context from source code repositories to improve fill-in-the-middle
code completions for Python and Kotlin. We constructed a large dataset of
real-world code in these two programming languages using permissively licensed
open-source projects. The submissions were evaluated based on their ability to
maximize completion quality for multiple state-of-the-art neural models using
the chrF metric. During the public phase of the competition, nineteen teams
submitted solutions to the Python track and eight teams submitted solutions to
the Kotlin track. In the private phase, six teams competed, of which five
submitted papers to the workshop.

</details>


### [266] [MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models](https://arxiv.org/abs/2510.04363)
*Hyunjun Kim,Sejong Kim*

Main category: cs.SE

TL;DR: MacroBench是一个评估LLMs合成浏览器自动化程序的基准测试，结果显示模型在简单任务上表现良好，但在复杂任务上完全失败。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估LLMs在合成可重用浏览器自动化程序方面的能力，以填补现有基准测试的空白。

Method: MacroBench实例化了七个自托管站点，覆盖681个任务，通过静态检查、沙盒执行和结果验证（包括DOM断言和数据库快照）来验证生成的代码。

Result: GPT-4o-Mini成功率为96.8%，GPT-4.1为95.3%，Gemini-2.5-Pro为89.0%，DeepSeek-V3.1为83.4%。模型在简单任务上表现良好，但在复杂工作流上完全失败。

Conclusion: 论文提出了MacroBench，一个评估LLMs从自然语言目标合成可重用浏览器自动化程序能力的代码优先基准测试。尽管模型在简单任务上表现良好（91.7%成功率），但在复杂工作流上完全失败（0.0%成功率），且均未达到生产级编码实践。

Abstract: We introduce MacroBench, a code-first benchmark that evaluates whether LLMs
can synthesize reusable browser automation programs from natural language goals
by reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates
seven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,
Facebook-like, Discord-like, and Threads-like, covering 681 tasks across
interaction complexity and targeting difficulty. Our end-to-end protocol
validates generated code via static checks, sandboxed execution, and outcome
verification including DOM assertions and database snapshots, and includes a
safety suite for scraping, spam/abuse, and credential/privacy prompts. Across
2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8
percent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,
and DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at
91.7 percent but fail on complex workflows at 0.0 percent, and none meet
production-quality coding practices despite functional completion. We release
our complete benchmark pipeline, evaluation framework, and experimental results
to enable reproducible assessment of macro synthesis for web automation.

</details>


### [267] [Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development](https://arxiv.org/abs/2510.04380)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: AI在需求工程中的应用可提升效率和准确性，但也带来伦理和透明度等挑战，需加强学术与行业合作以创建可信赖的解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管RE在软件开发中扮演关键角色，但仍面临诸如模糊性、利益相关者需求冲突等挑战，AI被认为有潜力提升RE过程的效率和准确性。

Method: 探讨了AI如何通过自动化劳动密集型任务、支持需求优先级排序以及促进利益相关者与AI系统之间的协作来增强传统RE实践。

Result: 论文详细描述了AI为RE带来的机遇与挑战，包括自动化、需求优先级支持和协作促进，同时也指出了伦理、偏见和透明度等新问题。

Conclusion: 该论文强调在需求工程中应用AI时需注重伦理实践，并加强学术界与行业专业人士的合作，旨在创建既强大又可信赖且实用的AI解决方案。

Abstract: Requirement Engineering (RE) is the foundation of successful software
development. In RE, the goal is to ensure that implemented systems satisfy
stakeholder needs through rigorous requirements elicitation, validation, and
evaluation processes. Despite its critical role, RE continues to face
persistent challenges, such as ambiguity, conflicting stakeholder needs, and
the complexity of managing evolving requirements. A common view is that
Artificial Intelligence (AI) has the potential to streamline the RE process,
resulting in improved efficiency, accuracy, and management actions. However,
using AI also introduces new concerns, such as ethical issues, biases, and lack
of transparency. This paper explores how AI can enhance traditional RE
practices by automating labor-intensive tasks, supporting requirement
prioritization, and facilitating collaboration between stakeholders and AI
systems. The paper also describes the opportunities and challenges that AI
brings to RE. In particular, the vision calls for ethical practices in AI,
along with a much-enhanced collaboration between academia and industry
professionals. The focus should be on creating not only powerful but also
trustworthy and practical AI solutions ready to adapt to the fast-paced world
of software development.

</details>


### [268] [Smart Hiring Redefined: An Intelligent Recruitment Management Platform](https://arxiv.org/abs/2510.04437)
*Fangzhe Wu,Dongyang Lyu,Xiaoqi Li*

Main category: cs.SE

TL;DR: 智能招聘管理系统通过自动化和数据驱动提升招聘效率，解决传统模式的高成本和低效问题。


<details>
  <summary>Details</summary>
Motivation: 传统招聘模式因效率有限、成本高和信息不对称，难以满足企业对精准人才获取的日益增长需求。

Method: 通过自动化和数据驱动的方法，包括简历快速解析、候选人智能匹配和面试流程自动调度。

Result: 智能招聘系统显著提升了招聘效率和准确性，优化了招聘流程，降低了劳动力和时间成本。

Conclusion: 智能招聘管理系统通过自动化和数据驱动的方法显著提升了招聘效率和准确性，成为现代组织人才战略不可或缺的一部分。

Abstract: Against the backdrop of deepening digital and intelligent transformation in
human resource management, traditional recruitment models struggle to fully
meet enterprises' growing demand for precise talent acquisition due to limited
efficiency, high costs, and information asymmetry. As a vital tool for
optimizing recruitment processes, reducing labor and time costs, and enhancing
core competitiveness, intelligent recruitment management systems become an
indispensable component of modern organizational talent strategies.Compared
with the labor intensive tasks of resume screening, candidate position
matching, and interview coordination in traditional manual recruitment,
intelligent recruitment systems significantly enhance the efficiency and
accuracy of the hiring process through automation and data driven approaches.
These systems enable rapid parsing of massive resume volumes, intelligent
matching of candidates to positions, and automated scheduling of interview
processes.

</details>


### [269] [Improving IR-based Bug Localization with Semantics-Driven Query Reduction](https://arxiv.org/abs/2510.04468)
*Asif Mohammed Samir,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: IQLoc结合IR与LLM方法，通过Transformer模型理解程序语义，显著提升缺陷定位性能，尤其在处理复杂缺陷报告时表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于信息检索（IR）的方法在匹配源代码与缺陷报告时忽略了代码的上下文和语义，而大型语言模型（LLM）虽在理解文本和代码方面表现优异，但尚未很好地适应缺陷定位任务且可能资源密集。

Method: 提出IQLoc，一种新颖的缺陷定位方法，结合IR和基于LLM的方法的优势，利用基于Transformer的模型理解程序语义，并在缺陷定位过程中通过信息检索重新制定查询。

Result: 在Bench4BL基准数据集上评估，IQLoc在MAP、MRR和HIT@K三个性能指标上均优于四种基线技术，尤其对于包含堆栈跟踪、代码元素或仅自然语言描述的缺陷报告，改进幅度显著。

Conclusion: 通过结合程序语义理解与信息检索，IQLoc有效缓解了传统基于IR方法在软件缺陷定位中的长期挑战，展现出显著优势。

Abstract: Despite decades of research, software bug localization remains challenging
due to heterogeneous content and inherent ambiguities in bug reports. Existing
methods such as Information Retrieval (IR)-based approaches often attempt to
match source documents to bug reports, overlooking the context and semantics of
the source code. On the other hand, Large Language Models (LLM) (e.g.,
Transformer models) show promising results in understanding both texts and
code. However, they have not been yet adapted well to localize software bugs
against bug reports. They could be also data or resource-intensive. To bridge
this gap, we propose, IQLoc, a novel bug localization approach that capitalizes
on the strengths of both IR and LLM-based approaches. In particular, we
leverage the program semantics understanding of transformer-based models to
reason about the suspiciousness of code and reformulate queries during bug
localization using Information Retrieval. To evaluate IQLoc, we refine the
Bench4BL benchmark dataset and extend it by incorporating ~30% more recent bug
reports, resulting in a benchmark containing ~7.5K bug reports. We evaluated
IQLoc using three performance metrics and compare it against four baseline
techniques. Experimental results demonstrate its superiority, achieving up to
58.52% and 60.59% in MAP, 61.49% and 64.58% in MRR, and 69.88% and 100.90% in
HIT@K for the test bug reports with random and time-wise splits, respectively.
Moreover, IQLoc improves MAP by 91.67% for bug reports with stack traces,
72.73% for those that include code elements, and 65.38% for those containing
only descriptions in natural language. By integrating program semantic
understanding into Information Retrieval, IQLoc mitigates several longstanding
challenges of traditional IR-based approaches in bug localization.

</details>


### [270] [DynamiQ: Unlocking the Potential of Dynamic Task Allocation in Parallel Fuzzing](https://arxiv.org/abs/2510.04469)
*Wenqi Yan,Toby Murray,Benjamin Rubinstein,Van-Thuan Pham*

Main category: cs.SE

TL;DR: DynamiQ 是一种动态并行模糊测试工具，通过调用图结构优化任务分配，显著提升效率和漏洞发现能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将单个种子作为任务处理，缺乏对程序结构信息的利用，导致冗余探索和效率低下。DynamiQ 旨在通过动态任务分配和优化提升模糊测试的效率和效果。

Method: DynamiQ 利用程序的调用图结构信息定义任务，并通过运行时反馈持续优化任务分配，减少冗余探索。基于 LibAFL 框架，DynamiQ 实现了任务分配和任务感知模糊测试的多种优化。

Result: 在 OSS-Fuzz 和 FuzzBench 的 12 个真实目标上进行了 25,000 CPU 小时的评估，DynamiQ 在代码覆盖率和漏洞发现方面表现优异，发现了 9 个之前未知的广泛使用且经过大量模糊测试的开源软件漏洞。

Conclusion: DynamiQ 作为 AFLTeam 的优化后继者，通过动态和自适应并行模糊测试，显著提高了模糊测试的效率，并在代码覆盖率和漏洞发现方面优于现有最先进的并行模糊器。

Abstract: We present DynamiQ, a full-fledged and optimized successor to AFLTeam that
supports dynamic and adaptive parallel fuzzing. Unlike most existing approaches
that treat individual seeds as tasks, DynamiQ leverages structural information
from the program's call graph to define tasks and continuously refines task
allocation using runtime feedback. This design significantly reduces redundant
exploration and enhances fuzzing efficiency at scale. Built on top of the
state-of-the-art LibAFL framework, DynamiQ incorporates several practical
optimizations in both task allocation and task-aware fuzzing. Evaluated on 12
real-world targets from OSS-Fuzz and FuzzBench over 25,000 CPU hours, DynamiQ
outperforms state-of-the-art parallel fuzzers in both code coverage and
vulnerability discovery, uncovering 9 previously unknown bugs in widely used
and extensively fuzzed open-source software.

</details>


### [271] [Detecting and Characterizing Low and No Functionality Packages in the NPM Ecosystem](https://arxiv.org/abs/2510.04495)
*Napasorn Tevarut,Brittany Reid,Yutaro Kashiwa,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Hajimu Iida*

Main category: cs.SE

TL;DR: 研究发现npm生态中17.92%的包是trivial包，其安全风险与非trivial包相当，数据包虽少但也有风险。检测工具准确率高，建议加强依赖管理。


<details>
  <summary>Details</summary>
Motivation: npm生态系统中存在大量功能简单的小模块（trivial包），尽管功能简单但仍可能带来安全风险。此外，数据包（不含可执行逻辑）的风险尚未被充分研究。

Method: 采用基于规则的静态分析方法来检测trivial和数据包，并在2025年npm生态系统中评估其普遍性和相关风险。

Result: 分析显示，17.92%的包是trivial包，其漏洞水平与非trivial包相当；数据包虽罕见但也存在风险。检测工具准确率达94%（macro-F1 0.87）。

Conclusion: 研究发现，即便是功能简单的trivial包和数据包也存在安全风险，建议在依赖管理中给予更多关注以减少潜在的技术债务和安全暴露。

Abstract: Trivial packages, small modules with low functionality, are common in the npm
ecosystem and can pose security risks despite their simplicity. This paper
refines existing definitions and introduce data-only packages that contain no
executable logic. A rule-based static analysis method is developed to detect
trivial and data-only packages and evaluate their prevalence and associated
risks in the 2025 npm ecosystem. The analysis shows that 17.92% of packages are
trivial, with vulnerability levels comparable to non-trivial ones, and
data-only packages, though rare, also contain risks. The proposed detection
tool achieves 94% accuracy (macro-F1 0.87), enabling effective large-scale
analysis to reduce security exposure. This findings suggest that trivial and
data-only packages warrant greater attention in dependency management to reduce
potential technical debt and security exposure.

</details>


### [272] [Spec2Control: Automating PLC/DCS Control-Logic Engineering from Natural Language Requirements with LLMs - A Multi-Plant Evaluation](https://arxiv.org/abs/2510.04519)
*Heiko Koziolek,Thilo Braun,Virendra Ashiwal,Sofia Linsbauer,Marthe Ahlgreen Hansen,Karoline Grotterud*

Main category: cs.SE

TL;DR: Spec2Control是一种基于LLM的自动化工作流，直接从自然语言生成DCS图形控制逻辑，实验证明其高效且节省人力。


<details>
  <summary>Details</summary>
Motivation: 分布式控制系统（DCS）的软件编程过程仍主要依赖手动且繁琐，导致高昂成本。现有基于大型语言模型（LLM）的工具在文本符号上提供有限自动化，且未在大型数据集和真实测试案例中验证。

Method: 引入Spec2Control，一种高度自动化的LLM工作流，直接从自然语言用户需求生成图形控制逻辑。

Result: 实验表明，Spec2Control能成功识别控制策略，自主生成98.6%的正确控制策略连接，节省94-96%的人力。

Conclusion: Spec2Control被集成到商业ABB工程工具中，同时也提供开源版本供独立验证。

Abstract: Distributed control systems (DCS) manage the automation for many industrial
production processes (e.g., power plants, chemical refineries, steel mills).
Programming the software for such systems remains a largely manual and tedious
process, incurring costs of millions of dollars for extensive facilities. Large
language models (LLMs) have been found helpful in generating DCS control logic,
resulting in commercial copilot tools. Today, these tools are focused on
textual notations, they provide limited automation, and have not been tested on
large datasets with realistic test cases. We introduce Spec2Control, a highly
automated LLM workflow to generate graphical control logic directly from
natural language user requirements. Experiments using an open dataset with 10
control narratives and 65 complex test cases demonstrate that Spec2Control can
successfully identify control strategies, can generate 98.6% of correct control
strategy connections autonomously, and can save between 94-96% of human labor.
Spec2Control is being integrated into commercial ABB engineering tools, but is
also available as an open-source variant for independent validation.

</details>


### [273] [Advancing Digital Government: Integrating Open Source Software Enablement Indicators in Maturity Indexes](https://arxiv.org/abs/2510.04603)
*Johan Linåker,Sachiko Muto*

Main category: cs.SE

TL;DR: 研究分析了16个数字成熟国家的OSS政策，提出数字政府成熟度指标的潜在指标，强调OSS对公共部门数字化转型的战略价值及制度支持的重要性。


<details>
  <summary>Details</summary>
Motivation: 开源软件（OSS）是现代软件栈的重要组成部分，显著影响GDP和国家技术增长，支持互操作性、主权和透明度。然而，对政府采用OSS的系统性测量仍有限。

Method: 采用定性方法，结合政策文件的案头研究与政府代表的半结构化访谈，生成详细国家报告，并进行交叉分析，聚焦OSS政策推广、理论基础和实施支持。

Result: 促进OSS重用的政策广泛存在，主要针对内外向共享，且主要由中央公共部门组织管理。政策目标包括互操作性、数字主权、透明度和成本效率，安全性被同时视为风险和优势。实施支持来自多级政府的多样化开源项目办公室（OSPOs），促进能力建设、资源共享和可持续项目治理。

Conclusion: OSS是公共部门数字化转型的战略推动者。明确的政策框架与制度支持（如OSPOs）至关重要。国际数字成熟度框架应扩展OSS指标，以更好地指导和评估政府采用及影响。

Abstract: Context: Open Source Software (OSS) is a vital public good, included across
most of modern software stacks, significantly impacting GDP and national tech
growth, while supporting interoperability, sovereignty, and transparency.
However, systematic measurement of governmental OSS adoption remain limited.
  Research Aim: This study contributes to digital government maturity indexes
by analyzing policies and support actions leveraging OSS for software reuse and
collaborative development across 16 digitally mature countries, and proposing
potential indicators for said indexes. It examines OSS policy formation, stated
goals, key actors, and support mechanisms.
  Methodology: A qualitative approach is used combining desk research of policy
documents with semi-structured interviews of government representatives,
producing detailed country reports. These are cross-analyzed, focusing on OSS
policy promotion, rationale, and implementation support.
  Results: Policies facilitating OSS reuse are widespread, targeting both
inbound acquisition and outbound sharing, and are predominantly governed by
central public sector organizations. Policy goals include interoperability,
digital sovereignty, transparency, and cost efficiency, with security framed
both as a risk and strength. Implementation is supported by diverse Open Source
Program Offices (OSPOs) at multiple government levels, which foster capacity
building, resource pooling, and sustainable project governance. Indicators are
synthesized and proposed across 14 areas covering policy incentives and design,
and implementation and support.
  Conclusions: OSS is a strategic enabler for public sector digital
transformation. Clear policy frameworks, coupled with institutional support
such as OSPOs, are essential. International digital maturity frameworks should
expand OSS indicators to better guide and assess government adoption and
impact.

</details>


### [274] [Exploring the Power of Diffusion Large Language Models for Software Engineering: An Empirical Investigation](https://arxiv.org/abs/2510.04605)
*Jingyao Zhang,Tianlin Li,Xiaoyu Zhang,Qiang Hu,Bin Shi*

Main category: cs.SE

TL;DR: DLLMs在软件工程任务中全面超越AR-LLMs，尤其在跨文件修复任务中表现突出，准确率提升113%，且效率更高、延迟更低。


<details>
  <summary>Details</summary>
Motivation: AR-LLMs在软件工程中应用广泛，但在处理代码结构信息时存在局限性，且推理延迟较高。DLLMs因其全局双向编码和解耦生成步骤，被认为是一种有潜力的替代方案。

Method: 本研究对DLLMs在软件开发生命周期中的多个任务（包括代码生成、缺陷检测和程序修复）进行了全面评估，使用了包含52,937个任务的大规模基准测试。

Result: 7B参数的DLLMs在基准测试中平均准确率提升了30%，在跨文件修复任务中表现尤为突出，准确率提升了113%，同时保持了更高的效率和更低的延迟。

Conclusion: DLLMs（扩散大语言模型）在软件工程任务中表现出色，优于AR-LLMs（自回归大语言模型），尤其是在跨文件修复任务中取得了113%的准确率提升，同时保持了更高的效率和更低的延迟。

Abstract: Autoregressive Large Language Models (AR-LLMs) are widely used in software
engineering (SE) but face limitations in processing code structure information
and suffer from high inference latency. Diffusion LLMs (DLLMs) offer a
promising alternative with global bidirectional encoding and decoupled
generation steps. This work presents the first comprehensive evaluation of
DLLMs across the software development lifecycle, including code generation,
defect detection, and program repair. On a large-scale benchmark of 52,937
tasks, 7Bparameter DLLMs outperform AR-LLMs with a 30% average accuracy
improvement achieving a 113% gain on cross-file repair, while maintaining
superior efficiency and reduced latency. Our results establish DLLMs as a
superior paradigm for SE tasks.

</details>


### [275] [A survey on the impact of emotions on the productivity among software developers](https://arxiv.org/abs/2510.04611)
*Pawel Weichbroth,Maciej Lotysz,Michal Wrobel*

Main category: cs.SE

TL;DR: 研究发现开发者情绪状态显著影响其感知生产力，建议通过改善情绪健康提升绩效。


<details>
  <summary>Details</summary>
Motivation: 探讨软件开发中时间压力等因素导致的情绪状态变化是否影响开发者的感知生产力。

Method: 采用两阶段方法：首先通过九位专家验证测量模型，然后对88位软件开发者进行问卷调查，使用偏最小二乘法（PLS）进行数据分析。

Result: 路径分析证实情绪状态对感知生产力有显著正向影响（beta = 0.893, p < 0.001）。

Conclusion: 研究强调了管理和改善开发者情绪健康对提升软件开发环境中生产力的重要性，并建议通过减少倦怠、压力等负面因素的干预措施来提高绩效。

Abstract: The time pressure associated with software development, among other factors,
often leads to a diminished emotional state among developers. However, whether
emotions affect perceived productivity remains an open question. This study
aims to determine the strength and direction of the relationship between
emotional state and perceived productivity among software developers. We
employed a two-stage approach. First, a survey was conducted with a pool of
nine experts to validate the measurement model. Second, a survey was
administered to a pool of 88 software developers to empirically test the
formulated hypothesis by using Partial Least Squares, as the data analysis
method. The results of the path analysis clearly confirm the formulated
hypothesis, showing that the emotional state of a software developer has a
strong positive, and significant impact (beta = 0.893, p < 0.001) on perceived
productivity among software developers. The findings highlight the importance
of managing and improving developers emotional well-being to enhance
productivity in software development environments. Additionally, interventions
aimed at reducing burnout, stress, and other negative factors could have a
considerable impact on their performance outcomes.

</details>


### [276] [Evolaris: A Roadmap to Self-Evolving Software Intelligence Management](https://arxiv.org/abs/2510.04689)
*Chengwei Liu,Wenbo Guo,Yuxin Zhang,Limin Wang,Sen Chen,Lei Bu,Yang Liu*

Main category: cs.SE

TL;DR: Evolaris 是一个自演化的多代理系统，旨在通过非正式渠道及时捕获和分析软件威胁信息，以提升安全响应的精确性和时效性。


<details>
  <summary>Details</summary>
Motivation: 软件威胁的动态性和分布式特性使得通过非正式渠道获取关键威胁信息变得至关重要，但数据源的碎片化和技术难度使得这一过程复杂化。

Method: Evolaris 是一个基于多代理框架的自演化软件智能系统，支持全栈工作流程，代理通过共享上下文独立操作并协调执行任务。

Result: Evolaris 能够从新输入中学习，优化内部知识，并适应新兴威胁模式，从而持续提高软件威胁分析的精确性、及时性和可扩展性。

Conclusion: Evolaris 提供了一个可持续的基础，用于主动安全决策，并加强了对安全威胁理解的更广泛生态系统。

Abstract: In recent years, the landscape of software threats has become significantly
more dynamic and distributed. Security vulnerabilities are no longer discovered
and shared only through formal channels such as public vulnerability databases
or vendor advisories. Increasingly, criti- cal threat information emerges
informally through blogs, social media, developer forums, open source
repositories, and even underground com- munities. To this end, capturing such
intelligence in a timely manner is essential for maintaining situational
awareness and enabling prompt security responses. However, this remains a
complex challenge due to the fragmented nature of data sources and the
technical difficulty of collecting, parsing, mapping, and validating
information at scale. To ad- dress this, we propose Evolaris, a self-evolving
software intelligence sys- tem built on a multi-agent framework. Evolaris is
designed to support a full-stack workflow, where agents operate independently
but coordinate through shared context to perform tasks such as information
discovery, reasoning, gap completion, validation, and risk detection. This
archi- tecture enables the platform to learn from new inputs, refine its
internal knowledge, and adapt to emerging threat patterns over time, which
could continuously improve the precision, timeliness, and scalability of
software threat analysis, and offers a sustainable foundation for proactive
secu- rity decision-making and strengthens the broader ecosystem of security
threat understanding.

</details>


### [277] [An Empirical Study of SOTA RCA Models: From Oversimplified Benchmarks to Realistic Failures](https://arxiv.org/abs/2510.04711)
*Aoyang Fang,Songhan Zhang,Yifan Yang,Haotong Wu,Junjielong Xu,Xuyang Wang,Rui Wang,Manyi Wang,Qisheng Lu,Pinjia He*

Main category: cs.SE

TL;DR: 论文指出现有RCA基准测试过于简化，导致模型性能被高估，开发了更真实的基准测试并揭示SOTA模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 云原生微服务架构的复杂性使得根因分析（RCA）既关键又具挑战性，现有基准测试过于简化，导致模型性能被高估。

Method: 系统分析了流行的RCA基准测试，识别了故障注入、调用图设计和遥测模式的关键限制，开发了自动化框架生成更真实的基准测试，包含1,430个验证过的故障案例。

Result: 在生成的更真实基准测试上重新评估11个SOTA模型，结果显示较低的Top@1准确率（平均0.21，最佳0.37）和显著更长的执行时间。

Conclusion: 现有的RCA基准测试过于简化，未能反映真实世界条件，导致模型性能被高估。通过系统分析并开发自动化框架生成更真实的基准测试，揭示了现有SOTA模型的局限性，并识别了三种常见故障模式。

Abstract: While cloud-native microservice architectures have transformed software
development, their complexity makes Root Cause Analysis (RCA) both crucial and
challenging. Although many data-driven RCA models have been proposed, we find
that existing benchmarks are often oversimplified and fail to capture
real-world conditions. Our preliminary study shows that simple rule-based
methods can match or even outperform state-of-the-art (SOTA) models on four
widely used benchmarks, suggesting performance overestimation due to benchmark
simplicity. To address this, we systematically analyze popular RCA benchmarks
and identify key limitations in fault injection, call graph design, and
telemetry patterns. Based on these insights, we develop an automated framework
to generate more realistic benchmarks, yielding a dataset of 1,430 validated
failure cases from 9,152 injections, covering 25 fault types under dynamic
workloads with hierarchical ground-truth labels and verified SLI impact.
Re-evaluation of 11 SOTA models on this dataset shows low Top@1 accuracy
(average 0.21, best 0.37) and significantly longer execution times. Our
analysis highlights three common failure patterns: scalability issues,
observability blind spots, and modeling bottlenecks.

</details>


### [278] [Agile Software Effort Estimation using Regression Techniques](https://arxiv.org/abs/2510.04760)
*Sisay Deresa Sima,Ayalew Belay Habtie*

Main category: cs.SE

TL;DR: 该研究通过LASSO和Elastic Net回归技术开发了一个基于故事点的敏捷工作量估算模型，实验证明LASSO回归在预测性能上更优。


<details>
  <summary>Details</summary>
Motivation: 软件工作量估算是软件开发过程中的关键环节，准确的估算直接影响项目的成败。目前敏捷工作量估算领域仍需进一步研究。

Method: 使用LASSO和Elastic Net回归技术，结合默认参数和网格搜索调优的5折交叉验证，对21个软件项目进行实验。

Result: LASSO回归在各项指标上均优于Elastic Net，具体表现为PRED(8%)和PRED(25%)为100.0，MMRE为0.0491，MMER为0.0551，MdMRE为0.0593，MdMER为0.063，MSE为0.0007。

Conclusion: LASSO回归在软件项目敏捷工作量估算中表现出更优的预测性能，其PRED(8%)和PRED(25%)结果均为100.0，且其他误差指标如MMRE、MMER、MdMRE、MdMER和MSE均较低。

Abstract: Software development effort estimation is one of the most critical aspect in
software development process, as the success or failure of the entire project
depends on the accuracy of estimations. Researchers are still conducting
studies on agile effort estimation. The aim of this research is to develop a
story point based agile effort estimation model using LASSO and Elastic Net
regression techniques. The experimental work is applied to the agile story
point approach using 21 software projects collected from six firms. The two
algorithms are trained using their default parameters and tuned grid search
with 5-fold cross-validation to get an enhanced model. The experiment result
shows LASSO regression achieved better predictive performance PRED (8%) and
PRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593,
MdMER of 0.063, and MSE of 0.0007. The results are also compared with other
related literature.

</details>


### [279] [GUISpector: An MLLM Agent Framework for Automated Verification of Natural Language Requirements in GUI Prototypes](https://arxiv.org/abs/2510.04791)
*Kristian Kolthoff,Felix Kretzer,Simone Paolo Ponzetto,Alexander Maedche,Christian Bartelt*

Main category: cs.SE

TL;DR: GUISpector是一个利用多模态(M)LLM代理自动验证GUI原型中自然语言需求的新框架，提供可操作反馈并支持自动化开发流程。


<details>
  <summary>Details</summary>
Motivation: 现有GUI测试方法难以应对现代界面的复杂性，且缺乏可操作反馈和与自动化开发代理的有效集成。

Method: GUISpector采用多模态(M)LLM代理解释和操作化自然语言需求，自主规划和执行GUI应用的验证轨迹，并系统提取详细自然语言反馈。

Result: 在基于900个验收标准注释的150项需求上评估，GUISpector展示了有效检测需求满足与违规的能力。

Conclusion: GUISpector框架通过多模态(M)LLM代理自动验证GUI原型中的自然语言需求，有效检测需求满足与违规，并展示了将可操作反馈无缝集成到自动化LLM驱动开发流程中的潜力。

Abstract: GUIs are foundational to interactive systems and play a pivotal role in early
requirements elicitation through prototyping. Ensuring that GUI implementations
fulfill NL requirements is essential for robust software engineering,
especially as LLM-driven programming agents become increasingly integrated into
development workflows. Existing GUI testing approaches, whether traditional or
LLM-driven, often fall short in handling the complexity of modern interfaces,
and typically lack actionable feedback and effective integration with automated
development agents. In this paper, we introduce GUISpector, a novel framework
that leverages a multi-modal (M)LLM-based agent for the automated verification
of NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to
interpret and operationalize NL requirements, enabling to autonomously plan and
execute verification trajectories across GUI applications. Second, GUISpector
systematically extracts detailed NL feedback from the agent's verification
process, providing developers with actionable insights that can be used to
iteratively refine the GUI artifact or directly inform LLM-based code
generation in a closed feedback loop. Third, we present an integrated tool that
unifies these capabilities, offering practitioners an accessible interface for
supervising verification runs, inspecting agent rationales and managing the
end-to-end requirements verification process. We evaluated GUISpector on a
comprehensive set of 150 requirements based on 900 acceptance criteria
annotations across diverse GUI applications, demonstrating effective detection
of requirement satisfaction and violations and highlighting its potential for
seamless integration of actionable feedback into automated LLM-driven
development workflows. The video presentation of GUISpector is available at:
https://youtu.be/JByYF6BNQeE, showcasing its main capabilities.

</details>


### [280] [RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across Git Platforms](https://arxiv.org/abs/2510.04796)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: RevMine是一个利用LLMs简化代码审查挖掘流程的工具，降低技术门槛，支持更广泛的实证研究。


<details>
  <summary>Details</summary>
Motivation: 当前收集和分析代码审查数据的过程耗时且技术密集，大多数研究者需要编写临时脚本来完成这些任务。

Method: RevMine利用大型语言模型（LLMs）简化代码审查挖掘流程，包括认证、端点发现和自然语言驱动的数据收集。

Result: RevMine显著减少了对手动脚本的需求，并支持基于用户定义过滤器或LLM推断模式的定量和定性分析。

Conclusion: RevMine通过降低技术门槛，旨在使代码审查挖掘民主化，并支持更广泛的实证软件工程研究。

Abstract: Empirical research on code review processes is increasingly central to
understanding software quality and collaboration. However, collecting and
analyzing review data remains a time-consuming and technically intensive task.
Most researchers follow similar workflows - writing ad hoc scripts to extract,
filter, and analyze review data from platforms like GitHub and GitLab. This
paper introduces RevMine, a conceptual tool that streamlines the entire code
review mining pipeline using large language models (LLMs). RevMine guides users
through authentication, endpoint discovery, and natural language-driven data
collection, significantly reducing the need for manual scripting. After
retrieving review data, it supports both quantitative and qualitative analysis
based on user-defined filters or LLM-inferred patterns. This poster outlines
the tool's architecture, use cases, and research potential. By lowering the
barrier to entry, RevMine aims to democratize code review mining and enable a
broader range of empirical software engineering studies.

</details>


### [281] [InsightQL: Advancing Human-Assisted Fuzzing with a Unified Code Database and Parameterized Query Interface](https://arxiv.org/abs/2510.04835)
*Wentao Gao,Renata Borovica-Gajic,Sang Kil Cha,Tian Qiu,Van-Thuan Pham*

Main category: cs.SE

TL;DR: InsightQL是一个辅助分析模糊测试阻塞点的框架，通过数据库和查询接口提升分析效率，实验证明其能显著提高代码覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有的模糊测试技术在面对覆盖率瓶颈时，依赖人工分析效率低下，因此需要一种自动化辅助工具来提升分析效率。

Method: InsightQL通过统一数据库和直观的参数化查询接口，系统化地提取分析模糊测试中的阻塞点。

Result: 在FuzzBench基准测试的14个流行库上，InsightQL成功解除了多个模糊阻塞点，代码覆盖率最高提升了13.90%。

Conclusion: InsightQL作为一种辅助框架，有效帮助开发者分析和解决模糊测试中的阻塞问题，显著提升了代码覆盖率。

Abstract: Fuzzing is a highly effective automated testing method for uncovering
software vulnerabilities. Despite advances in fuzzing techniques, such as
coverage-guided greybox fuzzing, many fuzzers struggle with coverage plateaus
caused by fuzz blockers, limiting their ability to find deeper vulnerabilities.
Human expertise can address these challenges, but analyzing fuzzing results to
guide this support remains labor-intensive. To tackle this, we introduce
InsightQL, the first human-assisting framework for fuzz blocker analysis.
Powered by a unified database and an intuitive parameterized query interface,
InsightQL aids developers in systematically extracting insights and efficiently
unblocking fuzz blockers. Our experiments on 14 popular real-world libraries
from the FuzzBench benchmark demonstrate the effectiveness of InsightQL,
leading to the unblocking of many fuzz blockers and considerable improvements
in code coverage (up to 13.90%).

</details>


### [282] [FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration](https://arxiv.org/abs/2510.04852)
*Victor May,Diganta Misra,Yanqi Luo,Anjali Sridhar,Justine Gehring,Silvio Soares Ribeiro Junior*

Main category: cs.SE

TL;DR: FreshBrew是一个用于评估AI代理在Java项目迁移中表现的新基准，Gemini 2.5 Flash表现最佳，迁移成功率达52.3%。


<details>
  <summary>Details</summary>
Motivation: 评估AI驱动的代理框架在项目级Java迁移中的有效性，特别是测量其保留程序语义和避免奖励黑客的能力。

Method: 引入FreshBrew基准，对多个最先进的LLM进行基准测试，并与传统基于规则的工具进行性能比较。

Result: 表现最佳的模型Gemini 2.5 Flash成功将52.3%的项目迁移至JDK 17。

Conclusion: 通过发布FreshBrew基准，旨在促进严谨、可重复的评估，并推动AI驱动代码库现代化的进展。

Abstract: AI coding assistants are rapidly becoming integral to modern software
development. A key challenge in this space is the continual need to migrate and
modernize codebases in response to evolving software ecosystems. Traditionally,
such migrations have relied on rule-based systems and human intervention. With
the advent of powerful large language models (LLMs), AI-driven agentic
frameworks offer a promising alternative-but their effectiveness has not been
systematically evaluated. In this paper, we introduce FreshBrew, a novel
benchmark for evaluating AI agents on project-level Java migrations, with a
specific focus on measuring an agent's ability to preserve program semantics
and avoid reward hacking, which we argue requires projects with high test
coverage for a rigorous and reliable evaluation. We benchmark several
state-of-the-art LLMs, and compare their performance against established
rule-based tools. Our evaluation of AI agents on this benchmark of 228
repositories shows that the top-performing model, Gemini 2.5 Flash, can
successfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis
reveals novel insights into the critical strengths and limitations of current
agentic approaches, offering actionable insights into their real-world
applicability. Our empirical study reveals failure modes of current AI agents
in realistic Java modernization tasks, providing a foundation for evaluating
trustworthy code-migration systems. By releasing FreshBrew, we aim to
facilitate rigorous, reproducible evaluation and catalyze progress in AI-driven
codebase modernization.

</details>


### [283] [Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches](https://arxiv.org/abs/2510.04905)
*Yicheng Tao,Yao Qin,Yepang Liu*

Main category: cs.SE

TL;DR: 本文综述了检索增强代码生成（RACG）的研究，重点探讨了仓库级别的方法，分类了现有工作，总结了数据集和基准，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管函数级别和文件级别的代码生成已取得显著成果，但现实中的软件开发通常需要跨整个仓库进行推理。这引发了仓库级别代码生成（RLCG）的挑战性任务，要求模型能够捕获长距离依赖关系、确保全局语义一致性并生成跨多个文件或模块的连贯代码。

Method: 本文对检索增强代码生成（RACG）的研究进行了全面回顾，特别是在仓库级别的方法上。文章对现有工作进行了多个维度的分类，包括生成策略、检索方式、模型架构、训练范式和评估协议。

Result: 文章总结了广泛使用的数据集和基准，分析了当前的局限性，并概述了未来研究的关键挑战和机遇。

Conclusion: 本文旨在为快速发展的检索增强代码生成领域建立一个统一的分析框架，并激发人工智能驱动的软件工程的持续进步。

Abstract: Recent advancements in large language models (LLMs) have substantially
improved automated code generation. While function-level and file-level
generation have achieved promising results, real-world software development
typically requires reasoning across entire repositories. This gives rise to the
challenging task of Repository-Level Code Generation (RLCG), where models must
capture long-range dependencies, ensure global semantic consistency, and
generate coherent code spanning multiple files or modules. To address these
challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm that integrates external retrieval mechanisms with LLMs, enhancing
context-awareness and scalability. In this survey, we provide a comprehensive
review of research on Retrieval-Augmented Code Generation (RACG), with an
emphasis on repository-level approaches. We categorize existing work along
several dimensions, including generation strategies, retrieval modalities,
model architectures, training paradigms, and evaluation protocols. Furthermore,
we summarize widely used datasets and benchmarks, analyze current limitations,
and outline key challenges and opportunities for future research. Our goal is
to establish a unified analytical framework for understanding this rapidly
evolving field and to inspire continued progress in AI-powered software
engineering.

</details>


### [284] [Why Software Signing (Still) Matters: Trust Boundaries in the Software Supply Chain](https://arxiv.org/abs/2510.04964)
*Kelechi G. Kalu,James C. Davis*

Main category: cs.SE

TL;DR: 研究发现，签名在不同软件分发边界中仍必不可少，即使注册表安全，签名也能增强供应链保障。


<details>
  <summary>Details</summary>
Motivation: 探讨在集中式注册表（如PyPI、npm等）时代，强化注册表安全控制是否消除了端到端工件签名的必要性。

Method: 综合历史实践并提出了一个信任模型，用于识别在不同分发模式下何时需要签名以扩展信任。

Result: 签名在跨越不同软件分发边界（如镜像、企业代理等）时，其核心保证（来源、完整性和责任）不会自动传递。

Conclusion: 签名作为防御的基础层，即使在注册表安全的情况下也能增强软件供应链的保障。

Abstract: Software signing provides a formal mechanism for provenance by ensuring
artifact integrity and verifying producer identity. It also imposes tooling and
operational costs to implement in practice. In an era of centralized registries
such as PyPI, npm, Maven Central, and Hugging Face, it is reasonable to ask
whether hardening registry security controls obviates the need for end-to-end
artifact signing. In this work, we posit that the core guarantees of signing,
provenance, integrity, and accountability are not automatically carried across
different software distribution boundaries. These boundaries include mirrors,
corporate proxies, re-hosting, and air-gapped transfers, where registry
security controls alone cannot provide sufficient assurance. We synthesize
historical practice and present a trust model for modern distribution modes to
identify when signing is necessary to extend trust beyond registry control.
Treating signing as a baseline layer of defense strengthens software supply
chain assurance even when registries are secure.

</details>


### [285] [Quantum Computing as a Service - a Software Engineering Perspective](https://arxiv.org/abs/2510.04982)
*Aakash Ahmad,Muhammad Waseem,Bakheet Aljedaani,Mahdi Fahmideh,Peng Liang,Feras Awaysheh*

Main category: cs.SE

TL;DR: 本文通过系统性映射和架构开发，提出了量子计算即服务（QCaaS）的4阶段开发生命周期和参考架构，为量子服务工程提供了软件工程视角。


<details>
  <summary>Details</summary>
Motivation: 量子计算作为一种颠覆性技术，其即服务模式（QCaaS）可以为不具备量子计算机资源的个人和组织提供实用计算服务。

Method: 采用了两阶段研究方法，包括系统性映射研究（SMS）和基于架构的开发，以识别量子服务开发生命周期的阶段并整合到参考架构中。

Result: 研究确定了41项同行评审研究，提出了一个包含4阶段开发生命周期的参考架构，涵盖了量子重要需求（QSRs）、建模符号、模式目录、编程语言和部署平台。

Conclusion: 本研究提出了一种以过程为中心、架构驱动的方法，为量子计算即服务（QCaaS）提供了软件工程的视角，包括一个4阶段的开发生命周期和分层参考架构。

Abstract: Quantum systems have started to emerge as a disruptive technology and
enabling platforms - exploiting the principles of quantum mechanics via
programmable quantum bits (QuBits) - to achieve quantum supremacy in computing.
Academic research, industrial projects (e.g., Amazon Braket, IBM Qiskit), and
consortiums like 'Quantum Flagship' are striving to develop practically capable
and commercially viable quantum computing (QC) systems and technologies.
Quantum Computing as a Service (QCaaS) is viewed as a solution attuned to the
philosophy of service-orientation that can offer QC resources and platforms, as
utility computing, to individuals and organisations who do not own quantum
computers. This research investigates a process-centric and architecture-driven
approach to offer a software engineering perspective on enabling QCaaS - a.k.a
quantum service-orientation. We employed a two-phase research method comprising
(a) a systematic mapping study and (b) an architecture-based development, first
to identify the phases of the quantum service development life cycle and
subsequently to integrate these phases into a reference architecture that
supports QCaaS. The SMS process retrieved a collection of potentially relevant
research literature and based on a multi-step selection and qualitative
assessment, we selected 41 peer-reviewed studies to answer three RQs. The RQs
investigate (i) demographic details in terms of frequency, types, and trends of
research, (ii) phases of quantum service development lifecycle to derive a
reference architecture for conception, modeling, assembly, and deployment of
services, and (iii) The results identify a 4-phased development lifecycle along
with quantum significant requirements (QSRs), various modeling notations,
catalogue of patterns, programming languages, and deployment platforms that can
be integrated in a layered reference architecture to engineer QCaaS.

</details>


### [286] [AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis](https://arxiv.org/abs/2510.04997)
*Jiongchi Yu,Weipeng Jiang,Xiaoyu Zhang,Qiang Hu,Xiaofei Xie,Chao Shen*

Main category: cs.SE

TL;DR: 本文探索了LLMs在软件故障分析中的应用，显著提高了效率，并提出了未来研究的潜在方向和挑战。


<details>
  <summary>Details</summary>
Motivation: 传统故障分析虽然有价值，但涉及多个专家驱动的步骤，耗时且劳动密集，阻碍了大规模故障研究的开展和迭代实证研究的进度。

Method: 将实证软件故障研究过程分解为三个关键阶段：（1）研究目标定义，（2）数据准备，（3）故障分析，并探索了应用大型语言模型（LLMs）进行开源软件故障分析的初步研究。

Result: 评估了来自高质量实证研究的3,829个软件故障，结果显示LLMs可以显著提高故障分析效率，平均处理时间约为两小时，而传统手动分析通常需要数周。

Conclusion: 本文提出了一个详细的研究计划，强调了LLMs在推进实证故障研究中的潜力，以及实现完全自动化端到端软件故障分析所需解决的开放性挑战。

Abstract: Understanding software faults is essential for empirical research in software
development and maintenance. However, traditional fault analysis, while
valuable, typically involves multiple expert-driven steps such as collecting
potential faults, filtering, and manual investigation. These processes are both
labor-intensive and time-consuming, creating bottlenecks that hinder
large-scale fault studies in complex yet critical software systems and slow the
pace of iterative empirical research.
  In this paper, we decompose the process of empirical software fault study
into three key phases: (1) research objective definition, (2) data preparation,
and (3) fault analysis, and we conduct an initial exploration study of applying
Large Language Models (LLMs) for fault analysis of open-source software.
Specifically, we perform the evaluation on 3,829 software faults drawn from a
high-quality empirical study. Our results show that LLMs can substantially
improve efficiency in fault analysis, with an average processing time of about
two hours, compared to the weeks of manual effort typically required. We
conclude by outlining a detailed research plan that highlights both the
potential of LLMs for advancing empirical fault studies and the open challenges
that required be addressed to achieve fully automated, end-to-end software
fault analysis.

</details>
